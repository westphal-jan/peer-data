{"id": "1212.5091", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2012", "title": "Maximally Informative Observables and Categorical Perception", "abstract": "We formulate the problem of perception in the framework of information theory, and prove that categorical perception is equivalent to the existence of an observable that has the maximum possible information on the target of perception. We call such an observable maximally informative. Regardless whether categorical perception is real, maximally informative observables can form the basis of a theory of perception. We conclude with the implications of such a theory for the problem of speech perception.\n\n\n\nThe question is whether the use of the term \"truth\" as the name for an observable object can make a difference between the value of truth and the value of truth. In this regard, it is necessary to think of \"truth\" as a means to explain a claim about truth.\nThe principle underlying the theory is that the value of truth is to be determined on a general level using the knowledge of an observable object that is not true. If a claim about truth is true (i.e., a certain number of facts) it is because of a number of facts or things. If a claim about truth is false, it is because of a certain number of facts or things. If a claim about truth is false, it is because of a certain number of facts or things. The claim that the truth of an observable object, its physical form, the physical type, the mathematical type, the mathematical type, the mathematical type, and the mathematical type can be understood as valid. This view is not universal. However, the truth of the physical form cannot be explained by a fundamental rule about the form of truth.\nThe basic definition of truth is that the essence of truth is the absolute truth of an observable object, not the absolute truth of an observable object. This is what the concept of truth is for. The basic definition of truth is that the physical form of truth is the absolute truth of an observable object, not the absolute truth of an observable object. The physical form of truth is that of the physical form of truth is the absolute truth of an observable object.\nIn this sense, the definition of truth as the absolute truth of an observable object is not a fundamental rule about the form of truth that the concept of truth is. However, the fundamental definition of truth as the absolute truth of an observable object is not a fundamental rule about the form of truth that the concept of truth is. However, the fundamental definition of truth is that the physical form of truth is the absolute truth of an observable object.\nIn conclusion, we can use the term \"truth\" as the term", "histories": [["v1", "Wed, 19 Dec 2012 17:40:07 GMT  (232kb)", "http://arxiv.org/abs/1212.5091v1", "9 pages, 1 figure"]], "COMMENTS": "9 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["elaine tsiang"], "accepted": false, "id": "1212.5091"}, "pdf": {"name": "1212.5091.pdf", "metadata": {"source": "META", "title": "Maximally Informative Observables and Categorical Perception", "authors": ["Elaine Tsiang"], "emails": [], "sections": [{"heading": null, "text": "* Monowave Corporation, Seattle, WA., USA\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License.\n1 of 9\nCategorical Perception The term was coined by Liberman et al[Liberman1] to highlight the discovery that an observable of certain speech gestures producing plosive consonants, that of the change in frequency of the second formant, although a continuous variable, leads to the acoustic signals being perceived as one of the three categories of plosive consonants employed in their experiment. The identification function, namely the distribution of the plosive consonants over the variable formant frequency change was close to three indicator functions, one for each plosive (Figure 11).\nThis was startling because it seems to imply that our senses, with which we are supposed to assess the real world, are heavily influenced, or even determined, by our mental constructs.\nCategorical perception has since been observed in other sense modalities in humans, and also in animals. On the other hand, the degree to which the perception is unconditionally categorical has been disputed[Schouten1]. Early explanations attribute the indicator-function-like distribution to the unique psychology of speech. This attribution is unsatisfactory in view of the pervasive and uncertain findings.\nEarly work relied on subjects' report of their responses, using synthesized speech with relatively coarse sampling resolution of the random variable at issue. Current work[Chang1] measures neural responses directly with cortical surface arrays, with higher sampling resolution and more complex stimuli. Despite all the controversy, categorical perception is undeniably part of how we perceive the world.\nWe will argue in the following that categorical perception is, far from being hallucinating reality, an efficacious, in fact, the best possible way to observe the real world2.\n1 Original diagram by Lieberman, reprinted in [Goldstone1]. 2 A more precise statement would be: regardless whether what we call \u201creality\u201d is itself hallucination, within the limits of\nour considerations, categorical perception is not an independent, ad hoc way of hallucinating.\n2 of 9\nObservations vs. Measurements We assume the conventional meaning of a measurement, but distinguish it from an observation in that a measurement directly, or physically, assesses some variable, but an observation assesses a variable as indicative of another variable. So by definition, an observation involves at least two variables, one directly assessed, or measured, and one assessed by inference, or computation.\nObservables We formulate all variables that we assess as random variables. For example, the acoustic signal at its arrival at the human ear is a random variable, that of the varying pressure of air when it is compressed or rarefied.\nWe may analyze this random variable in various ways. The result may be a multitude of random variables. In our example of the acoustic signal, we may perform some sort of frequency analysis. The result would be a 2-dimensional array of random variables, one at each chosen frequency and instant of time over a specified period of time. Such a collection of random variables, all taking on values from the same value space, is called a random field. The example of the varying air pressure is a degenerate case where the field is a singleton. We can subject the first random field to alternative analysis, or we can subject the derived random field to further analysis, to generate other random fields. We call these random fields observables.\nWe define a system of observation as a finite set of such observables. We define perception as a special subset of such systems of observation in which there are target observables. A target observable is a 1-dimensional random field, where the value space is a finite discrete set. It is a target in the sense that the objective of the perception is the determination of this random field. We will sometimes call them targets, for short. The one dimension is usually time. A simple example might be a random variable of two elements, one is represented by the phrase \u201cthere is a cheetah\u201d, and one by the phrase \u201cthere is no cheetah\u201d. A more complex example is a spoken language, where the target random variable is the repertory of gestures of articulation.\nIn principle, we can ascertain the probability distribution of each observable, except the target, by measurements and computations. Perception as a problem for scientific inquiry is solving the as-yet unknown computations for inferring the target observable. In any experiment, or ensembles of repeated observations, we mandate the distribution of the target in accordance with the design of the experiment, or it is dictated by the circumstances requiring the observations. For speech perception, we do so by the choice of the articulatory gestures performed, or the playback of recordings of such performances.\nMutual Information Let the random field A represent the target, and \u2126 be one of the observables derivable from some measurement related to the target. Each set of values of a random field is called a configuration. We will assume that the random variables of the random fields A and \u2126 may be indexed by a finite set of integers respectively3. The notation is that by default, we use the upper case font to represent a set, and the lower case font a corresponding member of the set. We will explicitly declare a symbol when we need to disambiguate.\n3 This does not restrict the dimensionality of the domain of the random fields.\n3 of 9\nA configuration of \u2126 is the set of instances of the random variables:\n{... ,\u03c91 ,\u03c92 , ... ,\u03c9k , ...} where the subscript k stands for a member of some indexing set of integers, K. For notational parsimony, we will use the more convenient \u03c9K as equivalent. Likewise, for the configuration space of A, \u03b1J . We will denote the corresponding configuration space, which consists of the product space of the value spaces of the random variable at every index in the domain of the random field as \u03a9K and \u0391K .\nWe regard a random variable as \u201ccontinuous\u201d if all probability distributions and arithmetic operations remain well defined for any sampling resolution of interest. For perception, there is always a finite upper limit to the sampling resolution.\nThe probability distribution of a random field is over the configuration space into the real-valued interval [0,1]:\n\u03a9 K \u2192[0,1] : p(\u03c9 K ) ,\n\u0391 J \u2192[0,1] : p (\u03b1 J ) .\nUse of the same letter 'p' is again merely notational convenience, and does not imply that the distributions are the same function.\nWe regard a random variable as \u201ccontinuous\u201d if all probability distributions and arithmetic operations remain well defined for any sampling resolution of interest. For perception, there is a finite upper limit to the sampling resolution.\nThe random field of the joint observation of the random fields A and \u2126 is denoted A\u2126. The configuration space of A\u2126 is just the product space of the configuration spaces of A and \u2126:\n\u03b1 J \u03c9 K ={... ,\u03b11 ,\u03b12 ,... ,\u03b1 j ,... ,\u03c91 ,\u03c92 , ... ,\u03c9k ,... } .\nIn the context of the joint random field, p(\u03c9K ) and p(\u03b1J ) now denote their marginal distributions:\np(\u03c9 K )= \u2211\n\u03b1 J \u2208\u0391 J\np (\u03b1 J \u03c9 K ) ,\nwhere the summation is over the configuration space \u0391J ; and likewise :\np(\u03b1 J )= \u2211\n\u03c9 K \u2208\u03a9 K\np(\u03b1 J \u03c9 K ) .\nWe may now write the entropy of A and \u2126 as\n\u0395(\u0391)= \u2211 \u03b1 J \u2208\u0391 J p(\u03b1 J ) ln p(\u03b1 J ) ,\n\u0395(\u03a9)= \u2211 \u03c9 K \u2208\u03a9 K p(\u03c9 K ) ln p (\u03c9 K ) .\nFor the joint random field A\u2126,\n4 of 9\n\u0395(\u0391\u03a9)= \u2211 \u03b1 J \u03c9 K \u2208\u0391 J \u03a9 K p(\u03b1 J \u03c9 K ) ln p(\u03b1 J \u03c9 K ) . (1)\nWe will interpret the entropy as quantifying the amount of variability in the configurations of a random field. That the entropy is numerically less than or equal to 0 is regarded as a convention, meaning that if it is re-defined to be positive definite, all statements retain their truth values. But interpretively, its numerically negative definiteness suggests the state of indeterminacy prior to any observation when we have a deficit of information. When we reach a state of certainty about A, our deficit of information about A is canceled out as a result of gaining an amount of information\nI (\u0391) , such that\n\u0395(\u0391)+I (\u0391)=0 . (2)\nThe amount of variability in the jointly observed A\u2126 together may be less than the sum of the variability in A or \u2126 separately, if certain configurations of \u2126 tend to occur together with certain configurations of A; in other words, if A and \u2126 are correlated.\nThe problem of perception becomes: what can we infer about the configurations the random field A may take on, when we have observed that the random field \u2126 has the configuration \u03c9K ? Let us first denote the subspace of \u0391J given \u03c9 K\n\u0391 J \u2223\u03c9 K .\nThe distribution of A given a particular \u03c9K is called the conditional distribution of A with respect to\u03c9 K :\np(\u03b1 J \u2223\u03c9 K )=\np(\u03b1 J \u03c9 K )\np(\u03c9 K )\n. (3)\nSo we can define the remaining deficit in information about A, or the residual entropy of A given \u03c9 K in terms of this conditional distribution:\n\u0395(\u0391\u2223\u03c9 K )= \u2211\n\u03b1J\u2208\u0391J \u2223\u03c9K\np(\u03b1 J \u2223\u03c9 K ) ln p(\u03b1 J \u2223\u03c9 K ) .\n(4)\nBy observing , \u03c9K we have not reached certainty about A, but we have gained an amount of information, I (\u0391\u2223\u03c9\nK ) , such that the remaining deficit in information about A is \u0395(\u0391\u2223\u03c9 K ) :\n\u0395(\u0391)+I (\u0391\u2223\u03c9 K )=\u0395(\u0391\u2223\u03c9 K ) . (5)\nIn principle, we can arrange a series of experiments, in which we record the instances of the joint occurrence of \u03b1 J \u03c9 K for all possible configurations of \u2126, assuming that we can either directly perform \u03b1J , or be informed by some oracle (human). The expected gain in information, which we denote by I (\u0391\u2223\u03a9) , over such a series of experiments would be\nI (\u0391\u2223\u03a9)= \u2211 \u03c9 K \u2208\u03a9 K p (\u03c9 K ) I (\u0391\u2223\u03c9 K ) . (6)\n5 of 9\nFrom (4) and (5),\nI (\u0391\u2223\u03a9)=\u0395(\u0391\u2223\u03a9)\u2212\u0395(\u0391) , (7)\nwhere\n\u0395(\u0391\u2223\u03a9)= \u2211 \u03c9 K \u2208\u03a9 K p (\u03c9 K )\u0395(\u0391\u2223\u03c9 K ) .\n(8)\nis the expected residual entropy about A when we observe \u2126. Further, from (1), (2) and (3),\nI (\u0391\u2223\u03a9)=\u0395(\u0391\u03a9)\u2212\u0395(\u03a9)\u2212\u0395(\u0391) .\nThe expected gain in information of A by observing \u2126 was termed the correlation information [Everett1], but is now called mutual information[Wikipedia1]. The non-negativity of the mutual information can be proven in general[Everett1].\nIf there is no tendency for certain configurations of A to occur with certain configurations of \u2126, or A is independent of \u2126, then we should expect to gain zero information, a well known result.\nOtherwise, if A is correlated with \u2126 to any degree, we would expect to gain some information. Furthermore, the mutual information may not exceed the lesser of the information that can be gained about A or \u2126 separately. This is because the entropy of a joint random field is always less than or equal to the entropy of each random field separately4\n\u0395(\u0391\u03a9)\u2264\u0395(\u03a9) , \u0395(\u0391\u03a9)\u2264\u0395(\u0391) .\nFor perception, the target A, as a finite discrete random field, has less variability than \u2126, and therefore\nI (\u0391\u2223\u03a9)\u2264\u2212\u0395(\u0391) .\nIn other words, the maximum amount of information we can expect to gain about A from \u2126 cannot cancel out more than the entropy of A. In the case of maximal I (\u0391\u2223\u03a9) ,\nI (\u0391\u2223\u03a9)=\u2212\u0395(\u0391) .\nBy (1), the maximum mutual information is equal to the information that can ever be gained about A,\nI (\u0391\u2223\u03a9)=I (\u0391) .\nAnd by (7), the expected residual entropy of A is 0\n\u0395(\u0391\u2223\u03a9)=0 . (9)\n4 Regard the joint random field as a fine-graining of its separate random fields. The entropy monotonically decreases under fine-graining [Everett1].\n6 of 9\nMaximally Informative Observables We will now show that (9) is true if and only if the conditional distribution p(\u03b1\nJ \u2223\u03c9 K ) assumes the\nvalues 0 or 1. Expanding (9) with (4),\n\u0395(\u0391\u2223\u03a9)= \u2211 \u03c9 K \u2208\u03a9 K \u2211 \u03b1 J \u2208\u0391 J \u2223\u03c9 K p (\u03c9 K ) p(\u03b1 J \u2223\u03c9 K ) ln p (\u03b1 J \u2223\u03c9 K ) . (10)\nEach term in this sum is \u22640 . Therefore the sum = 0 if and only if each term = 0. This means that except for a subset of measure 0 ( p(\u03c9\nK ) = 0 ), either p(\u03b1 J \u2223\u03c9 K )=0 , or ln p(\u03b1 J \u2223\u03c9 K )=0 , in which\ncase p(\u03b1 J \u2223\u03c9 K )=1 .\nThis means upon observing any configuration of \u2126, we can conclude that there is one and only one particular configuration A can assume. We say the random field \u2126 is maximally informative about the random field A, or \u2126 is a maximally informative observable(MIO) and the target A is maximally observable with respect to \u2126.\nIn addition, let us postulate that the configuration space \u03a9K allows us to define neighborhoods in it and that the conditional distribution p(\u03b1\nJ \u2223\u03c9 K ) is a piecewise continuous function of \u03a9 K , all\nreasonable assumptions for the problem of perception. Then for each configuration \u03b1 J , there exists in the configuration space \u03a9\nK a neighborhood \u03bd(o K \u2223\u03b1 J ) of some o K 5 such that\np(\u03b1 J \u2223\u03c9 K )={ 1 ,\u2200\u03c9K \u2208\u03bd(oK\u2223\u03b1J )0, otherwise } . (11)\nThis is the experimentally observed identification function of categorical perception.\nModel Distributions The most important property of a maximally informative observable is that the indicator form of\np(\u03b1 J \u2223\u03c9 K ) is independent of the marginal distribution of A, or of \u2126. By definition (3), the\nconditional distribution of \u2126 given A is given by\np(\u03c9 K \u2223\u03b1 J )={ p(\u03c9K )p(\u03b1J ) ,\u2200\u03c9K\u2208\u03bd(oK\u2223\u03b1J )0, otherwise } . (12)\nInterpretively, these are the \u201cmodels\u201d of the configurations of A in terms of what configurations of \u2126 each configuration of A tends to give rise to. We find that the support of the distribution of \u2126 for\n5 In general, there could be more than one such neighborhood for each configuration of A, not necessarily connected. All statements we make here about the one neighborhood apply to all such neighborhoods without qualifications.\n7 of 9\neach configuration \u03b1 J is restricted to \u03bd(o K \u2223\u03b1 J ) , is distinct from that of any other configuration and that the distribution is proportional to the marginal distribution p(\u03c9 K ) , the factor of proportionality being the inverse of the marginal probability p(\u03b1 J ) . Regardless of the target observable's distribution, the model distributions are of the same form. Summing over both sides of (12) :\n\u2211 \u03c9 K \u2208\u03bd(o K \u2223\u03b1 J ) p(\u03c9 K ) = p(\u03b1 J ) . (13)\nThus varying the target distribution serves only to vary the relative occurrences of the configurations in the distinct regions of support by the same proportions. This makes the physical system comprising \u2126 and A eminently suitable for use in communication. For speech, the distribution of A may be arbitrary per ensemble of speech gestures, such as a particular language, or even a particular practical application.\nFurther, (13) implies that the conditional distribution p(\u03b1 J \u2223\u03c9 K ) has induced a partition of the configuration space \u03a9K by the configuration space \u0391J such that the probability for the occurrence of any configuration in each partition is equal to the probability of the corresponding configuration of A, thus A is a coarse-graining of \u2126 .\nSub-Maximally Informative Observables It is easy to see that a somewhat less \u201cperfect\u201d coarse-graining of \u2126 can be achieved with desirable properties similar to (11), (12) and (13), that would reduce the residual entropy \u0395(\u0391\u2223\u03a9) substantially but short of zero.\nLet {\u0392J l\u2223l\u2208L} be a partition of \u0391J . Then the conditional distribution\np(\u0392 J l\u2223\u03c9 K ) \u2261 \u2211\n\u03b1 J \u2208\u0392 J\nl\np(\u03b1 J \u2223\u03c9 K ) = { 1 ,\u2200\u03c9K \u2208\u03bd(oK\u2223\u03b1J \u2208\u0392J l)0, otherwise }\nwould induce a partition of \u03a9K such that\np(\u03c9 K \u2223\u0392 J l)\u2261 \u2211 \u03b1 J \u2208\u0392 J l p(\u03c9 K \u2223\u03b1 J ) p(\u03b1\nJ ) = { p(\u03c9K ) ,\u2200\u03c9K \u2208\u03bd(oK\u2223\u03b1J \u2208\u0392J l)0,otherwise }\n\u2211 \u03c9 K \u2208\u03bd(o K \u2223\u03b1 J \u2208\u0392 J l) p(\u03c9 K ) = p(\u0392 J l)\nWe call an observable with such a conditional distribution a sub-maximally informative observable (sub-MIO). It induces a coarser coarse-graining than a MIO. Several such sub-MIOs, however, can jointly provide a finer coarse-graining equivalent to, or even redundant to that of an MIO.\n8 of 9\nImplications for Speech Perception The original (and subsequent variations of) categorical perception of plosive consonants is a very constrained experimental design in which the target is a single random variable, the place of articulation of 3 plosive consonants, and the putative MIO is also a single random variable, the change in the frequency of the second formant. All other degrees of freedom are frozen. The full problem of speech perception is more complex. The change in frequency can be regarded as the projection of the neighborhood, \u03bd(o\nK \u2223\u03b1 J ) , which is a multidimensional space, onto a one-\ndimensional subspace. The full neighborhood could be a rather complicated blob in configuration space. The categorical property of the identification function could become attenuated as more degrees of freedom are involved. This does not nullify the original observed categorical perception, but it is likely that the selected observable, the change in the frequency of the second formant, is not the MIO, or a sub-MIO of speech gestures, but its shadow.\nIn general, we would expect a complex target like speech gestures to be sub-maximally observable via a multitude of sub-MIOs. Such redundancy would enable the identification of a target from noise. This may be the reason that the neurophysiology of audition is now known to be replete with multidimensional random fields from the cochlea up to and including the primary auditory cortex[Wang1]. Note that the sub-MIOs need not be orthogonal, or even independent. The experimental results on categorical perception of speech imply that there is at least one sub-MIO.\nGiven a physical system in nature, it is highly unlikely that all of its observables are maximally informative. In fact, it is unlikely that any of its observables is maximally informative. However, even a sub-maximally informative observable is useful if the target to be inferred, such as the presence or absence of a cheetah in the tall grass on the savannah, is incidental, the penalty for failure to infer possibly fatal, and the cost of false alarms not very high.\nIf speech has evolved for human to human communication, then there would have been selective pressure for the co-evolution of the vocal tract with the auditory neurophysiology to craft observables that tend asymptotically to be maximally informative on the target.\nReferences [Chang1] Chang E.F., Rieger J.W., Johnson K., Berger M.S., Barbaro N.M., Knight R.T.; Categorical speech representation in human superior temporal gyrus. Nature Neuroscience 2010, 13 p 1428\u20131432. [Everett1] Everett III, H.; Theory of the Universal Wave Function. Thesis 1956, Appendix I p 121. [Goldstone1] Goldstone R.L., Hendrickson A.T.; Categorical perception. WIREs Cogn Sci 2010, 1 p 69-78. doi: 10.1002/wcs.26. http://wires.wiley.com/WileyCDA/WiresArticle/wisId-WCS26.html [Liberman1] Liberman A.M., Harris K.S., Hoffman H.S., Griffith B.C.; The discrimination of speech sounds within and across phoneme boundaries. Journal of Experimental Psychology 1957, 54 p 358-368. [Schouten1] Schouten, B., Gerrits, E., van Hessen, A.; The end of categorical perception as we know it. Speech Communication 2003, 41 p 71-80. [Wang1] Wang K., Shamma S.A.; Spectral Shape Analysis in the Central Auditory System. IEEE Trans. Speech and Audio Processing 1995, 3, 5 p 382-395. [Wikipedia1] Mutual Information . http://en.wikipedia.org/wiki/Mutual_information\n9 of 9"}], "references": [{"title": "Categorical speech representation in human superior temporal gyrus", "author": ["E.F. Chang", "J.W. Rieger", "K. Johnson", "M.S. Berger", "N.M. Barbaro", "R.T. Knight"], "venue": "Nature Neuroscience 2010,", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Categorical perception", "author": ["R.L. Goldstone", "A.T. Hendrickson"], "venue": "WIREs Cogn Sci 2010,", "citeRegEx": "Goldstone and Hendrickson,? \\Q2010\\E", "shortCiteRegEx": "Goldstone and Hendrickson", "year": 2010}, {"title": "The discrimination of speech sounds within and across phoneme boundaries", "author": ["A.M. Liberman", "K.S. Harris", "H.S. Hoffman", "B.C. Griffith"], "venue": "Journal of Experimental Psychology 1957,", "citeRegEx": "Liberman et al\\.,? \\Q1957\\E", "shortCiteRegEx": "Liberman et al\\.", "year": 1957}, {"title": "The end of categorical perception as we know it", "author": ["B. Schouten", "E. Gerrits", "A. van Hessen"], "venue": "Speech Communication 2003,", "citeRegEx": "Schouten et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Schouten et al\\.", "year": 2003}, {"title": "Spectral Shape Analysis in the Central Auditory System", "author": ["K. Wang", "S.A. Shamma"], "venue": "IEEE Trans. Speech and Audio Processing 1995,", "citeRegEx": "Wang and Shamma,? \\Q1995\\E", "shortCiteRegEx": "Wang and Shamma", "year": 1995}], "referenceMentions": [], "year": 2012, "abstractText": null, "creator": "Writer"}}}