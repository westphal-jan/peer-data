{"id": "1212.6273", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2012", "title": "Human-Recognizable Robotic Gestures", "abstract": "For robots to be accommodated in human spaces and in humans daily activities, robots should be able to understand messages from the human conversation partner. In the same light, humans must also understand the messages that are being communicated by robots, including the non-verbal ones. We conducted a web-based video study wherein participants gave interpretations on the iconic gestures and emblems that were produced by an anthropomorphic robot in a human-designed laboratory. We found that participants who spoke the human language were more likely to be more sensitive than those who gave the robot feedback. The findings suggest that the human interaction with robots can improve the human interaction between human and robot communication, and help scientists better understand the nature of communication.\n\n\n\n\n\n\n\n\nThe findings are based on a short survey of 1,200 participants with whom we conducted. The first survey was conducted to follow about 1,500 participants in a 12-person session of the U.S. Army Corps of Engineers in 2010, and the second to follow the same year. The first survey was conducted to examine the characteristics of individuals who spoke the human language. Participants were asked a questionnaire for which they described themselves in a series of three types of phrases: \u202a\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c\u202c", "histories": [["v1", "Wed, 26 Dec 2012 22:10:14 GMT  (5293kb)", "http://arxiv.org/abs/1212.6273v1", "21 pages, 5 figures"]], "COMMENTS": "21 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.HC", "authors": ["john-john cabibihan", "wing-chee so", "soumo pramanik"], "accepted": false, "id": "1212.6273"}, "pdf": {"name": "1212.6273.pdf", "metadata": {"source": "CRF", "title": "Human-Recognizable Robotic Gestures", "authors": ["John-John Cabibihan", "Wing-Chee So", "Soumo Pramanik"], "emails": ["(elecjj@nus.edu.sg)"], "sections": [{"heading": null, "text": "PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nshould be able to understand messages from the human conversation partner. In the same\nlight, humans must also understand the messages that are being communicated by robots,\nincluding the non-verbal ones. We conducted a web-based video study wherein participants\ngave interpretations on the iconic gestures and emblems that were produced by an\nanthropomorphic robot. Out of the 15 gestures presented, we found 6 robotic gestures that\ncan be accurately recognized by the human observer. These were nodding, clapping, hugging,\nexpressing anger, walking, and flying. We reviewed these gestures for their meaning from\nliteratures in human and animal behavior. We conclude by discussing the possible\nimplications of these gestures for the design of social robots that are aimed to have engaging\ninteractions with humans.\nIndex Terms\u2014social robotics, iconic gestures, emblematic gestures, human-robot\ninteraction, humanoid robots, robot gestures.\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nI. INTRODUCTION There is a growing trend of robots that socially interact with people (e.g. [1-6]). Likewise, there are now robots that can assist in the provision of care and therapy to children, elderly, and atypical populations (e.g. [7-9]). Such kinds of robots are called social robots. Social robots are autonomous robots that are able to interact and communicate among themselves, with humans, and with the environment and are designed to operate according to the established social and cultural norms [10, 11].\nFor robots to achieve socially engaging interactions with humans, researchers have argued that robots are expected to learn and produce human-like attributes such as body movements [12-14]. Among different types of body movements, abundant research has shown that gestures accompanying speech can evoke meaningful social interaction. Thus, robotic gestures have become one of the key design features for engaging human-robot interaction [1].\nSo far, the majority of the works were on the development of robots that can understand human facial expressions, hand gestures, and body movements. For example, robots with sophisticated vision systems can now track the faces or movements of people. Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20]. Another paradigm is the programming of robots by demonstration. In this approach, the robot captures the human demonstrator\u2019s motions through its on-board vision system [21]. The robot then processes the data through various machine learning algorithms and the gestures are replicated. Alternatively, the robot can also learn from the demonstrator\u2019s actions through the motion sensors that the demonstrator wears and the robot repeats the movements [22, 23]. To complement the initial data from the wearable sensors, some researchers have also taught robots by physically moving the robot\u2019s limbs to the desired positions or to correct the initial motion from the wearable sensors, i.e. kinesthetic teaching [24-27].\nWhat remains to be addressed is whether human beings can understand the meaning of the head and body gestures that are produced by anthropomorphic robots. Research on the interpretation of robotic gestures, however, is relatively scarce. Among a few studies, Kanda et al. [28] found that human beings responded to body movements and utterances by a route guidance robot, while Oberman et al. [29] further reported that comprehending robotic actions might activate the mirror neuron system that was previously thought to be specifically selective for biological actions. However, little is known on whether humans can derive meaning from the gestures produced by social robots.\nIn this paper, we investigate various human-like gestures made by an anthropomorphic robot and we examine which among these gestures the human observer can recognize. Should a pattern of commonly recognizable robotic gestures emerge, robot designers and programmers can implement these to a robot with the likelihood that the human interaction partner can understand the gestures. This is relevant because the development of robots with lifelike appearance and behaviors require hardware and software systems that work in synchrony. Only after the hardware and software systems are completed can human-robot interaction experiments begin. All these take time and effort. Knowing which gestures to implement to the robot can reduce the development cycle time.\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nThe primary focus of this paper is on iconic gestures and emblems (cf. [30]), whereby the gestures carry the semantic meaning. We presented participants with iconic gestures and emblems that were performed by a human and a robot demonstrator. The participants were then asked to identify the meaning of those gestures. We examined whether human beings can interpret the robot gestures in the same way as the human gestures.\nThe next section gives a background on the communicative gestures that humans are able to do. Section III describes the robot and the procedures on how it was programmed with the gestures. Section IV describes our experimental procedure. Section V presents the results for the agreement rates and response time. The findings and implications are discussed in Section VI. The concluding remarks are given in Section VII.\nII. COMMUNICATIVE HUMAN GESTURES Language is more than words. \u201cAs the tongue speaketh to the ear, so the gesture speaketh to the eye,\u201d Sir Francis Bacon once said [31]. Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35]. Interestingly, humans have been found to be very sensitive to these nonverbal cues [36]. Previous research has shown that among conversation participants, the appropriate nonverbal gestures play a key role in helping communicate intent, instruct, lead, and build rapport [37-39]. Furthermore, earlier research has shown that nonverbal signals from the face, voice, posture, gesture, interpersonal distance, and positioning have physiological effects on the other person, which have been found to be distinct from the effects of linguistic information [40].\nGestures are the spontaneous movements exhibited by speakers from all cultural and linguistic backgrounds as they are engaged in conversations [41-43]. From a speaker's perspective, gesture facilitates communication. Iconic gestures and emblems are often used to complement speech since they are meaningful hand configurations, and thus, are clearly communicative [44]. For example, a thumb that is pointing upward while the rest of the fingers are curled indicates approval; an index finger drawing an arc may indicate the motion of a long jump.\nGesture not only facilitates speech production, but also speech comprehension [45, 46]. It was suggested that speakers gesture to maximize the information conveyed to listeners [47]. Thus, when information crucial to communication is not conveyed in speech, gestures assist in conveying the information instead. For example, a speaker produces gestures to show directions to the listeners, because gesture is a better modality to convey spatial information.\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nIII. SYSTEM CONFIGURATION We used the upper-body of a 10 degrees-of-freedom (DOF) anthropomorphic robot for the experiments (Scout, Dr. Robot, Ottawa, Canada; see Fig. 1). The robot has 2 DOF in the neck, 2 DOF in each shoulder, 1 DOF on each elbow and 1 DOF on each wrist. Inspired by typical human gestures, we evaluated an initial set of 25 gestures in a pilot study [48]. Fifteen gestures were selected for this paper. These robot gestures range from simple movements requiring only 1 DOF such as nodding, to more complex gestures such as hugging, which can be accomplished with the 8 DOF of the arm motion. Due to the limited DOF by the robot, we only included gestures that involved the head and arms.\nFor the human gesture to be replicated (Fig. 2a), a software application was developed to control the servomotors at the joints of the robot. Microsoft Visual Studio (version 2008) was used as the platform to\ndevelop the code in C# language, while Microsoft Robotics Developer Studio (MRDS; version R2) provided the environment for controlling the robot and for simulating the motion. As shown in Fig. 2b, a graphical user interface (GUI) was designed for simplifying the recording and playback of the gestures. Through the sliders in the GUI, the user can adjust the robot\u2019s head or arms to the desired positions. Each of the gestures was first broken down into multiple via points. These points are intermediate locations through which the robot joints can move. For each via point, the joint angles were defined using the GUI and the resulting sequence of joint angles was stored in the database (Fig. 2c).\nThe general configuration of the Scout robot matches that of a Lynx 6 Robotic Arm1 (Lynxmotion, IL, USA). Because the MRDS supports the Decentralized System Service (DSS; Fig. 2d), we implemented the Lynx6Arm service, which was made available by Lynxmotion, for us to control the robot\u2019s movements. The Lynx6Arm service has a service partner named SSC32 service and this was used to send the commands to the servo-controller. At the time of gesture execution, the joint angles corresponding to the via points are passed in sequence as inputs to the DSS. The service uses inverse kinematics to determine the motor angles, which are required to achieve the corresponding joint angles of the robot. Finally, the servo controller (Fig. 2e) instructs the servomotors to move according to the calculated motor angles for the robot gesture to be performed (Fig. 2f).\n1 http://www.promrds.com/Chapter15/Lynx6Arm.htm\nFig. 1. The robot that was used to demonstrate the gestures. The arrows illustrate the 10 degrees of freedom of the robot.\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nIn addition, gesture execution speed was one of the important factors considered in mapping the human gestures to the robot. Research has shown that gesture speed influences emotional perception [49, 50], i.e., gestures expressing emotions of sadness are usually associated with slow movements whereas gestures expressing happiness are usually associated with faster movements. As the servomotors in the robot could not be operated at variable speed, varying the number of via points of a particular gesture controlled the speed of gesture execution. For example, gestures expressing sadness were typically modeled with approximately 200 via points whereas only 50 via points were used in the case of gestures for expressing happiness. During the execution of the gestures, these points were sent at a constant interval of 25 ms to the servo controller, implying that increasing the via points would produce a slower motion of the links. The resulting\nmotions for each of the joints can then be recorded and can be played backed when necessary. Screen shots of the robot are shown in Fig. 3a.\nIV. EXPERIMENT A. Research Design and Participants We designed an experiment in which we presented the participants a set of 15 gestures that both a human and a robot can perform. By having the human gesture as a reference, we can establish that a particular gesture is familiar and is thus recognizable by the viewer. Furthermore, we counterbalanced the presentation order such that one group of participants were presented the human gestures first, then the robot gestures, while another group of participants were shown the robot gestures, then the human gestures. We then compared whether the same gestures were recognized when a robot and a human produced them. We also compared the response latency across conditions. One hundred and twenty-two undergraduate and graduate students (70 males, 52 females, all 18-30 years old) from the National University of Singapore (NUS) were recruited by email. Of all participants, 65 participants were presented with robot gestures, followed by human gestures (Robot-Human, RH, condition); 57 students were presented with human gestures, followed by robot gestures (Human-Robot, HR, condition). Three units of a music player (iPod Shuffle, Apple, CA, USA) were raffled off to the participants.\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nB. Human Gestures A male demonstrator was selected to perform the gestures that the robot can do. To replicate the limited movements of the robot, the demonstrator was instructed not to use any body part other than his hands, arms, and neck while acting the gesture. He was instructed to keep a neutral expression and not to use any facial cues like blinking, smiling or frowning. The screen shots of the human gestures are shown in Fig. 3b. Please see the Appendix for the list of gestures and the corresponding interpretations that were given by the participants.\nC. Experimental Procedure A web-based video study was conducted. The website was hosted on the NUS server. To ensure that an individual participates in the experiment only once, participants had to first register themselves with a valid email address. Because the presentation order was counterbalanced, two different versions of invitation emails were circulated for the Robot-Human2 and the Human-Robot 3 groups. The invitation email contained the website\u2019s link to the experiment. The link opened the registration page that explained the objective of the experiment and that states the requirements to participate in the survey. A valid email address and the year of birth were required for registration. Upon successful registration, another email containing an activation link was sent to the participant\u2019s email address, which instructed the participant to click on the\n2 Robot-Human: http://robotics.nus.edu.sg/jjc/home.php?gesture=R 3 Human-Robot: http://robotics.nus.edu.sg/jjc/home.php?gesture=H\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nlink to complete the activation process and proceed to the experiment. The webpage instructed the participants that each video will be played twice. Only then will they be able to enter their response. For gestures that they could not identify, participants were instructed to enter \u2018x\u2019 as their response.\nThe presentation sequence was played in accordance to the participant\u2019s assigned group. The participants were presented with the first set of 15 videos. Each gesture video lasts for 3 s and was repeated once. Then, they were asked to type the gesture that first came to their mind. To ensure that participants responded only with their first impression of the gesture, they were not allowed to replay the video and were given only 15 s to type the answer for each gesture. Our pilot tests showed that 15 s was the optimal time for a participant to type their response on the webpage. Response time was calculated as the time between the clicking of the \u2018Play\u2019 button for starting the video and the \u2018Next\u2019 button for proceeding to the subsequent gesture video. From this, 6 s was subtracted in order to account for the total running time of the video. Upon clicking the \u2018Next\u2019 button, the participant\u2019s interpretation of the gesture and the response time for the current video were stored in the database. To ensure that the earlier gestures did not influence the succeeding set of gestures, the gestures were presented in random order. The order of gestures in videos 1 to 10 was randomized separately from videos 11 to 15. This prevented two similar human or robot gestures to be shown close to one another.\nD. Coding We analyzed the participants\u2019 interpretation of each gesture and identified the meaning of gesture that the majority of participants agreed with. Next, we counted the number of participants who agreed with the gesture\u2019s meaning (i.e., agreement rate). A 70% agreement rate indicates high similarity of responses among participants. This cut-off threshold was similar to the human gesture experiments in [51, 52]. A second coder then analyzed the similarity of the participants\u2019 interpretation of the gestures. The inter-rater similarity was 99.24% for the RH condition and 99.7% in the HR condition. All statistical analyses were performed using a commercial statistical package (IBM SPSS, version 19, NY, USA). A p-value of < 0.05 was considered statistically significant for analysis."}, {"heading": "V. RESULTS", "text": "Response latencies longer than 15 s were excluded from the analysis. Results with response time beyond 2.5 SDs of the sample mean were also excluded. This criterion removed 3.24% of the data in the Robot-Human condition and 4.71% in the Human-Robot condition.\nA. Agreement Rates On average, the agreement rate for robot gestures and human gestures in the two conditions was 62.94% (SD = 33.88%). A mixed analysis of variance (ANOVA) with the type of gestures (robot, human) as the within-subject independent variable, condition (HR, RH) as the between-subject independent variable, and agreement rate as the dependent variable was conducted. We found a significant effect of gesture type, F (1, 1323) = 41.99, p < .001, no effect of condition, F (1, 1323) = 1.46, p = ns, and no interaction, F (1, 1323) = 2.88, p = ns. Human gestures (M = 64%, SD = 48%) generally received higher agreement rate than robot gestures (M = 57%, SD = 50%).\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nOf all gestures, eight human gestures had mean agreement rates above 70%. These were clapping, nodding, walking, hugging, flying, expressing anger, shaking hands, and showing directions. Six robot gestures had mean agreement rates above 70%, notwithstanding the presentation order. These gestures were nodding, clapping, walking, hugging, flying, and expressing anger. More importantly, these robotic gestures were also well recognized in the pool of human gestures. Fig. 4 shows the mean agreement rates across different gestures.\nB. Response Latencies On average, participants spent 4.89 s\n(SD = 2.63 s) to respond to a gesture. A mixed ANOVA with the type of gestures (robot, human) as the within-subject independent variable, condition (HR, RH) as the between-subject independent variable, and response time as the dependent variable was conducted. We found a significant effect of condition, F (1, 1323) = 82.58, p < .001, no effect of the gesture type, F (1, 1323) = 1.97, p = ns, and a significant interaction, F (1, 1323) = 197.27, p < .001. In order to explore the interaction further, we conducted two separate paired-sample t-tests for the response time for both types of gestures in HR and RH conditions respectively. In the HR condition, participants responded longer to the human gestures (M = 4.90 s, SD = 2.53 s) than to the robot gestures (M = 3.88 s, SD = 1.81 s), t(666) = 9.85, p < .001. In the RH\ncondition, we found the opposite pattern. Participants responded longer to the robot gestures (M = 6.03 s, SD = 3.05 s) than to the human gestures (M = 4.78 s, SD = 2.52 s), t(657) = 10.05, p < .001. Therefore, our findings showed that participants responded to the first set of gestures (either human or robot gestures) longer than to the second set of gestures.\nAmong the human gestures, there were 8 gestures that had response times below the average response time of 4.86 s. These were hugging, the gesture to point to oneself (i.e.\nFig. 4. The participants\u2019 responses for the mean agreement rates for the 15 gestures. The agreement rate denotes the agreement of the participant\u2019s responses to the gesture\u2019s meaning. The cut-off line represents the threshold agreement rate of 70%.\nFig. 5. The participants\u2019 mean response time for the 15 gestures. The cut-off line denotes the average time of 4.89 s in which participants responded to a gesture.\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nmyself), flying, swinging arms, pushing-pulling, clapping, driving, and expressing anger. Among the human gestures that had agreement rates higher than 70%, hugging, flying, clapping, and expressing anger can be recognized within 4.86 s. As for the mean response time of the other four gestures within the 70% agreement rate, walking, shaking hands, nodding, and showing directions can be recognized in about 6 s.\nAmong the robot gestures, there were 7 gestures that had mean response time below 4.86 s. These were swinging arms, hugging, pushing-pulling, driving, myself, welcome, and walking. Among these, hugging and walking had high agreement rates as shown earlier. The mean response times for the rest of the gestures were about 6 s or less. Fig. 5 shows the average response time for the different gestures.\nVI. DISCUSSION We presented the participants with gestures that were being acted out by a human and a robot. Among the 15 gestures that the human demonstrator performed, the participants recognized 8 gestures, consisting of nodding, clapping, walking, hugging, flying, expressing anger, shaking hands and showing directions. In other words, there were approximately half of the gestures being recognized by human participants. Indeed, a 50% recognition rate, or thereabouts, is not uncommon in human gesture studies. In an earlier paper [52], experimental subjects were presented with 80 videotaped gestures and only 40 gestures (i.e. 50%) reached 70% of agreement rate. Note that the same cut-off rate was used in the current paper.\nAmong the 8 gestures that were recognized by the participants, six of the gestures were recognized by the participants when the robot performs those gestures. The gestures are: nodding, clapping, walking, hugging, flying, and the gesture for expressing anger. Other gestures were below the cut-off rate. The robotic gestures of shaking hands and showing directions\u2014both using wrist motion and slight curling of the fingers\u2014were less recognized presumably because the robot\u2019s hands have limited motion as compared to the hands of the human demonstrator. Regardless of the type of gestures, the participants were able to provide their responses within 5 s on average. They also responded faster to the set of gestures that were presented first.\nWe review the literatures in human and animal behavior to determine the meaning of the 6 gestures. We then discuss the possible implications of these gestures for the design of robots that are aimed to socially interact with humans.\nA. Gesture Meanings and Design Implications"}, {"heading": "Nodding", "text": "In human-to-human interactions, head movements can indicate agreement or affirmation, disapproval or negation, and many other semantic messages [37, 53-56]. It was observed that the nods by the listener in a conversation encouraged utterances of the speaker, which achieved an animated conversation between the speaker and the listener [57]. The recognition of nodding may have come easy as it is a primitive form of communication that has been observed in mother-infant interaction [58, 59].\nWhen implemented on a sociable robotic penguin, it was discovered that the human interaction partner nodded more often when the robot nodded deliberately in response to the\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nhuman\u2019s nods, as compared to when the robot did not nod [60]. Riek et al. [61] confirmed this co-nodding phenomenon through a robotic monkey\u2019s head that mimicked the nodding behavior of the human partner. They demonstrated that when the human nods, the robot nods in response. Interestingly, the human acknowledges the robot\u2019s nod. In a study on a route guiding robot, Kanda et al. [28] observed that people have generally rated their interactions with a robot highly when robots know when to nod and to gaze appropriately as compared with robots that have not learned these behaviors. Taken together, earlier research have shown that even with simple nodding gestures, robots have the ability to influence the interaction partner\u2019s behavior. Thus, nodding is a simple, yet meaningful gesture that should be implemented to a social robot."}, {"heading": "Clapping", "text": "Humans clap with their hands as a sign of appreciation or approval. An audience normally expresses their appreciation for a good performance by the strength and length of their applause [62]. Clapping is one of human activities that have been studied in the field of video streaming analysis for sports, music and human-machine interaction, among many others [63-67]. Researchers have discovered that for apes in captivity, the apes clap in order to attract the attention of humans [68-70]; for apes in the wild, apes use clapping as a form of long distance communication to maintain group cohesiveness during instances of alarm [71, 72]. From these accounts in human and animal behavior, clapping appears to be a useful action for human-robot interaction.\nHanahara and Tada [73] proposed a new communication language based on clapping for humans to communicate better with robots. They described how the sound from clapping can be used to simplify how humans communicate with a robot in a manner that is analogous to a Morse-code type of message. They extended this idea further to develop a new clapping language that has syllables, words, and syntax of its own. We have yet to see examples in human-robot interaction wherein robots clap in order to show appreciation, approval or as a gesture to attract attention, similar to how humans and apes employ it."}, {"heading": "Hugging", "text": "Emotions are better expressed through nonverbal behavior (e.g., [74, 75]). Among the nonverbal behaviors, social touching (e.g. hugs, patting, caress, handshake) appears to be an important modality [76]. Hugging or embrace is a form of affective touching that involves the clasping or clinging of one's arms to another person. It has been suggested that the warmth from a parent\u2019s touch may help children feel secure in their exploration of their environments [77, 78]. On the contrary, the lack of parental warmth can cause children to experience more stress (e.g., [79]). Recent medical evidence shows that hugging can have health benefits to help reduce blood pressure and increase levels of oxytocin. Oxytocin is the hormone that is involved in social bonding, the formation of trust and the increase of generosity in humans [80, 81]. Hugging has been recognized as an important component of parent-child interaction. This may explain why hugging is an easily recognizable gesture. A robotic nurse named Nancy was developed as a research platform to explore the effects of robotic gestures and social touching on humans [82]. Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962"}, {"heading": "Expressing Anger", "text": "In the current paper, the human and robot demonstrators adopted the body position in which the hands touched the hips while the elbows were flexed outwards. This gesture is also known as arms akimbo. The majority of the participants have associated this movement as a gesture to express anger. Anger is an emotional reaction when situations or circumstances are unfair or unjust, personal rights are not expected, or realistic expectations are not met [88]. Although anger is considered as a normal human emotion, the fields of robotics and virtual reality have been very active in trying to replicate this emotion and other emotional behaviors on robotic faces and body movements. Programming emotions into robots has been identified as an important step to create lifelike social robots for improving human-robot interaction [89]. For example, the Waseda Eye No.4 Refined II (WE4-RII) humanoid robot was designed to show human-like emotions [90]. Its robotic face has a total of 26 DOFs that can control the facial Action Units (cf. [91]) of the eyebrows, eyelids, eyes, mouth, and lips. When the authors compared the robot face and the human face for an angry facial gesture, the participants rated the emotional intensity of an angry human face significantly higher as compared to an angry robotic face (i.e. mean: human = 82% vs. robot = 60%; for more details on the experimental results, the interested reader is referred to Fig. 2 of ref. [90]). In other words, even with a high DOF robotic face, such as that of the WE4-RII, the participants rated the angry emotion from a human face to be higher as compared to when a robot expressed it.\nLike the behavioral experimental results of WE4-RII, more participants in our experiment recognized the human angry gesture as compared to when the robot expressed it (human = 80% vs. robot = 75%; see Fig. 4). In contrast, the robot we used performed the arms akimbo gesture without additional facial movements. Due to the tight coupling of the robot's appearance and behavior, there are still many issues that have to be addressed in replicating human emotions through the facial expressions and full body gestures by robots [92]. Nonetheless, participants are able to recognize an angry gesture from a robot although the emotional intensity may not be as strong as when humans express it."}, {"heading": "Walking and Flying", "text": "Walking has been extensively studied in the context of human motion recognition [63, 65, 67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures. Considering that robot companions will be expected to engage in playful interactions, one study investigated the possible full-body motions that a human playmate will do to a small humanoid robot [98]. Cooney et al. found that walking, flying, hugging and rocking-the-baby are some of the few gestures that emerged from the recorded interactions.\nIf robots were to be used for education and entertainment applications, storytelling was suggested to be a necessary skill [99]. Story telling has been found to be one of the most powerful tools to teach a new language to a child [100]. Thus, robots are now being used as teaching assistants in the classroom. Robots have been considered for the task because they can help enunciate foreign words that the teachers may find difficult. Moreover, robots have a special appeal to many children. The children\u2019s early acceptance of robotics technologies naturally leads to interactions that are fun and exciting. As a result, students can be more receptive to new knowledge. For example, iRobi is a home-based personal robot that teaches\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962\nthe English language [101] while Robovie teaches English vocabulary to Japanese children [102]. The commercially available RoboSapien (Wowwee, HK, China) was programmed to tell English stories to Taiwanese children in a classroom setting [103]. Story telling robots will benefit from action gestures, like walking or flying, to make their lexical meanings easily known. These behaviors will be especially useful to the beginning learners of foreign languages.\nB. Limitation of the Study The robot that we used in this work will not be able to fully replicate the gestures that can be performed by humans due to the limited degrees of freedom that the robot can operate. Take the handshake gesture, for example (Please see Fig. 4). This gesture was recognized with close to 80% agreement rate when the human demonstrator made the gesture. When the robot demonstrated the same gesture, the agreement rate was just 60%. A posteriori analyses of the handshake videos show that there were movements in the human demonstrator\u2019s wrist and the fingers were oriented in a grasping pose as he was performing the handshake gesture. Additional movements at the robot\u2019s wrist and fingers were not possible due to the absence of actuators to perform a similar motion.\nNevertheless, it is important for us to know which among the robot\u2019s gestures can be perceived as similar to when the humans perform the gestures. We understand that a large number of DOFs on a robot causes it to be bulky, which can result in difficulties for practical implementation. In addition, more flexibility on the robotic arms requires an increase in the DOFs. As a drawback, increasing the DOFs has an effect on the robot's controllability and leads to higher costs. Nonetheless, a minimal set of robotic gestures has emerged. A more compact and cost effective robot will have strong implications for the toy industry, entertainment and educational robots, among others."}, {"heading": "VII. CONCLUSION", "text": "Communication is a two-way street. In other words, not only should robots understand messages from the human conversation partner, humans must also understand the messages that are being communicated by robots, including the non-verbal ones. In this study, we found six robotic gestures that can be accurately recognized by the human observer. Nodding and clapping are gestures that are common for acknowledgement or agreement; hugging and the angry gestures are gestures that express emotions while walking and flying are action gestures, which can be used for storytelling or teaching languages. The development cycle time for robot programming and testing can be reduced if roboticists know at the onset the basic robotic gestures that humans can understand. When programmed into robots, these gestures can lead to human-robot interactions that are natural, appropriate, and engaging.\nPLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan, Wing Chee So, Soumo Pramanik, \u201cHuman-Recognizable Robotic Gestures,\u201d Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962"}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Wendy Yusson for the preparation of the robot videos and for the pilot\nexperiments."}], "references": [{"title": "A survey of socially interactive robots", "author": ["T. Fong", "I. Nourbakhsh", "K. Dautenhahn"], "venue": "Robotics and Autonomous Systems, vol. 42, pp. 143-166, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward sociable robots", "author": ["C. Breazeal"], "venue": "Robotics and Autonomous Systems, vol. 42, pp. 167-175, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Socialization between toddlers and robots at an early childhood education center", "author": ["F. Tanaka", "A. Cicourel", "J.R. Movellan"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 104, pp. 17954-17958, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Social and collaborative aspects of interaction with a service robot", "author": ["K. Severinson-Eklundh", "A. Green", "H. H\u00fcttenrauch"], "venue": "Robotics and Autonomous Systems, vol. 42, pp. 223-234, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A social robot that stands in line", "author": ["Y. Nakauchi", "R. Simmons"], "venue": "Autonomous Robots, vol. 12, pp. 313-324, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "A biologically inspired architecture for an autonomous and social robot", "author": ["M. Malfaz", "A. Castro-Gonz\u00e1lez", "R. Barber", "M.A. Salichs"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 3, pp. 232-246, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robot therapy: A new approach for mental healthcare of the elderly - A mini-review", "author": ["T. Shibata", "K. Wada"], "venue": "Gerontology, vol. 57, pp. 378-386, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Socially intelligent robots: Dimensions of human-robot interaction", "author": ["K. Dautenhahn"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 362, pp. 679-704, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Children-robot interaction: a pilot study in autism therapy", "author": ["H. Kozima", "C. Nakagawa", "Y. Yasuda"], "venue": "Progress in Brain Research. vol. 164, 2007, pp. 385-400.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Social robotics: Integrating advances in engineering and computer science (keynote speech)", "author": ["S.S. Ge"], "venue": "Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI) Mae Fah Luang University, Chang Rai, Thailand, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards an effective design of social robots ", "author": ["H. Li", "J.J. Cabibihan", "Y.K. Tan"], "venue": "International Journal of Social Robotics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Human-Inspired Robots", "author": ["S. Coradeschi", "H. Ishiguro", "M. Asada", "S.C. Shapiro", "M. Thielscher", "C. Breazeal", "M.J. Mataric", "H. Ishida"], "venue": "Intelligent Systems, IEEE, vol. 21, pp. 74-85, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Android science: Conscious and subconscious recognition", "author": ["H. Ishiguro"], "venue": "Connection Science, vol. 18, pp. 319-332, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Integration of action and language knowledge: A roadmap for developmental robotics", "author": ["A. Cangelosi", "G. Metta", "G. Sagerer", "S. Nolfi", "C. Nehaniv", "K. Fischer", "J. Tani", "T. Belpaeme", "G. Sandini", "F. Nori", "L. Fadiga", "B. Wrede", "K. Rohlfing", "E. Tuci", "K. Dautenhahn", "J. Saunders", "A. Zeschel"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 2, pp. 167-195, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The intelligent ASIMO: System overview and integration", "author": ["Y. Sakagami", "R. Watanabe", "C. Aoyama", "S. Matsunaga", "N. Higaki", "K. Fujimura"], "venue": "IEEE International Conference on Intelligent Robots and Systems, 2002, pp. 2478-2483.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Natural interface using pointing behavior for human-robot gestural interaction", "author": ["E. Sato", "T. Yamaguchi", "F. Harashima"], "venue": "IEEE Transactions on Industrial Electronics, vol. 54, pp. 1105-1112, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Explorations in engagement for humans and robots", "author": ["C.L. Sidner", "C. Lee", "C.D. Kidd", "N. Lesh", "C. Rich"], "venue": "Artificial Intelligence, vol. 166, pp. 140-164, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Gesture based interface for human-robot interaction", "author": ["S. Waldherr", "R. Romero", "S. Thrun"], "venue": "Autonomous Robots, vol. 9, pp. 151-173, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Using the rhythm of nonverbal human-robot interaction as a signal for learning", "author": ["P. Andry", "A. Blanchard", "P. Gaussier"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 3, pp. 30-42, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Cognitive development in partner robots for information support to elderly people", "author": ["A. Yorita", "N. Kubota"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 3, pp. 64-73, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Human-Recognizable Robotic Gestures", "author": ["C. Breazeal", "B. Scassellati", "\"Robots that imitate humans", "Trends in Cognitive Sciences", "vol. 6", "pp. 481-487", "2002. 15 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards gesture-based programming: Shape from motion primordial learning of sensorimotor primitives", "author": ["R.M. Voyles", "J.D. Morrow", "P.K. Khosla"], "venue": "Robotics and Autonomous Systems, vol. 22, pp. 361-375, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Teaching and learning of robot tasks via observation of human performance", "author": ["R. Dillmann"], "venue": "Robotics and Autonomous Systems. vol. 47, 2004, pp. 109-116.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Mimetic communication model with compliant physical contact in human-humanoid interaction", "author": ["D. Lee", "C. Ott", "Y. Nakamura"], "venue": "International Journal of Robotics Research, vol. 29, pp. 1684-1704, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamical system modulation for robot learning via kinesthetic demonstrations", "author": ["M. Hersch", "F. Guenter", "S. Calinon", "A. Billard"], "venue": "IEEE Transactions on Robotics, vol. 24, pp. 1463-1467, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Incremental learning of gestures by imitation in a humanoid robot", "author": ["S. Calinon", "A. Billard"], "venue": "HRI 2007 - Proceedings of the 2007 ACM/IEEE Conference on Human-Robot Interaction - Robot as Team Member, 2007, pp. 255-262.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning and reproduction of gestures by imitation", "author": ["S. Calinon", "F. D'Halluin", "E.L. Sauser", "D.G. Caldwell", "A.G. Billard"], "venue": "IEEE Robotics and Automation Magazine. vol. 17, June 2010, pp. 44-54.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A humanoid robot that pretends to listen to route guidance from a human", "author": ["T. Kanda", "M. Kamasima", "M. Imai", "T. Ono", "D. Sakamoto", "H. Ishiguro", "Y. Anzai"], "venue": "Autonomous Robots, vol. 22, pp. 87-100, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "EEG evidence for mirror neuron activity during the observation of human and robot actions: Toward an analysis of the human qualities of interactive robots", "author": ["L.M. Oberman", "J.P. McCleery", "V.S. Ramachandran", "J.A. Pineda"], "venue": "Neurocomputing, vol. 70, pp. 2194-2203, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Iconic Gestures of Children and Adults", "author": ["D. McNeill"], "venue": "Semiotica, vol. 62, pp. 107-128, 1986.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1986}, {"title": "Strategy signals in face-to-face interaction", "author": ["S. Duncan", "L. Brunner", "D. Fiske"], "venue": "Journal of Personality and Social Psychology. vol. 37, 1979, pp. 301-313.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1979}, {"title": "On signalling that it's your turn to speak", "author": ["S. Duncan", "G. Niederehe"], "venue": "Journal of Experimental Social Psychology. vol. 10, 1974, pp. 234-247.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1974}, {"title": "Some signals and rules for taking speaking turns in conversations", "author": ["S. Duncan"], "venue": "Journal of Personality and Social Psychology. vol. 23, 1972, pp. 283-292.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1972}, {"title": "Some functions of gaze-direction in social interaction", "author": ["A. Kendon"], "venue": "Acta psychologica. vol. 26, 1967, pp. 22-63.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1967}, {"title": "Ellesworth, Person Perception, 2nd ed", "author": ["D.J. Schneider", "A.H. Hastorf", "P. C"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1979}, {"title": "The nature of rapport and its nonverbal correlates", "author": ["L. Tickle-Degnen", "R. Rosenthal"], "venue": "Psychological Inquiry, vol. 1, pp. 285-293, 1990.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1990}, {"title": "Nonverbal Communication in Human Interaction, 4th ed", "author": ["M. Knapp", "J. Hall"], "venue": "Fort Worth, TX: Harcourt Brace College,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Nonverbal Signals", "author": ["J.K. Burgoon"], "venue": "Handbook of Interpersonal Communication, 2nd ed, M. L. Knapp and G. R. Miller, Eds. Thousand Oaks, CA: Sage, 1994, pp. 229-285.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1994}, {"title": "Interpersonal perception in Japanese and British observers", "author": ["T. Kito", "B. Lee"], "venue": "Perception, vol. 33, pp. 957-974, 2004.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "Mind, Self and Society from the Standpoint of a Social Behaviorist", "author": ["G.H. Mead"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1934}, {"title": "Why do we gesture when we speak", "author": ["R.M. Krauss"], "venue": "Current Directions in Psychological Science, vol. 7, pp. 54-59, 1998.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Gesture and speech: How they interact", "author": ["A. Kendon"], "venue": "Nonverbal interaction, J. M. Weimann and R. P. Harrison, Eds. Beverly Hills, CA: Sage, 1983.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1983}, {"title": "The communicative functions of hand illustrators", "author": ["A.A. Cohen"], "venue": "Journal of Communication, vol. 27, pp. 54-63, 1977.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1977}, {"title": "Human-Recognizable Robotic Gestures", "author": ["J.P. de Ruiter", "\"Can gesticulation help aphasic people speak", "or rather", "communicate?", "Advances Speech-Lang. Pathol.", "vol. 8", "pp. 124-127", "2006. 16 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Minimal Set of Recognizable Gestures for a 10 DOF Anthropomorphic Robot", "author": ["J.J. Cabibihan", "W. Yusson", "S. Salehi", "S.S. Ge"], "venue": "Social Robotics. vol. 6414, S. S. Ge, H. Li, J. J. Cabibihan, and Y. K. Tan, Eds.: Springer Berlin / Heidelberg, 2010, pp. 63-70.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Evidence for distinct contributions of form and motion information to the recognition of emotions from body gestures", "author": ["A.P. Atkinson", "M.L. Tunstall", "W.H. Dittrich"], "venue": "Cognition, vol. 104, pp. 59-72, 2007.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Perceiving affect from arm movement", "author": ["F.E. Pollick", "H.M. Paterson", "A. Bruderlin", "A.J. Sanford"], "venue": "Cognition, vol. 82, pp. B51-B61, 2001.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2001}, {"title": "Distributional analyses in auditory lexical decision: Neighborhood density and word-frequency effects", "author": ["W.D. Goh", "S. Lidia", "M.J. Yap", "S. Hui Tan"], "venue": "Psychonomic Bulletin and Review, vol. 16, pp. 882-887, 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Iconic gestures prime words", "author": ["D.F. Yap", "W.C. So", "J.M. Yap", "Y.Q. Tan", "R.L. Teoh"], "venue": "Cognitive Science, vol. 35, pp. 171-183, 2011.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaze and task performance in shared virtual environments", "author": ["J.N. Bailenson", "A.C. Beall", "J. Blascovich"], "venue": "Journal of Visualization and Computer Animation, vol. 13, pp. 313-320, 2002/// 2002.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2002}, {"title": "Head gestures, gaze and the principles of conversational structure", "author": ["D. Heylen"], "venue": "International Journal of Humanoid Robotics, vol. 3, pp. 241-267, 2006.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "Some Uses of the Head Shake", "author": ["A. Kendon"], "venue": "Gesture, vol. 2, pp. 147-182, 2002.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2002}, {"title": "Linguistic functions of head movements in the context of speech", "author": ["E.Z. McClave"], "venue": "Journal of Pragmatics. vol. 32, 2000, pp. 855-878.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2000}, {"title": "Interviewer Head Nodding and Interviewee Speech Durations", "author": ["J.D. Matarazzo", "G. Saslow", "A.N. Wiens", "M. Weitman", "B.V. Allen"], "venue": "Psychotherapy: Theory, Research and Practice, vol. 1, pp. 54-63, 1964.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1964}, {"title": "Neonate movement is synchronized with adult speech: interactional participation and language acquisition", "author": ["W.S. Condon", "L.W. Sander"], "venue": "Science, vol. 183, pp. 99-101, 1974.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1974}, {"title": "Quantitative evaluation of infant behaviour and mother\u2013infant interaction", "author": ["N. Kobayashi", "T. Ishii", "T. Watanabe"], "venue": "Early Development and Parenting, vol. 1, pp. 23-31, 1992.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1992}, {"title": "The effect of head-nod recognition in human-robot conversation", "author": ["C.L. Sidner", "C. Lee", "L.P. Morency", "C. Forlines"], "venue": "HRI 2006: Proceedings of the 2006 ACM Conference on Human-Robot Interaction, 2006, pp. 290-296.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2006}, {"title": "When my robot smiles at me: Enabling human-robot rapport via real-time head gesture mimicry", "author": ["L.D. Riek", "P.C. Paul", "P. Robinson"], "venue": "Journal on Multimodal User Interfaces, vol. 3, pp. 99-108, 2010.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "The sound of many hands clapping", "author": ["Z. Neda", "E. Ravasz", "Y. Brechet", "T. Vicsek", "A.L. Barabasi"], "venue": "Nature, vol. 403, pp. 849-850, 2000.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2000}, {"title": "A novel eigenspace-based method for human action recognition", "author": ["A. Diaf", "R. Benlamri", "B. Boufama", "R. Ksantini"], "venue": "2010 5th International Conference on Digital Information Management, ICDIM 2010, 2010, pp. 182-187.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2010}, {"title": "Detection of goal event in soccer videos", "author": ["H.G. Kim", "S. Roeber", "A. Samour", "T. Sikora"], "venue": "Proceedings of SPIE - The International Society for Optical Engineering, 2005, pp. 317-325.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2005}, {"title": "Action recognition for surveillance applications using optic flow and SVM", "author": ["Part", "S. Danafar", "N. Gheissari"], "venue": "Lecture Notes in Computer Science. vol. 4844 LNCS, 2007, pp. 457-466.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2007}, {"title": "Human action recognition using Dynamic Time Warping", "author": ["S. Sempena", "N.U. Maulidevi", "P.R. Aryan"], "venue": "Proceedings of the 2011 International Conference on Electrical Engineering and Informatics, ICEEI 2011, 2011.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2011}, {"title": "Human activity recognition in videos: A systematic approach", "author": ["S. Singh", "J. Wang"], "venue": "Lecture Notes in Computer Science vol. 4224 LNCS, 2006, pp. 257-264.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2006}, {"title": "Differential use of attentional and visual communicative signaling by orangutans (Pongo pygmaeus) and gorillas (Gorilla gorilla) in response to the attentional status of a human", "author": ["S.R. Poss", "C. Kuhar", "T.S. Stoinski", "W.D. Hopkins"], "venue": "American Journal of Primatology, vol. 68, pp. 978-992, 2006.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2006}, {"title": "Clapping in chimpanzees: Evidence of exclusive hand preference in a spontaneous, bimanual gesture", "author": ["A.W. Fletcher"], "venue": "American Journal of Primatology, vol. 68, pp. 1081-1088, 2006.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2006}, {"title": "Ape gestures and language evolution", "author": ["A.S. Pollick", "F.B.M. De Waal"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 104, pp. 8184-8189, 2007.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}, {"title": "Hand-clapping as a communicative gesture by wild female swamp gorillas", "author": ["A.K. Kalan", "H.J. Rainey"], "venue": "Primates, vol. 50, pp. 273-275, 2009.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2009}, {"title": "Human-Recognizable Robotic Gestures", "author": ["J.M. Fay", "\"Hand-clapping in western low land gorillas", "Mammalia", "vol. 53", "pp. 457-458", "1989. 17 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-robot communication with hand-clapping language", "author": ["K. Hanahara", "Y. Tada"], "venue": "Journal of Computers, vol. 3, pp. 58-66, 2008.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2008}, {"title": "Lying and nonverbal behavior: Theoretical issues and new findings", "author": ["P. Ekman"], "venue": "Journal of Nonverbal Behavior, vol. 12, pp. 163-175, 1988.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 1988}, {"title": "Nonverbal behavior and communication", "author": ["A.W. Siegman", "S. Feldstein"], "venue": "Hillsdale, NJ: Erlbaum,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1987}, {"title": "Towards Humanlike Social Touch for Sociable Robotics and Prosthetics: Comparisons on the Compliance, Conformance and Hysteresis of Synthetic and Human Fingertip Skins", "author": ["J.J. Cabibihan", "S. Pattofatto", "M. Jomaa", "A. Benallal", "M.C. Carrozza"], "venue": "International Journal of Social Robotics vol. 1, pp. 29-40, 2009.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonverbal maternal warmth and children's locus of control of reinforcement", "author": ["J.S. Carton", "E.E.R. Carton"], "venue": "Journal of Nonverbal Behavior, vol. 22, pp. 77-86, 1998.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1998}, {"title": "Antecedents of individual differences in locus of control of reinforcement: A critical review", "author": ["J.S. Carton", "S. Nowicki", "Jr."], "venue": "Genetic, Social, and General Psychology Monographs, vol. 120, pp. 31-81, 1994.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 1994}, {"title": "Origins of Generalized Control Expectancies: Reported Child Stress and Observed Maternal Control and Warmth", "author": ["J.S. Carton", "S. Nowicki Jr"], "venue": "Journal of Social Psychology, vol. 136, pp. 753-760, 1996.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1996}, {"title": "Oxytocin increases trust in humans", "author": ["M. Kosfeld", "M. Heinrichs", "P.J. Zak", "U. Fischbacher", "E. Fehr"], "venue": "Nature, vol. 435, pp. 673-676, 2005.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2005}, {"title": "Oxytocin increases generosity in humans", "author": ["P.J. Zak", "A.A. Stanton", "S. Ahmadi"], "venue": "PLoS ONE, vol. 2, 2007.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2007}, {"title": "Design and Development of Nancy, a Social Robot", "author": ["S.S. Ge", "J.J. Cabibihan", "Z. Zhang", "Y. Li", "C. Meng", "H. He", "M.R. Safizadeh", "Y.B. Li", "J. Yang"], "venue": "Proc of the 8th Intl Conf on Ubiquitous Robots and Ambient Intelligence, Incheon, Korea, 2011.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2011}, {"title": "Model-free impedance control for safe human-robot interaction", "author": ["Y. Li", "S.S. Ge", "C. Yang", "K.P. Tee"], "venue": "Proc of IEEE Intl Conf on Robotics and Automation (ICRA), Shanghai, China, 2011, pp. 6021-6026.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2011}, {"title": "Impedance control for multi-point human-robot interaction", "author": ["Y. Li", "S.S. Ge", "C. Yang"], "venue": "Proc of ASCC 2011 - 8th Asian Control Conference, 2011, pp. 1187-1192.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2011}, {"title": "Prosthetic finger phalanges with lifelike skin compliance for low-force social touching interactions", "author": ["J.J. Cabibihan", "R. Pradipta", "S.S. Ge"], "venue": "Journal of NeuroEngineering and Rehabilitation, vol. 8, p. 16, 2011.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards humanlike social touch for prosthetics and sociable robotics: Handshake experiments and finger phalange indentations", "author": ["J.J. Cabibihan", "R. Pradipta", "Y.Z. Chew", "S.S. Ge"], "venue": "Lecture Notes in Computer Science vol. 5744 LNCS, 2009, pp. 73-79.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2009}, {"title": "Force and motion analyses of the human patting gesture for robotic social touching", "author": ["J.J. Cabibihan", "I. Ahmed", "S.S. Ge"], "venue": "Proc of IEEE Cybernetics and Intelligent Systems, Robotics, Automation and Mechatronics (CIS-RAM), Qingdao, China, 2011.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards social robots: Designing a emotion-based architecture", "author": ["J. Hirth", "N. Schmitz", "K. Berns"], "venue": "International Journal of Social Robotics, vol. 3, pp. 273-290, 2011.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2011}, {"title": "Umilta \"Brain response to a humanoid robot in areas implicated in the perception of human emotional gestures,", "author": ["T. Chaminade", "M. Zecca", "S.J. Blakemore", "A. Takanishi", "C.D. Frith", "S. Micera", "P. Dario", "G. Rizzolatti", "V. Gallese", "M. A"], "venue": "PLoS ONE,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2010}, {"title": "Facial expression and emotion", "author": ["P. Ekman"], "venue": "American Psychologist, vol. 48, pp. 384-392, 1993.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 1993}, {"title": "Scientific Issues Concerning Androids", "author": ["H. Ishiguro"], "venue": "International Journal of Robotics Research, vol. 26, pp. 105-117, 2007.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient bipedal robots based on passive-dynamic walkers", "author": ["S. Collins", "A. Ruina", "R. Tedrake", "M. Wisse"], "venue": "Science, vol. 307, pp. 1082-1085, 2005.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2005}, {"title": "Effort-Shape and kinematic assessment of bodily expression of emotion during gait", "author": ["M.M. Gross", "E.A. Crane", "B.L. Fredrickson"], "venue": "Human Movement Science, 2011.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}, {"title": "Measurement of lower extremity kinematics during level walking", "author": ["M.P. Kadaba", "H.K. Ramakrishnan", "M.E. Wootten"], "venue": "Journal of Orthopaedic Research, vol. 8, pp. 383-392, 1990.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 1990}, {"title": "Human-Recognizable Robotic Gestures", "author": ["A. Koenig", "X. Omlin", "L. Zimmerli", "M. Sapa", "C. Krewer", "M. Bolliger", "F. Muller", "R. Riener", "\"Psychological state estimation from physiological recordings during robot-assisted gait rehabilitation", "Journal of rehabilitation research", "development", "vol. 48", "pp. 367-385", "2011. 18 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Emotional influences on locomotor behavior", "author": ["K.M. Naugle", "J. Joyner", "C.J. Hass", "C.M. Janelle"], "venue": "Journal of Biomechanics, vol. 43, pp. 3099-3103, 2011.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2011}, {"title": "Full-body gesture recognition using inertial sensors for playful interaction with small humanoid robot", "author": ["M.D. Cooney", "C. Becker-Asano", "T. Kanda", "A. Alissandrakis", "H. Ishiguro"], "venue": "IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings, 2010, pp. 2276-2282.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2010}, {"title": "A storytelling robot: Modeling and evaluation of human-like gaze behavior", "author": ["B. Mutlu", "J. Forlizzi", "J. Hodgins"], "venue": "Proceedings of the 2006 6th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS, 2006, pp. 518-523.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "Culture in the elementary foreign language classroom", "author": ["C.A. Pesola"], "venue": "Foreign Language Annals, vol. 24, pp. 331-346, 1991.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 1991}, {"title": "The educational use of Home Robots for children", "author": ["J. Han", "M. Jo", "S. Park", "S. Kim"], "venue": "Proceedings - IEEE International Workshop on Robot and Human Interactive Communication, 2005, pp. 378-383.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2005}, {"title": "Interactive robots as social partners and peer tutors for children : A field trial", "author": ["T. Kanda", "T. Hirano", "D. Eaton", "H. Ishiguro"], "venue": "Human-Computer Interaction, vol. 19, pp. 61-84, 2004.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 4, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 5, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 6, "context": "[7-9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 7, "context": "[7-9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "[7-9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 9, "context": "Social robots are autonomous robots that are able to interact and communicate among themselves, with humans, and with the environment and are designed to operate according to the established social and cultural norms [10, 11].", "startOffset": 217, "endOffset": 225}, {"referenceID": 10, "context": "Social robots are autonomous robots that are able to interact and communicate among themselves, with humans, and with the environment and are designed to operate according to the established social and cultural norms [10, 11].", "startOffset": 217, "endOffset": 225}, {"referenceID": 11, "context": "For robots to achieve socially engaging interactions with humans, researchers have argued that robots are expected to learn and produce human-like attributes such as body movements [12-14].", "startOffset": 181, "endOffset": 188}, {"referenceID": 12, "context": "For robots to achieve socially engaging interactions with humans, researchers have argued that robots are expected to learn and produce human-like attributes such as body movements [12-14].", "startOffset": 181, "endOffset": 188}, {"referenceID": 13, "context": "For robots to achieve socially engaging interactions with humans, researchers have argued that robots are expected to learn and produce human-like attributes such as body movements [12-14].", "startOffset": 181, "endOffset": 188}, {"referenceID": 0, "context": "Thus, robotic gestures have become one of the key design features for engaging human-robot interaction [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 15, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 16, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 17, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 18, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 19, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 20, "context": "In this approach, the robot captures the human demonstrator\u2019s motions through its on-board vision system [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "can also learn from the demonstrator\u2019s actions through the motion sensors that the demonstrator wears and the robot repeats the movements [22, 23].", "startOffset": 138, "endOffset": 146}, {"referenceID": 22, "context": "can also learn from the demonstrator\u2019s actions through the motion sensors that the demonstrator wears and the robot repeats the movements [22, 23].", "startOffset": 138, "endOffset": 146}, {"referenceID": 23, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 24, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 25, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 26, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 27, "context": "[28] found that human beings responded to body movements and utterances by a route guidance robot, while Oberman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] further reported that comprehending robotic actions might activate the mirror neuron system that was previously thought to be specifically", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30]), whereby the gestures carry the semantic meaning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 31, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 32, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 33, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 34, "context": "Interestingly, humans have been found to be very sensitive to these nonverbal cues [36].", "startOffset": 83, "endOffset": 87}, {"referenceID": 35, "context": "Previous research has shown that among conversation participants, the appropriate nonverbal gestures play a key role in helping communicate intent, instruct, lead, and build rapport [37-39].", "startOffset": 182, "endOffset": 189}, {"referenceID": 36, "context": "Previous research has shown that among conversation participants, the appropriate nonverbal gestures play a key role in helping communicate intent, instruct, lead, and build rapport [37-39].", "startOffset": 182, "endOffset": 189}, {"referenceID": 37, "context": "Previous research has shown that among conversation participants, the appropriate nonverbal gestures play a key role in helping communicate intent, instruct, lead, and build rapport [37-39].", "startOffset": 182, "endOffset": 189}, {"referenceID": 38, "context": "posture, gesture, interpersonal distance, and positioning have physiological effects on the other person, which have been found to be distinct from the effects of linguistic information [40].", "startOffset": 186, "endOffset": 190}, {"referenceID": 39, "context": "Gestures are the spontaneous movements exhibited by speakers from all cultural and linguistic backgrounds as they are engaged in conversations [41-43].", "startOffset": 143, "endOffset": 150}, {"referenceID": 40, "context": "Iconic gestures and emblems are often used to complement speech since they are meaningful hand configurations, and thus, are clearly communicative [44].", "startOffset": 147, "endOffset": 151}, {"referenceID": 41, "context": "Gesture not only facilitates speech production, but also speech comprehension [45, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 42, "context": "Gesture not only facilitates speech production, but also speech comprehension [45, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 43, "context": "It was suggested that speakers gesture to maximize the information conveyed to listeners [47].", "startOffset": 89, "endOffset": 93}, {"referenceID": 44, "context": "Inspired by typical human gestures, we evaluated an initial set of 25 gestures in a pilot study [48].", "startOffset": 96, "endOffset": 100}, {"referenceID": 45, "context": "emotional perception [49, 50], i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 46, "context": "emotional perception [49, 50], i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 47, "context": "This cut-off threshold was similar to the human gesture experiments in [51, 52].", "startOffset": 71, "endOffset": 79}, {"referenceID": 48, "context": "This cut-off threshold was similar to the human gesture experiments in [51, 52].", "startOffset": 71, "endOffset": 79}, {"referenceID": 48, "context": "In an earlier paper [52], experimental subjects were presented with 80 videotaped gestures and only 40 gestures (i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 35, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 49, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 50, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 51, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 52, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 53, "context": "It was observed that the nods by the listener in a conversation encouraged utterances of the speaker, which achieved an animated conversation between the speaker and the listener [57].", "startOffset": 179, "endOffset": 183}, {"referenceID": 54, "context": "of nodding may have come easy as it is a primitive form of communication that has been observed in mother-infant interaction [58, 59].", "startOffset": 125, "endOffset": 133}, {"referenceID": 55, "context": "of nodding may have come easy as it is a primitive form of communication that has been observed in mother-infant interaction [58, 59].", "startOffset": 125, "endOffset": 133}, {"referenceID": 56, "context": "2208962 human\u2019s nods, as compared to when the robot did not nod [60].", "startOffset": 64, "endOffset": 68}, {"referenceID": 57, "context": "[61] confirmed this co-nodding phenomenon through a robotic monkey\u2019s head that mimicked the nodding", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] observed that people have generally rated their interactions with a robot highly when robots know when to nod and to gaze appropriately as compared with robots that have not learned these behaviors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "An audience normally expresses their appreciation for a good performance by the strength and length of their applause [62].", "startOffset": 118, "endOffset": 122}, {"referenceID": 59, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 60, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 61, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 62, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 63, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 64, "context": "Researchers have discovered that for apes in captivity, the apes clap in order to attract the attention of humans [68-70]; for apes in the wild, apes use clapping as a form of", "startOffset": 114, "endOffset": 121}, {"referenceID": 65, "context": "Researchers have discovered that for apes in captivity, the apes clap in order to attract the attention of humans [68-70]; for apes in the wild, apes use clapping as a form of", "startOffset": 114, "endOffset": 121}, {"referenceID": 66, "context": "Researchers have discovered that for apes in captivity, the apes clap in order to attract the attention of humans [68-70]; for apes in the wild, apes use clapping as a form of", "startOffset": 114, "endOffset": 121}, {"referenceID": 67, "context": "long distance communication to maintain group cohesiveness during instances of alarm [71, 72].", "startOffset": 85, "endOffset": 93}, {"referenceID": 68, "context": "long distance communication to maintain group cohesiveness during instances of alarm [71, 72].", "startOffset": 85, "endOffset": 93}, {"referenceID": 69, "context": "Hanahara and Tada [73] proposed a new communication language based on clapping for humans to communicate better with robots.", "startOffset": 18, "endOffset": 22}, {"referenceID": 70, "context": ", [74, 75]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 71, "context": ", [74, 75]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 72, "context": "hugs, patting, caress, handshake) appears to be an important modality [76].", "startOffset": 70, "endOffset": 74}, {"referenceID": 73, "context": "It has been suggested that the warmth from a parent\u2019s touch may help children feel secure in their exploration of their environments [77, 78].", "startOffset": 133, "endOffset": 141}, {"referenceID": 74, "context": "It has been suggested that the warmth from a parent\u2019s touch may help children feel secure in their exploration of their environments [77, 78].", "startOffset": 133, "endOffset": 141}, {"referenceID": 75, "context": ", [79]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 76, "context": "[80, 81].", "startOffset": 0, "endOffset": 8}, {"referenceID": 77, "context": "[80, 81].", "startOffset": 0, "endOffset": 8}, {"referenceID": 78, "context": "A robotic nurse named Nancy was developed as a research platform to explore the effects of robotic gestures and social touching on humans [82].", "startOffset": 138, "endOffset": 142}, {"referenceID": 79, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 69, "endOffset": 77}, {"referenceID": 80, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 69, "endOffset": 77}, {"referenceID": 81, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 91, "endOffset": 99}, {"referenceID": 82, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 91, "endOffset": 99}, {"referenceID": 83, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 113, "endOffset": 117}, {"referenceID": 84, "context": "Programming emotions into robots has been identified as an important step to create lifelike social robots for improving human-robot interaction [89].", "startOffset": 145, "endOffset": 149}, {"referenceID": 85, "context": "4 Refined II (WE4-RII) humanoid robot was designed to show human-like emotions [90].", "startOffset": 79, "endOffset": 83}, {"referenceID": 86, "context": "[91]) of the eyebrows, eyelids, eyes, mouth, and lips.", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "[90]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 87, "context": "Due to the tight coupling of the robot's appearance and behavior, there are still many issues that have to be addressed in replicating human emotions through the facial expressions and full body gestures by robots [92].", "startOffset": 214, "endOffset": 218}, {"referenceID": 88, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 89, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 90, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 91, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 92, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 93, "context": "Considering that robot companions will be expected to engage in playful interactions, one study investigated the possible full-body motions that a human playmate will do to a small humanoid robot [98].", "startOffset": 196, "endOffset": 200}, {"referenceID": 94, "context": "If robots were to be used for education and entertainment applications, storytelling was suggested to be a necessary skill [99].", "startOffset": 123, "endOffset": 127}, {"referenceID": 95, "context": "Story telling has been found to be one of the most powerful tools to teach a new language to a child [100].", "startOffset": 101, "endOffset": 106}, {"referenceID": 96, "context": "2208962 the English language [101] while Robovie teaches English vocabulary to Japanese children [102].", "startOffset": 29, "endOffset": 34}, {"referenceID": 97, "context": "2208962 the English language [101] while Robovie teaches English vocabulary to Japanese children [102].", "startOffset": 97, "endOffset": 102}], "year": 2012, "abstractText": null, "creator": "Word"}}}