{"id": "1301.2115", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Domain Generalization via Invariant Feature Representation", "abstract": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 10 Jan 2013 13:29:17 GMT  (271kb)", "http://arxiv.org/abs/1301.2115v1", "The 30th International Conference on Machine Learning (ICML 2013)"]], "COMMENTS": "The 30th International Conference on Machine Learning (ICML 2013)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["krikamol muandet", "david balduzzi", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1301.2115"}, "pdf": {"name": "1301.2115.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["krikamol@tuebingen.mpg.de,", "david.balduzzi@inf.ethz.ch,", "bs@tuebingen.mpg.de."], "sections": [{"heading": null, "text": "ar X\niv :1\n30 1.\n21 15\nv1 [\nst at\n.M L\n] 1\n0 Ja\nn 20\n13\nlearning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice."}, {"heading": "1. Introduction", "text": "Domain generalization considers how to take knowledge acquired from an arbitrary number of related domains, and apply it to previously unseen domains. To illustrate the problem, consider an example taken from Blanchard et al. (2011) which studied automatic gating of flow cytometry data. For each of N patients, a set of ni cells are obtained from peripheral blood samples using a flow cytometer. The cells are then labeled by an expert into different subpopulations, e.g., as a lymphocyte or not. Correctly identifying cell subpopulations is vital for diagnosing the health of patients. However, manual gating is very time consuming. To automate gating, we need to construct a classifier that generalizes well to previously unseen patients, where the distribution of cell types may differ dramatically from the training data.\nUnfortunately, we cannot apply standard machine learning techniques directly because the data violates the basic assumption that training data and test data come from the same distribution. Moreover, the training set consists of heterogeneous samples from several distributions, i.e., gated cells from several patients. In this case, the data exhibits covariate (or dataset) shift (Widmer and Kurat 1996, Quionero-Candela et al. 2009, Bickel et al. 2009b): although the marginal distributions PX on cell attributes vary due to biological or technical variations, the functional relationship P(Y |X) across different domains is largely stable (cell type is a stable function of a cell\u2019s chemical attributes).\nMax Planck Institute for Intelligent Systems, Spemannstra\u00dfe 38, 72076 Tu\u0308bingen,\nGermany\nDepartment of Computer Science, ETH Zurich, Universita\u0308tstrasse 6, 8092 Zurich,\nSwitzerland\nMax Planck Institute for Intelligent Systems, Spemannstra\u00dfe 38, 72076 Tu\u0308bingen,\nGermany\nE-mail addresses: krikamol@tuebingen.mpg.de, david.balduzzi@inf.ethz.ch, bs@tuebingen.mpg.de.\nDate: January 11, 2013. Key words and phrases. domain generalization, domain adaptation, support vector machines, transfer learning, sufficient dimension reduction, central subspace, kernel inverse regression, invariant representation, covariance operator inverse regression.\n1\nA considerable effort has been made in domain adaptation and transfer learning to remedy this problem, see Pan and Yang (2010a), Ben-David et al. (2010) and references therein. Given a test domain, e.g., a cell population from a new patient, the idea of domain adaptation is to adapt a classifier trained on the training domain, e.g., a cell population from another patient, such that the generalization error on the test domain is minimized. The main drawback of this approach is that one has to repeat this process for every new patient, which can be time-consuming \u2013 especially in medical diagnosis where time is a valuable asset. In this work, across-domain information, which may be more informative than the domain-specific information, is extracted from the training data and used to generalize the classifier to new patients without retraining.\n1.1. Overview. The goal of (supervised) domain generalization is to estimate a functional relationship that handles changes in the marginal P(X) or conditional P(Y |X) well, see Figure 1. We assume that the conditional probability P(Y |X) is stable or varies smoothly with the marginal P(X). Even if the conditional is stable, learning algorithms may still suffer from model misspecification due to variation in the marginal P(X). That is, if the learning algorithm cannot find a solution that perfectly captures the functional relationship between X and Y then its approximate solution will be sensitive to changes in P(X).\nIn this paper, we introduce Domain Invariant Component Analysis (DICA), a kernel-based algorithm that finds a transformation of the data that (i) minimizes the difference between marginal distributions PX of domains as much as possible while (ii) preserving the functional relationship P(Y |X).\nThe novelty of this work is twofold. First, DICA extracts invariants : features that transfer across domains. It not only minimizes the divergence between marginal distributions P(X), but also preserves the functional relationship encoded in the posterior P(Y |X). The resulting learning algorithm is very simple. Second, while prior work in domain adaptation focused on using data from many different domains to specifically improve the performance on the target task, which is observed during the training time (the classifier is adapted to the specific target task), we assume access to abundant training data and are interested in the generalization ability of the invariant subspace to previously unseen domains (the classifier generalizes to new domains without retraining).\nMoreover, we show that DICA generalizes or is closely related to many wellknown dimension reduction algorithms including kernel principal component analysis (KPCA) (Scho\u0308lkopf et al. 1998, Fukumizu et al. 2004a), transfer component analysis (TCA) (Pan et al. 2011), and covariance operator inverse regression (COIR) (Kim and Pavlovic 2011), see \u00a72.5. The performance of DICA is analyzed theoretically \u00a72.6 and demonstrated empirically \u00a73.\n1.2. Related work. Domain generalization is a form of transfer learning, which applies expertise acquired in source domains to improve learning of target domains (cf. Pan and Yang (2010a) and references therein). Most previous work assumes the availability of the target domain to which the knowledge will be transferred. In contrast, domain generalization focuses on the generalization ability on previously unseen domains. That is, the test data comes from domains that are not available during training.\nRecently, Blanchard et al. (2011) proposed an augmented SVM that incorporates empirical marginal distributions into the kernel. A detailed error analysis showed universal consistency of the approach. We apply methods from Blanchard et al. (2011) to derive theoretical guarantees on the finite sample performance of DICA.\nLearning a shared subspace is a common approach in settings where there is distribution mismatch. For example, a typical approach in multitask learning is to uncover a joint (latent) feature/subspace that benefits tasks individually (Argyriou et al. 2007, Gu and Zhou 2009, Passos et al. 2012). A similar idea has been adopted in domain adaptation, where the learned subspace reduces mismatch between source and target domains (Gretton et al. 2009, Pan et al. 2011). Although these approaches have proven successful in various applications, no previous work has fully investigated the generalization ability of a subspace to unseen domains."}, {"heading": "2. Domain-Invariant Component Analysis", "text": "Let X denote a nonempty input space and Y an arbitrary output space. We define a domain to be a joint distribution PXY on X \u00d7 Y, and let PX\u00d7Y denote the set of all domains. Let PX and PY|X denote the set of probability distributions PX on X and PY |X on Y given X respectively.\nWe assume domains are sampled from probability distribution P on PX\u00d7Y which has a bounded second moment, i.e., the variance is well-defined. Domains are not observed directly. Instead, we observe N samples S = {Si}Ni=1, where Si = {(x (i) k , y (i) k )} ni k=1 is sampled from P i XY and each P 1 XY , . . . ,P N XY is sampled from P. Since in general PiXY 6= P j XY , the samples in S are not i.i.d. Let P\u0302 i denote empirical distribution associated with each sample Si. For brevity, we use P and PX interchangeably to denote the marginal distribution.\nLet H and F denote reproducing kernel Hilbert spaces (RKHSes) on X and Y with kernels k : X \u00d7 X \u2192 R and l : Y \u00d7 Y \u2192 R, respectively. Associated with H and F are mappings x \u2192 \u03c6(x) \u2208 H and y \u2192 \u03d5(y) \u2208 F induced by the kernels k(\u00b7, \u00b7) and l(\u00b7, \u00b7). Without loss of generality, we assume the feature maps of X and Y have zero means, i.e., \u2211n k=1 \u03c6(xk) = 0 = \u2211n k=1 \u03d5(yk). Let \u03a3xx, \u03a3yy, \u03a3xy, and \u03a3yx be the covariance operators in and between the RKHSes of X and Y .\n2.1. Objective. Using the samples S, our goal is to produce an estimate f : PX \u00d7 X \u2192 R that generalizes well to test samples St = {x (t) k } nt k=1 drawn according to some unknown distribution Pt \u2208 PX (Blanchard et al. 2011). Since the performance of f depends in part on how dissimilar the test distribution Pt is from those in the training samples, we propose to preprocess the data to actively reduce the dissimilarity between domains. Intuitively, we want to find transformation B in H that (i) minimizes the distance between empirical distributions of the transformed\nsamples B(Si) and (ii) preserves the functional relationship between X and Y , i.e., Y \u22a5 X | B(X). We formulate an optimization problem capturing these constraints below.\n2.2. Distributional Variance. First, we define the distributional variance, which measures the dissimilarity across domains. It is convenient to represent distributions as elements in an RKHS (Berlinet and Agnan 2004, Smola et al. 2007, Sriperumbudur et al. 2010) using the mean map\n(1) \u00b5 : PX \u2192 H : P 7\u2192\n\u222b\nX\nk(x, \u00b7) dP(x) =: \u00b5P .\nWe assume that k(x, x) is bounded for any x \u2208 X such that Ex\u223cP[k(x, \u00b7)] < \u221e. If k is characteristic then (1) is injective, i.e., all the information about the distribution is preserved (Sriperumbudur et al. 2010). It also holds that EP[f ] = \u3008\u00b5P, f\u3009H for all f \u2208 H and any P.\nWe decompose P into PX , which generates the marginal distribution PX , and PY |X , which generates posteriors PY |X . The data generating process begins by generating the marginal PX according to PX . Conditioned on PX , it then generate conditional PY |X according to PY |X . The data point (x, y) is generated according to PX and PY |X , respectively. Given set of distributions P = {P 1,P2 . . . ,PN} drawn according to PX , define N \u00d7N Gram matrix G with entries\nGij := \u3008\u00b5Pi , \u00b5Pj\u3009H =\n\u222b\u222b k(x, z) dPi(x) dPj(z),(2)\nfor i, j = 1, . . . , N . Note that Gij is the inner product between kernel mean embeddings of Pi and Pj in H. Based on (2), we define the distributional variance, which estimates the variance of the distribution PX :\nDefinition 1. Introduce probability distribution P on H with P(\u00b5Pi) = 1 N\nand center G to obtain the covariance operator of P, denoted as \u03a3 := G \u2212 1NG \u2212 G1N + 1NG1N . The distributional variance is\n(3) VH(P) := 1\nN tr(\u03a3) =\n1\nN tr(G)\u2212\n1\nN2\nN\u2211\ni,j=1\nGij .\nThe following theorem shows that the distributional variance is suitable as a measure of divergence between domains.\nTheorem 1. Let P\u0304 = 1 N \u2211N i=1 P\ni. If k is a characteristic kernel, then VH(P) = 1 N \u2211N i=1\u2016\u00b5Pi \u2212 \u00b5P\u0304\u2016 2 H = 0 if and only if P 1 = P2 = \u00b7 \u00b7 \u00b7 = PN .\nTo estimate VH(P) from N sample sets S = {Si}Ni=1 drawn from P 1, . . . ,PN , we\ndefine block kernel and coefficient matrices\nK =   K1,1 \u00b7 \u00b7 \u00b7 K1,N ... . . . ...\nKN,1 \u00b7 \u00b7 \u00b7 KN,N\n  \u2208 Rn\u00d7n , Q =   Q1,1 \u00b7 \u00b7 \u00b7 Q1,N ... . . . ...\nQN,1 \u00b7 \u00b7 \u00b7 QN,N\n  \u2208 Rn\u00d7n ,\nwhere n = \u2211N\ni=1 ni and [Ki,j]k,l = k(x (i) k , x (j) l ) is the Gram matrix evaluated be-\ntween the sample Si and Sj . Following (3), elements of the coefficient matrix Qi,j \u2208 Rni\u00d7nj equal (N \u2212 1)/(N2n2i ) if i = j, and \u22121/(N\n2ninj) otherwise. Hence, the empirical distributional variance is\n(4) V\u0302H(S) = 1\nN tr(\u03a3\u0302) = tr(KQ) .\nTheorem 2. The empirical estimator V\u0302H(S) = 1 N tr(\u03a3\u0302) = tr(KQ) obtained from Gram matrix\nG\u0302ij := 1\nni \u00b7 nj\nni\u2211\nk=1\nnj\u2211\nl=1\nk(x (i) k , x (j) l )\nis a consistent estimator of VH(P).\n2.3. Formulation of DICA. DICA finds an orthogonal transform B onto a lowdimensional subspace (m \u226a n) that minimizes the distributional variance VH(S) between samples from S, i.e. the dissimilarity across domains. Simultaneously, we require that B preserves the functional relationship between X and Y , i.e. Y \u22a5 X | B(X).\n2.3.1. Minimizing distributional variance. In order to simplify notation, we \u201cflatten\u201d {(x (i) k , y (i) k ) ni k=1} N i=1 to {(xk, yk)} n k=1 where n = \u2211N i=1 ni. Let bk = \u2211n i=1 \u03b2 i k\u03c6(xi) = \u03a6x\u03b2k be the k th basis function of B where \u03a6x = [\u03c6(x1), \u03c6(x2), . . . , \u03c6(xn)] and \u03b2k are n-dimensional coefficient vectors. Let B = [\u03b21,\u03b22, . . . ,\u03b2m] and \u03a6\u0303x denote the projection of \u03a6x onto bk, i.e., \u03a6\u0303x = b \u22a4 k \u03a6x = \u03b2 \u22a4 k \u03a6 \u22a4 x \u03a6x = \u03b2 \u22a4 k K. The kernel on the B-projection of X is\n(5) K\u0303 := \u03a6\u0303\u22a4x \u03a6\u0303x = KBB \u22a4K .\nAfter applying transformation B, the empirical distributional variance between sample distributions is\n(6) V\u0302H(BS) = tr(K\u0303Q) = tr(B \u22a4KQKB) .\n2.3.2. Preserving the functional relationship. The central subspace C is the minimal subspace that captures the functional relationship between X and Y , i.e. Y \u22a5 X |C\u22a4X . Note that in this work we generalize a linear transformation C\u22a4X to nonlinear one B(X). To find the central subspace we use the inverse regression framework, (Li 1991):\nTheorem 3. If there exists a central subspace C = [c1, . . . , cm] satisfying Y \u22a5 X |C\u22a4X, and for any a \u2208 Rd, E[a\u22a4X |C\u22a4X ] is linear in {c\u22a4i X} m i=1, then E[X |Y ] \u2282 span{\u03a3xxci}mi=1.\nIt follows that the bases C of the central subspace coincide with the m largest eigenvectors of V(E[X |Y ]) premultiplied by \u03a3\u22121xx . Thus, the basis c is the solution to the eigenvalue problem V(E[X |Y ])\u03a3xxc = \u03b3\u03a3xxc. Alternatively, for each ck one may solve\nmax ck\u2208Rd\nc\u22a4k \u03a3 \u22121 xx V(E[X |Y ])\u03a3xxck\nc\u22a4k ck\nunder the condition that ck is chosen to not be in the span of the previously chosen ck. In our case, x is mapped to \u03c6(x) \u2208 H induced by the kernel k and B has nonlinear basis functions ck \u2208 H, k = 1, . . . ,m. This nonlinear extension implies that E[X |Y ] lies on a function space spanned by {\u03a3xxck}mk=1, which coincide with the eigenfunctions of the operator V(E[X |Y ]) (Wu 2008, Kim and Pavlovic 2011). Since we always work in H, we drop \u03c6 from the notation below.\nTo avoid slicing the output space explicitly (Li 1991, Wu 2008), we exploit its kernel structure when estimating the covariance of the inverse regressor. The following result from Kim and Pavlovic (2011) states that, under a mild assumption, V(E[X |Y ]) can be expressed in terms of covariance operators:\nTheorem 4. If for all f \u2208 H, there exists g \u2208 F such that E[f(X)|y] = g(y) for almost every y, then\n(7) V(E[X |Y ]) = \u03a3xy\u03a3 \u22121 yy \u03a3yx .\nLet \u03a6y = [\u03d5(y1), . . . , \u03d5(yn)] and L = \u03a6 \u22a4 y \u03a6y. The covariance of inverse regressor (7) is estimated from the samples S as V\u0302(E[X |Y ]) = \u03a3\u0302xy\u03a3\u0302\u22121yy \u03a3\u0302yx = 1 n \u03a6xL(L + n\u03b5In) \u22121\u03a6\u22a4x where \u03a3\u0302xy = 1 n \u03a6x\u03a6 \u22a4 y and \u03a3\u0302yy = 1 n \u03a6y\u03a6 \u22a4 y . Assuming inverses \u03a3\u0302 \u22121 yy and \u03a3\u0302\u22121xx exist, a straightforward computation (see Supplementary) shows\nb\u22a4k \u03a3\u0302 \u22121 xx V\u0302(E[X |Y ])\u03a3\u0302xxbk =\n1 n \u03b2\u22a4k L(L+ n\u03b5I) \u22121K2\u03b2k\nb\u22a4k bk = \u03b2 \u22a4 k K\u03b2k,(8)\nwhere \u03b5 smoothes the affinity structure of the output space Y , thus acting as a kernel regularizer. Since we are interested in the projection of \u03c6(x) onto the basis functions bk, we formulate the optimization in terms of \u03b2k. For a new test sample xt, the projection onto basis function bk is kt\u03b2k, where kt = [k(x1, xt), . . . , k(xn, xt)].\n2.3.3. The optimization problem. Combining (6) and (8), DICA findsB = [\u03b21,\u03b22, . . . ,\u03b2m] that solves\n(9) max B\u2208Rn\u00d7m\n1 n tr ( B\u22a4L(L+ n\u03b5In) \u22121K2B )\ntr (B\u22a4KQKB +BKB)\nThe numerator requires that B aligns with the bases of the central subspace. The denominator forces both dissimilarity across domains and the complexity of B to be small, thereby tightening generalization bounds, see \u00a72.6. Rewriting (9) as a constrained optimization (see Supplementary) yields Lagrangian\nL = 1 n tr ( B\u22a4L(L+ n\u03b5In) \u22121K2B ) \u2212 tr (( B\u22a4KQKB +BKB \u2212 Im ) \u0393 ) ,(10)\nwhere \u0393 is a diagonal matrix containing the Lagrange multipliers. Setting the derivative of (10) w.r.t. B to zero yields the generalized eigenvalue problem:\n(11) 1\nn L(L+ n\u03b5In)\n\u22121K2B = (KQK +K)B\u0393 .\nTransformation B corresponds to the m leading eigenvectors of the generalized eigenvalue problem (11)1.\nThe inverse regression framework based on covariance operators has two benefits. First, it avoids explicitly slicing the output space, which makes it suitable for highdimensional output. Second, it allows for structured outputs on which explicit slicing may be impossible, e.g., trees and sequences. Since our framework is based entirely on kernels, it is applicable to any type of input and output variables, as long as the corresponding kernels can be defined.\n2.4. Unsupervised DICA. In some application domains, such as image denoising, information about the target may not be available. We therefore derive an unsupervised version of DICA. Instead of preserving the central subspace, unsupervised DICA (UDICA) maximizes the variance of X in the feature space, which is estimated as 1\nn tr(B\u22a4K2B). Thus, UDICA solves\n(12) max B\u2208Rn\u00d7m\n1 n tr(B\u22a4K2B)\ntr(B\u22a4KQKB +B\u22a4KB) .\nSimilar to DICA, the solution of (12) is obtained by solving the generalized eigenvalue problem\n(13) 1\nn K2B = (KQK +K)B\u0393 .\n1In practice, it is more numerically stable to solve the generalized eigenvalue problem 1 n L(L+\nn\u03b5In)\u22121K2B = (KQK +K + \u03bbI)B\u0393, where \u03bb is a small constant.\nAlgorithm 1 Domain-Invariant Component Analysis\nInput: Parameters \u03bb, \u03b5, and m \u226a n. Sample S = {Si = {(x (i) k , y (i) k )} ni k=1} N i=1. Output: Projection Bn\u00d7m and kernel K\u0303n\u00d7n.\n1: Calculate gram matrix [Kij ]kl = k(x (i) k , x (j) l ) and [Lij ]kl = l(y (i) k , y (j) l ). 2: Supervised: C = L(L+ n\u03b5I)\u22121K2. 3: Unsupervised: C = K2. 4: Solve 1\nn CB = (KQK +K + \u03bbI)B\u0393 for B.\n5: Output B and K\u0303 \u2190 KBB\u22a4K. 6: The test kernel K\u0303t \u2190 KtBB\u22a4K where Ktnt\u00d7n is the joint kernel between test\nand training data.\nUDICA is a special case of DICA where L = 1 n I and \u03b5 \u2192 0. Algorithm 1 summarizes supervised and unsupervised domain-invariant component analysis.\n2.5. Relations to Other Methods. The DICA and UDICA algorithms generalize many well-known dimension reduction techniques. In the supervised setting, if dataset S contains samples drawn from a single distribution PXY then we have KQK = 0. Substituting \u03b1 := KB gives the eigenvalue problem 1\nn L(L +\nn\u03b5I)\u22121K\u03b1 = K\u03b1\u0393, which corresponds to covariance operator inverse regression (COIR) (Kim and Pavlovic 2011).\nIf there is only a single distribution then unsupervised DICA reduces to KPCA since KQK = 0 and finding B requires solving the eigensystem KB = B\u0393 which recovers KPCA (Scho\u0308lkopf et al. 1998). If there are two domains, source PS and target PT , then UDICA is closely related \u2013 though not identical to \u2013 Transfer Component Analysis (Pan et al. 2011). This follows from the observation that VH({PS ,PT}) = \u2016\u00b5PS \u2212 \u00b5PT \u2016 2, see proof of Theorem 1.\n2.6. A Learning-Theoretic Bound. We bound the generalization error of a classifier trained after DICA-preprocessing. The main complication is that samples are not identically distributed. We adapt an approach to this problem developed in Blanchard et al. (2011) to prove a generalization bound that applies after transforming the empirical sample using B. Recall that B = \u03a6xB.\nDefine kernel k\u0304 on P\u00d7X as k\u0304((P, x), (P\u2032, x\u2032)) := kP(P,P\u2032) \u00b7kX (x, x\u2032). Here, kX is the kernel on HX and the kernel on distributions is kP(P,P\u2032) := \u03ba(\u00b5P, \u00b5P\u2032) where \u03ba is a positive definite kernel (Christmann and Steinwart 2010, Muandet et al. 2012). Let \u03a8P denote the corresponding feature map.\nTheorem 5. Under reasonable technical assumptions, see Supplementary, it holds with probability at least 1\u2212 \u03b4 that,\nsup \u2016f\u2016H\u22641\n\u2223\u2223\u2223E\u2217PEP\u2113(f(X\u0303ijB), Yi)\u2212 EP\u0302\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n\u2264 c1 1\nN tr(B\u22baKQKB) + tr(B\u22a4KB)\n( c2 N(log 1 \u03b4 + 2 logN)\nn +\nc3 log 1 \u03b4 + c4\nN\n) .\nThe LHS is the difference between the training error and expected error (with respect to the distribution on domains P\u2217) after applying B.\nThe first term in the bound, involving tr(B\u22baKQKB), quantifies the distributional variance after applying the transform: the higher the distributional variance, the worse the guarantee, tying in with analogous results in Ben-David et al. (2007; 2010). The second term in the bound depends on the size of the distortion tr(B\u22baKB) introduced by B: the more complicated the transform, the worse the guarantee.\nThe bound reveals a tradeoff between reducing the distributional variance and the complexity or size of the transform used to do so. The denominator of (9) is a sum of these terms, so that DICA tightens the bound in Theorem 5.\nPreserving the functional relationship (i.e. central subspace) by maximizing the numerator in (9) should reduce the empirical risk E P\u0302 \u2113(f(X\u0303ijB), Yi). However, a rigorous demonstration has yet to be found."}, {"heading": "3. Experiments", "text": "We illustrate the difference between the proposed algorithms and their singledomain counterparts using a synthetic dataset. Furthermore, we evaluate DICA in two tasks: a classification task on flow cytometry data and a regression task for Parkinson\u2019s telemonitoring.\n3.1. Toy Experiments. We generate 10 collections of ni \u223c Poisson(200) data points. The data in each collection is generated according to a five-dimensional zeromean Gaussian distribution. For each collection, the covariance of the distribution is generated from Wishart distribution W(0.2 \u00d7 I5, 10). This step is to simulate different marginal distributions. The output value is y = sign(b\u22a41 x+ \u01eb1) \u00b7 log(|b \u22a4 2 x+ c + \u01eb2|), where b1, b2 are the weight vectors, c is a constant, and \u01eb1, \u01eb2 \u223c N (0, 1). Note that b1 and b2 form a low-dimensional subspace that captures the functional relationship between X and Y . We then apply the KPCA, UDICA, COIR, and DICA algorithms on the dataset with Gaussian RBF kernels for both X and Y with bandwidth parameters \u03c3x = \u03c3y = 1, \u03bb = 0.1, and \u03b5 = 10\n\u22124. Fig. 2 shows projections of the training and three previously unseen test datasets onto the first two eigenvectors. The subspaces obtained from UDICA and DICA are more stable than for KPCA and COIR. In particular, COIR shows a substantial difference between training and test data, suggesting overfitting.\n3.2. Gating of Flow Cytometry Data. Graft-versus-host disease (GvHD) occurs in allogeneic hematopoietic stem cell transplant recipients when donor-immune cells in the graft recognize the recipient as \u201cforeign\u201d and initiate an attack on the skin, gut, liver, and other tissues. It is a significant clinical problem in the field of allogeneic blood and marrow transplantation. The GvHD dataset (Brinkman et al. 2007) consists of weekly peripheral blood samples obtained from 31 patients following allogenic blood and marrow transplant. The goal of gating is to identify CD3+CD4+CD8\u03b2+ cells, which were found to have a high correlation with the development of GvHD (Brinkman et al. 2007). We expect to find a subspace of cells that is consistent to the biological variation between patients, and is indicative of the GvHD development. For each patient, we select a dataset that contains sufficient numbers of the target cell populations. As a result, we omit one patient due to insufficient data. The corresponding flow cytometry datasets from 30 patients have sample sizes ranging from 1,000 to 10,000, and the proportion of the CD3+CD4+CD8\u03b2+ cells in each dataset ranges from 10% to 30%, depending on the development of the GvHD.\nTo evaluate the performance of the proposed algorithms, we took data from N = 10 patients for training, and the remaining 20 patients for testing. We subsample the training sets and test sets to have 100, 500, and 1,000 data points (cells) each. We compare the SVM classifiers under two settings, namely, a pooling SVM and a distributional SVM. The pooling SVM disregards the inter-patient variation by combining all datasets from different patients, whereas the distributional SVM also takes the inter-patient variation into account via the kernel function (Blanchard et al. 2011)\n(14) K(x\u0303 (i) k , x\u0303 (j) l ) = k1(P i,Pj) \u00b7 k2(x (i) k , x (j) l )\nKPCA\nUDICA\nCOIR\nDICA\nFigure 2. Projections of a synthetic dataset onto the first two eigenvectors obtained from the KPCA, UDICA, COIR, and DICA. The colors of data points corresponds to the output values. The shaded boxes depict the projection of training data, whereas the unshaded boxes show projections of unseen test datasets. The feature representations learnt by UDICA and DICA are more stable across test domains than those learnt by KPCA and COIR.\nwhere x\u0303 (i) k = (P i, x (i) k ) and k1 is the kernel on distributions. We use k1(P i,Pj) =\nexp ( \u2212\u2016\u00b5Pi \u2212 \u00b5Pj\u2016 2 H/2\u03c3 2 1 ) and k2(x (i) k , x (j) l ) = exp(\u2212\u2016x (i) k \u2212 x (j) l \u2016 2/2\u03c322), where \u00b5Pi\nis computed using k2. For pooling SVM, the kernel k1(P i,Pj) is constant for any i and j. Moreover, we use the output kernel l(y (i) k , y (j) l ) = \u03b4(y (i) k , y (j) l ) where \u03b4(a, b) is 1 if a = b, and 0 otherwise. We compare the performance of the SVMs trained on the preprocessed datasets using the KPCA, COIR, UDICA, and DICA algorithms. It is important to note that we are not defining another kernel on top of the preprocessed data. That is, the kernel k2 for KPCA, COIR, UDICA, and DICA is exactly (5). We perform 10-fold cross validation on the parameter grids to optimize for accuracy.\nTable 1 reports average accuracies and their standard deviation over 30 repetitions of the experiments. For sufficiently large number of samples, DICA outperforms other approaches. The pooling SVM and distributional SVM achieve comparable accuracies. The average leave-one-out accuracies over 30 subjects are reported in Table 2 (see supplementary for more detail).\n3.3. Parkinson\u2019s Telemonitoring. To evaluate DICA in a regression setting, we apply it to a Parkinson\u2019s telemonitoring dataset2. The dataset consists of biomedical voice measurements from 42 people with early-stage Parkinson\u2019s disease recruited for a six-month trial of a telemonitoring device for remote symptom progression monitoring. The aim is to predict the clinician\u2019s motor and total UPDRS scoring of Parkinson\u2019s disease symptoms from 16 voice measures. There are around 200 recordings per patient.\nWe adopt the same experimental settings as in \u00a73.2, except that we employ two independent Gaussian Process (GP) regression to predict motor and total UPDRS scores. For COIR and DICA, we consider the output kernel l(y (i) k , y (j) l ) = exp(\u2212\u2016y (i) k \u2212 y (j) l \u2016 2/2\u03c323) to fully account for the affinity structure of the output\n2http://archive.ics.uci.edu/ml/datasets/Parkinson\u2019s+Telemonitoring\nvariable. We set \u03c33 to be the median of motor and total UPDRS scores. The voice measurements from 30 patients are used for training and the rest for testing.\nFig. 3 depicts the results. DICA consistently, though not statistically significantly, outperforms other approaches, see Table 3. Inter-patient (i.e. across domain) variation worsens prediction accuracy on new patients. Reducing this variation with DICA improves the accuracy on new patients. Moreover, incorporating the inter-subject variation via distributional GP regression further improves the generalization ability, see Fig. 3."}, {"heading": "4. Conclusion and Discussion", "text": "To conclude, we proposed a simple algorithm called Domain-Invariant Component Analysis (DICA) for learning an invariant transformation of the data which has proven significant for domain generalization both theoretically and empirically. Theorem 5 shows the generalization error on previously unseen domains grows with the distributional variance. We also showed that DICA generalizes KPCA and COIR, and is closely related to TCA. Finally, experimental results on both synthetic and real-world datasets show DICA performs well in practice. Interestingly, the results also suggest that the distributional SVM, which takes into account inter-domain variation, outperforms the pooling SVM which ignores it.\nThe motivating assumption in this work is that the functional relationship is stable or varies smoothly across domains. This is a reasonable assumption for automatic gating of flow cytometry data because the inter-subject variation of cell population makes it impossible for domain expert to apply the same gating on all subjects, and similarly makes sense for Parkinson\u2019s telemonitoring data. Nevertheless, the assumption does not hold in many applications where the conditional distributions are substantially different. It remains unclear how to develop techniques that generalize to previously unseen domains in these scenarios.\nDICA can be adapted to novel applications by equipping the optimization problem with appropriate constraints. For example, one can formulate a semi-supervised extension of DICA by forcing the invariant basis functions to lie on a manifold or preserve a neighborhood structure. Moreover, by incorporating the distributional variance as a regularizer in the objective function, the invariant features and classifier can be optimized simultaneously.\nAcknowledgments. We thank Samory Kpotufe and Kun Zhang for fruitful discussions and the three anonymous reviewers for insightful comments and suggestions that significantly improved the paper."}, {"heading": "Appendix A. Domain Generalization and Related Frameworks", "text": "The most fundamental assumption in machine learning is that the observations are independent and identically distributed (i.i.d.). That is, each observation comes from the same probability distribution as the others and all are mutually independent. However, this assumption is often violated in practice, in which case the standard machine learning algorithms do not perform well. In the past decades, many techniques have been proposed to tackle scenarios where there is a mismatch between training and test distributions. These include domain adaptation (Bickel et al. 2009a), multitask learning (Caruana 1997), transfer learning (Pan and Yang 2010b), covariate/dataset shift (Quionero-Candela et al. 2009) and concept drift (Widmer and Kurat 1996). To better understand domain generalization, we briefly discuss how it relates to some of these approaches.\nA.1. Transfer learning (see e.g., Pan and Yang (2010b) and references therein). Transfer learning aims at transferring knowledge from some previous tasks to a target task when the latter has limited training data. That is, although there may be few labeled examples, \u201cknowledge\u201d obtained in related tasks may be available. Transfer learning focuses on improving the learning of the target predictive function using the knowledge in the source task. Although not identical, domain generalization can be viewed as a transfer learning when knowledge of the target task is unavailable during training.\nA.2. Multitask learning (see e.g., Caruana (1997) and references therein). The goal of multitask learning is to learn multiple tasks simultaneously \u2013 especially when training examples in each task are scarce. By learning all tasks simultaneously, one expects to improve generalization on individual tasks. An important assumption is therefore that all the tasks are related. Multitask learning differs from domain generalization because learning the new task often requires retraining.\nA.3. Domain adaptation (see e.g., Bickel et al. (2009a) and references therein). Domain adaptation, also known as covariate shift, deals primarily with a mismatch between training and test distributions. Domain generalization deals with a broader setting where training instances may have been collected from multiple source domains. A second difference is that in domain adaptation one observes the target domain during the training time whereas in domain generalization one does not.\nTable 4 summarizes the main differences between the various frameworks."}, {"heading": "Appendix B. Proof of Theorem 1", "text": "Lemma 6. Given a set of distributions P = {P1,P2 . . . ,PN}, the distributional variance of P is VH(P) = 1 N \u2211N i=1\u2016\u00b5Pi \u2212 \u00b5P\u0304\u2016 2 H where \u00b5P\u0304 = (1/N) \u2211N i=1 \u00b5Pi and P\u0304 = 1 N \u2211N i=1 P i. Proof. Let P\u0304 be the probability distribution defined as (1/N) \u2211N\ni=1 P i, i.e., P\u0304(x) =\n(1/N) \u2211N\ni=1 P i(x). It follows from the linearity of the expectation that \u00b5P\u0304 =\n(1/N) \u2211N\ni=1 \u00b5Pi . For brevity, we will denote \u3008\u00b7, \u00b7\u3009H by \u3008\u00b7, \u00b7\u3009. Then, expanding (3) gives\nVH(P) = 1\nN tr(\u03a3) =\n1\nN tr(G)\u2212\n1\nN2\nN\u2211\ni,j=1\nGij\n= 1\nN\nN\u2211\ni=1\n\u3008\u00b5Pi , \u00b5Pi\u3009 \u2212 1\nN2\nN\u2211\ni,j=1\n\u3008\u00b5Pi , \u00b5Pj \u3009\n= 1\nN\n  N\u2211\ni=1\n\u3008\u00b5Pi , \u00b5Pi\u3009 \u2212 2\nN\nN\u2211\ni,j=1\n\u3008\u00b5Pi , \u00b5Pj \u3009+ 1\nN\nN\u2211\ni,j=1\n\u3008\u00b5Pi , \u00b5Pj\u3009\n \n= 1\nN\n  N\u2211\ni=1\n\u3008\u00b5Pi , \u00b5Pi\u3009 \u2212 2 N\u2211\ni=1\n\u2329 \u00b5Pi , 1\nN\nN\u2211\nj=1\n\u00b5Pj\n\u232a +N \u2329 1\nN\nN\u2211\ni=1\n\u00b5Pi , 1\nN\nN\u2211\nj=1\n\u00b5Pj\n\u232a \n= 1\nN\n[ N\u2211\ni=1\n\u3008\u00b5Pi , \u00b5Pi\u3009 \u2212 2 N\u2211\ni=1\n\u3008\u00b5Pi , \u00b5P\u0304\u3009+N\u3008\u00b5P\u0304, \u00b5P\u0304\u3009\n]\n= 1\nN\nN\u2211\ni=1\n( \u3008\u00b5Pi , \u00b5Pi\u3009 \u2212 2 \u00b7 \u3008\u00b5Pi , \u00b5P\u0304\u3009+ \u3008\u00b5P\u0304, \u00b5P\u0304\u3009 )\n= 1\nN\nN\u2211\ni=1\n\u2016\u00b5Pi \u2212 \u00b5P\u0304\u2016 2 H ,\nwhich completes the proof.\nTheorem 1 For a characteristic kernel k, VH(P) = 0 if and only if P1 = P2 = \u00b7 \u00b7 \u00b7 = PN .\nProof. Since k is characteristic, \u2016\u00b5P \u2212 \u00b5Q\u20162H is a metric and is zero iff P = Q for any distributions P and Q (Sriperumbudur et al. 2010). By Lemma 6, VH(P) = 1 N \u2211N i=1\u2016\u00b5Pi \u2212 \u00b5P\u0304\u2016 2 H. Thus, \u2016\u00b5Pi \u2212 \u00b5P\u0304\u2016 2 H = 0 iff P i = P\u0304. Consequently, if VH(P) is zero, this implies that Pi = P\u0304 for all i, meaning that P1 = \u00b7 \u00b7 \u00b7 = P\u2113. Conversely, if P1 = \u00b7 \u00b7 \u00b7 = P\u2113, then \u2016\u00b5Pi \u2212 \u00b5P\u0304\u2016 2 H = 0 is zero for all i and thereby VH(P) = 1 N \u2211N i=1\u2016\u00b5Pi \u2212 \u00b5P\u0304\u2016 2 H is zero."}, {"heading": "Appendix C. Proof of Theorem 2", "text": "Theorem 2 The empirical estimator V\u0302H(S) = 1 N tr(\u03a3\u0302) = tr(KQ) obtained from Gram matrix\nG\u0302ij := 1\nni \u00b7 nj\nni\u2211\nk=1\nnj\u2211\nl=1\nk(x (i) k , x (j) l )\nis a consistent estimator of VH(P).\nProof. Recall that\nVH(P) = 1\nN tr(G)\u2212\n1\nN2\nN\u2211\ni,j=1\nGij and V\u0302H(S) = 1\nN tr(G\u0302)\u2212\n1\nN2\nN\u2211\ni,j=1\nG\u0302ij\nwhere\nGij = \u3008\u00b5Pi , \u00b5Pj\u3009H =\n\u222b\u222b k(x, z) dPi(x) dPj(z)\nG\u0302ij = \u3008\u00b5\u0302Pi , \u00b5\u0302Pj\u3009H = 1\nninj\nni\u2211\nk=1\nnj\u2211\nl=1\nk(x (i) k , x (j) l )\nBy Theorem 15 in Altun and Smola (2006), we have a fast convergence of \u00b5\u0302P to \u00b5P. Consequently, we have G\u0302 \u2192 G, which implies that V\u0302H(S) \u2192 VH(P). Hence, V\u0302H(S) is a consistent estimator of VH(P).\nAppendix D. Derivation of Eq. (8)\nDICA employs the covariance of inverse regressor V(E[\u03c6(X)|Y ]), which can be written in terms of covariance operators. Let H and F be the RKHSes of X and Y endowed with reproducing kernels k and l, respectively. Let \u03a3xx, \u03a3yy, \u03a3xy, and \u03a3yx be the covariance operators in and between the corresponding RKHSes of X and Y . We define the conditional covariance operator of X given Y , denoted by \u03a3xx|y, as\n(15) \u03a3xx|y , \u03a3xx \u2212 \u03a3xy\u03a3 \u22121 yy \u03a3yx .\nThe following theorem from Fukumizu et al. (2004b) states that, under mild conditions, \u03a3xx|y equals the expected conditional variance of \u03c6(X) given Y .\nTheorem 7. For any f \u2208 H, if there exists g \u2208 F such that E[f(X)|Y ] = g(Y ) for almost every Y , then \u03a3xx|y = E[V(\u03c6(X)|Y )].\nUsing the E-V -V -E identity3, the covariance V(E[\u03c6(X)|Y ]) can be expressed in terms of the conditional covariance operators as follow:\n(16) V(E[\u03c6(X)|Y ]) = V(\u03c6(X)) \u2212 E[V(\u03c6(X)|Y )],\nassuming that the inverse regressor E[f(x)|y] is a smooth function of y for any f \u2208 H.\nBy virtue of Theorem 7, the second term in the r.h.s. of (16) is \u03a3xx|y. Since V(\u03c6(X)) = Cov(\u03c6(x), \u03c6(x)) = \u03a3xx, it follows from (15) that the covariance of the inverse regression V(E[\u03c6(X)]|Y ) can be expressed as\n(17) V(E[\u03c6(X)|Y ]) = \u03a3xy\u03a3 \u22121 yy \u03a3yx .\nThe covariance (17) can be estimated from finite samples (x1, y1), . . . , (xn, yn)\nby V\u0302(E[\u03c6(X)|Y ]) = \u03a3\u0302xy\u03a3\u0302 \u22121 yy \u03a3\u0302yx where \u03a3\u0302xy = 1 n \u03a6x\u03a6 \u22a4 y and \u03a6x = [\u03c6(x1), . . . , \u03c6(xn)] and \u03a6y = [\u03d5(y1), . . . , \u03d5(yn)]. Let K and L denote the kernel matrices computed over samples {x1, x2, . . . , xn} and {y1, y2, . . . , yn}, respectively. We have\nV\u0302(E[\u03c6(X)|Y ]) =\n( 1\nn \u03a6x\u03a6\n\u22a4 y\n)( 1\nn (\u03a6y\u03a6\n\u22a4 y + n\u03b5I)\n)\u22121 ( 1\nn \u03a6y\u03a6\n\u22a4 x\n)\n= 1\nn \u03a6x\u03a6\n\u22a4 y \u03a6y ( \u03a6\u22a4y \u03a6y + n\u03b5In )\u22121 \u03a6\u22a4x\n= 1\nn \u03a6xL (L+ n\u03b5In)\n\u22121 \u03a6\u22a4x(18)\n3V(X) = E[V(X|Y )] + V(E[X|Y ]) for any X, Y .\nwhere L = \u03a6\u22a4y \u03a6y and I is the identity operator. The second equation is obtained by applying the fact that (\u03a6y\u03a6 \u22a4 y + n\u03b5I)\u03a6y = \u03a6y(\u03a6 \u22a4 y \u03a6y + n\u03b5In).\nFinally, using \u03a3\u0302xx = 1 n \u03a6x\u03a6 \u22a4 x and recalling that K = \u03a6 \u22a4 x \u03a6x, we obtain\nb\u22a4k \u03a3\u0302 \u22121 xx V\u0302(E[X |Y ])\u03a3\u0302xxbk = b \u22a4 k\n( 1\nn \u03a6x\u03a6\n\u22a4 x\n)\u22121 ( 1\nn \u03a6xL (L+ n\u03b5In)\n\u22121 \u03a6\u22a4x\n)( 1\nn \u03a6x\u03a6\n\u22a4 x ) bk\n= 1\nn \u03b2\u22a4k \u03a6 \u22a4 x\n( \u03a6x\u03a6 \u22a4 x )\u22121 \u03a6xL (L+ n\u03b5In) \u22121 \u03a6\u22a4x ( \u03a6x\u03a6 \u22a4 x ) \u03a6x\u03b2k\n= 1\nn \u03b2\u22a4k \u03a6 \u22a4 x \u03a6x\n( \u03a6\u22a4x \u03a6x )\u22121 L (L+ n\u03b5In) \u22121 \u03a6\u22a4x ( \u03a6x\u03a6 \u22a4 x ) \u03a6x\u03b2k\n= 1\nn \u03b2\u22a4k L(L+ n\u03b5I) \u22121K2\u03b2k\nand\nb\u22a4k bk = \u03b2 \u22a4 k \u03a6 \u22a4 x \u03a6x\u03b2k = \u03b2 \u22a4 k K\u03b2k\nas desired.\nAppendix E. Derivation of Lagrangian (10)\nObserve that optimization\n(19) max B\u2208Rn\u00d7m\ntr ( B\u22a4XB )\ntr (B\u22a4Y B)\nis invariant to rescaling B 7\u2192 \u03b1 \u00b7 B. Optimization (19) is therefore equivalent to\nmax B\u2208Rn\u00d7m\ntr ( B\u22a4XB )\nsubject to: tr ( B\u22a4Y B ) = 1,\nwhich yields Lagrangian (20) L = tr ( B\u22a4XB ) \u2212 tr (( B\u22a4Y B \u2212 I ) \u0393 ) ."}, {"heading": "Appendix F. Proof of Theorem 5", "text": "We consider a scenario where distributions Pi are drawn according to P\u2217 with probability \u00b5i. Introduce shorthand X\u0303ij for (P\n(i), Xij) for a distribution on PX and a corresponding random variable on X .\nThe quantity of interest is the difference between the expected and empirical loss of a classifier f : PX \u00d7X \u2192 Y under loss function \u2113 : Y \u00d7 Y \u2192 R+.\nAssumptions. The loss function \u2113 : R \u00d7 Y \u2192 R+ is \u03c6\u2113-Lipschitz in its first variable and bounded by U\u2113. The kernel kX is bounded by UX . Assume that all distributions in P\u2217 are mapped into a ball of size UP by \u03a8P. Finally, since kP is a is a square exponential, there is a constant LP such that\n\u2016\u03a6P(v)\u2212 \u03a6P(w)\u2016 \u2264 LP\u2016v \u2212 w\u2016 for all v, w.\nRecall that N is the number of sampled domains, ni is the number of samples\nin domain i, and n = \u2211N\ni=1 ni is the total number of samples. The proof assumes ni = nj for all i, j.\nTheorem 5. Assumes the conditions above hold. Then with probability at least 1\u2212 \u03b4\nsup \u2016f\u2016H\u22641\n\u2223\u2223\u2223E\u2217PEP\u2113(f(X\u0303ijB), Yi)\u2212 EP\u0302\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n\u2264 c1 1\nN tr(B\u22baKLKB) + tr(B\u22a4KB)\n( c2 N \u00b7 (log \u03b4\u22121 + 2 logN)\nn + c3\nlog \u03b4\u22121\nN + c4 N\n) .\nRemark 1. Recall that \u03a6x = [\u03c6(x1), . . . , \u03c6(xn)]. The composition xt 7\u2192 kt \u00b7 B, where kt = [k(x1, xt), . . . , k(xn, xt)], can therefore be rewritten as \u03c6(xt) \u00b7B = \u03c6(xt) \u00b7 \u03a6x \u00b7 B.\nProof. The proof modifies the approach taken in Blanchard et al. (2011) to handle the preprocessing via transform B, and the fact that we work with squared errors. Parts of the proof that pass through largely unchanged are omitted.\nWe repeatedly apply the inequality |a+b|2 \u2264 2|a|2+2|b|2. However, we only incur the multiplication-by-2 penalty once since |a1 + \u00b7 \u00b7 \u00b7+ an|2 \u2264 2|a1|2 + \u00b7 \u00b7 \u00b7+ 2|an|2.\nDecompose\nsup \u2016f\u2016H\u22641\n\u2223\u2223\u2223E\u2217PEP\u2113(f(X\u0303ijB), Yi)\u2212 EP\u0302\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n\u2264 sup \u2016f\u2016H\u22641\n2\nN\nN\u2211\ni=1\n\u2223\u2223\u2223E\u2217PEP\u2113(f(X\u0303ijB), Yi)\u2212 EPi\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n+ sup \u2016f\u2016H\u22641\n2\nN\nN\u2211\ni=1\n\u2223\u2223\u2223EPi\u2113(f(X\u0303ijB), Yi)\u2212 EP\u0302i\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n+ sup \u2016f\u2016H\u22641\n2\nN\nN\u2211\ni=1\n\u2223\u2223\u2223EP\u0302i\u2113(f(X\u0303ijB), Yi)\u2212 EP\u0302\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n= (A) + (B) + (C) .\nControl of (C):\n(C) = sup \u2016f\u2016H\u22641\n2\nN\nN\u2211\ni=1\n\u2223\u2223\u2223EP\u0302i\u2113(f(X\u0303ijB), Yi)\u2212 EP\u0302\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n\u2264 \u03c62\u2113 sup \u2016f\u2016H\u22641\n2\nN\nN\u2211\ni=1\n\u2223\u2223\u2223EP\u0302if(X\u0303ijB)\u2212 EP\u0302f(X\u0303ijB) \u2223\u2223\u2223 2\n= \u03c62\u2113 \u00b7 2\nN\nN\u2211\ni=1\n\u2225\u2225\u2225\u03a8P(P\u0302i)\u2297 \u00b5P\u0302iB \u2212\u03a8P(P\u0302)\u2297 \u00b5P\u0302B \u2225\u2225\u2225 2\nNote that \u2016\u03a8P(\u00b5(P))\u20162 \u2264 LP \u00b7 \u2016\u00b5P\u20162 \u2264 LPUP. Therefore,\n(C) \u2264 \u03c62\u2113LPUP 2\nN\nN\u2211\ni=1\n\u2225\u2225\u00b5 P\u0302i B \u2212 \u00b5 P\u0302 B \u2225\u22252 .\nBy the proof of Theorem 1 and since \u03a6\u22a4x B = KB, we have\n(C) \u2264 2\u03c62\u2113LPUP 1\nN tr(KBB\u22baKL).\nControl of (B): Similarly,\n(B) = sup \u2016f\u2016H\u22641\n2\nN\nN\u2211\ni=1\n\u2223\u2223\u2223EPi\u2113(f(X\u0303ijB), Yi)\u2212 EP\u0302i\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\n\u2264 2\u03c62\u2113LPUP \u00b7 1\nN\nN\u2211\ni=1\n\u2225\u2225\u00b5PiB \u2212 \u00b5P\u0302iB \u2225\u22252\n\u2264 2\u03c62\u2113LPUP \u00b7 \u2016B\u2016 2 HS \u00b7\n1\nN\nN\u2211\ni=1\n\u2225\u2225\u00b5Pi \u2212 \u00b5P\u0302i \u2225\u22252\nHere we follow the strategy applied by Blanchard et al. (2011) to control their\nterm (I) in Theorem 5.1. Assume ni = nj for all i, j and recall n = \u2211N\ni=1 ni so ni = n/N for all i.\nBy Hoeffding\u2019s inequality in Hilbert space, with probability greater than 1 \u2212 \u03b4 the following inequality holds\n\u2225\u2225\u2225\u2225\u2225\u2225 1 ni ni\u2211\nj=1\n\u00b5(X\u0302ij)\u2212 EP(i)\u00b5(Xij) \u2225\u2225\u2225\u2225\u2225\u2225 2 \u2264 9UX N \u00b7 log 2\u03b4\u22121 n .\nApplying the union bound obtains\n(Ib) \u2264 18\u03c62\u2113LPUPUX \u00b7 \u2016B\u2016 2 HS \u00b7\nN \u00b7 (log \u03b4\u22121 + 2 logN)\nn .\nControl of (A):\n(A) = sup \u2016f\u2016H\u22641\n2\nN\nN\u2211\ni=1\n\u2223\u2223\u2223E\u2217PEP\u2113(f(X\u0303ijB), Yi)\u2212 EPi\u2113(f(X\u0303ijB), Yi) \u2223\u2223\u2223 2\nFollowing the strategy used by Blanchard et al. (2011) to control (II) in Theorem 5.1, we obtain\n(A) \u2264 c3 \u03c62\u2113U 2 XUP + U\u2113 log \u03b4 \u22121\nN \u00b7 \u2016B\u20162HS.\nEnd of proof: We have that K is invertible since \u03a3\u0302xx is assumed to be invertible. It follows that the trace tr(B\u22a4KB) defines a norm which coincides with the HilbertSchmidt norm \u2016B\u20162HS. Combining the three inequalities above concludes the proof."}, {"heading": "Appendix G. Leave-one-out accuracy", "text": "Figure 4 depicts the leave-one-out accuracies of different approaches evaluated on each subject in the dataset. Average leave-one-out accuracies are reported in Table 2. The distributional SVM outperforms the pooling SVM in this setting, possibly because of the relatively large number of training subjects, i.e., 29 subjects. Using the invariant features learnt by DICA also gives higher accuracies than other approaches."}], "references": [{"title": "Unifying divergence minimization and statistical inference via convex duality", "author": ["Y. Altun", "A. Smola"], "venue": "In Proc. of Conf. on Learning Theory (COLT),", "citeRegEx": "Altun and Smola.,? \\Q2006\\E", "shortCiteRegEx": "Altun and Smola.", "year": 2006}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "T.C. Agnan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Berlinet and Agnan.,? \\Q2004\\E", "shortCiteRegEx": "Berlinet and Agnan.", "year": 2004}, {"title": "Discriminative learning under covariate shift", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "Journal of Machine Learning Research, 10:2137\u20132155,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Discriminative learning under covariate shift", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Generalizing from several related classification tasks to a new unlabeled sample", "author": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Blanchard et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2011}, {"title": "High-content flow cytometry and temporal data analysis for defining a cellular signature of graft-versus-host disease", "author": ["R.R. Brinkman", "M. Gasparetto", "S.-J.J. Lee", "A.J. Ribickas", "J. Perkins", "W. Janssen", "R. Smiley", "C. Smith"], "venue": "Biol Blood Marrow Transplant,", "citeRegEx": "Brinkman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brinkman et al\\.", "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Universal kernels on Non-Standard input spaces", "author": ["A. Christmann", "I. Steinwart"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Christmann and Steinwart.,? \\Q2010\\E", "shortCiteRegEx": "Christmann and Steinwart.", "year": 2010}, {"title": "Kernel Dimensionality Reduction for Supervised Learning", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "andM.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Dataset Shift in Machine Learning, chapter Covariate Shift by Kernel Mean Matching, pages 131\u2013160", "author": ["A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Learning the shared subspace for multi-task clustering and transductive transfer classification", "author": ["Q. Gu", "J. Zhou"], "venue": "In Proceedings of the 9th IEEE International Conference on Data Mining,", "citeRegEx": "Gu and Zhou.,? \\Q2009\\E", "shortCiteRegEx": "Gu and Zhou.", "year": 2009}, {"title": "Central subspace dimensionality reduction using covariance operators", "author": ["M. Kim", "V. Pavlovic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kim and Pavlovic.,? \\Q2011\\E", "shortCiteRegEx": "Kim and Pavlovic.", "year": 2011}, {"title": "Sliced inverse regression for dimension reduction", "author": ["K.-C. Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li.,? \\Q1991\\E", "shortCiteRegEx": "Li.", "year": 1991}, {"title": "Learning from distributions via support measure machines", "author": ["K. Muandet", "K. Fukumizu", "F. Dinuzzo", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Muandet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Muandet et al\\.", "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Flexible modeling of latent task structures in multitask learning", "author": ["A. Passos", "P. Rai", "J. Wainer", "H.D. III"], "venue": "In Proceedings of the 29th international conference on Machine learning,", "citeRegEx": "Passos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2012}, {"title": "Dataset Shift in Machine Learning", "author": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": null, "citeRegEx": "Quionero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quionero.Candela et al\\.", "year": 2009}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the 18th International Conference In Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B.K. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "G.R.G. Lanckriet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Learning in the Presence of Concept Drift and Hidden Contexts", "author": ["G. Widmer", "M. Kurat"], "venue": "Machine Learning,", "citeRegEx": "Widmer and Kurat.,? \\Q1996\\E", "shortCiteRegEx": "Widmer and Kurat.", "year": 1996}, {"title": "Kernel sliced inverse regression with applications to classification", "author": ["H.-M. Wu"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Wu.,? \\Q2008\\E", "shortCiteRegEx": "Wu.", "year": 2008}, {"title": "2009a) and references therein). Domain adaptation, also known as covariate shift, deals primarily with a mismatch between training and test distributions. Domain generalization deals with a broader setting where training instances", "author": ["Bickel"], "venue": null, "citeRegEx": "Bickel,? \\Q2009\\E", "shortCiteRegEx": "Bickel", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "To illustrate the problem, consider an example taken from Blanchard et al. (2011) which studied automatic gating of flow cytometry data.", "startOffset": 58, "endOffset": 82}, {"referenceID": 16, "context": "A considerable effort has been made in domain adaptation and transfer learning to remedy this problem, see Pan and Yang (2010a), Ben-David et al.", "startOffset": 107, "endOffset": 128}, {"referenceID": 2, "context": "A considerable effort has been made in domain adaptation and transfer learning to remedy this problem, see Pan and Yang (2010a), Ben-David et al. (2010) and references therein.", "startOffset": 129, "endOffset": 153}, {"referenceID": 20, "context": "2004a), transfer component analysis (TCA) (Pan et al. 2011), and covariance operator inverse regression (COIR) (Kim and Pavlovic 2011), see \u00a72.", "startOffset": 42, "endOffset": 59}, {"referenceID": 14, "context": "Pan and Yang (2010a) and references therein).", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Recently, Blanchard et al. (2011) proposed an augmented SVM that incorporates empirical marginal distributions into the kernel.", "startOffset": 10, "endOffset": 34}, {"referenceID": 6, "context": "Recently, Blanchard et al. (2011) proposed an augmented SVM that incorporates empirical marginal distributions into the kernel. A detailed error analysis showed universal consistency of the approach. We apply methods from Blanchard et al. (2011) to derive theoretical guarantees on the finite sample performance of DICA.", "startOffset": 10, "endOffset": 246}, {"referenceID": 7, "context": "Using the samples S, our goal is to produce an estimate f : PX \u00d7 X \u2192 R that generalizes well to test samples S = {x (t) k } nt k=1 drawn according to some unknown distribution P \u2208 PX (Blanchard et al. 2011).", "startOffset": 183, "endOffset": 206}, {"referenceID": 25, "context": ", all the information about the distribution is preserved (Sriperumbudur et al. 2010).", "startOffset": 58, "endOffset": 85}, {"referenceID": 15, "context": "This nonlinear extension implies that E[X |Y ] lies on a function space spanned by {\u03a3xxck}k=1, which coincide with the eigenfunctions of the operator V(E[X |Y ]) (Wu 2008, Kim and Pavlovic 2011). Since we always work in H, we drop \u03c6 from the notation below. To avoid slicing the output space explicitly (Li 1991, Wu 2008), we exploit its kernel structure when estimating the covariance of the inverse regressor. The following result from Kim and Pavlovic (2011) states that, under a mild assumption, V(E[X |Y ]) can be expressed in terms of covariance operators: Theorem 4.", "startOffset": 172, "endOffset": 462}, {"referenceID": 23, "context": "If there is only a single distribution then unsupervised DICA reduces to KPCA since KQK = 0 and finding B requires solving the eigensystem KB = B\u0393 which recovers KPCA (Sch\u00f6lkopf et al. 1998).", "startOffset": 167, "endOffset": 190}, {"referenceID": 20, "context": "If there are two domains, source PS and target PT , then UDICA is closely related \u2013 though not identical to \u2013 Transfer Component Analysis (Pan et al. 2011).", "startOffset": 138, "endOffset": 155}, {"referenceID": 7, "context": "We adapt an approach to this problem developed in Blanchard et al. (2011) to prove a generalization bound that applies after transforming the empirical sample using B.", "startOffset": 50, "endOffset": 74}, {"referenceID": 8, "context": "The GvHD dataset (Brinkman et al. 2007) consists of weekly peripheral blood samples obtained from 31 patients following allogenic blood and marrow transplant.", "startOffset": 17, "endOffset": 39}, {"referenceID": 8, "context": "The goal of gating is to identify CD3CD4CD8\u03b2 cells, which were found to have a high correlation with the development of GvHD (Brinkman et al. 2007).", "startOffset": 125, "endOffset": 147}, {"referenceID": 7, "context": "The pooling SVM disregards the inter-patient variation by combining all datasets from different patients, whereas the distributional SVM also takes the inter-patient variation into account via the kernel function (Blanchard et al. 2011)", "startOffset": 213, "endOffset": 236}], "year": 2012, "abstractText": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.", "creator": "gnuplot 4.4 patchlevel 3"}}}