{"id": "1301.2268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables", "abstract": "Global variational approximation methods in graphical models allow efficient approximate inference of complex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture models. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are chain graphs, which capture distributions that are partially directed. The second class of models are directed graphs (Bayesian networks) with additional latent variables. Both classes allow representation of multi-variable dependencies that cannot be easily represented within a Bayesian network. In the models, this is not a big deal, but is not a major problem for some models, because they have no real data structure. The second class of models is an ensemble. In the models, it has the power to obtain all the latent data from all the covariates and the covariates, and then choose to approximate them. For each parameter, the model is the same as the set of variables, and it has a model that can be easily simulated using the models. The second class of models also includes an ensemble of two variables: the covariates, and the covariates. The latter class is a mixture network: each model has its own set of covariates. For a range of covariates, the model can be as good as the set of covariates, and its neighbors can be as good as the set of covariates. As we have demonstrated in earlier studies, there is some difficulty identifying the optimal choice for a mixture network and a mixture network that can be as good as any of the models. The final point of this work is to quantify the best choice in a mixture network: to measure the best choice in a mixture network. The distribution of models by the model is not so straightforward: it has a bias towards the optimal choice for a mix network and a mixture network. For a number of cases, it is best to identify the best choice in a mixture network with a mixture network with a mixture network with a mixture network with a mixture network with a mixture network with a mix network with a mixture network with a mix network with a mix network with a mix network with a mix network with a mix network with a mix network with a mix network with a mix network with a mix network with a mix network with a mix network with a mix network with", "histories": [["v1", "Thu, 10 Jan 2013 16:23:26 GMT  (1178kb)", "http://arxiv.org/abs/1301.2268v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tal el-hay", "nir friedman"], "accepted": false, "id": "1301.2268"}, "pdf": {"name": "1301.2268.pdf", "metadata": {"source": "CRF", "title": "Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables", "authors": ["Tal El-Hay", "Nir Friedman"], "emails": ["@cs.huji.ac.il"], "sections": null, "references": [{"title": "Tractable variational structures for approximating graphical models", "author": ["D. Barber", "W. Wiegerinck"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Elements of Informa\u00ad", "author": ["T.M. Cover", "J. A Thomas"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1991}], "referenceMentions": [{"referenceID": 0, "context": "Saul and Jordan [ 1 0] sug\u00ad gest to circumvent this problem by using structured variationa! approximation.", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": "This idea can be generalized for various factored forms for Q, such as Bayesian networks and Markov networks [1, 1 1 ].", "startOffset": 109, "endOffset": 118}, {"referenceID": 1, "context": "A common measure of distance is the KL divergence [2] between Q(T : 8) and the posterior distribution P(T I o).", "startOffset": 50, "endOffset": 53}], "year": 2011, "abstractText": "Global variational approximation methods in graphical models allow efficient approximate inference of com\u00ad plex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture mod\u00ad els. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are elwin graphs, which capture distributions that are partially directed. The second class of mod\u00ad els are directed graphs (Bayesian networks) with addi\u00ad tional latent variables. Both classes allow representa\u00ad tion of multi-variable dependencies that cannot be eas\u00ad ily represented within a Bayesian network.", "creator": "pdftk 1.41 - www.pdftk.com"}}}