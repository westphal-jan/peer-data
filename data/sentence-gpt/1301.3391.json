{"id": "1301.3391", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Feature grouping from spatially constrained multiplicative interaction", "abstract": "We present a feature learning model that learns to encode relationships between images. The model is defined as a Gated Boltzmann Machine, which is constrained such that hidden units that are nearby in space can gate each other's connections. We show how frequency/orientation \"columns\" as well as topographic filter maps follow naturally from training the model on image pairs. The model also helps explain why square-pooling models yield feature groups with similar grouping properties. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformation-learning model.", "histories": [["v1", "Tue, 15 Jan 2013 16:06:11 GMT  (769kb,D)", "https://arxiv.org/abs/1301.3391v1", "9 pages, 6 figures"], ["v2", "Wed, 16 Jan 2013 16:43:56 GMT  (1145kb,D)", "http://arxiv.org/abs/1301.3391v2", "9 pages, 6 figures"], ["v3", "Mon, 11 Mar 2013 15:38:05 GMT  (1209kb,D)", "http://arxiv.org/abs/1301.3391v3", "(new version:) added training formulae; added minor clarifications"]], "COMMENTS": "9 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["felix bauer", "roland memisevic"], "accepted": true, "id": "1301.3391"}, "pdf": {"name": "1301.3391.pdf", "metadata": {"source": "CRF", "title": "Feature grouping from spatially constrained multiplicative interaction", "authors": ["Felix Bauer", "Roland Memisevic"], "emails": ["fbauer@fias.uni-frankfurt.de", "memisevr@iro.umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Feature-learning methods have started to become a standard component in many computer-vision pipelines, because they can generate representations which are better at encoding the content of images than raw images themselves. Feature learning works by projecting local image patches onto a set of feature vectors (aka. \u201cfilters\u201d), and using the vector of filter responses as the representation of the patch. This representation gets passed on to further processing modules like spatial pooling and classification. Filters can be learned using a variety of criteria, including maximization of sparseness across filter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others. Under any of these learning criteria, Gabor features typically emerge when training on natural image patches.\nThere has been an increasing interest recently in imposing group structure on learned filters. For learning, filters are encouraged to come in small groups, such that all members of a group share certain properties. The motivation for this is that group structure can explain several biological phenomena such as the presence of complex cells [6], it provides a simple way to model dependencies between features (eg., [9] and references therein), and it can make learned representations more robust to small transformations which is useful for recognition [10]. Filter grouping is referred to also as \u201cstructured sparse coding\u201d or \u201cgroup sparse coding\u201d. Feature grouping can also be used as a way to obtain topographic feature maps [8]. To this end, features are layed out in a 2-dimensional grid, and groups are defined on this grid such that each filter group shares filters with its neighboring groups. In other words, groups overlap with each other. Training feature grouping and topographic models on natural image patches typically yields Gabor filters whose frequency, orientation and position is similar for all the filters within a group. Phase, in contrast, tends to vary randomly across the filters within a group (eg. [7, 10, 8]).\nVarious approaches to performing group sparse coding and topographic feature learning have been proposed. Practically all of these are based on the same recipe: The set of filters is pre-partitioned\nar X\niv :1\n30 1.\n33 91\nv3 [\ncs .L\nG ]\n1 1\nM ar\ninto groups before training. Then during learning, a second layer computes a weighted sum over the squares of all filter responses within a group (eg., [6, 9, 10]). The motivation for using squarepooling architectures to learn group structure is based in part on empirical evidence (it seems to work across many models and learning objectives). There have also been various attempts to explain it, based on a variety of heuristics: One explanation, for example, follows from the fact that some nonlinearity must be used before pooling features in a group, because in the absence of a nonlinearity, we would wind up with a standard (linear) feature learning model. The square is a non-linearity that is simple and canonical. It can also be shown that even-symmetric functions, like the square, applied to filter responses, show strong dependencies. So they seem to capture a lot of what we are missing after performing a single layer of feature learning [8]. Another motivation is that Gabor features are local Fourier components, so computing squares is like computing spectral energies. Finally, it can be shown that in the presence of a upstream square-root non-linearity, using squared features generalizes standard feature learning, since it degenerates to a standard feature-learning model when using group size 1 [10]. For a summary of these various heuristics, see [8] (page 215). Topographic grouping of frequency, orientation and position is also a well-known feature of mammalian brains (eg., [8]). Thus, another motivation for using square-pooling in feature learning in general has been that, by means of replicating this effect, it may yield models that are biologically consistent.\nIn this work, we show that a natural motivation for the use of squared filter responses can be derived from the perspective of encoding relationships between images. In particular, we show that the emergence of group structure and topography follows automatically from the computation of binocular disparity or motion, if we assume that neurons that are nearby in space exert multiplicative influences on each other. Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14]. It may help shed light onto the close relationship between topographic organization and motion processing as well as binocular vision. That way, it may also help shed light onto the phenomenon that topographic filter maps do not seem to be present in rodents [20]."}, {"heading": "2 Factored Gated Boltzmann Machine", "text": "While feature learning has been applied predominantly to single, static images in the past, there has been an increasing interest recently in learning features to encode relationships between multiple images, for example, to encode motion (eg., [11, 22]). We focus in this work on the Gated Boltzmann Machine (GBM) [15, 22] which models the relationship between two binary images x and y using the three-way energy function\nE(x,y,h) = \u2211 ijk wijkxiyjhk (1)\nThe energy gets exponentiated and normalized to define the probability over image pairs (we drop any bias-terms here to avoid clutter):\np(x,y) = 1\nZ \u2211 h exp ( E(x,y,h) ) , Z = \u2211 x,y,h exp ( E(x,y,h) ) (2)\nBy adding appropriate penalty terms to the log-likelihood, one can extend the model to learn realvalued images [15].\nSince there is a product involving every triplet of an input pixel, an output pixel and a mapping unit, the number of parameters is roughly cubic in the number of pixels. To reduce that number, [16] suggested factorizing the three-way parameter tensor W with entries wijk in Equation 1 into a three-way inner product:\nwijk = \u2211 f wxifw y jfw h kf (3)\nHere, f is a latent dimension that has to be chosen by hand or by cross-validation. This form of tensor factorization is also known as PARAFAC or \u201ccanonical decomposition\u201d in the literature, and it can be viewed as a three-way generalization of the SVD [2]. It is illustrated in Figure 1 (left). The factorization makes use of a diagonal core tensor, that contains ones along its diagonal and that is zero elsewhere. Plugging in the factorized representation forW and using the distributive law yields\nthe factorized energy function: E = \u2211 f (\u2211 i wxifxi )(\u2211 j wyjfyj )(\u2211 k whkfhk )\n(4)\nInferring the transformation h, given two images, x and y, is efficient, because the hidden variables are independent, given the data [16]. More specifically we have p(h|x,y) =\u220fk p(hk|x,y) with\np(hk|x,y) = \u03c3 (\u2211\nf\nwhkf (\u2211\ni\nwxifxi )(\u2211\nj\nwyjfyj )) , (5)\nwhere \u03c3 is the logistic sigmoid \u03c3(z) = 11+exp(\u2212z) . Thus, to perform inference, images are projected onto F basis functions (\u201cfilters\u201d) and those basis functions that are in correspondence (i.e., that have the same index f ) are multiplied. Finally, each hidden unit, hk, receives a weighted sum over all products as input. An illustration is shown in Figure 1 (middle, right).\nIt is important to note that projections onto filters typically do not serve to reduce the dimensionality of inputs. Rather, it is the restricted connectivity in the projected space that leads to the reduction in the number of parameters. In fact, it is not uncommon to use a number of factors that is larger than the dimensionality of the input data (for example, [16, 19]).\nTo train the model, one can use contrastive divergence [16], score-matching or a variety of other approximations. Recently, [21, 13] showed that one may equivalently add a decoder network, effectively turning the model into a \u201cgated\u201d version of a de-noising auto-encoder [24] and train it using back-prop. Inference is then still the same as in a Boltzmann machine. We use this approach in most of our experiments, after verifying that the performance is similar to contrastive divergence. More specifically, for a training image pair (x,y), let ph(x,y) denote the vector of inferred hidden probabilities (Eq. 5) and let Wx,Wy,Wh denote the matrices containing the input filters, output filters, and hidden filters, respectively (stacked column-wise). Training now amounts to minimizing the average reconstruction error ( y \u2212 y\u0302(x) )2 + ( x\u2212 x\u0302(y) )2 , with\ny\u0302(x) =Wy (( WTh ph(x,y) ) \u2217 ( WTx x )) , (6)\nwhere \u2217 denotes element-wise multiplication. x\u0302(y) is defined analogously, with all occurrences of x and y exchanged. One may add noise to the data during training (but reconstruct the original, not noisy, data to compute the cost) [24]. We observed that this helps localize filters on natural video, but on synthetic data (shifts and rotations) it is possible to train without noise."}, {"heading": "2.1 Phase usage in modeling transformations", "text": "Figure 2 (left, top) shows filter-pairs that were learned from translated random-dot images using a Factored Gated Boltzmann Machine (for details on training, see Section 4 below). Training on translations turns filters into Fourier components, as was initially observed by [16]. The left-bottom plot shows histograms over the occurrence of frequencies and orientations for input- and outputimage filters, respectively. These were generated by first performing a 2D-DFT on each filter and\nthen picking the frequency and orientation of the strongest component for the filter. The histograms show that the learned filters evenly cover the space of frequencies and orientations. This is to be expected, as all frequencies and orientations contribute equally to the set of random translations (e.g., [4]).\nIt is also well-known, however, that multiple different translations will affect each frequency and orientation differently. More specifically, any given translation induces a set of phase-shifts for every component of a given frequency and orientation. The phase-shift depends linearly on frequency [4]. Likewise, two different image translations of the same orientation will induce two different phase-shifts for each frequency/orientation. In order to represent translations, it is necessary to specify the phase at every frequency and orientation. This shows that mapping units in a GBM must have access to multiple different phase-shifts at every frequency and orientation to be able to represent translations. Also, there is no need to multiply filter responses of different frequencies and/or orientations, only of different phases.\nAs a repercussion, for each input filter in Figure 2 (left, top) there need to be multiple phase-shifted copies present in the output-filter bank, so that filters with varying phase differences can be matched (which means the filters\u2019 responses will be multiplied with each other). Likewise for each output filter. When the number of factors is small, the model has to find a compromise, for example, by connecting hidden variables, hk, to multiple phase-shifted versions of filters with the correct phaseshift but with only a similar, but not the same frequency and orientation. Figure 2 (right) depicts the occurrences of phase differences between input- and output-image filters. It shows that the model learned to use a variety of phase differences at each frequency/orientation to represent the training transformations. In the model, each phase difference corresponds to exactly one filter pair.\nThe analysis generalizes to other transformations, such as rotations or natural videos (e.g., [14]). In particular, it also generalizes to Gabor features which are localized Fourier features that emerge when training the GBM on natural video (e.g., [22])."}, {"heading": "3 Group gating", "text": "The analysis in the previous section strongly suggests using a richer connectivity that supports the re-use of filters. In this case the model can learn to match any filter wxf with multiple phase-shifted copies wyf of itself rather than with a single one. All phase differences in Figure 2 (right) can then in principle be obtained using a much smaller number of filters.\nOne way to support the re-use of filters to represent multiple phase-shifts is by relaxing the diagonal factorization (Eq. 3) with a factorization that allows for a richer connectivity:\nwijk = \u2211 def Cdefw x idw y jew z kf (7)\nwhere we Cdef are the components of a (non-diagonal) core tensor C. Note that, if C is diagonal so that Cdef = 1 iff d = e = f , we would recover the PARAFAC factorization (Eq. 3). The energy function now turns into (cf. Eq. 10):\nE = \u2211 def Cdef (\u2211 i wxidxi )(\u2211 j wyjeyj )(\u2211 k whkfhk )\n(8)\nand inference into p(hk|x,y) = \u03c3 (\u2211\ndef\nCdefw h kf (\u2211 i wxidxi )(\u2211 j wyjeyj ))\n(9)\nAs the number of factors is typically large, a full matrix C would be computationally too expensive. In fact, as we discuss in Section 2 there is no other reason to project onto filters than reducing connectivity in the projected representation. Also, by the discussion in the previous section, there is very a strong inductive bias towards allowing groups of factors to interact.\nThis suggests using a core-tensor that allows features to come in groups of a fixed, small size, such that all pairs of filters within a group can provide products to mapping units. By the analysis in Section 2, training on translations or natural video is then likely to yield groups of roughly constant frequency and orientation and to differ with respect to phase. We shall refer to this model as \u201cgroupgating\u201d model in the following. As the values Cdef may be absorbed into the factor matrices and are learned from data, it is sufficient to distinguish only between non-zero and zero entries in C, and we set all non-zero entries to one in what follows.\nBy defining the filter groups Gg, g = 1, . . . , G, we can write inference in the model consequently as p(hk|x,y) = \u03c3 (\u2211\ng \u2211 d\u2208Gg \u2211 e\u2208Gg whk,d\u00b7|Gg|+e (\u2211 i wxidxi )(\u2211 j wyjeyj ))\n(10)\nwhich is illustrated in Figure 1 (right). Note that each hidden unit can still pool over all pair-wise products between features. The overall number of feature products is equal to the number of groups times the group size. In practice, it makes sense to set the number of factors to be a multiple of the group size, so that all groups can have the same size.\nA convenient way to implement the group-gating model is by computing all required products by creating G copies of the input-factor matrix and G copies of the output-factor matrix and permuting the columns (factors) of one of the two matrices appropriately. (Note in particular that when using a large number of factors, masking, i.e. forcing a subset of entries of C to be zero, would not be feasible.) It is possible to reduce the number of filters further by allowing for multiplicative interactions between only one filter per group from the input image and all filters from the output image (or vice versa). This leads to an asymmetric model, where the number of filters is not equal for both images."}, {"heading": "3.1 Significance for square-pooling models", "text": "It is interesting to note that the same analysis applies to the responses of energy-model complex cells (e.g., [7, 19]), too, if images and filters are contrast normalized [14]. In this case, the response of the energy model is the same as a factored GBM mapping unit applied to a single image, i.e. x = y (see, for example, [19, 14]). This shows that the gating interpretation of frequency/orientation groups and topographic structure applies to these models, too.\nIn the same way, we can interpret a square-pooling model applied to a single image as an encoding of the relationship between, say, the rows or the columns within the patch. In natural images, the predominant transformation that takes a row to the next row is a local translation. The emergence of oriented Gabor features can therefore also be viewed as the result of modeling these local translations."}, {"heading": "4 Experiments", "text": "Cross-correlation models vs. energy models: To get some quantitative insights into the relationship between cross-correlation and energy models, we compared the standard Gated Boltzmann Machine with a square-pooling GBM [e.g. 19, 13] trained on the concatenation of the images. We used the task of classifying transformations from the mapping units, using transformations for which we can control the ground truth. The models are trained on image patches of size 13 \u00d7 13 pixels, which are cropped from larger images to ensure that no border artifacts are introduced. Training-, validation- and test-data contain 100.000 cases each. We use logistic regression to classify transformations from mapping units, where we determine the optimal number of mapping units and factors, as well as the learning rates for the transformation models on the validation data. Using random images ensures that single images contain no information about the transformation; in other words, a single image cannot be used to predict the transformation.\nWe compare the models on translations and rotations. Shifts are drawn uniform-randomly in the range [\u22123, 3] pixels. We used four different labels corresponding to the four quadrants of motion direction. Rotations are drawn from a von Mises distribution (range [\u2212\u03c0, \u03c0] rad), which we scaled down to a maximal angle of 36\u25e6. We used 10 labels by equally dividing the set of rotation angles.\nThe results are shown in Figure 3 and they demonstrate that both types of model do a reasonably good job at prediction the transformations from the image pairs. The experiment verifies the approximate equivalence of the two types of model derived, for example, in [3, 14]. The cross-correlation model does show slightly better performance than the energy model for large data-set sizes, and the difference gets more pronounced as the training dataset size is decreased, which is also in line with the theory. Energy models have been the standard approach to feature grouping in the past [8].\nLearning simple transformations: We trained the group-gating model with group size 3, 128 mapping units and 392 filters on translating random-dot images of size 13 \u00d7 13 pixels. Figure 4 shows three pairs of plots, where the left, center and right pair depict the dominant frequencies, orientations and phases of the filters, respectively. We extracted these from the filters using FFT. Each pair shows properties of input (top) and output filters (bottom).\nWithin an image, each filter is represented by a single uni-colored square. For frequency plots, the squares are in gray-scale with the brightness corresponding to the frequency of the filter (black represents frequency zero, white is the highest frequency across all filters); in the orientation and phase plots, the squares differ in color, where the angle determines the color according to the HSV representation of RGB color space. The figures confirm that filter-groups tend to be homogeneous with respect to frequency and orientation1. Contrary to that, phases differ within each group, as expected. The same can be seen in Figure 5 (left and middle), which shows subsets of filter groups learned from translations and rotations of random images, using patchsize 25\u00d7 25 and group size 5.\n1In some groups, there appear to be some outliers, whose frequency or orientation does not match the other filters in the group. These are typically the result of spurious maxima in the FFT amplitude due to small irregularities in the filters.\nLearning transformation features from natural videos: Figure 5 (right) shows a subset of filter groups learned from about one million patches of size 12 \u00d7 12 pixels that we crop from the van Hateren broadcast television database [23]. We trained the asymmetric model to predict each frame from its predecessor. All patches were PCA-whitened, retaining 95% of the variance. We trained a model with 128 mapping units and 256 filters, and we used a group size of 4. The figure shows that the model learns Gabor features, where frequency, orientation and position are nearly constant within groups, and the set of possible phases is covered relatively evenly per group.\nQuantitative results for group-gating: Classification accuracies for the same model and data as described in Section 4 are reported in Table 1. The equivalent number of group-gating filters is shown in parentheses, where equivalence means that the group-gating model has the same number of parameters in total as the factored GBM. This makes it possible to obtain a comparison that is fair in terms of pure parameter count by comparing performance across the columns of the tables. However, the table also shows that even along columns, the group-gating model robustly outperforms the factored GBM, except when using a very small number of factors. In this case, the parameter-equivalent number of 44 factors is probably too small to represent the transformations with a sufficiently high resolution."}, {"heading": "4.1 Topographic feature maps from local gating", "text": "We also experimented with overlapping group structures by letting Gg share filters. This makes it necessary to define which filters are shared among which groups. A convenient approach is to lay out all features (\u201csimple cells\u201d) in a low-dimensional space and to define groups over all those units which reside within some pre-defined neighborhoods, such as in a grid of size n\u00d7n units (e.g., [8]). Figure 6 shows the features learned from the van Hateren data (cf., Section 4) with patchsize 16\u00d716 and 99.9% variance retained after whitening. We used 400 filters that we arranged in a 2-D grid (with wrap-around) using a group size of 5 \u00d7 5. We found that learning is simplified when we also\ninitialize the factor-to-mapping parameters so that each mapping unit has access to the filters of only one group.\nThe figure shows how learned filters are arranged in two topographic feature maps with slowly varying frequency and orientation within each map. It also shows how low-frequency filters have the tendency of being grouped together [8]. From the view of phase analysis (see Section 2), the topographic organization is a natural result of imposing an additional constraint: Filter sets that come in similar frequency, orientation and position are now forced to share coefficients with those of neighboring groups. The simplest way to satisfy this constraint is by having nearby groups be similar with respect to the properties they share. The apparent randomness of phase simply follows from the requirement that multiple phases be present within each neighborhood. It is interesting to note that using shared group structure is equivalent to letting units that are nearby in space affect each other multiplicatively. Localized gating may provide a somewhat more plausible explanation for the emergence of pinwheel-structures than squaring non-linearities, which are used, for example, in subspace models or topographic ICA (see [6, 7])."}, {"heading": "5 Conclusions", "text": "Energy mechanisms and \u201csquare-pooling\u201d are common approaches to modeling feature dependencies in sparse coding, and to learn group-structured or invariant dictionaries. In this work we revisited group-structured sparse coding from the perspective of learning image motion and disparity from local multiplicative interactions. Our work shows that the commonly observed constancy of frequency and orientation of filter-groups in energy models can be explained as a result of representing transformations with local, multiplicative feature gating. Furthermore, topographically\nstructured representations (\u201cpinwheels\u201d) can emerge naturally as the result of binocular or spatiotemporal learning that utilizes spatially constrained multiplicative interactions. Our work may provide some support for the claim that localized multiplicative interactions are a biologically plausible alternative to square pooling for implementing stereopsis and motion analysis [12]. It may also help explain why the development of pinwheels in V1 may be tied the presence of binocular vision and why topographic organization of features does not appear to occur, for example, in rodents [20]."}], "references": [{"title": "Spatiotemporal energy models for the perception of motion", "author": ["E.H. Adelson", "J.R. Bergen"], "venue": "J. Opt. Soc. Am. A,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition", "author": ["J. Douglas Carroll", "Jih-Jie Chang"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1970}, {"title": "Neural encoding of binocular disparity: Energy models, position shifts and phase shifts", "author": ["D. Fleet", "H. Wagner", "D. Heeger"], "venue": "Vision Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Digital Image Processing (3rd Edition)", "author": ["Rafael C. Gonzalez", "Richard E. Woods"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Emergence of phase- and shift-invariant features by decomposition of natural images into independent feature subspaces", "author": ["Aapo Hyv\u00e4rinen", "Patrik Hoyer"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Topographic ICA as a model of natural image statistics", "author": ["Aapo Hyv\u00e4rinen", "Patrik O. Hoyer", "Mika Inki"], "venue": "In Biologically Motivated Computer Vision. Springer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision", "author": ["Aapo Hyv\u00e4rinen", "J. Hurri", "Patrik O. Hoyer"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski", "Francis Bach"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Learning invariant features through topographic filter maps", "author": ["Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Rob Fergus", "Yann LeCun"], "venue": "In CVPR 2009. IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Toward a single-cell account for binocular disparity tuning: an energy model may be hiding in your dendrites", "author": ["Bartlett W. Mel", "Daniel L. Ruderman", "Kevin A. Archie"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Gradient-based learning of higher-order image features", "author": ["Roland Memisevic"], "venue": "In ICCV 2011,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "On multi-view feature learning", "author": ["Roland Memisevic"], "venue": "In ICML 2012,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey Hinton"], "venue": "In CVPR 2007,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order Boltzmann machines", "author": ["Roland Memisevic", "Geoffrey E Hinton"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Stereoscopic Depth Discrimination in the Visual Cortex: Neurons Ideally Suited as Disparity Detectors", "author": ["Izumi Ohzawa", "Gregory C. Deangelis", "Ralph D. Freeman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B. Olshausen", "D. Field"], "venue": "Nature,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "Modeling Pixel Means and Covariances Using Factorized Third-Order Boltzmann Machines", "author": ["Marc\u2019Aurelio Ranzato", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A universal design principle for visual system pinwheels", "author": ["CF Stevens"], "venue": "Brain, behavior and evolution,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "On autoencoders and score matching for energy based models", "author": ["Kevin Swersky", "Marc\u2019Aurelio Ranzato", "David Buchman", "Benjamin Marlin", "Nando Freitas"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Convolutional learning of spatiotemporal features", "author": ["Graham", "W. Taylor", "Rob Fergus", "Yann LeCun", "Christoph Bregler"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex", "author": ["L. van Hateren", "J. Ruderman"], "venue": "Proc. Biological Sciences,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "Filters can be learned using a variety of criteria, including maximization of sparseness across filter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others.", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "Filters can be learned using a variety of criteria, including maximization of sparseness across filter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others.", "startOffset": 151, "endOffset": 155}, {"referenceID": 4, "context": "Filters can be learned using a variety of criteria, including maximization of sparseness across filter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others.", "startOffset": 179, "endOffset": 182}, {"referenceID": 5, "context": "The motivation for this is that group structure can explain several biological phenomena such as the presence of complex cells [6], it provides a simple way to model dependencies between features (eg.", "startOffset": 127, "endOffset": 130}, {"referenceID": 8, "context": ", [9] and references therein), and it can make learned representations more robust to small transformations which is useful for recognition [10].", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": ", [9] and references therein), and it can make learned representations more robust to small transformations which is useful for recognition [10].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Feature grouping can also be used as a way to obtain topographic feature maps [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "[7, 10, 8]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 9, "context": "[7, 10, 8]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 7, "context": "[7, 10, 8]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 5, "context": ", [6, 9, 10]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": ", [6, 9, 10]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 9, "context": ", [6, 9, 10]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 7, "context": "So they seem to capture a lot of what we are missing after performing a single layer of feature learning [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "Finally, it can be shown that in the presence of a upstream square-root non-linearity, using squared features generalizes standard feature learning, since it degenerates to a standard feature-learning model when using group size 1 [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 7, "context": "For a summary of these various heuristics, see [8] (page 215).", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 110, "endOffset": 117}, {"referenceID": 16, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 110, "endOffset": 117}, {"referenceID": 2, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 164, "endOffset": 171}, {"referenceID": 13, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 164, "endOffset": 171}, {"referenceID": 19, "context": "That way, it may also help shed light onto the phenomenon that topographic filter maps do not seem to be present in rodents [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": ", [11, 22]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 21, "context": ", [11, 22]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "We focus in this work on the Gated Boltzmann Machine (GBM) [15, 22] which models the relationship between two binary images x and y using the three-way energy function", "startOffset": 59, "endOffset": 67}, {"referenceID": 21, "context": "We focus in this work on the Gated Boltzmann Machine (GBM) [15, 22] which models the relationship between two binary images x and y using the three-way energy function", "startOffset": 59, "endOffset": 67}, {"referenceID": 14, "context": "By adding appropriate penalty terms to the log-likelihood, one can extend the model to learn realvalued images [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "To reduce that number, [16] suggested factorizing the three-way parameter tensor W with entries wijk in Equation 1 into a three-way inner product: wijk = \u2211", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "This form of tensor factorization is also known as PARAFAC or \u201ccanonical decomposition\u201d in the literature, and it can be viewed as a three-way generalization of the SVD [2].", "startOffset": 169, "endOffset": 172}, {"referenceID": 15, "context": "Inferring the transformation h, given two images, x and y, is efficient, because the hidden variables are independent, given the data [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "In fact, it is not uncommon to use a number of factors that is larger than the dimensionality of the input data (for example, [16, 19]).", "startOffset": 126, "endOffset": 134}, {"referenceID": 18, "context": "In fact, it is not uncommon to use a number of factors that is larger than the dimensionality of the input data (for example, [16, 19]).", "startOffset": 126, "endOffset": 134}, {"referenceID": 15, "context": "To train the model, one can use contrastive divergence [16], score-matching or a variety of other approximations.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Recently, [21, 13] showed that one may equivalently add a decoder network, effectively turning the model into a \u201cgated\u201d version of a de-noising auto-encoder [24] and train it using back-prop.", "startOffset": 10, "endOffset": 18}, {"referenceID": 12, "context": "Recently, [21, 13] showed that one may equivalently add a decoder network, effectively turning the model into a \u201cgated\u201d version of a de-noising auto-encoder [24] and train it using back-prop.", "startOffset": 10, "endOffset": 18}, {"referenceID": 23, "context": "Recently, [21, 13] showed that one may equivalently add a decoder network, effectively turning the model into a \u201cgated\u201d version of a de-noising auto-encoder [24] and train it using back-prop.", "startOffset": 157, "endOffset": 161}, {"referenceID": 23, "context": "One may add noise to the data during training (but reconstruct the original, not noisy, data to compute the cost) [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Training on translations turns filters into Fourier components, as was initially observed by [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": ", [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "The phase-shift depends linearly on frequency [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": ", [14]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": ", [22]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": ", [7, 19]), too, if images and filters are contrast normalized [14].", "startOffset": 2, "endOffset": 9}, {"referenceID": 18, "context": ", [7, 19]), too, if images and filters are contrast normalized [14].", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": ", [7, 19]), too, if images and filters are contrast normalized [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "x = y (see, for example, [19, 14]).", "startOffset": 25, "endOffset": 33}, {"referenceID": 13, "context": "x = y (see, for example, [19, 14]).", "startOffset": 25, "endOffset": 33}, {"referenceID": 2, "context": "The experiment verifies the approximate equivalence of the two types of model derived, for example, in [3, 14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 13, "context": "The experiment verifies the approximate equivalence of the two types of model derived, for example, in [3, 14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 7, "context": "Energy models have been the standard approach to feature grouping in the past [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 22, "context": "Learning transformation features from natural videos: Figure 5 (right) shows a subset of filter groups learned from about one million patches of size 12 \u00d7 12 pixels that we crop from the van Hateren broadcast television database [23].", "startOffset": 229, "endOffset": 233}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "It also shows how low-frequency filters have the tendency of being grouped together [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "Localized gating may provide a somewhat more plausible explanation for the emergence of pinwheel-structures than squaring non-linearities, which are used, for example, in subspace models or topographic ICA (see [6, 7]).", "startOffset": 211, "endOffset": 217}, {"referenceID": 6, "context": "Localized gating may provide a somewhat more plausible explanation for the emergence of pinwheel-structures than squaring non-linearities, which are used, for example, in subspace models or topographic ICA (see [6, 7]).", "startOffset": 211, "endOffset": 217}, {"referenceID": 11, "context": "Our work may provide some support for the claim that localized multiplicative interactions are a biologically plausible alternative to square pooling for implementing stereopsis and motion analysis [12].", "startOffset": 198, "endOffset": 202}, {"referenceID": 19, "context": "It may also help explain why the development of pinwheels in V1 may be tied the presence of binocular vision and why topographic organization of features does not appear to occur, for example, in rodents [20].", "startOffset": 204, "endOffset": 208}], "year": 2013, "abstractText": "We present a feature learning model that learns to encode relationships between images. The model is defined as a Gated Boltzmann Machine, which is constrained such that hidden units that are nearby in space can gate each other\u2019s connections. We show how frequency/orientation \u201ccolumns\u201d as well as topographic filter maps follow naturally from training the model on image pairs. The model also offers a simple explanation why group sparse coding and topographic feature learning yields features that tend to by grouped according to frequency, orientation and position but not according to phase. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformationlearning model.", "creator": "LaTeX with hyperref package"}}}