{"id": "1301.3485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "A Semantic Matching Energy Function for Learning with Multi-relational Data", "abstract": "Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.\n\n\n\n\n\nThe computational and technical design of the network is the central aspect of the study of relational learning. A basic problem for many applications is that the underlying underlying model is not the underlying model itself, but the model itself. The underlying model is the underlying model itself. The underlying model is the underlying model itself.\n\nAs the model itself is represented by several simple and very complex representations, this is an important principle for our work. The model itself is used to generate the underlying neural network, and in this context the model is represented by several simple and very complex representations, including a simple linear vector and a linear linear vector. The underlying model is based on one type of model, called a neural network. A model is based on one type of model, called the neural network, which describes information stored in a particular data type, such as data sets. The model also defines the state of the data. A model is built on the basis of an explicit model and a model model is created as a linear vector. An instance of a neural network is called the network.\nThe neural network is a hierarchical system, in which each point of a network is represented by two sub-par and contains two sub-par and three sub-par sub-par networks. The model also identifies the relevant properties of the underlying model. These properties are defined in a hierarchical hierarchy and represent the corresponding state of the network.\nThis hierarchical hierarchy of the model is called the neural network. A model is an arbitrary system that consists of two sub-par and two sub-par networks. The model is an arbitrary system that consists of two sub-par and two sub-par networks. The model is composed of two sub-par and two sub-par networks.\nThe neural network is built on a set of information elements. The model is a simple representation of a network that is based on one or more properties. The model also defines the most common, common and rare instances of information.", "histories": [["v1", "Tue, 15 Jan 2013 20:52:50 GMT  (12kb)", "https://arxiv.org/abs/1301.3485v1", null], ["v2", "Thu, 21 Mar 2013 17:02:48 GMT  (12kb)", "http://arxiv.org/abs/1301.3485v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xavier glorot", "antoine bordes", "jason weston", "yoshua bengio"], "accepted": false, "id": "1301.3485"}, "pdf": {"name": "1301.3485.pdf", "metadata": {"source": "CRF", "title": "A Semantic Matching Energy Function for Learning with Multi-relational Data", "authors": ["Xavier Glorot", "Antoine Bordes", "Jason Weston", "Yoshua Bengio"], "emails": ["glorotxa@iro.umontreal.ca", "bengioy@iro.umontreal.ca", "bordesan@hds.utc.fr", "jweston@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 1.\n34 85\nv2 [\ncs .L\nG ]\n2 1\nM ar\n2 01\n3\nA Semantic Matching Energy Function for Learning with Multi-relational Data\nXavier Glorot(1), Antoine Bordes(2), Jason Weston(3), Yoshua Bengio(1) (1) DIRO, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, QC, Canada \u2013 {glorotxa,bengioy}@iro.umontreal.ca (2) CNRS - Heudiasyc, Universit\u00e9 de Technologie de Compi\u00e8gne, France \u2013 bordesan@hds.utc.fr (3) Google, New York, NY, USA \u2013 jweston@google.com"}, {"heading": "1 Introduction", "text": "Multi-relational data, which refers to graphs whose nodes represent entities and edges correspond to relations that link these entities, plays a pivotal role in many areas such as recommender systems, the Semantic Web, or computational biology. Relations are modeled as triplets of the form (subject, relation, object), where a relation either models the relationship between two entities or between an entity and an attribute value; relations are thus of several types. In spite of their appealing ability for representing complex data, multi-relational graphs remain complicated to manipulate for several reasons (noise, heterogeneity, large-scale dimensions, etc.), and conveniently represent, summarize or de-noise this kind of data is now a central challenge in statistical relational learning [2].\nIn this work, we propose a new model to learn multi-relational semantics, that is, to encode multirelational graphs into representations that capture the inherent complexity in the data, while seamlessly defining similarities among entities and relations and providing predictive power. Our work is based on an original energy function, which is trained to assign low energies to plausible triplets of a multirelational graph. This energy function, termed semantic matching energy, relies on a compact distributed representation: all elements (entity and relation type) are represented into the same relatively low (e.g. 50) dimensional embedding vector space. The embeddings are learnt by a neural network whose particular architecture and training process force them to capture the structure implicit in the training data and generalize the graph formed from training triplets. Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities. In this way, entities can also play the role of relation type, as in natural language for instance, and this requires less parameters when the number of relation types grows. We show empirically that this model achieves competitive results on benchmark tasks of link prediction, i.e., generalizing outside of the set of given valid triplets."}, {"heading": "2 Semantic Matching Energy Function", "text": "This work considers multi-relational databases as graph models. To each individual node of the graph corresponds an element of the database, which we term an entity, and each link defines a relation between entities. Relations are directed and there are typically several different kinds of relations. Let C denote the dictionary which includes all entities and relation types, and let R \u2282 C be the subset of entities which are relation types. A relation is denoted by a triplet (lhs, rel, rhs), where lhs is the left entity, rhs the right one and rel the type of relation between them."}, {"heading": "2.1 Main ideas", "text": "The main ideas behind our semantic matching energy function are the following.\n\u2022 Named symbolic entities (entities and relation types) are associated with a d-dimensional vector\nspace, termed the \u201cembedding space\u201d. The ith entity is assigned a vector Ei \u2208 Rd. Note that more general mappings from an entity to its embedding are possible.\n\u2022 The semantic matching energy value associated with a particular triplet (lhs, rel, rhs) is computed\nby a parametrized function E that starts by mapping all symbols to their embeddings and then combines them in a structured fashion. Our model is termed \u201csemantic matching\u201d because E relies on a matching criterion computed between both sides of the triplet.\n\u2022 The energy function E is optimized to be lower for training examples than for other possible\nconfigurations of symbols."}, {"heading": "2.2 Neural network parametrization", "text": "The energy function E (denoted SME) is encoded using a neural network, whose architecture first processes each entity in parallel, like in siamese networks [1]. The intuition is that the relation type should first be used to extract relevant components from each argument\u2019s embedding, and put them in a space where they can then be compared.\n(1) Each symbol of the input triplet (lhs, rel, rhs) is mapped to its embedding Elhs, Erel, Erhs \u2208 Rd.\n(2) The embeddings Elhs and Erel respectively associated with the lhs and rel arguments are used to\nconstruct a new relation-dependent embedding Elhs(rel) for the lhs in the context of the relation type represented by Erel, and similarly for the rhs: Elhs(rel) = gleft(Elhs, Erel) and Erhs(rel) = gright(Erhs, Erel), where gleft and gright are parametrized functions whose parameters are tuned during training. The dimension of Elhs(rel) and Erhs(rel), which we denote p, is low-dimensional but not necessarily equal to d, the dimension of the entity embedding space.\n(3) The energy is computed by \"matching\" the transformed embeddings of the left-hand and right-\nhand sides: E((lhs, rel, rhs)) = h(Elhs(rel), Erhs(rel)), h is a dot product in our experiments.\nWe studied two options for the g functions, which lead to two versions of SME:\n\u2022 Linear form (denoted SME(linear)), in this case g functions are simply linear layers:\nElhs(rel) = gleft(Elhs, Erel) = Wl1E \u22ba lhs +Wl2E \u22ba rel + b \u22ba l . Erhs(rel) = gright(Erhs, Erel) = Wr1E \u22ba rhs +Wr2E \u22ba rel + b \u22ba r .\nwith Wl1, Wl2, Wr1, Wr2 \u2208 Rp\u00d7d, bl, br \u2208 Rp and E\u22ba denotes the transpose of E. This leads to\nthe energy: E((lhs, rel, rhs)) = \u2212 (Wl1E \u22ba lhs +Wl2E \u22ba rel + b \u22ba l ) \u22ba (Wr1E \u22ba rhs +Wr2E \u22ba rel + b \u22ba r ).\n\u2022 Bilinear form (denoted SME(bilinear)), g functions are using 3-modes tensors as core weights:\nElhs(rel) = gleft(Elhs, Erel) = (Wl\u00d7\u03043E \u22ba rel)E \u22ba lhs + b \u22ba l . Erhs(rel) = gright(Erhs, Erel) = (Wr\u00d7\u03043E \u22ba rel)E \u22ba rhs + b \u22ba r .\nwith Wl, Wr \u2208 Rp\u00d7d\u00d7d (weights) and bl, br \u2208 Rp (biases). \u00d7\u03043 denotes the n-mode vector-tensor\nproduct along the 3rd mode. This leads to the following form for the energy: E((lhs, rel, rhs)) = \u2212 ((Wl\u00d7\u03043E \u22ba rel)E \u22ba lhs + b \u22ba l ) \u22ba ((Wr\u00d7\u03043E \u22ba rel)E \u22ba rhs + b \u22ba r ).\nuse stochastic gradient descent with a ranking objective inspired by [7]."}, {"heading": "3 Empirical Evaluation", "text": "To evaluate against existing methods, we performed link prediction experiments on benchmarks from the literature, whose statistics are in Table 1.\nThe link prediction task consists in predicting whether two entities should be connected by a given relation type. This is useful for completing missing values of a graph, forecasting the behavior of a network, etc. but also to assess the quality of a representation. We evaluate our model on UMLS, Nations and Kinships, following the setting introduced in [4]. The standard evaluation metric is area under the precision-recall curve (AUC). Table 2 presents results of SME along with those of RESCAL, MRC, IRM, CP (CANDECOMP-PARAFAC) and LFM, which have been extracted from [5, 3].\nThe linear formulation of SME is outperformed by SME(bilinear) on all three tasks. The largest differences for Nations and Kinships indicate that, for these problems, a joint interaction between both lhs, rel and rhs is crucial to represent the data well: relations cannot be simply decomposed as a sum of bigrams. This is particularly true for the complex kinship systems of the Alyawarra. On the contrary, interactions within the UMLS network can be represented by simply considering the various (entity,entity) and (entity,relation type) bigrams. Compared to other methods, SME(bilinear) performs similarly to LFM on UMLS but is slightly outperfomed on Nations. On Kinships, it is outperformed by CP, RESCAL and LFM: on this dataset with complex ternary interactions, either the training process of the tensor factorization methods, based on reconstruction, or the combination of bigram and trigram interactions seems to be beneficial compared to our predictive approach. Compared to MRC, which is not using a matrix-based encoding, SME(bilinear) is highly competitive.\nEven if experimental results on these benchmarks are mixed, it is worth noting that, contrary to all previous methods, SME models relation types as vectors, lying in the same space as entities. From a conceptual viewpoint, this is powerful, since it models any relation types as a standard entity (and viceversa). Hence, SME is the only method that could be directly applied on data for which any entity can also create relationships between other entities."}, {"heading": "Acknowledgements", "text": "This work was supported by the French ANR (EVEREST-12-JS02-005-01), the Pascal2 European NoE, the DARPA DL Program, NSERC, CIFAR, the Canada Research Chairs, and Compute Canada."}], "references": [{"title": "Signature verification using a siamese time delay neural network", "author": ["Jame Bromley", "Jim W. Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann Le Cun", "C. Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)", "author": ["Lise Getoor", "Ben Taskar"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A latent factor model for highly multi-relational data", "author": ["Rodolphe Jenatton", "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Charles Kemp", "Joshua B. Tenenbaum", "Thomas L. Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In Proceedings of the 21st national conference on Artificial intelligence - Volume 1,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A three-way model for collective learning on multirelational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["Ilya Sutskever", "Ruslan Salakhutdinov", "Josh Tenenbaum"], "venue": "In Adv. in Neur. Inf. Proc. Syst", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "), and conveniently represent, summarize or de-noise this kind of data is now a central challenge in statistical relational learning [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 5, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 4, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 2, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 0, "context": "2 Neural network parametrization The energy function E (denoted SME) is encoded using a neural network, whose architecture first processes each entity in parallel, like in siamese networks [1].", "startOffset": 189, "endOffset": 192}, {"referenceID": 6, "context": "To train the parameters of the energy function E we loop over all of the training data resources and use stochastic gradient descent with a ranking objective inspired by [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "We evaluate our model on UMLS, Nations and Kinships, following the setting introduced in [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "Table 2 presents results of SME along with those of RESCAL, MRC, IRM, CP (CANDECOMP-PARAFAC) and LFM, which have been extracted from [5, 3].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "Table 2 presents results of SME along with those of RESCAL, MRC, IRM, CP (CANDECOMP-PARAFAC) and LFM, which have been extracted from [5, 3].", "startOffset": 133, "endOffset": 139}], "year": 2013, "abstractText": null, "creator": "LaTeX with hyperref package"}}}