{"id": "1301.3878", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "PEGASUS: A Policy Search Method for Large MDPs and POMDPs", "abstract": "We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng (1999), but with \"sample complexity\" bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learning to ride a bicycle. The only difference between our proposed approach and our model is the degree of uncertainty. For example, we propose a simple solution for a PoS model with non-linear state transitions with the current state and action spaces. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. The only difference between our proposed approach and our model is the degree of uncertainty. For example, we propose a simple solution for a PoS model with non-linear state transitions with the current state and action spaces. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and", "histories": [["v1", "Wed, 16 Jan 2013 15:51:42 GMT  (472kb)", "http://arxiv.org/abs/1301.3878v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["andrew y ng", "michael i jordan"], "accepted": false, "id": "1301.3878"}, "pdf": {"name": "1301.3878.pdf", "metadata": {"source": "CRF", "title": "PEGASUS: A policy search method for large MDPs and POMDPs", "authors": [], "emails": [], "sections": [{"heading": null, "text": "1 Introduction\nIn recent years, there has been growing interest in algo rithms for approximate planning in (exponentially or even infinitely) large Markov decision processes (MDPs) and partially observable MDPs (POMDPs). For such large do mains, the value and Q-functions are sometimes compli cated and difficult to approximate, even though there may be simple, compactly representable policies that perform very well. This observation has led to particular interest in direct policy search methods (e.g., [16, 8, 15, 1, 7]), which\nattempt to choose a good policy from some restricted class of policies.\nMost approaches to policy search assume access to the POMDP either in the form of the ability to execute trajec tories in the POMDP, or in the form of a black-box \"gen erative model\" that enables the learner to try actions from arbitrary states. In this paper, we will assume a stronger model than these: roughly, we assume we have an imple mentation of a generative model, with the difference that it has no internal random number generator, so that it has to ask us to provide it with random numbers whenever it needs them (such as if it needs a source of randomness to draw samples from the POMDP's transition distributions). This small change to a generative model results in what we will call a deterministic simulative model, and makes it surprisingly powerful.\nWe show how, given a deterministic simulative model, we can reduce the problem of policy search in an ar bitrary POMDP to one in which all the transitions are deterministic-that is, a POMDP in which taking an ac tion a in a state s will always deterministically result in transitioning to some fixed state s'. (The initial state in this POMDP may still be random.) This reduction is achieved by transforming the original POMDP into an \"equivalent\" one that has only deterministic transitions.\nOur policy search algorithm then operates on these \"sim plified\" transformed POMDPs. We call our method PEGA SUS (for Policy Evaluation-of-Goodness And Search Us ing Scenarios, for reasons that will become clear). Our algorithm also bears some similarity to one used in Van Roy [12] for value determination in the setting of fully ob servable MDPs.\nThe remainder of this paper is structured as follows: Sec tion 2 defines the notation that will be used in this pa per, and formalizes the concepts of deterministic simulative models and of families of realizable dynamics. Section 3 then describes how we transform POMDPs into ones with only deterministic transitions, and gives our policy search algorithm. Section 4 goes on to establish conditions un der which we may give guarantees on the performance of\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 407\nthe algorithm, Section 5 describes our experimental results, and Section 6 closes with conclusions.\n2 Preliminaries\nThis section gives our notation, and introduces the concept of the set of realizable dynamics of a POMDP under a pol icy class.\nA Markov decision process (MDP) is a tuple (S,D,A,{Psa(\u00b7)},/,R) where: Sis a set of states; D is the initial-state distribution, from which the start-state s0 is drawn; A is a set of actions; { Psa ( \u00b7)} are the tran sition probabilities, with Psa giving the next-state distri bution upon taking action a in state s; 'Y E [0, 1) is the discount factor; and R is the reward function, bounded by Rmax\u00b7 For the sake of concreteness, we will assume, un less otherwise stated, that S = [0, 1jds is ads-dimensional hypercube. For simplicity, we also assume rewards are de terministic, and written R(s) rather than R(s, a), the ex tensions being trivial. Lastly, everything that needs to be measurable is assumed to be measurable.\nA policy is any mapping 1r : S 1-t A. The value function of a policy 7r is a map v11\" : s f-t IR, so that v11\" ( s) gives the expected discounted sum of rewards for executing 1r starting from state s. With some abuse of notation, we also define the value of a policy, with respect to the initial-state distribution D, according to\nV(1r) = Eso\ufffdD [V1r(so)] (1)\n(where the subscript s0 ,...., D indicates that the expectation is with respect to s0 drawn according to D). When we are considering multiple MDPs and wish to make explicit that a value function is for a particular MDP M, we will also write VA1(s), VM(7r), etc. In the policy search setting, we have some fixed class IT of policies, and desire to find a good policy 1r E IT. More precisely, for a given MDP M and policy class IT, define\nopt(M,IT) =sup VM(7r). (2) 1rEll\nOur goal is to find a policy ir E IT so that V ( ir) is close to opt(M, IT).\nNote that this framework also encompasses cases where our family IT consists of policies that depend only on certain as pects of the state. In particular, in POMDPs, we can restrict attention to policies that depend only on the observables. This restriction results in a subclass of stochastic memory free policies.1 By introducing artificial \"memory vari ables\" into the process state, we can also define stochastic limited-memory policies [9] (which certainly permits some belief state tracking).\n1 Although we have not explicitly addressed stochastic policies so far, they are a straightforward generalization (e.g. using the transformation to deterministic policies given in [7]).\nSince we are interested in the \"planning\" problem, we as sume that we are given a model of the (PO)MDP. Much pre vious work has studied the case of (PO)MDPs specified via a generative model [7, 13], which is a stochastic function that takes as input any ( s, a) state-action pair, and outputs s' according to Psa (-) (and the associated reward). In this paper, we assume a stronger model. We assume we have a deterministic function g : S x A x [0, 1]dp 1-t S, so that for any fixed (s, a)-pair, if pis distributed Uniform[O, 1]dp, then g(s, a,jf) is distributed according to the transition dis tribution Psa ( \u00b7 ) . In other words, to draw a sample from Psa ( \u00b7) for some fixed s and a, we need only draw p uni formly in [0, 1]dp, and then take g(s, a,jf) to be our sample. We will call such a model a deterministic simulative model for a (PO)MDP.\nSince a deterministic simulative model allows us to simu late a generative model, it is clearly a stronger model. How ever, most computer implementations of generative models also provide deterministic simulative models. Consider a generative model that is implemented via a procedure that takes s and a, makes at most dp calls to a random number generator, and then outputs s' drawn according to Psa ( \u00b7) . Then this procedure is already providing a deterministic simulative model. The only difference is that the determin istic simulative model has to make explicit (or \"expose\") its interface to the random number generator, via P. (A gen erative model implemented via a physical simulation of an MDP with \"resets\" to arbitrary states does not, however, readily lead to a deterministic simulative model.)\nLet us examine some simple examples of deterministic sim ulative models. Suppose that for a state-action pair ( s1, a1) and some states s' and s\", Ps1a1 (s') = 1/3, Ps1a1 (s\") = 2/3. Then we may choose dp = 1 so that p = pis just a real number, and let g(s1, a1,p) = s' if p \ufffd 1/3, and g(sl,al,P) = s\" otherwise. As another example, suppose S = IR, and Psa ( \u00b7) is a normal distribution with a cumula tive distribution function Fsa(\u00b7) . Again letting dp = 1, we may choose g to be g(s, a,p) = F8\ufffd1(p). It is a fact of probability and measure theory that, given any transition distribution Psa ( \u00b7) , such a deterministic sim ulative model g can always be constructed for it. (See, e.g. [4].) Indeed, some texts (e.g. [2]) routinely define POMDPs using essentially deterministic simulative mod els. However, there will often be many different choices of g for representing a (PO)MDP, and it will be up to the user to decide which one is most \"natural\" to implement. As we will see later, the particular choice of g that the user makes can indeed impact the performance of our algorithm, and \"simpler\" (in a sense to be formalized) implementations are generally preferred.\nTo close this section, we introduce a concept that will be useful later, that captures the family of dynamics that a (PO)MDP and policy class can exhibit. Assume a deter ministic simulative model g, and fix a policy 1r. If we are\n408 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nexecuting 1r from some state s, the successor-state is deter mined by frr(s,if) = g(s, 1r(s),if), which is a function of s and p. Varying 1r over II, we get a whole family of func tions :F = Urrlfrr(s,P) = g(s,1r(s),P)} mapping from S x [0, 1]dp into successor states S. This set of functions :F should be thought of as the family of dynamics realiz able by the POMDP and II, though since its definition does depend on the particular deterministic simulative model g that we have chosen, this is \"as expressed with respect to g. \" For each f, also let fi be the i-th coordinate function (so that fi(s,if) is the i-th coordinate of f(s,P)) and let :F; be the corresponding families of coordinate functions map ping from S x [0, 1]dp into [0, 1]. Thus, :F; captures all the ways that coordinate i of the state can evolve.\nWe are now ready to describe our policy search method.\n3 Policy search method\nIn this section, we show how we transform a (PO)MDP into an \"equivalent\" one that has only deterministic transitions. This then leads to natural estimates V(1r) of the policies' values V ( 1r). Finally, we may search over policies to opti mize V(1r), to find a (hopefully) good policy.\n3.1 Transformation of (PO)MDPs\nGiven a (PO)MDP M = (S,D,A,{Psa(\u00b7)},')',R) and a policy class II, we describe how, using a determinis tic simulative model g for M, we construct our trans formed POMDP M1 = (S1,D1,A,{P\ufffdJ)},')',R1) and corresponding class of policies II1, so that M1 has only de terministic transitions (though its initial state may still be random). To simplify the exposition, we assume dp = 1, so that the terms pare just real numbers.\nM1 is constructed is as follows: The action space and dis count factor for M1 are the same as in M. The state space for M1 is S x [0, 1]00\u2022 In other words, a typical state in M1 can be written as a vector ( s, PI, P2, . . . ) -this consists of a state s from the original state space S, followed by an infinite sequence of real numbers in [0, 1].\nThe rest of the transformation is straightforward. Upon taking action a in state ( s, PI, P2, . . . ) in M1, we deter ministically transition to the state ( S1, P2, P3, . . . ) , where s1 = g(s, a, pi)\u00b7 In other words, the s portion of the state (which should be thought of as the \"actual\" state) changes to s1, and one number in the infinite sequence (pi, P2, . . . ) is used up to generate s1 from the correct distribution. By the definition of the deterministic simulative model g, we see that so long as PI ,...., Uniform[O, 1], then the \"next state\" distribution of s1 is the same as if we had taken action a in state s (randomization over PI). Finally, we choose D1, the initial-state distribution over S1 = S x [0, 1]00, so that (s, PI, P2, . . . ) drawn according to D1 will be so that s ,...., D, and the p; 's are distributed i.i.d. Uniform[O, 1]. For each policy 1r E II, also let there be a\ncorresponding 7r1 E II1, given by 7r1(s,pi,P2, . . . ) = 1r(s), and let the reward be given by R1(s, PI ,p2, . .. ) = R(s). If one observes only the \"s\"-portion (but not the pi's) of a sequence of states generated in the POMDP M1 using pol icy 1r1, one obtains a sequence that is drawn from the same distribution as would have been generated from the original (PO)MDP M under the corresponding policy 1r E II. It fol lows that, for corresponding policies 1r E II and 1r1 E II1, we have that VM(7r) = VM' (1r1). This also implies that the best possible expected returns in both (PO)MDPs are the same: opt(M, II) = opt(M1, II1).\nTo summarize, we have shown how, using a deterministic simulative model, we can transform any POMDP M and policy class II into an \"equivalent\" POMDP M1 and policy class II1, so that the transitions in M1 are deterministic; i.e., given a state s E S1 and an action a E A, the next-state in M1 is exactly determined. Since policies in II and II1 have the same values, if we can find a policy 1r1 E II1 that does well in M1 starting from D1, then the corresponding policy 1r E II will also do well for the original POMDP M starting from D. Hence, the problem of policy search in general POMDPs is reduced to the problem of policy search in POMDPs with deterministic transition dynamics. In the next section, we show how we can exploit this fact to derive a simple and natural policy search method.\n3.2 PEGASUS: A method for policy search\nAs discussed, it suffices for policy search to find a good policy 1r1 E II1 for the transformed POMDP, since the cor responding policy 1r E II will be just as good. To do this, we first construct an approximation VM' (-) to VM(\u00b7), and then search over policies 1r1 E II1 to optimize VM' (1r1) (as a proxy for optimizing the hard-to-compute VM(7r)), and thus find a (hopefully) good policy.\nRecall that V M' is given by\n(3)\nwhere the expectation is over the initial state so E S1 drawn according to D1\u2022 The first step in the approximation is to replace the expectation over the distribution with a finite sample of states. More precisely, we first draw a sam-\n1 { (I) (2) (m)} f . . . 1 d\" p e s0 , s0 , \u2022 . . , s0 o m mitla states accor mg to D1\u2022 These states, also called \"scenarios\" (a term from the stochastic optimization literature; see, e.g. [3]), define an approximation to V M' ( 1r):\n(4)\nSince the transitions in M1 are deterministic, for a given state s E S1 and a policy 1r E II1, the sequence of states that will be visited upon executing 1r from s is exactly deter mined; hence the sum of discounted rewards for executing\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 409\n1r from s is also exactly determined. Thus, to calculate one of the terms VM-, (sbi)) in the summation in Equation (4) corresponding to scenario s6i), we need only use our de terministic simulative model to find the sequence of states visited by executing 1r from Sbi), and sum up the result ing discounted rewards. Naturally, this would be an infinite sum, so the second (and standard) part of the approxima tion is to truncate this sum after some number H of steps, where H is called the horizon time. Here, we choose H to be theE-horizon time H, = log1(E(l-!)/2Rmax). so that (because of discounting) the truncation introduces at most E/2 error into the approximation. ...., . . . (1) (m) \ufffdo summarize, given m scenanos s0 , ... , s0 , our approximation to V M' is the deterministic function\nVM'(-rr) = \ufffd fR'(sbi))+!R'(s\ufffdi))+\u00b7 \u00b7 \u00b7+!H'R'(s}%) i=I where (s\ufffdi), s\ufffdi), ... , s}%) is the sequence of states deter ministically visited by 1r starting from s\ufffdi). Given m sce narios, this defines an approximation to V M' ( 1r) for all poli cies 1r E TI'. The final implementational detail is that, since the states s\ufffdi) E S x [0, 1]00 are infinite-dimensional vectors, we have no way of representing them (and their successor states) explicitly. But because we will be simulating only ( i) (i) ( i) H, steps, we need only represent PI ,p2 , ... ,PH,' of the state s\ufffdi) = (s(i) ,p\ufffdi) ,p\ufffdi), . . . ), and so we will do just that. Viewed in the space of the original, untrans formed POMDP, evaluating a policy this way is therefore also akin to generating m Monte Carlo trajectories and tak ing their empirical average return, but with the crucial dif ference that all the randomization is \"fixed\" in advance and \"reused\" for evaluating different -rr.\nHaving used m scenarios to define V M' ( 1r) for all 1r, we may search over policies to optimize V M' ( 1r) . We call this policy search method PEGASUS: Policy Evaluation-of Goodness And Search Using Scenarios. Since V M' ( 1r) is a deterministic function, the search procedure only needs to optimize a deterministic function, and any number of stan dard optimization methods may be used. In the case that the action space is continuous and n = { 7ro IO E ll\ufffd_l} is a smoothly parameterized family of policies (so -rro (s) is differentiable in(} for all s) then if all the relevant quanti ties are differentiable, it is also possible to find the deriva tives (djdO)VM' (-rr0), and gradient ascent methods can be used to optimize VM' (no). One common barrier to doing this is that R is often discontinuous, being (say) 1 within a goal region and 0 elsewhere. One approach to dealing with this problem is to smooth R out, possibly in com bination with \"continuation\" methods that gradually un smooth it again. An alternative approach that may be use ful in the setting of continuous dynamical systems is to al ter the reward function to use a continuous-time model of\ndiscounting. Assuming that the time at which the agent en ters the goal region is differentiable, then V M' ( 1r o ) is again differentiable. 2\n4 Main theoretical results\nPEGASUS samples a number of scenarios from D', and uses them to form an approximation V ( 1r) to V ( 1r) . If V is a uniformly good approximation to V, then we can guaran tee that optimizing V will result in a policy with value close to opt(M, TI). This section establishes conditions under which this occurs .\n4.1 The case of finite action spaces\nWe begin by considering the case of two actions, A = {a I , a2}. Studying policy search in a similar setting, Keams, Mansour and Ng [7] established conditions under which their algorithm gives uniformly good estimates of the values of policies. A key to that result was that uniform convergence can be established so long as the policy class TI has low \"complexity.\" This is analogous to the setting of supervised learning, where a learning algorithm that uses a hypothesis class 1i that has low complexity (such as in the sense of low VC-dimension) will also enjoy uniform convergence of its error estimates to their means.\nIn our setting, since TI is just a class of functions mapping from S into { a1, a2}, it is just a set of boolean functions. Hence, VC(TI), its Vapnik-Chervonenkis dimension [14], is well defined. That is, we say TI shatters a set of m states if it can realize each of the 2m possible action combina tions on them, and VC(TI) is just the size of the largest set shattered by TI. The result of Keams et al. then suffices to give the following theorem. 3\nTheorem 1 Let a POMDP with actions A = { a1, a2} be given, and let TI be a class of strategies for this POMDP, with Vapnik-Chervonenkis dimension d = VC(TI). Also let any E, c5 > 0 be fixed, and let V be the policy-value estimates determined by PEGASUS using m scenarios and\n2More precisely, if the agent enters the goal region on some time step, then rather than giving it a reward of 1, we figure out what fraction r E [0, 1] of that time step (measured in continuous time) the agent had taken to enter the goal region, and then give it reward'\"( instead. Assuming r is differentiable in the system's dynamics, then, ... and hence V M' ( 1re) are now also differentiable (other than on a usually-measure 0 set, for example from trunca tion at H, steps).\n3The algorithm of Keams, Mansour and Ng uses a \"trajectory tree\" method to find the estimates V ( 1r); since each trajectory tree is of size exp(O(H,)), they were very expensive to build. Each scenario in PEGASUS can be viewed as a compact representation of a trajectory tree (with a technical difference that different sub trees are not constructed independently), and the proof given in Kearns et al. then applies without modification to give Theorem 1.\n410 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\na horizon time of H<. If\n( ( Rmax 1 1 )) m = 0 poly d, -\u20ac-, log J, 1 _ 'Y , (5)\nthen with probability at least 1 - 8, V will be uniformly close to V:\njv(1r)- V(1r) l ::; E for al11r E II (6) Using the transformation given in Kearns et al., the case of a finite action space with JAJ > 2 also gives rise to essen tially the same uniform-convergence result, so long as II has low \"complexity.\"\nThe bound given in the theorem has no dependence on the size of the state space or on the \"complexity\" of the POMDP's transitions and rewards. Thus, so long as II has low VC-dimension, uniform convergence will occur, inde pendently of how complicated the POMDP is. As in Kearns et al., this theorem therefore recovers the best analogous results in supervised learning, in which uniform conver gence occurs so long as the hypothesis class has low VC dimension, regardless of the size or \"complexity\" of the underlying space and target function.\n4.2 The case of infinite action spaces: \"Simple\" II is insufficient for uniform convergence\nWe now consider the case of infinite action spaces. Whereas, in the 2-action case, II being \"simple\" was suffi cient to ensure uniform convergence, this is not the case in POMDPs with infinite action spaces.\nSuppose A is a (countably or uncountably) infinite set of actions. A \"simple\" class of policies would be II = { 1r a j1r a ( s) = a, a E A} - the set of all policies that al ways choose the same action, regardless of the state. Intu itively, this is the simplest policy that actually uses an infi nite action space; also, any reasonable notion of complexity of policy classes should assign II a low \"dimension.\" If it were true that simple policy classes imply uniform conver gence, then it is certainly true that this II should always enjoy uniform convergence. Unfortunately, this is not the case, as we now show.\nTheorem 2 Let A be an infinite set of actions, and let II = {7raJ7ra(s) = a, a E A} be the corresponding set of all \"constant valued\" policies. Then there exists a finite state MDP with action space A, and a deterministic simu lative model for it, so that PEGASUS' estimates using the deterministic simulative model do not uniformly converge to their means. i.e. There is an f > 0, so that for estimates V derived using any finite number m of scenarios and any finite horizon time, there is a policy 1r E II so that\n(7)\nThe proof of this Theorem, which is not difficult, is in Ap pendix A. This result shows that simplicity of II is not suf ficient for uniform convergence in the case of infinite ac tion spaces. However, the counterexample used in the proof of Theorem 2 has a very complex g despite the MDP be ing quite simple. Indeed, a different choice for g would have made uniform convergence occur.4 Thus, it is natu ral to hypothesize that assumptions on the \"complexity\" of g are also needed to ensure uniform convergence. As we will shortly see, this intuition is roughly correct. Since ac tions affect transitions only through g, the crucial quantity is actually the composition of policies and the determinis tic simulative model -in other words, the class :F of the dynamics realizable in the POMDP and policy class, us ing a particular deterministic simulative model. In the next section, we show how assumptions on the complexity of :F leads to uniform convergence bounds of the type we desire.\n4.3 Uniform convergence in the case of infinite action\nspaces\nFor the remainder of this section, assume S = [0, 1]ds. Then :F is a class of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1]ds, and so a simple way to capture its \"complexity\" is to capture the complexity of its families of coordinate functions, :Fi, i = 1, . . . , ds. Each :Fi is a family of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1], the i-th coordinate of the state vector. Thus, :Fi is just a family of real-valued functions -the family of i-th coordinate dynamics that II can realize, with respect to g. The complexity of a class of boolean functions is measured by its VC dimension, defined to be the size of the largest set shattered by the class. To capture the \"complexity\" of real valued families of functions such as :Fi, we need a general ization of the VC dimension. The pseudo-dimension, due to Pollard [10] is defined as follows:\nDefinition (Pollard, 1990). Let 1i be a family of functions mapping from a space X into Ilt Let a sequence of d points x1, .. . , Xd E X be given. We say 1i shatters x1, ... , Xd if there exists a sequence of real numbers h, . . . , td such that the subset of JRd given by {(h(x1) - tb . . . , h(xd) - td) Jh E H} intersects all 2d orthants of JRd (equivalently, if for any sequence of d bits b1, . . \u2022 , bd E {0, 1 }, there is a function h E 1i such that h(xi) \ufffd ti {::} bi = 1, for all i = 1, . . . , d). The pseudo-dimension of H, denoted dimp(H), is the size of the largest set that 1i shatters, or infinite if 1i can shatter arbitrarily large sets.\nThe pseudo-dimension generalizes the VC dimension, and coincides with it in the case that 1i maps into {0, 1}. We will use it to capture the \"complexity\" of the classes of the POMDP's realizable dynamics :Fi. We also remind readers of the definition of Lipschitz continuity.\n4For example, g(so, a,p) = S-1 if p $ 0.5, s1 otherwise; see Appendix A.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 411\nDefinition. A function f : m_n H ffi. is Lipschitz con tinuous (with respect to the Euclidean norm on its range and domain) if there exists a constant B such that for all x,y E dom(f) , llf(x)- f(y)lh ::::; Bllx- Yll2\u00b7 Here, B is called a Lipschitz bound. A family of functions 1l mapping from m_n into ffi. is uniformly Lipschitz contin uous with Lipschitz bound B if every function h E 1l is Lipschitz continuous with Lipschitz bound B.\nWe now state our main theorem, with a corollary regarding when optimizing V will result in a provably good policy.\nTheorem 3 Let a POMDP with state spaceS = [0, 1]ds, and a possibly infinite action space be given. Also let a policy class II, and a deterministic simulative model g : S x A x [0, 1Jdp H S for the POMDP be given. Let :F be the corresponding family of realizable dynamics in the POMDP, and :Fi the resulting families of coordinate func tions. Suppose that dimp(:Fi) ::::; dfor each i = 1, . . . , ds, and that each family :Fi is uniformly Lipschitz continuous\nUsing tools from [5], it is also possible to show similar uniform convergence results without Lipschitz continuity assumptions, by assuming that the family 1r is parameter ized by a small number of real numbers, and that 1r (for all 1r E II), g, and Rare each implemented by a function that calculates their results using only a bounded number of the usual arithmetic operations on real numbers.\nThe proof of Theorem 3 , which uses techniques first intro duced by Haussler [6] and Pollard [10], is quite lengthy, and is deferred to Appendix B.\n5 Experiments\nIn this section, we report the results from two experiments. The first, run to examine the behavior of PEGASUS para metrically, involved a simple gridworld POMDP. The sec ond studied a complex continuous state/continuous action problem involving riding a bicycle.\nwith Lipschitz bound at most B, and that the reward june- Figure I a shows the finite state and action POMDP used tion R: S H [-Rmax, Rmax] is also Lipschitz continuous in our first experiment. In this problem, the agent starts with Lipschitz bound at most BR. Finally, let E, 8 > 0 be in the lower-left comer, and receives a -1 reinforcement given, and let V be the policy-value estimates determined per step until it reaches the absorbing state in the upperby PEGASUS using m scenarios and a horizon time of H,. right comer. The eight possible observations, also shown If m = in the figure, indicate whether each of the eight squares ( ( R 1 1 B ) )adjoining the current position contains a wall. The policy 0 poly d,\nmax , log s; , ----=- ' log B, log R\nR , ds, dp class is small, consisting of al148 = 65536 functions map-E u 1 \"f max ping from the eight possible observations to the four acthen with probability at least 1 - 6, V will be uniformly tions corresponding to trying to move in each of the comclose to V: pass directions. Actions are noisy, and result in moving\nfor all 1r E II (8)\nCorollary 4 Under the conditions of Theorem 1 or 3, let m be chosen as in the Theorem. Then with probability at least 1 - 6, the policy it chosen by optimizing the value estimates, given by it = argmax,.En V(1r), will be near optimal in II:\nV(it) \ufffd opt(M,IT)- 2t: (9)\nRemark. The (Lipschitz) continuity assumptions give a sufficient but not necessary set of conditions for the the orem, and other sets of sufficient conditions can be en visaged. For example, if we assume that the distribution on states induced by any policy at each time step has a bounded density, then we can show uniform convergence for a large class of (\"reasonable\") discontinuous reward functions such as R(s) = 1 if s1 > 0.5, 0 otherwise.5\n5Space constraints preclude a detailed discussion, but briefly, this is done by constructing two Lipschitz continuous reward functions Ru and RL that are \"close to\" and which upper- and lower-bound R (and which hence give value estimates that also upper- and lower-bound our value estimates under R); using the assumption of bounded densities to show our values under Ru and RL are \u20ac-close to that of R; applying Theorem 3 to show uni form convergence occurs with Ru and RL; and lastly deducing from this that uniform convergence occurs with R as well.\nin a random direction 20% of the time. Since the policy class is small enough to exhaustively enumerate, our opti mization algorithm for searching over policies was simply exhaustive search, trying a1148 policies on them scenarios, and picking the best one. Our experiments were done with 'Y = 0.99 and a horizon time of H = 100, and all results re ported on this problem are averages over 10000 trials. The deterministic simulative model was\n16(s, up) 6(s, left)\ng(s, a,p) = o(s, down) 6(s, right) 6(s, a) ifp::::; 0.05 if 0.05 < p::::; 0.10 if 0.10 < p::::; 0.15 if 0.15 < p::::; 0.20 otherwise\nwhere 6 ( s, a) denotes the result of moving one step from s in the direction indicated by a, and is s if this move would result in running into a wall.\nFigure 1 b shows the result of running this experiment, for different numbers of scenarios. The value of the best policy within II is indicated by the topmost horizontal line, and the solid curve below that is the mean policy value when using our algorithm. As we see, even using surprisingly small numbers of scenarios, the algorithm manages to find good policies, and as m becomes large, the value also approaches the optimal value.\n412 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nWe had previously predicted that a \"complicated\" deter ministic simulative model g can lead to poor results. For each (s, a)-pair, let hs,a : [0, 1] I-t [0, 1] be a hash function that maps any Uniform(O, 1] random variable into another Uniform[O, 1] random variable.6 Then if g is a determin istic simulative model, g'(s, a,p) = g(s, a, hs,a(P)) is an other one that, because of the presence of the hash function, is a much more \"complex\" model than g. (Here, we appeal to the reader's intuition about complex functions, rather than formal measures of complexity.) We would therefore predict that using PEGASUS with g' would give worse re sults than g, and indeed this prediction is borne out by the results as shown in Figure 1b (dashed curve). The differ ence between the curves is not large, and this is also not unexpected given the small size of the problem. 7\nOur second experiment used Randl!?)v and Alstr!?)m's [11] bicycle simulator, where the objective is to ride to a goal one kilometer away. The actions are the torque r applied to the handlebars and the displacement v of the rider's center of-gravity from the center. The six-dimensional state used in [11] includes variables for the bicycle's tilt angle and orientation, and the handlebar's angle. If the bicycle tilt exceeds 1r /15, it falls over and enters an absorbing state, receiving a large negative reward. The randomness in the simulator is from a uniformly distributed term added to the intended displacement of the center-of-gravity. Rescaled appropriately, this became the p term of our deterministic simulative model.\nWe performed policy search over the following space: We\n6In our experiments, this was implemented by choosing, for each (s, a) pair, a random integer k(s, a) from {1, ... , 1000}, and then letting hs,aCP) = fract(k(s,a) \u00b7 p), where fract(x) denotes the fractional part of x.\n7Theory predicts that the difference between g and g' 's perfor mance should be at most 0( Jiog IIII/m); see [7].\nselected a vector x of fifteen (simple, manually-chosen but not fine-tuned) features of each state; actions were then chosen With sigmoids: T = a-(w1 \u00b7 x)(rmax- Tmin) + Tmin\u2022 V = a-(w2 \u00b7 x)(vmax - Vmin) + Vmin\u2022 Where a-(z) = 1/(1 + e-z). Note that since our approach can handle continuous actions directly, we did not, unlike [11], have to discretize the actions. The initial-state distribution was manually chosen to be representative of a \"typical\" state distribution when riding a bicycle, and was also not fine tuned. We used only a small number m = 30 of scenarios, 'Y = 0.998, H = 500, with the continuous-time model of discounting discussed earlier, and (essentially) gradient as cent to optimize over the weights. 8 Shaping rewards, to reward progress towards the goal, were also used.9\nWe ran 10 trials using our policy search algorithm, testing each of the resulting solutions on 50 rides. Doing so, the median riding distances to the goal of the 10 different poli cies ranged from about 0.995km1 0 to 1.07km. In all 500 evaluation runs for the 10 policies, the worst distance we observed was also about 1.07km. These results are signifi cantly better than those of [11], which reported riding dis tances of about 7km (since their policies often took very \"non-linear\" paths to the goal), and a single \"best-ever\" trial of about 1. 7km.\n8Running experiments without the continuous-time model of discounting, we also obtained, using a non-gradient based hill climbing algorithm, equally good results as those reported here. Our implementation of gradient ascent, using numerically evalu ated derivates, was run with a bound on the length of a step taken on any iteration, to avoid problems near V ( 1r9) 's discontinuities.\n90ther experimental details: The shaping reward was propor tional to and signed the same as the amount of progress towards the goaL As in [11], we did not include the distance-from-goal as one of the state variables during training; training therefore pro ceeding \"infinitely distant\" from the goal.\n10Distances under lkm are possible since, as in [11], the goal has a lOrn radius.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 413\n6 Conclusions\nWe have shown how any POMDP can be transformed into an \"equivalent\" one in which all transitions are determin istic. By approximating the transformed POMDP's initial state distribution with a sample of scenarios, we defined an estimate for the value of every policy, and finally performed policy search by optimizing these estimates. Conditions were established under which these estimates will be uni formly good, and experimental results showed our method working well. It is also straightforward to extend these methods and results to the cases of finite-horizon undis counted reward, and infinite-horizon average reward with \u00a3-mixing time H<.\nAcknowledgements\nWe thank Jette Randl\ufffdv and Preben Alstr\ufffdm for the use of their bicycle simulator, and Ben Van Roy for helpful comments. A. Ng is supported by a Berkeley Fellowship. This work was also supported by ARO MURI DAAH0496-0341, ONR MURI N00014-00-1-0637, and NSF grant IIS-9988642.\nReferences\n[1] L. Baird and A.W. Moore. Gradient descent for gen eral Reinforcement Learning. In NIPS II, 1999.\n[2] Dimitri Bertsekas. Dynamic Programming and Opti mal Control, Vol. I. Athena Scientific, 1995.\n[3] John R. Birge and Francois Louveaux. Introduction to Stochastic Programming. Springer, 1997.\n[ 4] R. Durrett. Probability : Theory and Examples, 2nd edition. Duxbury, 1996.\n[5] P. W. Goldberg and M. R. Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. Machine Learning, 18:131-148, 1995.\n[6] D. Haussler. Decision-theoretic generalizations of the PAC model for neural networks and other appli cations. Information and Computation, 100:78-150, 1992.\n[7] M. Keams, Y. Mansour, and A. Y. Ng. Approximate planning in large POMDPs via reusable trajectories. (extended version of paper in NIPS 12), 1999.\n[8] H. Kimura, M. Yamamura, and S. Kobayashi. Re inforcement learning by stochastic hill climbing on discounted reward. In Proceedings of the Twelfth In ternational Conference on Machine Learning, 1995.\n[9] N. Meuleau, L. Peshkin, K-E. Kim, and L.P. Kael bling. Learning finite-state controllers for partially\nobservable environments. In Uncertainty in Artificial Intelligence, Proceedings of the Fifteenth conference, 1999.\n[10] D. Pollard. Empirical Processes: Theory and Appli cations. NSF-CBMS Regional Conference Series in Probability and Statistics, Vol. 2. Inst. of Mathemati cal Statistics and American Statistical Assoc., 1990.\n[11] J. Randl\ufffdv and P. Alstr\ufffdm. Learning to drive a bicy cle using reinforcement learning and shaping. In Pro ceedings of the Fifteenth International Conference on\nMachine Learning, 1998.\n[12] Benjamin Van Roy. Learning and Value Function Approximation in Complex Decision Processes. PhD thesis, Massachusetts Institute of Technology, 1998.\n[13] R. S. Sutton and A. G. Barto. Reinforcement Learn ing. MIT Press, 1998.\n[14] V.N. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, 1982.\n[15] J.K. Williams and S. Singh. Experiments with an al gorithm which learns stochastic memoryless policies for POMDPs. In NIPS II, 1999.\n[16] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256, 1992.\nAppendix A: Proof of Theorem 2\nProof (of Theorem 2). We construct an MDP with states s_1, s0, and s1 plus an absorbing state. The reward func tion is R(s;) = i for i = - 1, 0, 1. Discounting is ignored in this construction. Both s_1 and s1 transition with proba bility 1 to the absorbing state regardless of the action taken. The initial-state s0 has a .5 chance of transitioning to each of s-1 and s1. We now construct g, which will depend in a complicated way on the p term. Let U = {U\ufffd1 [a;, b;]ia;, b; E [0, 1] n Q, a; < b;, 1 \ufffd N < oo} be the countable set of all finite unions of intervals with rational endpoints in [0, 1]. Let U' be the countable subset of U that contains all elements of U that have total length (Lebesgue measure) exactly 0.5. For example, [1/3, 5/6] and [0.0, 0.25] U [0.5, 0.75] are both in U'. Let U1 , U2, . . . be an enumeration of the elements of U'. Also let { a1, a2, . . . } be an enumeration of (some countably infinite subset of) A. The deterministic simula tive model on these actions is given by:\nifp E U; otherwise\nSo, Psaa; (sl) = Psaa; (s_l) = 0.5 for all a;, and this is a correct model for the MDP. Note also that V ( 1r) = 0 for all 1r E II.\n414 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nFor any finite sample of m scenarios ( n(l}) ( n(2}) ( n(m)) h \u00b7 U so,p , so,p , ... , so,p , t ere exists some i such that Pij) f/. Ui for all j = 1, ... , m. Thus, evaluating 1fi = ai using this set of scenarios, all m simulated trajec tories will transition from s0 from s1, so the value estimate (assuming H, 2: 1) for 7f; is V(1ri) = 1. Since this argu ment holds for any finite number m of scenarios, we have shown that V does not uniformly converge to V ( 1r) = 0 (over 7f E II). 0\nAppendix B: Proof of Theorem 3\nDue to space constraints, this proof will be slightly dense. The proof techniques we use are due to Haussler [6] and Pollard [10]. Haussler [6], to which we will be repeatedly referring, provides a readable introduction to most of the methods used here.\nWe begin with some standard definitions from [6]. For a subset T of a space X endowed with (pseudo-)metric p, we say To C X is an t:-cover forT if, for every t E T, there is some t' E T0 such that p(t, t') :::; t:. For each t: > 0, let N ( t:, T, p) denote the size of the smallest :-cover forT. Let 1i be a family of functions mapping from a set X into a bounded pseudo metric space (A, p ), and let P be a prob ability measure on X. Define a pseudo metric on 1i by du(P,p) (!,g) = Ex\ufffdP[p(f (x), g(x))]. Define the capac ity of 1{ to be C(t:, 1{, p) = sup N(t, 11., d\u00a3l(P,p)), where the sup is over all probability measures P on X. The quan tity C( t, 1{, p) thus measures the \"richness\" of the class H. Note that C and N are both decreasing functions oft:, and that C(t:, 1{, p) = C(kt, 1{, kp) for any k > 0. The main results obtained with pseudo-dimension are uni form convergence of the empirical means of classes of random variables to their true means. Let 1{ be a fam ily of functions mapping from X into [0, M], and let x (the \"training set\") be m i.i.d. draws from some prob ability measure P over X. Then for each h E 1{, let fh(x) = (1/m) 2:7:1 h(xi) be the empirical mean of h(x). Also let rh(P) = Ex\ufffdP[h(x)] be the true mean. We now state a few results from [6]. In [6], these are The orem 6 combined with Theorem 12; Lemma 7; Lemma 8; and Theorem 9 (withY being a singleton set, f(y, a) = a, a = t/4M, and v = 2M). Below, \u00a31 and \u00a32 respectively denote the Manhattan and Euclidean metrics on \ufffdn. e.g. f1(x,Y) = 2:\ufffd=1 jxi- y;j.U Lemma 5 Let 1{ be a family of functions mapping from X into [0, M], and d = dimp(N). Then for any probabil ity measure P on X and any 0 < t: :::; M, we have that N(t, H,d\u00a31 (P,t2)):::; 2((2eM/t:) ln(2eM/t:))d.\nLemma 6 Let H1, . . . , 1ik each be a family of functions mapping from X into [0, 1]. The free product of the 1i; 's\n11This is inconsistent with the definition used in [6], which has an additional (1/n) factor.\nis the class of functions 1i = { (h, ... , fk) : /j E Hi} mapping from X into [0, l]k (where (h, ... , fk)(x) = (h (x), . . . , !k(x))). Then for any probability measure P on X andt > 0,\nk N(t, H, du(P,t1)):::; II N(t/k, Hi> du(P,t2)) (10)\nj=1\nLemma 7 Let (XI, pi), . . . , (Xk+I, Pk+d be bounded metric spaces, and for each j = 1, . . . , k, let 1ij be a class of functions mapping from Xi into Xj+l\u00b7 Suppose that each Hi is uniformly Lipschitz continuous (with respect to the metric Pi on its domain, and Pi+I on its range), with some Lipschitz bound bj 2: 1. Let 1i = {!k o \u00b7 \u00b7 \u00b7 o h : /j E 'Hj, 1 :::; j :::; k} be the class of functions mapping from xl into xk+1 given by composition of the functions in the 'Hj 's. Let to > 0 be given, and lett: = k(TI;=I bj )to. Then\nk C(t,1i,pk+!):::; II C(to,1ij,Pi+I) (11)\nj=l Lemma 8 Let 1{ be a family of functions mapping from X into [0, M], and let P be a probability measure on X. Let x be generated bym independent draws from X, and assume t: > 0. Then\n(12)\nWe are now ready to prove Theorem 3. No serious attempt has been made to tighten polynomial factors in the bound.\nProof (of Theorem 3). Our proof is in three parts. First, V gives an estimate of the discounted rewards summed over (H, +I)-steps; we reduce the problem of showing uniform convergence of V to one of proving that our estimates of the expected rewards on the H -th step, H = 0, ... , H,, all converge uniformly. Second, we carefully define the map ping from the scenarios s(il to the H -th step rewards, and use Lemmas 5, 6 and 7 to bound its capacity. Lastly, apply ing Lemma 8 gives our result. To simplify the notation in this proof, assume Rmax = 1, and B, BR 2: 1. Part 1: Re\ufffduction to uniform convergence of H -th step rewards. V was defined by\nV(7r) = \ufffd f R(sbi)) + rR(sii)) + . . . +,H. R(sw.). i=l\nFor each H, let VH(7r) = \ufffd 2:7:1 R(s(j/) be the empirical mean of the reward on the H -th step, and let V H ( 1r) = EsH [R(sH )] be the true expected reward on the H-th step (starting from s0 \"' D and executing 1r). Thus, V(1r) = l:'tl=o 1HVH(1r). Suppose we can show, for each H = 0, . . . , H., that with probability 1- Jj(H, + 1),\nIVH(7r)- VH(7r)j :::; t:/2(H, + 1) \\17f E II (13)\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 415\nThen by the union bound, we know that with probability 1- J, rVH(7r) - VH(7r)l \ufffd \u00a3/2(H, + 1) holds simulta neously for all H = 0, ... , H, and for all 1r E II. This implies that, for all 1r E II,\nH, H, \ufffd jV(1r)- L ,,HvH(7r) l + 1 L I'HvH(7r) - V(1r)l\nH=O H,\n\ufffd L rVH(7r)- VH(7r)l + \u20ac/2 H=O \ufffd\u20ac.\nwhere we used the fact that I I:Z::o ')'HVH(7r)- V(1r) l \ufffd \u00a3/2, by construction of the \u20ac-horizon time. But this is ex actly the desired result. Thus, we need only prove that Equation (13) holds with high probability for each H = o, ... ,H,. Part II: Bounding the capacity. Let H \ufffd H, be fixed. We now write out the mapping from a scenario s(i) E S x ([0, 1]dp)oo to the H-th step reward. Since this mapping depends only on the first dp \u00b7 H elements of the \"p\"s portion of the scenario, we will, with some abuse of notation, write the scenario as s(i) E S x [0, 1]dpH, and ignore its other coordinates. Thus, a scenario s<i) may now be written as (s,p l,P2, ... ,pdp H)\u00b7 Given a family of functions (such as F;) mapping from S x [0, 1]dp into [0, 1], we extend its domain to S x [0, 1]dp+n for any finite n ;::: 0 simply by having it ignore the ex tra coordinates. Note this extension of the domain does not change the pseudo-dimension of a family of functions. Also, for each n = 1, ... , n, define a mapping In from\nS x [0, 1]n t-+ [0, 1] according to In(s,p l,P2, ... ,Pn) = Pn\u00b7 For each n, let In = {In} be singleton sets. Where necessary, In's domain is also extended as we have just de scribed. For each i = 1, ... , H + 1, define X; = S x ([0, 1]dp)H+1-i. For example, xl is just the space of scenarios (with only the first dpH elements of the p's kept), and XH+I = S. For each i = 1, . . . , H, de fine a family of maps from X; into Xi+1 according to 1-l; = F1 X F2 X \u00b7 \u00b7 \u00b7 X Fds X Idp+l X Idp+2 X \u00b7 \u2022 \u2022 X I(H-i+l)dp (where the definition of the free product of sets of functions is as given in Lemma 6); note such an 1-l; has Lipschitz bound at most B o = (ds + Hdp)B. Also let 1-l H+I = {R} be a singleton set containing the reward function, and XH+2 = [-Rmax,Rmax]\u00b7 Finally, let}{ = 1-l H+I o 1-l H o \u00b7 \u00b7 \u00b7 o 1{1 be the family of maps from S X ([0, 1]dp)H into [-Rmax, Rmax]\u00b7 Now, let VM-, H : S' t-+ [-Rmax, Rmax] be the reward received on th\ufffd H -th step when executing 1r from a scenario s E S'. As we let 1r vary over II, this defines a family of maps from scenarios into [-Rmax, Rmaxl\u00b7 Clearly, this family of maps is a subset of 1-l. Thus, if we can bound the\ncapacity of}{ (and hence prove uniform converge over 1-l), we have also proved uniform convergence for VM-, ,H (over all 1r E II). For each i = 1, ... , ds, since dimp(F;) \ufffd d, Lemma 5 implies that N(\u20ac, F;, dL'(P,\u00a32)) \ufffd 2((2e/\u20ac) ln(2e/\u20ac))d. Moreover, clearly N(\u00a3,I;,dL'(P,\u00a32)) = 1 since each I; is a singleton set. Combined with Lemma 6, this implies that, for each i = 1, ... , H and \u20ac \ufffd 1,\nN(\u20ac, 1-l;, dL'(P,\u00a31)) ds\n\ufffdII N(\u00a3/(ds + (H- i)dp),Fj,dL'(P,\u00a32)) j=l ds \ufffd II N(\u00a3/(ds + H,dp),Fj,dLl(P.\u00a32)) j=l \ufffd 2ds (2e(ds : H,dp) ln 2e(ds : H,dp)) dds \ufffd 2ds ( 2e( ds : H,dp)) 2dds\nwhere we have used the fact that N is decreasing in its \u20ac parameter. By taking a sup over probability measures P, this is also a bound on C(\u20ac, 1-l;, \u00a31). Now, as metrics over JR(ds +(H -i)dp) , \u00a32 \ufffd \u00a31. Thus, this also gives\nFinally, applying Lemma 7 with each of the Pk 's being the \u00a32 norm on the appropriate space, k = H + 1, and \u20ac = (H + 1)Bf! BR\u00a3 o, we find\nC(\u20ac,1-l,\u00a32) H+ l\n\ufffd II C(\u20ac/((H+1)B/!BR),1-lj,f2) j=l\n<:; Q 2\n,, ce(ds + H,dp) , (H + l)B,lf BR) '\"'\n<:; 2,,n, ce(ds + H,dp )\ufffdH, + l)B[!\u2022 BR) \"''\"\u00b7 Part III: Proving uniform convergence. Applying Lemma 8 with the above bound on C(\u20ac, 1-l, \u00a32), we find that for there to be a 1 - J probability of our estimate of the expected H -th step reward to be \u20ac-close to the mean, it suffices that\n256 ( 1 ) m = \ufffd log-;s+log(4C(\u00a3/16,1i,\u00a32)) = 0 (poly (d, \ufffd , log\ufffd , 1 \ufffd l',logB,logBR,ds,dp)) . This completes the proof of the Theorem. D"}], "references": [{"title": "Gradient descent for gen\u00ad eral Reinforcement Learning", "author": ["L. Baird", "A.W. Moore"], "venue": "In NIPS II,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Dynamic Programming and Opti\u00ad mal Control, Vol. I", "author": ["Dimitri Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": ", [16, 8, 15, 1, 7]), which Michael Jordan", "startOffset": 2, "endOffset": 19}, {"referenceID": 0, "context": "We assume we have a deterministic function g : S x A x [0, 1]dp 1-t S, so that for any fixed (s, a)-pair, if pis distributed Uniform[O, 1]dp, then g(s, a,jf) is distributed according to the transition dis\u00ad tribution Psa ( \u00b7 ) .", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "In other words, to draw a sample from Psa ( \u00b7) for some fixed s and a, we need only draw p uni\u00ad formly in [0, 1]dp, and then take g(s, a,jf) to be our sample.", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "[2]) routinely define POMDPs using essentially deterministic simulative mod\u00ad els.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "S x [0, 1]dp into successor states S.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "\" For each f, also let fi be the i-th coordinate function (so that fi(s,if) is the i-th coordinate of f(s,P)) and let :F; be the corresponding families of coordinate functions map\u00ad ping from S x [0, 1]dp into [0, 1].", "startOffset": 195, "endOffset": 201}, {"referenceID": 0, "context": "\" For each f, also let fi be the i-th coordinate function (so that fi(s,if) is the i-th coordinate of f(s,P)) and let :F; be the corresponding families of coordinate functions map\u00ad ping from S x [0, 1]dp into [0, 1].", "startOffset": 209, "endOffset": 215}, {"referenceID": 0, "context": "The state space for M1 is S x [0, 1]00\u2022 In other words, a typical state in M1 can be written as a vector ( s, PI, P2, .", "startOffset": 30, "endOffset": 36}, {"referenceID": 0, "context": ") -this consists of a state s from the original state space S, followed by an infinite sequence of real numbers in [0, 1].", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "S1 = S x [0, 1]00, so that (s, PI, P2, .", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "The final implementational detail is that, since the states s\ufffdi) E S x [0, 1]00 are infinite-dimensional vectors, we have no way of representing them (and their successor states) explicitly.", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "2More precisely, if the agent enters the goal region on some time step, then rather than giving it a reward of 1, we figure out what fraction r E [0, 1] of that time step (measured in continuous time) the agent had taken to enter the goal region, and then give it reward'\"( instead.", "startOffset": 146, "endOffset": 152}, {"referenceID": 0, "context": "For the remainder of this section, assume S = [0, 1]ds.", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "Then :F is a class of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1]ds, and so a simple way to capture its \"complexity\" is to capture the complexity of its families of coordinate functions, :Fi, i = 1, .", "startOffset": 45, "endOffset": 51}, {"referenceID": 0, "context": "Then :F is a class of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1]ds, and so a simple way to capture its \"complexity\" is to capture the complexity of its families of coordinate functions, :Fi, i = 1, .", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Then :F is a class of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1]ds, and so a simple way to capture its \"complexity\" is to capture the complexity of its families of coordinate functions, :Fi, i = 1, .", "startOffset": 70, "endOffset": 76}, {"referenceID": 0, "context": "Each :Fi is a family of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1], the i-th coordinate of the state vector.", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "Each :Fi is a family of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1], the i-th coordinate of the state vector.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Each :Fi is a family of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1], the i-th coordinate of the state vector.", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "Theorem 3 Let a POMDP with state spaceS = [0, 1]ds, and a possibly infinite action space be given.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "For each (s, a)-pair, let hs,a : [0, 1] I-t [0, 1] be a hash function that maps any Uniform(O, 1] random variable into another Uniform[O, 1] random variable.", "startOffset": 33, "endOffset": 39}, {"referenceID": 0, "context": "For each (s, a)-pair, let hs,a : [0, 1] I-t [0, 1] be a hash function that maps any Uniform(O, 1] random variable into another Uniform[O, 1] random variable.", "startOffset": 44, "endOffset": 50}], "year": 2011, "abstractText": "We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observ\u00ad able Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Pol\u00ad icy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value esti\u00ad mates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng [7], but with \"sample complexity\" bounds that have only a polynomial rather than exponential depen\u00ad dence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infi\u00ad nite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learn\u00ad ing to ride a bicycle.", "creator": "pdftk 1.41 - www.pdftk.com"}}}