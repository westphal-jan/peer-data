{"id": "1301.4862", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2013", "title": "Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots", "abstract": "We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy pa- rameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot. These results have been replicated in the automated tasks, in which the robot can be continuously engaged in a task space for learning inverse models and learning models.", "histories": [["v1", "Mon, 21 Jan 2013 13:26:07 GMT  (6765kb,D)", "http://arxiv.org/abs/1301.4862v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE cs.RO", "authors": ["adrien baranes", "pierre-yves oudeyer"], "accepted": false, "id": "1301.4862"}, "pdf": {"name": "1301.4862.pdf", "metadata": {"source": "CRF", "title": "Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots", "authors": ["Adrien Baranes", "Pierre-Yves Oudeyer"], "emails": [], "sections": [{"heading": null, "text": "We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters.\n\u2217Baranes, A., Oudeyer, P-Y. (2012) Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and Autonomous Systems, 61(1), pp. 49-73. http://dx.doi.org/10.1016/j.robot.2012.05.008\nar X\niv :1\n30 1.\nWe present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.\nKeywords: Active Learning, Competence Based Intrinsic Motivation, Curiosity-Driven Task Space Exploration, Inverse Models, Goal Babbling, Autonomous Motor Learning, Developmental Robotics, Motor Development."}, {"heading": "1 Motor Learning and Exploration of Forward", "text": "and Inverse Models\nTo operate robustly and adaptively in the real world, robots need to know how to predict the consequences of their actions (called here forward models, mapping typically X = (S, \u03c0\u03b8), where S is the state of a robot and \u03c0\u03b8 : S \u2192 A is a parameterized action policy, to the space of effect, or task space, Y ). Reversely, they need to be able to compute the action policies that can generate given effects (called here inverse models, (S, Y ) \u2192 \u03c0\u03b8). These models can be quite varied, for example mapping joint angles to hand position in the visual field, oscillation of the legs to body translation, movement of the hand in the visual field to movement of the end point of a tool, or properties of a hand tap an object to the sound it produces. Some of these models can be analytically elaborated by an engineer and provided to a robot (e.g. forward and inverse kinematics of a rigid body robot). But in many cases, this is impossible either because the physical properties of the body itself cannot be easily modeled (e.g. compliant bodies with soft materials), or because it is impossible to anticipate all possible objects the robot might interact with, and thus the properties of objects. More generally, it is impossible to model a priori all the possible effects a robot can produce on its environment, especially when robots are targeted to interact with in everyday human environments, such as in assistive robotics. As a consequence, learning these models through experience becomes necessary. Yet, this poses highly difficult technical challenges, due in particular to the combination of the following facts: 1) these models are often high-dimensional, continuous and highly non-stationary spatially, and sometimes temporally; 2) learning examples have to be collected autonomously and incrementally by robots; 3) learning, as we will detail below, can happen either through self-experimentation or observation, and both of these takes significant\nphysical time in the real world. Thus, the number of training examples that can be collected in a life-time is strongly limited with regards to the size and complexity of the spaces. Advanced statistical learning techniques dedicated to incremental high-dimensional regression have been elaborated recently, such as [107, 72]. Yet, these regression mechanisms are efficient only if the quality and quantity of data is high enough, which is not the case when using unconstrained exploration such as random exploration. Fundamental complementary mechanisms for guiding and constraining autonomous exploration and data collection for learning are needed.\nIn this article, we present a particular approach to address constrained exploration and learning of inverse models in robots, based on an active learning process inspired by mechanisms of intrinsically motivated learning and exploration in humans. As we will explain, the approach studies the combination of two principles for learning efficiently inverse models in high-dimensional redundant continuous spaces:\n\u2022 Active goal/task exploration in a parameterized task space: The architecture makes the robot sample actively novel parameterized tasks in the task space, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. This allows to leverage the redundancies of the sensorimotor mapping, leading the system to explore densely only subregions of the space of action policies that are enough to achieve all possible effects. Thus, it does not need to learn a complete forward model and contrasts with approaches that directly sample action policy parameters and observe their effects in the task space. The system also leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters.\n\u2022 Interestingness as empirically evaluated competence progress: The measure of interestingness for a given goal/task is based on competence progress empirically evaluated, i.e. how previous attempts of low-level optimization directed at similar goals allowed to improve the capability of the robot to reach these goals.\nIn the rest of the section, we review various related approaches to constraining exploration for motor learning."}, {"heading": "1.1 Constraining the Exploration", "text": "A common way to carry out exploration is to use a set of constraints on guiding mechanisms and maximally reduce the size and/or dimensionality of explored spaces. Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60]. Typically, a robot teacher manually interacts with the robot by showing it a few\nbehaviors corresponding to a desired movement or goal that it will then have to reproduce. This strategy prevents the robot from performing any autonomous exploration of its space and requires an attentive demonstrator. Some other techniques allow more freedom to the human teacher and the robot by allowing the robot to explore. This is typically what happens in the reinforcement learning (RL) framework where no demonstration is originally required and only a goal has to be fixed (as a reward) by the engineer who conceives the system [114, 84, 117]. Nevertheless, when the robot evolves in high-dimensional and large spaces, the exploration still has to be constrained. For instance, studies presented in [80] combine RL with the framework of learning by demonstration. In their experiments, an engineer has to first define a specific goal in a task space as a handcrafted reward function, then, a human demonstrator provides a few examples of successful motor policies to reach that goal, which is then used to initialize an optimization procedure. The Shifting Setpoint Algorithm (SSA) introduced by Schaal and Atkeson [92] proposes another way to constrain the exploration process. Once a goal fixed in an handcrafted manner, a progressive exploration process is proposed: the system explores the world gradually from the start position and toward the goal by creating a local model around the current position and shifting in direction of the goal once this model is reliable enough, and so on. These kinds of techniques therefore restrain the exploration to narrow tubes of data targeted at learning specific tasks/goals that have to be defined by a human, either the programmer or a non-engineer demonstrator.\nThese methods are efficient and useful in many cases. Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.e. having a robot learn to generate in a controlled manner many effects rather than only a single goal), it is not conceivable that a human being interacts with a robot at each instant or that an engineer designs and tunes a specific reward function for each novel task to be learned. For this reason, it is necessary to introduce mechanisms driving the learning and exploration of robots in an autonomous manner."}, {"heading": "1.2 Driving Autonomous Exploration", "text": "Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60]. In the regression setting, they are used to learn a regression mapping between an input space X and an output space Y while minimizing the sample complexity, i.e. with a minimal number of examples necessary to reach a given performance level. These methods, typically beginning by random and sparse exploration, build meta-models of performances of the motor learning mechanisms and concurrently drive the exploration in various sub-spaces for which a notion of interest is defined, often consisting in variants of expected informational gain. A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-\nment [51], or maximal uncertainty of the model [119] among others. There have been active-extensions to most of the existent learning methods, e.g. logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57]. Only very recently have these approaches been applied to robotic problems, and even more recently if we consider examples with real robots. Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.\nAnother approach to exploration came from an initially different problem, that of understanding how robots could achieve cumulative and open-ended learning autonomously. This raised the question of the task-independent mechanisms that may allow a robot to get interested in practicing skills and learn new tasks that were not specified at design time. Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].\nAs argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries. Yet, in spite of these similarities between work in active learning and intrinsic motivation, these two strands of approaches often differ in their underlying assumptions and constraints, leading to sometimes very different active learning algorithms. In many active learning models, one often assumes that it is possible to learn a model of the complete world within lifetime, and/or that the world is learnable everywhere, and/or where noise is homogeneous everywhere. Given those assumptions, heuristics based on the exploration of parts of the space where the learned model has maximal uncertainties or where its prediction are maximally wrong are often very efficient. Yet, these assumptions typically do not hold in real world robots in an unconstrained environment: the sensorimotor spaces, including the body dynamics and its interactions with the external world, are simply much too large to be learned entirely in a life time; there are typically subspaces which are unlearnable due to inadequate learning biases or unobservable variables; noise can be strongly homogeneous. Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101]. This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any\nexternal reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40]. Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77]. For instance, the architecture called RobustIntelligent Adaptive Curiosity (RIAC) [8], which is a refinement of the IAC architecture which was elaborated for open-ended learning of affordances and skills in real robots [77], defines the interestingness of a sensorimotor subspace by the velocity of the decrease of the errors made by the robot when predicting the consequences of its actions, given a context, within this subspace. As shown in [77, 8], it biases the system to explore subspaces of progressively increasing complexity.\nNevertheless, RIAC and similar \u201dknowledge based\u201d approaches (see [74]) have some limitations: first, while they can deal with the spatial or temporal non-stationarity of the model to be learned, they face the curse-of-dimensionality and can only be efficient when considering a moderate number of control dimensions (e.g. up to 9/10). Indeed, as many other active learning methods, RIAC needs a certain level of sampling density in order to extract and compare the interest of different areas of the space. Also, because performing these measure costs time, this approach becomes more and more inefficient as the dimensionality of the control space grows [19]. Second, they focus on the active choice of motor commands and measures of their consequences, which allows learning forward models that can be re-used as a side effect for achieving goals/tasks through online inversion: this approach is sub-optimal in many cases since it explores in the high-dimensional space of motor commands and consider the achievement of tasks only indirectly.\nA more efficient approach consists in directly actively exploring task spaces, which are also often much lower-dimensional, by actively self-generating goals within those task spaces, and then learn associated local coupled forward/inverse models that are useful to achieve those goals. Yet, as we will see, the process is not as straightforward as learning the forward model, since because of the space redundancy it is not possible to learn directly the inverse model (and this is the reason why learning the forward model and then only inversing it has often been achieved). In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108]."}, {"heading": "1.3 Driving the Exploration at a Higher Level", "text": "In a framework where a system should be able to learn to perform a maximal amount of different tasks (here this means achieving many goals/tasks in a parameterized task space) before focusing on different ways to perform the same tasks (here this means finding several alternative actions to achieve the same goal), knowledge-based exploration techniques like RIAC cannot be efficient in robots with redundant forward models. Indeed, they typically direct a robotic\nsystem to spend copious amounts of time exploring variations of action policies that produce the same effect, at the disadvantage of exploring other actions that might produce different outcomes, useful to achieve more tasks. An example of this is learning 10 ways to push a ball forward instead of learning to push a ball in 10 different directions. One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123]. Once a goal/task is chosen, the system would then try to reach it with a lower-level goal-reaching architecture typically based on coupled inverse and forward models, which might include a lower-level goal-directed active exploration mechanism.\nTwo other developmental constraints, playing an important role in infant motor development, and presented in the experimentations of this paper, can also play an important role when considering such a task-level exploration process. First, we use motor synergies which have been shown as simplifying motor learning by reducing the number of dimensions for control (nevertheless, even with motor synergies, the dimensionality of the control space can easily go over several dozens, and exploration still needs to be organized). These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning. Second, we will use a heuristic inspired by observations of infants who sometimes prepare their reaching movements by starting from a same rest position [17], by resetting the robot to such a rest position, which allows reducing the set of starting states used to perform a task.\nIn this paper, we propose an approach which allows us to transpose some of the basic ideas of IAC and RIAC architectures, combined with ideas from the SSA algorithm, into a multi-level active learning architecture called SelfAdaptive Goal Generation RIAC algorithm (SAGG-RIAC) (an outline and initial evaluation of this architecture was presented in [9]). Unlike RIAC which was made for active learning of forward models mapping action policy parametes to effects in a task space, we show that this new algorithm allows for efficient learning of inverse models mapping parameters of tasks to parameters of action policies that allow to achieve these tasks in redundant robots. This is achieved through active sampling of novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. This takes advantage of both the typical redundancy of the mapping and of the fact that very often the dimensionality of the task space considered is much smaller than the dimensionality of motor primitives/action parameter space. Such an architecture also leverages both techniques for optimizing action policy parameters for a single predefined tasks (e.g. [79, 112]), as well as regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters (e.g. [20, 8, 55, 108]). While\napproaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.\nSAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127]. In a competence based active exploration mechanism, according to the definition [74], the robot is pushed to perform an active exploration in the goal/operational space as opposed to motor babbling in the actuator space.\nSeveral strands of previous research have began exploration various aspects of this family of mechanisms. First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102]. While the high expressivity of these formalisms allows in principle to tackle a wide diversity of problems, they were not designed nor experimented for the particular family of problems of learning high-dimensional continuous models in robotics. While SAGG-RIAC also actively and adaptively self-generates goals, this is achieved with a formalism based on applied mathematics and dedicated to the problem of learning inverse models in continuous redundant spaces.\nMeasures of interestingness based on a measure of competence to perform a skill were studied in [6], as well as in [94] where a selector chooses to perform different skills depending on the temporal difference error to reach each skill. The study proposed in [111] is based on the competence progress, which they use to select goals in a pre-specified set of skills considered in a discrete world. As we will show, SAGG-RIAC also uses competence progress, but targets learning in high-dimensional continuous robot spaces.\nA mechanism for passive exploration in the task space for learning inverse models in high-dimensional continuous robotics spaces was presented in [85, 86], where a robot has to learn its arm inverse kinematics while trying to reach in a preset order goals put on a pre-specified grid informing the robot about the limits of its reachable space. In SAGG-RIAC exploration is actively driven in the task space, allowing the learning process to minimize its sample complexity, and as we will show, to reach a high-level of performances in generalization and to discover automatically its own limits of reachability.\nIn the following sections we introduce the global architecture and formalization of the Self-Adaptive Goal-Generation SAGG-RIAC architecture. Then, we study experimentally its capabilities to allow a robot efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals, and in the context of three experimental setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible\nwire. More precisely, we focus on the following aspects and contributions of the architecture:\n\u2022 SAGG-RIAC creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity of learnability;\n\u2022 Drives the learning of a high variety of parameterized tasks (i.e. capability to reach various regions of the goal/task space) instead of numerous ways to perform the same task;\n\u2022 Allows learning fields of tasks in high-dimensional high-volume control spaces as long as the task space is low-dimensional (it can be high-volume);\n\u2022 Allows learning in task-spaces where only small and initially unknown subparts are reachable;\n\u2022 Drives the learning of inverse models of highly-redundant robots with different body schemas;\n\u2022 Guides the self-discovery of the limits of what the robot can achieve in its task space;\n\u2022 Allows improving significantly the quality of learned inverse models in terms of speed of learning and generalization performance to reach goals in the task space, compared to different methods proposed in the literature;"}, {"heading": "2 Competence Based Intrinsic Motivation: The", "text": "Self-Adaptive Goal Generation RIAC Architecture"}, {"heading": "2.1 Global Architecture", "text": "Let us consider the definition of competence based models outlined in [74], and extract from it two different levels for active learning defined at different time scales (Fig. 1):\n1. The higher level of active learning (higher time scale) takes care of the active self-generation and self-selection of goals/tasks in a parameterized task space, depending on a measure of interest based on the level of competences to reach previously generated goals (e.g. competence progress);\n2. The lower level of active learning (lower time scale) considers the goaldirected active choice and active exploration of lower-level actions to be taken to reach the goals selected at the higher level, and depending on local measures of interest related to the evolution of the quality of learned inverse and/or forward models;"}, {"heading": "2.2 Model Formalization", "text": "Let us consider a robotic system described in both a state/context space S, and a task space Y which is a field of parameterized tasks/goals that can be viewed as defining a field of parameterized reinforcement learning problems. For a given context s \u2208 S, a sequence of actions a = {a1, a2, ..., an} \u2208 A, potentially generated by a parameterized motor synergy \u03c0\u03b8 : S \u2192 A (alternatively called an \u201coption\u201d and including a self-termination mechanism), allows a transition toward the new states y \u2208 Y such that (s, a) \u2192 y, also written (s, \u03c0\u03b8) \u2192 y. For instance, in the first experiment introduced in the following sections where we use a robotic manipulator, S represents its actuator/joint space, Y the operational space corresponding to the cartesian position of its end-effector, and A relates to velocity commands in the joints. Also, in the second experiment involving a quadruped where we use motor synergies, the context s is always reset to a same state and has thus no influence on the learning, A relates to the 24 dimensional parameters of a motor synergy which considers the frequency and amplitude of sinusoids controlling the position of each joints over time, and Y relates to the position and orientation of the robot after the execution of the synergy during a fixed amount of time.\nSAGG-RIAC drives the exploration and learning of how to reach goals given starting contexts/states. Starting states are formalized as configurations s \u2208 S and goals as a desired yg \u2208 Y . All states are considered to be potential starting\nstates; therefore, once a goal has been generated, the low-level goal directed exploration and learning mechanism always tries to reach it by starting from the current state of the system as formalized and explained below.\nWhen the initiation position sstart, the goal yg and constraints \u03c1 (e.g. linked with the spent energy) are chosen, it generates a motor policy \u03c0\u03b8(Data)(sstart, yg, \u03c1) parameterized by sstart, yg and \u03c1 as well as parameters \u03b8 of internal forward and inverse models already learned with previously acquired data Data. Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].\nWe can make an analogy of this formalization with the Semi-Markov Option framework introduced by Sutton [116]. In the case of SAGG-RIAC, when considering an option \u3008I, \u03c0, \u03b2\u3009, we can first define the initiation set I : S \u2192 [0; 1], where I is true everywhere, because, as presented before, every state can here be considered as a starting state. Also, goals are related to the terminal condition \u03b2 and \u03b2 = 1 when the goal is reached, and the policy \u03c0 encodes the skill learned through the process induced by the lower-level of active learning and shall be indexed by the goal yg, i.e. \u03c0yg . More formally, as induced by the use of semi-markov options, we define policies and termination conditions as dependent on all events between the initiation of the option, and the current instant. This means that the policy \u03c0, and \u03b2 are depending on the history ht\u03c4 = {st, at, st+1, at+1..., s\u03c4} where t is the initiation time of the option, and \u03c4 , the time of the latest event. Denoting the set of all histories by \u2126, the policy and termination condition become defined by \u03c0 : \u2126\u00d7A\u2192 [0; 1] and \u03b2 : \u2126\u2192 [0; 1].\nMoreover, because we have to consider cases where goals are not reachable (either because of physical impossibility or because the robot is not capable of doing it at that point of its development), we need to define a timeout tmax which can stop a goal reaching attempt once a maximal number of actions has been executed. ht\u03c4 is thus needed to stop \u03c0, (i.e. the low-level active learning process), if \u03c4 > tmax.\nEventually, using the framework of options, we can define the process of goal self-generation, as the self-generation and self-selection of parameterized options, and a goal reaching attempt corresponding to the learning of a particular option. Therefore, the global SAGG-RIAC process can be described as exploring and learning fields of options."}, {"heading": "2.3 Lower Time Scale: Active Goal Directed Exploration and Learning", "text": "In SAGG-RIAC, once a goal has been actively chosen at the high-level, the goal directed exploration and learning mechanism at the lower can be carried out in numerous ways: the architecture makes only little assumptions about them, and thus is compatible with many methods such as those described below (this is the reason why SAGG-RIAC is an architecture defining a family of algorithms).\nIts main idea is to guide the system toward the goal by executing low-level actions which allow a progressive exploration of the world toward this specific goal and that updates at the same time the local corresponding forward and inverse models, leveraging previously learnt correspondences with regression. The main assumptions about the methods that can be used for this lower level are:\n\u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108];\n\u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45];\nA optional feature, which is a variant of the second assumption above, is:\n\u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal. A learning feedback mechanism has to be added such that the exploration is active, and the selection of new actions depends on local measures about the quality of the learned model.\nIn the following experiments that will be introduced, we will use two different methods: one mechanism where optimization is inspired by the SSA algorithm [92], coupled with memory-based local forward and inverse regression models using local Moore-Penrose pseudo-inverses, and a more generic optimization algorithm mixing stochastic optimization with memory-based regression models using pseudo-inverse. Other kinds of techniques could be used. For the optimization part, algorithms such as natural actor-critic architectures in model based reinforcement learning [78], algorithms of convex optimization [33], algorithms of stochastic optimization like CMA (e.g. [45]), or path-integral methods (e.g. [113, 112]).\nFor the regression part, we are here using a memory-based approach, which if combined with efficient data storage and access structures [4, 70], scales well from a computational point of view. Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108]."}, {"heading": "2.4 Higher Time Scale: Goal Self-Generation and Self-Selection", "text": "The Goal Self-Generation and Self-Selection process relies on a feedback defined using the concept of competence, and more precisely on the competence\nimprovement in given regions (or subspaces) of the task space where goals are chosen. The measure of competence can be computed at different instants of the learning process. First, it can be estimated once a reaching attempt in direction of a goal has been declared as terminated. Second, for robotic setups which are compatible with this option, competence can be computed during low-level reaching attempts. In the following sections, we detail these two different cases:"}, {"heading": "2.4.1 Measure of Competence for a Terminated Reaching Attempt", "text": "A reaching attempt for a goal is considered terminated according to two conditions:\n\u2022 A timeout related to a maximum number of iterations allowed by the low-level of active learning has been exceeded.\n\u2022 The goal has effectively been reached.\nWe introduce a measure of competence for a given goal reaching attempt as dependent on two metrics: the similarity between the point in the task space yf attained when the reaching attempt has terminated, and the actual goal yg; and the respect of constraints \u03c1. These conditions are represented by a cost, or competence, function C defined in [\u2212\u221e; 0], such that higher C(yg, yf , \u03c1) will be, the more a reaching attempt will be considered as efficient. From this definition, we set a measure of competence \u0393yg directly linked with the value of C(yg, yf , \u03c1):\n\u0393yg = { C(yg, yf , \u03c1) if C(yg, yf , \u03c1) \u2264 \u03b5sim < 0 0 otherwise\nwhere \u03b5sim is a tolerance factor such that C(yg, yf , \u03c1) > \u03b5sim corresponds to a goal reached. We note that a high value of \u0393yg (i.e. close to 0) represents a system that is competent to reach the goal yg while respecting constraints \u03c1. A typical instantiation of C, without constraints \u03c1, is defined as C(yg, yf , \u2205) = \u2212\u2016yg\u2212yf\u20162, and is the direct transposition of prediction error in RIAC [77, 8] to the task space in SAGG-RIAC. Yet, this competence measure might take some other forms in the SAGG-RIAC architecture, such as the variants explored in the experiments below."}, {"heading": "2.4.2 Measure of Competence During a Reaching Attempt or During Goal-Directed Optimization", "text": "When the system exploits its previously learnt models to reach a goal yg, using a computed \u03c0\u03b8 through adequate local regression, or when it is using the low-level goal-directed optimization to optimize the best current \u03c0\u03b8 to reach a self-generated goal yg, it does not only collect data allowing to measure its competence to reach yg, but since the computed \u03c0\u03b8 might lead to a different effect ye 6= yg, it also allows to collect new data for improving the inverse model and the measure of competence to reach other goals in the locality of ye. This allows to use all experiments of the robot to update the model of competences over the space of paremeterized goals."}, {"heading": "2.4.3 Definition of Local Competence Progress", "text": "The active goal self-generation and self-selection relies on a feedback linked with the notion of competence introduced above, and more precisely on the monitoring of the progress of local competences. We first need to define this notion of local competence. Let us consider a subspace called a region R \u2282 Y . Then, let us consider different measures of competence \u0393yi computed for different attempted goals yi \u2208 R, in a time window consisting of the \u03b6 last attempted goals. For the region R, we can compute a measure of competence \u0393 that we call a local measure such that:\n\u0393 =\n(\u2211 yj\u2208R(\u0393yj )\n|R|\n) (1)\nwith |R|, cardinal of R. Let us now consider different regions Ri of Y such that Ri \u2282 Y , \u22c3 iRi = Y (initially, there is only one region which is then progressively and recursively split; see below and see Fig. 2). Each Ri contains attempted goals {yi1,t1 , yi2,t2 , ..., yik,tk}Ri and corresponding competences obtained {\u0393yi1,t1 ,\u0393yi2,t2 , ...,\u0393yik,tk }Ri , indexed by their relative time order of experimentation t1 < t2 < ... < tk|tn+1 = tn + 1 inside this precise subspace Ri (ti are not the absolute time, but integer indexes of relative order in the given region).\nAn estimation of interest is computed for each region Ri. The interest interesti of a region Ri is described as the absolute value of the derivative of local competences inside Ri, hence the amplitude of local competence progress, over a sliding time window of the \u03b6 more recent goals attempted inside Ri (equation 2):\ninteresti =\n\u2223\u2223\u2223\u2223\u2223\u2223  |Ri|\u2212 \u03b62\u2211 j=|Ri|\u2212\u03b6 \u0393yj \u2212  |Ri|\u2211 j=|Ri|\u2212 \u03b62 \u0393yj \u2223\u2223\u2223\u2223\u2223\u2223 \u03b6\n(2)\nBy using a derivative, the interest considers the variation of competences, and by using an absolute value, it considers cases of increasing and decreasing competences. In SAGG-RIAC, we will use the term competence progress with its general meaning to denote this increase and decrease of competences.\nAn increasing competence signifies that the expected competence gain in Ri is important. Therefore, potentially, selecting new goals in regions of high competence progress could bring both a high information gain for the learned model, and also drive the reaching of not previously achieved goals.\nDepending on the starting position and potential evolution of the environment or of the body (e.g. breaking of a body part), a decrease of competences inside already well-reached regions can arise. In this case, the system should be able to focus again in these regions in order to at least verify the possibility to\nre-establish a high level of competence inside. This explains the usefulness to consider the absolute value of the competence progress as shown in equation 2.\nUsing a sliding window in order to compute the value of interest prevents the system from keeping each measure of competence in its memory, and thus limits the storage resource needed by the core of the SAGG-RIAC architecture."}, {"heading": "2.4.4 Goal Self-Generation Using the Measure of Interest", "text": "Using the previous description of interest, the goal self-generation and selfselection mechanism carries out two different processes:\n1. Splitting of the space Y where goals are chosen, into subspaces, according to heuristics that allows to maximally discriminate areas according to their levels of interest.\n2. Selecting the next goal to perform.\nSuch a mechanism has been described in the RIAC algorithm introduced in [8], but was previously applied to the actuator space S rather than to the goal/task space Y as is done in SAGG-RIAC. Here, we use the same kind of methods such as a recursive split of the space, each split being triggered once a predefined maximum number of goals gmax has been attempted inside. Each split is performed such that it maximizes the difference of the interest measure described above in the two resulting subspaces. This allows the easy separation of areas of differing interest and therefore of differing reaching difficulty. More precisely, here the split of a region Rn into Rn+1 and Rn+2 is done by selecting among m randomly generated splits, a split dimension j \u2208 |Y | and then a position vj such that:\n\u2022 All the yi of Rn+1 have a jth component smaller than vj ;\n\u2022 All the yi of Rn+2 have a jth component higher than vj ;\n\u2022 The quantityQual(j, vj) = card(Rn+1).card(Rn+2).\u2016interestRn+1\u2212interestRn+2\u2016 is maximal;\nFinally, as soon as at least two regions exist after an initial random exploration of the whole space, goals are chosen according to the following heuristics, selected according to probabilistic distributions:\n1. mode(1): in p1% percent (typically p1 = 70%) of goal selections, a random goal is chosen along a uniform distribution inside a region which is selected with a probability proportional to its interest value:\nPn = interestn \u2212min(interesti)\u2211|Rn| i=1 interesti \u2212min(interesti)\n(3)\nWhere Pn is the selection probability of the region Rn, and interesti corresponds to the current interest of the region Ri. 2. mode(2): in p2% (typically p2 = 20% of cases), a random goal is chosen inside the whole space Y . 3. mode(3): in p3% (typically p3 = 10%), a region is first selected according to the interest value (like in mode(1)) and then a new goal is generated close to the already experimented one which received the lowest competence estimation."}, {"heading": "2.4.5 Reduction of the Initiation Set", "text": "In order to improve the quality of the learned inverse model, we add a heuristic inspired by two observations on infant motor exploration and learning. The first one, proposed by Berthier et al. [17] is that infant\u2019s reaching attempts are often preceded by movements that either elevate their hand or move their hand back to their side. And the second one, noticed in [85], is that infants do not try to reach for objects forever but sometimes relax their muscles and rest. Practically, these two characteristics allow them to reduce the number of initiation positions that they use to reach an object, which simplifies the reaching problem by letting them learn a reduced number of reaching movements.\nSuch mechanism can be transposed in robotics to motor learning of arm reaching tasks as well as other kind of skills such as locomotion or fishing as shown in experiments below. In such a framework, it directly allows a highlyredundant robotic system to reduce the space of initiation states used to learn to reach goals, and also typically prevent it from experimenting with too complex actuator configurations. We add such a process in SAGG-RIAC, by specifying a rest position (srest, yrest) reachable without any need of planning from the system, that is set for each r subsequent reaching attempts (we call r the reset value, with r > 0)."}, {"heading": "2.5 New Challenges of Unknown Limits of the Task Space", "text": "In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval\n(for instance, the range of angles that can be taken by a motor, or the phases and amplitudes of CPGs which can be easily identified). In these cases, the challenge is to select which areas would potentially give the most information to the system, to improve its knowledge, inside this fixed range of possibilities. As argued earlier, a limit of these approaches is that they become less and less efficient as the dimensionality of the control space increases. Competence based approaches allow to address this issue when a low-dimensional task space can be identified. Nevertheless, in that case, a new problem arises when considering unbounded learning: the space where goals are reachable can be extremely large and it is generally very difficult to predict its limits and undesirable to ask the engineer to identify them. Therefore, when carried out in large spaces where the reachable area is only a small part of it, the algorithm could necessitate numerous random goal self-generations to be able to estimate interests of different subregions. In order to reduce this number, and help the system to converge easily toward regions where competence can be improved, we emphasize two different mechanisms that can be used in SAGG-RIAC, during a reaching attempt:\n1. Conservation of every point reached inside the task space even if they do not correspond to the attempted goal (see section 2.4.2): when the robot performs a reaching attempt toward a goal y, and, instead of reaching it, terminates at another state y\u2032, we consider y\u2032 as a goal reached with a value of competence depending on constraints \u03c1. In cases where no constraints are studied, we can consider the y\u2032 as another goal reached with the highest level of competence.\n2. Addition of subgoals: in robotic setups where the process of goal reaching can be subdivided and described using subgoals which could be fixed on the pathway toward the goal, we artificially add states y1, y2, ..., yn that have to be reached before y while also respecting the constraints \u03c1, and estimate a competence measure for each one.\nThe consideration of these two heuristics has important advantages: first, they can significantly increase the number of estimations of competence, and thus the quantity of feedback returned to the goal self-generation mechanism. This reduces the number of goals that have to be self-generated to bootstrap the system, and thus the number of low-level iteration required to extract first interesting subspaces. Also, by creating areas of different competence values around already reached states, they influence the discovery of reachable areas. Finally, they result in an interesting emergent phenomena: they create a growing area of increasing competence around the first discovered reachable areas. Indeed, by obtaining values of competences inside reachable areas, the algorithm is able to split the space first in these regions, and compute values of interest. These values of interest will typically be high in already reached areas and influence the goal self-generation process to create new goals in its proximity. Once the level of competence becomes important and stabilized in efficiently reached ar-\neas, the interest becomes null, then, new areas of interest close to these ones will be discovered, and so on."}, {"heading": "2.6 PseudoCode", "text": "Pseudo-code 1 and algorithm 2 present the flow of operations in the SAGGRIAC architecture. Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like PI2 \u2212CMA [112], CMA [45], or those presented in [79]. The function Inefficient can also be built in numerous manners and will not be described in details in the pseudo-code (examples will be described then for each experimentation). Its function is to judge if the current model has been efficient enough to reach or come closer to the decided goal, or if the model has to be improved in order to reach it.\nIn the following sections, we will present two different kinds of experiments. The first one is a reaching experiment where a robotic arm has to learn its inverse kinematics to reach self-generated end-effector positions. It uses an evolving context s \u2208 S, also called setpoint in SSA, representing its current joint configuration. Therefore, it can be described by the relationship (s, a)\u2192 y where s, a and y can evolve. It is thus possible to use a goal-directed optimization algorithm very similar to SSA in this experiment, like the one in algorithm 3.\nIn the two other experiments, in contrast, we control the robots using parameterized motor synergies and consider a fixed context (a rest position) s \u2208 S where the robot is reset before each action: we will first consider a quadruped learning omnidirectional locomotion, and then an arm controlling a flexible fishing rod learning to put the float in precise self-generated positions on top of the water. Thus, these systems can be described by the relationship (s, \u03c0\u03b8) \u2192 y, where s will here be fixed and \u03b8 will be the parameters of the motor synergy used to control the robots. Thus, a variation of setpoint being prevented here, a variant of SSA will be proposed for such experiments (similar to a more traditional optimization algorithm), where the context will not evolve and always be reset, like in algorithm 4."}, {"heading": "3 Experimental Setup 1: Learning Inverse Kine-", "text": "matics with a Redundant Arm\nIn this section, we propose an experiment carried out with a robotic arm which has to explore and learn its forward and inverse kinematics. Also, before discussing the details of our active exploration approach in this first experimentation case, we firstly define the representations of the models and control paradigms involved in this experiment. Here, we focus on robotic systems whose actuators are settable by positions and velocities, and restrict our analysis to discrete time models.\nAllowing robots to be self-adaptive to environmental conditions and changes in their own geometry is an important challenge of machine learning. These\nchanges in the robot geometry directly have an impact on its Inverse Kinematics IK, relating workspace coordinates (where tasks are usually specified), to actuators coordinates (like joint position, velocity, or torque used to command the robot). Learning inverse kinematics is useful in numerous machine learning cases, such as when no accurate kinematic model of a robot is available or when an online calibration is needed due to sensor or motor imprecision. Moreover, in developmental robotics studies, the a priori knowledge of a precise model of the body is often avoided, because of its implausibility from the biological point of view. In the following experiment, we assume that the inverse kinematics of our system is totally unknown, and we are interested in studying how SAGG-RIAC can efficiently guide the discovery and learning of its inverse kinematics."}, {"heading": "3.1 Control Paradigms for Learning Inverse Kinematics", "text": "Let us mathematically formulate forward and inverse kinematics relations. We define the intrinsic coordinates (joint/actuator positions) of a manipulator as the n-dimensional vector S = \u03b1 \u2208 Rn, and the position and orientation of the manipulator\u2019s end-effector as the m-dimensional vector y \u2208 Rm. Relative to this formalization, actions of the robot corresponds to speed commands parameterized by a vector \u03b8 = \u03b1\u0307 \u2208 Rn which controls the instantaneous speed of each of the n joints of the arm. The forward kinematic function of this system is generally written as y = f(\u03b1), and inverse kinematics relationship is defined as \u03b1 = f\u22121(y).\nWhen a redundant manipulator is considered (n > m), or when m = n, solutions to the inverse relationship are generally non-unique [106]. The problem posed to inverse learning algorithms is thus to determine particular solutions to \u03b1 = f\u22121(y), when multiple solutions exists. A typical approach used for solving this problem considers local methods, which learn relationships linking small changes \u2206\u03b1 and \u2206y :\ny\u0307 = J(\u03b1)\u03b1\u0307 (4)\nwhere J(\u03b1) is the Jacobian matrix. Then, using the Jacobian matrix and inverting it to get a single solution \u03b1\u0307 corresponding to a desired y\u0307 raises the problem of the non-convexity property of this last equation. A solution to this non-convex problem has then been proposed by Bullock in [23] who converted it into a convex problem, by only\nconsidering the learning task within the spatial vicinity \u0302\u0307\u03b1 of a particular \u03b1 : y\u0307 = J(\u03b1)\u0302\u0307\u03b1 (5)"}, {"heading": "3.2 Representation of Forward and Inverse Models to be Learnt", "text": "We use here non-parametric models which typically determine local models in the vicinity of a current datapoint. By computing a model using parameterized\nfunctions on datapoints restrained to a locality, they have been proposed as useful for real time queries, and incremental learning. Learning inverse kinematics typically deals with these kind of constraints, and these local methods have thus been proposed as an efficient approach to IK learning [125, 107]. In the following study, we use an incremental version of the Approximate Nearest Neighbors algorithm (ANN) [70], based on a tree split using the k-means process, to determine the vicinity of the current \u03b1. Also, in the environments that we use to introduce our contribution, we do not need highly robust, and computationally very complex regression methods. Using the pseudo-inverse of Moore-Penrose [2] to compute the pseudo-inverse J+(\u03b1) of the Jacobian J(\u03b1) in\na vicinity \u0302\u0307\u03b1 is thus sufficient. Possible problems happening due to singularities [106, 28, 91] being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).\nAlso, in the following equation, we use this method to deduce the change \u2206\u03b1 corresponding to a \u2206x, for a given joint position \u03b1:\n\u03b1\u0307 = J+(\u03b1)y\u0307 (6)"}, {"heading": "3.3 Robotic Setup", "text": "In the following experiments, we consider a n-dimensional manipulator controlled in position and speed (as many of today\u2019s robots), updated at discrete time values. The vector \u03b1 \u2208 Rn which represents joint angles corresponds to the context/state space S and the vector y \u2208 Rm which is the position of the manipulator\u2019s end-effector in m dimensions in the Euclidian space Rm corresponds to the task space Y (see Fig. 3 where n = 7 and m = 2). We evaluate how\nthe SAGG-RIAC architecture can be used by a robot to learn how to reach all reachable points in the environment Y with this arm\u2019s end-effector. Learning the inverse kinematics is here an online process that arises each time a micro-action \u03b8 = \u2206\u03b1 \u2208 A is executed by the manipulator: by doing each micro-action, the robot stores measures (\u03b1,\u2206\u03b1,\u2206x) in its memory and creates a database Data which contains elements (\u03b1i,\u2206\u03b1i,\u2206yi) representing the discovered change \u2206yi corresponding to a given \u2206\u03b1i in the configuration \u03b1i (this learning entity can be called a schema according to the terminology of Drescher [38]). These measures are then reused online to compute the Jacobian J(\u03b1) = \u2206y/\u2206\u03b1 locally to move the end-effector in a desired direction \u2206ydesired fixed toward the selfgenerated goal. Therefore, we consider a learning problem of 2n dimensions, the relationship that the system has to learn being (\u03b1,\u2206\u03b1) \u2192 \u2206y. Also, in this experiment, where we suppose Y Euclidian, and do not consider obstacles, the direction to a goal can be defined as following a straight line between the current end-effector\u2019s position and the goal."}, {"heading": "3.4 Evaluation of Competence", "text": "In this experiment, in order to clearly illustrate the main contribution of our algorithm, we do not consider constraints \u03c1 and only focus on the reaching of goal positions yg. It is nevertheless important to notice that a constraint \u03c1 has a direct influence on the low-level of active learning of SAGG-RIAC, and thus an indirect influence on the higher level. As using a constraint can require a more complex exploration process guided at the low-level, a more important number of iterations at this level can be required to reach a goal, which could have an influence on the global evolution of the performances of the learning process used by the higher-level of SAGG-RIAC.\nWe define here the competence function C with the Euclidian distance D(yg, yf ), between the goal position and the final reached position yf , which is normalized by the starting distance D(ystart, yg), where ystart is the endeffector\u2019s starting position. This allows, for instance, to give a same competence level when considering a goal at 1cm from the origin position, which the robot approaches at 0.5cm and a goal at 1mm, which the robot approaches at 0.5mm.\nC(yg, yf , ystart) = \u2212 D(yg, yf )\nD(ystart, yg) (7)\nwhere C(yg, yf , ystart) = 0 if D(ystart, yg) < \u03b5C (the goal is too close from the start position) and C(yg, yf , ystart) = \u22121 if D(yg, yf ) > D(ystart, yg) (the end-effector moved away from the goal)."}, {"heading": "3.5 Addition of subgoals", "text": "Computing local competence progress in subspaces/regions typically requires the reaching of numerous goals. Because reaching a goal can necessitate several micro-actions, and thus time, obtaining competence measures can be long. Also,\nwithout biasing the learning process and as already explained in section 2.5, we improve this mechanism by taking advantage of the Euclidian nature of Y : we increase the number of goals artificially, by adding subgoals on the pathway between the starting position and the goal, where competences are computed. Therefore, considering a starting state ystart in Y , and a self-generated goal yg, we define the set of l subgoals {y1, y2, ..., yl} where yi = (i/l) \u00d7 (yg \u2212 ystart), that have to be reached before attempting to reach the terminal goal yg.\nWe also consider another way to increase the number of competence measures which is to take into consideration each experimented position of the end-effector as a goal reached with a maximal competence value. This will typically help the system to distinguish which regions are efficiently covered, and to discover new regions of interest."}, {"heading": "3.6 Active Goal Directed Exploration and Learning", "text": "Here we propose a method inspired by the SSA algorithm to guide the system to learn on the pathway toward the selected goal position yg. This instantiation of the SAGG-RIAC architecture uses algorithm 3 and considers evolving contexts, as explained below."}, {"heading": "3.6.1 Reaching Phase", "text": "The reaching phase deals with creating a pathway to the current goal position yg. This phase consists of determining, from the current position yc, an optimal micro-action which would guide the end-effector toward yg. For this purpose, the system computes the needed end-effector\u2019s displacement \u2206ynext = v. yc\u2212yg \u2016yc\u2212yg\u2016 (where v is the velocity bounded by vmax and yc\u2212yg \u2016yc\u2212yg\u2016 a normalized vector in direction of the goal), and performs the action \u2206\u03b1next = J +.\u2206ynext, with J\n+, pseudo-inverse of the Jacobian estimated in the close vicinity of \u03b1 and given the data collected by the robot so far. After each action \u2206ynext, we compute the error \u03b5 = \u2016\u2206\u0303ynext \u2212 \u2206ynext\u2016, and trigger the exploration phase in cases of a too high value \u03b5 > \u03b5max > 0. \u03b5max is thus a parameter which has to be set depending on the range of error \u03b5 that can be experienced, and will be set depending on a tolerance that can be conceded to allow reaching goal positions with the current learned data. While a too high value of \u03b5max will prevent exploring and learning new data (the system spending potentially too important amounts of time exploring around a same configuration and get trapped in local minima), too low values of \u03b5max will prevent an efficient local optimization."}, {"heading": "3.6.2 Exploration Phase", "text": "This phase consists in performing q \u2208 N small random explorative actions \u2206\u03b1i, around the current position \u03b1, where the variations can be derandomized such as in [45]. This allows the learning system to improve its regression model of the relationship (\u03b1,\u2206\u03b1) \u2192 \u2206y, in the close vicinity of \u03b1, which is needed to compute the inverse kinematics model around \u03b1. During both phases, a counter\nis incremented for each micro-action and reset for each new goal. The timeout used to define a goal as unreached and to stop a reaching attempt uses this counter. A maximal quantity of micro-actions is fixed for each goal as directly proportional to the number of micro-action it requires to be reached. In the next experiments, the system is allowed to perform up to 1.5 times the distance between ystart and yg before stopping the reaching attempt."}, {"heading": "3.7 Qualitative Results for a 15 DOF Simulated Arm", "text": "In the simulated experiment introduced in this section, we consider the robotic arm presented Fig. 3 with 15 DOF, each limb of the robot having the same length (considering a 15 DOF arm corresponds to a problem of 32 continuous dimensions, with 30 dimensions in the actuator/state space and 2 dimensions in the goal/task space). We set the dimensions of the task space Y as bounded in intervals yg \u2208 [0; 150]\u00d7 [\u2212150; 150], where 50 units is the total length of the arm, which means that the arm covers less than 1/9 of the space Y where goals can be chosen (i.e. the majority of areas in the operational/task space are not reachable, which has to be self-discovered by the robot). We fix the number of subgoal per goal to 5, and the maximal number of elements inside a region before a split to gmax = 50. We also set the desired velocity v = 2 units/micro-action, and the number of explorative actions q = 20. Moreover, we reset the arm to the rest position (\u03b1rest, yrest) (position displayed in Fig. 3) every r = 1 reaching attempts. This allows reducing the initiation set and prevent the system from experimenting with too complex joint positions where the arm is folded, and where the Jacobian is more difficult to compute. Using a low value of r is an important characteristic for the beginning of the learning process. A too high value of r prevents learning rapidly how to achieve a maximal amount of goal position, due to the difficulty to reuse the previously learned data when the arm is folded in unknown positions.\nThe bent character of the rest position is also useful to avoid to begin a micro-action close to a singularity like when the arm is totally unfolded. Also, in this experiment, we consider each experimented position of the end-effector as if it was a goal reached with the maximal competence level (these numerous positions are not displayed in the following figures in order to not overload the illustrations)."}, {"heading": "3.7.1 Evolution of Competences over Time", "text": "Fig. 4 represents the whole distribution of self-generated goals and sub-goals selected by the higher-level of active learning module, and their corresponding competences after the execution of 30000 micro-actions. The global shape of the distribution of points allows observing the large values of competence levels inside the reachable space and its close vicinity, and the global low competence inside the remaining space.\nThe progressive increase of competences is displayed on Fig. 5 where we evaluate over time (indexed here by the number of goals self-generated) the\nglobal competence of the system to reach positions situated on a grid which covers the entire task space. From these estimations of competence, we can extract two interesting phenomena: first of all, the two first subfigures, estimated after the self-generation of 42 and 83 goals, show that the system is, at the beginning of the exploration and learning process, competent to only attain areas situated close to the limits of the reachable space. Then, the 4 other subfigures show the progressive increase of competences inside the reachable space following an increasing radius whose the origin is situated around the end-effector rest position.\nThe first observation is due to the reaching mechanism in itself, which, when possessing a few data acquired, does not allow the robot to experiment complex joint movements, but only simple ones which typically leads to the limits of the arm. The second phenomenon is due to the coupling of the lower-level of active learning inspired by SSA with the heuristic of returning to yrest every subsequent goals. Indeed, the necessity to be confident in the local model of\nthe arm to shift toward new positions makes the system progressively explore the space, and resetting it to its rest position makes it progressively explore the space by beginning close to yrest. Finally, goal positions that are physically reachable but far from this radius typically present a low competence to be reached initially, before the radius spreads enough to reach them."}, {"heading": "3.7.2 Global Exploration over Time", "text": "Fig. 6 shows histograms of goal positions self-generated during the execution of the 30000 micro-actions (only goals, not subgoals for an easy reading of the figure). Each subfigure corresponds to a specified time window indexed by the number of generated goals: the first one (upper-left) shows that, at the onset of learning, the system already focuses in a small area around the end-effector\u2019s rest position, and thus discriminates differences between a subpart of the reachable area and the remaining space (the whole reachable zone being represented by the black half-circle on each subfigure of Fig. 6). In the second subfigure, the system is, inversely, focusing almost only on regions of the space which are not reachable by the arm. This is due to the imprecise split of the space at this level of the exploration, which left very small reachable areas (which have already been reached with a high competence), at the edge inside each large unreachable regions. This typically gives a high mean competence to each of these region when they are created. Then, due to the very large part of unreachable\nareas, in comparison to reachable ones, the mean competence decreases over time. This brings interest to the region, thanks to the mathematical definition of the interest level, which, by using an absolute value, pushes the robot toward areas where the competence is decreasing. This complex process which allows driving the exploration in these kind of heterogeneous regions then allows dividing efficiently the task space into reachable and unreachable regions.\nThen, considering a global observation of subfigures 3 to 6, we can conclude that the system effectively autonomously discovers its own limits by focusing the goal self-generation inside reachable areas during the largest part of the exploration period. The system is indeed discovering that only a subpart is reachable due to the interest value becoming null in totally unreachable areas where the competence value is low."}, {"heading": "3.7.3 Exploration over Time inside Reachable Areas", "text": "A more precise observation of subfigures 3 to 6 is presented in Fig. 8 where we can specifically observe the self-generated goals inside the reachable area. First, we can perceive that the system is originally focusing in an area around the end-effector\u2019s rest position yrest (shown by gray points in Fig. 8).\nThen, it increases the radius of its exploration around yrest and focuses on areas further afield to the end-effector\u2019s rest position. Subfigures 2 and 3 shows\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\n1\n150\n-15 0 150\n42 Goals 83 Goals 125 Goals\n166 Goals 208 Goals 249 Goals 150 150\n150\n-15 150 150 0 150\n0\n0\nFigure 7: Evolution of the splitting of the task/goal space and creation of subregions indexed by the number of goals self-generated (without counting subgoals), for the experiment presented in Fig. 6.\n167-208 Goals 208-249 Goals125-166 Goals84-125 Goals -50\n50\n0\n-50\n50\n0\n-50\n50\n0\n-50\n50\n0\nxrest\nFigure 8: Details of the evolution of the distribution of self-generated goals inside the reachable area for the experiment presented in Fig. 6. Gray points represent the end-effector rest position yrest.\nthat the system explores new reachable parts corresponding to the right part close to its basis (subfigure 2), and then, the left part close to its basis (subfigures 3).\nAlso, comparing the two first subfigures, and the two last ones, we observe a shift of the maximum exploration peak toward the arm basis. This is first linked with the loss of interest of self-generating goals around the end-effector\u2019s rest position. Indeed, because the system becomes highly efficient inside this region, the competence level becomes high and stationary over time, which leads to low interest values. At the same time, this phenomenon is also linked with the increase of competences in new reachable positions far from the end-effector rest position yrest, closer to its basis, which creates new regions of interest (see the four last subfigures of Fig. 5)."}, {"heading": "3.7.4 Emergent Process", "text": "The addition of subgoals and the consideration of each end-effector\u2019s position as a goal reached with the highest competence level have important influences on the learning process. If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest. Thanks to the addition of sub-goals and/or the consideration of every endeffector\u2019s position in SAGG-RIAC, in addition to exploring in the task space, we reduce the number of needed random global exploration, and improve the capability of the system to deal with large (i.e. when the volume of reachable space is small as compared to the volume of the whole space) task spaces. Using subgoals indeed creates a concentration of goals around the current end-effector\u2019s position, which progressively grows according to new experimented positions.\nFurthermore, the consideration of each end-effector\u2019s position for the estimation of competence allows discovering progressively which positions are reachable with a high competence level, and gives a fast indication of first subregions where these high competences are situated. This increases the number of subregions close to the reachable areas and allows computing the interest values in the growing vicinity of the end-effector\u2019s experimented positions (see Fig. 7 where the progressive split of subregions in reachable areas is displayed).\nTherefore, these additions of competence measures allow the system to discover and focus on areas where the competence is high in a very low number of goal self-generation, and tackle the typical problem of fast estimation and distinction of interesting areas. Nevertheless, this emergent process only helps to increase the number of feedbacks required by the goal self-generation mechanism to split the space, and do not influence the low-level active learning. Then, the timeout which defines a goal as unreached during a single reaching attempt becomes crucial when considering high-volume task spaces with large unreachable parts as introduced in the following section."}, {"heading": "3.7.5 Robustness in High-Volume Task Spaces", "text": "in the previous experiment, the timeout which describes a goal as not reached and stops a reaching attempt is defined as directly proportional to the number of micro-actions required to reach each goal. Practically, as introduced section 3.6.2, we allowed the system to perform 1.5 times the distance between ystart and yg before declaring a goal as not reached (including explorative movements).\nThis timeout is efficient enough to learn efficiently by discriminating regions of different complexities in the middle-size space S\u2032 = [0; 150] \u00d7 [\u2212150; 150] considered in this experiment. Nevertheless, it can have an important influence on the SAGG learning process when considering extremely large task spaces\n1 to 336 Goals\n200 400 600\n100 200 300\n400 500 600\n337 to 672 Goals\n200 400 600\n100 200 300\n400 500 600\n673 to 1008 Goals\n200 400 600\n100 200\n300 400\n500 600\n1009 to 1344 Goals\n200 400 600\n100 200\n300 400\n500 600\n1345 to 1680 Goals\n200 400 600\n100 200\n300 400\n500 600\n1680 to 2015 Goals\n200 400 600\n100 200\n300 400\n500 600\n1 to 336 Goals\n200 400 600\n100 200 300\n400 500 600\n337 to 672 Goals\n200 400 600\n100 200 300\n400 500 600\n673 to 1008 Goals\n200 400 600\n100 200\n300 400\n500 600\n1009 to 1344 Goals\n200 400 600\n100 200\n300 400\n500 600\n1345 to 1680 Goals\n200 400 600\n100 200\n300 400\n500 600\n1680 to 2015 Goals\n200 400 600\n100 200\n300 400\n500 600\n1 to 336 Goals\n200 400 600\n100 200 300\n400 500 600\n337 to 672 Goals\n200 400 600\n100 200 300\n400 500 600\n673 to 1008 Goals\n200 400 600\n100 200\n300 400\n500 600\n1009 to 1344 Goals\n200 400 600\n100 200\n300 400\n500 600\n1345 to 1680 Goals\n200 400 600\n100 200\n300 400\n500 600\n1680 to 2015 Goals\n200 400 600\n100 200\n300 400\n500 600\n1 to 336 Goals\n200 400 600\n100 200 300\n400 500 600\n337 to 672 Goals\n200 400 600\n100 200 300\n400 500 600\n673 to 1008 Goals\n200 400 600\n100 200\n300 400\n500 600\n1009 to 1344 Goals\n200 400 600\n100 200\n300 400\n500 600\n1345 to 1680 Goals\n200 400 600\n100 200\n300 400\n500 600\n1680 to 2015 Goals\n200 400 600\n100 200\n300 400\n500 600\n1 to 336 Goals\n200 400 600\n100 200 300\n400 500 600\n337 to 672 Goals\n200 400 600\n100 200 300\n400 500 600\n673 to 1008 Goals\n200 400 600\n100 200\n300 400\n500 600\n1009 to 1344 Goals\n200 400 600\n100 200\n300 400\n500 600\n1345 to 1680 Goals\n200 400 600\n100 200\n300 400\n500 600\n1680 to 2015 Goals\n200 400 600\n100 200\n300 400\n500 600\n1 to 336 Goals\n200 400 600\n100 200 300\n400 500 600\n337 to 672 Goals\n200 400 600\n100 200 300\n400 500 600\n673 to 1008 Goals\n200 400 600\n100 200\n300 400\n500 600\n1009 to 1344 Goals\n200 400 600\n100 200\n300 400\n500 600\n1345 to 1680 Goals\n200 400 600\n100 200\n300 400\n500 600\n1680 to 2015 Goals\n200 400 600\n100 200\n300 400\n500 600\n1 to 336 Goals 337 to 672 Goals 673 to 1 08 Goals\n1009 to 1 34 Goals 345 to 680 Goals 1681 to 2 5 Goal\n-50\n500\n0 500\n-500\n500\n0\n0 500 0 500 0 500\n0 50 0 50\nFigure 9: Histograms of self-generated goals displayed over time windows indexed by the number of performed goals, for an experiment of 30000 microactions on a 15 DOF arm, for a high-volume task space S = [\u2212500; 500]\u00d7[0; 500], according to the reachable space contained in [\u221250; 50]\u00d7 [0; 50] (the black halfcircle represents the contour of the area reachable by the arm according to its length of 50 units).\nwith small underlying reachable areas. For instance, if we consider a task space Y = [\u2212500; 500]\u00d7 [\u22120; 500] where only [\u221250; 50]\u00d7 [0; 50] is reachable, the lowlevel of active learning will spend an extremely large number of iterations trying to reach each unreachable goal if this kind of timeout is used.\nTherefore, when considering such high-volume spaces, the definition of a new timeout becomes crucial. In Fig. 9, we demonstrate the high discriminating factor of SAGG-RIAC in such a task space (Y = [\u2212500; 500]\u00d7 [\u22120; 500]) when using a timeout which is not only based on the distance to the goal. This one has also been designed to stop a reaching attempt according to the following blocking criteria: let us consider a self-generated goal yg that the low-level exploration and reaching mechanisms try to reach. Then, if the system is not coming closer to the goal even after some low-level explorations, the exploration toward this precise goal stops. In a practical way, when w consecutive low-level explorations are triggered (typically w \u2265 2) and thus no progress to the goal was made, we declare a goal as unreached, and compute the corresponding competence level. Using such a definition, the rapidity of discovering blocking situations will depend on both values of w and number of explorative actions q. Minimal values of these two parameters allows the fastest discoveries, but decrease the quality of the low-level exploration mechanism when exploring reachable spaces (in the experiment presented in Fig. 9 we use q = 5 and w = 3)."}, {"heading": "3.7.6 Conclusion of Qualitative Results", "text": "When considering low-level mechanisms allowing an efficient progressive learning, the SAGG-RIAC algorithm is capable to discriminate very efficiently reachable areas in such high-volume spaces. Then, it is also able to drive a progressive self-generation of goals through reachable subspaces of progressively growing complexities of reachability.\nIn this experiment, the reachable region in the task space was convex and with no obstacles. Yet, as we will see in the fishing experiment below, SAGGRIAC is capable of identifying correctly its zones of reachability, given a low-level optimization algorithm, even if there are \u201choles\u201d or obstacles: goals initially generated in unreachable positions or in positions for which obstacles prevent their reaching provide a low level of competence progress, and thus the system stops trying to reach them. It is also possible to imagine that some given selfgenerated goals might be reachable only by an action policy going around an obstacle. Such a capability is not a property of the SAGG-RIAC architecture by itself, but a property of the optimization algorithm, and action representation, that is used at the low-level goal-directed mechanism. In the present experiment, low-level optimization was a simple one only considering action policies going in a straight line to the goal. Yet, if one would have used more complex optimization leveraging continuous domain planning techniques (e.g. [121]), the zones of reachability would be increased if obstacles are introduced since the low-level system could learn to go around them."}, {"heading": "3.8 Quantitative Results for Experiments with Task Spaces of Different Sizes", "text": "In the following evaluation, we consider the same robotic system than previously described (15DOF arm of 50 units) and design different experiments. For each one, we estimate the efficiency of the inverse model learned by testing how it allows in average the robot to reach positions selected inside a test database of 100 reachable positions (uniformly distributed in the reachable area and independent from the exploration of the robot). We will also compare SAGG-RIAC to three other types of exploration techniques:\n1. SAGG-RANDOM, where goals are chosen randomly (higher-level of active learning (RIAC) disabled)\n2. ACTUATOR-RANDOM, where small random micro-actions \u2206\u03b1 are executed. This method corresponds to classical random motor babbling.\n3. ACTUATOR-RIAC, which corresponds to the original RIAC algorithm that uses the decrease in prediction errors (\u03b1,\u2206\u03b1) \u2192 \u2206x to compute an interest value and split the space (\u03b1,\u2206\u03b1).\nAlso, to be comparable to SAGG-RIAC, each ACTUATOR technique will have the position of the arm reset to the rest position every max micro-actions, max being the number of micro-actions needed to reach the more distant reachable\nposition. max is proportional to the desired velocity which is here of v = 2 units/micro-action as well as the size of the task space (this will explain the different results of each ACTUATOR methods when used with task spaces of different sizes). In every graph, we present statistical results obtained after launching the same experiment with different random seeds 15 times."}, {"heading": "3.8.1 Exploration in the Reachable Space", "text": "The first quantitative experiment is designed to compare the quality of inverse models learned using babbling in the task/operational space (i.e. using goals), instead of more traditional motor babbling heuristics executed in the configuration/actuator space. We still consider a n=15 DOF arm of 50 units, also, to be suited for the first study, dimensions of Y will be bounded in intervals yg \u2208 [0; 50]\u00d7 [\u221250; 50] which means that the arm can reach almost all the space Y where goals can be chosen (the limits of reachability are thus almost given to the robot). In this experiment, we fix q = 20 for the SAGG methods and use a timeout only relative to the distance to the current goal (a end-effector movement of 1.5 times the one needed is allowed).\nFig. 10 shows the evolution of the capability of the system to reach the 100 test goals using the inverse model learned by each technique, starting from the rest position. This capability is computed using the mean Euclidian distance between the goal and the final state of a reaching attempt.\nGlobally, these results show that in order to learn inverse kinematics of this highly-redundant arm, exploration in the goal/operational space is significantly more efficient than exploration in the actuator space using either random exploration or RIAC-like active learning. Moreover, better performances of ACTUATOR-RANDOM compared to ACTUATOR-RIAC emphasizes that the original version of RIAC has not been designed for the efficient learning of inverse models of highly-redundant systems (high-dimension in the actuator space).\nFocusing on the evaluation of the two mechanisms which use SAGG, we can also make the important observation that SAGG-RIAC is here more efficient than SAGG-RANDOM when considering a system which already knows its own limits of reachability. More precisely, we observe both increase in learning speed and final generalization performances (this results resonates with results from more classic active learning, see [29]). These improvement signifies that SAGGRIAC is efficiently able to progressively discriminate and focus on areas which bring the highest informational amount (i.e. areas which have not been visited enough). It brings to the learning system more useful data to create an efficient inverse model, contrarily to the SAGG-RANDOM approach which continues to select goals in already efficiently reached areas."}, {"heading": "3.8.2 Robustness in Large Task Spaces", "text": "in the following experiment, we would like to test the capability of SAGG-RIAC to focus on reachable areas when facing high volume task spaces (will call this\nx 104\nphenomenon the discrimination capability). Therefore, we will here consider a task space Y = [0; 500] \u00d7 [\u2212500; 500]. Fig. 11 shows the learning efficiency of SAGG-RIAC using the timeout with blocking criteria as described in the section 3.7.5. This allows to test the quantitative aspect of the discrimination capability of SAGG-RIAC and its comparison with the three other techniques when facing high volume task spaces where only small subparts are reachable. As Fig. 11 shows, SAGG-RIAC is here the only method able to drive an efficient learning in such a space. SAGG-RANDOM actually spends the majority of the time trying to reach unreachable positions. Also, the size of the task space has an influence on the two ACTUATOR algorithms if we compare results in Y = [0; 50] \u00d7 [\u221250; 50] introduced Fig. 10 and in Y = [0; 500] \u00d7 [\u2212500; 500] introduced Fig. 11. This is due to the value max of micro-actions performed by ACTUATOR methods which is proportional to the size of the task space as explained section 3.8. Results considering the space Y = [0; 500] \u00d7 [\u2212500; 500] seems more efficient for these methods, where the value of max is higher than in Y = [0; 50] \u00d7 [\u221250; 50]. An increase of max thus allows these methods to\n25\nexplore more efficiently the reachable space whose exploration is limited when considering a too low value of max."}, {"heading": "3.8.3 Robustness in Very Large Task Spaces", "text": "Finally, we test the robustness of SAGG-RIAC in task spaces larger than in the previous section. Fig. 12 shows the behavior of SAGG-RIAC when used with task spaces of different sizes, from 1 to 900 times the size of the reachable space, and compare these results with a random exploration in the actuator space when the value of max is fixed as when Y = [0; 500] \u00d7 [\u2212500; 500]. We can notice here that, although the high discriminative capacity of SAGG-RIAC in large spaces such as Y = [0; 500] \u00d7 [\u2212500; 500], as shown previously, the performances of this technique decrease when the size of the considered task space increases. Therefore, we can observe that SAGG-RIAC obtains better results than ACTUATOR-RANDOM since 5000 micro-actions when considering spaces smaller than Y = [0; 500] \u00d7 [\u2212500; 500]. Then, this method shows bet-\nter results than ACTUATOR-RANDOM only after 10000 micro-actions when considering the space Y = [0; 500]\u00d7 [\u2212500; 500]. And finally, this one becomes less efficient than ACTUATOR-RANDOM when the considered space increases in comparison to the reachable space, as shown by results when considering spaces Y = [0; 1000] \u00d7 [\u22121000; 1000] and Y = [0; 1500] \u00d7 [\u22121500; 1500]. These results clearly show that SAGG-RIAC is robust in spaces up to 100 times larger than the reachable space, but has some difficulties to explore even larger spaces. Therefore, despite the fact that SAGG-RIAC is very efficient in large spaces, it seems that the challenge of autonomous exploration in un-prepared spaces can not be totally resolved by this algorithm, a human supervisor being still necessary to define a set of (even very approximate) limits for the task space. As it will be emphasized in the perspective of this work, some complementary techniques should be used in order to bring robustness to such spaces, such as mechanisms inspired by the notion of maturational constraints which are able to fix limits on the task space since the beginning of the exploration process."}, {"heading": "3.9 Quantitative Results for Experiments with Arm of Different Number of DOF and Geometries", "text": "In every experiment, we set the dimensions of Y as bounded by the intervals yg \u2208 [0; 150]\u00d7 [\u2212150; 150], where 50 units is the total length of the arm, which means that the arm covers less than 1/9 of the space Y where goals can be chosen (i.e. the majority of areas in the operational/task space are not reachable, which has to be discovered by the robot).\nFor each experiment, we set the desired velocity v = 0.5 units/micro-action, and the number of explorative actions q = 20. Moreover, we reset the arm to the rest position (\u03b1rest, yrest) every r = 2 reaching attempts, which increases the complexity of the reaching process.\nWe present a series of experiments aiming to test the robustness of SAGGRIAC in arm setups with different shapes and numbers of degrees-of-freedom. Performed tests used 7, 15, and 30 DOF arms whose each limb has either the same length or a decreasing length depending on its distance from the arm\u2019s base (we use the golden number to specify the relative size of each part, taking inspiration from the architecture of human limbs). These experiments permit testing the efficiency of the algorithm for highly redundant systems (considering a 30 DOF arm corresponds to a problem of 62 continuous dimensions, with 60\ndimensions in the actuator/state space and 2 dimensions in the goal/task space), and different morphologies.\nAlso, to stress the capability of the system to make the robot self-discover its own limits, we remove the consideration of each end-effector position experimented as a goal reached with the highest level of competence (see 3.7.4). In these experiments, the competence level is therefore evaluated only for goals and subgoals. We fix q = 100, and compute tests of inverse models over 200000 micro-actions."}, {"heading": "3.9.1 Quantitative Results", "text": "Fig. 13 illustrates the performances of the learned inverse models when used to reach goals from an independent test database and evolving along with the number of experimented micro-actions. First, we can globally observe the slower decreasing velocity (over the number of micro-actions) of SAGG-RANDOM and SAGG-RIAC, compared to the previous experiment, which is due to the higher value of q and the removed consideration of every end-effector position. Graphs on the first line of Fig. 13 present the reaching errors of 7, 15 and 30 DOF arms with decreasing lengths. The first subfigure shows that when considering 7 DOF, which is a relatively low number of degrees of freedom, SAGGRANDOM is not the second more efficient algorithm. Indeed, the ACTUATORRANDOM method is here more efficient than SAGG-RANDOM after 25000 micro-actions and is then stabilized, while SAGG-RANDOM is progressively decreasing, reaching the same level as ACTUATOR-RANDOM at the end of the experiment. This is due to the high focalization of SAGG-RANDOM outside the reachable area, which leads to numerous explorations toward unreachable positions. As shown also in this subfigure, adding the RIAC active component to SAGG efficiently improves the learning capabilities of the system; SAGG-RIAC reaching errors were indeed the lowest for this 7 DOF system.\nExperiments with 15 DOF and 30 DOF shows that both SAGG methods are here more efficient than actuator methods, SAGG-RIAC showing a significant improvement compared to every other algorithm (for 15DOF, the level of significance is p = 0.002 at the end of the experiment (200000 micro-actions)).\nExperiments presented with 7, 15 and 30 DOF arms where each limb has the same length show the same kind of results. The 7 DOF experiment shows that ACTUATOR-RANDOM can be more efficient than SAGG-RANDOM, and that the addition of RIAC allows obtaining a significant improvement in this case, but also when considering 15 and 30 DOF."}, {"heading": "3.9.2 Conclusion of Quantitative Results", "text": "Globally, quantitative results presented here emphasize the high efficiency and robustness of SAGG-RIAC when carried out with highly redundant robotic setups of different morphologies, compared to more traditional approaches which explore in the actuator (input) space. They also showed that random exploration in the goal (output) space can be very efficient when used in high-"}, {"heading": "1 to 29 Goals", "text": ""}, {"heading": "57 to 85 Goals", "text": ""}, {"heading": "113 to 141 Goals", "text": ""}, {"heading": "1 to 29 Goals", "text": ""}, {"heading": "57 to 85 Goals", "text": ""}, {"heading": "113 to 14 G als", "text": "dimensional systems, even when considering a task space more than 9 times larger than the reachable subspace. These results therefore indicate the high potential of competence based motor learning for IK learning in highlyredundant robots."}, {"heading": "3.10 Qualitative Results for a Real 8 DOF Arm", "text": "In this section, we test the robustness of the algorithm in a qualitative point of view when considering a real robotic setup (not simulated) which corresponds to the simulation presented above: we use a 8 DOF arm controlled in position. Also, helping to test the robustness of our method, we use low quality motors whose averaged noise is 20% for each movement. The fixed task space corresponds to the whole surface observable by a camera fixed on top of the robot, which is more than three times larger than the reachable space (see the left part of Fig. 14). In order to allow the camera to distinguish the end-effector of the arm and to create a visual referent framework on the 2D surface, we used visual tags and the software ARToolKit Tracker [81].\nFig. 14 (right part) shows histograms of self-generated goals displayed over sliding time windows indexed by the number of performed goals (without counting subgoals) for an experiment of 10000 micro-actions. We can observe that the algorithm manages to discover the limits of the reachable area and drives the exploration inside after the goal 57. Then, the system continues to focus on the reachable space until the end of the experimentation, alternating between different areas inside. More precisely, we can notice while comparing the bottom-left subfigure to the two positioned on the second line, that the system seems to concentrate only after some time on the areas situated close to its basis, and therefore more difficult to reach. The progressive increase of the complexity of\npositions explored which appeared in simulation therefore also happens here. Finally, the last subfigure shows that the system continues its exploration toward an area more central of the reachable part. This is due to the high level of noise of the motor control: while the system is originally not very robust in this part of the space, an improvement of the generalization capacity of the learning algorithm allows obtaining an increase of competences in already visited areas of the task space.\nThis experiment shows the efficiency of the SAGG-RIAC architecture to drive the learning process in real noisy robotic setups with only a few iterations, as well as its capacity to still control the complexity of the exploration when considering highly-redundant systems."}, {"heading": "4 Experimental Setup 2: Learning Omnidirec-", "text": "tional Quadruped Locomotion with Motor Synergies\nSometimes stemming from pre-wired neuronal structures (e.g. central pattern generators [44, 73, 49]), motor synergies are defined as the coherent activations (in space or time) of a group of muscles. They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120]. Described as crucial for the development of motor abilities, they can be seen as encoding an unconscious continuous control of muscles which simplifies the complexity of the learning process: learning complex tasks using parameterized motor synergies (such as walking, or swimming) indeed corresponds to the tuning of relatively low-dimensional (but yet which can have a few dozen dimensions) high-level control parameters, compared to the important number of degrees of freedom which have to be controlled (thousand in the human body, see [16])."}, {"heading": "4.1 Formalization", "text": "In the two following experiments, we simplify the learning process by using such parameterized motor synergies controlling amplitude, phase, and velocity of Central Pattern Generators (CPGs). Mathematically, using motor synergies simplifies the description of the considered robotic system. In the framework introduced above (section 2.2) we defined our system as being represented by the relationship (s, a)\u2192 y, where for a given configuration s \u2208 S, a sequence of actions a = {a1, a2, ..., an} \u2208 A allows a transition toward y \u2208 Y .\nIn the current framework we consider the sequence of actions as being generated directly by parameterized motor synergies \u03c0\u03b8, which means that the sequence of actions is directly encoded and controlled (using feedbacks internal to the synergy) by setting parameters \u03b8 specified at the beginning of an action. For instance, in the experiment described in this section, we define a synergy as\n11\na set of parameterized sinusoids (one on each joint) that a motor joint has to\ntrack with a low-level pre-programmed PID-like controller. Eventually, motor\nsynergies can be seen as a way to encapsulate the low-level generation of sequences of micro-actions, allowing the system to directly focus on the learning of models (s, \u03c0\u03b8)\u2192 y, with s \u2208 S fixed (the rest position of the robot) and \u03b8 a set of parameters controlling the synergy (we will remove the fixed context s in the next notations for a easier reading and only write \u03c0\u03b8 \u2192 y)."}, {"heading": "4.2 Robotic Setup", "text": "In the following experiment, we consider a quadruped robot simulated using the Breve simulator [54] (physics simulation is based on ODE). Each of its leg is composed of 2 joints, the first (closest to the robot\u2019s body) is controlled by two rotational DOF, and the second, one rotation (1 DOF). Each leg therefore consists of 3 DOF, the robot having in its totality 12 DOF (See Fig. 15).\nThis robot is controlled using motor synergies pi\u03b8 whose parameters \u03b8 \u2208 Rn directly specify the phase and amplitude of each sinusoid which controls the precise rotational value of each DOF over time. These synergies are parameterized using a set of 24 continuous values, 12 representing the phase ph of each joint, and the 12 others, the amplitude am; \u03b8 = {ph1,2,..,12; am1,2,..,12}, where each joint i receives the command am\u00d7 sin(\u03c9t+ph), with \u03c9 a fixed frequency. Each experimentation consists of launching a motor synergy \u03c0\u03b8 for a fixed amount of time, starting from a fixed position. After this time period, the resulting position yf of the robot is extracted into 3 dimensions: its position (u, v), and\nits rotation \u03c6. The correspondence \u03b8 \u2192 (u, v, \u03c6) is then kept in memory as a learning exemplar.\nThe three dimensions u, v, \u03c6 are used to define the task space of the robot. Also, it is important to notice that precise areas reachable by the quadruped using these motor synergies cannot be estimated beforehand. In the following, we set the original dimensions of the task space to [\u221245; 45]\u00d7[\u221245; 45]\u00d7[\u22122\u03c0; 2\u03c0] on axis (u, v, \u03c6), which was a priori larger than the reachable space. Then, after having carried out numerous experimentations, it appeared that this task space was actually more than 25 times the size of the area accessible by the robot (see red contours in Fig. 16).\nThe implementation of our algorithm in such a robotic setup aims to test if the SAGG-RIAC driving method allows the robot to learn efficiently and accurately to attain a maximal amount of reachable positions, avoiding the selection of many goals inside regions which are unreachable, or that have previously been visited."}, {"heading": "4.3 Measure of competence", "text": "In this experiment, we do not consider constraints \u03c1 and only focus on reaching of the goal positions yg = (ug, vg, \u03c6g). In every iteration the robot is reset to a same configuration called the origin position (see Fig. 17). We define the competence function C using the Euclidian distance goal/robot\u2019s position D(yg, yf ) after a reaching attempt, which is normalized by the original distance between the origin position yorigin, and the goal D(yorigin, yg) (See Fig. 17).\nIn this measure of competence, we compute the Euclidian distance using (u, v, \u03c6) where dimensions are rescaled in [0; 1]. Each dimension therefore has the same weight in the estimation of competence (an angle error of \u03c6 = 12\u03c0 is as important as an error u = 190 or v = 1 90 ).\nC(yg, yf , ystart) = \u2212 D(yg, yf )\nD(ystart, yg) (8)\nwhere C(yg, yf , ystart) = 0 if D(ystart, yg) = 0."}, {"heading": "4.4 Active Goal Directed Exploration and Learning", "text": "Reaching a goal yg necessitates the estimation of a motor synergy \u03c0\u03b8i leading to this chosen state yg. Considering a single starting configuration for each experimentation, and motor synergies \u03c0\u03b8, the forward model which defines this system can be written as the following:\n\u03b8 \u2192 (u, v, \u03c6) (9)\nHere, we have a direct relationship which only considers the 24 dimensional parameter vector \u03b8 = {ph1,2,..,12; am1,2,..,12} of the synergy as inputs of the system, and a position in (u, v, \u03c6) as output. We thus have a fixed context and use\nhere an instantiation of the SAGG-RIAC architecture with local optimization algorithm Alg. 4, detailed below."}, {"heading": "4.4.1 Reaching Phase", "text": "The reaching phase deals with reusing the data already acquired and use local regression to compute an inverse model ((u, v, \u03c6) \u2192 \u03b8)L in the locality L of the intended goal yg = (ug, vg, \u03c6g). In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method:\nWe first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]:\nL = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, ..., {u, v, \u03c6, \u03b8}l} (10)\nThen, we consider the set M which contains l sets of m elements:\nM =  M1 : {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, ..., {u, v, \u03c6, \u03b8}m}1 M2 : {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, ..., {u, v, \u03c6, \u03b8}m}2\n... Ml : {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, ..., {u, v, \u03c6, \u03b8}m}l  (11) where each set {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, ..., {u, v, \u03c6, \u03b8}m}i corresponds to the m nearest neighbors of each \u03b8i, i \u2208 L, and their corresponding resulting position (u, v, \u03c6).\nFor each set {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, ..., {u, v, \u03c6, \u03b8}m}i, we estimate the standard deviation \u03c3 of the parameters of their motor synergies \u03b8 :\n\u03c3(Mj) = \u03c3 (\u03b8j \u2208 {{u, v, \u03c6, \u03b8}1,...,m}) (12)\nFinally, we select the set Mk = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, ..., {u, v, \u03c6, \u03b8}m} inside M such that it minimizes the standard deviation of its synergies:\nMk = argmini \u03c3(Mi) (13)\nFrom Mk, we estimate a local linear inverse model ((u, v, \u03c6) \u2192 \u03b8) by using a pseudo-inverse as introduced in the reaching experiment, and use it to estimate the motor synergy parameters \u03b8g which correspond to the desired goal (ug, vg, \u03c6g)."}, {"heading": "4.4.2 Exploration Phase", "text": "The system here continuously estimates the distance between the goal yg and already reached position yc which is the closest from the goal. If the reaching phase does not manage to make the system come closer to yg, i.e. D(yg, yt) > D(yg, yc), with yt as last effectively reached point in an attempt toward yg, the exploration phase is triggered.\nIn this phase the system first considers the nearest neighbor yc = (uc, vc, \u03c6c) of the goal (ug, vg, \u03c6g) and gets the corresponding known synergy \u03b8c. Then, it adds a random noise rand(24) to the 24 parameters {ph1,2,..,12, am1,2,..,12}c of this synergy \u03b8c which is proportional to the Euclidian distance D(yg, yc). The next synergy \u03b8t+1 = {ph1,2,..,12, am1,2,..,12}t+1 to experiment can thus be described using the following equation:\n\u03b8t+1 = ( {ph1,2,..,12, am1,2,..,12}c + \u03bb.rand(24).D(yg, yc) ) (14)\nwhere rand(i) returns a vector of i random values in [\u22121; 1], \u03bb > 0 and {ph1,2,..,12, am1,2,..,12}c the motor synergy which corresponds to yc."}, {"heading": "4.5 Qualitative Results", "text": "Fig. 16 presents the positions explored by the quadruped inside the task space u, v, \u03c6 after 10000 experimentations (running of motor synergies during the same fixed amount of time) using the exploration mechanisms introduced previously. ACTUATOR-RANDOM and ACTUATOR-RIAC select parameters of motor synergies in this experiment, whereas SAGG-RANDOM and SAGG-RIAC selfgenerate goals (u, v, \u03c6).\nComparing the two first exploration mechanisms (ACTUATOR-RANDOM and ACTUATOR-RIAC) we cannot distinguish any notable difference, the space explored appears similar and the extent of explored space on the (u, v) axis is comprised in the interval [\u22125; 5] for u and [\u22122.5; 2.5] for v on both graphs. Moreover, we notice that the difference between u and v scales is due to the inherent structure of the robot, which simplifies the way to go forward and backward rather than shifting left or right.\nConsidering SAGG methods, it is important to note the difference between the reachable area and the task space. In Fig. 16, red lines correspond to the estimated reachable area which is comprised of [\u221210; 10] \u00d7 [\u221210; 10] \u00d7 [\u2212\u03c0;\u03c0], whereas the task space is much larger: [\u221245; 45]\u00d7 [\u221245; 45]\u00d7 [\u22122\u03c0; 2\u03c0]. We are also able to notice the asymmetric aspect of its repartition according to the v axis, which is due to the decentered weight of the robot\u2019s head.\nFirst, the SAGG-RANDOM method seems to slightly increase the space covered on the u and v axis compared to ACTUATOR methods, as shown by the higher concentration of positions explored in the interval [\u22125;\u22123] \u222a [3; 5] of u. However, this change does not seem very important when comparing SAGG-RANDOM to these two algorithm.\nSecond, SAGG-RIAC, contrary to SAGG-RANDOM, shows a large exploration range: the surface in u has almost twice as much coverage than using previous algorithms, and in v, up to three times; there is a maximum of 7.5 in v where the previous algorithms were at 2.5. These last results emphasize the capability of SAGG-RIAC to drive the learning process inside reachable areas which are not easily accessible (hardly discovered by chance)."}, {"heading": "4.6 Quantitative Results", "text": "In this section, we aim to test the efficiency of the learned forward/inverse models to guide the quadruped to reach a set of goal positions from an independently generated test database. Here we consider a test database of 100 goals, generated independantly and covering approximately uniformly the reachable part of the task space, and compute the distance between each goal attempted, and\nthe reached position. Fig. 18 shows performances of the 4 methods introduced previously. First of all, we can observe the higher efficiency of SAGG-RIAC compared to the other three methods which can be observed after only 1000 iterations. The high decreasing velocity of the reaching error (in the number of experimentations) is due to the consideration of regions limited to a small number of elements (30 in this experiment). It allows creating a very high number of regions within a small interval of time, which helps the system to discover and focus on reachable regions and its surrounding area.\nACTUATOR-RIAC shows slightly more efficient performances than ACTUATORRANDOM. Also, even if SAGG-RANDOM is less efficient than SAGG-RIAC, we can observe its highly decreasing reaching errors compared to ACTUATOR methods, which allows it to be significantly more efficient than these method when considered at 10000 iterations. Again, as in the previous experiment, we can also observe that SAGG-RIAC does not only allow to learn faster how to master the sensorimotor space, but that the asymptotic performances also seem to be better [30]."}, {"heading": "4.7 Conclusion of Results for the Quadruped Experiment", "text": "These experiments first emphasize the high efficiency of methods which drives the exploration of motor synergies in terms of their effects in the task space. As illustrated by qualitative results, SAGG methods, and especially SAGG-RIAC, allows driving the exploration in order to explore large spaces containing areas hardly discovered by chance, when limits of reachability are very difficult to predict. Then, quantitative results showed the capability of SAGG-RANDOM and SAGG-RIAC methods to learn inverse models efficiently when considering highly-redundant robotic systems controlled with motor synergies."}, {"heading": "5 Experimental Setup 3: Learning to Control a", "text": "Fishing Rod with Motor Synergies"}, {"heading": "5.1 Robotic Setup", "text": "This experiments consists of having a robot learning to control a fishing rod (with a flexible wire) in order to attain certain positions of the float when it touches the water. This setup is simulated using the Breve simulator, such as in the previous experiment. The rod is fixed on a 4 DOF arm controlled with motor synergies which affect the velocity of each joint, and are parameterized by the values \u03b8 = (v1, v2, v3, v4), vi \u2208 [0; 1]. More precisely, for each experimentation of the robot we use a low-level pre-programmed PID controller which tracks the desired velocity vi of each joint i during a fixed short amount of time (2 seconds), starting from a fixed rest position, until suddenly stopping the movement. During the movement, as well as a few second after, we monitor the 3D position of the float in order to detect a potential contact with the water (a flat plane corresponding to the water level). If the water is touched, we extract\nthe 2D coordinates (x, y) of the float on the plane (if not, we do not consider this trial). These coordinates, as well as the parameters of the synergies will be used to describe the forward model of the system as (v1, v2, v3, v4)\u2192 (x, y). Learning will thus be performed while recording each set {(v1, v2, v3, v4), (x, y)}i as a learning exemplar. In such a sensorimotor space, studying the behavior of SAGG-RIAC is relevant according to the flexible aspect of the line, which makes this system very difficult to model analytically, because it is highly redundant and highly sensitive to small variations of inputs. In the following experiment, the task space will consist of a limited area of the water surface. We will consider the basis of the arm as fixed on the coordinates (0, 0), the limits of the task space will be fixed to [\u22123; 3]\u00d7 [\u22123; 3] while the reachable region corresponds to a disk whose radius is 1, and can be contained in [\u22121; 1]\u00d7 [\u22121; 1] (see Fig. 19)."}, {"heading": "5.2 Qualitative Results", "text": "Fig. 20 shows histograms of the repartition of positions reached by the float on the water surface computed after 10000 \u201dwater touched\u201d trials (a \u201dwater touched\u201d trial corresponds to a reaching attempt where the float effectively touches the surface), after running ACTUATOR-RANDOM and SAGGRIAC exploration processes. The point situated at the center corresponds to the base to which the arm handling the fishing rod is situated (see Fig. 19). While observing the two figures, we can note a repartition of positions situated inside a disk, which radius delimits position reached when the line is maximally slack. Yet, the distribution of reached (and reachable) positions within this disk is both asymetrical among and between the two exploration processes. The asymetries on each figure are in fact reflecting the asymetries\nof the robot setup (see Fig. 19): the geometry of the robot is not symmetric and its starting/rest configuration is also not symmetric. Coupled with the structure of motor primitives, this makes that the structure of the reachable positions is complex and asymetric, and this can be observed especially in the ACTUATOR-RANDOM sub-figure, since it shows the asymetric distribution of float position reached when the parameters of the action primitives are sampled uniformly (and thus symmetrically). Comparing the two histograms, we note that SAGG-RIAC drives the exploration toward positions of the float not explored by ACTUATOR-RANDOM, such as the large part situated at the bottom of the reachable area. Thus, SAGG-RIAC drives here the exploration toward more diverse regions of the space. SAGG-RIAC is therefore able to avoid spending large amounts of time exclusively guiding the exploration toward the same areas, as ACTUATOR-RANDOM does. Extended experimentation with this setup showed that the distribution of reached points with SAGG-RIAC (right sub-figure) corresponds closely to the actual whole reachable space. Eventually, these qualitative results emphasize that SAGG-RIAC is able to drive the exploration process efficiently when carried out with highly redundant and complex robots with compliant/soft parts."}, {"heading": "5.3 Quantitative Results", "text": "Fig. 21 shows the mean reaching errors obtained using ACTUATOR-RANDOM and SAGG-RIAC, statistically computed after 10 experiments with different random seeds. Here, the comparison of these two methods shows that SAGGRIAC led to significantly more efficient results after 1000 successful trials. Also, after 6000 trials, we can observe a small increase in reaching errors of SAGGRIAC. This phenomenon is due to the discovery of new motor synergies which\nled to already mastered goal positions. This discovered redundancy reduces the generalization capability for computing the inverse model for a small amount of time until these new parameters of motor synergy have been explored enough to disambiguate the invert model (i.e. two distinct local inverse models are well encoded and do not interfere)."}, {"heading": "6 Conclusion and Future Work", "text": "This paper introduced the Self-Adaptive Goal Generation architecture, SAGGRIAC, for active learning of inverse models in robotics through intrinsically motivated goal exploration. First, we demonstrated the high efficiency of learning inverse models by performing an exploration driven by the active self-generation of high-level goals in the parameterized task space instead of traditional motor babbling specified inside a low-level control space. Active exploration in the task space leverages the redundancy often characterizing sensorimotor robotic spaces: this strategy drives robots to learn a maximal amount of tasks (i.e. learn to generate in a controlled manner a maximal number of effects in the task space), instead of numerous ways to perform the same tasks (i.e. learn\nmany action policies to achieve the same effect in the task space). Coupling goal babbling and sophisticated intrinsically motivated active learning also allows a robot to perform efficient autonomous learning of its limits of reachability, and of inverse models with unknown high-dimensional body schemas of different architectures. Intrinsically motivated active learning was here driven by the active stochastic search of areas in the task space where competence progress is maximal. This also allowed emerging developmental trajectories by driving the robot to progressively focus and learn tasks of increasing complexities, while discovering its own limits of reachability, avoiding to spend much exploration time trying to perform impossible tasks.\nWhile we showed that such an approach could allow efficient learning when the action space was continuous and high-dimensional, the experiments performed here were assuming that a low-dimensional task space was initially provided. It is frequent to have such low-dimensional task spaces for useful engineering problems in robotics, where one can assume that an engineer helps the robot learner by designing by hand the task space (including the choice of the variables and parameters specifying the task space). On the other hand, if one would like to use an architecture like SAGG-RIAC in a developmental framework, where one would not assume low-dimensional task spaces pre-specified to the robot, some additional mechanisms should be added to equip the robot with the following two related capabilities:\n\u2022 Find autonomously low-dimensional task spaces. Indeed, a too high dimension of a task space would make the evaluation of \u201ccompetence progress\u201d suffer from the curse of dimensionality;\n\u2022 Explore actively multiple task spaces (potentially an open-ended number of task space), thus opening the possibility to learn fields of skills which may be of different kinds;\nThere are several potential approaches that could be used to address these issues that include:\n\u2022 Mechanisms for higher-level stochastic generation of task spaces, and their active selection through global measures of competence progress, forming an architecture with three levels of active learning (active choice of a task space inside a space of tasks spaces, active choice of goals inside the chosen task space, and active choice of actions to learn to reach the chosen goal) would be a natural extension of the work presented in this article.\n\u2022 Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive the attention of a robot toward particular task spaces, through physical guidance [24, 71] or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment [88], may be introduced. Inverse reinforcement learning mechanisms, which are able to extract reward functions thanks\nto examples of action policies could also be seen as a mean to infer interesting task spaces from human demonstrations [50]. Social guidance may also be used as a mechanism to bootstrap the evaluation of competence progress, and the identification of zones of reachability, in very large or high-dimensional spaces such as shown in [71], which presents an approach to combine intrinsically motivated learning like SAGG-RIAC with techniques for learning by demonstration.\n\u2022 Maturational constraints: Although SAGG-RIAC highly simplifies the learning process by using goal babbling and drives it efficiently thanks to intrinsic motivations, learning still have to begin by a period of random exploration in order to discriminate unreachable areas as well as areas of differing interests. This becomes a problem when the volume of reachable areas in the task space is a lot smaller than the task space itself or when the task space becomes itself high-dimensional. An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66]. For instance, infants have a reduced visual acuity which prevents them from accessing high visual frequencies as well as distinguishing distant objects. This acuity then progressively grows as the maturation process evolves. Using such constraints in synergy with goal babbling and intrinsic motivation, such as explored in [10], would potentially allow to constrain and simplify further learning since the first actions of the robot [39, 42], and could be crucial when considering lifelong learning in unbounded task spaces."}, {"heading": "Acknowledgment", "text": "We thank everyone who gave us their feedback on the paper. This research was partially funded by ERC Grant EXPLORERS 240007"}, {"heading": "7 Biographies", "text": "Adrien Baranes received the M.S. degree in artificial intelligence and robotics from the University Paris VI, France, in 2008 and the a Ph.D. degree in artificial intelligence from the French National Institute of Computer Sciences and Control (INRIA)/University Bordeaux 1, France, in 2011. During his PhD, he studied developmental mechanisms allowing to constrain and drive the exploration process of robots in order to allow them to progressively learn high quantities of knowledge and know-how in unprepared open-ended spaces. Since January 2012, he has been studying intrinsic motivations with a biological/neurological point of view as a Post-Doctoral Fellow at Columbia University Medical Center, New-York, thanks to a Fulbright grant, and will pursue his research thanks to an HFSP Cross-Disciplinary Fellowship.\nDr. Pierre-Yves Oudeyer is permanent researcher at Inria and responsible of the FLOWERS team at Inria and Ensta-ParisTech. Before, he has been a permanent researcher in Sony Computer Science Laboratory for 8 years (1999- 2007). He studied theoretical computer science at Ecole Normale Suprieure in Lyon, and received his Ph.D. degree in artificial intelligence from the University Paris VI, France. After having worked on computational models of language\nevolution, he is now working on developmental and social robotics. He has published a book, more than 80 papers in international journals and conferences, holds 8 patents, gave several invited keynote lectures in international conferences, and received several prizes for his work. In particular, he is a laureate of the ERC Starting Grant EXPLORERS. He is editor of the IEEE CIS Newsletter on Autonomous Mental Development, and an associate editor of IEEE Transactions on Autonomous Mental Development, Frontiers in Neurorobotics, and of the International Journal of Social Robotics.\nAlgorithm 1 The SAGG-RIAC Architecture\nS: State/Context space \u03a0: Space of paremeterized action policies \u03c0\u03b8 Y : Space of parameterized tasks yi M: regression model of the forward mapping (S,\u03a0)\u2192 Y M\u22121: regression model of the inverse mapping (S, Y )\u2192 \u03a0 R: set of regions Ri \u2282 Y and corresponding measures interesti; input: thresholds \u03b5C ; \u03b5max; timeout input: rest position srest \u2208 S; reset value: r input: starting position sstart \u2208 S input: number of explorative movements q \u2208 N input: starting time: t input: q budget of physical experiments for goal-directed optimization loop\nReset the system in the resting state (sstart = srest) every r iteration of the loop; Active Goal Self-Generation (high-level): Self-generate a goal yg \u2208 Y using the mode(m \u2208 [1; 2; 3]) with probability pm (see Section 2.4.4.) Active Goal-Directed Exploration and Learning (low-level): Let st represent the current context of the system if Made possible by the sensorimotor space then\nCompute a set of subgoals {y1, y2, ..., yn} \u2208 Y on the pathway toward yg; (e.g. with a planning algorithm that takes s, M, M\n\u22121 and yg into account);\nelse {y1, y2, ..., yn} = \u2205; end if for each yj in {y1, y2, ..., yn} \u222a yg do\nwhile \u0393yj \u2264 \u03b5C & timeout not exceeded do Compute and execute an action/synergy \u03c0\u03b8j \u2208 \u03a0 using M\u22121 such that it targets yj , e.g. using techniques such as in [20, 8, 55, 108]; Get the resulting actually performed y\u0303j and update M and M\n\u22121 with new data (st, \u03b8j , y\u0303j) Compute the competence \u0393y\u0303j (see section 2.4.1.) UpdateRegions(R, y\u0303j ,\u0393y\u0303j ); if experiment with evolving context then\nGoal-directed optimization of \u03b8j to reach yj , with SSA like algorithm such as Algorithm 3, and given a budget of q allowed physical experiments; else Goal-directed optimization of \u03b8j to reach yj such as algorithm 4, or alternatively algorithms such as [112, 45, 79], and given a budget of q allowed physical experiments;\nend if end while Compute the competence \u0393yj (see section 2.4.1.) UpdateRegions(R, yj , \u0393yj );\nend for end loop\n62\nAlgorithm 2 Pseudo-Code of UpdateRegions\ninput: R: : set of regions Ri \u2282 Y and corresponding measures interesti; input: yt: current goal input: \u0393yt : competence measure for yt Let gMax be the maximal number of elements inside a region Let \u03b6 be a time window used to compute the interest Find the region Rn in R such that yt \u2208 Rn; Let k = card(Rn) Add \u0393yt,k in Rn, where k is an indice indicating the ordinal order in which \u0393yt was added in the region as compared to other measures of competences in Rn ; Compute the new value of interestn of Rn according to each \u0393yi,l \u2208 Rn such that:\ninterestn =\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223  |Rn|\u2212 \u03b62\u2211 l=|Rn|\u2212\u03b6 \u0393yi,l \u2212  |Rn|\u2211 l=|Rn|\u2212 \u03b62 \u0393yi,l  \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\n\u03b6\nif card(Rn) > gmax then Split Rn; (see text, section 2.4.4) end if\nAlgorithm 3 Example of Pseudo-Code for the Low-Level Goal-Directed Exploration with Evolving Context (used in the experimentation introduced section 3.3)\ninput: q is the budget of physical experiment allowed to the robot for local optimization; Update the current context st = sj ; {where sj is the context after having performed \u03c0\u03b8j} if Inefficient(M\u22121, y\u0303j , yj) then\nLocal Exploration Phase: for i = 1 to q do\nPerform action policy \u03c0\u03b8i with \u03b8i drawn randomly in the vicinity of \u03b8j ; Measure the resulting yi and si; Update M and M\u22121 with (st, \u03b8i, yi); Update the context st = si; Compute the competence \u0393yi ; UpdateRegions(R, yi, \u0393yi);\nend for end if\nAlgorithm 4 Example of Pseudo-Code for the Low-Level Goal-Directed Exploration with a Fixed or Resettable Context (used in the experiments introduced sections 4 and 5)\ninput: q is the budget of physical experiment allowed to the robot for local optimization; Reset the current context: st = srest; if Inefficient(M\u22121, y\u0303j , yj) then\nLocal Exploration Phase: Initialize \u03b8k = \u03b8j and yk = yj and \u0393yk = \u0393yj for i = 1 to q do\nPerform \u03c0\u03b8i where \u03b8i is drawn randomly in the vicinity of \u03b8k; Observe the resulting yi; Update M and M\u22121 with the resulting (st, \u03b8i, yi); Reset the current context: st = srest; Compute the competence \u0393yi ; UpdateRegions(R, yi, \u0393yi); if \u0393yi > \u0393yk then \u03b8k = \u03b8i yk = yi\nend if end for\nend if"}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "Proceedings of the 21st International Conference on Machine Learning (2004), pages 1\u20138. ACM Press, New York,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Regression and the moore-penrose pseudo inverse", "author": ["A. Albert"], "venue": "Mathematics in science and engineering. Academic Press, Inc.,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1972}, {"title": "Optimal level theories", "author": ["H.R. Arkes", "J.P. Garske"], "venue": "Psychological theories of motivation, volume 2, pages 172\u2013195.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["S. Arya", "D.M. Mount", "N.S. Netanyahu", "R. Silverman", "A.Y. Wu"], "venue": "Journal of the ACM (JACM), 45(6):891\u2013923, November", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Cognitive developmental robotics: A survey", "author": ["M. Asada", "K. Hosoda", "Y. Kuniyoshi", "H. Ishiguro", "T. Inui", "Y. Yoshikawa", "M. Ogino", "C. Yoshida"], "venue": "IEEE Trans. Autonomous Mental Development, 1(1),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["B. Bakker", "J. Schmidhuber"], "venue": "Proc. 8th Conf. on Intelligent Autonomous Systems (2004),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "What are intrinsic motivations? a biological perspective", "author": ["G. Baldassare"], "venue": "Proceeding of the IEEE ICDL-EpiRob Joint Conference,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Riac: Robust intrinsically motivated exploration and active learning", "author": ["A. Baranes", "P-Y. Oudeyer"], "venue": "IEEE Trans. on Auto. Ment. Dev., 1(3):155\u2013 169,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study", "author": ["A. Baranes", "P.Y. Oudeyer"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Taipei, Taiwan,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "The interaction of maturational constraints and intrinsic motivations in active motor development", "author": ["A. Baranes", "P-Y. Oudeyer"], "venue": "Proceedings of ICDL-EpiRob 2011,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Intrinsic motivation for reinforcement learning systems", "author": ["A. Barto", "O. Simsek"], "venue": "CT New Haven, editor, Proceedings of the Thirteenth Yale Workshop on Adaptive and Learning Systems (2005),", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Intrinsically motivated learning of hierarchical collections of skills", "author": ["A Barto", "S Singh", "N. Chenatez"], "venue": "Proc. 3rd Int. Conf. Dvp. Learn., pages 112\u2013119, San Diego, CA,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Conflict, Arousal and Curiosity", "author": ["D. Berlyne"], "venue": "McGraw-Hill,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1960}, {"title": "Curiosity and exploration", "author": ["D. Berlyne"], "venue": "Science, 153(3731):25\u201333,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1966}, {"title": "Simplified and effective motor control based on muscle synergies to exploit musculoskeletal dynamics", "author": ["M. Berniker", "A. Jarc", "E. Bizzi", "M.C. Tresch"], "venue": "Proceedings of the National Academy of Sciences of the United States of America (PNAS), 106(18):7601\u20137606, May", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The Coordination and Regulation of Movements", "author": ["N Bernstein"], "venue": "Pergamon,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1967}, {"title": "Proximodistal structure of early reaching in human infants", "author": ["N.E. Berthier", "R.K. Clifton", "D.D. McCall", "D.J. Robin"], "venue": "Exp Brain Res,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Robot programming by demonstration", "author": ["Aude Billard", "Sylvain Calinon", "Rudiger Dillmann", "Stefan Schaal"], "venue": "Handbook of Robotics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "Information Science and Statistics. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent spaces for dynamic movement primitives", "author": ["S. Bitzer", "S. Vijayakumar"], "venue": "Proceedings of IEEE/RAS International Conference on Humanoid Robots,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Bringing up robot: Fundamental mechanisms for creating a self-motivated, self-organizing architecture", "author": ["D. Blank", "D. Kumar", "L. Meeden", "J. Marshall"], "venue": "Cybernetics and Systems, 36(2),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Morphological change in machines accelerates the evolution of robust behavior", "author": ["Josh C. Bongard"], "venue": "Proceedigns of the National Academy of Sciences of the United States of America (PNAS),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "A self-organizing neural model of motor equivalent reaching and tool use by a multijoint arm", "author": ["D. Bullock", "S. Grossberg", "F.H. Guenther"], "venue": "Journal of Cognitive Neuroscience, 5(4):408\u2013435,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Statistial learning by imitation of competing constraints in joint space and task space", "author": ["S. Calinon", "A. Billard"], "venue": "Advanced Robotics, 23(15):2059\u2013 2076,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Body schema acquisition through active learning", "author": ["R. Cantin-Martinez", "M. Lopes", "L. Montesano"], "venue": "EEE - International Conference on Robotics and Automation (ICRA), Anchorage, Alaska, USA,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Incremental local inline gaussian mixture regression for imitation learning of multiple tasks", "author": ["T. Cederborg", "M. Li", "A. Baranes", "P-Y. Oudeyer"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (2010), Taipei, Taiwan,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Interactive policy learning through confidencebased autonomy", "author": ["S. Chernova", "M. Veloso"], "venue": "J. Artificial Intelligence Research, 34:1\u201325,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Singularity-robust task-priority redundancy resolution for real-time kinematic control of robot manipulators", "author": ["S Chiaverini"], "venue": "IEEE Transactions on Robotics and Automation, 13(3):398\u2013410,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Mach. Learn., 15(2):201\u2013221,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Active learning with statistical models", "author": ["David A. Cohn", "Zoubin Ghahramani", "Michael I. Jordan"], "venue": "J Artificial Intelligence Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Creativity-Flow and the Psychology of Discovery and Invention", "author": ["M. Csikszentmihalyi"], "venue": "Harper Perennial, New York,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1996}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "Adv. Neural Inform. Process. Systems, 17,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Convex Optimization and Euclidean Distance Geometry", "author": ["J. Dattorro"], "venue": "Meboo Publishing USA,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Combinations of muscle synergies in the construction of a natural motor behavior", "author": ["A. D\u2019Avella", "P. Saltiel", "E. Bizzi"], "venue": "Nature neuroscience,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Intrinsic Motivation and self-determination in human behavior", "author": ["E.L. Deci", "M. Ryan"], "venue": "Plenum Press, New York,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1985}, {"title": "Neural basis for rhythmic behaviour in animals", "author": ["F. Delcomyn"], "venue": "Science, 210:492\u2013498,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1980}, {"title": "Mastery motivation: Appropriate tasks for toddlers", "author": ["T.B. Dichter", "N.A. Busch", "D.E. Knauf"], "venue": "Infant Behavior and Development, 20(4):545\u2013 548,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}, {"title": "Made-Up Minds: A Constructivist Approach to Artificial Intelligence", "author": ["G.L. Drescher"], "venue": "MIT Press,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["J. Elman"], "venue": "Cognition, 48:71\u201399,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1993}, {"title": "Intrinsically motivated information foraging", "author": ["I. Fasel", "A. Wilt", "N. Mafi", "C.T. Morrison"], "venue": "Proceedings of the IEEE 9th International Conference on Development and Learning (2010),", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Theory of Optimal Experiment", "author": ["V. Fedorov"], "venue": "Academic Press, Inc., New York, NY,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1972}, {"title": "The importance of starting blurry: Simulating improved basic-level category learning in infants due to weak visual acuity", "author": ["R.M. French", "M. Mermillod", "P.C. Quinn", "A. Chauvin", "D. Mareschal"], "venue": "LEA, editor, Proceedings of the 24th Annual Conference of the Cognitive Science Society (2002), pages 322\u2013327, New Jersey,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, 28(2-3):133\u2013168,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1997}, {"title": "Central pattern generators for locomotion, with special reference to vertebrates", "author": ["S. Grillner", "P. Wallen"], "venue": "Annual Review of Neuroscience, 8:233\u2013 261, march", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1985}, {"title": "Completely derandomized self- adaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary Computation, 9(2):159\u2013195,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "Intrinsically motivated hierarchical manipulation", "author": ["S. Hart", "R. Grupen"], "venue": "Proceedings of the IEEE Conference on Robots and Automation (2008),", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalization and transfer in robot control", "author": ["S. Hart", "S. Sen", "R. Grupen"], "venue": "Lund Univeristy Cognitive Studies, editor, Proc. Of the 8th International Conference On Epigenetic Robotics (2008), University of Sussex,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Novelty and reinforcement learning in the value system of developmental robots", "author": ["X. Huang", "J. Weng"], "venue": "C. Prince, Y. Demiris, Y. Marom, H. Kozima, and C. Balkenius, editors, Proc 2nd Int. Workshop Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems, volume 94, pages 47\u201355. Lund University Cognitive Studies,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2002}, {"title": "Central pattern generators for locomotion control in animals and robots: A review", "author": ["A.J. Ijspeert"], "venue": "Neural Networks, 21(4):642\u2013653,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2008}, {"title": "Task space retrieval using inverse feedback control", "author": ["N. Jetchev", "M. Toussaint"], "venue": "L. Getoor and T. Scheffer, editors, International Conference on Machine Learning (ICML-11), volume 28, pages 449\u2013456, New York, NY, USA,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D. Jones", "M. Schonlau", "W. Welch"], "venue": "Global Optimization, 13(4):455\u2013492,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Dopamine: Generalization and bonuses", "author": ["S. Kakade", "P. Dayan"], "venue": "Neural Networks, 15(4-6):549\u201359,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}, {"title": "Active learning with gaussian processes for object categorization", "author": ["A. Kapoor", "K. Grauman", "R. Urtasun", "T. Darrell"], "venue": "Proceeding of the IEEE 11th Int. Conf. Comput. Vis. (2007), Crete, Greece,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2007}, {"title": "Breve: a 3d environment for the simulation of decentralized systems and artificial life", "author": ["J. Klein"], "venue": "MIT Press, editor, Proceeding of the eighth international conference on artificial life (2003),", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcement learning to adjust robot movements to new situations", "author": ["J. Kober", "E. Oztop", "J. Peters"], "venue": "Proceedings of Robotics: Science and Systems,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonmyopic active learning of gaussian processes: an exploration-exploitation approach", "author": ["A. Krause", "C. Guestrin"], "venue": "24th international conference on Machine learning,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficientalgorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research, 9:235\u2013284,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2008}, {"title": "Neuromotor synergies as a basis for coordinated intentional action", "author": ["W.A. Lee"], "venue": "J. Mot. Behav., 16:135\u2013170,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1984}, {"title": "A computational model of social-learning mechanisms", "author": ["M. Lopes", "F. Melo", "B. Kenward", "J. Santos-Victor"], "venue": "Adaptive Behavior, 467(17),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "Abstraction levels for robotic imitation: Overview and computational approaches", "author": ["M. Lopes", "f. Melo", "J. Santos-Victor"], "venue": "In From Motor Learning to Interaction Learning in Robots. SpringerLink,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Active learning for reward estimation in inverse reinforcement learning", "author": ["M. Lopes", "F.S. Melo", "L. Montesano"], "venue": "European Conference on Machine Learning (ECML/PKDD), Bled, Slovenia,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2009}, {"title": "Active learning and intrinsically motivated exploration in robots: Advances and challenges (guest editorial)", "author": ["M. Lopes", "P-Y. Oudeyer"], "venue": "IEEE Transactions on Autonomous Mental Development, 2(2):65\u201369,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "A developmental roadmap for learning by imitation in robots", "author": ["M. Lopes", "J. Santos-Victor"], "venue": "IEEE Transactions in Systems Man and Cybernetic, Part B: Cybernetics, 37(2),", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2007}, {"title": "Artificial curiosity with planning for autonomous perceptual and cognitive development", "author": ["M. Luciw", "V. Graziano", "M. Ring", "J. Schmidhuber"], "venue": "Proceeding of the First IEEE ICDL-EpiRob Joint Conference (2011),", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "An emergent framework for selfmotivation in developmental robotics", "author": ["J. Marshall", "D. Blank", "L. Meeden"], "venue": "Proc. 3rd Int. Conf. Development Learn. (2004), pages 104\u2013111, San Diego, CA,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2004}, {"title": "On the influence of sensor morphology on eye motion coordination", "author": ["Harold Martinez", "Max Lungarella", "Rolf Pfeifer"], "venue": "In Proc. of the IEEE 9th International Conference on Development and Learning", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2010}, {"title": "A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot. Autonomous Robots ", "author": ["R. Martinez-Cantin", "N. de Freitas", "E. Brochu", "J. Castellanos", "A. Doucet"], "venue": "Special Issue on Robot Learning, Part B,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2009}, {"title": "Motivated learning from interesting events: Adaptive, multitask learning agents for complex environments. Adaptive Behavior - Animals, Animats, Software Agents, Robots", "author": ["Kathryn Merrick", "Mary Lou Maher"], "venue": "Adaptive Systems,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2009}, {"title": "Autonomously learning an action hierarchy using a learned qualitative state representation", "author": ["J. Mugan", "B. Kuipers"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (2009),", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast approximate nearest neighbors with automatic algorithm", "author": ["M. Muja", "D.G. Lowe"], "venue": "International Conference on Computer Vision Theory and Applications (2009),", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2009}, {"title": "Bootstrapping intrinsically motivated learning with human demonstrations", "author": ["M. Nguyen", "A. Baranes", "P-Y. Oudeyer"], "venue": "proceedings of the IEEE International Conference on Development and Learning, Frankfurt, Germany,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2011}, {"title": "Model learning for robot control: A survey", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": "Cognitive Processing, 12(4):319\u2013340,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2011}, {"title": "Mathematical models for the swimming pattern of a lamprey", "author": ["J. Nishii", "Y. Uno", "R. Suzuki"], "venue": "Biological Cybernetics, volume 72. Springer,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1994}, {"title": "How can we define intrinsic motivations ? In Proc", "author": ["P. Oudeyer", "F. Kaplan"], "venue": "Of the 8th Conf. On Epigenetic Robotics (2008),", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2008}, {"title": "The playground experiment: Task-independent development of a curious robot", "author": ["P. Oudeyer", "F. Kaplan", "V. Hafner", "A. Whyte"], "venue": "Proceedings of the AAAI Spring Symposium on Developmental Robotics, pages 42\u201347,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2005}, {"title": "What is intrinsic motivation? a typology of computational approaches", "author": ["P.-Y. Oudeyer", "F. Kaplan"], "venue": "Frontiers of Neurorobotics, page 1:6,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2007}, {"title": "Intrinsic motivation systems for autonomous mental development", "author": ["P-Y. Oudeyer", "F. Kaplan", "V. Hafner"], "venue": "IEEE Transactions on Evolutionary Computation, 11(2):pp. 265\u2013286,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2007}, {"title": "Natural actor critic", "author": ["J. Peters", "S. Schaal"], "venue": "Neurocomputing, (7-9):1180\u2013 1190,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2008}, {"title": "reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "(4):682\u201397,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2008}, {"title": "Reinforcement learning for humanoid robotics", "author": ["J. Peters", "S. Vijayakumar", "S. Schaal"], "venue": "Third IEEE-RAS International Conference on Humanoid Robots (2003),", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2003}, {"title": "Artoolkit user manual, version 2.33", "author": ["I. Poupyrev", "H. Kato", "M. Billinghurst"], "venue": "Technical report, University of Washington,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2000}, {"title": "Mastery motivation in infants and toddlers: Is it greatest when tasks are moderately challenging", "author": ["R.E. Redding", "G.A. Morgan", "R.J. Harmon"], "venue": "Infant Behavior and Development,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 1988}, {"title": "The short-latency dopamine signal: A role in discovering novel actions? Nat", "author": ["P. Redgrave", "K. Gurney"], "venue": "Rev. Neurosci., 7(12):967\u201375, Nov", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning for robot soccer", "author": ["M. Riedmiller", "T. Gabel", "R. Hafner", "S. Lange"], "venue": "Autonomous Robot, 27:55\u201373,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2009}, {"title": "Goal babbling permits direct learning of inverse kinematics", "author": ["M. Rolf", "J. Steil", "M. Gienger"], "venue": "IEEE Trans. Autonomous Mental Development, 2(3):216\u2013229, 09/2010", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2010}, {"title": "Online goal babbling for rapid bootstrapping of inverse models in high dimensions", "author": ["M. Rolf", "J. Steil", "M. Gienger"], "venue": "Proceeding of the IEEE ICDL-EpiRob Joint Conference (2011),", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2011}, {"title": "Neonatal finger and arm movements as determined by a social and an object context", "author": ["L. Ronnquist", "C. von Hofsten"], "venue": "Early Develop. Parent.,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 1994}, {"title": "An integrated system for teaching new visually grounded words to a robot for non-expert users using a mobile device", "author": ["P. Rouanet", "P-Y. Oudeyer", "D. Filliat"], "venue": "Proceedings of IEEE-RAS International Conference on Humanoid Robots (HUMANOIDS 2010), Paris, France,", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards optimal active learning through sampling estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "Proc. 18th Int. Conf. Mach. Learn. (2001), volume 1, pages 143\u2013160,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2001}, {"title": "Intrinsic and extrinsic motivations: Classic definitions and new directions", "author": ["Richard M. Ryan", "Edward L. Deci"], "venue": "Contemporary Educational Psychology,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2000}, {"title": "Learning forward models for the operational space control of redundant robots", "author": ["C. Salaun", "V. Padois", "O. Sigaud"], "venue": "From Motor Learning to Interaction Learning in Robots, volume 264, pages 169\u2013192. Springer,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2010}, {"title": "robot juggling: an implementation of memory-based learning", "author": ["S. Schaal", "C.G. Atkeson"], "venue": "Control systems magazine, pages 57\u201371,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1994}, {"title": "Active learning for logistic regression: An evaluation", "author": ["A. Schein", "L.H. Ungar"], "venue": "Machine Learning, 68:235\u2013265,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolution and learning in an intrinsically motivated reinforcement learning robot", "author": ["M. Schembri", "M. Mirolli", "Baldassarre G"], "venue": "In Springer, editor, Proceedings of the 9th European Conference on Artificial Life", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2007}, {"title": "Evolving childhood\u2019s length and learning parameters in an intrinsically motivated reinforcement learning robot", "author": ["M. Schembri", "M. Mirolli", "Baldassarre G"], "venue": "In Proceedings of the Seventh International Conference on Epigenetic Robotics. Lund University Cognitive Studies,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2007}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "Proc. Int. Joint Conf. Neural Netw. (1991), volume 2, pages 1458\u20131463,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 1991}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "J. A. Meyer and S. W. Wilson, editors, Proc. SAB\u201991, pages 222\u2013227,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1991}, {"title": "Artificial curiosity based on discovering novel algorithmic predictability through coevolution", "author": ["J. Schmidhuber"], "venue": "P. Angeline, Z. Michalewicz, M. Schoenauer, X. Yao, and Z. Zalzala, editors, Congress on Evolutionary Computation, pages 1612\u20131618, Piscataway, NJ,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploring the Predictable, pages 579\u2013612", "author": ["J. Schmidhuber"], "venue": "Springer,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal artificial curiosity, developmental robotics, creativity, music, and the fine arts", "author": ["J. Schmidhuber"], "venue": "Connection Science, 18(2),", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2006}, {"title": "Formal theory of creativity, fun, and intrinsic motivation", "author": ["J. Schmidhuber"], "venue": "IEEE Transaction on Autonomous Mental Development, 2(3):230\u2013 247,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2010}, {"title": "Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem", "author": ["J. Schmidhuber"], "venue": "Report arXiv:1112.5309,", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2011}, {"title": "Less is more: Active learning with support vector machines", "author": ["G. Schohn", "D. Cohn"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2000}, {"title": "A neural substrate of prediction and reward", "author": ["W. Schultz", "P. Dayan", "P. Montague"], "venue": "Science, 275:1593\u20131599,", "citeRegEx": "104", "shortCiteRegEx": null, "year": 1997}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "CS Tech. Rep. 1648, Univ. Wisconsin-Madison, Madison, WI,", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2009}, {"title": "On-line regression algorithms for learning mechanical models of robots: a survey", "author": ["O. Sigaud", "C. Salaun", "V. Padois"], "venue": "Robotics and Autonomous System, 59(12):1115\u20131129, July", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning parameterized skills", "author": ["B. Castro Da Silva", "G. Konidaris", "A. Barto"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2012}, {"title": "An intrinsic reward mechanism for efficient exploration", "author": ["\u00d6. \u015eim\u015fek", "A.G. Barto"], "venue": "Proceedings of the Twenty-Third International Conference on Machine Learning (2006),", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2006}, {"title": "Instrinsically motivated reinforcement learning: An evolutionary perspective", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto", "J. Sorg"], "venue": "IEEE Transactions on Autonomous Mental Development (IEEE TAMD), 2(2):70\u201382,", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2010}, {"title": "Competence progress intrinsic motivation", "author": ["A. Stout", "A Barto"], "venue": "Proceedings of the International Conference on Development and Learning (2010),", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2010}, {"title": "Path integral policy improvement with covariance matrix adaptation", "author": ["F. Stulp", "O. Sigaud"], "venue": "Proceedings of International Conference of Machine Learning,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning motion primitive goals for robust manipulation", "author": ["F. Stulp", "E. Theodorou", "M. Kalakrishnan", "P. Pastor", "L. Righetti", "S. Schaal"], "venue": "Int. Conference on Intelligent Robots and Systems (IROS),,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "Proceedings of the International Machine Learning Conference, pages 212\u2013218,", "citeRegEx": "115", "shortCiteRegEx": null, "year": 1990}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence, 1123(181-211),", "citeRegEx": "116", "shortCiteRegEx": null, "year": 1999}, {"title": "Reinforcement learning for optimal control of arm movements", "author": ["E Theodorou", "J Peters", "S. Schaal"], "venue": "Abstracts of the 37st meeting of the society of neuroscience (2007),", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploration in active learning", "author": ["S. Thrun"], "venue": "M Arbib, editor, Handbook of Brain Science and Neural Networks, Cambridge, MA,", "citeRegEx": "118", "shortCiteRegEx": null, "year": 1995}, {"title": "Active exploration in dynamic environments", "author": ["S. Thrun", "K. Moller"], "venue": "Proceedings of Advances of Neural Information Processing Systems,", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1992}, {"title": "Neuromechanics of muscle synergies for posture and movement", "author": ["L. Ting", "J. McKay"], "venue": "Curr. Opin. Neubiol., 7:622\u2013628,", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["Marc Toussaint", "Amos Storkey"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 2006}, {"title": "The role of developmental limitations of sensory input on sensory/perceptual organization", "author": ["G. Turkewitz", "P.A. Kenny"], "venue": "J Dev Behav. Pediatr., 6(5):302\u20136,", "citeRegEx": "122", "shortCiteRegEx": null, "year": 1985}, {"title": "Keeping the arm in the limelight: Advanced visual control of arm movements in neonates", "author": ["A. van der Meer"], "venue": "Eur. J. Paediatric Neurol,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 1997}, {"title": "The functional significance of arm movements in neonates", "author": ["A. van der Meer", "F. van der Weel", "D. Lee"], "venue": null, "citeRegEx": "124", "shortCiteRegEx": "124", "year": 1995}, {"title": "Incremental online learning in high dimensions", "author": ["S. Vijayakumar", "A. D\u2019Souza", "S. Schaal"], "venue": "Neural Computation,", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 2005}, {"title": "An action perspective on motor an action perspective on motor development", "author": ["C. von Hofsen"], "venue": "TRENDS in Cognitive Science,", "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2004}, {"title": "Mind and society: The development of higher mental processes", "author": ["L.S. Vygotsky"], "venue": "Cambridge, MA: Harvard University Press,", "citeRegEx": "127", "shortCiteRegEx": null, "year": 1978}, {"title": "Developmental robotics: Theory and experiments", "author": ["J. Weng"], "venue": "Int. J. Humanoid Robotics, 1(2):199\u2013236,", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2004}, {"title": "Autonomous mental development by robots and animals", "author": ["J. Weng", "J. McClelland", "A. Pentland", "O. Sporns", "I. Stockman", "M. Sur", "E. Thelen"], "venue": "Science, 291(599-600),", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2001}, {"title": "Motivation reconsidered: The concept of competence", "author": ["R. White"], "venue": "Psychol. Rev., 66:297\u2013333,", "citeRegEx": "130", "shortCiteRegEx": null, "year": 1959}, {"title": "A study of cooperative mechanisms for faster reinfocement learning", "author": ["S. Whitehead"], "venue": "Technical Report 365, Univ. Rochester, Rochester, NY, 1991. 7 Biographies Adrien Baranes received the M.S. degree in artificial intelligence and robotics from the University Paris VI, France, in 2008 and the a Ph.D. degree in artificial intelligence from the French National Institute of Computer Sciences and Control (INRIA)/University Bordeaux 1, France, in", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 105, "context": "Advanced statistical learning techniques dedicated to incremental high-dimensional regression have been elaborated recently, such as [107, 72].", "startOffset": 133, "endOffset": 142}, {"referenceID": 71, "context": "Advanced statistical learning techniques dedicated to incremental high-dimensional regression have been elaborated recently, such as [107, 72].", "startOffset": 133, "endOffset": 142}, {"referenceID": 0, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 62, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 17, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 23, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 58, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 25, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 59, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 83, "context": "This is typically what happens in the reinforcement learning (RL) framework where no demonstration is originally required and only a goal has to be fixed (as a reward) by the engineer who conceives the system [114, 84, 117].", "startOffset": 209, "endOffset": 223}, {"referenceID": 114, "context": "This is typically what happens in the reinforcement learning (RL) framework where no demonstration is originally required and only a goal has to be fixed (as a reward) by the engineer who conceives the system [114, 84, 117].", "startOffset": 209, "endOffset": 223}, {"referenceID": 79, "context": "For instance, studies presented in [80] combine RL with the framework of learning by demonstration.", "startOffset": 35, "endOffset": 39}, {"referenceID": 91, "context": "The Shifting Setpoint Algorithm (SSA) introduced by Schaal and Atkeson [92] proposes another way to constrain the exploration process.", "startOffset": 71, "endOffset": 75}, {"referenceID": 126, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 76, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 125, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 4, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 40, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 29, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 88, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 104, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 59, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 115, "context": "A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-", "startOffset": 142, "endOffset": 147}, {"referenceID": 128, "context": "A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-", "startOffset": 193, "endOffset": 198}, {"referenceID": 29, "context": "A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-", "startOffset": 258, "endOffset": 262}, {"referenceID": 50, "context": "ment [51], or maximal uncertainty of the model [119] among others.", "startOffset": 5, "endOffset": 9}, {"referenceID": 116, "context": "ment [51], or maximal uncertainty of the model [119] among others.", "startOffset": 47, "endOffset": 52}, {"referenceID": 92, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 20, "endOffset": 24}, {"referenceID": 102, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 50, "endOffset": 55}, {"referenceID": 52, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 76, "endOffset": 88}, {"referenceID": 55, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 76, "endOffset": 88}, {"referenceID": 56, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 76, "endOffset": 88}, {"referenceID": 66, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 126, "endOffset": 135}, {"referenceID": 115, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 126, "endOffset": 135}, {"referenceID": 11, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 160, "endOffset": 164}, {"referenceID": 24, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 187, "endOffset": 191}, {"referenceID": 60, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 203, "endOffset": 211}, {"referenceID": 26, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 203, "endOffset": 211}, {"referenceID": 76, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 256, "endOffset": 260}, {"referenceID": 45, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 275, "endOffset": 279}, {"referenceID": 112, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 95, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 11, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 99, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 47, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 20, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 74, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 76, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 94, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 127, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 404, "endOffset": 417}, {"referenceID": 34, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 404, "endOffset": 417}, {"referenceID": 12, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 404, "endOffset": 417}, {"referenceID": 103, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 482, "endOffset": 495}, {"referenceID": 51, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 482, "endOffset": 495}, {"referenceID": 82, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 482, "endOffset": 495}, {"referenceID": 95, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 11, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 7, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 61, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 99, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 76, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 7, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 100, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 89, "context": "This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any", "startOffset": 174, "endOffset": 186}, {"referenceID": 34, "context": "This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any", "startOffset": 174, "endOffset": 186}, {"referenceID": 13, "context": "This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any", "startOffset": 174, "endOffset": 186}, {"referenceID": 95, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 11, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 107, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 108, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 64, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 67, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 93, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 96, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 47, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 6, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 75, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 63, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 39, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 95, "context": "Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77].", "startOffset": 181, "endOffset": 194}, {"referenceID": 99, "context": "Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77].", "startOffset": 181, "endOffset": 194}, {"referenceID": 76, "context": "Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77].", "startOffset": 181, "endOffset": 194}, {"referenceID": 7, "context": "For instance, the architecture called RobustIntelligent Adaptive Curiosity (RIAC) [8], which is a refinement of the IAC architecture which was elaborated for open-ended learning of affordances and skills in real robots [77], defines the interestingness of a sensorimotor subspace by the velocity of the decrease of the errors made by the robot when predicting the consequences of its actions, given a context, within this subspace.", "startOffset": 82, "endOffset": 85}, {"referenceID": 76, "context": "For instance, the architecture called RobustIntelligent Adaptive Curiosity (RIAC) [8], which is a refinement of the IAC architecture which was elaborated for open-ended learning of affordances and skills in real robots [77], defines the interestingness of a sensorimotor subspace by the velocity of the decrease of the errors made by the robot when predicting the consequences of its actions, given a context, within this subspace.", "startOffset": 219, "endOffset": 223}, {"referenceID": 76, "context": "As shown in [77, 8], it biases the system to explore subspaces of progressively increasing complexity.", "startOffset": 12, "endOffset": 19}, {"referenceID": 7, "context": "As shown in [77, 8], it biases the system to explore subspaces of progressively increasing complexity.", "startOffset": 12, "endOffset": 19}, {"referenceID": 73, "context": "Nevertheless, RIAC and similar \u201dknowledge based\u201d approaches (see [74]) have some limitations: first, while they can deal with the spatial or temporal non-stationarity of the model to be learned, they face the curse-of-dimensionality and can only be efficient when considering a moderate number of control dimensions (e.", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "Also, because performing these measure costs time, this approach becomes more and more inefficient as the dimensionality of the control space grows [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 7, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 54, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 106, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 123, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 86, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 121, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 120, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 48, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 35, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 33, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 57, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 76, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 269, "endOffset": 277}, {"referenceID": 46, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 269, "endOffset": 277}, {"referenceID": 16, "context": "Second, we will use a heuristic inspired by observations of infants who sometimes prepare their reaching movements by starting from a same rest position [17], by resetting the robot to such a rest position, which allows reducing the set of starting states used to perform a task.", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "In this paper, we propose an approach which allows us to transpose some of the basic ideas of IAC and RIAC architectures, combined with ideas from the SSA algorithm, into a multi-level active learning architecture called SelfAdaptive Goal Generation RIAC algorithm (SAGG-RIAC) (an outline and initial evaluation of this architecture was presented in [9]).", "startOffset": 350, "endOffset": 353}, {"referenceID": 78, "context": "[79, 112]), as well as regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters (e.", "startOffset": 0, "endOffset": 9}, {"referenceID": 110, "context": "[79, 112]), as well as regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters (e.", "startOffset": 0, "endOffset": 9}, {"referenceID": 19, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 7, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 54, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 106, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 78, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 19, "endOffset": 28}, {"referenceID": 110, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 19, "endOffset": 28}, {"referenceID": 19, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 32, "endOffset": 45}, {"referenceID": 54, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 32, "endOffset": 45}, {"referenceID": 106, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 32, "endOffset": 45}, {"referenceID": 73, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 138, "endOffset": 142}, {"referenceID": 36, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 30, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 81, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 2, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 124, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 73, "context": "In a competence based active exploration mechanism, according to the definition [74], the robot is pushed to perform an active exploration in the goal/operational space as opposed to motor babbling in the actuator space.", "startOffset": 80, "endOffset": 84}, {"referenceID": 97, "context": "First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102].", "startOffset": 264, "endOffset": 277}, {"referenceID": 98, "context": "First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102].", "startOffset": 264, "endOffset": 277}, {"referenceID": 101, "context": "First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102].", "startOffset": 264, "endOffset": 277}, {"referenceID": 5, "context": "Measures of interestingness based on a measure of competence to perform a skill were studied in [6], as well as in [94] where a selector chooses to perform different skills depending on the temporal difference error to reach each skill.", "startOffset": 96, "endOffset": 99}, {"referenceID": 93, "context": "Measures of interestingness based on a measure of competence to perform a skill were studied in [6], as well as in [94] where a selector chooses to perform different skills depending on the temporal difference error to reach each skill.", "startOffset": 115, "endOffset": 119}, {"referenceID": 109, "context": "The study proposed in [111] is based on the competence progress, which they use to select goals in a pre-specified set of skills considered in a discrete world.", "startOffset": 22, "endOffset": 27}, {"referenceID": 84, "context": "A mechanism for passive exploration in the task space for learning inverse models in high-dimensional continuous robotics spaces was presented in [85, 86], where a robot has to learn its arm inverse kinematics while trying to reach in a preset order goals put on a pre-specified grid informing the robot about the limits of its reachable space.", "startOffset": 146, "endOffset": 154}, {"referenceID": 85, "context": "A mechanism for passive exploration in the task space for learning inverse models in high-dimensional continuous robotics spaces was presented in [85, 86], where a robot has to learn its arm inverse kinematics while trying to reach in a preset order goals put on a pre-specified grid informing the robot about the limits of its reachable space.", "startOffset": 146, "endOffset": 154}, {"referenceID": 73, "context": "1 Global Architecture Let us consider the definition of competence based models outlined in [74], and extract from it two different levels for active learning defined at different time scales (Fig.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 7, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 54, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 106, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 113, "context": "We can make an analogy of this formalization with the Semi-Markov Option framework introduced by Sutton [116].", "startOffset": 104, "endOffset": 109}, {"referenceID": 19, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 7, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 54, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 106, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 77, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 572, "endOffset": 581}, {"referenceID": 110, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 572, "endOffset": 581}, {"referenceID": 44, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 609, "endOffset": 613}, {"referenceID": 91, "context": "In the following experiments that will be introduced, we will use two different methods: one mechanism where optimization is inspired by the SSA algorithm [92], coupled with memory-based local forward and inverse regression models using local Moore-Penrose pseudo-inverses, and a more generic optimization algorithm mixing stochastic optimization with memory-based regression models using pseudo-inverse.", "startOffset": 155, "endOffset": 159}, {"referenceID": 77, "context": "For the optimization part, algorithms such as natural actor-critic architectures in model based reinforcement learning [78], algorithms of convex optimization [33], algorithms of stochastic optimization like CMA (e.", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "For the optimization part, algorithms such as natural actor-critic architectures in model based reinforcement learning [78], algorithms of convex optimization [33], algorithms of stochastic optimization like CMA (e.", "startOffset": 159, "endOffset": 163}, {"referenceID": 44, "context": "[45]), or path-integral methods (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 111, "context": "[113, 112]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 110, "context": "[113, 112]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "For the regression part, we are here using a memory-based approach, which if combined with efficient data storage and access structures [4, 70], scales well from a computational point of view.", "startOffset": 136, "endOffset": 143}, {"referenceID": 69, "context": "For the regression part, we are here using a memory-based approach, which if combined with efficient data storage and access structures [4, 70], scales well from a computational point of view.", "startOffset": 136, "endOffset": 143}, {"referenceID": 105, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 327, "endOffset": 332}, {"referenceID": 19, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 7, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 54, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 106, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 76, "context": "A typical instantiation of C, without constraints \u03c1, is defined as C(yg, yf , \u2205) = \u2212\u2016yg\u2212yf\u2016, and is the direct transposition of prediction error in RIAC [77, 8] to the task space in SAGG-RIAC.", "startOffset": 153, "endOffset": 160}, {"referenceID": 7, "context": "A typical instantiation of C, without constraints \u03c1, is defined as C(yg, yf , \u2205) = \u2212\u2016yg\u2212yf\u2016, and is the direct transposition of prediction error in RIAC [77, 8] to the task space in SAGG-RIAC.", "startOffset": 153, "endOffset": 160}, {"referenceID": 7, "context": "Such a mechanism has been described in the RIAC algorithm introduced in [8], but was previously applied to the actuator space S rather than to the goal/task space Y as is done in SAGG-RIAC.", "startOffset": 72, "endOffset": 75}, {"referenceID": 16, "context": "[17] is that infant\u2019s reaching attempts are often preceded by movements that either elevate their hand or move their hand back to their side.", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "And the second one, noticed in [85], is that infants do not try to reach for objects forever but sometimes relax their muscles and rest.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 76, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 64, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 95, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 20, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 110, "context": "Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like PI \u2212CMA [112], CMA [45], or those presented in [79].", "startOffset": 204, "endOffset": 209}, {"referenceID": 44, "context": "Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like PI \u2212CMA [112], CMA [45], or those presented in [79].", "startOffset": 215, "endOffset": 219}, {"referenceID": 78, "context": "Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like PI \u2212CMA [112], CMA [45], or those presented in [79].", "startOffset": 243, "endOffset": 247}, {"referenceID": 22, "context": "A solution to this non-convex problem has then been proposed by Bullock in [23] who converted it into a convex problem, by only considering the learning task within the spatial vicinity \u0302\u0307 \u03b1 of a particular \u03b1 : \u1e8f = J(\u03b1)\u0302\u0307 \u03b1 (5)", "startOffset": 75, "endOffset": 79}, {"referenceID": 122, "context": "Learning inverse kinematics typically deals with these kind of constraints, and these local methods have thus been proposed as an efficient approach to IK learning [125, 107].", "startOffset": 164, "endOffset": 174}, {"referenceID": 105, "context": "Learning inverse kinematics typically deals with these kind of constraints, and these local methods have thus been proposed as an efficient approach to IK learning [125, 107].", "startOffset": 164, "endOffset": 174}, {"referenceID": 69, "context": "In the following study, we use an incremental version of the Approximate Nearest Neighbors algorithm (ANN) [70], based on a tree split using the k-means process, to determine the vicinity of the current \u03b1.", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "Using the pseudo-inverse of Moore-Penrose [2] to compute the pseudo-inverse J(\u03b1) of the Jacobian J(\u03b1) in a vicinity \u0302\u0307 \u03b1 is thus sufficient.", "startOffset": 42, "endOffset": 45}, {"referenceID": 27, "context": "Possible problems happening due to singularities [106, 28, 91] being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).", "startOffset": 49, "endOffset": 62}, {"referenceID": 90, "context": "Possible problems happening due to singularities [106, 28, 91] being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).", "startOffset": 49, "endOffset": 62}, {"referenceID": 85, "context": "Possible problems happening due to singularities [106, 28, 91] being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).", "startOffset": 127, "endOffset": 131}, {"referenceID": 37, "context": "Learning the inverse kinematics is here an online process that arises each time a micro-action \u03b8 = \u2206\u03b1 \u2208 A is executed by the manipulator: by doing each micro-action, the robot stores measures (\u03b1,\u2206\u03b1,\u2206x) in its memory and creates a database Data which contains elements (\u03b1i,\u2206\u03b1i,\u2206yi) representing the discovered change \u2206yi corresponding to a given \u2206\u03b1i in the configuration \u03b1i (this learning entity can be called a schema according to the terminology of Drescher [38]).", "startOffset": 459, "endOffset": 463}, {"referenceID": 44, "context": "2 Exploration Phase This phase consists in performing q \u2208 N small random explorative actions \u2206\u03b1i, around the current position \u03b1, where the variations can be derandomized such as in [45].", "startOffset": 181, "endOffset": 185}, {"referenceID": 29, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 96, "endOffset": 108}, {"referenceID": 42, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 96, "endOffset": 108}, {"referenceID": 31, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 96, "endOffset": 108}, {"referenceID": 76, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 64, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 93, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 68, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 95, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 7, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 118, "context": "[121]), the zones of reachability would be increased if obstacles are introduced since the low-level system could learn to go around them.", "startOffset": 0, "endOffset": 5}, {"referenceID": 28, "context": "More precisely, we observe both increase in learning speed and final generalization performances (this results resonates with results from more classic active learning, see [29]).", "startOffset": 173, "endOffset": 177}, {"referenceID": 80, "context": "In order to allow the camera to distinguish the end-effector of the arm and to create a visual referent framework on the 2D surface, we used visual tags and the software ARToolKit Tracker [81].", "startOffset": 188, "endOffset": 192}, {"referenceID": 43, "context": "central pattern generators [44, 73, 49]), motor synergies are defined as the coherent activations (in space or time) of a group of muscles.", "startOffset": 27, "endOffset": 39}, {"referenceID": 72, "context": "central pattern generators [44, 73, 49]), motor synergies are defined as the coherent activations (in space or time) of a group of muscles.", "startOffset": 27, "endOffset": 39}, {"referenceID": 48, "context": "central pattern generators [44, 73, 49]), motor synergies are defined as the coherent activations (in space or time) of a group of muscles.", "startOffset": 27, "endOffset": 39}, {"referenceID": 33, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 57, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 14, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 117, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 15, "context": "Described as crucial for the development of motor abilities, they can be seen as encoding an unconscious continuous control of muscles which simplifies the complexity of the learning process: learning complex tasks using parameterized motor synergies (such as walking, or swimming) indeed corresponds to the tuning of relatively low-dimensional (but yet which can have a few dozen dimensions) high-level control parameters, compared to the important number of degrees of freedom which have to be controlled (thousand in the human body, see [16]).", "startOffset": 540, "endOffset": 544}, {"referenceID": 53, "context": "2 Robotic Setup In the following experiment, we consider a quadruped robot simulated using the Breve simulator [54] (physics simulation is based on ODE).", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 7, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 54, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 106, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 69, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 346, "endOffset": 350}, {"referenceID": 29, "context": "Again, as in the previous experiment, we can also observe that SAGG-RIAC does not only allow to learn faster how to master the sensorimotor space, but that the asymptotic performances also seem to be better [30].", "startOffset": 207, "endOffset": 211}, {"referenceID": 23, "context": "\u2022 Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive the attention of a robot toward particular task spaces, through physical guidance [24, 71] or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment [88], may be introduced.", "startOffset": 195, "endOffset": 203}, {"referenceID": 70, "context": "\u2022 Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive the attention of a robot toward particular task spaces, through physical guidance [24, 71] or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment [88], may be introduced.", "startOffset": 195, "endOffset": 203}, {"referenceID": 87, "context": "\u2022 Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive the attention of a robot toward particular task spaces, through physical guidance [24, 71] or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment [88], may be introduced.", "startOffset": 313, "endOffset": 317}, {"referenceID": 49, "context": "to examples of action policies could also be seen as a mean to infer interesting task spaces from human demonstrations [50].", "startOffset": 119, "endOffset": 123}, {"referenceID": 70, "context": "Social guidance may also be used as a mechanism to bootstrap the evaluation of competence progress, and the identification of zones of reachability, in very large or high-dimensional spaces such as shown in [71], which presents an approach to combine intrinsically motivated learning like SAGG-RIAC with techniques for learning by demonstration.", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 119, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 21, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 65, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 9, "context": "Using such constraints in synergy with goal babbling and intrinsic motivation, such as explored in [10], would potentially allow to constrain and simplify further learning since the first actions of the robot [39, 42], and could be crucial when considering lifelong learning in unbounded task spaces.", "startOffset": 99, "endOffset": 103}, {"referenceID": 38, "context": "Using such constraints in synergy with goal babbling and intrinsic motivation, such as explored in [10], would potentially allow to constrain and simplify further learning since the first actions of the robot [39, 42], and could be crucial when considering lifelong learning in unbounded task spaces.", "startOffset": 209, "endOffset": 217}, {"referenceID": 41, "context": "Using such constraints in synergy with goal babbling and intrinsic motivation, such as explored in [10], would potentially allow to constrain and simplify further learning since the first actions of the robot [39, 42], and could be crucial when considering lifelong learning in unbounded task spaces.", "startOffset": 209, "endOffset": 217}], "year": 2013, "abstractText": "We introduce the Self-Adaptive Goal Generation Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. \u2217Baranes, A., Oudeyer, P-Y. (2012) Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and Autonomous Systems, 61(1), pp. 49-73. http://dx.doi.org/10.1016/j.robot.2012.05.008 1 ar X iv :1 30 1. 48 62 v1 [ cs .L G ] 2 1 Ja n 20 13 We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.", "creator": "LaTeX with hyperref package"}}}