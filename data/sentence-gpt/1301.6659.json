{"id": "1301.6659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2013", "title": "Clustering-Based Matrix Factorization", "abstract": "Recommender systems are emerging technologies that nowadays can be found in many applications such as Amazon, Netflix, and so on. These systems help users find relevant information, recommendations, and their preferred items. Matrix Factorization is a popular method in Recommendation Systems showing promising results in accuracy and complexity of the recommended recommendation algorithm. It allows a user to rank recommendations, recommendations, and recommendations for each other. Matrix Factorization offers a wide variety of services that can be found across all platforms and provides a variety of advanced options for recommending more.\n\n\n\n\n\nThe Matrix Factorization algorithm allows users to easily rank recommendations, recommendations, and recommendations for each other. Matrix Factorization is one of the most popular, best-selling, and most widely utilized, algorithms used in the research of artificial intelligence (AI). It has been used in most mainstream algorithms for the last five years to determine the accuracy of recommendations, recommendations, and recommendations.\n\nThe Matrix Factorization algorithms are widely used as the basis for many other algorithms as well as many other algorithms in the scientific world, but the algorithms which help to help people navigate their research data are currently being used to provide more information and recommendations for other scientific organizations. In addition, the algorithms which can be found are also available for download and use in a wide range of areas such as Artificial Intelligence and Artificial Intelligence.\nWith all of the latest technology being developed and the new advancements in artificial intelligence, this is especially important in an industry where the market value of this technology is still relatively low, with the most demand coming from online users. The value of this technology is so high that a number of other software companies and technology companies have announced their latest product to market, including Amazon and Google.\nTo make this possible, there are several factors to consider, including the ability to make a recommendation and providing feedback in the field.\nThe number of users a person makes, for example, the algorithm allows for for providing the recommendations that have been discussed over the past few years. The value of this algorithm is so high that the algorithm can take the user up to four years to recommend them as a part of the study. The algorithm can be considered a very advanced method for calculating the correct recommendations for every specific individual.\nThe algorithms that are used by a company and their products, such as Amazon, Google, and Amazon are used in this industry, but in the fields of Artificial Intelligence and Artificial Intelligence, the number of users per month can greatly increase, as the number of users per month increases, and the", "histories": [["v1", "Mon, 28 Jan 2013 20:01:57 GMT  (202kb,D)", "http://arxiv.org/abs/1301.6659v1", null], ["v2", "Fri, 8 Feb 2013 22:16:44 GMT  (0kb,I)", "http://arxiv.org/abs/1301.6659v2", "This paper has been withdrawn by the authors. As the problem in the source code the results are nor valid anymore"], ["v3", "Wed, 27 Feb 2013 01:04:55 GMT  (344kb,D)", "http://arxiv.org/abs/1301.6659v3", null], ["v4", "Thu, 1 Aug 2013 22:06:49 GMT  (0kb,I)", "http://arxiv.org/abs/1301.6659v4", "This paper has been withdrawn by the author due to crucial typo and the poor grammatical text"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nima mirbakhsh", "charles x ling"], "accepted": false, "id": "1301.6659"}, "pdf": {"name": "1301.6659.pdf", "metadata": {"source": "META", "title": "Clustering-Based Matrix Factorization (under review)", "authors": ["Nima Mirbakhsh", "Charles X. Ling"], "emails": ["smirbakh@uwo.ca", "cling@csd.uwo.ca"], "sections": [{"heading": "1. Introduction", "text": "Recommender systems are emerging technologies that can be found in many present-day applications. Netflix1 recommends users a list of movies that they may be interested in; Google News2 tracks the news that users are following and gives a list of recommended articles, Amazon3 suggests everything from books to magazines. All of these recommendations come from engines that call recommendation systems. These are\n1http://www.netflix.com 2http://news.google.com 3http://www.amazon.com\nUnder Review\nsystems that find relevant information and recommendations for users based on located users\u2019 preferences (news, books, movies, musics, etc. ). Any information about users and items can be applied in a recommendation system, but there are two problems with information: 1) sometimes this information is barely accessible 2) with so much information, we should have algorithms that practically process it, and accurate results should be attained in a reasonable time. That is why scalability and accuracy have been the two most focused measures in recommendation systems in the last two decades (Konstan & Riedl, 2012).\nFor example, Figure 1 presents an artificial dataset which includes 5 users who rate newly released movies. The solid arrows reflect the users\u2019 preferences and the dashed arrows reflect users\u2019 non-preferences on these items. The main problem is to find related users and items based on their profiles or known preferences and infer the users\u2019 unknown preferences considering these relations. Collaborative filtering is a well-known solution in recommendation systems that uses only the known preferences for modeling this problem. It relates two items based on the fact that many users have purchased or rated those items, or it relates two users based on their similar purchases or preferences. This solution is in contrast to the content filtering solution which focuses on analyzing sets of user profiles and product features to find similarities.\nNeighborhood based approaches are popular collaborative filtering methods (To\u0308scher & Jahrer, 2009). This approach uses the known preferences to calculates similarities between users and finds the h most similar users to every users (it does same for items). It then deduces an unknown rating of user u for an item i, rui, considering ratings of that item by the h highest similar user to user u. Or, considering ratings that user u has revealed on the h highest similar items to item i (To\u0308scher et al., 2008). For example this approach finds user 2 and 3 as the most similar\nar X\niv :1\n30 1.\n66 59\nv1 [\ncs .L\nG ]\n2 8\nJa n\n20 13\nusers to user 1 and then applies their preferences on item Madagascar to predict the unknown preference of user1 on item Madagascar. In practice, finding similar users and items pairs is costly as there are a large number of users and items. On the other hand, using only related items and users to find unknown preferences is too rigid. It just reflects the low-level interests of users and cannot be generalized easily to find users\u2019 abstract tastes.\nMatrix factorization ((Koren et al., 2009), (To\u0308scher et al., 2008), (Jamali & Ester, 2010)) is a common dimension reduction algorithm that generalizes users\u2019 preferences and items\u2019 histories in a very limited number of latent features (such as 5 or 10 features). It uses these latent features to predict possible preferences or ratings of users on items. Matrix factorization works based on the singular value decomposition (SVD) but in a lower dimension which finds a set of latent vectors that reflect the rating behavior of users. In our example, first matrix factorization deduces user 1\u2019s interests and item Madagascar\u2019s features in their latent vectors. It then uses these latent vectors to predict if user 1 is interested in item Madagascar. Hence, it is a good solution to deal with the complexity of finding relations as well as generalizing them. It also shows promising results in experiment using collaborative information (Koren et al., 2009).\nNeighborhood aware matrix factorization is an extension of matrix factorization which applies the neighborhood model in the matrix factorization algorithm. There are two approaches, going back to our example: 1) by predicting the possible ratings of most similar users to user1 on item Madagascar based on their latent vectors. This is then used to predict rui (by weighted averaging), or 2) by using matrix factorization technique similarities are found between users and\nitems with less cost and more generality. It then uses these similarities (similarity between user 1, 2, and 3, and items IceAge,Brave, Madagascar) and known preferences to predict the unknown rating. They both share the same rationality but are different regarding how they calculate the similarity and if they use rigid known preferences or predicted unknown preferences in the prediction time. The Netflix prize competition has shown that this approach outperforms almost every other approach on prediction accuracy (To\u0308scher & Jahrer, 2009), (To\u0308scher et al., 2008), (Koren & Bell, 2011).\nInspired by this idea, in our proposed model we try to find the communities between users and items and then apply the effect of the latent factors of these homogeneous communities in the basic model. It has two advantages opposed to the common neighborhood models:\n1. We generalize the users\u2019 interests and items\u2019 features for the communities that they belong to. For example, user 1 may belongs to community Adults and item Madagascar may belongs to community Cartoons. Thus, in addition to considering if user1 is interested in item Madagascar, we apply the effect that community Adults is interested in community Cartoons as well.\n2. We consider deeper similarities. Item-item models check if user 1 is interested in item Madagascar and its similar items (IceAge and Brave). Useruser models also check if user 1 and its similar users (2 and 3) are interested in item Madagascar. In our approach, we check to see if user 1, 2 and 3 are interested in items Madagascar, IceAge and Brave.\nThus, the previous approaches likely predict a high interest for user 1 on item Madagascar because of a high similarity between user 3 and 1. But in a more general view they belong to the communities that do not share many interests (Adults on Cartoons). Hence in our proposed model, we first try to find these homogeneous communities (Adults, Cartoons, Drama, and so on.) and then consider the effect of these communities\u2019 behaviors beside the behavior of each user and item in an extension of Matrix Factorization. However, these communities are not usually easily understandable (like Drama and Adults in our example) by using collaborative information.\nWe successfully apply a standard clustering algorithm on a small dataset which works great. However, common clustering methods such as Kmeans and Hierarchical Clustering cannot handle large datasets. These\nmethods need to compare all the items and users pairs to find their similarities for clustering them. Thus, we employ Locality Sensitive Hashing (LSH) methods by a new technique to have a fast clustering method in our proposed model. LSH methods are usually used for dimension reduction purposes and other methods are applied then on the reduced dimension for clustering or classification objectives. In our method, we first use Minhash as the hash function of LSH to reduce the dimension and then consider each reduced dimension vector (bucket) as a cluster. This approach causes a less accurate and estimated clustering. In our experiment results, we show how using this estimated clustering method instead of using common clustering methods will affect the model\u2019s accuracy.\nWe use Root Mean Squared Error (RMSE) as the accuracy measure through some well known datasets including Netflix, Epinions, Flixster , and movielens 100K which are previously used in many publications such as (Koren et al., 2009), (Jamali & Ester, 2010), (Salakhutdinov & Mnih, 2008), and (Herlocker et al., 1999). Our empirical experiment show that our proposed algorithm make a significant improvement of accuracy without adding much complexity to the model. For instance, our proposed method\u2019s accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner\u2019s accuracy of 0.8567. To the best of our knowledge, our proposed method outperforms all other published neighborhood models in accuracy and complexity."}, {"heading": "2. Problem Definition", "text": "In a general recommendation problem, we have a set of users U = {u1, u2, . . . , un} and a set of items I = {i1, i2, . . . , im} that they are companied by a rating matrix R = [rui]n\u00d7m where rui represents the rating of user u on item i. This rating values are limited in a range [a, b] depending on applications but a range [0, 5] or a boolean value {0, 1} are usually quite common forms of rating in real-world applications. A recommendation system\u2019s goal is to find out an unknown rating rui for user u and item i using information including the rating matrix R.\nCollaborating filtering consists of predicting unknown rijs based on the known ri\u2032j\u2032 s inside the matrix R:\nR =\n i1 i2 . . . im u1 r11 r12 . . . r1m u2 r21 r22 . . . r2m ... ... ... . . . ... un rn1 rn2 . . . rnm \nMatrix Factorization addresses this problem by decomposing the ratings matrix, R, into two lower dimension matrices Q and P which contain corresponding vectors in the length of k for every item and user respectively, where k m,n. Such a model is close to the singular value decomposition (SVD) technique for finding latent factors in information retrieval (Koren et al., 2009).\nR =  i1 i2 . . . im q11 q12 . . . q1m q21 q22 . . . q2m ... ... . . .\n... qk1 qk2 . . . qkm\n T \u00d7  u1 u2 . . . un p11 p12 . . . p1n p21 p22 . . . p2n ... ... . . .\n... pk1 pk2 . . . pkn\n\nThe learning algorithm starts by a random initialization of matricesQ and P . In every learning step it then tries to change the initialized variables in the way that the product matrix converge to the known values of R. In the prediction case, the product of learned matrices will be used to predict unknown rijs.\nThus Matrix Factorization characterizes every user and item by corresponding them a latent vector. Assume qi \u2208 Rk as the corresponding vector for item i and pu \u2208 Rk as the corresponding vector for user u. It is supposed that the dot product of the user-item vectors results in the user\u2019s rating on the item:\nrui = q T i pu\nLets define the error as the real rating minus the predicted value in each step, eui. In the learning phase, the goal will be minimization of the regularized squared error on a train set, K, as follows:\nmin( \u2211\n(u,i)\u2208K\n(rui \u2212 qTi pu)2 + \u03bb(|qi|2 + |pu|2))\nRegularized value prevents over-fitting on the model. Thus, the problem will change to minimizing the square error of perdition as well as keeping absolute values of the latent variables as minimal as possible. Also, a bias value usually is corresponded to each user and item to reflect their mean ratings. Adding the biases, the above statement will change to:\nmin( \u2211\n(u,i)\u2208K\n(rui \u2212 qTi pu \u2212 b2i \u2212 b2u)2\n+\u03bb(|qi|2 + |pu|2 + |bu|2 + |bi|2))\nAlgorithm 1 presents the learning pseudo using a stochastic gradient descent technique giving a learning rate \u03b3, and a regularizing rate \u03bb .\nAlgorithm 1 Updating\nInput: Train set K Initializing matrices P , Q and bias values repeat\nfinding the error of predictions, eui, for every known rui in train set. for i = 1 to m\u2212 1 do qi \u2190 qi + \u03b3(eui.pu \u2212 \u03bb.qi) pu \u2190 pu + \u03b3(eui.qi \u2212 \u03bb.pu) bi \u2190 bi + \u03b3(eui \u2212 \u03bb.bi) bu \u2190 bu + \u03b3(eui \u2212 \u03bb.bu)\nend for until for limited number of epoches"}, {"heading": "3. Clustering-Based Matrix Factorization", "text": "In our proposed extension, we add neighborhood information on the basic Matrix Factorization model in a novel way. There are two common approaches in neighborhood aware matrix factorization models: 1) item \u2212 item models which consider if user u is interested in item i its similar items. 2) user\u2212user models that consider if user u and its similar users are interested in item i. In addition to complexity of finding similar users and items these models ignore the effect of the interests or features of communities that they belong to. Hence, we first try to find these communities and then apply the effect of the latent factors of these homogeneous communities in the basic model. As discussed in Section 1, it has two advantages opposed to the common neighborhood models:\n1. We generalize the users\u2019 interests and items\u2019 features for the communities that they belong to. Thus, we consider if general interests of users are matched with general features of items as well.\n2. We consider deeper similarities. More specifically, we consider if user u and its similar users are interested in item i as well as its similar items.\nHence, our proposed model is an improvement of the item-item/user-user fusion models. We assume the ratings in a range of [1, 5]. For every user u and item i, vectors Iu =< I(u, i1), I(u, i2), . . . , I(u, im) > and Ii =< I(u1, i), I(u2, i), . . . , I(un, i) > are made respectively in the following form:\nI(uj , i) = { 1 ruj ,i > \u00b5 0 otherwise\nand:\nI(u, ij) = { 1 ru,ij > \u00b5 0 otherwise\nwhich \u00b5 is the average of ratings. In this way users(items) with the same co-rating(co-rated) profile will go to the same clusters. Although for the recommendation problem with a boolean rating {0, 1}, the exact rating values can be used to make the vectors.\nWe cluster the users and items to find their homogeneous communities. Then, we correspond every cluster (homogeneous communities) a latent vector as well as corresponding each user and item a vector like basic matrix factorization.\nAfter finding n\u2032 < n and m\u2032 < m homogeneous communities of users and items respectively, we consider the rating between these communities in a new matrix R\u2217 then. In the new rating matrix every r\u2217u\u2032i\u2032 reflects the rating of community u\u2032 of users on the community i\u2032 of items as follows:\nR\u2217 =\n Ci1 Ci2 . . . Cim\u2032 Cu1 r \u2217 11 r \u2217 12 . . . r \u2217 1m\u2032 Cu2 r \u2217 21 r \u2217 22 . . . r \u2217 2m\u2032 ... ... ... . . .\n... Cun\u2032 r \u2217 n\u20321 r \u2217 n\u20322 . . . r \u2217 n\u2032m\u2032  Hence, in this method the predictor function for every r\u2217u\u2032i\u2032 would change to:\nr\u2217u\u2032i\u2032 = q T ci\u2032 pcu\u2032 (1)\nwhere qci\u2032 and pcu\u2032 are corresponding values for clusters ci\u2032 and cu\u2032 . Thus, instead of predicting a rating for pair of users and items, in this method we will find a rating for their cluster pairs as well and use a fusion of both ratings in the final predictor function as follows:\nrui = \u03b1q T i pu + (1\u2212 \u03b1)qTcipcu + bi + bu (2)\nwhere \u03b1 will control the effect of both sentences in the predictor\u2019s result and ci and cu are the clusters that item i and user u belong to respectively. We name the combination form in our experimental results as the Clustering-Based Matrix Factorization (CBMF) . Our experiment shows that CBMF significantly improves the accuracy. However, using common clustering algorithms for large datasets is not practically. Thus, we use Locality Sensitive Hashing functions with a new technique to keep our model practically efficient."}, {"heading": "3.1. Locality Sensitive Hashing", "text": "As noticed earlier, the computation cost dealing with large datasets of recommendation systems is important. Hence, we use Locality-sensitive hashing (LSH)\nmethods establishing a new technique to estimate the clusters instead of using common similarity-based clustering methods. In the literature, LSH is usually employed for reducing dimension and then clustering algorithms such as k-means are applied on the reduced dimension for clustering purposes. Even in this way, a similarity check between all users and items is needed which is still costly. LSH is a method of performing probabilistic dimension reduction to hash the input items in the way that similar items are mapped to same buckets with a high probability (Broder et al., 1998). We use Min-wise independent permutations (MinHash) as the hash function in our method. It consists of \u03b2 hash functions, like H =< h1, h2, . . . , h\u03b2 >, in the following form:\nhi = min{\u03c0(a)}\nwhere a is the input vector, and \u03c0(a) is a permutation on the indexes of the input vector a. The min function will return the first non-zero index of the permuted vector.\nMore specifically, \u03b2 is the size of new dimension. These achieved low dimensional vectors (name buckets) can be considered as the estimated clusters. Reducing \u03b2 leads to making fewer clusters and vice versa. However in this way we will loose information as well as the output clusters are weak estimations. In our model we try \u03b2 = 1, 2, . . . , 6 to evaluate using a different number of estimated clusters (instead of using common clustering methods) on the accuracy of the model. Applying this LSH method in our proposed model does not add a costly extension on the basic matrix factorization mode. Hence, the time of learning algorithm will increase only less than twice of basic matrix factorization in the worst case scenario (m\u2032 = m and n\u2032 = n)."}, {"heading": "3.2. Repeating clustering and using hierarchical clusters in CBMF", "text": "LSH algorithms rely on probabilistic logic. Thus, it is supposed that using the LSH\u2019s buckets for several times will decrease the clustering error. This can be modeled by re-clustering fixed \u03b2 for h times. We then add the effect of new clusters in the predictor function (Repeated CBMF). Or, by applying all h most accurate clusters with different size of buckets in the predictor function(Hierarchical CBMF). However, using large h will linearly increase the learning time. Thus using re-clustering or more and less general clusters will decrease the effect of LSH clustering error and will add more information to the model. As noticed earlier, changing \u03b2 will affect the number of clusters and the generality of clusters. Thus starting with \u03b2 = 1,\nthis technique achieves the most general clusters and by increasing the \u03b2 the number of clusters would be increased and consequently achieving less general clusters. In our experiment, we first test our CBMF algorithm with different size of buckets (\u03b2) on the validation set and then:\n1. We select the h most accurate \u03b2s in a new predictor function as follows:\nrui = \u03b1q T i pu + (1\u2212\u03b1)( h\u2211 j=1 W \u03b2j .qT c \u03b2j i p c \u03b2j u ) + bi + bu\n(3)\nThe weights (W \u03b2j ) are selected in a proportion to how accurate the CBMF algorithm is using the \u03b2j on the validation set.\n2. We select the most accurate \u03b2 and then apply Minhash algorithm h times and with same \u03b2 on the same train set. By adding the effect of the new-clusters, we change the predictor function in same way as formula 3 using equals weights (W \u03b2j )."}, {"heading": "4. Experiment Results", "text": "We setup our experiment on several well-known recommendation system datasets including Netflix, Epinions, Flixster, and MovieLens100k1. These datasets are previously used in related articles such as (To\u0308scher & Jahrer, 2009), (Jamali & Ester, 2010), and (Herlocker et al., 1999). Table 1 shows a general statistics of these datasets.\nWe apply a repeated random sub-sampling validation which in each fold divides the known ratings in two sets; 80% for train set and the rest for test set. We use three folds and get the mean of these folds\u2019 test set accuracies as the final accuracy of the model. We first collect all the items that were rated over 3 by users (4 for Netflix) in the train set and do the same for items. Then, we establish the LSH method on this collection\n1The implementation package is publicly accessible at: https://www.dropbox.com/sh/x710ouz9ytdaeqf/0OHGQtQTv9\nto cluster the items and the users. As described in section Locality Sensitive Hashing, we try different size of buckets to see the effect of increasing and decreasing in the number of clusters.\nRoot Mean Squared Error (RMSE) is used as the evaluation metric as follows:\nRMSE =\n\u221a\u2211 (u,i)\u2208Ktest(rui \u2212 r \u2032 ui) 2\n|Ktest|\nwhere Ktest is the test set, rui is the actual rating, and rui is the predicted rating for user u on item i. We use a threshold of 0.005 as the stopping point in all the implemented algorithms. The learning process will be stopped if the difference of the train set\u2019s RMSEs between two epochs goes down the threshold. Experiments show us that using a threshold in opposed to using a fix number of epochs prevents over-fitting in the learning model. For example, employing 100-500 epochs to learn the Basic Matrix Factorization on the MovieLens dataset reaches 0.90 of RMSE for the test set but using the threshold to stop the learning process improves the RMSE to 0.81.\nFigure 2 illustrates a comparison between basic MF, neighborhood aware MF by (To\u0308scher et al., 2008), and our proposed model on movielens100k dataset. In Basic CBMF, we calculates all the similarities between users and items and employ them in a bottom-up hierarchical clustering. In the CBMF, the costly clustering is replaced by assuming Minhash\u2019s buckets as clusters. As it is expected, using more accurate clustering method will improve the accuracy for our proposed model. But, it is usually unpractical for large datasets. Although, using Minhash buckets as clusters still improves the RMSE more than other costly neighborhood aware matrix factorization.\nAs figure 3 shows, CBMF not only has an expressive less RMSE against the other well-known neighborhood extensions ((To\u0308scher et al., 2008) and (Koren & Bell, 2011)) it also outperforms the 2009 Netflix prize winner\u2019s ensemble algorithm by (To\u0308scher & Jahrer, 2009)."}, {"heading": "4.1. Number of Clusters", "text": "Unfortunately using the buckets as the clusters limits the choices to select the number of clusters. Cause it cannot be defined before hand, and increasing in the length of buckets, \u03b2, will increase the number of clusters. Figure 5 shows the change of similarity among the items in same clusters for different \u03b2 values. Increasing \u03b2, the similarity obviously will be increased but the number of clusters will be increased as well. Figure 4 shows how changing \u03b2 affects the RMSE results for the Netflix dataset. A question is: How the\n\u03b2 should be selected?\nTo determine \u03b2, we apply a cross-validation on the train set, and then changing the buckets\u2019 length we select the \u03b2 with lowest RMSE. Figure 6 illustrates applying our proposed method on the movielens dataset changing \u03b2 and in a 3-folds cross-validation. We are using same \u03b2 for users and items. As figure 6 shows \u03b2 = 2 has the best RMSE result for the verification sets. The average of test sets\u2019 RMSEs is smoothly similar to the average of validation sets\u2019 RMSEs for all the different \u03b2s."}, {"heading": "4.2. Complexity", "text": "As an advantage of employing LSH methods, the proposed model does not add a costly extension on the basic matrix factorization. It increases the learning\ntime of the basic matrix factorization by less than twice in each epoch. Table 2 shows the complexity of our proposed model, Basic Matrix Factorization, and two Neighborhood Aware Matrix Factorization models by (To\u0308scher et al., 2008), and (Koren & Bell, 2011). The second statements show complexity of pre-processing for each model. Basic matrix factorization and Neighborhood Aware Matrix Factorization by (Koren & Bell, 2011) have no preprocessing but the (To\u0308scher et al., 2008) proposed model add a preprocessing complexity of O(n2 + m2) to find the similar users and items. R(u) is the set of items for which their ratings by u are available, and likewise, R(i) denotes the set of users who rated item i. Thus, \u2211 u |R(u)| will be the number of known rating values in matrix R. The preprocessing of our algorithm includes hashing each input vector to a new lower dimension applying Minhash function. Thus, our preprocessing complexity would be a tracing through users and items which is O(m+ n) ."}, {"heading": "4.3. Selecting the Alpha Paremeter (\u03b1)", "text": "For selecting \u03b1, two approaches can be applied:\n1. To train the rating matrix for users-items and the rating matrix for the clusters separately. Thus,\nFigure 8 shows the RMSE result of changing \u03b1 for the first case and figure 7 shows the results for the second case of selecting \u03b1. For \u03b1 = 1 the effect of predictor function of clusters is not considered in both cases. Thus, the RMSE result is as same as the basic matrix factorization for users-items rating matrix."}, {"heading": "4.4. Using Contents", "text": "So far we only use collaborative information in our model. It is supposed that applying users\u2019 profiles and items\u2019 features to the model will increase the clustering\naccuracy which will consequently improve the model\u2019s RMSE. As the datasets that we have already used do not provide any profiles for items and users, we use a smaller version of the dataset that is provided in the \u201dKDD Cup 2012- track1\u201d to experiment this effect. The dataset consists of a Chinese social network, Tencent Weibo, users\u2019 and items\u2019 profiles and list of recommended items to some users where they have accepted or rejected them (boolean values). Our Experiment shows that using users\u2019 and items\u2019 keywords in the model improves the accuracy by 1%."}, {"heading": "4.5. Using RCBMF and HCBMF", "text": "At the end, we use the validation results of CBMF to apply the effect of using multiple buckets on the model.\nAs noticed earlier, we are doing this in two cases: 1) Repeating the most accurate CBMF model changing the \u03b2 (RCBMF), and 2) Applying h most accurate models changing the \u03b2 (HCBMF). We set h = 3 in our experiment in both algorithms.\nAs figure 9 shows, we experience different improvement of accuracies for the models in different datasets. For instance, HCBMF shows significant improvement of RMSE for Epinions datasets, but on the other hand its result for Flixter is not better than simple CBMF. RCBMF has better result for Movielens100k, Netflix, and Flixter datasets. We tune the parameters (including W \u03b2j and \u03b1 ) using cross-validation in our experiment but we still employ same learning rate (\u03b3) and regularizing parameter (\u03bb) as CBMF."}, {"heading": "5. Related Works", "text": "(To\u0308scher et al., 2008) presents a neighborhood-aware matrix factorization which includes neighborhood information in the basic Matrix factorization. Their proposed algorithm computes three predictions for every user-item pair: a prediction rMFui based on basic matrix factorization; a prediction ruserui based on a userneighborhood model; and finally a prediction ritemui which is based on a item-neighborhood model. A combination of these three predictions is the final prediction of this algorithm. The rating prediction ruserui is computed as follows:\nruserui =\n\u2211 v\u2208Uj(u) c\nuser uv rvi\u2211\nv\u2208Uj(u) c user uv\nwhere UJ(u) denotes the set of J users with highest correlation to user u. These correlations are reached by counting the number of co-rating of users, and co-\nbeing-rated of items. The paper has a similar approach in computing ritemui . It is mentioned by (To\u0308scher et al., 2008) that using this neighborhood-aware improves the accuracy and even learning convergence. However, it is still sensitive to the choice of J . By increasing J the processing time of learning in each epoch will be increased linearly. On the other hand counting correlations between pairs of users and item would be costly in large datasets.\nKoren et al. in (Koren & Bell, 2011) proposes an itemitem model and a user-user model in a new factorization approach and then, as a fusion, use both in a single model to predict unknown ratings. They associate each user u with two vectors pu, zu \u2208 Rk and use the dot product of these two vectors to model relation between user u and v as wuv = p T u zv. Proposed user-user model is as follows:\nr\u0302ui = \u00b5+ bu+ bi+ |R(i)|\u22121/2 \u2211 v\u2208R(i) (r\u0302vi \u2212 bvi)pTu zv\nwhere R(i) contains all the users who rated item i. They employ same rationality for their proposed itemitem model. Finally, (Koren & Bell, 2011) proposes a fusion of item-item and user-user models in a single model to have the advantage of both sides\u2019 information.\n(Desrosiers & Karypis, 2011) presents a complete survey on neighborhood-based recommendation methods that covers many other extensions on matrix factorization. However, as noticed with more details in sections 1 and 3, our proposed model apply deeper similarities as well as more general interests in the predictor function. Hence, the CBMF is an improvement of itemitem/user-user fusion models.\nShahabi in (Shahabi et al., 2001) uses LSH methods to make a scalable web-base recommendation system (named Yoda). They customize LSH method to a Fuzzy Locality Sensitive Hashing method and use it to reduct the items dimension space. First a large wish list of items will be refined for a user, and then k-nearest-neighbor (KNN) is used in the reduced dimension space to find the closest items to the user\u2019s preferences in the item wish list. (Shahabi et al., 2001) reports improvement of accuracy with less cost against the basic KNN method. However, the KNN\u2019s accuracy is far away to even basic matrix factorization in practice (To\u0308scher & Jahrer, 2009)."}, {"heading": "6. Conclusion", "text": "Matrix Factorization is a popular method in Recommendation Systems showing promising results on ac-\ncuracy and complexity. In this paper we propose an extension of matrix factorization that use clustering paradigm to cluster similar users and items in several communities. We then establish the effect of these communities on the prediction model. More specifically, our proposed model is an improvement of itemitem/user-user fusion models where we apply deeper similarities and more generalized interests. As the datasets are usually huge in recommendation systems, we use a new technique for clustering which keeps our model practically efficient. Our experiments show very promising accuracy that to the best of our knowledge it outperforms all other proposed recommender methods. For instance, our proposed method\u2019s accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner\u2019s accuracy of 0.8567."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recommender systems are emerging technologies that nowadays can be found in many applications such as Amazon, Netflix, and so on. These systems help users find relevant information, recommendations, and their preferred items. Matrix Factorization is a popular method in Recommendation Systems showing promising results in accuracy and complexity. In this paper we propose an extension of matrix factorization that uses the clustering paradigm to cluster similar users and items in several communities. We then establish their effects on the prediction model then. To the best of our knowledge, our proposed model outperforms all other published recommender methods in accuracy and complexity. For instance, our proposed method\u2019s accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner\u2019s accuracy of 0.8567.", "creator": "LaTeX with hyperref package"}}}