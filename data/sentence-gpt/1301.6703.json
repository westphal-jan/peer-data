{"id": "1301.6703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Faithful Approximations of Belief Functions", "abstract": "A conceptual foundation for approximation of belief functions is proposed and investigated. It is based on the requirements of consistency and closeness. An optimal approximation is studied with a focus on the most important function of belief function. The goal is to determine whether or not the function is compatible with reality. However, in practice, the definition of a valid belief function is constrained to a set of questions, which are very difficult.\n\n\n\n\n\n\nIn addition, there are many different approaches to assessing the best validity of a belief function. For example, a model of an estimated probability of a true belief is a reasonable estimate. In the case of a true belief, a model of an estimated probability of a true belief is a good assumption because the likelihood of a true belief is good enough to account for the possibility of being true.\n\nA model of an estimated probability of a true belief is a good assumption because it allows for the possibility of being true. Thus, the model should allow an assumption that can be considered to be true because the probability of a true belief is a reasonable assumption. This assumption is not as likely as the model itself.\nOne possibility is for the possibility of being true: one possibility would have to prove that an actual value is false if it were to be true. The question of what to believe is the best of both worlds. For example, suppose if an element is false if an element is true if an element is true if an element is true if an element is true if an element is false if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true if an element is true", "histories": [["v1", "Wed, 23 Jan 2013 15:58:34 GMT  (366kb)", "http://arxiv.org/abs/1301.6703v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["david harmanec"], "accepted": false, "id": "1301.6703"}, "pdf": {"name": "1301.6703.pdf", "metadata": {"source": "CRF", "title": "Faithful Approximations of Belief Functions", "authors": ["David Harmanec"], "emails": [], "sections": [{"heading": null, "text": "A conceptual foundation for approximation of belief functions is proposed and investi gated. It is based on the requirements of consistency and closeness. An optimal ap proximation is studied. Unfortunately, the computation of the optimal approximation turns out to be intractable. Hence, various heuristic methods are proposed and exper imantally evaluated both in terms of their accuracy and in terms of the speed of com putation. These methods are compared to the earlier proposed approximations of belief functions.\n1 INTRODUCTION\nIt is now widely accepted fact that many situations involving uncertainty cannot be very well represented within classical probability framework. Ignorance, lack of time, or a small sample size are just a few of the reasons making it hard, if not impossible, to obtain reliable point probabilities. One may then ask why the various theories of imprecise probabilities are so rarely applied to solve practical problems compared to the classical probability theory. I believe, the explanation lies in the fact that, in general, using imprecise proba bilities has the computational complexity exponential in the size of the set of possible alternatives consid ered while using classical probabilities is only linear in the size of the universal set. Hence, the price for better accuracy or reliability seems too high. A pos sible solution of this dilemma is to use a subclass of imprecise probabilities that is expressive enough and at the same time inferences within this subclass are computationally tractable.\nOne possible candidate for such a class is the class of belief functions with the number of focal elements limited by a small constant (e.g., twice the size of the\noutcome space). Most algorithms involving belief func tions (or their various transformations) have compu tational complexity polynomial in the number of focal elements. However, some algorithms may produce a belief function with a larger number of focal elements than the number of focal elements of its input belief function(s). Moreover, the original belief function may itself have more focal elements than the prescribed limit. To solve this problem we need a method that would efficiently produce ideally the best, or at least a good, approximation of a given general belief func tion. Addressing the development of such methods is the topic of the current paper.\nThis work is by no means the first attempt to develop an approximation method for belief functions. Previ ous work can be divided into two streams. One type of proposal was to approximate a given belief function by either a probability or possibility measure [3, 4, 17]. While it might be useful method for utilizing some of the existing techniques from probability and possibil ity theories on input in the form of belief function(s), it hardly qualifies as a general approximation method. The reason is that the structure of both probability and possibility measures is very simple and hence too restrictive from the point of view of belief functions. In general, it is possible to allow a more general structure of the approximating belief function while preserving the same computational complexity. Approximating belief functions by probability measures also intro duces \"information\" that is not present in the original belief function 1. The other direction of the research on approximation of belief functions focused on the devel opment of a general approximation method. However, the proposals found in the literature [16, 1, 2] are not based on a well-founded approach to approximation. They are evaluated only on the basis of computer ex periments using questionable measure( s) of goodness\n1 Although it is possible to preserve the amount of infor mation carried by both the belief function and the proba bility measure [5].\n272 Harmanec\nof approximation. I discuss these proposals in detail in Section 4.1. I should also mention that Monte-Carlo techniques have been applied to reasoning with belief functions (8].\nAnother approach to reduction of computational com plexity found in the literature is to use a factorization of belief functions over a hypertree of sets of variables and a local computation algorithm to reason with the belief functions (7, I3, 10]. This approach is indepen dent of using approximations and could be used in combination with it.\n2 PRELIMINARIES\nLet X = { x1, x2, ... , xn} denote a finite and non empty universal set, usually referred to as a frame of discernment in the Dempster-Shafer theory [I2), and let P (X) denote the power set of X. A function Bel : P (X) -+ [0, I], which satisfies Bel (0) = 0, Bel (X) = I, and\nBel(A1 U . . . U AN) \ufffd :\ufffd::>-I)'fl+l Bel (n;e1A;),\nwhere the summation goes over all 0 of I \ufffd {I, ... , N} for all positive integers N and all {A;} ;:1 \ufffd P (X), is called a belief function.\nA function PI : P (X) -t [0, I] is called a plausibility function if it satisfies PI (0) = 0, Pl (X) = I, and\nPl (A1 n ... nAN) :S L:> -I)Ill+l Pl (U;e1A;)\nwhere the summation goes over all 0 of I \ufffd {I, ... , N} for all positive integers Nand all {A;};:1 s; P (X). A basic probability assignment is a function m : P (X) -t [0, I] such that m (0) = 0 and L:Ae\"P(X) m (A) = I. A subset A of X such that m (A) > 0 is called a focal element of m.\nIt is well known [I2) that there are one-to-one corre spondences between belief functions, plausibility func tions and basic probability assignments. Due to these correspondences we can freely use any of them in our argumentation. Moreover, any notion associated with any of the functions rn, Bel or Pl naturally translates to the remaining two by these correspondences. For example, we can talk about focal elements of a belief function meaning focal elements of its associated basic probability assignment.\nIn the algorithms in this paper, a basic probability assignment m is represented as a set of pairs of all focal elements of m and their corresponding value of basic probability assignment, i.e.,\nrn = {(A, m (A)) 10 #A<; X, rn (A) > 0} .\nIf a particular set B is not in any pair in rn then m (B) = 0.\n3 OPTIMAL APPROXIMATION\nIn this section, I look into the problem of finding an \"optimal\" or \"best\" approximation for a given belief function. A belief function has to satisfy two basic re quirements to be considered an approximation. First, it has to be a \"simple\" belief function. 2 In this pa per, a belief function is considered simple, if it has at most k focal elements, where k is a small positive integer. k is intentionally left unspecified as a param eter to the approximation method(s). Second, it has to be faithful. For an approximation to be faithful, the inferences made with it should be, in some sense, consistent with the inferences made with the original belief function. For an approximation to be considered best, it has to be the approximation that is the closest to the original. The next two subsections make these notions more precise.\n3.1 CONSISTENCY OF BELIEF FUNCTIONS\nThe faithfulness of the approximation is made precise by the notion of consistency.\nDefinition 1 Let Bel and Bel' denote two belief func tions on X. We say that Bel' is (weakly) consis tent with Bel if and only if Bel (A) \ufffd Bel' (A) for all A<; X.\nThe definition states that Bel' is consistent with Bel if it does not ascribe a larger belief than Bel to any subset of X, i.e., Bel is more precise - the interval [Bel' (A), Pl' (A)] contains the interval [Bel (A), Pl (A)] for each A<; X. It appears very nat ural to ask that an approximation is consistent with the original belief function, no matter what interpreta tion of belief functions is used. Some authors (e.g.,[4]) use the name (weak) inclusion instead of consistence for the same property. I prefer the name consistence as it is not tied to the random set interpretation of belief functions.\nAlthough the notion of consistence is very natural, it does not offer a guidance how to actually compute an approximation of a given belief function. The notion of strong consistency, though admittedly less intuitive, offers such a guidance.\n21 could consider \"approximations\" by arbitrarily com plex belief functions, but as I am interested in reducing computational complexity, I explicitly exclude complex be lief functions from the class of approximations.\nDefinition 2 Let Bel, Bel' denote two belief function on X and let m, m' denote their corresponding basic probability assignments. Moreover, let A1, A2, . . . , Ap (p 2: 1) denote all focal elements of m and let B1, B2, \u2022 \u2022 . , Bq (q 2 1) denote all focal elements of m'. We say that Bel' is strongly consistent with Bel if and only if there is a collection of numbers Wij, i = 1, 2 , ... , p, j = 1, 2, ... . ,q, such that\np\nL Wij = m' (Bi), for all j, i=l\nq L Wij = m (Ai), for all i, j=l\nand Bj i. Ai ==> Wii = 0.\nA belief function Bel' is strongly consistent with an other belief function Bel if the basic probability as signment value of every focal element of Bel' is a sum of fractions of basic probability assignment values of some focal elements of Bel that are its subsets. In other words, (some fractions of) basic probability val ues of focal elements of the \"original\" belief function are moved to a superset in the \"approximation\". As expected, the strong consistence implies consistence; the inverse does not hold in general (3].\nNow we can define precisely what is meant by \"ap proximation\" in this paper.\nDefinition 3 A belief function Bel' is called (strong) k-approximation of a given belief function Bel if Bel' is (strongly) consistent with Bel and Bel' has at most k focal elements.\n3.2 MEASURE OF CLOSENESS\nTo measure the \"closeness\" of an approximation, I use the function DFeel defined by\nDFBel (Bel') = L (Bel (A) -Bel' (A)) AEP(X)\nfor any belief function Bel' on X consistent with Bel. To obtain the best approximation we need to minimize this function over all (strong) k-approximations of Bel. That is, we try to minimize the sum of differences of a given belief function and the \"approximating\" belief function.\n3.3 AN ALGORITHM FOR COMPUTING AN OPTIMAL APPROXIMATION\nIn the rest of the paper, I investigate only strong k approximations of a given belief function. However,\nFaithful Approximations of Belief Functions 273\nif the following conjecture holds, finding optimal k approximation is the same thing as finding optimal strong k-approximation.\nConjecture 1 Let Bel denote a belief function on X. Any belief function Bel' that is consistent with Bel and minimizes DFeel among all the belief functions con sistent with Bel is also strongly consistent with Bel.\nOur goal is to find optimal strong k-approximation of a given belief function Bel. In general, there may be many optimal strong k-approximations. We want to find at least one. The theorem below suggests where to start looking.\nTheorem 1 Let Bel denote a belief function on X and let Bel' denote a strong k-approximation of Bel. Then there exists a strong k-approximation Bel\" of Bel such that its focal elements are also focal elements of Bel', (under the notation of Definition 2} w\ufffdel'' E {0, m (A;)} and DFeel (Bel') 2: DFeel (Bel\").\nAs a consequence of the above theorem, we only need to explore the partitions of the set of focal elements of the original belief function into k parts to find the opti mal strong k-approximation. The following algorithm is a formalization of this idea.\nAlgorithm 1 (Optimal approximation) Input: a basic probability assignment\nM = {(Bi, m (B;)) I i = 1, 2, ... ,s}, number of focal elements for the approximation k\nOutput: optimal approximation of M if s 5 k then\nreturn M else\nFind the first partition K = { H, P2, ... , Pk} of{1, 2, . . . , s} OPT= { (UiEP, Bi, \ufffdiEP, m (Bi)) I PtE K, l = 1, 2, . . . , k} while there is an unexamined partition of { 1, 2, ... , s} do\nFind the next partition K = { P1, P2, ... , Pk} of {1, 2, . .. , s} W M = { (UieP, BJ, \ufffdJEP, m (BJ )) I f} E K, l = 1, 2, . . . ,k} if DF (W M) < DF (OPT) then\nOPT=WM end if\nend while return OPT\nend if\nUnfortunately, going through all possible partitions is not computationally tractable.\n274 Harmanec\n4 HEURISTIC APPROXIMATIONS\nAs seen in the previous section, it is not computa tionally feasible to find an optimal approximation of a given belief function. As usual in such a situation, one needs to turn to a heuristic method that does not guar antee optimal results but empirically provides good ap proximations. In this section, I propose several such heuristic methods. I also overview and discuss pro posals from the literature for heuristic approximation methods. The experimental evaluation of these meth ods is the topic of the next section.\n4.1 PREVIOUS WORK\nThis subsection is an overview of the previous propos als for approximations of belief functions. As all the proposals are heuristic methods, I discuss them within a section on heuristic algorithms.\nVoorbraak [17) proposed Bayesian approximation of belief functions that approximates given belief func tion by a probability measure. It is well known [12) that a probability measure is a special type of be lief function with only singleton focal elements. For a given basic probability assignment m the basic prob ability assignment m' of the Bayesian approximation of misgiven by\nm'(A) = { LB!ACBm(B) !:C!C(;;X m(C)\u00b7JCJ' 0, IAI = 1 otherwise.\nObviously the Bayesian approximation has at most lXI focal elements. Unfortunately, the Bayesian approxi mation is generally not consistent with the original belief function, which is one of our fundamental re quirements.3 Bayesian approximation commutes with the Dempster rule of combination, which is its distin guishing characteristic.\nIn my opinion, the most well founded approximation method found in the literature is the consonant ap proximation of a belief function proposed by Dubois and Prade [4). Their goal is to find a maximal conso nant strongly consistent belief function that maximizes imprecision (i.e., the expression I; m (A) \u00b7 I AI). This is an NP-hard problem so they proposed a heuristic algorithm as a computationally tractable approxima tion. Like the Bayesian approximation, the consonant approximation has at most lXI focal elements. As mentioned above, the consonant approximation is a faithful approximation. Its main limitation is its re striction to consonant belief functions. See Section 5\n3This is true for any \"approximation\" by a probability measure, which also excludes using Smet's pignistic proba bility as an approximation (14].\nfor quantitative comparison with other faithful heuris tic methods.\nLowrance et al [9) appear to be the first to pro pose a general method, called summarization, for ap proximating belief functions. Let m denote a given basic probability assignment with s focal elements A1, A2, ... , A, ordered in such a way that m (A;) \ufffd m(Ai+1) for all i E {1,2, . . . ,s - 1 } . Then the sum marization approximation m' is given by { m(A), A=A;,iE{1, ... ,k-1} m' (A)= l:j=k m (Ai) A = Uj=k Ai\n0, otherwise (1)\nThis approximation is also strongly consistent with the original belief function.\nThe first systematic study of approximations of belief functions was done by Tessem [16). He also proposed a new method called k-l-x approximation. The k-l-x approximation works by preserving the original focal elements with the highest basic probability assignment values and renormalizing the values. Unfortunately, the resulting belief function is not, in general, consis tent with the original belief function. Tessem uses the maximum difference over all subsets of X between the pignistic probability [14] corresponding to the original and approximating belief function as the measure of closeness (or error measure in his terminology). The use of this measure is questionable. There are other possible 'representations' of a belief function by a prob ability measure (e.g., maximum entropy probability) and one can also use the belief function directly in decision making [6, 11, 15, 18].\nBauer [1, 2] did a second, and, to my knowledge, last systematic study of approximations of belief functions. He also proposed a new approximation method, called Dl. The Dl approximation works by keeping k- 1 focal elements from the original basic probability as signment and distributing the mass from the rest of the focal elements to these or to X. The distri bu tion is done either to the minimal supersets if any, or to minimal non-disjoint sets with larger cardinality if any (according to the proportion of their intersec tion with the distributed focal element), or to X as the last resort. Again, the Dl approximation method does not always produce a belief function that is con sistent with the original belief function. Bauer uses the main Tessem's error measure as well as two new ones. These measures suffer from the same drawbacks as the main Tessem 's error measure as they are also based on the pignistic probability. Moreover, the moti vation of these measures is dubious. The author seems to suggest that the user would make the decision on the basis of the highest pignistic probability. How-\never, this is not the standard decision making set up, when the user has a choice of actions and makes a decision based on utility expectations with respect to probabilities (belief functions) conditional on taking a particular action .\n4.2 PAIR APPROXIMATION\nThe first heuristic approximation method is a varia tion of the standard one-step-look-ahead (or greedy) heuristic. It reduces the number of focal elements of the current basic probability assignment by one at each step (starting from the original) until the desired number k of focal elements is reached. The reduction is done by merging the two focal elements, merging of which results in the smallest DFBel, where Bel is the belief function corresponding to the current basic probability assignment. The following result is used in the selection.\nProposition 1 Let m denote a basic probability as signment on X and A, B denote two different focal elements of m. Let m' denote a basic probability as signment obtained from m by merging A and B, i.e., { 0,\nm'(C)= m(A)+m(B) m(C)\nThen\nCE{A, B} C=AUB otherwise\nDFBel (Bel') = m (A) \u00b7 2IX -AI + m (B) \u00b7 2IX -BI (2) _ (m(A) +m(B)) \u00b721X-AuBI,\nwhere Bel and Bel' denote, respectively, the belief function corresponding to m and m' .\nLet C P (A, B) denote the expression on the left hand side of (2). The algorithm is presented below.\nAlgorithm 2 (Pair approximation) Input: a basic probability assignment\nM = {(B;, m(B,)) I i = 1,2, ... , s}, number of focal elements for the approximation k\nOutput: Pair approximation of M if s:::; k then\nreturn M else\nWM=M for (A, m(A)) E WM do\nc\ufffdM= minBI(B,m(B))E{WM -{ (A,m(A))}} C P (A, B)\nend for while \\W M\\ > k do\nB \u00b7 CWM = arg mmAI(A,m{A))EWM A C= arg min AI (A,m(A))E{W M -{ (B ,m(B))}} 0 P (A, B)\nFaithful Approximations of Belief Functions 275\nWM=(WM{(B, m(B)) , (O, m(C)) ,(BU O, m(B UC))})U {(B U C, m (B) +m (C)+ m (B U C))} recompute C\ufffd M\nend while return WM\nend if\n4.3 SINGLE APPROXIMATION\nThe Pair approximation is still quite computationally complex. It has the worst case computational com plexity 0 (s3). This is due to the fact that the cost of merging is associated with pairs of focal elements. To reduce the complexity one could try to associate a \"cost\" with the individual focal elements instead of their pairs, and, at each step, merge the two focal ele ments with the lowest \"cost\" . This is what the Single approximation is doing. Here, the amount of increase of DF by merging a focal element with X is taken as the \"cost\". From (2) we have\nCS (A)= m (A)\u00b7 21X-AI + m (X)\u00b7 21X-XI -(m (A)+ m (X)) . 21X-AuXJ = m (A) \u00b7 ( 2IX -AI - 1) .\nThe algorithm is presented below.\nAlgorithm 3 (Single approximation) Input: a basic probability assignment\nM = {(B,, m (B;)) I i = 1,2, ... , s}, number of focal elements for the approximation k\nOutput: Single approximation of M if s:::; k then\nreturn M else\nWM=M while IW Ml > k do\nB = arg minAI(A,m(A))EWM OS (A) C= argminAI(A,m(A))E{WM-{(B,m(B))}} OS (A) WM=(WM{(B, m (B)}, (C, m (C)}, (B u O, m (B u C)}})u {(B u C, m (B) +m (C)+ m (B u C))}\nend while return WM\nend if\n4.4 RATIO APPROXIMATION\nThe choice of X as the foci of the merging in the Sin gle approximation is somewhat arbitrary. The Ratio approximation goes to the other extreme and com putes the \"cost\" based on the assumption of merging with a superset containing just one extra focal element.\n276 Harmanec\nAgain, from (2) we have\nCR' (A)= m (A)\u00b7 2/X-A/ + m (A U {a}) . 2/X-AU{<>}I\n_ (m (A)+ m (X)), 2JX-AuAu {a}J 2/X/-1 = m (A)\u00b7 2iAI'\nAs we are interested only in the comparison of the values of C R' and C R' (A) < C R' (B) if and only if \ufffdliP < \ufffd\ufffd\ufffd) , it is simpler to use\nCR(A) = m(A) 2/A/\ninstead of CR'. The algorithm of the Ratio approxi mation is the same as the algorithm of the Single ap proximation with C R replacing C S .\nObviously, one could interpolate between the two ex tremes presented by the Single and Ratio approxima tions and select a superset with any number of focal elements between IAI + 1 and lXI as the basis of the heuristic \"cost\". One could even try to select it on the basis of the average size of focal elements in the original. Or, alternatively, one could try to drop using supersets at all. However, I have not pursued these ideas further (yet).\n4.5 LUMP APPROXIMATION\nIf even the speed of the Single or Ratio approxima tions is not acceptable, one can use similar idea to that of the summarization method given by (1) and merge all lXI + 1- k focal elements with the smallest \"cost\" in one step instead of doing it iteratively. I call the resulting method the Lump approximation. Let m denote a given basic probability assignment with s focal elements A1 , A2, . . . , A, ordered in such a way that CS (Ai) \ufffd CS (Ai+l) (or CR(A;) \ufffd CR(Ai+l) or any other \"cost\")4 for all i E {1, 2, .. . , s- 1 }. Then the Lump approximation m' is given by\n4.6 ITERATIVE APPROXIMATION\nThe last method I considered is a variation on the \"greatest descent\" theme. The method starts from a random partition of s focal elements into k parts. The partition corresponds to a belief function consistent with the original belief function the same way as in the case of the optimal approximation. It then iteratively\n4In my experiments I used C S as the \"cost\" in the Lump approximation.\nimproves (hence the name Iterative approximation) its current approximation by looking at the neighboring partition until no improvement is possible or the spec ified time or number of iterations is exceeded. 5 A par tition is neighboring if it can be obtained from the given partition by moving one element from one part to a different part. The exact algorithm follows.\nAlgorithm 4 (Iterative approximation) Input: a basic probability assignment\nM = {(B;,m (B;)) I i = 1, 2, ... , s }, number of focal elements for the approximation k, max.running time Time, max. number of iterations Numlt\nOutput: Pair approximation of M if s:::; k then\nreturn M else\nFind a random partition B!C = { P1 , P2, . . . , Pk} of {1, 2, ... ,s } BM = {(UjEP, Bi, L:iEP, m (Bj)) I P1 E B!C, l = 1, 2, . .. , k} Numlt = 1 while Time and Numlt is not exceeded do\nKM=BM !C!C = B!C for pE{1,2, . .. , k }, qE{1, 2, ... ,k},p#q, IPpl > 1 do\nfor AEPp do K. = ({P1, P2 , . . . , Pk}- {Pp , Pq}) U {Pp- {A} , Pq u {A}} WM= {(UjEP,Bi,L:jEP,m(Bj)) I P1 E!C, l= 1,2, ... , k} if DF (W M) < DF (BM) then\nBM=WM B!C = K.\nend if end for\nend for if DF(KM)=DF(BM) then\nreturnBM end if Numlt = Numlt + 1\nend while returnBM\nend if\n5 EXPERIMENTS\nIn this section I report on the experiments I conducted to evaluate the accuracy and speed of the computation\n51 did not limit the time or the number of iterations in the experiments.\nof the heuristic approximation methods I am propos ing in this paper as well as the Consonant and Sum marization approximations (the only earlier proposals that are faithful). The experimental framework was implemented in C++. Most of the experiments were conducted on a Sun Ultra 1 workstation running So laris 2.5, but some were also run on a 166Mhz Pentium MMX PC running Windows 95 to eliminate the pos sibility of dependency of the results on the computing platform.\nIn each run of an experiment, I have randomly gen erated 1000 basic probability assignments of a given specification and computed the various heuristic ap proximations recording their average scaled D F from the original as well as the average actual time it took to compute them. The minimum and maximum D F of a consistent belief function depends on the original belief function. To get comparable results, I computed the optimal approximation and scaled the DF values of the individual heuristic approximation methods be tween the D F of the optimal approximation and the DF of the vacuous belief function (which is trivially consistent) for each original belief function. A huge limitation of this approach is the need to compute the optimal approximation. This can be done only for be lief functions with very small number of focal elements. I was able to do it for belief functions with up to 15 fo cal elements. In another set of experiments I scaled the D F values only by the D F of the vacuous belief func tion. Unlike Tessem ( 16] and Bauer (1, 2], I generated the basic probability assignments uniformly random as I see no compelling reason to do otherwise. To be able to compare the other approximation methods with the Consonant approximation, I used k = lXI in all the ex periments, as this requirement is hard-wired into the Consonant approximation. I did the experiments for lXI E {3 , 4, . . . , 8} and for a particular fixed lXI I gen erated originals with lXI + 1, lXI + 2, ... , 2IXI -1 focal elements. The experiments seem to suggest that the Summarization approximation method is the fastest one to compute but the least (or second least) accu rate. The Pair method is the most accurate but the second slowest after the Iterative method. The Itera tive method is the only clearly dominated method as for larger number of focal elements it both takes the longest time to compute6 and still is the least (or sec ond least) accurate. A sam pie of typical results of the experiments can be found at Figure 1 and Table 1.\n60f course, this could be improved by restricting the number of iterations or allocated time. But that would mean further reducing the accuracy of the method.\n6 CONCLUSIONS\nIn this paper, I propose a foundation for approxima tions of belief functions. Besides simplicity, consis tency with the original and closeness to the original are viewed as basic requirements of an approximation. An algorithm for computing optimal approximation is developed. As the algorithm is not computationally feasible, I suggest several heuristic methods that can be used in practical situations. I also report the results of an experimental evaluation of several heuristic ap proximation methods both suggested in this paper and found in the literature. The results show that the Pair approximation is on average the closest to the original from the methods considered and the Summarization approximation of Lowrance et a! (9] takes the shortest time to compute. Somewhat surprisingly and contrary to the previous studies ( 16, 1, 2] the Consonant approx imation of Dubois and Prade (4] does not perform as badly as one might expect. It is my hope that the re sults presented here will help to make belief functions a practical alternative to precise probability as a tool for dealing with uncertainty.\nAcknowledgments\nThe work on this paper has been supported by a Strategic Research Grant No. RP960351 from the Na tional Science and Technology Board and the Ministry of Education, Singapore.\nReferences\n(1] M. Bauer. Approximations for decision making in the Dempster-Shafer theory of evidence. In E. Horwitz and F. V. Jensen, editors, Proceed ings of the Twelfth Annual Conference on Uncer-\n278 Hannanec\ntainty in Artificial Intelligence (UAI-96}, pages 73-80, San Francisco, California, 1996. Morgan Kaufmann.\n[2] M. Bauer. Approximation algorithms and deci sion making in the Dempster-Shafer theory of ev idence- An empirical study. International Jour nal of Approximate Reasoning, 17{2-3):217-237, 1997.\n[3] D. Dubois and H. Prade. A set-theoretic view of belief functions: Logical operations and approx imations by fuzzy sets. International Journal of General Systems, 12:193-226, 1986.\n[4] D. Dubois and H. Prade. Consonant approxima tions of belief functions. International Journal of Approximate Reasoning, 4{5-6):419-449, 1990.\n[5] D. Harmanec and G. J. Klir. On information preserving transformations. International Journal of General Systems, 26{3):265-290, 1997.\n[6] J .-Y. Jaffray. Dynamic decision making with be lief functions. In Yager et a!. [19], chapter 15, pages 331-352.\n[7] A. Kong. Multivariate belief functions and graph ical models. PhD thesis, Department of Statistics, Harvard University, 1986.\n[8] V. Y . Kreinovich, A. Bernat, W. Borrett, Y . Mariscal, and E. Villa. Monte-Carlo meth ods make Dempster-Shafer formalism feasible. In Yager et a!. [19], chapter 9, pages 175-191.\n[9] J. D. Lowrance, T. D. Garvey, and T. M. Strat. A framework for evidential-reasoning systems. In T. Kehler, S. Rosenschein, R. Filman, and P. F. Patel-Schneider, editors, Proceedings of the 5th National Conference of the American Association for Artificial Intelligence ( AAAI-86}, volume 2, pages 896-903. AAAI, 1986.\n[10] K. Mellouli. On the propagation of beliefs in net works using the Dempster-Shafer theory of evi dence. PhD thesis, School of Business, University of Kansas, 1987.\n[11] H. T. Nguyen and E. A. Walker. On decision making using belief functions. In Yager et a!. [19], chapter 14, pages 311-330.\n[12] G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, Princeton, New Jer sey, 1976.\n[13] P. P. Shenoy and G. Shafer. Propagating belief functions with local computations. IEEE Expert, 1:43-52, 1986.\n[14] P. Smets. Constructing the pignistic probability function in a context of uncertainty. In M. Hen rion, R. D. Shachter, and J. F. Lemmer, editors, Uncertainty in Artificial Intelligence, volume 5, pages 29-39. Elsevier Science Publishers, 1990.\n[15] T. M. Strat. Decision analysis using belief func tions. In Yager et a!. [19], chapter 13, pages 275- 309.\n[16] B. Tessem. Approximations for efficient compu tation in the theory of evidence. Artificial Intel ligence, 61(2):315-329, 1993.\n[17] F. Voorbraak. A computationally efficient ap proximation of Dempster-Shafer theory. Interna tional Journal of Man-Machine Studies, 30:525- 536, 1989.\n[18] P. Walley. Statistical Reasoning With Imprecise Probabilities. Chapman and Hall, New York, 1991.\n[19] R. R. Yager, M. Fedrizzi, and J. Kacprzyk, ed itors. Advances In The Dempster-Shafer The ory Of Evidence. Wiley Professional Computing. John Wiley & Sons, New York, 1994."}], "references": [{"title": "On the propagation of beliefs in net\u00ad works using the Dempster-Shafer theory of evi\u00ad dence", "author": ["K. Mellouli"], "venue": "PhD thesis, School of Business, University of Kansas,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1987}, {"title": "A Mathematical Theory of Evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1976}, {"title": "Propagating belief functions with local computations", "author": ["P.P. Shenoy", "G. Shafer"], "venue": "IEEE Expert,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1986}, {"title": "Constructing the pignistic probability function in a context of uncertainty", "author": ["P. Smets"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1990}, {"title": "Approximations for efficient compu\u00ad tation in the theory of evidence", "author": ["B. Tessem"], "venue": "Artificial Intel\u00ad ligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Statistical Reasoning With Imprecise Probabilities", "author": ["P. Walley"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}], "referenceMentions": [], "year": 2011, "abstractText": "A conceptual foundation for approximation of belief functions is proposed and investi\u00ad gated. It is based on the requirements of consistency and closeness. An optimal ap\u00ad proximation is studied. Unfortunately, the computation of the optimal approximation turns out to be intractable. Hence, various heuristic methods are proposed and exper\u00ad imantally evaluated both in terms of their accuracy and in terms of the speed of com\u00ad putation. These methods are compared to the earlier proposed approximations of belief functions.", "creator": "pdftk 1.41 - www.pdftk.com"}}}