{"id": "1301.6721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Learning Finite-State Controllers for Partially Observable Environments", "abstract": "Reactive (memoryless) policies are sufficient in completely observable Markov decision processes (MDPs), but some kind of memory is usually necessary for optimal control of a partially observable MDP. Policies with finite memory can be represented as finite-state automata. In this paper, we extend Baird and Moore's VAPS algorithm to the problem of learning general finite-state automata.\n\n\n\nIn the original paper, the VAPS algorithm was designed to predict a large number of specific instances of memory. In addition, in the original paper, a number of new instances of memory were identified. These are generally associated with an increase in the number of different memory types. These are not necessarily significant, as the new VAP algorithm does not change the model and therefore results in more information about memory and its performance.\nThe VAPS algorithm provides a set of invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invariant invari", "histories": [["v1", "Wed, 23 Jan 2013 15:59:46 GMT  (381kb)", "http://arxiv.org/abs/1301.6721v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI cs.SY", "authors": ["nicolas meuleau", "leonid peshkin", "kee-eung kim", "leslie pack kaelbling"], "accepted": false, "id": "1301.6721"}, "pdf": {"name": "1301.6721.pdf", "metadata": {"source": "CRF", "title": "Learning Finite-State Controllers for Partially Observable Environments", "authors": ["Nicolas Meuleau", "Leonid Peshkin", "Leslie Pack Kaelbling"], "emails": ["@cs.brown.edu"], "sections": [{"heading": null, "text": "Reactive (memoryless) policies are sufficient in completely observable Markov decision pro cesses (MDPs), but some kind of memory is usually necessary for optimal control of a par tially observable MDP. Policies with finite mem ory can be represented as finite-state automata. In this paper, we extend Baird and Moore's YAPS algorithm to the problem of learning gen eral finite-state automata. Because it performs stochastic gradient descent, this algorithm can be shown to converge to a locally optimal finite state controller. We provide the details of the algorithm and then consider the question of un der what conditions stochastic gradient descent will outperform exact gradient descent. We con clude with empirical results comparing the per formance of stochastic and exact gradient de scent, and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step.\n1 INTRODUCTION\nLearning an optimal policy in a large partially observable environment is a recurrent problem in many application do mains of AI. However, there is no known technique that scales up well to increasing size and difficulty of the prob lem. This situation is due in part to the fact that plan ning in partially observable environments is itself a diffi cult task, hence learning to plan cannot be much easier. The partially observable Markov decision process (POMDP) provides a formal framework for studying these problems [1, 27, 28, 7, II, 6, 16]. The difficulty of planning in par tially observable environments is illustrated by the fact that the optimal policy of a POMDP may use the complete pre vious history of the system (i.e., the whole sequence of ob servations, actions and rewards since time 0) to determine\nthe next action to perform. Therefore, we need an infinite memory if we want to act optimally over an infinite hori zon.\nA general way to represent policies is in the form of state automata, or as we will call them, policy graphs. Ev ery policy has a representation in the form of a (possi bly infinite) policy graph. A priori, the optimal solution of a POMDP may well be an infinite policy graph. How ever, because of evident computational limits, we may re duce the search to policies representable as finite policy graphs. Many existing algorithms for learning to plan in POMDPs rely on a similar assumption. For instance, some researchers [14, 3, 32] try to learn memoryless (or re active) policies, McCallum's learning algorithm [19, 20] uses a finite-horizon memory, Wiering and Schmidhuber's HQL [31]learns finite sequences of reactive policies using an implicit memory of some of the previous observations, and Peshkin et al. [23] look for optimal finite-external memory policies. All these finite-memory architectures correspond to finite policy graphs with a particular struc ture in each case (i.e. not every node-transition and choice of action is possible in the graph). 1\nMost previous examples of search in the finite policy graph-space [25, 8, 9, 10] use the criterion off-optimality: they search for a finite graph whose value is less than f from the value of the optimal-Bayesian-solution. There fore, they need to work explicitly in the continuous space of belief functions, which is a cumbersome and sometimes intractable process. Another approach uses EM to find a finite controller that is optimal over a finite horizon [12].\nIn a companion paper [22], we proposed to solve problems with a very large state-space by fixing the size of the policy graph and trying to find the best graph of this size. We may then hope to find a graph-size that realizes a good com promise between the quality of the solution and the time required for finding it. This approach allows by-passing\n1 Note that, even though they can remember only a finite num ber of events, general (unconstrained) finite policy graphs can re member events arbitrarily far in the past.\n428 Meuleau, Peshkin, Kim, and Kaelbling\nthe belief-state space, and performing all the computation in a discrete setting, like in completely observable Markov decision processes (MDPs) [13, 24].2 However, the algo rithms do not provide any evaluation of the quality of the solution produced relative to the optimal performance.\nAs we showed in the companion paper [22], finding the best finite policy graph of a given size is NP-hard. However, some classical optimization techniques such as branch-and bound search and gradient descent can be accelerated using previous knowledge about the structure of the problem at hand and its optimal solution. Despite this leverage, these techniques do not escape enumerating the set of all states of the POMDP at least once per iteration (they are at least in O(ISI), where Sis the state-space of the POMDP). Hence, they cannot be applied to problems with a very large num ber of states, such as combinatorial problems where the state of the process is a vector of several state-features, and, therefore, the number of states is exponential in the num ber of features. Moreover, they require a complete initial knowledge of the parameters of the POMDP, i.e., they can not be used to learn a policy without first learning a model of the environment.\nDirect (model-free) learning of a policy during a (possi bly simulated) interaction with the process is becoming a classical technique for planning in very large state-spaces [ 4, 17, 30]. The idea is to perform stochastic gradient de scent by sampling state transitions and rewards during the experience. Because we sample only possible (and even reasonably probable) trajectories, the algorithm may be much more efficient than an exact method that enumerates every trajectory, including impossible and very low proba bility ones. This is the principle at the basis of most suc cessful application of reinforcement learning (RL) to real world problems.\nIn this paper, we propose a model-free algorithm for learn ing general finite policy graphs of a given size. This algo rithm can be used to learn finite-memory policies in some environments with a large number of states. As it is per forming stochastic gradient descent in the parameters of the policy graph, it is ensured to converge to a local op timum. It is basically an extension of Baird and Moore's YAPS algorithm [3] for learning simple reactive policies. This constitutes a significant improvement to the original VA PS, since the restriction to reactive policies is a severe handicap in most partially observable domains. 3\nThe paper is organized as follows. First, we give a quick\n'However the optimality criterion used is the same as in the Bayesian approach, i.e., the expected discounted cumulative re wards, the expectation being relative to the prior belief on the states\n3Singh et al. [26] showed that stochastic reactive policies can perform arbitrarily better than deterministic ones. However, it is also proven that the best stochastic reactive policy can be arbitrar ily worse that the optimal memory-based policy.\nintroduction to POMDPs and policy graphs. Second, we de velop the formalism of Baird and Moore's YAPS algorithm in the general framework of finite policy graphs. This rep resents the main contribution of the paper. Then we discuss the conditions under which stochastic gradient descent can outperform exact gradient descent (which is possible only when the problem is known in advance). Finally, we use the pole-balancing problem to show that our algorithm can solve difficult real-world problems with limited observabil ity of the state of the system.\n2 POMDPs AND FINITE POLICY GRAPH\n2.1 POMDPs\nA partially observable Markov decision process (POMDP) is defined as a tuple (S, 0, A, B, T, R) where:\n\u2022 S is the (finite) set of states;\n\u2022 0 is the (finite) set of observations;\n\u2022 A is the (finite) set of actions;\n\u2022 B(s, o) = Pr(o' = o Is' = s) for all t;\n\u2022 T(s, a, s') = Pr(s'+1 = s' Is' = s, a' =a) for all t;\n\u2022 r' = R(s, a, s') if s' = s, a' =a and s<+l = s', for all t.\nThe underlying Markov decision process (MDP) (S, A, T, R) is optimized in the following way [13, 24]: given an initial state s0, the aim is to maximize the expected discounted cumulative reward\nwhere \"/ E [0, 1) is the discount factor. The optimal solu tion is a mapping p.* : S -t A. It is a remarkable property of MDPs that there exists an optimal policy that always exe cutes the same action in the same state. Unfortunately, this policy can not be used in the partially observable frame work, because of the residual uncertainty on the current state of the process.\nIn a POMDP, a policy is a rule specifying the action to per form at each time step as a function of the whole previous history, i.e., the complete sequence of observation-action pairs since time 0. A particular kind of policy, the so-called reactive policies (RPs), condition the choice of the next ac tion only on the last observation. Thus, they can be rep resented as mappings p. : 0 -t A. Given a probability\ndistribution rr0 over the starting state, each policy f.1- (reac tive or not) realizes an expected cumulative reward:\n(l)\nThe classical-Bayesian-approach allows us to determine the policy that maximizes this value. It is based on up dating the state distribution (or belief) at each time step, depending on the most recent observations [7, 11, 6, 16). The problem is reformulated as a new MDP using belief states instead of the original states. Generally, the optimal solution is not a reactive policy. It is a sophisticated behav ior, with optimal balance between exploration and exploita tion. Unfortunately, the Bayesian calculation is highly in tractable as it searches the continuous space of beliefs and considers every possible sequence of observations.\n2.2 FINITE POLICY GRAPHS\nA policy graph for a given POMDP is a graph where the nodes are labeled with actions a E A, the arcs are labeled with observations o E 0, and there is one and only one arc emanating from each node for each possible observa tion. When the system is in a certain node, it executes the action associated with this node. This implies a state transi tion in the POMDP and eventually a new observation (which depends on the arrival state of the underlying MDP). This observation itself conditions a transition in the policy graph to the destination node of the arc associated with the new observation. We are interested in stochastic policy graphs where action-choices and node-transitions are probabilis tic. We will use the following notation:\n\u2022 N is the set of nodes of the graph,\n\u2022 n' E N is the current node at time t,\n\u2022 ,P( n, a) is the probability of choosing action a in node n E N:\n,P(n,a) \ufffdr Pr(a' =a In'= n), for all t,\n\u2022 TJ( n, o, n') is the probability of moving from node n E N to node n' E N, after observation o E 0:\nry(n, o, n') \ufffdf Pr(n'+1 = n' In'= n II o'+1 = o), for allt,\n\u2022 ry0 is the probability distribution of the initial node n\u00b0 conditioned on the first observation o0:\nFigure 1 illustrates the functioning of policy graphs in POMDPs.\nEvery policy has a representation as a possibly infinite pol icy graph. A policy that chooses a different action for each possible previous history will be represented by an infinite tree with a branch for each possible history. Reactive poli cies correspond to a special kind of finite policy graph with as many nodes as there are observations in the POMDP, and whose structure is fixed. Other finite-memory architectures such as HQL's finite RP-sequences and finite external mem ory policies also correspond to finite policy graph with spe cial structural constraints. 4\n2.3 FINDING AN OPTIMAL POLICY GRAPH\nThe problem of finding the optimal policy graph of a given size is studied in a companion paper [22]. The principle of this study is to exploit the Markov property of the associa tion between POMDPs and finite policy graphs. It ends up with the proposition of algorithms that scale up relatively well with respect to the size of the problem, but that are more sensitive to the size of the policy graph.\nAlthough these methods enable the solution of problems with up to 1000 states in a reasonable time, this approach is fundamentally limited .by the necessity to enumerate the complete set of states of the POMDP, at least once at each time step. Thus, they will fail to solve problems with ex ponentially many states, such as the huge combinatorial problems often met in the real world. Moreover, these al gorithms are basically planning algorithms, i.e., they re quire complete and accurate preliminary knowledge of the POMDP parameters, and they cannot be used to learn the policy on-line.\nA possible solution to overcome the curse of dimension ality of the state space consists of having a direct (model free) reinforcement learning (RL) algorithm learn a policy during a (possibly simulated) interaction with the process [4, 17, 30]. We then concentrate the computation on the\n4ln the most general definition of finite-state automata, the next node depends not only on the previous node and observation, but also on the last action (this is the case, for instance, of graphs representing external-memory policies). The algorithm presented here can easily be generalized to this framework.\n430 Meuleau, Peshkin, Kim, and Kaelbling\nmost interesting parts of the state-space, neglecting highly unlikely state-transitions. The rest of the paper presents a model-free algorithm for learning finite-state controllers of a given size. It can be used in a simulated experience pro tocol, as well as for learning in a direct interaction with the real environment or process.\n3 STOCHASTIC GRADIENT DESCENT IN GENERAL FINITE POLICY\nGRAPHS\nBaird and Moore's YAPS algorithm [3] learns a reactive pol icy through trial-based interaction with the process to be optimized. It is based on performing stochastic gradient de scent of a general error measure, and hence it can be tuned to converge to a local optimum of this error m\ufffdasure with probability 1. The formalism proposed encompasses any kind of error ranging from the classical Belman-residual often used in Markovian environments (i.e., TD(O)), to the TD( 1) error that uses the sum of all the rewards received during the trial [29, 30]. This is the origin of the name of the algorithm: value (TD(O)) and policy (TD(l)) search. Others possible errors include those used in SARSA and in advantage learning.\nDespite this robustness to the type of error used, YAPS is limited because it learns only memory less policies. Hence it will not be effective in many partially observable envi ronments. In this paper, we extend it so that the structure of the policy graph does not have to be completely fixed in advance, as is the case with RPs. More precisely, our algo rithm can learn a general finite policy graph of a given size, possibly with simple structural constraints. We now de velop the formalism of YAPS in general finite policy graphs. The presentation directly follows that of Baird and Moore [3].\n3.1 ERROR FUNCTIONS\nFirst we assume that the problem is a goal-achievement task, i.e., that there exists an absorbing goal-state that the system must reach as fast as possible. We also assume that the goal-state is associated with a unique observation oa that no other state produces (the system always knows with certainty when it has reached its goal). Then we can write our high-level optimality criterion as an expectation over trajectories:\n00\nB\ufffd = L L Pr(s l1r0,p)c(s) , T=O iEST\n(2)\nwhere Sr is the set of all experience sequences that termi nate at timeT, i.e.,\n- ( 0 0 0 0 t t t t T T T T) s = o , n , a , r , ... , o, n , a , r , ... , o , n , a , r ,\nand c( s) represents the total error associated with the se quence s. 5 The total error c must be additively separable, so that\nT c(s) = :E e(T(s,t)) for all s E Sr,\nt=O where e ( s) is an instantaneous error function associated with each (finite) sequence prefix\n( 0 0 0 0 t t t t) s = o , n ,a , r , ... , o, n , a , r ( ot being any observation, not necessarily the goal observation oa), and T(s, t) represents the sequences trun cated after time t. We will denote by St the set of all se quence prefixes of length t (St C St ). There are many possibilities for defining the immediate er ror e, including the squared Bellman residual, the error used in SARSA and the error of advantage learning (see [2, 3] for details). These three definitions make complete sense in Markovian environments only. However, they can be used for POMDPs in an approximate approach (for in stance, we can use the error of SARSA to learn RPs in POMDPs). The algorithm still finds a local optimum of the error, but nothing guarantees that it will correspond to an optimal policy. The immediate error\nfor all s ESt, induces a TD(l) search adapted to non-Markovian environ ment. Notably, if we use this error, then the two criteria of optimality of a policy, equation (1) and equation (2), are equal (with opposite signs however). Therefore, it will be rational to try to minimize BJJ withe equal to epolicy\u00b7\n3.2 STOCHASTIC GRADIENT DESCENT\nIn a general framework, 1/J and 1J are represented as para metric functions with weights { Wk}. The objective func tion BJJ can be re-written as\n00\nBJJ = L L Pr(s l1r0,p)e(s) t=O sESt\nHence we have\n5Note that if we are learning an RP as in the initial VAPS al gorithm, then n' is completely detennined by o' and thus can be omitted in sequence s.\nfor each weight wk. The partial derivative of e is in general easy to calculate. In the case of \u20acpolicy it is always 0. The only difficulty is then to differentiate Pr(s Jrr0, J.J.). For all s E S1 we have:\nt Pr(s Jrr0,J1.) = IT Pr(d Jrr0, T(s, j - 1 ))\nj=O j-1 . j j j . 0 . . j 1J(n ,d,n )1/!(n , a ) Pr(r-ljrr , T(s,J-1), o1,a ),\nwith the conventions T( s, -1) = 0 and 1J( n -I, o0, n\u00b0) = 11o(0o, no). If 1/!(n, a) > 0 for all (n, a), 1J(n, o,n') > 0 for all (n,o, n') and 11\u00b0(o,n) > 0 for all (o, n) then it can be shown that 6\n{) 00 [{) &B\ufffd = L L Pr(s Jrr0,J.J.) &e(s) Wk t=OsESt Wk t\n+ e(s) L {)\ufffd In 1/J(ni, ai) (J) j=O k\n+e(s) \ufffd {)\ufffdk ln1J(ni-l, d,ni)] Therefore, stochastic gradient descent of the error can be performed by repeating several trials of interaction with the process. Each experienced trial of length T provides one sample of s E S, for each t \ufffd T, which is used to estimate the expectation over s in the above equation. Of course these samples are not independent, but it does not intro duce any bias since we sum the different estimates. During each trial, the weights are kept constant and the approxi mate gradients of the error at each time t\n/ e(s) + e(s) t (/ In 1/J(ni-I, ai-1) Wk j=l Wk + {):k In 1J(ni-I, d, ni))\nare accumulated. Weights are updated at the end of each trial, using the sum of these immediate gradients. An in cremental implementation of the algorithm can be obtained by using, at every step t, the following update rules:\n6If this condition on .P and '7 is not satisfied, then there exist zero probability trajectories that have a non-zero contribution to the gradient [21, 15].\nLearning Finite-State Controllers 431\nh - ( 0 0 0 0 t t t ') th w ere St - o , n , a , r , ... , o , , n , a , r represents e experience prefix at time t, a is the step-size parameter (or learning rate), and T! and r: are the trace sassociated with weight k in the representation of 1/! and 1], respectively. The complete policy-update performed at the end of the trial is then given by\nT D.wk = L D.wk(t) ,\nt=O\n8 where T is the length of the trial. Note that the traces Tt { t) and r: ( t) are independent of the immediate error e used. They only depend on the way the policy-graph pa rameters vary with the weight wk, i.e., on the representa tion chosen for these parameters. The main novelty of our algorithm (compared to the original YAPS) is the use of a second trace (T\ufffd ), which is analogous to the original trace (T11>), but summarizes the node-transition executed during the trial instead of the action-choices.\n3.3 EXAMPLES\nIf we use look-up tables to store the parameters of the pol icy graphs, then there is one weight, denoted 1/!n,a, for each possible (n, a), one weight 1Jn,o,n' for each possible (n, o, n'). and one weight 1]\ufffd n for each (o, n), such that 1/!(n, a)= 1/!n,a.1J(n, o, n') =' 1]n,o,n' and 1]0(o, n) = 1J\ufffd,n\u00b7 Suppose also that we are using the immediate error epolicy. i.e., we are performing a TD(l) search. Then the contribu tion to the update of each weight at each time-step in the sequence can be expressed as:\nD-1/!n,a(t) ttNna (t) -a1 r __:..:.!.::\u2022 '-'--'-1/Jn,a '\nt t _N-'n-'-', o-\", nc... ' _,_{ t-'-) -a1 r T/n,o,n' t ,N\ufffdn -a1 r -0'- , TJo,n\nwhere Nn,a (t) is the number of times that action a has been executed in node nat timet, Nn,o,n\u2022(t) is the number of times that we moved from node n to node n' after observa tion o between time 0 and t, and N\u00b0(o, n) = 1 if o0 = o and n\u00b0 = n, and N\u00b0(o, n) = 0 otherwise.\nDespite its simplicity, the look-up table representation has several drawbacks. First, the weights Wk represent proba bilities, and thus they are subject to constraints. As a matter of fact, nothing guarantees that the probabilities will still belong to (0, 1] and sum to 1 if we apply the update rule de scribed above. A classical solution to this problem involves projecting the gradient on the simplex before applying it. However it does not eradicate the second drawback of the look-up table representation, i.e., there is still no guarantee that 1/!(n, a) > 0 and 7J(n, o, n') > 0 for all n, a, o, n'. Hence, the derivative of B\ufffd may not be equal to equation\n432 Meuleau, Peshkin, Kim, and Kaelbling\n(3) in all the points of the policy-graph space. Studying how to express the gradient in such cases falls beyond the scope of this paper (see [21, 15]).\nIn our experiments, we use the soft-max function (or Boltz mann law) to represent the parameters of the graphs. In this case, the weights wk are \"Q-values\" Q'i' ( n, a), Q\ufffd ( n, o, n') and Q\ufffd0 ( o, n) such that\n1/>(n,a)\nry(n, o, n')\n'<' eQ\"(n,a')/8 ' L.Ja'EA eQ\"(n,o,n')/B\n'<' Q\u2022(n o n\")/8 ' L .. m\"ENe ''\neQ\" 0 (o,n)/8\nwhere (J is a temperature parameter. Although it compli cates the calculation of the gradient slightly, this represen tation avoids both problems of look-up tables: the Q-values can take any real values, and the induced policy never gives probability 0 to any choice. The use of the Boltzmann law may strongly modify the shape of the error function with respect to the weights Wk. Hence, it influences the perfor mance of gradient algorithms such as YAPS. It is difficult to say a priori if its influence will be beneficial or negative, for a given problem.\n3.4 VARIANTS AND REMARKS\nIt is straightforward to extend the algorithm so that it han dles simple constraints on the policy graph. If we con straint the graph to represent a RP, then the algorithm is equivalent to Baird and Moore's original YAPS. Consider as another example the finite external-memory architecture used by Peshkin et al. [23]. There are two ways to model this architecture: either we augment the POMDP state-, observation- and action-spaces but still use a RP, or we leave the POMDP unchanged and use a more complex pol icy graph than a simple RP (this graph contains 2\u00a3101 nodes, where L is the number of external memory bits). In the first case, the probability of changing the content of the memory is represented in 1/>, in the second case it is rep resented in ry. Our results are coherent in the sense that, as the update rule uses 1/> and ry in a completely similar way, the algorithm will be the same whatever the interpretation chosen. Another possibility is to learn finite RP-sequences such as in HQL, either using epolicy\u2022 or defining a new error function eHQL based on the HQ-values of the algorithm. In the first case, we will converge to an RP-sequence which is locally optimal in the sense of the expected total reward (1). In the second, we will find a local minimum of the er ror, but it may not correspond to a policy that maximizes (even locally) the expected discounted reward.\nAnother question is how to treat discounted problems where there is no goal state, and, therefore, no natural no-\nI uj\nFigure 2: The load/unload problem with 8 locations: the agent starts in the \"Unload\" location (U) and receives a re ward each time it returns to this place after passing through the \"Load\" location (L). The problem is partially observ able because the agent cannot distinguish the different lo cations in between Load and Unload, and because it can not perceive if it is loaded or not CISI = 14, 101 = 3 and IAI = 2).\ntion of trial (the so-called maintainance tasks). One possi bility for dealing with discounted maintainance task is the following: at each time step we execute an independent random drawing to determine if we terminate the trial. We set the probability to end the trial to be constant and equal to ( 1 - 1) and we do not discount the rewards received during the trial, i.e., we use\n1 def t e policy == - r Then the policy (graph) that maximizes B\ufffd is also an op timal policy graph in the usual sense (equation (1)). This trick allows TD(l) learning of maintainance tasks, but it is not adapted to other kinds of immediate error e. Baird and Moore argue that YAPS can be adapted to discounted POMDPs whatever the immediate error, but it is not clear to us how to do this without introducing a bias in the esti mates.\n4 NUMERICAL SIMULATIONS\nIn this section, we present the results of two experiments. The first aims at comparing an exact gradient algorithm [22] with the stochastic gradient approach of YAPS. The second shows that our algorithm can solve a moderately difficult real-world problem. In all the experiments, we used the immediate error epolicy and we initialized the pol icy graph with uniform distributions.\n4.1 COMPARISON WITH EXACT GRADIENT\nDESCENT\nA model-free learning algorithm such as YAPS may be used to learn a policy when we do not know all the parameters of the POMDP in advance. As explained in section 2.3, it is also useful when the problem is perfectly known in ad vance: the protocol of simulated experience allows opti mizing huge problems with sparse structure, by sampling only the probable trajectories, instead of considering all trajectories. It is interesting to look at the conditions un der which YAPS would be expected to outperform the exact algorithm.\nFirst, it is to be noted that the exact gradient calculation is\nvery sensitive to the size of the state space of the POMDP: each step of the computation has complexity at least in O(ISI). The influence of the size of Son YAPS is less clear: the complexity of updating the weight is independent of lSI. however, a bigger state-space would require (and in duce) longer experience trials. In practice, it has been very easy for us to build a problem with many states and few observations where YAPS completely outperforms the ex act gradient (in terms of real computing time). Therefore, the first rule is that YAPS scales up much better than ex act gradient to problems with big state-spaces. This is not surprising since handling big state-spaces was precisely the original motivation of this work.\nThe second important variable in our comparison is the dis count factor I\u00b7 In general, a bigger 1 helps (both exact and stochastic) gradient based algorithms because it increases the value function and thus makes the gradient steepest. However, 1 may have many other (contradictory) effects on the algorithms. In the case of YAPS, as the trials are ended with probability 1 -1, bigger 1 will make longer and hence more instructive trials. On another hand, the exact gradient calculation requires solving several Belman equa tions in the cross-product MDP (cf. section 2.3, [22)). This is done by successive approximation (or value iteration), which is very sensitive to I\u00b7 The bigger the 1, the more iterations needed to reach a given accuracy. There are then two opposite tendencies in both algorithms: increasing 1 could accelerate them as well as slow them down.\nTo clarify this point, we ran the exact gradient algorithm and YAPS on the simple load/unload problem presented in figure 2 (with 5 locations). The number of nodes of the graph is fixed to 2, which is the optimal number for this problem. We tried several values of 1 ranging from 0.9 to 0.995, and plotted the learning curves produced by both\nalgorithms. These learning curves represent the evolution of the performance of the current policy (expressed as a percentage of the optimal performance), as a function of the real-time spent learning (expressed in computer time ticks). Although very simple, load/unload provides an il lustration of the mechanism depicted above. With 1 = 0.9 the exact gradient clearly outperforms YAPS (cf. figure 3). When 1 increases, the difference between the algorithm vanishes. At 1 = 0.995 (figure 4), the two techniques are roughly equivalent. Beyond this point, YAPS dominates exact gradient descent. The conclusion of this experiments is that the execution time of both exact and stochastic gra dient descent do increase with 1\u00b7 but this increase is more dramatic in the case of exact gradient.\nAs a conclusion, stochastic gradient descent can outper form exact gradient in problems with large state space and large discount factor.\n4.2 EXPERIMENTS WITH POLE BALANCING\nWe ran a number of experiments with the pole balancing problem [30]. This famous problem is known to be solved by an RP, if the observation at each time-step is composed of four elements: the cart posit\\on x and speed x, and the pole angle\u00a2 and angular speed \u00a2. To measure the difficulty of the task and the performance of our algorithm, we used two different settings: a completely observable one where the four relevant variables x, x, () and iJ can be seen by the algorithms at each time-step, and a partially observable setting where both x and iJ are always hidden. We ran three different algorithms in both settings: SARSA, Baird and Moore's original YAPS (learning an RP) and our extension of YAPS allowing to learn policy graphs, varying the number of nodes of the graph. SARSA and the origi nal YAPS can be expected to succeed in the completely ob servable setting, and to fail in the partially observable one where there is no reactive policy that performs the task.\n434 Meuleau, Peshkin, Kim, and Kaelbling\nThese two algorithms differ radically. On one hand we use YAPS with the immediate error epolicy which makes it equivalent to TD(l), Baird and Moore would call this a pure policy search. On the other hand SARSA is basically a value-search in the line of TD(O). Our algorithm can be expected to succeed in both settings, provided that we use a sufficiently large policy graph, and that algorithm does not get stuck on a local optimum. Two nodes should be enough in the completely observable setting, since every reactive policy using only two actions (as it is the case here) can be represented by a two-node policy graph. In the partially observable framework, more nodes must be added to allow the algorithm memorize past observations.\nIn all experiments the discount factor 1 was set to . 99 and increased gradually as learning progressed. The learning rate a was optimized independently for each algorithm. The performance of the algorithm was measured by fixing the policy and executing 200 trials, measuring the length of each trial in terms of control decisions, and averaging these measures. The value intervals of cart position and pole po sition were partitioned into 6 and 3 unequal parts (smaller size of partition towards the center) in the completely ob servable setting, and into 8 and 6 parts in the partially ob servable setting, correspondingly. We were making deci sions at the rate of 50 Hz, meaning, for example, that the actual physical time of learning to balance a pole for 500 sequential ticks corresponds to 10 seconds of balancing. Other parameters of the cart and pole balancing problem were taken as described in the supplementary WWW page for [30].\nFigure 5 presents the learning curves obtained in the com pletely observable framework. The horizontal axis repre sents the number of trials, which corresponds to the num-\nber of times we have dropped the pole. The vertical axis represents the performance of the algorithm, measured as explained above. We see that:\n\u2022 SARSA learns much faster than the original YAPS, showing that value search is much more efficient that policy search for this control problem,\n\u2022 our extension of YAPS with 2-node policy graph learns slower than the original YAPS. This phenomenon can be explained by the fact that the space of 2-nodes pol icy graphs is bigger than the space of RPs.\nFigure 6 presents the results obtained in the partially ob servable framework. These results confirm our expecta tion that algorithms limited to reactive policy will fail. In contrast, our algorithm increases its performance gradually, showing that it is able to compensate the lack of observabil ity. The more nodes are given to the algorithm, the better it performs. It is also striking to see that the performance of the algorithm seems to improve by steps, which makes difficult to predict where learning will stop. Because of limited time, we could not continue the experiments be yond 500000 iterations so that we do not know if the per formance would continue to increase until the system may balance infinitely long. We are currently running this ex periment and the results will be shown in a forthcoming technical report [15). The most significant current result is that we can learn the structure of the policy graph that extracts some useful information contained in the string of past observations, to compensate, at least partially, for the lack of observability. Pole balancing is a widely accepted benchmark problem for dynamic system control and to the best of our knowledge it has not been learned with partial information.\n5 CONCLUSION\nWe have derived an extension of a general algorithm that enables it to Jearn policies using a memory. The basic prin ciple of this algorithm is to perform stochastic gradient de scent on finite-state controller parameters, which guaran tees local optimality of the solution produced. Moreover, we have led an experimental study of this approach, and compared it to classic (non-adaptive) algorithms in terms of execution time and learning speed. At last, we showed that our algorithm can solve a difficult problem such as pole balancing without having access to all the information usu ally required to solve it. Therefore, it is able to find the structure of the policy graph that extracts the useful infor mation contained in the sequence of past observations to compensate for the lack of observability at each time-step. We believe that this constitutes a significant achievement and proves that our algorithm can be efficient in some real world problems.\nReferences\n[ 1] K.J. Astrom. Optimal control of Markov decision pro cesses with incomplete state estimation. J. Math. An/. Appl., 10, 1965.\n[2] L.C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning: Proceedings of the Twelfth International Conference, San Francisco, CA, 1995. Morgan Kauf mann.\n[3] L.C. Baird and A.W. Moore. Gradient descent for general reinforcement learning. In Advances in Neu ral information Processing Systems, 12. MIT Press, Cambridge, MA, 1999.\n[4] D.P. Bertsekas and J.N. Tsitsiklis. Neuro-dynamic Programming. Athena, Belmont, MA, 1996.\n[5] C. Boutillier, T.L. Dean, and S. Hanks. Decision the oretic planning: structural assumptions and computa tional leverage. Journal of AI Research, To appear, 1999.\n[6] A.R. Cassandra. Exact and Approximate Algorithms for Partially Observable Markov Decision Processes. PhD thesis, Brown University, 1998.\n[7] A.R. Cassandra, L.P. Kaelbling, and M.L. Littman. Acting optimally in partially observable stochastic domains. In Proceedings of the Twelfth National Con ference on Artificial Intelligence, 1994.\n[8] E.A. Hansen. An improved policy iteration algorithm for partially observable MDPs. In Advances in Neu ral Information Processing Systems, 10. MIT Press, Cambridge, MA, 1997.\nLearning Finite-State Controllers 435\n[9] E.A. Hansen. Finite-Memory Control of Partially Observable Systems. PhD thesis, Department of Computer Science, University of Massachusetts at Amherst, 1998.\n[ 10] E.A. Hansen. Solving POMDPs by searching in pol icy space. In Proceedings of the Eighth Conference on Uncertainty in Arti.ficia/Jntelligence, pages 2 11-219, Madison, WI, 1998.\n[11] M. Hauskrecht. Planning and Control in Stochas tic Domains with Imperfect Information. PhD thesis, MIT, Cambridge, MA, 1997.\n[12] 0. Higelin. Optimal Control of Complex Structured Processes. PhD thesis, University of Caen, France, 1999.\n[13] R.A. Howard. Dynamic Programming and Markov Processes. MIT Press, Cambridge, 1960.\n[14] T. Jaakkola, S. Singh, and M.R. Jordan. Rein forcement learning algorithm for partially observable Markov problems. In Advances in Neural Informa tion Processing Systems, 7. MIT Press, Cambridge, MA, 1994.\n[15] L.P. Kaelbling, K.E. Kim, N. Meuleau, and L. Peshkin. Searching for finite-state POMDP controllers. Technical Report CS-99-06, Brown University, 1999.\n[16] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101, 1998.\n[17] L.P. Kaelbling, M.L. Littman, and A.W. Moore. Re inforcement learning: a survey. Journal of Artificial Intelligence Research, 4, 1996.\n[ 18] M.L. Littman. Memory less policies: Theoretica1 lim itations and practical results. In From Animals to Animats 3: Proceedings of the T hird International Conference on Simulation of Adaptive Behavior. MIT Press, Cambridge, MA, 1994.\n[19] R.A. McCallum. Overcoming incomplete perception with utile distinction memory. In The Proceedings of the Tenth International Machine Learning Confer ence, Amherst, MA, 1993.\n[20] R.A. McCallum. Reinforcement Learning with Selec tive Perception and Hidden State. PhD thesis, Univer sity of Rochester, Rochester, NY, 1995.\n[21] N. Meuleau. The importance of impossible trajecto ries in the VAPS algorithm. In preparation, 1999.\n436 Meuleau, Peshkin, Kim, and Kaelbling\n[22] N. Meuleau, K.E. Kim, L.P. Kaelbling, and A.R. Cas sandra. Solving POMDPs by searching the space of finite policies. Proceedings of the Fifteenth Confer ence on Uncertainty in Artificial Intelligence, To ap pear, 1999.\n[23] L. Peshkin, N. Meuleau, and L.P. Kaelbling. Learn ing policies with external memory. Proceedings of the Sixteenth International Conference on Machine Learning, To appear, 1999.\n[24] M.L. Puterman. Markov Decision Processes: Dis crete Stochastic Dynamic Programming. Wiley, New York, NY, 1994.\n[25] J.K. Satia and R.E. Lave. Markov decision processes with probabilistic observation of states. Management Science, 20(1): 1-13, 1973.\n[26] S. Singh, T. Jaakkola, and M.R. Jordan. Learn ing without state-estimation in partially observable Markovian decision processes. In Machine Learn ing: Proceedings of the Eleventh International Con ference. 1994.\n[27] R.D. Smallwood and E.J. Sondik. The optimal con trol of partially observable Markov decision processes over a finite horizon. Operations Research, 21:1071- 1098, 1973.\n[28] E.J. Sondik. The optimal control of partially observ able Markov decision processes over the infinite hori zon: Discounted costs. Operations Research, 26, 1978.\n[29] R.S. Sutton. Learning to predict by the method of temporal differences. Machine Learning, 3: 9-44, 1989.\n[30] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.\n[31] M. Wiering and J. Schmidhuber. HQ-Learning. Adap tive Behavior, 6(2):219-246, 1997.\n[32] R.J. Williams. Towards a theory of reinforcement learning connectionist systems. Technical Re port NU-CCS-88-3, Northeastern University, Boston, MA, 1988."}], "references": [{"title": "Optimal control of Markov decision pro\u00ad cesses with incomplete state estimation", "author": ["K.J. Astrom"], "venue": "J. Math. An/. Appl.,", "citeRegEx": "Astrom.,? \\Q1965\\E", "shortCiteRegEx": "Astrom.", "year": 1965}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "In Machine Learning: Proceedings of the Twelfth International Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Gradient descent for general reinforcement learning", "author": ["L.C. Baird", "A.W. Moore"], "venue": "In Advances in Neu\u00ad ral information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Neuro-dynamic Programming. Athena", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Decision the\u00ad oretic planning: structural assumptions and computa\u00ad tional leverage", "author": ["C. Boutillier", "T.L. Dean", "S. Hanks"], "venue": "Journal of AI Research, To appear,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Exact and Approximate Algorithms for Partially Observable Markov Decision Processes", "author": ["A.R. Cassandra"], "venue": "PhD thesis, Brown University,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Acting optimally in partially observable stochastic domains", "author": ["A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": "In Proceedings of the Twelfth National Con\u00ad ference on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "An improved policy iteration algorithm for partially observable MDPs", "author": ["E.A. Hansen"], "venue": "In Advances in Neu\u00ad ral Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Finite-Memory Control of Partially Observable Systems", "author": ["E.A. Hansen"], "venue": "PhD thesis, Department of Computer Science, University of Massachusetts at Amherst,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Solving POMDPs by searching in pol\u00ad icy space", "author": ["E.A. Hansen"], "venue": "In Proceedings of the Eighth Conference on Uncertainty in Arti.ficia/Jntelligence,", "citeRegEx": "Hansen.,? \\Q1998\\E", "shortCiteRegEx": "Hansen.", "year": 1998}, {"title": "Planning and Control in Stochas\u00ad tic Domains with Imperfect Information", "author": ["M. Hauskrecht"], "venue": "PhD thesis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Optimal Control of Complex Structured Processes", "author": ["0. Higelin"], "venue": "PhD thesis, University of Caen, France,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1960}, {"title": "Rein\u00ad forcement learning algorithm for partially observable Markov problems", "author": ["T. Jaakkola", "S. Singh", "M.R. Jordan"], "venue": "In Advances in Neural Informa\u00ad tion Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Searching for finite-state POMDP controllers", "author": ["L.P. Kaelbling", "K.E. Kim", "N. Meuleau", "L. Peshkin"], "venue": "Technical Report CS-99-06,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Re\u00ad inforcement learning: a survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Memory less policies: Theoretica1 lim\u00ad itations and practical results", "author": ["M.L. Littman"], "venue": "In From Animals to Animats 3: Proceedings of the T hird International Conference on Simulation of Adaptive Behavior", "citeRegEx": "Littman.,? \\Q1994\\E", "shortCiteRegEx": "Littman.", "year": 1994}, {"title": "Overcoming incomplete perception with utile distinction memory", "author": ["R.A. McCallum"], "venue": "In The Proceedings of the Tenth International Machine Learning Confer\u00ad ence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Reinforcement Learning with Selec\u00ad tive Perception and Hidden State", "author": ["R.A. McCallum"], "venue": "PhD thesis, Univer\u00ad sity of Rochester,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "The importance of impossible trajecto\u00ad ries in the VAPS algorithm", "author": ["N. Meuleau"], "venue": "In preparation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Cas\u00ad sandra. Solving POMDPs by searching the space of finite policies", "author": ["N. Meuleau", "K.E. Kim", "L.P. Kaelbling", "A.R"], "venue": "Proceedings of the Fifteenth Confer\u00ad ence on Uncertainty in Artificial Intelligence, To ap\u00ad pear,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Learn\u00ad ing policies with external memory", "author": ["L. Peshkin", "N. Meuleau", "L.P. Kaelbling"], "venue": "Proceedings of the Sixteenth International Conference on Machine Learning, To appear,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Markov Decision Processes: Dis\u00ad crete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Lave. Markov decision processes with probabilistic observation of states", "author": ["R.E.J.K. Satia"], "venue": "Management Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1973}, {"title": "Learn\u00ad ing without state-estimation in partially observable Markovian decision processes", "author": ["S. Singh", "T. Jaakkola", "M.R. Jordan"], "venue": "In Machine Learn\u00ad ing: Proceedings of the Eleventh International Con\u00ad ference", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Sondik. The optimal con\u00ad trol of partially observable Markov decision processes over a finite horizon", "author": ["E.J.R.D. Smallwood"], "venue": "Operations Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1973}, {"title": "The optimal control of partially observ\u00ad able Markov decision processes over the infinite hori\u00ad zon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1978}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Towards a theory of reinforcement\u00ad learning connectionist systems", "author": ["R.J. Williams"], "venue": "Technical Re\u00ad port NU-CCS-88-3,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1988}], "referenceMentions": [], "year": 2011, "abstractText": "Reactive (memoryless) policies are sufficient in completely observable Markov decision pro\u00ad cesses (MDPs), but some kind of memory is usually necessary for optimal control of a par\u00ad tially observable MDP. Policies with finite mem\u00ad ory can be represented as finite-state automata. In this paper, we extend Baird and Moore's YAPS algorithm to the problem of learning gen\u00ad eral finite-state automata. Because it performs stochastic gradient descent, this algorithm can be shown to converge to a locally optimal finite\u00ad state controller. We provide the details of the algorithm and then consider the question of un\u00ad der what conditions stochastic gradient descent will outperform exact gradient descent. We con\u00ad clude with empirical results comparing the per\u00ad formance of stochastic and exact gradient de\u00ad scent, and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step.", "creator": "pdftk 1.41 - www.pdftk.com"}}}