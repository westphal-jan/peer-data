{"id": "1301.6748", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Contextual Weak Independence in Bayesian Networks", "abstract": "It is well-known that the notion of (strong) conditional independence (CI) is too restrictive to capture independencies that only hold in certain contexts. This kind of contextual independency, called context-strong independence (CSI), can be used to facilitate the acquisition, representation, and inference of probabilistic knowledge. In this paper, we suggest the use of contextual weak independence (CWI) in Bayesian networks to facilitate the acquisition and inference of probabilistic knowledge. We show that the presence of CWI in a Bayesian network is associated with a robust CI structure. In this paper we use CWI to perform exploratory analyses of the Bayesian network, using the Bayesian Bayesian network, by including a number of statistical features related to CWI in a Bayesian network.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 23 Jan 2013 16:01:33 GMT  (353kb)", "http://arxiv.org/abs/1301.6748v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael s k m wong", "c j butz"], "accepted": false, "id": "1301.6748"}, "pdf": {"name": "1301.6748.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wong@cs.", "butz@cs.uregina.ca"], "sections": [{"heading": null, "text": "It is well-known that the notion of (strong) conditional independence ( CI) is too restric tive to capture independencies that only hold in certain contexts. This kind of contextual independency, called context-strong indepen dence (CSI), can be used to facilitate the acquisition, representation, and inference of probabilistic knowledge. In this paper, we suggest the use of contextual weak indepen dence (CWI) in Bayesian networks. It should be emphasized that the notion of CWI is a more general form of contextual indepen dence than CSI. Furthermore, if the contex tual strong independence holds for all con texts, then the notion of CSI becomes strong Cl. On the other hand, if the weak contextual independence holds for all contexts, then the notion of CWI becomes weak independence (WI) which is a more general noncontextual independency than strong Cl. More impor tantly, complete axiomatizations are studied for both the class of WI and the class of CI and WI together. Finally, the interesting property of WI being a necessary and suf ficient condition for ensuring consistency in granular probabilistic networks is shown.\n1 INTRODUCTION\nIn the probabilistic approach to uncertainty manage ment, one assumes that knowledge can be represented as a joint probability distribution [6]. In practice it may not be feasible to elicit and store the required probabil ity values when the number of variables becomes large. However, we can utilize the notion of (strong) condi tional independence ( CI) to economically represent a joint probability distribution as the product of con ditional probability tables (CPTs). The required joint\nprobability values can then be obtained indirectly by eliciting the corresponding conditional probabilities.\nMany researchers including [1, 2, 3, 4, 6] have pointed out that the notion of strong CI is too restrictive to capture independencies that hold in some but not necessarily all contexts. This contextual strong inde pendency, referred to as context-strong independence (CSI) [1], asymmetric independence (ASI) [3], or prob abilistic causal irrelevance (PCI) [2] , can be used to facilitate the acquisition, representation, and infer ence of probabilistic knowledge. Geiger and Heck erman [3] proposed the use of multiple Bayesian networks (Bayesian multinets) to represent and rea son with contextual strong independence statements. That is, each Bayesian network reflects the CI state ments that hold for a given context. The particular Bayesian network used during inference is determined by the evidence. An alternative method suggested by Boutilier et a!. [1], is to incorporate auxiliary nodes into a single Bayesian network. In this case, addi tional nodes are used to reflect the strong conditional independence statements that hold in a given context. In [2], on the other hand, Galles and Pearl develop axioms for inferring contextual strong independencies in a similar spirit to inferring strong conditional inde pendencies using the semi-graphoid axioms [6].\nIn this paper, we suggest the use of contextual weak independence (CWI) in Bayesian networks. It should be emphasized that the notion of CWI is a more gen eral form of contextual independence than CSI, ASI and PCI. If the contextual strong independence holds for all contexts, then the notions of CSI, ASI and PCI become strong Cl. On the other hand, if the weak con textual independence holds for all contexts, then the notion of CWI becomes weak independence (WI). Just as contextual weak independence is more general than the notions of contextual strong independence, it is ex plicitly demonstrated that the notion of WI is a more general noncontextual independency than strong CI. More importantly, complete axiomatizations are stud-\nContextual Weak Independence in Bayesian Networks 671\nied for both the class of WI and the class of CI and WI together. Finally, the interesting property of WI being a necessary and sufficient condition for ensur ing consistency in granular probabilistic networks is shown. We use the term granular to mean the ability to coarsen and refine parts of a probabilistic network depending on whether they are of interest or not (5].\nThis paper is organized as follows. Section 2 contains background knowledge. In Section 3, we introduce the notion of CWI. The noncontextual independency WI is presented in Section 4. In Section 5, a complete axiomatization is shown for both the class of WI only and the class of CI and WI together. The application of WI to granular probabilistic networks is discussed in Section 6. The conclusion is presented in Section 7.\n2 BASIC NOTIONS\nIn this section, we review pertinent notions including (strong) probabilistic conditional independence and the more general notion of contextual strong indepen dence (I, 2, 3].\nConsider a finite set U = {At, A2, ... , An} of discrete random variables, where each variable A E U takes on values from a finite domain VA. We may use capital letters, such as X, Y, Z, for variable names and low ercase letters x, y, z to denote specific values taken by those variables. Sets of variables will be denoted by boldfaced capital letters X, Y, Z, and assignments of values to the variables in these sets (called configura tions or tuples) will be denoted by boldfaced lowercase letters x, y, z. We use Vx in the obvious way. We shall also use the short notation P( x) for the probabilities P(X = x), x E Vx, and P(z) for the set of variables Z = {X, Y } = X Y meaning\nP(Z=z) = P(X= x, Y= y) = P(x, y),\nwhere x E Vx, y E Vy.\nLet P be a joint probability distribution over the vari ables in U and X, Y, Z be subsets of U. We say X and Z are conditionally independent given Y, denoted I(X l_ Z I Y), if given any x E Vx, y E Vy, then for all z E Vz:\nP(x I y,z) = P(x I y), whenever P(y,z) > 0. (I)\nFor convenience we write equation (I) as P(X I Y, Z) = P(X I Y). Alternatively, the same strong condi tional independence ( CI) can be defined as\nP( ) P(x,y)\u00b7P(y,z)\nx, y,z = P(y)\n(2)\nwhere P(x, y), P(y, z) and P(y) are marginal distri butions of P(x, y,z).\nThe notion of strong CI is extensively used economi cally representing a joint probability distribution as a Bayesian network. A Bayesian network [6] is a directed acyclic graph (DAG) together with a corresponding set of conditional probability tables. By definition, a Bayesian network only reflects conditional indepen dencies P(x I y, z) = P(x I y) which hold for all y E Vy. In some situations, however, the conditional independence may only hold for certain specific values in Vy. For example, consider the CPT depicted in\nMany different approaches including (I, 2, 3, 4, 6] have been proposed for representing and reasoning with in dependencies that are more general than Cl. In (4, 6], the notion of causal independence is suggested to facil itate knowledge acquisition and increase the speed of inference. However, the work most relevant to this pa per involves the notions of contextual strong indepen dence, namely, asymmetric independence (3], context strong independence (I], and probabilistic causal irrel evance [2].\nGeiger and Heckerman [3] studied the notion of asym metric independence (ASI), which states that variables are independent for some but not necessarily for all\n672 Wong and Butz\nof their values. The use of multiple Bayesian net works (Bayesian multinets) are then proposed since contextual strong independence statements cannot be represented naturally in a Bayesian network. For example, consider again the CPT in Table 1. The Bayesian network constructed using the notion of CI does not reflect any independence between variables X and { Z, W} given Y. However, one Bayesian network can be constructed to capture the conditional inde pendence of X and {Z, W} given Y = 0, and another to show their dependence when Y = 1 as shown in Figure 1 (left). The particular Bayesian network to be used in the inference process is determined by the evidence.\nBoutilier et a!. [1] formalized the notion of contex tual strong independence with context-strong indepen dence ( CSI). ( CSI is called context-specific indepen dence in [1].) Let X, Y, C, Z be pairwise disjoint sets of variables. We say X and Z are strongly in dependent given Y and the context C = c, denoted I(X j_ Z I Y,C = c), if:\nP(X I Y,C = c,Z) = P(X I Y,C = c), (3)\nwhenever P(Y, C = c, Z) > 0. It should be clear from equation ( 3) that strong CI is a special case of CSI, namely, CSI becomes strong CI when the CSI holds for all c E Vc. That is, if the context-strong independence of X and Z given Y and C = c holds for all c E Vc, then we simply say X and Z are conditionally indepen dent given YC. Instead of using Bayesian multinets to capture contextual strong independencies, Boutilier et a!. [1] represent these contextual strong statements with a single Bayesian network by introducing auxil iary nodes. Given the CPT in Table 1, the contextual strong independence of variables X and { Z, W} given Y = 0, and dependence when Y = 1, is captured with auxiliary nodes such as X y =O as shown in Figure 1 (right).\nIn [2], Galles and Pearl studied the notion of prob abilistic causal irrelevance (PCI). We say Z is prob abilistically causally irrelevant to X given Y if for a fixed y E Vy, the following relationship holds:\nP(XIY = y, Z) = P(XIY = y). (4)\nThus, PCI tries to capture the same contextual strong independencies as defined by equation (3). It follows that if the causal irrelevance holds for all y E Vy, then X and Z are probabilistically conditionally inde pendent given Y in the usual sense.\nIt should be clear that the notions of ASI, CSI and PCI all try to capture contextual strong independence, where the partition of the CPT is defined by Y = y. In the next section, however, we introduce a weak contextual independence in which the partition of the CPT is not explicitly defined by the context Y = y.\n3 CONTEXTUAL WEAK INDEPENDENCE (CWI)\nWe now introduce the notion of contextual weak inde pendence (CWI), namely, variables X and Z are weakly independent given the context Y = y. We show that this contextual independence is more general than the notions of ASI, CSI, and PCI.\nThe notions of ASI, CSI and PCI are too strong to capture a weaker form of independence. Let P be a joint probability distribution over the set of vari ables U, where X, Y, Z, W E U. Let Vx = {0, 1, 2}, Vy = {0, 1} and Vz = Vw = {0, 1,2,3}. Consider the CPT shown in Table 2, where configurations c with P(c) = 0 are not shown. By equation (1), X and {Z, W} are not conditionally independent given Y. More importantly, there is no contextual strong in dependency between variables X and {Z, W} given the context Y = 0 or Y = 1. For example, if Y = O,then\nP(X = 0 I y = 0, z = 0, w = 0)\nPt # P(X = 0 I y == 0, z = 0, w = 3) = 0, (5)\nand if Y = 1, then\nP(X = 0 I y = 1, z = 0, w = 0)\n= Ps # P(X = 0 I Y = 1, Z = 0, W = 1) = 0. (6)\nBy equations (5) and (6), the notion of ASI does not capture any form of independence between variables X and { Z, W} given Y = 0 or Y = 1 as reflected by the Bayesian multinets depicted in Figure 2 (left). Furthermore, equations (5) and (6) also indicate that variables X and { Z, W} are not contextually indepen dent given either the context Y = 0 or the context\nContextual Weak Independence in Bayesian Networks 673\nY = l. Consequently, the Bayesian network in Figure 2 (right) using the notion of CSI by incorporating auxiliary nodes does not reflect any independence between X and {Z, W} given Y = 0 or Y = 1. Figure 2 clearly illustrates that the notions of ASI and CSI do not capture any independency between variabies X and {Z, W} given Y = 0. (Equations (5) and ( 6) also indicate that variables { Z, W} are not causally irrelevant to X when Y = 0 or Y = 1.)\nTable 2: Variables X and {Z, W} are weakly independent given the context Y = 0, but not when Y = 1.\nX y z w P(X I Y, Z, W) tl 0 0 0 0 P1 t2 0 0 0 1 P1 t3 0 0 1 0 P1 t4 0 0 1 1 P1 ts 1 0 0 0 P2 t6 1 0 0 1 P2 t7 1 0 1 0 P2 ts 1 0 1 1 P2 tg 2 0 2 2 P3 t1o 2 0 2 3 P3 tu 2 0 3 2 P4 t12 2 0 3 3 P4 t13 0 1 0 0 Ps t14 1 1 0 2 P6 t1s 2 1 1 3 P7\nWe now informally demonstrate a weak independence between variables X and {Z, W} given the context Y = 0. Let us focus on the configurations (x, Y = 0, z, w) in Table 2, namely, the set of configurations {t1, t2, . .. , t12}. Recall that the domain of variables X, Y, Z, and W are Vx = {0, 1, 2}, Vy = {0}, and Vz = Vw = {0, 1, 2, 3}. We make a few observations. When Y = 0, only specific values x E Vx appear with specific values z E Vz and w E Vw. That is,\nX= 0, 1 <==> Z = 0, 1 and W = 0, 1,\nand\nX = 2 <==> Z = 2, 3 and W = 2, 3.\nIn other words, when X = 0 then Z is never 2 or 3 and so forth. Based on this observation let us parti tion the CPT in Table 2 into the three separate CPTs {t1,t2, ... , ts}, {tg, tlo, tn, tl2}, and {t13, t14, t14} . Consider the CPT {t1, t2, . . . , t8}. Let us define new domains V.k, Vy, Vz, Vw, based on the values that appear. We obtain\nVx = {0, 1}, Vy = {0}, Vz = {0, 1}, Vw = {0, 1}. (7)\nWith respect to the new domains in equation (7), by definition variables X and { Z, W} are conditionally independent given Y in the CPT {t1, t2, \u2022 \u2022 \u2022 , ts}. Now consider the CPT {tg, t1o, tu, t12}. Similarly, we define new domains VJ(, V{', V\u00a3, V\ufffd, for the variables based on the values that appear. Hence,\nv}( = {2}, v.y = {O}, v; = {2, 3}, v\ufffd = {2, 3}. (8)\nWith respect to the domains in equation (8), however, variables X and { Z, W} are not conditionally indepen dent given Y since\nP(X = 2 I y = 0, z = 2, w = 2)\nP3 f. P(X = 2 I y = 0, z = 3, w = 3) = P4\u00b7\nWe now formalize this idea.\nWe begin by recalling some familiar notions about re lations. Given a distribution P over the set of variables U = XYZ, let\nC = { t = (x,y, z)IP(xly,z)>O}. (9)\nGiven any subset V \ufffd U, we can define an equivalence relation O(V) on C: for all t;, tiE C,\n{10)\nwhere tk [V] denotes the value of V in the tuple tk.\nConsider two equivalence relations 11(V) and O(W) on T, where V, W \ufffd U. The binary operator o, called the composition, is defined by: for t;, tk E T,\nt; 11(V) o 11(W) tk, ( 11)\nif for some ti E T both\nGiven two equivalence relations 11(V) and O(W) on T, it can be shown that 11(V) o 11(W) is an equivalence relation (a partition) if and only if 11(V) o 11(W) = 11(W) o 11(V). We now define contextual weak inde pendence (CWI) as follows.\n674 Wong and Butz\nLet X, Y, Z be pairwise disjoint sets of variables in U. Let O(XY = y) and O(Y = yZ) be partitions on C in equation (9). We say variables X and Z are weakly independent given the context Y = y, denoted W I(X l.. ZIY = y), if the following two conditions hold:\n(i) O(XY = y) o O(Y = yZ) O(XY = y), and\nO(Y = yZ) o\n(ii) there exists an equivalence class 1r in O(XY = y) o O(Y = yZ) such that: for any given x E V\ufffd, then for all z E Vz:\nP(x I y,z) = P(x I y), (12)\nwhere v\ufffd, vzr are defined as:\nv\ufffd = { x 1 t = (X = x, Y = y, z) E 1r }, (13)\nand\nVz = { z I t = (X, y = y, z = z) E 7r }. (14)\nCondition (i) says that the composite relation O(XY = y) o O(Y = yZ) is an equivalence relation. This is the necessary condition for W I(X l.. Z I Y = y) to hold. Condition (ii) says that variables X and Z are conditionally independent given Y = y with respect to the new domains v\ufffd and vzr. For example, let us verify that variables X and {Z, W} are weakly independent given the context Y = 0 in the CPT shown in Table 2. By equation (9),\nc = { t1,t2, . . . ,t12 }.\nBy equation (10), we obtain the following equivalence relations on C:\nO(XY = 0)\n{ {t1,t2,ta, t4}, {ts, ts,t7,ts}, {tg, t1o, t11,t12} },\nand\nO(Y = OZW) { {t1, ts},{t2, ts},{ta, t7}, {t4, ts}, {tg}, {t1o}, {tn}, {t12} }.\nApplying equation (11), we obtain:\nO(XY = 0) o O(Y = OZW)\n{ 1r1 = {t1, t2, . . . ,ts}, 1r2 = {tg, t1o,tu, t12} } O(Y = OZW) o O(XY = 0). (15)\nApplying equations (13) and (14) on the configurations 1r1 = {t1, t2, . . . , ts}, we obtain\nv;\u00b7 { x 1 t = (X = x, Y = y, z, w) E 1r1 } = {0, 1}, (16)\nand\nv;\ufffdv = { (z, w) It = (X, Y = y, Z = z, W = w) E 1r1 }\n{(0,0),(0, 1), (1,0), (1, 1)}. (17)\nIt can be verified that variables X and {Z, W} are conditionally independent given Y = 0 in equivalence class 1r1 with respect to the new domains v;\u2022 = {0, 1} and Vztv = {(0, 0), (0, 1), (1, 0), (1, 1)}. This pro cess can be repeated for the set of configurations 1r2 = {tg, t1o, tu, t12}. The computed new domains are v;, = {2} and Vzw = {(2, 2), (2, 3), (3, 2), (3, 3)}. As already mentioned, however,\nP(X = 2 I y = 0, z = 2, w = 2)\nPa # P(X = 2 I Y = 0, Z = 3, W = 3) = P4\u00b7\nThus, variables X and {Z, W} are not condition ally independent given Y = 0 with respect to the new domains in equivalence class 1r2. However, con dition (ii) is still satisfied since the conditional in dependence holds for at least one equivalence class, i.e., 1r1 = {t1, t2, . . . , ts} . By definition, variables X and {Z, W} are weakly independent given the context y = 0.\nThe conditional independence in equivalence class 1r1 is reflected by the Bayesian network in Figure 3 (left). The dependency in equivalence class 1r2 is reflected by the Bayesian network in Figure 3 (middle). As with ASI and CSI in Figure 2 (left, right), variables X and { Z, W} are not weakly independent given the context Y = 1 as shown in Figure 3 (right).\nThe important point is that the notion of contextual weak independence (CWI) can capture a more general form of independence than the notions of contextual strong independence. In the above example, the con textual strong notions of ASI and CSI do not capture any independence between variables X and { Z, W} given Y = 0 as shown by the various Bayesian net works in Figure 2. On the other hand, the notion of\nContextual Weak Independence in Bayesian Networks 675\nvariables X and Z being weakly independent given the context Y = 0 is captured by the Bayesian network in Figure 3 (left).\n4 WEAK INDEPENDENCE (WI)\nIn this section, we show another important distinction between the notion of contextual weak independence and the contextual strong notions of ASI, CSI and PC I. If the contextual strong independence of X and Z holds for all contexts Y = y, y E Vy, then the notions of ASI, CSI and PCI become strong CI, namely, X and Z are strongly conditional independent given Y. This is not necessarily the case with contextual weak independence. If X and Z are weakly independent for all contexts Y = y, y E Vy, then we say X and Z are weakly independent given Y. Just as contextual weak independence (CWI) is more general than the notions of contextual strong independence (CSI,ASI,PCI), we show that the notion of weak independence (WI) is a more general noncontextual independency than strong Cl. In the next section, we show that the class of WI and CI together has a complete axiomatization.\nLet X, Y, Z be pairwise disjoint sets of variables in U. Let B(XY) and B(YZ) be partitions on C in equation (9). We say variables X and Z are weakly independent given Y, denoted W !(X .l Z I Y), if the following two conditions hold:\n(i) B(XY) o B(YZ) = B(YZ) o B(XY), (18)\nand (ii) for every equivalence class 1r in the equivalence relation B(YZ) o B(XY), if given any x E Vx, y E V{, then for all z E V{:\nP(x I y,z) = P(x I y), (19)\nwhere the new domains Vx, V{ and V{ are defined as:\nvx { x 1 t = (x = x, Y, z) E 1r J, Vy = { y It= (X, y = y, Z) E 7r }, V{ { y It= (X, Y, Z = z) E 1r }.\n(20)\n(21)\n(22)\nFor example, consider the CPT shown in Table 3, where Vx = Vz = Vw = {0, 1, 2, 3} and Vy = {0, 1} . Variables X and {Z, W} are not conditionally inde pendent given Y. However, variables X and { Z, W} are weakly independent given Y. By equation (9), C = { t1, t2, . . . , t32 }. By equation {10), we obtain the equivalence relations B(XY) and B(Y ZW) on C:\nB(XY) =\n{ {t1, t2, t3, t4}, {ts, ts, t7, ts}, {tg, tlo, tll, tl2},\n{ {tl, ts}, {t2, ts},{t3,t7},{t4, ts},{tg,tl3}, {t1o,t14}, {tn, t1s}, {t12, t1s}, {t17, t21}, {t1s, t22}, {t19, t23}, {t2o, t24},{t2s, t29}, {t2s, t3o}, {t27, t31}, {t2s, t32} }.\nApplying equation {ll), we obtain:\nB(XY) o B(Y ZW)\n{ 1r1 = {t1,t2, t3, t4,ts, ts, t7, ts}, 1r2 = {tg, tlo,tll, t12, t13, tl4,tls, tls}, 1r3 = {t17, tls, t19,t2o, t21, t22,t23, t24}, 1!\"4 = {t2s, t2s, t27, t2s,t29,t3o,t31, t32} }\nB(Y ZW) o B(XY). (23) {t13, t14, t1s, t1s}, {t17, t1s, t19, t2o}, {t21. t22, t23, t24}, Condition (i) of the definition of WI is then satis {t25, t2s, t21, t2s}, {t29, t3o, t31, t32} }, fied. With respect to the equivalence class 1r1 =\n676 Wong and Butz\n{t1, t2, .. . , t8}, we obtain by equations (20)-(22) the new domains v;\u00b7, v;\u00b7 and v;w:\nv;\u2022 = {o, 1}, v;\u00b7 = {O}, v;w = {(o, o), (o, 1), (1, o), (1, 1)}.\nWith respect to the new domains v;\u2022, v;\u2022 and v;w, variables X and { Z, W} are conditionally indepen dent given Y. This conditional independence can be verified similarly in the other equivalence classes 1r2 = {tg, t1Q, . . . ,t16}, 1r3 = {t17,t1s, . .. ,t24}, and 1!\"4 = {t2s, t26, . .. , ta2}\u00b7 By definition, variables X and { Z, W} are weakly independent given Y.\nThere are two equivalent ways to view the represen tation of WI. One is in terms of multiple Bayesian networks as in [3]. The difference here is that each Bayesian network has the same dependency structure as shown in Figure 4. As with the Bayesian multinets approach [3], the particular CPT to be used is defined by the evidence. An alternative viewpoint is to rep resent WI in a single Bayesian network as with the notion of CSI [1]. The difference here is that no arti ficial nodes need to be incorporated into the Bayesian network. Instead each node is simply associated with a set of CPTs compared to a single CPT as with the notion of CI in traditional Bayesian networks.\nIn the next section, we turn our attention to developing an axiomatic basis for WI.\n5 COMPLETE AXIOMATIZATION FOR WI AND CI\nThere are both theoretical and practical reasons for developing a complete axiomatization for a new type of dependency. When designing a probabilistic net work [6], it is always desirable that the designer have complete knowledge about the given input dependen cies and all of their logical consequences. On the prac tical side, in the absence of knowing a logical conse quence, an inference engine may spend precious time\non derivations bearing no relevance to the task at hand.\nIn [2], Galles and Pearl develop axioms to formalize the notion of probabilistic causal irrelevance (PCI) in a similar spirit to the semi-graphoid axioms [6] for Cl. As shown in Section 3, however, the contextual strong notion of PCI is too restrictive to capture contextual weak independence. In this section, we study com plete axiomatizations for both the class of weak inde pendence (WI) and the class of CI and WI together. Note that we only consider CI and WI statements de fined with respect to the same fixed set of variables called nonembedded (full) independencies. That is, we do not consider axioms that involve statements involv ing a mixture of variables such as Pearl's contraction axiom (3.6d) [6].\nTheorem 1 Let X, Y, Z, W be subsets of variables from U such that XYZ = U. A complete set of infer ence axioms for WI is:\nWll: WI(X j_ U-Y I Y), X\ufffd Y; WI2 : w !(X j_ u- XY I Y) :::?\nWI(X-W j_ U -X(Y -W) I Y),\nWI(XW j_U-XYW IY), W\ufffdY;\nWI3 : w !(X j_ u-XY I Y) :::?\nWI(X j_ U-XYW) I YW).\nWll, WI2, and WI3 are called reflexivity, transport, and augmentation axioms, respectively. It is interest ing to note that there is no transitivity axiom for WI.\nTheorem 2 Let X, Y, Z1, Z2 be pairwise disjoint sub sets of variables from U such that XYZ1Z2 = U. A complete set of inference axioms for CI and WI to gether is:\nCI &Wil: !(X j_ U-XY I Y) :::?\nWI(Y j_ U-XY) I Y),\nCI &WI2 : WI(X j_ z2 1 YZ1), WI(X j_ z1 1 YZ2), I(Z1 j_ Z2 1 YX) :::? w I(X j_ Z1Z2 1 Y).\nCI &Wil and CI &WI2 are called weaken and transi tivity axioms, respectively.\n6 GRANULAR PROBABILISTIC NETWORKS\nIn this section, we present a brief discussion on granu lar probabilistic networks [5]. We use the term granu lar to mean the ability to coarsen and refine parts of a\nContextual Weak Independence in Bayesian Networks 677\nprobabilistic network. We will show that WI is a nec essary and sufficient condition for ensuring consistency in such granular networks.\nKoller and Pfeffer [5] suggested a framework for the modeling of and inference in a large Bayesian network. In this framework, parts of the network can be coars ened and refined. For example, consider the Bayesian network for a car accident [5] as shown in Figure 5. One can refine the node Car to reveal the internal structure as shown in Figure 6, where the rest of the network is not illustrated due to space limitations.\nFigure 5: A Bayesian network for a car accident.\nFigure 6: Refining the Bayesian network in Figure 5 to reveal the internal structure of the variable Car.\nOur purpose here is to present two operators NEST and UNNEST for coarsening and refining a probabilis tic network. The main result of this section is that WI is a necessary and sufficient condition for ensuring con sistency in such granular networks. (The notion of CI is only a sufficient condition.)\nWe introduce the operator NEST to coarsen a joint probability distribution. The NEST operator, denoted\nN, is used to coarsen parts of a network. Intuitively, NB=Y(Pxy) groups together all the Y-values into a nested distribution given the same X-value.\nIn the following definitions, we use boldface letters such as t to denote an entire configuration c along with P(c), i.e., t = (c, P(c)). Recall t[X] denotes the value of X in configuration c. The straightforward idea of Nest and Unnest involve rather cumbersome definitions. The examples that follow should clarify any confusion.\nCoarsening variables Y in distribution P as at tribute B is the distribution NB=Y(P) with variables X, B, P(X, B) defined by\nNB=Y(P) {t I t[X] = u[X] and t[B] = {u[Y, P]}, and t[P(X, B)]= L u[P]},\nu\nwhere u E P. The attribute P in the value of B is relabelled P(Y) and the values normalized.\nFor example, consider the distribution in Figure 7. Coarsening variables {A2, A3} as B is the distribution NB={A,,A,}(P) depicted in Figure 8.\nWe now introduce the operator UNNEST to refine parts of the network. The UNNEST operator, denoted U, reveals the nested variables. Intuitively, U B=Y (P) joins each X-value with each tuple in the correspond ing B-value.\nRevealing the nested variables Y in attribute B of PxB is the distribution UB=Y(P) with variables XY, P(X, Y) defined by\nUB:v(P) {t I t[X] = u[X] and t[Y] E u[B]\nand t[P] = L v[P(XB)] \u00b7 w[P(Y)]}, v\nwhere u E P, v[X] = u[X], w E v[B] and w[Y] = t[YJ. Note that we may write UB=Y(P) as UB(P) since Y is implicitly implied by B.\nFor example, revealing the nested variables in coars ened distribution P' depicted in Figure 8 results in the refined distribution U B ( P') shown in Figure 7.\nThe following example demonstrates that inconsis tency may arise due to the fact that the NEST op erator is not commutative. (In contrast, the UNNEST operator is commutative.) Consider the distribution Pin Figure 9.\nSuppose we wish to coarsen the variables A1 and Aa. Nesting A3 as Ba followed by A1 as B1 results in the distribution N B,={A.} (N Bs={As} (P)) depicted in Figure 10. On the other hand, coarsening A1 as B1 followed by Aa as B3 results in the distribution NB,={A3}(NB,={A,}(P)) illustrated in Figure 11.\nWe now show that WI is a necessary and sufficient condition for NEST to commute. In other words, WI ensures consistency in granular networks.\nTheorem 3 Let P be a joint probability distribution over U, and X, Y, Z pairwise disjoint subsets such that XYZ = U. X and Z are weakly independent given Y if and only if\nProof: (:::?) Suppose X and X are weakly independent given Y. By definition, X and Z are conditionally independent given Y in each equivalence class in the equivalence relation II(XY) o II(YZ). One equivalence class is shown in Figure 12, where c = a1 +a2 = b1 +b2 by equation (2) .\nIt is clear that when X and Z are weakly independent given Y, computing NB2=z(P) only groups together tuples in the same equivalence class in the equivalence relation II(XY) o II(YZ). Computing NB2=z(P) mod ifies the equivalence class in Figure 12 into the one depicted in Figure 13. The expressions for probability values of B2 can be simplified since, for example\n(a1b!)c b1 (a1b1)c + (a1b2)c = b1 + b2 \u00b7\nSince the B2-values are identical, computing NB,=x(NB2=z(P)) modifies the equivalence class in Figure 13 into the one illustrated in Figure 14.\nz P(Z) X2 Y1 Zl (a2b1)/c = b1 a2(b1 + b2)/c = a2\nZ2 (a2b2)/c= b2\nFigure 13: The distribution NB2=z(P).\nB1 y B2 P(B1, Y, B2)\nX P(X) z P(Z) Xl al Yl Zl bl c X2 a2 Z2 b2\nFigure 14: The distribution NB,=x(NB,=z(P)).\nThis demonstrates that each equivalence class in 9(XY) o 9(YZ) is reduced into a single tuple with all the Z-values and X-values represented in one B1-value and B2-value, respectively. Since the conditional inde pendence of X and Z given Y is symmetric, this argu ment can be applied to show that NB,=z(NB,=x(P)) also reduces the initial equivalence class in Figure 12 into the single tuple shown in Figure 14. This argu ment holds for each equivalence class in the equiva lence relation 9(XY) o 9(YZ).\n( \u00a2) Let P be a distribution such that NB,=x(NB,=z(P)) NB2=z(NB,=x(P)) holds, where one tuple in NB,=x(NB,=z(P)) is depicted in Figure 14. The equality holding indicates that c = a1 + a2 = b1 + b2, as otherwise the order of nesting becomes relevant. The result of UB, (NB,=x(NB2=z(P))) is de picted in Figure 13. It can be easily verified that X and Z are conditionally independent given Y in the nor malized distribution UB2(UB, (NB,=x(NB,=z(P)))), shown in Figure 15. This argument holds for each tuple in NB,=x(NB2=z(P)). Therefore, X and Z are weakly independent given Y in the distribution NB,=x(NB,=z(P)). D\n7 CONCLUSION\nMany researchers including (1, 2, 3, 4, 6] have pointed out that the notion of (strong) CI is too restrictive to capture independencies that hold in some but not necessarily all contexts. This kind of contextual inde pendency, referred to as context-specific independence (CSI) (1], asymmetric independence (ASI) (3], or prob-\nabilistic causal irrelevance (PCI) (2], can be used to facilitate the acquisition, representation, and inference of probabilistic knowledge.\nIn this paper, we suggest the use of a more general form of contextual independence called contextual weak in dependence (CWI) in Bayesian networks. It was ex plicitly demonstrated that CWI can detect indepen dence that remains undetected by CSI, ASI and PCI. Furthermore, if the contextual strong independency holds for all contexts, then the notions of CSI, ASI and PCI become (strong) CI. On the other hand, if the contextual weak independency holds for all contexts, then CWI becomes weak independence (WI). Just as contextual weak independence ( CWI) is more general than the notions of contextual strong independence, it was explicitly demonstrated that the notion of weak independence (WI) is a more general noncontextual in dependency than strong CI. We also studied complete axiomatizations for both the class of WI and the class of CI and WI together. Finally, the interesting prop erty of WI being a necessary and sufficient condition for ensuring consistency in granular probabilistic net works (5] was demonstrated.\nReferences\n(1] C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. Context-specific independence in bayesian networks. In UAI-96, pages 115-123, 1996.\n(2] D. Galles and J. Pearl. Axioms of causal relevance. Artificial Intelligence, 97:9-43, 1997.\n[3] D. Geiger and D. Heckerman. Advances in proba bilistic reasoning. In UAI-91, pages 118-126, 1991.\n[4] D. Heckerman and J. Breese. A new look at causal independence. In UAI-94, pages 286-292, 1994.\n[5] D. Koller and A. Pfeffer. Object-oriented bayesian networks. In UAI-97, pages 302-313, 1997.\n[6] J. Pearl. Probabilistic Reasoning in Intelligent Sys tems: Networks of Plausible Inference. 1988."}], "references": [{"title": "Context-specific independence in bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "In UAI-96,", "citeRegEx": "Boutilier et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Axioms of causal relevance", "author": ["D. Galles", "J. Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "Galles and Pearl.,? \\Q1997\\E", "shortCiteRegEx": "Galles and Pearl.", "year": 1997}, {"title": "Advances in proba\u00ad bilistic reasoning", "author": ["D. Geiger", "D. Heckerman"], "venue": "In UAI-91,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "A new look at causal independence", "author": ["D. Heckerman", "J. Breese"], "venue": "In UAI-94,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Object-oriented bayesian networks", "author": ["D. Koller", "A. Pfeffer"], "venue": "In UAI-97,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Probabilistic Reasoning in Intelligent Sys\u00ad tems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}], "referenceMentions": [], "year": 2011, "abstractText": "It is well-known that the notion of (strong) conditional independence ( CI) is too restric\u00ad tive to capture independencies that only hold in certain contexts. This kind of contextual independency, called context-strong indepen\u00ad dence (CSI), can be used to facilitate the acquisition, representation, and inference of probabilistic knowledge. In this paper, we suggest the use of contextual weak indepen\u00ad dence (CWI) in Bayesian networks. It should be emphasized that the notion of CWI is a more general form of contextual indepen\u00ad dence than CSI. Furthermore, if the contex\u00ad tual strong independence holds for all con\u00ad texts, then the notion of CSI becomes strong Cl. On the other hand, if the weak contextual independence holds for all contexts, then the notion of CWI becomes weak independence (WI) which is a more general noncontextual independency than strong Cl. More impor\u00ad tantly, complete axiomatizations are studied for both the class of WI and the class of CI and WI together. Finally, the interesting property of WI being a necessary and suf\u00ad ficient condition for ensuring consistency in granular probabilistic networks is shown.", "creator": "pdftk 1.41 - www.pdftk.com"}}}