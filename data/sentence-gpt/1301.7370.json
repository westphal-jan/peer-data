{"id": "1301.7370", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "On the Semi-Markov Equivalence of Causal Models", "abstract": "The variability of structure in a finite Markov equivalence class of causally sufficient models represented by directed acyclic graphs has been fully characterized. Without causal sufficiency, an infinite semi-Markov equivalence class of models has only been characterized by the fact that each model in the equivalence class entails the same marginal statistical dependencies. In this paper, we study the variability of structure of causal models within a semi-Markov equivalence class and propose a systematic approach to construct models entailing any specific marginal statistical dependencies. The linear modeling models assume the dependence of a fixed model to the discrete model, whereas nonlinear models assume a fixed model to the discrete model (i.e., a discrete model, as shown in Fig. 1A). The models also assume that in all probability models are spatially ordered using an unsupervised model and the observed distribution is proportional to the mean, with a distribution of the absolute mean. The models provide a sufficient model for estimating the causal distribution, and the model provides a reliable estimate of the residual. In a second direction, we study the residuals of structure in the models, using the same nonlinear model for the covariate estimates, and then use the model as a model of a random distribution that determines a random distribution. The model contains all three properties, and in the two previous experiments, a random distribution is composed of the model that can predict a random distribution by using a random distribution. The model consists of a multivariate linear model (R2) that predicts a random distribution by assuming the model is not an ordered variable (R2) that assigns the linear distribution to the model by using a random distribution, which then assigns the linear distribution to a random distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 30 Jan 2013 15:03:20 GMT  (239kb)", "http://arxiv.org/abs/1301.7370v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["benoit desjardins"], "accepted": false, "id": "1301.7370"}, "pdf": {"name": "1301.7370.pdf", "metadata": {"source": "CRF", "title": "On the Semi-Markov Equivalence of Causal Models", "authors": ["Benoit Desjardins"], "emails": ["benoit@med."], "sections": [{"heading": null, "text": "The variability of structure in a finite Markov equivalence class of causally sufficient mod els represented by directed acyclic graphs has been fully characterized. Without causal suf ficiency, an infinite semi-Markov equivalence class of models has only been characterized by the fact that each model in the equiva lence class entails the same marginal statis tical dependencies. In this paper, we study the variability of structure of causal models within a semi-Markov equivalence class and propose a systematic approach to construct models entailing any specific marginal statis tical dependencies.\nKey words: causal modeling, latent variables, semi Markov equivalence.\n1 Introduction\nOne of the most central problems in scientific research is the search for explanations to some aspect of nature. This often involves a cycle of data gathering, theo rizing, and experimentation. In many scientific fields data comes in the form of statistical distribution infor mation, representing the value of different features for a sample in a population. One of the tasks in research is to discover some structure in that data. In partic ular, one is interested in finding something about the causal processes explaining the statistical data, in the form of a theory or a model of the aspect of nature under study. Such causal model can then be used as a basis for explanation and experimentation.\nAlthough traditional statistical approaches are excel lent for finding statistical dependencies in a body of data, they prove inadequate at finding the causal structure in the data [8, 11] . New graphical algorith mic approaches have been proposed to automatically\ndiscover the causal structure in a body of data, given certain hypotheses [2, 5, 6, 9, 12, 14] . Graph the ory [17] has now emerged as a mathematical language for causality. Graphs provide a strong notational sys tem for concepts and relationships not easily expressed by equations and probabilities, and there has been an increased understanding of the fundamental relation ships between graphs, causality, and probability.\nConstraint-based approaches [9, 14] use the condi tional independence relations present in a body of statistical data to graphically infer the causal struc ture present in the data. Based on strong connections between graph theoretic properties and statistical as pects of causal influences, fundamental assumptions about the data (the Markov condition and the faithful ness condition) are used to infer a graphical structure, which represents statistical dependency relations on a set of variables. Such (marginal) dependency graph is used to construct models describing the exact causal relations in the data. Since causal relations cannot be inferred from statistical data alone, a marginal depen dency graph is entailed by a possibly infinite equiv alence class of models representing competing causal alternatives.\nUnder the assumption of causal sufficiency, there is a one to one correspondence between statistical depen dency and direct causal relation. Two causal mod els without latents which entail the same statistical dependencies are called (local) Markov equivalent. Every Markov equivalence class of models is finite, and the variability of structure of models within the same Markov equivalence class is very limited. Indeed any two Markov equivalent causal models always share the same causal connections between their variables, but the direction of the causal influences can vary [1, 7, 16] .\nIf the data contains correlated errors, the assumption of causal sufficiency fails, and latent variables must be introduced in models to explain the causal structure in the data [13] . Two causal models with latent variables which entail the same marginal dependencies are called\n106 Desjardins\nsemi-Markov equivalent. However marginal statis tical dependency does not entail direct causal connec tion. Given a model M with latent variables, the set of competing semi-Markov equivalent alternatives is not only large, it is infinite. One can indeed always add new latent variables to a model without chang ing the pattern of statistical dependencies between the measured variables. Although there is a well defined test to determine the semi-Markov equivalence of two causal models with latent variables (they must entail the same marginal dependency graph) (14, 15], there is currently no systematic approach for exploring the variability of structure within a semi-Markov equiva lence class of causal models.\nIn this paper, given an infinite semi-Markov equiva lence class of models entailing a marginal dependency graph G, we present a finite subset of that class which captures the essential variability of causal structure within the equivalence class, and offer graphical rules for systematically constructing from G any model in that finite subset. After demonstrating that current attempts at generating semi-Markov equivalent alter natives to any given model using local graphical trans formation rules are too limited, we show that our graphical rules can also be used for that purpose.\n2 Formal preliminaries\nCausal models are best represented by graphs. We first introduce all the important graph theoretical elements required in this paper (14, 17] . We assume that the reader is already familiar with the very basic concepts from graph theory.\nA directed acyclic graph (DAG) is a pair (V, E) such that V is a non empty finite set of elements called vertices (or variables), E is a finite set of ordered pairs of elements of V called edges, and in which there are no cycles. If (V1, V2) E E, then there is an edge from V1 into V2, represented as V1 -+ V2. Figure 1 (left) shows an example of such a DAG. \ufffdy conven tion latent variables are represented by cucles, and obs\ufffdrvables by squares. A causal graph is a directed acyclic graph in which for each (V1 , V2) E E, V1 is a direct cause of V2. A causal model is a structure M =< G, P >,where G is a causal graph over a set of variables V, and P is a probability distribution over V. We often identify causal models with their causal graphs in situations not involving P specifically.\nThe expression I(A, C, B) represents the conditional independence relation of variable A and B given the variables in set C. A causal model < G, P > satis fies the Markov condition if every variable in G is conditionally independent of its non-parents and non descendants given its parents. < G, P > satisfies the\nfaithfulness condition if all and only the conditional independence relations true in P are entailed by the Markov condition applied to< G, P >.\nA collider on a path U is a variable V (excluding the endpoints of U) receiving causal edges from both its two adjacent variables on U (-+ V f-). In a causal graph, two different variables vl and v2 are d-separated given a set of (other) variables W if and only if there is no undirected path u between vl and v2 such that every collider on u has a descendent in W and no other variable on U is in W. They are d connected if and only if they are not d-separated. An important theorem is: in a causal model M, if variables vl and v2 are d-separated given a set of variables w then I(V1 , W, V2) (9].\nAn inducing path (relative to a set S of variables) from variable vl to variable v2 in a causal graph is a path U from V1 into V2 (V1 . . . -+V2, V1 # V2) such that every variable in S \\ { V1 , V2} is a collider on U , and every collider on u is an ancestor of either vl or v2. There is an inducing path from variable V1 to variable V2 if and only if V1 and V2 are not d-separated given any subset of S \\ {V1, V2}. An inducing path represents a statistical dependency implied by a causal model on a marginal distribution.\nThe inducing path graph (IPG) entailed by a causal model M relative to a subset S of the varial;>les in M is a DAG with two kinds of edges: (1) V1 \ufffd V2, in dicating that there is an inducing path in M relative to S between V1 and V2 into V2, but not into Vt , and\nOn the Semi-Markov Equivalence of Causal Models 107\nsame adjacencies as IP G ( M) , but contains only edge orientations that are common to the IPGs of all mod els semi-Markov equivalent toM. Missing orientations are indicated by small circles.\n3 Variability of causal models\nCurrent constraint-based causal model discovery algo rithms do not discover causal models from data. They rather discover marginal dependency graphs [14], rep resenting an infinite equivalence class of semi-Markov equivalent causal models . We seek to study the vari ability of structure of models in that equivalence class, and to offer an approach to construct these models.\nA marginal dependency graph G represents a finite set S of semi-Markov equivalent IPGs entailing G, and contains only edge orientations that are common to all the IPGs in S. A semi-Markov equivalence class of models can thus be finitely partitioned into infinite subclasses of models, the models within each subclass entailing the same IPG. Each IPG inS is a completion of G. Any completion of G must obey certain rules to represent an IPG: it must be acyclic, and must respect certain internal consistency rules, indicated in the fol lowing theorem (proved in appendix) :\nTheorem 1 Let G be an acyclic graph with !.!+ edges and \ufffd edges. Let RI be the following rule: if in G . \ufffd \ufffd there zs an edge A -=t B and an edge B f+ C and B is an ancestor of C in G, then there must be an edge\nA \ufffd C in G. RI must hold in all IPGs. Let R2 be the following rule: if in G there is a path of \ufffd edges between variables A and B, and every variable on that path is an ancestor of A or B, then there must be an\nedge A \ufffd B in G. R2 must hold in all IPGs. An extended DAG containing \ufffd edges is an IPG if and only if it is closed under the set of rules {RI, R2}.\nGiven a marginal dependency graph G, the finite set of all of its acyclic completions closed under rules RI and R2 is the set of all semi-Markov equivalent IPGs entail ing G. Without causal sufficiency, any given IPG Gi is entailed by an infinite set Si of semi-Markov equiva lent causal models. We seek to identify a finite subset of Si capturing the essential variability of causal struc ture in Si by expanding Pearl's [9] notion of \"minimal models\" to models with latent variables.\n3.1 Minimal models\nIn [9], Pearl intuitively defines a causally sufficient minimal model as a graph in which every proper sub graph does not satisfy the Markov condition. We now extend this definition to models with latents [3].\nLet SI be the set of all DAGs. Let M be in SI. Let e be in E, the set of all edges of M. Let VI , v2 be in V, the set of all vertices of M. The operator REe () : S I --+ S 1 is defined such that REe(M) is the same DAG as M, but with edge e removed. The operator CVv1,v2 () : SI --+ SI is defined such that CV v1,v2 (M) is the same DAG as M, but with variable v2 substituted for VI in all edges of M, and all self-pointing edges removed. Thus VI and v2 become collapsed in the new DAG. RE stands for \"remove edge\" and CV for \"collapse variables\" .\nLet M be a model. Let OI be the set of all possible operators REe(), where e is an edge in M. Let 02 be the set of all possible operators CV v1 ,v2 (), where VI and v2 are distinct variables in M. A reducing transformation R is a finite composition of operators in OI U 02. If there is a reducing transformation R such that IPG(R(M)) = IPG(M), then the model R( M) is a reduction of M. The model is minimal (or irreducible) if for every reduction R(M) of M, we have R(M) = M.\nIn other words, a minimal model is a model in which it is not possible to remove an edge or to collapse two variables in its graph without always ending up with a new model which has a different inducing path graph. Although minimal models are defined in terms of in ducing path graphs for practical reasons, this turns out to be equivalent to a definition using conditional inde pendence relations (as in the causally sufficient case) if only latent variables are allowed to be collapsed.\nAn edge in a causal model M is an essential edge if it cannot be removed without changing the induc ing path structure of M. Minimal models have only essential edges, and are the simplest models satisfying a causal theory. Minimal models do not imply simple models, but rather simplest models. They can be quite complex and contain many latent variables, including embedded and causally related latents. The essential variability of an infinite set of semi-Markov equivalent causal models is fully captured by its finite subset of minimal models [4], although a formal proof of this fact is beyond the scope of this paper.\n3.2 Constructing causal models\nWe now clarify the relation between IPGs and causal models. Consider a causal model M over the set V of variables { V1 , ... , Vn}. The IPG of M over the sub set V' of the observable variables in V is a graph G with variables V'. In G, if there is a directed edge\nA \ufffd B, then either A is a direct cause of B, or there is an indirect inducing path between A and B in M. As a reminder, an inducing path between A and B relative to V' is a path U from A into B involving a\n108 Desjardins\ncombination of variables in V' (observables) and vari ables in V\" = V \\ V' (latents). Variables in V' on U must all be colliders and ancestors of A or B. Vari ables in V\" on U can be pretty much anything, but if they are colliders, they must be ancestors of A or B. So the causal relations between observable variables in the causal model are strongly determined by the edges in the lPG, while the causal relations involving latent variables are not as constrained. In G, if there is a\nbidirected edge A \ufffd B, then by acyclicity, there must be a latent common cause T between A and B. This might be a direct common cause, or a common cause of some other pair of variables (that could include A or B), which spreads to A and B through the structure of the inducing paths.\nLet G be an lPG. We show that each model M in the set of semi-Markov equivalent minimal models entail ing G is a systematic graphical transformation of G.\nAn expansion M; of IPG G is a graphical transforma tion of G made by replacing in G every edge of the form\niP. A --'+ B by the causal edge A --+ B, and every edge of iP. the form A A B by a latent path A f-LAB --+ B, possibly combined with a hidden edge A --+ B or B --+ A (making sure the hidden edge does not create new in ducing paths or cycles). Each M; is a causal model entailing G and is uniquely determined by the choice of hidden edges in G.\nGraphical operators RE() and CV() have already been defined. We now define two additional operators. First, the operator ELv,,v,,v,,l, () : S1 --+ S1 is defined such that ELv,,v2,v3,z, (M) has the same causal graph as M, but if there is a pattern [ v1 --+ v2, l1 --+ v2, l1 --+ v3] in the model where l1 is a latent variable and v1, v2 and v3 are observable variables, it is replaced by the pattern [v1--+ l1,h--+ v2,h--+ vs,l2--+ v2,l2--+ v3], where l2 is a new latent. Second, the operator CLz,,t,,v() : S1--+ S1 is defined such that CLt,,z,,v(M) has the same causal graph as M, but if there is a pat tern ll1 --+ v, l2 --+ v] in the model where h and l2 are latent variables and v is an observable variable, it is replaced by the pattern [l1 --+ l2, l2 --+ v]. The first operator produces embedded latents (EL stands for \"embed latent\"), while the second produces causally connected latents ( CL stands for \"connect I a tents\"). Note that the EL() operator introduces a new latent, because of the fact that minimal models with embed ded latents can have in some rare cases more latent variables than the number of \ufffd edges in the lPG [4]. In most cases in practice, a simpler version EL*() is used, which does not introduce a new latent. We now have the following important theorem (the algorithm is sketched in appendix, and the full proof can be found in [4]): \u00b7\nTheorem 2 Let G be an IPG. Every minimal model M; entailing G is a well defined graphical transforma tion of an expansion of G using operators RE, CV,"}, {"heading": "ELand CL.", "text": "We not only have the tools to construct a minimal model entailing a given marginal dependency graph, but we have the tools to construct all such minimal models. The computational complexity of minimal model construction is on average O (n3) per model, where n is the number of variables in the marginal dependency graph [4]. Given a marginal dependency graph G, the number of minimal models entailing G increases exponentially with the number of edges in G, and strongly depends on the number of missing orien tations in G. For marginal dependency graphs with many missing orientations, one can most efficiently constrain the set of IPGs (and therefore the number of minimal models) by using background know ledge on the observable variables.\n4 Constructing equivalent alternatives\nIn the causally sufficient case, it has been formally demonstrated that any model Markov equivalent to some model M can be constructed from M by a se quence of edge reversal operations using only local constraints [1, 7, 16]. A natural approach with semi Markov equivalent models is to extend this set of local graphical rules to take into account latent variables. This is what Pearl attempts in [10]. In this paper, Pearl uses B edges in causal graphs to indicate corre lated errors between two variables. He then proposes two graphical transformation rules to construct linear semi-Markov equivalent models:\nRule 1: X --+ Y is interchangeable with X B Y if every neighbor (variable connected by a B edge) or parent of X is adjacent to Y.\nRule 2: X --+ Y can be reversed into X f- Y if every neighbor or parent of Y (excluding X) is adjacent to X, and every neighbor or parent of X is adjacent to Y.\nThese rules are unfortunately flawed (for both general\u00b7 models and linear models). Let M be a causal model, in which every latent variable has no parent and is the common parent of two observable variables. In such models, subpaths A t- L --+ B can be replaced by edges A B B, representing correlated errors. Pearl uses such models with B edges in his study of semi Markov equivalence. We now include a counterexam ple to Pearl's rule 2. A counterexample to Pearl's rule 1 can be easily produced using the same principles. Consider the following causal model M1 of Figure 2, where B B C indicates a correlated error between B\nOn the Semi-Markov Equivalence of Causal Models 109\nAccording to Pearl's rule 2, the edge C-+ D can be re versed into C +- D to produce semi-Markov equivalent model M2, since the parent B of D is adjacent to C, and the neighbor B of C is adjacent to D. But M2 is not semi-Markov equivalent to M1. Indeed since B is an ancestor of C in M2, there is now an inducing path from A to C in M2, which is not present in M1. Since M1 and M2 do not entail IPGs of same adjacencies, they cannot be semi-Markov equivalent.\nPearl's rules may be flawed, but they can be trans formed into more specialized rules, which are not flawed.\nRule 1': X -+ Y is interchangeable with X f+ Y if every neighbor of X is a neighbor of Y, and every parent of X is a parent of Y.\nRule 2': X -+ Y can be reversed into X f- Y if every neighbor of Y is a child of X, every neighbor of X is a child of Y, and every parent of X is a parent of Y.\nThese rules are sufficient, but neither is necessary. This set can be complemented by many additional suf ficient and specialized rules. Such large sets of special ized sufficient rules enable the construction of a hand ful of models semi-Markov equivalents to any given model. But this approach fails to consider the exten sive variability of graphical structure in sets of semi Markov equivalent causal models.\ncause of three different observable variables, while M2 contains an embedded latent. In M2, observables B and D are neither connected directly nor through a common latent. One cannot construct M2 from M1 using simple edge replacement or edge reversal oper ations. Any single edge transformational approach to generating semi-Markov equivalent causal models is thus too limited. A larger and more powerful set of graphical operators is required. The set of operators proposed in the previous section turns out to be suffi cient for that purpose. We now offer an approach to generate models semi-Markov equivalent to any model M by first determining the marginal dependency graph of M, and then constructing minimal models entailing the same marginal dependency graph.\nIn [14, 15] , it is formally demonstrated that two causal models with latent variables are semi-Markov equiva lent if and only if they entail the same marginal de pendency graph. In order to find models semi-Markov equivalent to some causal model M, we first extract from M it's marginal dependency graph. This is a two step process: (1) determine the lPG forM, and (2) ex tract the marginal dependency graph of this lPG. The first step is a well defined graphical transformation of M. The second step is unfortunately an open prob lem, which we do not attempt to solve in this paper. Instead, we use a modified version of the TETRAD al gorithm of Spirtes, et al. [14] to reconstruct it in three steps: (1) remove all edge orientations from the lPG, (2) let A, B and C be any triple of vertices such that A is adjacent to B and B is adjacent to C, but A is not adjacent to C in the lPG; if there is no set Z of observ able variables in M such that A and C are d-separated by B U Z, then make B a collider between A and C, otherwise make B a definite non-collider between A and C, and (3) use the TETRAD orientation rules to determine as many missing orientations as possible. The problem is open because it is not formally proven that TETRAD's set of orientation rules is complete.\nOnce the marginal dependency graph G is determined, it suffices to use the already introduced graphical ma chinery to generate any causal model entailing G. All such models are semi-Markov equivalent toM.\n4.1 Example\nFigure 4 illustrates how the two semi-Markov equiva lent minimal models of Figure 3 are constructed from their common marginal dependency graph (MDG): two self-consistent semi-Markov equivalent inducing path graphs are constructed (IPG1 and IP G2), by performing different completions of the missing orien tations (small circles). The non minimal expansions EX Pt and EX P2 are then produced from the respec tive inducing path graphs by replacing correlated er-\n110 Desjardins\nrors by common latents causes and choosing which hid den causal edges to include: no hidden causal edge for EXP1 and hidden edge C--+ D for EXP2. Note that a hidden edge D --+ C would not be allowed in EX P2, as it would entail the inducing path E $. C, which is not present in I P G2. Then M1 is constructed from EX P1 by collapsing its three latents into a single la tent L, and M2 is constructed from EX P2 by remov ing the non essential causal edge B --+ D (since the path B --+ C t- L --+ D generates the inducing path B $.D) and embedding L between A, C and D. To perform the inverse operation (dotted arrows) and construct the marginal dependency graph from each minimal model, it suffices to compute the inducing path graph for each model, and then use the Tetrad algorithm to compute their common marginal depen dency graph.\nThe same example is used to illustrate how to con struct semi-Markov equivalent alternatives to any given model: by first determining the marginal de pendency graph, and then making choices of missing orientations and hidden edges to produce IPG expan sions, which are then reduced by graphical transfor mations to produ.ce different minimal models. There are obviously finitely many different IPG expansions produced from a marginal dependency graph. It is formally demonstrated in [4) that the total number of semi-Markov equivalent minimal models entailing a marginal dependency graph is also finite. The en tire set of such minimal models can be efficiently con structed using our approach, whose main computa tional advantage is the important restriction of the search space of models.\n5 Conclusion\nThese results are part of a larger study on the theoret ical limits to reliable causal inference [ 4). Constraint based causal models discovery algorithms determine marginal dependency relations between variables, and implicitly represent sets of semi-Markov equivalent models by a single graphical structure. In this paper, we provided the formal tools to generate causal mod els entailing this graphical structure. Neither Markov equivalence nor semi-Markov equivalence entail distri bution equivalence. By explicitly capturing the essen tial variability of graphical structure within a semi Markov equivalence class of models, insights can be obtained about the formal properties of these mod els, and methods can be proposed to experimentally or observationally discriminate between non distribu tion equivalent models within the same semi-Markov equivalence class [4).\nThese results have also practical usefulness. Using our approach, a researcher using causal models can now, given any model satisfying a body of statistical data over a set of observable variables, automatically and systematically generate any simple alternative model explaining the data. Such alternatives might prove a valuable source of explanatory insight.\nOn the Semi-Markov Equivalence of Causal Models 111\n6 Appendix: Proofs\nTheorem 1 Let G be an acyclic graph with $ edges and \ufffd edges. Let R1 be the following rule: if in G there is an edge A $ B and an edge B \ufffd C and B is an ancestor of C in G, then there must be an edge A $ C in G. R1 must hold in all IPGs. Let R2 be the following rule: if in G there is a path of \ufffd edges between variables A and B, and every variable on that path is an ancestor of A orB, then there must be an edge A \ufffd B in G. R2 must hold in all IPGs. An extended DA G containing \ufffd edges is an IPG if and only if it is closed under the set of rules {R1, R2}.\nTo prove the theorem, we first prove this short lemma:\nLemma 3 Let M be a causal model with IPG G. If there is a directed path from variable A to variable B in G, then A is an ancestor of B in M.\n\u2022 iP. iP. iP. Proof: Let U be the directed path A -=+ V1 -=+ . . . -=+ Vn $ B in G. Each inducing path Vk $ Vk+l in G corresponds to a path Uk in M that is out of Vk, and in which every collider is an ancestor of either Vk or Vk+l\u00b7 Let Wk be the first collider on Uk starting at Vk. By acyclicity Wk must be an ancestor of Vk+l and not of Vk. Therefore Vk must be an ancestor of Vk+l in M. Since this applies to every edge on U, then A must be an ancestor of B in M. 0\nThe theorem can now be proven.\niP. Proof: Assume M entails IPG G. An edge A -=+ B in G implies that there is an inducing path U1 from A into B in M, and by the previous lemma that A is an ancestor of B in M. An edge B \ufffd C in G implies that there is an inducing path U2 between A and B into both B and C in M. By concatenating the two paths U1 and U2 at B into a new path U , B becomes a collider on U, and is an ancestor of C. Thus U is an inducing path from A into C. So R1 holds in every iP. iP. iP. iP. IPG. Let A B vl B . . . B Vn B B be a path between A and B in G, with each Vk ancestor of A or B (by convention A and B are ancestors of themselves) . The edge Vk \ufffd Vk+1 in G implies that there is an induc ing path uk between vk and vk+l into both vk and Vk+l in M, with every collider being an ancestor of Vk or Vk+l, and therefore of A or B by hypothesis. The concatenation of all these Uk creates a path in which every observable is a collider, and every collider is an ancestor of either A or B, thus an inducing path into A and into B. So R2 holds in every IPG. Closure un der R1 and R2 is therefore necessary in an IPG. Since\nany observable in an inducing path (except the end points) must be a collider, the only other combination of inducing paths not already covered that creates a new inducing path is A $ V1 \ufffd . . . \ufffd Vn \ufffd B, with each Vk ancestor of A or B, which implies the inducing path A .$ B. But by acyclicity V1 is necessarily ancestor of B, thus each Vk is ancestor of B, which\nimplies by R2 the path V1 \ufffd B. But then R1 with A$ V1 \ufffd B implies A$ B. Thus closure under R1 and R2 is sufficient. 0\nTheorem 2 Let G be an IPG. Every minimal model"}, {"heading": "M; entailing G is a well defined graphical transforma tion of an expansion of G using operators RE, CV, ELand CL.", "text": "Here we only provide a sketch of the model construc tion algorithm. The full detailed proof, which is quite long and involves the reverse construction of minimal models of decreasing complexity, can be found in (4] .\nProof: Given an expansion of G, the following sets of minimal models of increasing complexity are sequen tially generated: those involving only external latents, those also involving embedded latents, and those also involving connected latents. All generated models en tail the same IPG G.\nFirst, given an expansion M1 of G, all non essential edges A -+ B (A, B observables) are removed from M1 to produce model M2 entailing the same IPG G. A non essential edge A-+ B is easily identified in M1 by a simple graphical rule ( lemma 4), and removing any such non essential edge from M1 does not make an initially non essential edge become essential. Hidden edges are treated as special edges in the processing. The next step is to remove all non-essential latents L from M2 (equivalent to removing two non-essential edges per latent) to produce model M3 entailing the same IPG. All non essential latents in M2 are identified using simple graphical rules. Unfortunately removing a non-essential latent L can occasionally make some other non-essential latent L ' become essential. The order of removal of non-essential latents is therefore important, and M3 is usually not unique, but rather represents a very small set of alternatives.\nNext, for each model M3, collapsibility of latents is as sessed using simple graphical rules, and latents are col lapsed to produce minimal model M4. Unfortunately, collapsing two latents can make two initially collapsi ble latents become non-collapsible. The order of col lapsibility is therefore important, and M4 is again not unique, but represents a finite set of alternatives. The entire set of minimal models with external latents is generated using the previous few steps.\n112 Desjardins\nNext, for each model M4, embeddability of latents is assessed using simple graphical rules, and latents are embedded to produce minimal model M5. Many com bination of embeddings can be performed, with the result still being a minimal model. Thus M5 is not unique. This generates the entire set of minimal mod els with embedded latents entailing the same lPG.\nFinally, for each model M5, connectibility of latents is assessed using simple graphical rules, and latents are connected to produce minimal model M6\u2022 Many combination of latent connections can be performed, with the net result still being a minimal model. Thus M6 is not unique. But this generates the entire set of minimal models entailing the same lPG. 0\nIn the algorithm, simple graphical rules are used to as sess features of variables or edges. Most of the power of the algorithm comes from the simplicity of such rules. Here as an example is the rule for determining the non essentiality of edges in M1. A complete description of the other rules can be found in [4]\nLemma 4 Let E = A -+ B be a non hidden edge in the expansion M1 of IPG G. E is non essential {in all models of G) if and only if there is a path U1"}, {"heading": "A-+ C +- L-+ B in M1 with C ancestor of B.", "text": "Proof: Assume there is such a path U1 in G1. U1 is an inducing path from A into B in M1 not involving E , so E is non essential. For the converse, assume that E is non essential. Then there must be an inducing path U between A and B into B in M1. Then U starts with A -+ C for some observable C and continues with alternating latent common causes and observable col liders. Since by acyclicity C cannot be an ancestor of A, it is an ancestor of B. Every collider on U is an ancestor of A or B, thus every collider is an ancestor of C or B and therefore of B. This implies the ex istence of an inducing path between C and B that is\ninto both C and B. Thus there is a C \ufffd B edge in G and therefore there is an A -+ C +- L -+ B path in M1, with C ancestor of B. 0\nReferences\n[1] Chickering DM: A transformational character ization of Bayesian network structures. In P. Besnard and S. Hanks (Eds.), Uncertainty in AI 11, Morgan Kaufmann, San Francisco, CA, 87-98, 1995.\n[2] Cooper, GF, Herskovits E: A Bayesian method for the induction of probabilistic networks from data. Machine Learning 9, 1992, pp 309-347.\n[3] Desjardins B: Equivalence of causal theories. lOth Internat Congr of Logic, Methodology, and Phi losophy of Science, Florence, Italy, August 1995.\n[4] Desjardins B: On the theoretical limits to reliable causal inference. PhD dissertation, University of Pittsburgh, 1998.\n[5] Glymour C, Scheines R, Spirtes P, Kelly K: Dis covering causal structure. Academic Press, New York, 1987.\n[6] Beckerman D, Geiger D, Chickering, D: Learning Bayesian networks: The combination of knowl edge and statistical data. Machine Learning 20, 1995, pp 197-243.\n[7] Meek C: Causal inference and causal explanation with background knowledge. In P. Besnard and S. Hanks (Eds.), Uncertainty in AI 11, Morgan Kaufmann, San Francisco, CA, 403-410, 1995.\n[8] Mosteller F, Tukey J: Data analysis and regres sion , A second course in regression. Addison Wesley, Massachusetts, 1977.\n[9] Pearl J: Probabilistic reasoning in intelligent sys tems. Morgan Kauffman, San Mateo CA, 1988.\n[10] Pearl J: Graphs, Causality and Structural Equa tion Models, Technical Report (R-253), UCLA Cognitive Systems Laboratory, August 1997. Pre pared for Sociological Methods and Research, Special Issue on Causality.\n[11] Rawlings J: Applied regression analysis. Wadsworth, Belmont, CA, 1988.\n[12] Schacter R: Evaluating influence diagrams. Oper ations Research, 34(6), 1986.\n[13] Spirtes P: Building causal graphs from statisti cal data in the presence of latent variables. In B Skyrms (ed), Proc IX Intern Congr on Logic, Methodology and Philosophy of Science, Uppsala, Sweden, 1991.\n[14] Spirtes P, Glymour C, Scheines R: Causation , Prediction, and Search. Springer-Verlag, New York, 1993.\n[15] Spirtes P, Verma T: Equivalence of causal mod els with latent variables. Technical Report CMU Phil-33, June 1994.\n[16] Verma T, Pearl J: Equivalence and synthesis of causal models. Proc Sixth conf on Uncertainty in AI, Mountain View, CA, pp 220-227, 1991.\n[17] Wilson RJ: Introduction to graph theory. Long man, Essex, England, 1985."}], "references": [{"title": "A transformational character\u00ad ization of Bayesian network structures", "author": ["DM Chickering"], "venue": "Uncertainty in AI", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["GF Cooper", "Herskovits E"], "venue": "Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Equivalence of causal theories. lOth Internat Congr of Logic, Methodology, and Phi\u00ad losophy of Science, Florence", "author": ["B Desjardins"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "On the theoretical limits to reliable causal inference", "author": ["B Desjardins"], "venue": "PhD dissertation, University of Pittsburgh,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Dis\u00ad covering causal structure", "author": ["C Glymour", "R Scheines", "P Spirtes", "K Kelly"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1987}, {"title": "Learning Bayesian networks: The combination of knowl\u00ad edge and statistical data", "author": ["D Beckerman", "D Geiger", "Chickering D"], "venue": "Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Causal inference and causal explanation with background knowledge", "author": ["C Meek"], "venue": "Uncertainty in AI", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Data analysis and regres\u00ad sion", "author": ["F Mosteller", "J Tukey"], "venue": "A second course in regression. Addison\u00ad Wesley, Massachusetts,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1977}, {"title": "Probabilistic reasoning in intelligent sys\u00ad tems", "author": ["J Pearl"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1988}, {"title": "Graphs, Causality and Structural Equa\u00ad tion Models, Technical Report (R-253)", "author": ["J Pearl"], "venue": "UCLA Cognitive Systems Laboratory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Applied regression analysis", "author": ["J Rawlings"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "Evaluating influence diagrams", "author": ["R Schacter"], "venue": "Oper\u00ad ations Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Building causal graphs from statisti\u00ad cal data in the presence of latent variables", "author": ["P Spirtes"], "venue": "In B Skyrms (ed), Proc IX Intern Congr on Logic, Methodology and Philosophy of Science, Uppsala,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1991}, {"title": "Equivalence of causal mod\u00ad els with latent variables", "author": ["P Spirtes", "T Verma"], "venue": "Technical Report CMU\u00ad", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Equivalence and synthesis of causal models", "author": ["T Verma", "J Pearl"], "venue": "Proc Sixth conf on Uncertainty in AI, Mountain View,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1991}, {"title": "Introduction to graph theory", "author": ["RJ Wilson"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1985}], "referenceMentions": [{"referenceID": 7, "context": "Although traditional statistical approaches are excel\u00ad lent for finding statistical dependencies in a body of data, they prove inadequate at finding the causal structure in the data [8, 11] .", "startOffset": 182, "endOffset": 189}, {"referenceID": 10, "context": "Although traditional statistical approaches are excel\u00ad lent for finding statistical dependencies in a body of data, they prove inadequate at finding the causal structure in the data [8, 11] .", "startOffset": 182, "endOffset": 189}, {"referenceID": 1, "context": "New graphical algorith\u00ad mic approaches have been proposed to automatically discover the causal structure in a body of data, given certain hypotheses [2, 5, 6, 9, 12, 14] .", "startOffset": 149, "endOffset": 169}, {"referenceID": 4, "context": "New graphical algorith\u00ad mic approaches have been proposed to automatically discover the causal structure in a body of data, given certain hypotheses [2, 5, 6, 9, 12, 14] .", "startOffset": 149, "endOffset": 169}, {"referenceID": 5, "context": "New graphical algorith\u00ad mic approaches have been proposed to automatically discover the causal structure in a body of data, given certain hypotheses [2, 5, 6, 9, 12, 14] .", "startOffset": 149, "endOffset": 169}, {"referenceID": 8, "context": "New graphical algorith\u00ad mic approaches have been proposed to automatically discover the causal structure in a body of data, given certain hypotheses [2, 5, 6, 9, 12, 14] .", "startOffset": 149, "endOffset": 169}, {"referenceID": 11, "context": "New graphical algorith\u00ad mic approaches have been proposed to automatically discover the causal structure in a body of data, given certain hypotheses [2, 5, 6, 9, 12, 14] .", "startOffset": 149, "endOffset": 169}, {"referenceID": 15, "context": "Graph the\u00ad ory [17] has now emerged as a mathematical language for causality.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "Constraint-based approaches [9, 14] use the condi\u00ad tional independence relations present in a body of statistical data to graphically infer the causal struc\u00ad ture present in the data.", "startOffset": 28, "endOffset": 35}, {"referenceID": 0, "context": "Indeed any two Markov equivalent causal models always share the same causal connections between their variables, but the direction of the causal influences can vary [1, 7, 16] .", "startOffset": 165, "endOffset": 175}, {"referenceID": 6, "context": "Indeed any two Markov equivalent causal models always share the same causal connections between their variables, but the direction of the causal influences can vary [1, 7, 16] .", "startOffset": 165, "endOffset": 175}, {"referenceID": 14, "context": "Indeed any two Markov equivalent causal models always share the same causal connections between their variables, but the direction of the causal influences can vary [1, 7, 16] .", "startOffset": 165, "endOffset": 175}, {"referenceID": 12, "context": "If the data contains correlated errors, the assumption of causal sufficiency fails, and latent variables must be introduced in models to explain the causal structure in the data [13] .", "startOffset": 178, "endOffset": 182}, {"referenceID": 8, "context": "We seek to identify a finite subset of Si capturing the essential variability of causal struc\u00ad ture in Si by expanding Pearl's [9] notion of \"minimal models\" to models with latent variables.", "startOffset": 127, "endOffset": 130}, {"referenceID": 8, "context": "In [9], Pearl intuitively defines a causally sufficient minimal model as a graph in which every proper sub\u00ad graph does not satisfy the Markov condition.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "We now extend this definition to models with latents [3].", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "The essential variability of an infinite set of semi-Markov equivalent causal models is fully captured by its finite subset of minimal models [4], although a formal proof of this fact is beyond the scope of this paper.", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "because of the fact that minimal models with embed\u00ad ded latents can have in some rare cases more latent variables than the number of \ufffd edges in the lPG [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 3, "context": "We now have the following important theorem (the algorithm is sketched in appendix, and the full proof can be found in [4]): \u00b7 Theorem 2 Let G be an IPG.", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "The computational complexity of minimal model construction is on average O ( n3) per model, where n is the number of variables in the marginal dependency graph [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 0, "context": "In the causally sufficient case, it has been formally demonstrated that any model Markov equivalent to some model M can be constructed from M by a se\u00ad quence of edge reversal operations using only local constraints [1, 7, 16].", "startOffset": 215, "endOffset": 225}, {"referenceID": 6, "context": "In the causally sufficient case, it has been formally demonstrated that any model Markov equivalent to some model M can be constructed from M by a se\u00ad quence of edge reversal operations using only local constraints [1, 7, 16].", "startOffset": 215, "endOffset": 225}, {"referenceID": 14, "context": "In the causally sufficient case, it has been formally demonstrated that any model Markov equivalent to some model M can be constructed from M by a se\u00ad quence of edge reversal operations using only local constraints [1, 7, 16].", "startOffset": 215, "endOffset": 225}, {"referenceID": 9, "context": "This is what Pearl attempts in [10].", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "In [14, 15] , it is formally demonstrated that two causal models with latent variables are semi-Markov equiva\u00ad lent if and only if they entail the same marginal de\u00ad pendency graph.", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "A complete description of the other rules can be found in [4]", "startOffset": 58, "endOffset": 61}], "year": 2011, "abstractText": "The variability of structure in a finite Markov equivalence class of causally sufficient mod\u00ad els represented by directed acyclic graphs has been fully characterized. Without causal suf\u00ad ficiency, an infinite semi-Markov equivalence class of models has only been characterized by the fact that each model in the equiva\u00ad lence class entails the same marginal statis\u00ad tical dependencies. In this paper, we study the variability of structure of causal models within a semi-Markov equivalence class and propose a systematic approach to construct models entailing any specific marginal statis\u00ad tical dependencies.", "creator": "pdftk 1.41 - www.pdftk.com"}}}