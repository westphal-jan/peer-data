{"id": "1301.7381", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Hierarchical Solution of Markov Decision Processes using Macro-actions", "abstract": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macroactions as local policies that act in certain regions of state space, and by restricting states in the abstract MDP to those at the boundaries of regions where they do not have the capacity for the state space to exist. In contrast, we propose that a hierarchical model (an abstract MDP) will represent the same local policy as a state-based approach to policy analysis.", "histories": [["v1", "Wed, 30 Jan 2013 15:04:16 GMT  (413kb)", "http://arxiv.org/abs/1301.7381v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["milos hauskrecht", "nicolas meuleau", "leslie pack kaelbling", "thomas l dean", "craig boutilier"], "accepted": false, "id": "1301.7381"}, "pdf": {"name": "1301.7381.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Solution of Markov Decision Processes using Macro-actions", "authors": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "emails": ["tld}@cs.brown.edu", "cebly@cs.ubc.ca"], "sections": [{"heading": null, "text": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of\nMarkov decision processes. Unlike current mod\nels that combine both primitive actions and macro-actions and leave the state space un changed, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the\nstate space. This is achieved by treating macro actions as local policies that act in certain regions of state space, and by restricting states in the ab stract MDP to those at the boundaries of regions.\nThe abstract MDP approximates the original and can be solved more efficiently. We discuss sev\neral ways in which macro-actions can be gen erated to ensure good solution quality. Finally, we consider ways in which macro-actions can be\nreused to solve multiple, related MDPs; and we\nshow that this can justify the computational over head of macro-action generation.\n1 Introduction\nMarkov decision processes (MDPs) [11, 22] have proven tremendously useful as models of stochastic planning and\ndecision problems. However, traditional dynamic pro gramming remains computationally intractable for practi cal problems, requiring time polynomial in the size of the\nstate and action spaces, but where these spaces are gener ally too large to be explicitly enumerated. Considerable\nresearch has been directed toward the solution of Markov decision processes (MDPs) with large state and action\nspaces. These include function approximation [2], reach ability analyses [5] and aggregation techniques [7, 3, 4].\nDespite these advances, little attention has been paid to the reuse of policies or value functions generated for one MDP in the solution of a related MDP. While such reasoning is\ncommon in classical planning-for instance, through the use of macros [8, 16, 13] or plan repair strategies [15]-its application in stochastic settings is less common. Suitable techniques of this type could lead to the amortization of solution costs over a large number of problems, and the ability to solve future problem instances quickly, which is\ncritical to on-line reasoning.\nOne of the few models to deal with solution reuse within the MDP framework is the Skills model of Thrun and Schwartz [24], which attempts to learn how to reuse policy fragments (or skills) for different tasks. Another is found in the work of Sutton and his colleagues [23, 20, 2 1], who have developed models of macro-actions for MDPs that can be reused to solve multiple MDPs when objectives (or goals) change. In particular, macros are viewed as \"local\"\npolicies that are implemented until some termination con dition is met, at which point a new macro (or any other ac\ntion) can be applied. Key to the success of this framework\nis the ability to construct models of macro-actions that al low them to be treated as if they were ordinary actions in\nthe original MDP.\nIn this paper, we continue the investigation of the use of macros in MDPs; specifically, we focus on the problem of\nplanning with macro-actions addressed by Precup, Sutton and Singh [21]. Our main aim is the development of a dif ferent model for planning with macros that deals with some\nof the computational problems associated with this earlier model (the PSS model). In particular, while the PSS model allows macros designed for one MDP to be applied to a re lated MDP, it still relies on explicit dynamic programming over the state space of the related MDP (and a larger ac\ntion space). Thus it does nothing to alleviate the problem\nof large state spaces. Furthermore, the use of macros is not\nguaranteed to reduce the time required to find an optimal solution.\nWe present in Section 2 a hierarchical model for the use of macro-actions that specifically addresses the difficulties of large state and action spaces. We take a macro to be a local policy, defined over a region of state space, that terminates when that region is left. We show how an abstract MDP\ncan be constructed that consists only of states that lie on the borders of adjacent regions, and whose solution determines a policy that consists of macros only. Hierarchical models similar to the one we propose have been investigated by\nForestier and Varaiya [9], and recently by Parr [ 18, 19].\nTwo limitations of this model are then addressed. The first\nrelates to solution quality. Since the policy generated by solving the abstract MDP can contain only macros, certain\nbehaviors cannot be realized, thus the resulting policy may be suboptimal. In Section 3, we identify conditions un\nder which the set of macros that comprise the action space of the abstract MDP give rise to an .::-optimal policy for the original MDP. We then consider both systematic and\nheuristic techniques for macro generation that ensure high\nquality behavior.\nThe second limitation relates to solution time, specifically the time needed to generate a set of good macros. This generally requires that we perform some form of dynamic\nprogramming within specific regions of the state space. Since our regions cover the state space and macros should capture a variety of control behaviors in different regions, macro generation can become computationally more inten sive than solving the original MDP.1 This problem can be\ndiminished if we can generate macros off-line for fast on line reasoning, or reuse them to solve multiple problems. In Section 4, we briefly analyze the requirements for feasible macro reuse and describe a hybrid model in which changes in the original MDP (either in the reward function or the\nsystem dynamics) lead to an expansion of some parts of the abstract MDP, which can then be solved. Since this hy brid MDP consists primarily of abstract states and macro actions, it can be solved effectively and provide for fast, on-line response to changes in problem specification. The\nuse of macros for the on-line solution of multiple related\nMDPs is the main advantage of our hierarchical model.\n2 A Hierarchical Model of Macro-actions\n2.1 Markov Decision Processes\nA (finite) Markov decision process is a tuple (S, A, T, R) where: S is a finite set of states; A is a finite set of actions; Tis a transition distribution T: S x A x S --7 [0, 1], such that T(s, a,\u00b7) is a probability distribution over S for any s E S and a E A; and R : S x A --7 R is a bounded reward function. Intuitively, T( s, a, w) denotes the proba bility of moving to state w when action a is performed at states, while R(s, a) is the immediate reward associated with action a in s. Given an MDP, the objective is to construct a policy that maximizes expected accumulated reward over some hori-\n1We note that aggregation and approximation techniques can be used within a region, though we do not address this issue here.\nHierarchical MDPs 221\nzon of interest. We focus on infinite horizon, discounted decision problems, where we adopt a policy that maximizes E(L:\ufffdo f3t \u00b7rt) , where rt is a reward obtained at timet and 0 < f3 < 1 is a discount factor. In such a setting, we restrict our attention to stationary policies of the form rr : S --7 A, with rr( s) denoting the action to be executed in states. The value of a policy 1r can be shown to satisfy [ 1 1]\nV1r(s) == R(s, rr(s)) + {3 L T(s, rr(s), t) \u00b7 V1r(t). tES\nA policy 1r is optimal if V1r ( s) 2: V1r' ( s) for all s E S and policies rr'. The optimal value function V* is the value function for any optimal policy.\nA number of techniques for constructing optimal policies exist. An especially simple algorithm is value iteration [1]. We produce a sequence of value functions vn by starting from an arbitrary V0, and defining\nVi+1(s) = max{R(s, a)+ f3 L T(s, a, t). V;(t)}. (1) aEA tES\nThe sequence of functions V; converges to V* in the limit. Each iteration is known as a Bellman backup. After some finite number n of iterations, the choice of maximizing ac tion for each s forms an optimal policy 7r and vn approxi mates its value.\n2.2 Macro-actions and their Models\nSutton [23] has argued that it is crucial to be able to model MDPs at multiple time scales. The ability to determine\nthe value-within an underlying MDP-of a complex se\nquence of actions or program is important in, say for ex ample, robot programming. In the navigation problem illustrated in Figure 1(a), a programmer may have provided a program (or partial policy) that enables the robot to exit\none of the rooms through a particular door. Integrating such a partial policy into the decision process is a difficult task\ngiven that: (a) the robot usually \"commits\" to the execution of this program; and (b) the program extends over some\nperiod of time. To deal with this problem, Precup, Sutton and Singh [23, 20, 2 1] have developed multi-time models and applied them to planning with MDPs. In what fol\nlows, we draw heavily on the use of these multi-time mod\nels. Parr and Russell [ 17] have proposed a related model in\nwhich a (partial) policy is modeled using a finite-state ma\nchine. These policies are then \"abstracted\" hierarchically and treated as primitive actions to be invoked by higher\nlevel behaviors.\nWhile such temporally abstract actions, or macro-actions, are useful for modeling constrained behavior-such as par tially specified policies-we also view them as a useful tool that allows the reuse of a solution generated for one MDP in the solution of another. However, this perspective casts\n222 Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier\nmacros in a very different light. While Sutton and his col leagues have not explicitly considered how macros arise,\nwe focus on the issues associated with the automatic gen eration of macro actions. Rather than supposing a tempo rally abstract behavior has been provided, we imagine that the decision maker will be forced to deal with a number of related problem-solving episodes, and desires a set of\nmacros that will help solve these MDPs more quickly. Thus\nthe effort required to generate these macros (something not considered in the PSS model) will \"pay for itself\" either with decreased reaction time to changing circumstances, or with total computational savings over multiple problem in stances. The Skills model of Thrun and Schwartz [24] has\na similar motivation, though they do not address the use of multi-time models for learned skills. Parr [18, 19] has inde\npendently investigated the use of hierarchical models with an eye toward macro generation, and has considered many of the same problems we address here.\nFormally, our model relies on a region-based decomposi tion of a given MDP (S, A, T, R) as defined by Dean and Lin [6].\nDefinition 1 A region-based decomposition II of an MDP M = (S, A, T, R) is a partitioning II = {SI. \u00b7 \u00b7 \u00b7, Sn } of the state space S. We call the elements S; of II the regions of M. For any region S;, the exit periphery of S; is XPer(Si ) = {t E S-S; : T(s,a,t) > Ofor somea,s E S;}. The entrance periphery of S; is\nEPer( S i) = {t E S; : T(s, a, t) > Ofor some a, s E S-Si}. We call elements ofXPer( S;) exit states for S; and elements of EPer( S;) entrance states. The collection of all peripheral states is denoted\nPerrr(S) = Ui{EPer(S;): i :S n} = Ui{XPer(S;): i :S n}.\nFigure 1 (b) shows the set of peripheral states obtained if we partition the problem of Figure 1(a) into the eleven regions\ncorresponding to different rooms .\nA macro-action is simply a local policy defined for a par ticular region S;. Intuitively, this policy can be executed whenever an agent is in the region and terminates when the\nagent leaves the region (if ever).\nDefinition 2 A macro-action for region S; is a local policy 7r; : S; -+A.\nOur definition is much more specific than that of PSS, who define macros using arbitrary starting and termination\nconditions, and allow mappings that depend on the time\nelapsed or the trajectory followed since the macro action was initiated. Within our framework, the starting condition for macro 1r; would simply be st E S; (we are in the re gion) and the termination condition would best (j. S; (we are out of the region).\nA key insight of PSS (which finds its roots in earlier work by Sutton [23]) is that one can treat a macro-action of this type as a primitive action in the original MDP if one has an appropriate reward and transition model for the macro. They propose the following method of modeling macros.\nDefinition 3 A discounted transition model T;(\u00b7, 1r;, \u00b7) for macro 1r; (defined on region S;) is a mapping T; : S; x XPer(S;) -+ [0, 1] such that\nT;(s,1r;,s') Er(f3r-l \u00b7 Pr(sr = s' I s0 = s, 7r;)), 00\nL pt-l \u00b7 Pr (r = t, st = s' I s0 = s, 1r;) t=l\nwhere the expectation is taken with respect to time T of ter mination of7r;. A discounted reward model R;(\u00b7, 1r;) for 1r; is a mapping R; : S; -+ R s.t.\nT\nR;(s,1r;) = Er(Lf3tR(st,7r;(st)) I s0 = s,7r;), t=O\nwhere the expectation is taken with respect to completion time T of 1r;.\nThe discounted transition model is a standard stochastic\ntransition matrix specifying the probability of leaving S; via a specific exit state given that 1r; was initiated at a spe cific state inside the region, with one exception: the proba bility is discounted according to the expected time at which that exit occurs. As demonstrated by PSS, this clever addi\ntion allows the transition model to be used as a normal tran sition matrix in any standard MDP solution technique, such as policy or value iteration. 2 The discounted reward model is similar, simply measuring the expected accrued reward during execution of 1r; starting from a particular state in S;.\n20ur definition of the discounted transition model differs slightly from that of PSS: their transition model is obtained by\n2.3 Constructing Macro Models\nSince we are concerned with the automatic generation of\nmacros, we now consider the construction of discounted\ntransition and reward models for macros. Issues related to\nthe macro-model construction are discussed also in [19].\nLet 1r; be a macro defined on S;. The discounted tran sition probability 1i(s, 1r;, s') for s E S;, macro 1r; and s' E X Per(S;) satisfies:\nT;(s, 1r;, s') =\nT(s, 1r;(s), s') + f3 L T(s, 1r;(s), s\")T;(s\", 1r;, s'). s\"ES;\nThis leads to IX Per(S;) I systems of linear equations, one set for every exit state. Each system consists of IS; I equa tions with IS; I unknowns. The systems can be solved either directly or using iterative methods. Thus, the time com plexity of finding all transition probability parameters is O( IX Per(S;) liS; 13). We can construct the reward model in a similar fashion. Let R;(s, 1r;) be the expected discounted reward for following the policy 1r; starting at states E S;. Then we have:\nR;(s, 1r;) = R(s, 7r;(s))+f3 L T(s, 1r;(s), s')R;(s', 1r;). s'ES;\nThis defines a set of IS; I linear equations, which can be solved in O(IS; I3) time.\nOverall, the computation of macro parameters takes O((IXPer(S;) I + l) IS;I3) time per macro. Note that an overhead for generating macros (finding suitable policies\ndefining macros and computing their parameters) may be come, in many instances, computationally more expensive\nthan solving the original MDP problem. Thus we must\ncarefully consider what kinds of planning situations justify the computational effort.\n2.4 The Hierarchical Solution ofMDPs with Macros\nSuppose we are given an MDP M, a decomposition II, and a set of macros A; = { 1r[, \u00b7 \u00b7 \u00b7 , 1r\ufffd'} for each region S; associated with this partition. There are two reasonably direct ways in which these can be used to solve M more efficiently.\nFirst, we can simply add these macro-actions to M; let Ma denote the augmented MDP constructed by extending the action space from A to A U A1 U \u00b7 \u00b7 \u00b7 U An, assuming that macro models are used to determine transitions and re wards associated with these new actions. Ma can be solved multiplying our variable T; by the constant /3. Our definition is consistent with the update formula in Equation 1, while PSS use a formula where the discount factor is folded into the transition model (this requires multiplying the one-step transition probabil ities by j3 before using them).\nHierarchical MDPs 223\nby standard methods, such as value iteration. Because all base level actions (those in A) are present, the policy so constructed is guaranteed to be optimal. Furthermore, the\npresence of macros can enhance the convergence of value\niteration, as demonstrated by Sutton et al. [20, 21]. This\nis due to the fact that the single \"application\" of a macro can propagate values through a large number of states and\nover a large period of time in a single step. In general, this model requires more work per iteration because of the in\ncreased action space, but potentially fewer iterations. We note that this savings does not account for the overhead as sociated with generating macros and constructing the mod els for each macro.\nWe also note that, depending on the initial value function used to begin value iteration, macros can actually increase the number of steps required for convergence compared to the value iteration with primitive actions alone. Specifi cally, Hauskrecht [10] showed that if V0 is an upper bound on the optimal value function V*, value iteration in the aug mented MDP is guaranteed to require at least as many iter\nations as in the original MDP (the same holds for a lower bound and minimization of costs). An empirical demon stration of this phenomenon is provided in next section.\nAlternatively, we can imagine a reduced MDP, Mr, formed by replacing A with A1 U \u00b7 \u00b7 \u00b7 U An. Mr will generally be more efficiently solvable because there are fewer actions to consider, and convergence will be enhanced as above.\nHowever, because the possible behaviors one can consider are limited to the application of these macros, there is no\nguarantee that the resulting solution is optimal: this will depend crucially on the macros introduced.\nWhile these models offer some advantages, they do not use macros to alleviate the problem qf state space size. Each\nmethod requires explicit value iteration over the state space, with possibly a larger number of actions. We instead wish to solve a much smaller MDP, taking advantage of the fact that, by committing to the execution of a macro, decisions need only be made at peripheral states, not at states that\nlie strictly within a region. To capture this intuition, we consider the hierarchical application of macro operators within a high-level, or abstract, MDP. This model is closely related to the \"landmark\" technique developed by Kael bling [12] for learning policies for hierarchical stochastic\nshortest path problems.\nDefinition 4 Let II = { S1, \u00b7 \u00b7 \u00b7 , Sn} be a decomposition of MDP M = {S, A, T, R), and let A = {A; : i \ufffd n} be a collection of macro-action sets, where A; = { 1rf, \u00b7 \u00b7 \u00b7 , 1r\ufffd'} is a set of macros for regionS;. The abstract MDP M' = {S', A', T', R') induced by II and A, is given by:\n\u2022 S' = Perrr(S) = U{EPer(S;): i \ufffd n};\n\u2022 A' = U;A; with 7rf E A; feasible only at states s E EPer(S;);\n224 Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier\nRoom] Room2 0 0\n0 0 0 0\n0 0 Room3 Room4\nFigure 2: Abstract MDP for a four-room example. Grey circles mark peripheral states of the original MDP, i.e. states of the abstract MDP.\n\u2022 T' ( s, 11'f, t) is given by the discounted transition model for11'f, for anys E EPer(Si) andt E XPer(S;); T' (s, 11'f, t) = 0 for any t \u00a2 XPer(Si);\n\u2022 R' ( s, 11'7) is given by the discounted rew ard model for 11'f, for anys E EPer(S;).\nThe transition and reward models required by the abstract MDP are restricted to peripheral states and make no men tion of states \"internal\" to a region. Due to discounting in T' these definitions do not describe an MDP, but they do preserve the Markov property;3 thus, we may use dynamic programming techniques to solve the abstract MDP. An abstract MDP for a simple four-room navigation problem is shown in Figure 2. Regions are formed by the rooms and the peripheral states make up the abstract MDP. We as sume macros exist that can take the robot out of any room through any door, accounting for the connectivity of the abstract MDP.\nNotice that the abstract MDP induced by a given decompo sition can be substantially smaller than the original MDP, especially if the problem can be decomposed into a num ber of regions with relatively small peripheries-this is the case in our running example, and in the types of domains considered in [20, 21].\nWe call a policy 1!'1 : S' -+ A' for M' that maps periph eral states to macro-actions a macro-policy. Such a policy 1!'1, when considered in the context of the original MDP M, defines a non-M arkovian policy 11'; that is, the choice of action at a state s can depend on previous history. In par ticular, the action 11'( s) to be executed at some state s E S; will generally depend on the state se by which S; was most recently entered: 11'( s) = 1!'1 ( se) ( s) .4\n3Specifically, the probability of moving from any entrance state to an exit state for a given macro is independent of previ ous history.\n4Note that the macro-policy does not dictate the actions to take if the process begins in an internal state s of some region S;. To deal with this, we can use a greedy macro choice with regard to the \"intermediate macro models;' R; and T;, and the values of the abstract MDP at XPer( S;). This is required only for the initial state, all other decisions are made at peripheral states in the ab stract MDP. Note that the greedy approach can be applied also to\nOnce a set of macros has been provided, along with their models, our hierarchical approach induces a problem with a considerably smaller state space (and often a smaller ac tion space). This computational advantage comes at a price however-the possibility of generating a suboptimal pol icy. This is due to the fact that the abstract MDP allows the decision maker to consider only a limited range of be haviors. Therefore it is important to ensure that the macros provided (or generated) offer a choice of behaviors that are of acceptable value. We will turn our attention to this issue in Section 3.\n2.5 Experimental results\nTo demonstrate the computational savings made possible by our hierarchical approach to planning with macros, we have performed experiments on the simple navigation prob lem in Figure 1. The agent can move in any compass di rection to an adjacent cell or stay in place. The move ac tions are stochastic, so the agent .can move in an unintended direction with some small probability. The objective is to minimize the expected discounted cost incurred by navigat ing the maze, with each state, except the zero-cost absorb ing goal state, incurring some cost. The costs and transition probabilities are not uniform across the maze.\nWe compared the results of value iteration for the original MDP, the augmented MDP and the abstract MDP, the latter two formed using the rooms in the problem as regions. The macros were formed heuristically using the simple strategy described in Section 3.2, giving IX Per(Si) I + 1 macros for every region S; . Figure 3 shows how the estimated value (minimal expected cost) of a particular state improves with the time (in seconds) taken by value iteration on each of the three models. When th_e initial value function es timate is an upper bound, both the augmented MDP and the abstract MDP lead to faster convergence of the value function. In the augmented MDP, the ability of macros to propagate value through a large number of states produces large changes in the value function in a single iteration step, overcoming the increased number of actions. Note, how ever, that when the initial estimate of the value function is a lower bound, the augmented MDP actually performs worse than the original MDP. These effects would be reversed if we were maximizing rewards instead of minimizing costs. The abstract MDP has significantly reduced state and ac tion spaces sizes. Although in general, macros can lead to suboptimal value functions (and subsequently policies), in our example, the abstract MDP produced nearly optimal policies (and did so very quickly). The average time (in seconds) taken per value iteration step in this example is 0.045 for the original MDP, 0.12 for the augmented MDP, and 0.019 for the abstract MDP. This reflects the increased action space of the augmented MDP and the reduced action\ngenerate a markovian policy for all states in the region (see [1 0] for details).\nHierarchical MDPs 225\nand state spaces for the abstract MDP, as expected.\n3 Construction and Quality of a Macro Set\nWhile macros can speed up computation, the question re mains just how good the resulting policies will be. In par ticular, within our hierarchical model, the space of policies that can be considered is severely restricted. Thus, we wish to ensure that the macros used admit the \"flexibility\" of be havior needed to discover good policies. The problem is less pressing for augmented MDPs-since base actions are available, optimality is assured-but still important if con vergence is to be enhanced.\nA primary goal of a macro-selection strategy is to find a small set of good macros, that is, macros that are likely to produce, when combined, a good approximation of the optimal solution.\n3.1 Macro Generation using Peripheral Values\nSuppose we offer the robot in our running example two macros for possible execution in Room 1 of Figure 2, each corresponding to a policy that attempts to leave the room by one of the two exits. We are making an implict assumption that one of these two behaviors is desirable, and thus that there is no good reason to hang around in that room. We may prescribe rather different local policies for the room containing the goal; there is a reason not to leave the region.\nThis suggests a general way to automatically generate macro actions for region S;. We want to trade off the re wards associated with the states in S; with the values of leaving the region via some exit state. This tradeoff is naturally modeled and analyzed as a local MDP where re wards are attached to states in S; and estimated values are attached to elements of its exit periphery.\nDefinition 5 Let S; be a region of MDP M\n(S, A, T, R}, and let <T : XPer(S; ) -t R be a seed func tion/or S;. The local MDP M; ( <T) associated with S; and u consists of: (a) state spaceS; UXPer( S;) U{ a}; (b) actions, dynamics and rewards associated with S; as in M; (c) a reward u(s ) associated with each s E XPer(Si ); and (d) a single cost-free action applicable at each s E XPer(S; ) that leads with certainty to a (a cost-free absorbing state).\nThe local MDP is depicted graphically in Figure 4. Solving M; ( <T) results in a local policy rr; whose behavior is opti mal if the seed function <T reflects the true value of reaching specific exit states.\nIntuitively, if we could seed the exit periphery of each local MDP using a function u within c: of the true value function at these states, we could generate a single macro for each region, and \"string them together\" to obtain an approxi mately optimal policy. More precisely, we have:\nTheorem 1 Let II = { S1, \u00b7 \u00b7 \u00b7 , Sn} be a decomposition of MDP M, and let V be the optimal value function for M. Let A = U{ A; : i \ufffd n} be a set of macro actions such that each A; contains some macro rr; generated by the local MDP Mi(u;) where lu;(s) - V(s )l \ufffd c:for all s E XPer(S;). If M' is the abstract MDP induced by II using action set A, and V' is the optimal value function for M',\n226 Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier\nthen 2cj3 IV'(s)- V(s)i \ufffd 1 _ j3 for all s E S' (the abstract state space). Furthermore, ifr is a lower bound on the completion time of all macros, then\n2cj3r IV'(s)- V(s)i \ufffd 1_ ;3r.\nNote that more precise error bounds can be found when \"ef fective\" discounting rates are considered for every macro transition.\n3.2 Construction of Macro Sets\nThe previous result indicates that knowledge of the (op timal) value function for an MDP can give rise to good macros. Of course, such prescience is rare: if we knew the value function, we would have no decision problem to solve. However, we often have heuristic knowledge regard ing the range of the value function at certain states, or con straints on its possible values. It is precisely this type of knowledge that comes into play when one imposes partial policies (say, in the form of a control routine). Even some information can be used to construct a good set of macros that guarantees approximately optimal performance. We consider several methods for exploiting such knowledge.\nIf one knows the range of the value function, this can be used to construct a set of macros systematically. For in stance, when constructing macros for Room I in Figure 2, suppose our knowledge of the value function is sparse-all we know is that the values of the two exit states lie be tween Vmin and Vmax\u00b75 In order to generate a set of macros for Room 1 that is guaranteed to contain a good macro, we can use the coverage technique: intuitively, for each of the two exit states, we consider values that lie in the range [Vmin, Vmax] spaced some 6 apart; that is, we con sider a grid or mesh covering (Vmin, VmaxF. By construct ing macros for each a lying on a grid point, we are assured that one such a is within \ufffd6 of the optimal value function and that (assuming other regions have \"good\" macros from which to choose) close-to-optimal behavior results when the abstract MDP is solved.\nThis coverage technique can be extremely expensive: given such generic knowledge of the value function, we will generate [(Vmax- Vmin)/6)]lXPer(S;)I macros per region. However, we can often do much better. First, the number of macros is usually smaller than the number of grid points covering [Vmin, Vmaxl\u00b7 Thus it is often more appropriate to search a local policy space. One technique for doing so was suggested recently by Parr [18]. Second, we can apply various forms of domain-specific knowledge. For in stance, the values of several exit states for a region Si may\n5Such bounds are easily obtainable using the maximum and minimum rewards.\nnot be known, but we may know that these values are (ap proximately) the same (e.g., they are equidistant from any rewarding or dangerous states). This effectively reduces the dimensionality of the required grid. Tighter constraints on the value function can reduce the range of values that need to be tried. Furthermore, in circumstances where no reward can be obtained within the region, only differences in the relative values of exit states impact the local policy: this too can reduce the number of macros needed.\nThe systematic coverage technique can lead to a genera tion of a large number of macros per region. Thus, unless tight constraints are known on the value function, this can involve substantial overhead and, in many instances, be un profitable. Heuristic methods for macro generation can al leviate this difficulty if they require the construction of a small number of macros. One such strategy, suggested by Sutton et al. [21] for robot navigation problems such as our example, involves creating macros for each region Si that try to lead the agent out of si via different exit states. To do so requires seeding a local MDP such that one exit state gets high value and all others get low value. We de scribed experiments with this heuristic technique in the pre vious section, but we also added a stay-in-region macro that keeps the agent in the region, by seeding all exit states with low values. This technique leads to a set of IXPer( Si) I + 1 macros per region.\nIn general, the above heuristic strategy assures that exits and potential goals within the region will not be overlooked while planning at the abstract level. Note, however, that this technique does not guarantee that the necessary cover age will be obtained. For example, while implementing a policy to exit in one way, the agent may find itself actually \"slipping\" closer to another exit due to uncertainty in its actions. However, the policy will ensure the agent persists in its attempt to leave as planned. If both exit states have equal value, forcing the agent to choose one or the other can be far from optimal. Instead, we would like to use a third macro that takes the agent to the nearest exit. However, we cannot discard the original macros unless we know in ad vance that the values are similar. In addition, unless one ac counts for potential variability in the actual value assigned to an exit state, sound decisions to stay within a region or leave it cannot be made.\nFinally, we mention the possibility of using iterative refine ment techniques for macro construction. A simple refine ment strategy uses the value function produced by solv ing the abstract MDP as seeds for an entirely new set of macros. In particular, we choose an initial set of seeds, generate a single macro per region, then solve the induced abstract MDP. The resulting value function is used as a seed to generate a new set of macros (again one per region), and the new abstract MDP is solved. This iterative macro refinement method is a special case of asynchronous policy iteration [2] and is similar to Dantzig-Wolfe (D-W) decom-\nposition techniques [6, 14]. D-W techniques can be viewed as iterative schemes for evaluating and modifying macro sets generated by assigning values to peripheral states.\nIn general, iterative macro-refinement methods overcome the threat of poor initial seeding (and the generation of poor macros) by gradually improving the macro set using infor mation as it becomes available. These approaches require the repeated construction of new macros, which may limit their applicability. We leave deeper investigation of itera tive techniques for future work.\n4 Multiple MDPs and the Reuse of Macros\n4.1 Hybrid MDPs\nAs discussed above, generating macro-actions and con structing their transition and reward models is an intensive process, requiring explicit state space enumeration. If a large number of macros is generated, the overhead asso ciated with this process will outweigh any speed-up pro vided by macros during value iteration. Thus our hierar chical approach (or any approach requiring macro model generation) may not be worthwhile as a technique to solve a single MDP.\nThe main reason to incur the overhead of macro construc tion lies in the reuse of macros to solve multiple related MDPs. In our running example, the robot may have con structed a policy that gets it to the goal consistently, but at some point the goal location might change, or the penal ties associated with other locations may be revised, or per haps the environment (or its abilities) changes so that the uncertainty associated with its moves -at particular loca tions increases. Any of these changes requires the solution of a new MDP, reflecting a change in reward structure or change in system dynamics. However, the changes to the MDP are often local: the reward function and the dynam ics remain the same in all but a few regions of state space. For instance, it may be that the goal location moves within Room 3, but no other part of the reward function changes.\nLocal changes in MDP structure can induce global changes in the value function (and can induce dramatic qualitative changes in the optimal behavior). If macros have been gen erated for a region such that they cover a set of different behaviors, they can be applied and reused in solving these revised MDPs. However, there is one impediment to the application of macroactions to revised MDPs, namely, the fact that revising an MDP requires that the local informa tion (rewards or dynamics) for some region must change. In our example, the macros for most regions can be reused; but those generated for Room 3 do not reflect the revisions in reward or transition probabilities described above. One possibility would be to generate new macros for revised regions. However, this could lead to computational ineffi ciencies and delays as discussed earlier. Instead, it is often\neasier to solve revised MDPs using a hybrid MDP, contain ing both abstract and base level states.\nDefinition 6 Let TI = { S1, \u00b7 \u00b7 \u00b7, Sn} be a decomposition of MDP M = (S, A , T, R), and let M' = (S', A', T', R') be the abstract MDP induced by TI and macro set A = {A; : i :::; n }. Let M = (S, A, T, R) be a local revision of M with regard to regionS;; that is, T(8, a, t) = T(8, a, t) and R(8, a) = R(8, a) for all 8 \u00a2 S;. The hybrid expansion M* = (S*, A*, T*, R*) of M' by M is:\n\u2022 S* = Pern(S) US;;\n\u2022 A* = U{ Aj E A : j =f. i} U A, where 1rj E Aj is feasible only at states 8 E EPer( Sj ), and a E A is feasible only at S;;\n\u2022 T* (8, 1rj, t) is given by the discounted transition model for 1rj ,for any 8 E EPer( Sj) and t E XPer( Sj) (j =f. i ); T*(8,1rj,t) = Ofor any t \u00a2 XPer(Sj); T* (8, a, t) = T(8, a, t) for any 8 E S; andt E S*;\n\u2022 R* (8, 1rj) is given by the discounted reward model for 1rj for any 8 E Sj (j =f. i), while R* (8, a) = R(8, a) for any 8 E S;.\nThus the hybrid MDP M*, constructed when the structure within region S; changes, consists of the original abstract MDP with the abstract states in EPer(S;) replaced by the region S; itself. This is depicted graphically in Figure 5. We note that this expansion is easily defined for changes in any number of regions.\nW hile there may be substantial overhead in creating macros, these can be reused to solve multiple problems, thus amortizing the cost over a number of problem-solving episodes. More importantly, the use of hybrid MDPs has considerable advantages when real-time response is re quired to changing circumstances. Given a new MDP Mk that differs from a base MDP M0 in a single region\nS; (or, more generally, some small set of regions), this new problem can be solved using a hybrid MDP of size I S'l + IS; - EPer( S;) I (recall S' is the set of peripheral states, or states in the abstract MDP). For example, if an MDP is partitioned into k regions of roughly uniform size,\n228 Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier\nand the average size of the entrance periphery of any re gion is p, then a hybrid MDP with one expanded region has roughly kp + \ufffd states. Without the use of macros and ab stract/hybrid MDPs, the solution of a new problem requires\nvalue or policy iteration over the entire state space of size I Sl. Thus a new problem can be solved much more quickly. The off-line generation of macros can lead to very efficient\non-line solution of new problem instances.\n4.2 Experimental results\nTo illustrate the potential for speed-up in on-line response time for multiple related MDPs using macro actions and our hybrid MDP model, we compared response time of value iteration for both the base level MDP and the hybrid MDP on three sequences of related problems. We exam ined three robot navigation problems of increasing com plexity, shown in Figure 6: Maze 36 with 36 states and 4 re\ngions; Maze66 with 66 states and 7 regions; and Maze 121\nwith 121 states and 11 regions. In each instance, the un derlying MDP was modified locally by changing the goal, represented by a zero cost absorbing state (this required changes to both the dynamics and reward model).\nTable 1 summarizes results obtained for 25 problem in stances (using different randomly selected goal states) and\ntwo value iteration methods working with the base level\nMDP and the hybridMDP. A heuristic set of macro-actions, described in Section 3.2, was used for the hybrid MDP. Value iteration was started using the solution obtained for\nthe original (locally unmodified) MDP and stopped when a\nfixed precision (0.01 cost units) was achieved.\nThe results illustrate that the hybrid MDP model, given suitable macros, can solve new problem instances much\nmore quickly than resolving the MDP with the original\nstate and action spaces. We also see that the savings offered\nby the hybrid model are greater for larger problems, exactly\nas expected. This is due to the fact that local changes affect\na significantly smaller proportion of the original model. For a hybrid MDP this means that most of the structure of the abstract MDP is preserved and only the regions in which the change has occured are elaborated.\nA disadvantage of the hybrid MDP framework is that one has to generate and precompute a set of macros, which can be computationally very costly.6 However, if the macro construction process is performed in advance (off-line), this\ndelay may be unimportant in relation to the improved abil\nity to solve new problem instances quickly. Alternatively, the delay can be justified when the computational cost\ncould be amortized over multiple problem instances. For\nexample, based on our test results, the hybrid MDP method\nin this example would start to dominate (in terms of a total solution time, which counts both the delay and time to solve n tasks) after 22, 23, and 24 tasks are solved for Maze 36, Maze 66, and Maze 121, respectively. Notice that amor tization threshold (the number of tasks after which macro\npreparation \"pays off\") increases slowly with problem size,\neven though this sequence of problems is such that a more complex maze has roughly double the state space size of its predecessor. This trend seems promising for the appli cation of macros in very large domains with many possible tasks or goals.\nThe hybrid MDPs used in our experiments rely on a set of heuristically generated macros (see Section 3.2). The macro set is relatively small and performed very well on the set of maze navigation problems we tested. This is\ndocumented by comparing AEC scores, measuring aver age expected cost for all peripheral states and for 25 ran domly generated goal tasks. The increase in the cost score for larger problems is caused by an increase in distances\nbetween peripheral and possible goal states. The practical creation of good macro sets for different types of problems\nremains an interesting open issue.\n5 Conclusions\nWe have proposed a new hierarchical model for solving MDPs using macro actions. Our abstract MDP allows po tentially dramatic reductions in the size of state and ac\ntion spaces. This requires commitment to the execution of macro actions-they cannot be reconsidered at each stage-thus leading to potentially inflexible, suboptimal behavior. We have elaborated conditions and macro con\nstruction techniques that provide guarantees on solution\nquality. Within this model, anytime tradeoffs can be made rather easily. Furthermore, with hybrid MDPs, we have a technique that allows macros to be reused to solve multiple\nMDPs, providing for fast, on-line decision making, and al\nlowing macro construction costs to be amortized over many\nproblem solving episodes.\nThere are a number of questions and open issues that re main to be addressed within this framework and many in\nteresting directions in which this work can be extended. We have ignored the question of where partitionings of state\n6We note that approximation techniques can be used to allevi ate this problem.\nspace come from: apart from handcrafted decompositions, one can imagine several strategies for automatic decompo\nsition. However, there are several dimensions along which\npartitionings can be compared: larger regions often lead to\nsmaller peripheries, which result in smaller abstract MDPs\n(which in turn can be solved more readily), and increase the odds that a revision of the MDP will be localized to a\nsmall number of regions; smaller regions, in contrast, al\nlow macros to be generated more quickly when revisions\nare required and often lead to smaller hybrid MDPs (fewer\nbase states are added to the expanded MDP). These trade offs need to be addressed in a systematic fashion.\nOther interesting questions surround the use of concise MDP representations (e.g., Bayes nets) to form decomposi\ntions and to solve local, abstract and hybrid MDPs. Related\nis the need to concisely represent macros and macro models without explicit enumeration of the state space.\nThe reuse of macros naturally suggests an extension of the\nanalysis provided here, and the questions posed above, to deal with known distributions over problem instances. If we have information pertaining to the ways in which sys tem dynamics and reward functions may be revised, we\nwould like to exploit it in forming our decomposition of state space and the macros one provides."}, {"heading": "Acknowledgements", "text": "We would like to thank Ronald Parr for a motivating discussion on macro-actions and for pointing out additional references. This\nwork was supported in part by DARPA/Rome Labs Planning Ini\ntiative grant F30602-95-l -0020 and in parts by NSF grants IRI-\n9453383 and IRI-9312395. Craig Boutilier was supported by\nNSERC Research Grant OGP0121843 and IRIS-II Project IC-7,\nand this work was undertaken while the author was visiting Brown\nUniversity. Thanks also to the generous support of the Killam\nFoundation.\nReferences\n(1] R. E. Bellman. Dynamic Programming. Princeton Univer sity Press, Princeton, 1957. [2] D. P. Bertsekas and J.. N. Tsitsiklis. Neuro-dynamic Pro gramming. Athena, 1996. [3] C. Boutilier, R. Dearden, and M. Goldszmidt. Exploiting structure in policy construction. IJCAI-95, pp.l1 04-1111, Montreal, 1995.\n[4] T. Dean and R. Givan. Model minimization in Markov de cision processes. AAA/-97, pp.l 06-111, Providence, 1997. [5] T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. Plan ning under time constraints in stochastic domains. Artif. In tell., 76:35-74, 1995. [6] T. Dean and S.-H. Lin. Decomposition techniques for plan ning in stochastic domains. /JCA/-95, pp.l121-1127, Mon treal, 199 5. [7] R. Dearden and C. Boutilier. Abstraction and approximate decision theoretic planning. Artif. lntell., 89:2 19-283, 1997. [8] R. Fikes, P. Hart, and N. Nilsson. Learning and executing generalized robot plans, Artif. /ntell. , 3:251-288, 1972. [9] J. P. Forestier, P. Varaiya. Multilayer control of large Markov chains. IEEE Trans. on Aut. Control, 23:298-304, 1978. [ 10] M. Hauskrecht. Planning with temporally abstract actions. Technical report, CS-98-01, Brown University, Providence, 1998. [11] R. A. Howard. Dynamic Programming and Markov Pro cesses. MIT Press, 1960. [12] L. Pack Kaelbling. Hierarchical reinforcement learning: Preliminary results. ICML-93, pp.167-173, Amherst, 1993. [13] R. Korf. Macro-operators: A weak method for learning. Artif. Intel/. , 26:35-77, 1985. [14] H. J. Kushner and C.-H. Chen. Decomposition of systems governed by Markov chains. IEEE Trans. Automatic Con trol, 19(5):501-507, 1974.\n[ 15] J. E. Laird, A. Newell, P. S. Rosenbloom. SOAR: An archi tecture for general intelligence. Art. Intel/., 33:1-64, 1987.\n[ 16] S. Minton. Selectively generalizing plans for problem solv ing. IJCA/-85, pp.596-599, Boston, 1985. [17] R. Parr and S. Russell. Reinforcement learning with hier archies of machines. In M. Mozer, M. Jordan, T. Petsche, eds., N/PS-11. MIT Press, 1998. [18] R. Parr. Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Processes. In this proceedings, 1998. [19] R. Parr. Hierarchical control and learning with hierarchies of machines. Chapters 1-3, under preparation, 1998. [20] D. Precup and R. S. Sutton. Multi-time models for tem porally abstract planning. In M. Mozer, M. Jordan, and T. Petsche, eds., NIPS-fl. MIT Press, 1998. [21] D. Precup, R. S. Sutton, and S. Singh. Theoretical results on reinforcement learning with temporally abstract behaviors. lOth Eur. Con f. Mach. Learn., Chemnitz, 1998. [22] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley, 1994. [23] R. S. Sutton. TD models: Modeling the world at a mixture of time scales. In ICML-95, pp.531-539, Lake Tahoe, 1995. [24] S. Thrun and A. Schwartz. Finding structure in reinforce ment learning. In G. Tesauro, D. Touretzky, and T. Leen, eds., N/PS-7, pp.385-392, MIT Press, 1995."}], "references": [{"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": "Princeton Univer\u00ad sity Press, Princeton,", "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Plan\u00ad ning under time constraints in stochastic domains", "author": ["T. Dean", "L.P. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": "Artif. In\u00ad tell.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Abstraction and approximate decision theoretic planning", "author": ["R. Dearden", "C. Boutilier"], "venue": "Artif. lntell.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Learning and executing generalized robot plans, Artif. /ntell", "author": ["R. Fikes", "P. Hart", "N. Nilsson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1972}, {"title": "Multilayer control of large Markov chains", "author": ["J.P. Forestier", "P. Varaiya"], "venue": "IEEE Trans. on Aut. Control,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1978}, {"title": "Planning with temporally abstract actions", "author": ["M. Hauskrecht"], "venue": "Technical report,", "citeRegEx": "Hauskrecht.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht.", "year": 1998}, {"title": "Dynamic Programming and Markov Pro\u00ad cesses", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1960}, {"title": "Hierarchical reinforcement learning: Preliminary results", "author": ["L. Pack Kaelbling"], "venue": "ICML-93, pp.167-173,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Macro-operators: A weak method for learning", "author": ["R. Korf"], "venue": "Artif. Intel/. ,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Decomposition of systems governed by Markov chains", "author": ["H.J. Kushner", "C.-H. Chen"], "venue": "IEEE Trans. Automatic Con\u00ad trol,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1974}, {"title": "SOAR: An archi\u00ad tecture for general intelligence", "author": ["J.E. Laird", "A. Newell", "P.S. Rosenbloom"], "venue": "Art. Intel/.,", "citeRegEx": "Laird et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Laird et al\\.", "year": 1987}, {"title": "Selectively generalizing plans for problem solv\u00ad", "author": ["S. Minton"], "venue": "ing. IJCA/-85,", "citeRegEx": "Minton.,? \\Q1985\\E", "shortCiteRegEx": "Minton.", "year": 1985}, {"title": "Reinforcement learning with hier\u00ad archies of machines", "author": ["R. Parr", "S. Russell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Processes", "author": ["R. Parr"], "venue": "In this proceedings,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Hierarchical control and learning with hierarchies of machines", "author": ["R. Parr"], "venue": "Chapters 1-3, under preparation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Multi-time models for tem\u00ad porally abstract planning", "author": ["D. Precup", "R.S. Sutton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Theoretical results on reinforcement learning with temporally abstract behaviors", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "lOth Eur. Con f. Mach. Learn.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "Lake Tahoe,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Finding structure in reinforce\u00ad ment learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "eds., N/PS-7,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}], "referenceMentions": [], "year": 2011, "abstractText": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current mod\u00ad els that combine both primitive actions and macro-actions and leave the state space un\u00ad changed, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macro\u00ad actions as local policies that act in certain regions of state space, and by restricting states in the ab\u00ad stract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss sev\u00ad eral ways in which macro-actions can be gen\u00ad erated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational over\u00ad head of macro-action generation.", "creator": "pdftk 1.41 - www.pdftk.com"}}}