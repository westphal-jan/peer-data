{"id": "1302.1532", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "A Standard Approach for Optimizing Belief Network Inference using Query DAGs", "abstract": "This paper proposes a novel, algorithm-independent approach to optimizing belief network inference. rather than designing optimizations on an algorithm by algorithm basis, we argue that one should use an unoptimized algorithm to generate a Q-DAG, a compiled graphical representation of the belief network, and then optimize the Q-DAG and its evaluator instead. We present a set of Q-DAG optimizations that supplant optimizations designed for traditional inference algorithms, including zero compression, network pruning and caching. To test our algorithm, we perform a regression of the probability of a fixed value that is not significantly more than the probability of a fixed value. The results are then averaged to obtain a given estimate of the probability of an optimization using a nonoptimized algorithm. It then performs a regression of the probability of a single target value. For this analysis, we generate a set of Q-DAGs with some optimization optimization features that perform both optimization and control. As the results are presented, the algorithm algorithm for this optimization has the same results as the algorithm for the target value.\n\n\n\n\nThe results of this post demonstrate that optimizing for the optimal prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best prediction of the best", "histories": [["v1", "Wed, 6 Feb 2013 15:54:47 GMT  (1020kb)", "http://arxiv.org/abs/1302.1532v1", "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)"]], "COMMENTS": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adnan darwiche", "gregory m provan"], "accepted": false, "id": "1302.1532"}, "pdf": {"name": "1302.1532.pdf", "metadata": {"source": "CRF", "title": "A Standard Approach for Optimizing Belief Network Inference using Query DAGs", "authors": ["Adnan Darwiche"], "emails": ["darwiche@aub.", "@rise."], "sections": null, "references": [{"title": "Query DAGs: A practical paradigm for implementing belief network inference", "author": ["Adnan Darwiche", "Gregory Provan"], "venue": "In Proceedings of the 12th Conference on Uncertainty in Artificial In\u00ad telligence (UAI),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Query DAGs: A practical paradigm for implementing belief-network inference", "author": ["Adnan Darwiche", "Gregory Provan"], "venue": "Journal of Artificial In\u00ad telligence Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Inference in belief networks: A procedural guide", "author": ["Cecil Huang", "Adnan Darwiche"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Bayesian updating in recursive graphical models by local computation", "author": ["F.V. Jensen", "S.L. Lauritzen", "K.G. Olesen"], "venue": "Computational Statistics Quar\u00ad terly,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Approxima\u00ad tions in Bayesian belief universes for knowledge based systems", "author": ["Frank Jensen", "Stig K. Andersen"], "venue": "In Proceedings of the Sixth Con\u00ad ference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "is the same as the time complexity of the underlying belief network algorithm; and (c) a Q-DAG evaluator is a very simple piece of software [1, 2] .", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "is the same as the time complexity of the underlying belief network algorithm; and (c) a Q-DAG evaluator is a very simple piece of software [1, 2] .", "startOffset": 140, "endOffset": 146}, {"referenceID": 2, "context": "In a nutshell, when us\u00ad ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero\u00ad compression, and network-pruning [3, 4, 5].", "startOffset": 211, "endOffset": 220}, {"referenceID": 3, "context": "In a nutshell, when us\u00ad ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero\u00ad compression, and network-pruning [3, 4, 5].", "startOffset": 211, "endOffset": 220}, {"referenceID": 4, "context": "In a nutshell, when us\u00ad ing a belief network algorithm to generate a Q-DAG, one need not worry about optimizing the algorithm using techniques such as computation-caching, zero\u00ad compression, and network-pruning [3, 4, 5].", "startOffset": 211, "endOffset": 220}, {"referenceID": 0, "context": "Details of this generation pro\u00ad cess can be found in [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 1, "context": "Details of this generation pro\u00ad cess can be found in [1, 2].", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "Each root node in a Q-DAG is either a nu\u00ad meric node, Num, which is labeled with a number p in [0, 1], or an evidence-specific node, Esn, which is la\u00ad beled with a pair (V, v) where V is a variable and v is a value of the variable.", "startOffset": 95, "endOffset": 101}, {"referenceID": 4, "context": "Zero compression is an optimization technique that is typically implemented in algorithms based on join trees [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "Zero compression, as presented in [5], addresses this wasteful propagation by visiting entries in cliques to identify and annihilate the zero entries.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].", "startOffset": 120, "endOffset": 129}, {"referenceID": 3, "context": "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].", "startOffset": 120, "endOffset": 129}, {"referenceID": 4, "context": "But before we substantiate these claims, we review how dynamic evidence is typically handled in the join tree algorithm [3, 4, 5].", "startOffset": 120, "endOffset": 129}, {"referenceID": 2, "context": "Figure 15, which is borrowed from [3], depicts the over\u00ad all control of the join tree algorithm.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Details of these operations are beyond the scope of this paper, but see [3] for a relatively comprehen\u00ad sive discussion.", "startOffset": 72, "endOffset": 75}], "year": 2011, "abstractText": "This paper proposes a novel, algorithm\u00ad independent approach to optimizing belief network inference. Rather than designing op\u00ad timizations on an algorithm by algorithm ba\u00ad sis, we argue that one should use an unop\u00ad timized algorithm to generate a Q-DAG, a compiled graphical representation of the be\u00ad lief network, and then optimize the Q-DAG and its evaluator instead. We present a set of Q-DAG optimizations that supplant opti\u00ad mizations designed for traditional inference algorithms, including zero compression, net\u00ad work pruning and caching. We show that our Q-DAG optimizations require time linear in the Q-DAG size, and significantly simplify the process of designing algorithms for opti\u00ad mizing belief network inference.", "creator": "pdftk 1.41 - www.pdftk.com"}}}