{"id": "1302.1542", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "Learning Bayesian Nets that Perform Well", "abstract": "A Bayesian net (BN) is more than a succinct way to encode a probabilistic distribution; it also corresponds to a function used to answer queries. A BN can therefore be evaluated by the accuracy of the answers it returns. Many algorithms for learning BNs, however, attempt to optimize another criterion (usually likelihood, possibly augmented with a regularizing term), which is independent of the distribution of queries that are posed by its parameters. For example, in a simple, continuous loop, a BN can be computed as a probabilistic distribution (where a BN can be expressed as a probabilistic distribution) by the probability distribution (when any BN is equal to a probabilistic distribution). In this scenario, the problem is that a BN can be treated as a probabilistic distribution, so that it is more complex to understand.\n\n\n\nThe BN is a function of a BN . It consists of a set of five parameters, called a BN , which represent a series of finite parameters of a BN . The parameters are derived by a set of parameters, called a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN . The parameters are derived by a set of parameters, called a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a series of finite parameters of a BN , which represent a", "histories": [["v1", "Wed, 6 Feb 2013 15:55:43 GMT  (1541kb)", "http://arxiv.org/abs/1302.1542v1", "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)"]], "COMMENTS": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["russell greiner", "adam j grove", "dale schuurmans"], "accepted": false, "id": "1302.1542"}, "pdf": {"name": "1302.1542.pdf", "metadata": {"source": "CRF", "title": "Learning Bayesian Nets that Perform Well", "authors": ["Russell Greiner"], "emails": ["greiner@scr", "grove@research", "daes@linc.cis.upenn.edu"], "sections": null, "references": [{"title": "Approximating prob\u00ad abilistic inference in Bayesian belief networks is NP\u00ad hard", "author": ["P. Dagum", "M. Luby"], "venue": "A rtificial Intelligence, 60: 141-153, April", "citeRegEx": "DL93", "shortCiteRegEx": null, "year": 1993}, {"title": "PALO: A probabilistic hill\u00ad climbing algorithm", "author": ["Russell Greiner"], "venue": "Artificial Intelligence, 83(1-2) , July", "citeRegEx": "Gre96", "shortCiteRegEx": null, "year": 1996}, {"title": "Technical Report MSR\u00ad TR-95-06", "author": ["David E. Heckerman. A tutorial on learning with Bayesian networks"], "venue": "Microsoft Research,", "citeRegEx": "Hec95", "shortCiteRegEx": null, "year": 1995}, {"title": "58(301) :13-30", "author": ["Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical A ssociation"], "venue": "March", "citeRegEx": "Hoe63", "shortCiteRegEx": null, "year": 1963}, {"title": "Learning and robust learn\u00ad ing of product distributions", "author": ["Klaus-U. Hoffgen"], "venue": "Proc. COLT-93, pages 77-83, 1993", "citeRegEx": "Ho\u00a393", "shortCiteRegEx": null, "year": 1993}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["M. Kearns"], "venue": "Proc. STOC-93, pages 392-401", "citeRegEx": "Kea93", "shortCiteRegEx": null, "year": 1993}, {"title": "AAAI-94", "author": ["Roni Khardon", "Dan Roth . Learning to reason. In Proc"], "venue": "pages 682-687,", "citeRegEx": "KR94", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning Bayesian belief networks: An approach based on the MDL principle", "author": ["Wai Lam", "Fahiem Bacchus"], "venue": "Computation Intelligence, 10( 4) :269-293,", "citeRegEx": "LB94", "shortCiteRegEx": null, "year": 1994}, {"title": "Probabilistic Reasoning in In\u00ad telligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": "Morgan Kaufmann,", "citeRegEx": "Pea88", "shortCiteRegEx": null, "year": 1988}, {"title": "Local learning in prob\u00ad abilistic networks with hidden variables", "author": ["Stuart Russell", "John Binder", "Daphne Koller", "Keiji Kanazawa"], "venue": "Proc. IJCAI-95, Montreal , Canada,", "citeRegEx": "RBKK95", "shortCiteRegEx": null, "year": 1995}, {"title": "On the hardness of approximate rea\u00ad soning", "author": ["D . Roth"], "venue": "Artificial Intelligence, 82(1-2 ) , April", "citeRegEx": "Rot96", "shortCiteRegEx": null, "year": 1996}, {"title": "Estimating the dimension of a model", "author": ["G. Schwartz"], "venue": "Annals of Statistics, 6:461-464", "citeRegEx": "Sch78", "shortCiteRegEx": null, "year": 1978}], "referenceMentions": [{"referenceID": 7, "context": "\u2022 Also: NEC Research Institute, Princeton, NJ on some measure such as log-likelihood (possibly aug\u00ad mented with a \"regularizing\" term, leading to mea\u00ad sures like MDL [LB94], and Bayesian Information Cri\u00ad terion (BIC) [Sch78]) .", "startOffset": 166, "endOffset": 172}, {"referenceID": 11, "context": "\u2022 Also: NEC Research Institute, Princeton, NJ on some measure such as log-likelihood (possibly aug\u00ad mented with a \"regularizing\" term, leading to mea\u00ad sures like MDL [LB94], and Bayesian Information Cri\u00ad terion (BIC) [Sch78]) .", "startOffset": 217, "endOffset": 224}, {"referenceID": 5, "context": "dard class of \"statistical queries\", discussed by Kearns and others [Kea93] in the context of noise-tolerant learners.", "startOffset": 68, "endOffset": 75}, {"referenceID": 8, "context": "(Readers unfamiliar with these ideas are referred to [Pea88].", "startOffset": 53, "endOffset": 60}, {"referenceID": 8, "context": "Note, however, that this computation is much easier in the SQB case, because there is an trivial way to evaluate a Bayesian net on any Markov-blanket query [Pea88]; and hence to compute the score.", "startOffset": 156, "endOffset": 163}, {"referenceID": 2, "context": "There is an obvious parallel between estimating en=O' (B) when dealing with SQB queries Q1, and estimating KL D' ( B) from complete tuples D1 [Hec95]: both tasks are quite straightforward, basically because their respective Bayesian net computations are simple.", "startOffset": 142, "endOffset": 149}, {"referenceID": 2, "context": "the log-likelihood framework: Given complete train\u00ad ing examples (and some beni gn assumptions), the CP\u00ad table that produces the optimal maximal-likelihood BN corresponds simply to the observed frequency es\u00ad timates [Hec95].", "startOffset": 216, "endOffset": 223}, {"referenceID": 9, "context": "It should not be surprising that finding the optimal CP-tables was computationally hard, as this problem has a lot in common with the challenge of l earn in g the KL( \u00b7 )-bes t network, given partially specified tuples; a task for which people often use iterative steepest\u00ad ascent climbing methods [RBKK95].", "startOffset": 298, "endOffset": 306}, {"referenceID": 9, "context": "This is analogous to to the known result in the tra\u00ad ditional model [RBKK95].", "startOffset": 68, "endOffset": 76}, {"referenceID": 1, "context": "The obvi\u00ad ous approach is to hill-climb in the discrete, but combi\u00ad natorial space of BN structures, perhaps using a sys\u00ad tem like PALO [Gre96] , after augmenting it to climb from one structure S; to a \"neighboring\" structure Si+b if Si+b filled with some CP-table entries, appears better than S; with (near) optimal CP-table values, over a distribution of queries.", "startOffset": 136, "endOffset": 143}], "year": 2011, "abstractText": "A Bayesian net (BN) is more than a succinct way to encode a probabilistic distribution; it also corresponds to a function used to answer queries. A BN can therefore be evaluated by the accuracy of the answers it returns. Many algorithms for learning BNs, however, attempt to optimize another criterion (usu\u00ad ally likelihood, possibly augmented with a regularizing term) , which is independent of the distribution of queries that are posed. This paper takes the \"performance criteria\" seriously, and considers the challenge of com\u00ad puting the BN whose performance read \"accuracy over the distribution of queries\" is optimal. We show that many aspects of this learning task are more difficult than the corresponding subtasks in the standard model.", "creator": "pdftk 1.41 - www.pdftk.com"}}}