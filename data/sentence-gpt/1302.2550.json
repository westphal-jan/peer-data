{"id": "1302.2550", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2013", "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning", "abstract": "We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Holder continuity of rewards and transition probabilities of the appropriate recovery interval for the probability of a given given error with which the probability of a given error has passed the Poisson equations.\n\n\n\n\n\n\n\nWe believe that the following prediction of a particular reward is a valid prediction.\n\n\n\nThis prediction has been evaluated using a weighted mean, so that we can estimate how likely a given reward is to be associated with an expected outcome in future life (p=p=1).", "histories": [["v1", "Mon, 11 Feb 2013 17:44:10 GMT  (19kb)", "http://arxiv.org/abs/1302.2550v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ronald ortner", "daniil ryabko"], "accepted": true, "id": "1302.2550"}, "pdf": {"name": "1302.2550.pdf", "metadata": {"source": "CRF", "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning", "authors": ["Ronald Ortner", "Daniil Ryabko"], "emails": ["rortner@unileoben.ac.at", "daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 2.\n25 50\nv1 [\ncs .L\nG ]\n1 1\nFe b"}, {"heading": "1 Introduction", "text": "Real world problems usually demand continuous state or action spaces, and one of the challenges for reinforcement learning is to deal with such continuous domains. In many problems there is a natural metric on the state space such that close states exhibit similar behavior. Often such similarities can be formalized as Lipschitz or more generally Ho\u0308lder continuity of reward and transition functions.\nThe simplest continuous reinforcement learning problem is the 1-dimensional continuum-armed bandit, where the learner has to choose arms from a bounded interval. Bounds on the regret with respect to an optimal policy under the assumption that the reward function is Ho\u0308lder continuous have been given in [15, 4]. The proposed algorithms apply the UCB algorithm [2] to a discretization of the problem. That way, the regret suffered by the algorithm consists of the loss by aggregation (which can be bounded using Ho\u0308lder continuity) plus the regret the algorithm incurs in the discretized setting. More recently, algorithms that adapt the used discretization (making it finer in more promising regions) have been proposed and analyzed [16, 8].\nWhile the continuous bandit case has been investigated in detail, in the general case of continuous state Markov decision processes (MDPs) a lot of work is confined to rather particular settings, primarily with respect to the considered transition model. In the simplest case, the transition function is considered to be deterministic as in [19], and mistake bounds for the respective discounted setting have been derived in [6]. Another common assumption is that transition functions are linear functions of state and action plus some noise. For such settings sample complexity bounds have been given in [23, 7], while O\u0303( \u221a T ) bounds for the regret after T steps are shown in [1]. However, there is also some research considering more general transition dynamics under the assumption that close states behave similarly, as will be considered here. While most of this work is purely experimental [12, 24], there are also some contributions with theoretical guarantees. Thus, [13] considers PAC-learning for continuous reinforcement learning in metric state spaces, when generative sampling is possible. The proposed algorithm is a generalization of the E3 algorithm [14] to continuous domains. A respective adaptive discretization approach is suggested in [20]. The PAC-like bounds derived there however depend on the (random) behavior of the proposed algorithm.\nHere we suggest a learning algorithm for undiscounted reinforcement learning in continuous state space. The proposed algorithm is in the tradition of algorithms like UCRL2 [11] in that it implements\nthe \u201coptimism in the face of uncertainty\u201d maxim, here combined with state aggregation. Thus, the algorithm does not need a generative model or access to \u201cresets:\u201d learning is done online, that is, in a single continual session of interactions between the environment and the learning policy.\nFor our algorithm we derive regret bounds of O\u0303(T (2+\u03b1)/(2+2\u03b1)) for MDPs with 1-dimensional state space and Ho\u0308lder-continuous rewards and transition probabilities with parameter \u03b1. These bounds also straightforwardly generalize to dimension d where the regret is bounded by O\u0303(T (2d+\u03b1)/(2d+2\u03b1)). Thus, in particular, if rewards and transition probabilities are Lipschitz, the regret is bounded by O\u0303(T (2d+1)/(2d+2))) in dimension d and O\u0303(T 3/4) in dimension 1. We also present an accompanying lower bound of \u2126( \u221a T ). As far as we know, these are the first regret bounds for a general undiscounted continuous reinforcement learning setting."}, {"heading": "2 Preliminaries", "text": "We consider the following setting. Given is a Markov decision process (MDP) M with state space S = [0, 1]d and finite action space A. For the sake of simplicity, in the following we assume d = 1. However, proofs and results generalize straightforwardly to arbitrary dimension, cf. Remark 5 below. The random rewards in state s under action a are assumed to be bounded in [0, 1] with mean r(s, a). The transition probability distribution in state s under action a is denoted by p(\u00b7|s, a). We will make the natural assumption that rewards and transition probabilities are similar in close states. More precisely, we assume that rewards and transition probabilities are Ho\u0308lder continuous. Assumption 1. There are L, \u03b1 > 0 such that for any two states s, s\u2032 and all actions a,\n|r(s, a)\u2212 r(s\u2032, a)| \u2264 L|s\u2212 s\u2032|\u03b1. Assumption 2. There are L, \u03b1 > 0 such that for any two states s, s\u2032 and all actions a,\n\u2225 \u2225p(\u00b7|s, a)\u2212 p(\u00b7|s\u2032, a) \u2225 \u2225 1 \u2264 L|s\u2212 s\u2032|\u03b1.\nFor the sake of simplicity we will assume that \u03b1 and L in Assumptions 1 and 2 are the same.\nWe also assume existence of an optimal policy \u03c0\u2217 : S \u2192 A which gives optimal average reward \u03c1\u2217 = \u03c1\u2217(M) on M independent of the initial state. A sufficient condition for state-independent optimal reward is geometric convergence of \u03c0\u2217 to an invariant probability measure. This is a natural condition which e.g. holds for any communicating finite state MDP. It also ensures (cf. Chapter 10 of [10]) that the Poisson equation holds for the optimal policy. In general, under suitable technical conditions (like geometric convergence to an invariant probability measure \u00b5\u03c0) the Poisson equation\n\u03c1\u03c0 + \u03bb\u03c0(s) = r(s, \u03c0(s)) +\n\u222b\nS\np(ds\u2032|s, \u03c0(s)) \u00b7 \u03bb\u03c0(s\u2032) (1)\nrelates the rewards and transition probabilities under any measurable policy \u03c0 to its average reward \u03c1\u03c0 and the bias function \u03bb\u03c0 : S \u2192 R of \u03c0. Intuitively, the bias is the difference in accumulated rewards when starting in a different state. Formally, the bias is defined by the Poisson equation (1) and the normalizing equation \u222b\nS \u03bb\u03c0 d\u00b5\u03c0 = 0 (cf. e.g. [9]). The following result follows from the\nbias definition and Assumptions 1 and 2 (together with results from Chapter 10 of [10]). Proposition 3. Under Assumptions 1 and 2, the bias of the optimal policy is bounded.\nConsequently, it makes sense to define the bias span H(M) of a continuous state MDP M satisfying Assumptions 1 and 2 to be H(M) := sups \u03bb\u03c0\u2217(s) \u2212 infs \u03bb\u03c0\u2217(s). Note that since infs \u03bb\u03c0\u2217(s) \u2264 0 by definition of the bias, the bias function \u03bb\u03c0\u2217 is upper bounded by H(M).\nWe are interested in algorithms which can compete with the optimal policy \u03c0\u2217 and measure their performance by the regret (after T steps) defined as T\u03c1\u2217(M) \u2212 \u2211Tt=1 rt, where rt is the random reward obtained by the algorithm at step t. Indeed, within T steps no canonical or even bias optimal optimal policy (cf. Chapter 10 of [10]) can obtain higher accumulated reward than T\u03c1\u2217 +H(M)."}, {"heading": "3 Algorithm", "text": "Our algorithm UCCRL, shown in detail in Figure 1, implements the \u201coptimism in the face of uncertainty maxim\u201d just like UCRL2 [11] or REGAL [5]. It maintains a set of plausible MDPs M and\nAlgorithm 1 The UCCRL algorithm Input: State space S = [0, 1], action space A, confidence parameter \u03b4 > 0, aggregation parameter n \u2208 N, upper bound H on the bias span, Lipschitz parameters L, \u03b1. Initialization: \u22b2 Let I1 := [ 0, 1n ] , Ij := ( j\u22121 n , j n ]\nfor j = 2, 3, . . . , n. \u22b2 Set t := 1, and observe the initial state s1 and interval I(s1).\nfor episodes k = 1, 2, . . . do \u22b2 Let Nk (Ij , a) be the number of times action a has been chosen in a state \u2208 Ij prior to episode k, and vk(Ij , a) the respective counts in episode k.\nInitialize episode k: \u22b2 Set the start time of episode k, tk := t. \u22b2 Compute estimates r\u0302k(s, a) and p\u0302 agg k (Ii|s, a) for rewards and transition probabilities, using all samples from states in the same interval I(s), respectively.\nCompute policy \u03c0\u0303k: \u22b2 Let Mk be the set of plausible MDPs M\u0303 with H(M\u0303) \u2264 H and rewards r\u0303(s, a) and transition probabilities p\u0303(\u00b7|s, a) satisfying\n\u2223 \u2223r\u0303(s, a)\u2212 r\u0302k(s, a) \u2223 \u2223 \u2264 Ln\u2212\u03b1 + \u221a\n7 log(2nAtk/\u03b4) 2max{1,Nk(I(s),a)}\n, (2) \u2225 \u2225 \u2225 p\u0303agg(\u00b7|s, a)\u2212 p\u0302aggk (\u00b7|s, a) \u2225 \u2225 \u2225\n1 \u2264 Ln\u2212\u03b1 +\n\u221a\n56n log(2Atk/\u03b4) max{1,Nk(I(s),a)} . (3)\n\u22b2 Choose policy \u03c0\u0303k and M\u0303k \u2208 Mk such that\n\u03c1\u03c0\u0303k(M\u0303k) = argmax{\u03c1\u2217(M) |M \u2208 Mk}. (4) Execute policy \u03c0\u0303k: while vk(I(st), \u03c0\u0303k(st)) < max{1, Nk(I(st), \u03c0\u0303k(st))} do\n\u22b2 Choose action at = \u03c0\u0303k(st), obtain reward rt, and observe next state st+1.\n\u22b2 Set t := t+ 1.\nend while end for\nchooses optimistically an MDP M\u0303 \u2208 M and a policy \u03c0\u0303 such that the average reward \u03c1\u03c0\u0303(M\u0303) is maximized, cf. (4). Whereas for UCRL2 and REGAL the set of plausible MDPs is defined by confidence intervals for rewards and transition probabilities for each individual state-action pair, for UCCRL we assume an MDP to be plausible if its aggregated rewards and transition probabilities are within a certain range. This range is defined by the aggregation error (determined by the assumed Ho\u0308lder continuity) and respective confidence intervals, cf. (2), (3). Correspondingly, the estimates for rewards and transition probabilities for some state action-pair (s, a) are calculated from all sampled values of action a in states close to s.\nMore precisely, for the aggregation UCCRL partitions the state space into intervals I1 := [ 0, 1n ]\n, Ik := ( k\u22121 n , k n ]\nfor k = 2, 3, . . . , n. The corresponding aggregated transition probabilities are defined by\npagg(Ij |s, a) := \u222b\nIj\np(ds\u2032|s, a). (5)\nGenerally, for a (transition) probability distribution p(\u00b7) over S we write pagg(\u00b7) for the aggregated probability distribution with respect to {I1, I2 . . . , In}. Now, given the aggregated state space {I1, I2 . . . , In}, estimates r\u0302(s, a) and p\u0302agg(\u00b7|s, a) are calculated from all samples of action a in states in I(s), the interval Ij containing s. (Consequently, the estimates are the same for states in the same interval.)\nAs UCRL2 and REGAL, UCCRL proceeds in episodes in which the chosen policy remains fixed. Episodes are terminated when the number of times an action has been sampled from some interval Ij has been doubled. Only then estimates are updated and a new policy is calculated.\nSince all states in the same interval Ij have the same confidence intervals, finding the optimal pair M\u0303k, \u03c0\u0303k in (4) is equivalent to finding the respective optimistic discretized MDP M\u0303 agg k and an optimal policy \u03c0\u0303aggk on M\u0303 agg k . Then \u03c0\u0303k can be set to be the extension of \u03c0\u0303 agg k to S, that is, \u03c0\u0303k(s) := \u03c0\u0303 agg k (I(s)) for all s. However, due to the constraint on the bias even in this finite case efficient computation of M\u0303aggk and \u03c0\u0303 agg k is still an open problem. We note that the REGAL.C algorithm [5] selects optimistic MDP and optimal policy in the same way as UCCRL.\nWhile the algorithm presented here is the first modification of UCRL2 to continuous reinforcement learning problems, there are similar adaptations to online aggregation [21] and learning in finite state MDPs with some additional similarity structure known to the learner [22]."}, {"heading": "4 Regret Bounds", "text": "For UCCRL we can derive the following bounds on the regret. Theorem 4. Let M be an MDP with continuous state space [0, 1], A actions, rewards and transition probabilities satisfying Assumptions 1 and 2, and bias span upper bounded by H . Then with probability 1\u2212 \u03b4, the regret of UCCRL (run with input parameters n and H) after T steps is upper bounded by\nconst \u00b7 nH \u221a AT log (\nT \u03b4\n)\n+ const\u2032 \u00b7HLn\u2212\u03b1T. (6) Therefore, setting n = T 1/(2+2\u03b1) gives regret upper bounded by\nconst \u00b7HL \u221a A log (\nT \u03b4\n)\n\u00b7 T (2+\u03b1)/(2+2\u03b1). With no known upper bound on the bias span, guessing H by logT one still obtains an upper bound on the regret of O\u0303(T (2+\u03b1)/(2+2\u03b1)).\nIntuitively, the second term in the regret bound of (6) is the discretization error, while the first term corresponds to the regret on the discretized MDP. A detailed proof of Theorem 4 can be found in Section 5 below. Remark 5 (d-dimensional case). The general d-dimensional case can be handled as described for dimension 1, with the only difference being that the discretization now has nd states, so that one has nd instead of n in the first term of (6). Then choosing n = T 1/(2d+2\u03b1) bounds the regret by O\u0303(T (2d+\u03b1)/(2d+2\u03b1)). Remark 6 (unknown horizon). If the horizon T is unknown then the doubling trick (executing the algorithm in rounds i = 1, 2, . . . guessing T = 2i and setting the confidence parameter to \u03b4/2i) gives the same bounds. Remark 7 (unknown Ho\u0308lder parameters). The UCCRL algorithm receives (bounds on) the Ho\u0308lder parameters L as \u03b1 as inputs. If these parameters are not known, then one can still obtain sublinear regret bounds albeit with worse dependence on T . Specifically, we can use the modelselection technique introduced in [17]. To do this, fix a certain number J of values for the constants L and \u03b1; each of these values will be considered as a model. The model selection consists in running UCCRL with each of these parameter values for a certain period of \u03c40 time steps (exploration). Then one selects the model with the highest reward and uses it for a period of \u03c4 \u20320 time steps (exploitation), while checking that its average reward stays within (6) of what was obtained in the exploitation phase. If the average reward does not pass this test, then the model with the second-best average reward is selected, and so on. Then one switches to exploration with longer periods \u03c41, etc. Since there are no guarantees on the behavior of UCCRL when the Ho\u0308lder parameters are wrong, none of the models can be discarded at any stage. Optimizing over the parameters \u03c4i and \u03c4 \u2032i as done in [17], and increasing the number J of considered parameter values, one can obtain regret bounds of O\u0303(T (2+2\u03b1)/(2+3\u03b1)), or O\u0303(T 4/5) in the Lipschitz case. For details see [17]. Since in this modelselection process UCCRL is used in a \u201cblack-box\u201d fashion, the exploration is rather wasteful, and thus we think that this bound is suboptimal. Recently, the results of [17] have been improved [18], and it seems that similar analysis gives improved regret bounds for the case of unknown Ho\u0308lder parameters as well.\nThe following is a complementing lower bound on the regret for continuous state reinforcement learning.\nTheorem 8. For any A,H > 1 and any reinforcement learning algorithm there is a continuous state reinforcement learning problem with A actions and bias span H satisfying Assumption 1 such that the algorithm suffers regret of \u2126( \u221a HAT ).\nProof. Consider the following reinforcement learning problem with state space [0, 1]. The state space is partitioned into n intervals Ij of equal size. The transition probabilities for each action a are on each of the intervals Ij concentrated and equally distributed on the same interval Ij . The rewards on each interval Ij are also constant for each a and are chosen as in the lower bounds for a multi-armed bandit problem [3] with nA arms. That is, giving only one arm slightly higher reward, it is known [3] that regret of \u2126( \u221a nAT ) can be forced upon any algorithm on the respective bandit problem. Adding another action giving no reward and equally distributing over the whole state space, the bias span of the problem is n and the regret \u2126( \u221a HAT ).\nRemark 9. Note that Assumption 2 does not hold in the example used in the proof of Theorem 8. However, the transition probabilities are piecewise constant (and hence Lipschitz) and known to the learner. Actually, it is straightforward to deal with piecewise Ho\u0308lder continuous rewards and transition probabilities where the finitely many points of discontinuity are known to the learner. If one makes sure that the intervals of the discretized state space do not contain any discontinuities, it is easy to adapt UCCRL and Theorem 4 accordingly.\nRemark 10 (comparison to bandits). The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space). Thus, in particular one cannot freely sample an arbitrary state of the state space as assumed in continuous-armed bandits."}, {"heading": "5 Proof of Theorem 4", "text": "For the proof of the main theorem we adapt the proof of the regret bounds for finite MDPs in [11] and [5]. Although the state space is now continuous, due to the finite horizon T , we can reuse some arguments, so that we keep the structure of the original proof of Theorem 2 in [11]. Some of the necessary adaptations made are similar to techniques used for showing regret bounds for other modifications of the original UCRL2 algorithm [21, 22], which however only considered finite-state MDPs."}, {"heading": "5.1 Splitting into Episodes", "text": "Let vk(s, a) be the number of times action a has been chosen in episode k when being in state s, and denote the total number of episodes by m. Then setting \u2206k := \u2211 s,a vk(s, a)(\u03c1 \u2217 \u2212 r(s, a)), with probability at least 1 \u2212 \u03b4 12T 5/4\nthe regret of UCCRL after T steps is upper bounded by (cf. Section 4.1 of [11]),\n\u221a\n5 8T log\n(\n8T \u03b4\n) + \u2211m\nk=1 \u2206k . (7)"}, {"heading": "5.2 Failing Confidence Intervals", "text": "Next, we consider the regret incurred when the true MDP M is not contained in the set of plausible MDPs Mk. Thus, fix a state-action pair (s, a), and recall that r\u0302(s, a) and p\u0302agg(\u00b7|s, a) are the estimates for rewards and transition probabilities calculated from all samples of state-action pairs contained in the same interval I(s). Now assume that at step t there have been N > 0 samples of action a in states in I(s) and that in the i-th sample a transition from state si \u2208 I(s) to state s\u2032i has been observed (i = 1, . . . , N).\nFirst, concerning the rewards one obtains as in the proof of Lemma 17 in Appendix C.1 of [11] \u2014 but now using Hoeffding for independent and not necessarily identically distributed random variables \u2014 that\nPr { \u2223 \u2223r\u0302(s, a)\u2212 E[r\u0302(s, a)] \u2223 \u2223 \u2265 \u221a\n7 2N log\n(\n2nAt \u03b4\n)\n} \u2264 \u03b4 60nAt7 . (8)\nConcerning the transition probabilities, we have for a suitable x \u2208 {\u22121, 1}n \u2225\n\u2225 \u2225 p\u0302agg(\u00b7|s, a)\u2212 E[p\u0302agg(\u00b7|s, a)]\n\u2225 \u2225 \u2225\n1 =\nn \u2211\nj=1\n\u2223 \u2223 \u2223 p\u0302agg(Ij |s, a)\u2212 E[p\u0302agg(Ij |s, a)] \u2223 \u2223 \u2223\n=\nn \u2211\nj=1\n( p\u0302agg(Ij |s, a)\u2212 E[p\u0302agg(Ij |s, a)] ) x(Ij)\n= 1N\nN \u2211\ni=1\n( x(I(s\u2032i))\u2212 \u222b\nS\np(ds\u2032|si, a) \u00b7 x(I(s\u2032)) ) . (9)\nFor any x \u2208 {\u22121, 1}n, Xi := x(I(s\u2032i))\u2212 \u222b S p(ds\u2032|si, a)\u00b7x(I(s\u2032)) is a martingale difference sequence with |Xi| \u2264 2, so that by Azuma-Hoeffding inequality (e.g., Lemma 10 in [11]), Pr{ \u2211N\ni=1 Xi \u2265 \u03b8} \u2264 exp(\u2212\u03b82/8N) and in particular\nPr {\n\u2211N i=1 Xi \u2265\n\u221a\n56nN log ( 2At \u03b4 )\n} \u2264 (\n\u03b4 2At\n)7n \u2264 \u03b4 2n20nAt7 .\nA union bound over all sequences x \u2208 {\u22121, 1}n then yields from (9) that\nPr {\u2225 \u2225 \u2225 p\u0302agg(\u00b7|s, a)\u2212 E[p\u0302agg(\u00b7|s, a)] \u2225 \u2225 \u2225\n1 \u2265\n\u221a\n56n N log\n(\n2At \u03b4\n)\n} \u2264 \u03b4 20nAt7 . (10)\nAnother union bound over all t possible values for N , all n intervals and all actions shows that the confidence intervals in (8) and (10) hold at time t with probability at least 1 \u2212 \u03b415t6 for the actual counts N(I(s), a) and all state-action pairs (s, a). (Note that the equations (8) and (10) are the same for state-action pairs with states in the same interval.)\nNow, by linearity of expectation E[r\u0302(s, a)] can be written as 1N \u2211N\ni=1 r(si, a). Since the si are assumed to be in the same interval I(s), it follows that |E[r\u0302(s, a)] \u2212 r(s, a)| < Ln\u2212\u03b1. Similarly, \u2225 \u2225E[p\u0302agg(\u00b7|s, a)] \u2212 pagg(\u00b7|s, a) \u2225 \u2225\n1 < Ln\u2212\u03b1. Together with (8) and (10) this shows that with proba-\nbility at least 1\u2212 \u03b415t6 for all state-action pairs (s, a) \u2223\n\u2223r\u0302(s, a)\u2212 r(s, a) \u2223 \u2223 < Ln\u2212\u03b1 + \u221a\n7 log(2nAt/\u03b4) 2max{1,N(I(s),a)} , (11)\n\u2225 \u2225 \u2225 p\u0302agg(\u00b7|s, a)\u2212 pagg(\u00b7|s, a) \u2225 \u2225 \u2225\n1 < Ln\u2212\u03b1 +\n\u221a\n56n log(2At/\u03b4) max{1,N(I(s),a)} . (12)\nThis shows that the true MDP is contained in the set of plausible MDPs M(t) at step t with probability at least 1\u2212 \u03b415t6 , just as in Lemma 17 of [11]. The argument that\nm \u2211\nk=1\n\u2206k1M 6\u2208Mk \u2264 \u221a T (13)\nwith probability at least 1\u2212 \u03b4 12T 5/4 then can be taken without any changes from Section 4.2 of [11]."}, {"heading": "5.3 Regret in Episodes with M \u2208 Mk", "text": "Now for episodes with M \u2208 Mk, by the optimistic choice of M\u0303k and \u03c0\u0303k in (4) we can bound \u2206k = \u2211\ns\nvk(s, \u03c0\u0303k(s)) ( \u03c1\u2217 \u2212 r(s, \u03c0\u0303k(s)) )\n\u2264 \u2211\ns\nvk(s, \u03c0\u0303k(s)) ( \u03c1\u0303\u2217k \u2212 r(s, \u03c0\u0303k(s)) )\n= \u2211\ns\nvk(s, \u03c0\u0303k(s)) ( \u03c1\u0303\u2217k \u2212 r\u0303k(s, \u03c0\u0303k(s)) ) + \u2211\ns\nvk(s, \u03c0\u0303k(s)) ( r\u0303k(s, \u03c0\u0303k(s))\u2212 r(s, \u03c0\u0303k(s)) ) .\nAny term r\u0303k(s, a) \u2212 r(s, a) \u2264 |r\u0303k(s, a) \u2212 r\u0302k(s, a)| + |r\u0302k(s, a) \u2212 r(s, a)| is bounded according to (2) and (11), as we assume that M\u0303k,M \u2208 Mk, so that summarizing states in the same interval Ij\n\u2206k \u2264 \u2211\ns\nvk(s, \u03c0\u0303k(s)) ( \u03c1\u0303\u2217k \u2212 r\u0303k(s, \u03c0\u0303k(s)) ) + 2\nn \u2211\nj=1\n\u2211\na\u2208A\nvk(Ij , a)\n(\nLn\u2212\u03b1 + \u221a\n7 log(2nAtk/\u03b4) 2max{1,Nk(Ij ,a)}\n)\n.\nSince max{1, Nk(Ij , a)} \u2264 tk \u2264 T , setting \u03c4k := tk+1 \u2212 tk to be the length of episode k we have \u2206k \u2264 \u2211\ns\nvk(s, \u03c0\u0303k(s)) ( \u03c1\u0303\u2217k \u2212 r\u0303k(s, \u03c0\u0303k(s)) )\n+ 2Ln\u2212\u03b1\u03c4k + \u221a 14 log ( 2nAT \u03b4 )\nn \u2211\nj=1\n\u2211\na\u2208A\nvk(Ij , a) \u221a\nmax{1, Nk(Ij , a)} . (14)\nWe continue analyzing the first term on the right hand side of (14). By the Poisson equation (1) for \u03c0\u0303k on M\u0303k, denoting the respective bias by \u03bb\u0303k := \u03bb\u0303\u03c0\u0303k we can write\n\u2211\ns\nvk(s, \u03c0\u0303k(s)) ( \u03c1\u0303\u2217k \u2212 r\u0303k(s, \u03c0\u0303k(s)) )\n= \u2211\ns\nvk(s, \u03c0\u0303k(s)) (\n\u222b\nS\np\u0303k(ds \u2032|s, \u03c0\u0303k(s)) \u00b7 \u03bb\u0303k(s\u2032)\u2212 \u03bb\u0303k(s)\n)\n= \u2211\ns\nvk(s, \u03c0\u0303k(s)) (\n\u222b\nS\np(ds\u2032|s, \u03c0\u0303k(s)) \u00b7 \u03bb\u0303k(s\u2032)\u2212 \u03bb\u0303k(s) )\n(15)\n+ \u2211\ns\nvk(s, \u03c0\u0303k(s))\nn \u2211\nj=1\n\u222b\nIj\n(\np\u0303k(ds \u2032|s, \u03c0\u0303k(s)) \u2212 p(ds\u2032|s, \u03c0\u0303k(s))\n)\n\u00b7 \u03bb\u0303k(s\u2032). (16)"}, {"heading": "5.4 The True Transition Functions", "text": "Now \u2225 \u2225p\u0303aggk (\u00b7|s, a)\u2212 pagg(\u00b7|s, a) \u2225 \u2225 1 \u2264 \u2225 \u2225p\u0303aggk (\u00b7|s, a)\u2212 p\u0302 agg k (\u00b7|s, a) \u2225 \u2225 1 + \u2225 \u2225p\u0302aggk (\u00b7|s, a)\u2212 pagg(\u00b7|s, a) \u2225 \u2225 1 can be bounded by (3) and (12), because we assume M\u0303k,M \u2208 Mk. Hence, since by definition of the algorithm H bounds the bias function \u03bb\u0303k, the term in (16) is bounded by\n\u2211\ns\nvk(s, \u03c0\u0303k(s))\nn \u2211\nj=1\n\u222b\nIj\n\u03bb\u0303k(s \u2032) ( p\u0303k(ds \u2032|s, \u03c0\u0303k(s))\u2212 p(ds\u2032|s, \u03c0\u0303k(s)) )\n\u2264 \u2211\ns\nvk(s, \u03c0\u0303k(s)) \u00b7H \u00b7 n \u2211\nj=1\n( p\u0303aggk (Ij |s, \u03c0\u0303k(s))\u2212 pagg(Ij |s, \u03c0\u0303k(s)) )\n\u2264 \u2211\ns\nvk(s, \u03c0\u0303k(s)) \u00b7H \u00b7 2 ( Ln\u2212\u03b1 + \u221a\n56n log(2AT/\u03b4) max{1,Nk(I(s),at)}\n)\n= 2HLn\u2212\u03b1\u03c4k + 4H \u221a 14n log ( 2AT \u03b4 )\nn \u2211\nj=1\n\u2211\na\u2208A\nvk(Ij , a) \u221a\nmax{1, Nk(Ij , a)} , (17)\nwhile for the term in (15) \u2211\ns\nvk(s, \u03c0\u0303k(s)) (\n\u222b\nS\np(ds\u2032|s, \u03c0\u0303k(s)) \u00b7 \u03bb\u0303k(s\u2032)\u2212 \u03bb\u0303k(s) )\n=\ntk+1\u22121 \u2211\nt=tk\n(\n\u222b\nS\np(ds\u2032|st, at) \u00b7 \u03bb\u0303k(s\u2032)\u2212 \u03bb\u0303k(st) )\n=\ntk+1\u22121 \u2211\nt=tk\n(\n\u222b\nS\np(ds\u2032|st, at) \u00b7 \u03bb\u0303k(s\u2032)\u2212 \u03bb\u0303k(st+1) ) + \u03bb\u0303k(stk+1)\u2212 \u03bb\u0303k(stk).\nLet k(t) be the index of the episode time step t belongs to. Then the sequence Xt := \u222b\nS p(ds \u2032|st, at) \u00b7 \u03bb\u0303k(t)(s\u2032) \u2212 \u03bb\u0303k(t)(st+1) is a sequence of martingale differences so that AzumaHoeffding inequality shows (cf. Section 4.3.2 and in particular eq. (18) in [11]) that after summing over all episodes we have\nm \u2211\nk=1\n( tk+1\u22121 \u2211\nt=tk\n(\n\u222b\nS\np(ds\u2032|st, at) \u00b7 \u03bb\u0303k(s\u2032)\u2212 \u03bb\u0303k(st+1) ) + \u03bb\u0303k(stk+1)\u2212 \u03bb\u0303k(stk) )\n\u2264 H \u221a\n5 2T log\n(\n8T \u03b4\n) +HnA log2 ( 8T nA ) , (18)\nwhere the second term comes from an upper bound on the number of episodes, which can be derived analogously to Appendix C.2 of [11]."}, {"heading": "5.5 Summing over Episodes with M \u2208 Mk", "text": "To conclude, we sum (14) over all the episodes with M \u2208 Mk, using (15), (17), and (18). This yields that with probability at least 1\u2212 \u03b4\n12T 5/4\nm \u2211\nk=1\n\u2206k1M\u2208Mk \u2264 2HLn\u2212\u03b1T + 4H \u221a 14n log ( 2AT \u03b4 )\n\u00b7 m \u2211\nk=1\nn \u2211\nj=1\n\u2211\na\u2208A\nvk(Ij , a) \u221a\nmax{1, Nk(Ij , a)}\n+H \u221a\n5 2T log\n(\n8T \u03b4\n) +HnA log2 ( 8T nA )\n+ 2Ln\u2212\u03b1T + \u221a 14 log (\n2nAT \u03b4\n)\nm \u2211\nk=1\nn \u2211\nj=1\n\u2211\na\u2208A\nvk(Ij , a) \u221a\nmax{1, Nk(Ij , a)} . (19)\nAnalogously to Section 4.3.3 and Appendix C.3 of [11], one can show that n \u2211\nj=1\n\u2211\na\u2208A\n\u2211\nk\nvk(Ij , a) \u221a\nmax{1, Nk(Ij , a)} \u2264\n( \u221a 2 + 1 ) \u221a nAT ,\nand we get from (19) after some simplifications that with probability \u2265 1\u2212 \u03b4 12T 5/4\nm \u2211\nk=1\n\u2206k1M\u2208Mk \u2264 H \u221a 5 2T log ( 8T \u03b4 ) +HnA log2 ( 8T nA )\n+ ( (4H + 1) \u221a 14n log (\n2AT \u03b4\n)\n) ( \u221a 2 + 1 ) \u221a nAT + 2(H + 1)Ln\u2212\u03b1T . (20)\nFinally, evaluating (7) by summing \u2206k over all episodes, by (13) and (20) we have with probability \u2265 1\u2212 \u03b4\n4T 5/4 an upper bound on the regret of \u221a\n5 8T log\n(\n8T \u03b4\n)\n+\nm \u2211\nk=1\n\u2206k1M/\u2208Mk +\nm \u2211\nk=1\n\u2206k1M\u2208Mk\n\u2264 \u221a\n5 8T log\n(\n8T \u03b4\n) + \u221a T +H \u221a\n5 2T log\n(\n8T \u03b4\n) +HnA log2 ( 8T nA )\n+ ( (4H + 1) \u221a 14n log (\n2AT \u03b4\n)\n) ( \u221a 2 + 1 ) \u221a nAT + 2(H + 1)Ln\u2212\u03b1T.\nA union bound over all possible values of T and further simplifications as in Appendix C.4 of [11] finish the proof."}, {"heading": "6 Outlook", "text": "We think that a generalization of our results to continuous action space should not pose any major problems. In order to improve over the given bounds, it may be promising to investigate more sophisticated discretization patterns.\nThe assumption of Ho\u0308lder continuity is an obvious, yet not the only possible assumption one can make about the transition probabilities and reward functions. A more general problem is to assume a set F of functions, find a way to measure the \u201csize\u201d of F , and derive regret bounds depending on this size of F ."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the three anonymous reviewers for their helpful suggestions and Re\u0301mi Munos for useful discussion which helped to improve the bounds. This research was funded by the Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and FEDER (Contrat de Projets Etat Region CPER 2007-2013), ANR projects EXPLO-RA (ANR-08-COSI004), Lampada (ANR-09-EMER-007) and CoAdapt, and by the European Community\u2019s FP7 Program under grant agreements n\u25e6 216886 (PASCAL2) and n\u25e6 270327 (CompLACS). The first author is currently funded by the Austrian Science Fund (FWF): J 3259-N13."}], "references": [{"title": "Regret bounds for the adaptive control of linear quadratic systems", "author": ["Yasin Abbasi-Yadkori", "Csaba Szepesv\u00e1ri"], "venue": "COLT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Finite-time analysis of the multi-armed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Improved rates for the stochastic continuum-armed bandit problem", "author": ["Peter Auer", "Ronald Ortner", "Csaba Szepesv\u00e1ri"], "venue": "COLT", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["Peter L. Bartlett", "Ambuj Tewari"], "venue": "In Proc. UAI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Adaptive-resolution reinforcement learning with polynomial exploration in deterministic domains", "author": ["Andrey Bernstein", "Nahum Shimkin"], "venue": "Mach. Learn.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Provably efficient learning with typed parametric models", "author": ["Emma Brunskill", "Bethany R. Leffler", "Lihong Li", "Michael L. Littman", "Nicholas Roy"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Online optimization of \u03c7-armed bandits", "author": ["S\u00e9bastien Bubeck", "R\u00e9mi Munos", "Gilles Stoltz", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Discrete-time Markov control processes, volume 30 of Applications of mathematics", "author": ["On\u00e9simo Hern\u00e1ndez-Lerma", "Jean Bernard Lasserre"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Further topics on discrete-time Markov control processes, volume 42 of Applications of mathematics", "author": ["On\u00e9simo Hern\u00e1ndez-Lerma", "Jean Bernard Lasserre"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Model-based exploration in continuous state spaces", "author": ["Nicholas K. Jong", "Peter Stone"], "venue": "SARA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Exploration in metric state spaces", "author": ["Sham Kakade", "Michael J. Kearns", "John Langford"], "venue": "In Machine Learning, Proc. 20th International Conference,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael J. Kearns", "Satinder P. Singh"], "venue": "Mach. Learn.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Nearly tight bounds for the continuum-armed bandit problem", "author": ["Robert Kleinberg"], "venue": "In Advances Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proc. 40th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Selecting the state-representation in reinforcement learning", "author": ["Odalric-Ambrym Maillard", "R\u00e9mi Munos", "Daniil Ryabko"], "venue": "In Advances Neural Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Optimal regret bounds for selecting the state representation in reinforcement learning", "author": ["Odalric-Ambrym Maillard", "Phuong Nguyen", "Ronald Ortner", "Daniil Ryabko"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Efficient continuous-time reinforcement learning with adaptive state graphs", "author": ["Gerhard Neumann", "Michael Pfeiffer", "Wolfgang Maass"], "venue": "ECML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Multi-resolution exploration in continuous spaces", "author": ["Ali Nouri", "Michael L. Littman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Adaptive aggregation for reinforcement learning in average reward Markov decision processes", "author": ["Ronald Ortner"], "venue": "Ann. Oper. Res.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Regret bounds for restless Markov bandits", "author": ["Ronald Ortner", "Daniil Ryabko", "Peter Auer", "R\u00e9mi Munos"], "venue": "In Proc. 23rd Conference on Algorithmic Learning Theory, ALT", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Online linear regression and its application to model-based reinforcement learning", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "NIPS", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Tree based discretization for continuous state space reinforcement learning", "author": ["William T.B. Uther", "Manuela M. Veloso"], "venue": "In Proc. AAAI 98, IAAI", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}], "referenceMentions": [{"referenceID": 14, "context": "Bounds on the regret with respect to an optimal policy under the assumption that the reward function is H\u00f6lder continuous have been given in [15, 4].", "startOffset": 141, "endOffset": 148}, {"referenceID": 3, "context": "Bounds on the regret with respect to an optimal policy under the assumption that the reward function is H\u00f6lder continuous have been given in [15, 4].", "startOffset": 141, "endOffset": 148}, {"referenceID": 1, "context": "The proposed algorithms apply the UCB algorithm [2] to a discretization of the problem.", "startOffset": 48, "endOffset": 51}, {"referenceID": 15, "context": "More recently, algorithms that adapt the used discretization (making it finer in more promising regions) have been proposed and analyzed [16, 8].", "startOffset": 137, "endOffset": 144}, {"referenceID": 7, "context": "More recently, algorithms that adapt the used discretization (making it finer in more promising regions) have been proposed and analyzed [16, 8].", "startOffset": 137, "endOffset": 144}, {"referenceID": 18, "context": "In the simplest case, the transition function is considered to be deterministic as in [19], and mistake bounds for the respective discounted setting have been derived in [6].", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "In the simplest case, the transition function is considered to be deterministic as in [19], and mistake bounds for the respective discounted setting have been derived in [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 22, "context": "For such settings sample complexity bounds have been given in [23, 7], while \u00d5( \u221a T ) bounds for the regret after T steps are shown in [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 6, "context": "For such settings sample complexity bounds have been given in [23, 7], while \u00d5( \u221a T ) bounds for the regret after T steps are shown in [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 0, "context": "For such settings sample complexity bounds have been given in [23, 7], while \u00d5( \u221a T ) bounds for the regret after T steps are shown in [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 11, "context": "While most of this work is purely experimental [12, 24], there are also some contributions with theoretical guarantees.", "startOffset": 47, "endOffset": 55}, {"referenceID": 23, "context": "While most of this work is purely experimental [12, 24], there are also some contributions with theoretical guarantees.", "startOffset": 47, "endOffset": 55}, {"referenceID": 12, "context": "Thus, [13] considers PAC-learning for continuous reinforcement learning in metric state spaces, when generative sampling is possible.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "The proposed algorithm is a generalization of the E algorithm [14] to continuous domains.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "A respective adaptive discretization approach is suggested in [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "The proposed algorithm is in the tradition of algorithms like UCRL2 [11] in that it implements 1", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "Given is a Markov decision process (MDP) M with state space S = [0, 1] and finite action space A.", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "The random rewards in state s under action a are assumed to be bounded in [0, 1] with mean r(s, a).", "startOffset": 74, "endOffset": 80}, {"referenceID": 9, "context": "Chapter 10 of [10]) that the Poisson equation holds for the optimal policy.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The following result follows from the bias definition and Assumptions 1 and 2 (together with results from Chapter 10 of [10]).", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Chapter 10 of [10]) can obtain higher accumulated reward than T\u03c1 +H(M).", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "3 Algorithm Our algorithm UCCRL, shown in detail in Figure 1, implements the \u201coptimism in the face of uncertainty maxim\u201d just like UCRL2 [11] or REGAL [5].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "3 Algorithm Our algorithm UCCRL, shown in detail in Figure 1, implements the \u201coptimism in the face of uncertainty maxim\u201d just like UCRL2 [11] or REGAL [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "Algorithm 1 The UCCRL algorithm Input: State space S = [0, 1], action space A, confidence parameter \u03b4 > 0, aggregation parameter n \u2208 N, upper bound H on the bias span, Lipschitz parameters L, \u03b1.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "C algorithm [5] selects optimistic MDP and optimal policy in the same way as UCCRL.", "startOffset": 12, "endOffset": 15}, {"referenceID": 20, "context": "While the algorithm presented here is the first modification of UCRL2 to continuous reinforcement learning problems, there are similar adaptations to online aggregation [21] and learning in finite state MDPs with some additional similarity structure known to the learner [22].", "startOffset": 169, "endOffset": 173}, {"referenceID": 21, "context": "While the algorithm presented here is the first modification of UCRL2 to continuous reinforcement learning problems, there are similar adaptations to online aggregation [21] and learning in finite state MDPs with some additional similarity structure known to the learner [22].", "startOffset": 271, "endOffset": 275}, {"referenceID": 0, "context": "Let M be an MDP with continuous state space [0, 1], A actions, rewards and transition probabilities satisfying Assumptions 1 and 2, and bias span upper bounded by H .", "startOffset": 44, "endOffset": 50}, {"referenceID": 16, "context": "Specifically, we can use the modelselection technique introduced in [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Optimizing over the parameters \u03c4i and \u03c4 \u2032 i as done in [17], and increasing the number J of considered parameter values, one can obtain regret bounds of \u00d5(T ), or \u00d5(T ) in the Lipschitz case.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "For details see [17].", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "Recently, the results of [17] have been improved [18], and it seems that similar analysis gives improved regret bounds for the case of unknown H\u00f6lder parameters as well.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "Recently, the results of [17] have been improved [18], and it seems that similar analysis gives improved regret bounds for the case of unknown H\u00f6lder parameters as well.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Consider the following reinforcement learning problem with state space [0, 1].", "startOffset": 71, "endOffset": 77}, {"referenceID": 2, "context": "The rewards on each interval Ij are also constant for each a and are chosen as in the lower bounds for a multi-armed bandit problem [3] with nA arms.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "That is, giving only one arm slightly higher reward, it is known [3] that regret of \u03a9( \u221a nAT ) can be forced upon any algorithm on the respective bandit problem.", "startOffset": 65, "endOffset": 68}, {"referenceID": 14, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 3, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 15, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 7, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 10, "context": "5 Proof of Theorem 4 For the proof of the main theorem we adapt the proof of the regret bounds for finite MDPs in [11] and [5].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "5 Proof of Theorem 4 For the proof of the main theorem we adapt the proof of the regret bounds for finite MDPs in [11] and [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 10, "context": "Although the state space is now continuous, due to the finite horizon T , we can reuse some arguments, so that we keep the structure of the original proof of Theorem 2 in [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 20, "context": "Some of the necessary adaptations made are similar to techniques used for showing regret bounds for other modifications of the original UCRL2 algorithm [21, 22], which however only considered finite-state MDPs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 21, "context": "Some of the necessary adaptations made are similar to techniques used for showing regret bounds for other modifications of the original UCRL2 algorithm [21, 22], which however only considered finite-state MDPs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 10, "context": "1 of [11]),", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "1 of [11] \u2014 but now using Hoeffding for independent and not necessarily identically distributed random variables \u2014 that Pr { \u2223 r\u0302(s, a)\u2212 E[r\u0302(s, a)] \u2223", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": ", Lemma 10 in [11]), Pr{ \u2211N i=1 Xi \u2265 \u03b8} \u2264 exp(\u2212\u03b82/8N) and in particular Pr { \u2211N i=1 Xi \u2265 \u221a", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "(12) This shows that the true MDP is contained in the set of plausible MDPs M(t) at step t with probability at least 1\u2212 \u03b4 15t6 , just as in Lemma 17 of [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "2 of [11].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "(18) in [11]) that after summing over all episodes we have m \u2211", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "2 of [11].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "3 of [11], one can show that n \u2211", "startOffset": 5, "endOffset": 9}], "year": 2013, "abstractText": "We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are H\u00f6lder continuity of rewards and transition probabilities.", "creator": "LaTeX with hyperref package"}}}