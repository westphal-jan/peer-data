{"id": "1302.4922", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Structure Discovery in Nonparametric Regression through Compositional Kernel Search", "abstract": "Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process.\n\n\n\nThe Kernel\nA typical kernel structure is a tree that consists of a binary tree of two main elements, the root of which is all in the tree tree. The root of the root is a tree with its parent tree (which the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the tree of the", "histories": [["v1", "Wed, 20 Feb 2013 14:53:13 GMT  (2806kb,D)", "http://arxiv.org/abs/1302.4922v1", "9 pages, 7 figures"], ["v2", "Tue, 5 Mar 2013 11:48:12 GMT  (2810kb,D)", "http://arxiv.org/abs/1302.4922v2", "9 pages, 7 figures, Submitted for review"], ["v3", "Fri, 5 Apr 2013 16:53:30 GMT  (2358kb,D)", "http://arxiv.org/abs/1302.4922v3", "9 pages, 7 figures, Submitted for review"], ["v4", "Mon, 13 May 2013 13:10:31 GMT  (2372kb,D)", "http://arxiv.org/abs/1302.4922v4", "9 pages, 7 figures, To appear in proceedings of the 2013 International Conference on Machine Learning"]], "COMMENTS": "9 pages, 7 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["david k duvenaud", "james robert lloyd", "roger b grosse", "joshua b tenenbaum", "zoubin ghahramani"], "accepted": true, "id": "1302.4922"}, "pdf": {"name": "1302.4922.pdf", "metadata": {"source": "META", "title": "Structure Discovery in Nonparametric Regression through Compositional Kernel Search", "authors": ["David Duvenaud", "James Robert Lloyd", "Roger Grosse", "Joshua B. Tenenbaum", "Zoubin Ghahramani"], "emails": ["dkd23@cam.ac.uk", "jrl44@cam.ac.uk", "rgrosse@mit.edu", "jbt@mit.edu", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Kernel-based nonparametric models, such as support vector machines and Gaussian processes (gps), have been one of the dominant paradigms for supervised machine learning over the last 20 years. These methods depend on defining a kernel function, k(x, x\u2032), which specifies how similar or correlated outputs y and y\u2032 are expected to be at two inputs x and x\u2032. By defining the measure of similarity between inputs, the kernel determines the pattern of inductive generalization.\nThe ability to learn kernels automatically has helped greatly in making kernel methods accessible to nonexperts. Examples include learning hyperparameters (Rasmussen & Williams, 2006), linear combination weights (Bach, 2009), and mappings from the input space to an embedding space (Salakhutdinov & Hinton, 2008). These techniques all pose kernel learning\n\u2217These authors contributed equally. \u2020University of Cambridge. \u2021Massachusetts Institute of Technology.\nas a (possibly high dimensional) parameter estimation problem. However, to apply existing kernel learning algorithms, the user must specify the parametric form of the kernel, and this can require considerable expertise, as well as trial and error.\nTo make kernel learning more generally applicable, we reframe the kernel learning problem as one of structure discovery. In particular, we formulate a space of kernel structures defined compositionally in terms of sums and products of a small number of base kernel structures. This provides an expressive modeling language which concisely captures many widely used techniques for constructing kernels. We focus on Gaussian process regression, where the kernel specifies a covariance function, because the Bayesian framework is a convenient way to formalize structure discovery. Borrowing discrete search techniques which have proved successful in equation discovery (Todorovski & Dzeroski, 1997) and unsupervised learning (Grosse et al., 2012), we automatically search over this space of kernel structures using marginal likelihood as the search criterion.\nWe found that our structure discovery algorithm is able to automatically recover known structures from synthetic data as well as plausible structures for a variety of real-world datasets. On a variety of time series datasets, the learned kernels yield decompositions of the unknown function into interpretable components that enable accurate extrapolation beyond the range of the observations. Furthermore, the automatically discovered kernels outperform a variety of widely used kernel classes and kernel combination methods on supervised prediction tasks.\nWhile we focus on Gaussian process regression, we believe our kernel search method can be extended to other supervised learning frameworks such as classification or ordinal regression, or to other kinds of kernel architectures such as kernel SVMs. We hope that\nar X\niv :1\n30 2.\n49 22\nv1 [\nst at\n.M L\n] 2\n0 Fe\nb 20\nthe algorithm developed in this paper will help replace the current and often opaque art of kernel engineering with a more transparent science of automated kernel discovery."}, {"heading": "2. Expressing structure through kernels", "text": "Gaussian process models use a kernel to define the covariance between any two function values: Cov(y, y\u2032) = k(x, x\u2032). The kernel specifies which structures are likely under the gp prior, which in turn determines the generalization properties of the model. In this section, we review the ways in which kernel families1can be composed to express diverse priors over functions.\nThere has been significant work on constructing gp kernels and analyzing their properties, summarized in Chapter 4 of (Rasmussen & Williams, 2006). Commonly used kernels families include the squared exponential (SE), periodic (Per), linear (Lin), and rational quadratic (RQ) (see Figure 1 and the appendix).\nPositive semidefinite kernels (i.e. those which define valid covariance functions) are closed under addition and multiplication. This allows one to create richly structured and interpretable kernel from well understood base components. All of the base kernels we use are one-dimensional; kernels over multidimensional inputs are constructed by adding and multiplying kernels over individual dimensions. These dimensions are rep-\n1When unclear from context, we use \u201dkernel family\u201d to refer to the parametric forms of the functions given in the appendix. A kernel is a kernel family with all of the parameters specified.\nresented using subscripts, e.g. SE2 represents an SE kernel over the second dimension of x.\nSummation By summing kernels, we can model the data as a superposition of independent functions, possibly representing different structure. Suppose functions f1, f2 are draw from independent gp priors, f1 \u223c GP(\u00b51, k1), f2 \u223c GP(\u00b52, k2). Then it follows that f := f1 + f2 \u223c GP(\u00b51 + \u00b52, k1 + k2).\nIn time series models, sums of kernels can express superposition of different processes, possibly operating at different scales. In multiple dimensions, summing kernels gives additive structure over different dimensions, similar to generalized additive models (Hastie & Tibshirani, 1990). These two kinds of structure are demonstrated in rows 2 and 4 of figure 2, respectively.\nMultiplication Multiplying kernels allows us to account for interactions between different input dimensions or different notions of similarity. For instance, in multidimensional data, the multiplicative kernel SE1 \u00d7 SE3 represents a smoothly varying function of dimensions 1 and 3 which is not constrained to be\nadditive. In univariate data, multiplying a kernel by SE gives a way of converting global structure to local structure. For instance, Per corresponds to globally periodic structure, whereas Per \u00d7 SE corresponds to locally periodic structure, as shown in row 1 of figure 2.\nMany architectures for learning complex functions, such as convolutional networks (LeCun et al., 1989) and sum-product networks (Poon & Domingos, 2011), include units which compute AND-like and OR-like operations. Composite kernels can be viewed in this way too. A sum of kernels can be understood as an OR-like operation: two points are considered similar if either kernel has a high value. Similarly, multiplying kernels is an AND-like operation, since two points are considered similar only if both kernels have high values. Because we are applying these operations to the similarity functions rather than the regression functions themselves, compositions of even a few base kernels are able to capture complex relationships in data which do not have a simple parametric form.\nExample expressions In addition to the examples given in Figure 2, many common motifs of supervised learning can be captured using sums and products of one-dimensional base kernels:\nBayesian linear regression Lin Bayesian polynomial regression Lin\u00d7 Lin\u00d7 . . . Generalized Fourier decomposition Per + Per + . . .\nGeneralized additive models \u2211D d=1 SEd\nAutomatic relevance determination \u220fD d=1 SEd Linear trend with deviations Lin + SE Linearly growing amplitude Lin\u00d7 SE\nWe use the term \u2018generalized Fourier decomposition\u2019 to express that the periodic functions expressible by a gp with a periodic kernel are not limited to sine and cosine."}, {"heading": "3. Searching over structures", "text": "As discussed above, we can construct a wide variety of kernel structures compositionally by adding and multiplying a small number of base kernels. In particular, we consider the four base kernel families discussed in Section 2: SE, Per, Lin, and RQ. Any algebraic expression combining these kernels using the operations + and \u00d7 defines a kernel family, whose parameters are the concatenation of the parameters for the base kernel families.\nOur search procedure begins by proposing all base kernel families applied to all input dimensions. We allow the following search operators over our set of expressions:\n(1) Any subexpression S can be replaced with S +B, where B is any base kernel family.\n(2) Any subexpression S can be replaced with S \u00d7B, where B is any base kernel family.\n(3) Any base kernel B may be replaced with any other base kernel family B\u2032.\nThese operators can generate all possible algebraic expressions. To see this, observe that if we restricted the + and \u00d7 rules only to apply to base kernel families, we would obtain a context-free grammar (CFG) which generates the set of algebraic expressions. However, the more general versions of these rules allow more flexibility in the search procedure, which is useful because the CFG derivation may not be the most straightforward way to arrive at a kernel family.\nOur algorithm searches over this space using a greedy search: at each stage, we choose the highest scoring kernel and expand it by applying all possible operators.\nOur search operators are motivated by strategies researchers often use to construct kernels. In particular,\n\u2022 One can look for structure, e.g. periodicity, in the residuals of a model, and then extend the model to capture that structure. This corresponds to applying rule (1).\n\u2022 One can start with structure, e.g. linearity, which is assumed to hold globally, but find that it only holds locally. This corresponds to applying rule (2) to obtain the structure shown in rows 1 and 3 of figure 2.\n\u2022 One can add features incrementally, analgous to algorithms like boosting, backfitting, or forward selection. These correspond to applying rules (1) or (2) to new dimensions not yet included in the model.\nScoring kernel families Choosing kernel structures requires a criterion for evaluating the structures. We choose marginal likelihood as our criterion since it balances the fit and complexity of a structure (e.g. Rasmussen & Ghahramani, 2001). To avoid an expensive integration over kernel parameters, we used the Bayesian information criterion (Schwarz, 1978) as an approximation.\nUnfortunately, optimizing over parameters is not a convex optimization problem, and the space can have many local optima. For example, in data with periodic structure, integer multiples of the true period (i.e. harmonics) are often local optima.\nTo alleviate this difficulty, we take advantage of our search procedure to construct smart initializations: all of the parameters which were part of the previous kernel are initialized to their previous values. The parameters are then optimized using conjugate gradients, randomly restarting the newly introduced parameters. This procedure is not guaranteed to find the global optimum, but it implements the commonly used heuristic of iteratively modeling residuals."}, {"heading": "4. Related Work", "text": "Nonparametric regression in high dimensions Nonparametric regression methods such as splines, locally weighted regression, and gp regression are popular because they are capable of learning arbitrary smooth functions of the data. Unfortunately, they suffer from the curse of dimensionality: it is very difficult for the basic versions of these methods to generalize well in more than a few dimensions. Applying nonparametric methods in high-dimensional spaces can require imposing additional structure on the model.\nOne such structure is additivity. Generalized additive models (GAM) assume the regression function is a transformed sum of functions defined on the individual dimensions: E[f(x)] = g\u22121( \u2211D d=1 fd(xd)). These models have a limited compositional form, but one which is interpretable and often generalizes well. In our grammar, we can capture analogous structure through sums of base kernels along different dimensions.\nIt is possible to add more flexibility to additive models by considering higher-order interactions between different dimensions. Additive Gaussian processes (Duvenaud et al., 2011) are a gp model whose kernel implicitly sums over all possible products of onedimensional base kernels. Plate (1999) constructs a gp with a composite kernel, summing an SE kernel along each dimension, with an SE-ARD kernel (i.e. a product of SE over all dimensions). Both of these models can be expressed in our grammar.\nA closely related procedure is smoothing-splines ANOVA (Wahba, 1990; Gu, 2002). This model is a linear combinations of splines along each dimension, all pairs of dimensions, and possibly higher-order combinations. Because the number of terms to consider grows exponentially in the order, in practice, only terms of first and second order are usually considered.\nSemiparametric regression (e.g. Ruppert et al., 2003) attempts to combine interpretability with flexibility by building a composite model out of an interpretable, parametric part (such as linear regression) and a \u2018catch-all\u2019 nonparametric part (such as a gp with an\nSE kernel). In our approach, this can be represented as a sum of SE and Lin.\nKernel learning There is a large body of work attempting to construct a rich kernel through a weighted sum of base kernels (e.g. Christoudias et al., 2009; Bach, 2009). While these approaches find the optimal solution in polynomial time, speed comes at a cost: the component kernels, as well as their hyperparameters, must be specified in advance.\nAnother approach to kernel learning is to learn an embedding of the data points. Lawrence (2005) learns an embedding of the data points into a low-dimensional space, and constructs a fixed kernel structure over that space. This model is typically used in unsupervised tasks and requires an expensive integration or optimisation over potential embeddings when generaliing to new data points. Salakhutdinov & Hinton (2008) use a deep neural network to learn an embedding; this is a flexible approach to kernel learning but potentially less interpretable.\nDiosan et al. (2007) and Bing et al. (2010) learn composite kernels for support vector machines and relevance vector machines, using genetic search algorithms. Our work goes beyond this prior work by demonstrating the structure implied by composite kernels, employing a Bayesian search criterion, and allowing for the automatic discovery of interpretable structure from data.\nStructure discovery There have been several attempts to uncover the structural form of a dataset by searching over a grammar of structures. For example, (Schmidt & Lipson, 2009), (Todorovski & Dzeroski, 1997) and (Washio et al., 1999) attempt to learn parametric forms of equations to describe time series, or relations between quantities. Because we learn expressions describing the covariance structure rather than the functions themselves, we are able to capture structure which does not have a simple parametric form.\nKemp & Tenenbaum (2008) learned the structural form of a graph used to model human similarity judgments. Examples of graphs included planes, trees, and cylinders. Some of their (discrete) graph structures have continous analogues in our own space; e.g. SE1 \u00d7 SE2 and SE1 \u00d7 Per2 can be seen as mapping the data to a plane and a cylinder, respectively.\nGrosse et al. (2012) performed a greedy search over a compositional model class for unsupervised learning, using a grammar and a search procedure which parallel our own. This model class contained a large number of existing unsupervised models as special cases and\nwas able to discover such structure automatically from data. Our work is tackling a similar problem, but in a supervised setting."}, {"heading": "5. Structure discovery in time series", "text": "In order to investigate our method\u2019s ability to discover structure, we performed the search over kernels on several time series datasets. Our method discovers rich structure in these datasets, and produces plausible extrapolations.\nAs discussed in section 2, a gp whose kernel is a sum of kernels can be viewed as a sum of functions drawn from component gps. This provides another method of visualizing the learned structures. In particular, all kernels in our search space can be equivalently written as sums of products of base kernels by applying distributivity e.g.\nSE\u00d7 (RQ + Lin) = SE\u00d7RQ + SE\u00d7 Lin.\nWe visualize the decompositions into sums of components using the formulae given in the appendix. The search was run to depth 10, using the base kernels from Section 2.\nMauna Loa atmospheric CO2 First, we analyzed records of carbon dioxide levels recorded at the Mauna Loa observatory. Since this dataset was analyzed in detail by Rasmussen & Williams (2006), it serves as a test of our algorithm\u2019s ability to recover known structure.\nFigure 3. Posterior mean and variance for different depths of kernel search. The dashed line marks the extent of the dataset. In the first column, the function is only modeled as a locally smooth function, and the extrapolation is poor. Next, a periodic component is added, and the extrapolation improves. At depth 3, the kernel can capture most of the relevant structure, and is able to extrapolate reasonably.\nFigure 3 shows the posterior function estimates under increasingly structured models, as measured by approximate marginal likelihood, as the search depth increases on this dataset.\n=\n+\n+\n+\nFigure 4 shows the complete posterior of the final model chosen by our method, together with its decomposition into additive components. The final model exhibits both a plausible extrapolation and interpretable components: a long-term trend, annual periodicity and medium-term deviations from the trend. We also plot the residuals, observing that there is little obvious structure left in the data.\nAirline passenger data Figure 6 shows the decomposition produced by applying our method to monthly totals of international airline passengers (Box et al., 1976). We observe similar components to the previous dataset i.e. a long term trend, annual periodicity and medium term deviations. In addition, the composite kernel captures the near-linearity of the trend as well as the linearly growing amplitude of the annual oscillations.\nSolar irradiance Data Finally, we analyzed annual solar irradiation data from 1610 to 2011 (Lean et al., 1995). The posterior and residuals of the learned kernel are shown in figure 5. None of the models in our search space are capable of parsimoniously representing the lack of variation from 1645 to 1715.\n=\n+\nDespite this, our approach fails gracefully: the learned kernel still captures the periodic structure, and the quickly growing posterior variance demonstrates that the model is uncertain about long term structure."}, {"heading": "6. Validation on synthetic data", "text": "We validated our method\u2019s ability to recover known structure on a set of synthetic datasets. For several composite kernel expressions, we constructed synthetic data by first sampling 300 points uniformly at random, then sampling function values at those points from a gp prior. We then added i.i.d. Gaussian noise to the function values, with variance chosen such that the standard deviation of the noise \u03c3n relative to the sample variance of the function was 0.1. Table 1 lists the composite kernels we used to generate data (subscripts indicate which dimension each kernel was applied to), the dimensionality D of the input space, the kernel chosen by our search, and the estimated noise variance.\nOur method finds all of the relevant structure in all but one test. In fact, it also discovers unintentionally introduced linear structure: functions sampled from SE kernels with long length scales occasionally resulted in near-linear trends in the data.\nThe estimated noise level \u03c3n numerically assesses the quality of the fit to the data; large positive or negative deviations from 0.1 would indicate under-fitting and over-fitting, respectively. The values reported show that the model is not severely over- or under-fitting."}, {"heading": "7. Quantitative evaluation", "text": "In addition to the qualitative evaluation in section 5, we also investigated quantitatively how our method performs on both extrapolation and interpolation tasks."}, {"heading": "7.1. Extrapolation", "text": "We compared the extrapolation capabilities of our model against standard baselines. Dividing the airline dataset into contiguous training and test sets, we computed the predictive mean-squared-error (MSE) of each method. We varied the size of the training set from the first 10% to the first 90% of the range of the data.\nFigure 7 shows the learning curves of linear regression, a variety of fixed kernel family gp models, and our method. gp models with only SE and Per kernels did not capture the long-term trends, since the best parameter values in terms of gp marginal likelihood only capture short term structure. Linear regression approximately captured the long-term trend, but quickly plateaued in predictive performance. The more richly structured gp models (SE + Per and SE\u00d7Per) eventually captured more structure and performed better, but the full structures discovered by our search outperformed the other approaches in terms of predictive performance for all data amounts."}, {"heading": "7.2. High-dimensional prediction", "text": "To evaluate the predictive accuracy of our method in a high-dimensional setting, we extended the comparison of (Duvenaud et al., 2011) to include our method. We performed 10 fold cross validation on 5 datasets2 comparing 5 methods in terms of MSE and predictive likelihood. Our structure search was run up to depth 10, using SE and RQ as the base kernel families.\nThe comparison included three methods with fixed kernel families: Additive gps, Generalized Additive Models (GAM), and a gp with a standard SE kernel using Automatic Relevance Determination (gp SEARD). Also included was the related kernel-search method of Hierarichical Kernel Learning (HKL).\nResults are presented in table 2; our method outperformed all other methods in all tests.\n2The data sets had dimensionalities ranging from 4 to 13 and the number of data points ranged from 150 to 450."}, {"heading": "8. Discussion", "text": "\u201cIt would be very nice to have a formal apparatus that gives us some \u2018optimal\u2019 way of recognizing unusual phenomena and inventing new classes of hypotheses that are most likely to contain the true one; but this remains an art for the creative human mind.\u201d\nE. T. Jaynes, 1985\nThe ability to learn kernel parameters and combination weights automatically has been an important factor in enabling the widespread use of kernel methods. For the most part, however, it has been up to the user to choose the proper form for the kernel, a task which requires considerable expertise.\nTowards the goal of automating this process, we introduced a space of composite kernels defined compositionally as sums and products of a small number of base kernels. We proposed a search procedure for this space of kernels which parallels the process of scientific discovery.\nWe found that the learned structures are often capable of accurate extrapolation in complex time series datasets and are competitive with widely used kernel classes and kernel combination methods on a variety of prediction tasks. The learned kernels often yield decompositions of a signal into diverse and interpretable components, which provides an additional degree of reassurance that the learned structure reflects the world. We believe that a data-driven approach to choosing\nkernel structures automatically can help make nonparametric regression and classification methods accessible to non-experts."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Carl Rasmussen for helpful discussions."}, {"heading": "A. Appendix", "text": "Kernel definitions For scalar-valued inputs, the squared exponential (SE), periodic (Per), linear (Lin), and rational quadratic (RQ) kernels are defined as follows:\nkSE(x, x \u2032) = \u03c32 exp ( \u2212 (x\u2212x \u2032)2\n2`2 ) kPer(x, x \u2032) = \u03c32 exp ( \u2212 2 sin 2(\u03c0|x\u2212x\u2032|/p) `2\n) kLin(x, x \u2032) = \u03c32b + \u03c3 2 v(x\u2212 `)(x\u2032 \u2212 `)\nkRQ(x, x \u2032) =\n( 1 + (x\u2212x \u2032)2\n2\u03b1`2 )\u2212\u03b1 Posterior decomposition We can analytically decompose a gp posterior distribution over additive components using the following identity: The conditional distribution of a Gaussian vector f1 conditioned on its sum with another Gaussian vector f = f1 + f2 where f1 \u223c N (\u00b51,K1) and f2 \u223c N (\u00b52,K2) is given by\nf1|f \u223c N ( \u00b51 + K1 T(K1 + K2) \u22121 (f \u2212 \u00b51 \u2212 \u00b52) ,\nK1 \u2212K1T(K1 + K2)\u22121K1 ) ."}], "references": [{"title": "Exploring large feature spaces with hierarchical multiple kernel learning", "author": ["F. Bach"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Bach,? \\Q2009\\E", "shortCiteRegEx": "Bach", "year": 2009}, {"title": "A GP-based kernel construction and optimization method for RVM", "author": ["W. Bing", "Z. Wen-qiong", "C. Ling", "L. Jia-hong"], "venue": "In International Conference on Computer and Automation Engineering (ICCAE),", "citeRegEx": "Bing et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bing et al\\.", "year": 2010}, {"title": "Time series analysis: forecasting and control", "author": ["G.E.P. Box", "G.M. Jenkins", "G.C. Reinsel"], "venue": null, "citeRegEx": "Box et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Box et al\\.", "year": 1976}, {"title": "Bayesian localized multiple kernel learning", "author": ["M. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "Technical report, EECS Department,", "citeRegEx": "Christoudias et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Christoudias et al\\.", "year": 2009}, {"title": "Evolving kernel functions for SVMs by genetic programming", "author": ["L. Diosan", "A. Rogozan", "J.P. Pecuchet"], "venue": "In Machine Learning and Applications,", "citeRegEx": "Diosan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Diosan et al\\.", "year": 2007}, {"title": "Additive Gaussian processes", "author": ["D. Duvenaud", "H. Nickisch", "C.E. Rasmussen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Duvenaud et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2011}, {"title": "Exploiting compositionality to explore a large space of model structures", "author": ["R.B. Grosse", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Grosse et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2012}, {"title": "Smoothing spline ANOVA models", "author": ["C. Gu"], "venue": null, "citeRegEx": "Gu,? \\Q2002\\E", "shortCiteRegEx": "Gu", "year": 2002}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "Highly informative priors", "author": ["E.T. Jaynes"], "venue": "In Proceedings of the Second International Meeting on Bayesian Statistics,", "citeRegEx": "Jaynes,? \\Q1985\\E", "shortCiteRegEx": "Jaynes", "year": 1985}, {"title": "The discovery of structural form", "author": ["C. Kemp", "J.B. Tenenbaum"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kemp and Tenenbaum,? \\Q2008\\E", "shortCiteRegEx": "Kemp and Tenenbaum", "year": 2008}, {"title": "Probabilistic non-linear principal component analysis with gaussian process latent variable models", "author": ["N. Lawrence"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lawrence,? \\Q2005\\E", "shortCiteRegEx": "Lawrence", "year": 2005}, {"title": "Reconstruction of solar irradiance since 1610: Implications for climate change", "author": ["J. Lean", "J. Beer", "R. Bradley"], "venue": "Geophysical Research Letters,", "citeRegEx": "Lean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Lean et al\\.", "year": 1995}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Accuracy versus interpretability in flexible modeling: Implementing a tradeoff using Gaussian process models", "author": ["T.A. Plate"], "venue": "Behaviormetrika,", "citeRegEx": "Plate,? \\Q1999\\E", "shortCiteRegEx": "Plate", "year": 1999}, {"title": "Sum-product networks: a new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Conference on Uncertainty in AI,", "citeRegEx": "Poon and Domingos,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos", "year": 2011}, {"title": "Occam\u2019s razor", "author": ["C.E. Rasmussen", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmussen and Ghahramani,? \\Q2001\\E", "shortCiteRegEx": "Rasmussen and Ghahramani", "year": 2001}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Semiparametric regression, volume 12", "author": ["D. Ruppert", "M.P. Wand", "R.J. Carroll"], "venue": null, "citeRegEx": "Ruppert et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ruppert et al\\.", "year": 2003}, {"title": "Using deep belief nets to learn covariance kernels for Gaussian processes", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Advances in Neural information processing systems,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2008}, {"title": "Distilling free-form natural laws from experimental data", "author": ["M. Schmidt", "H. Lipson"], "venue": null, "citeRegEx": "Schmidt and Lipson,? \\Q2009\\E", "shortCiteRegEx": "Schmidt and Lipson", "year": 2009}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics,", "citeRegEx": "Schwarz,? \\Q1978\\E", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "Declarative bias in equation discovery", "author": ["L. Todorovski", "S. Dzeroski"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Todorovski and Dzeroski,? \\Q1997\\E", "shortCiteRegEx": "Todorovski and Dzeroski", "year": 1997}, {"title": "Spline models for observational data", "author": ["G. Wahba"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "Wahba,? \\Q1990\\E", "shortCiteRegEx": "Wahba", "year": 1990}, {"title": "Discovering admissible model equations from observed data based on scale-types and identity constraints", "author": ["T. Washio", "H. Motoda", "Y Niwa"], "venue": "In International Joint Conference On Artifical Intelligence,", "citeRegEx": "Washio et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Washio et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Examples include learning hyperparameters (Rasmussen & Williams, 2006), linear combination weights (Bach, 2009), and mappings from the input space to an embedding space (Salakhutdinov & Hinton, 2008).", "startOffset": 99, "endOffset": 111}, {"referenceID": 6, "context": "Borrowing discrete search techniques which have proved successful in equation discovery (Todorovski & Dzeroski, 1997) and unsupervised learning (Grosse et al., 2012), we automatically search over this space of kernel structures using marginal likelihood as the search criterion.", "startOffset": 144, "endOffset": 165}, {"referenceID": 13, "context": "Many architectures for learning complex functions, such as convolutional networks (LeCun et al., 1989) and sum-product networks (Poon & Domingos, 2011), include units which compute AND-like and OR-like operations.", "startOffset": 82, "endOffset": 102}, {"referenceID": 21, "context": "To avoid an expensive integration over kernel parameters, we used the Bayesian information criterion (Schwarz, 1978) as an approximation.", "startOffset": 101, "endOffset": 116}, {"referenceID": 5, "context": "Additive Gaussian processes (Duvenaud et al., 2011) are a gp model whose kernel implicitly sums over all possible products of onedimensional base kernels.", "startOffset": 28, "endOffset": 51}, {"referenceID": 5, "context": "Additive Gaussian processes (Duvenaud et al., 2011) are a gp model whose kernel implicitly sums over all possible products of onedimensional base kernels. Plate (1999) constructs a gp with a composite kernel, summing an SE kernel along each dimension, with an SE-ARD kernel (i.", "startOffset": 29, "endOffset": 168}, {"referenceID": 23, "context": "A closely related procedure is smoothing-splines ANOVA (Wahba, 1990; Gu, 2002).", "startOffset": 55, "endOffset": 78}, {"referenceID": 7, "context": "A closely related procedure is smoothing-splines ANOVA (Wahba, 1990; Gu, 2002).", "startOffset": 55, "endOffset": 78}, {"referenceID": 0, "context": "Kernel learning There is a large body of work attempting to construct a rich kernel through a weighted sum of base kernels (e.g. Christoudias et al., 2009; Bach, 2009).", "startOffset": 123, "endOffset": 167}, {"referenceID": 11, "context": "Lawrence (2005) learns an embedding of the data points into a low-dimensional space, and constructs a fixed kernel structure over that space.", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "Lawrence (2005) learns an embedding of the data points into a low-dimensional space, and constructs a fixed kernel structure over that space. This model is typically used in unsupervised tasks and requires an expensive integration or optimisation over potential embeddings when generaliing to new data points. Salakhutdinov & Hinton (2008) use a deep neural network to learn an embedding; this is a flexible approach to kernel learning but potentially less interpretable.", "startOffset": 0, "endOffset": 340}, {"referenceID": 1, "context": "(2007) and Bing et al. (2010) learn composite kernels for support vector machines and relevance vector machines, using genetic search algorithms.", "startOffset": 11, "endOffset": 30}, {"referenceID": 24, "context": "For example, (Schmidt & Lipson, 2009), (Todorovski & Dzeroski, 1997) and (Washio et al., 1999) attempt to learn parametric forms of equations to describe time series, or relations between quantities.", "startOffset": 73, "endOffset": 94}, {"referenceID": 2, "context": "Airline passenger data Figure 6 shows the decomposition produced by applying our method to monthly totals of international airline passengers (Box et al., 1976).", "startOffset": 142, "endOffset": 160}, {"referenceID": 12, "context": "Solar irradiance Data Finally, we analyzed annual solar irradiation data from 1610 to 2011 (Lean et al., 1995).", "startOffset": 91, "endOffset": 110}, {"referenceID": 5, "context": "To evaluate the predictive accuracy of our method in a high-dimensional setting, we extended the comparison of (Duvenaud et al., 2011) to include our method.", "startOffset": 111, "endOffset": 134}], "year": 2017, "abstractText": "Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.", "creator": "LaTeX with hyperref package"}}}