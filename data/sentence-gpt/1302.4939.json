{"id": "1302.4939", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Conditioning Methods for Exact and Approximate Inference in Causal Networks", "abstract": "We present two algorithms for exact and approximate inference in causal networks. The first algorithm, dynamic conditioning, is a refinement of cutset conditioning that has linear complexity on some networks for which cutset conditioning is exponential. The second algorithm, B-conditioning, is an algorithm for approximate inference that allows one to trade-off the quality of approximations with the computation time. The second algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The final algorithm uses one algorithm and another and uses the same methods as the original algorithm (see Supplementary Appendix 4, below).\n\n\n\n\n\n\nThe second algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The third algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fourth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fourth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fourth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fourth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fourth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fourth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fifth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fifth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fifth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fifth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear. The fifth algorithm is a proof-of-concept approximation that is nonlinear (as a consequence of B-conditioning), which is linear", "histories": [["v1", "Wed, 20 Feb 2013 15:19:47 GMT  (308kb)", "http://arxiv.org/abs/1302.4939v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adnan darwiche"], "accepted": false, "id": "1302.4939"}, "pdf": {"name": "1302.4939.pdf", "metadata": {"source": "CRF", "title": "Conditioning Algorithms for Exact and Approximate Inference in Causal Networks", "authors": ["Adnan Darwiche"], "emails": ["darwiche@risc."], "sections": [{"heading": null, "text": "1 INTRODUCTION\nCutset conditioning is one of the earliest algorithms for evaluating multiply connected networks [6]. Cut set conditioning works by reducing multiply connected networks into a number of conditioned singly con nected networks, each corresponding to a particular instantiation of a loop cutset [6, 7]. Cutset condition ing is simple, but leads to an exponential number of conditioned networks. Therefore, cutset conditioning is not practical unless the size of a loop cutset is rela tively small.\nIn this paper, we introduce the notions of relevant and local cutsets, which seem to be very effective in im proving the efficiency of cutset conditioning. Relevant and local cutsets are subsets of a loop cutset [8]. We use these new notions in developing a refined algo rithm, called dynamic conditioning. Dynamic condi tioning has a linear computational complexity on net works such as the diamond ladder and cascaded n-bit adders, where cutset conditioning leads to an exponen tial behavior.\nRelevant and local cutsets play the following comple mentary roles with respect to cutset conditioning. Rel evant cutsets reduce the time required for evaluating a conditioned network using the polytree algorithm. Specifically, relevant cutsets characterize cutset vari ables that affect the value of each message passed by\nthe polytree algorithm. Therefore, relevant cutsets tell us whether two conditioned networks lead to the same value of a polytree message so that the message will be computed only once. Relevant cutsets can be identi fied in linear time given a loop cutset and they usually lead to exponential savings when utilized by cutset conditioning.\nLocal cutsets, on the other hand, eliminate the need for considering an exponential number of conditioned networks. As it turns out, one need not condition on a loop cutset in order for the polytree algorithm to com mence. Instead, each polytree step can be validated in a multiply connected network by only conditioning on a local cutset, which is a subset of a loop cutset. Lo cal cutsets can be computed in polynomial time from relevant cutsets and since they eliminate the need for conditioning on a full loop cutset, they also lead to exponential savings when utilized by cutset condition ing.\nDynamic conditioning, our first algorithm in this pa per, is only a refinement of cutset conditioning using the notions of local and relevant cutsets.\nThe second algorithm, B-conditioning, is an algorithm for approximate reasoning that combines dynamic con ditioning (or another exact algorithm) with a satisfia bility tester or a kappa algorithm to yield an algorithm in which one can trade the quality of approximate in ference with computation time. As we shall discuss, the properties of B-conditioning depend heavily on the underlying satisfiability tester or kappa algorithm. We discuss B-conditioning and provide some experimental results to illustrate its behavior.\n2 DYNAMIC CONDITIONING\nWe start this section by a review of cutset condition ing and then follow by discussing relevant and local cutsets.\n2.1 A review of cutset conditioning\nWe adopt the same notation used in [6] for describ ing the polytree algorithm. In particular, variables\n100 Darwiche\nare denoted by uppercase letters, sets of variables are denoted by boldface uppercase letters, and instantia tions are denoted by lowercase letters. The notations ei, e_x, e&x and e_xy have the usual meanings. We also have the following definitions:\nBEL(x) =def Pr(x 1\\ e)\n7r(x) =def Pr(x 1\\ ei) A(x) =def Pr(e.X I x)\n1rx(u) =def Pr(u 1\\ etx) Ay(x) =def Pr(exy 1 x).\nFollowing [7], we defined BEL( x) as Pr( x 1\\ e) instead of Pr(x I e) to avoid computing the probability of evidence e when applying cutset conditioning. The polytree equations are given below for future reference:\nBEL(x)\n7r(x)\nA(x)\n1ry, (x)\n>.x(u;)\n1r(x)A(x) ( 1)\nI: Pr(x I u1, .. . , un) II 7rx(u;J2) tL11\u2022\u2022\u20221Un\nII Ay;(x) (3)\n1r(x) II Ayk(x) (4) k:j:i\nL >.(x) L Pr(x I u) II 7rx (uk)(5) X tlk:k:j:i k:j:i\nThe above polytree equations are valid when the network is singly connected. When the network is not singly connected, we appeal to the notion of a conditioned network in order to apply the polytree al gorithm. Conditioning a network on some instantia tion C = c involves three modifications to the network. First, we remove as many outgoing arcs of C as possi ble without destroying the connectivity of the network (arc absorption) [7]. Next, if the arc from C to V is eliminated, the probability matrix of V is changed by keeping only entries that are consistent with C = c. Third, the instantiation C =c is added as evidence to the network.\nGiven the notion of a conditioned network, we can now describe how cutset conditioning works. Cutset condi tioning involves three major steps. First, we identify a loop cutset C, which is a set of variables the condi tioning on which leads to a singly connected network. Next, we condition the network on all possible instan tiations c of the loop cutset and then use the polytree algorithm to compute Pr(x 1\\ e 1\\ c) with respect to each conditioned network. Finally, we sum up these probabilities to obtain Pr(x 1\\ e)= BEL(x).\nFrom here on, we will use the notations 1r(x I c), A(x I c), 1rx(u; I c) , and Ay;(x I c) to denote the sup ports 1r(x), A(x), 1rx(u;), and >.y;(x) in a network that is conditioned on c. Using this notation, cutset condi tioning can be described as computing BEL( x) using the sum L:c BEL(x I c), where C is a loop cutset.1\n1 Before we end this section, we would like to stress that\n2.2 Relevant cutsets\nThe notion of a relevant cutset was born out of the fol lowing observations. First, the multiple applications of the polytree algorithm in the context of cutset con ditioning involve many redundant computations. Sec ond, most of this redundancy can be characterized and avoided using only a linear time preprocessing on the given network and its loop cutset. We will elaborate on these observations with an example first and then provide a more general treatment.\nConsider the singly connected network in Figure 1(b) , for example, which results from conditioning the multi ply connected network in Figure 1(a) on a loop cutset. Assuming that all variables are binary, cutset condi tioning will apply the polytree algorithm 210 = 1024 times to this network. Note, however, that when two of these applications agree on the instantiation of vari ables U1, M3, Y2, Ms, Mu, they also agree on the value of the diagnostic support A( x) , independently of the in stantiation of other cutset variables. This means that cutset conditioning can get away with computing the diagnostic support A( x) only 25 times. This also means that 992 of the 1024 computations performed by cutset conditioning are redundant!\nThe cutset variables U1, M3, Y2, M5 , M11 are called the relevant cutset for A( x) in this case. This relevant cut set can be identified in linear time and when taken into consideration will save 992 redundant computations of A( x) . These savings can be achieved by storing each computed value of A( x) in a cache that is indexed by instantiations of relevant cutsets. When cutset condi tioning attempts to compute the value of A( x) under some conditioning case cc1, the cache is checked to see whether A(x) was computed before under a condition ing case cc2 that agrees with cc1 on the relevant cutset for A( x). In such case, the value of A( x) is retrieved and no additional computation is incurred.\nMore generally, each causal or diagnostic support computed by the polytree algorithm is affected by only a subset of the loop cutset, which is called its relevant cutset. We will use the notations Ri, R.X, R& x and Rxy to denote the relevant cutsets for the supports 1r(x), A(x), 1rx(u), and Ay(x), respectively. Before we define these cutsets formally, consider the following examples of relevant cutsets in connection to Figure 1(b) :\nRi = U1, N2, N8, Ng, Nlo, N16 is the relevant cutset for 1r( x ) .\nthe notations e:t, e:X, etx and e:Xy could be well defined even with respect to multiply connected networks. This means that the notations 1r(x), A(x), 1rx(u;) and AY;(x) could also be well defined with respect to multiply con nected networks. For example, the causal and diagnostic supports for variable X, 1r(x) and A(x), are well defined in Figure l (c) because the evidence decompositions ej( and e:X are also well defined. This observation is crucial for understanding local cutsets to be discussed in Section 2.3.\nConditioning Algorithms for Exact and Approximate Inference in Causal Networks 101\nRx = U1, M3, M5, Y2, M11 is the relevant cutset for A(x).\nRt5N4 = Ns, Ng, N1o, N16 is the relevant cutset for 11\"N4 (n5 ) \u00b7\nRxy, = U1, M3, M5 is the relevant cutset for Ay,(x).\nIn general, a cutset variable is irrelevant to a particular message if the value of the message does not dependent on the particular instantiation of that variable.\nDefinition 1 (Relevant Cutsets) R:t-, Rx, Rtx and Rx are relevant cutsets for 1r(x I c), A(x I c), 1rx(u I c), and Ay(x I c), respectively, precisely when the values of messages 1r(x I c), A(x I c), 1rx(u I c), and Ay(x I c) do not dependent on the specific instan tiations ofC\\Rk, C\\Rx, C\\Rtx andC\\Rxy, respectively.\nNote that both {C1, C2, C3} and {C1, C2} could be relevant cutsets for some message, according to Defi nition 1. This means that the instantiation of C3 is irrelevant to the message. We say in this case that the relevant cutset {C1, C2} is tighter than {C1, C2, C3}. Following is a proposal for computing relevant cutsets in time linear in the size of a network, but that is not guaranteed to compute the tightest relevant cutsets.\nLet A k denote variables that are parents of X in a multiply connected network M but are not parents of X in the network that results from conditioning M on a loop cutset. Moreover, let Ax denote {X} if X belongs to the loop cutset and 0 otherwise.2 Then (1) Rt x can be At U A(j union all cutset variables that are relevant to 'messages coming into U; except from X; (2) Rxy can be At U Ay. union all cutset variables that are 'relevant to messa\ufffdes coming into Y; except from X; (3) R:k can be Ax union all cutset variables relevant to causal messages into X; and ( 4) Rx can be Ax union all cutset variables relevant to diagnostics messages coming into X. As we shall see later, relevant cutsets are the key ele ment dictating the performance of dynamic condition ing. The tighter relevant cutsets are, the better the performance of dynamic conditioning. This will be discussed further in Section 2.6 .\n2.3 Local cutsets\nRelevant cutsets eliminate many redundant compu tations in cutset conditioning. But relevant cutsets do not change the computational complexity of cutset conditioning. That is, one still needs to consider an exponential number of conditioned networks, one for each instantiation of the loop cutset.\n21\u00a3 C E A i, then the instantiation of C dictates the matrix of X in the conditioned network. And if C E A)(, then the instantiation of C corresponds to an observation about X.\nThe notion of a local cutset addresses the above issue. We will illustrate the concept of a local cutset by an ex ample first and then follow with a more general treat ment. Consider again the multiply connected network in Figure 1(a) and suppose that we want to compute the belief in variable X. According to the textbook definition of cutset conditioning, one must apply the polytree algorithm to each instantiation of the cutset, which contains 10 variables in this case. This leads to 210 applications of the polytree algorithm, assuming again that all variables are binary. Suppose, however, that we condition the network on cutset variable U1, thus leading to the network in Figure 1(c). In this net work, the causal and diagnostic supports for variable X, 1r(x I ul) and A(x I u1), are well defined and can be computed independently. Moreover, the belief in variable X can be computed using the polytree Equa tion 1:\nBEL(x) = L 1r(x I u1)A(x I ul).\nNote that computing the causal support for X involves a network with a cutset of 5 variables, while computing the diagnostic support for X involves a network with a cutset of 4 variables. If we compute these causal and diagnostic supports using cutset conditioning, we are effectively considering only 2(25+24) = 96 conditioned networks as opposed to the 210 = 1024 networks con sidered by cutset conditioning.\nThe variable U1 is called a belief cutset for variable X in this case. The reason is that although the network is not singly connected (therefore, the polytree algo rithm is not applicable), conditioning on u1 leads to a network in which Equation 1 is valid. In general, one does not need a singly connected network for Equa tion 1 to be valid. One only needs to make sure that X is on every path that connects one of its descendants to one of its ancestors. But this can be guaranteed by conditioning on a local cutset:\nDefinition 2 (Belief Cutset) A belief cutset for variable X, written C x, is a set of variables the con ditioning on which makes X part of every undirected path connecting one of its descendants to one of its ancestors.\nIn general, by conditioning a multiply connected net work on a belief cutset for variable X, the network becomes partitioned into two parts. The first part is connected to X through its parents while the second part is connected to X through its children - see Fi\ufffd ure 1(c). This makes the evidence decompositions ex and ex well defined. It also makes the causal and diag nostic supports 1r(x I ex) and A(x I ex) well defined. By appealing to belief cutsets, Equation 1 can be gen eralized to multiply connected networks as follows:\nBEL(x) = L 1r(x I ex )A(x I ex). (6) Cx\nThe same applies to computing the causal and diag nostic supports for a variable. Each of Equations 2\n102 Darwiche\nand 3 do not require a singly connected network to be valid. Instead, they only require, respectively, that (1) X be on every path that goes between variables con nected to X through different parents, and (2) X be on every path that goes between variables connected to X through different children.\nTo satisfy these conditions, one need not condition on a loop cutset:\nDefinition 3 (Causal Cutset) A causal cutset for variable X, written Ci, is a set of variables such that conditioning on Cx uct makes X part of every undi rected path that goes between variables that are con nected to X through different parents.\nDefinition 4 (Diagnostic Cutset) A diagnostic cutset for variable X, written Cx, is a set of variables such that conditioning on Cx U Cx makes X part of every undirected path that goes between variables that are connected to X through different children.\nBelief, causal, and diagnostic cutsets are what we call local cutsets.\nIn general, by conditioning a multiply connected net work on a causal cutset for variable X (after condition ing on a belief cutset for X), we generalize Equation 2 \u00b7 to multiply connected networks:\n1r(x I c)= L L Pr(x I u1, . . . , un) II 1rx(u; I c,ci). ct Ut1 \u2022 \u2022 \u2022 1Un\n(7)\nSimilarly, by conditioning a multiply connected net work on a diagnostic cutset for variable X (after con ditioning on a belief cutset for X), we generalize Equa tion 3 to multiply connected networks:\n.A(x I c) = L II .Ay.(x I c, cx ). (8) c:;.; i\nFollowing is an example of using diagnostic cutsets. In Figure 1 (c) , Equation 3 is not valid for computing the diagnostic support for variable X. But if we condition the network on M5, thus obtaining the network in Fig ure 1(d) , Equation 3 becomes valid. This is equivalent to using Equation 8 with M5 as a diagnostic cutset for variable X:\n.A(x I u!) = L II .Ay,(x I u1, m5 ) . ms i\nEquations 6, 7, and 8 are generalizations of their poly tree counterparts. They apply to multiply connected networks as well as to singly connected ones. These equations are similar to the polytree equations except for the extra conditioning on local cutsets. Comput ing local cutsets is very efficient given a loop cutset, a topic that will be explored in Section 2.4. But before we end this section, we need to show how to compute\nthe causal and diagnostic supports that variables send to their neighbors.\nIn the polytree algorithm, the message 1ry, ( x) that variable X sends to its child Y; can be computed from the causal support for X and from the messages that X receives from its children except child Y;. In multiply connected networks, however, these supports are not well defined unless we condition on local cutsets first. That is, to compute the message 1ry, ( x), we must first condition on a belief cutset for X to split the network into two parts, one above and one below X. We must then condition on a diagnostic cutset for X to split the network below X into a number of sub-networks, each connected to a child of X. That is, the message that variable X sends to its child Y; is computed as follows:\n(9) This generalizes Equation 4 to multiply connected net works.\nSimilarly, we compute the message -Ax ( u;) as follows:\n-Ax(u; I c)= LL.A(x I c, cx) Cx X\nL L Pr(x I u) II 1rx(uk I c, cx, ci). ct uk:k;ti k;ti\n(10)\nThis generalizes Equation 5 to multiply connected net works.\nEquations 6, 7, 8, 9 & 10 are the core of the dynamic conditioning algorithm. Again, these equations paral lel the ones defining the polytree algorithm [6, 7]. The only difference is the extra conditioning on local cut sets, which makes the equations applicable to multiply connected networks.\n2.4 Relating local and relevant cutsets\nWhat is most intriguing about local and relevant cut sets is the way they relate to each other. As we shall see, local cutsets can be computed in polynomial time from relevant cutsets and the computation has a very intuitive meaning. First, cutset variables that are rele vant to both the causal support 1r( x) and the diagnos tic support .A( x) constitute a belief cutset for variable X. Next, cutset variables that are relevant to more than two causal messages 7rx( u;) constitute a causal cutset for variable X. Finally, cutset variables that are relevant to more than two diagnostic messages .Ay, ( x) constitute a diagnostic cutset for variable X.\nTheorem 1 We have the following:\n1. Ri n Rx constitutes a belief cutset for variable X.\nConditioning Algorithms for Exact and Approximate Inference in Causal Networks 103\n2. k\ufffd u (w R&,x n R&ix) constitutes a causal t,J\ncutset for variable X.\n3. A Xu (w Rxy, n Rxyj) constitutes a diagnost,J\ntic cutset for variable X.\nHere, i'#j.\nIntuitively, if two computations are to be made in dependent, one must fix the instantiation of cutset variables that are relevant to both of them. Local cutsets attempt to make computations independent. Relevant cutsets tell us what computations depend on what variables. Hence the above relation between the two classes of cutsets.\n2.5 Dynamic conditioning\nThe dynamic conditioning algorithm as described in this section is oriented towards computing the belief in a single variable. To compute the belief in every variable of a network, one must apply the algorithm to each variable individually. But since a cache is be ing maintained, the results of computations for one variable are utilized in the computations for another variable.\nTo compute the belief in variable X, the algorithm proceeds as follows. For each instantiation of C x, it computes the supports 7r(x I ex) and .\\(x I ex), com bines them to obtain BEL(x I ex), and then sums the results of all instantiations to obtain BEL(x ). This implements Equation 6. To compute the causal sup port 7r(x I ex), Equation 7 is used. And to compute the diagnostic support .\\(x I ex), Equation 8 is used. Applying these two equations invokes the application of Equations 9 and 10, which are used to compute the messages directed from one variable to another.\nIf we view the application of an equation as a request for computing some support, then computing the belief in a variable causes a chain reaction in which each request leads to a set of other requests. This sequence of requests ends at the boundaries of the network.\nTherefore, the control flow in the dynamic condition ing algorithm is similar to the first pass in the revised polytree algorithm [7]. The only difference is the ex tra conditioning on local cutsets. For example, the causal support for variable X in Figure 1(b) will be computed twice, once for each instantiation of U1; and the diagnostic message for X from its child Y1 will be computed four times, once for each instantiation of the variables {U1, M5}. To avoid redundant computations, dynamic condition ing stores the value of each computed support together with the instantiation of its relevant cutset in a cache. Whenever the support is requested again, the cache\nis checked to see whether the support has been com puted under the same instantiation of its relevant cut set. For example, when computing the belief in V6 in Figure 2, the causal support 11'V3 ( v!) will be requested four times, once for each instantiation of the variables in {V2, V5} . But the relevant cutset of this support is Rt v, = {V2}. Therefore, two of these computations are redundant since the instantiation of variable Vs is irrelevant to the value of 7rv3 ( v1) in this case.\nTo summarize, the control flow in the dynamic con ditioning algorithm is similar to the first pass in the revised polytree algorithm except for the conditioning on local cutsets and for the maintenance of a cache that indexes computed supports by the instantiation of their relevant cutsets.\nTo give a sense of the savings that relevant and local cutsets lead to, we mention the following examples. First, to compute the belief in variable V6 in the di amond ladder of Figure 2, the dynamic conditioning algorithm passes only two messages between any two variables, independently of the ladder's size. Note, however, that the performance of cutset conditioning is exponential in the ladder's size. A linear behavior is also obtained in a network structure that corresponds to an n-bit adder. Here again, cutset conditioning will lead to a behavior that is exponential in the size of the adder. Finally, in the network of Figure 1(a), which has a cutset of 10 variables, dynamic conditioning com putes the belief in variable X by passing at most eight messages between any two variables in the network. The textbook definition of cutset conditioning passes 1024 across each arc in this case.\n2.6 Relevant Cutsets: The Hook for Independence\nRelevant cutsets are the key element dictating the per formance of dynamic conditioning. The tighter rel evant cutsets are, the better the performance of dy namic conditioning. We can see this in two ways. First, tighter relevant cutsets mean smaller local cut sets and, therefore, less conditioning cases. Second, tighter relevant cutsets mean less redundant computa tions.\nDeciding what cutsets are relevant to what messages is a matter of identifying independence. Therefore, the tightest relevant cutsets would require complete uti lization of independence information, which explains the title of this section.\nTherefore, if one is to compute the t.ightest relevant cutsets, then one must take available evidence into consideration. Evidence could be an important factor because some cutset variables may become irrelevant to some messages given certain evidence.\nMost existing algorithms ignore evidence in the sense that they are justified for all patterns of evidence that might be available. This might simplify the discus sion and justification of algorithms, but may also lead\n104 Darwiche\nto unnecessary computational costs. This fact is well known in the literature and is typically handled by various optimizations that are added to algorithms or by preprocessing that prunes part of a network. Most of these pruning and optimizations are rooted in con siderations of independence, but there does not seem to be a way to account for them consistently.\nIt is our belief that the notion of relevant cutsets is a useful start for addressing this issue. Relevant cutsets do provide a very simple mechanism for translating in dependence information into computational gains. We have clearly not utilized this mechanism completely in this paper, but this is the subject of our current work in which we are targeting an algorithm for comput ing relevant cutsets that is complete with respect to d-separation. 3\n3 B-CONDITIONING\nB-conditioning is a method for the approximate up dating of causal networks. B-conditioning is based on an intuition that has underlied formal reasoning for quite a while: \"Assumptions about the world do sim plify computations.\" The difficulty in formalizing this intuition, however, has been in (a) characterizing what assumptions are good to make and (b) utilizing these assumptions computationally.\nThe answer to (a) is very task dependent. What makes a good assumption in one task may be a very unwise assumption in another. But in this paper, we are only concerned with the task of updating probabilities in causal networks. In this regard, suppose that comput ing Pr(x I\\ a) is easier than computing Pr(x). There fore, the assumption a would be good from a com putational viewpoint as long as Pr(x I\\ a) is a good approximation of Pr(x). But this would hold only if Pr(x I\\ -.a) is very small. Therefore, the value of Pr( x I\\ -.a) measures the quality of the assumption a from an approximation viewpoint. 4\nThe answer to (b) is clear in causal networks: we can utilize assumptions computationally by using them to instantiate variables, thus cutting out arcs, and sim plifying the topology of a causal network. At one ex treme, we can assume the value of each cutset variable, which would reduce a network to a polytree and make our inference polynomial. But this may not lead to a good approximation of the exact probabilities. Typ ically, one would instantiate some of the cutset vari ables, thus reducing the number of loops in a network but not eliminating them completely.\nIn utilizing assumptions as mentioned above, one must adjust the underlying algorithm so that it computes\n3 Even then we would not be finished since there are in dependences that are uncovered by d-separation, those hid den in the probability matrices associated with variables.\n4We can also use Pr(...,a) as the measure since Pr(x 1\\ ...,a) :::; Pr(...,a), but Pr(x 1\\ ..,a) is more informative.\nPr(x I\\ e) as opposed to Pr(x I e) since Pr(x I\\ a) is the approximation to Pr(x) in this case, not Pr(x I a). Now suppose that a variable V has multiple values, say three of them v1, v2 and v3. Suppose further that our assumption is that v2 is impossible. This assump tion is typically called a \"finding,\" as opposed to an \"observation.\" Therefore, it cannot really help in ab sorbing some of the outgoing arcs from V. Does this mean that this assumption is not useful computation ally? Not really! Whenever the algorithm sums over states of variables, we can eliminate those states that contradict with the assumption (finding).5 This could lead to great computational savings, especially in con ditioning algorithms.\nThese are the two ways in which assumptions are uti lized computationally by B-conditioning.\nWhat are good assumptions?\nThe question now is, How do we decide what assump tions to make? Since the quality of assumptions affect both the quality of approximation and the computa tion time, it would be best to allow the user to trade off these parameters. Therefore, B-conditioning allows the user to specify a parameter f E (0, 1) and uses it as a cutoff to decide on which assumptions to make. That is, as f gets smaller, fewer assumptions are made, and a better approximation is obtained, but a longer computation time is expected. As f gets bigger, more assumptions are made, and a worse approximation is obtained, but the computation is faster.\nThe user of B-conditioning would iterate over differ ent values of f, starting from large epsilon to smaller ones, or even automate this iteration through code that takes the increment for changing f as a param eter.\nBefore we specify how f is used, we mention a useful property of B-conditioning: we can judge the quality of its approximations without knowing the chosen value of c6\nPr(x I\\ a):::; Pr(x):::; Pr(x I\\ a)+ 1- 2: Pr(y I\\ a). X=y\nThat is, B-conditioning provides an upper and a lower bound on the exact probability. If these bounds are not satisfactory, the user would then choose a smaller f and re-apply B-conditioning.\nFrom f to assumptions\nThe parameter f is used to abstract the probabilistic causal network into a propositional database Ll. In\n5ln dynamic conditioning, this is implemented by sim ply modifying the code for summing over instantiations of local cutsets so that it ignores instantiations that contra dict with the assumptions.\n6Note that 1- \"Ex=y Pr(y 1\\ a) is Pr(...,a) which is no less than Pr(x 1\\ ..,a) .\nFigure 2: A causal network. Loop cutset variables are denoted by bold circles.\n106 Darwiche\nparticular, for each conditional probability Pr( c I d) = p, we add to .6. the propositional formula ..,( c 1\\ d) iff p \ufffd f. We then make a an assumption iff dU{ xf\\-,a} is unsatisfiable. Intuitively, when x 1\\ -,a is inconsistent with the logical abstraction of a causal network, we interpret this as meaning that the probability of x 1\\ -,a is very small (relative to the choice of f) . Note that whether .6. U { x 1\\ -,a} is unsatisfiable depends mainly on .6. which depends on both the causal network and the chosen f.\nAlternatively, we can abstract the probabilistic net work into a kappa network as suggested in [2]. We can then use a kappa algorithm to test whether x:(xf\\-,a) > 0. This leads to similar results since the kappa calculus is isomorphic to propositional logic in the case where all we care about is whether the kappa ranking is equal to zero.7 As we shall see later, our implementation of B-conditioning utilizes this transformation.\nComplexity issues\nIf the satisfiability tester or kappa algorithm takes no time (gives almost immediate response), then B conditioning is a good idea. But if they take more considerable time, then more issues need to be con sidered. But in general, we expect that the time for running satisfiability tests or kappa algorithms would be low compared to applying the exact inference al gorithm. The evidence for this stems from (a) the advances that have been made on satisfiability testers recently and (b) the results on kappa algorithms as reported in [4], where a linear (but incomplete) algo rithm for prediction is presented. We have used this algorithm, called k-predict, in our implementation of B-conditioning and the results were very satisfying [3]. A sample of these results are reported in the following section.8\nPrediction?\nB-conditioning, as described here, is a method for pre dictive inference since we assumed no evidence (ex cept possibly on root nodes). To handle non-predictive inference, the algorithm can be used to approximate Pr( x 1\\ e) and Pr( e) and then use these results to ap proximate Pr(x I e). But this may lead to low quality approximations. Other extensions of B-conditioning to non-predictive inference is the subject of current research.\nThe choice of epsilon\nTable 1 shows a number of experiments that illustrate how the value of epsilon affects the quality of approx imation and time of computation. 9 The experiments\n7The mapping is \ufffd\ufffd:(x) > 0 iff\ufffd U {x} is unsatisfiable, where the kappa ranking and the database \ufffd are obtained as given above.\n8The combination of k-predict with cutset/dynamic conditioning has been called c-bounded conditioning in [1].\n9These experiments are not meant to evaluate the per-\nconcern an action network (temporal network) with 60 nodes in the domain of non-combatant evacuation [3]. Each scenario corresponds to computing the probabil ity of successful arrival of civilians to a safe heaven given some actions (evidence) - this is a prediction task since actions are always root nodes.\nA number of observations are in order about these ex periments. First, a smaller epsilon may improve the quality of approximation without incurring a big com putational cost. Consider the change from f = .2 to f = .1 in the first set of experiments. Here, the time of computation (in seconds) did not change, but the lower bound on the probability of unsuccessful arrival went up from .81 to .95. Note, however, that the change from f = .1 to f = .02 more than doubled the compu tation time, but only improved the bound with .04.\nThe quality of an approximation, although low, may suffice for a particular application. For example, if the probability that a plan will fail to achieve its goal is greater than .4, then one might really not care how much greater is the probability of failure [3].\nThe bigger the epsilon, the more the assumptions, and the lower the quality of approximations. Note, how ever, that some of these assumptions may not be sig nificant computationally, that is, they do not cut any loops. Therefore, although they may degrade the qual ity of the approximation, they may not buy us com putational time. The first two experiments illustrate this since the three additional assumptions going from f = . 1 to f = .2 did not reduce computational time.\nCONCLUSION\nWe introduced a refinement of cutset conditioning, called dynamic conditioning, which is based on the notions of relevant and local cutsets. Relevant cutsets seem to be the critical element in dictating the com putational performance of dynamic conditioning since they identify which members of a loop cutset affect the value of polytree messages. The tighter relevant cut sets are, the better the performance of dynamic con ditioning. We did not show, however, how one can compute the tightest relevant cutsets in this paper.\nWe also introduced a method for approximate infer ence, called B-conditioning, which requires an exact inference method, together with either a satisfiability tester or a kappa algorithm. B-conditioning allows the user to trade-off the quality of a approximation with\nformance of B-conditioning, which is outside the scope of this paper. We have also eliminated experimental re sults that were reported in a previous version of this paper on comparing the performance of implementations of dy namic conditioning and the Jensen algorithm. Such results are hard to interpret given the vagueness in what consti tutes preprocessing. In this paper, we refrain from making claims about relative computational performance and focus on stressing our contribution as a step further in making conditioning methods more competitive practically.\nConditioning Algorithms for Exact and Approximate Inference in Causal Networks 107\ncomputational time and seems to be a practical tool as long as the satisfiability tester or the kappa algorithm has the right computational characteristics.\nThe literature contains other proposals for improv ing the computational behavior of conditioning meth ods. For example, the method of bounded condition ing ranks the conditioning cases according to their probabilities and applies the polytree algorithm to the more likely cases first [5], which is closely related to B-conditioning. This leads to a flexible inference algo rithm that allows for varying amounts of incomplete ness under bounded resources. Another algorithm for enhancing the performance of cutset conditioning is described in [9], which also appeals to the intuition of local conditioning but seems to operationalize it in a completely different manner. The concept of knots has been suggested in [7] to partition a multiply connected network into parts containing local cutsets. This is very related to relevant cutsets, because when a mes sage is passed between two knots, only cutset variables in the originating knot will be relevant to the message.\nAcknowledgment\nI would like to thank Paul Dagum, Eric Horvitz, Moi ses Goldszmidt, Mark Peot, and Sampath Srinivas for various helpful discussions on the ideas in this pa per. In particular, Moises Goldszmidt's work on the k-predict algorithm was a major source of insight for conceiving B-conditioning. This work has been sup ported in part by ARPA contract F30602-91-C-0031.\nReferences\n[1] Adnan Darwiche. \u20ac-bounded conditioning: A method for the approximate updating of causal networks. Technical report, Rockwell Science Cen ter, Palo Alto, 1994.\n[2] Adnan Darwiche and Moises Goldszmidt. Action networks: A framework for reasoning about actions and change under uncertainty. In Proceedings of\nthe Tenth Conference on Uncertainty in Artificial Intelligence (UAI), pages 136-144, 1994.\n[3] M. Goldszmidt and A. Darwiche. Plan simulation using Bayesian networks. In 11th IEEE Conference on Artificial Intelligence Applications, pages 155- 161, 1995.\n[4] Moises Goldszmidt. Fast belief update using order of-magnitude probabilities. In Proceedings of the 11th Conference on Uncertainty in Artificial Intel ligence (UAI}, 1995.\n[5] Eric J. Horvitz, Gregory F. Cooper, and H. Jacques Suerdmont. Bounded conditioning: Flexible infer ence for decisions under scarce resources. Techni cal Report KSL-89-42, Knowledge Systems Labo ratory, Stanford University, 1989.\n[6] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, Inc., San Mateo, California, 1988.\n[7] Mark A. Peot and Ross D. Shachter. Fusion and propagation with multiple observations in belief networks. Artificial Intelligence, 48(3):299-318, 1991.\n[8] H. Jacques Suermondt and Gregory F. Cooper. Probabilistic inference in multiply connected net works using loop cutsets. International Journal of Approximate Reasoning, 4:283-306, 1990.\n[9] F. J. Dtez Vegas. Loc.al conditioning in bayesian networks. Technical Report R-181, Cognitive Sys tems Laboratory, UCLA, Los Angeles, CA 90024, 1992."}], "references": [{"title": "\u20ac-bounded conditioning: A method for the approximate updating of causal networks", "author": ["Adnan Darwiche"], "venue": "Technical report,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Action networks: A framework for reasoning about actions and change under uncertainty", "author": ["Adnan Darwiche", "Moises Goldszmidt"], "venue": "In Proceedings of  the Tenth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Plan simulation using Bayesian networks", "author": ["M. Goldszmidt", "A. Darwiche"], "venue": "IEEE Conference on Artificial Intelligence Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Fast belief update using order\u00ad of-magnitude probabilities", "author": ["Moises Goldszmidt"], "venue": "In Proceedings of the 11th Conference on Uncertainty in Artificial Intel\u00ad ligence (UAI},", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Bounded conditioning: Flexible infer\u00ad ence for decisions under scarce resources", "author": ["Eric J. Horvitz", "Gregory F. Cooper", "H. Jacques Suerdmont"], "venue": "Techni\u00ad cal Report KSL-89-42,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "Fusion and propagation with multiple observations in belief networks", "author": ["Mark A. Peot", "Ross D. Shachter"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Probabilistic inference in multiply connected net\u00ad works using loop cutsets", "author": ["H. Jacques Suermondt", "Gregory F. Cooper"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Loc.al conditioning in bayesian networks. Technical Report R-181", "author": ["F.J. Dtez Vegas"], "venue": "Cognitive Sys\u00ad tems Laboratory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}], "referenceMentions": [{"referenceID": 5, "context": "Cutset conditioning is one of the earliest algorithms for evaluating multiply connected networks [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "instantiation of a loop cutset [6, 7].", "startOffset": 31, "endOffset": 37}, {"referenceID": 6, "context": "instantiation of a loop cutset [6, 7].", "startOffset": 31, "endOffset": 37}, {"referenceID": 7, "context": "Relevant and local cutsets are subsets of a loop cutset [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "We adopt the same notation used in [6] for describ\u00ad", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Following [7], we defined BEL( x) as Pr( x 1\\ e) instead of Pr(x I e) to avoid computing the probability of evidence e when applying cutset conditioning.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "ble without destroying the connectivity of the network (arc absorption) [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Again, these equations paral\u00ad lel the ones defining the polytree algorithm [6, 7].", "startOffset": 75, "endOffset": 81}, {"referenceID": 6, "context": "Again, these equations paral\u00ad lel the ones defining the polytree algorithm [6, 7].", "startOffset": 75, "endOffset": 81}, {"referenceID": 6, "context": "Therefore, the control flow in the dynamic condition\u00ad ing algorithm is similar to the first pass in the revised polytree algorithm [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "Alternatively, we can abstract the probabilistic net\u00ad work into a kappa network as suggested in [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "The evidence for this stems from (a) the advances that have been made on satisfiability testers recently and (b) the results on kappa algorithms as reported in [4], where a linear (but incomplete) algo\u00ad rithm for prediction is presented.", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "We have used this algorithm, called k-predict, in our implementation of B-conditioning and the results were very satisfying [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 0, "context": "8The combination of k-predict with cutset/dynamic conditioning has been called c-bounded conditioning in [1].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "9These experiments are not meant to evaluate the perconcern an action network (temporal network) with 60 nodes in the domain of non-combatant evacuation [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "4, then one might really not care how much greater is the probability of failure [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "For example, the method of bounded condition\u00ad ing ranks the conditioning cases according to their probabilities and applies the polytree algorithm to the more likely cases first [5], which is closely related to B-conditioning.", "startOffset": 178, "endOffset": 181}, {"referenceID": 8, "context": "Another algorithm for enhancing the performance of cutset conditioning is described in [9], which also appeals to the intuition of local conditioning but seems to operationalize it in a completely different manner.", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "The concept of knots has been suggested in [7] to partition a multiply connected network into parts containing local cutsets.", "startOffset": 43, "endOffset": 46}], "year": 2011, "abstractText": "We present two algorithms for exact and ap\u00ad proximate inference in causal networks. The first algorithm, dynamic conditioning, is a re\u00ad finement of cutset conditioning that has lin\u00ad ear complexity on some networks for which cutset conditioning is exponential. The sec\u00ad ond algorithm, B-conditioning, is an algo\u00ad rithm for approximate inference that allows one to trade-off the quality of approxima\u00ad tions with the computation time. We also present some experimental results illustrating the properties of the proposed algorithms.", "creator": "pdftk 1.41 - www.pdftk.com"}}}