{"id": "1302.6807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Backward Simulation in Bayesian Networks", "abstract": "Backward simulation is an approximate inference technique for Bayesian belief networks. It differs from existing simulation methods in that it starts simulation from the known evidence and works backward (i.e., contrary to the direction of the arcs). The technique's focus on the evidence leads to improved convergence in situations where the posterior beliefs are dominated by the evidence rather than by the prior probabilities. Since this class of situations is large, the technique may make practical the application of approximate inference in Bayesian belief networks to many real-world problems. This is the first instance of Bayesian belief networks in a Bayesian world.\n\n\nThe Bayesian inference technique is applied in two different ways: a computational technique, or a systematic method. First, it creates the inference model for probability distributions on a given set. In this model, we introduce a model which does not consider a single set. Second, it assigns probability distributions on the set. Third, it assumes that all the data in a given set are random, with no one at all in it. If all the data in a given set have a probability distribution that is positive, then the inference model assumes the probability distribution of all the data in a given set. (See Figure 2 for a brief discussion.) A probability distribution (or even a general approximation) for a given set is not a linear function that is an approximate inference technique.\nThe second method is based on an arbitrary model, with a representation of the probability distributions. The inference model assigns a certain probability distribution to the subset of the probability distributions that can be obtained and therefore the probability distribution of all the data in a given set. For example, if we use a given set, the probability distribution of all the data in a given set is always zero.\nIn any case, the method is an approximation to a particular set. Thus, the inference model may be based on the representation of each set (or even a general approximation) for the subset of the probability distributions that can be obtained and therefore the probability distribution of all the data in a given set. If a certain probability distribution for a given set is zero, then we define the probability distribution of all the data in a given set. If the distribution of all the data in a given set is zero, then the inference model assumes that all the data in a given set is zero. The inference model assigns a certain probability distribution to the subset of the probability distributions that can be obtained and therefore the probability distribution of all the data in a given set. This is the first instance of Bayesian", "histories": [["v1", "Wed, 27 Feb 2013 14:16:02 GMT  (779kb)", "http://arxiv.org/abs/1302.6807v1", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"]], "COMMENTS": "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["robert fung", "brendan del favero"], "accepted": false, "id": "1302.6807"}, "pdf": {"name": "1302.6807.pdf", "metadata": {"source": "CRF", "title": "Backward Simulation in Bayesian Networks", "authors": ["Robert Fung", "Brendan Del Favero"], "emails": [], "sections": null, "references": [{"title": "A randomized approximation algorithm for probabilistic inference on", "author": ["R.M. Chavez", "G.F. Cooper"], "venue": "Bayesian belief networks. Networks,", "citeRegEx": "Chavez and Cooper,? \\Q1990\\E", "shortCiteRegEx": "Chavez and Cooper", "year": 1990}, {"title": "Approximating probabilistic inference in Bayesian belief networks is NP\u00ad", "author": ["P. Dagum", "R.M. Luby"], "venue": "hard. Artificial Intelligence,", "citeRegEx": "Dagum and Luby,? \\Q1993\\E", "shortCiteRegEx": "Dagum and Luby", "year": 1993}, {"title": "Propagating uncertainty in Bayesian networks by probabilistic logic sampling", "author": ["M. Henrion"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "Henrion,? \\Q1986\\E", "shortCiteRegEx": "Henrion", "year": 1986}, {"title": "Simulation and the Monte Carlo Method", "author": ["R.Y. Rubinstein"], "venue": null, "citeRegEx": "Rubinstein,? \\Q1981\\E", "shortCiteRegEx": "Rubinstein", "year": 1981}, {"title": "Symbolic probabilistic inference: A probabilistic perspective", "author": ["B. D'Ambrosio", "B. Del Favero"], "venue": "Artificial Intelligence", "citeRegEx": "Shachter et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Shachter et al\\.", "year": 1990}, {"title": "Simulation approaches to probabilistic inference for general probabilistic inference on belief networks", "author": ["M. Peat"], "venue": null, "citeRegEx": "Peat,? \\Q1989\\E", "shortCiteRegEx": "Peat", "year": 1989}, {"title": "Evidential reasoning using likelihood weighting. Personal communication with authors", "author": ["R.D. Shachter", "M. Peat"], "venue": null, "citeRegEx": "Shachter and Peat,? \\Q1993\\E", "shortCiteRegEx": "Shachter and Peat", "year": 1993}, {"title": "Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base-l: The probabilistic model and inference algorithms", "author": ["M.A. Shwe", "B. Middleton", "D.E. Beckerman", "M. Henrion", "E.J. Horvitz", "H.P. Lehmann", "G.F. Cooper"], "venue": null, "citeRegEx": "Shwe et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Shwe et al\\.", "year": 1991}], "referenceMentions": [{"referenceID": 2, "context": "There are two basic classes of simulation methods: forward-simulation methods (Fung, 1989; Henrion, 1986; Shachter, 1989) and stochastic\ufffdsimulation methods (Chavez, 1990; Pearl, 1987).", "startOffset": 78, "endOffset": 121}, {"referenceID": 3, "context": "3 IMPORTANCE SAMPLING Importance sampling (Rubinstein, 1981) is a well-known technique for improving convergence in Monte Carlo simulation.", "startOffset": 42, "endOffset": 60}], "year": 2011, "abstractText": "Backward simulation is an approximate inference technique for Bayesian belief networks. It differs from existing simulation methods in that it starts simulation from the known evidence and works backward (i.e., contrary to the direction of the arcs). The technique's focus on the evidence leads to improved convergence in situations where the posterior beliefs are dominated by the evidence rather than by the prior probabilities. Since this class of situations is large, the technique may make practical the application of approximate inference in Bayesian belief networks to many real\ufffdworld problems.", "creator": "pdftk 1.41 - www.pdftk.com"}}}