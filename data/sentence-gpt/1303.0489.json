{"id": "1303.0489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2013", "title": "A Semantic approach for effective document clustering using WordNet", "abstract": "Now a days, the text document is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. This is the most important element of the search and use of word selection, information security, word recognition, data compression, keyword search and retrieval.\n\n\nThe following example uses word selection:\nIf you search for a text document you will find several words in the title. You will find the titles, keywords, or the type of word for which you wish to read. The search results are displayed in the window of your computer and you will find the search results of the search results in the window of your computer and you will find the information in the title. You can also search for information about which words the search results are for which you wish to read. This way you can search the relevant words and information and you can use the search results to identify keywords for which you wish to read. This way you can identify keywords for which you wish to read.\nTo generate these results, you need to add the term search to the word.\nTo generate these results, you must add the term search to the word. If you are using words to name or type their word in a word, use the word search to name or type their word in a word. For example:\nNote that you cannot find the keyword search to the word. If you are using words to name or type their word in a word, use the word search to name or type their word in a word.\nExample of Word-processing\nThere is a list of keywords that we will use in this post. The following example uses word selection to search for a text document you wish to read. If you are using words to name or type their word in a word, use the word search to name or type their word in a word.\nTo generate these results, you need to add the term search to the word. If you are using words to name or type their word in a word.\nTo generate these results, you need to add the term search to the word. If you are using words to name or type their word in a word.\nTo generate these results, you must add the term search to the word. If you are using words to name", "histories": [["v1", "Sun, 3 Mar 2013 12:19:18 GMT  (219kb)", "http://arxiv.org/abs/1303.0489v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["leena h patil", "mohammed atique"], "accepted": false, "id": "1303.0489"}, "pdf": {"name": "1303.0489.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["harshleena23@rediffmail.com).", "atique_shaikh1@rediffmail.com)."], "sections": [{"heading": null, "text": "increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc.\nIndex Terms\u2014 Introduction, Document Preprocessing,\nWordNet, Term Selection approach, Experimental results.\nI. INTRODUCTION\nA substantial portion of the available information is stored in\nText databases, which consist of large collections of documents from various sources, such as news articles,\nresearch papers, books, digital libraries, e-mail messages and web pages. [1]Text documents are growing rapidly due to the increasing amount of information available in electronic and digitized form, such as electronic publications, various kinds\nof electronic documents, e-mail, and the World Wide Web. Nowadays most of the information regarding government, industry, business, and other institutions are stored\nelectronically, in the form of text databases. Most of the text\nProf. Mrs. Leena H. Patil, Computer Science and Engineering, Amravati University/ SGBAU / SGBAU CSE Deptt, Amravati, India, 9373612549, (e-mail: harshleena23@rediffmail.com).\nDr. Mohamed Atique, Computer Science and Engineering, Amravati University/ SGBAU/ SGBAU CSE Deptt., Amravati, India ,09011039313, (e-mail: atique_shaikh1@rediffmail.com).\ndatabases are semi structured data format which they are\nneither completely structured. Document Preprocessing and clustering is very useful tool in today\u2019s world where large amount of documents and information are stored and retrieved electronically. As text data are inherently\nunstructured, some researchers applied different technique for document management. Researchers has presented knowledge discovery in text system, which uses the simplest\ninformation extraction to get interesting information and knowledge from unstructured text collection. [10]To enable the representation process and effective transformation, the\nword frequencies require to be normalized in terms of their relative frequencies which are present in document and over the entire collection. To organize the large document corpus\nmaking later navigating, the document browsing becomes\nmore easy, friendly and efficient. Almost, it is impossible for the human being to read through all the text documents and find out the relative for a specific topic and how to organize\nlarge document. To organize the large amount of data and stored in a structured formats certain data mining techniques are able to use or extract the necessary information from the\nunstructured document collections. [9][10]Because of this,\ntext mining techniques are useful in processing these documents. The goal of text mining is to structure document\ncollections to improve the ability of users to retrieve and apply knowledge implicitly contained within those collections. Text mining proceeds through different phases to\ncomplete the goal: pre-processing, applying WordNet and term selection approach. Attributes and dimension reduction\nare the important parameters in text mining. This improves the performance of term selection methods by reducing\ndimensions so that text mining procedures process data with a reduced number of terms and accuracy get improved. WordNet is the product of a research project in Princeton University which has attempted to model the lexical knowledge of a native speaker of English [12]. In WordNet Nouns, verbs, adjectives, and adverbs are connected to each other into hierarchies by well-defined types of semantic relations. These semantic relations for nouns include Hyponym/Hypernym (is-a), Part Meronym/Part Holonym (part-of), Member Meronym/Member Holonym (member-of), Substance Meronym/Substance Holonym (substance-of) and so on. For example, a car is a wheeled vehicle (is-a), and a cell is part of organism (part-of). Hyponym/hypernym (is-a) is the most common relations. [13]Language semantics are mostly captured by nouns and noun phrases, in this the similarity measures based on nouns and is-a relations in WordNet is discussed. To overcome such problem we design an algorithm which will deal and work on large scale data sets with high dimensions.\nIn this approach document are preprocessed, dimension get reduced, WordNet is applied for arranging words as noun, verb, adverb, and adjectives, for further having better clustering, term selection methods are used. Documents get preprocessed by several steps: Firstly stopwords are removed, Secondly stemming is perform by using porter\u2019s stemmer algorithm, Thirdly WordNet senses is applied, Global unique words and frequent word set gets generated by using feature selection approaches tf-idf, tf-df, tf2. Finally the attribute reduction methods are used based on their entropy and significant measures by using forward greedy attribute reduction algorithm. Experimental evaluations are performed on the above approaches on Reuters transcription subset, wheat, grain, ship, money, trade datasets, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc. From these evaluations the best and better approach for document clustering is establish"}, {"heading": "II. DOCUMENT PREPROCESSING", "text": "To Organize and browse thousands of documents smoothly, document preprocessing becomes an most important step, which affects on the result. It describes the required transformation processing of documents to obtain the designated representation of documents. [7]Thousands of words are present in a document set, the aim of this is to reduce dimensionality for having the better accuracy for classification. Document preprocessing are divided into following stages: 1. Each sentences gets divided into terms\n2. Stop words are removed: Stopword List is used that contains the words to be excluded. The Stopword list is applied to remove terms that have a special meaning but do not discriminate for topics\n3. Word Stemming: Developed stemming algorithm such as porters is used to reduce a word to its stem or root form.\n4. WordNet Senses Disambiguation is applied as an English Database.\n5. Global Unique words and frequent word set gets generated.\nA. Stop-Word Removing\nStop-words are words that from non-linguistic view do not carry information. Stop-words remove the non-information behavior words from the text documents and reduce noisy data. [9]One major property is that there are extremely common words present and the explanation of the sentences still held after these stop-words are removed. Most existent search engines do not record stop-words in order to reduce the space and speed up the searches. To organize large corpus, removing the stop words affords the similar advantages. Firstly it could save huge amount of space. Secondly it helps to deduce the noises and keep the core words, and it will make later processing more effective and efficient.\nB. Stemming\nThis process is used for transforming the words into their stem. In many languages the various syntactic form of words are used and explain with the same basic concepts. The most important technique called stemming is used for the reduction of words into their root. Many words from the\nEnglish language can be reduced to their base form or stem e.g. agreed, agreeing, disagree, agreement and disagreement belong to agree. Porter Stemmer is a widely applied method to stem documents. [11]It is compact, simple and relatively accurate. It does not require creating a suffix list before applied.\nC. Wordnet\nUsing Lexical database the WordNet approach measures the relatedness of terms from the words. Based on these, we can compute scores of semantic relatedness of terms found from WordNet. In general as a dictionary, WordNet covers some specific terms from every subject related to their terms. Wordnet as a lexical database map all the stemmed words from the standard documents into their specifies lexical categories. However, in different fields, the semantic relation of terms may be different as proposed in [13]. In this approach the WordNet 2.1 is used which contains 41 lexical categories as nouns and verbs. For example, the word \u201cdog\u201d and \u201ccat\u201d both belong to the same category \u201cnoun.animal\u201d. Some words also has multiple categories like word \u201cWashington\u201d has 3 lexical categories (noun.location, noun.group, noun.person) because it can be the name of the American president, the city place, or a group in the concept of capital."}, {"heading": "III. FEATURE SELECTION", "text": "Feature selection is the most common technique easy to\nuse in the problem of text categorization, as mostly available for the feature selection process. However many unsupervised method are also used as a feature selection.\n[10]The terms selected after applying the wordnet, the metric weights all higher than the pre-specified threshold are selected as a key term. In our approach three different feature\nselection methods are used: tf-idf, tf-df, and tf2 to select\nrepresentative terms of each document which are defined as follow:. (1) tf-idf (term frequent - inverse document frequency): It is denoted as tfi-dfij and used for the measure of the importance of term tj within document di.\ntfidfij = X log\nWhere is the frequency of term in the document , and\nthe denominator is the total frequencies of all terms in\ndocument . |D| is the total number of documents in the\ndocument set D, and is the number of\ndocuments containing term\n(2) tf-df (term frequency x document frequency): It is\nrepresented by and evaluated for the value calculated\nby dividing the term frequency (TF) by the document\nfrequency (DF), where TF is the number of times a term\nappears in a document divided by the total frequencies of\nall terms in , and DF is used to determine the number of\ndocuments containing term divided by the total number of\ndocuments in the document set D.\n= , where TF = and\nDF=\n(3) tf 2 : It is the multiplication of and\nand we denote it as\n= *\nTF-IDF, a vector space based representation is the\ncommon technique used for text processing. [4][5]In this\nrepresentation, the term frequency for each word is normalized by the inverse document frequency, or IDF. It\nnormalizes and reduces the weight of each any term which occurs frequently in the collection. This reduces the importance of common terms in the collection, ensuring that the matching of documents be more influenced by that of\nmore discriminative words which have relatively low frequencies. After these weights of each term in each document have been calculated, those which have weights all\nhigher than pre-specified thresholds are retained. Subsequently, these retained terms form a set of key terms for the document set D. This includes only meaningful key\nterms, which do not appear in a well-defined stop word list,\nand satisfy the Pre-defined minimum tf-idf threshold. threshold \u221d, the minimum tf-df threshold and the minimum tf2 threshold Based on the above approach, the representation of a document can be derived by algorithm. The algorithm defined is as follow: Algorithm 1. Document pre-processing algorithm Input 1. A document set D= {d1, d2, d3\u2026.di,..dn}\n2. A well defines stop word list. 3 The minimum tf-idf threshold 4. The minimum tf-df threshold 5. The minimum tf2 threshold\nOutput: The key terms of D, KD Method: Step 1: Extract the term set TD= {t1, t2, t3\u2026tj,..ts} Step 2. Remove all stop words from TD. Step 3. Apply word stemming for TD using porter stemmer algorithm Step 4. WordNet Senses Disambiguation is applied as an\nEnglish Database. Step 5. Find the Global unique words and obtain frequent key words from TD. Step 6. Evaluate and retain all its weights tf-idf, tf-df, tf2 Step 7. Obtain the key term set KD"}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "In this section, we experimentally evaluate the performance of the proposed algorithm. All the experiments have been performed on a Intel I5 Processor, Windows 7 OS machine with 8 GB memory.\nA. Datasets\nOn three to four different datasets the experiments are\nperformed these datasets are widely adopted as standard\nbenchmarks for the text categorization task. To find key terms, stop words were removed and stemming was\nperformed. Documents then were represented as TF (Term Frequency) vectors, and unimportant terms were useless. This process implies a significant dimensionality reduction\nwithout loss of clustering performance. The statistics of these datasets, after the document pre-processing is summarized in\ntable 1.\nTable:1 Statistics of the datasets\nData Sets Number of\nDocuments\nNumber of Natural classes Class size Average length of Documents\nReuters-\n21578 21578 04 986 12\nClassic 30 5697 03 2568 35\n20 News Group\n(atheism) 1000 02 1206 15\n20 News Group\n(Computer\nGraphics)\n1000 03 1204 15\n20 News Group\n( Hardware) 1000 02 945 10\nReuters\nTranscription\nSubset (Wheat)\n20 02 185 19\nReuters\nTranscription\nSubset (Trade)\n21 03 195 21\nReuters\nTranscription\nSubset (Ship)\n20 02 185 19\nReuters\nTranscription\nSubset (Money)\n20 02 185 19\nReuters\nTranscription\nSubset (Grain)\n20 02 185 19\nReuters\nTranscription\nSubset(Corn)\n20 02 185 19\nThey are heterogeneous in terms of document size, cluster size, number of classes, and document distribution. The smallest document set contains 20 documents, and the largest one contains 21578 documents. Table 1 summarizes the statistics of these datasets. The detailed information of these datasets is described as follows:\nReuters Transcription subset: This data was created by selecting 20 files each from the 10 largest classes 20 News Group: This dataset is a collection of 18,828 newsgroup documents, partitioned almost evenly across 20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. This dataset contain 6 classes with their subclasses. Classic 30: This document set is a combination of the three classes CACM, CISI, and MED abstracts. Classic dataset includes 3203 CACM documents, 1460 CISI documents from information retrieval papers, and 1033 MEDLINE documents from medical journals [1]. Reuters 21578: For each text categorization, a specification of text is given of what categories that text belongs to. For the Reuters-21578 collection the documents are Reuter\u2019s newswire stories, and the categories are five different sets of content related categories. For each document, a human indexer decided which categories from which sets that document belonged to. Reuters-21578 text categorization test\ncollection Distribution 1.0. Re0 includes 1504 documents with 13 classes [2].\nB. The Impact of Feature Selection\nFor Document Clustering, Feature selection is essential to make clustering task efficient and more accurate. The most important goal of feature selection is to extract topic related terms, which could present the content of each document. [11]For selecting the most representative features, the formulas are used to obtain the three weights and select these terms, which their weights are all higher than the pre- defined thresholds. The details of the tested dataset with their specified minimum threshold value are shown below in the table.\nTable:2 - Datasets for minimum threshold = 0.028 using WordNet\nTable:3 - Datasets for minimum threshold, = 0.01 using WordNet\nData Sets Number\nof Terms\nNumber of\nKey terms\nTf-idf\nPercentage\nof term\nremoved\nReuters -21578 462 447 3.24\nClassic 30 309807 257758 16.80\n20 News Group (athesim) 185684 42076 77.34\n20 News Group (Computer Graphics) 164667 50596 69.27\n20 News Group (Hardware) 138843 51851 62.65\nReuters Transcription Subset (Wheat) 1940 965 50.25\nReuters Transcription Subset (Trade) 3394 759 77.63\nReuters Transcription Subset (Ship) 1397 939 32.78\nReuters Transcription Subset (Money) 2755 706 74.37\nReuters Transcription Subset (Grain) 2102 1009 51.99\nReuters Transcription Subset(Corn) 2331 1084 53.34\nData Sets\nNumber\nof\nTerms\nNumber\nof Key\nterms\nTf-Df\nPercentage\nof term\nremoved\nReuters -21578 462 70 84.84\nClassic 30 309807 8974 97.10\n20 News Group (athesim)\n185684 3433 98.15\n20 News Group (Computer Graphics)\n164667 5662 96.56\n20 News Group (Hardware) 138843 5008 96.39\nReuters Transcription Subset (Wheat)\n1940 421 78.29\nReuters Transcription Subset (Trade)\n3394 417 87.71\nReuters Transcription Subset (Ship)\n1397 167 88.04\nReuters Transcription Subset (Money)\n2755 272 90.12\nReuters Transcription Subset (Grain)\n2102 392 81.35\nReuters Transcription Subset(Corn)\n2331 555 76.19\nTable: 4 - Datasets for minimum threshold, \u03c4 = 0.005 using WordNet\nCONCLUSIONS In this paper the key terms are extracted from the documents by using effective feature selection method tf-idf, tfdf, and tf2 based on its predefined threshold value and using background knowledge WordNet. In the total processes, we begin with the process of document pre-processing, removing the stop words, stemming by using porter stemmer algorithm, Wordnet lexical; database extracting global unique word, finding frequent word sets and further key terms are extracted from all documents by using feature selection methods. Based on the predefined threshold value and the key terms of tf-idf. tfdf, tf2 technique, the percentage of terms removed are evaluated. From the results it is observed that even if the percentage of terms removed is higher for tf2, tf-df than tf-idf but there is the higher possibility of data loss. The advantage of using tf-idf is that the dimensionality of the document gets reduced without data loss. Hence from the above process an effective feature selection method tf-idf is the better one for further mining process. Our future work focus on this tf-idf effective feature selection method using WordNet which can be used for attribute reduction by using rough set approach and k-means."}, {"heading": "13. Lingling Meng and Junzhong Gu, \u201cA New Method for Calculating Word Sense Similarity in WordNet\u201d,", "text": "International Journal of Signal Processing, Image Processing and Pattern Recognition Vol. 5, No. 3, September, 2012.\nData Sets Number\nof Terms\nNumber\nof Key\nterms\nTf2\nPercentag\ne of term\nremoved\nReuters - 21578 462 55 88.10\nClassic 30 309807 8974 97.10\n20 News Group (athesim) 185684 1004 99.46\n20 News Group (Computer Graphics) 164667 1005 99.38\n20 News Group (Hardware) 138843 1000 99.27\nReuters Transcription Subset (Wheat) 1940 310 84.02\nReuters Transcription Subset (Trade) 3394 215 93.66\nReuters Transcription Subset (Ship) 1397 167 88.04\nReuters Transcription Subset (Money) 2755 166 93.97\nReuters Transcription Subset (Grain) 2102 290 86.20\nReuters Transcription Subset(Corn) 2331 437 81.25"}], "references": [{"title": "Reuters-21578 text categorization test collection,\u201d1999.[Online", "author": ["D.D. Lewis"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "Proc. of the 6th ACM SIGKDD int'l conf. on Knowledge Discovery and Data Mining ,KDD,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Hierarchical document clustering using frequent itemsets", "author": ["B. Fung", "K. Wang", "M. Ester"], "venue": "Proc. of SIAM Int'l Conf. on Data Mining, SDM'", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "A new unsupervised method for document clustering by using WordNet lexical and conceptual relations", "author": ["D.R. Recupero"], "venue": "Information Retrieval", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Frequent term-based text clustering", "author": ["F. Beil", "M. Ester", "X. Xu"], "venue": "Proc. of Int'l Conf. on knowledge Discovery and Data Mining, KDD'", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "An integration of fuzzy association rules and WordNet for document clustering", "author": ["C.L.Chen", "F.S.C. Tseng", "T. Liang"], "venue": "Proc. of the 13th Paci.c-Asia Conference on Knowledge Discovery and Data Mining, pp", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "An integration of WordNet and fuzzy association rule mining for multi-label document clustering, Data", "author": ["Chun-Ling Chen a", "Frank S.C. Tseng b", "Tyne Liang"], "venue": "Knowledge Engineering", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "[9][10]Because of this, text mining techniques are useful in processing these documents.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7]Thousands of words are present in a document set, the aim of this is to reduce dimensionality for having the better accuracy for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9]One major property is that there are extremely common words present and the explanation of the sentences still held after these stop-words are removed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4][5]In this representation, the term frequency for each word is normalized by the inverse document frequency, or IDF.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4][5]In this representation, the term frequency for each word is normalized by the inverse document frequency, or IDF.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Re0 includes 1504 documents with 13 classes [2].", "startOffset": 44, "endOffset": 47}], "year": 2013, "abstractText": "Abstract\u2014 Now a days, the text document is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in data mining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc.", "creator": "Microsoft\u00ae Office Word 2007"}}}