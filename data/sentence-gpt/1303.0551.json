{"id": "1303.0551", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2013", "title": "Sparse PCA through Low-rank Approximations", "abstract": "We introduce a novel algorithm that computes the $k$-sparse principal component of a positive semidefinite matrix $A$. Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of $A$. We obtain provable approximation guarantees that depend on the spectral profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation. For example, if the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-time approximation algorithm for any desired accuracy. We implement our algorithm and test it on multiple artificial and real data sets. Due to a feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sun, 3 Mar 2013 19:08:55 GMT  (1269kb,D)", "https://arxiv.org/abs/1303.0551v1", "23 pages, 6 figures, 4 tables, submitted for publication"], ["v2", "Thu, 8 May 2014 00:30:12 GMT  (1262kb,D)", "http://arxiv.org/abs/1303.0551v2", "Long version of the ICML 2013 paper:this http URL"]], "COMMENTS": "23 pages, 6 figures, 4 tables, submitted for publication", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["dimitris s papailiopoulos", "alexandros g dimakis", "stavros korokythakis"], "accepted": true, "id": "1303.0551"}, "pdf": {"name": "1303.0551.pdf", "metadata": {"source": "CRF", "title": "Sparse PCA through Low-rank Approximations", "authors": ["Dimitris S. Papailiopoulos", "Alexandros G. Dimakis", "Stavros Korokythakis"], "emails": ["dimitris@utexas.edu", "dimakis@austin.utexas.edu", "stavros@stochastictechnologies.com"], "sections": [{"heading": null, "text": "A key algorithmic component of our scheme is a combinatorial feature elimination step that is provably safe and in practice significantly reduces the running complexity of our algorithm. We implement our algorithm and test it on multiple artificial and real data sets. Due to the feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets."}, {"heading": "1 Introduction", "text": "Principal component analysis (PCA) reduces the dimensionality of a data set by projecting it onto principal subspaces spanned by the leading eigenvectors of the sample covariance matrix. The statistical significance of PCA partially lies in the fact that the principal components capture the largest possible data variance. The first principal component (i.e., the first eigenvector) of an n\u00d7 n matrix A is the solution to\narg max \u2016x\u20162=1\nxTAx\nwhere A = SST and S is the n\u00d7m data-set matrix consisting of m data-points, or entries, each evaluated on n features, and \u2016x\u20162 is the `2-norm of x. PCA can be efficiently computed using the singular value decomposition (SVD). The statistical properties and computational tractability of PCA renders it one of the most used tools in data analysis and clustering applications.\nA drawback of PCA is that the generated vectors typically have very few zero entries, i.e., they are not sparse. Sparsity is desirable when we aim for interpretability in the analysis of principal components. An example where sparsity implies interpretability is document analysis, where principal components can be used to cluster documents and detect trends. When the principal components are sparse, they can be easily mapped to topics (e.g., newspaper article classification into politics, sports, etc.) using the few keywords in their support (Gawalt et al., 2010; Zhang and El Ghaoui, 2011). For that reason it is desirable to find sparse eigenvectors.\nar X\niv :1\n30 3.\n05 51\nv2 [\nst at\n.M L\n] 8\nM ay"}, {"heading": "1.1 Sparse PCA", "text": "Sparsity can be directly enforced in the principal components. The sparse principal component x\u2217 is defined as\nx\u2217 = arg max \u2016x\u20162=1,\u2016x\u20160=k\nxTAx. (1)\nThe `0 cardinality constraint limits the optimization over vectors with k non-zero entries. As expected, sparsity comes at a cost since the optimization in (1) is NP-hard (Moghaddam et al., 2006a) and hence computationally intractable in general."}, {"heading": "1.2 Overview of main results", "text": "We introduce a novel algorithm for sparse PCA that has a provable approximation guarantee. Our algorithm generates a k-sparse, unit length vector xd that gives an objective provably within a 1 \u2212 d factor from the optimal: xTdAxd \u2265 (1\u2212 d)xT\u2217Ax\u2217 with\nd \u2264min { n\nk \u00b7 \u03bbd+1 \u03bb1 , \u03bbd+1\n\u03bb (1) 1\n} , (2)\nwhere \u03bbi is the ith largest eigenvalue of A and \u03bb (1) 1 is the maximum diagonal element of A. For any desired value of the parameter d, our algorithm runs in time O(nd+1 logn+ SVD(A,d)), where SVD(A,d) is the time to compute the d principal eigenvectors of A. Our approximation guarantee is directly related to the spectrum of A: the greater the eigenvalue decay, the better the approximation. Equation (2) contains two bounds: one that uses the largest eigenvalue \u03bb1 and one that uses the largest diagonal element of A, \u03bb (1) 1 . Either bound can be tighter, depending on the structure of the A matrix. We subsequently rely on our approximation result to establish guarantees for considerably general families of matrices."}, {"heading": "1.2.1 Constant-factor approximation", "text": "If we only assume that there is an arbitrary decay in the eigenvalues of A, i.e., there exists a constant d = O(1) such that \u03bb1 > \u03bbd+1, then we can obtain a constant-factor approximation guarantee for the linear sparsity regime. Specifically, we find a constant \u03b40 such that for all sparsity levels k > \u03b40 n we obtain a constant approximation ratio for sparse PCA, partially solving the open problem discussed in (Zhang et al., 2012; d\u2019Aspremont et al., 2012). This result easily follows from our main theorem."}, {"heading": "1.2.2 PTAS under a power-law decay", "text": "When the data matrix spectrum exhibits a power-law decay, we can obtain a much stronger performance guarantee: we can solve sparse PCA for any desired accuracy in time polynomial in n,k (but not in 1 ). This is sometimes called a polynomial-time approximation scheme (PTAS). Further, the power-law decay is not necessary: the spectrum does not have to follow exactly that decay, but only exhibit a substantial spectral drop after a few eigenvalues."}, {"heading": "1.2.3 Algorithmic details", "text": "Our algorithm operates by scanning a low-dimensional subspace of A. It first computes the leading eigenvectors of the covariance input matrix, and then scans this subspace for k sparse vectors that have large explained variance.\nThe constant dimensional search is possible after a hyperspherical transformation of the n dimensional problem space to one of constant d dimension. This framework was introduced by the foundational work of (Karystinos and Liavas, 2010) in the context of solving quadratic form maximization problems over\u00b11 vectors. This framework was consequently used in (Asteris et al., 2011) to develop a constant rank solver that computes\nthe sparse principal component of a constant rank matrix in polynomial time O(nd+1). We use as a subroutine a modified version of the solver of (Asteris et al., 2011), to examine a polynomial number of special vectors that lead to a sparse principal component which admits provable performance. For matrices with nonnegative entries, we are able to tweak the solver and improve computation time by a factor of 2d.\nAlthough the complexity of our algorithm is polynomial in n, the cost to run it on even moderately large sets with n > 1000 becomes intractable even for small values of d = 2, our accuracy parameter. A key algorithmic innovation that we introduce is a provably safe feature elimination step that allows the scalability of our algorithm for data-sets with millions of entries. We introduce a test that discards features that are provably not in the support of the sparse PC, in a similar manner as (Zhang and El Ghaoui, 2011), but using a different combinatorial criterion."}, {"heading": "1.2.4 Experimental Evaluation", "text": "We evaluate and compare our algorithm against state of the art sparse PCA approaches on synthetic and real data sets. Our real data-set is a large Twitter collection of more than 10 million tweets spanning approximately six months. We executed several experiments on various subsets of our data set: collections of tweets during a specific time-window, tweets that contained a specific word, etc. Our implementation executes in less than one second for 50k \u2212 100k documents and in a few minutes for millions of documents, on a personal computer. Our scheme typically comes closer than 90% of the optimal performance, even for d < 3, and empirically outperforms previously proposed sparse PCA algorithms."}, {"heading": "1.3 Related Work", "text": "There has been a substantial volume of prior work on sparse PCA. Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995). Then, a modified PCA technique based on the LASSO (SCoTLASS) was introduced in (Jolliffe et al., 2003). In (Zou et al., 2006), a nonconvex regression-type approximation, penalized a\u0300 la LASSO was used to produce sparse PCs. A nonconvex technique was presented in (Sriperumbudur et al., 2007). In (Moghaddam et al., 2006b), the authors used spectral arguments to motivate a greedy branch-and-bound approach, further explored in (Moghaddam et al., 2007). In (Shen and Huang, 2008), a similar technique to SVD was used employing sparsity penalties on each round of projections. A significant body of work based on semidefinite programming (SDP) approaches was established in (d\u2019Aspremont et al., 2007a; Zhang et al., 2012; d\u2019Aspremont et al., 2008). A variation of the power method was used in (Journe\u0301e et al., 2010). When computing multiple PCs, the issue of deflation arises as discussed in (Mackey, 2009). In (Yuan and Zhang, 2011), the authors introduced a very efficient sparse PCA approximation based on truncating the well-known power method to obtain the exact level of sparsity desired. A fast algorithm based on Rayleigh quotient iteration was developed in (Kuleshov, 2013).\nSeveral guarantees are established under the statistical model of the spiked covariance. In (Amini and Wainwright, 2008), the first theoretical optimality guarantees were established under the spiked covariance for diagonal thresholding and the SDP relaxation of (d\u2019Aspremont et al., 2007a). In (Yuan and Zhang, 2011), the authors provide peformance guarantees for the truncated power method under specific assumptions of data model, similar to the restricted isometry property. In (d\u2019Aspremont et al., 2012) the authors provide detection guarantees under the single spike covariance model. Then, in (Cai et al., 2012) and (Cai et al., 2013) the authors provide guarantees under the assumption of multiple spikes in the covariance.\nThere has also been a significant effort in understanding the hardness of the problem. Sparse PCA is NPhard in the general case as it can be recast to the problem subset selection and the problem of finding the largest clique in a graph. It is also suspected that it is computationally challenging to recover the sparse spikes of a spiked covariance model, under optimal sample complexity as was shown in (Berthet and Rigollet, 2013b), (Berthet and Rigollet, 2013a), and (Berthet and Rigollet, 2012). There, the problem of recovering the correct spike under the minimum possible sample complexity is connected to the problem of recovering a planted clique below the \u0398( \u221a n) barrier.\nDespite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013)."}, {"heading": "2 Sparse PCA through Low-rank Approximations", "text": ""}, {"heading": "2.1 Proposed Algorithm", "text": "Our algorithm is technically involved and for that reason we start with a high-level informal description. For any given accuracy parameter d we follow the following steps:\nStep 1: Obtain Ad, a rank-d approximation of A. We obtain Ad, the best-fit rank-d approximation of A, by keeping the first d terms of its eigen-decomposition:\nAd = d\u2211 i=1 \u03bbiviv T i ,\nwhere \u03bbi is the i-th largest eigenvalue of A and vi the corresponding eigenvector. Step 2: Use Ad to obtain O(nd) candidate supports. For any matrix A, we can exhaustively search for the optimal x\u2217 by checking all ( n k ) possible k \u00d7 k submatrices of A: x\u2217 is the k-sparse vector with the same support as the submatrix of A with the maximum largest eigenvalue. However, we show how sparse PCA can be efficiently solved on Ad if the rank d is constant with respect to n, using the machinery of (Asteris et al., 2011). The key technical fact proven there is that there are only O(nd) candidate supports that need to be examined. That is, a set of candidate supports Sd = {I1, . . . ,IT }, where It is a subset of k indices from {1, . . . , n}, contains the optimal support. The number of these supports is1\nStep 3: Check each candidate support from Sd on A. For a given support I it is easy to find the best vector supported on I: it is the leading eigenvector of the principal submatrix of A, with rows and columns indexed by I. In this step, we check all the supports in Sd on the original matrix A and output the best. Specifically, define AI to be the zeroed-out version of A, except on the support I. That is, AI is an n\u00d7n matrix with zeros everywhere except for the principal submatrix indexed by I. If i \u2208 I and j \u2208 I, then AI = Aij , else it is 0. Then, for any AI matrix, with I \u2208 Sd, we compute its largest eigenvalue and corresponding eigenvector.\nOutput: Finally, we output the k-sparse vector xd that is the principal eigenvector of the AI matrix, I \u2208 Sd, with the largest maximum eigenvalue. We refer to this approximate sparse PC solution as the rank-d optimal solution.\nThe exact steps of our algorithm are given in the pseudocode tables denoted as Algorithm 1 and 2. The spannogram subroutine, i.e., Algorithm 2, computes the T candidate supports in Sd, and is presented and explained in Section 3. The complexity of our algorithm is equal to calculating d leading eigenvectors of A (O(SV D(A,d))), running the spannogram algorithm (O(nd+1 logn)), and finding the leading eigenvector of O(nd) matrices of size k\u00d7 k (O(ndk3)). Hence, the total complexity is O(nd+1 logn+ ndk3 + SV D(A,d)).\nElimination Step: This step is run before Step 2. By using a feature elimination subroutine we can identify that certain variables provably cannot be in the support of xd, the rank-d optimal sparse PC. We have a test which is related to the norms of the rows of Vd that identifies which of the n rows cannot be in the optimal support. We use this step to further reduce the number of candidate supports |Sd|. The elimination algorithm is very important when it comes to large scale data sets. Without the elimination step, even the rank-2 version of the algorithm becomes intractable for n > 104. However, after running the subroutine we empirically observe that even for n that is in the orders of 106 the elimination strips down the number of features to only around 50\u2212 100 for values of k around 10. This subroutine is presented in detail in the Appendix."}, {"heading": "2.2 Approximation Guarantees", "text": "The desired sparse PC is x\u2217 = arg max\n\u2016x\u20162=1,\u2016x\u20160=k xTAx.\n1In fact, in our proof we show a better dependency on d, which however has a more complicated expression. |Sd| \u2264 22d (n d ) .\nThe above set Sd is efficiently created by the Spannogram algorithm described in the next subsection.\nAlgorithm 1 Sparse PCA via a rank-d approximation 1: Input: k, d, A 2: p\u2190 1 if A has nonnegative entries, 0 if mixed 3: Ad\u2190 \u2211d i=1 \u03bbiviv T i\n4: A\u0302d\u2190 feature elimination(k, p,Ad) 5: Sd\u2190 Spannogram ( k, p, A\u0302d ) 6: for each I \u2208 Sd do 7: Calculate \u03bb1(AI) 8: end for 9: Ioptd = arg maxI\u2208Sd \u03bb1(AI)\n10: OPTd = \u03bb1(AIoptd ) 11: x opt d \u2190 the principal eigenvector of AIoptd . 12: Output: xoptd\nWe instead obtain the k-sparse, unit length vector xd which gives an objective\nxTdAxd = maxI\u2208Sd \u03bb(AI).\nWe measure the quality of our approximation using the standard approximation factor:\n\u03c1d = xTdAxd xT\u2217Ax\u2217\n= max I\u2208Sd \u03bb(AI)\n\u03bb (k) 1\n,\nwhere \u03bb(k)1 = x T \u2217Ax\u2217 is the k-sparse largest eigenvalue of A.2 Clearly, \u03c1d \u2264 1 and as it approaches 1, the approximation becomes tighter. Our main result follows:\nTheorem 1. For any d, our algorithm outputs xd, where ||xd||0=k, ||xd||2=1 and\nxTdAxd \u2265 (1\u2212 )xT\u2217Ax\u2217,\nwith an error bound\nd \u2264 \u03bbd+1\n\u03bb (k) 1\n\u2264min { n\nk \u03bbd+1 \u03bb1 , \u03bbd+1\n\u03bb (1) 1\n} .\nProof. The proof can be found in the Appendix. The main idea is that we obtain i) an upper bound on the performance loss using Ad instead of A and ii) a lower bound for \u03bb (k) 1 .\nWe now use our main theorem to provide the following model specific approximation results.\nCorollary 1. Assume that for some constant value d, there is an eigenvalue decay \u03bb1 > \u03bbd+1 in A. Then there exists a constant \u03b40 such that for all sparsity levels k > \u03b40n we obtain a constant approximation ratio.\nCorollary 2. Assume that the first d+ 1 eigenvalues of A follow a power-law decay, i.e., \u03bbi = Ci\u2212\u03b1, for some C,\u03b1 > 0. Then, for any k = \u03b4n and any > 0 we can get a (1\u2212 )-approximate solution xd in time O ( n1/( \u03b4) \u03b1+1 logn ) .\nThe above corollaries can be established by plugging in the values for \u03bbi in the error bound. We find the above families of matrices interesting, because in practical data sets (like the ones we tested), we observe a significant decay in the first eigenvalues of A which in many cases follows a power law. The main point of the above approximability result is that any matrix with decent decay in the spectrum endows a good sparse PCA approximation.\n2Notice that the k-sparse largest eigenvalue of A for k = 1 denoted by \u03bb(1)1 is simply the largest element on the diagonal of A."}, {"heading": "3 The Spannogram Algorithm", "text": "In this section, we describe how the Spannogram algorithm constructs the candidate supports in Sd and explain why this set has tractable size. We build up to the general algorithm by explaining special cases that are easier to understand.\n3.1 Rank-1 case\nLet us start with the rank 1 case, i.e., when d = 1. For this case\nA1 = \u03bb1v1v T 1 .\nAssume, for now, that all the eigenvector entries are unique. This simplifies tie-breaking issues that are formally addressed by a perturbation lemma in our Appendix. For the rank-1 matrix A1, a simple thresholding procedure solves sparse PCA: simply keep the k largest entries of the eigenvector v1. Hence, in this simple case S1 consists of only 1 set.\nTo show this, we can rewrite (1) as\nmax x\u2208Sk xTA1x = \u03bb1 \u00b7max x\u2208Sk\n( vT1 x )2 = \u03bb1 \u00b7max\nx\u2208Sk ( n\u2211 i=1 v1ixi )2 , (3)\nwhere Sk is the set of all vectors x \u2208 Rn with ||x||2 = 1 and ||x||0 = k. We are trying to find a k-sparse vector x that maximizes the inner product with a given vector v1. It is not hard to see that this problem is solved by sorting the absolute elements of the eigenvector v1 and keeping the support of the k entries in v1 with the largest amplitude.\nDefinition 1. Let Ik(v) denote the set of indices of the top k largest absolute values of a vector v. We can conclude that for the rank-1 case, the optimal k-sparse PC for A1 will simply be the k-sparse vector that is co-linear to the k-sparse vector induced on this unique candidate support. This will be the only rank-1 candidate optimal support\nS1 = {Ik(v1)}.\n3.2 Rank-2 case\nNow we describe how to compute S2 using the constant rank solver of (Asteris et al., 2011). This is the first nontrivial d which exhibits the details of the spannogram algorithm. We have the rank 2 matrix\nA2 = 2\u2211 i=1 \u03bbiviv T i = V2V T 2 ,\nwhere V2 = [\u221a \u03bb1 \u00b7 v1 \u221a \u03bb2 \u00b7 v2 ] . We can rewrite (1) on A2 as\nmax x\u2208Sk xTA2x = max x\u2208Sk \u2225\u2225V T2 x\u2225\u222522. (4) In the rank-1 case we could write the quadratic form maximization as a simple maximization of a dot product\nmax x\u2208Sk xTA1x = max x\u2208Sk\n( vT1 x )2 .\nSimilarly, we will prove that in the rank-2 case we can write\nmax x\u2208Sk xTA2x = max x\u2208Sk\n( vTc x )2 ,\nfor some specific vector vc in the span of the eigenvectors v1, v2; this will be very helpful in solving the problem efficiently.\nTo see this, let c be a 2\u00d7 1 unit length vector, i.e., \u2016c\u20162 = 1. Using the Cauchy-Schwartz inequality for the inner product of c and V T2 x we obtain ( cTV T2 x )2 \u2264 \u2016V T2 x\u201622, where equality holds, if and only if, c is co-linear to V T2 x. By the previous fact, we have a variational characterization of the `2-norm:\n\u2016V T2 x\u201622 = max\u2016c\u20162=1 ( cTV T2 x )2 . (5)\nWe can use (5) to rewrite (4) as\nmax x\u2208Sk,\u2016c\u20162=1\n( cTV T2 x )2 = max x\u2208Sk max \u2016c\u20162=1 ( vTc x )2 = max \u2016c\u20162=1 max x\u2208Sk ( vTc x )2 , (6)\nwhere vc = V2c. We would like to note two important facts here. The first is that for all unit vectors c, vc = V2c generates all vectors in the span of V2 (up to scaling factors). The second fact is that if we fix c, then the maximization maxx\u2208Sk ( vTc x\n)2 is a rank-1 instance, similar to (3). Therefore, for each fixed unit vector c there will be one candidate support (denote it by Ik(V2c)) to be added in S2.\nIf we could collect all possible candidate supports Ik(V2c) in\nS2 = \u22c3\nc\u2208R2\u00d71,\u2016c\u20162=1\n{Ik(V2c)} , (7)\nthen we could solve exactly the sparse PCA problem on A2: we would simply need to test all locally optimal solutions obtained from each support in S2 and keep the one with the maximum metric. The issue is that there are infinitely many vc vectors to check. Naively, one could think that all possible ( n k ) k-supports could appear for some vc vector. The key combinatorial fact is that if a vector vc lives in a two dimensional subspace, there are tremendously fewer possible supports3:\n|S2| \u2264 4 ( n\n2\n) ."}, {"heading": "3.2.1 Spherical variables and the spannogram", "text": "Here we use a transformation of our problem space into a 2-dimensional space as was done in (Karystinos and Liavas, 2010). The transformation is performed through spherical variables that enable us to visualize the 2-dimensional span of V2. For the rank-2 case, we have a single phase variable \u03c6 \u2208 \u03a6 = ( \u2212\u03c02 , \u03c02 ] and use it to rewrite c, without loss of generality, as\nc = [ sin\u03c6 cos\u03c6 ] ,\nwhich is again unit norm and for all \u03c6 it scans all4 2 \u00d7 1 unit vectors. Under this characterization, we can express vc in terms of \u03c6 as\nv(\u03c6) = V2c = sin\u03c6 \u00b7 \u221a \u03bb1v1 + cos\u03c6 \u00b7 \u221a \u03bb2v2. (8)\nObserve that each element of v(\u03c6) is a continuous curve in \u03c6:\n[v(\u03c6)]i = [\u221a \u03bb1v1 ] i sin(\u03c6) + [\u221a \u03bb2v2 ] 2 cos(\u03c6),\nfor all i = 1, . . . , n. Therefore, the support set of the k largest absolute elements of v(\u03c6) (i.e., Ik(v(\u03c6))) is itself a function of \u03c6.\n3This is a special case of the general d dimensional lemma of (Asteris et al., 2011) (found in the Appendix), but we prove the special case to simplify the presentation.\n4Note that we restrict ourselves to ( \u2212\u03c0\n2 , \u03c0 2\n] , instead of the whole (\u2212\u03c0, \u03c0] angle region. First observe that the vectors in the complement\nof \u03a6 are opposite to the ones evaluated on \u03a6. Omitting the opposite vectors poses no issue due to the squaring in (4), i.e., vectors c and\u2212c map to the same solutions.\nIn Fig. 1, we draw an example plot of 3 (absolute) curves |[v(\u03c6)]i|, i = 1,2,3, from a randomly generated matrix V2. We call this a spannogram, because at each \u03c6, the values of curves correspond to the absolute values of the elements in the column span of V2. Computing [v(\u03c6)]i for all i, \u03c6 is equivalent to computing the span of V2. From the spannogram in Fig. 1, we can see that the continuity of the curves implies a local invariance property of the support sets I(v(\u03c6)), around a given \u03c6. As a matter of fact, a support set Ik(v(\u03c6)) changes, if and only if, the respective sorting of two absolute elements |[v(\u03c6)]i| and |[v(\u03c6)]j | changes. Finding these interesection points |[v(\u03c6)]i| = |[v(\u03c6)]j | is the key to find all possible support sets.\nThere are n curves and each pair intersects on exactly two points.5 Therefore, there are exactly 2 ( n 2 ) intersection points. The intersection of two absolute curves are exactly two points \u03c6 that are a solution to [v(\u03c6)]i = [v(\u03c6)]j and [v(\u03c6)]i = \u2212[v(\u03c6)]j . These are the only points where local support sets might change. These 2 ( n 2 ) intersection points partition \u03a6 in 2 ( n 2 ) + 1 regions within which the top k support sets remain invariant."}, {"heading": "3.2.2 Building S2", "text": "To build S2, we need to i) determine all c intersection vectors that are defined at intersection points on the \u03c6-axis and ii) compute all distinct locally optimal support sets Ik(vc). To determine an intersection vector we need to solve all 2 ( n 2 ) equations [v(\u03c6)]i =\u00b1[v(\u03c6)]j for all pairs i, j \u2208 [n]. This yields [v(\u03c6)]i =\u00b1[v(\u03c6)]j \u21d2 eTi V c=\u00b1eTj V c, that is ( eTi \u00b1 eTj ) Vc=0\u21d2c=nullspace (( eTi \u00b1 eTj ) V ) . (9)\nSince c needs to be unit norm, we simply need to normalize the solution c. We will refer to the intersection vector calculated on the \u03c6 of the intersection of two curves i and j as c+i,j and c \u2212 i,j , depending on the corresponding sign in (9). For the intersection vectors c+i,j and c \u2212 i,j we compute Ik(V2c+i,j) and Ik(V2c\u2212i,j). Observe that since the i and j curves are equal on the intersection points, there is no prevailing sorting among the two corresponding elements i and j of V2c+i,j or V2c \u2212 i,j . Hence, for each intersection vector c + i,j and c \u2212 i,j , we create two candidate support sets, one where element i is larger than j, and vice versa. This is done to secure that both support sets, left and right of the \u03c6 of the intersection, are included in S2. With the above methodology, we can compute all possible Ik(V2c) rank-2 optimal candidate sets and we obtain\n|S2| \u2264 4 ( n\n2\n) = O(n2).\n5As we mentioned, we assume that the curves are in \u201cgeneral position,\u201d i.e., no three curves intersect at the same point and this can be enforced by a small perturbation argument presented in the Appendix.\nThe time complexity to build S2 is then equal to sorting ( n 2 ) vectors and solving 2 ( n 2 ) equations in the 2\nunknowns of c+i,j and c + i,j . That is, the total complexity is equal to ( n 2 ) n logn+ ( n 2 ) 22 = O ( n3 logn ) .\nRemark 1. The spannogram algorithm operates by simply solving systems of equations and sorting vectors. It is not iterative nor does it attempt to solve a convex optimization problem. Further, it computes solutions that are exactly k-sparse, where the desired sparsity can be set a-priori.\nThe spannogram algorithm presented here is a subroutine that can be used to find the leading sparse PC of Ad in polynomial time. The general rank-d case is given as Algorithm 2. The details of the constant rank algorithm, the elimination step, and tune-ups for matrices with nonnegative entries can be found in the Appendix."}, {"heading": "4 Experimental Evaluation", "text": "We now empirically evaluate the performance of our algorithm and compare it to the full regularization path greedy approach (FullPath) of (d\u2019Aspremont et al., 2007b), the generalized power method (GPower) of (Journe\u0301e et al., 2010), and the truncated power method (TPower) of (Yuan and Zhang, 2011). We omit the DSPCA semidefinite approach of (d\u2019Aspremont et al., 2007a), since the FullPath algorithm is experimentally shown to have similar or better performance (d\u2019Aspremont et al., 2008).\nWe start with a synthetic experiment: we seek to estimate the support of the first two sparse eigenvectors of a covariance matrix from sample vectors. We continue with testing our algorithm on gene expression data sets. Finally, we run experiments on a large-scale document-term data set, comprising of millions of Twitter posts."}, {"heading": "4.1 Spiked Covariance Recovery", "text": "We first test our approximation algorithm on an artificial data set generated in the same manner as in (Shen and Huang, 2008; Yuan and Zhang, 2011). We consider a covariance matrix \u03a3, which has two sparse eigenvectors with very large eigenvalues and the rest of the eigenvectors correspond to small eigenvalues. Here, we consider \u03a3 = \u2211n i=1 \u03bbiviv T i with \u03bb1 = 400, \u03bb2 = 300, \u03bb3 = 1, . . . , \u03bb500 = 1. where the first two eigenvectors are sparse and each has 10 nonzero entries and non-overlapping supports. The remaining eigenvectors are picked as n\u2212 2 orthogonal vectors in the nullspace of [v1 v2].\nWe have two sets of experiments, one for few samples and one for extremely few. First, we generate m = 50 samples of length n = 500 distributed as zero mean Gaussian with covariance matrix \u03a3 and repeat the experiment 5000 times. We repeat the same experiment for m = 5. We compare our rank-1 and rank-2 algorithms against FullPath, GPower with `1 penalization and `0 penalization, and TPower. After estimating the first eigenvector with v\u03031, we deflate A to obtain A\u2032. We use the projection deflation method (Mackey, 2009) to obtain A\u2032 = (I \u2212 v\u03031v\u0303T1 )A(I \u2212 v\u03031v\u0303T1 ) and work on it to obtain v\u03032, the second estimated eigenvector of \u03a3.\nIn Table 1, we report the probability of correctly recovering the supports of v1 and v2: if both estimates v\u03031 and v\u03032 have matching supports with the true eigenvectors, then the recovery is considered successful.\nIn our experiments for m = 50, all algorithms were comparable and performed near-optimally, apart from the rank-1 approximation (PCA+thresholding). The success of our rank-2 algorithm can be in parts suggested\nby the fact that the true covariance \u03a3 is almost rank 2: it has very large decay between its 2nd and 3rd eigenvalue. The average approximation guarantee that we obtained from the generating experiments for the rank 2 case and for m = 50 was xT2 Ax2 \u2265 0.7 \u00b7 x\u2217AxT\u2217 , that is before running our algorithm, we know that it could on average perform at least 70% as good as the optimal solution. For m = 5 samples we observe that the performance of the rank-1 and GPower methods decay and FullPath, TPower, and rank-2 find the correct support with probability approximately equal to 96%. This overall decay in performance of all schemes is due to the fact that 5 samples are not sufficient for a perfect estimate. Interesting tradeoffs of sample complexity and probability of recovery where derived in (Amini and Wainwright, 2008). Conducting a theoretical analysis for our scheme under the spiked covariance model is left as an interesting future direction."}, {"heading": "4.2 Gene Expression Data Set", "text": "In the same manner as in the relevant sparse PCA literature, we evaluate our approximation on two gene expression data-sets used in (d\u2019Aspremont et al., 2007b, 2008; Yuan and Zhang, 2011). We plot the ratio of the explained variance coming from the first sparse PC to the explained variance of the first eigenvector (which is equal to the first eigenvalue). We also plot the performance outer bound derived in (d\u2019Aspremont et al., 2008). We observe that our approximation follows the same optimality pattern as most previous methods, for many values of sparsity k. In these experiments we did not test the GPower method since the output sparsity cannot be explicitly predetermined. However, previous literature indicates that GPower is also near-optimal in this scenario."}, {"heading": "4.3 Large-scale Twitter data-set", "text": "We proceed our experimental evaluation of our algorithm by testing it on a large-scale data set. Our data-set comprises of millions of tweets coming from Greek Twitter users. Each tweet corresponds to a list of words and has a character limit of 140 per tweet. Although each tweet was associated with metadata, such us hyperlinks, user id, hash tags etc., we strip these features out and just use the word list. We use a simple Python script to normalize each Tweet. Words that are not contextual (me, to, what, etc) are discarded in an ad-hoc way. We also discard all words that are less than three characters, or words that appear once in the corpus. We represent each tweet as a long vector consisting of n words, with a 1 whenever a word appears, and 0 if it does not appear. Further details about our data set can be found in the Appendix.\nDocument-term data sets have been observed to follow power-laws on their eigenvalues. Empirical results have been reported that indicate power-law like decays for eigenvalues where no cutoff is observed (Dhillon and Modha, 2001) and some derived power-law generative models for 0/1 matrices (Mihail and Papadimitriou, 2002; Chung et al., 2003). In our experiments, we also observe power-law decays on the spectrum of the twitter matrices. Further experimental observations of power laws can be found in the Appendix. These underlying decay laws on the spectrum were sufficient to give good approximation guarantees; for many of our data sets 1\u2212 was between 0.5 to 0.7, even for d = 2,3. Further, our algorithm empirically performed better than these guarantees.\nIn the following tests, we compare against TPower and FullPath. TPower is run for 10k iterations, and is initialized with a vector having 1s on the k words of highest variance. For FullPath we restrict the covariance to its first 5k words of highest variance, since for larger numbers the algorithm became slow to test on a personal desktop computer. In our experiments, we use a simpler deflation method, than the more sophisticated ones used before. Once k words appear in the first k-sparse PC, we strip them from the data set, recompute the new convariance, and then run all algorithms. A benefit of this deflation is that it forces all sparse PCs to be orthogonal to each other which helps for a more fair comparison with respect to explained variance. Moreover, this deflation preserves the sparsity of the matrix A after each deflation step; sparsity on A facilitates faster execution times for all methods tested. The performance metric here is again the explained variance over its\nmaximum possible value: if we compute L PCs, x1, . . . , xL, we measure their performance as \u2211L i=1 x T i Axi\u2211L\ni=1 \u03bbi . We\nsee that in many experiments, we come very close to the optimal value of 1. In Table 3, we show our results for all tweets that contain the word Japan, for a 5-day (May 1-5, 2011) and then a month-length time window (May, 2011). In all these tests, our rank-3 approximation consistently captured more variance than all other compared methods.\nIn Table 2, we show a day-length experiment (May 10th, 2011), where we had 65k Tweets and 64k unique words. For this data-set we report the first 5 sparse PCs generated by all methods tested. The average computation times for this time-window where less than 1 second for the rank-1 approximation, less than 5 seconds for rank-2, and less than 2 minutes for the rank-3 approximation on a Macbook Pro 5.1 running MATLAB 7. The main reason for these tractable running times is the use of our elimination scheme which left only around 40\u2212 80 rows of the initial matrix of 64k rows. In terms of running speed, we empirically observed that our algorithm is slower than Tpower but faster than FullPath for the values of d tested. In Table 2, words with strike-through are what we consider non-matching to the \u201cmain topic\u201d of that PC. Words marked with G are translated from Greek. From the PCs we see that the main topics are about Skype\u2019s acquisition by Microsoft, the European Music Contest \u201cEurovision,\u201d a crime that occurred in the downtown of Athens, and the Greek census that was carried for the year 2011. An interesting observation is that a general \u201cexcitement\u201d sparse principal component appeared in most of our queries on the Twitter data set. It involves words like like, love, liked, received, great, etc, and was generated by all algorithms."}, {"heading": "5 Conclusions", "text": "We conclude that our algorithm can efficiently provide interpretable sparse PCs while matching or outperforming the accuracy of previous methods. A parallel implementation in the MapReduce framework and larger data studies are very interesting future directions."}, {"heading": "B Nonnegative matrix speed-up", "text": "In this section we show that if A has nonnegative entries then we can speed up computations by a factor of 2d\u22121. The main idea behind this speed-up is that whenA has only nonnegative entries, then in our intersection equations in Eq. (13) we do not need to check all possible signed combinations of the d curves. In the following we explain this point.\nWe first note that the Perron-Frobenius theorem (Horn and Johnson, 1990) grants us the fact that the optimal solution x\u2217 will have nonnegative entries. That is, if A has nonnegative entries, then x\u2217 will also have nonnegative entries. This allows us to pose a redundant nonnegativity constraint on our optimization\nmax x\u2208Sk xTAx = max x\u2208Sk,x 0 xTAx. (14)\nOur approximation uses the above constraint to reduce the cardinality of Sd by a factor of 2d\u22121. Let us consider for example the rank 1 case:\nmax x\u2208Sk,x 0\n( vTx )2 = max x\u2208Sk,x 0 ( n\u2211 i=1 vixi )2\nHere,the optimal solution can be again found in time O(n logn). First, we sort the elements of v. The optimal support I the for above problem corresponds to either the top k, or the bottom k unsigned elements of the sorted v. The fact that is important here is that the optimal vector can only have entries of the same sign.6 The implication of the previous fact is that on our curve intersection points, we can only account for intersections of the sort [v(\u03d5)]i = [v(\u03d5)]j . Intersection of the form [v(\u03d5)]i = \u2212[v(\u03d5)]j are not to be considered due to the fact that the locally optimal vector can only have one of the two signs. This means that in Eq. (13), we only have a single sign pattern. This eliminates exactly a factor of 2d\u22121 from the cardinality of the Sd set."}, {"heading": "C Feature Elimination", "text": "In this section we present our feature elimination algorithm. This step reduces the dimension n of the problem and this reduction in practice is empirically shown to be significant and allows us to run our algorithm for very large matrices A. Our elimination algorithm is combinatorial and is based on sequentially checking the rows of Vd, depending on the value of their norm. This step is based again on the spannogram framework used in our approximation algorithm for sparse PCA. In Fig. 4, we sketch the main idea of our elimination step.\nThe essentials of the elimination. First note that a locally optimal support set Ik(V c) for a fixed c in (10), corresponds to the top k elements of vc. As we mentioned before, all elements of vc correspond to hypersurfaces |[v(\u03d5)]i| that are functions of the d\u2212 1 spherical variables in \u03d5. For a fixed \u03d5 \u2208 \u03a6d\u22121, the candidate support set corresponds exactly to the top k (in absolute value) elements in vc = v(\u03d5), or the top k surfaces |[v(\u03d5)]i| for that particular \u03d5. There is a very simple observation here: a surface |[v(\u03d5)]i| belongs to the set of top k surfaces if |[v(\u03d5)]i| is below at most k \u2212 1 other surfaces on that \u03d5. If it is below k surfaces at that point \u03d5, then |[v(\u03d5)]i| does not belong in the set of k top surfaces.\nA second key observation is the following: the only points \u03d5 that we need to check are the critical intersection points between d surfaces. For example, we could construct a set Yk of all intersection points of all d sets of curves, such that for any point in this set the number of curves above it is at least k\u2212 1. In other words,\n6If there are less than k elements of the same sign in either of the two support sets in I1, then, and in order to satisfy the sparsity constraint, we can put weight > 0 on elements with the least amplitude in such set and opposite sign. This will only perturb the objective by a component proportional to , which can then be driven arbitrarily close to 0, while respecting the sparsity constraint.\nAlgorithm 3 Elimination Algorithm. 1: Input: k, p, Vd = [\u221a \u03bb1v1 . . . \u221a \u03bb1v1 ] 2: Initialize Pk \u2190 \u2205 3: Sort the rows of Vd in descending order according to their norms \u2016eTi Vd\u2016. 4: n\u0303\u2190 k+ d+ 1. 5: V\u0303 \u2190 V1:n\u0303,:. 6: for all ( n\u0303 d ) subsets (i1, . . . , id) from {1, . . . , n\u0303} do\n7: for all sequences (b1, . . . , bd\u22121) \u2208 B do\n8: c\u2190 nullspace  eTi1 \u2212 b1 \u00b7 eTi2... eTi1 \u2212 bd\u22121 \u00b7 e T id Vd \n9: if there are k\u2212 1 elements of |vc| greater than |e1Vdc| then 10: Pk \u2190Pk \u222a {|e1Vdc|} 11: end if 12: if \u2016Vn\u0303+1,:\u2016 < min{x \u2208 Pk} then 13: STOP ITERATIONS. 14: end if 15: n\u0303\u2190 n\u0303+ 1 16: V\u0303 \u2190 V1:n\u0303,: 17: for each element x in Pk do 18: check the elements |vc| greater than x 19: if there are more than k\u2212 1 then 20: discard it 21: end if 22: end for 23: end for 24: end for 25: Output: A\u0303d = V\u0303dV\u0303 Td , where V\u0303d comprises of the first n\u0303 rows of Vd of highest norm.\nYk defines a boundary: if a curve is above this boundary then it may become a top k curve; if not it can never appear in a candidate set. This means that we could test each curve against the points in Yk and discard it if its amplitude is less than the amplitudes of all intersection points in Yk. However, the above elimination technique implies that we would need to calculate all intersection points on the n surfaces. Our goal is to use the above idea by serially checking one by one the intersection points of high amplitude curves.\nElimination algorithm description. We use the above ideas, to build our elimination algorithm. We first compute the norms of each row \u2016[Vd]:,i\u20162 of Vd. This norm corresponds to the amplitude of [v(\u03d5)]i. Then, we sort all n rows according to their norms. We first start with the k+ d rows of Vd (i.e., surfaces) of highest norm (i.e., amplitude) and compute their ( k+d d ) intersection points. For each intersection point, say \u03c6, we compute the number of |[v(\u03d5)]i| surfaces above it. If there are less than k \u2212 1 surfaces above an intersection point, then this means that such point is a potential intersection point where a new curve enters a local top k set. We keep all these points in a set Pk.\nWe then move to the (k+ d+ 1)-st surface of highest amplitude; we test it against the minimum amplitude point in Pk. If the amplitude of the (k+ d+ 1)-st surface is less than the minimum amplitude point in Pk, then we can safely eliminate this surface (i.e., this row of Vd), and all surfaces with maximum amplitude smaller than that (i.e., all rows of Vd with norm smaller than the row of interest). If its amplitude is larger than the amplitude of this point, then we compute the new set of ( k+d+1 d ) intersection points obtained by adding this new surface. We check if some of these can be added in Pk, using the test of whether there are at most k \u2212 1 curves above each point. We need also re-check all previous points in Pk, since some may no longer be eligible to be in the set; if some are not, then we delete them from the set Pk. We then move on the next row of Vd, and continue this process until we reach a row with norm less than the minimum amplitude of the points in Pk.\nA pseudo-code for our feature elimination algorithm can be found as Algorithm 1. In Fig. 5, we give an example of how our elimination works."}, {"heading": "D Approximation Guarantees", "text": "In this section, we prove the approximation guarantees for our algorithm. Let us define two quantities, namely\nOPT = max x\u2208Sk xTAx and OPTd = max x\u2208Sk xTAdx,\nwhich correspond to the optimal values of the initial maximization under the full-rank matrix A and its rank-d approximation Ad, respectively. Then, we establish the following lemma.\nLemma 2. Our approximation factor is lower bounded as\n\u03c1d = max I\u2208Sd \u03bb(AI)\n\u03bb (k) 1\n\u2265 OPTd OPT . (15)\nProof. The first technical fact that we use is that an optimizer vector xd for Ad (i.e., the one with the maximum performance for Ad), can achieve at least the same performance for A, i.e., xTdAxd \u2265 xTdAdxd. The proof is straightforward: since A is PSD, each quadratic form inside the sum \u2211n i=1 \u03bbix T viv T i x is a positive number.\nHence, \u2211n i=1 \u03bbix T viv T i x \u2265 \u2211d i=1 \u03bbix T viv T i x, for any vector x and any d.\nThe second technical fact is that if we are given a vector xd with nonzero support I, then calculating qd, the principal eigenvector of AI , results in a solution for A with better performance compared to xd. To show that, we first rewrite xd as xd = PIxd, where PI is an n\u00d7nmatrix that has 1s on the diagonal elements that multiply the nonzero support of xd and has 0s elsewhere. Then, we have\nOPTd \u2264 xTdAxd = xTd PIAPIxd = xTdAIxd (16) \u2264 max \u2016x\u20162=1 xTAIx = q T d AIqd = q T d Aqd.\nUsing the above fact for all sets I \u2208 Sd, we obtain that max I\u2208Sd \u03bb(AI) \u2265 OPTd, which proves our lower bound.\nSparse spectral ratio. A basic quantity that is important in our approximation ratio as we see in the following, is what we define as the sparse spectral ratio, which is equal to \u03bbd+1/\u03bb (k) 1 . This ratio will be shown to be directly related to the (non-sparse) spectrum of A. Here we prove the the following lemma.\nLemma 3. Our approximation ratio is lower bounded as follows.\n\u03c1d \u2265 1\u2212 \u03bbd+1\n\u03bb (k) 1\n. (17)\nProof. We first decompose the quadratic form in (1) in two parts xTAx =xT (\nn\u2211 i=1 \u03bbiviv T i\n) x = xTAdx+ x TAdcx (18)\nwhere Adc = A\u2212Ad = \u2211n i=d+1 \u03bbiviv T i . Then, we take maximizations on both parts of (18) over our feasible set of vectors with unity `2-norm and cardinality k and obtain\nmax x\u2208Sk xTAx = max x\u2208Sk\n( xTAdx+ x TAdcx )\n(i)\u21d4max x\u2208Sk xTAx \u2264max x\u2208Sk xTAdcx+ max x\u2208Sk xTAdcx \u21d4OPT \u2264 OPTd + max x\u2208Sk xTAdcx\n(ii)\u21d4OPT \u2264 OPTd + max \u2016x\u20162=1 xTAdcx\n(iii)\u21d4 OPT \u2264 OPTd + \u03bbd+1, (19)\nwhere (i) comes from the fact that the maximum of the sum of two quantities is always upper bounded by the sum of their maximum possible values, (ii) is due to the fact that we lift the `0 constraint on the optimizing vector x, and (iii) is due to the fact that the largest eigenvalue of A\u2212Ad is equal to \u03bbd+1. Moreover, due to the fact that OPT \u2265 OPTd, we have OPT\u2212 \u03bbd+1 \u2264 OPTd \u2264 OPT. (20) Dividing both the terms of (20) with OPT yields\n1\u2212 \u03bbd+1 \u03bb (k) 1 = 1\u2212 \u03bbd+1 OPT \u2264 OPTd OPT \u2264 \u03c1d \u2264 1. (21)\nLower-bounding \u03bb(k)1 . We will now give two lower-bounds on OPT.\nLemma 4. The sparse eigenvalue of A is lower bounded as\n\u03bb (k) 1 \u2265max\n{ k\nn \u03bb1, \u03bb\n(1) 1\n} . (22)\nProof. The second bound is straightforward: if we assume the feasible solution emax, being the column of the identity matrix which has a 1 in the same position as the maximum diagonal element of A, then we get\nOPT \u2265 eTmaxCeTmax = max i=1,...,n Aii = \u03bb (1) 1 . (23)\nThe first bound for OPT will be obtained by examining the rank-1 optimal solution on A1. Observe that\nOPT \u2265 OPT1 = max x\u2208Sk xTA1x\n= \u03bb1 max x\u2208Sk\n(vT1 x) 2. (24)\nBoth v1 and x have unit norm; this means that (vT1 x)2 \u2264 1. The optimal solution for this problem is to allocate all k nonzero elements of x on Ik(v1): the top k absolute elements of v1. An optimal solution vector, will give a metric of (vT1 x)2 = \u2016[v1]Ik(v1)\u201622. The norm of the k largest elements of v1 is then at least equal to kn times the norm of v1. Therefore, we have\nOPT \u2265max { k\nn \u03bb1, \u03bb\n(1) 1\n} . (25)\nThe above lemmata can be combined to establish Theorem 1."}, {"heading": "E Resolving singularities", "text": "In our algorithmic developments, we have made an assumption on the curves studied, i.e., on the rows of the Vd matrix. This assumption was made so that tie-breaking cases are evaded, where more than d curves intersect in a single point in the d dimensional space \u03a6d. Such a singularity is possible even for full-rank matrices Vd and can produce enumerating issues in the generation of locally optimal candidate vectors that are obtained through the intersection equations:  e T i1 \u2212 b1eTi2\n... eTi1 \u2212 bd\u22121eTid  d\u22121\u00d7n Vdc = 0d\u22121\u00d71. (26)\nThe above requirement can be formalized as: no system of equations of the following form has a nontrivial (i.e., nonzero) solution  eTi1 \u2212 b1eTi2 ...\neTi1 \u2212 bd\u22121eTid eTi1 \u2212 bd\u22121eTid+1  d\u00d7n Vdc 6= 0d\u00d71 (27)\nfor all c 6= 0 and all possible d+ 1 row indices i1, . . . , id+1 (where two indices cannot be the same). We show here that the above issues can be avoided by slightly perturbing the matrix Vd. We will also show that this perturbation is not changing the approximation guarantees of our scheme, guaranteed that it is sufficiently small. We can thus rewrite our requirement as a full-rank assumption on the following matrices\nrank   eTi1 \u2212 b1eTi2 ...\neTi1 \u2212 bd\u22121eTid eTi1 \u2212 bd\u22121eTid+1  d\u00d7n Vd  = d (28) for all i1 6= i2 6= . . . 6= id. Observe that we can rewrite the above matrix as eTi1 \u2212 b1eTi2 ...\neTi1 \u2212 bd\u22121eTid eTi1 \u2212 bd\u22121eTid+1  d\u00d7n Vd =  [Vd]i1,: \u2212 b1[Vd]i2,: ... [Vd]i1,: \u2212 b1[Vd]id,: [Vd]i1,: \u2212 b1[Vd]id+1,:  =  [Vd]i1,: ... [Vd]i1,: [Vd]i1,: \u2212  b1[Vd]i2,: ... b1[Vd]id,: b1[Vd]id+1,:  = Ri1 +Gi2,...,id+1 where Ri1 is a rank-1 matrix. Observe that the rank of the above matrix depends on the ranks of both of its components and how the two subspaces interact. It should not be hard to see that we can add a d\u00d7 d random matrix \u2206 = [\u03b41\u03b42 . . . \u03b4d] to the above matrix, so that Ri1 +Gi2,...,id+1 + \u2206 is full-rank with probability 1.\nLet Ed be an n\u00d7 d matrix with entries that are uniformly distributed and bounded as |Ei,j | \u2264 . Instead of working on Vd we will work on the perturbed matrix V\u0303d = Vd +Ed. Then, observe that for any matrix of the previous form Ri1 +Gi2,...,id+1 we now have Ri1 +Gi2,...,id+1 + [Ed]i1,: \u2297 1d\u00d71 +Ei2,...,id+1 , where\nEi2,...,id =  [Ed]i2,: [Ed]i3,:\n... [Ed]id+1,:  . (29) Conditioned on the randomness of [Ed]i1,:, the matrix Ri1 +Gi2,...,id+1 + [Ed]i1,:\u2297 1d\u00d71 +Ei2,...,id+1 is full rank. Then due to the fact that there are d random variables in [Ed]i1,: and d2 random variable in Ri1 +Gi2,...,id+1 + [Ed]i1,: \u2297 1d\u00d71 +Ei2,...,id+1 , the latter matrix will be full-rank with probability 1 using a union bounding argument. This means that all ( n d ) submatrices of V\u0303d will be full rank, hence obtaining the property that no d+ 1 curves intersect in a single point in \u03a6d. Now we will show that this small perturbation does not change our metric of interest significantly. The following holds for any unit norm vector x\nxT (Vd +Ed)(Vd +Ed) Tx = xTVdV T d x+ x TEdE T d x+ 2x TVdE T d x \u2265 xTVdV Td x+ 2xTVdETd x \u2265 xTVdV Td x\u2212 2\u2016V Td x\u2016 \u00b7 \u2016ETd x\u2016 \u2265 xTVdV Td x\u2212 2 \u221a \u03bb1 \u00b7 \u03bb1(EdETd )\nand\n\u2016(Vd +Ed)Tx\u20162 \u2264 \u2016V Td x\u20162 + 2\u2016Edx\u2016\u2016V Td x\u2016+ \u2016ETd x\u20162 \u2264 \u2016V Td x\u20162 + 2 \u221a \u03bb1 \u00b7 \u03bb1(EdETd ) + \u03bb1(EdE T d )\n\u2264 \u2016V Td x\u20162 + 3 \u221a \u03bb1 \u00b7 \u03bb1(EdETd ).\nCombining the above we obtain the following bound\nxTVdV T d x\u2212 3 \u221a \u03bb1 \u00b7 \u03bb1(EdETd ) \u2264 \u2016(Vd +Ed)Tx\u20162 \u2264 \u2016V Td x\u20162 + 3 \u221a \u03bb1 \u00b7 \u03bb1(EdETd ).\nBy the above, we can appropriately pick such that 3 \u221a \u03bb1 \u00b7 \u03bb1(EdETd ) = o(1). An easy way to get a bound on is via the Gershgorin circle theorem (Horn and Johnson, 1990), which yields \u03bb1(EdETd ) < nd \u00b7 2. Hence, an < 1\u221a\n\u03bb1nd logn works for our purpose.\nTo summarize, in the above we show that there is an easy way to avoid singularities in our problem. Instead of solving the original rank-d problem on Vd, we can instead solve it on Vd +Ed, with an Ed random matrix with sufficiently small entries. This slight perturbation only incurs an error of at most 1logn in the objective, which asymptotically becomes zero as n increases."}, {"heading": "F Twitter data-set description", "text": "In Table 4, we give an overview of our Twitter data set.\nF.1 Power Laws\nIn this subsection we provide empirical evidence that our tested data-sets exhibit a power law decay on their spectrum We report these observations as a proof of concept for our approximation guarantees. Based on the spectrum of some subsets of our data-set, we provide the exact approximation guarantees derived using our bounds.\nIn Fig. 6, we plot the best fit power law for the spectrum and degrees with data-set parameters given on the figures. The plots that we provide are for hour-length, day-length, and month-length analysis, and subsets of our data set based on a specific query. We observe that for all these subsets of our data set, the spectrum indeed follows a power-law. An interesting observation is that a very similar power law is followed by the degrees of the terms in the data set. This finding is compatible to the generative models and analysis of (Mihail and Papadimitriou, 2002; Chung et al., 2003). The rough overview is that eigenvalues of A can be well approximated using the diagonal elements of A. In the same figure, we show how our approximation guarantees that based on the spectrum of A scales with d, for the various data-sets tested. We only plot for d up to 5, since for any larger d our algorithm becomes impractical for moderately large small data sets."}], "references": [{"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["A.A. Amini", "M.J. Wainwright"], "venue": "In Information Theory,", "citeRegEx": "Amini and Wainwright.,? \\Q2008\\E", "shortCiteRegEx": "Amini and Wainwright.", "year": 2008}, {"title": "Sparse principal component of a rank-deficient matrix", "author": ["M. Asteris", "D.S. Papailiopoulos", "G.N. Karystinos"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Asteris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Asteris et al\\.", "year": 2011}, {"title": "Optimal detection of sparse principal components in high dimension", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "arXiv preprint arXiv:1202.5070,", "citeRegEx": "Berthet and Rigollet.,? \\Q2012\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2012}, {"title": "Complexity theoretic lower bounds for sparse principal component detection, 2013a", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": null, "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Computational lower bounds for sparse pca", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "arXiv preprint arXiv:1304.0828,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Loading and correlations in the interpretation of principle compenents", "author": ["J. Cadima", "I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "Cadima and Jolliffe.,? \\Q1995\\E", "shortCiteRegEx": "Cadima and Jolliffe.", "year": 1995}, {"title": "Sparse pca: Optimal rates and adaptive estimation", "author": ["T Tony Cai", "Zongming Ma", "Yihong Wu"], "venue": "arXiv preprint arXiv:1211.1309,", "citeRegEx": "Cai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2012}, {"title": "Optimal estimation and rank detection for sparse spiked covariance matrices", "author": ["Tony Cai", "Zongming Ma", "Yihong Wu"], "venue": "arXiv preprint arXiv:1305.3235,", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Eigenvalues of random power law graphs", "author": ["F. Chung", "L. Lu", "V. Vu"], "venue": "Annals of Combinatorics,", "citeRegEx": "Chung et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2003}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L.E. Ghaoui"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Approximation bounds for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L.E. Ghaoui"], "venue": "arXiv preprint arXiv:1205.0121,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2012\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2012}, {"title": "Full regularization path for sparse principal component analysis", "author": ["Alexandre d\u2019Aspremont", "Francis R. Bach", "Laurent El Ghaoui"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Concept decompositions for large sparse text data using clustering", "author": ["I.S. Dhillon", "D.S. Modha"], "venue": "Machine learning,", "citeRegEx": "Dhillon and Modha.,? \\Q2001\\E", "shortCiteRegEx": "Dhillon and Modha.", "year": 2001}, {"title": "Sparse pca for text corpus summarization and exploration", "author": ["B. Gawalt", "Y. Zhang", "L. El Ghaoui"], "venue": "NIPS 2010 Workshop on Low-Rank Matrix Approximation,", "citeRegEx": "Gawalt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gawalt et al\\.", "year": 2010}, {"title": "Matrix analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge university press,", "citeRegEx": "Horn and Johnson.,? \\Q1990\\E", "shortCiteRegEx": "Horn and Johnson.", "year": 1990}, {"title": "Rotation of principal components: choice of normalization constraints", "author": ["I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "Jolliffe.,? \\Q1995\\E", "shortCiteRegEx": "Jolliffe.", "year": 1995}, {"title": "A modified principal component technique based on the lasso", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Jolliffe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jolliffe et al\\.", "year": 2003}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "The varimax criterion for analytic rotation in factor analysis", "author": ["H.F. Kaiser"], "venue": null, "citeRegEx": "Kaiser.,? \\Q1958\\E", "shortCiteRegEx": "Kaiser.", "year": 1958}, {"title": "Efficient computation of the binary vector that maximizes a rank-deficient quadratic form", "author": ["G.N. Karystinos", "A.P. Liavas"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Karystinos and Liavas.,? \\Q2010\\E", "shortCiteRegEx": "Karystinos and Liavas.", "year": 2010}, {"title": "Fast algorithms for sparse principal component analysis based on rayleigh quotient iteration", "author": ["Volodymyr Kuleshov"], "venue": null, "citeRegEx": "Kuleshov.,? \\Q2013\\E", "shortCiteRegEx": "Kuleshov.", "year": 2013}, {"title": "Deflation methods for sparse pca", "author": ["L. Mackey"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Mackey.,? \\Q2009\\E", "shortCiteRegEx": "Mackey.", "year": 2009}, {"title": "On the eigenvalue power law. Randomization and approximation techniques in computer science, pages", "author": ["M. Mihail", "C. Papadimitriou"], "venue": null, "citeRegEx": "Mihail and Papadimitriou.,? \\Q2002\\E", "shortCiteRegEx": "Mihail and Papadimitriou.", "year": 2002}, {"title": "Generalized spectral bounds for sparse lda", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "Spectral bounds for sparse pca: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "Fast pixel/part selection with sparse eigenvectors", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "In Computer Vision,", "citeRegEx": "Moghaddam et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2007}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["H. Shen", "J.Z. Huang"], "venue": "Journal of multivariate analysis,", "citeRegEx": "Shen and Huang.,? \\Q2008\\E", "shortCiteRegEx": "Shen and Huang.", "year": 2008}, {"title": "Sparse eigen methods by dc programming", "author": ["B.K. Sriperumbudur", "D.A. Torres", "G.R.G. Lanckriet"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2007}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["X.T. Yuan", "T. Zhang"], "venue": "arXiv preprint arXiv:1112.2679,", "citeRegEx": "Yuan and Zhang.,? \\Q2011\\E", "shortCiteRegEx": "Yuan and Zhang.", "year": 2011}, {"title": "Large-scale sparse principal component analysis with application to text data", "author": ["Y. Zhang", "L. El Ghaoui"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang and Ghaoui.,? \\Q2011\\E", "shortCiteRegEx": "Zhang and Ghaoui.", "year": 2011}, {"title": "Sparse pca: Convex relaxations, algorithms and applications", "author": ["Y. Zhang", "A. d\u2019Aspremont", "L.E. Ghaoui"], "venue": "Handbook on Semidefinite, Conic and Polynomial Optimization,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 14, "context": ") using the few keywords in their support (Gawalt et al., 2010; Zhang and El Ghaoui, 2011).", "startOffset": 42, "endOffset": 90}, {"referenceID": 31, "context": "Specifically, we find a constant \u03b40 such that for all sparsity levels k > \u03b40 n we obtain a constant approximation ratio for sparse PCA, partially solving the open problem discussed in (Zhang et al., 2012; d\u2019Aspremont et al., 2012).", "startOffset": 184, "endOffset": 230}, {"referenceID": 11, "context": "Specifically, we find a constant \u03b40 such that for all sparsity levels k > \u03b40 n we obtain a constant approximation ratio for sparse PCA, partially solving the open problem discussed in (Zhang et al., 2012; d\u2019Aspremont et al., 2012).", "startOffset": 184, "endOffset": 230}, {"referenceID": 20, "context": "This framework was introduced by the foundational work of (Karystinos and Liavas, 2010) in the context of solving quadratic form maximization problems over\u00b11 vectors.", "startOffset": 58, "endOffset": 87}, {"referenceID": 1, "context": "This framework was consequently used in (Asteris et al., 2011) to develop a constant rank solver that computes", "startOffset": 40, "endOffset": 62}, {"referenceID": 1, "context": "We use as a subroutine a modified version of the solver of (Asteris et al., 2011), to examine a polynomial number of special vectors that lead to a sparse principal component which admits provable performance.", "startOffset": 59, "endOffset": 81}, {"referenceID": 19, "context": "Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995).", "startOffset": 113, "endOffset": 170}, {"referenceID": 16, "context": "Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995).", "startOffset": 113, "endOffset": 170}, {"referenceID": 5, "context": "Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995).", "startOffset": 113, "endOffset": 170}, {"referenceID": 17, "context": "Then, a modified PCA technique based on the LASSO (SCoTLASS) was introduced in (Jolliffe et al., 2003).", "startOffset": 79, "endOffset": 102}, {"referenceID": 32, "context": "In (Zou et al., 2006), a nonconvex regression-type approximation, penalized \u00e0 la LASSO was used to produce sparse PCs.", "startOffset": 3, "endOffset": 21}, {"referenceID": 28, "context": "A nonconvex technique was presented in (Sriperumbudur et al., 2007).", "startOffset": 39, "endOffset": 67}, {"referenceID": 26, "context": ", 2006b), the authors used spectral arguments to motivate a greedy branch-and-bound approach, further explored in (Moghaddam et al., 2007).", "startOffset": 114, "endOffset": 138}, {"referenceID": 27, "context": "In (Shen and Huang, 2008), a similar technique to SVD was used employing sparsity penalties on each round of projections.", "startOffset": 3, "endOffset": 25}, {"referenceID": 31, "context": "A significant body of work based on semidefinite programming (SDP) approaches was established in (d\u2019Aspremont et al., 2007a; Zhang et al., 2012; d\u2019Aspremont et al., 2008).", "startOffset": 97, "endOffset": 170}, {"referenceID": 10, "context": "A significant body of work based on semidefinite programming (SDP) approaches was established in (d\u2019Aspremont et al., 2007a; Zhang et al., 2012; d\u2019Aspremont et al., 2008).", "startOffset": 97, "endOffset": 170}, {"referenceID": 18, "context": "A variation of the power method was used in (Journ\u00e9e et al., 2010).", "startOffset": 44, "endOffset": 66}, {"referenceID": 22, "context": "When computing multiple PCs, the issue of deflation arises as discussed in (Mackey, 2009).", "startOffset": 75, "endOffset": 89}, {"referenceID": 29, "context": "In (Yuan and Zhang, 2011), the authors introduced a very efficient sparse PCA approximation based on truncating the well-known power method to obtain the exact level of sparsity desired.", "startOffset": 3, "endOffset": 25}, {"referenceID": 21, "context": "A fast algorithm based on Rayleigh quotient iteration was developed in (Kuleshov, 2013).", "startOffset": 71, "endOffset": 87}, {"referenceID": 0, "context": "In (Amini and Wainwright, 2008), the first theoretical optimality guarantees were established under the spiked covariance for diagonal thresholding and the SDP relaxation of (d\u2019Aspremont et al.", "startOffset": 3, "endOffset": 31}, {"referenceID": 29, "context": "In (Yuan and Zhang, 2011), the authors provide peformance guarantees for the truncated power method under specific assumptions of data model, similar to the restricted isometry property.", "startOffset": 3, "endOffset": 25}, {"referenceID": 11, "context": "In (d\u2019Aspremont et al., 2012) the authors provide detection guarantees under the single spike covariance model.", "startOffset": 3, "endOffset": 29}, {"referenceID": 6, "context": "Then, in (Cai et al., 2012) and (Cai et al.", "startOffset": 9, "endOffset": 27}, {"referenceID": 7, "context": ", 2012) and (Cai et al., 2013) the authors provide guarantees under the assumption of multiple spikes in the covariance.", "startOffset": 12, "endOffset": 30}, {"referenceID": 2, "context": "It is also suspected that it is computationally challenging to recover the sparse spikes of a spiked covariance model, under optimal sample complexity as was shown in (Berthet and Rigollet, 2013b), (Berthet and Rigollet, 2013a), and (Berthet and Rigollet, 2012).", "startOffset": 233, "endOffset": 261}, {"referenceID": 0, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 29, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 11, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 7, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 1, "context": "However, we show how sparse PCA can be efficiently solved on Ad if the rank d is constant with respect to n, using the machinery of (Asteris et al., 2011).", "startOffset": 132, "endOffset": 154}, {"referenceID": 1, "context": "2 Rank-2 case Now we describe how to compute S2 using the constant rank solver of (Asteris et al., 2011).", "startOffset": 82, "endOffset": 104}, {"referenceID": 20, "context": "1 Spherical variables and the spannogram Here we use a transformation of our problem space into a 2-dimensional space as was done in (Karystinos and Liavas, 2010).", "startOffset": 133, "endOffset": 162}, {"referenceID": 1, "context": "3This is a special case of the general d dimensional lemma of (Asteris et al., 2011) (found in the Appendix), but we prove the special case to simplify the presentation.", "startOffset": 62, "endOffset": 84}, {"referenceID": 18, "context": ", 2007b), the generalized power method (GPower) of (Journ\u00e9e et al., 2010), and the truncated power method (TPower) of (Yuan and Zhang, 2011).", "startOffset": 51, "endOffset": 73}, {"referenceID": 29, "context": ", 2010), and the truncated power method (TPower) of (Yuan and Zhang, 2011).", "startOffset": 52, "endOffset": 74}, {"referenceID": 10, "context": ", 2007a), since the FullPath algorithm is experimentally shown to have similar or better performance (d\u2019Aspremont et al., 2008).", "startOffset": 101, "endOffset": 127}, {"referenceID": 27, "context": "1 Spiked Covariance Recovery We first test our approximation algorithm on an artificial data set generated in the same manner as in (Shen and Huang, 2008; Yuan and Zhang, 2011).", "startOffset": 132, "endOffset": 176}, {"referenceID": 29, "context": "1 Spiked Covariance Recovery We first test our approximation algorithm on an artificial data set generated in the same manner as in (Shen and Huang, 2008; Yuan and Zhang, 2011).", "startOffset": 132, "endOffset": 176}, {"referenceID": 22, "context": "We use the projection deflation method (Mackey, 2009) to obtain A\u2032 = (I \u2212 \u1e7d1\u1e7d 1 )A(I \u2212 \u1e7d1\u1e7d 1 ) and work on it to obtain \u1e7d2, the second estimated eigenvector of \u03a3.", "startOffset": 39, "endOffset": 53}, {"referenceID": 0, "context": "Interesting tradeoffs of sample complexity and probability of recovery where derived in (Amini and Wainwright, 2008).", "startOffset": 88, "endOffset": 116}, {"referenceID": 29, "context": "In the same manner as in the relevant sparse PCA literature, we evaluate our approximation on two gene expression data-sets used in (d\u2019Aspremont et al., 2007b, 2008; Yuan and Zhang, 2011).", "startOffset": 132, "endOffset": 187}, {"referenceID": 10, "context": "We also plot the performance outer bound derived in (d\u2019Aspremont et al., 2008).", "startOffset": 52, "endOffset": 78}, {"referenceID": 13, "context": "Empirical results have been reported that indicate power-law like decays for eigenvalues where no cutoff is observed (Dhillon and Modha, 2001) and some derived power-law generative models for 0/1 matrices (Mihail and Papadimitriou, 2002; Chung et al.", "startOffset": 117, "endOffset": 142}, {"referenceID": 23, "context": "Empirical results have been reported that indicate power-law like decays for eigenvalues where no cutoff is observed (Dhillon and Modha, 2001) and some derived power-law generative models for 0/1 matrices (Mihail and Papadimitriou, 2002; Chung et al., 2003).", "startOffset": 205, "endOffset": 257}, {"referenceID": 8, "context": "Empirical results have been reported that indicate power-law like decays for eigenvalues where no cutoff is observed (Dhillon and Modha, 2001) and some derived power-law generative models for 0/1 matrices (Mihail and Papadimitriou, 2002; Chung et al., 2003).", "startOffset": 205, "endOffset": 257}], "year": 2014, "abstractText": "We introduce a novel algorithm that computes the k-sparse principal component of a positive semidefinite matrix A. Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of A. We obtain provable approximation guarantees that depend on the spectral decay profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation. For example, if the eigenvalues of A follow a power-law decay, we obtain a polynomial-time approximation algorithm for any desired accuracy. A key algorithmic component of our scheme is a combinatorial feature elimination step that is provably safe and in practice significantly reduces the running complexity of our algorithm. We implement our algorithm and test it on multiple artificial and real data sets. Due to the feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets.", "creator": "LaTeX with hyperref package"}}}