{"id": "1303.2789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2013", "title": "A Cooperative Q-learning Approach for Real-time Power Allocation in Femtocell Networks", "abstract": "In this paper, we address the problem of distributed interference management of cognitive femtocells that share the same frequency range with macrocells (primary user) using distributed multi-agent Q-learning. We formulate and solve three problems representing three different Q-learning algorithms: namely, centralized, distributed and partially distributed power control using Q-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest, characterizes the global optimum for RLP-based Q-learning in the clinical setting (M. P. A. (2007), in contrast to standard Q-learning (M. P. A. (2011), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2013), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast to standard Q-learning (M. P. A. (2014), in contrast", "histories": [["v1", "Tue, 12 Mar 2013 07:00:04 GMT  (1500kb)", "http://arxiv.org/abs/1303.2789v1", null]], "reviews": [], "SUBJECTS": "cs.MA cs.LG", "authors": ["hussein saad", "amr mohamed", "tamer elbatt"], "accepted": false, "id": "1303.2789"}, "pdf": {"name": "1303.2789.pdf", "metadata": {"source": "CRF", "title": "A Cooperative Q-learning Approach for Real-time Power Allocation in Femtocell Networks", "authors": ["Hussein Saad", "Amr Mohamed"], "emails": ["hussein.saad@nileu.edu.eg", "amrm@qu.edu.qa", "telbatt@nileuniversity.edu.eg"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n27 89\nv1 [\ncs .M\nA ]\n1 2\nM ar\n2 01\nI. INTRODUCTION\nFemtocells have been recently proposed as a promising solution to the indoor coverage problem. Although femtocells offer significant benefits to both the operator and the user, several challenges have to be solved to fully reap these benefits. One of the most daunting challenges is their interference on macro-users and other femtocells [1], [2]. Typically, femtocells are installed by the end user and hence, their number and positions are random and unknown to the network operator a priori. Adding to this the typical dynamics of the wireless environment, a centralized approach to handle the interference problem can not be feasible which, in turn, calls for a distributed interference management strategies.\nBased on these observations, in this paper, we focus on closed access femtocells [3] working in the same bandwidth with macrocells (i.e. cognitive femtocells), where the femtocells will be the secondary users who try to perform power control to maximize their own performance while maintain the macrocell capacity at certain level. In order to handle the interference generated by the femtocells on the macrocell users, we will use a distributed reinforcement learning [4] technique called multi-agent Q-learning [5] and [6]. In our\ncontext, a prior model of the environment cannot be achieved due to 1) the unplanned placement of the femtocells, 2) the typical dynamics of the wireless enshrinement. In such context, Q-Learning offers significant advantages to achieve optimal decision policies through realtime learning of the environment [7].\nIn the literature, Q-learning has been used several times to perform power allocation in femtocell networks. In [8], authors used independent learning (IL) Q-learning to perform power allocation in order to control the interference generated by the femtocells on the macrocell user. In [7], authors introduced a new concept called docitive femtocells where a new femtocell can fasten its learning process by learning the policies acquired by the already deployed femtocells, instead of learning from scratch. The policies are shared by Q-table exchange between the femtocells. However, after the Q-tables are exchanged, all the femtocells take their actions (powers) independently, which may generate an oscillating behavior in the system. In [9], we developed a distributed power allocation algorithm called distributed power control using Q-learning (DPC-Q). In DPC-Q, two different learning paradigms were proposed: independent learning (IL) and cooperative learning (CL). It was shown that both paradigms achieves convergence. Moreover, the CL paradigm outperforms the IL one through achieving higher aggregate femtocells capacity and better fairness (in terms of capacity) among the learning femtocells.\nHowever, in [9] we did not evaluate the performance of DPC-Q against the networks dynamics, specially after convergence. Also, we did not have any benchmarking algorithm to compare the performance of DPC-Q to. Thus, the contribution of this paper can be summed up as follows:\n\u2022 we propose two new Q-learning based power allocation algorithms: namely, centralized power control using Qlearning (CPC-Q) and partially distributed power control using Q-learning (PDPC-Q). CPC-Q is used for benchmarking purposes, where a central controller, which has all the information about the system (channel gains of all femtocells, system noise, \u00b7 \u00b7 \u00b7 ), is responsible for calculating the optimal powers that the femtocells should use. PDPC-Q, which is an agent based power control algorithm, is proposed as: 1) it gives the operator the flexibility to work on a global base (e.g. aggregate femtocell capacity instead of subcarrier based femtocell\ncapacity as in DPC-Q), 2) it makes DPC-Q comparable to CPC-Q. \u2022 we evaluate the robustness and scalability of DPC-Q, in both IL and CL paradigms, against two of the dynamics that typically exist in the wireless environment: namely, the random activity of femtocells (when new femtocells are deployed in the system during the learning process) and the density of the femtocells in the macrocell coverage area (the number of femtocells that are interfering on the macro users). \u2022 we compare our proposed DPC-Q in both IL and CL paradigms to the idea of docitive femtocells presented in [7].\nThe rest of this paper is organized as follows. In section II, the system model is described. Section III presents a brief background about multi-agent Q-learning. In section IV, the proposed Q-learning based power allocation algorithms are presented. The simulation scenario and the results are discussed in section V. Finally the conclusions are drawn in section VI."}, {"heading": "II. SYSTEM MODEL", "text": "We consider a wireless network composed of one macro cell (with one single transmit and receive antenna Macro Base Station (MBS)) that coexists with Nf femtocells, each with one single transmit and receive antenna Femto Base Station (FBS). The Nf femtocells are placed indoors within the macrocell coverage area. Both the MBS and the FBSs\u2019 transmit over the same K subcarriers where orthogonal downlink transmission is assumed. Um and Uf macro and femto users are located randomly inside the macro and femto cells respectively. Femtocells within the same range can share partial information during the learning process to enhance their performance. p (k) o and p (k) n denote the transmission powers of the MBS and FBS n on subcarrier k respectively. Moreover, the maximum transmission powers for the MBS and any FBS n are Pmmax and P f max respectively, where \u2211K k=1 p (k) o \u2264 Pmmax and \u2211K\nk=1 p (k) n \u2264 P fmax.\nThe system performance is analyzed in terms of the capacity measured in (bits/sec/Hz). The capacity achieved by the MBS at its associated user on subcarrier k is:\nC(k)o = log2(1 + h (k) oo p (k) o\n\u2211Nf n=1 h (k) no p (k) n + \u03c32\n) (1)\nwhere h(k)oo indicates the channel gain between the transmitting MBS and its associated user on subcarrier k; h(k)no indicates the channel gain between FBS n transmitting on subcarrier k and the macro user. Finally \u03c32 indicates the noise power. The capacity achieved by FBS n at its associated user on subcarrier k is:\nC(k)n = log2(1+ h (k) nnp (k) n\n\u2211Nf n\u2032=1,n\u2032 6=n h (k) n\u2032np (k) n\u2032 + h (k) on p (k) o + \u03c32\n) (2)\nwhere h(k)nn indicates the channel gain between FBS n transmitting on subcarrier k and its associated user; h(k)n\u2032n\nindicates the channel gain between FBS n\u2032 transmitting on subcarrier k and the femto user associated to FBS n."}, {"heading": "III. MULTI-AGENT Q-LEARNING", "text": "The scenario of distributed cognitive femtocells can be mathematically formulated using stochastic games [10], where the learning process of each femtocell is described by a task defined by the quintuple {N,S,A, P,R(s,~a)}, where:\n\u2022 N = {1, 2, \u00b7 \u00b7 \u00b7 , Nf} is the set of agents (i.e. femtocells). \u2022 S = {s1, s2, \u00b7 \u00b7 \u00b7 , sm} is the set of possible states that\neach agent can occupy, where m is the number of possible states. \u2022 A = {a1, a2, \u00b7 \u00b7 \u00b7 , al} is the set of possible actions that each agent can perform for each task, where l is the number of possible actions. \u2022 P is the probabilistic transition function that defines the probability that an agent transits from one state to another, given the joint action performed by all agents. \u2022 R(s,~a) is the reward function that determines the reward fed back to an agent n by the environment when the joint action ~a is performed in state s \u2208 S.\nIn the distributed cognitive femtocells scenario, P can not be deduced due to the dynamics of the wireless environment. Thus, one of the most famous techniques that calculates optimal policies without any prior model of the environment is Q-learning . Q-learning assigns each task of each agent a Q-table whose entries are known as Q-values Q(sm, al), for each state sm \u2208 S and action al \u2208 A. Thus, the dimension of this table is m \u00d7 l. The Q-value Q(sm, al) is defined to be the expected discounted reward over an infinite time when action al is performed in state sm, and an optimal policy is followed thereafter [5]. The learning process of each agent n at time t can be described as follows: 1) the agent senses the environment and observes its current state snm \u2208 S, 2) based on snm, the agent selects its action a n l randomly with probability \u01eb or according to: anl = argmaxa\u2208A Q t n(s n m, a) with probability 1\u2212 \u01eb, where Qtn(sm, a) is the row of the Qtable of agent n that corresponds to state snm at time t, and \u01eb is an exploration parameter (a random number) that guarantees that all the state-action pairs of the Q-table is visited at least once, 3) the environment makes a transition to a new state snm\u2032 \u2208 S and the agent receives a reward r t n = R(s n m,~a) due to this transition, 4) the Q-value is updated using equation 3 and the process is repeated.\nQt+1n (s n m, a n l ) :=(1\u2212 \u03b1)Q t n(s n m, a n l )+\n\u03b1(rtn + \u03b3max a \u2032\u2208A Qtn(s n m\u2032 , a\n\u2032 )) (3)\nwhere \u03b1 is called the learning rate and 0 \u2264 \u03b3 \u2264 1 is the discount factor that determines how much effect the future rewards have on the decisions at each moment. It should be noticed that the reward rtn depends on the joint action ~a of all agents not on the individual action al. This is the main difference between the multi-agent scenario described here and the single-agent one (when Nf = 1). In the single-agent case, one of the conditions needed to guarantee that the Qvalues converges to the optimal ones is that: the reward of the\nagent must be dependent only on its individual actions (i.e. the reward function is stationary for each state-action pair) [5], [11]. However, for the multi-agent scenario, the reward function is not stationary from the agent point of view, since it now depends on the actions of other agents. Thus, the convergence proof used for the single-agent case can not be used in the multi-agent one."}, {"heading": "IV. POWER ALLOCATION USING Q-LEARNING", "text": "In this section, the three proposed Q-learning based power allocation algorithms will be presented:"}, {"heading": "A. Distributed Power Control Using Q-learning (DPC-Q)", "text": "DPC-Q is a distributed algorithm where multiple agents (i.e: femtocells) aim at learning a sub-optimal decision policy (i.e: power allocation) by repeatedly interacting with the environment. The DPC-Q algorithm is proposed in two different learning paradigms:\n\u2022 Independent learning (IL): In this paradigm, each agent learns independently from other agents (i.e: ignores other agents\u2019 actions and considers other agents as part of the environment). Although, this may lead to oscillations and convergence problems, the IL paradigm showed good results in many applications [8]. \u2022 Cooperative learning (CL): In this paradigm, each agent shares a portion of its Q-table with all other cooperating agents1, aiming at enhancing the femtocells\u2019 performance. CL is performed as follows: each agent shares the row of its Q-table that corresponds to its current state with all other cooperating agents (i.e. femtocells in the same range). Then, each agent n selects its action anl according to the following equation:\nanl = argmax a\u2208A\n( \u2211\n1\u2264n\u2032\u2264Nf\nQn\u2032(s n\u2032 , a)) (4)\nThe main idea behind this strategy is explained in details in [9]. In terms of overhead, if the number of femtocells is Nf , then the total overhead needed is Nf .(Nf \u2212 1) messages (each of size l) per unit time (i.e. the overhead is quadratic in the number of cooperating femtocells).\nDPC-Q is an agent and subcarrier based algorithm (i.e. the capacities, states, actions, reward functions are defined for each agent over each subcarrier) [9]:\n\u2022 Agents: FBSn, \u22001 \u2264 n \u2264 Nf \u2022 States: At time t for femtocell n on subcarrier k, the\nstate is defined as: sn,kt = {I k t ,P n t } where I k t \u2208 {0, 1} indicates the level of interference measured at the macrouser on subcarrier k at time t:\nIkt =\n{\n1, C (k) o < \u0393o 0, C (k) o \u2265 \u0393o\n(5)\nwhere \u0393o is the target capacity determining the QoS performance of the macrocell. We assume that the macrocell\n1We assume that the shared row of the Q-table is put in the control bits of the packets transmitted between the femtocells. The details of the exact protocol lie out of the scope of this paper.\nreports the value of C(k)o to all FBSs through the backhaul connection. Pnt defines the power levels used to quantize the total power FBS n is using for transmission at time t:\nPnt =\n\n \n \n0, \u2211K k=0 p n,k t < (P f max \u2212A1) 1, (P fmax \u2212A2) \u2264 \u2211K k=0 p n,k t \u2264 P f max 2, \u2211K\nk=0 p n,k t > P f max\n(6)\nwhere A1 and A2 are arbitrary selected thresholds (several values for A1 and A2 as well as more power levels were tried through the simulations and the performance gain between these values was marginal). \u2022 Actions: The action here is scalar, where the set of actions available for each FBS is defined as the set of possible powers that a FBS can use for transmission on each subcarrier. In the simulations, a range from \u221220 to P fmax dBm with step of 2 dBm is used. \u2022 Reward Functions: The reward fed back to agent n on subcarrier k at time t is defined as:\nr n,k t =\n{\ne\u2212(C (k) o \u2212\u0393 o)2 \u2212 e\u2212C (k) n , \u2211K k=0 p n,k t \u2264 P f max \u22122, \u2211K\nk=0 p n,k t > P f max\n(7) The rationale behind this reward function is that each femtocell will aim at maximizing its own capacity while: 1) maintaining the capacity of the macrocell around the target capacity \u0393o (convergence is assumed to be within a range of \u00b11 bits/sec/Hz from \u0393o), 2) not exceeding the allowed P fmax. This reward function was compared to the reward function defined in [9]:\nr n,k t =\n{\ne\u2212(C (k) o \u2212\u0393 o)2 , \u2211K k=0 p n,k t \u2264 P f max \u22121, \u2211K\nk=0 p n,k t > P f max\n(8)\nwhere it was shown that both reward functions maintain the capacity of the macrocell within the convergence range. However, reward function 7 was able to achieve higher aggregate femtocell capacity. In this paper, we show another advantage for reward function 7, which is: it learns (explores or reacts to network dynamics) better than reward function 8 even when the exploration parameter \u01eb is not used. This mainly depends on the initial value of the Q-values. In this paper, we initially set all the Q-values to zero. Thus, when \u01eb is not used, using reward function 8 will always feed the agent back with a positive reward (given that P fmax is not exceeded). Thus, if initially agent n was in state sn,kt on subcarrier k and took action pn,kt , the Q-value of this action Qn(s\nn,k, pn,k) will be updated using a positive valued reward, thus this Q-value will increase with time, and agent n will keep using the same action forever (since the action is chosen according to the maximum Q-value). Thus, using \u01eb with reward function 8 is a must to have better exploration behavior. On the other hand, using reward function 7 may feed the agent back with positive or negative valued rewards (e\u2212(C (k) o \u2212\u0393 o)2 could be smaller than e\u2212C (k) n ). Thus, given the same initial conditions, agent n could\nreceive a negative valued reward after taking action pn,kt , leading to the decrease of its Q-value with time. Once the Q-value decreases below zero, the agent will take another action whose Q-value is greater than the decreased one. Thus, reward function 7 learns (explores) better than reward function 8.\nIn this paper, we also evaluate the robustness and scalability of DPC-Q, in both IL and CL paradigms. We believe that the CL paradigm is much more robust and scalable against the network dynamics compared to the IL paradigm. The reason is that after sharing the row of the Q-table, each femtocell will know the states that all other cooperating femtocells are occupying, and since a state at a certain moment can be defined as: how the agent sees the environment at that moment, each femtocell can implicitly know 1) how all other femtocells can react to the network dynamics, 2) what actions other femtocells are going to take. However, if the femtocells took their actions independently (i.e. IL paradigm), even after knowing the states of each other, oscillating behaviors that may not reach convergence may be generated. One way to overcome this problem is to force the femtocells to make use of the information shared while taking their actions (i.e. taking the actions cooperatively: equation 4). This could decrease the oscillations in the system, making the femtocells more robust towards the increase of the number of deployed femtocells, and towards the sudden effect caused by any new deployed femtocell."}, {"heading": "B. Partially Distributed Power Control Using Q-learning (PDPC-Q)", "text": "PDPC-Q is a partial distributed algorithm, where it is a multi-agent algorithm but only agent dependent (i.e. the states, actions, reward functions are defined for each agent over all subcarriers). As DPC-Q, PDPC-Q works in both IL and CL paradigms. The agents, states, actions and reward functions used for the PDPC-Q algorithm are defined as follows:\n\u2022 Agents: FBSn, \u22001 \u2264 n \u2264 Nf \u2022 States: At time t the state is defined as: st = {It} where\nIt \u2208 {0, 1} indicates the level of interference measured at the macro-user over all subcarriers at time t:\nIt =\n{\n1, Co < \u03b2 o 0, Co \u2265 \u03b2 o\n(9)\nwhere Co = \u2211K k=1 C (k) o is the aggregate macrocell\ncapacity and \u03b2o is the target aggregate macrocell capacity. \u2022 Actions: For FBS n, the set of actions is defined to be\na set of vectors where each vector represents the powers FBS n is using on all subcarriers. \u2022 Reward Functions: Reward function 7 can be redefined as:\nrnt = e \u2212(Co\u2212\u03b2\no)2 \u2212 e\u2212Cn (10)\nwhere Cn = \u2211K k=1 C (k) n is the aggregate capacity of FBS n. Note that since PDPC-Q is not subcarrier based, a power vector in which P fmax is exceeded will never be assigned for any FBS. Thus, there is no need to put a"}, {"heading": "C. Centralized Power Control Using Q-learning (CPC-Q)", "text": "CPC-Q is a centralized power control algorithm used to evaluate the performance of our proposed DPC-Q algorithm. CPC-Q can be regarded as the single-agent version of the DPC-Q, and hence, its convergence to the optimal Q-values and thus optimal powers is guaranteed. However, using a centralized controller is not feasible in terms of overhead in multi-agent scenarios. Thus, CPC-Q works only for small scale problems. The agent, states, actions and reward functions used for CPC-Q are defined as follows:\n\u2022 Agents: A centralized controller. \u2022 States: The same as PDPC-Q. \u2022 Actions: For the central controller, the set of actions\nis defined to be a set of matrices where each matrix represents the powers of all femtocells over all subcarriers. However, the size of this set grows exponentially with both the number of femtocells and the number of subcarriers. Thus, forming the matrices (all possible actions) from a large set of powers such as the one used in DPC-Q will be infeasible2. \u2022 Reward Functions: Since CPC-Q is global, reward function 7 can be redefined as:\nrt = e \u2212(Co\u2212\u03b2 o)2 \u2212 e\u2212Cfemto (11)\nwhere Cfemto is defined as Cfemto = \u2211Nf\nn=1\n\u2211K k=1 C (k) n .\nFinally, for the rest of the paper, reward functions 7, 11 and 10 will be referred to as R1, while reward function 8 will be referred to as R0. The three proposed algorithms are compared qualitatively in table I."}, {"heading": "V. PERFORMANCE EVALUATION", "text": ""}, {"heading": "A. Simulation Scenario", "text": "We consider a wireless network consisting of one macrocell serving Um = 1 macro user underlaid with Nf femtocells. Each femtocell serves Uf = 1 femto-user, which is randomly located in the femtocell coverage area. All of the macro\n2In the simulations, the set of powers used to form the matrices and the vectors in CPC-Q and PDPC-Q respectively is: {0, 6, 12} dBm.\nand femto cells share the same frequency band composed of K subcarriers, where orthogonal downlink transmission is assumed. In the simulations, K will change according to the algorithm used: for DPC-Q, K = 6, while for both CPC-Q and PDPC-Q, K = 3. The channel gain between any transmitter i and any receiver j on subcarrier k is assumed to be path-loss dominated and is given by:\nh (k) ij = d (\u2212PL) ij (12)\nwhere dij is the physical distance between transmitter i and receiver j, and PL is the path loss exponent. In the simulations PL = 2 is used. The distances are calculated according to the following assumptions: 1) The maximum distance between the MBS and its associated user is set to 1000 meters, 2) The maximum distance between the MBS and a femto-user is set to 800 meters, 3) The maximum distance between a FBS and its associated user is set to 80 meters, 4)The maximum distance between a FBS and another femtocell\u2019s user is set to 300 meters, 5) The maximum distance between a FBS and the macro-user is set to 800 meters.\nWe used MatLab on a cluster computing facility with 300 cores to simulate such scenario, where in the simulations we set the noise power \u03c32 to 10\u22127, the maximum transmission power of the macrocell Pmmax to 43 dBm, the maximum transmission power of each femtocell P fmax to 15 dBm, each of the power levels A1 and A2 is set to 5 dBm, the learning rate \u03b1 to 0.5, the discounted rate \u03b3 to 0.9 and the random number \u01eb to 0.1 [7] and [9]."}, {"heading": "B. Numerical Results", "text": "Figure 1(a) shows the aggregate femtocells capacity (as a function of the number of femtocells) using CPC-Q and PDPC-Q with R1 in both IL and CL paradigms. It can be observed that CL is much better than IL, where from the figure it can be shown that the aggregate capacity gain of CPC-Q over PDPC-Q in case of CL is marginal. Since CPC-Q is considered the single agent version of DPC-Q, it should converge to the global optimal values. This is shown in the figure at small number of femtocells (Nf = 1 and 2). The optimal values are calculated using exhaustive search over all possible actions, where the optimal value is defined to be the maximum aggregate capacity the system can achieve while maintaining the capacity of the macrocell in the convergence range (\u00b11 bits/sec/Hz from \u03b2o). However, starting from Nf = 3, CPC-Q begins to be infeasible since the size of the possible actions set A becomes very large (at Nf = 5: |A| = 320, 000, 0). So, besides the computational problems, the condition of visiting all state-action pair becomes infeasible. Thus, getting the optimal value is also not feasible (that\u2019s why we stopped CPC-Q at Nf = 4). Note also that we stopped the exhaustive search at Nf = 5 due to complexity and memory problems, while PDPC-Q is shown at Nf = 6 and 7 just to illustrate the continuity of our algorithm.\nFigures 2 and 3 show the robustness of the proposed DPCQ algorithm. In these figures, we started with Nf = 5, then we added a new femtocell after every 4000 iterations3to reach\nNf = 29 at the 96000th iteration. Finally, we add another femtocell at the 99000th iteration. The figures show how DPCQ using the CL paradigm is more robust to the deployment of new femtocells compared to the IL paradigm. Moreover, in these figures we compare the performance of DPC-Q to the docitive idea presented in [7]. We investigated two cases: 1) the already deployed femtocells share their Q-tables with the new femtocells when they first join the system (suffixed with share on the figure), 2) the new deployed femtocells starts with a zero initialized Q-tables (suffixed with scratch on the figure). Figure 2 shows the macrocell convergence on a certain subcarrier using DPC-Q with R1 in both IL and CL paradigms, where it can be observed that the CL paradigm maintains the macrocell capacity within the range of convergence (6 \u00b1 1 bits/sec/Hz) and reacts well to the effect of the new deployed femtocells, without the need to have a learning phase again every time a new femtocell is deployed, which is a very interesting observation. It can also be observed that our proposed CL paradigm converges to the same value regardless the already deployed femtocells shared its Q-tables with the new ones or not. So, sharing could be ignored, thus decreasing the overall overhead. On the other hand, the IL paradigm showed a very bad reaction to the network dynamics, where 1) convergence is not attained (i.e. an oscillating behavior is generated), 2) as Nf increases, IL paradigm may push the macrocell capacity out of the convergence range when the network becomes more dense. Thus, CL is more scalable than IL. However, it can be noticed that the docitive idea is useful in the IL paradigm, where sharing the Q-tables of the already deployed femtocells with the new ones is much better (in terms of the value that the macrocell capacity oscillates around) than beginning with zero-initialized (scratch) Q-tables. In terms of speed of convergence, it can be noticed that, although the learning process may need large number of iterations initially, CL decreases the dynamics of the learning process, and hence, making it faster. This can be noticed from the figure, where\nCL converged almost at the 300th iteration, which is much earlier than the IL paradigm. Also, after the deployment of each new femtocell, CL took less than 10 iterations only - around 0.01 seconds - to re-achieve convergence.\nFigure 3 shows the aggregate femtocells capacity over the learning iterations. It can be noticed that using the CL paradigm, the aggregate capacity increases as more femtocells are deployed in the network, while in the IL paradigms, since convergence is already not maintained, the aggregate capacity behavior has a sporadic behavior, which indicates clearly that IL is not efficient to react to the network dynamics. However, in the CL paradigm from the 64000th to the 72000th iteration(640th to 720th according to figure\u2019s scale), it can be noticed that the aggregate capacity decreases. The reason is that as more femtocells are being deployed, the network becomes very dense and since using the CL paradigm makes the cooperating femtocells use the same powers, this may force the macrocell capacity to violate the range of convergence. Thus, all the femtocells will have to decrease the power used to maintain again the macrocell capacity within the range of convergence leading to the decrease of their aggregate capacity. Note that at the 64000th and 72000th iterations, \u01eb is already removed, which proves that R1 learns well even when \u01eb is removed.\nFinally, in order to compare the aggregate capacity the CL paradigm achieves, after the incremental deployment of femtocells, to the ideal value, we used the small scale problem again. This is shown in figure 1(b), where we started with Nf = 2 and added an extra femtocell at the 8000th, 12000th and 13500th iterations4. Again, it can be observed that CL achieves aggregate capacity that is very close to the optimal one while the IL paradigm is far from it.\n3In figures 2 and 3, \u01eb is removed at the 50000th iteration and the figures were drawn with step = 100 in order to achieve better resolution.\n4In figure 1(b), \u01eb is removed at the 12000th iteration and the figure was drawn with step = 10 in order to achieve better resolution."}, {"heading": "VI. CONCLUSION", "text": "In this paper, three Q-learning based power allocation algorithms for cognitive femtocells scenario are presented: namely, DPC-Q, CPC-Q and PDPC-Q. Although DPC-Q was presented in previous work, in this paper it is extended, in both of its learning paradigms: IL and CL, to evaluate its performance, robustness and scalability. In terms of performance, DPC-Q is extended to PDPC-Q and then compared to CPC-Q, where the simulations showed that the CL paradigm outperforms the IL and achieves aggregate femtocell capacity that is very close the optimum one. In terms of robustness, the CL paradigm was found to be much more robust against the deployment of new femtocells during the learning process, where the results showed that the CL paradigm outperforms the IL paradigm in: 1) maintaining convergence, 2) learning better (i.e. reacting better to the network dynamics), especially when a suitable reward function such as the one defined in the simulations is used, 3) converging to the target capacity regardless the old femtocells share their experience (i.e. Q-tables) with the new deployed ones or not and 4) speeding up the convergence. Finally, in terms of scalability, CL paradigm reacted better to the network dynamics and maintained convergence, even when the number of the femtocells is large ."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by the Qatar Telecom (Qtel) Grant No.QUEX-Qtel-09/10-10."}], "references": [{"title": "Femtocell networks: a survey", "author": ["V. Chandrasekhar", "J. Andrews", "A. Gatherer"], "venue": "Communications Magazine, IEEE, vol. 46, September 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Femtocells: Opportunities and Challenges for Business and Technology", "author": ["S. Saunders", "S. Carlaw", "A. Giustina"], "venue": "Great Britain: John Wiley and Sons Ltd,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Open vs closed access femtocells in the uplink", "author": ["P. Xia", "V. Chandrasekhar", "J.G. Andrews"], "venue": "CoRR, vol. abs/1002.2964, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge MA, MIT press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Technical note Q-learning", "author": ["J.C.H. Watkins", "P. Dayan"], "venue": "Journal of Machine Learning, vol. 8, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Collaborative multiagent reinforcement learning by payoff propagation", "author": ["J.R. Kok", "N. Vlassis"], "venue": "J. Mach. Learn. Res., vol. 7, December 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Cognition and docition in OFDMA-based femtocell networks", "author": ["A. Galindo-Serrano", "L. Giupponi", "M. Dohler"], "venue": "Proceeding of the IEEE Global Telecommunications Conference (GLOBECOM), may 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed Q-learning for interference control in OFDMA-based femtocell networks", "author": ["A. Galindo-Serrano", "L. Giupponi"], "venue": "Proceedings of the 71st IEEE Vehicular Technology Conference, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed cooperative Qlearning for power allocation in cognitive femtocell networks", "author": ["H. Saad", "A. Mohamed", "T. ElBatt"], "venue": "Proceedings of the IEEE 76th Vehicular Technology Conference, Sep. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Labeled initialized adaptive play qlearning for stochastic games", "author": ["A. Burkov", "B. Chaib-draa"], "venue": "Proceedings of the AAMAS\u201907 Workshop on Adaptive and Learning Agents (ALAg\u201907), May 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Convergence of Q-learning: A simple proof", "author": ["F.S. Melo"], "venue": "Institute Of Systems and Robotics, Tech. Rep., 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "One of the most daunting challenges is their interference on macro-users and other femtocells [1], [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "One of the most daunting challenges is their interference on macro-users and other femtocells [1], [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Based on these observations, in this paper, we focus on closed access femtocells [3] working in the same bandwidth with macrocells (i.", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "In order to handle the interference generated by the femtocells on the macrocell users, we will use a distributed reinforcement learning [4] technique called multi-agent Q-learning [5] and [6].", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "In order to handle the interference generated by the femtocells on the macrocell users, we will use a distributed reinforcement learning [4] technique called multi-agent Q-learning [5] and [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "In order to handle the interference generated by the femtocells on the macrocell users, we will use a distributed reinforcement learning [4] technique called multi-agent Q-learning [5] and [6].", "startOffset": 189, "endOffset": 192}, {"referenceID": 6, "context": "In such context, Q-Learning offers significant advantages to achieve optimal decision policies through realtime learning of the environment [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "In [8], authors used independent learning (IL) Q-learning to perform power allocation in order to control the interference generated by the femtocells on the macrocell user.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], authors introduced a new concept called docitive femtocells where a new femtocell can fasten its learning process by learning the policies acquired by the already deployed femtocells, instead of learning from scratch.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [9], we developed a distributed power allocation algorithm called distributed power control using Q-learning (DPC-Q).", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "However, in [9] we did not evaluate the performance of DPC-Q against the networks dynamics, specially after convergence.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "\u2022 we compare our proposed DPC-Q in both IL and CL paradigms to the idea of docitive femtocells presented in [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 9, "context": "The scenario of distributed cognitive femtocells can be mathematically formulated using stochastic games [10], where the learning process of each femtocell is described by a task defined by the quintuple {N,S,A, P,R(s,~a)}, where:", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "The Q-value Q(sm, al) is defined to be the expected discounted reward over an infinite time when action al is performed in state sm, and an optimal policy is followed thereafter [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "the reward function is stationary for each state-action pair) [5], [11].", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "the reward function is stationary for each state-action pair) [5], [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "Although, this may lead to oscillations and convergence problems, the IL paradigm showed good results in many applications [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "The main idea behind this strategy is explained in details in [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "the capacities, states, actions, reward functions are defined for each agent over each subcarrier) [9]:", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "This reward function was compared to the reward function defined in [9]:", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "1 [7] and [9].", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "1 [7] and [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Moreover, in these figures we compare the performance of DPC-Q to the docitive idea presented in [7].", "startOffset": 97, "endOffset": 100}], "year": 2013, "abstractText": "In this paper, we address the problem of distributed interference management of cognitive femtocells that share the same frequency range with macrocells (primary user) using distributed multi-agent Q-learning. We formulate and solve three problems representing three different Q-learning algorithms: namely, centralized, distributed and partially distributed power control using Q-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest, characterizes the global optimum. Each of DPC-Q and PDPC-Q works in two different learning paradigms: Independent (IL) and Cooperative (CL). The former is considered the simplest form for applying Qlearning in multi-agent scenarios, where all the femtocells learn independently. The latter is the proposed scheme in which femtocells share partial information during the learning process in order to strike a balance between practical relevance and performance. In terms of performance, the simulation results showed that the CL paradigm outperforms the IL paradigm and achieves an aggregate femtocells capacity that is very close to the optimal one. For the practical relevance issue, we evaluate the robustness and scalability of DPC-Q, in real time, by deploying new femtocells in the system during the learning process, where we showed that DPC-Q in the CL paradigm is scalable to large number of femtocells and more robust to the network dynamics compared to the IL paradigm.", "creator": "LaTeX with hyperref package"}}}