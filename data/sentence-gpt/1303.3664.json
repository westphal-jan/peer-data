{"id": "1303.3664", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2013", "title": "Topic Discovery through Data Dependent and Random Projections", "abstract": "We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic.\n\n\n\n\nWe can perform the same function using the simple equation:\n(1) (2) (3) (4) (5) (6)\nWe are interested in the relationship between topic models and the probability of the search to answer the problem. It is important to note that the fact that, despite the absence of a single word, we find many of the hypotheses that make the most sense, the most commonly used of these two propositions will remain in the literature as the most relevant. In particular, we can distinguish between a given value from that of other statements. This can result in a \"true\" outcome which can be regarded as a false, but may be a false, and a false. For example, the first argument can be considered \"true\" because it does not show that the number of words in the first argument has no relation to what is true. To use the term \"true\" in this way, we could use the phrase \"false.\"\nThe following sentence, we define:\n(1) The following sentence will appear in a text file or document: (2) There is no relationship between a word and the probability of that word.\n(3) The following sentence will appear in a text file or document: (4) The following sentence will appear in a text file or document: (5) This is the condition where the word is a word.\n(6) If there is no relationship between a word and the probability of that word, the probability of that word is a word.\n(7) If the word is a word.\n(8) If there is no relationship between a word and the probability of that word, the probability of that word is a word.\n(9) If the word is a word.\n(10) If there is no relationship between a word and the probability of that word, the probability of that word is a word.\nThis approach is also supported by our example. We can also use the term \"false\" to specify the probability of that word.\nThe first step we consider is the relation between a word and the probability of that word. This is the relation between a word and the probability of that word. It is important to note that the same argument can be used", "histories": [["v1", "Fri, 15 Mar 2013 02:37:19 GMT  (416kb)", "https://arxiv.org/abs/1303.3664v1", "This paper was submitted to the 30th International Conference on Machine Learning (ICML 2013) on February 15, 2013"], ["v2", "Mon, 18 Mar 2013 13:11:02 GMT  (416kb)", "http://arxiv.org/abs/1303.3664v2", null]], "COMMENTS": "This paper was submitted to the 30th International Conference on Machine Learning (ICML 2013) on February 15, 2013", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["weicong ding", "mohammad hossein rohban", "prakash ishwar", "venkatesh saligrama"], "accepted": true, "id": "1303.3664"}, "pdf": {"name": "1303.3664.pdf", "metadata": {"source": "META", "title": "Topic Discovery through Data Dependent and Random Projections", "authors": ["Weicong Ding", "Mohammad H. Rohban"], "emails": ["dingwc@bu.edu", "mhrohban@bu.edu", "pi@bu.edu", "srv@bu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n36 64\nv2 [\nst at\n.M L\n] 1\n8 M\nar 2"}, {"heading": "1. Introduction", "text": "We consider a corpus of M documents composed of words chosen from a vocabulary of W distinct words indexed by w = 1, . . . ,W . We adopt the classic \u201cbags of words\u201d modeling paradigm widely-used in probabilistic topic modeling (Blei, 2012). Each document is modeled as being generated by N independent and identically distributed (iid) drawings of words from an unknown W \u00d7 1 document word-distribution vector. Each document word-distribution vector is itself modeled as an unknown probabilistic mixture of\nK < min(M,W ) unknown W \u00d7 1 latent topic worddistribution vectors that are shared among the M documents in the corpus. Documents are generated independently. For future reference, we adopt the following notation. We denote by \u03b2 the unknown W \u00d7K topicmatrix whose columns are the K latent topic worddistribution vectors. \u03b8 denotes the K \u00d7 M weightmatrix whose M columns are the mixing weights over K topics for the M documents. These columns are assumed to be iid samples from a prior distribution. Each column of the W \u00d7 M matrix A = \u03b2\u03b8 corresponds to a document word-distribution vector. X denotes the observed W \u00d7M word-by-document matrix realization. TheM columns ofX are the empirical word-frequency vectors of the M documents. Our goal is to estimate the latent topic word-distribution vectors (\u03b2) from the empirical word-frequency vectors of all documents (X).\nA fundamental challenge here is that word-bydocument distributions (A) are unknown and only a realization is available through sampled word frequencies in each document. Another challenge is that even when these distributions are exactly known, the decomposition into the product of topic-matrix, \u03b2, and topic-document distributions, \u03b8, which is known as Nonnegative Matrix Factorization (NMF), has been shown to be an NP-hard problem in general. In this paper, we develop computationally efficient algorithms with provable guarantees for estimating \u03b2 for topic matrices satisfying the separability condition (Donoho & Stodden, 2004; Arora et al., 2012b).\nDefinition 1. (Separability) A topic matrix \u03b2 \u2208 RW\u00d7K is separable if for each topic k, there is some word i such that \u03b2i,k > 0 and \u03b2i,l = 0, \u2200l 6= k.\nThe condition suggests the existence of novel words that are unique to each topic. Our algorithm has three main steps. In the first step, we identify novel words by means of data dependent or random projections. A key insight here is that when each word is associated with\na vector consisting of its occurrences across all documents, the novel words correspond to extreme points of the convex hull of these vectors. A highlight of our approach is the identification of novel words based on data-dependent and random projections. Our idea is that whenever a convex object is projected along a random direction, the maximum and minimum values in the projected direction correspond to extreme points of the convex object. While our method identifies novel words with negligible false and miss detections, evidently multiple novel words associated with the same topic can be an issue. To account for this issue, we apply a distance based clustering algorithm to cluster novel words belonging to the same topic. Our final step involves linear regression to estimate topic word frequencies using novel words.\nWe show that our scheme has a similar sample complexity to that of state-of-art such as (Arora et al., 2012a). On the other hand, the computational complexity of our scheme can scale as small as O( \u221a MW + MN) for a corpora containing M documents, with an average of N words per document from a vocabulary containing W words. We then present a set of experiments on synthetic and real-world datasets. The results demonstrates qualitative and quantitative superiority of our scheme in comparison to other stateof-art schemes."}, {"heading": "2. Related Work", "text": "The literature on topic modeling and discovery is extensive. One direction of work is based on solving a nonnegative matrix factorization (NMF) problem. To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function. Nevertheless, this joint optimization is non-convex and suboptimal strategies have been used in this context. Unfortunately, when N \u226a W which is often the case, many words do not appear in X and such methods often fail in these cases.\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling. In this approach, the columns of \u03b8 are modeled as iid random drawings from some prior distributions such as Dirichlet. The goal is to compute MAP (maximum aposteriori probability) estimates for the topic matrix. This setup is inherently non-convex and MAP estimates are computed using variational Bayes approximations of the posterior distribution, Gibbs sampling or expectation propagation.\nA number of methods with provable guarantees have also been proposed. (Anandkumar et al., 2012) describe a novel method of moments approach. While their algorithm does not impose structural assumption on topic matrix \u03b2, they require Dirichlet priors for \u03b8 matrix. One issue is that such priors do not permit certain classes of correlated topics (Blei & Lafferty, 2007; Li & McCallum, 2007). Also their algorithm is not agnostic since it uses parameters of the Dirichlet prior. Furthermore, the algorithm suggested involves finding empirical moments and singular decompositions which can be cumbersome for large matrices.\nOur work is closely related to recent work of (Arora et al., 2012b) and (Arora et al., 2012a) with some important differences. In their work, they describe methods with provable guarantees when the topic matrix satisfies the separability condition. Their algorithm discovers novel words from empirical word co-occurrence patterns and then in the second step the topic matrix is estimated. Their key insight is that when each word, j, is associated with aW dimensional vector1 the novel words correspond to extreme points of the convex hull of these vectors. (Arora et al., 2012a) presents combinatorial algorithms to recover novel words with computational complexity scaling as O(MN2 +W 2 +WK/\u01eb2), where \u01eb is the element wise tolerable error of the topic matrix \u03b2. An important computational remark is that \u01eb often scales with W , i.e. probability values in \u03b2 get small when W is increased, hence one needs smaller \u01eb to safely estimate \u03b2 when W is too large. The other issue with their method is that empirical estimates of joint probabilities in the word-word co-occurrence matrix can be unreliable, especially when M is not large enough. Finally, their novel word detection algorithm requires linear independence of the extreme points of the convex hull. This can be a serious problem in some datasets where word co-occurrences lie on a low dimensional manifold.\nMajor Differences: Our work also assumes separability and existence of novel words. We associate each word with a M -dimensional vector consisting of the word\u2019s frequency of occurrence in the M -documents rather than word co-occurrences as in (Arora et al., 2012b;a). We also show that extreme points of the convex hull of these cross-document frequency patterns are associated with novel words. While these differences appear technical, it has important consequences. In several experiments our approach appears to significantly outperform (Arora et al., 2012a) and mir-\n1kth component is probability of occurrence of word j and word k in the same document in the entire corpus\nror performance of more conventional methods such as LDA (Griffiths & Steyvers, 2004). Furthermore, our approach can deal with degenerate cases found in some image datasets where the data vectors can lie on a lower dimensional manifold than the number of topics. At a conceptual level our approach appears to hinge on distinct cross-document support patterns of novel words belonging to different topics. This is typically robust to sampling fluctuations when support patterns are distinct in comparison to word cooccurrences statistics of the corpora. Our approach also differs algorithmically. We develop novel algorithms based on data-dependent and random projections to find extreme points efficiently with computational complexity scaling as O(MN + \u221a MW ) for the random scheme.\nOrganization: We illustrate the motivating Topic Geometry in Section 3. We then present our threestep algorithm in Section 4 with intuitions and computational complexity. Statistical correctness of each step of proposed approach are summarized in Section 5. We address practical issues in Section 6."}, {"heading": "3. Topic Geometry", "text": "Recall that X and A respectively denote the W \u00d7M empirical and actual document word distribution matrices, and A = \u03b2\u03b8, where \u03b2 is the latent topic word distribution matrix and \u03b8 is the underlying weight matrix. Let A\u0303, \u03b8\u0303 and X\u0303 denote the A, \u03b8 and X matrices after \u21131 row normalization. We set \u03b2\u0303 = diag(A1)\u22121\u03b2 diag(\u03b81), so that A\u0303 = \u03b2\u0303\u03b8\u0303. Let Xi and Ai respectively denote the i \u2212 th row of X and A representing the cross-document patterns of word i. We assume that \u03b2 is separable (Def. 1). Let Ck be the set of novel words of topic k and let C0 be the set of non-novel words.\nThe geometric intuition underlying our approach is formulated in the following proposition :\nProposition 1. Let \u03b2 be separable. Then for all novel words i \u2208 Cj, A\u0303i = \u03b8\u0303j and for all non-novel words i \u2208 C0, A\u0303i is a convex combination of \u03b8\u0303j\u2019s, for j = 1, . . . ,K.\nProof: Note that for all i,\nK\u2211\nk=1\n\u03b2\u0303ik = 1\nand for all i \u2208 Cj , \u03b2\u0303ij = 1. Moreover, we have\nA\u0303i =\nK\u2211\nk=1\n\u03b2\u0303ik\u03b8\u0303k\nHence A\u0303i = \u03b8\u0303j for i \u2208 Cj . In addition, A\u0303i = K\u2211\nk=1\n\u03b2\u0303ik\u03b8\u0303k\nfor i \u2208 C0. Fig. 1 illustrates this geometry. Without loss of generality, we could assume that novel word vectors \u03b8\u0303i are not in the convex hull of the other rows of \u03b8\u0303. Hence, The problem of identifying novel words reduces to finding extreme points of all A\u0303i\u2019s.\nFurthermore, retrieving topic matrix \u03b2 is straightforward given all K distinct novel words :\nProposition 2. If the matrix A and K distinct novel words {i1, . . . , iK} are given, then \u03b2 can be calculated using W linear regressions.\nProof: By Proposition 1, we have \u03b8\u0303 = (A\u22a4i1 , . . . ,A \u22a4 iK )\n\u22a4. Next A\u0303i = \u03b2\u0303i\u03b8\u0303. So \u03b2\u0303i can be computed by solving a linear system of equations. Specifically, if we let \u03b2\u2032 = diag(A1)\u03b2\u0303 = \u03b2 diag(\u03b81)\u22121, \u03b2 can be obtained by column normalizing \u03b2\u2032.\nProposition 1 and 2 validate the approach to estimate \u03b2 via identifying novel words given access to A. However, only X, a realization of A, is available in the real problem which is not close to A in typical settings of interest (N \u226a W ). However, even when the number of samples per document (N) is limited, if we collect enough documents (M \u2192 \u221e), the proposed algorithm could still asymptotically estimate \u03b2 with arbitrary precision, as we will discuss in the following sections."}, {"heading": "4. Proposed Algorithm", "text": "The geometric intuition mentioned in Propositions 1 and 2 motivates the following three-step approach for topic discovery :\n(1) Novel Word Detection: Given the empirical word-by-document matrix X, extract the set of all novel words I. We present variants of projection-based algorithms in Sec. 4.1.\n(2) Novel Word Clustering: Given a set of novel words I with |I| \u2265 K , cluster them into K groups corresponding to K topics. Pick a representative for each group. We adopt a distance based clustering algorithm. (Sec. 4.2).\n(3) Topic Estimation: Estimate topic matrix as suggested in Proposition 2 by constrained linear regression. (Section 4.3)."}, {"heading": "4.1. Novel Word Detection", "text": "Fig. 1 illustrates the key insight to identify novel words as extreme points of some convex body. When we project every point of a convex body onto some direction d, the maximum and minimum correspond to extreme points of the convex object. Our proposed approaches, data dependent and random projection, both exploit this fact. They only differ in the choice of projected directions."}, {"heading": "A. Data Dependent Projections (DDP)", "text": "To simplify our analysis, we randomly split each document into two subsets, and obtain two statistically independent document collections X and X\u2032, both distributed as A, and then row normalize as X\u0303 and X\u0303\u2032. For some threshold, d, to be specified later, and for each word i, we consider the set, Ji, of all other words that are sufficiently different from word i in the following sense:\nJi = {j | M(X\u0303i \u2212 X\u0303j)(X\u0303\u2032i \u2212 X\u0303\u2032j)\u22a4 \u2265 d/2} (1)\nWe then declare word i as a novel word if all words j \u2208 Ji are uniformly uncorrelated to word i with some margin, \u03b3/2 to be specified later.\nM\u3008X\u0303i, X\u0303\u2032i\u3009 \u2265 M\u3008X\u0303i, X\u0303\u2032j\u3009+ \u03b3/2, \u2200j \u2208 Ji (2)\nThe correctness of DDP Algorithm is established by the following Proposition and will be further discussed in section 5. The proof is given in the Supplementary section.\nProposition 3. Suppose conditions P1 and P2 (will be defined in section 5) on prior distribution of \u03b8 hold. Then, there exists two positive constants d and \u03b3 such that if i is a novel word, for all j \u2208 Ji, M\u3008X\u0303i, X\u0303\u2032i\u3009 \u2212 M\u3008X\u0303i, X\u0303\u2032j\u3009 \u2265 \u03b3/2 with high probability (converging to one as M \u2192 \u221e). In addition, if i is a non-novel word, there exists some j \u2208 Ji such that M\u3008X\u0303i, X\u0303\u2032i\u3009 \u2212 M\u3008X\u0303i, X\u0303\u2032j\u3009 \u2264 \u03b3/2 with high probability.\nAlgorithm 1 Novel Word Detection - DDP\n1: Input X\u0303, X\u0303\u2032, d, \u03b3,K 2: Output: The indices of the novel words I 3: C \u2190 M X\u0303\u2032X\u0303\u22a4 4: I \u2190 \u2205 5: for all 1 \u2264 i \u2264 W do 6: Ji \u2190 All indices j 6= i : Ci,i \u2212 2Ci,j + Cj,j \u2265 d2 7: if \u2200j \u2208 Ji : Ci,i \u2212 Ci,j \u2265 \u03b3/2 then 8: I \u2190 I \u222a {i} 9: end if\n10: end for\nThe algorithm is elaborated in Algorithm 1. The running time of the algorithm is summarized in the following proposition. Detailed justification is provided in the Supplementary section.\nProposition 4. The running time of Algorithm 1 is O(MN2 +W 2).\nProof Sketch. Note that X is sparse since N \u226a W . Hence by exploiting the sparsity C = MXX\u2032\u22a4 can be computed in O(MN2 +W ) time. For each word i, finding Ji and calculating Ci,i\u2212Ci,j \u2265 \u03b3/2 costO(W 2) time in the worst case."}, {"heading": "B. Random Projections (RP)", "text": "DDP usesW different directions to find all the extreme points. Here we use random directions instead. This significantly reduces the time complexity by decreasing the number of required projections.\nThe Random Projection Algorithm (RP) uses roughly P = O(K) random directions drawn uniformly iid over the unit sphere. For each direction d, we project all X\u0303i\u2019s onto it and choose the maximum and minimum. Note that X\u0303id will converge to A\u0303id conditioned on d\nAlgorithm 2 Novel Word Detection - RP\n1: Input X\u0303, P 2: Output : The indices of the novel words I 3: I \u2190 \u2205 4: for all 1 \u2264 j \u2264 P do 5: Generate d \u223c Uniform(unit-sphere in RM ) 6: imax = argmax X\u0303id, imin = argmax X\u0303id 7: I \u2190 I \u222a {imax, imin} 8: end for\nand \u03b8 as M increases. Moreover, only for the extreme points i, A\u0303id can be the maximum or minimum projection value. This provides intuition of consistency for RP. Since the directions are independent, we expect to find all the novel words using P = O(K) number of random projections."}, {"heading": "C. Random Projections with Binning", "text": "Another alternative to RP is a Binning algorithm which is computationally more efficient. Here the corpus is split into \u221a M equal sized bins. For each bin j a random direction d(j) is chosen and the word with the maximum projection along d(j) is chosen as a winner. Then, we find the number of wins for each word i. We then divide these winning frequencies by \u221a M as an estimate for pi , Pr(\u2200j 6= i : A\u0303id \u2265 A\u0303jd). pi can be shown to be zero for all non-novel words. For nondegenerate prior over \u03b8, these probabilities converge to strictly positive values for novel words. Hence, estimating pi\u2019s helps in identifying novel words. We then choose the indices of O(K) largest pi values as novel words. The Binning algorithm is outlined in Algorithm 3.\nAlgorithm 3 Novel Word Detection - Binning\n1: Input : X\u0303, X\u0303\u2032, d, K 2: Output : The indices of the novel words I 3: Split documents in X into \u221a M equal sized groups\nof documents X(1), . . . ,X( \u221a M) and normalize each one separately to obtain X\u0303(1), . . . , X\u0303( \u221a M) as well.\n4: for all 1 \u2264 j \u2264 \u221a M do 5: d(j) \u2190 a sample from U(S \u221a M\u22121) 6: l \u2190 argmax 1\u2264i\u2264W X\u0303 (j) i d (j) 7: p\u0302 (j) l \u2190 p\u0302 (j) l + 1 8: end for 9: for all 1 \u2264 i \u2264 W do\n10: p\u0302i \u2190 1\u221aM \u2211\u221aM j=1 p\u0302 (j) i 11: end for 12: k \u2190 0, I \u2190 \u2205 and i \u2190 1 13: repeat 14: j \u2190 the index of the ith largest value of\n(p\u03021, . . . , p\u0302W )\n15: if I = \u2205 or \u2200l \u2208 I : M(X\u0303j\u2212X\u0303l)(X\u0303\u2032j\u2212X\u0303\u2032l) \u2265 d/2 then 16: I \u2190 I \u222a {j} 17: k \u2190 k + 1 18: end if 19: i \u2190 i+ 1 20: until k = K\nIn contrast with DDP, the RP algorithm is completely agnostic and parameter-free. This means that it requires no parameters like d and \u03b3 to find the novel words. Moreover, it significantly reduces the computational complexity :\nProposition 5. The running times of the RP and Binning algorithms are O(MNK+WK) and O(MN+\n\u221a MW ), respectively.\nProof. We will sketch the proof and provide a more detailed justification in the Supplementary section. Note that the number of operations needed to find the projections is O(MN+W ) in Binning and O(MNK+W ) in RP. In addition, finding the the maximum takes O(WK) for RP and O( \u221a MW ) for Binning. In sum,\nit takes O(MNK+WK) for RP and O(MN+ \u221a MW ) for Binning to find all the novel words."}, {"heading": "4.2. Novel Word Clustering", "text": "Since there may be multiple novel words for a single topic, our DDP or RP algorithm can extract multiple novel words for each topic. This necessitates clustering to group the copies. We can show that our clustering scheme is consistent if we assume that R = 1M E(\u03b8\u03b8\n\u22a4) is positive definite: Proposition 6. Let Ci,j , MX\u0303iX\u0303 \u2032\u22a4 j , and Di,j , Ci,i \u2212 2Ci,j + Cj,j. If R is positive definite, then Di,j converges to zero in probability whenever i and j are novel words of the same topic as M \u2192 \u221e. Moreover, if i and j are novel words of different types, it converges in probability to some strictly positive value greater than some constant d .\nThe proof is presented in the Supplementary section. As the Proposition 6 suggests, we construct a bi-\nAlgorithm 4 Novel Word Clustering\n1: Input : I, X\u0303, X\u0303\u2032, d, K 2: Output : J which is a set of K novel words of\ndistinct topics 3: C \u2190 M X\u0303\u2032X\u0303\u22a4 4: B \u2190 a |I| \u00d7 |I| zero matrix 5: for all i, j \u2208 I, i 6= j do 6: if Ci,i \u2212 2Ci,j + Cj,j \u2264 d/2 then 7: Bi,j \u2190 1 8: end if 9: end for\n10: J \u2190 \u2205 11: for all 1 \u2264 j \u2264 K do 12: c \u2190 one of the indices of the jth connected component vertices in B 13: J \u2190 J \u222a {c} 14: end for\nnary graph with its vertices correspond to the novel words. An edge between word i and j is established if Di,j \u2264 d/2. Then, the clustering reduces to finding K connected components. The procedure is described in Algorithm 4.\nIn Algorithm 4, we simply choose any word of a cluster as the representative for each topic. This is simply\nfor theoretical analysis. However, we could set the representative to be the average of data points in each cluster, which is more noise resilient."}, {"heading": "4.3. Topic Matrix Estimation", "text": "Given K novel words of different topics (J ), we could directly estimate (\u03b2) as in Proposition 2. This is described in Algorithm 5. We note that this part of the algorithm is similar to some other topic modeling approaches, which exploit separability. Consistency of this step is also validated in (Arora et al., 2012b). In fact, one may use the convergence of extremum estimators (Amemiya, 1985) to show the consistency of this step.\nAlgorithm 5 Topic Matrix Estimation\n1: Input: J = {j1, . . . , jK}, X, X\u2032 2: Output: \u03b2\u0302, which is the estimation of \u03b2 matrix 3: Y = (X\u0303\u22a4j1 , . . . , X\u0303 \u22a4 jK )\u22a4,Y\u2032 = (X\u0303\u2032\u22a4j1 , . . . , X\u0303 \u2032\u22a4 jK )\u22a4 4: for all 1 \u2264 i \u2264 W do 5: \u03b2\u0302i \u2190 ( 1MXi1) argmin\nbj\u22650, \u2211 K j=1 bj=1\nM(X\u0303i \u2212 bY)(X\u0303\u2032i \u2212\nbY\u2032)\u22a4\n6: end for 7: column normalize \u03b2\u0302"}, {"heading": "5. Statistical Complexity Analysis", "text": "In this section, we describe the sample complexity bound for each step of our algorithm. Specifically, we provide guarantees for DDP algorithm under some mild assumptions on the distribution over \u03b8. The analysis of the random projection algorithm is much more involved and requires elaborate arguments. We will omit it in this paper.\nWe require following technical assumptions on the correlation matrix R and the mean vector a of \u03b8 :\n(P1) R is positive definite with its minimum eigenvalue being lower bounded by \u03bb\u2227 > 0. In addition, \u2200i, ai \u2265 a\u2227 > 0. (P2) There exists a positive value \u03b6 such that for i 6= j, Ri,i/(aiai)\u2212Ri,j/(aiaj) \u2265 \u03b6. The second condition captures the following intuition : if two novel words are from different topics, they must appear in a substantial number of distinct documents. Note that for two novel words i and j of different topics, MA\u0303i(A\u0303i \u2212 A\u0303j)\u22a4 p\u2212\u2192 Ri,i/(aiai) \u2212 Ri,j/(aiaj). Hence, this requirement means that M(A\u0303i \u2212 A\u0303j) should be fairly distant from the origin, which implies that the number of documents these two words co-\noccur in, with similar probabilities, should be small. This is a reasonable assumption, since otherwise we would rather group two related topics into one. In fact, we show in the Supplementary section (Section A.5) that both conditions hold for the Dirichlet distribution, which is a traditional choice for the prior distribution in topic modeling. Moreover, we have tested the validity of these assumptions numerically for the logistic normal distribution (with non-degenerate covariance matrices), which is used in Correlated Topic Modeling (CTM) (Blei & Lafferty, 2007)."}, {"heading": "5.1. Novel Word Detection Consistency", "text": "In this section, we provide analysis only for the DDP Algorithm. The sample complexity analysis of the randomized projection algorithms is however more involved and is the subject of the ongoing research. Suppose P1 and P2 hold. Denote \u03b2\u2227 and \u03bb\u2227 to be positive lower bounds on non-zero elements of \u03b2 and minimum eigenvalue of R, respectively. We have:\nTheorem 1. For parameter choices d = \u03bb\u2227\u03b22\u2227 and \u03b3 = \u03b6a\u2227\u03b2\u2227 the DDP algorithm is consistent as M \u2192 \u221e. Specifically, true novel and non-novel words are asymptotically declared as novel and non-novel, respectively. Furthermore, for\nM \u2265 C1\n( logW + log ( 1 \u03b41 ))\n\u03b22\u2227\u03b78 min(\u03bb 2 \u2227\u03b2 2 \u2227, \u03b62a 2 \u2227)\nwhere C1 is a constant, Algorithm 1 finds all novel words without any outlier with probability at least 1\u2212 \u03b41, where \u03b7 = min\n1\u2264i\u2264W \u03b2ia.\nProof Sketch. The detailed justification is provided in the Supplementary section. The main idea of the proof is a sequence of statements :\n\u2022 Given P1, for a novel word i, Ji defined in the Algorithm 1 is a subset of J\u2217i asymptotically with high probability, where J\u2217i = {j : supp(\u03b2j) 6= supp(\u03b2i)}. Moreover Ji is a superset of J\u2217i with high probability for a non-novel word with J\u2217i = {j : | supp(\u03b2j)| = 1}.\n\u2022 Given P2, for a novel word i, Ci,i\u2212Ci,j converges to a strictly positive value greater than \u03b3 for j \u2208 J\u2217i , and if i is non-novel, \u2203j \u2208 J\u2217i such that Ci,i \u2212 Ci,j converges to a non-positive value.\nThese statements imply Proposition 3, which proves the consistency of the DDP Algorithm.\nThe term \u03b7\u22128 seems to be the dominating factor in the sample complexity bound. Basically,\n\u03b7 = min 1\u2264i\u2264W\n1 M E(Xi1) represents the minimum propor-\ntion of documents that a word would appear in. This is not surprising as the rate of convergence of Ci,j = M\u3008X\u0303i, X\u0303\u2032j\u3009 is dependent on the values of 1M E(Xi1) and 1M E(Xj1). As these values are decreased, Ci,j converges to a larger value and the convergence get slower. In another view, given that the number of words per document N is bounded, in order to have Ci,j converge, a large number of documents is needed to observe all the words sufficiently. It is remarkable that a similar term p\u22126 would also arise in the sample complexity bound of (Arora et al., 2012b), where p is the minimum non-zero element of diagonal part of \u03b2. It may be noted that although it seems that the sample complexity bound scales logarithmically with W , \u03b7 and p would be decreased typically as W increases."}, {"heading": "5.2. Novel Word Clustering Consistency", "text": "We similarly prove the consistency and sample complexity of the novel word clustering algorithm :\nTheorem 2. For d = \u03bb\u2227\u03b22\u2227, given all true novel words as the input, the clustering algorithm, Algorithm 4 (ClusterNovelWords) asymptotically (as M \u2192 \u221e recovers K novel word indices of different types, namely, the support of the corresponding \u03b2 rows are different for any two retrieved indices. Furthermore, if\nM \u2265 C2\n( logW + log ( 1 \u03b42 ))\n\u03b78\u03bb2\u2227\u03b2 4 \u2227\nthen Algorithm 4 clusters all novel words correctly with probability at least 1\u2212 \u03b42.\nProof Sketch. More detailed analysis is provided in the Supplementary section. We can show that Ci,i\u22122Ci,j +Cj,j converges to a strictly positive value d if i and j are novel words of different topics. Moreover, it converges to zero if they are novel words of the same topic. Hence all novel words of the same topic are connected in the graph with high probability asymptotically. Moreover, there would not be an edge between the novel words of different topics with high probability. Therefore, the connected components of the graph corresponds to the true clusters asymptotically. The detailed discussion of the convergence rate is provided in the Supplementary section.\nIt is noticeable that the sample complexity of the clustering is similar to that of the novel word detection. This means that the hardness of novel word detection and distance based clustering using the proposed algorithms are almost the same."}, {"heading": "5.3. Topic Estimation Consistency", "text": "Finally, we show that the topic estimation by regression is also consistent.\nTheorem 3. Suppose that Algorithm 5 outputs \u03b2\u0302 given the indices of K distinct novel words. Then, \u03b2\u0302 p\u2212\u2192 \u03b2. Specifically, if\nM \u2265 C3W 4(log(W ) + log(K) + log(1/\u03b43))\n\u03bb2\u2227\u03b78\u01eb4a 8 \u2227\nthen for all i and j, \u03b2\u0302i,j will be \u01eb close to \u03b2i,j with probability at least 1 \u2212 \u03b43, with \u01eb < 1, C3 being a constant, a\u2227 = mini ai and \u03b7 = min\n1\u2264i\u2264W \u03b2ia.\nProof Sketch. We will provide a detailed analysis in the Supplementary section. To prove the consistency of the regression algorithm, we will use a consistency result for the extremum estimators : If we assume QM (\u03b2) to be a stochastic objective function which is minimized at \u03b2\u0302 under the constraint \u03b2 \u2208 \u0398 (for a compact \u0398), and QM (\u03b2) converges uniformly to Q\u0304(\u03b2), which in turn is minimized uniquely in \u03b2\u2217, then \u03b2\u0302 p\u2212\u2192 \u03b2\u2217 (Amemiya, 1985). In our setting, we may take QM to be the objective function in Algorithm 5. Then, QM (b) p\u2212\u2192 Q\u0304(b) = bDRDb\u22a4\u22122bDR \u03b2 \u22a4 i\n\u03b2ia + \u03b2i \u03b2ia R \u03b2\u22a4i \u03b2ia ,\nwhere D = diag(a)\u22121. Note that if R is positive definite, Q\u0304 is uniquely minimized at b\u2217 = \u03b2i\n\u03b2ia D\u22121, which\nsatisfies the conditions of the optimization. Moreover, QM converges to Q uniformly as a result of Lipschitz continuity of QM . Therefore, according to Slutsky\u2019s theorem, ( 1MXi1)b \u2217 = \u03b2\u0302i converges to \u03b2iD\u22121, and hence the column normalization of \u03b2\u0302 converges to \u03b2. We will provide a more detailed analysis of this part in the Supplementary section.\nIn sum, consider the approach outlined at the beginning of section 4 based on data-dependent projections method, and assume that \u03b2\u0302 is the output. Then,\nTheorem 4. The output of the topic modeling algorithm \u03b2\u0302 converges in probability to \u03b2 element-wise. To be precise, if\nM \u2265 max { C\u20322W\n4 log WK\u03b4 \u03bb2\u2227\u03b78\u01eb4a 8 \u2227 , C\u20321 log W \u03b4 \u03b22\u2227\u03b78 min(\u03bb 2 \u2227\u03b2 2 \u2227, \u03b62a 2 \u2227)\n}\nthen with probability at least 1 \u2212 3\u03b4, for all i and k, \u03b2\u0302i,k will be \u01eb close to \u03b2i,k, with \u01eb < 1, C \u2032 1 and C \u2032 2 being two constants.\nThe proof is a combination of Theorems 1, 2 and 3."}, {"heading": "6. Experimental Results", "text": ""}, {"heading": "6.1. Practical Considerations", "text": "DDP algorithm requires two parameters \u03b3 and d. In practice, we can apply DDP without knowing them adaptively and agnostically. Note that d is for the construction of Ji. We can otherwise construct Ji by finding r < W words that are maximally distant from i in the sense of Eq. 1. To bypass \u03b3, we can rank the values of minj\u2208Ji M\u3008X\u0303i, X\u0303\u2032i\u3009 \u2212M\u3008X\u0303i, X\u0303\u2032j\u3009 across all i and declare the topmost s values as the novel words.\nThe clustering algorithm also requires parameter d. Note that d is just for thresholding a 0 \u2212 1 weighted graph. In practice, we could avoid hard thresholding by using exp(\u2212(Ci,i \u2212 2Ci,j +Cj,j)) as weights for the graph and apply spectral clustering. To point out, typically the size of I in Algorithm 4 is of the same order as K. Hence the spectral clustering is on a relative small graph which typically adds O(K3) computational complexity.\nImplementation Details: We choose the parameters of the DDP and RP in the following way. For DDP in all datasets except the Donoho image corpus, we use the agnostic algorithm discussed in section 6.1 with r = W/2. Moreover, we take s = 10 \u00d7 K. For the image dataset, we used d = 1 and \u03b3 = 3. For RP, we set the number of projections P \u2248 50 \u00d7 K in all datasets to obtain the results."}, {"heading": "6.2. Synthetic Dataset", "text": "In this section, we validate our algorithm on synthetic examples. We generate a W \u00d7K separable topic matrix \u03b2 with W1/K > 1 novel words per topic as follows: first, iid 1\u00d7K row-vectors corresponding to nonnovel words are generated uniformly on the probability simplex. Then, W1 iid Uniform[0, 1] values are generated for the nonzero entries in the rows of novel words. The resulting matrix is then column-normalized to get one realization of \u03b2. Let \u03c1 := W1/W . Next, M iid K \u00d7 1 column-vectors are generated for the \u03b8 matrix according to a Dirichlet prior c\nK\u220f i=1 \u03b8\u03b1i\u22121i . Following\n(Griffiths & Steyvers, 2004), we set \u03b1i = 0.1 for all i. Finally, we obtain X by generating N iid words for each document.\nFor different settings of W , \u03c1, K, M and N , we calculate the \u21131 distance of the estimated topic matrix to the ground truth after finding the best matching between two sets of topics. For each setting we average the error over 50 random samples. For RP & DDP we set parameters as discussed in the implementation details.\nWe compare the DDP and RP against the Gibbs sampling approach (Griffiths & Steyvers, 2004) (Gibbs), a state-of-art NMF-based algorithm (Tan & Fe\u0301votte, in press) (NMF) and the most recent practical provable algorithm in (Arora et al., 2012a) (RecL2). The NMF algorithm is chosen because it compensates for the type of noise in our topic model. Fig. 2 depicts the estimation error as a function of the number of documents M (Upper) and the number of words/document N (bottom). RP and DDP have similar performance and are uniformly better than comparable techniques. Gibbs performs relatively poor in the first setting and NMF in the second. RecL2 perform worse in all the settings. Note that M is relatively small (\u2264 1, 000) compared to W = 500. DDP/RP outperform other methods with fairly small sample size. Meanwhile, as is also observed in (Arora et al., 2012a), RecL2 has a poor performance with small M .\n6.3. Swimmer Image Dataset\nIn this section we apply our algorithm to the synthetic swimmer image dataset introduced in (Donoho & Stodden, 2004). There are M = 256 binary images, each with W = 32 \u00d7 32 = 1024 pixels. Each image represents a swimmer composed of four limbs, each of which can be in one of 4 distinct positions, and a torso. We interpret pixel positions (i, j) as words. Each image is interpreted as a document composed of pixel positions with non-zero values. Since each position of a limb features some unique pixels in the image, the topic matrix \u03b2 satisfies the separability assumption with K = 16 \u201cground truth\u201d topics that\ncorrespond to 16 single limb positions.\nFollowing the setting of (Tan & Fe\u0301votte, in press), we set body pixel values to 10 and background pixel values to 1. We then take each \u201cclean\u201d image, suitably normalized, as an underlying distribution across pixels and generate a \u201cnoisy\u201d document of N = 200 iid \u201cwords\u201d according to the topic model. Examples are shown in Fig. 3. We then apply RP and DDP algorithms to the \u201cnoisy\u201d dataset and compare against Gibbs (Griffiths & Steyvers, 2004), NMF (Tan & Fe\u0301votte, in press), and RecL2 (Arora et al., 2012a). Results are shown in Figs. 4 and 5. We set the parameters as discussed in the implementation details.\nThis dataset is a good validation test for different algorithms since the ground truth topics are known and unique. As we see in Fig. 4, both Gibbs and NMF produce topics that do not correspond to any pure left/right arm/leg positions. Indeed, many of them are composed of multiple limbs. Nevertheless, as shown in Fig. 5, no such errors are realized in RP and DDP and our topic-estimates are closer to the ground truth images. In the meantime, RecL2 algorithm failed to work even with the clean data. Although it also extracts extreme points of a convex body, the algorithm additionally requires these points to be linearly independent. It is possible that extreme points of a convex body are linearly dependent (for example, a 2-D square on a 3-D simplex). This is exactly the case in the swimmer dataset. As we see in the last row in Fig. 5, RecL2 produces only a few topics close to ground truth. Its extracted topics for the noisy im-\nages are shown in Fig. 4. Results of RecL2 on noisy images are no close to ground truth as shown in Fig. 4."}, {"heading": "6.4. Real World Text Corpora", "text": "In this section, we apply our algorithm on two different real world text corpora from (Frank & Asuncion,\n2010). The smaller corpus is NIPS proceedings dataset with M = 1, 700 documents, a vocabulary of W = 14, 036 words and an average of N \u2248 900 words in each document. Another is a large corpus New York (NY) Times articles dataset, with M = 300, 000, W = 102, 660, and N \u2248 300. The vocabulary is obtained by deleting a standard \u201cstop\u201d word list used in computational linguistics, including numbers, individual characters, and some common English words such as \u201cthe\u201d.\nIn order to compare with the practical algorithm in (Arora et al., 2012a), we followed the same pruning in their experiment setting to shrink the vocabulary size to W = 2, 500 for NIPS and W = 15, 000 for NY Times. Following typical settings in (Blei, 2012) and (Arora et al., 2012a), we set K = 40 for NIPS and K = 100 for NY Times. We set our parameters as discussed in implementation details.\nWe compare DDP and RP algorithms against RecL2 (Arora et al., 2012a) and a practically widely successful algorithm (Griffiths & Steyvers, 2004)(Gibbs). Table 1 and 22 depicts typical topics extracted by the different methods. For each topic, we show its most frequent words, listed in descending order of the estimated probabilities. Two topics extracted by different algorithms are grouped if they are close in \u21131 distance.\nDifferent algorithms extract some fraction of similar topics which are easy to recognize. Table 1 indicates most of the topics extracted by RP and DDP are similar and are comparable with that of Gibbs. We observe that the recognizable themes formed with DDP or RP topics are more abundant than that by RecL2. For example, topic on \u201cchip design\u201d as shown in the first panel in Table 1 is not extracted by RecL2, and topics in Table 2 on \u201cweather\u201d and \u201cemotions\u201d are missing in RecL2. Meanwhile, RecL2 method produces some obscure topics. For example, in the last panel of Table 1, RecL2 contains more than one theme, and in the last panel of Table 2 RecL2 produce some unfathomable combination of words. More details about the topics extracted are given in the Supplementary section."}, {"heading": "7. Conclusion and Discussion", "text": "We summarize our proposed approaches (DDP, Binning and RP) while comparing with other existing methods in terms of assumptions, computational complexity and sample complexity (see Table 3). Among the list of the algorithms, DDP and RecL2 are the best and competitive methods. While the DDP algorithm has a polynomial sample complexity, its running time\n2the zzz prefix annotates the named entity.\nis better than that of RecL2, which depends on 1/\u01eb2. Although \u01eb seems to be independent of W , by increasing W the elements of \u03b2 would be decreased and the precision (\u01eb) which is needed to recover \u03b2 would be decreased. This results in a larger time complexity in RecL2. In contrast, time complexity of DDP does not scale with \u01eb. On the other hand, the sample complexity of both DDP and RecL2, while polynomially scaling, depend on too many different terms. This makes the comparison of these sample complexities difficult.\nHowever, terms corresponding to similar concepts appeared in the two bounds. For example, it can be seen that pa\u2227 \u2248 \u03b7, because the novel words are possibly the most rare words. Moreover, \u03bb\u2227 and \u03b3 which are the \u21132 and \u21131 condition numbers of R are closely related. Finally, a = a\u2228a\u2227 , with a\u2228 and a\u2227 being the maximum and minimum values in a."}, {"heading": "Supplementary Materials", "text": ""}, {"heading": "A. Proofs", "text": "Given \u03b2 is separable, we can reorder the rows of \u03b2 such\nthat \u03b2 = [ D \u03b2\u2032 ] , where D is diagonal. We will assume the same structure for \u03b2 throughout the section."}, {"heading": "A.1. Proof of Proposition 3", "text": "Proposition 3 is a direct result of Theorem 1. Please refer to section A.7 for more details."}, {"heading": "A.2. Proof of Proposition 4", "text": "Recall that Proposition 4 summarizes the computational complexity of the DDP Algorithm 1. Here we provide more details.\nProposition 4 (in Section 4.1). The running time of Data dependent projection Algorithm DDP 1 is O(MN2 +W 2). Proof : We can show that, because of the sparsity of X, C = MXX\u2032\u22a4 can be computed in O(MN2 + W ) time. First, note that C is a scaled word-word cooccurrence matrix, which can be calculated by adding up the co-occurrence matrices of each document. This running time can be achieved, if all W words in the vocabulary are first indexed by a hash table (which takes O(W )). Then, since each document consists of at most N words, O(N2) time is needed to compute the co-occurrence matrix of each document. Finally, the summation of these matrices to obtain C would cost O(MN2), which results in total O(MN2 + W ) time complexity. Moreover, for each word i, we have to find Ji and test whether Ci,i \u2212 Ci,j \u2265 \u03b3/2 for all j \u2208 Ji. Clearly, the cost to do this is O(W 2) in the worst case."}, {"heading": "A.3. Proof of Proposition 5", "text": "Recall that Proposition 5 summarizes the computational complexity of RP ( Algorithm 2) and Binning (and see Section B in appendix for more details). Here we provide a more detailed proof.\nProposition 5 (in Section 4.1) Running time of RP (Algorithm 2) and Binning algorithm (in Appendix Section B) are O(MNK+WK) and O(MN+ \u221a MW ), respectively.\nProof : Note that number of operations needed to find the projections is O(MN + W ) in Binning and O(MNK + W ) in RP. This can be achieved by first indexing the words by a hash table and then finding\nthe projection of each document along the corresponding component of the random directions. Clearly, that takes O(N) time for each document. In addition, finding the word with the maximum projection value (in RP) and the winner in each bin (in Binning) will take O(W ). This counts to be O(WK) for all projections in RP and O( \u221a MW ) for all of the bins in Binning. Adding running time of these two parts, the computational complexity of the RP and Binning algorithms will be O(MNK + WK) and O(MN + \u221a MW ), respectively."}, {"heading": "A.4. Proof of Proposition 6", "text": "Proposition 6 (in Section 4.2) is a direct result of Theorem 2. Please read section A.8 for the detailed proof.\nA.5. Validation of Assumptions in Section 5 for Dirichelet Distribution\nIn this section, we prove the validity of the assumptions P1 and P2 which were made in Section 5. For x \u2208 RK with \u2211Ki=1 xi = 1, xi \u2265 0, x \u223c Dir(\u03b11, . . . , \u03b1K) has pdf P(x) = c \u220fK i=1 x \u03b1i\u22121 i . Let \u03b1\u2227 = min 1\u2264i\u2264K \u03b1i and \u03b10 = \u2211K i=1 \u03b1i.\nProposition A.1 For a Dirichlet prior Dir(\u03b11, . . . , \u03b1K):\n1. The correlation matrix R is positive definite with minimum eigenvalue \u03bb\u2227 \u2265 \u03b1\u2227\u03b10(\u03b10+1) ,\n2. \u22001 \u2264 i 6= j \u2264 K, Ri,iaiai \u2212 Ri,j aiaj = \u03b10\u03b1i(\u03b10+1) > 0.\nProof. The covariance matrix of Dir(\u03b11, . . . , \u03b1K), denoted as \u03a3, can be written as\n\u03a3i,j =\n{ \u2212\u03b1i\u03b1j \u03b12\n0 (\u03b10+1) if i 6= j \u03b1i(\u03b10\u2212\u03b1i) \u03b12\n0 (\u03b10+1)\notherwise (3)\nCompactly we have \u03a3 = 1 \u03b12\n0 (\u03b10+1)\n( \u2212\u03b1\u03b1\u22a4 + \u03b10 diag(\u03b1) )\nwith \u03b1 = (\u03b11, . . . , \u03b1K). The mean vector \u00b5 = 1 \u03b10\n\u03b1. Hence we obtain\nR = 1\n\u03b120(\u03b10 + 1)\n( \u2212\u03b1\u03b1\u22a4 + \u03b10 diag(\u03b1) ) + 1\n\u03b120 \u03b1\u03b1\u22a4\n= 1\n\u03b10(\u03b10 + 1)\n( \u03b1\u03b1\u22a4 + diag(\u03b1) )\nNote that \u03b1i > 0 for all i, \u03b1\u03b1 \u22a4 and diag(\u03b1) are positive definite. Hence R is strictly positive definite, with eigenvalues \u03bbi =\n\u03b1i \u03b10(\u03b10+1) . Therefore \u03bb\u2227 \u2265 \u03b1\u2227\u03b10(\u03b10+1) . The second property follows by directly plug in equation (3)."}, {"heading": "A.6. Convergence Property of the co-occurrence Matrix", "text": "In this section, we prove a set of Lemmas as ingredients to prove the main Theorems 1, 2 and 3 in Section 5. These Lemmas in sequence show :\n\u2022 Convergence of C = MX\u0303X\u0303\u2032\u22a4; (Lemma 1) \u2022 Convergence of Ci,i \u2212 2Ci,j + Cj,j to a strictly positive value if i, j are not novel words of the same topic; (Lemma 2) \u2022 Convergence of Ji to J\u2217i such that if i is novel, Ci,i \u2212 Ci,j converges to a strictly positive value for j \u2208 J\u2217i , and if i is non-novel, \u2203j \u2208 J\u2217i such that Ci,i \u2212 Ci,j converges to a non-positive value (Lemmas 3 and 4).\nRecall that in Algorithm 1, C = MX\u0303X\u0303\u2032\u22a4. Let\u2019s fur-\nther define Ei,j = \u03b2i \u03b2ia R \u03b2\u22a4j \u03b2ja . \u03b7 = min 1\u2264i\u2264W \u03b2ia. Let R and a be the correlation matrix and mean vector of prior distribution of \u03b8.\nBefore we dig into the proofs, we provide two limit analysis results of Slutsky\u2019s theorem :\nProposition 7. For random variables Xn and Yn and real numbers x, y \u2265 0, if Pr(|Xn \u2212 x| \u2265 \u01eb) \u2264 gn(\u01eb) and Pr(|Yn \u2212 y| \u2265 \u01eb) \u2264 hn(\u01eb), then\nPr(|Xn/Yn\u2212x/y| \u2265 \u01eb) \u2264 gn (y\u01eb 4 ) +hn\n( \u01eby2\n4x\n) +hn (y 2 )\nAnd if 0 \u2264 x, y \u2264 1\nPr(|XnYn \u2212 xy| \u2265 \u01eb) \u2264 gn ( \u01eb 2 ) + hn ( \u01eb 2 )\n+ gn\n( \u01eb\n2y\n) + hn ( \u01eb 2x )\nLemma 1. Let Ci,j , MX\u0303iX\u0303 \u2032 j . Then Ci,j p\u2212\u2192 Ei,j = \u03b2i \u03b2ia R \u03b2\u22a4j \u03b2ja . Specifically,\nPr (|Ci,j \u2212 Ei,j | \u2265 \u01eb) \u2264 8 exp(\u2212M\u01eb2\u03b78/32)\nProof. By the definition of Ci,j , we have :\nCi,j = 1 MXiX \u2032\u22a4 j\n( 1MXi1)( 1 MX \u2032 j1)\np\u2212\u2192 E( 1MXiX \u2032\u22a4 j )\nE( 1MXi1)E( 1 MX \u2032 j1)\n(4)\nas M \u2192 \u221e, where 1 = (1, 1, . . . , 1)\u22a4 and the convergence follows because of convergence of numerator and denominator and then applying the Slutsky\u2019s theorem. The convergence of numerator and denominator are results of strong law of large numbers due to the fact that entries in Xi and X \u2032 i are independent.\nTo be precise, we have:\nE( 1MXiX \u2032\u22a4 j )\nE( 1MXi1)E( 1 MX \u2032 j1)\n= E\u03b8 EX|\u03b8( 1 MXiX \u2032\u22a4 j )\nE\u03b8 EX|\u03b8( 1 MXi1)E\u03b8 EX|\u03b8( 1 MX \u2032 j1)\n= E\u03b8( 1MAiA \u22a4 j )\nE\u03b8( 1MAi1)E\u03b8( 1 MAj1)\n= E\u03b8( 1M \u03b2i\u03b8\u03b8 \u22a4\u03b2j)\nE\u03b8( 1M \u03b2i\u03b81)E\u03b8( 1 M\u03b2j\u03b81)\n= \u03b2iR\u03b2\n\u22a4 j\n(\u03b2ia)(\u03b2ja)\n=Ei,j\nTo show the convergence rate explicitly, we use proposition 7. For simplicity, define Ci,j =\nFi,j GiHj . Note that\nentries in Xi and X \u2032 i are independent and bounded, by Hoeffding\u2019s inequality, we obtain:\nPr(|Fi,j \u2212 E(Fi,j)| \u2265 \u01eb) \u2264 2 exp(\u22122M\u01eb2) Pr(|Gi \u2212 E(Gi)| \u2265 \u01eb) \u2264 2 exp(\u22122M\u01eb2) Pr(|Hj \u2212 E(Hj)| \u2265 \u01eb) \u2264 2 exp(\u22122M\u01eb2)\nHence,\nPr(|GiHj \u2212 E(Gi)E(Hj)| \u2265 \u01eb) \u2264 8 exp(\u2212M\u01eb2/2)\nand\nPr (\u2223\u2223\u2223\u2223 Fi,j GiHj \u2212 E(Fi,j) E(Gi)E(Hj) \u2223\u2223\u2223\u2223 \u2265 \u01eb ) \u2264\n2 exp(\u2212M\u01eb2(\u03b2ja\u03b2ia)2/8)+8 exp(\u2212M\u01eb2(\u03b2ja\u03b2ia)4/32) + 8 exp(\u2212M(\u03b2ja\u03b2ia)2/8)\n(5)\nLet \u03b7 = min 1\u2264i\u2264W\n\u03b2ia \u2264 1. We obtain\nPr (\u2223\u2223\u2223\u2223 Fi,j GiHj \u2212 E(Fi,j) E(Gi)E(Hj) \u2223\u2223\u2223\u2223 \u2265 \u01eb )\n\u2264 18 exp(\u2212M\u01eb2\u03b78/32)\nCorollary 1. Ci,i\u22122Ci,j+Cj,j converges as M \u2192 \u221e. The convergence rate is c1 exp(\u2212Mc2\u01eb2\u03b78) for \u01eb error, with c1 and c2 being constants in terms of M . Corollary 2. Ci,i \u2212 Ci,j converges as M \u2192 \u221e. The convergence rate is d1 exp(\u2212Md2\u01eb2\u03b78) for \u01eb error, with d1 and d2 being constants in terms of M .\nRecall that we define Ck, k = 1, . . . ,K to be the novel words of topic k, and C0 to be the set of non-novel words. supp(\u03b2i) denotes the column indices of nonzero entries of a row vector \u03b2i of \u03b2 matrix. Lemma 2. If i, j \u2208 Ck, (i, j are novel words of the same topic), then Ci,i \u2212 2Ci,j + Cj,j p\u2212\u2192 0. Otherwise, \u2200k, if i \u2208 Ck, j /\u2208 Ck, then Ci,i\u2212 2Ci,j +Cj,j p\u2212\u2192 f(i,j) \u2265 d > 0 where d = \u03bb\u2227\u03b22\u2227. Especially, if i \u2208 C0 and j /\u2208 C0, then Ci,i \u2212 2Ci,j + Cj,j p\u2212\u2192 f(i,j) \u2265 d > 0\nProof. It was shown in lemma 1 that Ci,j p\u2212\u2192\n\u03b2i \u03b2ia R \u03b2\u22a4j \u03b2ja , where R is the correlation matrix and a = (a1, . . . , aK) \u22a4 is the mean of the prior. Hence Ci,i \u2212 2Ci,j + Cj,j p\u2212\u2192 ( \u03b2i\n\u03b2ia \u2212 \u03b2j \u03b2ja\n) R ( \u03b2i\n\u03b2ia \u2212 \u03b2j \u03b2ja\n)\n\u2265 \u03bb\u2227 \u2225\u2225\u2225\u2225 \u03b2i\n\u03b2ia \u2212 \u03b2j \u03b2ja\n\u2225\u2225\u2225\u2225 2\nNote that we\u2019ve assumedR to be positive definite with its minimum eigenvalue lower bounded by a positive value, \u03bb\u2227 > 0.\nIf i, j \u2208 Ck for some k, then \u03b2i\u03b2ia \u2212 \u03b2j \u03b2ja = 0 and hence Ci,i \u2212 2Ci,j + Cj,j p\u2212\u2192 0 . Otherwise, if supp(\u03b2i) 6= supp(\u03b2j), then\u2225\u2225\u2225 \u03b2i\u03b2ia \u2212 \u03b2j \u03b2ja \u2225\u2225\u2225 2\n\u2265 \u03b22\u2227, (note that \u03b2ia \u2264 1) which proves the first part of the lemma.\nFor the second part, note that if i \u2208 C0 and j /\u2208 C0, the support of \u03b2i and \u03b2j is necessarily different. Hence, the previous analysis directly leads to the conclusion.\nRecall that in Algorithm 1, Ji = {j : j 6= i, Ci,i \u2212 2Ci,j + Cj,j \u2265 d/2}. we have : Lemma 3. Ji converges in probability in the following senses:\n1. For a novel word i \u2208 Ck, define J\u2217i = Ckc . Then for all novel words i, lim\nM\u2192\u221e Pr(Ji \u2286 J\u2217i ) = 1.\n2. For a nonnovel word i \u2208 C0, define J\u2217i = C0c. Then for all non-novel words i, lim\nM\u2192\u221e Pr(Ji \u2287 J\u2217i ) = 1.\nProof. Let d , \u03bb\u2227\u03b22\u2227. According to the lemma 2, whenever supp(\u03b2j) 6= supp(\u03b2i), Di,j , Ci,i \u2212 2Ci,j + Cj,j\np\u2212\u2192 f(i,j) \u2265 d for the novel word i. In another word, for a novel word i \u2208 Ck and j /\u2208 Ck, Di,j will be concentrated around a value greater than or equal to\nd. Hence, the probability that Di,j be less than d/2 will vanish. In addition, by union bound we have\nPr(Ji * J \u2217 i ) \u2264 Pr(Ji 6= J\u2217i )\n= Pr(\u2203j \u2208 J\u2217i : j /\u2208 Ji) \u2264 \u2211\nj\u2208J\u2217 i\nPr(j /\u2208 Ji)\n\u2264 \u2211\nj /\u2208Ck\nPr(Di,j \u2264 d/2)\nSince \u2211\nj /\u2208Ck Pr(Di,j \u2264 d/2) is a finite sum of vanishing terms given i \u2208 Ck, Pr(Ji * J\u2217i ) also vanish asM \u2192 \u221e and hence we prove the first part.\nFor the second part, note that for a non-novel word i \u2208 C0,Di,j converges to a value no less than d provided that j /\u2208 C0 (according to the lemma 2). Hence\nPr(Ji + J \u2217 i ) \u2264 Pr(Ji 6= J\u2217i )\n= Pr(\u2203j \u2208 J\u2217i : j /\u2208 Ji) \u2264 \u2211\nj\u2208J\u2217 i\nPr(j /\u2208 Ji)\n\u2264 \u2211\nj /\u2208C0\nPr(Di,j \u2264 d/2)\nSimilarly \u2211\nj /\u2208C0 Pr(Di,j \u2264 d/2) vanishes for a nonnovel word i \u2208 C0 as M \u2192 \u221e, Pr(Ji + J\u2217i ) will also vanish and hence concludes the second part.\nAs a result of Lemma 1, 2 and 3, the convergence rate of events in Lemma 3 is :\nCorollary 3. For a novel word i \u2208 Ck we have Pr(Ji * J\u2217i ) \u2264 Wc1 exp(\u2212Mc3d2\u03b78). And for a non-novel word i \u2208 C0, Pr(Ji + J\u2217i ) \u2264 Kc1 exp(\u2212Mc4d2\u03b78), where c1, c3, and c4 are constants and d = \u03bb\u2227\u03b22\u2227.\nLemma 4. If \u2200i 6= j, Ri,iaiai \u2212 Ri,j aiaj \u2265 \u03b6, we have the following results on the convergence of Ci,i \u2212 Ci,j :\n1. If i is a novel word, \u2200j \u2208 Ji \u2286 J\u2217i : Ci,i\u2212Ci,j p\u2212\u2192\ng(i,j) \u2265 \u03b3 > 0, where J\u2217i is defined in lemma 3, \u03b3 , \u03b6a\u2227\u03b2\u2227 and a\u2227 is the minimum component of a. 2. If i is a non-novel word, \u2203j \u2208 J\u2217i such that Ci,i \u2212 Ci,j p\u2212\u2192 g(i,j) \u2264 0.\nProof. Let\u2019s reorder the words so that i \u2208 Ci. Using the equation (4), Ci,i p\u2212\u2192 Ri,iaiai and Ci,j p\u2212\u2192\u2211Kk=1 bk Ri,k aiak with bk , \u03b2j,kak\u2211 K l=1 \u03b2j,lal . Not that bk\u2019s are non-negative and sum up to one.\nBy the assumption, Ri,i aiai \u2212 Ri,jaiaj \u2265 \u03b6 for j 6= i. Note that \u2200j \u2208 Ji \u2286 J\u2217i , there exists some index k 6= i such that bk 6= 0. Then\nCi,i \u2212 Ci,j p\u2212\u2192 Ri,i aiai\n\u2212 K\u2211\nk=1\nbk Ri,k aiak\n=\nK\u2211\nk=1\nbk ( Ri,i aiai \u2212 Ri,k aiak )\n\u2265 \u03b6 \u2211\nk 6=i bk\nSince \u03b2ja \u2264 1, we have \u2211\nk 6=i bk \u2265 \u03b2\u2227a\u2227\u03b2ja \u2265 \u03b2\u2227a\u2227, and the first part of the lemma is concluded.\nTo prove the second part, note that for i \u2208 C0 and j /\u2208 C0,\nCi,j p\u2212\u2192\nK\u2211\nk=1\nbk Rj,k ajak\nwith bk = \u03b2i,k \u03b2ia . Now define :\nj\u2217i , argmax j\u2208J\u2217\ni\nK\u2211\nk=1\nbk Rj,k ajak\n(6)\nWe obtain,\nCi,i p\u2212\u2192\nK\u2211\nl=1\nbl\nK\u2211\nk=1\nbk Rl,k alak\n\u2264 K\u2211\nk=1\nbk Rj\u2217 i ,k\naj\u2217 i ak\nAs a result, Ci,i \u2212 Ci,j\u2217 i p\u2212\u2192 \u2211Kl=1 bl \u2211K k=1 bk Rl,k alak\n\u2212 \u2211K\nk=1 bk Rj\u2217 i ,k\naj\u2217 i ak\n\u2264 0 and the proof is complete."}, {"heading": "A.7. Proof of Theorem 1", "text": "Now we can prove the Theorem 1 in Section 5. To summarize the notations, let \u03b2\u2227 be a strictly positive lower bound on non-zero elements of \u03b2, \u03bb\u2227 be the minimum eigenvalue of R, and a\u2227 be the minimum component of mean vector a. Further we define \u03b7 = min\n1\u2264i\u2264W \u03b2ia\nand \u03b6 , min 1\u2264i6=j\u2264K Ri,i aiai \u2212 Ri,jaiaj > 0.\nTheorem 1 (in Section 5.1)\nFor parameter choices d = \u03bb\u2227\u03b22\u2227 and \u03b3 = \u03b6a\u2227\u03b2\u2227 the DDP algorithm is consistent as M \u2192 \u221e. Specifically, true novel and non-novel words are asymptotically declared as novel and non-novel, respectively. Furthermore, for\nM \u2265 C1\n( logW + log ( 1 \u03b41 ))\n\u03b22\u2227\u03b78 min(\u03bb 2 \u2227\u03b2 2 \u2227, \u03b62a 2 \u2227)\nwhere C1 is a constant, Algorithm 1 finds all novel words without any outlier with probability at least 1\u2212 \u03b41, where \u03b7 = min\n1\u2264i\u2264W \u03b2ia.\nProof of Theorem 1. Suppose that i is a novel word. The probability that i is not detected by the DDP Algorithm can be written as\nPr(Ji * J \u2217 i or (Ji \u2286 J\u2217i and \u2203j \u2208 Ji : Ci,i \u2212 Ci,j \u2264 \u03b3/2))\n\u2264 Pr(Ji * J\u2217i ) + Pr((Ji \u2286 J\u2217i and \u2203j \u2208 Ji : Ci,i \u2212 Ci,j \u2264 \u03b3/2)) \u2264 Pr(Ji * J\u2217i ) + Pr(\u2203j \u2208 J\u2217i : Ci,i \u2212 Ci,j \u2264 \u03b3/2) \u2264 Pr(Ji * J\u2217i ) + \u2211\nj\u2208J\u2217 i\nPr(Ci,i \u2212 Ci,j \u2264 \u03b3/2)\nThe first and second term in the right hand side converge to zero according to Lemma 3 and 4, respectively. Hence, this probability of failure in detecting i as a novel word converges to zero.\nOn the other hand, the probability of claiming a nonnovel word as a novel word by the Algorithm DDP can be written as :\nPr(Ji + J \u2217 i or (Ji \u2287 J\u2217i and \u2200j \u2208 Ji : Ci,i \u2212 Ci,j \u2265 \u03b3/2))\n\u2264 Pr(Ji + J\u2217i ) + Pr((Ji \u2287 J\u2217i and \u2200j \u2208 Ji : Ci,i \u2212 Ci,j \u2265 \u03b3/2)) \u2264 Pr(Ji + J\u2217i ) + Pr(\u2200j \u2208 J\u2217i : Ci,i \u2212 Ci,j \u2265 \u03b3/2) \u2264 Pr(Ji + J\u2217i ) + Pr(Ci,i \u2212 Ci,j\u2217i \u2265 \u03b3/2)\nwhere j\u2217i was defined in equation (6). We have shown in Lemma 3 and 4 that both of the probabilities in the right hand side converge to zero. This concludes the consistency of the algorithm.\nCombining the convergence rates given in the Corollaries 1, 2 and 3, the probability that the DDP Algorithm fails in finding all novel words without any outlier will be bounded by We1 exp(\u2212Me2min(d2, \u03b32)\u03b78), where e1 and e2 are constants and d and \u03b3 are defined in the Theorem."}, {"heading": "A.8. Proof of Theorem 2", "text": "Theorem 2 (in Section 5.2) For d = \u03bb\u2227\u03b22\u2227, given all true novel words as the input, the clustering algorithm, Algorithm 4 (ClusterNovelWords) asymptotically (as M \u2192 \u221e recovers K novel word indices of different types, namely, the support of the corresponding \u03b2 rows are different for any two retrieved indices."}, {"heading": "Furthermore, if", "text": "M \u2265 C2\n( logW + log ( 1 \u03b42 ))\n\u03b78\u03bb2\u2227\u03b2 4 \u2227\nthen Algorithm 4 clusters all novel words correctly with probability at least 1\u2212 \u03b42.\nProof of Theorem 2. The statement follows using (|I|\n2\n)\nnumber of union bounds on the probability that Ci,i\u2212 2Ci,j + Cj,j is outside an interval of the length d/2 centered around the value it converges to. The convergence rate of the related random variables are given in Lemma 1. Hence the probability that the clustering algorithm fails in clustering all the novel words truly is bounded by e1W\n2 exp(\u2212Me2\u03b78d2), where e1 and e2 are constants and d is defined in the theorem."}, {"heading": "A.9. Proof of Theorem 3", "text": "Theorem 3 (in Section 5.3) Suppose that Algorithm"}, {"heading": "5 outputs \u03b2\u0302 given the indices of K distinct novel words. Then, \u03b2\u0302", "text": "p\u2212\u2192 \u03b2. Specifically, if\nM \u2265 C3W 4(log(W ) + log(K) + log(1/\u03b43))\n\u03bb2\u2227\u03b78\u01eb4a 8 \u2227\nthen for all i and j, \u03b2\u0302i,j will be \u01eb close to \u03b2i,j with probability at least 1 \u2212 \u03b43, with \u01eb < 1, C3 being a constant, a\u2227 = mini ai and \u03b7 = min\n1\u2264i\u2264W \u03b2ia.\nProof. We reorder the rows so that Y and Y\u2032 be the first K rows of X and X\u2032, respectively. For the optimization objective function in Algorithm 5, if i < K, b = ei achieves the minimum, where all components of ei are zero, except its i\nth component, which is one. Now fix i, we denote the objective function as QM (b) = M(X\u0303i \u2212 bY)(X\u0303\u2032i \u2212 bY\u2032)\u22a4, and denote the optimal solution as b\u2217M . By the previous lemmas, QM (b) p\u2212\u2192 Q\u0304(b) = bDRDb\u22a4\u22122bDR \u03b2 \u22a4 i\n\u03b2ia + \u03b2i \u03b2ia R \u03b2\u22a4i \u03b2ia ,\nwhere D = diag(a)\u22121. Note that if R is positive definite, Q\u0304 is uniquely minimized at b\u2217 = \u03b2i\n\u03b2ia D\u22121.\nFollowing the notation in Lemma 1 and its proof,\nPr (|Ci,j \u2212 Ei,j | \u2265 \u01eb) \u2264 8 exp(\u2212M\u01eb2\u03b78/32)\nwhere Ci,j = MX\u0303iX\u0303 \u22a4 j , Ei,j = \u03b2i \u03b2ia R \u03b2\u22a4j \u03b2ja , and \u03b7 =\nmin 1\u2264i\u2264W \u03b2ia. Note that b \u2208 B = {b : 0 \u2264 bk \u2264 1, \u2211 bk = 1}. Therefore, \u2200s, r \u2208 {1, . . . ,K, i} :\n|Cs,r \u2212 Es,r| \u2264 \u01eb implies that\n\u2200b \u2208 B :|QM (b)\u2212 Q\u0304(b)| \u2264 |Ci,i \u2212 Ei,i|\n+\nK\u2211\nk=1\nbk|Ck,i \u2212 Ek,i|+ K\u2211\nk=1\nbk|Ci,k \u2212 Ei,k|\n+\nK\u2211\nr=1\nK\u2211\ns=1\nbrbs|Cr,s \u2212 Er,s|\n\u2264 4\u01eb\nHence\nPr ( \u2203b \u2208 B : |QM (b)\u2212 Q\u0304(b)| \u2265 4\u01eb )\n\u2264 Pr (\u2203i, j \u2208 {1, . . . ,K, i} : |Ci,j \u2212 Ei,j | \u2265 \u01eb) (7)\nUsing (K+1)2 union bounds for the right hand side of the equation 7, we obtain the following equation with c1 and c2 being two constants:\nPr ( \u2203b \u2208 B : |QM (b)\u2212 Q\u0304(b)| \u2265 \u01eb )\n\u2264 c1(K + 1)2 exp(\u2212c2M\u01eb2\u03b78) (8)\nNow we show that b\u2217M converge to b \u2217. Note that b\u2217 is the unique minimizer of the strictly convex function Q\u0304(b). The strict convexity of Q\u0304 is followed by the fact that R is assumed to be positive definite. Therefore, we have, \u2200\u01eb0 > 0, \u2203\u03b4 > 0 such that \u2016b \u2212 b\u2217\u2016 \u2265 \u01eb0 \u21d2 Q\u0304(b)\u2212 Q\u0304(b\u2217) \u2265 \u03b4. Hence,\nPr(\u2016b\u2217M \u2212 b\u2217\u2016 \u2265 \u01eb0) \u2264Pr(Q\u0304(b\u2217M )\u2212 Q\u0304(b\u2217) \u2265 \u03b4) \u2264Pr(Q\u0304(b\u2217M )\u2212QM (b\u2217M ) +QM (b\u2217M )\u2212QM (b\u2217)+ QM (b\n\u2217)\u2212 Q\u0304(b\u2217) \u2265 \u03b4) (i)\n\u2264 Pr(Q\u0304(b\u2217M )\u2212QM (b\u2217M ) +QM (b\u2217)\u2212 Q\u0304(b\u2217) \u2265 \u03b4) (ii)\n\u2264 Pr(2 sup b\u2208B |QM (b)\u2212 Q\u0304(b)| \u2265 \u03b4)\n\u2264Pr(\u2203b \u2208 B : |QM (b)\u2212 Q\u0304(b)| \u2265 \u03b4/2) (iii)\n\u2264 c1(K + 1)2 exp ( \u2212c2\n4 \u03b42\u03b78M\n)\nwhere (i) follows because QM (b \u2217 M ) \u2212QM (b\u2217) \u2264 0 by definition, (ii) holds considering the fact that b,b\u2217 \u2208 B and (iii) follows as a result of equation 8. For the \u01eb0 and \u03b4 relationship, let y = b\u2212 b\u2217,\nQ\u0304(b)\u2212 Q\u0304(b\u2217) = y(DRD)y\u22a4 \u2265 \u2016y\u20162\u03bb\u2217\nwhere \u03bb\u2217 > 0 is the minimum eigenvalue of DRD. Note that \u03bb\u2217 \u2265 ( min\n1\u2264j\u2264K a\u22121j ) 2\u03bb\u2227, where \u03bb\u2227 > 0 is a\nlower bound on the minimum eigenvalues of R. But 0 < aj \u2264 1, hence \u03bb\u2217 \u2265 \u03bb\u2227. Hence we could set \u03b4 = \u03bb\u2227\u01eb20. In sum, we could obtain\nPr(\u2016b\u2217M \u2212 b\u2217\u2016 \u2265 \u01eb0) \u2264 c1(K + 1)2 exp(\u2212c\u20322M\u01eb40\u03bb2\u2227\u03b78)\nfor the constants c1 and c\u20322. Or simply b \u2217 M p\u2212\u2192 b\u2217. Note that before column normalization, we let \u03b2\u0302i = ( 1MXi1)(b \u2217 M ). The convergence of the first term (to \u03b2ia), as we have already verified in Lemma 1, and using Slutsky\u2019s theorem, we get \u03b2\u0302i p\u2212\u2192 \u03b2iD\u22121. Hence after column normalization, which involves convergence of W random variables, by Slutsky\u2019s theorem again we can prove that \u03b2\u0302i p\u2212\u2192 \u03b2i for any 1 \u2264 i \u2264 W . This concludes our proof and directly implies the convergence in the Mean-Square sense.\nTo show the exact convergence rate, we apply the Proposition 7. For \u03b2\u0302i before column normalization, note that 1MXi1 converges to \u03b2ia with error probability 2 exp ( \u22122\u01eb2M ) , we obtain\nPr(|\u03b2\u0302i,j\u2212\u03b2i,jaj | \u2265 \u01eb) \u2264 e1(K+1)2 exp(\u2212e2\u03bb2\u2227\u03b78M\u01eb4) + e3 exp ( \u22122e4\u01eb2M )\nfor constants e1, . . . , e4. On the other hand, the column normalization factors can be obtained by 1\u22a4\u03b2\u0302. Denote normalization factor of the jth column by Pj =\u2211W\ni=1 \u03b2\u0302i,j and hence Pr(|Pj \u2212 aj | \u2265 \u01eb) \u2264 e1W (K + 1)2 exp(\u2212e2\u03bb2\u2227\u03b78M\u01eb4/W 4) + e3W exp ( \u2212e4\u01eb2M/W 2 ) . Now using the Proposition 7 again we obtain that after column normalization,\nPr (\u2223\u2223\u2223\u2223\u2223 \u03b2\u0302i,j\u2211W\nk=1 \u03b2\u0302k,j \u2212 \u03b2i,j\n\u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb )\n\u2264 f1(K + 1)2 exp(\u2212f2\u03bb2\u2227\u03b78M\u01eb4a4\u2227) + f3 exp ( \u22122f4\u01eb2Ma2\u2227 )\n+ f5W (K + 1) 2 exp(\u2212f6\u03bb2\u2227\u03b78M\u01eb4a8\u2227/W 4) + f7W exp ( \u2212f8\u01eb2Ma4\u2227/W 2 )\nfor constants f1, . . . , f8 and a\u2227 being the minimum value of ai\u2019s. Assuming \u01eb < 1, we can simplify the previous expression to obtain\nPr (\u2223\u2223\u2223\u2223\u2223 \u03b2\u0302i,j\u2211W\nk=1 \u03b2\u0302k,j \u2212 \u03b2i,j\n\u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb )\n\u2264 b1W (K + 1)2 exp(\u2212b2\u03bb2\u2227\u03b78M\u01eb4a8\u2227/W 4)\nfor constants b1 and b2. Finally, to get the error probability of the whole matrix, we can use WK union\nbounds. Hence we have :\nPr ( \u2203i, j : \u2223\u2223\u2223\u2223\u2223 \u03b2\u0302i,j\u2211W\nk=1 \u03b2\u0302k,j \u2212 \u03b2i,j\n\u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb )\n\u2264 b1W 2K(K + 1)2 exp(\u2212b2\u03bb2\u2227\u03b78M\u01eb4a8\u2227/W 4)\nTherefore, the sample complexity of \u01eb-close estimation of \u03b2i,j by the Algorithm 5 with probability at least 1\u2212 \u03b43 will be given by:\nM \u2265 C \u2032W 4(log(W ) + log(K) + log(1/\u03b43))\n\u03bb2\u2227\u03b78\u01eb4a 8 \u2227"}, {"heading": "B. Experiment results", "text": "B.1. Sample Topics extracted on NIPS dataset\nTables 4, 5, 6, and 7 show the most frequent words in topics extracted by various algorithms on NIPS dataset. The words are listed in the descending order. There are M = 1, 700 documents. Average words per document is N \u2248 900. Vocabulary size is W = 2, 500. It is difficult and confusing to group four sets of topics. We simply show topics extracted by each algorithm individually.\nB.2. Sample Topics extracted on New York Times dataset\nTables 8 to 11 show the most frequent words in topics extracts by algorithms on NY Times dataset. There are M = 300, 000 documents. Average words per document is N \u2248 300. Vocabulary size is W = 15, 000."}], "references": [{"title": "Advanced econometrics", "author": ["T. Amemiya"], "venue": null, "citeRegEx": "Amemiya,? \\Q1985\\E", "shortCiteRegEx": "Amemiya", "year": 1985}, {"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A Practical Algorithm for Topic Modeling with Provable Guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "Zhu", "Michael"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Learning topic models \u2013 going beyond", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "SVD. arXiv:1204.1956v2 [cs.LG],", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A correlated topic model of science. annals of applied statistics", "author": ["D. Blei", "J. Lafferty"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Blei and Lafferty,? \\Q2007\\E", "shortCiteRegEx": "Blei and Lafferty", "year": 2007}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Commun. ACM,", "citeRegEx": "Blei,? \\Q2012\\E", "shortCiteRegEx": "Blei", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S. Amari"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "In Advances in Neural Information Processing Systems 16,", "citeRegEx": "Donoho and Stodden,? \\Q2004\\E", "shortCiteRegEx": "Donoho and Stodden", "year": 2004}, {"title": "Finding scientific topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "In Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers", "year": 2004}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791,", "citeRegEx": "Lee and Seung,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Pachinko allocation: Dagstructured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li and McCallum,? \\Q2007\\E", "shortCiteRegEx": "Li and McCallum", "year": 2007}, {"title": "Factoring nonnegative matrices with linear programs", "author": ["B. Recht", "C. Re", "J. Tropp", "V. Bittorf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Recht et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "We adopt the classic \u201cbags of words\u201d modeling paradigm widely-used in probabilistic topic modeling (Blei, 2012).", "startOffset": 99, "endOffset": 111}, {"referenceID": 7, "context": "To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function.", "startOffset": 88, "endOffset": 174}, {"referenceID": 12, "context": "To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function.", "startOffset": 88, "endOffset": 174}, {"referenceID": 6, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling.", "startOffset": 34, "endOffset": 65}, {"referenceID": 5, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling.", "startOffset": 34, "endOffset": 65}, {"referenceID": 1, "context": "(Anandkumar et al., 2012) describe a novel method of moments approach.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "In fact, one may use the convergence of extremum estimators (Amemiya, 1985) to show the consistency of this step.", "startOffset": 60, "endOffset": 75}, {"referenceID": 0, "context": "\u03b2\u0302 p \u2212\u2192 \u03b2\u2217 (Amemiya, 1985).", "startOffset": 11, "endOffset": 26}, {"referenceID": 5, "context": "Following typical settings in (Blei, 2012) and (Arora et al.", "startOffset": 30, "endOffset": 42}, {"referenceID": 1, "context": ", 2012a); ECA from (Anandkumar et al., 2012); Gibbs from (Griffiths & Steyvers, 2004); NMF from (Lee & Seung, 1999).", "startOffset": 19, "endOffset": 44}, {"referenceID": 1, "context": "R) max { C1aK 3 log(W ) \u01eb\u03b36p6 , C2a K log(W ) \u01eb3\u03b34p4 } Separable \u03b2; Robust Simplicial Property of R Pr(Error) \u2192 0; Requires Novel words to be linearly independent; ECA (Anandkumar et al., 2012) O(W 3 +MN) N/A : For the provided basic algorithm, the probability of error is at most 1/4 but does not converge to zero LDA model; The concentration parameter of the Dirichlet distribution \u03b10 is known Requires solving SVD for large matrix, which makes it impractical; Pr(Error) 9 0 for the basic algorithm Gibbs (Griffiths & Steyvers, 2004) N/A N/A LDA model No convergence guarantee NMF (Tan & F\u00e9votte, in press) N/A N/A General model Non-convex optimization; No convergence guarantee", "startOffset": 168, "endOffset": 193}], "year": 2013, "abstractText": "We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics. We will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme.", "creator": "LaTeX with hyperref package"}}}