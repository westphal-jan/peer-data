{"id": "1303.4207", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "Improving CUR Matrix Decomposition and the Nystr\\\"{o}m Approximation via Adaptive Sampling", "abstract": "The Nystrom method has been widely used to compute low-rank approximations to positive semidefinite matrices. Much work has been done on the sampling techniques for the Nystrom method, and many were guaranteed with upper error bounds. However, the lower bounds, especially the relative error bounds, were largely unknown. We explore in this paper the lower bounds of the Nystrom method to show how bad the Nystrom method can perform. We establish both additive and relative error lower bounds for the Nystrom method. Interestingly, some of the relative-error lower bounds we established can be attained by some algorithms, which indicates some lower bounds we establish are tight. However, some lower bounds reveal the downside of the Nystrom method that they are much worse than the column selection problem and the CUR problem. Sadly, we also show that some bounds cannot be improved by the ensemble Nystrom method. For example, the ensemble Nystrom method uses is very simple, and it can take some very sophisticated modeling.\n\n\nIn a previous article, we focused on the Nystrom method in this article. The Nystrom method is a small method for estimating a matrix of negative dimensional vectors, and the data that it produces is very simple. We use it to estimate the sum of the coefficients of the negative-dimensional vectors. The algorithm is a simple algorithm that gives us an approximation of a matrix of positive-dimensional vector. We also use it to find an approximation of a matrix of negative-dimensional vectors. We also apply this to the average value of the values of the non-negative-dimensional vectors in the range of 1,000. These values are given as the value of the value of a normalized number of values. It is important to note that each of these values can be computed in a single step, and there are no limits.\nThe Nystrom method is not in the correct order; we use a different method. This is called the inverse function, and it performs the inverse function, the number of values of values of the negative-dimensional vectors and the mean number of values of values of the positive-dimensional vectors.\nIn this article we demonstrate the inverse function, and then show the inverse function, the mean number of values of the negative-dimensional vectors and the mean number of values of the negative-dimensional vectors. We test the inverse function as a function of the normalized number of values of the negative-dimensional vectors and the mean number of values of the negative-dimensional vectors and the mean number of values of the negative-", "histories": [["v1", "Mon, 18 Mar 2013 11:17:55 GMT  (76kb)", "http://arxiv.org/abs/1303.4207v1", "15 pages, 2 figures, preprint"], ["v2", "Wed, 20 Mar 2013 04:20:45 GMT  (61kb)", "http://arxiv.org/abs/1303.4207v2", "15 pages, 2 figures, preprint"], ["v3", "Tue, 9 Apr 2013 08:28:43 GMT  (168kb)", "http://arxiv.org/abs/1303.4207v3", "A proportion of this work has appeared in NIPS 2012 [arXiv:1210.1461]"], ["v4", "Tue, 28 May 2013 07:17:40 GMT  (208kb,D)", "http://arxiv.org/abs/1303.4207v4", "A proportion of this work has appeared in NIPS 2012 [arXiv:1210.1461]"], ["v5", "Fri, 31 May 2013 05:12:41 GMT  (217kb,D)", "http://arxiv.org/abs/1303.4207v5", "A proportion of this work has appeared in NIPS 2012"], ["v6", "Thu, 12 Sep 2013 06:56:13 GMT  (226kb,D)", "http://arxiv.org/abs/1303.4207v6", null], ["v7", "Tue, 1 Oct 2013 06:31:11 GMT  (226kb,D)", "http://arxiv.org/abs/1303.4207v7", null]], "COMMENTS": "15 pages, 2 figures, preprint", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["shusen wang", "zhihua zhang"], "accepted": false, "id": "1303.4207"}, "pdf": {"name": "1303.4207.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhihua Zhang"], "emails": ["wss@zju.edu.cn", "zhzhang@zju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 3.\n42 07\nv1 [\ncs .L\nG ]\n1 8"}, {"heading": "1. Introduction", "text": "In Section 1.1 we give a non-technical introduction to the Nystro\u0308m method as well as the related column selection problem and the CUR problem. In Section 1.2 we summarize our contributions and list our main results. In Section 1.3 we provide an outline of the rest of this paper."}, {"heading": "1.1 Backgrounds", "text": "The Nystro\u0308m method is a kind of low-rank matrix approximation approach mainly used for speeding up kernel methods. The Nystro\u0308m method can approximate any given positive semidefinite matrix using only a subset of its columns, so it alleviates the computation and storage problems when the kernel matrix is large in size. The Nystro\u0308m method has been extensively used in the machine learning community. For example, the Nystro\u0308m method has been applied to Gaussian process Williams and Seeger (2001), kernel SVM (Kumar et al., 2012, Zhang et al., 2008), spectral clustering (Fowlkes et al., 2004), and kernel PCA (Talwalkar et al., 2008, Zhang et al., 2008, ?).\nWilliams and Seeger (2001) are the first to use the Nystro\u0308m method for low-rank matrix approximation. The uniform column selection proposed by Williams and Seeger (2001) is\nthe simplest but the most widely used in practice (Fowlkes et al., 2004, Talwalkar et al., 2008, Gittens and Mahoney, 2013). Following the work of Williams and Seeger (2001), many uniform and non-uniform sampling techniques are proposed and analyzed (Drineas and Mahoney, 2005, Zhang et al., 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013).\nMuch work has been done on the upper bounds of the sampling techniques for the Nystro\u0308m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al. (2011). However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds. The recent work of Gittens (2011) provided the relative-error bounds for the first time, and then the bounds were sharpened by Gittens and Mahoney (2013). Furthermore, a method called the ensemble Nystro\u0308m method was proposed by Kumar et al. (2009a, 2012) to improve the standard Nystro\u0308m method by combining a collection of sampled columns. The ensemble Nystro\u0308m method demonstrated significant empirical improvement over the standard Nystro\u0308m method.\nHowever, the lower error bounds of the Nystro\u0308m method were largely unknown. To our knowledge, the only known lower bound was established in (Jin et al., 2011, Theorem 4) on the additive-error in spectral norm. However, this lower bound is not quite informative because it lower bounds the additive-error on a matrix with potentially very large norm. We will give technical analyses of this lower bound in Section 4.1.\nThe Nystro\u0308m method is closely related to the column selection problem and can benefit from the column selection techniques (Kumar et al., 2012, Gittens and Mahoney, 2013). Column selection has been very widely studied in the literature (Frieze et al., 2004, Deshpande et al., 2006, Drineas et al., 2008, Deshpande and Rademacher, 2010, Boutsidis et al., 2011b, Guruswami and Sinop, 2012). The most advanced relative-error columns selection techniques include the adaptive sampling algorithm in (Deshpande et al., 2006), the nearoptimal algorithm in (Boutsidis et al., 2011b), and the optimal algorithm in (Guruswami and Sinop, 2012) whose upper bound matches the lower bound of column selection problem (Boutsidis et al., 2011a).\nThe Nystro\u0308m method is also related to the CUR matrix decomposition which selects both columns and rows to form a low-rank approximation. As well as the Nystro\u0308m method, CUR is also widely studied in the literature (Goreinov et al., 1997a,b, Stewart, 1999, Tyrtyshnikov, 2000, Drineas, 2003, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Bien et al., 2010, Wang and Zhang, 2012). The CUR problem is actually an extension of the Nystro\u0308m method to a general matrix. We will show in Section 4.3 that the lower error bound of the Nystro\u0308m method is even worse than the upper bound of the state-of-the-art CUR algorithm in (Wang and Zhang, 2012)."}, {"heading": "1.2 Summary of Contributions", "text": "In this paper we establish lower bounds, both additive-error and relative-error, for the Nystro\u0308m method and the ensemble Nystro\u0308m method. To our knowledge, we are the first\nto establish relative-error lower bounds for the Nystro\u0308m method. Interestingly, two of our relative-error lower bounds can be attain by some algorithms in (Gittens and Mahoney, 2013), which indicates the two lower bounds are tight and the corresponding upper bounds (Gittens and Mahoney, 2013) are near optimal.\nWe summarize the lower bounds in the table below. Here A denotes the original matrix, A\u0303 denotes the approximation to A, Ak denotes the best rank-k approximation to A.\n\u2016A\u2212A\u0303\u2016F maxi,j |aij | \u2016A\u2212A\u0303\u20162 maxi,j |aij | \u2016A\u2212A\u0303\u2016\u2217 maxi,j |aij | \u2016A\u2212A\u0303\u2016F \u2016A\u2212Ak\u2016F\n\u2016A\u2212A\u0303\u20162 \u2016A\u2212Ak\u20162 \u2016A\u2212A\u0303\u2016\u2217 \u2016A\u2212Ak\u2016\u2217\nStandard Nystro\u0308m \u2126 (\nm c\n)\n\u2126 (\nm c\n)\n\u2126 ( m )\n\u2126 (\u221a\nm c\n)\n\u2126 (\nm c\n)\u22c6 \u2126 ( 1 )\u22c6\nEnsemble Nystro\u0308m \u2126 (\nm c\n)\n\u2013 \u2013 \u2126 (\u221a\nm c\n)\n\u2013 \u2013\nThe lower bounds marked with \u2018\u22c6\u2019 are tight; the tightness of the rest lower bounds are unknown at present. The blanks indicate the lower bounds are unknown to us; we conjecture that the lower bounds of the ensemble Nystro\u0308m method are identical to the standard Nystro\u0308m method."}, {"heading": "1.3 Organization of This Paper", "text": "In Section 2 we define the notations, describe the Nystro\u0308m method and the ensemble Nystro\u0308m method, and construct the adversary case for deriving the lower bounds. In Section 3 we establish the lower bounds for the Nystro\u0308m method and the ensemble Nystro\u0308m method. In Section 4 we discuss some previous results related to this work. All technical proofs are in the appendix."}, {"heading": "2. Preliminaries", "text": "In Section 2.1 we introduce the notations used in this paper. In Section 2.2 we briefly describe the Nystro\u0308m method and the ensemble Nystro\u0308m method. In Section 2.3 we construct the adversary case which is used throughout this paper for deriving the lower bounds."}, {"heading": "2.1 Notations", "text": "For a matrix A = [aij ] \u2208 Rm\u00d7n, let a(i) be its i-th row, aj be its j-th column, and Ai:j be a submatrix consisting of its i to j-th columns. Moreover, let Im denote the m\u00d7m identity matrix, 1m denote an all-one column vector of size m, 1mn denote an m\u00d7n all-one matrix, and 0 denote an all-zero matrix with proper size.\nLet \u03c1 = rank(A) and k \u2264 \u03c1, the singular value decomposition (SVD) of A can be written as\nA =\n\u03c1 \u2211\ni=0\n\u03c3A,iuA,iv T A,i = UA\u03a3AV T A =\n[ UA,k UA,k\u22a5 ]\n[\n\u03a3A 0 0 \u03a3A,k\u22a5\n] [\nVT A,k\nVT A,k\u22a5\n]\n,\nwhere UA,k, \u03a3A,k, and VA,k correspond to the top k singular values. We denote Ak = UA,k\u03a3A,kV T A,k which is the closest rank-k approximation to A. We also use \u03c3i(A) = \u03c3A,i to denote the i-th greatest singular value. When A is positive semidefinite, the SVD is identical to the eigenvalue decomposition and UA = VA.\nWe define the matrix norms as follows. Let \u2016A\u20161 = \u2211 i,j |aij | be the \u21131-norm, \u2016A\u2016F = ( \u2211\ni,j a 2 ij) 1/2 = \u2211 i \u03c3 2 A,i be the Frobenius norm, \u2016A\u20162 = max\u2016x\u20162=1 \u2016Ax\u20162 = \u03c3A,1 be the\nspectral norm, and \u2016A\u2016\u2217 = \u2211\ni \u03c3A,i be the nuclear norm. Furthermore, let A\u2020 = VA,\u03c1\u03a3 \u22121 A,\u03c1U T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). When A is nonsingular, the Moore-Penrose inverse is identical to the matrix inverse. Given matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXU T X A \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y \u2208 Rm\u00d7n is the projection of A onto the row space of Y."}, {"heading": "2.2 The Nystro\u0308m Methods", "text": "Given an m\u00d7m positive semidefinite matrix\nA =\n[\nW AT21 A21 A22\n]\n,\nthe standard Nystro\u0308m method samples c columns of A to construct C \u2208 Rm\u00d7c. Without loss of generality, we assume that the first c columns are selected, that is,\nC =\n[\nW A21\n]\n.\nThen the rank-c Nystro\u0308m approximation to A is\nA\u0303nysc = CW \u2020CT =\n[\nW AT21 A21 A21W \u2020AT21\n]\n.\nA little differently, the ensemble Nystro\u0308m method in (Kumar et al., 2009a) selects a collections of t samples, each sample C(i), (i = 1, \u00b7 \u00b7 \u00b7 , t), contains c columns of A. Then the ensemble method combine the samples to construct an approximation\nA\u0303enst,c =\nt \u2211\ni=1\n\u00b5(i)C(i)W(i) \u2020 C(i) T ,\nwhere \u00b5(i) are the weights of the samples. Then the ensemble Nystro\u0308m method seeks to find out the weights that minimize \u2016A \u2212 A\u0303enst,c \u2016F or \u2016A \u2212 A\u0303enst,c \u20162. A simple but effective strategy is setting the weights to be \u00b5(1) = \u00b7 \u00b7 \u00b7 = \u00b5(t) = 1t ."}, {"heading": "2.3 Construction of The Adversary Case", "text": "We construct an m\u00d7m positive definite matrix A as follows:\nA = (1\u2212 \u03b1)Im + \u03b11mm =\n\n    1 \u03b1 \u00b7 \u00b7 \u00b7 \u03b1 \u03b1 1 \u00b7 \u00b7 \u00b7 \u03b1 ... ... . . .\n... \u03b1 \u03b1 \u00b7 \u00b7 \u00b7 1\n\n    =\n[\nW AT21 A21 A22\n]\n, (1)\nwhere \u03b1 \u2208 [0, 1). It is easy to verify xTAx \u2265 0 for all x \u2208 Rm and xTAx > 0 if x 6= 0. We show some properties of A in Lemma 1.\nLemma 1 Let Ak be the closest rank-k approximation to the matrix A constructed in (1). Then we have that\n\u2016A\u2016F = \u221a m2\u03b12 +m(1\u2212 \u03b12), \u2016A\u2212Ak\u2016F = \u221a m\u2212 k (1\u2212 \u03b1),\n\u2016A\u20162 = 1 +m\u03b1\u2212 \u03b1 , \u2016A\u2212Ak\u20162 = 1\u2212 \u03b1, \u2016A\u2016\u2217 = m, \u2016A\u2212Ak\u2016\u2217 = (m\u2212 k)(1 \u2212 \u03b1),\nwhere 1 \u2264 k \u2264 m\u2212 1."}, {"heading": "3. Main Results", "text": "We show in this section the lower bounds, both additive and relative, of the Nystro\u0308m method and the ensemble Nystro\u0308m method."}, {"heading": "3.1 Lower Bounds of The Standard Nystro\u0308m Method", "text": "Theorem 2 (Additive-error lower bounds of the Nystro\u0308m method) For a matrix with diagonal entries equal to one and off-diagonal entries equal to \u03b1 \u2208 [0, 1), the approximation error incurred by the Nystro\u0308m method is lower bounded by\n\u2225 \u2225A\u2212 A\u0303nysc \u2225 \u2225 F \u2265 (1\u2212 \u03b1)\n\u221a\n(m\u2212 c) ( 1 + m+ c\n(c+ 1\u2212\u03b1\u03b1 ) 2\n)\n,\n\u2225 \u2225A\u2212 A\u0303nysc \u2225 \u2225 2 \u2265\n(1\u2212 \u03b1) ( m+ 1\u2212\u03b1\u03b1 )\nc+ 1\u2212\u03b1\u03b1 ,\n\u2225 \u2225A\u2212 A\u0303nysc \u2225 \u2225 \u2217 \u2265 (m\u2212 c)(1\u2212 \u03b1) 1 + c\u03b1\n1 + c\u03b1\u2212 \u03b1 .\nTheorem 3 (Relative-error lower bounds of the Nystro\u0308m method) The relative-error ratios of the Nystro\u0308m method are lower bounded by\n\u2016A\u2212CW\u2020CT\u2016F \u2016A\u2212Ak\u2016F \u2265 \u221a m\u2212 c m\u2212 k ( 1 + m+ c (c+ 1)2 ) ,\n\u2016A\u2212CW\u2020CT \u20162 \u2016A\u2212Ak\u20162 \u2265 m+ 1 c+ 1 , \u2016A\u2212CW\u2020CT \u2016\u2217 \u2016A\u2212Ak\u2016\u2217 \u2265 m\u2212 c m\u2212 k ( 1 + 1 c\u2212 1 ) ,\nwhere k < m is an arbitrary positive integer. The relative-error lower bounds are interesting when A is nonsingular, otherwise \u2016A\u2212Ak\u2016\u03be, (\u03be = 2 or F ), can be zero and the bounds are thereby trivial.\nProof We use the matrix A constructed in (1) and set \u03b1 = 1/2, then Theorem 3 follow directly from Lemma 1 and Theorem 2.\nRemark 4 As for the spectral norm bounds, we will show in Section 4.1 the lower bounds established in Theorem 2 and 3 are better than the previous work of Jin et al. (2011). Actually, relative-error lower bounds in spectral norm and nuclear norm we established are tight because they are attained by some algorithms in (Gittens and Mahoney, 2013, Lemma 4). This will be discussed in Section 4.2."}, {"heading": "3.2 Lower Bounds of The Ensemble Nystro\u0308m Method", "text": "Theorem 5 (Additive-error lower bounds of the ensemble Nystro\u0308m method) Assume that the ensemble Nystro\u0308m method selects a collection of t samples, each sample C(i), (i = 1, \u00b7 \u00b7 \u00b7 , t), contains c columns of A without overlapping. For a matrix with all diagonal entries equal to one and off-diagonal entries equal to \u03b1 \u2208 [0, 1), the approximation error incurred by the ensemble Nystro\u0308m method is lower bounded by\n\u2225 \u2225A\u2212 A\u0303enst,c \u2225 \u2225 F \u2265 (1\u2212 \u03b1)\n\u221a\n( m\u2212 2c+ c t )( 1 + m+ 2\u03b1 \u2212 2 (c+ 1\u2212\u03b1\u03b1 ) 2 ) ,\nwhere A\u0303enst,c = 1 t \u2211t i=1 C\n(i)W(i) \u2020 C(i) T .\nTheorem 6 (Relative-error lower bounds of the ensemble Nystro\u0308m method) Assume that the ensemble Nystro\u0308m method selects a collection of t samples, each sample C(i), (i = 1, \u00b7 \u00b7 \u00b7 , t), contains c columns of A without overlapping. The relative-error ratio of the ensemble Nystro\u0308m method is lower bounded by\n\u2016A\u2212 A\u0303enst,c \u2016F \u2016A\u2212Ak\u2016F\n\u2265 \u221a\nm\u2212 2c+ c/t m\u2212 k ( 1 + m+ 2 (c+ 1)2 ) .\nwhere A\u0303enst,c = 1 t \u2211t i=1 C\n(i)W(i) \u2020 C(i) T .\nProof The theorem follows directly from Theorem 5 and Lemma 1 by setting \u03b1 = 1/2.\nRemark 7 Theorem 6 shows that, when m \u226b c, k, it is impossible for the ensemble Nystro\u0308m method to achieve a relative error ratio \u2016A\u2212CW\u2020CT \u20162 F\n\u2016A\u2212Ak\u20162F \u2264 1 + \u01eb unless c is large\nenough: c > \u221a m \u01eb +Const. This shows that the ensemble Nystro\u0308m method does not improve the lower bounds of the Nystro\u0308m method."}, {"heading": "4. Comparisons with Related Work", "text": "In Section 4.1 we compare with the previous work of Jin et al. (2011) and show the superiority of our results. In Section 4.2 we show that the relative-error lower bounds in spectral norm and nuclear norm is actually tight for it matches an upper bound exactly. In Section 4.3 we compare the Nystro\u0308m method with the CUR method and show the downside of the Nystro\u0308m method."}, {"heading": "4.1 An Additive-Error Lower Bound in Spectral Norm", "text": "The previous work of Jin et al. (2011) established a lower error bound for the Nystro\u0308m method. We show it in the following proposition.\nProposition 8 There exists a positive semidefinite matrix A \u2208 Rm\u00d7m, where (i) all its diagonal entries are 1, and (ii) the first c+ 1 eigenvalues of A are in the order of \u2126(m/c), such that for any sampling strategy that selects c columns to form C, the approximation error of the Nystro\u0308m method is lower bounded by\n\u2225 \u2225A\u2212CW\u2020CT \u2225 \u2225\n2 \u2265 \u2126\n(m\nc\n)\n,\nprovided that c > 64(log 4)2m2.\nRemark 9 This error bound in (Jin et al., 2011, Theorem 4) is not particularly interesting because it lower bounds the approximation error on a matrix with very large norms. According to the construction of A we have that\n\u2016A\u20162 = \u2016A\u2212Ac\u20162 \u223c \u2126 (m\nc\n)\n,\nso the corresponding relative error bounds are \u2225\n\u2225A\u2212CW\u2020CT \u2225 \u2225\n2 \u2016A\u2212Ac\u20162 \u223c O(1).\nIt is well known that any rank-c approximation A\u0303approxc of A obeys \u2225\n\u2225A\u2212 A\u0303approxc \u2225 \u2225\n\u2016A\u2212Ac\u2016 \u2265 1.\nThus the error bound in (Jin et al., 2011, Theorem 4) does not actually show how bad the Nystro\u0308m method can perform compared with the truncated SVD. Furthermore, Theorem 2 in this paper is superior over (Jin et al., 2011, Theorem 4) because Theorem 2 make no assumption on c. In other words, our lower bound still holds when c is small, but (Jin et al., 2011, Theorem 4) fails."}, {"heading": "4.2 Optimal Relative-Error Upper Bounds", "text": "The very recent work of Gittens and Mahoney (2013) established several relative-error upper bound for the Nystro\u0308m method. We show their results in the following proposition.\nProposition 10 Given a positive semidefinite matrix A \u2208 Rm\u00d7m and an integer k \u2264 m, let \u03c4 denote the coherence of a dominant k-dimensional invariant subspace of A. Fix a nonzero failure probability \u03b4 and accuracy factor \u01eb \u2208 (0, 1). If c \u2265 2\u03c4\u01eb\u22122k log(k/\u03b4) columns of A are chosen uniformly at random, then\n\u2225 \u2225A\u2212CW\u2020CT \u2225 \u2225\n2 \u2016A\u2212Ak\u20162 \u2264 1 + m\n(1\u2212 \u01eb)c , \u2225 \u2225A\u2212CW\u2020CT \u2225 \u2225\n\u2217 \u2016A\u2212Ak\u2016\u2217 \u2264 1 + 1 (1\u2212 \u01eb)\u03b4\nhold with probability at least 1\u2212 4\u03b4.\nProposition 10 shows that, when c is large, the relative-error upper bounds in spectral norm and nuclear norm match our lower bounds (Theorem 3) exactly! It indicates that our relative-error lower bounds in spectral norm and nuclear norm are tight and attainable. It also shows the relative-error upper bounds in spectral norm and nuclear norm in Gittens and Mahoney (2013) are near optimal."}, {"heading": "4.3 Relative-Error Upper Bound of CUR", "text": "We have discussed in the introduction section that the Nystro\u0308m method is closely related to the CUR problem. We show in the following proposition that the state-of-the-art CUR algorithm in (Wang and Zhang, 2012) achieved a relative-error ratio of E\u2016A\u2212CUR\u20162 F\n\u2016A\u2212Ak\u20162F \u2264\n1 + 4kc ( 1 + o(1) ) .\nProposition 11 Given a matrix A \u2208 Rm\u00d7n and a positive integer k \u226a min{m,n}, a CUR algorithm randomly selects c = 4k\u01eb (1+ o(1)) columns of A to construct C \u2208 Rm\u00d7c, and then selects r = 4c\u01eb (1 + o(1)) rows of A to construct R \u2208 Rr\u00d7n. The relative-error ratio is upper bounded by\nE\u2016A\u2212CUR\u20162F = E\u2016A\u2212C(C\u2020AR\u2020)R\u20162F \u2264 (1 + \u01eb)\u2016A\u2212Ak\u20162F .\nIt is quite often that the spectrum of a matrix is dominated by several top singular values, so k is usually set to be small. Thus the relative-error upper bound of the CUR algorithm is small. In comparison, the lower bound of the Nystro\u0308m method is roughly \u2016A\u2212A\u0303nysc \u20162F \u2016A\u2212Ak\u20162F\n\u2265 1+ mc2 . Consider that the Nystro\u0308m method is designed for situations where m is huge, so the lower bound of the Nystro\u0308m method can be even much worse than the upper bound of CUR! It seems that CUR is a better approximation approach than the Nystro\u0308m method."}, {"heading": "5. Conclusions", "text": "We have explored in this paper the lower bounds of the Nystro\u0308m method and the ensemble Nystro\u0308m method. This is the first work that conducts a systematic analysis of the lower bounds of the Nystro\u0308m method. Among the lower bounds we established, two relative-error bounds are tight and attainable, while the tightness of the rest bounds remains unknown. The lower bounds reveals that the Nystro\u0308m method can be sometimes very bad; the lower error bounds can be even much worse than the upper bound of some CUR algorithms. So we further explored the ensemble Nystro\u0308m method for a remedy, but only to have found the ensemble Nystro\u0308m method does not help to improve the lower bounds."}, {"heading": "Appendix A. Proof of Lemma 1", "text": "Proof The squared Frobenius norm of A is \u2016A\u20162F = \u2211 i,j a 2 ij = m + (m 2 \u2212 m)\u03b12. The spectral norm, i.e. the greatest singular value of A is\n\u2016A\u20162 = \u03c3A,1 = max \u2016x\u20162\u22641 \u2016Ax\u20162 = 1 +m\u03b1\u2212 \u03b1.\nIt is also easy to verify that \u03c3A,2 = \u00b7 \u00b7 \u00b7 = \u03c3A,m. Since \u2016A\u20162F = \u2211m\ni=1 \u03c3A,i, for any j > 1, we have that\n\u03c32A,j = 1 m\u2212 1 ( \u2016A\u20162F \u2212 \u03c32A,1 ) = (1\u2212 \u03b1)2.\nSo \u2016A\u2212Ak\u20162 = 1\u2212 \u03b1 for all 1 \u2264 k < m. Finally we have that\n\u2016A\u2212Ak\u20162F = \u2016A\u20162F \u2212 k \u2211\ni=1\n\u03c32A,i = (m\u2212 k)(1\u2212 \u03b1)2,\n\u2016A\u2212Ak\u2016\u2217 = (m\u2212 k)\u03c3A,2 = (m\u2212 k)(1\u2212 \u03b1),\n\u2016A\u2016\u2217 = m \u2211\ni=1\n\u03c3A,i = (1 +m\u03b1\u2212 \u03b1) + (m\u2212 1)(1 \u2212 \u03b1) = m,\nwhich complete our proofs."}, {"heading": "Appendix B. Proof of Theorem 2", "text": "Proof The residual of the Nystro\u0308m approximation is\n\u2016A\u2212 A\u0303nysc \u2016\u03be = \u2016A22 \u2212A21W\u2020AT21\u2016\u03be, (2)\nwhere \u03be = 2 or F . Since (1 \u2212 \u03b1)Ic + \u03b11cc is nonsingular when \u03b1 \u2208 [0, 1), so W\u2020 = W\u22121. We apply the Woodbury approximation\n(A+BCD)\u22121 = A\u22121 \u2212A\u22121B(C\u22121 +DA\u22121B)\u22121DA\u22121\nto W\u2020 and obtain that\nW\u2020 = 1 1\u2212 \u03b1Ic \u2212 \u03b1 (1\u2212 \u03b1)(1 \u2212 \u03b1+ c\u03b1)1cc.\nAccording to the construction, A21 is an (m\u2212 c)\u00d7 c matrix with all entries equal to \u03b1, it follows that A21W \u2020AT21 is an (m\u2212 c)\u00d7 (m\u2212 c) matrix with all entries equal to\n\u03b7 , \u03b121Tc W \u20201c =\nc\u03b12\n1\u2212 \u03b1+ c\u03b1 . (3)\nCombining with (2), we have that\n\u2016A\u2212 A\u0303nysc \u20162F = \u2225 \u2225(1\u2212 \u03b1)Im\u2212c + \u03b11m\u2212c,m\u2212c \u2212 \u03b71m\u2212c,m\u2212c \u2225 \u2225 2 F (4)\n= (m\u2212 c) ( 1\u2212 \u03b7 )2 + ( (m\u2212 c)2 \u2212 (m\u2212 c) ) ( \u03b1\u2212 \u03b7 )2\n= (m\u2212 c)(1 \u2212 \u03b1)2 ( 1 + \u03b12(m+ c) + 2(\u03b1 \u2212 \u03b12) (1\u2212 \u03b1+ c\u03b1)2 ) \u2265 (m\u2212 c)(1 \u2212 \u03b1)2 ( 1 + m+ c\n(c+ 1\u2212\u03b1\u03b1 ) 2\n)\n, (5)\nwhich proves the Frobenius norm of the residual. Now we compute the spectral norm of the residual. Based on the results above we have that \u2016A\u2212 A\u0303nysc \u20162 = \u2016(1 \u2212 \u03b1)Im\u2212c + (\u03b1\u2212 \u03b7)1m\u2212c,m\u2212c\u20162.\nIt is easy to show the top singular vector of the matrix (1 \u2212 \u03b1)Im\u2212c + (\u03b1 \u2212 \u03b7)1m\u2212c,m\u2212c is \u00b11\u221a m\u2212c1m\u2212c, so the top singular value is\n\u03c31 ( A\u2212 A\u0303nysc )\n= (m\u2212 c)(\u03b1\u2212 \u03b7) + 1\u2212 \u03b1 = (1\u2212 \u03b1)\n(\nm+ 1\u2212\u03b1\u03b1\n)\nc+ 1\u2212\u03b1\u03b1 , (6)\nwhich proves the spectral norm bound because \u2016A\u2212 A\u0303nysc \u20162 = \u03c31 ( A\u2212 A\u0303nysc )\n. It is also easy to show the rest singular values obey\n\u03c32 ( A\u2212 A\u0303nysc ) = \u00b7 \u00b7 \u00b7 = \u03c3m\u2212c ( A\u2212 A\u0303nysc ) \u2265 0, \u03c3m\u2212c+1 ( A\u2212 A\u0303nysc ) = \u00b7 \u00b7 \u00b7 = \u03c3m ( A\u2212 A\u0303nysc ) = 0.\nThus we have, for i = 2, \u00b7 \u00b7 \u00b7 ,m\u2212 c,\n\u03c32i ( A\u2212 A\u0303nysc )\n= \u2016A\u2212 A\u0303nysc \u20162F \u2212 \u03c321 ( A\u2212 A\u0303nysc )\nm\u2212 c\u2212 1 = (1\u2212 \u03b1) 2.\nThe nuclear norm of the residual ( A\u2212 A\u0303nysc ) is\n\u2016A\u2212 A\u0303nysc \u2016\u2217 = m \u2211\ni=1\n\u03c3 ( A\u2212 A\u0303nysc )\n= \u03c31 ( A\u2212 A\u0303nysc ) + (m\u2212 c+ 1)\u03c32 ( A\u2212 A\u0303nysc ) = (m\u2212 c)(1 \u2212 \u03b7)\n= (m\u2212 c)(1 \u2212 \u03b1) ( 1 + 1\nc\u2212 1\u2212\u03b1\u03b1\n)\n. (7)\nInequality (5) and equalities (6), (7) prove the theorem."}, {"heading": "Appendix C. Proof of Theorem 5", "text": "Proof We use the matrix A constructed in Theorem 3. It is easy to check that W(1) = \u00b7 \u00b7 \u00b7 = W(t), so we use the notation W instead. We assume that the samples contains the firs tc columns of A and each sample contains neighboring columns, that is,\nA = [ C(1), \u00b7 \u00b7 \u00b7 ,C(t), A(tc+1):m ] . (8)\nSection 2.2 shows that, if a sample C contains the first c columns of A, then\nCW\u2020CT =\n[\nW AT 21\nA21 A21W \u2020AT\n21\n] , and A\u2212CW\u2020CT = [\n0 0 0 A22 \u2212A21W\u2020AT21\n]\n;\nright. The indices i correspond to the block matrices defined in (8).\notherwise, after permuting the rows and columns of A\u2212CW\u2020CT , we get the same result:\n\u03a0 ( A\u2212CW\u2020CT ) \u03a0 = A\u2212\u03a0 ( CW\u2020CT ) \u03a0 =\n[\n0 0 0 A22 \u2212A21W\u2020AT21\n]\n,\nwhere \u03a0 is a permutation matrix. As was shown in equation (3), A21W \u2020AT21 is an (m\u2212\nc)\u00d7 (m\u2212 c) matrix with all entries equal to\n\u03b7 = c\u03b12\n1\u2212 \u03b1+ c\u03b1 .\nTo better understand the residual matrices A\u2212C(i)W(i)\u2020C(i)T , we give an illustrate of in Figure 1. We can see from the figures that A\u2212C(i)W(i)\u2020C(i)T can be partitioned to three kinds of blocks. Some blocks are all-zero; some are all-(\u03b1\u2212\u03b7); the rest are all-(\u03b1\u2212\u03b7) except for the diagonal entries: the diagonal entries are all-(1 \u2212 \u03b7).\nBased on the properties of the matrix A \u2212 C(i)W(i)\u2020C(i)T , we study the value of the entries of A\u2212 A\u0303enst,c . By expressing it as\nA\u2212 A\u0303enst,c = A\u2212 1\nt\nt \u2211\ni=1\nC(i)W(i) \u2020 C(i) T = 1\nt\nt \u2211\ni=1\n( A\u2212C(i)W\u2020C(i)T ) ,\na discreet examination reveals that A \u2212 A\u0303enst,c can be partitioned into four kinds of regions as illustrated in Figure 2. We annotate the regions in the figure and summarize the values of entries in each region in the table below. (Region 1 and 4 are further partitioned into diagonal entries and off-diagonal entries.)\nRegion 1 (diag) 1 (off-diag) 2 3 4 (diag) 4 (off-diag)\n# Entries tc tc2 \u2212 tc (tc)2 \u2212 tc2 2tc(m\u2212 tc) m\u2212 tc (m\u2212 tc)2 \u2212 (m\u2212 tc) Value t\u22121\nt (1\u2212 \u03b7) t\u22121 t (\u03b1\u2212 \u03b7) t\u22122 t (\u03b1\u2212 \u03b7) t\u22121 t (\u03b1\u2212 \u03b7) 1\u2212 \u03b7 \u03b1\u2212 \u03b7\nNow we do a summation over the entries of A\u2212 A\u0303enst,c to compute its squared Frobenius norm:\n\u2016A\u2212 A\u0303enst,c \u20162F = tc [ t\u2212 1\nt (1\u2212 \u03b7)\n]2 + \u00b7 \u00b7 \u00b7+ [ (m\u2212 tc)2 \u2212 (m\u2212 tc) ] (\u03b1\u2212 \u03b7)2\n= (1\u2212 \u03b1)(1 + \u03b1\u2212 2\u03b7)(m \u2212 2c+ c t ) + (\u03b1\u2212 \u03b7)2 ( 4c2 \u2212 4cm+m2 + 2cm\u2212 3c 2 t ) = (1\u2212 \u03b1)2 (\nm\u2212 2c+ c t ) + (1\u2212 \u03b1)2 (c+ 1\u2212\u03b1\u03b1 ) 2 [ (m\u2212 2c+ c t ) ( 2 \u03b1 \u2212 2 +m ) + c(m\u2212 c) t ]\n\u2265 (1\u2212 \u03b1)2 ( m\u2212 2c+ c t )( 1 + m+ 2\u03b1 \u2212 2 (c+ 1\u2212\u03b1\u03b1 ) 2 ) ,\nwhere the last inequality follows from c(m\u2212c)t \u2265 0."}], "references": [{"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["Mohamed-Ali Belabbas", "Patrick J Wolfe"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Belabbas and Wolfe.,? \\Q2009\\E", "shortCiteRegEx": "Belabbas and Wolfe.", "year": 2009}, {"title": "Generalized Inverses: Theory and Applications", "author": ["Adi Ben-Israel", "Thomas N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "Ben.Israel and Greville.,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville.", "year": 2003}, {"title": "CUR from a sparse optimization viewpoint", "author": ["Jacob Bien", "Ya Xu", "Michael W Mahoney"], "venue": null, "citeRegEx": "Bien et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bien et al\\.", "year": 2010}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "CoRR, abs/1103.0995,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Near optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["Corinna Cortes", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["Amit Deshpande", "Luis Rademacher"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Deshpande and Rademacher.,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher.", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["Amit Deshpande", "Luis Rademacher", "Santosh Vempala", "Grant Wang"], "venue": "Theory of Computing,", "citeRegEx": "Deshpande et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2006}, {"title": "Pass-efficient algorithms for approximating large matrices", "author": ["Petros Drineas"], "venue": "Proceeding of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms,", "citeRegEx": "Drineas.,? \\Q2003\\E", "shortCiteRegEx": "Drineas.", "year": 2003}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["Petros Drineas", "Michael W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Spectral grouping using the nystrom method", "author": ["Charless Fowlkes", "Serge Belongie", "Fan Chung", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Fowlkes et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fowlkes et al\\.", "year": 2004}, {"title": "Fast monte-carlo algorithms for finding low-rank approximations", "author": ["Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "The spectral norm error of the naive nystrom extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "Gittens.,? \\Q2011\\E", "shortCiteRegEx": "Gittens.", "year": 2011}, {"title": "Revisiting the nystrom method for improved largescale machine learning", "author": ["Alex Gittens", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1303.1849,", "citeRegEx": "Gittens and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Gittens and Mahoney.", "year": 2013}, {"title": "A theory of pseudoskeleton approximations", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Pseudo-skeleton approximations by matrices of maximal volume", "author": ["S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov"], "venue": "Mathematical Notes,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["Venkatesan Guruswami", "Ali Kemal Sinop"], "venue": "In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Guruswami and Sinop.,? \\Q2012\\E", "shortCiteRegEx": "Guruswami and Sinop.", "year": 2012}, {"title": "Improved bound for the Nystr\u00f6m method and its application to kernel classification", "author": ["Rong Jin", "Tianbao Yang", "Mehrdad Mahdavi"], "venue": "CoRR, abs/1111.2262,", "citeRegEx": "Jin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2011}, {"title": "Ensemble nystrom method", "author": ["Sanjiv Kumar", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "On sampling-based approximate spectral decomposition", "author": ["Sanjiv Kumar", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["Sanjiv Kumar", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Making large-scale nystr\u00f6m approximation possible", "author": ["Mu Li", "James T Kwok", "Bao-Liang Lu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Divide-and-conquer matrix factorization", "author": ["Lester Mackey", "Ameet Talwalkar", "Michael I. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mackey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["Michael W. Mahoney", "Petros Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Mahoney and Drineas.,? \\Q2009\\E", "shortCiteRegEx": "Mahoney and Drineas.", "year": 2009}, {"title": "Fast embedding of sparse music similarity graphs", "author": ["John C Platt"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Platt.,? \\Q2004\\E", "shortCiteRegEx": "Platt.", "year": 2004}, {"title": "Global versus local methods in nonlinear dimensionality reduction", "author": ["Vin de Silva", "Joshua B Tenenbaum"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Silva and Tenenbaum.,? \\Q2002\\E", "shortCiteRegEx": "Silva and Tenenbaum.", "year": 2002}, {"title": "Four algorithms for the the efficient computation of truncated pivoted QR approximations to a sparse matrix", "author": ["G.W. Stewart"], "venue": "Numerische Mathematik,", "citeRegEx": "Stewart.,? \\Q1999\\E", "shortCiteRegEx": "Stewart.", "year": 1999}, {"title": "Matrix coherence and the nystrom method", "author": ["Ameet Talwalkar", "Afshin Rostamizadeh"], "venue": "arXiv preprint arXiv:1004.2008,", "citeRegEx": "Talwalkar and Rostamizadeh.,? \\Q2010\\E", "shortCiteRegEx": "Talwalkar and Rostamizadeh.", "year": 2010}, {"title": "Large-scale manifold learning", "author": ["Ameet Talwalkar", "Sanjiv Kumar", "Henry Rowley"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Talwalkar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Talwalkar et al\\.", "year": 2008}, {"title": "Incomplete cross approximation in the mosaic-skeleton", "author": ["Eugene E. Tyrtyshnikov"], "venue": "method. Computing,", "citeRegEx": "Tyrtyshnikov.,? \\Q2000\\E", "shortCiteRegEx": "Tyrtyshnikov.", "year": 2000}, {"title": "A scalable CUR matrix decomposition algorithm: Lower time complexity and tighter bound", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wang and Zhang.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2012}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["Christopher Williams", "Matthias Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Improved nystr\u00f6m low-rank approximation and error analysis", "author": ["Kai Zhang", "Ivor W Tsang", "James T Kwok"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 12, "context": ", 2008), spectral clustering (Fowlkes et al., 2004), and kernel PCA (Talwalkar et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 29, "context": "For example, the Nystr\u00f6m method has been applied to Gaussian process Williams and Seeger (2001), kernel SVM (Kumar et al.", "startOffset": 69, "endOffset": 96}, {"referenceID": 12, "context": ", 2008), spectral clustering (Fowlkes et al., 2004), and kernel PCA (Talwalkar et al., 2008, Zhang et al., 2008, ?). Williams and Seeger (2001) are the first to use the Nystr\u00f6m method for low-rank matrix approximation.", "startOffset": 30, "endOffset": 144}, {"referenceID": 12, "context": ", 2008), spectral clustering (Fowlkes et al., 2004), and kernel PCA (Talwalkar et al., 2008, Zhang et al., 2008, ?). Williams and Seeger (2001) are the first to use the Nystr\u00f6m method for low-rank matrix approximation. The uniform column selection proposed by Williams and Seeger (2001) is 1", "startOffset": 30, "endOffset": 287}, {"referenceID": 25, "context": "However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds.", "startOffset": 43, "endOffset": 58}, {"referenceID": 7, "context": "The most advanced relative-error columns selection techniques include the adaptive sampling algorithm in (Deshpande et al., 2006), the nearoptimal algorithm in (Boutsidis et al.", "startOffset": 105, "endOffset": 129}, {"referenceID": 18, "context": ", 2011b), and the optimal algorithm in (Guruswami and Sinop, 2012) whose upper bound matches the lower bound of column selection problem (Boutsidis et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 33, "context": "3 that the lower error bound of the Nystr\u00f6m method is even worse than the upper bound of the state-of-the-art CUR algorithm in (Wang and Zhang, 2012).", "startOffset": 127, "endOffset": 149}, {"referenceID": 1, "context": "the simplest but the most widely used in practice (Fowlkes et al., 2004, Talwalkar et al., 2008, Gittens and Mahoney, 2013). Following the work of Williams and Seeger (2001), many uniform and non-uniform sampling techniques are proposed and analyzed (Drineas and Mahoney, 2005, Zhang et al.", "startOffset": 51, "endOffset": 174}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al.", "startOffset": 8, "endOffset": 614}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al.", "startOffset": 8, "endOffset": 633}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al. (2011). However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds.", "startOffset": 8, "endOffset": 655}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al. (2011). However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds. The recent work of Gittens (2011) provided the relative-error bounds for the first time, and then the bounds were sharpened by Gittens and Mahoney (2013).", "startOffset": 8, "endOffset": 824}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al. (2011). However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds. The recent work of Gittens (2011) provided the relative-error bounds for the first time, and then the bounds were sharpened by Gittens and Mahoney (2013). Furthermore, a method called the ensemble Nystr\u00f6m method was proposed by Kumar et al.", "startOffset": 8, "endOffset": 944}, {"referenceID": 15, "context": "Interestingly, two of our relative-error lower bounds can be attain by some algorithms in (Gittens and Mahoney, 2013), which indicates the two lower bounds are tight and the corresponding upper bounds (Gittens and Mahoney, 2013) are near optimal.", "startOffset": 90, "endOffset": 117}, {"referenceID": 15, "context": "Interestingly, two of our relative-error lower bounds can be attain by some algorithms in (Gittens and Mahoney, 2013), which indicates the two lower bounds are tight and the corresponding upper bounds (Gittens and Mahoney, 2013) are near optimal.", "startOffset": 201, "endOffset": 228}, {"referenceID": 1, "context": "Furthermore, let A\u2020 = VA,\u03c1\u03a3 \u22121 A,\u03c1U T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003).", "startOffset": 76, "endOffset": 107}, {"referenceID": 17, "context": "1 the lower bounds established in Theorem 2 and 3 are better than the previous work of Jin et al. (2011). Actually, relative-error lower bounds in spectral norm and nuclear norm we established are tight because they are attained by some algorithms in (Gittens and Mahoney, 2013, Lemma 4).", "startOffset": 87, "endOffset": 105}, {"referenceID": 19, "context": "1 we compare with the previous work of Jin et al. (2011) and show the superiority of our results.", "startOffset": 39, "endOffset": 57}, {"referenceID": 19, "context": "1 An Additive-Error Lower Bound in Spectral Norm The previous work of Jin et al. (2011) established a lower error bound for the Nystr\u00f6m method.", "startOffset": 70, "endOffset": 88}, {"referenceID": 14, "context": "2 Optimal Relative-Error Upper Bounds The very recent work of Gittens and Mahoney (2013) established several relative-error upper bound for the Nystr\u00f6m method.", "startOffset": 62, "endOffset": 89}, {"referenceID": 33, "context": "We show in the following proposition that the state-of-the-art CUR algorithm in (Wang and Zhang, 2012) achieved a relative-error ratio of E\u2016A\u2212CUR\u20162 F \u2016A\u2212Ak\u2016F \u2264 1 + 4k c (", "startOffset": 80, "endOffset": 102}, {"referenceID": 14, "context": "It also shows the relative-error upper bounds in spectral norm and nuclear norm in Gittens and Mahoney (2013) are near optimal.", "startOffset": 83, "endOffset": 110}], "year": 2017, "abstractText": "The Nystr\u00f6m method has been widely used to compute low-rank approximations to positive semidefinite matrices. Much work has been done on the sampling techniques for the Nystr\u00f6m method, and many were guaranteed with upper error bounds. However, the lower bounds, especially the relative error bounds, were largely unknown. We explore in this paper the lower bounds of the Nystr\u00f6m method to show how bad the Nystr\u00f6m method can perform. We establish both additive and relative error lower bounds for the Nystr\u00f6m method. Interestingly, some of the relative-error lower bounds we established can be attained by some algorithms, which indicates some lower bounds we establish are tight! However, some lower bounds reveal the downside of the Nystr\u00f6m method that they are much worse than the column selection problem and the CUR problem. Sadly, we also show that some bounds cannot be improved by the ensemble Nystr\u00f6m method.", "creator": "LaTeX with hyperref package"}}}