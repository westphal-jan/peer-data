{"id": "1303.5745", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Pulcinella: A General Tool for Propagating Uncertainty in Valuation Networks", "abstract": "We present PULCinella and its use in comparing uncertainty theories. PULCinella is a general tool for Propagating Uncertainty based on the Local Computation technique of Shafer and Shenoy. It may be specialized to different uncertainty theories: at the moment, Pulcinella can propagate probabilities, belief functions, Boolean values, and possibilities. Moreover, Pulcinella allows the user to easily define his own specializations. To illustrate Pulcinella, we analyze two examples by using each of the four theories above. In the first one, we mainly focus on intrinsic differences between theories. In the second one, we take a knowledge engineer viewpoint, and check the adequacy of each theory to a given problem. Since the discovery of new theories, we then focus on understanding one theory to the problem. In the third, we apply these theories in a more general way. First, we consider an information scientist viewpoint. We will use a theory that combines knowledge as its own generalization. Second, we consider a theory that combines knowledge as its own generalization. Third, we look at a theory that combines knowledge as its own generalization. Finally, we use a theory that combines knowledge as its own generalization.\n\n\n\nThe fourth idea is to combine knowledge as its own generalization. For example, we will use a theory that combines knowledge as its own generalization. Second, we use a theory that combines knowledge as its own generalization. Third, we use a theory that combines knowledge as its own generalization. Third, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Third, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization.\nWe use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use a theory that combines knowledge as its own generalization. Fourth, we use", "histories": [["v1", "Wed, 20 Mar 2013 15:32:58 GMT  (419kb)", "http://arxiv.org/abs/1303.5745v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alessandro saffiotti", "elisabeth umkehrer"], "accepted": false, "id": "1303.5745"}, "pdf": {"name": "1303.5745.pdf", "metadata": {"source": "CRF", "title": "A General Tool for Propagating Uncertainty in Valuation Networks", "authors": ["Alessandro Saffiotti"], "emails": ["r01507@bbrbfuOl.bitnet"], "sections": [{"heading": null, "text": "1. INTRODUCTION A new interest has grown up recently in the uncertainty management community. Moving from consideration of efficiency, ease of representation, and generality, a num ber of techniques for representing and propagating uncer tainty in networks have been proposed (e.g. Chatalic et a!., 1987; Lauritzen and Spiegelhalter, 1988; Pearl, 1988; Shafer et a!., 1987). Moreover, implementations of these techniques have been developed (e.g. Andersen et a!., 1989; Hsia and Shenoy, 1989; Zarley et a!., 1988; Xu, 1991). However, all the existing systems only propagate uncer tainty values according to a single uncertainty theory. This is unfortunate: if we accept that uncertainty theories should be seen as alternatives, rather than rivals (Fox, 1986; Saffiotti, 1987) then we must also accept that for each problem there is a \"most adequate\" theory, and this theory is in general different from problem to problem. It would be advisable to have a general tool capable of propagating uncertainty according to different uncer tainty theories. This would allow us to use the same piece of software for solving different problems that call for different uncertainty management techniques. Moreover, such a tool would be useful for analyzing and comparing different theories in an experimental way.\nIn the Platonic world of formal theories, a system having the above characteristics exists. Building on their work on belief function propagation (Shafer et a!., 1987), Shafer\nand Shenoy developed a general framework for local computation (Shafer and Shenoy, 1988b) in which the process of network propagation in itself has been abstracted from what is actually propagated. Shafer and Shenoy have shown (Shafer and Shenoy, 1989a) that their framework is capable of modelling both probability and belief function propagation, and that it can capture other existing propagation schemas (i.e. Lauritzen and Spiegelhalter's and Pearl's). This framework has been further generalized by Shenoy (1989), who proposes a class of languages (\"valuation-based languages\") for building knowledge-based systems. Besides probability and belief functions, other existing uncertainty theories have already been formalized as valuation languages (e.g. Dubois and Prade, 1990).\nIn this paper, we introduce PULCinella, a tool for Propagating Uncertainty based on the Local Computation technique of Shafer and Shenoy. Pulcinella is a general implementation of valuation based languages, abstracted from a belief function propagation system (Xu, 1991), and it is fully described in (Saffiotti and Umkehrer, 1991a). As such, it may be instantiated to any of the theories which have been formalized as valuation based languages. In particular, four specialization of Pulcinella have already been implemented, namely for propagating probabilities, belief functions, Boolean values, and possibilities. Moreover, Pulcinella makes it easy to implement new theories in it (provided that they can be modelled in the valuation language formalism). Besides describing the tool and the underlying theory, we illustrate the use of Pulcinella for comparing uncertainty theory. In this sense, the AI researcher will find in this paper an analysis of the different results obtained applying different theories to the same problem; and the knowledge engineer will find a discussion of the pros and cons of using different uncertainty theories for modelling the same test-bed problem. Both discussions are based on the results of experiments carried out using Pulcinella.\nThe rest of this paper is organized as follows. Section 2 reminds some formal background on valuation-based languages and local computations. Section 3 presents Pulcinella. Section 4 shows two full examples of applica tion of Pulcinella. Section 5 discusses these examples and analyzes the differences detected in using different uncer tainty theories. Finally, Section 6 concludes.\n324 Saffiotti and Umkehrer\n2. THEORETICALBACKGROUND\n2.1. VALUATION-BASED LANGUAGES\nShenoy' s valuation-based languages (Shenoy, 1989) have been abstracted from the axiomatic framework for probabilities and belief functions propagation of Shafer and Shenoy (Shafer, Shenoy and Mellouli, 1987; Shafer and Shenoy, 1988a; Shenoy and Shafer, 1988b). They have been proposed as an alternative to rule-based languages for constructing knowledge-based systems. The language consists of objects, which are used to represent knowledge, and operators, which operate on these objects to make inferences on the knowledge. Two kinds of objects are considered, variables and valuations, and two operators, combination and marginalization!. We first remind the formal definitions of these elements, and will discuss their interpretation and use later.\nVariables, Frames and Configurations. We consider a finite set of variables. Each variable may range over a finite set of possible values, called the frame for that variable. A configuration of a finite non-empty set of variables is an element of the Cartesian product of the frames of the variables in this set. Denotations: X for the set of variables; g, h, k for subsets\nof X; Wg for the set of configurations of g; x,y for single configurations; a,b,c for sets of configurations.\nSometimes we need to project a configuration of one set to another set. A configuration x of g is projected to h,\ng::>h, by dropping all the elements in x belonging to g-h. It is extended to k, k::>g, by building the Cartesian product between the configuration and Wk-g\u00b7 Denotations: x.l.h for the projection of x to h; xth for the extension of x to h.\nValuations. Given a set of variables h, we consider a set V h\u00b7 The elements of Vh are called v aluations on the set h2. In our case, valuations are the objects that represent the uncertainty about a set of variables. Denotations: V g for the set of valuations on g; V for the\nset of all valuations on subsets of X; G, H for single valuations.\nCombination is any mapping \u00ae: VXV\ufffd V, such that, if G and H are valuations on g and h, respectively, then\nG\u00aeH is a valuation on guh.\nMarginalization. For each hb:X, there is a mapping J.h: U {V 8 I hb:g}\ufffd Vh, called marginalization to h, such that, if G is a valuation on g and h b: g, then GJ. h is a valuation on h.\n1 A third operator, solution, is used for \"decoding\" the result obtained the propagation. This operator is not relevant to the present discussion, and so it will not be considered. 2 For the sake of simplicity, we do not take here into account \"proper valuations\", a subset of the valuations used to restrict the applicabil ity of operators. Thus, the definitions given here are not complete, but they preserve the basic ideas of valuation-based languages.\n2.2. INTERPRETATIONS\nIt will be useful to give now some examples of possible interpretations for the syntactical entities of a valuation based language. This will show in which way a valuation-based language can be used for modelling different existing uncertainty theories. For probability theory, belief-functions, and a Boolean case, the mapping of the theory into the concepts of a valuation-based language have been proposed by Shenoy and Shafer (Shenoy and Shafer, 1988b; Shenoy, 1989). For possibility theory, we use the mapping proposed by Dubois and Prade (1990) building on a previous work by Zadeh (1979).\nProbability: Valuations on h are (unnormalized) probability distributions on the configurations of h Combination: If G and H are probability distributions on g and h, respectively, then their combination is the probability distribution on guh defined by\n(G\u00aeH)(x) = G(x.l.g)H(x.l.h) for all xe W gUh\u00b7 Marginalization: If hb: g and G is a probability distribution on g, then the marginal of G for h is the probability distribution on h defined by3:\nGJ.h(x) = \ufffd{G(x,y) lyeWg-hl for all xeWh\nBelief Functions: Valuations on h are basic probability assignment (bpa) functions on sets of configurations of h. Combination: If G, H are bpa's on g, h -respectively then their combination is the bpa on guh defined by4\n(G\u00aeH)(c) = \ufffd{G(a)H(b) I (a t(gUhl)li(b t(gUhl)=c} for all cb:Wguh, ab:W8, bb:Wh\nMarginalization: If hb:g and G is a bpa on g, then the marginal of G for h is the bpa on h defined by\nG.l.h(a) = \ufffd {G(b) lbb:Wg such that b .l.h =a}\nfor all ab:Wh.\nBoolean:\nValuations on h are functions H: Wh\ufffd {true, false}. Combination: If G and H are valuations on g and h, respectively, then their combination is the valuation on guh defined, for all xe W gUh. by true if G(x.l.g) = true and H(x.l.h) = true (G\u00aeH)(x) = false otherwise Marginalization: If hb: g and G is a valuation on g, then the marginal of G for h is the valuation on h defined, for all XE Wh, by\ntrue if there is yeWg-h s.t. G(x,y) =true\nfalse otherwise\n3 In all interpretations, we let G \ufffdh(x) = G(x) if h = g .. 4 This corresponds to usual (but un-normalized) Dempster's rule of combination (Dempster, 1966).\nPulcinella: A General Tool for Propagating Uncertainty in Valuation Networks 325\nPossibility: Valuations on h are possibility distributions on sets of configurations of h. Combination: If G and H are possibility distributions on g and h, respectively, then their combination is the possibility distribution on guh defined by\n(G\u00aeH)(x) =min (G(x.l.g), H(x.l.h)) for all xeW8uh Marginaliza tion: If hk g and G is a possibility distribution on g, then the marginal of G for h is the possibility distribution on h defined by\ncJ.h(x) =sup {G(x,y) I yew g-hl for all xeWh.\n2.3. REPRESENTING PROBLEMS BY VALUATION SYSTEMS\nA set of variables, with their frames, together with a set of valuations, is called a valuation system. Intuitively, a valuation system corresponds to a knowledge-base. When we want to use a valuation-based language to solve a problem, we first have to define a valuation sys tem that represents our problem. While doing that, we have to keep in mind the intended intuitive meaning of the syntactical entities of valuation-based languages (which holds independently from the interpretation). To this respect, variables can be seen as representing entities of the domain of discourse, and valuations Vh as repre senting relational knowledge among the entities repre sented by the variables in h. The combination operator models aggregation of different fragments of knowledge, and the marginalization operator models a narrowing of the focus of interest by concentrating knowledge on a subset of variables. The following is a pictorial represen tation of the intended interpretation in terms of real world knowledge of variables and valuations.\nset of variables\n[x1, ... , xnl\n0\nL , represent,\nentities of th\u00a3 dnmain\nset of valuations\n(vhl h\" X} /\". 0.:\u202211111111111111111111111111\ufffd \u00b7/./f' associated to \"-J\n. /\n. . J . . ' represent \u00b7 .. . . . . .\n/.'\u00b7 \ufffd\u2022llllllllll!llllllllllllill 0 anoc1aled to (relational)\nknowledge abouJ the enlities\n2.4. LOCAL COMPUTATION IN VALUATION SYSTEMS\nHaving defined a valuation system we want now to evaluate it. This means to compute a global valuation on X obtained by combining together all the valuations in our valuation system, and find the marginals of this global valuation to each variable in X. In terms of\nknowledge, this corresponds to aggregating all the available knowledge together, and to finding the effect of this knowledge on the individual variables. Computing explicitly the global valuation is often unfeasible from the computational viewpoint. However, Shafer and Shenoy (1988b) proposed a general local computation schema for evaluating valuation systems. The computation is local in the sense that combinations of valuations can be performed without extending each valuation to the whole space of the configurations. Shenoy and Shafer have shown that this schema can be applied if the combination and marginalization operators satisfy the following three axioms:\nA 1 (Commutativity and associativity of \u00ae). Let G, H, K be valuations on g, h and k, respectively. Then:\nG\u00aeH = H\u00aeG and G\u00ae(H\u00aeK) = (G\u00aeH)\u00aeK\nA2 (Consonance of J. ). Let G be a valuation on g, and suppose kk hk g. Then:\nccJ.h)J.k = cJ.k.\nA3 (Distributivity of J. over \u00ae). Let G, H be valuations on g and h, respectively. Then\n(G\u00aeH)J.g = G\u00ae(H J.gllh)\nAll four interpretations described in Section 2.2 satisfy these axioms. Thus, local computations can be used for propagating probabilities, belief functions, Boolean val ues, and possibilities.\nAlgorithms based on the above technique have already been proposed and implemented (Zarley et a!., 1988; Hsia and Shenoy, 1989; Xu, 1991). These algorithms use a network representation for the valuation system called Markov Tree. Mapping a valuation system on a Markov Tree is done in two steps. The first step is to represent the valuation system by a hypergraph: in it, each variable is associated to a node, and each valuation is associated to a hyperedge. The second step is to find a Markov Tree representative of the hypergraph by clustering variables.\n3. PULCINELLA\nPulcinella is a system for building and evaluating valuation systems based on Shenoy and Shafer's local computation technique. The system implements the general framework discussed above: it may be specialized to a given uncertainty theory by choosing an interpretation for the objects and the operators. Pulcinella is written in Lisp, and appears as a library of Lisp functions for creating, modifying and evaluating a valuation system. Alternatively, the user can choose to interact with Pulcinella via a graphical interface. The system builds on Hong Xu's implementation of the belief functions propagation technique of Shafer and Shenoy (Xu, 1991). The key move for generalizing Xu's program has been to parametrize it over a set of functions. At the moment, four interpretations of the general framework have been implemented: belief function, probability, Boolean and possibility (in the following, we will use the term \"specialization\" to refer to an implemented\n326 Saffiotti and Umkehrer\ninterpretation). By selecting one of these specializations, the user can transform Pulcinella in a probability propagation system, in a belief function propagation system, and so on. Moreover, Pulcinella is meant to be an open system: the set of functions which have to be defined to create a new specialization is very small and with a well defined semantics (reflecting the elements of a valuation-based language). Thus, it is easy for the user to define further specializations. In this Section, we will first show how to use the existing specializations, and then discuss how a new specialization can be created.\n3.1 USING A PULCINELLA SPECIALIZATION\nAs far as modelling quantitative knowledge is not concerned, the way to work with Pulcinella is the same for all specializations. We first describe this common part, and then show how qualitative knowledge is modelled in each of the four provided specializations.\nThe user can model his problem, either graphically or by calling Lisp functions. In terms of Shafer and Shenoy's framework, modelling the problem means to create a valuation system representing his problem. This modelling process comprises two steps. In the first step, the user specifies the structural knowledge of his problem. This means defining all the variables to be used (along with their frames), and indicating which variables are linked together by a relation. By defining the variables and the relations the user implicitly fixes the subsets of variables for which valuations can be specified in the second step. The second step consists in modelling the quantitative knowledge: i.e. in defining the valuations on (some of) the subsets identified by the structural model created. This step depends from the specialization chosen, and will be discussed below. However, during this step, the user should keep in mind that a default valuation is given by the system to each variable and relation if no valuation is defined by the user. Once the valuation system has been completely defined, the user can evaluate it by asking Pulcinella to start propagation. The user can choose if the results must be shown normalized or unnormalized. If the user wants to apply different uncertainty theories on the same problem, he has to specify the structural knowledge only once. He can use the same variables and relations for different specializations.\nProbability Specialization. In the probability specializa tion, the user models quantitative knowledge by defining probability distributions for single variables and rela tions. Notice that this means that a dependency between variables is encoded by a joint probability distribution rather than by conditional probabilities. The default val uation is the uniform probability distribution: if nothing is known about a variable or relation, all configurations are considered to have the same probability. A probability distribution is called normalized if the values attached to the configurations adds up to one.\nBelief-Function Specialization. In the belief function spe cialization, the user models the quantitative knowledge by defining basic probability assignment functions on the\nsets of variables that constitute the structural knowledge. A basic probability assignment function on a set of vari ables reflects to which extent some subsets of configura tions are believed to contain the true configuration, and is expressed by a mapping from subsets of configurations of a set of variables to the interval [0,1]. The value associ ated to the empty set is always zero. The default valua tion is the basic probability assignment function which attach the value 1 to the whole set of configurations. A basic probability assignment function is called normal ized if the sum of all its values is 1. Boolean Specialization. The Boolean specialization can be seen as a way to represent categorical knowledge. The quantitative knowledge is modelled by attaching to the configurations of the defined sets of variables either true or false. These values are better understood in term of satisfaction of constraints: a value true for a configura tion means that this configuration is acceptable given the constraints of our problem. Accordingly, a relation among variables will be encoded by selecting all those configurations that are admissible, and by associating true to them. The default valuation attaches true to each con figuration: if nothing is known about a set of variables, each configuration could be the case. No normalization is defined for Boolean specialization.\nPossibility Specialization. In possibility theory the user models quantitative knowledge by specifying possibility distributions on the defined sets of variables. A possibil ity distribution on a set of variables reflects to what extent each configuration of the set is regarded as possible, and is expressed by a mapping from configurations to the [0,1] interval. The default possibility distribution attaches to each configuration the value 1: if nothing is known about a set of variables, all configurations are regarded as completely possible. A possibility distribution is called normalized if at least one element of the frame has pos sibility value 1.\n3.2 HOW TO BUILD A NEW SPEOALIZA TION\nPulcinella is an open system. The user can build his own specialization with Pulcinella. To build a new specialization the first thing he has to do is to express his theory in terms of the syntactical entities of valuation based languages, and to prove that the axioms of local computation are satisfied. Having expressed the theory in this way, he may now implement the functions specific to the new specialization. This is made easier by the clear semantics given to these functions: basically, they mirror the concepts and entities of a valuation-based language. These functions may be divided up into three groups:\n1. two functions defining the default valuations for variables and relations\n2. two functions that implement the combination and marginalization operators.\n3. two extra functions for changing the valuations in \"some way\": one implements the normalization procedure for valuations; the other is called after the\nPulcinella: A General Tool for Propagating Uncertainty in Valuation Networks 327\npropagation has been completed, and allows the builder of a specialization to do some housekeeping before the results are shown to the user.\n4. EXAMPLES In this section we will give a couple of examples aimed at illustrating the use of Pulcinella for modelling and solving uncertain problems. We will insist on the difference between modelling the structural knowledge of the problem, and modelling its quantitative knowledge. In each example, the same problem will be formalized in all the four specializations discussed above. However, and interestingly, the structural model remains the same for all of them. These examples will highlight how using different uncertainty theories may require different input and produce different results.\n4.1. ExAMPLE 1\nOur first example is adapted from (Saffiotti, 1987). We want to guess if Francesco will came wearing a black (B), white (W) or polka-dot (P) suite. We do not have any information about Francesco's preference (state 0), but we do know that his \"Philco\" washing machine is out; this makes us believe (say 80%) that he cannot choose W ( state 1). Later, we remember that Francesco said yesterday that he dislikes mono-chromatic clothes; this is, for the notorious coherence of Francesco, a strong (say 90%) evidence both against B and W (state 2). We first build a structural model for our problem. We define three variables: Dre ss, with frame (B, W, P}; Philco, with frame (ok, out}; and Speech, with frame (uttered, unuttered}. We then define two relations: Washing, between Philco and Dress; and Coherence, between Speech and Dress. The intended meaning of these elements should be self evident. The following is a graphical representation of our model, as appearing on the screen of Pulcinella:\nThe next step consists in deciding one uncertainty calculus to use, and to specialize Pulcinella to it. Suppose we choose to specialize Pulcinella to probability5. We can then enter the (unnormalized) joint probability distributions for our variables and relations. These distributions, which encode the quantitative knowledge in our problem, are shown below6:\nPwashin W P Pcoherence B W P ok 1/6 l/6 unered l 0.025 1 0.0\ufffd5 \ufffd 0.45 I .I unuttered. 1/6 . I/ . 1/6 .\n5 In practice, this reduces to selecting a menu item, or to evaluating the form \"(specialize-uncertainty 'probability)\". 6 The valuations for variables are obvious, and will not shown.\nNotice that we are assuming uniform prior distributions whenever no explicit information is available. Finally, we ask Pulcinella to start propagation. The following table gives the marginal probabilities computed for Dress at different moments?:\nw p\n0.33 0.33\n0.20 0.40\n0.026 0.923\nSuppose that we now want to try to use belief functions for modelling our problem. All we need to do, is to spe cialize Pulcinella to belief functions, and to input the quantitative knowledge of our problem in the form of two basic probability assignments for our relations:\nSubset Subset\nokEEE 0.8 uneredEEE 0.9 out unuttered\nIntuitively, the subset to which mwashing assigns a 0.8 mass represents the fact that the answer to our problem may be any of B, W and P when Philco = Qk, and any of B and P when Philco = out. The remaining 0.2 mass is au tomatically given to the whole frame. After propagation, we get the following results for the variable DressB\nStale 0 Stale 1 Stale 2 bel I bel I bel I\n0 I I 0 .I W 0 I 0 0.2 0 0.02 P 0 I 0 I 0.9 1\nNext, we consider using possibility theory: we switch Pulcinella accordingly, and enter two possibility distributions for our relations. These distributions, and the one obtained for the variable Dress after propagation, are shown below.\nIT washing 0\ufffd\ufffd1 B w I I 0\ufffd21 I Value\nB w p\np I I\nStale 0 I I I\nIlcohere..:e B w uneredl 0:1 I 0.1 I unuttered 1 I\nStale 1 Stale 2 I .I\n0.2 0.1 I I\np I\nFinally, we consider the case in which we collapse uncertainty to true/false values. The following tables show the values for our relations, and the results obtained, with the Boolean specialization9.\ntr' washing B W p tr' cohere..:e B W P ok I true I true I true I uttered I false I false I true I out true false true unuttered . true . true . true .\n7 States in the table refer to the states in the statement of our story. 8 For greater readability, we show the results using bel and pi func tions (Shafer, 1976). Remind that pl(A) = 1- bel(-A). 9 As noticed above, these values are better understood in term of satisfaction of constraints.\n328 Saffiotti and Umkehrer\nValue\nw p\n4.2. ExAMPLE 2\nStateO true true true\nState 1 true false true\nState 2 se\nfalse true\nThe next example has been tailored on an experiment made in modelling the uncertainty present in a problem of fault diagnoses in electricity networks (Gallastegui et a!., 1989). For the sake of clarity, both the qualitative and the quantitative knowledge have been greatly simplified. The full experiment, and the actual figures used, are reported in (5affiotti and Umkehrer, 1991b). We consider here the following fragment of an electricity network:\nI \u2022 \u2022 l3 : ..,. f*\ufffd------\ufffdT--i . . . \ufffd .... \ufffd .. !!\ufffd .013 .............. '\nThis fragment comprises four \"substations\", linked by three electricity lines Ll, L2 and L3. The substation in the middle includes 51, a big conductive bar used for con necting more lines together. The Oi's are \"circuit break ers\": automatic switches that can isolate two lines when they detect an over load on the part of the network on their \"hot\" side (marked by a dot in the picture). When an overload is detected, a circuit breaker generate an alarm. There are two kinds of alarm: \"instantaneous\", for \"big\" overloads (normally caused by a fault in the line the device is on); or \"delayed\", for \"small\" overloads (normally caused by a fault in a neighbour line). All the alarms are sent to the \"control room\" of some power sta tion. Here, a system engineer is constantly analyzing the incoming alarms to find out what is happening in the network. His goal is to determine if and where there is a fault. We model our electricity network in Pulcinella by the following variables and relations:\nwhere Di's are variables representing circuit breaker states, with possible values ok (no alarm), del (delayed alarm), and inst (instantaneous alarm); Li's and S 1 represent line states, with frame {Qk, fault!; and the a/arm-i's relate generation of alarms by breakers with states of neighbour lines.\nThe quantitative knowledge given by the experts is not very rich. Essentially, it says that:\n1. alarms are not very reliable: in roughly 10% of the cases, they do not correspond to the real situation (alarm generated without fault, or fault occurring without alarm); 2. if an instantaneous alarm is generated (correctly), the fault is 70% of the cases in the line the breaker is on, and 30% in the next one; the reverse holds for delayed alarms.\nThe following tables show how this knowledge has been coded into the relation alarm-1 by an (unnormalized)\njoint probability distribution 1', a possibility distribution n, a Boolean constraint tr, or a basic probability assign ment m, respectively. Distributions for the other alarm i's are similar; while those for the alarm-ij's are different.\n1'(\u2022) D1 n(\u2022) Ll, 51 -7i o \ufffd k 'T\"iiT'Tll7, ok, ok 1 Lt, 51 ok del ins!\nok, fault fault, ok\nfault, fault\nok, ok ok, fault fault, ok I 0.1\n\ufffdI <.;;.;.;.;;.;;..L...=..J'-='-' fault, fault 0.1\nLl\ntr'(\u2022) ok del ins! ok,ok\nok, fault fault, ok true false [lalse \ufffdse true [ talse true\nfault, faul t\ufffdse true\ni ___ ;;: ___ @\ntrue true true\nm(\u2022)\n0.096\n0.256\n0.348\n0.016\n0.024\n0.064\n0.096\n\ufffdI 0.1 0.7 0.3 \ufffd 0.7 I I\nWe now imagine a scenario in which a fault occurs on line L2 very near to 012 (see the above picture): as a consequence, a \"delayed\" alarm is sent by 02, and a \"instantaneous\" alarm by 012. Moreover, we imagine that 01 has been adjusted incorrectly (it happens), and is too sensitive: thus, also 01 sends a \"delayed\" alarm.\nThese three alarms will be received in the control room, and the system engineer will have to find out what has happened. Three main hypotheses are compatible with this set of alarms, shown in order of preference:\n1) Fault in L2; alarm from 01 spurious;"}, {"heading": "2) Fault in 51; missing alarm from 03, and alarm from 012 spurious;", "text": ""}, {"heading": "3) Fault in Ll; 011 not working (missing alarm & did not open), missing alarm from 03, and alarm from", "text": "012 spurious.\nPulcinella: A General Tool for Propagating Uncertainty in Valuation Networks 329\nThe following tables summarize the results obtained for this example with the probability, possibility, Boolean, and belief function specializations of PulcinellalO\n1. After receiving the alarm from D2\nProbab Possibility Belief Functions Boolean Var 1'(fault) N(fault) fl(fault) Bel( fault) PI( fault) lr(fault) lr(dt)\nLl 0.002 0.3 1 0.02 1 true true L2 0.226 0.3 I 0.02 I true true L3 0.002 0.3 I 0.02 I true true Sl 0.087 0.7 I 0.16 I true true\n2. After receiving the alarm from D12\nProbab Belief Functions Boolean Possibility Var 1'(fault) N(fault) fl(fault) Bel( fault) Pl(fault) lr(fault) lr(dt)\nLl 0.002 0.3 I 0.02 I true true L2 0.972 0.9 1 0.60 I true false L3 0.002 0.3 1 0.02 I true true Sl 0.013 0.7 I 0.16 I true true\n3. After receiving the alarm from D1\nProbab Possibility Belief Functions Boolean Var 1'(fault) N(fault) Il(fault) Bel( fault) PI( fault) lr(fault) lr(dt)\nLl 0.219 0.3 I 0.03 I true true L2 0.919 0.9 I 0.60 I true false L3 0.002 0.3 I 0.03 I true true Sl 0.141 0.7 I 0.29 I true true\n5. DISCUSSION Beside illustrating the use of Pulcinella for modelling uncertain problems within a given specialization, the examples above show how Pulcinella may provide useful help in comparing different uncertainty management techniques. The advantage of having this capability is twofold. In theoretical research, Pulcinella allows us to play with uncertainty theories, and to track differences between them. In knowledge engineering, it may be used as a tool for choosing, on an experimental basis, the uncertainty treatment technique that better fits our problem and our needs. The following discussion will illustrate both usesll. In our examples, we recognize three basic categories of differences:\n1) differences in the data we have to provide (which data, how many, in which form, ... );\n2) differences in the results d uc to differences in the data we provided; and\n3) differences in the results due to differences in the mechanisms used in the theories.\nWe first consider Example 1. Concerning category 1 above, the quantitative knowledge stated in the problem has to be coded differently in the four specializations. In the probabilistic case, the need to specify completely the joint distributions 1' w\"hing and 1' coherent has obliged us to replace in some way the missing information. In\n10 For better readability, we show the results of the possibilistic case as necessity/possibility pairs. Remind that N(A) = 1- n<-A). 11 This is not meant to be a general comparative analysis of different UR techniques. Our sole aim here is to show how Pulcine11a may provide useful information in this perspective.\nparticular, in 1'washing, the 80% probability of (BvW) has been converted into exact probability values for B and P individually (assuming equiprobability); a similar operation has been made for 1' coherent\u00b7 On the other hand, in both the belief function and the possibilistic specializations knowledge has been expressed at exactly the level of granularity that is available. However, it must be noticed that using basic probability assignments to encode knowledge may sometimes be less intuitive (mainly because we have to work on subsets). Finally, our knowledge has easily been reduced in the obvious way in the Boolean specialization.\nMoving now to the analysis of the results (categories 2 and 3 above), we first notice that -as expected- all four specializations agree from the qualitative viewpoint. The major quantitative differences show up between the results of the probabilistic specialization and those of the belief function (or possibilistic) one. These differences must be tracked back to differences in the data used, mainly because of the additional hypotheses (equiprobability) introduced in the probabilistic case. Remarkably, these differences are particularly evident in those cases (States 0 and 1) in which ignorance is predominant: here, in spite of our ignorance, the probabilistic approach needs (and gives) precise figures12. Another interesting difference arises between the belief function and the possibilistic cases. Here, differences in results do not originate from differences in the inputs given, but rather from the different combination mechanisms. To wit, consider State 2: the Philco evidence and the Speech evidence are both denying the W hypothesis. These two items of evidence have been combined into one much stronger evidence against W by Dempster's rule in the belief function case, resulting in a very small (0.02) plausibility for W. On the other hand, the stronger of them has simply been selected through the MIN operator in the possibilistic approach, giving a possibility value of 0.1 for W.\nThe above considerations illustrate how to use Pulcinella for analyzing the different behaviours of uncertainty treating theories. However, we do not have a pragmatic unit for measuring how much do a given theory fits our needs. Still, Pulcinella may be also used as a tool for evaluating uncertainty formalisms in the light of the uses to which they are to be put. To this respect, we now consider Example 2. As before, we first consider the differences in the inputs required by the different techniques (category 1 above), and than the differences in the output produced (categories 2 and 3). However, we try to take here a more \"knowledge engineering\" viewpoint.\nProbability theory requires several values that are not available in our data. It is common practice to produce missing values by resorting to some symmetry consideration, or by using the principle of maximum entropy. Accordingly, we had to introduce a number of\n12 \u00abWovon man nicht sprechen kann, dariiber muB man schweigen\u00bb (What we cannot speak about we must consign to silence) (Wittgenstein, 1921).\n330 Saffiotti and Umkehrer\nequiprobability hypotheses for our missing values (e.g. in computing :P((ok, ok, fault)). Also, notice that the amount of subjective estimates required by the probabilistic approach greatly increases the risk of inconsistency among them (but see Duda et al., 1986). On the contrary, the process of supplying data is extremely simple in the possibility and the belief function specializations: the expert must supply just the data she knows, and consign the rest to silence. We do not need to force her (or the figures she supplies) to say something they were not meant to. However, in the belief function case, the translation of this data into basic probability assignments may sometimes be hard. The difficulty of the Boolean case is somewhat complementary to that of the probabilistic case: the data given by the expert has to be rounded very roughly, and she may feel uncomfortable with the approximations obtained. A possible reaction to this rude attitude is to try to split general rules into more specific ones. The aim of this would be to reduce uncertainty by explicitly accounting for the possible exceptions to rules. Though this constitutes a stimulus for the expert that may sometimes result in a better explanation of her knowledge, the strongly empirical, tangled and \"artistic\" nature of electrical fault diagnostic knowledge discouraged us from using the Boolean specialization for our problem.\nWe now switch to analyzing the results. Our four specializations have produced results that are both quantitatively and qualitatively different. The first phenomenon we notice is that the Boolean specialization does not suggest the possibility of a fault in 51. The causes of this reside both in the input given and in the combination mechanism used. As for the input, the knowledge encoded in our relations allows us to infer that the fault is, e.g., either in 51 or in L 1. Yet, no relation encodes knowledge which allows us to infer the 51 hypothesis alone. Discrimination between 51 and L1 is, on the other side, captured by differences in the given weights in the other approaches. As for the mechanism, aggregating knowledge by an AND operation does not allow to perform that \"counting\" of evidence that seems necessary if we want to accumulate items of weak evidence together. In our example, the evidence given by 02 only partly supports the hypothesis 51; however, we would expect further support for 51 coming from 01 to reinforce the 51 hypothesis. One way to fix this problem in the Boolean specialization is to add new rules (e.g. \"IF at least 2 among 01, 02 and 03 send a delayed alarm, THEN 51 is faulty\"); but the lesson taken from this story seems to be that considering uncertainty in our problem is necessary, if we want to preserve inferential power without having to drown in a see of specialized rules.\nThe second qualitative difference we want to notice regards the hypothesis \"L1 = fault\", which is suggested by the possibilistic and the probabilistic specializations, but not by the belief function one. The origin of this is again in the way we have coded our data: a delayed alarm does not support, in the belief function case, a fault in the\nadjacent line individually, but a fault in the adjacent or in the next lines13. On the contrary, in the possibilistic and the probabilistic cases, the evidence given by the alarm is spread among each hypothesis individually. A related difference concerns the hypothesis \"L3 = fault\", suggested by the possibilistic specialization only. This \"over-inferencing\" is rooted in our rather strong attitude when defining the possibility distributions for the alarm i's relations (viz. the frequential knowledge about the localization of the fault has been converted to a measure of (im)possibility). A less committed interpretation of our data could lead to the following distribution\nn( 0) ok del inst ok, ok\nok, fault fault, ok fault, faul\nl 0.] 0.1 t 0.1 0.] 0.1 1 1 1 1 1 1\nwhere the information about the relative support given by an alarm to a near fault or a far fault (which is not, strictly speaking, a matter of possibility) has been ignored. Using this encoding, we would get a result suggesting L2 alone (like in the Boolean case). The lesson to be taken here seems to be that possibility theory may provide the additional inference power that we need in our problem, but this power cannot be \"fine-tuned\" easily.\nAs for the quantitative differences, the most important one (which might be regarded as qualitative as well) concerns the hypothesis \"51 = fault\". This hypothesis is reinforced by the arrival of the alarm from Dl in the probabilistic and belief function specializations, but not in the possibilistic one. The cause here is only the combination mechanism used: like the AND of the Boolean case, the MIN operator does not allow us to perform that \"counting\", which seems necessary in our problem to accumulate evidence correctly. As a consequence, the answers given by the possibilistic specialization to our problem are meaningful mainly from the qualitative viewpoint (they allow us to focus on those hypotheses which are possible), but they are fairly poor from the quantitative one. To this respect, we notice that the results given by the probabilistic specialization are very rich from the quantitative viewpoint; though, because of our straining the input data, this precision may be somehow unjustified. The belief function specialization seems to provide the trade-off between precision and non-commitment that best fits our knowledge. Yet, the computational complexity inherent in belief functions shows up in running the full experiment: the final choice of the uncertainty treating technique to be used for solving the full scale diagnostic problem will have to take this factor into consideration.\n6. CONCLUSIONS\nWe have presented PULCinella (Propagating Uncertainty using Local Computation), and illustrated its use as a\n13 But notice that, differently from the Boolean case, further evidence may convert this support to one for the adjacent line individually.\nPulcinella: A General Tool for Propagating Uncertainty in Valuation Networks 331\ncomparison tool. The key for Pulcinella's comparison power lies in the separation made between the process of modelling the structural knowledge of a problem, and that of modelling its qualitative knowledge. Once a struc tural model has been decided, we can superimpose any of the available uncertainty calculi on it. There are at least two places where Pulcinella is expected to be useful. First, on the desk of the theorist involved in the comparative study of uncertain reasoning models. For her, Pulcinella might play the role of a slide-rule when empirically test ing the behaviour of uncertainty theories over sample problems of academic interest. The fact that the structural model of the problem is the same for all theories ensures the soundness of the test. Second, on the desk of the knowledge engineer. Here, Pulcinella might prove help ful in checking out different uncertainty management techniques for solving the problem at hands, and then judging-on this experimental basis-which one appears to be the most adequate to our case. In particular, we may test, for each theory, both the input requirements and the results, and evaluate how the available data is accommodated for, or our expectations satisfied.\nAcknowledgements\nThe work reported here has greatly benefit from discussions with (and comments from) Yen-Teh Hsia, Robert Kennes, Bruno Marchal, and Philippe Smets. Hong Xu provided invaluable assistance during the development of Pulcinella. lnaki Laresgoiti and Francesco Ferri have been worthy sources of inspiration for our examples. Work by the first author has been partially supported by the ARCHON project, funded by the Commission of the European Communities under the ESPRIT-II Program, P-2256. Work by the second author has been supported by a grant of IRIDIA, Universite Libre de Bruxelles.\nReferences\nAndersen, S.K., Olesen, K.G., Jensen, F.V. and Jensen, F. (1989) \"HUGIN - a Shell for Building Bayesian Belief Universes for Expert Systems\", Procs. of the 11th Int. Congress on AI (Detroit)."}, {"heading": "Chatalic, P., Dubois, D. and Prade, H. (1987) \"A System for Handling", "text": "Relational Dependencies in Approximate Reasoning\", Procs. of the 3rd Int. Expert System Con[. (London, GB) 495-502.\nDempster, A.P. (1966) ''Upper and Lower Probabilities Induced by a Multivalued Mapping\", Annals of Mathematical Statistic 38: 325- 339."}, {"heading": "Dubois, D. and Prade, H. (1990) \"Inference in Possibilistic Hypergraphs\", Procs. of the 6th IPMU Conference (Paris, Fr). Duda, R.O., Hart, P.E. and Nilsson, N.J. (1986) \"Subjective Bayesian", "text": "Methods for Rule-Based Inference Systems\", AFIPS Procs. 45 (New York)."}, {"heading": "Fox, J. (1986) \"Three Arguments for Extending the Framework of", "text": "Probability\", in L. Kana! and J. Lemmer (Eds.) Uncertainty in AI (North Holland).\nGallastegui, 1., Laresgoiti, I., Perez, j., Amantegui, j., Echavarri, J. (1989) \"Operating experience of an expert system for fault analysis in electrical networks\", Procs of Int. Working Conf. on Expert Systems in Electr. & Power Systems (Avignon, Fr)."}, {"heading": "Hsia, Y. and Shenoy, P.P. (1989) \"MacEvidence: A Visual", "text": "Environment for Constructing and Evaluating Evidential Systems\", Working Paper No. 211 (School of Business, Univ. of Kansas, Lw).\nLauritzen, S.L. and Spiegelhalter, D.J. (1988) \"Local Computations with Probabilities on Graphical Structures and Their Applications to Expert Systems\", J. of the Royal Stat. Soc. 8 50(2) 157-224.\nPearl, J. (1988) Probabilistic Reasoning in Intelligent Systems (Morgan Kaufman, CA).\nSaffiotti, A. (1987) \"An AI view of the treatment of uncertainty\", The Knowledge Engineering Review 2(2) (Cambridge University Press, UK) 75-97."}, {"heading": "Saffiotti, A. and Umkehrer, E. (1991a) \"PULCINELLA User's", "text": "Manual\", Technical Report IRIDIA/91-5, (Universite Libre de Bruxelles, Be).\nSaffiotti, A. and Umkehrer, E. (1991b) \"What do you want to propagate for breakfast ?\", Technical Report IRIDIA/91-7, (Universite Libre de Bruxelles, Be).\nShafer, G. (1976) A Mathematical Theory of Evidence (Princeton University Press, Princeton)."}, {"heading": "Shafer, G., Shenoy, P.P. and Mellouli, K. (1987) \"Propagating Belief", "text": "Functions in Qualitative Markov Trees\", Int. f. of Approx. Reas. 1:349-400."}, {"heading": "Shafer, G.R. and Shenoy, P.P. (1988a) \"Bayesian and Belief-Function", "text": "Propagation\", Working Paper No. 192 (School of Business, Univ. of Kansas, LW)."}, {"heading": "Shenoy, P.P. (1989) \"A Valuation-based Language for Expert", "text": "Systems\", Int. J. of Approx. Reas. 3: 383-411.\nShenoy, P.P. and Shafer, G. (1988b) \"An Axiomatic Framework for Bayesian and Belief-Function Propagation\", Procs. of AAAI Workshop on Uncertainty in AI: 307-314."}, {"heading": "Smets, Ph. (1988) \"Belief Functions\", in: Smets, Ph., Mamdani, E.H.,", "text": "Dubois, D. and Prade, H. (Eds.) Non-Standard Logics for Automated Reasoning (Academic Press, London).\nWittgenstein, L. (1921) Tractatus Logico-Philosophicus (Annalen der Naturphilosophie)."}, {"heading": "Xu, H. (1991) \"An Efficient Implementation of Belief Function", "text": "Propagation\", Technical Report IRIDIA/91-4, (Universite Libre de Bruxelles, Be)."}, {"heading": "Zadeh, L.A. (1979) \"A Theory of Approximate Reasoning\", in: J.E.", "text": "Hayes, D. Mitchie, L.l. Mikulich (Eds.) Machine Intelligence 10 (Elsevier) 149-194."}, {"heading": "Zarley, O.K., Hsia, Y. and Shafer, G. R. (1988) \"Evidential Reasoning", "text": "using DELIEF\", Working Paper 193 (School of Business, Univ. of Kansas, Lw)."}], "references": [{"title": "A System for Handling", "author": ["P. Chatalic", "D. Dubois", "H. Prade"], "venue": null, "citeRegEx": "Chatalic et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Chatalic et al\\.", "year": 1987}, {"title": "'Upper and Lower Probabilities Induced by a Multivalued Mapping", "author": ["A.P. Dempster"], "venue": "Annals of Mathematical Statistic", "citeRegEx": "Dempster,? \\Q1966\\E", "shortCiteRegEx": "Dempster", "year": 1966}, {"title": "Three Arguments for Extending the Framework of Probability\", in L", "author": ["J. Fox"], "venue": "Kana! and J. Lemmer (Eds.) Uncertainty in AI", "citeRegEx": "Fox,? \\Q1986\\E", "shortCiteRegEx": "Fox", "year": 1986}, {"title": "MacEvidence: A Visual Environment for Constructing and Evaluating Evidential Systems\", Working Paper No. 211 (School of Business, Univ. of Kansas, Lw)", "author": ["Y. Hsia", "P.P. Shenoy"], "venue": null, "citeRegEx": "Hsia and Shenoy,? \\Q1989\\E", "shortCiteRegEx": "Hsia and Shenoy", "year": 1989}, {"title": "Local Computations with Probabilities on Graphical Structures and Their Applications to Expert Systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "J. of the Royal Stat. Soc", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "An AI view of the treatment of uncertainty", "author": ["A. Saffiotti"], "venue": "The Knowledge Engineering Review", "citeRegEx": "Saffiotti,? \\Q1987\\E", "shortCiteRegEx": "Saffiotti", "year": 1987}, {"title": "PULCINELLA User's Manual\", Technical Report IRIDIA/91-5, (Universite Libre de Bruxelles, Be)", "author": ["A. Saffiotti", "E. Umkehrer"], "venue": null, "citeRegEx": "Saffiotti and Umkehrer,? \\Q1991\\E", "shortCiteRegEx": "Saffiotti and Umkehrer", "year": 1991}, {"title": "What do you want to propagate for breakfast ?", "author": ["A. Saffiotti", "E. Umkehrer"], "venue": "Technical Report IRIDIA/91-7, (Universite Libre de Bruxelles, Be)", "citeRegEx": "Saffiotti and Umkehrer,? \\Q1991\\E", "shortCiteRegEx": "Saffiotti and Umkehrer", "year": 1991}, {"title": "A Mathematical Theory of Evidence (Princeton University Press, Princeton)", "author": ["G. Shafer"], "venue": null, "citeRegEx": "Shafer,? \\Q1976\\E", "shortCiteRegEx": "Shafer", "year": 1976}, {"title": "Propagating Belief Functions in Qualitative Markov Trees", "author": ["G. Shafer", "P.P. Shenoy", "K. Mellouli"], "venue": "Int. f. of Approx. Reas", "citeRegEx": "Shafer et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Shafer et al\\.", "year": 1987}, {"title": "Bayesian and Belief-Function Propagation", "author": ["G.R. Shafer", "P.P. Shenoy"], "venue": "Working Paper No. 192 (School of Business,", "citeRegEx": "Shafer and Shenoy,? \\Q1988\\E", "shortCiteRegEx": "Shafer and Shenoy", "year": 1988}, {"title": "A Valuation-based Language for Expert Systems", "author": ["P.P. Shenoy"], "venue": "Int. J. of Approx. Reas", "citeRegEx": "Shenoy,? \\Q1989\\E", "shortCiteRegEx": "Shenoy", "year": 1989}, {"title": "1988b) \"An Axiomatic Framework for Bayesian and Belief-Function Propagation", "author": ["P.P. Shenoy", "G. Shafer"], "venue": "Procs. of AAAI Workshop on Uncertainty in AI:", "citeRegEx": "Shenoy and Shafer,? \\Q1988\\E", "shortCiteRegEx": "Shenoy and Shafer", "year": 1988}, {"title": "Non-Standard Logics for Automated Reasoning", "author": ["Smets", "Ph"], "venue": "\"Belief Functions\",", "citeRegEx": "Smets and Ph.,? \\Q1988\\E", "shortCiteRegEx": "Smets and Ph.", "year": 1988}, {"title": "Tractatus Logico-Philosophicus (Annalen der Naturphilosophie)", "author": ["L. Wittgenstein"], "venue": null, "citeRegEx": "Wittgenstein,? \\Q1921\\E", "shortCiteRegEx": "Wittgenstein", "year": 1921}, {"title": "An Efficient Implementation of Belief Function Propagation\", Technical Report IRIDIA/91-4, (Universite Libre de Bruxelles, Be)", "author": ["H. Xu"], "venue": null, "citeRegEx": "Xu,? \\Q1991\\E", "shortCiteRegEx": "Xu", "year": 1991}, {"title": "Evidential Reasoning using DELIEF\", Working Paper 193 (School of Business, Univ. of Kansas, Lw)", "author": ["O.K. Zarley", "Y. Hsia", "G.R. Shafer"], "venue": null, "citeRegEx": "Zarley et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Zarley et al\\.", "year": 1988}], "referenceMentions": [{"referenceID": 4, "context": "Moving from consideration of efficiency, ease of representation, and generality, a num\u00ad ber of techniques for representing and propagating uncer\u00ad tainty in networks have been proposed (e.g. Chatalic et a!., 1987; Lauritzen and Spiegelhalter, 1988; Pearl, 1988; Shafer et a!., 1987).", "startOffset": 184, "endOffset": 281}, {"referenceID": 5, "context": "Moving from consideration of efficiency, ease of representation, and generality, a num\u00ad ber of techniques for representing and propagating uncer\u00ad tainty in networks have been proposed (e.g. Chatalic et a!., 1987; Lauritzen and Spiegelhalter, 1988; Pearl, 1988; Shafer et a!., 1987).", "startOffset": 184, "endOffset": 281}, {"referenceID": 3, "context": "Moreover, implementations of these techniques have been developed (e.g. Andersen et a!., 1989; Hsia and Shenoy, 1989; Zarley et a!., 1988; Xu, 1991).", "startOffset": 66, "endOffset": 148}, {"referenceID": 16, "context": "Moreover, implementations of these techniques have been developed (e.g. Andersen et a!., 1989; Hsia and Shenoy, 1989; Zarley et a!., 1988; Xu, 1991).", "startOffset": 66, "endOffset": 148}, {"referenceID": 2, "context": "This is unfortunate: if we accept that uncertainty theories should be seen as alternatives, rather than rivals (Fox, 1986; Saffiotti, 1987) then we must also accept that for each problem there is a \"most adequate\" theory, and this theory is in general different from problem to problem.", "startOffset": 111, "endOffset": 139}, {"referenceID": 6, "context": "This is unfortunate: if we accept that uncertainty theories should be seen as alternatives, rather than rivals (Fox, 1986; Saffiotti, 1987) then we must also accept that for each problem there is a \"most adequate\" theory, and this theory is in general different from problem to problem.", "startOffset": 111, "endOffset": 139}, {"referenceID": 4, "context": "Lauritzen and Spiegelhalter's and Pearl's). This framework has been further generalized by Shenoy (1989), who proposes a class of languages (\"valuation-based languages\") for building knowledge-based systems.", "startOffset": 0, "endOffset": 105}, {"referenceID": 16, "context": "Pulcinella is a general implementation of valuation based languages, abstracted from a belief function propagation system (Xu, 1991), and it is fully described in (Saffiotti and Umkehrer, 1991a).", "startOffset": 122, "endOffset": 132}, {"referenceID": 12, "context": "Shenoy' s valuation-based languages (Shenoy, 1989) have been abstracted from the axiomatic framework for probabilities and belief functions propagation of Shafer and Shenoy (Shafer, Shenoy and Mellouli, 1987; Shafer and Shenoy, 1988a; Shenoy and Shafer, 1988b).", "startOffset": 36, "endOffset": 50}, {"referenceID": 12, "context": "For probability theory, belief-functions, and a Boolean case, the mapping of the theory into the concepts of a valuation-based language have been proposed by Shenoy and Shafer (Shenoy and Shafer, 1988b; Shenoy, 1989).", "startOffset": 176, "endOffset": 216}, {"referenceID": 9, "context": "For probability theory, belief-functions, and a Boolean case, the mapping of the theory into the concepts of a valuation-based language have been proposed by Shenoy and Shafer (Shenoy and Shafer, 1988b; Shenoy, 1989). For possibility theory, we use the mapping proposed by Dubois and Prade (1990) building on a previous work by Zadeh (1979).", "startOffset": 169, "endOffset": 297}, {"referenceID": 9, "context": "For probability theory, belief-functions, and a Boolean case, the mapping of the theory into the concepts of a valuation-based language have been proposed by Shenoy and Shafer (Shenoy and Shafer, 1988b; Shenoy, 1989). For possibility theory, we use the mapping proposed by Dubois and Prade (1990) building on a previous work by Zadeh (1979).", "startOffset": 169, "endOffset": 341}, {"referenceID": 1, "context": "4 This corresponds to usual (but un-normalized) Dempster's rule of combination (Dempster, 1966).", "startOffset": 79, "endOffset": 95}, {"referenceID": 9, "context": "However, Shafer and Shenoy (1988b) proposed a general local computation schema for evaluating valuation systems.", "startOffset": 9, "endOffset": 35}, {"referenceID": 3, "context": "Algorithms based on the above technique have already been proposed and implemented (Zarley et a!., 1988; Hsia and Shenoy, 1989; Xu, 1991).", "startOffset": 83, "endOffset": 137}, {"referenceID": 16, "context": "Algorithms based on the above technique have already been proposed and implemented (Zarley et a!., 1988; Hsia and Shenoy, 1989; Xu, 1991).", "startOffset": 83, "endOffset": 137}, {"referenceID": 16, "context": "The system builds on Hong Xu's implementation of the belief functions propagation technique of Shafer and Shenoy (Xu, 1991).", "startOffset": 113, "endOffset": 123}, {"referenceID": 6, "context": "Our first example is adapted from (Saffiotti, 1987).", "startOffset": 34, "endOffset": 51}, {"referenceID": 9, "context": "8 For greater readability, we show the results using bel and pi func\u00ad tions (Shafer, 1976).", "startOffset": 76, "endOffset": 90}, {"referenceID": 15, "context": "12 \u00abWovon man nicht sprechen kann, dariiber muB man schweigen\u00bb (What we cannot speak about we must consign to silence) (Wittgenstein, 1921).", "startOffset": 119, "endOffset": 139}], "year": 2011, "abstractText": "We present PULCinella and its use in comparing uncertainty theories. PULCinella is a general tool for J:ropagating U ncertainty based on the l,ocal \ufffdamputation technique of Shafer and Shenoy. It may be specialized to different uncertainty theories: at the moment, Pulcinella can propagate probabilities, belief functions, Boolean values, and possibilities. Moreover, Pulcinella allows the user to easily define his own specializations. To illustrate Pulcinella, we analyze two examples by using each of the four theories above. In the first one, we mainly focus on intrinsic differences between theories. In the second one, we take a knowledge engineer viewpoint, and check the adequacy of each theory to a given problem.", "creator": "pdftk 1.41 - www.pdftk.com"}}}