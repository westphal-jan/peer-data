{"id": "1304.1574", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2013", "title": "Generalization Bounds for Domain Adaptation", "abstract": "In this paper, we provide a new framework to obtain the generalization bounds of the learning process for domain adaptation, and then apply the derived bounds to analyze the asymptotical convergence of the learning process. Without loss of generality, we consider two kinds of representative domain adaptation: one is with multiple sources and the other is combining source and target data. Our understanding of the relationship between a learning process and a target dataset is well documented by several authors and is in the current work of the field.\n\n\n\n\nThe paper is part of an ongoing series of articles in Psychological Science entitled \"Understanding and Constrain the Impact of Adaptation on Learning\", which are part of the Journal of the Society for Psychological Science.\n\n\nThis paper examines how the learning process and the specific targets for the learning process in the domain adaptation, and also considers its implications for the generalization of the learning process for domain adaptation. This paper summarizes the paper's analysis and provides further information about the relationship between a learning process and a target dataset.\nAn original paper appeared in the Journal of the Society for Psychological Science.\nIn the present paper we analyze data in the domain adaptation for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the learning process for the", "histories": [["v1", "Thu, 4 Apr 2013 22:34:55 GMT  (421kb)", "http://arxiv.org/abs/1304.1574v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.PR", "authors": ["chao zhang", "lei zhang", "jieping ye"], "accepted": true, "id": "1304.1574"}, "pdf": {"name": "1304.1574.pdf", "metadata": {"source": "CRF", "title": "Generalization Bounds for Domain Adaptation", "authors": ["Chao Zhang", "Lei Zhang", "Jieping Ye"], "emails": ["zhangchao1015@gmail.com;", "jieping.ye@asu.edu", "zhanglei.njust@yahoo.com.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 4.\n15 74\nv1 [\ncs .L\nIn particular, we use the integral probability metric to measure the difference between two domains. For either kind of domain adaptation, we develop a related Hoeffding-type deviation inequality and a symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number. We also generalized the classical McDiarmid\u2019s inequality to a more general setting where independent random variables can take values from different domains. By using this inequality, we then obtain generalization bounds based on the Rademacher complexity. Afterwards, we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process and the numerical experiments support our theoretical findings as well."}, {"heading": "1 Introduction", "text": "The generalization bound measures the probability that a function, chosen from a function class by an algorithm, has a sufficiently small error and plays an important role in statistical learning theory (see Vapnik, 1999; Bousquet et al., 2004). The generalization bounds have been widely used to study the consistency of the ERM-based learning process (Vapnik, 1999), the asymptotic convergence of empirical process (Van der Vaart and Wellner, 1996) and the learnability of learning models (Blumer et al., 1989). Generally, there are three essential aspects to obtain the generalization bounds of a specific learning process: complexity measures of function classes, deviation (or concentration) inequalities and symmetrization inequalities related to the learning process. For example, Van der Vaart and Wellner (1996) presented the generalization bounds\nbased on the Rademacher complexity and the covering number, respectively. Vapnik (1999) gave the generalization bounds based on the Vapnik-Chervonenkis (VC) dimension. Bartlett et al. (2005) proposed the local Rademacher complexity and obtained a sharp generalization bound for a particular function class {f \u2208 F : Ef 2 < \u03b2Ef, \u03b2 > 0}. Hussain and Shawe-Taylor (2011) showed improved loss bounds for multiple kernel learning.\nIt is noteworthy that the aforementioned results of statistical learning theory are all built under the assumption that training and test data are drawn from the same distribution (or briefly called the assumption of same distribution). This assumption may not be valid in the situation that training and test data have different distributions, which will arise in many practical applications including speech recognition (Jiang and Zhai, 2007) and natural language processing (Blitzer et al., 2007). Domain adaptation has recently been proposed to handle this situation and it is aimed to apply a learning model, trained by using the samples drawn from a certain domain (source domain), to the samples drawn from another domain (target domain) with a different distribution (see Bickel et al., 2007; Wu and Dietterich, 2004; Blitzer et al., 2006; Ben-David et al., 2010; Bian et al., 2012).\nWithout loss of generality, this paper is mainly concerned with two types of representative domain adaptation. In the first type, the learner receives training data from several source domains, known as domain adaptation with multiple sources (see Crammer et al., 2006, 2008; Mansour et al., 2008, 2009a). In the second type, the learner minimizes a convex combination of the source and the target empirical risks, termed as domain adaptation combining source and target data (see Ben-David et al., 2010; Blitzer et al., 2008)."}, {"heading": "1.1 Overview of Main Results", "text": "In this paper, we present a new framework to obtain the generalization bounds of the learning process for the aforementioned two kinds of representative domain adaptation, respectively. Based on the resultant bounds, we then analyze the asymptotical properties of the learning processes for the two types of domain adaptation. There are four major aspects in the framework:\n\u2022 the quantity measuring the difference between two domains;\n\u2022 the complexity measure of function class;\n\u2022 the deviation inequalities of the learning process for domain adaptation;\n\u2022 the symmetrization inequality of the learning precess for domain adaptation. Generally, in order to obtain the generalization bounds of a learning process, one needs to develop the related deviation (or concentration) inequalities of the learning process. For either kind of domain adaptation, we use a martingale method to develop the related Hoeffding-type deviation inequality. Moreover, in the situation of domain adaptation, since the source domain differs from the target domain, the desired symmetrization inequality for domain adaptation should incorporate some quantity to reflect the difference. From the point of this view, we then obtain the related symmetrization inequality incorporating the integral probability metric that measures the difference between the distributions of the source and the target domains. Next, we present the generalization bounds based on the uniform entropy number for both kinds of domain adaptation. Also, we generalize the classical McDiarmid\u2019s inequality to a more general setting, where independent random variables take values from different domains. By using the derived\ninequality, we obtain the generalization bounds based on the Rademacher complexity. Following the resultant bounds, we study the asymptotic convergence and the rate of convergence of the learning process in addition to a discussion on factors that affect the asymptotic behaviors. The numerical experiments support our theoretical findings as well. Meanwhile, we give a comparison with the related results under the assumption of same distribution."}, {"heading": "1.2 Organization of the Paper", "text": "The rest of this paper is organized as follows. Section 2 introduces the problems studied in this paper. Section 3 introduces the integral probability metric to measure the difference between two domains. In Section 4, we introduce two kinds of complexity measures of function classes including the uniform entropy number and the Rademacher complexity. In Section 5 (resp. Section 6), we present the generalization bounds of the learning process for domain adaptation with multiple sources (resp. combining source and target data), and then analyze the asymptotic behavior of the learning process in addition to the related numerical experiment supporting our findings. In Section 7, we list the existing works on the theoretical analysis of domain adaptation as a comparison and the last section concludes the paper. In the appendices, we prove main results of this paper. For clarity of presentation, we also postpone the discussion of the deviation inequalities and the symmetrization inequalities in the appendices."}, {"heading": "2 Problem Setup", "text": "In this section, we formalize the main issues of this paper by introducing some necessary notations"}, {"heading": "2.1 Domain Adaptation with Multiple Sources", "text": "We denote Z(Sk) := X (Sk)\u00d7Y (Sk) \u2282 RI \u00d7RJ (1 \u2264 k \u2264 K) and Z(T ) := X (T )\u00d7Y (T ) \u2282 RI \u00d7RJ as the k-th source domain and the target domain, respectively. Set L = I + J . Let D(Sk) and D(T ) stand for the distributions of the input spaces X (Sk) (1 \u2264 k \u2264 K) and X (T ), respectively. Denote g (Sk) \u2217 : X (Sk) \u2192 Y (Sk) and g(T )\u2217 : X (T ) \u2192 Y (T ) as the labeling functions of Z(Sk) (1 \u2264 k \u2264 K) and Z(T ), respectively. In the situation of domain adaptation with multiple sources, the input-space distributions D(Sk) (1 \u2264 k \u2264 K) and D(T ) differ from each other, or g(Sk)\u2217 (1 \u2264 k \u2264 K) and g(T )\u2217 differ from each other, or both of the cases occur. There are sufficient amounts of i.i.d. samples ZNk1 = {z(k)n }Nkn=1 drawn from each source domain Z(Sk) (1 \u2264 k \u2264 K) but little or no labeled samples drawn from the target domain Z(T ).\nGiven w = (w1, \u00b7 \u00b7 \u00b7 , wK) \u2208 [0, 1]K with \u2211K\nk=1wk = 1, let gw \u2208 G be the function that minimizes the empirical risk\nE(S) w\n(\u2113 \u25e6 g) = K\u2211\nk=1\nwkE (Sk) Nk\n(\u2113 \u25e6 g) = K\u2211\nk=1\nwk Nk\nNk\u2211\nn=1\n\u2113(g(x(k)n ),y (k) n ) (1)\nover G with respect to sample sets {ZNk1 }Kk=1, and it is expected that gw will perform well on the target expected risk:\nE(T )(\u2113 \u25e6 g) := \u222b \u2113(g(x(T )),y(T ))dP(z(T )), g \u2208 G, (2)\ni.e., gw approximates the labeling g (T ) \u2217 as precisely as possible.\nIn the learning process of domain adaptation with multiple sources, we are mainly interested in the following two types of quantities:\n\u2022 E(T )(\u2113 \u25e6 gw)\u2212E(S)w (\u2113 \u25e6 gw), which corresponds to the estimation of the expected risk in the target domain Z(T ) from the empirical quantity that is the weighted combination of the empirical risks in the multiple sources {Z(Sk)}Kk=1;\n\u2022 E(T )(\u2113\u25e6gw)\u2212E(T )(\u2113\u25e6 g\u0303\u2217), which corresponds to the performance of the algorithm for domain adaptation with multiple sources,\nwhere g\u0303\u2217 \u2208 G is the function that minimizes the expected risk E(T )(\u2113 \u25e6 g) over G. Recalling (1) and (2), since\nE(S) w (\u2113 \u25e6 g\u0303\u2217)\u2212 E(S)w (\u2113 \u25e6 gw) \u2265 0,\nwe have\nE(T )(\u2113 \u25e6 gw) =E(T )(\u2113 \u25e6 gw)\u2212 E(T )(\u2113 \u25e6 g\u0303\u2217) + E(T )(\u2113 \u25e6 g\u0303\u2217) \u2264E(S)\nw (\u2113 \u25e6 g\u0303\u2217)\u2212 E(S)w (\u2113 \u25e6 gw) + E(T )(\u2113 \u25e6 gw)\u2212 ET (\u2113 \u25e6 g\u0303\u2217) + ET (\u2113 \u25e6 g\u0303\u2217)\n\u22642 sup g\u2208G\n\u2223\u2223E(T )(\u2113 \u25e6 g)\u2212 E(S) w (\u2113 \u25e6 g) \u2223\u2223+ E(T )(\u2113 \u25e6 g\u0303\u2217), (3)\nand thus\n0 \u2264 E(T )(\u2113 \u25e6 gw)\u2212 E(T )(\u2113 \u25e6 g\u0303\u2217) \u2264 2 sup g\u2208G\n\u2223\u2223E(T )(\u2113 \u25e6 g)\u2212 E(S) w (\u2113 \u25e6 g) \u2223\u2223.\nThis shows that the asymptotic behaviors of the aforementioned two quantities when the sample numbers N1, \u00b7 \u00b7 \u00b7 , NK go to infinity can both be described by the supremum\nsup g\u2208G\n\u2223\u2223E(T )(\u2113 \u25e6 g)\u2212 E(S) w (\u2113 \u25e6 g) \u2223\u2223, (4)\nwhich is the so-called generalization bound of the learning process for domain adaptation with multiple sources.\nFor convenience, we define the loss function as class\nF := {z 7\u2192 \u2113(g(x),y) : g \u2208 G}, (5)\nand call F as the function class in the rest of this paper. By (1) and (2), given sample sets {ZNk1 }Kk=1 drawn from {Z(Sk)}Kk=1 respectively, we briefly denote for any f \u2208 F ,\nE(T )f := \u222b f(z(T ))dP(z(T )) , (6)\nand\nE(S) w f :=\nK\u2211\nk=1\nwk Nk\nNk\u2211\nn=1\nf(z(k)n ). (7)\nThus, we rewrite the generalization bound (4) for domain adaptation with multiple sources as\nsup f\u2208F\n\u2223\u2223E(T )f \u2212 E(S) w f \u2223\u2223. (8)"}, {"heading": "2.2 Domain Adaptation Combining Source and Target Data", "text": "Denote Z(S) := X (S)\u00d7Y (S) \u2282 RI\u00d7RJ and Z(T ) := X (T )\u00d7Y (T ) \u2282 RI\u00d7RJ as the source domain and the target domain, respectively. Let D(S) and D(T ) stand for the distributions of the input spaces X (S) and X (T ), respectively. Denote g(S)\u2217 : X (S) \u2192 Y (S) and g(T )\u2217 : X (T ) \u2192 Y (T ) as the labeling functions of Z(S) and Z(T ), respectively. In the situation of domain adaptation combining source and target data (see Blitzer et al., 2008; Ben-David et al., 2010), the input-space distributions D(S) and D(T ) differ from each other, or the labeling functions g(S)\u2217 and g(T )\u2217 differ from each other, or both cases occur. There are some (but not enough) samples ZNT1 := {z(T )n }NTn=1 drawn from the target domain Z(T ) in addition to a large amount of samples ZNS1 := {z(S)n }NSn=1 drawn from the source domain Z(S) with N (T ) \u226a N (S). Given a \u03c4 \u2208 [0, 1), we denote g\u03c4 \u2208 G as the function that minimizes the convex combination of the source and the target empirical risks over G:\nE\u03c4 (\u2113 \u25e6 g) := \u03c4E(T )NT (\u2113 \u25e6 g) + (1\u2212 \u03c4)E (S) NS (\u2113 \u25e6 g), (9) and it is expected that g\u03c4 will perform well for any pair z\n(T ) = (x(T ),y(T )) \u2208 Z(T ), i.e., g\u03c4 approximates the labeling function g (T ) \u2217 as precisely as possible.\nAs mentioned by Blitzer et al. (2008); Ben-David et al. (2010), setting \u03c4 involves a tradeoff between the source data that are sufficient but not accurate and the target data that are accurate but not sufficient. Especially, setting \u03c4 = 0 provides a learning process of the basic domain adaptation with one single source (see Ben-David et al., 2006).\nSimilar to the situation of domain adaptation with multiple sources, two types of quantities: E(T )(\u2113 \u25e6 g\u03c4 )\u2212E\u03c4 (\u2113 \u25e6 g\u03c4 ) and E(T )(\u2113 \u25e6 g\u03c4 )\u2212E(T )(\u2113 \u25e6 g\u0303\u2217) also play an essential role in analyzing the asymptotic behavior of the learning process for domain adaptation combining source and target data. By the similar way of (3), we need to consider the supremum\nsup g\u2208G\n\u2223\u2223E(T )(\u2113 \u25e6 g)\u2212 E\u03c4 (\u2113 \u25e6 g) \u2223\u2223, (10)\nwhich is the so-called generalization bound of the learning process for domain adaptation combining source and target data. Following the notation of (5) and taking f = \u2113 \u25e6 g, we can equivalently rewrite the generalization bound (10) as\nsup f\u2208F\n\u2223\u2223E(T )f \u2212 E\u03c4f \u2223\u2223. (11)"}, {"heading": "3 Integral Probability Metric", "text": "As shown in some existing works (see Mansour et al., 2008, 2009a; Ben-David et al., 2010, 2006), one of major challenges in the theoretical analysis of domain adaptation is to find a quantity to measure the difference between the source domain Z(S) and the target domain Z(T ). Then, one can use the quantity to achieve generalization bounds for domain adaptation. In this section, we use the integral probability metric to measure the difference between the distributions of Z(S) and Z(T ), and then discuss the relationship between the integral probability metric and other quantities proposed in existing works, e.g., the H-divergence and the discrepancy distance (see Ben-David et al., 2010; Mansour et al., 2009b). Moreover, we will show that there is a special situation of domain adaptation, where the integral probability metric performs better than other quantities (see Remark 3.1)"}, {"heading": "3.1 Integral Probability Metric", "text": "In Ben-David et al. (2010, 2006), the H-divergence was introduced to derive the generalization bounds based on the VC dimension under the condition of \u201c\u03bb-close\u201d. Mansour et al. (2009b) obtained the generalization bounds based on the Rademacher complexity by using the discrepancy distance. Both quantities are aimed to measure the difference between two input-space distributions D(S) and D(T ). Moreover, Mansour et al. (2009a) used the Re\u0301nyi divergence to measure the distance between two distributions. In this paper, we use the following quantity to measure the difference between the distributions of the source and the target domains:\nDefinition 3.1 Given two domains Z(S),Z(T ) \u2282 RL, let z(S) and z(T ) be the random variables taking values from Z(S) and Z(T ), respectively. Let F \u2282 RZ be a function class. We define\nDF(S, T ) := sup f\u2208F\n|E(S)f \u2212 E(T )f |, (12)\nwhere the expectations E(S) and E(T ) are taken on the distributions Z(S) and Z(T ), respectively.\nThe quantity DF(S, T ) is termed as the integral probability metric that has played an important role in probability theory for measuring the difference between the two probability distributions (see Zolotarev, 1984; Rachev, 1991; Mu\u0308ller, 1997; Reid and Williamson, 2011). Recently, Sriperumbudur et al. (2009, 2012) gave the further investigation and proposed an empirical method to compute the integral probability metric. As mentioned by Mu\u0308ller (1997)[page 432], the quantity DF(S, T ) is a semimetric and it is a metric if and only if the function class F separates the set of all signed measures with \u00b5(Z) = 0. Namely, according to Definition 3.1, given a non-trivial function class F , the integral probability metric DF(S, T ) is equal to zero if the domains Z(S) and Z(T ) have the same distribution.\nBy (5), the quantity DF(S, T ) can be equivalently rewritten as\nDF(S, T ) = sup g\u2208G\n\u2223\u2223\u2223E(S)\u2113(g(x(S)),y(S))\u2212 E(T )\u2113(g(x(T )),y(T )) \u2223\u2223\u2223\n=sup g\u2208G\n\u2223\u2223\u2223E(S)\u2113 ( g(x(S)), g(S)\u2217 (x (S)) ) \u2212 E(T )\u2113 ( g(x(T )), g(T )\u2217 (x (T )) )\u2223\u2223\u2223. (13)\nNext, based on the equivalent form (13), we discuss the relationships between the quantity DF(S, T ) and other quantities including the H-divergence and the discrepancy distance."}, {"heading": "3.2 Relationship with Other Quantities", "text": "Before the formal discussion, we briefly introduce the related quantities proposed in existing works (see Ben-David et al., 2010; Mansour et al., 2009b).\n3.2.1 H-Divergence and Discrepancy Distance In classification tasks, by setting \u2113 as the absolute-value loss function (\u2113(x,y) = |x \u2212 y|), Ben-David et al. (2010) introduced a variant of the H-divergence:\ndH\u25b3H(D(S),D(T )) = sup g1,g2\u2208H\n\u2223\u2223\u2223E(S)\u2113 ( g1(x (S)), g2(x (S)) ) \u2212 E(T )\u2113 ( g1(x (T )), g2(x (T )) )\u2223\u2223\u2223 (14)\nto achieve VC-dimension-based generalization bounds for domain adaptation under the condition of \u201c\u03bb-close\u201d: there exists a \u03bb > 0 such that\n\u03bb \u2265 inf g\u2208G\n{\u222b \u2113(g(x(S)),y(S))dP(z(S)) + \u222b \u2113(g(x(T )),y(T ))dP(z(T )) } .\nIn both of the classification and regression tasks, given a function class G and a loss function \u2113, Mansour et al. (2009b) defined the discrepancy distance as\ndisc\u2113(D(S),D(T )) = sup g1,g2\u2208G\n\u2223\u2223\u2223E(S)\u2113 ( g1(x (S)), g2(x (S)) ) \u2212 E(T )\u2113 ( g1(x (T )), g2(x (T )) )\u2223\u2223\u2223, (15)\nand then used this quantity to obtain the generalization bounds based on the Rademacher complexity.\nAs mentioned by Mansour et al. (2009b), the quantities (14) and (15) match in the setting of classification tasks by setting \u2113 as the absolute-value loss function, while the usage of (15) does not require the condition of \u201c\u03bb-close\u201d but the usage of (14) does. Recalling Definition 3.1, since there is no limitation on the function class F , the integral probability metric DF(S, T ) can be used in both classification and regression tasks. Therefore, we only consider the relationship between the integral probability metric DF(S, T ) and the discrepancy distance disc\u2113(D(S),D(T )).\n3.2.2 Relationship between DF(S, T ) and disc\u2113(D(S),D(T )) From Definition 3.1 and (13), we can find that the integral probability metric DF(S, T ) measures the difference between the distributions of the two domains Z(S) and Z(T ). However, as addressed in Section 2, if a domain Z(S) differs from another domain Z(T ), there are three possibilities: the input-space distribution D(S) differs from D(T ), or g(S)\u2217 differs from g(T )\u2217 , or both of them occur. Therefore, it is necessary to consider two kinds of differences: the difference between the input-space distributions D(S) and D(T ) and the difference between the labeling functions g(S)\u2217 and g (T ) \u2217 . Next, we will show that the integral probability metric DF(S, T ) can be bounded by using two separate quantities that can measure the difference between D(S) and D(T ) and the difference between g (S) \u2217 and g (T ) \u2217 , respectively.\nAs shown in (15), the quantity disc\u2113(D(S),D(T )) actually measures the difference between the input-space distributions D(S) and D(T ). Moreover, we introduce another quantity to measure the difference between the labeling functions g (S) \u2217 and g (T ) \u2217 :\nDefinition 3.2 Given a loss function \u2113 and a function class G, we define\nQ (T ) G (g (S) \u2217 , g (T ) \u2217 ) := sup\ng1\u2208G\n\u2223\u2223\u2223E(T )\u2113 ( g1(x (T )), g(T )\u2217 (x (T )) ) \u2212 E(T )\u2113 ( g1(x (T )), g(S)\u2217 (x (T )) )\u2223\u2223\u2223. (16)\nNote that if the loss function \u2113 and the function class G are both non-trivial (i.e., F is nontrivial), the quantity Q\n(T ) G (g (S) \u2217 , g (T ) \u2217 ) is a (semi)metric between the labeling functions g (S) \u2217 and\ng (T ) \u2217 . In fact, it is not hard to verify that Q (T ) G (g (S) \u2217 , g (T ) \u2217 ) satisfies the triangle inequality and is equal to zero if g (S) \u2217 and g (T ) \u2217 match.\nBy combining (13), (15) and (16), we have\ndisc\u2113(D(S),D(T )) = sup g1,g2\u2208G\n\u2223\u2223\u2223E(S)\u2113 ( g1(x (S)), g2(x (S)) ) \u2212 E(T )\u2113 ( g1(x (T )), g2(x (T )) )\u2223\u2223\u2223\n\u2265 sup g1\u2208G\n\u2223\u2223\u2223E(S)\u2113 ( g1(x (S)), g(S)\u2217 (x (S)) ) \u2212 E(T )\u2113 ( g1(x (T )), g(S)\u2217 (x (T )) )\u2223\u2223\u2223\n= sup g1\u2208G\n\u2223\u2223\u2223E(S)\u2113 ( g1(x (S)), g(S)\u2217 (x (S)) ) \u2212 E(T )\u2113 ( g1(x (T )), g(T )\u2217 (x (T )) )\n+ E(T )\u2113 ( g1(x (T )), g(T )\u2217 (x (T )) ) \u2212 E(T )\u2113 ( g1(x (T )), g(S)\u2217 (x (T )) )\u2223\u2223\u2223\n\u2265 sup g1\u2208G\n\u2223\u2223\u2223E(S)\u2113 ( g1(x (S)), g(S)\u2217 (x (S)) ) \u2212 E(T )\u2113 ( g1(x (T )), g(T )\u2217 (x (T )) )\u2223\u2223\u2223\n\u2212 sup g1\u2208G\n\u2223\u2223\u2223E(T )\u2113 ( g1(x (T )), g(T )\u2217 (x (T )) ) \u2212 E(T )\u2113 ( g1(x (T )), g(S)\u2217 (x (T )) )\u2223\u2223\u2223\n=DF(S, T )\u2212Q(T )G (g(S)\u2217 , g(T )\u2217 ), (17)\nand thus DF(S, T ) \u2264 disc\u2113(D(S),D(T )) +Q(T )G (g(S)\u2217 , g(T )\u2217 ), (18)\nwhich implies that the integral probability metric DF(S, T ) can be bounded by the summation of the discrepancy distance disc\u2113(D(S),D(T )) and the quantity Q(T )G (g (S) \u2217 , g (T ) \u2217 ), which measure the difference between the input-space distributions D(S) and D(T ) and the difference between the labeling functions g (S) \u2217 and g (T ) \u2217 , respectively.\nRemark 3.1 Note that there is a specific case in the situation of domain adaptation: D(S) differs from D(T ) and meanwhile g(S)\u2217 differs from g(T )\u2217 , while the distribution of the domain Z(S) matches with that of the domain Z(T ). In this case, the integral probability metric DF(S, T ) equals to zero, but disc\u2113(D(S),D(T )) or Q(T )G (g(S)\u2217 , g(T )\u2217 ) neither equals to zero. Therefore, the integral probability metric DF(S, T ) is more suitable for this case than the discrepancy distance disc\u2113(D(S),D(T ))."}, {"heading": "4 Complexity Measures of Function Classes", "text": "Generally, the generalization bound of a certain learning process is achieved by incorporating some complexity measure of the function class, e.g., the covering number, the VC dimension and the Rademacher complexity. In this paper, we are mainly concerned with the uniform entropy number and the Rademacher complexity."}, {"heading": "4.1 Uniform Entropy Number", "text": "The uniform entropy number is derived from the concept of the covering number and we refer to Mendelson (2003) for details. The covering number of a function class F is defined as follows:\nDefinition 4.1 Let F be a function class and d be a metric on F . For any \u03be > 0, the covering number of F at radius \u03be with respect to the metric d, denoted by N (F , \u03be, d) is the minimum size of a cover of radius \u03be.\nIn some classical results of statistical learning theory, the covering number is applied by letting d be the distribution-dependent metric. For example, as shown in Theorem 2.3 of Mendelson (2003), one can set d as the norm \u21131(Z N 1 ) and then derive the generalization bound of the i.i.d. learning process by incorporating the expectation of the covering number, i.e., EN (F , \u03be, \u21131(ZN1 )). However, in the situation of domain adaptation, we only know the information of the source domain, while the expectation EN (F , \u03be, \u21131(ZN1 )) is dependent on the distributions of the source and the target domains because z = (x,y). Therefore, the covering number is no longer suitable for our framework to obtain the generalization bounds for domain adaptation. In contrast, the uniform entropy number is distribution-free and thus we choose it as the complexity measure of function classes to derive the generalization bounds for domain adaptation.\nNext, we will consider the uniform entropy number of F in the situations of two types of domain adaptation: (i) domain adaptation with multiple sources; (ii) domain adaptation combining source and target data, respectively."}, {"heading": "4.1.1 Domain Adaptation with Multiple Sources", "text": "For clarity of presentation, we give a useful notation for the following discussion. Let {ZNk1 }Kk=1 := {{z(k)n }Nkn=1}Kk=1 be the collection of sample sets drawn from multiple sources {Z(Sk)}Kk=1, respectively. Denote {Z\u2032Nk1 }Kk=1 := {{z\u2032(k)n }Nkn=1}Kk=1 as the collection of the ghost sample sets drawn from {Z(Sk)}Kk=1 such that the ghost sample z\u2032(k)n has the same distribution as z(k)n for any 1 \u2264 k \u2264 K and any 1 \u2264 n \u2264 Nk. Denote Z2Nk1 := {ZNk1 ,Z\u2032Nk1 } for any 1 \u2264 k \u2264 K. Moreover, given an f \u2208 F and a w = (w1, \u00b7 \u00b7 \u00b7 , wK) \u2208 [0, 1]K with \u2211K k=1wk = 1, we introduce a variant of the \u21131 norm:\n\u2016f\u2016 \u2113w1 ({Z 2Nk 1 } K k=1) :=\nK\u2211\nk=1\nwk Nk\nNk\u2211\nn=1\n|f(z(k)n )|. (19)\nIt is noteworthy that the variant \u2113w1 of the \u21131 norm is still a norm on the functional space, which can be directly verified by using the definition of norm, so we omit it here.\nIn the situation of domain adaptation with multiple sources, by setting the metric d as \u2113w1 ({Z2Nk1 }Kk=1), we then define the uniform entropy number of F with respect to the metric \u2113w1 ({Z2Nk1 }Kk=1) as\nlnNw1 ( F , \u03be, 2 K\u2211\nk=1\nNk ) := sup\n{Z 2Nk 1 } K k=1\nlnN ( F , \u03be, \u2113w1 ({Z2Nk1 }Kk=1) ) . (20)"}, {"heading": "4.1.2 Domain Adaptation Combining Source and Target Data", "text": "In the situation of domain adaptation combining source and target data, we have to introduce another variant of the \u21131 norm on F . Let ZNS1 = {z(S)n }NSn=1 and Z NT 1 = {z(T )n }NTn=1 be two sets of samples drawn from the domains Z(S) and Z(T ), respectively. Given an f \u2208 F , we define for any \u03c4 \u2208 [0, 1),\n\u2016f\u2016 \u2113\u03c41(Z NS 1 ,Z NT 1 )\n:= \u03c4\nNT\nNT\u2211\nn=1\n|f(z(T )n )|+ 1\u2212 \u03c4 NS\nNS\u2211\nn=1\n|f(z(S)n )|. (21)\nNote that the variant \u2113\u03c41 (\u03c4 \u2208 [0, 1)) of the norm \u21131 is still a norm on the functional space, which can be easily verified by using the definition of norm, so we omit it here.\nMoreover, let Z\u2032NS1 and Z \u2032NT 1 be the ghost sample sets of Z NS 1 and Z NT 1 , respectively. Denote\nZ2NS1 := {ZNS1 ,Z\u2032NS1 } and Z 2NT 1 := {Z NT 1 ,Z\n\u2032NT\n1 }, respectively. Then, the uniform entropy number of F with respect to the metric \u2113\u03c41(Z) is defined as\nlnN \u03c41 (F , \u03be, 2(NS +NT )) := sup Z lnN (F , \u03be, \u2113\u03c41(Z)) , (22)\nwhere Z := {Z2NS1 ,Z 2NT 1 }."}, {"heading": "4.2 Rademacher Complexity", "text": "The Rademacher complexity is one of the most frequently used complexity measures of function classes and we refer to Van der Vaart and Wellner (1996); Mendelson (2003) for details.\nDefinition 4.2 Let F be a function class and {zn}Nn=1 be a sample set drawn from Z. Denote {\u03c3n}Nn=1 be a set of random variables independently taking either value from {\u22121, 1} with equal probability. Rademacher complexity of F is defined as\nR(F) := E sup f\u2208F\n{ 1\nN\n\u2223\u2223 N\u2211\nn=1\n\u03c3nf(zn) \u2223\u2223 }\n(23)\nwith its empirical version\nRN(F) := E\u03c3 sup f\u2208F\n{ 1\nN\n\u2223\u2223 N\u2211\nn=1\n\u03c3nf(zn) \u2223\u2223 } , (24)\nwhere E stands for the expectation taken with respect to all random variables {zn}Nn=1 and {\u03c3n}Nn=1, and E\u03c3 stands for the expectation only taken with respect to the random variables {\u03c3n}Nn=1."}, {"heading": "5 Learning Processes of Domain Adaptation with Multi-", "text": "ple Sources\nIn this section, we present two generalization bounds of the learning process for domain adaptation with multiple sources. They are based on the uniform entropy number and the Rademacher complexity, respectively. By using the derived bounds based on the uniform entropy number, we then analyze the asymptotic convergence and the rate of convergence of the learning process. The numerical experiment supports our theoretical analysis as well."}, {"heading": "5.1 Generalization Bounds", "text": "Based on the uniform entropy number defined in (20), a generalization bound for domain adaptation with multiple sources is presented in the following theorem.\nTheorem 5.1 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let w = (w1, \u00b7 \u00b7 \u00b7 , wK) \u2208 [0, 1]K with \u2211K k=1wk = 1. Then, given an arbitrary \u03be > D (w) F (S, T ), we have for any (\u220fK k=1Nk ) \u2265 8(b\u2212a)2 (\u03be\u2032)2 and any \u01eb > 0, with probability at least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223E(S) w f \u2212 E(T )f \u2223\u2223 \u2264 D(w)F (S, T ) +   ( lnNw1 ( F , \u03be\u2032/8, 2\u2211Kk=1Nk ) \u2212 ln(\u01eb/8) ) (\u220fK k=1 Nk )\n32(b\u2212a)2 (\u2211K\nk=1 w 2 k ( \u220f i6=k Ni) )\n  1 2 , (25)\nwhere \u03be\u2032 = \u03be \u2212D(w)F (S, T ) and\nD (w) F (S, T ) :=\nK\u2211\nk=1\nwkDF(Sk, T ). (26)\nIn the above theorem, we show that the generalization bound supf\u2208F |E(T )f \u2212 E(S)w f | can be bounded by the right-hand side of (25). Compared to the classical result under the assumption of same distribution (see Mendelson, 2003, Theorem 2.3 and Definition 2.5): with probability at least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223ENf \u2212 Ef \u2223\u2223 \u2264 O\n  ( lnN1 ( F , \u03be, N ) \u2212 ln(\u01eb/8)\nN\n) 1 2   (27)\nwith ENf being the empirical risk with respect to the sample set Z N 1 , there is a discrepancy quantity D (w) F (S, T ) that is determined by the two factors: the choice of w and the integral probability metrics DF(Sk, T ) (1 \u2264 k \u2264 K). The two results will coincide if any source domain and the target domain match, i.e., DF(Sk, T ) = 0 holds for any 1 \u2264 k \u2264 K.\nIn order to prove this result, we develop the specific Hoeffding-type deviation inequality and the symmetrization inequality for domain adaptation with multiple sources, respectively. The detailed proof is arranged in Appendix A. Subsequently, we give another generalization bound based on the Rademacher complexity:\nTheorem 5.2 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let w = (w1, \u00b7 \u00b7 \u00b7 , wK) \u2208 [0, 1]K with \u2211K k=1wk = 1. Then, we have with probability at least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223E(S) w f \u2212 E(T )f \u2223\u2223 \u2264 D(w)F (S, T ) + 2 K\u2211\nk=1\nwkR(k)(F) +\n\u221a\u221a\u221a\u221a K\u2211\nk=1\n(b\u2212 a)2w2k ln(1/\u01eb) 2Nk , (28)\nwhere D (w) F (S, T ) is defined in (26) and R(k)(F) (1 \u2264 k \u2264 K) are the Rademacher complexities on the source domains Z(Sk), respectively.\nSimilarly, the derived bound (28) coincides with the related classical result under the assumption of same distribution (see Bousquet et al., 2004, Theorem 5), when any source domain of\n{Z(Sk)}Kk=1 and the target domain Z(T ) match, i.e., D(w)F (S, T ) = DF(Sk, T ) = 0 holds for any 1 \u2264 k \u2264 K. The proof of this theorem is processed by introducing a generalized version of McDiarmid\u2019s inequality which allows independent random variables to take values from different domains (see Appendix C).\nSubsequently, based on the derived bound (25), we can analyze the asymptotic behavior of the learning process for domain adaptation with multiple sources."}, {"heading": "5.2 Asymptotic Convergence", "text": "In statistical learning theory, it is well-known that the complexity of function class is one of main factors to the asymptotic convergence of the learning process under the assumption of same distribution (Vapnik, 1999; Van der Vaart and Wellner, 1996; Mendelson, 2003).\nFrom Theorem 5.1, we can directly arrive at the following result showing that the asymptotic convergence of the learning process for domain adaptation with multiple sources is affected by the three aspects: the choice of w, the discrepancy quantity D\n(w) F (S, T ) and the uniform entropy\nnumber lnNw1 ( F , \u03be/8, 2\u2211Kk=1Nk ) . Theorem 5.3 Assume that F is a function class consisting of the bounded functions with the range [a, b]. Let w = (w1, \u00b7 \u00b7 \u00b7 , wK) \u2208 [0, 1]K with \u2211K k=1wk = 1. If the following condition holds:\nlim N1,\u00b7\u00b7\u00b7 ,NK\u2192+\u221e\nlnNw1 ( F , \u03be/8, 2\u2211Kk=1Nk ) (\u220fK\nk=1 Nk\n)\n32(b\u2212a)2 (\u2211K\nk=1 w 2 k ( \u220f i6=k Ni) )\n< +\u221e, (29)\nthen we have for any \u03be > D (w) F (S, T ),\nlim N1,\u00b7\u00b7\u00b7 ,NK\u2192+\u221e Pr { sup f\u2208F \u2223\u2223E(T )f \u2212 E(S) w f \u2223\u2223 > \u03be } = 0. (30)\nAs shown in Theorem 5.3, if the choice of w \u2208 [0, 1]K and the uniform entropy number lnNw1 ( F , \u03be\u2032/8, 2\u2211Kk=1Nk ) satisfy the condition (29) with \u2211K k=1wk = 1, the probability of the\nevent that supf\u2208F \u2223\u2223E(T )f \u2212 E(S)w f \u2223\u2223 > \u03be will converge to zero for any \u03be > D(w)F (S, T ), when the sample numbers N1, \u00b7 \u00b7 \u00b7 , NK of multiple sources go to infinity, respectively. This is partially in accordance with the classical result of the asymptotic convergence of the learning process under the assumption of same distribution (cf. Theorem 2.3 and Definition 2.5 of [22]): the probability of the event that supf\u2208F \u2223\u2223Ef \u2212 ENf \u2223\u2223 > \u03be will converge to zero for any \u03be > 0, if the uniform entropy number lnN1 (F , \u03be, N) satisfies the following:\nlim N\u2192+\u221e lnN1 (F , \u03be, N) N < +\u221e. (31)\nNote that in the learning process of domain adaptation with multiple sources, the uniform convergence of the empirical risk on the source domains to the expected risk on the target domain may not hold, because the limit (30) does not hold for any \u03be > 0 but for any \u03be > D\n(w) F (S, T ).\nBy contrast, the limit (30) holds for all \u03be > 0 in the learning process under the assumption of same distribution, if the condition (31) is satisfied. Again, these two results coincide when any source domain and the target domain match, i.e., D (w) F (S, T ) = DF(Sk, T ) = 0 holds for any 1 \u2264 k \u2264 K. Next, we study the rate of convergence of the learning process for domain adaptation with multiple sources."}, {"heading": "5.3 Rate of Convergence", "text": "Recalling (25), we can find that the rate of convergence is affected by the choice of w. According to the Cauchy-Schwarz inequality, setting wk = Nk/ \u2211K k=1Nk (1 \u2264 k \u2264 K), we have\nmax\n{ (\u220fK k=1Nk )\n32(b\u2212 a)2 (\u2211K k=1w 2 k( \u220f i 6=k Ni) ) } = N1 +N2 + \u00b7 \u00b7 \u00b7+NK 32(b\u2212 a)2 , (32)\nwhich minimizes the second term of the right-hand side of (25). Thus, by (25), (27) and (32), we find that the fastest rate of convergence of the learning process is up to O(1/ \u221a N) which is the same as the classical result (27) of the learning process under the assumption of same distribution if the discrepancy D (w) F (S, T ) is ignored.\nIn addition, the bound (28) based on the Rademacher complexity also implies that the rate of convergence of the learning process is affected by the choice of w. Again, according to CauchySchwarz inequality, setting wk =\nNk\u2211K k=1 Nk (1 \u2264 k \u2264 K) leads to the fastest rate of convergence: \u221a\n(b\u2212 a)2 ln(1/\u01eb) 2 \u2211K k=1Nk = O(1/ \u221a N),\nwhich is in accordance with the aforementioned analysis. The following numerical experiments support our theoretical findings (see Fig. 1)."}, {"heading": "5.4 Numerical Experiment", "text": "We have performed the numerical experiments to verify the theoretic analysis of the asymptotic convergence of the learning processes for domain adaptation with multiple sources. Without loss of generality, we only consider the case of K = 2, i.e., there are two source domains and one target domain. The experiment data are generated in the following way:\nFor the target domain Z(T ) = X (T ) \u00d7 Y (T ) \u2282 R100 \u00d7 R, we consider X (T ) as a Gaussian distribution N(0, 1) and draw {x(T )n }NTn=1 (NT = 4000) from X (T ) randomly and independently. Let \u03b2 \u2208 R100 be a random vector of a Gaussian distribution N(1, 5), and let the random vector R \u2208 R100 be a noise term with R \u223c N(0, 0.5). For any 1 \u2264 n \u2264 NT , we randomly draw \u03b2 and R from N(1, 5) and N(0, 0.01) respectively, and then generate y (T ) n \u2208 Y as follows:\ny(T )n = \u3008x(T )n , \u03b2\u3009+R. (33)\nThe derived {(x(T )n , y(T )n )}NTn=1 (NT = 4000) are the samples of the target domain Z(T ) and will be used as the test data.\nIn the similar way, we generate the sample set {(x(1)n , y(1)n )}N1n=1 (N1 = 2000) of the source domain Z(S1) = X (1) \u00d7Y (1) \u2282 R100 \u00d7 R: for any 1 \u2264 n \u2264 N1,\ny(1)n = \u3008x(1)n , \u03b2\u3009+R, (34)\nwhere x (1) n \u223c N(0.5, 1), \u03b2 \u223c N(1, 5) and R \u223c N(0, 0.5).\nFor the source domain Z(S2) = X (2) \u00d7 Y (2) \u2282 R100 \u00d7 R, the samples {(x(2)n , y(2)n )}N2n=1 (N2 = 2000) are derived in the following way: for any 1 \u2264 n \u2264 N2,\ny(2)n = \u3008x(2)n , \u03b2\u3009+R, (35)\nwhere x (2) n \u223c N(2, 5), \u03b2 \u223c N(1, 5) and R \u223c N(0, 0.5).\nIn this experiment, we use the method of Least Square Regression1 to minimize the empirical risk\nE(S)w (\u2113 \u25e6 g) = w\nN1\nN1\u2211\nn=1\n\u2113(g(x(1)n ), y (1) n ) + (1\u2212 w) N2\nN2\u2211\nn=1\n\u2113(g(x(2)n ), y (2) n ) (36)\nfor different combination coefficients w \u2208 {0.1, 0.3, 0.5, 0.9} and then compute the discrepancy |E(S)w f \u2212E(T )NT f | for each N1+N2. The initial N1 and N2 both equal to 200. Each test is repeated 30 times and the final result is the average of the 30 results. After each test, we increment N1 and N2 by 200 until N1 = N2 = 2000. The experiment results are shown in Fig. 1.\nFrom Fig. 1, we can find that for any choice of w, the curve of |E(S)w f \u2212E(T )NT f | is decreasing when N1+N2 increases, which is in accordance with the results presented in Theorems 5.1 & 5.3. Moreover, when w = 0.5, the discrepancy |E(S)w f \u2212 E(T )NT f | has the fastest rate of convergence, and the rate becomes slower as w is further away from 0.5. In this experiment, we set N1 = N2 that implies that N2/(N1 + N2) = 0.5. Recalling (25), we have shown that w = N2/(N1 + N2) will provide the fastest rate of convergence and this proposition is supported by the experiment results shown in Fig. 1."}, {"heading": "6 Learning Process of Domain Adaptation Combining", "text": "Source and Target Data\nIn this section, we present two generalization bounds of the learning process for domain adaptation combining source and target data, which are based on the uniform entropy number and\n1SLEP Package: http://www.public.asu.edu/\u223cjye02/Software/SLEP/index.htm\nthe Rademacher complexity, respectively. We then analyze the asymptotic convergence and the rate of convergence of the learning process in addition to the numerical experiments supporting our theoretical analysis."}, {"heading": "6.1 Generalization Bounds", "text": "The following theorem provides a generalization bound based on the uniform entropy number with respect to the metric \u2113\u03c41 defined in (22). Similar to the situation of domain adaptation with multiple sources, the proof of this theorem is achieved by using a specific Hoeffding-type deviation inequality and a symmetrization inequality for domain adaptation combining source and target data (see Appendix B).\nTheorem 6.1 Assume that F is a function class consisting of the bounded functions with the range [a, b]. Let ZNS1 = {z(S)n }NSn=1 and Z NT 1 = {z(T )n }NTn=1 be two sets of i.i.d. samples drawn from domains Z(S) and Z(T ), respectively. Then, for any \u03c4 \u2208 [0, 1) and given an arbitrary \u03be > (1\u2212 \u03c4)DF (S, T ), we have for any NSNT \u2265 8(b\u2212a) 2 (\u03be\u2032)2 , with probability at least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223E\u03c4f \u2212 E(T )f \u2223\u2223 \u2264 (1\u2212 \u03c4)DF(S, T ) +\n( lnN \u03c41 (F , \u03be\u2032/8, 2(NS +NT ))\u2212 ln(\u01eb/8)\nNSNT 32(b\u2212a)2((1\u2212\u03c4)2NT+\u03c42NS)\n) 1 2\n, (37)\nwhere DF(S, T ) is defined in (12) and \u03be \u2032 := \u03be \u2212 (1\u2212 \u03c4)DF(S, T ).\nCompared to the classical result (27) under the assumption of same distribution, the derived bound (37) contains a term of discrepancy quantity (1 \u2212 \u03c4)DF (S, T ) that is determined by two factors: the combination coefficient \u03c4 and the quantity DF(S, T ). The two results coincide when the source domain Z(S) and the target domain Z(T ) match, i.e., DF (S, T ) = 0.\nBased on the Rademacher complexity, we then get another generalization bound of the learning process for domain adaptation combining source and target data. Its proof is postponed in Appendix C.\nTheorem 6.2 Assume that F is a function class consisting of the bounded functions with the range [a, b]. Let ZNS1 = {z(S)n }NSn=1 and Z NT 1 = {z(T )n }NTn=1 be two sets of i.i.d. samples drawn from the domains Z(S) and Z(T ), respectively. Then, given \u03c4 \u2208 [0, 1) and for any \u01eb > 0, we have with probability at least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223E\u03c4f \u2212 E(T )f \u2223\u2223 \u2264(1\u2212 \u03c4)DF (S, T ) + 2(1\u2212 \u03c4)R(S)(F)\n+ 2\u03c4R(T )NT (F) + 3\u03c4 \u221a (b\u2212 a) ln(4/\u01eb) 2NT + (1\u2212 \u03c4) \u221a\n(b\u2212 a)2 ln(2/\u01eb) 2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n) , (38)\nwhere DF(S, T ) is defined in (12).\nNote that in the derived bound (38), we adopt an empirical Rademacher complexity R(T )NT (F) that is based on the data drawn from the target domain Z(T ), because the distribution of Z(T ) is unknown in the situation of domain adaptation. Similar to the aforementioned discussion, the generalization bound (38) coincides with the result under the assumption of same distribution (see Bousquet et al., 2004, Theorem 5), when the source domain of Z(S) and the target domain Z(T ) match, i.e., DF(S, T ) = 0.\nThe two results (37) and (38) exhibit a tradeoff between the sample numbers NS and NT , which is associated with the choice of \u03c4 . Although the tradeoff has been mentioned in some previous works (see Blitzer et al., 2008; Ben-David et al., 2010), the following will show a rigorous theoretical analysis of the tradeoff."}, {"heading": "6.2 Asymptotic Convergence", "text": "Following Theorem 6.1, we can directly obtain the concerning result pointing out that the asymptotic convergence of the learning process for domain adaptation combining source and target data is affected by three factors: the uniform entropy number lnN \u03c41 (F , \u03be/8, 2(NS +NT )), the integral probability metric DF(S, T ) and the choice of \u03c4 \u2208 [0, 1).\nTheorem 6.3 Assume that F is a function class consisting of bounded functions with the range [a, b]. Given a \u03c4 \u2208 [0, 1), if the following condition holds:\nlim NS\u2192+\u221e\nlnN \u03c41 (F , \u03be\u2032/8, 2(NS +NT )) NSNT\n((1\u2212\u03c4)2NT+\u03c42NS)\n< +\u221e (39)\nwith \u03be\u2032 := \u03be \u2212 (1\u2212 \u03c4)DF (S, T ), then we have for any \u03be > (1\u2212 \u03c4)DF (S, T ),\nlim NS\u2192+\u221e Pr { sup f\u2208F \u2223\u2223E(T )f \u2212 E\u03c4f \u2223\u2223 > \u03be } = 0. (40)\nAs shown in Theorem 6.3, if the choice of \u03c4 \u2208 [0, 1) and the uniform entropy number lnN \u03c41 (F , \u03be\u2032/8, 2(NS+NT )) satisfy the condition (39), the probability of the event supf\u2208F \u2223\u2223E(T )f\u2212 E\u03c4f\n\u2223\u2223 > \u03be will converge to zero for any \u03be > (1 \u2212 \u03c4)DF (S, T ), when NS goes to infinity. This is partially in accordance with the classical result under the assumption of same distributions derived from the combination of Theorem 2.3 and Definition 2.5 of Mendelson (2003).\nNote that in the learning process for domain adaptation combining source and target data, the uniform convergence of the empirical risk E\u03c4f to the expected risk E\n(T )f may not hold, because the limit (40) does not hold for any \u03be > 0 but for any \u03be > (1 \u2212 \u03c4)DF (S, T ). By contrast, the limit (40) holds for all \u03be > 0 in the learning process under the assumption of same distribution, if the condition (31) is satisfied. The two results coincide when the source domain Z(S) and the target domain Z(T ) match, i.e., DF(S, T ) = 0."}, {"heading": "6.3 Rate of Convergence", "text": "We consider the choice of \u03c4 that is an essential factor to the rate of convergence for the learning process and is associated with the tradeoff between the sample numbers NS and NT . Recalling\n(37), if we fix the value of lnN \u03c41 (F , \u03be\u2032/8, 2(NS +NT )), setting \u03c4 = NTNT+NS minimizes the second term of the right-hand side of (37) and then we arrive at\nsup f\u2208F\n\u2223\u2223E\u03c4f \u2212 E(T )f \u2223\u2223 \u2264 NSDF(S, T )\nNS +NT +\n( (lnN \u03c41 (F , \u03be\u2032/8, 2(NS +NT ))\u2212 ln(\u01eb/8))\nNS+NT 32(b\u2212a)2\n) 1 2\n, (41)\nwhich implies that setting \u03c4 = NT NT+NS\ncan result in the fastest rate of convergence, while it can also cause the relatively larger discrepancy between the empirical risk E\u03c4f and the expected risk E(T )f , because the situation of domain adaptation is set up in the condition that NT \u226a NS, which implies that NS\nNS+NT \u2248 1. Moreover, this choice of \u03c4 associated with a trade off between\nsample numbers NS and NT is also suitable to the Rademacher-complexity-based bound (38). It is noteworthy that the value \u03c4 = NT\nNT+NS has been mentioned in the section of \u201cExperimental\nResults\u201d in Blitzer et al. (2008). Here, we show a rigorous theoretical analysis of this value and the following numerical experiment also supports this finding (see Fig. 2)."}, {"heading": "6.4 Numerical Experiments", "text": "In the situation of domain adaptation combining source and target data, the samples {(x(T )n , y(T )n )}NTn=1 (NT = 4000) of the target domain Z(T ) are generated in the aforementioned way (see (33)). We randomly pick N \u2032T = 100 samples from them to form the objective function and the rest N \u2032\u2032T = 3900 are used to test.\nIn the similar way, the samples {(x(S)n , y(S)n )}NSn=1 (NS = 4000) of the source domain Z(S) are generated as follows: for any 1 \u2264 n \u2264 NS,\ny(S)n = \u3008x(S)n , \u03b2\u3009+R, (42)\nwhere x (S) n \u223c N(1, 2), \u03b2 \u223c N(1, 5) and R \u223c N(0, 0.5).\nWe also use the method of Least Square Regression to minimize the empirical risk\nE\u03c4 (\u2113 \u25e6 g) = 1\u2212 \u03c4 NS\nNS\u2211\nn=1\n\u2113(g(x(1)n ), y (1) n ) +\n\u03c4\nN \u2032T\nN \u2032 T\u2211\nn=1\n\u2113(g(x(T )n ), y (T ) n )\nfor different combination coefficients \u03c4 \u2208 {0.1, 0.3, 0.5, 0.9} and then compute the discrepancy |E\u03c4f \u2212E(T )N \u2032\u2032 T f | for each NS. Since it has to be satisfied that NS \u226b N \u2032T , the initial NS is set to be 200. Each test is repeated 100 times and the final result is the average of the 100 results. After each test, we increment NS by 200 until NS = 4000. The experiment results are shown in Fig. 2.\nFigure (2) illustrates that for any choice of \u03c4 \u2208 {0.1, 0.3, 0.5, 0.9}, the curve of |E\u03c4f \u2212E(T )N \u2032\u2032 T | is decreasing as NS increases. This is in accordance with our results of the asymptotic convergence of the learning process for domain adaptation with multiple sources (see Theorems 6.1 and 6.3). Furthermore, Fig. 2 also shows that when \u03c4 \u2248 N \u2032T /(NS +N \u2032T ), the discrepancy |E(S)\u03c4 f \u2212 E(T )N \u2032\u2032 T f | has the fastest rate of convergence, and the rate becomes slower as \u03c4 is further away from N \u2032T/(NS + N \u2032 T ). Thus, this is in accordance with the theoretical analysis of the asymptotic convergence presented above."}, {"heading": "7 Prior Works", "text": "There have been some previous works on the theoretical analysis of domain adaptation with multiple sources (see Ben-David et al., 2010; Crammer et al., 2006, 2008; Mansour et al., 2008, 2009a) and domain adaptation combining source and target data (see Blitzer et al., 2008; Ben-David et al., 2010).\nIn Crammer et al. (2006, 2008), the function class and the loss function are assumed to satisfy the conditions of \u201c\u03b1-triangle inequality\u201d and \u201cuniform convergence bound\u201d. Moreover, one has to get some prior information about the disparity between any source domain and the target domain. Under these conditions, some generalization bounds were obtained by using the classical techniques developed under the assumption of same distribution.\nMansour et al. (2008) proposed another framework to study the problem of domain adaptation with multiple sources. In this framework, one has to know some prior knowledge including the exact distributions of the source domains and the hypothesis function with a small loss on each source domain. Furthermore, the target domain and the hypothesis function on the target domain were deemed as the mixture of the source domains and the mixture of the hypothesis functions on the source domains, respectively. Then, by introducing the Re\u0301nyi divergence, Mansour et al. (2009a) extended their previous work (Mansour et al., 2008) to a more general setting, where the distribution of the target domain can be arbitrary and one only needs to know an approximation of the exact distribution of each source domain. Ben-David et al. (2010) also discussed the situation of domain adaptation with the mixture of source domains.\nIn Ben-David et al. (2010); Blitzer et al. (2008), domain adaptation combining source and target data was originally proposed and meanwhile a theoretical framework was presented to analyze its properties for the classification tasks by introducing the H-divergence. Under the condition of \u201c\u03bb-close\u201d, the authors applied the classical techniques developed under the assumption of same distribution to achieve the generalization bounds based on the VC dimension.\nMansour et al. (2009b) introduced the discrepancy distance disc\u2113(D(S),D(T )) to capture the difference between domains and this quantity can be used in both classification and regression tasks. By applying the classical results of statistical learning theory, the authors obtained the generalization bounds based on the Rademacher complexity."}, {"heading": "8 Conclusion", "text": "In this paper, we propose a new framework to obtain generalization bounds of the learning process for two representative types of domain adaptation: domain adaptation with multiple sources and domain adaptation combining source and target data. This framework is suitable for a variant of learning tasks including classification and regression. Based on the derived bounds, we theoretically analyze the asymptotic convergence and the rate of convergence of the learning process for domain adaptation. There are four important aspects of this framework: the quantity measuring the difference between two domains; the complexity measure of function class, the deviation inequality and the symmetrization inequality for domain adaptation.\n\u2022 We use the integral probability metric DF(S, T ) to measure the difference between two domains Z(S) and Z(T ). We show that the integral probability metric is well-defined and is a (semi)metric on the space of the probability distributions. It can be bounded by the\nsummation of the discrepancy distance disc\u2113(D(S),D(T )) and the quantity Q(T )G (g (S) \u2217 , g (T ) \u2217 ), which measure the difference between the input-space distributions D(S) and D(T ) and the difference between labeling functions g (S) \u2217 and g (T ) \u2217 , respectively. Note that there is a special case that is more suitable to the integral probability metric DF(S, T ) than other quantities (see Remark 3.1).\n\u2022 The uniform entropy number and the Rademacher complexity are adopted to achieved the generalization bounds (25); (37) and (28); (38), respectively. It is noteworthy that the generalization bounds (25) and (37) can lead to the results based on the fat-shattering dimension, respectively (see Mendelson, 2003, Theorem 2.18). According to Theorem 2.6.4 of Van der Vaart and Wellner (1996), the bounds based on the VC dimension can also be obtained from the results (25) and (37), respectively.\n\u2022 Instead of directly applying the classical techniques, we present the specific deviation inequalities for the learning process of domain adaptation. In order to obtain the generalization bounds based on the uniform entropy numbers, we develop the specific Hoeffding-type deviation inequalities for the two types of domain adaptation, respectively (see Appendices A & B). Furthermore, we also generalize the classical McDiarmid\u2019s inequality to a more general setting where the independent random variables can take value from different domains (see Appendix C).\n\u2022 We also develop the related symmetrization inequalities of the learning process for domain adaptation. The derived inequalities incorporate the discrepancy term that is determined by the difference between the source and the target domains and reflects the learningtransfering from the source to the target domains.\nBased on the derived generalization bounds, we provide a rigorous theoretical analysis of the asymptotic convergence and the rate of convergence of the learning process for either kind of\ndomain adaptation. We also consider the choices of w and \u03c4 that affect the rate of convergence of the learning processes for the two types of domain adaptation, respectively. Moreover, we give a comparison with the previous works Ben-David et al. (2010); Crammer et al. (2006, 2008); Mansour et al. (2008, 2009a); Blitzer et al. (2008) as well as the related results of the learning process under the assumption of same distribution (see Bousquet et al., 2004; Mendelson, 2003). The numerical experiments support our theoretical findings as well.\nIn our future work, we will attempt to find a new distance between distributions to develop the generalization bounds based on other complexity measures, and analyze other theoretical properties of domain adaptation."}, {"heading": "A Proof of Theorem 5.1", "text": "In this appendix, we provide the proof of Theorem 5.1. In order to achieve the proof, we need to develop the specific Hoeffding-type deviation inequality and the symmetrization inequality for domain adaptation with multiple sources.\nA.1 Hoeffding-Type Deviation Inequality for Multiple Sources\nDeviation (or concentration) inequalities play an essential role in obtaining the generalization bounds for a certain learning process. Generally, specific deviation inequalities need to be developed for different learning processes. There are many popular deviation and concentration inequalities, e.g., Hoeffding\u2019s inequality, McDiarmid\u2019s inequality, Bennett\u2019s inequality, Bernstein\u2019s inequality and Talagrand\u2019s inequality. These results are all built under the assumption of same distribution, and thus they are not applicable (or at least cannot be directly applied) to the setting of multiple sources. Next, based on Hoeffding\u2019s inequality (Hoeffding, 1963), we present a deviation inequality for multiple sources.\nTheorem A.1 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let ZNk1 = {z(k)n }Nkn=1 be the set of i.i.d. samples drawn from the source domain Z(Sk) \u2282 RL (1 \u2264 k \u2264 K). Given w \u2208 [0, 1]K with \u2211Kk=1wk = 1 and for any f \u2208 F , we define a function Fw : R L \u2211K k=1 Nk \u2192 R as\nFw ( {XNk1 }Kk=1 ) := K\u2211\nk=1\nwk\n(\u220f\ni 6=k\nNi\n) Nk\u2211\nn=1\nf(x(k)n ), (43)\nwhere for any 1 \u2264 k \u2264 K and given Nk \u2208 N, the set XNk1 is denoted as\nXNk1 := {x(k)1 ,x(k)2 , \u00b7 \u00b7 \u00b7 ,x(k)Nk} \u2208 (R L)Nk .\nThen, we have for any \u03be > 0,\nPr {\u2223\u2223E(S)Fw \u2212 Fw ( {ZNk1 }Kk=1 )\u2223\u2223 > \u03be }\n\u22642 exp   \u2212\n2\u03be2\n(b\u2212 a)2 (\u220fK k=1Nk )(\u2211K k=1w 2 k (\u220f i 6=k Ni ))\n   , (44)\nwhere E(S) stands for the expectation taken on all source domains {Z(Sk)}Kk=1.\nThis result is an extension of the classical Hoeffding-type deviation inequality under the assumption of same distribution (see Bousquet et al., 2004, Theorem 1). Compared to the classical result, the resultant deviation inequality (44) is suitable to the setting of multiple sources. These two inequalities coincide when there is only one source, i.e., K = 1\nThe proof of Theorem A.1 is processed by a martingale method. Before the formal proof, we introduce some essential notations.\nLet {ZNk1 }Kk=1 be sample sets drawn from multiple sources {Z(Sk)}Kk=1, respectively. Define a random variable\nS(k)n := E (S) { Fw({ZNk1 }Kk=1)|ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,Z Nk\u22121 1 ,Z n 1 } , 1 \u2264 k \u2264 K, 0 \u2264 n \u2264 Nk, (45)\nwhere Zn1 = {z(k)1 , z(k)2 , \u00b7 \u00b7 \u00b7 , z(k)n } \u2286 ZNk1 , and Z01 = \u2205.\nIt is clear that S (1) 0 = E (S)Fw and S (K) NK\n= Fw({ZNk1 }Kk=1), where E(S) stands for the expectation taken on all source domains {Z(Sk)}Kk=1.\nThen, according to (43) and (45), we have for any 1 \u2264 k \u2264 K and 1 \u2264 n \u2264 Nk,\nS(k)n \u2212 S(k)n\u22121 =E(S) { Fw({ZNk1 }Kk=1) \u2223\u2223ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,Z Nk\u22121 1 ,Z n 1 }\n\u2212 E(S) { Fw({ZNk1 }Kk=1) \u2223\u2223ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,ZNk\u221211 ,Zn\u221211 }\n=E(S)\n{ K\u2211\nk=1\nwk\n(\u220f\ni 6=k\nNi\n) Nk\u2211\nn=1\nf(z(k)n ) \u2223\u2223ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,Z Nk\u22121 1 ,Z n 1\n}\n\u2212 E(S) { K\u2211\nk=1\nwk\n(\u220f\ni 6=k\nNi\n) Nk\u2211\nn=1\nf(z(k)n ) \u2223\u2223ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,ZNk\u221211 ,Zn\u221211\n}\n= k\u22121\u2211\nl=1\nwl\n(\u220f\ni 6=l\nNi\n) Nl\u2211\nj=1\nf(z (l) j ) + wk\n(\u220f\ni 6=k\nNi\n) n\u2211\nj=1\nf(z (k) j )\n+ E(S)\n{ K\u2211\nl=k+1\nwl\n(\u220f\ni 6=l\nNi\n) Nl\u2211\nj=1\nf(z (l) j ) + wk\n(\u220f\ni 6=k\nNi\n) Nk\u2211\nj=n+1\nf(z (k) j )\n}\n\u2212 k\u22121\u2211\nl=1\nwl\n(\u220f\ni 6=l\nNi\n) Nl\u2211\nj=1\nf(z (l) j )\u2212 wk\n(\u220f\ni 6=k\nNi\n) n\u22121\u2211\nj=1\nf(z (k) j )\n\u2212 E(S) { K\u2211\nl=k+1\nwl\n(\u220f\ni 6=l\nNi\n) Nl\u2211\nj=1\nf(z (l) j ) + wk\n(\u220f\ni 6=k\nNi\n) Nk\u2211\nj=n\nf(z (k) j )\n}\n=wk\n(\u220f\ni 6=k\nNi ) ( f(z(k)n )\u2212 E(Sk)f ) . (46)\nTo prove Theorem A.1, we need the following inequality resulted from Hoeffding\u2019s lemma.\nLemma A.1 Let f be a function with the range [a, b]. Then, the following holds for any \u03b1 > 0:\nE { e\u03b1(f(z (S))\u2212E(S)f) } \u2264 e\u03b1 2(b\u2212a)2 8 . (47)\nProof. We consider (f(z(S))\u2212 E(S)f)\nas a random variable. Then, it is clear that\nE{f(z(S))\u2212 E(S)f} = 0.\nSince the value of E(S)f is a constant denoted as e, we have\na\u2212 e \u2264 f(z(S))\u2212 E(S)f \u2264 b\u2212 e.\nAccording to Hoeffding\u2019s lemma, we then have\nE { e\u03b1(f(z (S))\u2212E(S)f) } \u2264 e\u03b1 2(b\u2212a)2 8 . (48)\nThis completes the proof. We are now ready to prove Theorem A.1. Proof of Theorem A.1. According to (43), (46), Lemma A.1, Markov\u2019s inequality, Jensen\u2019s inequality and the law of iterated expectation, we have for any \u03b1 > 0,\nPr { Fw ( {ZNk1 }Kk=1 ) \u2212 E(S)Fw > \u03be }\n\u2264e\u2212\u03b1\u03beE { e \u03b1 ( Fw ( {Z Nk 1 } K k=1 ) \u2212E(S)Fw )}\n=e\u2212\u03b1\u03beE { E { e \u03b1 \u2211K k=1 \u2211Nk n=1 ( S (k) n \u2212S (k) n\u22121 )\u2223\u2223ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK\u221211 ,ZNK\u221211 }}\n=e\u2212\u03b1\u03beE { e \u03b1 (\u2211K k=1 \u2211Nk n=1 ( S (k) n \u2212S (k) n\u22121 ) \u2212 ( S (K) NK \u2212S (K) NK\u22121 )) E { e \u03b1 ( S (K) NK \u2212S (K) NK\u22121 )\u2223\u2223ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK\u221211 ,ZNK\u221211 }}\n=e\u2212\u03b1\u03beE { e \u03b1 (\u2211K k=1 \u2211Nk n=1 ( S (k) n \u2212S (k) n\u22121 ) \u2212 ( S (K) NK \u2212S (K) NK\u22121 )) E { e\u03b1wK( \u220f i6=K Ni)(f(z (K) N )\u2212E(SK )f) }} \u2264e\u2212\u03b1\u03beE { e \u03b1 (\u2211K k=1 \u2211Nk n=1 ( S (k) n \u2212S (k) n\u22121 ) \u2212 ( S (K) NK \u2212S (K) NK\u22121 ))} e \u03b12w2 K ( \u220f i6=K Ni) 2(b\u2212a)2 8 , (49)\nwhere ZNK\u221211 := {z(K)1 , \u00b7 \u00b7 \u00b7 , z(K)NK\u22121} \u2282 Z NK 1 . Therefore, we have\nPr { Fw ( {ZNk1 }Kk=1 ) \u2212 E(S)Fw > \u03be } \u2264 e\u03a6(\u03b1)\u2212\u03b1\u03be , (50)\nwhere\n\u03a6(\u03b1) = \u03b12(b\u2212 a)2\n(\u220fK k=1Nk )(\u2211K k=1w 2 k (\u220f i 6=k Ni ))\n8 . (51)\nSimilarly, we can obtain\nPr { E(S)Fw \u2212 Fw ( {ZNk1 }Kk=1 ) > \u03be } \u2264 e\u03a6(\u03b1)\u2212\u03b1\u03be . (52)\nNote that \u03a6(\u03b1)\u2212 \u03b1\u03be is a quadratic function with respect to \u03b1 > 0 and thus the minimum value \u201cmin\u03b1>0 {\u03a6(\u03b1)\u2212 \u03b1\u03be}\u201d is achieved when\n\u03b1 = 4\u03be\n(b\u2212 a)2 (\u220fK k=1Nk )(\u2211K k=1w 2 k (\u220f i 6=k Ni )) .\nBy combining (50), (51) and (52), we arrive at\nPr {\u2223\u2223E(S)Fw \u2212 Fw ( {ZNk1 }Kk=1 )\u2223\u2223 > \u03be }\n\u22642 exp   \u2212\n2\u03be2\n(b\u2212 a)2 (\u220fK k=1Nk )(\u2211K k=1w 2 k (\u220f i 6=k Ni ))\n   .\nThis completes the proof. In the following subsection, we present a symmetrization inequality for domain adaptation with multiple sources.\nA.2 Symmetrization Inequality\nSymmetrization inequalities are mainly used to replace the expected risk by an empirical risk computed on another sample set that is independent of the given sample set but has the same distribution. In this manner, the generalization bounds can be achieved by applying some kinds of complexity measures, e.g., the covering number and the VC dimension. However, the classical symmetrization results are built under the assumption of same distribution (see Bousquet et al., 2004). The symmetrization inequality for domain adaptation with multiple sources is presented in the following theorem:\nTheorem A.2 Assume that F is a function class with the range [a, b]. Let sample sets {ZNk1 }Kk=1 and {Z\u2032Nk1 }Kk=1 be drawn from the source domains {Z(Sk)}Kk=1. Then, given an arbitrary \u03be > D\n(w) F (S, T ) and w = (w1, \u00b7 \u00b7 \u00b7 , wK) \u2208 [0, 1]K with \u2211K k=1wk = 1, we have for any (\u220fK k=1Nk ) \u2265\n8(b\u2212a)2\n(\u03be\u2032)2 ,\nPr { sup f\u2208F \u2223\u2223E(T )f \u2212 E(S) w f \u2223\u2223 > \u03be } \u2264 2Pr { sup f\u2208F \u2223\u2223E\u2032(S) w f \u2212 E(S) w f \u2223\u2223 > \u03be \u2032 2 } , (53)\nwhere \u03be\u2032 = \u03be \u2212D(w)F (S, T ).\nThis theorem shows that given \u03be > D (w) F (S, T ), the probability of the event:\nsup f\u2208F\n\u2223\u2223E(T )f \u2212 E(S) w f \u2223\u2223 > \u03be\ncan be bounded by using the probability of the event:\nsup f\u2208F\n\u2223\u2223E\u2032(S) w f \u2212 E(S) w f \u2223\u2223 > \u03be \u2212D (w) F (S, T )\n2 (54)\nthat is only determined by the characteristics of the source domains {Z(Sk)}Kk=1 when \u220fK\nk=1Nk \u2265 8(b\u2212a)2\n(\u03be\u2032)2 with \u03be\u2032 = \u03be \u2212D(w)F (S, T ). Compared to the classical symmetrization result under the assumption of same distribution (see Bousquet et al., 2004), there is a discrepancy term D (w) F (S, T ) in the derived inequality. Especially, the two results coincide when any source domain and the target domain match, i.e., DF(Sk, T ) = 0 holds for any 1 \u2264 k \u2264 K. The following is the proof of Theorem A.2.\nProof of Theorem A.2. Let f\u0302 be the function achieving the supremum:\nsup f\u2208F\n\u2223\u2223E(T )f \u2212 E(S) w f \u2223\u2223\nwith respect to the sample set {ZNk1 }Kk=1. According to (6), (7), (12) and (26), we arrive at\n|E(T )f\u0302 \u2212 E(S) w f\u0302 | = |E(T )f\u0302 \u2212 E(S)f\u0302 + E(S)f\u0302 \u2212 E(S) w f\u0302 | \u2264 D(w)F (S, T ) + \u2223\u2223E(S)f\u0302 \u2212 E(S) w f\u0302 \u2223\u2223, (55)\nand thus,\nPr { |E(T )f\u0302 \u2212 E(S)\nw f\u0302 | > \u03be\n} \u2264 Pr { D\n(w) F (S, T ) + \u2223\u2223E(S)f\u0302 \u2212 E(S) w f\u0302 \u2223\u2223 > \u03be } , (56)\nwhere the expectation E(S)f\u0302 is defined as\nE (S) f\u0302 :=\nK\u2211\nk=1\nwkE (Sk)f\u0302 . (57)\nLet\n\u03be\u2032 := \u03be \u2212D(w)F (S, T ), (58) and denote \u2227 as the conjunction of two events. According to the triangle inequality, we have\n( |E(S)f\u0302 \u2212 E(S)\nw f\u0302 | \u2212 |E\u2032(S) w f\u0302 \u2212 E(S)f\u0302 |\n) \u2264 |E\u2032(S)\nw f\u0302 \u2212 E(S) w f\u0302 |,\nand thus for any \u03be\u2032 > 0, ( 1 |E (S) f\u0302\u2212E (S) w f\u0302 |>\u03be\u2032 )( 1 |E (S) f\u0302\u2212E\u2032 (S) w f\u0302 |< \u03be\u2032\n2\n)\n=1{ |E (S) f\u0302\u2212E (S) w f\u0302 |>\u03be\u2032 } \u2227 { |E (S) f\u0302\u2212E\u2032 (S) w f\u0302 |< \u03be \u2032\n2\n}\n\u22641 |E\u2032 (S) w f\u0302\u2212E (S) w f\u0302 |> \u03be\u2032\n2\n.\nThen, taking the expectation with respect to {Z\u2032Nk1 }Kk=1 gives ( 1 |E (S) f\u0302\u2212E (S) w f\u0302 |>\u03be\u2032 ) Pr\u2032 { |E(S)f\u0302 \u2212 E\u2032(S) w f\u0302 | < \u03be \u2032\n2\n}\n\u2264Pr\u2032 { |E\u2032(S)\nw f\u0302 \u2212 E(S) w f\u0302 | > \u03be\n\u2032\n2\n} . (59)\nBy Chebyshev\u2019s inequality, since {Z\u2032Nk1 }Kk=1 are the sets of i.i.d. samples drawn from the multiple sources {Z(Sk)}Kk=1 respectively, we have for any \u03be\u2032 > 0,\nPr\u2032 {\u2223\u2223E(S)f\u0302 \u2212 E\u2032(S) w f\u0302 \u2223\u2223 \u2265 \u03be \u2032\n2\n} \u2264Pr\u2032 { K\u2211\nk=1\nwk Nk\nNk\u2211\nn=1\n|E(Sk)f\u0302 \u2212 f\u0302(z\u2032(k)n )| \u2265 \u03be\u2032\n2\n}\n=Pr\u2032\n{ K\u2211\nk=1\nwk\n(\u220f\ni 6=k\nNi\n) Nk\u2211\nn=1\n|E(Sk)f\u0302 \u2212 f\u0302(z\u2032(k)n )| \u2265 \u03be\u2032 \u220fK\nk=1Nk 2\n}\n\u2264 4E\n{\u2211K k=1wk (\u220f i 6=k Ni )\u2211Nk n=1 \u2223\u2223E(Sk)f\u0302 \u2212 f\u0302(z\u2032(k)n ) \u2223\u22232 }\n(\u220fK k=1Nk )2 (\u03be\u2032)2\n= 4E\n{\u2211K k=1wk (\u220f i 6=k Ni ) Nk (b\u2212 a)2 }\n(\u220fK k=1Nk )2 (\u03be\u2032)2\n= 4 (\u220fK k=1Nk ) (b\u2212 a)2\n(\u220fK k=1Nk )2 (\u03be\u2032)2\n= 4 (b\u2212 a)2\n(\u03be\u2032)2 (\u220fK k=1Nk ) . (60)\nSubsequently, according to (59) and (60), we have for any \u03be\u2032 > 0,\nPr\u2032 { |E\u2032(S)\nw f\u0302 \u2212 E(S) w f\u0302 | > \u03be\n\u2032\n2\n} \u2265 ( 1 |E (S) f\u0302\u2212E (S) w f\u0302 |>\u03be\u2032 )( 1\u2212 4 (b\u2212 a) 2 (\u03be\u2032)2 (\u220fK k=1Nk ) ) . (61)\nBy combining (56), (58) and (61), taking the expectation with respect to {ZNk1 }Kk=1 and letting 4 (b\u2212 a)2\n(\u03be\u2032)2 (\u220fK k=1Nk ) \u2264 1 2\ncan lead to: for any \u03be > D (w) F (S, T ),\nPr { |E(T )f\u0302 \u2212 E(S)\nw f\u0302 | > \u03be\n} \u2264Pr { |E(S)f\u0302 \u2212 E(S)\nw f\u0302 | > \u03be\u2032\n}\n\u22642Pr { |E\u2032(S)\nw f\u0302 \u2212 E(S) w f\u0302 | > \u03be\n\u2032\n2\n} (62)\nwith \u03be\u2032 = \u03be \u2212D(w)F (S, T ). This completes the proof. By using the resultant deviation inequality and the symmetrization inequality, we can achieve the proof of Theorem 5.1.\nA.3 Proof of Theorem 5.1\nProof of Theorem 5.1. Consider \u01eb as an independent Rademacher random variables, i.e., an independent {\u22121, 1}-valued random variable with equal probability of taking either value. Given sample sets {Z2Nk1 }Kk=1, denote for any f \u2208 F and 1 \u2264 k \u2264 K,\n\u2212\u2192\u01eb (k) := (\u01eb(k)1 , \u00b7 \u00b7 \u00b7 , \u01eb(k)Nk ,\u2212\u01eb (k) 1 , \u00b7 \u00b7 \u00b7 ,\u2212\u01eb(k)Nk) \u2208 {\u22121, 1} 2Nk , (63)\nand for any f \u2208 F , \u2212\u2192 f (Z2Nk1 ) := ( f(z\u2032 (k) 1 ), \u00b7 \u00b7 \u00b7 , f(z\u2032(k)Nk), f(z (k) 1 ), \u00b7 \u00b7 \u00b7 , f(z(k)Nk) ) . (64)\nAccording to (6), (7) and Theorem A.2, given an arbitrary \u03be > D (w) F (S, T ), we have for any\n{Nk}Kk=1 \u2208 NK such that \u220fK k=1Nk \u2265 8(b\u2212a) 2 (\u03be\u2032)2 with \u03be\u2032 = \u03be \u2212D(w)F (S, T ),\nPr { sup f\u2208F \u2223\u2223E(T )f \u2212 E(S) w f \u2223\u2223 > \u03be }\n\u22642Pr { sup f\u2208F \u2223\u2223E\u2032(S) w f \u2212 E(S) w f \u2223\u2223 > \u03be \u2032 2 } (by Theorem A.2)\n=2Pr { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk Nk\nNk\u2211\nn=1\n( f(z\u2032\n(k) n )\u2212 f(z(k)n ) )\u2223\u2223\u2223 > \u03be \u2032\n2\n}\n=2Pr { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk Nk\nNk\u2211\nn=1\n\u01eb(k)n ( f(z\u2032 (k) n )\u2212 f(z(k)n ) )\u2223\u2223\u2223 > \u03be \u2032\n2\n}\n=2Pr { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192f (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n4\n} . (by (63) and (64)) (65)\nFix a realization of {Z2Nk1 }Kk=1 and let \u039b be a \u03be\u2032/8-radius cover of F with respect to the \u2113w1 ({Z2Nk1 }Kk=1) norm. Since F is composed of the bounded functions with the range [a, b], we assume that the same holds for any h \u2208 \u039b. If f0 is the function that achieves the following supremum\nsup f\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192f (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n4 ,\nthere must be an h0 \u2208 \u039b that satisfies K\u2211\nk=1\nwk 2Nk\n( |f0(z\u2032(k)n )\u2212 h0(z\u2032(k)n )|+ |f0(z(k)n )\u2212 h0(z(k)n )| ) < \u03be\u2032\n8 ,\nand meanwhile, \u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192h0(Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n8 .\nTherefore, for the realization of {Z2Nk1 }Kk=1, we arrive at\nPr { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192f (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n4\n}\n\u2264Pr { sup h\u2208\u039b \u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192h (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n8\n} . (66)\nMoreover, we denote the event\nA := { Pr { sup h\u2208\u039b \u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192h (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n8\n}} ,\nand let 1A be the characteristic function of the event A. By Fubini\u2019s Theorem, we have\nPr{A} = E { E\u2212\u2192\u01eb { 1A }\u2223\u2223 {Z2Nk1 }Kk=1 }\n= E { Pr { sup h\u2208\u039b \u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192h (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n8\n}\u2223\u2223\u2223 {Z2Nk1 }Kk=1 } . (67)\nFix a realization of {Z2Nk1 }Kk=1 again. According to (63), (64) and Theorem A.1, we have\nPr { sup h\u2208\u039b \u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192h (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n8\n}\n\u2264|\u039b|max h\u2208\u039b Pr\n{\u2223\u2223\u2223 K\u2211\nk=1\nwk 2Nk\n\u2329\u2212\u2192\u01eb (k),\u2212\u2192h (Z2Nk1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032\n8\n}\n=N ( F , \u03be\u2032/8, \u2113w1 ({Z2Nk1 }Kk=1) ) max h\u2208\u039b Pr {\u2223\u2223E\u2032(S) w h\u2212 E(S) w h \u2223\u2223 > \u03be \u2032 4 } \u2264N ( F , \u03be\u2032/8, \u2113w1 ({Z2Nk1 }Kk=1) ) max h\u2208\u039b Pr { |E(S)h\u2212 E\u2032(S) w h|+ |E(S)h\u2212 E(S) w h| > \u03be \u2032 4 } \u22642N ( F , \u03be\u2032/8, \u2113w1 ({Z2Nk1 }Kk=1) ) max h\u2208\u039b Pr {\u2223\u2223E(S)h\u2212 E(S) w h \u2223\u2223 > \u03be \u2032 8 }\n\u22644N ( F , \u03be\u2032/8, \u2113w1 ({Z2Nk1 }Kk=1) ) exp    \u2212 (\u220fK k=1Nk ) ( \u03be \u2212D(w)F (S, T ) )2 32(b\u2212 a)2 (\u2211K k=1w 2 k( \u220f i 6=k Ni) )    , (68)\nwhere the expectation E (S) is defined in (57).\nThe combination of (65), (66) and (68) leads to the result: given an arbitrary \u03be > D (w) F (S, T )\nand for any \u220fK\nk=1Nk \u2265 8(b\u2212a) 2 (\u03be\u2032)2 with \u03be\u2032 = \u03be \u2212D(w)F (S, T ),\nPr { sup f\u2208F \u2223\u2223E(T )f \u2212 E(S) w f \u2223\u2223 > \u03be }\n\u22648EN ( F , \u03be\u2032/8, \u2113w1 ({Z2Nk1 }Kk=1) ) exp    \u2212 (\u220fK k=1Nk ) ( \u03be \u2212D(w)F (S, T ) )2 32(b\u2212 a)2 (\u2211K k=1w 2 k( \u220f i 6=k Ni) )   \n\u22648Nw1\n( F , \u03be\u2032/8, 2 K\u2211\nk=1\nNk\n) exp    \u2212 (\u220fK k=1Nk ) ( \u03be \u2212D(w)F (S, T ) )2 32(b\u2212 a)2 (\u2211K k=1w 2 k( \u220f i 6=k Ni) )    . (69)\nAccording to (69), letting\n\u01eb := 8Nw1\n( F , \u03be\u2032/8, 2 K\u2211\nk=1\nNk\n) exp    \u2212 (\u220fK k=1Nk ) ( \u03be \u2212D(w)F (S, T ) )2 32(b\u2212 a)2 (\u2211K k=1w 2 k( \u220f i 6=k Ni) )    ,\nwe then arrive at with probability at least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223E(S) w f \u2212 E(T )f \u2223\u2223 \u2264 D(w)F (S, T ) +   lnNw1 ( F , \u03be\u2032/8, 2\u2211Kk=1Nk ) \u2212 ln(\u01eb/8)(\u220fK\nk=1 Nk\n)\n32(b\u2212a)2 (\u2211K\nk=1 w 2 k ( \u220f i6=k Ni) )\n  1 2 ,\nwhere \u03be\u2032 = \u03be \u2212D(w)F (S, T ). This completes the proof."}, {"heading": "B Proof of Theorem 6.1", "text": "Here, we provide the proof of Theorem 6.1. Similar to the situation of domain adaptation with multiple sources, we need to develop the related Hoeffding-type deviation inequality and the symmetrization inequality for domain adaptation combining source and target data.\nB.1 Hoeffding-Type Deviation Inequality\nBased on Hoeffding\u2019s inequality (Hoeffding, 1963), we derive a deviation inequality for the combination of the source and the target domains.\nTheorem B.1 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let ZNS1 := {z(S)n }NSn=1 and Z NT 1 := {z(T )n }NTn=1 be sets of i.i.d. samples drawn from the source domain Z(S) \u2282 RL and the target domain Z(T ) \u2282 RL, respectively. For any \u03c4 \u2208 [0, 1), define a function F\u03c4 : R L(NS+NT ) \u2192 R as\nF\u03c4 ( XNT1 ,Y NS 1 ) := \u03c4NS NT\u2211\nn=1\nf(xn) + (1\u2212 \u03c4)NT NS\u2211\nn=1\nf(yn), (70)\nwhere\nXNT1 := {x1, \u00b7 \u00b7 \u00b7 ,xNT } \u2208 (RL)NT ; YNS1 := {y1, \u00b7 \u00b7 \u00b7 ,yNS} \u2208 (RL)NS .\nThen, we have for any \u03c4 \u2208 [0, 1) and any \u03be > 0,\nPr {\u2223\u2223F\u03c4 ( ZNS1 ,Z NT 1 ) \u2212 E(\u2217)F\u03c4 \u2223\u2223 > \u03be }\n\u22642 exp { \u2212 2\u03be 2\n(b\u2212 a)2NSNT ((1\u2212 \u03c4)2NT + \u03c4 2NS)\n} , (71)\nwhere the expectation E(\u2217) is taken on both of the source domain Z(S) and the target domain Z(T ).\nIn this theorem, we present a deviation inequality for the combination of source and target domains, which is an extension of the classical Hoeffding-type deviation inequality under the assumption of same distribution (see Bousquet et al., 2004, Theorem 1). Compare to the classical result, the resultant deviation inequality (71) allows the random variables to take values from different domains. The two inequalities coincide when the source domain Z(S) and the target domain Z(T ) match, i.e., DF(S, T ) = 0.\nThe proof of Theorem B.1 is also processed by a martingale method. Before the formal proof, we introduce some essential notations.\nFor any \u03c4 \u2208 [0, 1), we denote\nFS(Z NS 1 ) := (1\u2212 \u03c4)NT\nNS\u2211\nn=1\nf(z(S)n ); FT (Z NT 1 ) := \u03c4NS\nNT\u2211\nn=1\nf(z(T )n ). (72)\nRecalling (70), it is evident that F\u03c4 ( ZNS1 ,Z NT 1 ) = FS(Z NS 1 ) + FT (Z NT 1 ). We then define two random variables:\nSn :=E (S) { FS(Z NS 1 )|Zn1 } , 0 \u2264 n \u2264 NS;\nTn :=E (T ) { FT (Z NT 1 )|Z n 1 } , 0 \u2264 n \u2264 NT , (73)\nwhere\nZn1 = {z(S)1 , \u00b7 \u00b7 \u00b7 , z(S)n } \u2286 ZNS1 with Z01 := \u2205; Z n\n1 = {z(T )1 , \u00b7 \u00b7 \u00b7 , z(T )n } \u2286 Z NT 1 with Z 0 1 := \u2205.\nIt is clear that S0 = E (S)FS; SNS = FS(Z NS 1 ) and T0 = E (T )FT ; TNT = FT (Z NT 1 ).\nAccording to (70) and (73), we have for any 1 \u2264 n \u2264 NS and any \u03c4 \u2208 [0, 1),\nSn \u2212 Sn\u22121 =E(S) { FS(Z NS 1 )|Zn1 } \u2212 E(S) { FS(Z NS 1 )|Zn\u221211 }\n=E(S) { (1\u2212 \u03c4)NT NS\u2211\nn=1\nf(z(S)n ) \u2223\u2223\u2223Zn1 } \u2212 E(S) { (1\u2212 \u03c4)NT NS\u2211\nn=1\nf(z(S)n ) \u2223\u2223\u2223Zn\u221211\n}\n=(1\u2212 \u03c4)NT n\u2211\nm=1\nf(z(S)m ) + E (S) { (1\u2212 \u03c4)NT NS\u2211\nm=n+1\nf(z(S)m )\n}\n\u2212 ( (1\u2212 \u03c4)NT n\u22121\u2211\nm=1\nf(z(S)m ) + E (S) { (1\u2212 \u03c4)NT NS\u2211\nm=n\nf(z(S)m )\n})\n=(1\u2212 \u03c4)NT ( f(z(S)n )\u2212 E(S)f ) . (74)\nSimilarly, we also have for any 1 \u2264 n \u2264 NT ,\nTn \u2212 Tn\u22121 = \u03c4NS ( f(z(T )n )\u2212 E(T )f ) . (75)\nWe are now ready to prove Theorem B.1. Proof of Theorem B.1. According to (70) and (72), we have\nF\u03c4 (Z N 1 )\u2212 E(\u2217)F\u03c4 =FS(ZNS1 ) + FT (Z NT 1 )\u2212 E(\u2217){FS + FT}\n=FS(Z NS 1 )\u2212 E(S)FS + FT (Z NT 1 )\u2212 E(T )FT . (76)\nAccording to Lemma A.1, (74), (75), (76), Markov\u2019s inequality, Jensen\u2019s inequality and the law of iterated expectation, we have for any \u03b1 > 0 and any \u03c4 \u2208 [0, 1),\nPr { F\u03c4 (Z N 1 )\u2212 E(\u2217)F\u03c4 > \u03be }\n=Pr { FS(Z NS 1 )\u2212 E(S)FS + FT (Z NT 1 )\u2212 E(T )FT > \u03be } \u2264e\u2212\u03b1\u03beE { e \u03b1 ( FS(Z NS 1 )\u2212E (S)FS+FT (Z NT 1 )\u2212E (T )FT )}\n=e\u2212\u03b1\u03beE { e\u03b1 (\u2211NS n=1(Sn\u2212Sn\u22121)+ \u2211NT n=1(Tn\u2212Tn\u22121) )}\n=e\u2212\u03b1\u03beE { E { e\u03b1 (\u2211NS n=1(Sn\u2212Sn\u22121)+ \u2211NT n=1(Tn\u2212Tn\u22121) )\u2223\u2223\u2223ZNS\u221211 }}\n=e\u2212\u03b1\u03beE { e\u03b1 (\u2211NS\u22121 n=1 (Sn\u2212Sn\u22121)+ \u2211NT n=1(Tn\u2212Tn\u22121) ) E { e\u03b1(SNS\u2212SNS\u22121) \u2223\u2223\u2223ZNS\u221211 }} \u2264e\u2212\u03b1\u03beE { e\u03b1 (\u2211NS\u22121 n=1 (Sn\u2212Sn\u22121)+ \u2211NT n=1(Tn\u2212Tn\u22121) )} e (1\u2212\u03c4)2N2 T \u03b12(b\u2212a)2 8\n=e\u2212\u03b1\u03beE { e\u03b1 (\u2211NS\u22121 n=1 (Sn\u2212Sn\u22121)+ \u2211NT \u22121 n=1 (Tn\u2212Tn\u22121) ) E { e\u03b1(TNT \u2212TNT\u22121) \u2223\u2223\u2223ZNT\u221211 }}\n\u00d7 e (1\u2212\u03c4)2N2 T \u03b12(b\u2212a)2 8 \u2264e\u2212\u03b1\u03beE { e\u03b1 (\u2211NS\u22121 n=1 (Sn\u2212Sn\u22121)+ \u2211NT \u22121 n=1 (Tn\u2212Tn\u22121) )} e \u03c42N2 S \u03b12(b\u2212a)2 8 e (1\u2212\u03c4)2N2 T \u03b12(b\u2212a)2 8 . (77)\nThen, we have\nPr { F\u03c4 ( ZNS1 ,Z NT 1 ) \u2212 E(\u2217)F\u03c4 > \u03be } \u2264 e\u03a6(\u03b1)\u2212\u03b1\u03be , (78)\nwhere\n\u03a6(\u03b1) = \u03b12(1\u2212 \u03c4)2(b\u2212 a)2NSN2T\n8 + \u03b12\u03c4 2(b\u2212 a)2N2SNT 8 . (79)\nSimilarly, we can arrive at\nPr { E(\u2217)F\u03c4 \u2212 F\u03c4 ( ZNS1 ,Z NT 1 ) > \u03be } \u2264 e\u03a6(\u03b1)\u2212\u03b1\u03be . (80)\nNote that \u03a6(\u03b1)\u2212 \u03b1\u03be is a quadratic function with respect to \u03b1 > 0 and thus the minimum value\nmin \u03b1>0\n{\u03a6(\u03b1)\u2212 \u03b1\u03be}\nis achieved when\n\u03b1 = 4\u03be\n(b\u2212 a)2NSNT ((1\u2212 \u03c4)2NT + \u03c4 2NS) .\nBy combining (78), (79) and (80), we arrive at\nPr { |F\u03c4 ( ZNS1 ,Z NT 1 ) \u2212 E(\u2217)F\u03c4 | > \u03be } \u2264 2 exp { \u2212 2\u03be 2\n(b\u2212 a)2NSNT ((1\u2212 \u03c4)2NT + \u03c4 2NS)\n} .\nThis completes the proof.\nB.2 Symmetrization Inequality\nIn the following theorem, we present the symmetrization inequality for domain adaptation combining source and target data.\nTheorem B.2 Assume that F is a function class with the range [a, b]. Let ZNS1 and Z\u2032NS1 be drawn from the source domain Z(S), and ZNT1 and Z\u2032 NT 1 be drawn from the target domain Z(T ). Then, for any \u03c4 \u2208 [0, 1) and given an arbitrary \u03be > (1 \u2212 \u03c4)DF (S, T ), we have for any NSNT \u2265 8(b\u2212a) 2\n(\u03be\u2032)2 ,\nPr { sup f\u2208F \u2223\u2223E(T )f \u2212 E\u03c4f \u2223\u2223 > \u03be } \u2264 2Pr { sup f\u2208F \u2223\u2223E\u2032\u03c4f \u2212 E\u03c4f \u2223\u2223 > \u03be \u2032 2 } (81)\nwith \u03be\u2032 = \u03be \u2212 (1\u2212 \u03c4)DF (S, T ).\nThis theorem shows that for any \u03be > (1\u2212 \u03c4)DF (S, T ), the probability of the event:\nsup f\u2208F\n\u2223\u2223E(T )f \u2212 E\u03c4f \u2223\u2223 > \u03be\ncan be bounded by using the probability of the event:\nsup f\u2208F\n\u2223\u2223E\u2032\u03c4f \u2212 E\u03c4f \u2223\u2223 > \u03be \u2032\n2\nthat is only determined by the samples drawn from the source domain Z(S) and the target domain Z(T ), when NSNT \u2265 8(b\u2212a) 2\n(\u03be\u2032)2 . Compared to the classical symmetrization result under\nthe assumption of same distribution (see Bousquet et al., 2004), there is a discrepancy term (1 \u2212 \u03c4)DF (S, T ). The two results will coincide when the source and the target domains match, i.e., DF(S, T ) = 0. The following is the proof of Theorem B.2.\nProof of Theorem B.2. Let f\u0302 be the function achieving the supremum:\nsup f\u2208F\n|E(T )f \u2212 E\u03c4f |\nwith respect to ZNS1 and Z NT 1 . According to (9) and (12), we arrive at\n\u2223\u2223E(T )f\u0302 \u2212 E\u03c4 f\u0302 \u2223\u2223 = \u2223\u2223\u03c4E(T )f\u0302 + (1\u2212 \u03c4)E(T )f\u0302 \u2212 (1\u2212 \u03c4)E(S)f\u0302 + (1\u2212 \u03c4)E(S)f\u0302 \u2212 E\u03c4 f\u0302 \u2223\u2223\n= \u2223\u2223\u03c4(E(T )f\u0302 \u2212 E(T )NT f\u0302) + (1\u2212 \u03c4)(E (T )f\u0302 \u2212 E(S)f\u0302) + (1\u2212 \u03c4)(E(S)f\u0302 \u2212 E(S)NS f\u0302) \u2223\u2223 \u2264(1\u2212 \u03c4)DF(S, T ) + \u2223\u2223\u03c4(E(T )f\u0302 \u2212 E(T )NT f\u0302) + (1\u2212 \u03c4)(E (S)f\u0302 \u2212 E(S)NS f\u0302) \u2223\u2223, (82)\nand thus\nPr {\u2223\u2223E(T )f\u0302 \u2212 E\u03c4 f\u0302 \u2223\u2223 > \u03be }\n\u2264Pr { (1\u2212 \u03c4)DF(S, T ) + \u2223\u2223\u03c4(E(T )f\u0302 \u2212 E(T )NT f\u0302) + (1\u2212 \u03c4)(E (S)f\u0302 \u2212 E(S)NS f\u0302) \u2223\u2223 > \u03be } , (83)\nwhere\nE (T ) NT\nf\u0302 := 1\nNT\nNT\u2211\nn=1\nf\u0302(z(T )n ); E (S) NS\nf\u0302 := 1\nNS\nNS\u2211\nn=1\nf\u0302(z(S)n ). (84)\nLet \u03be\u2032 = \u03be \u2212 (1\u2212 \u03c4)DF(S, T ) (85)\nand denote \u2227 as the conjunction of two events. According to the triangle inequality, we have ( 1{\n|\u03c4(E(T )f\u0302\u2212E (T ) NT f\u0302)+(1\u2212\u03c4)(E(S) f\u0302\u2212E (S) NS\nf\u0302)|>\u03be\u2032 } )( 1{ |\u03c4(E(T )f\u0302\u2212E\u2032\n(T ) NT f\u0302)+(1\u2212\u03c4)(E(S)f\u0302\u2212E\u2032 (S) NS f\u0302)|< \u03be \u2032 2\n} )\n=1{ |\u03c4(E(T )f\u0302\u2212E\n(T ) NT f\u0302)+(1\u2212\u03c4)(E(S) f\u0302\u2212E (S) NS\nf\u0302)|>\u03be\u2032 } \u2227 { |\u03c4(E(T )f\u0302\u2212E\u2032\n(T ) NT f\u0302)+(1\u2212\u03c4)(E(S)f\u0302)\u2212E\u2032 (S) NS f\u0302 |< \u03be \u2032 2\n}\n\u22641{ |\u03c4(E\u2032\n(T ) NT f\u0302\u2212E (T ) NT f\u0302)+(1\u2212\u03c4)(E\u2032 (S) NS f\u0302\u2212E (S) NS f\u0302)|> \u03be \u2032 2\n} .\nThen, taking the expectation with respect to Z\u2032NS1 and Z \u2032 NT 1 gives\n( 1{\n|\u03c4(E(T )f\u0302\u2212E (T ) NT f\u0302)+(1\u2212\u03c4)(E(S) f\u0302\u2212E (S) NS\nf\u0302)|>\u03be\u2032 } )\n\u00d7 Pr\u2032 {\u2223\u2223\u03c4(E(T )f\u0302 \u2212 E\u2032(T )NT f\u0302) + (1\u2212 \u03c4)(E (S)f\u0302 \u2212 E\u2032(S)NS f\u0302) \u2223\u2223 < \u03be \u2032\n2\n}\n\u2264 Pr\u2032 {\u2223\u2223\u03c4(E\u2032(T )NT f\u0302 \u2212 E (T ) NT f\u0302) + (1\u2212 \u03c4)(E\u2032(S)NS f\u0302 \u2212 E (S) NS f\u0302) \u2223\u2223 > \u03be \u2032\n2\n} . (86)\nBy Chebyshev\u2019s inequality, since Z\u2032NS1 = {z\u2032(S)n }NSn=1 and Z\u2032 NT 1 = {z\u2032(T )n }NTn=1 are sets of i.i.d. samples drawn from the source domain Z(S) and the target domain Z(T ) respectively, we have for any \u03be\u2032 > 0 and any \u03c4 \u2208 [0, 1),\nPr\u2032 {\u2223\u2223\u03c4(E(T )f\u0302 \u2212 E\u2032(T )NT f\u0302) + (1\u2212 \u03c4)(E (S)f\u0302 \u2212 E\u2032(S)NS f\u0302) \u2223\u2223 \u2265 \u03be \u2032\n2\n}\n\u2264Pr\u2032 { \u03c4\nNT\nNT\u2211\nn=1\n|E(T )f\u0302 \u2212 f\u0302(z\u2032(T )n )|+ 1\u2212 \u03c4 NS\nNS\u2211\nn=1\n|E(S)f\u0302 \u2212 f\u0302(z\u2032(S)n )| \u2265 \u03be\u2032\n2\n}\n\u2264 4E\n{ \u03c4NSNT (E (T )f\u0302 \u2212 f\u0302(z\u2032(T )))2 + (1\u2212 \u03c4)NSNT (E(S)f\u0302 \u2212 f\u0302(z\u2032(S)))2 }\nN2SN 2 T (\u03be \u2032)2\n\u22644E {\u03c4NSNT (b\u2212 a) 2 + (1\u2212 \u03c4)NSNT (b\u2212 a)2}\nN2SN 2 T (\u03be \u2032)2\n= 4(b\u2212 a)2 NSNT (\u03be\u2032)2 , (87)\nwhere z\u2032(S) and z\u2032(T ) stand for the ghost random variables taking values from the source domain Z(S) and the target domain Z(T ), respectively.\nSubsequently, according to (86) and (87), we have for any \u03be\u2032 > 0,\nPr\u2032 {\u2223\u2223\u03c4(E\u2032(T )NT f\u0302 \u2212 E (T ) NT f\u0302) + (1\u2212 \u03c4)(E\u2032(S)NS f\u0302 \u2212 E (S) NS f\u0302) \u2223\u2223 > \u03be \u2032\n2\n}\n\u2265 ( 1{\u2223\u2223\u03c4(E(T )f\u0302\u2212E(T )\nNT f\u0302)+(1\u2212\u03c4)(E(S)f\u0302\u2212E (S) NS f\u0302)\n\u2223\u2223>\u03be\u2032 }\n)( 1\u2212 4(b\u2212 a) 2\nNSNT (\u03be\u2032)2\n) . (88)\nAccording to (83), (85) and (88), by letting\n4(b\u2212 a)2 NSNT (\u03be\u2032)2 \u2264 1 2 ,\nand taking the expectation with respect to ZNS1 and Z NT 1 , we have for any \u03be \u2032 > 0,\nPr { |E(T )f\u0302 \u2212 E\u03c4 f\u0302 | > \u03be }\n\u2264Pr {\u2223\u2223\u03c4(E(T )f\u0302 \u2212 E(T )NT f\u0302) + (1\u2212 \u03c4)(E (S)f\u0302 \u2212 E(S)NS f\u0302) \u2223\u2223 > \u03be\u2032 } \u22642Pr {\u2223\u2223\u03c4(E\u2032(T )NT f\u0302 \u2212 E (T ) NT f\u0302) + (1\u2212 \u03c4)(E\u2032(S)NS f\u0302 \u2212 E (S) NS f\u0302) \u2223\u2223 > \u03be \u2032\n2\n}\n=2Pr {\u2223\u2223E\u2032\u03c4 f\u0302 \u2212 E\u03c4 f\u0302 \u2223\u2223 > \u03be \u2032\n2\n} (89)\nwith \u03be\u2032 = \u03be \u2212 (1\u2212 \u03c4)DF(S, T ). This completes the proof. We are now ready to prove Theorem 6.1.\nB.3 Proof of Theorem 6.1\nProof of Theorem 6.1. Consider {\u01ebn}Nn=1 as independent Rademacher random variables, i.e., independent {\u00b11}-valued random variables with equal probability of taking either value. Given {\u01ebn}NSn=1, {\u01ebn}NTn=1, Z2NS1 and Z 2NT 1 , denote\n\u2212\u2192\u01eb S :=(\u01eb1, \u00b7 \u00b7 \u00b7 , \u01ebNS ,\u2212\u01eb1, \u00b7 \u00b7 \u00b7 ,\u2212\u01ebNS) \u2208 {\u00b11}2NS ; \u2212\u2192\u01eb T :=(\u01eb1, \u00b7 \u00b7 \u00b7 , \u01ebNT ,\u2212\u01eb1, \u00b7 \u00b7 \u00b7 ,\u2212\u01ebNT ) \u2208 {\u00b11}2NT , (90)\nand for any f \u2208 F , \u2212\u2192 f (Z2NS1 ) := ( f(z\u20321), \u00b7 \u00b7 \u00b7 , f(z\u2032NS), f(z1), \u00b7 \u00b7 \u00b7 , f(zNS) ) \u2208 [a, b]2NS ;\n\u2212\u2192 f (Z2NT1 ) := ( f(z\u20321), \u00b7 \u00b7 \u00b7 , f(z\u2032NT ), f(z1), \u00b7 \u00b7 \u00b7 , f(zNT ) ) \u2208 [a, b]2NT . (91)\nWe also denote\nZ := { Z\n2NT 1 ,Z 2NS 1\n} \u2208 ( Z(T ) )2NT \u00d7 ( Z(S) )2NS ; \u2212\u2192\u01eb :=\n(\u2212\u2192\u01eb T , \u00b7 \u00b7 \u00b7 ,\u2212\u2192\u01eb T\ufe38 \ufe37\ufe37 \ufe38 NS ,\u2212\u2192\u01eb S, \u00b7 \u00b7 \u00b7 ,\u2212\u2192\u01eb S\ufe38 \ufe37\ufe37 \ufe38 NT ) \u2208 {\u00b11}4NSNT ;\n\u2212\u2192 f ( Z ) := (\u2212\u2192 f (Z\n2NT 1 ), \u00b7 \u00b7 \u00b7 ,\n\u2212\u2192 f (Z\n2NT 1 )\ufe38 \ufe37\ufe37 \ufe38\nNS\n, \u2212\u2192 f (Z2NS1 ), \u00b7 \u00b7 \u00b7 , \u2212\u2192 f (Z2NS1 )\ufe38 \ufe37\ufe37 \ufe38\nNT\n) \u2208 [a, b]4NSNT . (92)\nAccording to (6), (85) and Theorem B.2, for any \u03c4 \u2208 [0, 1) and given an arbitrary \u03be > (1\u2212 \u03c4)DF (S, T ), we have for any NSNT \u2265 8(b\u2212a) 2\n\u03be\u20322 with \u03be\u2032 = \u03be \u2212 (1\u2212 \u03c4)DF(S, T ),\nPr { sup f\u2208F \u2223\u2223E(T )f \u2212 E\u03c4f \u2223\u2223 > \u03be }\n\u22642Pr { sup f\u2208F \u2223\u2223E\u2032\u03c4f \u2212 E\u03c4f \u2223\u2223 > \u03be \u2032 2 } (by Theorem B.2)\n=2Pr { sup f\u2208F \u2223\u2223\u2223 \u03c4 NT NT\u2211\nn=1\n( f(z\u2032\n(T ) n )\u2212 f(z(T )n )\n) + 1\u2212 \u03c4 NS NS\u2211\nn=1\n( f(z\u2032\n(S) n )\u2212 f(z(S)n ) )\u2223\u2223\u2223 > \u03be \u2032\n2\n}\n=2Pr { sup f\u2208F \u2223\u2223\u2223 \u03c4 NT NT\u2211\nn=1\n\u01ebn ( f(z\u2032 (T ) n )\u2212 f(z(T )n ) ) + 1\u2212 \u03c4 NS NS\u2211\nn=1\n\u01ebn ( f(z\u2032 (S) n )\u2212 f(z(S)n ) )\u2223\u2223\u2223 > \u03be \u2032\n2\n}\n=2Pr { sup f\u2208F \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 f (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 f (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 4 } . (93)\nGiven a \u03c4 \u2208 [0, 1), fix a realization of Z and let \u039b be a \u03be\u2032/8-radius cover of F with respect to the \u2113\u03c41(Z) norm. Since F is composed of bounded functions with the range [a, b], we assume that the same holds for any h \u2208 \u039b. If f0 is the function that achieves the following supremum\nsup f\u2208F \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 f (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 f (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 4 ,\nthere must be an h0 \u2208 \u039b that satisfies that\n\u03c4\n2NT\nNT\u2211\nn=1\n( |f0(z\u2032(T )n )\u2212 h0(z\u2032Tn )|+ |f0(z(T )n )\u2212 h0(z(T )n )| )\n+ 1\u2212 \u03c4 2NS\nNS\u2211\nn=1\n( |f0(z\u2032(S)n )\u2212 h0(z\u2032Sn)|+ |f0(z(S)n )\u2212 h0(z(S)n )| ) < \u03be\u2032\n8 ,\nand meanwhile, \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 h 0(Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 h 0(Z 2NS 1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 8 .\nTherefore, for the realization of Z, we arrive at\nPr { sup f\u2208F \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 f (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 f (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 4 }\n\u2264Pr { sup h\u2208\u039b \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 h (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 h (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 8 } . (94)\nMoreover, we denote the event\nA := { Pr { sup h\u2208\u039b \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 h (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 h (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 8 }} ,\nand let 1A be the characteristic function of the event A. By Fubini\u2019s Theorem, we have\nPr{A} = E { E\u2212\u2192\u01eb { 1A }\u2223\u2223 Z }\n=E { Pr { sup h\u2208\u039b \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 h (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 h (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 8 } \u2223\u2223 Z } . (95)\nFix a realization of Z again. According to (21), (90), (91) and Theorem B.1, for any \u03c4 \u2208 [0, 1) and given an arbitrary \u03be\u2032 > 0, we have for any NSNT \u2265 8(b\u2212a) 2\n(\u03be\u2032)2 ,\nPr { sup h\u2208\u039b \u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 h (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 h (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 8 }\n\u2264|\u039b|max h\u2208\u039b Pr {\u2223\u2223\u2223 \u03c4 2NT \u2329\u2212\u2192\u01eb T , \u2212\u2192 h (Z 2NT 1 ) \u232a + 1\u2212 \u03c4 2NS \u2329\u2212\u2192\u01eb S, \u2212\u2192 h (Z2NS1 ) \u232a\u2223\u2223\u2223 > \u03be \u2032 8 }\n=N (F , \u03be\u2032/8, \u2113\u03c41(Z))max h\u2208\u039b Pr\n{\u2223\u2223E\u2032\u03c4h\u2212 E\u03c4h \u2223\u2223 > \u03be \u2032\n4\n}\n\u2264N (F , \u03be\u2032/8, \u2113\u03c41(Z))max h\u2208\u039b Pr\n{ |E\u0303h\u2212 E\u2032\u03c4h|+ |E\u0303h\u2212 E\u03c4h| > \u03be\u2032\n4\n}\n\u22642N (F , \u03be\u2032/8, \u2113\u03c41(Z))max h\u2208\u039b Pr\n{\u2223\u2223E\u0303h\u2212 E\u03c4h \u2223\u2223 > \u03be \u2032\n8\n}\n\u22644N (F , \u03be\u2032/8, \u2113\u03c41(Z)) exp { \u2212 NSNT (\u03be \u2212 (1\u2212 \u03c4)DF(S, T )) 2\n32(b\u2212 a)2 ((1\u2212 \u03c4)2NT + \u03c4 2NS)\n} , (96)\nwhere E\u0303h := \u03c4E(T )h + (1\u2212 \u03c4)E(S)h. The combination of (22), (93), (94) and (96) leads to the following result: for any \u03c4 \u2208 [0, 1) and given an arbitrary \u03be > (1\u2212 \u03c4)DF(S, T ), we have for any NSNT \u2265 8(b\u2212a) 2\n(\u03be\u2032)2 ,\nPr { sup f\u2208F \u2223\u2223E(T )f \u2212 E\u03c4f \u2223\u2223 > \u03be }\n\u22648EN (F , \u03be\u2032/8, \u2113\u03c41(Z)) exp { \u2212 NSNT (\u03be \u2212 (1\u2212 \u03c4)DF(S, T )) 2\n32(b\u2212 a)2 ((1\u2212 \u03c4)2NT + \u03c4 2NS)\n}\n\u22648N \u03c41 (F , \u03be\u2032/8, 2(NS +NT )) exp { \u2212 NSNT (\u03be \u2212 (1\u2212 \u03c4)DF (S, T )) 2\n32(b\u2212 a)2 ((1\u2212 \u03c4)2NT + \u03c4 2NS)\n} . (97)\nAccording to (97), letting\n\u01eb := 8N \u03c41 (F , \u03be\u2032/8, 2(NS +NT )) exp { \u2212 NSNT (\u03be \u2212 (1\u2212 \u03c4)DF (S, T )) 2\n32(b\u2212 a)2 ((1\u2212 \u03c4)2NT + \u03c4 2NS)\n} ,\nwe have given an arbitrary \u03be > (1 \u2212 \u03c4)DF(S, T ) and for any NSNT \u2265 8(b\u2212a) 2\n(\u03be\u2032)2 , with probability\nat least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223E\u03c4f \u2212 E(T )f \u2223\u2223 \u2264(1\u2212 \u03c4)DF(S, T ) +\n( lnN \u03c41 (F , \u03be\u2032/8, 2(NS +NT ))\u2212 ln(\u01eb/8)\nNSNT 32(b\u2212a)2((1\u2212\u03c4)2NT+\u03c42NS)\n) 1 2\n.\nThis completes the proof."}, {"heading": "C Proofs of Theorems 5.2 & 6.2", "text": "In this appendix, we will prove Theorem 5.2 and Theorem 6.2. In order to achieve the proofs, we need to generalize the classical McDiarmid\u2019s inequality (see Bousquet et al., 2004, Theorem 6) to a more general setting where independent random variables can independently take values from different domains.\nC.1 Generalized McDiarmid\u2019s inequality\nThe following is the classical McDiarmid\u2019s inequality that is one of the most frequently used deviation inequalities in statistical learning theory and has been widely used to obtain generalization bounds based on the Rademacher complexity under the assumption of same distribution (see Bousquet et al., 2004, Theorem 6).\nTheorem C.1 (McDiamid\u2019s Inequality) Let z1, \u00b7 \u00b7 \u00b7 , zN be N independent random variables taking value from the domain Z. Assume that the function H : Z \u2192 R satisfies the condition of bounded difference: for all 1 \u2264 n \u2264 N ,\nsup z1,\u00b7\u00b7\u00b7 ,zN ,z\u2032n\n\u2223\u2223\u2223H ( z1, \u00b7 \u00b7 \u00b7 , zn, \u00b7 \u00b7 \u00b7 , zN ) \u2212H ( z1, \u00b7 \u00b7 \u00b7 , z\u2032n, \u00b7 \u00b7 \u00b7 , zN )\u2223\u2223\u2223 \u2264 cn. (98)\nThen, for any \u03be > 0\nPr { H ( z1, \u00b7 \u00b7 \u00b7 , zn, \u00b7 \u00b7 \u00b7 , zN ) \u2212 E { H ( z1, \u00b7 \u00b7 \u00b7 , zn, \u00b7 \u00b7 \u00b7 , zN )} \u2265 \u03be } \u2264 exp { \u22122\u03be2/ N\u2211\nn=1\nc2n\n} .\nAs shown in Theorem C.1, the classical McDiarmid\u2019s inequality is valid under the condition that random variables z1, \u00b7 \u00b7 \u00b7 , zN are independent and drawn from the same domain. Next, we generalize this inequality to a more general setting, where the independent random variables can take values from different domains.\nTheorem C.2 Given independent domains Z(Sk) (1 \u2264 k \u2264 K), for any 1 \u2264 k \u2264 K, let ZNk1 := {z(Sk)n }Nkn=1 be Nk independent random variables taking values from the domain Z(Sk). Assume that the function H : ( Z(S1) )N1 \u00d7\u00b7 \u00b7 \u00b7\u00d7 ( Z(SK )\n)NK \u2192 R satisfies the condition of bounded difference: for all 1 \u2264 k \u2264 K and 1 \u2264 n \u2264 Nk,\nsup Z\nN1 1 ,\u00b7\u00b7\u00b7 ,Z NK 1 ,z \u2032(Sk) n\n\u2223\u2223\u2223H ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNk\u221211 , z(Sk)1 , \u00b7 \u00b7 \u00b7 , z(Sk)n , \u00b7 \u00b7 \u00b7 , z(Sk)Nk ,Z Nk+1 1 , \u00b7 \u00b7 \u00b7 ,ZNK1 )\n\u2212H ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNk\u221211 , z(Sk)1 , \u00b7 \u00b7 \u00b7 , z\u2032(Sk)n , \u00b7 \u00b7 \u00b7 , z(Sk)Nk ,Z Nk+1 1 , \u00b7 \u00b7 \u00b7 ,ZNK1 )\u2223\u2223\u2223 \u2264 c(k)n . (99)\nThen, for any \u03be > 0\nPr { H ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK1 ) \u2212 E { H ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK1 )} \u2265 \u03be } \u2264 exp { \u22122\u03be2/ K\u2211\nk=1\nNk\u2211\nn=1\n(c(k)n ) 2\n} .\nProof. Define a random variable\nT (k)n := E { H({ZNk1 }Kk=1)|ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,ZNk\u221211 ,Zn1 } , 1 \u2264 k \u2264 K, 0 \u2264 n \u2264 Nk, (100)\nwhere Zn1 = {z(k)1 , z(k)2 , \u00b7 \u00b7 \u00b7 , z(k)n } \u2286 ZNk1 , and Z01 = \u2205.\nIt is clear that T\n(1) 0 = E { H({ZNk1 }Kk=1) } and T (K) NK\n= H({ZNk1 }Kk=1), and thus\nH({ZNk1 }Kk=1)\u2212 E { H({ZNk1 }Kk=1) } = T (K) NK \u2212 T (1)0 = K\u2211\nk=1\nNk\u2211\nn=1\n(T (k)n \u2212 T (k)n\u22121). (101)\nDenote for any 1 \u2264 k \u2264 K and 1 \u2264 n \u2264 Nk,\nU (k)n = sup \u00b5\n{ T (k)n \u2223\u2223 z (k) n =\u00b5 \u2212 T (k)n\u22121 } ;\nL(k)n = inf \u03bd\n{ T (k)n \u2223\u2223 z (k) n =\u03bd \u2212 T (k)n\u22121 } .\nIt follows from the definition of (100) that L (k) n \u2264 (T (k)n \u2212 T (k)n\u22121) \u2264 U (k)n and thus results in\nT (k)n \u2212 T (k)n\u22121 \u2264 U (k)n \u2212 L(k)n = sup \u00b5,\u03bd\n{ T (k)n \u2223\u2223 z (k) n =\u00b5 \u2212 T (k)n \u2223\u2223 z (k) n =\u03bd } \u2264 c(k)n . (102)\nMoreover, by the law of iterated expectation, we also have for any 1 \u2264 k \u2264 K and 1 \u2264 n \u2264 Nk\nE { T (k)n \u2212 T (k)n\u22121|ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,Z Nk\u22121 1 ,Z n\u22121 1 } = 0. (103)\nAccording to Hoeffding inequality (see Hoeffding, 1963), given an \u03b1 > 0, the condition (99) leads to for any 1 \u2264 k \u2264 K and 1 \u2264 n \u2264 Nk,\nE { e\u03b1(T (k) n \u2212T (k) n\u22121)|ZN11 ,ZN21 , \u00b7 \u00b7 \u00b7 ,ZNk\u221211 ,Zn\u221211 } \u2264 e\u03b12(c(k)n )2/8. (104)\nSubsequently, according to Markov\u2019s inequality, (101), (102), (103) and (104), we have for any \u03b1 > 0,\nPr { H ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK1 ) \u2212 E { H ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK1 )} \u2265 \u03be }\n\u2264e\u2212\u03b1\u03beE { e\u03b1 ( H ( Z N1 1 ,\u00b7\u00b7\u00b7 ,Z NK 1 ) \u2212E { H ( Z N1 1 ,\u00b7\u00b7\u00b7 ,Z NK 1 )})} =e\u2212\u03b1\u03beE { e\u03b1 (\u2211K k=1 \u2211Nk n=1(T (k) n \u2212T (k) n\u22121) )}\n=e\u2212\u03b1\u03beE { E { e\u03b1 (\u2211K k=1 \u2211Nk n=1(T (k) n \u2212T (k) n\u22121) )} |ZN11 , \u00b7 \u00b7 \u00b7 ,ZNk\u221211 ,ZNK\u221211 } =e\u2212\u03b1\u03beE { e\u03b1 (\u2211K\u22121 k=1 \u2211Nk n=1(T (k) n \u2212T (k) n\u22121)+ \u2211NK\u22121 n=1 (T (K) n \u2212T (K) n\u22121) ){ e \u03b1 ( T (K) NK \u2212T (K) NK\u22121 ) |ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK\u221211 ,ZNK\u221211 }} \u2264e\u2212\u03b1\u03beE { e\u03b1 (\u2211K\u22121 k=1 \u2211Nk n=1(T (k) n \u2212T (k) n\u22121)+ \u2211NK\u22121 n=1 (T (K) n \u2212T (K) n\u22121) )} e \u03b12(c (K) NK )2/8\n\u2264e\u2212\u03b1\u03be K\u220f\nk=1\nNk\u220f\nn=1\nexp\n{ \u03b12(c (k) n )2\n8\n}\n=exp { \u2212\u03b1\u03be + \u03b12 K\u2211\nk=1\nNk\u2211\nn=1\n(c (k) n )2\n8\n} .\nThe above bound is minimized by setting\n\u03b1\u2217 = 4\u03be\n\u2211K k=1 \u2211NK n=1(c (k) n )2 ,\nand its minimum value is\nexp { \u22122\u03be2/ K\u2211\nk=1\nNk\u2211\nn=1\n(c(k)n ) 2\n} .\nThis completes the proof.\nC.2 Proof of Theorem 5.2\nBy using Theorem C.2, we prove Theorem 5.2 as follows: Proof of Theorem 5.2 Assume that F is a function class F consisting of bounded functions with the range [a, b]. Let sample sets {ZNk1 }Kk=1 := {{z(k)n }Nkn=1}Kk=1 be drawn from multiple sources Z(Sk) (1 \u2264 k \u2264 K), respectively. Given a choice of w \u2208 [0, 1]K with \u2211Kk=1wk = 1, denote\nH ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK1 ) := sup\nf\u2208F\n\u2223\u2223E(S) w f \u2212 E(T )f \u2223\u2223. (105)\nBy (1), we have\nH ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK1 ) = sup\nf\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E(T )f )\u2223\u2223\u2223, (106)\nwhere E (S) Nk f = 1 Nk \u2211Nk n=1 f(z (k) n ). Therefore, it is clear that such H ( ZN11 , \u00b7 \u00b7 \u00b7 ,ZNK1 ) satisfies the condition of bounded difference with\nc(k)n = (b\u2212 a)wk\nNk\nfor all 1 \u2264 k \u2264 K and 1 \u2264 n \u2264 Nk. Thus, according to Theorem C.2, we have for any \u03be > 0,\nPr { H ( ZNk1 , \u00b7 \u00b7 \u00b7 ,ZNk1 ) \u2212 E(S) { H ( ZNk1 , \u00b7 \u00b7 \u00b7 ,ZNk1 )} \u2265 \u03be } \u2264 exp    \u22122\u03be2 \u2211K\nk=1\n(b\u2212a)2w2 k\nNk\n   ,\nwhich can be equivalently rewritten as with probability at least 1\u2212 \u01eb,\nH ( ZNk1 , \u00b7 \u00b7 \u00b7 ,ZNk1 ) \u2264E(S) { H ( ZNk1 , \u00b7 \u00b7 \u00b7 ,ZNk1 )} +\n\u221a\u221a\u221a\u221a K\u2211\nk=1\n(b\u2212 a)2w2k ln(1/\u01eb) 2Nk\n=E(S) { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E(T )f )\u2223\u2223\u2223 } + \u221a\u221a\u221a\u221a K\u2211\nk=1\n(b\u2212 a)2w2k ln(1/\u01eb) 2Nk\n=E(S) { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E(Sk)f + E(Sk)f \u2212 E(T )f )\u2223\u2223\u2223 }\n+\n\u221a\u221a\u221a\u221a K\u2211\nk=1\n(b\u2212 a)2w2k ln(1/\u01eb) 2Nk\n\u2264E(S) { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E(Sk)f )\u2223\u2223\u2223 } + K\u2211\nk=1\nwk sup f\u2208F\n\u2223\u2223E(Sk)f \u2212 E(T )f \u2223\u2223\n+\n\u221a\u221a\u221a\u221a K\u2211\nk=1\n(b\u2212 a)2w2k ln(1/\u01eb) 2Nk\n\u2264E(S) { sup f\u2208F \u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E(Sk)f )\u2223\u2223\u2223 } +D (w) F (S, T ) [see (26)]\n+\n\u221a\u221a\u221a\u221a K\u2211\nk=1\n(b\u2212 a)2w2k ln(1/\u01eb) 2Nk . (107)\nNext, according to (23) and (106), we have\nE(S) sup f\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E(Sk)f )\u2223\u2223\u2223\n=E(S) sup f\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E\u2032(Sk){E\u2032(S)Nk f} )\u2223\u2223\u2223\n\u2264E(S)E\u2032(S) sup f\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk ( E (S) Nk f \u2212 E\u2032(S)Nk f )\u2223\u2223\u2223\n=E(S)E\u2032 (S)\nsup f\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk 1\nNk\nNk\u2211\nn=1\n( f(z(k)n )\u2212 f(z\u2032(k)n ) )\u2223\u2223\u2223\n\u2264E(S)E\u2032(S)E\u03c3 sup f\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk 1\nNk\nNk\u2211\nn=1\n\u03c3(k)n ( f(z(k)n )\u2212 f(z\u2032(k)n ) )\u2223\u2223\u2223\n\u22642E(S)E\u03c3 sup f\u2208F\n\u2223\u2223\u2223 K\u2211\nk=1\nwk 1\nNk\nNk\u2211\nn=1\n\u03c3(k)n f(z (k) n )\n\u2223\u2223\u2223\n\u22642E(S)E\u03c3 K\u2211\nk=1\nwk 1\nNk sup f\u2208F\n\u2223\u2223\u2223 Nk\u2211\nn=1\n\u03c3(k)n f(z (k) n )\n\u2223\u2223\u2223\n=2\nK\u2211\nk=1\nwkR(k)(F). (108)\nBy combining (105), (107) and (108), we obtain with probability at least 1\u2212 \u01eb\nsup f\u2208F\n\u2223\u2223E(S) w f \u2212 E(T )f \u2223\u2223 \u2264 D(w)F (S, T ) + 2 K\u2211\nk=1\nwkR(k)(F) +\n\u221a\u221a\u221a\u221a K\u2211\nk=1\n(b\u2212 a)2w2k ln(1/\u01eb) 2Nk .\nThis completes the proof.\nC.3 Proof of Theorem 6.2\nIn order to prove Theorem 6.2, we also need the following result (see Bousquet et al., 2004, Theorem 5):\nTheorem C.3 Let F \u2286 [a, b]Z . For any \u01eb > 0, with probability at least 1 \u2212 \u01eb, there holds that for any f \u2208 F ,\nEf \u2264ENf + 2R(F) + \u221a\n(b\u2212 a) ln(1/\u01eb) 2N\n\u2264ENf + 2RN (F) + 3 \u221a\n(b\u2212 a) ln(2/\u01eb) 2N . (109)\nAgain, we prove Theorem 6.2 by using Theorems C.2 and C.3. Proof of Theorem 6.2 We only consider the result of Theorem C.2 in a special setting of\nK = 2, N1 = NT , N2 = NS, w1 = \u03c4 and w2 = 1\u2212 \u03c4 . Given a choice of \u03c4 \u2208 [0, 1), denote\nH ( ZNS1 ,Z NT 1 ) := sup\nf\u2208F\n\u2223\u2223E\u03c4f \u2212 E(T )f \u2223\u2223. (110)\nBy (9), we have\nH ( ZNS1 ,Z NT 1 ) = sup\nf\u2208F\n\u2223\u2223\u2223\u03c4E(T )NT f + (1\u2212 \u03c4)E (S) NS f \u2212 E(T )f \u2223\u2223\u2223, (111)\nAccording to Theorem C.2, we have for any \u03be > 0,\nPr { H ( ZNS1 ,Z NT 1 ) \u2212 E(S) { H ( ZNS1 ,Z NT 1 )} \u2265 \u03be } \u2264 exp   \n\u22122\u03be2\n(b\u2212 a)2 ( \u03c42\nNT + (1\u2212\u03c4)\n2\nNS\n)    ,\nwhich can be equivalently rewritten as with probability at least 1\u2212 (\u01eb/2),\nH ( ZNS1 ,Z NT 1 )\n\u2264E(S) { H ( ZNS1 ,Z NT 1 )} +\n\u221a (b\u2212 a)2 ln(2/\u01eb)\n2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n)\n=E(S) { sup f\u2208F \u2223\u2223\u2223\u03c4E(T )NT f + (1\u2212 \u03c4)E (S) NS f \u2212 E(T )f \u2223\u2223\u2223 } +\n\u221a (b\u2212 a)2 ln(2/\u01eb)\n2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n)\n=E(S) { sup f\u2208F \u2223\u2223\u2223\u03c4 ( E (T ) NT f \u2212 E(T )f) + (1\u2212 \u03c4) ( E (S) NS f \u2212 E(T )f )\u2223\u2223\u2223 }\n+\n\u221a (b\u2212 a)2 ln(2/\u01eb)\n2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n)\n\u2264\u03c4 sup f\u2208F\n\u2223\u2223\u2223E(T )NT f \u2212 E (T )f \u2223\u2223\u2223+ (1\u2212 \u03c4)E(S) { sup f\u2208F \u2223\u2223\u2223E(S)NSf \u2212 E (T )f \u2223\u2223\u2223 }\n+\n\u221a (b\u2212 a)2 ln(2/\u01eb)\n2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n)\n=\u03c4 sup f\u2208F\n\u2223\u2223\u2223E(T )NT f \u2212 E (T )f \u2223\u2223\u2223+ (1\u2212 \u03c4)E(S) { sup f\u2208F \u2223\u2223\u2223E(S)NSf \u2212 E (S)f + E(S)f \u2212 E(T )f \u2223\u2223\u2223 }\n+\n\u221a (b\u2212 a)2 ln(2/\u01eb)\n2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n)\n\u2264\u03c4 sup f\u2208F\n\u2223\u2223\u2223E(T )NT f \u2212 E (T )f \u2223\u2223\u2223+ (1\u2212 \u03c4)E(S) { sup f\u2208F \u2223\u2223\u2223E(S)NSf \u2212 E (S)f \u2223\u2223\u2223 }\n+ (1\u2212 \u03c4) sup f\u2208F\n\u2223\u2223\u2223E(S)f \u2212 E(T )f \u2223\u2223\u2223+\n\u221a (b\u2212 a)2 ln(2/\u01eb)\n2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n)\n=\u03c4 sup f\u2208F\n\u2223\u2223\u2223E(T )NT f \u2212 E (T )f \u2223\u2223\u2223+ (1\u2212 \u03c4)E(S) { sup f\u2208F \u2223\u2223\u2223E(S)NSf \u2212 E (S)f \u2223\u2223\u2223 } + (1\u2212 \u03c4)DF(S, T )\n+\n\u221a (b\u2212 a)2 ln(2/\u01eb)\n2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n) . (112)\nAccording to Theorem C.3, for any \u01eb > 0, we have with at least 1\u2212 (\u01eb/2),\nsup f\u2208F\n\u2223\u2223\u2223E(T )NT f \u2212 E (T )f \u2223\u2223\u2223 \u2264 2R(T )NT (F) + 3 \u221a\n(b\u2212 a) ln(4/\u01eb) 2NT . (113)\nNext, according to (23), we have\nE(S) { sup f\u2208F \u2223\u2223\u2223E(S)NSf \u2212 E (S)f \u2223\u2223\u2223 }\n=E(S) sup f\u2208F\n\u2223\u2223\u2223E(S)NSf \u2212 E \u2032(S){E\u2032(S)NSf} \u2223\u2223\u2223\n\u2264E(S)E\u2032(S) sup f\u2208F\n\u2223\u2223\u2223E(S)Nk f \u2212 E \u2032(S) Nk f \u2223\u2223\u2223\n=E(S)E\u2032 (S)\nsup f\u2208F \u2223\u2223\u2223 1 NS NS\u2211\nn=1\n( f(z(S)n )\u2212 f(z\u2032(S)n ) )\u2223\u2223\u2223\n\u2264E(S)E\u2032(S)E\u03c3 sup f\u2208F \u2223\u2223\u2223 1 NS NS\u2211\nn=1\n\u03c3n ( f(z(S)n )\u2212 f(z\u2032(S)n ) )\u2223\u2223\u2223\n\u22642E(S)E\u03c3 sup f\u2208F \u2223\u2223\u2223 1 NS NS\u2211\nn=1\n\u03c3nf(z (S) n ) \u2223\u2223\u2223\n=2R(S)(F). (114)\nBy combining (110), (112), (113) and (114), we obtain with probability at least 1\u2212 \u01eb,\nsup f\u2208F\n\u2223\u2223E\u03c4f \u2212 E(T )f \u2223\u2223 \u2264(1\u2212 \u03c4)DF (S, T ) + 2(1\u2212 \u03c4)R(S)(F)\n+ 2\u03c4R(T )NT (F) + 3\u03c4 \u221a (b\u2212 a) ln(4/\u01eb) 2NT + (1\u2212 \u03c4) \u221a\n(b\u2212 a)2 ln(2/\u01eb) 2\n( \u03c4 2\nNT + (1\u2212 \u03c4)2 NS\n) .\nThis completes the proof."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "In this paper, we provide a new framework to obtain the generalization bounds of the<lb>learning process for domain adaptation, and then apply the derived bounds to analyze the<lb>asymptotical convergence of the learning process. Without loss of generality, we consider<lb>two kinds of representative domain adaptation: one is with multiple sources and the other<lb>is combining source and target data.<lb>In particular, we use the integral probability metric to measure the difference between<lb>two domains. For either kind of domain adaptation, we develop a related Hoeffding-type<lb>deviation inequality and a symmetrization inequality to achieve the corresponding gener-<lb>alization bound based on the uniform entropy number. We also generalized the classical<lb>McDiarmid\u2019s inequality to a more general setting where independent random variables can<lb>take values from different domains. By using this inequality, we then obtain generaliza-<lb>tion bounds based on the Rademacher complexity. Afterwards, we analyze the asymptotic<lb>convergence and the rate of convergence of the learning process for such kind of domain<lb>adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the<lb>learning process and the numerical experiments support our theoretical findings as well.", "creator": "LaTeX with hyperref package"}}}