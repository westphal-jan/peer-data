{"id": "1304.5299", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2013", "title": "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget", "abstract": "Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints twice in order to reach a single binary decision is computationally inefficient. We introduce an approximate Metropolis-Hastings rule based on a sequential hypothesis test which allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time. We show that the same idea can also be applied to Gibbs sampling in densely connected graphs.\n\n\n\nIt is difficult to say which features should be included in Bayesian posterior MCMC sampling. Given that Bayesian posterior MCMC sampling is less robust than the Bayesian posterior MCMC sampling, we argue that Bayesian posterior MCMC sampling is better at minimizing variance. However, the Bayesian posterior MCMC sampling is not as efficient when faced with very large datasets.\nFor example, with our Bayesian posterior MCMC sampling, our expectation of the probability of an N datapoints in two consecutive Bayesian data sets is a lower threshold than our expected, so it is not too hard to predict the probability that we should use more sampling in the data sets. Given that the probability of the probability of a Bayesian posterior MCMC sampling is lower than the Bayesian posterior MCMC sampling, we suggest that Bayesian posterior MCMC sampling may be applied to Bayesian posterior MCMC sampling. Furthermore, Bayesian posterior MCMC sampling may be applied to Bayesian posterior MCMC sampling. Moreover, Bayesian posterior MCMC sampling may be applied to Bayesian posterior MCMC sampling. The best known Bayesian posterior MCMC sampling (for example, a probability of being a Bayesian posterior MCMC sampling) is our ability to infer the probability of a Bayesian posterior MCMC sampling using two simple Bayesian posterior MCMC sampling schemes.\nTo evaluate Bayesian posterior MCMC sampling, we have to estimate the probability of an N datapoints in two consecutive Bayesian data sets. The Bayesian posterior MCMC sampling (for example, a probability of being a Bayesian posterior MCMC sampling) is typically the most reliable measure of Bayesian posterior MCMC sampling. This method is often used to evaluate Bayesian posterior MCMC sampling. Using the Bayesian posterior MCMC sampling, we can provide", "histories": [["v1", "Fri, 19 Apr 2013 02:51:52 GMT  (78kb)", "https://arxiv.org/abs/1304.5299v1", "13 pages, 15 figures"], ["v2", "Mon, 29 Apr 2013 21:13:59 GMT  (80kb)", "http://arxiv.org/abs/1304.5299v2", "13 pages, 15 figures, v2 - added new reference"], ["v3", "Fri, 19 Jul 2013 18:05:53 GMT  (80kb)", "http://arxiv.org/abs/1304.5299v3", "13 pages, 15 figures, v3 - added acknowledgements"], ["v4", "Fri, 14 Feb 2014 07:42:15 GMT  (616kb,D)", "http://arxiv.org/abs/1304.5299v4", "v4 - version accepted by ICML2014"]], "COMMENTS": "13 pages, 15 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["anoop korattikara balan", "yutian chen", "max welling"], "accepted": true, "id": "1304.5299"}, "pdf": {"name": "1304.5299.pdf", "metadata": {"source": "CRF", "title": "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget", "authors": ["Anoop Korattikara", "Yutian Chen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Markov chain Monte Carlo (MCMC) sampling has been the main workhorse of Bayesian computation since the 1990s. A canonical MCMC algorithm proposes samples from a distribution q and then accepts or rejects these proposals with a certain probability given by the Metropolis-Hastings (MH) formula [Metropolis et al., 1953, Hastings, 1970]. For each proposed sample, the MH rule needs to examine the likelihood of all dataitems. When the number of data-cases is large this is an awful lot of computation for one bit of information, namely whether to accept or reject a proposal.\nIn today\u2019s Big Data world, we need to rethink our Bayesian inference algorithms. Standard MCMC methods do not meet the Big Data challenge for the reason described above. Researchers have made some progress in terms of making MCMC more efficient, mostly by focusing on parallelization. Very few question the algorithm itself: is the standard MCMC paradigm really optimally efficient in achieving its goals? We claim it is not.\nAny method that includes computation as an essential ingredient should acknowledge that there is a finite amount of time, T , to finish a calculation. An efficient MCMC algorithm should therefore decrease the \u201cerror\u201d (properly defined) maximally in the given time T . For MCMC algorithms, there are two contributions to this error: bias and variance. Bias occurs because the chain needs to burn in during which it is sampling from the wrong distribution. Bias usually decreases fast, as evidenced by the fact that practitioners are willing to wait until the bias has (almost) completely vanished after which they discard these \u201cburn-in samples\u201d. The second cause of error is sampling variance, which occurs because of the random nature of the sampling process. The retained samples after burn-in will reduce the variance as O(1/T ).\nHowever, given a finite amount of computational time, it is not at all clear whether the strategy of retaining few unbiased samples and accepting an error dominated by variance is optimal. Perhaps, by decreasing the bias more slowly we could sample faster and thus reduce variance faster? In this paper we illustrate this effect by cutting the computational budget of the MH accept/reject step. To achieve that, we conduct sequential hypothesis tests to decide whether to accept or reject a given sample and find that the majority of these decisions can be made based on a small fraction of the data with high confidence. A\n\u2217akoratti@ics.uci.edu \u2020yutian.chen@eng.cam.edu \u2021welling@ics.uci.edu\nar X\niv :1\n30 4.\n52 99\nv4 [\ncs .L\nG ]\n1 4\nFe b\n20 14\nrelated method was used in Singh et al. [2012], where the factors of a graphical model are sub-sampled to compute fixed-width confidence intervals for the log-likelihood in the MH test.\nOur \u201cphilosophy\u201d runs deeper than the algorithm proposed here. We advocate MCMC algorithms with a \u201cbias-knob\u201d, allowing one to dial down the bias at a rate that optimally balances error due to bias and variance. We only know of one algorithm that would also adhere to this strategy: stochastic gradient Langevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al., 2012]. In their case the bias-knob was the stepsize. These algorithms do not have an MH step which resulted in occasional samples with extremely low probability. We show that our approximate MH step largely resolves this, still avoiding O(N) computations per iteration.\nIn the next section we introduce the MH algorithm and discuss its drawbacks. Then in Section 3, we introduce the idea of approximate MCMC methods and the bias variance trade-off involved. We develop approximate MH tests for Bayesian posterior sampling in Section 4 and present a theoretical analysis in Section 5. Finally, we show our experimental results in Section 6 and conclude in Section 7."}, {"heading": "2 The Metropolis-Hastings algorithm", "text": "MCMC methods generate samples from a distribution S0(\u03b8) by simulating a Markov chain designed to have stationary distribution S0(\u03b8). A Markov chain with a given stationary distribution can be constructed using the Metropolis-Hastings algorithm [Metropolis et al., 1953, Hastings, 1970], which uses the following rule for transitioning from the current state \u03b8t to the next state \u03b8t+1:\n1. Draw a candidate state \u03b8\u2032 from a proposal distribution q(\u03b8\u2032|\u03b8t)\n2. Compute the acceptance probability:\nPa = min [ 1, S0(\u03b8\u2032)q(\u03b8t|\u03b8\u2032) S0(\u03b8t)q(\u03b8\u2032|\u03b8t) ] (1)\n3. Draw u \u223c Uniform[0, 1]. If u < Pa set \u03b8t+1 \u2190 \u03b8\u2032, otherwise set \u03b8t+1 \u2190 \u03b8t.\nFollowing this transition rule ensures that the stationary distribution of the Markov chain is S0(\u03b8). The samples from the Markov chain are usually used to estimate the expectation of a function f(\u03b8) with respect\nto S0(\u03b8). To do this we collect T samples and approximate the expectation I = \u3008f\u3009S0 as I\u0302 = 1T \u2211T t=1 f(\u03b8t). Since the stationary distribution of the Markov chain is S0, I\u0302 is an unbiased estimator of I (if we ignore burn-in).\nThe variance of I\u0302 is V = E[(\u3008f\u3009S0 \u2212 1T \u2211T t=1 f(\u03b8t))\n2], where the expectation is over multiple simulations of the Markov chain. It is well known that V \u2248 \u03c32f,S0\u03c4/T , where \u03c3 2 f,S0 is the variance of f with respect to S0 and \u03c4 is the integrated auto-correlation time, which is a measure of the interval between independent samples [Gamerman and Lopes, 2006]. Usually, it is quite difficult to design a chain that mixes fast and therefore, the auto-correlation time will be quite high. Also, for many important problems, evaluating S0(\u03b8) to compute the acceptance probability Pa in every step is so expensive that we can collect only a very small number of samples (T ) in a realistic amount of computational time. Thus the variance of I\u0302 can be prohibitively high, even though it is unbiased."}, {"heading": "3 Approximate MCMC and the Bias-Variance Tradeoff", "text": "Ironically, the reason MCMC methods are so slow is that they are designed to be unbiased. If we were to allow a small bias in the stationary distribution, it is possible to design a Markov chain that can be simulated cheaply [Welling and Teh, 2011, Ahn et al., 2012]. That is, to estimate I = \u3008f\u3009S0 , we can use a Markov chain with stationary distribution S where is a parameter that can be used to control the bias in the algorithm. Then I can be estimated as I\u0302 = 1T \u2211T t=1 f(\u03b8t), computed using samples from S instead of S0.\nAs \u2192 0, S approaches S0 (the distribution of interest) but it becomes expensive to simulate the Markov chain. Therefore, the bias in I\u0302 is low, but the variance is high because we can collect only a small number of samples in a given amount of computational time. As moves away from 0, it becomes cheap to simulate\nthe Markov chain but the difference between S and S0 grows. Therefore, I\u0302 will have higher bias, but lower variance because we can collect a larger number of samples in the same amount of computational time. This is a classical bias-variance trade-off and can be studied using the risk of the estimator.\nThe risk can be defined as the mean squared error in I\u0302, i.e. R = E[(I \u2212 I\u0302)2], where the expectation is taken over multiple simulations of the Markov chain. It is easy to show that the risk can be decomposed as R = B2 + V , where B is the bias and V is the variance. If we ignore burn-in, it can be shown that B = \u3008f\u3009S \u2212 \u3008f\u3009S0 and V = E[(\u3008f\u3009S \u2212 1T f(\u03b8t))\n2] \u2248 \u03c32f,S \u03c4/T . The optimal setting of that minimizes the risk depends on the amount of computational time available. If we have an infinite amount of computational time, we should set to 0. Then there is no bias, and the variance can be brought down to 0 by drawing an infinite number of samples. This is the traditional MCMC setting. However, given a finite amount of computational time, this setting may not be optimal. It might be better to tolerate a small amount of bias in the stationary distribution if it allows us to reduce the variance quickly, either by making it cheaper to collect a large number of samples or by mixing faster.\nIt is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient Langevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn et al., 2012]. These algorithms are biased because they omit the required Metropolis-Hastings tests. However, in both cases, a knob (the step-size of the proposal distribution) is available to control the bias. As \u2192 0, the acceptance probability Pa \u2192 1 and the bias from not conducting MH tests disappears. However, when \u2192 0 the chain mixes very slowly and the variance increases because the auto-correlation time \u03c4 \u2192\u221e. As is increased from 0, the auto-correlation, and therefore the variance, reduces. But, at the same time, the acceptance probability reduces and the bias from not conducting MH tests increases as well.\nIn the next section, we will develop another class of approximate MCMC algorithms for the case where the target S0 is a Bayesian posterior distribution given a very large dataset. We achieve this by developing an approximate Metropolis-Hastings test, equipped with a knob for controlling the bias. Moreover, our algorithm has the advantage that it can be used with any proposal distribution. For example, our method allows approximate MCMC methods to be applied to problems where it is impossible to compute gradients (which is necessary to apply SGLD/SGFS). Or, we can even combine our method with SGLD/SGFS, to obtain the best of both worlds."}, {"heading": "4 Approximate Metropolis-Hastings Test for Bayesian Posterior", "text": "Sampling\nAn important method in the toolbox of Bayesian inference is posterior sampling. Given a dataset of N independent observations XN = {x1, . . . , xN}, which we model using a distribution p(x; \u03b8) parameterized by \u03b8, defined on a space \u0398 with measure \u2126, and a prior distribution \u03c1(\u03b8), the task is to sample from the\nposterior distribution S0(\u03b8) \u221d \u03c1(\u03b8) \u220fN i=1 p(xi; \u03b8).\nIf the dataset has a billion datapoints, it becomes very painful to compute S0(.) in the MH test, which has to be done for each posterior sample we generate. Spending O(N) computation to get just 1 bit of information, i.e. whether to accept or reject a sample, is likely not the best use of computational resources.\nBut, if we try to develop accept/reject tests that satisfy detailed balance exactly with respect to the posterior distribution using only sub-samples of data, we will quickly see the no free lunch theorem kicking in. For example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed by Lin et al. [2000] provide a way to conduct exact accept/reject tests using unbiased estimators of the likelihood. However, unbiased estimators of the likelihood that can be computed from mini-batches of data, such as the Poisson estimator [Fearnhead et al., 2008] or the Kennedy-Bhanot estimator [Lin et al., 2000] have very high variance for large datasets. Because of this, once we get a very high estimate of the likelihood, almost all proposed moves are rejected and the algorithm gets stuck.\nThus, we should be willing to tolerate some error in the stationary distribution if we want faster accept/reject tests. If we can offset this small bias by drawing a large number of samples cheaply and reducing the variance faster, we can establish a potentially large reduction in the risk.\nWe will now show how to develop such approximate tests by reformulating the MH test as a statistical decision problem. It is easy to see that the original MH test (Eqn. 1) is equivalent to the following procedure: Draw u \u223c Uniform[0, 1] and accept the proposal \u03b8\u2032 if the average difference \u00b5 in the log-likelihoods of \u03b8\u2032 and\n\u03b8t is greater than a threshold \u00b50, i.e. compute\n\u00b50 = 1\nN log\n[ u \u03c1(\u03b8t)q(\u03b8\n\u2032|\u03b8t) \u03c1(\u03b8\u2032)q(\u03b8t|\u03b8\u2032)\n] , and (2)\n\u00b5 = 1\nN N\u2211 i=1 li where li = log p(xi; \u03b8 \u2032)\u2212 log p(xi; \u03b8t) (3)\nThen if \u00b5 > \u00b50, accept the proposal and set \u03b8t+1 \u2190 \u03b8\u2032. If \u00b5 \u2264 \u00b50, reject the proposal and set \u03b8t+1 \u2190 \u03b8t. This reformulation of the MH test makes it very easy to frame it as a statistical hypothesis test. Given \u00b50 and a random sample {li1 , . . . , lin} drawn without replacement from the population {l1, . . . , lN}, can we decide whether the population mean \u00b5 is greater than or less than the threshold \u00b50? The answer to this depends on the precision in the random sample. If the difference between the sample mean l\u0304 and \u00b50 is significantly greater than the standard deviation s of l\u0304, we can make the decision to accept or reject the proposal confidently. If not, we should draw more data to increase the precision of l\u0304 (reduce s) until we have enough evidence to make a decision.\nMore formally, we test the hypotheses H1 : \u00b5 > \u00b50 vs H2 : \u00b5 < \u00b50. To do this, we proceed as follows: We compute the sample mean l\u0304 and the sample standard deviation sl = \u221a\n(l2 \u2212 (l\u0304)2) nn\u22121 . Then the standard deviation of l\u0304 can be estimated as:\ns = sl\u221a n\n\u221a 1\u2212 n\u2212 1\nN \u2212 1 (4)\nwhere \u221a\n1\u2212 n\u22121N\u22121 , the finite population correction term, is applied because we are drawing the subsample without replacement from a finite-sized population. Then, we compute the test statistic:\nt = l\u0304 \u2212 \u00b50 s\n(5)\nApproximate MH test Input: \u03b8t, \u03b8 \u2032, , \u00b50, XN , m Output: accept 1: Initialize estimated means l\u0304\u2190 0 and l2 \u2190 0 2: Initialize n\u2190 0, done\u2190 false 3: Draw u \u223c Uniform[0,1] 4: while not done do 5: Draw mini-batch X of size min (m, N \u2212 n) without replacement from XN and set XN \u2190 XN \\ X 6: Update l\u0304 and l2 using X , and n\u2190 n+ |X | 7: Estimate std s using Eqn. 4\n8: Compute \u03b4 \u2190 1\u2212 \u03c6n\u22121 (\u2223\u2223\u2223\u2223 l\u0304 \u2212 \u00b50s \u2223\u2223\u2223\u2223) 9: if \u03b4 < then\n10: accept\u2190 true if l\u0304 > \u00b50 and false otherwise 11: done\u2190 true 12: end if 13: end while\nIf n is large enough for the central limit theorem (CLT) to hold, the test statistic t follows a standard Student-t distribution with n \u2212 1 degrees of freedom, when \u00b5 = \u00b50 (see Fig. 7 in supplementary for an empirical verification). Then, we compute \u03b4 = 1 \u2212 \u03c6n\u22121(|t|) where \u03c6n\u22121(.) is the cdf of the standard Student-t distribution with n\u2212 1 degrees of freedom. If \u03b4 < (a fixed threshold) we can confidently say that \u00b5 is significantly different from \u00b50. In this case, if l\u0304 > \u00b50, we decide \u00b5 > \u00b50, otherwise we decide \u00b5 < \u00b50. If \u03b4 \u2265 , we do not have enough evidence to make a decision. In this case, we draw more data to reduce the uncertainty, s, in the sample mean l\u0304. We keep drawing more data until we have the required confidence (i.e. until \u03b4 < ). Note, that this procedure will terminate because when we have used all the available data, i.e.\nn = N , the standard deviation s is 0, the sample mean l\u0304 = \u00b5 and \u03b4 = 0 < . So, we will make the same decision as the original MH test would make. Pseudo-code for our test is shown in Algorithm . Here, we start with a mini-batch of size m for the first test and increase it by m datapoints when required.\nThe advantage of our method is that often we can make confident decisions with n < N datapoints and save on computation, although we introduce a small bias in the stationary distribution. But, we can use the computational time we save to draw more samples and reduce the variance. The bias-variance trade-off can be controlled by adjusting the knob . When is high, we make decisions without sufficient evidence and introduce a high bias. As \u2192 0, we make more accurate decisions but are forced to examine more data which results in high variance.\nOur algorithm will behave erratically if the CLT does not hold, e.g. with very sparse datasets or datasets with extreme outliers. The CLT assumption can be easily tested empirically before running the algorithm to avoid such pathological situations. The sequential hypothesis testing method can also be used to speed-up Gibbs sampling in densely connected Markov Random Fields. We explore this idea briefly in Section F of the supplementary."}, {"heading": "5 Error Analysis and Test Design", "text": "In 5.1, we study the relation between the parameter , the error E of the complete sequential test, the error \u2206 in the acceptance probability and the error in the stationary distribution. In 5.2, we describe how to design an optimal test that minimizes data usage given a bound on the error."}, {"heading": "5.1 Error Analysis and Estimation", "text": "The parameter is an upper-bound on the error of a single test and not the error of the complete sequential test. To compute this error, we assume a) n is large enough that the t statistics can be approximated with z statistics, and b) the joint distribution of the l\u0304\u2019s corresponding to different mini-batches used in the test is multivariate normal. Under these assumptions, we can show that the test statistic at different stages of the sequential test follows a Gaussian Random Walk process. This allows us to compute the error of the sequential test E(\u00b5std,m, ), and the expected proportion of the data required to reach a decision \u03c0\u0304(\u00b5std,m, ), using an efficient dynamic programming algorithm. Note that E and \u03c0\u0304 depend on \u03b8, \u03b8\u2032 and u\nonly through the \u2018standardized mean\u2019 defined as \u00b5std(u, \u03b8, \u03b8 \u2032) def =\n(\u00b5(\u03b8, \u03b8\u2032)\u2212 \u00b50(\u03b8, \u03b8\u2032, u)) \u221a N \u2212 1\n\u03c3l(\u03b8, \u03b8\u2032) where \u03c3l is\nthe true standard deviation of the li\u2019s. See Section A of the supplementary for a detailed derivation and an empirical validation of the assumptions.\nFig. 1 shows the theoretical and actual error of 1000 sequential tests for the logistic regression model described in Section 6.1. The error E(\u00b5std,m, ) is highest in the worst case when \u00b5 = \u00b50. Therefore, E(0,m, ) is an upper-bound on E . Since the error decreases sharply as \u00b5 moves away from \u00b50, we can get a more useful estimate of E if we have some knowledge about the distribution of \u00b5std\u2019s that will be encountered during the Markov chain simulation.\nNow, let Pa, (\u03b8, \u03b8 \u2032) be the actual acceptance probability of our algorithm and let \u2206(\u03b8, \u03b8\u2032) def = Pa, (\u03b8, \u03b8 \u2032)\u2212 Pa(\u03b8, \u03b8 \u2032) be the error in Pa, . In Section B of the supplementary, we show that for any (\u03b8, \u03b8 \u2032):\n\u2206 = \u222b 1 Pa E(\u00b5std(u))du\u2212 \u222b Pa 0 E(\u00b5std(u))du (6)\nThus, the errors corresponding to different u\u2019s partly cancel each other. As a result, although |\u2206(\u03b8, \u03b8\u2032)| is upper-bounded by the worst-case error E(0,m, ) of the sequential test, the actual error is usually much smaller. For any given (\u03b8, \u03b8\u2032), \u2206 can be computed easily using 1-dimensional quadrature.\nFinally, we show that the error in the stationary distribution is bounded linearly by \u2206max = sup\u03b8,\u03b8\u2032 |\u2206(\u03b8, \u03b8\u2032)|. As noted above, \u2206max \u2264 E(0,m, ) but is usually much smaller. Let dv(P,Q) denote the total variation distance1 between two distributions, P and Q. If the transition kernel T0 of the exact Markov chain satisfies the\n1The total variation distance between two distributions P and Q, that are absolutely continuous w.r.t. measure \u2126, is defined\nas dv(P,Q) def = 1\n2 \u222b \u03b8\u2208\u0398 |fP (\u03b8)\u2212 fQ(\u03b8)|d\u2126(\u03b8) where fP and fQ are their respective densities (or Radon-Nikodym derivatives to\nbe more precise).\ncontraction condition dv(PT0,S0) \u2264 \u03b7dv(P,S0) for all probability distributions P with a constant \u03b7 \u2208 [0, 1), we can prove (see supplementary Section C) the following upper bound on the error in the stationary distribution:\nTheorem 1. The distance between the posterior distribution S0 and the stationary distribution of our approximate Markov chain S is upper bounded as:\ndv(S0,S ) \u2264 \u2206max 1\u2212 \u03b7"}, {"heading": "5.2 Optimal Sequential Test Design", "text": "We now briefly describe how to choose the parameters of the algorithm: , the error of a single test and m, the mini-batch size. A very simple strategy we recommend is to choose m \u2248 500 so that the Central Limit Theorem holds and keep as small as possible while maintaining a low average data usage. This rule works well in practice and is used in Experiments 6.1 - 6.4.\nThe more discerning practitioner can design an optimal test that minimizes the data used while keeping the error below a given tolerance. Ideally, we want to do this based on a tolerance on the error in the stationary distribution S . Unfortunately, this error depends on the contraction parameter, \u03b7, of the exact transition kernel, which is difficult to compute. A more practical choice is a bound on the error \u2206 in the acceptance probability, since the error in S increases linearly with \u2206. Since \u2206 is a function of (\u03b8, \u03b8\u2032), we can try to control the average value of \u2206 over the empirical distribution of (\u03b8, \u03b8\u2032) that would be encountered while simulating the Markov chain. Given a tolerance \u2206\u2217 on this average error, we can find the optimal m and by solving the following optimization problem (e.g. using grid search) to minimize the average data usage :\nmin m,\nE\u03b8,\u03b8\u2032 [Eu\u03c0\u0304(\u00b5std(u, \u03b8, \u03b8\u2032),m, )]\ns.t. E\u03b8,\u03b8\u2032 |\u2206(m, , \u03b8, \u03b8\u2032)| \u2264 \u2206\u2217 (7)\nIn the above equation, we estimate the average data usage, Eu[\u03c0\u0304], and the error in the acceptance probability, \u2206, using dynamic programming with one dimensional numerical quadrature on u. The empirical distribution for computing the expectation with respect to (\u03b8, \u03b8\u2032) can be obtained using a trial run of the Markov chain.\nWithout a trial run the best we can do is to control the worst case error E(0,m, ) (which is also an upperbound on \u2206) in each sequential test by solving the following minimization problem:\nmin m,\n\u03c0\u0304(0,m, ) s.t. E(0,m, ) \u2264 \u2206\u2217 (8)\nBut this leads to a very conservative design as the worst case error is usually much higher than the average case error. We illustrate the sequential design in Experiment 6.5. More details and a generalization of this method is given in supplementary Section D."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Random Walk - Logistic Regression", "text": "We first test our method using a random walk proposal q(\u03b8\u2032|\u03b8t) = N (\u03b8t, \u03c32RW ). Although the random walk proposal is not efficient, it is very useful for illustrating our algorithm because the proposal does not contain any information about the target distribution, unlike Langevin or Hamiltonian methods. So, the responsibility of converging to the correct distribution lies solely with the MH test. Also since q is symmetric, it does not appear in the MH test and we can use \u00b50 = 1 N log [u\u03c1(\u03b8t)/\u03c1(\u03b8\n\u2032)]. The target distribution in this experiment was the posterior for a logistic regression model trained on the MNIST dataset for classifying digits 7 vs 9. The dataset consisted of 12214 datapoints and we reduced the dimensionality from 784 to 50 using PCA. We chose a zero mean spherical Gaussian prior with precision = 10, and set \u03c3RW = 0.01.\nIn Fig. 2, we show how the logarithm of the risk in estimating the predictive mean, decreases as a function of wall clock time. The predictive mean of a test point x\u2217 is defined as Ep(\u03b8|XN )[p(x\u2217|\u03b8)]. To calculate the risk, we first estimate the true predictive mean using a long run of Hybrid Monte Carlo. Then, we compute multiple estimates of the predictive mean from our approximate algorithm and obtain the risk as the mean squared error in these estimates. We plot the average risk of 2037 datapoints in the test set. Since the risk R = B2 +V = B2 + \u03c3 2f T , we expect it to decrease as a function of time until the bias dominates the variance. The figure shows that even after collecting a lot of samples, the risk is still dominated by the variance and the minimum risk is obtained with > 0."}, {"heading": "6.2 Independent Component Analysis", "text": "Next, we use our algorithm to sample from the posterior distribution of the unmixing matrix in Independent Component Analysis (ICA) [Hyva\u0308rinen and Oja, 2000]. When using prewhitened data, the unmixing matrix W \u2208 RD\u00d7D is constrained to lie on the Stiefel manifold of orthonormal matrices. We choose a prior that is uniform over the manifold and zero elsewhere. We model the data as p(x|W ) =\n|det(W )| \u220fD j=1 [ 4 cosh2( 12w T j x) ]\u22121 where wj are the rows of W . Since the prior is zero outside the manifold, the same is true for the posterior. Therefore we use a random walk on the Stiefel manifold as a proposal distribution [Ouyang, 2008]. Since this is a symmetric proposal distribution, it does not appear in the MH test and we can use \u00b50 = 1 N log [u].\nTo perform a large scale experiment, we created a synthetic dataset by mixing 1.95 million samples of 4 sources: (a) a Classical music recording (b) street / traffic noise (c) & (d) 2 independent Gaussian sources. To measure the correctness of the sampler, we measure the risk in estimating I = Ep(W |X) [dA(W,W0)] where the test function dA is the Amari distance [Amari et al., 1996] and W0 is the true unmixing matrix. We computed the ground truth using a long run (T = 100K samples) of the exact MH algorithm. Then we ran each algorithm 10 times, each time for \u2248 6400 secs. We calculated the risk by averaging the squared error in the estimate from each Markov chain, over the 10 chains. This is shown in Fig. 3. Note that even after 6400 secs the variance dominates the bias, as evidenced by the still decreasing risk, except for the most biased algorithm with = 0.2. Also, the lowest risk at 6400 secs is obtained with = 0.1 and not the exact MH algorithm ( = 0). But we expect the exact algorithm to outperform all the approximate algorithms if we were to run for an infinite time."}, {"heading": "6.3 Variable selection in Logistic Regression", "text": "Now, we apply our MH test to variable selection in a logistic regression model using the reversible jump MCMC algorithm of Green [1995]. We use a model that is similar to the Bayesian LASSO model for linear regression described in Chen et al. [2011]. Specifically, given D input features, our parameter \u03b8 = {\u03b2, \u03b3} where \u03b2 is a vector of D regression coefficients and \u03b3 is a D dimensional binary vector that indicates whether\na particular feature is included in the model or not. The prior we choose for \u03b2 is p(\u03b2j |\u03b3, \u03bd) = 12\u03bd exp { \u2212 |\u03b2j |\u03bd } if \u03b3j = 1. If \u03b3j = 0, \u03b2j does not appear in the model. Here \u03bd is a shrinkage parameter that pushes \u03b2j towards 0, and we choose a prior p(\u03bd) \u221d 1/\u03bd. We also place a right truncated Poisson prior p(\u03b3|\u03bb) \u221d \u03bb k(\nD k\n) k!\non \u03b3 to control the size of the model, k = \u2211D j=1 \u03b3j We set \u03bb = 10\n\u221210 in this experiment. Denoting the likelihood of the data by lN (\u03b2, \u03b3), the posterior distribution after integrating out \u03bd is p(\u03b2, \u03b3|XN , yN , \u03bb) \u221d lN (\u03b2, \u03b3)\u2016\u03b2\u2016\u2212k1 \u03bbkB(k,D\u2212k+1) where B(., .) is the beta function. Instead of integrating out \u03bb, we use it as a parameter to control the size of the model. We use the same proposal distribution as in Chen et al. [2011] which is a mixture of 3 type of moves that are picked randomly in each iteration: an update move, a birth move and a death move. A detailed description is given in Supplementary Section E.\nWe applied this to the MiniBooNE dataset from the UCI machine learning repository[Bache and Lichman,\n2013]. Here the task is to classify electron neutrinos (signal) from muon neutrinos (background). There are 130,065 datapoints (28% in +ve class) with 50 features to which we add a constant feature of 1\u2019s. We randomly split the data into a training (80%) and testing (20%) set. To compute ground truth, we collected T=400K samples using the exact reversible jump algorithm ( = 0). Then, we ran the approximate MH algorithm with different values of for around 3500 seconds. We plot the risk in predictive mean of test data (estimated from 10 Markov chains) in Fig. 4. Again we see that the lowest risk is obtained with > 0.\nThe acceptance rates for the birth/death moves starts off at \u2248 20% but dies down to \u2248 2% once a good model is found. The acceptance rate for update moves is kept at \u2248 50%. The model also suffers from local minima. For the plot in Fig. 4, we started with only one variable and we ended up learning models with around 12 features, giving a classification error \u2248 15%. But, if we initialize the sampler with all features included and initialize \u03b2 to the MAP value, we learn models with around 45 features, but with a lower classification error \u2248 10%. Both the exact reversible jump algorithm and our approximate version suffer from this problem. We should bear this in mind when interpreting \u201cground truth\u201d. However, we have observed that when initialized with the same values, we obtain similar results with the approximate algorithm and the exact algorithm (see e.g. Fig. 13 in supplementary)."}, {"heading": "6.4 Stochastic Gradient Langevin Dynamics", "text": "Finally, we apply our method to Stochastic Gradient Langevin Dynamics[Welling and Teh, 2011]. In each iteration, we randomly draw a mini-batch Xn of size n, and propose:\n\u03b8\u2032 \u223c q(.|\u03b8,Xn) = N ( \u03b8 + \u03b1\n2 \u2207\u03b8\n{ N\nn \u2211 x\u2208Xn log p(x|\u03b8) + log \u03c1(\u03b8)\n} , \u03b1 ) (9)\nThe proposed state \u03b8\u2032 is always accepted (without conducting any MH test). Since the acceptance probability approaches 1 as we reduce \u03b1, the bias from not conducting the MH test can be kept under control by using \u03b1 \u2248 0. However, we have to use a reasonably large \u03b1 to keep the mixing rate high. This can be problematic for some distributions, because SGLD relies solely on gradients of the log density and it can be easily thrown off track by large gradients in low density regions, unless \u03b1 \u2248 0.\nAs an example, consider an L1-regularized linear regression model. Given a dataset {xi, yi}Ni=1 where xi are predictors and yi are targets, we use a Gaussian error model p(y|x, \u03b8) \u221d exp { \u2212\u03bb2 (y \u2212 \u03b8 Tx)2 }\nand choose a Laplacian prior for the parameters p(\u03b8) \u221d exp(\u2212\u03bb0\u2016\u03b8\u20161). For pedagogical reasons, we will restrict ourselves to a toy version of the problem where \u03b8 and x are one dimensional. We use a synthetic dataset with N = 10000 datapoints generated as yi = 0.5xi + \u03be where \u03be \u223c N (0, 1/3). We choose \u03bb = 3 and \u03bb0 = 4950, so that the prior is not washed out by the likelihood. The posterior density and the gradient of the log posterior are shown in figures 5(a) and 5(b) respectively.\nAn empirical histogram of samples obtained by running SGLD with \u03b1 = 5\u00d710\u22126 is shown in Fig. 5(c). The effect of omitting the MH test is quite severe here. When the sampler reaches the mode of the distribution, the Langevin noise occasionally throws it into the valley to the left, where the gradient is very high. This propels the sampler far off to the right, after which it takes a long time to find its way back to the mode. However, if we had used an MH accept-reject test, most of these troublesome jumps into the valley would be rejected because the density in the valley is much lower than that at the mode.\nTo apply an MH test, note that the SGLD proposal q(\u03b8\u2032|\u03b8) can be considered a mixture of component kernels q(\u03b8\u2032|\u03b8,Xn) corresponding to different mini-batches. The mixture kernel will satisfy detailed balance with respect to the posterior distribution if the MH test enforces detailed balance between the posterior and each of the component kernels q(\u03b8\u2032|\u03b8,Xn). Thus, we can use an MH test with \u00b50 = 1\nN log\n[ u \u03c1(\u03b8t)q(\u03b8\n\u2032|\u03b8t,Xn) \u03c1(\u03b8\u2032)q(\u03b8t|\u03b8\u2032,Xn)\n] .\nThe result of running SGLD (keeping \u03b1 = 5\u00d7 10\u22126 as before) corrected using our approximate MH test, with = 0.5, is shown in Fig. 5(d). As expected, the MH test rejects most troublesome jumps into the valley because the density in the valley is much lower than that at the mode. The stationary distribution is almost indistinguishable from the true posterior. Note that when = 0.5, a decision is always made in the first step (using just m = 500 datapoints) without querying additional data sequentially."}, {"heading": "6.5 Optimal Design of Sequential Tests", "text": "We illustrate the advantages of the optimal test design proposed in Section 5.2 by applying it to the ICA experiment described in Section 6.2. We consider two design methods: the \u2018average design\u2019 (Eqn. 7) and the \u2018worst-case design\u2019 (Eqn. 8). For the average design, we collected 100 samples of the Markov chain to approximate the expectation of the error over (\u03b8, \u03b8\u2032). We will call these samples the training set. The worst case design does not need the training set as it does not involve the distribution of (\u03b8, \u03b8\u2032). We compute the optimal m and using grid search, for different values of the target training error, for both designs. We then collect a new set of 100 samples (\u03b8, \u03b8\u2032) and measure the average error and data usage on this test set\n(Fig. 6). For the same target error on the training set, the worst-case design gives a conservative parameter setting that achieves a much smaller error on the test set. In contrast, the average design achieves a test error that is almost the same as the target error (Fig. 6(a)). Therefore, it uses much less data than the worst-case design (Fig. 6(b)).\nWe also analyze the performance in the case where we fix m = 600 and only change . This is a simple heuristic we recommended at the beginning of Section 5.2. Although this usually works well, using the optimal test design ensures the best possible performance. In this experiment, we see that when the error is large, the optimal design uses only half the data (Fig. 6(b)) used by the heuristic and is therefore twice as fast."}, {"heading": "7 Conclusions and Future Work", "text": "We have taken a first step towards cutting the computational budget of the Metropolis-Hastings MCMC algorithm, which takes O(N) likelihood evaluations to make the binary decision of accepting or rejecting a proposed sample. In our approach, we compute the probability that a new sample will be accepted based on a subset of the data. We increase the cardinality of the subset until a prescribed confidence level is reached. In the process we create a bias, which is more than compensated for by a reduction in variance due to the fact that we can draw more samples per unit time. Current MCMC procedures do not take these trade-offs into account. In this work we use a fixed decision threshold for accepting or rejecting a sample, but in theory a better algorithm can be obtained by adapting this threshold over time. An adaptive algorithm can tune bias and variance contributions in such a way that at every moment our risk (the sum of squared bias and variance) is as low as possible. We leave these extensions for future work."}, {"heading": "Acknowledgments", "text": "We thank Alex Ihler, Daniel Gillen, Sungjin Ahn and Babak Shahbaba for their valuable suggestions. This material is based upon work supported by the National Science Foundation under Grant No. 1216045."}, {"heading": "A Distribution of the test statistic", "text": "In the sequential test, we first compute the test statistic from a mini-batch of size m. If a decision cannot be made with this statistic, we keep increasing the mini-batch size by m datapoints until we reach a decision. This procedure is guaranteed to terminate as explained in Section 4.\nThe parameter controls the probability of making an error in a single test and not the complete sequential test. As the statistics across multiple tests are correlated with each other, we should first obtain the joint distribution of these statistics in order to estimate the error of the complete sequential test. Let l\u0304j and sl,j be the sample mean and standard deviation respectively, computed using the first j mini-batches. Notice that when the size of a mini-batch is large enough, e.g. n > 100, the central limit theorem applies, and also sl,j is an accurate estimate of the population standard deviation. Additionally, since the degrees of freedom is high, the t-statistic in Eqn. 5 reduces to a z-statistic. Therefore, it is reasonable to make the following assumptions:\nAssumption 1. The joint distribution of the sequence (l\u03041, l\u03042, . . . ) follows a multivariate normal distribution.\nAssumption 2. sl = \u03c3l, where \u03c3l = std({li})\nFig. 7 shows that when \u00b5 = \u00b50 the empirical marginal distribution of tj (or zj) is well fitted by both a standard student-t and a standard normal distribution.\nUnder these assumptions, we state and prove the following proposition about the joint distribution of\nthe z-statistic z = (z1, z2, . . . ), where zj def = (l\u0304j \u2212 \u00b50)/\u03c3l \u2248 tj , from different tests.\nProposition 2. Given Assumption 1 and 2, the sequence z follows a Gaussian random walk process:\nP (zj |z1, . . . , zj\u22121) = N (mj(zj\u22121), \u03c32z,j) (10)\nwhere\nmj(zj\u22121) = \u00b5std \u03c0j \u2212 \u03c0j\u22121 1\u2212 \u03c0j\u22121 1\u221a \u03c0j(1\u2212 \u03c0j)\n+ zj\u22121 \u221a \u03c0j\u22121 \u03c0j 1\u2212 \u03c0j 1\u2212 \u03c0j\u22121\n(11)\n\u03c32z,j = \u03c0j \u2212 \u03c0j\u22121 \u03c0j(1\u2212 \u03c0j\u22121)\n(12)\nwith \u00b5std = (\u00b5\u2212\u00b50)\n\u221a N\u22121\n\u03c3l being the standardized mean, and \u03c0j = jm/N the proportion of data in the first j\nmini-batches.\nProof of Proposition 2. Denote by xj the average of m l\u2019s in the j-th mini-batch. Taking into account the fact that the l\u2019s are drawn without replacement, we can compute the mean and covariance of the xj \u2019s as:\nE[xj ] = \u00b5 (13)\nCov(xi, xj) =  \u03c32l m ( 1\u2212 m\u2212 1 N \u2212 1 ) , i = j \u2212 \u03c3 2 l\nN \u2212 1 , i 6= j\n(14)\nIt is trivial to derive the expression for the mean. For the covariance, we first derive the covariance matrix\nof single data points as\nCov(lk, lk\u2032) = Ek,k\u2032 [lklk\u2032 ]\u2212 Ek[lk]Ek\u2032 [lk\u2032 ] if k = k\u2032\n= l2k \u2212 \u00b5 2 def= \u03c32l\nif k 6= k\u2032\n= Ek 6=k\u2032 [lklk\u2032 ]\u2212 \u00b52\n= 1 N(N \u2212 1) ( \u2211 k,k\u2032 lkl \u2032 k \u2212 \u2211 k l2k)\u2212 \u00b52\n= N\nN \u2212 1 \u00b52 \u2212 l2k N \u2212 1 \u2212 \u00b52\n= \u2212 \u03c3 2 l\nN \u2212 1 (15)\nNow, as xj can be written as a linear combination of the elements in j-th mini-batch as xj = 1 m1 T lj , the expression for covariance in Eqn. 14 follows immediately from:\nCov(xi, xj) = E[xixj ]\u2212 E[xi]E[xj ] = 1\nm2 1TCov(lil T j )1 (16)\nAccording to Assumption 1, the joint distribution of zj \u2019s is Gaussian because zj is a linear combination of l\u0304j \u2019s. It is however easier to derive the mean and covariance matrix of zj \u2019s by considering the vector z as a linear function of x: z = Q(x\u2212 \u00b501) with\nQ = \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 d1 d2 . . .\ndj\n\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1 1 1 ... ... . . .\n1 1 . . . 1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 (17) where\ndj =\n\u221a N \u2212 1\nj\u03c3x \u221a N\u2212jm jm\n(18)\nThe mean and covariance can be computed as E[z] = Q1(\u00b5 \u2212 \u00b50) and Cov(z) = QCov(x)QT and the conditional distribution P (zj |z1, . . . , zj\u22121) follows straightforwardly. We conclude the proof by plugging the definition of \u00b5std and \u03c0j into the distribution.\nFig. 8 shows the mean and 95% confidence interval of the random walk as a function of \u03c0 with a few realizations of the z sequence. Notice that as the proportion of observed data \u03c0j approaches 1, the mean of zj approaches infinity with a constant variance of 1. This is consistent with the fact that when we observe all the data, we will always make a correct decision.\nIt is also worth noting that given the standardized mean \u00b5std and \u03c0j , the process is independent of the actual size of a mini-batch m, population size N , or the variance of l\u2019s \u03c32l . Thus, Eqns. 11 and 12 apply even if we use a different size for each mini-batch. This formulation allows us to study general properties of the sequential test, independent of any particular dataset.\nApplying the individual tests \u03b4 \u2277 \u21d4 |zj | \u2277 \u03a6(1 \u2212 ) def = G at the j-th mini-batch corresponds to thresholding the absolute value of zj at \u03c0j with a bound G as shown in Fig. 9. Instead of m and , we will use \u03c01 = m/N and G as the parameters of the sequential test in the supplementary. The probability of incorrectly deciding \u00b5 < \u00b50 when \u00b5 \u2265 \u00b50 over the whole sequential test is computed as:\nE(\u00b5std, \u03c01, G) = J\u2211 j=1 P (zj < \u2212G, |zi| \u2264 G,\u2200i < j) (19)\nwhere J = d1/\u03c01e is the maximum number of tests. Similarly the probability of incorrectly deciding \u00b5 \u2265 \u00b50 when \u00b5 < \u00b50 can be computed similarly by replacing zj < \u2212G with zj > G in Eqn. 19. We can also compute the expected proportion of data that will be used in the sequential test as:\n\u03c0\u0304(\u00b5std, \u03c01, G) = Ez[\u03c0j\u2032 ]\n= J\u2211 j=1 \u03c0jP (|zj | > G, |zi| \u2264 G,\u2200i < j) (20)\nwhere j\u2032 denotes the time when the sequential test terminates. Eqn. 19 and 20 can be efficiently approximated together using a dynamic programming algorithm by discretizing the value of zj between [\u2212G,G]. The time complexity of this algorithm is O(L2J) where L is the number of discretized values.\nThe error and data usage as functions of \u00b5std are maximum in the worst case scenario when \u00b5std \u2192 0\u21d4 \u00b5\u2192 \u00b50. In this case we have:\nE(0, \u03c01, G) = lim \u00b5std\u21920 E(\u00b5std, \u03c01, G) = (1\u2212 P (j\u2032 = J))/2\ndef = Eworst(\u03c01, G) (21)\nFigs. 1 and 10 show respectively that the theoretical value of the error (E) and the average data usage (\u03c0\u0304) estimated using our dynamic programming algorithm match the simulated values. Also, note that both error and data usage drop off very fast as \u00b5 moves away from \u00b50."}, {"heading": "B Error in One Metropolis-Hastings Step", "text": "In the approximate Metropolis-Hasting test, one first draws a uniform random variable u, and then conducts the sequential test. As \u00b5std is a function of u (and \u00b5, \u03c3l, both of which depend on \u03b8 and \u03b8\n\u2032), E measures the probability that one will make a wrong decision conditioned on u. One might expect that the average error in the accept/reject step of M-H using sequential test is the expected value of E w.r.t. to the distribution of u. But in fact, we can usually achieve a significantly smaller error than a typical value of E . This is because with a varying u, there is some probability that \u00b5 > \u00b50(u) and also some probability that \u00b5 < \u00b50(u). Part of the error one will make given a fixed u can be canceled when we marginalize out the distribution of u. Following the definition of \u00b50(u) for M-H in Eqn. 2, we can compute the actual error in the acceptance probability as:\n\u2206(\u00b5(\u03b8, \u03b8\u2032), \u03c3l(\u03b8, \u03b8 \u2032), \u03c01, G) = Pa, \u2212 Pa\n= \u222b 1 0 P (\u00b5 > \u00b50(u))du\u2212 \u222b Pa 0 du\n= \u222b 1 Pa P (\u00b5 > \u00b50(u))du\u2212 \u222b Pa 0 (1\u2212 P (\u00b5 > \u00b50(u)))du\n= \u222b 1 Pa E(\u00b5\u2212 \u00b50(u))du\u2212 \u222b Pa 0 E(\u00b5\u2212 \u00b50(u))du (22)\nTherefore, it is often observed in experiments (see Fig. 11 for example) that when Pa \u2248 0.5, a typical value of \u00b5std(u) is close to 0, and the average value of the absolute error |E| can be large. But due to the cancellation of errors, the actual acceptance probability Pa, can approximate Pa very well. Fig. 12 shows the approximate Pa in one step of M-H. This result also suggests that making use of some (approximate) knowledge about \u00b5 and \u03c3l will help us obtain a much better estimate of the error than the worst case analysis in Eqn. 21."}, {"heading": "C Proof of Theorem 1", "text": "C.1 Upper Bound Based on One Step Error\nWe first prove a lemma that will be used for the proof of Theorem 1.\nLemma 3. Given two transition kernels, T0 and T , with respective stationary distributions, S0 and S , if T0 satisfies the following contraction condition with a constant \u03b7 \u2208 [0, 1) for all probability distributions P :\ndv(PT0,S0) \u2264 \u03b7dv(P,S0) (23)\nand the one step error between T0 and T is upper bounded uniformly with a constant \u2206 > 0 as:\ndv(PT0, PT ) \u2264 \u2206,\u2200P (24)\nthen the distance between S0 and S is bounded as:\ndv(S0,S ) \u2264 \u2206\n1\u2212 \u03b7 (25)\nProof. Consider a Markov chain with transition kernel T initialized from an arbitrary distribution P . Denote the distribution after t steps by P (t) def = PT t . At every time step, t \u2265 0, we apply the transition kernel T on P (t). According to the one step error bound in Eqn. 24, the distance between P (t+1) and the distribution obtained by applying T0 to P (t) is upper bounded as:\ndv(P (t+1), P (t)T0) = dv(P (t)T , P (t)T0) \u2264 \u2206 (26)\nFollowing the contraction condition of T0 in Eqn. 23, the distance of P (t)T0 from its stationary distribution S0 is less than P (t) as\ndv(P (t)T0,S0) \u2264 \u03b7dv(P (t),S0) (27)\nNow let us use the triangle inequality to combine Eqn. 26 and 27 to obtain an upper bounded for the distance between P (t+1) and S0:\ndv(P (t+1),S0) \u2264 dv(P (t+1), P (t)T0) + dv(P (t)T0,S0)\n\u2264 \u2206 + \u03b7dv(P (t),S0) (28)\nLet r < 1\u2212 \u03b7 be any positive constant and consider the ball B(S0, \u2206r ) def = {P : dv(P,S0) < \u2206r }. When P (t) is outside the ball, we have \u2206 \u2264 rdv(P (t), S). Plugging this into Eqn. 28, we can obtain a contraction condition for P (t) towards S0:\ndv(P (t+1),S0) \u2264 (r + \u03b7)dv(P (t),S0) (29)\nSo if the initial distribution P is outside the ball, the Markov chain will move monotonically into the ball within a finite number of steps. Let us denote the first time it enters the ball as tr. If the initial distribution is already inside the ball, we simply let tr = 0. We then show by induction that P\n(t) will stay inside the ball for all t \u2265 tr.\n1. At t = tr, P (t) \u2208 B(S0, \u2206r ) holds by the definition of tr.\n2. Assume P (t) \u2208 B(S0, \u2206r ) for some t \u2265 tr. Then, following Eqn. 28, we have\ndv(P (t+1),S0) \u2264 \u2206 + \u03b7\n\u2206 r = r + \u03b7 r \u2206 < \u2206 r\n=\u21d2 P (t+1) \u2208 B(S0, \u2206\nr ) (30)\nTherefore, P (t) \u2208 B(S0, \u2206r ) holds for all t \u2265 tr. Since P (t) converges to S , it follows that:\ndv(S ,S0) < \u2206\nr ,\u2200r < 1\u2212 \u03b7 (31)\nTaking the limit r \u2192 1\u2212 \u03b7, we prove the lemma:\ndv(S ,S) \u2264 \u2206\n1\u2212 \u03b7 (32)\nC.2 Proof of Theorem 1\nWe first derive an upper bound for the one step error of the approximate Metropolis-Hastings algorithm, and then use Lemma 3 to prove Theorem 1. The transition kernel of the exact Metropolis-Hastings algorithm can be written as\nT0(\u03b8, \u03b8\u2032) = Pa(\u03b8, \u03b8\u2032)q(\u03b8\u2032|\u03b8) + (1\u2212 Pa(\u03b8, \u03b8\u2032))\u03b4D(\u03b8\u2032 \u2212 \u03b8) (33)\nwhere \u03b4D is the Dirac delta function. For the approximate algorithm proposed in this paper, we use an approximate MH test with acceptance probability P\u0303a, (\u03b8, \u03b8 \u2032) where the error, \u2206Pa def = Pa, \u2212 Pa, is upper bounded as |\u2206Pa| \u2264 \u2206max. Now let us look at the distance between the distributions generated by one step of the exact kernel T0 and the approximate kernel T . For any P ,\u222b\n\u03b8\u2032 d\u2126(\u03b8\u2032)|(PT )(\u03b8\u2032)\u2212 (PT0)(\u03b8\u2032)|\n= \u222b \u03b8\u2032 d\u2126(\u03b8\u2032) \u2223\u2223\u2223\u2223\u222b \u03b8 dP (\u03b8)\u2206Pa(\u03b8, \u03b8 \u2032) (q(\u03b8\u2032|\u03b8)\u2212 \u03b4D(\u03b8\u2032 \u2212 \u03b8)) \u2223\u2223\u2223\u2223 \u2264 \u2206max \u222b \u03b8\u2032 d\u2126(\u03b8\u2032) \u2223\u2223\u2223\u2223\u222b \u03b8 dP (\u03b8)(q(\u03b8\u2032|\u03b8) + \u03b4D(\u03b8\u2032 \u2212 \u03b8)) \u2223\u2223\u2223\u2223\n= \u2206max \u222b \u03b8\u2032 d\u2126(\u03b8\u2032) (gQ(\u03b8 \u2032) + gP (\u03b8 \u2032)) = 2\u2206max (34)\nwhere gQ(\u03b8 \u2032) def = \u222b \u03b8 dP (\u03b8)q(\u03b8\u2032|\u03b8) is the density that would be obtained by applying one step of MetropolisHastings without rejection. So we get an upper bound for the total variation distance as\ndv(PT , PT0) = 1\n2 \u222b \u03b8\u2032 d\u2126(\u03b8\u2032)|PT \u2212 PT0| \u2264 \u2206max (35)\nApply Lemma 3 with \u2206 = \u2206max and we prove Theorem 1."}, {"heading": "D Optimal Sequential Test Design", "text": "It is possible to design optimal tests that minimize the amount of data used while keeping the error below a given tolerance. Ideally, we want to do this based on a tolerance on the error in the stationary distribution S . Unfortunately, this error depends on the contraction parameter, \u03b7, of the exact transition kernel, which is difficult to compute. A more practical choice is a bound \u2206max on the error in the acceptance probability, since the error in S increases linearly with \u2206max.\nGiven \u2206max, we want to minimize the average data usage \u03c0\u0304 over the parameters (or G) and/or m (or \u03c01) of the sequential test. Unfortunately, the error is a function of \u00b5 and \u03c3l which depend on \u03b8 and \u03b8\n\u2032, and we cannot afford to change the test design at every iteration.\nOne solution is to base the design on the upper bound of the worst case error in Eqn. 21 which does not rely on \u00b5std. But we have shown in Section B that this is a rather loose bound and will lead to a very conservative design that wastes the power of the sequential test. Therefore, we instead propose to design the\ntest by bounding the expectation of the error w.r.t. the distribution P (\u00b5, \u03c3l). This leads to the following optimization problem:\nmin \u03c01,G E\u00b5,\u03c3lEu\u03c0\u0304(\u00b5, \u03c3l, \u00b50(u), \u03c01, G)\ns.t. E\u00b5,\u03c3l |\u2206(\u00b5, \u03c3l, \u03c01, G)| \u2264 \u2206max (36)\nThe expectation w.r.t. u can be computed accurately using one dimensional quadrature. For the expectation w.r.t. \u00b5 and \u03c3l, we collect a set of parameter samples (\u03b8, \u03b8\n\u2032) during burn-in, compute the corresponding \u00b5 and \u03c3l for each sample, and use them to empirically estimate the expectation. We can also consider collecting samples periodically and adapting the sequential design over time. Once we obtain a set of samples {(\u00b5, \u03c3l)}, the optimization is carried out using grid search.\nWe have been using a constant bound G across all the individual tests. This is known as the Pocock design [Pocock, 1977]. A more flexible sequential design can be obtained by allowing G to change as a function of \u03c0. Wang and Tsiatis [1987] proposed a bound sequence Gj = G0\u03c0 0.5\u2212\u03b1 j where \u03b1 \u2208 [0.5, 1] is a free parameter. When \u03b1 = 0, it reduces to the Pocock design, and when \u03b1 = 1, it reduces to O\u2019Brien-Fleming design [O\u2019Brien and Fleming, 1979]. We can adopt this more general form in our optimization problem straightforwardly, and the grid search will now be conducted over three parameters, \u03c01, G0, and \u03b1."}, {"heading": "E Reversible Jump MCMC", "text": "We give a more detailed description of the different transition moves used in experiment 6.3. The update move is the usual MCMC move which involves changing the parameter vector \u03b2 without changing the model \u03b3. Specifically, we randomly pick an active component j : \u03b3j = 1 and set \u03b2j = \u03b2j+\u03b7 where \u03b7 \u223c N (0, \u03c3update). The birth move involves (for k < D) randomly picking an inactive component j : \u03b3j = 0 and setting \u03b3j = 1. We also propose a new value for \u03b2j \u223c N (0, \u03c3birth). The birth move is paired with a corresponding death move (for k > 1) which involves randomly picking an active component j : \u03b3j = 1 and setting \u03b3j = 0. The corresponding \u03b2j is discarded. The probabilities of picking these moves p(\u03b3 \u2192 \u03b3\u2032) is the same as in Chen et al. [2011]. The value of \u00b50 used in the MH test for different moves is given below. 1. Update move:\n\u00b50 = 1\nN log [ u \u2016\u03b2\u2016\u2212k1 \u2016\u03b2\u2032\u2016\u2212k1 ] (37)\n2. Birth move:\n\u00b50 = 1\nN log\n[ u \u2016\u03b2\u2016\u2212k1 p(\u03b3 \u2192 \u03b3\u2032)N (\u03b2j |0, \u03c3birth)(D \u2212 k)\n\u2016\u03b2\u2032\u2016\u2212(k+1)1 p(\u03b3\u2032 \u2192 \u03b3)\u03bbk\n] (38)\n2. Death move:\n\u00b50 = 1\nN \u00d7\nlog [ u \u2016\u03b2\u2016\u2212k1 p(\u03b3 \u2192 \u03b3\u2032) \u2016\u03b2\u2032\u2016\u2212(k\u22121)1 p(\u03b3\u2032 \u2192 \u03b3) \u03bb(k \u2212 1) N (\u03b2j |0, \u03c3birth)(D \u2212 k + 1) ] (39)\nWe used \u03c3update = 0.01 and \u03c3birth = 0.1 in this experiment. As mentioned in the main text, both the exact reversible jump algorithm and our approximate version suffer from local minima. But, when initialized with the same values, we obtain similar results with both algorithms. For example, we plot the marginal posterior probability of including a feature in the model, i.e. p(\u03b3j = 1|XN , yN , \u03bb) in figure 13."}, {"heading": "F Application to Gibbs Sampling", "text": "The same sequential testing method can be applied to the Gibbs sampling algorithm for discrete models. We study a model with binary variables in this paper while the extension to multi-valued variables is also possible. Consider running a Gibbs sampler on a probability distribution over D binary variables P (X1, . . . , XD). At every iteration, it updates one variable Xi using the following procedure:\n1. Compute the conditional probability:\nP (Xi = 1|x\u2212i) = P (Xi = 1, x\u2212i)\nP (Xi = 1, x\u2212i) + P (Xi = 0, x\u2212i) (40)\nwhere x\u2212i denotes the value of all variables other than the i th one.\n2. Draw u \u223c Uniform[0, 1]. If u < P (Xi = 1|x\u2212i) set Xi = 1, otherwise set Xi = 0.\nThe condition in step 2 is equivalent to checking:\nlog u\nlog(1\u2212 u) <\nlogP (Xi = 1, x\u2212i) logP (Xi = 0, x\u2212i) (41)\nWhen the joint distribution is expensive to compute but can be represented as a product over multiple terms, P (X) = \u220fN n=1 fn(X), we can apply our sequential test to speed up the Gibbs sampling algorithm. In this case the variable \u00b50 and \u00b5 is given by\n\u00b50 = 1\nN\nlog u\nlog(1\u2212 u) (42)\n\u00b5 = 1\nN N\u2211 n=1 log fn(Xi = 1, x\u2212i) fn(Xi = 0, x\u2212i) (43)\nSimilar to the Metropolis-Hastings algorithm, given an upper bound in the error of the approximate conditional probability\n\u2206max = max i,x\u2212i\n|P (Xi is assigned 1|x\u2212i)\u2212 P (Xi = 1|x\u2212i)|\nwe can prove the following theorem:\nTheorem 4. For a Gibbs sampler with a Dobrushin coefficient \u03b7 \u2208 [0, 1) [Bre\u0301maud, 1999, \u00a77.6.2], the distance between the stationary distribution and that of the approximate Gibbs sampler S is upper bounded by\ndv(S0, S ) \u2264 \u2206max 1\u2212 \u03b7\nProof. The proof is similar to that of Theorem 1. We first obtain an upper bound for the one step error and then plug it into Lemma 3.\nThe exact transition kernel of the Gibbs sampler for variable Xi can be represented by a matrix T0,i of size 2D \u00d7 2D: T0,i(x, y) = {\n0 if x\u2212i 6= y\u2212i P (Yi = yi|y\u2212i) otherwise\n(44)\nwhere 1 \u2264 i \u2264 N, x, y \u2208 {0, 1}D. The approximate transition kernel T ,i can be represented similarly as T ,i(x, y) = {\n0 if x\u2212i 6= y\u2212i P (Yi = yi|y\u2212i) otherwise\n(45)\nwhere P is the approximate conditional distribution. Define the approximation error \u2206Ti(x, y) def = T ,i(x, y)\u2212 T0,i(x, y). We know that \u2206Ti(x, y) = 0 if y\u2212i 6= x\u2212i and it is upper bounded by \u2206max from the premise of Theorem 4.\nNotice that the total variation distance reduces to a half of the L1 distance for discrete distributions. For any distribution P , the one step error is bounded as\ndv(PT ,i, PT0,i) = 1\n2 \u2016PT ,i \u2212 PT0,i\u20161\n= 1\n2 \u2211 y \u2223\u2223\u2223\u2223\u2223\u2211 x P (x)\u2206T (x, y) \u2223\u2223\u2223\u2223\u2223 = 1\n2 \u2211 y \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\nxi\u2208{0,1}\nP (xi, y\u2212i)\u2206P (xi|y\u2212i) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 1\n2 \u2206max \u2211 y |P (Y\u2212i = y\u2212i)|\n= \u2206max (46)\nFor a Gibbs sampling algorithm, we have the contraction condition [Bre\u0301maud, 1999, \u00a77.6.2]:\ndv(PT , S) \u2264 \u03b7dv(P, S) (47)\nPlug \u2206 = \u2206max and \u03b7 into Lemma 3 and we obtain the conclusion.\nF.1 Experiments on Markov Random Fields\nWe illustrate the performance of our approximate Gibbs sampling algorithm on a synthetic Markov Random Field. The model under consideration has D = 100 binary variables and they are densely connected by potential functions of three variables \u03c8i,j,k(Xi, Xj , Xk),\u2200i 6= j 6= k. There are D(D \u2212 1)(D \u2212 2)/6 potential functions in total (we assume potential functions with permuted indices in the argument are the same potential function), and every function has 23 = 8 values. The entries in the potential function tables are drawn randomly from a log-normal distribution, log\u03c8i,j,k(Xi, Xj , Xk) \u223c N (0, 0.02). To draw a Gibbs sample for one variable Xi we have to compute (D \u2212 1)(D \u2212 2)/2 = 4851 pairs of potential functions as\nP (Xi = 1|x\u2212i) P (Xi = 0|x\u2212i) = \u220f i 6=j 6=k \u03c8i,j,k(Xi = 1, xj , xk)\u220f i 6=j 6=k \u03c8i,j,k(Xi = 0, xj , xk)\n(48)\nThe approximate methods use a mini-batches of 500 pairs of potential functions at a time. We compare the exact Gibbs sampling algorithm with approximate versions with \u2208 {0.01, 0.05, 0.1, 0.15, 0.2, 0.25}.\nTo measure the performance in approximating P (X) with samples xt, the ideal metric would be a distance between the empirical joint distribution and P . Since it is impossible to store all the 2100 probabilities, we instead repeatedly draw M = 1600 subsets of 5 variables, {sm}Mm=1, sm \u2282 {1, . . . , D}, |sm| = 5, and compute the average L1 distance of the joint distribution on these subsets between the empirical distribution and P :\nError = 1\nM \u2211 sm \u2016P\u0302 (Xsm)\u2212 P (Xsm)\u20161 (49)\nThe true P is estimated by running exact Gibbs chains for a long time. We show the empirical conditional probability obtained by our approximate algorithms (percentage of Xi being assigned 1) for different in Fig. 14. It tends to underestimate large probabilities and overestimate on the other end. When = 0.01, the observed maximum error is within 0.01.\nFig. 15 shows the error for different as a function of the running time. For small , we use fewer mini-batches per iteration and thus generate more samples in the same amount of time than the exact Gibbs sampler. So the error decays faster in the beginning. As more samples are collected the variance is reduced. We see that these plots converge towards their bias floor while the exact Gibbs sampler out-performs all the approximate methods at around 1000 seconds."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "A new learning algorithm for blind signal separation", "author": ["S.-i. Amari", "A. Cichocki", "H.H. Yang"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Amari et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Amari et al\\.", "year": 1996}, {"title": "The pseudo-marginal approach for efficient Monte Carlo computations", "author": ["C. Andrieu", "G.O. Roberts"], "venue": "The Annals of Statistics,", "citeRegEx": "Andrieu and Roberts.,? \\Q2009\\E", "shortCiteRegEx": "Andrieu and Roberts.", "year": 2009}, {"title": "Markov chains: Gibbs fields, Monte Carlo simulation, and queues, volume 31", "author": ["P. Br\u00e9maud"], "venue": null, "citeRegEx": "Br\u00e9maud.,? \\Q1999\\E", "shortCiteRegEx": "Br\u00e9maud.", "year": 1999}, {"title": "A Bayesian Lasso via reversible-jump MCMC", "author": ["X. Chen", "Z. Jane Wang", "M.J. McKeown"], "venue": "Signal Processing,", "citeRegEx": "Chen et al\\.,? \\Q1920\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1920}, {"title": "Particle filters for partially observed diffusions", "author": ["P. Fearnhead", "O. Papaspiliopoulos", "G.O. Roberts"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Fearnhead et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fearnhead et al\\.", "year": 2008}, {"title": "Markov chain Monte Carlo: stochastic simulation for Bayesian inference, volume 68", "author": ["D. Gamerman", "H.F. Lopes"], "venue": null, "citeRegEx": "Gamerman and Lopes.,? \\Q2006\\E", "shortCiteRegEx": "Gamerman and Lopes.", "year": 2006}, {"title": "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination", "author": ["P.J. Green"], "venue": null, "citeRegEx": "Green.,? \\Q1995\\E", "shortCiteRegEx": "Green.", "year": 1995}, {"title": "Monte Carlo sampling methods using Markov chains and their applications", "author": ["W.K. Hastings"], "venue": null, "citeRegEx": "Hastings.,? \\Q1970\\E", "shortCiteRegEx": "Hastings.", "year": 1970}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks,", "citeRegEx": "Hyv\u00e4rinen and Oja.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Oja.", "year": 2000}, {"title": "A noisy Monte Carlo algorithm", "author": ["L. Lin", "K. Liu", "J. Sloan"], "venue": "Physical Review D,", "citeRegEx": "Lin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2000}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The journal of chemical physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "A multiple testing procedure for clinical trials", "author": ["P.C. O\u2019Brien", "T.R. Fleming"], "venue": null, "citeRegEx": "O.Brien and Fleming.,? \\Q1979\\E", "shortCiteRegEx": "O.Brien and Fleming.", "year": 1979}, {"title": "Bayesian Additive Regression Kernels", "author": ["Z. Ouyang"], "venue": "PhD thesis, Duke University,", "citeRegEx": "Ouyang.,? \\Q2008\\E", "shortCiteRegEx": "Ouyang.", "year": 2008}, {"title": "Group sequential methods in the design and analysis of clinical trials", "author": ["S.J. Pocock"], "venue": null, "citeRegEx": "Pocock.,? \\Q1977\\E", "shortCiteRegEx": "Pocock.", "year": 1977}, {"title": "Monte Carlo MCMC: efficient inference by approximate sampling", "author": ["S. Singh", "M. Wick", "A. McCallum"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Singh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2012}, {"title": "Approximately optimal one-parameter boundaries for group sequential trials", "author": ["S.K. Wang", "A.A. Tsiatis"], "venue": null, "citeRegEx": "Wang and Tsiatis.,? \\Q1987\\E", "shortCiteRegEx": "Wang and Tsiatis.", "year": 1987}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y. Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Welling and Teh.,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "We only know of one algorithm that would also adhere to this strategy: stochastic gradient Langevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al.", "startOffset": 109, "endOffset": 132}, {"referenceID": 0, "context": "We only know of one algorithm that would also adhere to this strategy: stochastic gradient Langevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al., 2012].", "startOffset": 186, "endOffset": 204}, {"referenceID": 14, "context": "related method was used in Singh et al. [2012], where the factors of a graphical model are sub-sampled to compute fixed-width confidence intervals for the log-likelihood in the MH test.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "It is well known that V \u2248 \u03c3 f,S0\u03c4/T , where \u03c3 2 f,S0 is the variance of f with respect to S0 and \u03c4 is the integrated auto-correlation time, which is a measure of the interval between independent samples [Gamerman and Lopes, 2006].", "startOffset": 203, "endOffset": 229}, {"referenceID": 17, "context": "It is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient Langevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn et al.", "startOffset": 131, "endOffset": 154}, {"referenceID": 0, "context": "It is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient Langevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn et al., 2012].", "startOffset": 201, "endOffset": 219}, {"referenceID": 2, "context": "For example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed by Lin et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 5, "context": "However, unbiased estimators of the likelihood that can be computed from mini-batches of data, such as the Poisson estimator [Fearnhead et al., 2008] or the Kennedy-Bhanot estimator [Lin et al.", "startOffset": 125, "endOffset": 149}, {"referenceID": 10, "context": ", 2008] or the Kennedy-Bhanot estimator [Lin et al., 2000] have very high variance for large datasets.", "startOffset": 40, "endOffset": 58}, {"referenceID": 2, "context": "For example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed by Lin et al. [2000] provide a way to conduct exact accept/reject tests using unbiased estimators of the likelihood.", "startOffset": 46, "endOffset": 119}, {"referenceID": 9, "context": "2 Independent Component Analysis Next, we use our algorithm to sample from the posterior distribution of the unmixing matrix in Independent Component Analysis (ICA) [Hyv\u00e4rinen and Oja, 2000].", "startOffset": 165, "endOffset": 190}, {"referenceID": 13, "context": "Therefore we use a random walk on the Stiefel manifold as a proposal distribution [Ouyang, 2008].", "startOffset": 82, "endOffset": 96}, {"referenceID": 1, "context": "To measure the correctness of the sampler, we measure the risk in estimating I = Ep(W |X) [dA(W,W0)] where the test function dA is the Amari distance [Amari et al., 1996] and W0 is the true unmixing matrix.", "startOffset": 150, "endOffset": 170}, {"referenceID": 6, "context": "3 Variable selection in Logistic Regression Now, we apply our MH test to variable selection in a logistic regression model using the reversible jump MCMC algorithm of Green [1995]. We use a model that is similar to the Bayesian LASSO model for linear regression described in Chen et al.", "startOffset": 167, "endOffset": 180}, {"referenceID": 4, "context": "We use a model that is similar to the Bayesian LASSO model for linear regression described in Chen et al. [2011]. Specifically, given D input features, our parameter \u03b8 = {\u03b2, \u03b3} where \u03b2 is a vector of D regression coefficients and \u03b3 is a D dimensional binary vector that indicates whether a particular feature is included in the model or not.", "startOffset": 94, "endOffset": 113}, {"referenceID": 4, "context": "We use the same proposal distribution as in Chen et al. [2011] which is a mixture of 3 type of moves that are picked randomly in each iteration: an update move, a birth move and a death move.", "startOffset": 44, "endOffset": 63}, {"referenceID": 17, "context": "4 Stochastic Gradient Langevin Dynamics Finally, we apply our method to Stochastic Gradient Langevin Dynamics[Welling and Teh, 2011].", "startOffset": 109, "endOffset": 132}, {"referenceID": 14, "context": "This is known as the Pocock design [Pocock, 1977].", "startOffset": 35, "endOffset": 49}, {"referenceID": 12, "context": "When \u03b1 = 0, it reduces to the Pocock design, and when \u03b1 = 1, it reduces to O\u2019Brien-Fleming design [O\u2019Brien and Fleming, 1979].", "startOffset": 98, "endOffset": 125}, {"referenceID": 13, "context": "This is known as the Pocock design [Pocock, 1977]. A more flexible sequential design can be obtained by allowing G to change as a function of \u03c0. Wang and Tsiatis [1987] proposed a bound sequence Gj = G0\u03c0 0.", "startOffset": 21, "endOffset": 169}, {"referenceID": 4, "context": "The probabilities of picking these moves p(\u03b3 \u2192 \u03b3\u2032) is the same as in Chen et al. [2011]. The value of \u03bc0 used in the MH test for different moves is given below.", "startOffset": 69, "endOffset": 88}], "year": 2014, "abstractText": "Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.", "creator": "LaTeX with hyperref package"}}}