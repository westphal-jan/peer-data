{"id": "1305.1363", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2013", "title": "One-Pass AUC Optimization", "abstract": "AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data.\n\n\nThe data sets used in this analysis are very large and do not fit into any of our data sets and cannot be used as a test case to test a model. As this analysis is not supported by any standard, we would like to have the data set of these data sets in RAM in the same location and use the same configuration, which we can find in the same location (as well as in different memory spaces).", "histories": [["v1", "Tue, 7 May 2013 00:30:32 GMT  (104kb)", "https://arxiv.org/abs/1305.1363v1", "Proceeding of 30th International Conference on Machine Learning"], ["v2", "Thu, 16 May 2013 13:24:37 GMT  (107kb)", "http://arxiv.org/abs/1305.1363v2", "Proceeding of 30th International Conference on Machine Learning"]], "COMMENTS": "Proceeding of 30th International Conference on Machine Learning", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wei gao", "rong jin", "shenghuo zhu", "zhi-hua zhou"], "accepted": true, "id": "1305.1363"}, "pdf": {"name": "1305.1363.pdf", "metadata": {"source": "CRF", "title": "One-Pass AUC Optimization", "authors": ["Wei Gao", "Rong Jin", "Shenghuo Zhu", "Zhi-Hua Zhou"], "emails": ["gaow@lamda.nju.edu.cn", "rongjin@cse.msu.edu", "zsh@nec-labs.com", "zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 5.\n13 63\nv2 [\nKeywords: AUC optimization, learning to rank, large-scale learning, random projection, square loss"}, {"heading": "1. Introduction", "text": "AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011). Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).\nIn this work, we focus on AUC optimization that requires only one pass of training examples. This is particularly important for applications involving big data or streaming\nc\u00a9 Gao, Jin, Zhu and Zhou.\ndata in which a large volume of data come in a short time period, making it infeasible to store the entire data set in memory before an optimization procedure is applied. Although many online learning algorithms have been developed to find the optimal solution of some performance measures by only scanning the training data once (Cesa-Bianchi and Lugosi, 2006), few effort addresses one-pass AUC optimization.\nUnlike the classical classification and regression problems where the loss function can be calculated on a single training example, AUC is measured by the losses defined over pairs of instances from different classes, making it challenging to develop algorithms for one-pass optimization. An online AUC optimization algorithm was proposed very recently by Zhao et al. (2011). It is based on the idea of reservoir sampling, and achieves a solid regret bound by only storing \u221a T instances, where T is the number of training examples. Ideally, for one-pass approaches, it is crucial that the storage required by the learning process should be independent from the amount of training data, because it is often quite difficult to expect how many data will be received in those applications.\nIn this work, we propose a regression-based algorithm for one-pass AUC optimization in which a square loss is used to measure the ranking error between two instances from different classes. The main advantage of using the square loss lies in the fact that it only needs to store the first and second-order statistics for the received training examples. Consequently, the storage requirement is reduced to O(d2), where d is the dimension of data, independent from the number of training examples. To deal with high-dimensional data, we develop a randomized algorithm that approximates the covariance matrix of d \u00d7 d by a low-rank matrix. We show, both theoretically and empirically, the effectiveness of our proposal algorithm by comparing to state-of-the-art algorithms for AUC optimization.\nSection 2 introduces some preliminaries. Sections 3 proposes the OPAUC (One Pass AUC) framework, and Section 4 provides theoretical analysis and Section 5 presents detailed proofs. Section 6 summaries our experimental results. Section 7 concludes with future work."}, {"heading": "2. Preliminaries", "text": "We denote by X \u2208 Rd an instance space and Y = {+1,\u22121} the label set, and let D denote an unknown (underlying) distribution over X \u00d7Y. A training sample of n+ positive instances and n\u2212 negative ones\nS = {(x+1 ,+1), (x+2 ,+1), . . . , (x+n+ ,+1), (x\u22121 ,\u22121), (x\u22122 ,\u22121), . . . , (x\u2212n\u2212 ,\u22121)}\nis drawn identically and independently according to distribution D, where we do not fix n+ and n\u2212 before the training sample is chosen. Let f : X \u2192 R be a real-valued function. Then, the AUC of function f on the sample S is defined as\nn+\u2211\ni=1\nn\u2212\u2211\nj=1\nI[f(x+i ) > f(x \u2212 j )] + 1 2 I[f(x + i ) = f(x \u2212 j )]\nn+n\u2212\nwhere I[\u00b7] is the indicator function which returns 1 if the argument is true and 0 otherwise. Direct optimization of AUC often leads to an NP-hard problem as it can be cast into a combinatorial optimization problem. In practice, it is approximated by a convex optimiza-\ntion problem that minimizes the following objective function\nL(w) = \u03bb 2 |w|2 +\nn+\u2211\ni=1\nn\u2212\u2211\nj=1\n\u2113 ( w\u22a4(x+i \u2212 x\u2212j ) )\n2n+n\u2212 (1)\nwhere \u2113 is a convex loss function and \u03bb is the regularization parameter that controls the model complexity. Notice that each loss term \u2113(w\u22a4(x+i \u2212 x\u2212j )) involves two instances from different classes; therefore, it is difficult to extend online learning algorithms for one-pass AUC optimization without storing all the training instances. Zhao et al. (2011) addressed this challenge by exploiting the reservoir sampling technique."}, {"heading": "3. The OPAUC Approach", "text": "To address the challenge of one-pass AUC optimization, we propose to use the square loss in Eq. (1), that is,\nL(w) = \u03bb 2 |w|2 +\nn+\u2211\ni=1\nn\u2212\u2211\nj=1\n(1\u2212w\u22a4(x+i \u2212 x\u2212j ))2 2n+n\u2212 . (2)\nThe main advantage of using the square loss lies in the fact that it is sufficient to store the first and second-order statistics of training examples for optimization, leading to a memory requirement of O(d2), which is independent from the number of training examples. Another advantage is that the square loss is consistent with AUC, as will be shown by Theorem 1 (Section 4). In contrast, loss functions such as hinge loss are proven to be inconsistent with AUC (Gao and Zhou, 2012).\nAs aforementioned, the classical online setting cannot be applied to one-pass AUC optimization because, even if the optimization problem of Eq. (2) has a closed form, it requires going through the training examples multiple times. To address this challenge, we modify the overall loss L(w) in Eq. (2) (with a little variation) as a sum of losses for individual training instance \u2211T t=1 Lt(w), where\nLt(w) = \u03bb\n2 |w|2 +\n\u2211t\u22121 i=1 I[yi 6= yt](1\u2212 yt(xt \u2212 xi)\u22a4w)2\n2|{i \u2208 [t\u2212 1] : yiyt = \u22121}| for sequence St = {(x1, y1), . . . , (xt, yt)}. It is noteworthy that Lt(w) is an unbiased estimation to L(w) for i.i.d. sequence St. For notational simplicity, we denote by X+t and X\u2212t the sets of positive and negative instances in the sequence St, respectively, and we further denote by T+t and T \u2212 t their respective cardinalities. Also, we set Lt(w) = 0 for T+t T\u2212t = 0.\nIf yt = 1, we calculate the gradient as\n\u2207Lt(w) = \u03bbw + xtx\u22a4t w \u2212 xt + \u2211\ni : yi=\u22121\nxi + (xix \u22a4 i \u2212 xix\u22a4t \u2212 xtx\u22a4i )w\nT\u2212t . (3)\nIt is easy to observe that\nc\u2212t = \u2211\ni : yi=\u22121\nxi\nT\u2212t and S\u2212t =\n\u2211\ni : yi=\u22121\nxix \u22a4 i \u2212 c\u2212t [c\u2212t ]\u22a4\nT\u2212t\nAlgorithm 1 The OPAUC Algorithm Input: The regularization parameter \u03bb > 0 and stepsizes {\u03b7t}Tt=1. Initialization: Set T+0 = T \u2212 0 = 0, c + 0 = c \u2212 0 = 0, w0 = 0 and \u0393 + 0 = \u0393 \u2212 0 = [0]d\u00d7u for some u > 0\n1: for t = 1, 2, . . . , T do 2: Receive a training example (xt, yt) 3: if yt = +1 then 4: T+t = T + t\u22121 + 1 and T \u2212 t = T \u2212 t\u22121; 5: c+t = c + t\u22121 +\n1 T+t (xt \u2212 c+t\u22121) and c\u2212t = c\u2212t\u22121; 6: Update \u0393+t and \u0393 \u2212 t = \u0393 \u2212 t\u22121; 7: Calculate the gradient g\u0302t(wt\u22121) 8: else 9: T\u2212t = T \u2212 t\u22121 + 1 and T + t = T + t\u22121;\n10: c\u2212t = c \u2212 t\u22121 + 1 T\u2212t (xt \u2212 c\u2212t\u22121) and c+t = c+t\u22121; 11: Update \u0393\u2212t and \u0393 + t = \u0393 + t\u22121; 12: Calculate the gradient g\u0302t(wt\u22121) 13: end if 14: wt = wt\u22121 \u2212 \u03b7tg\u0302t(wt\u22121) 15: end for\ncorrespond to the mean and covariance matrix of negative class, respectively; thus, Eq. (3) can be further simplified as\n\u2207Lt(w) = \u03bbw \u2212 xt + c\u2212t + (xt \u2212 c\u2212t )(xt \u2212 c\u2212t )\u22a4w + S\u2212t w. (4) In a similar manner, we calculate the following gradient for yt = \u22121:\n\u2207Lt(w) = \u03bbw + xt \u2212 c+t + (xt \u2212 c+t )(xt \u2212 c+t )\u22a4w + S+t w (5) where\nc+t = \u2211\ni : yi=1\nxi\nT+t and S+t =\n\u2211\ni : yi=1\nxix \u22a4 i \u2212 c+t [c+t ]\u22a4\nT+t\nare the covariance matrix and mean of positive class, respectively. The storage cost for keeping the class means (c+t and c \u2212 t ) and covariance matrices (S + t\u22121 and S\u2212t\u22121) is O(d 2). Once we get the gradient \u2207Lt(w), by theory of stochastic gradient descent, the solution can be updated by\nwt+1 = wt \u2212 \u03b7t\u2207Lt(wt) where \u03b7t is the stepsize for the t-th iteration.\nAlgorithm 1 highlights the key steps of the proposed algorithm. We initialize \u0393\u22120 = \u0393+0 = [0]d\u00d7d, where u = d. At each iteration, we set \u0393 + t = S + t and \u0393 \u2212 t = S \u2212 t , and update \u0393+t (Line 6) and \u0393 \u2212 t (Line 11), respectively, by using the following equations\n\u0393+t = \u0393 + t\u22121 +\nxtx\u22a4t \u2212\u0393 + t\u22121\nT+t + c+t\u22121[c + t\u22121] \u22a4 \u2212 c+t [c+t ]\u22a4,\n\u0393\u2212t = \u0393 \u2212 t\u22121 +\nxtx\u22a4t \u2212\u0393 \u2212 t\u22121\nT\u2212t + c\u2212t\u22121[c \u2212 t\u22121] \u22a4 \u2212 c\u2212t [c\u2212t ]\u22a4.\nFinally, the stochastic gradient g\u0302t(wt\u22121) of Lines 7 and 12 in Algorithm 1 are given by \u2207Lt(wt\u22121) that are calculated by Eqs. (4) and (5), respectively. Dealing with High-Dimensional Data. One limitation of the approach in Algorithm 1 is that the storage cost of the two covariance matrices S+t and S \u2212 t is O(d\n2), making it unsuitable for high-dimensional data. We tackle this by developing a randomized algorithm that approximates the covariance matrices by low-rank matrices. We are motivated by the observation that S+t and S \u2212 t can be written, respectively, as\nS+t = 1 T+t ( X+t \u2212 c+t 1\u22a4T+t ) IT+t ( X+t \u2212 c+t 1T+t )\u22a4 ,\nS\u2212t = 1\nT\u2212t ( X\u2212t \u2212 c\u2212t 1\u22a4T\u2212t ) IT\u2212t ( X\u2212t \u2212 c\u2212t 1T\u2212t )\u22a4 ,\nwhere It is an identity matrix of size t\u00d7t and 1t is an all-one vector of size t. To approximate S+t and S \u2212 t , we approximate the identify matrix It by a matrix of rank \u03c4 \u226a d. To this end, we randomly sample ri \u2208 R\u03c4 , i = 1, . . . , t from a Gaussian distribution N (0, I\u03c4 ), and approximate It by RtR \u22a4 t , where Rt = 1 \u03c4 (r1, . . . , rt)\n\u22a4 \u2208 Rt\u00d7\u03c4 . We further divide Rt into two matrices where R+t \u2208 RT + t \u00d7\u03c4 and R\u2212t \u2208 RT \u2212 t \u00d7\u03c4 that contain the subset of the rows in Rt corresponding to all the positive and negative instances received before the t-th iteration, respectively. Therefore, the covariance matrices S+t and S \u2212 t can be approximated, respectively, by\nS\u0302+t = 1\nT+t Z+t [Z + t ]\n\u22a4 \u2212 c\u0302+t\u22121[c\u0302+t\u22121]\u22a4 and S\u0302\u2212t = 1\nT\u2212t Z\u2212t [Z \u2212 t ] \u22a4 \u2212 c\u0302\u2212t\u22121[c\u0302\u2212t\u22121]\u22a4\nwhere\nZ+t = X + t R + t , c\u0302 + t = c + t 1 \u22a4 T+t R+t /T + t Z\u2212t = X \u2212 t R \u2212 t , c\u0302 \u2212 t = c \u2212 t 1\n\u22a4 T\u2212t R\u2212t /T \u2212 t .\nBased on approximate covariance matrix S\u0302\u00b1t , the approximation algorithm essentially tries to minimize \u2211T t=1 L\u0302t(w), where\nL\u0302t(w) = w\u22a4(c\u2212t\u22121 \u2212 xt) + 1\n2 (1 +w\u22a4S\u0302\u2212t w) +\n\u03bb 2 |w|2 + 1 2 w\u22a4(xt \u2212 c\u2212t\u22121)(xt \u2212 c\u2212t\u22121)\u22a4w (6)\nif yt = 1; otherwise,\nL\u0302t(w) = w\u22a4(xt \u2212 c+t\u22121) + 1\n2 (1 +w\u22a4S\u0302+t w) +\n\u03bb 2 |w|2 + 1 2 w\u22a4(xt \u2212 c+t\u22121)(xt \u2212 c+t\u22121)\u22a4w. (7)\nFurther, we have the following recursive formulas:\nZ+t = Z + t\u22121 + xtr \u22a4 t I[yt = +1]/\n\u221a m, (8)\nZ\u2212t = Z \u2212 t\u22121 + xtr \u22a4 t I[yt = \u22121]/\n\u221a m. (9)\nIt is important to notice that we do not need to calculate and store the approximate covariance matrices S\u0302+t and S\u0302 \u2212 t explicitly. Instead, we only need to maintain matrices Z + t and\nZ\u2212t in memory. This is because the stochastic gradient g\u0302t(w) based on the approximate covariance matrices can be computed directly from Z+t and Z \u2212 t . More specifically, g\u0302t(w) is computed as\ng\u0302t(w) = c \u2212 t\u22121\u2212xt+\u03bbw+(xt\u2212c\u2212t\u22121)(xt\u2212c\u2212t\u22121)\u22a4w+ ( Z\u2212t [Z \u2212 t ] \u22a4/T\u2212t \u2212 c\u0302\u2212t\u22121[c\u0302\u2212t\u22121]\u22a4 ) w (10)\nfor yt = 1; otherwise\ng\u0302t(w) = xt\u2212c+t\u22121+\u03bbw+(xt\u2212c+t\u22121)(xt\u2212c+t\u22121)\u22a4w+ ( Z+t [Z + t ] \u22a4/T+t \u2212 c\u0302+t\u22121[c\u0302+t\u22121]\u22a4 ) w. (11)\nWe require a memory of O(\u03c4d) instead of O(d2) to calculate g\u0302t(w) by using the trick A[A]\u22a4w = A([A]\u22a4w), where A \u2208 Rd\u00d71 or Rd\u00d7\u03c4 .\nTo implement the approximate approach, we initialize \u0393\u22120 = \u0393 + 0 = [0]d\u00d7\u03c4 in Algorithm 1,\nwhere u = \u03c4 . At each iteration, we set \u0393+t = Z + t and \u0393 \u2212 t = Z \u2212 t , and compute the gradient g\u0302t(wt\u22121) of Lines 7 and 12 in Algorithm 1 by Eqs. (10) and (11), respectively. \u0393 + t and \u0393 \u2212 t are updated by Eqs. (8) and (9), respectively.\nRemark. An alternative approach for the high-dimensional case is through the random projection (Johnstone, 2006; Hsu et al., 2012). Let H \u2208 Rd\u00d7\u03c4 be a random Gaussian matrix, where \u03c4 \u226a d. By performing random projection using H, we compute a lowdimensional representation for each instance xt as x\u0302t = H\n\u22a4xt \u2208 R\u03c4 and will only maintain covariance matrices of size \u03c4 \u00d7 \u03c4 in memory. Despite that it is computationally attractive, this approach performs significantly worse than the randomized low-rank approximation algorithm, according to our empirical study. This may owe to the fact that the random projection approach is equivalent to approximating S\u00b1t = IdS \u00b1 t Id by HH \u22a4S\u00b1t HH \u22a4, which replaces both the left and right identity matrices of S\u00b1t withHH \u22a4. In contrast, our proposed approach only approximates one identity matrix in S\u00b1t , making it more reliable for tackling high-dimensional data."}, {"heading": "4. Main Theoretical Result", "text": "In this section, we present the main theoretical results for our proposed algorithm. The following theorem shows the consistency of square loss, and the detailed proof is deferred in Section 5.1.\nTheorem 1 For square loss \u2113(t) = (1\u2212 t)2, the surrogate loss \u03a8(f,x,x\u2032) = \u2113(f(x)\u2212f(x\u2032)) is consistent with AUC.\nDefine w\u2217 as w\u2217 = argmin\nw\n\u2211\nt\nLt(w).\nWe are in the position to present the following convergence rate for Algorithm 1 when the full covariance matrices are provided, and the detailed proof is deferred in Section 5.2 Theorem 2 For \u2016xt\u2016 \u2264 1 (t \u2208 [T ]), \u2016w\u2217\u2016 \u2264 B and TL\u2217 \u2265 \u2211T\nt=1 Lt(w\u2217), we have \u2211\nt Lt(wt)\u2212 \u2211 t Lt(w\u2217) \u2264 2\u03baB2 +B \u221a 2\u03baTL\u2217,\nwhere \u03ba = 4 + \u03bb and \u03b7t = 1/(\u03ba + \u221a (\u03ba2 + \u03baTL\u2217/B2).\nThis theorem presents an O(1/T ) convergence rate for the OPAUC algorithm if the distribution is separable, i.e., L\u2217 = 0, and an O(1/ \u221a T ) convergence rate for general case. Compared to the online AUC optimization algorithm (Zhao et al., 2011), which achieves at most O(1/ \u221a T ) convergence rate, our proposed algorithm clearly reduce the regret. The faster convergence rate of our proposed algorithm owes to the smoothness of the square loss, an important property that has been explored by some studies of online learning (Rakhlin et al., 2012) and generalization error bound analysis (Srebro et al., 2010).\nRemark: The bound in Theorem 2 does not explicitly explore the strongly convexity of Lt(w), which can lead to an O(1/T ) convergence rate. Instead, we focus on exploiting the smoothness of the loss function, since we did not introduce a bounded domain for w. Due to the regularizer \u03bb|w|2/2, we have |w\u2217| \u2264 1/\u03bb, and it is reasonable to restrict wt by |wt| \u2264 1/\u03bb, leading to a regret bound of O(lnT/[\u03bb3T ]) by applying the standard stochastic gradient descent with \u03b7t = 1/[\u03bbt]. This bound is preferred only when \u03bb = \u2126(T\n\u22121/6), a scenario which rarely occurs in empirical study. This problem may also be addressable by exploiting the epoch gradient method (Nocedal and Wright, 1999), a subject of our future study.\nWe now consider the case when covariance matrices are approximated by low-rank matrices. Note that the low-rank approximation is accurate only if the eigenvalues of covariance matrices follow a skewed distribution. To capture the skewed eigenvalue distribution, we introduce the concept of effective numerical rank (Hansen, 1987) that generalizes the rank of matrix:\nDefinition 3 For a positive constant \u00b5 > 0 and semi-positive definite matrix M \u2208 Rd\u00d7d of eigenvalues {\u03bdi}, the effective numerical rank w.r.t. \u00b5 is defined to be r(M,\u00b5) =\u2211d\ni=1 \u03bdi/(\u00b5+ \u03bdi).\nIt is evident that the effective numerical rank is upper bounded by the true rank, i.e., r(M,\u00b5) \u2264 rank(M). To further see how the concept of effective numerical rank captures the skewed eigenvalue distribution, consider a PSD matrix M of full rank with \u2211d i=k \u03bdi \u2264 \u00b5 for small k. It is easy to verify that r(M,\u00b5) \u2264 k, i.e., M can be well approximated by a matrix of rank k.\nDefine the effective numerical rank for a set of matrices {Mt}Tt=1 as r ( {Mt}Tt=1, \u00b5 ) = max\n1\u2264t\u2264T r(Mt, \u00b5).\nUnder the assumption that the effective numerical rank for the set of covariance matrices {S\u00b1t }Tt=1 is small (i.e., S\u00b1t can be well approximated by low-rank matrices), the following theorem gives the convergence rate for |\u2211t L\u0302t(wt) \u2212 \u2211 t Lt(w\u2217)|, where L\u0302t(wt) are given by Eqs. (6) and (7).\nTheorem 4 Let r = r({S\u00b1t }Tt=1, \u03bb) be the effective numerical rank for the sequence of covariance matrices {S\u00b1t }Tt=1. For 0 < \u03b4 < 1, 0 < \u01eb \u2264 1/2, |w\u2217| \u2264 B, \u2016xt\u2016 \u2264 1 (t \u2208 [T ]) and TL\u2217 \u2265 \u2211Tt=1 Lt(w\u2217), we have with probability at least 1\u2212 \u03b4,\u2223\u2223\u2223\n\u2211 t ( L\u0302t(wt)\u2212 Lt(w\u2217) )\u2223\u2223\u2223 \u2264 2\u01ebTL\u2217 + 2\u03baB2 +B \u221a 2\u03baTL\u2217\nprovided \u03c4 \u2265 32r\u03bb \u01eb2 log 2dT\u03b4 , where \u03ba = 4 + \u03bb and \u03b7t = 1/(\u03ba + \u221a (\u03ba2 + \u03baTL\u2217/B2).\nThe detailed proof is presented in Section 5.3. For the separable distribution L\u2217 = 0, we also obtain an O(1/T ) convergence rate when the covariance matrices are approximated by low-rank matrices. Compared with Theorem 2, Theorem 4 introduces an additional term 2\u01ebL\u2217 in the bound when using the approximate covariance matrices, and it is noteworthy that the approximation does not significantly increase the bound of Theorem 2 if 2\u01ebTL\u2217 \u2264 B \u221a 2(4 + \u03bb)TL\u2217, i.e., \u01eb \u2264 B \u221a 2(\u03bb+ 4)/TL\u2217. This implies that the approximate algorithm will achieve similar performance as the one using the full covariance matrices provided \u03c4 = \u2126(r\u03bbT (log d + log T )/(\u03bb + 4)). When \u03bb = O(1/T ), this requirement is reduced to \u03c4 = \u2126(r[log d+ log T ]), a logarithmic dependence on dimension d."}, {"heading": "5. Proofs", "text": "In this section, we present detailed proofs for our main theorems."}, {"heading": "5.1 Proof of Theorem 1", "text": "Let X = {x1,x2, . . . ,xn} with instance-marginal probability pi = Pr[xi] and conditional probability \u03bei = Pr[y = +1|xi], and we denote by the expected risk\nR\u03a8(f) = C0 + \u2211\ni 6=j\npipj (\u03bei(1\u2212 \u03bej)\u2113(f(xi)\u2212 f(xj)) + \u03bej(1\u2212 \u03bei)\u2113(f(xj)\u2212 f(xi)))\nwhere \u2113(t) = (1\u2212 t)2 and C0 is a constant with respect to f(xi) (1 \u2264 i \u2264 n). According to the analysis of (Gao and Zhou, 2012), it suffices to prove that, for every optimal solution f , i.e., R\u03a8(f) = inff \u2032 R\u03a8(f\n\u2032), we have f(xi) > f(xj) if \u03bei > \u03bej. If X = {x1,x2}, then minimizing R\u03c6(f) gives the optimal solution f = (f(x1), f(x2))\nsuch that f(x1)\u2212 f(x2) = sgn(\u03be(x1)\u2212 \u03be(x2)) for \u03be(x1) 6= \u03be(x2),\nwhich shows the consistency of least square loss. For X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} with n \u2265 3, if \u03bei(1 \u2212 \u03bei) = 0 for every 1 \u2264 i \u2264 n, then minimizing R\u03a8(f) gives the optimal solution f = (f1, f2, \u00b7 \u00b7 \u00b7 , fn) such that fj = fi + 1 for every \u03bei = 1 and \u03bej = \u22121,\nwhich shows the consistency of least square loss. If X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xn} with n \u2265 3, and there exists some i0 s.t. \u03bei0(1\u2212 \u03bei0) 6= 0, then the subgradient conditions give optimal solution such that \u2211\nk 6=i\npk(\u03bei + \u03bek \u2212 2\u03bei\u03bek)(f(xi)\u2212 f(xk)) = \u2211\nk 6=i\npk(\u03bei \u2212 \u03bek) for each 1 \u2264 i \u2264 n.\nSolving the above n linear equations, we obtain the optimal solution f = (f1, f2, . . . , fn), i.e.,\nf(xi)\u2212 f(xj) = (\u03bei \u2212 \u03bej) \u220f k 6=i,j \u2211n l=1 pl(\u03bel + \u03bek \u2212 2\u03bel\u03bek)\u2211\nsi\u22650 s1+\u00b7\u00b7\u00b7+sn=n\u22122\nps11 \u00b7 \u00b7 \u00b7 psnn \u0393(s1, s2, \u00b7 \u00b7 \u00b7 , sn)\nwhere \u0393 is a polynomial in \u03be[k1] + \u03be[k2] \u2212 2\u03be[k1]\u03be[k2] for 1 \u2264 k1, k2 \u2264 n. In the following, we will derive the specific expression for \u0393(s1, s2, \u00b7 \u00b7 \u00b7 , sn). Denote by A = {i : si \u2265 1} and B = {i : si = 0} = {b1, b2, \u00b7 \u00b7 \u00b7 , b|B|}.\n\u2022 If |A| = 1, i.e., A = {i1} for some 1 \u2264 i1 \u2264 n, then\n\u0393(s1, s2, \u00b7 \u00b7 \u00b7 , sn) = \u220f\nk\u2208B\n(\u03bei1 + \u03bek \u2212 2\u03bei1\u03bek).\n\u2022 If |A| = 2, i.e., A = {i1, i2} for some 1 \u2264 i1, i2 \u2264 n, then we denote by\nA1 = {si1 \u2299 i1} \u22c3 {si2 \u2299 i2}\nwhere {sik \u2299 ik} denotes the multi-set {ik, ik, . . . , ik} of size sik for k = 1, 2. It is clear that |B| = |A1| = n \u2212 2. Further, we denote by G(A1) the set of all permutations of A1. Therefore, we have\n\u0393(s1, s2, \u00b7 \u00b7 \u00b7 , sn) = (\u03bei1 + \u03bei2 \u2212 2\u03bei1\u03bei2) \u2211\n\u03c0=\u03c01\u00b7\u00b7\u00b7\u03c0n\u22122\u2208G(A1)\nn\u22122\u220f\nk=1\n(\u03be\u03c0i + \u03bebi \u2212 2\u03be\u03c0i\u03bebi).\n\u2022 If |A| > 2, then, for i1 6= i2 \u2208 A, we denote by the multi-set\nA1(i1, i2) = {si1 \u2299 i1} \u22c3 {si2 \u2299 i2} \u22c3\n  \u22c3\nk\u2208A\\{i1,i2}\n{(sk \u2212 1)\u2299 k}   ,\nand it is easy to derive |A1| = |B|. Further, we denote by G(A \\ {i1, i2}) and G(A1) the set of all permutations of A \\ {i1, i2} and A1, respectively. Therefore, we set\n\u03931(i1, i2,A) = \u2211\n\u03c0=\u03c01\u03c02\u00b7\u00b7\u00b7\u03c0|A|\u22122\u2208G(A\\{i1,i2})\n(\u03bei1 + \u03be\u03c01 \u2212 2\u03bei1\u03be\u03c01)(\u03be\u03c01 + \u03be\u03c02 \u2212 2\u03be\u03c01\u03be\u03c02)\n\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 (\u03be\u03c0|A|\u22123 + \u03be\u03c0|A|\u22122 \u2212 2\u03be\u03c0|A|\u22123\u03be\u03c0|A|\u22122)(\u03be\u03c0|A|\u22122 + \u03bei2 \u2212 2\u03bei2\u03be\u03c0|A|\u22122),\nand we have\n\u0393(s1, s2, \u00b7 \u00b7 \u00b7 , sn) = \u2211\ni1 6=i2 i1,i2\u2208A\n\u03931(i1, i2,A) \u2211\n\u03c0=\u03c01\u03c02...\u03c0|B|\u2208G(A1)\n|B|\u220f\nk=1\n(\u03be\u03c0k + \u03bebk \u2212 2\u03be\u03c0k\u03bebk)\nwhere B = {b1, b2, . . . , b|B|}.\nSince there exist some i0 s.t. \u03bei0(1\u2212 \u03bei0) 6= 0, we have \u220f\nk 6=i,j \u2211n l=1 pl(\u03bel + \u03bek \u2212 2\u03bel\u03bek)\u2211\nsi\u22650 s1+\u00b7\u00b7\u00b7+sn=n\u22122\nps11 \u00b7 \u00b7 \u00b7 psnn \u0393(s1, s2, \u00b7 \u00b7 \u00b7 , sn) > 0.\nTherefore, it is evident that f(xi) > f(xj) if \u03bei > \u03bej, and this theorem follows as desired."}, {"heading": "5.2 Proof of Theorem 2", "text": "This proof is motivated from (Shalev-Shwartz, 2007; Srebro et al., 2010). Recall\nLt(w) = \u03bb\n2 |w|2 +\n\u2211t\u22121 i=1 I[yi 6= yt](1\u2212 yt(xt \u2212 xi)\u22a4w)2\n2|{i \u2208 [t\u2212 1] : yi 6= yt}| .\nFor |w\u2217| \u2264 B and convex Lt(w), we have\nLt(wt)\u2212 Lt(w\u2217) \u2264 \u2207Lt(wt)\u22a4(wt \u2212w\u2217). (12)\nIt is easy to derive that \u2207Lt(wt) equals to\n\u03bbwt \u2212 \u2211t\u22121\ni=1 I[yi 6= yt](1 \u2212 yt(xt \u2212 xi)\u22a4wt)yt(xt \u2212 xi) |{i \u2208 [t\u2212 1] : yi 6= yt}| ,\nand therefore, for any w and |xi| \u2264 1\n|\u2207Lt(wt)\u2212\u2207Lt(w)| \u2264 (4 + \u03bb)|wt \u2212w|.\nDenote by\nwt\u2217 = argmin w\nLt(wt),\nwhich implies that \u2207Lt(wt\u2217) = 0 for convex and smooth Lt. Based on (Nesterov, 2003, Theorem 2.1.5), we have\n|\u2207Lt(wt)|2 = |\u2207Lt(wt)\u2212\u2207Lt(wt\u2217)|2 \u2264 2(\u03bb+ 4)Lt(wt) (13)\nwhere the inequality holds from Lt(wt\u2217) \u2265 0 and \u2207Lt(wt\u2217) = 0. Moreover, we have\n|wt+1\u2212w\u2217|2 = |wt\u2212\u03b7t\u2207Lt(wt)\u2212w\u2217|2 = |wt\u2212w\u2217|2\u22122\u03b7t\u2207Lt(wt)\u22a4(wt\u2212w\u2217)+\u03b72t |\u2207Lt(wt)|2,\nand this yields that, by using Eqs. (12) and (13),\n(1\u2212 (4 + \u03bb)\u03b7t)Lt(wt)\u2212 Lt(w\u2217) \u2264 1\n2\u03b7t |wt \u2212w\u2217|2 \u2212\n1\n2\u03b7t |wt+1 \u2212w\u2217|2.\nSumming over t = 0, . . . , T \u2212 1 and rearranging, we obtain T\u22121\u2211\nt=0\n(1\u2212 (4 + \u03bb)\u03b7t)Lt(wt)\u2212 T\u22121\u2211\nt=0\nLt(w\u2217)\n\u2264 1 2\u03b70 |w0 \u2212w\u2217|2 \u2212 1 2\u03b7T\u22121 |wT \u2212w\u2217|2 +\nT\u22122\u2211\ni=1\n( 1 2\u03b7i+1 \u2212 1 2\u03b7i )|wi \u2212w\u2217|2.\nBy setting \u03b7t = \u03b7, we have\n1\n2\u03b70 |w0 \u2212w\u2217|2 \u2212\n1\n2\u03b7T\u22121 |wT \u2212w\u2217|2 \u2264\n1\n2\u03b7 |w\u2217|2 \u2264\nB2\n2\u03b7\nfrom w0 = 0 and |w\u2217| \u2264 B, and we further get T\u22121\u2211\nt=0\nLt(wt)\u2212 T\u22121\u2211\nt=0\nLt(w\u2217) \u2264 1\n1\u2212 (4 + \u03bb)\u03b7\n( B2\n2\u03b7 + (4 + \u03bb)\u03b7\nT\u22121\u2211\nt=0\nLt(w\u2217) )\n\u2264 1 1\u2212 (4 + \u03bb)\u03b7\n( B2\n2\u03b7 + (4 + \u03bb)\u03b7TL\u2217\n) .\nThis theorem holds by putting\n\u03b7 = 1\n4 + \u03bb+ \u221a (4 + \u03bb)2 + (4 + \u03bb)TL\u2217/B2\ninto the above formula and using the formula \u221a a+ b \u2264 \u221aa+ \u221a b."}, {"heading": "5.3 Proof of Theorem 4", "text": "Before the detailed proof of Theorem 4, we begin with some useful results:\nLemma 5 Let S1 = diag(s1i) and S2 = diag(s2i) be two d\u00d7 d diagonal matrices such that s1i 6= 0 and s21i + s22i = 1 for all i. For a Gaussian random matrix R \u2208 Rd\u00d7\u03c4 , we set Z = S1S1 + S2RR \u22a4S2 and r = \u2211 i s 2 2i, and the followings hold\nPr[\u03bb1(Z) \u2265 1 + \u01eb] \u2264 d exp(\u2212\u03c4\u01eb2/32r) and Pr[\u03bbp(Z) \u2264 1\u2212 \u01eb] \u2264 d exp(\u2212\u03c4\u01eb2/32r), where \u03bbk(Z) denotes the k-th largest eigenvalue of matrix Z.\nProof This proof technique is motivated from (Gittens and Tropp, 2011) by adding a bias matrix. Let g(\u03b8) = \u03b8 2\n2\u22122\u03b8 . Then, we have\nPr[\u03bb1(M) \u2265 1 + \u01eb] \u2264 inf\n\u03b8>0 tr exp\n{ \u03b8 ( S1S1 + E[S2RR \u22a4S2]\u2212 (1 + \u01eb)I ) + g(\u03b8)E[(S2RR \u22a4S2) 2]/\u03c4 }\n\u2264 inf \u03b8>0 tr exp{\u2212\u03b8\u01eb+ 8g(\u03b8)tr(S22)S22} \u2264 inf \u03b8>0 d exp{\u2212\u03b8\u01eb+ 8rg(\u03b8)} \u2264 d exp(\u2212\u03c4\u01eb2/32r).\nIn a similar manner,\nPr[\u03bbp(M) \u2264 1\u2212 \u01eb] \u2264 inf\n\u03b8>0 tr exp\n{ \u03b8 ( S1S1 + E[S2RR \u22a4S2]\u2212 (1\u2212 \u01eb)I ) + g(\u03b8)E[(S2RR \u22a4S2) 2]/\u03c4 }\n\u2264 inf \u03b8>0 d exp{\u2212\u03b8\u01eb+ 8rg(\u03b8)} \u2264 d exp(\u2212\u03c4\u01eb2/32r).\nThis completes the proof.\nLet M \u2208 Rd\u00d7d be a positive semi-definite (PSD) matrix with effective numerical rank r(M,\u00b5) for \u00b5 > 0. We define two matrices K and K\u0302, respectively, as\nK := \u00b5Id +M and K\u0302 := \u00b5Id +M \u22121/2RR\u22a4M\u22121/2,\nwhere R \u2208 Rd\u00d7m is a (Gaussian) random matrix. Based on Lemma 5, we have the following theorem that bounds the difference between K \u2212 K\u0302:\nLemma 6 Let r(M,\u00b5) be the numerical rank for \u00b5 > 0 and PSD matrix M . Then, for \u03b4 > 0 and \u01eb > 0, the following holds with probability at least 1\u2212 \u03b4\n\u2016I \u2212K\u22121/2K\u0302K\u22121/2\u20162 \u2264 \u01eb, where \u2016Z\u20162 measures the spectral norm of matrix Z, provided\n\u03c4 \u2265 32r(M,\u00b5) \u01eb2 log(2d/\u03b4).\nProof Let M = Udiag(\u03c32i )V \u22a4 be the singular value decomposition of M . We define\nS1 = diag\n(\u221a \u00b5\n\u03c321 + \u00b5 , . . . ,\n\u221a \u00b5\n\u03c32d + \u00b5\n) and S2 = diag   \u03c31\u221a\n\u03c321 + \u00b5 , . . . , \u03c3d\u221a \u03c32d + \u00b5\n  .\nIt is easy to observe that\nZ = K\u22121/2K\u0302K\u22121/2 = U(S21 + S2V \u22a4RR\u22a4V )U\u22a4 = U(S21 + S2R\u0302R\u0302 \u22a4)U\u22a4\nwhere R\u0302 = V \u22a4R \u2208 Rd\u00d7\u03c4 is a also Gaussian random matrix because V is an orthonormal matrix. Parameter r in Lemma 5 is given by\nr =\nd\u2211\ni=1\ns22,i =\nd\u2211\ni=1\n\u03c32i \u03c32i + \u00b5 = r(M,\u00b5).\nUsing Lemma 5, the followings hold with a probability at least 1\u2212 \u03b4,\n\u03bbmax (Z) = \u2016K\u22121/2K\u0302K\u22121/2\u20162 \u2264 1 + \u01eb and \u03bbmin(Z) = \u03bbd ( K\u22121/2K\u0302K\u22121/2 ) \u2265 1\u2212 \u01eb,\nwhich yields that \u2016Z \u2212 I\u2016 \u2264 \u01eb provided\n\u03c4 \u2265 32r \u01eb2 log 2d \u03b4 .\nThis lemma follows as desired.\nRecall that\nL\u0302t(w) = \u03bb\n2 |w|2 +w\u22a4(c\u2212t\u22121 \u2212 xt) +\n1 2 + 1 2\n( w\u22a4(xt \u2212 c\u2212t\u22121)(xt \u2212 c\u2212t\u22121)\u22a4w +w\u22a4S\u0302\u2212t w )\nif yt = 1; otherwise,\nL\u0302t(w) = \u03bb\n2 |w|2 +w\u22a4(xt \u2212 c+t\u22121) +\n1 2 + 1 2\n( w\u22a4(xt \u2212 c+t\u22121)(xt \u2212 c+t\u22121)\u22a4w +w\u22a4S\u0302+t w ) .\nWe further define w\u0302\u2217 as the optimal solution that minimizes the loss based on approximate covariance matrices, i.e.\nw\u0302\u2217 = argmin w\nT\u2211\nt=1\nL\u0302t(w).\nBased on Lemma 6, the following theorem gives an upper bound for |\u2211t L\u0302t(w\u0302\u2217) \u2212\u2211 t Lt(w\u2217)|.\nTheorem 7 Let r({S\u00b1t }Tt=1, \u03bb) be the effective numerical rank for the set of covariance matrices S\u00b1t , t = 1, . . . , T with respect to the regularization parameter \u03bb. Then, for any 0 < \u03b4 and 0 < \u01eb \u2264 1/2, the followings hold with probability at least 1\u2212 \u03b4\n|w\u0302\u2217 \u2212w\u2217| \u2264 2\u01eb|w\u2217| (14)\u2223\u2223\u2223 \u2211 t L\u0302t(w\u0302\u2217)\u2212 \u2211 t Lt(w\u2217) \u2223\u2223\u2223 \u2264 2\u01eb \u2211 tLt(w\u2217) (15)\nprovided that\n\u03c4 \u2265 32r(S, \u03bb) \u01eb2 log 2d \u03b4T\nProof We first rewrite L(w) = \u2211Tt=1 Lt(w) as\nL(w) = 1 2 +w\u22a4a+ 1 2 w\u22a4 (A1 +A2)w\nwhere\na = T\u2211\nt=1\nI[yt = 1] ( c\u2212t\u22121 \u2212 xt ) + I[yt = \u22121] ( c+t\u22121 \u2212 xt )\nT\nA1 = \u03bbId + 1\nT\nT\u2211\nt=1\n( I[yt = 1]S \u2212 t\u22121 + I[yt = \u22121]S+t\u22121 )\nA2 = 1\nT\nT\u2211\nt=1\nI[yt = 1](xt \u2212 c\u2212t )(xt \u2212 c\u2212t )\u22a4 + 1\nT\nT\u2211\nt=1\nI[yt = \u22121](xt \u2212 c+t )(xt \u2212 c+t )\u22a4.\nSimilarly, we rewrite L\u0302(w) = \u2211Tt=1 L\u0302t(w) as\nL\u0302(w) = 1 2 +w\u22a4a+ 1 2 w\u22a4\n( A\u03031 +A2 ) w\nwhere\nA\u03031 = \u03bbId+ 1\nT\nT\u2211\nt=1\nI[yt = 1][S \u2212 t\u22121] 1/2RtR \u22a4 t [S \u2212 t\u22121]\n1/2+ 1\nT\nT\u2211\nt=1\nI[yt = \u22121][S+t\u22121]1/2RtR\u22a4t [S+t\u22121]1/2.\nThe optimal solutions for minimizing L(w) and L\u0302(w) are given, respectively, by\nw\u2217 = (A1 +A2) \u22121a and w\u0302\u2217 = (A\u03031 +A2) \u22121a.\nDefine \u2206 = I \u2212A\u22121/21 A\u03031A \u22121/2 1 and write A\u03031 in terms of \u2206 as\nA\u03031 = A1 \u2212A1/21 \u2206A 1/2 1\nUsing Lemma 6, it holds that \u01ebId \u2206 \u01ebId with probability at least 1\u2212 \u03b4T , and therefore\n(1\u2212 \u01eb)(A1 +A2) A\u03031 +A2 (1 + \u01eb)(A1 +A2)\nDenote by \u2126 = (A1 +A2) 1/2(A1 +A2) \u22121(A1 +A2)\n1/2 \u2212 I, and according to previous analysis, we have\n|\u2126| \u2264 \u01eb 1\u2212 \u01eb \u2264 2\u01eb (16)\nfor \u01eb < 1/2. Therefore,\n|w\u0302\u2217 \u2212w\u2217| = \u2223\u2223\u2223 ( (A\u03031 +A2) \u22121 \u2212 (A1 +A2)\u22121 ) a \u2223\u2223\u2223\n= |(A1 +A2)\u22121/2\u2126(A1 +A2)\u22121/2a| \u2264 2\u01eb|(A1 +A2)\u22121a| \u2264 2\u01eb|w\u2217|\nand \u2223\u2223\u2223L\u0302(w\u2217)\u2212 L(w\u2217)\n\u2223\u2223\u2223 = 3 2 \u2223\u2223\u2223a\u22a4 ( (A\u03031 +A2) \u22121 \u2212 (A1 +A2)\u22121 ) a \u2223\u2223\u2223\n= 3\n2\n\u2223\u2223\u2223a\u22a4 ( (A1 +A2) \u22121/2\u2126(A1 +A2) \u22121/2 ) a \u2223\u2223\u2223\n\u2264 2\u01eb \u2223\u2223\u2223\u2223 3\n2 a\u22a4(A1 +A2) \u22121a\n\u2223\u2223\u2223\u2223 \u2264 2\u01eb \u2223\u2223\u2223\u2223 1\n2 +\n3 2 a\u22a4(A1 +A2) \u22121a \u2223\u2223\u2223\u2223 = 2\u01ebL(w\u2217).\nThis theorem follows as desired.\nProof of Theorem 4: For |w\u2217| \u2264 B, L(w\u2217) \u2264 TL\u2217, and 0 < \u01eb \u2264 1/2, we have\n|w\u0302\u2217| \u2264 |w\u2217|+ |w\u0302\u2217 \u2212w\u2217| \u2264 2B (17)\nfrom Eq. (14), and we further have\nL\u0302\u2217(w\u0302\u2217) \u2264 L(w\u2217) + |L\u0302\u2217(w\u0302\u2217)\u2212 L(w\u2217)| \u2264 2L(w\u2217) \u2264 2TL\u2217 (18)\nfrom Eq. (15). Therefore, we have \u2223\u2223\u2223\u2223\u2223 \u2211\nt\nL\u0302(wt)\u2212 \u2211\nt\nL(w\u2217) \u2223\u2223\u2223\u2223\u2223 \u2264 \u2211\nt\nL\u0302(wt)\u2212 \u2211\nt\nL(w\u0302\u2217) + \u2223\u2223\u2223\u2223\u2223 \u2211\nt\nL\u0302(w\u0302\u2217)\u2212 \u2211\nt\nL(w\u2217) \u2223\u2223\u2223\u2223\u2223 . (19)\nWe use Theorem 1 (in the main paper) to bound the first term in the above by combining Eqs. (17) and (18), and the second term can be bounded by Eq. (15). This completes the proof as desired."}, {"heading": "6. Experiments", "text": "We evaluate the performance of OPAUC on benchmark datasets and high-dimensional datasets in Sections 6.1 and 6.2, respectively. Then, we study the parameter influence in Section 6.3."}, {"heading": "6.1 Comparison on Benchmark Data", "text": "We conduct our experiments on sixteen benchmark datasets1,2,3 as summarized in Table 1. Some datasets have been used in previous studies on AUC optimization, whereas the other are large ones requiring one-pass procedure. The features have been scaled to [\u22121, 1] for all datasets. Multi-class datasets have been transformed into binary ones by randomly partitioning classes into two groups, where each group contains the same number of classes.\n1. http://www.sigkdd.org/kddcup/ 2. http://www.ics.uci.edu/\u02dcmlearn/MLRepository.html 3. http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/\nIn addition to state-of-the-art online AUC approachesOAMseq andOAMgra (Zhao et al., 2011), we also compare with:\n\u2022 online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss (Kotlowski et al., 2011);\n\u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss;\n\u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005);\n\u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006);\n\u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss;\n\u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al., 2011);\n\u2022 batch Uni-Squ: A batch learning algorithm which optimizes the (weighted) univariate square loss.\nAll experiments are performed with Matlab 7 on a node of computational cluster with 16 CPUs (Intel Xeon Due Core 3.0GHz) running RedHat Linux Enterprise 5 with 48GB main memory. For batch algorithms, due to memory limit, 8,000 training examples are randomly chosen if training data size exceeds 8,000, whereas only 2,000 training examples are used for the epsilon dataset because of its high dimension.\nFive-fold cross-validation is executed on training sets to decide the learning rate \u03b7t \u2208 2[\u221212:10] for online algorithms, the regularized parameter \u03bb \u2208 2[\u221210:2] for OPAUC and \u03bb \u2208 2[\u221210:10] for batch algorithms. For OAMseq and OAMgra, the buffer sizes are fixed to be 100 as recommended in (Zhao et al., 2011). For univariate approaches, the weights (i.e., class ratios) are chosen as done in (Kotlowski et al., 2011).\nThe performances of the compared methods are evaluated by five trials of 5-fold cross validation, where the AUC values are obtained by averaging over these 25 runs. Table 2 shows that OPAUC is significant better than the other four online algorithms OAMseq, OAMgra, online Uni-Exp and online Uni-Squ, particularly for large datasets. The win/tie/loss counts show that OPAUC is clearly superior to these online algorithms, as it wins for most times and never loses. Table 3 shows shows that OPAUC is highly competitive to the other five batch learning algorithms; this is impressive because these batch algorithms require storing the whole training dataset whereas OPAUC does not store training data. Additionally, batch LS-SVM which optimizes the square loss is comparable to the other batch algorithms, verifying our argument that square loss is effective for AUC optimization.\nWe also compare the running time of OPAUC and the online algorithms OAMseq, OAMgra, online Uni-Exp and online Uni-Squ, and the average CPU time (in seconds) are shown in Figure 1. As expected, online Uni-Squ and online Uni-Exp takes the least time cost because they optimize on single-instance (univariate) loss, whereas the other algorithms work by optimizing pairwise loss. On most datasets, the running time of OPAUC is competitive to OAMseq and OAMgra, except on the mnist and epsilon datasets which have the highest dimension in Table 1."}, {"heading": "6.2 Comparison on High-Dimensional Data", "text": "Next, we study the performance of using low-rank matrices to approximate the full covariance matrices, denoted by OPAUCr. Six datasets4,5 with nearly or more than 50,000 features are used, as summarized in Table 4. The news20.binary dataset contains two classes, different from news20 dataset. The original news20 and sector are multi-class datesets; in our experiments, we randomly group the multiple classes into two meta-classes each containing the same number of classes, and we also use the sector.lvr dataset which regards the largest class as positive whereas the union of other classes as negative. The original ecml2012 and rcv1v2 are multi-label datasets; in our experiments, we only consider the label with the largest population, and remove the features in ecml2012 dataset that take zero values for all instances.\nBesides the online algorithms OAMseq, OAMgra, online Uni-Exp and online Uni-Squ, we also evaluate three variants of OPAUC to study the effectiveness of approximating full covariance matrices with low-rank matrices:\n\u2022 OPAUCf: Randomly selects 1, 000-dim features and then works with full covariance matrices;\n4. http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/ 5. http://www.ecmlpkdd2012.net/discovery-challenge\n\u2022 OPAUCrp: Projects into a 1, 000-dim feature space by Random Projection, and then works with full covariance matrices;\n\u2022 OPAUCpca: Projects into a 1, 000-dim feature space obtained by Principle Component Analysis, and then works with full covariance matrices.\nSimilar to Section 5.1, five-fold cross validation is executed on training sets to decide the learning rate \u03b7t \u2208 2[\u221212:10] and the regularization parameter \u03bb \u2208 2[\u221210:2]. Due to memory and computational limit, the buffer sizes are set to 50 for OAMseq and OAMgra, and the rank \u03c4 of OPAUCr is also set to 50. The performances of the compared methods are evaluated by five trials of 5-fold cross validation, where the AUC values are obtained by averaging over these 25 runs.\nThe comparison results are summarized in Table 5 and the average running time is shown in Figure 2. These results clearly show that our approximate OPAUCr approach is superior to the other compared methods. Compared with OAMseq and OAMgra, the running time costs are comparable whereas the performance of OPAUCr is better. Online Uni-Squ and Uni-Exp are more efficient than OPAUCr because it optimizes univariate loss, but the performance of OPAUCr is highly competitive or better, except on rcv1v2, the only dataset with less than 50,000 features. Compared with the three variants, OPAUCf and OPAUCrp are more efficient, but with much worse performances. OPAUCpca achieves a better performance on rcv1v2, but it is worse on datasets with more features; particularly, on the two datasets with the largest number of features, OPAUCpca cannot return results even after running out 106 seconds (almost 11.6 days). Our approximate OPAUCr approach is significantly better than all the other methods (if they return results) on the two datasets\nwith the largest number of features: news.binary with more than 1 million features, and ecml2012 with nearby 100 thousands features. These observations validate the effectiveness of the low-rank approximation used by OPAUCr for handling high-dimensional data."}, {"heading": "6.3 Parameter Influence", "text": "We study the influence of parameters in this section. Figure 3 shows that stepsize \u03b7t should not be set to values bigger than 1, whereas there is a relatively big range between [2\u221212, 2\u22124] where OPAUC achieves good results. Figures 4 shows that OPAUC is not sensitive to the value of regularization parameter \u03bb given that it is not set with a big value. Figure 5 shows that OPAUCr is not sensitive to the values of rank \u03c4 , and it works well even when \u03c4 = 50; this verifies Theorem 4 that a relatively small \u03c4 value suffices to lead to a good approximation performance. Figure 6 compares studies the influence of the iterations for OPAUC, OAMseq and OAMgra, and it is observable that OPAUC convergence faster than the other two algorithms, which verifies our theoretical argument in Section 4."}, {"heading": "7. Conclusion", "text": "In this paper, we study one-pass AUC optimization that requires going through the training data only once, without storing the entire dataset. Here, a big challenge lies in the fact that AUC is measured by a sum of losses defined over pairs of instances from different classes. We propose the OPAUC approach, which employs a square loss and requires the storing of only the first and second-statistics for the received training examples. A nice property of OPAUC is that its storage requirement is O(d2), where d is the dimension of data, independent from the number of training examples. To handle high-dimensional data, we develop an approximate strategy by using low-rank matrices. The effectiveness of our proposed approach is verified both theoretically and empirically. In particular, the performance of OPAUC is significantly better than state-of-the-art online AUC optimization approaches, even highly competitive to batch learning approaches; the approximate OPAUC is significantly better than all compared methods on large datasets with one hundred thousands or even more than one million features. An interesting future issue is to develop one-pass AUC optimization approaches not only with a performance comparable to batch approaches, but also with an efficiency comparable to univariate loss optimization approaches."}], "references": [{"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "AUC optimization vs. error rate minimization", "author": ["C. Cortes", "M. Mohri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cortes and Mohri.,? \\Q2004\\E", "shortCiteRegEx": "Cortes and Mohri.", "year": 2004}, {"title": "A coherent interpretation of AUC as a measure of aggregated classification performance", "author": ["P.A. Flach", "J. Hern\u00e1ndez-Orallo", "C.F. Ramirez"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Flach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Flach et al\\.", "year": 2011}, {"title": "On the consistency of AUC optimization", "author": ["W. Gao", "Z.-H. Zhou"], "venue": "CoRR, 1208.0645v3,", "citeRegEx": "Gao and Zhou.,? \\Q2012\\E", "shortCiteRegEx": "Gao and Zhou.", "year": 2012}, {"title": "Tail bounds for all eigenvalues of a sum of random matrices", "author": ["Alex Gittens", "Joel A. Tropp"], "venue": "CoRR, abs/1104.4513v2,", "citeRegEx": "Gittens and Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Gittens and Tropp.", "year": 2011}, {"title": "A method of comparing the areas under receiver operating characteristic curves derived from the same cases", "author": ["J.A. Hanley", "B.J. McNeil"], "venue": null, "citeRegEx": "Hanley and McNeil.,? \\Q1983\\E", "shortCiteRegEx": "Hanley and McNeil.", "year": 1983}, {"title": "Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion", "author": ["P.C. Hansen"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Hansen.,? \\Q1987\\E", "shortCiteRegEx": "Hansen.", "year": 1987}, {"title": "Optimising area under the ROC curve using gradient descent", "author": ["A. Herschtal", "B. Raskutti"], "venue": "In Proceedings of the 21st International Conference on Machine Learning, Alberta,", "citeRegEx": "Herschtal and Raskutti.,? \\Q2004\\E", "shortCiteRegEx": "Herschtal and Raskutti.", "year": 2004}, {"title": "Random design analysis of ridge regression", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Joachims.,? \\Q2005\\E", "shortCiteRegEx": "Joachims.", "year": 2005}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "High dimensional statistical inference and random matrices", "author": ["I. Johnstone"], "venue": "In Proceedings of the International Congress of Mathematicians,", "citeRegEx": "Johnstone.,? \\Q2006\\E", "shortCiteRegEx": "Johnstone.", "year": 2006}, {"title": "Bipartite ranking through minimization of univariate loss", "author": ["W. Kotlowski", "K. Dembczynski", "E. H\u00fcllermeier"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Kotlowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kotlowski et al\\.", "year": 2011}, {"title": "Exploratory undersampling for class-imbalance learning", "author": ["X.-Y. Liu", "J. Wu", "Z.-H. Zhou"], "venue": "IEEE Trans. Systems, Man, and Cybernetics - B,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Basic principles of ROC analysis", "author": ["C.E. Metz"], "venue": "Seminars in Nuclear Medicine,", "citeRegEx": "Metz.,? \\Q1978\\E", "shortCiteRegEx": "Metz.", "year": 1978}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2003\\E", "shortCiteRegEx": "Nesterov.", "year": 2003}, {"title": "The case against accuracy estimation for comparing induction algorithms", "author": ["F.J. Provost", "T. Fawcett", "R. Kohavi"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "Provost et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Provost et al\\.", "year": 1998}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Margin-based ranking and an equivalence between AdaBoost and RankBoost", "author": ["C. Rudin", "R.E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin and Schapire.,? \\Q2009\\E", "shortCiteRegEx": "Rudin and Schapire.", "year": 2009}, {"title": "Online Learning: Theory, Algorithms, and Applications", "author": ["S. Shalev-Shwartz"], "venue": "PhD thesis, Hebrew University of Jerusalem,", "citeRegEx": "Shalev.Shwartz.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2007}, {"title": "Smoothness, low noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Online AUC maximization", "author": ["P. Zhao", "S. Hoi", "R. Jin", "T. Yang"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al.", "startOffset": 40, "endOffset": 77}, {"referenceID": 5, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al.", "startOffset": 40, "endOffset": 77}, {"referenceID": 16, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 1, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 13, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 2, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 7, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 10, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 18, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 12, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 21, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 0, "context": "Although many online learning algorithms have been developed to find the optimal solution of some performance measures by only scanning the training data once (Cesa-Bianchi and Lugosi, 2006), few effort addresses one-pass AUC optimization.", "startOffset": 159, "endOffset": 190}, {"referenceID": 0, "context": "Although many online learning algorithms have been developed to find the optimal solution of some performance measures by only scanning the training data once (Cesa-Bianchi and Lugosi, 2006), few effort addresses one-pass AUC optimization. Unlike the classical classification and regression problems where the loss function can be calculated on a single training example, AUC is measured by the losses defined over pairs of instances from different classes, making it challenging to develop algorithms for one-pass optimization. An online AUC optimization algorithm was proposed very recently by Zhao et al. (2011). It is based on the idea of reservoir sampling, and achieves a solid regret bound by only storing \u221a T instances, where T is the number of training examples.", "startOffset": 160, "endOffset": 615}, {"referenceID": 21, "context": "Zhao et al. (2011) addressed this challenge by exploiting the reservoir sampling technique.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "In contrast, loss functions such as hinge loss are proven to be inconsistent with AUC (Gao and Zhou, 2012).", "startOffset": 86, "endOffset": 106}, {"referenceID": 11, "context": "An alternative approach for the high-dimensional case is through the random projection (Johnstone, 2006; Hsu et al., 2012).", "startOffset": 87, "endOffset": 122}, {"referenceID": 8, "context": "An alternative approach for the high-dimensional case is through the random projection (Johnstone, 2006; Hsu et al., 2012).", "startOffset": 87, "endOffset": 122}, {"referenceID": 21, "context": "Compared to the online AUC optimization algorithm (Zhao et al., 2011), which achieves at most O(1/ \u221a T ) convergence rate, our proposed algorithm clearly reduce the regret.", "startOffset": 50, "endOffset": 69}, {"referenceID": 17, "context": "The faster convergence rate of our proposed algorithm owes to the smoothness of the square loss, an important property that has been explored by some studies of online learning (Rakhlin et al., 2012) and generalization error bound analysis (Srebro et al.", "startOffset": 177, "endOffset": 199}, {"referenceID": 20, "context": ", 2012) and generalization error bound analysis (Srebro et al., 2010).", "startOffset": 48, "endOffset": 69}, {"referenceID": 6, "context": "To capture the skewed eigenvalue distribution, we introduce the concept of effective numerical rank (Hansen, 1987) that generalizes the rank of matrix: Definition 3 For a positive constant \u03bc > 0 and semi-positive definite matrix M \u2208 Rd\u00d7d of eigenvalues {\u03bdi}, the effective numerical rank w.", "startOffset": 100, "endOffset": 114}, {"referenceID": 3, "context": "According to the analysis of (Gao and Zhou, 2012), it suffices to prove that, for every optimal solution f , i.", "startOffset": 29, "endOffset": 49}, {"referenceID": 19, "context": "2 Proof of Theorem 2 This proof is motivated from (Shalev-Shwartz, 2007; Srebro et al., 2010).", "startOffset": 50, "endOffset": 93}, {"referenceID": 20, "context": "2 Proof of Theorem 2 This proof is motivated from (Shalev-Shwartz, 2007; Srebro et al., 2010).", "startOffset": 50, "endOffset": 93}, {"referenceID": 4, "context": "Proof This proof technique is motivated from (Gittens and Tropp, 2011) by adding a bias matrix.", "startOffset": 45, "endOffset": 70}, {"referenceID": 21, "context": "In addition to state-of-the-art online AUC approachesOAMseq andOAMgra (Zhao et al., 2011), we also compare with: \u2022 online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss (Kotlowski et al.", "startOffset": 70, "endOffset": 89}, {"referenceID": 12, "context": ", 2011), we also compare with: \u2022 online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss (Kotlowski et al., 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al.", "startOffset": 137, "endOffset": 161}, {"referenceID": 9, "context": ", 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al.", "startOffset": 179, "endOffset": 195}, {"referenceID": 10, "context": ", 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al.", "startOffset": 280, "endOffset": 296}, {"referenceID": 12, "context": ", 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al., 2011); \u2022 batch Uni-Squ: A batch learning algorithm which optimizes the (weighted) univariate square loss.", "startOffset": 483, "endOffset": 507}, {"referenceID": 21, "context": "For OAMseq and OAMgra, the buffer sizes are fixed to be 100 as recommended in (Zhao et al., 2011).", "startOffset": 78, "endOffset": 97}, {"referenceID": 12, "context": ", class ratios) are chosen as done in (Kotlowski et al., 2011).", "startOffset": 38, "endOffset": 62}], "year": 2013, "abstractText": "AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires going through the training data only once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second-order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high-dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low-rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}