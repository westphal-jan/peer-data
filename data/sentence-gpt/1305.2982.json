{"id": "1305.2982", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2013", "title": "Estimating or Propagating Gradients Through Stochastic Neurons", "abstract": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients. A more important example is how to calculate a Gaussian (or Gaussian) approximation. In particular, it can work from the Gaussian model for the average Gaussian surface area and the Gaussian surface area. In addition, it can be used as a model for the gradient in the first place, such as in the previous work, which is to estimate the gradient by using the Gaussian distribution. In addition, it can be used to calculate a Gaussian on-surface gradient, which is the first time that the gradient in the first place is fully accurate. As explained above, this approach allows the estimation of Gaussian on-surface gradient by using an all-inclusive (not zero) linear solution of Gaussian on-surface gradient for the gradient in the first place.", "histories": [["v1", "Tue, 14 May 2013 00:29:42 GMT  (15kb)", "http://arxiv.org/abs/1305.2982v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio"], "accepted": false, "id": "1305.2982"}, "pdf": {"name": "1305.2982.pdf", "metadata": {"source": "CRF", "title": "Estimating or Propagating Gradients Through Stochastic Neurons", "authors": ["Yoshua Bengio"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 5.\n29 82\nv1 [\ncs .L\nG ]"}, {"heading": "1 Introduction and Background", "text": "Many learning algorithms and in particular those based on neural networks or deep learning rely on gradient-based learning. To compute exact gradients, it is better if the relationship between parameters and the training objective is continuous and generally smooth. If it is only constant by parts, i.e., mostly flat, then gradient-based learning\nis impractical. This was what motivated the move from neural networks made of socalled formal neurons, with a hard threshold output, to neural networks whose units are based on a sigmoidal non-linearity, and the well-known back-propagation algorithm to compute the gradients (Rumelhart et al., 1986).\nWe call computational graph or flow graph the graph that relates inputs and parameters to outputs and training criterion. Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion.\nIn principle, even if there are hard decisions (such as the treshold function typically found in formal neurons) in the computational graph, it is possible to obtain estimated gradients by introducing perturbations in the system and observing the effects. Although finite-difference approximations of the gradient appear hopelessly inefficient (because independently perturbing each of N parameters to estimate its gradient would be N times more expensive than ordinary back-propagation), another option is to introduce random perturbations, and this idea has been pushed far (and experimented on neural networks for control) by Spall (1992) with the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm.\nAs discussed here (Section 2), semi-hard non-linearities and stochastic perturbations can be combined to obtain reasonably low-variance estimators of the gradient, and a good example of that success is with the recent advances with dropout (Hinton et al., 2012; Krizhevsky et al., 2012b; Goodfellow et al., 2013). The idea is to multiply the output of a non-linear unit by independent binomial noise. This noise injection is useful as a regularizer and it does slow down training a bit, but not apparently by a lot (maybe 2-fold), which is very encouraging. The symmetry-breaking and induced sparsity may also compensate for the extra variance and possibly help to reduce ill-conditioning, as hypothesized by Bengio (2013).\nHowever, it is appealing to consider noise whose amplitude can be modulated by the signals computed in the computational graph, such as with stochastic binary neurons, which output a 1 or a 0 according to a sigmoid probability. Short of computing an average over an exponential number of configurations, it would seem that computing the exact gradient (with respect to the average of the loss over all possible binary samplings of all the stochastic neurons in the neural network) is impossible in such neural networks. The question is whether good estimators (which might have bias and variance) can be computed. We show in Section 3 that one can indeed produce an unbiased estimator of the gradient, and that this gradient is even cheaper to compute than with the back-propagation algorithm, since it does not require a backwards pass. To compensate its possibly high variance, we also propose in Section 3.2 a biased but lower-variance estimator that is based on learning a correction factor to deterministically transform a biased but low-variance estimator into a less biased one, with the same variance.\nIn a first attempt to study the question of the efficiency of such estimators, in Section 4.1, we first show that the proposed unbiased estimator can be seen as basically estimating the correlation between the perturbation and the training loss. We then show\nthat the Boltzmann machine gradient can also be interpreted as a form of correlationbased estimator, but missing an additive normalization. This is encouraging, since various Boltzmann machines (in particular the restricted Boltzmann machine) have been quite successful in recent years (Hinton et al., 2006; Bengio, 2009)."}, {"heading": "1.1 More Motivations", "text": "One motivation for studying stochastic neurons is that stochastic behavior may be a required ingredient in modeling biological neurons. The apparent noise in neuronal spike trains could come from an actual noise source or simply from the hard to reproduce changes in the set of input spikes entering a neuron\u2019s dendrites. Until this question is resolved by biological observations, it is interesting to study how such noise, which has motivated the Boltzmann machine (Hinton et al., 1984), may impact computation and learning in neural networks.\nStochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013). Sparsity of the representation corresponds to the prior that, for a given input scene x, most of the explanatory factors are irrelevant (and that would be represented by many zeros in the representation).\nSemi-hard stochastic neurons such as those studied in Section 2 also give rise to sparse gradients, i.e., such that for most examples, the gradient vector (with respect to parameters) has many zeros. Indeed, for weights into units that are shut off or are in a flat saturation region (e.g., at 0 or 1), the derivative will be zero. As argued in Bengio (2013), sparse gradients may be useful to reduce the optimization difficulty due to ill-conditioning, by reducing the number of interactions between parameters to those parameters that are simultaneously \u201cactive\u201d (with a non-zero gradient) for a particular example.\nAs argued by Bengio (2013), sparse representations may be a useful ingredient of conditional computation, by which only a small subset of the model parameters are \u201cactivated\u201d (and need to be visited) for any particular example, thereby greatly reducing the number of computations needed per example. Sparse gating units may be trained to select which part of the model actually need to be computed for a given example.\nBinary representations are also useful as keys for a hash table, as in the semantic hashing algorithm (Salakhutdinov and Hinton, 2009). In the latter, zero-mean Gaussian noise is added to the pre-sigmoid activation of the code neuron (whose output will be used as hash keys). As the amount of noise is increased, this forces the weights and biases to also increase to avoid losing too much information in the sigmoid output, thereby gradually forcing these units into saturation, i.e., producing a nearly 0 or nearly 1 output. Hence one option would be to train hard-decision neurons by gradually annealing the magnitude of the parameters, turning soft-output easy to train units into hard-decision binary-output units. However, it means that during training, we would not be able to take advantage of the hard zeros (only available at the end of training). Since both conditional computatation and sparse gradients are motivated by speeding up training, this would not be an interesting solution.\nTrainable stochastic neurons would also be useful inside recurrent networks to take hard stochastic decisions about temporal events at different time scales. This would be useful to train multi-scale temporal hierarchies 1 such that back-propagated gradients could quickly flow through the slower time scales. Multi-scale temporal hierarchies for recurrent nets have already been proposed and involved exponential moving averages of different time constants (El Hihi and Bengio, 1996), where each unit is still updated after each time step. Instead, identifying key events at a high level of abstraction would allow these high-level units to only be updated when needed (asynchronously), creating a fast track (short path) for gradient propagation through time."}, {"heading": "1.2 Prior Work", "text": "The idea of having stochastic neuron models is of course very old, with one of the major family of algorithms relying on such neurons being the Boltzmann machine (Hinton et al., 1984). In Section 4.2 we study a connection between Boltzmann machine log-likelihood gradients and perturbation-based estimators of the gradient discussed here.\nAnother biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.i.d. perturbations applied at each stochastic neuron potential (prior to a non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end up proposing a gradient estimator that looks like a correlation between the reward and the perturbation, just like what we obtain in Section 3. However, their estimator is only unbiased in the limit of small perturbations.\nGradient estimators based on stochastic perturbations have been shown for long (Spall, 1992) to be much more efficient than standard finite-difference approximations. Consider N quantities ai to be adjusted in order to minimize an expected loss L(a). A finite difference approximation is based on measuring separately the effect of changing each one of the parameters, e.g., through L(a)\u2212L(a\u2212\u01ebei)\n\u01eb , or even better, through\nL(a+\u01ebei)\u2212L(a\u2212\u01ebei) 2\u01eb , where ei = (0, 0, \u00b7 \u00b7 \u00b7 , 1, 0, 0, \u00b7 \u00b7 \u00b7 , 0) where the 1 is at position i. With N quantities (and typically O(N) computations to calculate L(a)), the computational cost of the gradient estimator is O(N2). Instead, a perturbation-based estimator such as found in Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, 1992) chooses a random perturbation vector z (e.g., isotropic Gaussian noise of variance \u03c32) and estimates the gradient of the expected loss with respect to ai through L(\u03b8+z)\u2212L(\u03b8\u2212z)\n2zi . So long as the perturbation does not put too much probability around\n0, this estimator is as efficient as the finite-difference estimator but requires O(N) less computation. However, like the algorithm proposed by Fiete and Seung (2006) this estimator becomes unbiased only as the perturbations go towards 0. When we want to consider all-or-none perturbations (like a neuron sending a spike or not), it is not clear if these assumptions are appropriate. The advantage of the unbiased estimator proposed here is that it does not require that the perturbations be small.\nAnother estimator of the expected gradient through stochastic neurons was proposed by Hinton (2012) in his lecture 15b. The idea is simply to back-propagate through the hard threshold function (1 if the argument is positive, 0 otherwise) as if\n1Daan Wierstra, personal communication\nit had been the identity function. It is clearly a biased estimator, but when considering a single layer of neurons, it has the right sign (this is not guaranteed anymore when back-propagating through more hidden layers)."}, {"heading": "2 Semi-Hard Stochastic Neurons", "text": "One way to achieve gradient-based learning in networks of stochastic neurons is to build an architecture in which noise is injected so that gradients can sometimes flow into the neuron and can then adjust it (and its predecessors in the computational graph) appropriately.\nIn general, we can consider the output hi of a stochastic neuron as the application of a determinstic function that depends on its inputs xi (typically, a vector containing the outputs of other neurons), its internal parameters \u03b8i (typically the bias and incoming weights of the neuron) and on a noise source zi:\nhi = f(xi, zi, \u03b8i). (1)\nSo long as f(xi, zi, \u03b8i) in the above equation has a non-zero gradient with respect to xi and \u03b8i, gradient-based learning (with back-propagation to compute gradients) can proceed.\nFor example, if the noise zi is added or multiplied somewhere in the computation of hi, gradients can be computed as usual. Dropout noise (Hinton et al., 2012) and masking noise (in denoising auto-encoders (Vincent et al., 2008)) is multiplied (just after the neuron non-linearity), while in semantic hashing (Salakhutdinov and Hinton, 2009) noise is added (just before the non-linearity). For example, that noise can be binomial (dropout, masking noise) or Gaussian (semantic hashing).\nHowever, if we want hi to be binary (like in stochastic binary neurons), then f will have derivatives that are 0 almost everywhere (and infinite at the threshold), so that gradients can never flow.\nThere is an intermediat option, that we put forward here: choose f so that it has two main kinds of behavior, with zero derivatives in some regions, and with significantly non-zero derivatives in other regions. We call these two states of the neuron respectively the insensitive state and the sensitive state.\nA special case is when the insensitive state corresponds to hi = 0 and we have sparsely activated neurons. The prototypical example of that situation is the rectifier unit (Nair and Hinton, 2010; Glorot et al., 2011), whose non-linearity is simply max(0, arg). For example,\nhi = max(0, zi + bi + \u2211\nj\nWijxij)\nwhere zi \u223c N (0, \u03c32) is 0-mean Gaussian noise, \u03b8i = (bi,Wi1,Wi2, . . .) and xij is the j-th input of unit i, either a raw input (visible unit) or the output of some other unit in the computational graph (hidden unit).\nLet us consider two cases:\n1. If f(xi, 0, \u03b8i) > 0, the basic state is active, the unit is generally sensitive and non-zero, but sometimes it is shut off (e.g., when zi is sufficiently negative to push the argument of the rectifier below 0). In that case gradients will flow in most cases (samples of zi). If the rest of the system sends the signal that hi should have been smaller, then gradients will push it towards being more often in the insensitive state.\n2. If f(xi, 0, \u03b8i) = 0, the basic state is inactive, the unit is generally insensitive and zero, but sometimes turned on (e.g., when zi is sufficiently positive to push the argument of the rectifier above 0). In that case gradients will not flow in most cases, but when they do, the signal will either push the weighted sum lower (if being active was not actually a good thing for that unit in that context) and reduce the chances of being active again, or it will push the weight sum higher (if being active was actually a good thing for that unit in that context) and increase the chances of being active again.\nSo it appears that even though the gradient does not always flow (as it would with sigmoid or tanh units), it might flow sufficiently often to provide the appropriate training information. The important thing to notice is that even when the basic state (second case, above) is for the unit to be insensitive and zero, there will be an occasional gradient signal that can draw it out of there.\nOne concern with this approach is that one can see an asymmetry between the number of times that a unit with an active basic state can get a chance to receive a signal telling it to become inactive, versus the number of times that a unit with an inactive basic state can get a signal telling it to become active.\nAnother potential and related concern is that some of these units will \u201cdie\u201d (become useless) if their basic state is inactive in the vast majority of cases (for example, because their weights pushed them into that regime due to random fluctations). Because of the above asymmetry, dead units would stay dead for very long before getting a chance of being born again, while live units would sometimes get into the death zone by chance and then get stuck there. What we propose here is a simple mechanism to adjust the bias of each unit so that in average its \u201cfiring rate\u201d (fraction of the time spent in the active state) reaches some pre-defined target. For example, if the moving average of being non-zero falls below a threshold, the bias is pushed up until that average comes back above the threshold."}, {"heading": "3 Unbiased Estimator of Gradient for Stochastic Binary Neurons", "text": "Let us consider the case where we want some component of our model to take a hard decision but allow this decision to be stochastic, with a probability that is a continuous function of some quantities, through parameters that we wish to learn. We will also assume that many such decisions can be taken in parallel with independent noise sources zi driving the stochastic samples. Without loss of generality, we consider here a set of binary decisions, i.e., the setup corresponds to having a set of stochastic binary\nneurons, whose output hi influences an observed future loss L. In the framework of Eq. 1, we could have for example\nhi = f(xi, zi, \u03b8i) = 1zi>\u03c3(ai) (2)\nwhere zi \u223c U [0, 1] is uniform and \u03c3(u) = 1/(1 + exp(\u2212u)) is the sigmoid function. In the case of the traditional artificial neuron, we would have\nai = bi +Wi \u00b7 xi\nand \u03b8i = (bi,Wi) is the set of parameters for unit i (scalar bias bi and weight vector Wi). We would ideally like to estimate how a change in ai would impact L in average over the noise sources, so as to be able to propagate this estimated gradients into \u03b8i and possibly into xi."}, {"heading": "3.1 Derivation of Unbiased Estimator", "text": "Theorem 1. Let hi be defined as in Eq. 2, then g\u0302i = (hi \u2212 \u03c3(ai))L is an unbiased estimator of gi = \u2202Ezi,c\u2212i [L|ci]\n\u2202ai where the expectation is over zi and over all the noise\nsources c\u2212i, besides zi, that do not influence ai but may influence L, i.e., conditioned on the set of noise sources ci that influence ai.\nProof. We will compute the expected value of the estimator and verify that it equals the desired derivative. The set of all noise sources in the system is {zi} \u222a ci \u222a c\u2212i. We can consider L to be an implicit deterministic function of all the noise sources, i.e., L = L(hi, ci, c\u2212i), where hi contains everything about zi that is needed to predict L. Evz [\u00b7] denotes the expectation over variable vz , while E[\u00b7|vz ] denotes the expectation over all the other random variables besides vz , i.e., conditioned on vZ .\nE[L|ci] = Ec \u2212i [Ezi [L(hi, ci, c\u2212i)]]\n= Ec \u2212i [Ezi [hiL(1, ci, c\u2212i) + (1\u2212 hi)L(0, ci, c\u2212i)]]\n= Ec \u2212i [P (hi = 1|ai)L(1, ci, c\u2212i) + P (hi = 0|ai)L(0, ci, c\u2212i)] = Ec \u2212i [\u03c3(ai)L(1, ci, c\u2212i) + (1\u2212 \u03c3(ai))L(0, ci, c\u2212i)] (3)\nSince ai does not influence P (c\u2212i), differentiating with respect to ai gives\ngi def =\n\u2202E[L|ci]\n\u2202ai = Ec\n\u2212i [ \u2202\u03c3(ai)\n\u2202ai L(1, ci, c\u2212i)\u2212\n\u2202\u03c3(ai)\n\u2202ai L(0, ci, c\u2212i)|ci]\n= Ec \u2212i [\u03c3(ai)(1 \u2212 \u03c3(ai))(L(1, ci, c\u2212i)\u2212 L(0, ci, c\u2212i)|ci] (4)\nFirst consider that since hi \u2208 {0, 1},\nL(hi, ci, c\u2212i) = hiL(1, ci, c\u2212i) + (1\u2212 hi)L(0, ci, c\u2212i)\nh2i = hi and hi(1\u2212 hi) = 0, so\ng\u0302i def = (hi \u2212 \u03c3(ai))L(hi, ci, c\u2212i) = hi(hi \u2212 \u03c3(ai))L(1, ci, c\u2212i) + (hi \u2212 \u03c3(ai))(1 \u2212 hi)L(0, ci, c\u2212i))\n= hi(1\u2212 \u03c3(ai))L(1, ci, c\u2212i)\u2212 (1\u2212 hi)\u03c3(ai)L(0, ci, c\u2212i). (5)\nNow let us consider the expected value of the estimator g\u0302i = (hi\u2212\u03c3(ai))L(hi, ci, c\u2212i).\nE[g\u0302i] = E[hi(1\u2212 \u03c3(ai))L(1, ci, c\u2212i)\u2212 (1\u2212 hi)\u03c3(ai)L(0, ci, c\u2212i)]\n= Eci,c\u2212i [\u03c3(ai)(1 \u2212 \u03c3(ai))L(1, ci, c\u2212i)\u2212 (1\u2212 \u03c3(ai))\u03c3(ai)L(0, ci, c\u2212i)] = Eci,c\u2212i [\u03c3(ai)(1 \u2212 \u03c3(ai))(L(1, ci, c\u2212i)\u2212 L(0, ci, c\u2212i))] (6)\nwhich is the same as Eq. 4, i.e., the expected value of the estimator equals the gradient of the expected loss, E[g\u0302i] = gi.\nCorollary 1. Under the same conditions as Theorem 1, and for any (possibly unitspecific) constant L\u0304i the centered estimator\n(hi \u2212 \u03c3(ai))(L \u2212 L\u0304i),\nis also an unbiased estimator of gi = \u2202Ezi,c\u2212i [L|ci]\n\u2202ai . Furthermore, among all possible\nvalues of L\u0304i, the minimum variance choice is\nL\u0304i = E[(hi \u2212 \u03c3(ai))2L]\nE[(hi \u2212 \u03c3(ai))2] , (7)\nwhich we note is a weighted average of the loss values L, whose weights are specific to unit i.\nProof. The centered estimator (hi \u2212 \u03c3(ai))(L\u2212 L\u0304i) can be decomposed into the sum of the uncentered estimator g\u0302i and the term (hi \u2212 \u03c3(ai))L\u0304i. Since Ezi [hi|ai] = \u03c3(ai), E[L\u0304i(hi \u2212 \u03c3(ai))|ai] = 0, so that the expected value of the centered estimator equals the expected value of the uncentered estimator. By Theorem 1 (the uncentered estimator is unbiased), the centered estimator is therefore also unbiased, which completes the proof of the first statement.\nRegarding the optimal choice of L\u0304i, first note that the variance of the uncentered estimator is\nV ar[(hi \u2212 \u03c3(ai))L] = E[(hi \u2212 \u03c3(ai)) 2L2]\u2212 E[g\u0302i] 2.\nNow let us compute the variance of the centered estimator:\nV ar[(hi \u2212 \u03c3(ai))(L \u2212 L\u0304i)] = E[(hi \u2212 \u03c3(ai)) 2(L\u2212 L\u0304i) 2]\u2212 E[(hi \u2212 \u03c3(ai))(L \u2212 L\u0304i)] 2\n= E[(hi \u2212 \u03c3(ai)) 2L2] + E[(hi \u2212 \u03c3(ai)) 2L\u03042i ]\n\u22122E[(hi \u2212 \u03c3(ai)) 2LL\u0304i]\u2212 (E[g\u0302i]\u2212 0) 2\n= V ar[(hi \u2212 \u03c3(ai))L]\u2212\u2206 (8)\nwhere \u2206 = 2E[(hi \u2212 \u03c3(ai))2LL\u0304i]\u2212 E[(hi \u2212 \u03c3(ai))2L\u03042i ]. Let us rewrite \u2206:\n\u2206 = 2E[(hi \u2212 \u03c3(ai)) 2LL\u0304i]\u2212 E[(hi \u2212 \u03c3(ai)) 2L\u03042i ]\n= E[(hi \u2212 \u03c3(ai)) 2L\u0304i(2L\u2212 L\u0304i)]\n= E[(hi \u2212 \u03c3(ai)) 2(L2 \u2212 (L\u2212 L\u0304i) 2)] (9)\n\u2206 is maximized (to minimize variance of the estimator) when E[(hi \u2212 \u03c3(ai))2(L \u2212 L\u0304i)\n2] is minimized. Taking the derivative of that expression with respect to L\u0304i, we obtain\n2E[(hi \u2212 \u03c3(ai)) 2(L\u0304i \u2212 L)] = 0\nwhich is achieved for\nL\u0304i = E[(hi \u2212 \u03c3(ai))2L]\nE[(hi \u2212 \u03c3(ai))2]\nas claimed.\nPractically, we could get the lowest variance estimator (among all choices of the L\u0304i) by keeping track of two numbers (running or moving averages) for each stochastic neuron, one for the numerator and one for the denominator of the unit-specific L\u0304i in Eq. 7. This would lead the lowest-variance estimator\n(hi \u2212 \u03c3(ai))(L \u2212 L\u0304i).\nNote how the unbiased estimator only requires broadcasting L throughout the network, no back-propagation and only local computation. Note also how this could be applied even with an estimate of future rewards or losses L, as would be useful in the context of reinforcement learning (where the actual loss or reward will be measured farther into the future, much after hi has been sampled)."}, {"heading": "3.2 Training a Lower-Variance Biased Estimator", "text": "One potential problem with the above unbiased estimators is that their variance could be large enough to considerably slow training, when compared to using stochastic gradient descent with back-propagated gradients.\nWe propose here a general class of solutions to address that challenge, but for this purpose we need to have a biased but low-variance estimator."}, {"heading": "3.2.1 Biased Low-Variance Estimator", "text": "A plausible unbiased estimator is the developed below. Let us G\u0302j be an estimator of the gradient of the expected loss with respect to the activation (pre-nonlinearity) aj of unit j, and let unit j compute its activation as a deterministic smooth function of the output hi of unit i (for example, aj = \u2211 iWjihi). Then we can clearly get an estimator of the gradient with respect to hi by \u2211\nj G\u0302j \u2202aj \u2202hi\n. The problem is to back-propagate through the binary threshold function which produced hi from the noise zi and the activation ai (Eq. 2). The biased estimator we propose here2 is simply\nG\u0302i = \u2211\nj\nG\u0302j \u2202aj \u2202hi\nas the estimator of the gradient of the expected loss with respect to ai, i.e., we ignore the derivative of the threshold function f .\n2already explored by Goeff Hinton (Hinton, 2012), lecture 15b"}, {"heading": "3.2.2 Combining a Low-Variance High-Bias Estimator with a High-Variance Low-Bias Estimator", "text": "Let us assume someone hands us two estimators G\u0302i and g\u0302i, with the first having low variance but high bias, while the second has high variance and low bias. How could we take advantage of them to obtain a better estimator?\nWhat we propose is the following: train a function Gi which takes as input the high-bias low-variance estimator G\u0302i and predicts the low-bias high-variance estimator g\u0302i.\nBy construction, since Gi is a deterministic function of a low-variance quantity (we could add other inputs to help it in its prediction, but they should not be too noisy), it should also have low variance. Also by construction, and to the extent that the learning task is feasible, the prediction Gi will strive to be as close as possible to the expected value of the unbiased estimator, i.e., Gi \u2192 E[g\u0302i|G\u0302i]. It is therefore a way to unbias G\u0302i to the extent that it is possible. Note that adding appropriate auxiliary inputs to Gi could be helpful in this respect."}, {"heading": "4 Efficiency of Reward Correlation Estimators", "text": "One of the questions that future work should address is the efficiency of estimators such as those introduced above."}, {"heading": "4.1 The Unbiased Estimator as Reward Correlator", "text": "In this respect, it is interesting to note how the proposed unbiased estimator (in particular the centered one) is very similar in form to the just estimating the correlation between the stochastic decision hi and the ensuing loss L:\nCorrelation = E[(hi \u2212 E[hi|ci])(L \u2212 E[L|ci])|ci]\nNote how this is the correlation between hi and L in the context of the other noise sources ci that influence hi. Note that a particular \u201cnoise source\u201d is just the input of the model."}, {"heading": "4.2 The Boltzmann Machine Gradient as Unnormalized Reward Correlation", "text": "The log-likelihood gradient over a bias (offset) parameter bi associated with a unit Xi (visible or hidden) of a Boltzmann machine with distribution P and a training example v (associated with visible units V ) is\n\u2202 logP (V = v)\n\u2202bi = E[Xi|V = v]\u2212 E[Xi]\nwhere the expectation is over the model\u2019s distribution, which defines a joint distribution between all the units of the model, including the visible ones. The conditional\ndistribution of binary unit Xi given the other units is given by\nP (Xi = 1|X\u2212i) = \u03c3(ai) = \u03c3(bi + \u2211\nj 6=i\nWijXj).\nwhere ai is the unit activation (prior to applying the sigmoid). An unbiased estimator of the above gradient is\nX+i \u2212X \u2212 i\nwhere X+ is a configuration obtained while V was clamped to a training example v, and X\u2212 is a configuration obtained without this constraint. Similarly, the loglikelihood gradient over weight Wij is estimated unbiasedly by\nX+i X \u2212 j \u2212X \u2212 i X \u2212 j .\nWe call these estimators the Boltzmann machine log-likelihood gradient estimators. We now show that this log-likelihood gradient can also be interpreted as an unnormalized reward correlator in a particular setup, where the objective is to discriminate between examples coming from the data generating distribution and examples from the model distribution P .\nThe setup is the following. Let the model generate samples X = (V,H), where V are the visible units and H the hidden units. Let the reward for generating a \u201cnegative example\u201d V = V \u2212 in this way be R = \u22121. However, let us toss a coin to select from this stream some samples (V,H) such that we can declare X = X+ as coming from the training distribution of interest \u03c0, for example using rejection sampling. Then we let the reward be R = 1 because the model has generated a \u201cgood example\u201d V +. Although this might be a rare event let us randomly choose among the negative examples V \u2212 so that the average number of negative examples equals the average number of positive examples V +. Clearly the samples V + follow the training distribution \u03c0 while the samples V \u2212 follow the model distribution P . In our setup, let us imagine that X\u2019s were obtained by Gibbs sampling. The Gibbs chain corresponds to constructing a computational graph that deterministically or stochastically computes various quantities (one per node of the graph), here the activation ait = bi + \u2211 j 6=i WijXjt\u22121 and the binomially sampled bit Xit \u223c Bin(\u03c3(ait)) for each stochastic unit at each step t of the chain.\nUsing Theorem 1, (Xit \u2212 \u03c3(ait))R is an unbiased estimator of\n\u2202E[R|cit]\n\u2202ait .\nNow we consider an unnormalized variant of that estimator,\nXitR\nwhere X (rather than its centered version) is multiplied by R. This means in particular that XitR is a (possibly biased) estimator of the gradient with respect to bi, while, by the chain rule XitRXj,t\u22121 is an estimator of the gradient with respect to Wij .\nIf we separate the training examples into those with R = 1 and those with R = \u22121, we obtain the estimator associated with a (V +, V \u2212) pair,\nX+i \u2212X \u2212 i\nfor biases bi, and X+i X + j \u2212X \u2212 i X \u2212 j\nfor weights Wij . The above two estimators of the gradients \u2202E[R|cit] \u2202bi and \u2202E[R|cit] \u2202Wij correspond exactly to the Boltzmann machine log-likelihood gradient estimators."}, {"heading": "5 Conclusion", "text": "In this paper, we have motivated estimators of the gradient through highly non-linear non-differentiable functions (such as those corresponding to an indicator function), especially in networks involving noise sources, such as neural networks with stochastic neurons. They can be useful as biologically motivated models and they might be useful for engineering (computational efficiency) reasons when trying to reduce computation via conditional computation or to reduce interactions between parameters via sparse updates (Bengio, 2013). We have discussed a general class of stochastic neurons for which the gradient is exact for given fixed noise sources, but where the non-linearity is not saturating on all of its range (semi-hard stochastic neurons), and for which ordinary back-prop can be used. We have also discussed the case of completely saturating nonlinearities, for which we have demonstrated the existence of an unbiased estimator based on correlating the perturbation with the observed reward, which is related to but different from the SPSA (Spall, 1992) estimator. Indeed the latter divides the change in reward by the perturbation, instead of multiplying them.\nWe have shown that it was possible in principle to obtain lower variance estimators, in particular by training a function to turn a biased estimator but low variance estimator into one that is trained to be unbiased but has lower variance. We have also shown that the Boltzmann machine gradient could be interpreted as a particular form of reward correlation (without the additive normalization). Since training of Restricted Boltzmann Machines has been rather successful, this suggests that correlation-based estimators might actually work well in practice. Clearly, future work should investigate the relative practical merits of these estimators, and in particular how their variance scale with respect to the number of independent noise sources (e.g., stochastic neurons) present in the system."}, {"heading": "Acknowledgements", "text": "The author would like to thank NSERC, the Canada Research Chairs, and CIFAR for support."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers. 12", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Technical Report arXiv:1305.0445, Universite de Montreal.", "citeRegEx": "Bengio,? 2013", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI).", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. El Hihi", "Y. Bengio"], "venue": "NIPS 8. MIT Press.", "citeRegEx": "Hihi and Bengio,? 1996", "shortCiteRegEx": "Hihi and Bengio", "year": 1996}, {"title": "Gradient learning in spiking neural networks by dynamic perturbations of conductances", "author": ["I.R. Fiete", "H.S. Seung"], "venue": "Physical Review Letters, 97(4).", "citeRegEx": "Fiete and Seung,? 2006", "shortCiteRegEx": "Fiete and Seung", "year": 2006}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML\u20192013.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Neural networks for machine learning", "author": ["G. Hinton"], "venue": "Coursera, video lectures.", "citeRegEx": "Hinton,? 2012", "shortCiteRegEx": "Hinton", "year": 2012}, {"title": "Boltzmann machines: Constraint satisfaction networks that learn", "author": ["G.E. Hinton", "T.J. Sejnowski", "D.H. Ackley"], "venue": "Technical Report TR-CMU-CS-84-119, Carnegie-Mellon University, Dept. of Computer Science.", "citeRegEx": "Hinton et al\\.,? 1984", "shortCiteRegEx": "Hinton et al\\.", "year": 1984}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W."], "venue": "Neural Computation, 18, 1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical report, arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS\u20192012.", "citeRegEx": "Krizhevsky et al\\.,? 2012a", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25 (NIPS\u20192012).", "citeRegEx": "Krizhevsky et al\\.,? 2012b", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML\u201910.", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323, 533\u2013536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "International Journal of Approximate Reasoning.", "citeRegEx": "Salakhutdinov and Hinton,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation", "author": ["J.C. Spall"], "venue": "IEEE Transactions on Automatic Control, 37, 332\u2013341.", "citeRegEx": "Spall,? 1992", "shortCiteRegEx": "Spall", "year": 1992}], "referenceMentions": [{"referenceID": 14, "context": "This was what motivated the move from neural networks made of socalled formal neurons, with a hard threshold output, to neural networks whose units are based on a sigmoidal non-linearity, and the well-known back-propagation algorithm to compute the gradients (Rumelhart et al., 1986).", "startOffset": 259, "endOffset": 283}, {"referenceID": 5, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion.", "startOffset": 259, "endOffset": 331}, {"referenceID": 11, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion.", "startOffset": 259, "endOffset": 331}, {"referenceID": 6, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion.", "startOffset": 259, "endOffset": 331}, {"referenceID": 10, "context": "As discussed here (Section 2), semi-hard non-linearities and stochastic perturbations can be combined to obtain reasonably low-variance estimators of the gradient, and a good example of that success is with the recent advances with dropout (Hinton et al., 2012; Krizhevsky et al., 2012b; Goodfellow et al., 2013).", "startOffset": 240, "endOffset": 312}, {"referenceID": 12, "context": "As discussed here (Section 2), semi-hard non-linearities and stochastic perturbations can be combined to obtain reasonably low-variance estimators of the gradient, and a good example of that success is with the recent advances with dropout (Hinton et al., 2012; Krizhevsky et al., 2012b; Goodfellow et al., 2013).", "startOffset": 240, "endOffset": 312}, {"referenceID": 6, "context": "As discussed here (Section 2), semi-hard non-linearities and stochastic perturbations can be combined to obtain reasonably low-variance estimators of the gradient, and a good example of that success is with the recent advances with dropout (Hinton et al., 2012; Krizhevsky et al., 2012b; Goodfellow et al., 2013).", "startOffset": 240, "endOffset": 312}, {"referenceID": 3, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion. In principle, even if there are hard decisions (such as the treshold function typically found in formal neurons) in the computational graph, it is possible to obtain estimated gradients by introducing perturbations in the system and observing the effects. Although finite-difference approximations of the gradient appear hopelessly inefficient (because independently perturbing each of N parameters to estimate its gradient would be N times more expensive than ordinary back-propagation), another option is to introduce random perturbations, and this idea has been pushed far (and experimented on neural networks for control) by Spall (1992) with the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm.", "startOffset": 260, "endOffset": 1043}, {"referenceID": 0, "context": "The symmetry-breaking and induced sparsity may also compensate for the extra variance and possibly help to reduce ill-conditioning, as hypothesized by Bengio (2013). However, it is appealing to consider noise whose amplitude can be modulated by the signals computed in the computational graph, such as with stochastic binary neurons, which output a 1 or a 0 according to a sigmoid probability.", "startOffset": 151, "endOffset": 165}, {"referenceID": 9, "context": "This is encouraging, since various Boltzmann machines (in particular the restricted Boltzmann machine) have been quite successful in recent years (Hinton et al., 2006; Bengio, 2009).", "startOffset": 146, "endOffset": 181}, {"referenceID": 0, "context": "This is encouraging, since various Boltzmann machines (in particular the restricted Boltzmann machine) have been quite successful in recent years (Hinton et al., 2006; Bengio, 2009).", "startOffset": 146, "endOffset": 181}, {"referenceID": 8, "context": "Until this question is resolved by biological observations, it is interesting to study how such noise, which has motivated the Boltzmann machine (Hinton et al., 1984), may impact computation and learning in neural networks.", "startOffset": 145, "endOffset": 166}, {"referenceID": 0, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013).", "startOffset": 231, "endOffset": 266}, {"referenceID": 2, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013).", "startOffset": 231, "endOffset": 266}, {"referenceID": 15, "context": "Binary representations are also useful as keys for a hash table, as in the semantic hashing algorithm (Salakhutdinov and Hinton, 2009).", "startOffset": 102, "endOffset": 134}, {"referenceID": 0, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013). Sparsity of the representation corresponds to the prior that, for a given input scene x, most of the explanatory factors are irrelevant (and that would be represented by many zeros in the representation). Semi-hard stochastic neurons such as those studied in Section 2 also give rise to sparse gradients, i.e., such that for most examples, the gradient vector (with respect to parameters) has many zeros. Indeed, for weights into units that are shut off or are in a flat saturation region (e.g., at 0 or 1), the derivative will be zero. As argued in Bengio (2013), sparse gradients may be useful to reduce the optimization difficulty due to ill-conditioning, by reducing the number of interactions between parameters to those parameters that are simultaneously \u201cactive\u201d (with a non-zero gradient) for a particular example.", "startOffset": 232, "endOffset": 832}, {"referenceID": 0, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013). Sparsity of the representation corresponds to the prior that, for a given input scene x, most of the explanatory factors are irrelevant (and that would be represented by many zeros in the representation). Semi-hard stochastic neurons such as those studied in Section 2 also give rise to sparse gradients, i.e., such that for most examples, the gradient vector (with respect to parameters) has many zeros. Indeed, for weights into units that are shut off or are in a flat saturation region (e.g., at 0 or 1), the derivative will be zero. As argued in Bengio (2013), sparse gradients may be useful to reduce the optimization difficulty due to ill-conditioning, by reducing the number of interactions between parameters to those parameters that are simultaneously \u201cactive\u201d (with a non-zero gradient) for a particular example. As argued by Bengio (2013), sparse representations may be a useful ingredient of conditional computation, by which only a small subset of the model parameters are \u201cactivated\u201d (and need to be visited) for any particular example, thereby greatly reducing the number of computations needed per example.", "startOffset": 232, "endOffset": 1118}, {"referenceID": 8, "context": "The idea of having stochastic neuron models is of course very old, with one of the major family of algorithms relying on such neurons being the Boltzmann machine (Hinton et al., 1984).", "startOffset": 162, "endOffset": 183}, {"referenceID": 16, "context": "Gradient estimators based on stochastic perturbations have been shown for long (Spall, 1992) to be much more efficient than standard finite-difference approximations.", "startOffset": 79, "endOffset": 92}, {"referenceID": 16, "context": "Instead, a perturbation-based estimator such as found in Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, 1992) chooses a random perturbation vector z (e.", "startOffset": 115, "endOffset": 128}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.", "startOffset": 87, "endOffset": 110}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.i.d. perturbations applied at each stochastic neuron potential (prior to a non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end up proposing a gradient estimator that looks like a correlation between the reward and the perturbation, just like what we obtain in Section 3.", "startOffset": 87, "endOffset": 338}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.i.d. perturbations applied at each stochastic neuron potential (prior to a non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end up proposing a gradient estimator that looks like a correlation between the reward and the perturbation, just like what we obtain in Section 3. However, their estimator is only unbiased in the limit of small perturbations. Gradient estimators based on stochastic perturbations have been shown for long (Spall, 1992) to be much more efficient than standard finite-difference approximations. Consider N quantities ai to be adjusted in order to minimize an expected loss L(a). A finite difference approximation is based on measuring separately the effect of changing each one of the parameters, e.g., through L(a)\u2212L(a\u2212\u01ebei) \u01eb , or even better, through L(a+\u01ebei)\u2212L(a\u2212\u01ebei) 2\u01eb , where ei = (0, 0, \u00b7 \u00b7 \u00b7 , 1, 0, 0, \u00b7 \u00b7 \u00b7 , 0) where the 1 is at position i. With N quantities (and typically O(N) computations to calculate L(a)), the computational cost of the gradient estimator is O(N). Instead, a perturbation-based estimator such as found in Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, 1992) chooses a random perturbation vector z (e.g., isotropic Gaussian noise of variance \u03c3) and estimates the gradient of the expected loss with respect to ai through L(\u03b8+z)\u2212L(\u03b8\u2212z) 2zi . So long as the perturbation does not put too much probability around 0, this estimator is as efficient as the finite-difference estimator but requires O(N) less computation. However, like the algorithm proposed by Fiete and Seung (2006) this estimator becomes unbiased only as the perturbations go towards 0.", "startOffset": 87, "endOffset": 1765}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.i.d. perturbations applied at each stochastic neuron potential (prior to a non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end up proposing a gradient estimator that looks like a correlation between the reward and the perturbation, just like what we obtain in Section 3. However, their estimator is only unbiased in the limit of small perturbations. Gradient estimators based on stochastic perturbations have been shown for long (Spall, 1992) to be much more efficient than standard finite-difference approximations. Consider N quantities ai to be adjusted in order to minimize an expected loss L(a). A finite difference approximation is based on measuring separately the effect of changing each one of the parameters, e.g., through L(a)\u2212L(a\u2212\u01ebei) \u01eb , or even better, through L(a+\u01ebei)\u2212L(a\u2212\u01ebei) 2\u01eb , where ei = (0, 0, \u00b7 \u00b7 \u00b7 , 1, 0, 0, \u00b7 \u00b7 \u00b7 , 0) where the 1 is at position i. With N quantities (and typically O(N) computations to calculate L(a)), the computational cost of the gradient estimator is O(N). Instead, a perturbation-based estimator such as found in Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, 1992) chooses a random perturbation vector z (e.g., isotropic Gaussian noise of variance \u03c3) and estimates the gradient of the expected loss with respect to ai through L(\u03b8+z)\u2212L(\u03b8\u2212z) 2zi . So long as the perturbation does not put too much probability around 0, this estimator is as efficient as the finite-difference estimator but requires O(N) less computation. However, like the algorithm proposed by Fiete and Seung (2006) this estimator becomes unbiased only as the perturbations go towards 0. When we want to consider all-or-none perturbations (like a neuron sending a spike or not), it is not clear if these assumptions are appropriate. The advantage of the unbiased estimator proposed here is that it does not require that the perturbations be small. Another estimator of the expected gradient through stochastic neurons was proposed by Hinton (2012) in his lecture 15b.", "startOffset": 87, "endOffset": 2197}, {"referenceID": 10, "context": "Dropout noise (Hinton et al., 2012) and masking noise (in denoising auto-encoders (Vincent et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 15, "context": ", 2008)) is multiplied (just after the neuron non-linearity), while in semantic hashing (Salakhutdinov and Hinton, 2009) noise is added (just before the non-linearity).", "startOffset": 88, "endOffset": 120}, {"referenceID": 13, "context": "The prototypical example of that situation is the rectifier unit (Nair and Hinton, 2010; Glorot et al., 2011), whose non-linearity is simply max(0, arg).", "startOffset": 65, "endOffset": 109}, {"referenceID": 5, "context": "The prototypical example of that situation is the rectifier unit (Nair and Hinton, 2010; Glorot et al., 2011), whose non-linearity is simply max(0, arg).", "startOffset": 65, "endOffset": 109}, {"referenceID": 7, "context": "2already explored by Goeff Hinton (Hinton, 2012), lecture 15b", "startOffset": 34, "endOffset": 48}, {"referenceID": 1, "context": "They can be useful as biologically motivated models and they might be useful for engineering (computational efficiency) reasons when trying to reduce computation via conditional computation or to reduce interactions between parameters via sparse updates (Bengio, 2013).", "startOffset": 254, "endOffset": 268}, {"referenceID": 16, "context": "We have also discussed the case of completely saturating nonlinearities, for which we have demonstrated the existence of an unbiased estimator based on correlating the perturbation with the observed reward, which is related to but different from the SPSA (Spall, 1992) estimator.", "startOffset": 255, "endOffset": 268}], "year": 2013, "abstractText": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \u201cback-propagate\u201d through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.", "creator": "LaTeX with hyperref package"}}}