{"id": "1305.6650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2013", "title": "Active Sensing as Bayes-Optimal Sequential Decision Making", "abstract": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller) and C-DAC (Context-Dependent Active Controller) in a series of experiments on visual perception and language acquisition. The model provides a reliable means for understanding whether each participant or object is consciously or subconsciously aware of visual stimuli. However, if the participant or object is consciously or subconsciously aware of what they perceive in an unfamiliar context, it is difficult to distinguish between the perceptible and the real. To date, there have been no effective methods of applying these techniques. This study showed that the potential for the enhancement of neural processing in non-verbal subjects was enhanced by a similar measure when subjects are consciously or unconsciously aware of the visual stimulus. We are continuing to explore and refine models for enhanced visual processing in non-verbal subjects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 28 May 2013 22:46:35 GMT  (356kb,D)", "http://arxiv.org/abs/1305.6650v1", "Scheduled to appear in UAI 2013"]], "COMMENTS": "Scheduled to appear in UAI 2013", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["sheeraz ahmad", "angela j yu"], "accepted": false, "id": "1305.6650"}, "pdf": {"name": "1305.6650.pdf", "metadata": {"source": "CRF", "title": "Active Sensing as Bayes-Optimal Sequential Decision-Making", "authors": ["Sheeraz Ahmad", "Angela J. Yu"], "emails": [], "sections": [{"heading": null, "text": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko and Movellan, 2010] or one-step look-ahead accuracy [Najemnik and Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and sensor repositioning cost. We simulate these algorithms on a simple visual search task to illustrate scenarios in which contextsensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the CDAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate a more complex search problem involving peripheral vision, and we notice that the performance advantage of C-DAC over generic statistical policies is even more evident in this scenario."}, {"heading": "1 Introduction", "text": "In the realm of symbolic problem solving, computers are sometimes comparable, or even better than, typical human performance. In contrast, in sensory\nprocessing, especially under conditions of noise, uncertainty, or non-stationarity, human performance is often still the gold standard [Martin et al., 2001, Branson et al., 2011]. One important tool the brain has at its disposal is active sensing, a goal-directed, contextsensitive control strategy that prioritizes sensing resources toward the most rewarding or informative aspects of the environment [Yarbus, 1967]. Most theoretical models of sensory processing presume passiveness, considering only how to represent or compute with given inputs, and not how to actively intervene in the input collection process itself, especially with respect to behavioral goals or environmental constraints. Having a formal understanding of active sensing is not only important for advancing neuroscientific progress but also for engineering applications, such as developing context-sensitive, interactive artificial agents.\nThe most well-studied aspect of human active sensing is saccadic eye movements, and early work suggests that saccades are attracted to salient targets that differ from surround in one or more of feature dimensions such as orientation, motion, luminance, and color contrast [Koch and Ullman, 1985, Itti and Koch, 2000]. This passive explanation does not take into account the fact that the observations made while attending the task can affect the fixations decisions that follow. More recently, there has been a shift to relax this constraint of passiveness, and the notion of saliency has been reframed probabilistically in terms of maximizing the informational gain (Infomax) given the spatial and temporal context [Lee and Yu, 2000, Itti and Baldi, 2006, Butko and Movellan, 2010]. Separately, in another active formulation, it has been proposed that saccades are chosen to maximize the greedy, one-step look-ahead probability of finding the target (greedy MAP), conditioned on self knowledge about visual acuity map [Najemnik and Geisler, 2005].\nWhile both the Infomax and Greedy MAP algorithms brought a new level of sophistication \u2013 representing sensory processing as iterative Bayesian inference,\nar X\niv :1\n30 5.\n66 50\nv1 [\ncs .A\nI] 2\n8 M\nay 2\nquantifying the knowledge gain of different saccade choices, and incorporating knowledge about sensory noise \u2013 they are still limited in several key respects: (1) they optimize abstract computational quantities that do not directly relate to behavioral goals (eg, speed and accuracy) or task constraints (eg, cost of switching from one location to another); (2) relatedly, it is unclear how to adapt these algorithms to varying task goals (eg, locating someone in a crowd versus catching a moving object); (3) there is no explicit representation of time in these algorithms, and thus no means of trading off fixation duration or number of fixations with search accuracy. In the rest of the paper, we refer to Infomax and Greedy MAP as \u201cstatistical policies\u201d, in the sense that they optimize generic statistical objectives insensitive to behavioral objectives or contextual constraints.\nIn contrast to the statistical policies, we propose a Bayes-optimal inference and control framework for active sensing, which we call C-DAC (ContextDependent Active Controller). Specifically, we assume that the observer aims to optimize a context-sensitive objective function that takes into account behavioral costs such as temporal delay, response error, and the cost of switching from one sensing location to another. C-DAC uses this objective to choose when and where to collect sensory data, based on a continually updated statistically optimal (Bayesian) representation of the sequentially collected sensory data. This framework allows us to derive behaviorally optimal procedures for making decisions about (1) where to acquire sensory inputs, (2) when to move from one observation location to another, and (3) how to negotiate the exploration-exploitation tradeoff between collecting additional data and terminating the observation process. We also compare the performance of C-DAC and the statistical policies under different task parameters, and illustrate scenarios in which the latter perform particularly poorly. Finally, we present two approximate value iteration algorithms, based on a lowdimensional parametric and non-parametric approximation of the value function, which retain contextsensitivity while significantly reducing computational complexity.\nIn Sec. 2, we describe in detail the C-DAC model. In Sec. 3, we apply the model to a visual search task, simulating scenarios where C-DAC achieves a flexible trade-off between speed, accuracy and effort depending on the task demands, whereas the statistical policies fall short \u2013 this forms experimentally testable predictions for future investigations. We also present approximate value-iteration algorithms, and an extension of the search problem that incorporates peripheral vision. We conclude with a discussion of the implica-\ntions of this work, relationship to previous work, as well as pointers to future work (Sec. 4)."}, {"heading": "2 The Model: C-DAC", "text": "We consider a scenario in which the observer must produce a response based on sequentially observed noisy sensory inputs (e.g., identifying target location in a search task or scene category in a classification task), with the ability to choose where and how long to collect the sensory inputs."}, {"heading": "2.1 Sensory Processing: Bayesian Inference", "text": "We use a Bayesian generative model to capture the observer\u2019s knowledge about the statistical relationship among hidden causes or variables and how they give rise to noisy sensory inputs, as well as prior beliefs of hidden variables. We assume that they use exact Bayesian inference in the recognition model to maintain a statistically optimal representation of the hidden state of the world based on the noisy data stream.\nConditioned on the target location (s, hidden) and the sequence of fixation locations (\u03bbt := {\u03bb1, . . . , \u03bbt}, known), the agent sequentially observes iid inputs (xt := {x1, . . . , xt}):\np(xt|s;\u03bbt) = t\u220f i=1 p(xi|s;\u03bbi) = t\u220f i=1 fs,\u03bbi(xi) (1)\nwhere fs,\u03bb(xt) is the likelihood function. These variables can be scalars or vectors, depending on the specific problem.\nIn the recognition model, repeated applications of Bayes\u2019 Rule can be used to compute the iterative posterior distribution over the k possible target locations, or the belief state:\npt := (P (s = 1|xt;\u03bbt), . . . , P (s = k|xt;\u03bbt)) pit = P (s = i|xt;\u03bbt) \u221d p(xt|s = i;\u03bbt)P (s = i|xt\u22121;\u03bbt\u22121)\n= fs,\u03bbt(xt)p i t\u22121 (2)\nwhere p0 is the prior belief over target location."}, {"heading": "2.2 Action Selection: Bayes Risk Minimization", "text": "The action selection component of active vision is a stochastic control problem where the agent chooses the sensing location and the number of data points collected, and we assume the agent can optimize this process dynamically based on ongoing data collection and size of sensory data, but the exact consequence of each action is not perfectly known ahead of time.\nThe goal is to find a good decision policy \u03c0, which maps the augmented belief state (xt,\u03bbt) into an action a \u2208 A, where A consists of a set of termination actions, stopping and choosing a response, and a set of continuation actions, obtaining data point from a certain observation location. The policy \u03c0 produces for each observation sequence (x1, . . . , xt, . . .), a stopping time \u03c4 (number of data points observed), a sequence of fixation choices \u03bb\u03c4 := (\u03bb1, . . . , \u03bb\u03c4 ), and an eventual target choice \u03b4.\nIn the Bayes risk minimization framework, the optimization problem is formulated in terms of minimizing an expected cost function, L\u03c0 := E[l(\u03c4,\u03bb\u03c4 , \u03b4)]x,s, averaged over stochasticity in the true target location s and the data samples x. We assume that the cost incurred on each trial takes into account temporal delay, switch cost (cost associated with each switch in sensing location), and response error, respectively. In accordance with the typical Bayes risk formulation of the sequential decision problem, we assume the cost function to be a linear combination of the relevant factors:\nl(\u03c4, \u03b4;\u03bb\u03c4 , s) = c\u03c4 + csn\u03c4 + 1{\u03b4 6=s} (3)\nwhere n\u03c4 is the total number of switches (n\u03c4 :=\u2211\u03c4\u22121 t=1 1{\u03bbt+1 6=\u03bbt}), c parameterizes the cost of temporal delay, cs the cost of a switch, and unit cost for response errors is assumed (as we can always divide c and cs by the appropriate constant to make it 1). The expected cost is L\u03c0 := cE[\u03c4 ]+ csE[ns]+P (\u03b4 6= s), where the expectation is taken over \u03c4 , \u03bb, \u03b4, and x\u03c4 .\nBellman\u2019s dynamic programming equation [Bellman, 1952] tells us that the problem is optimized if at each time point, the agent chooses the action associated with the lowest expected cost (the Q-factor for that action), given his current knowledge or belief state, pt. The Q-factors for the stopping actions are straight forward: Q\u0304it(pt,\u03bbt) := E[l(t, i)|pt,\u03bbt] = ct + csnt + (1\u2212pit). Obviously, the best stopping action \u03b4 is to minimize the probability of error. Thus, the stopping cost associated with the optimal stopping action (i\u2217 := argmaxi p i t) is:\nQ\u0304\u2217t (pt,\u03bbt) := E[l(t, i\u2217)|pt,\u03bbt] = ct+ csnt + (1\u2212pi \u2217 t ) (4)\nThe Q-factor associated with each continuation action j (continue sensing in location j) is:\nQjt (pt = p,\u03bbt) := c(t+ 1) + cs(nt + 1{j 6=\u03bbt})+\nmin \u03c4 \u2032,\u03b4,\u03bb\u03c4\u2032\nE[l(\u03c4 \u2032, \u03b4)|p0 =p, \u03bb1 = j] (5)\nwith the optimal continuation action being Q\u2217t := minj Q j t = Q j\u2217\nt . The expected cost of continuing observing in location j is equivalent to solving the original optimization problem with the prior belief set to\nthe posterior after the previous t time-steps, and the first observation location being j. Suppose we define the value function V (p, i) as the expected cost associated with the optimal policy, given prior belief p0 = p and initial observation location \u03bb1 = i:\nV (p, i) := min \u03c4,\u03b4,\u03bb\u03c4\nE[l(\u03c4, \u03b4)|p0 =p, \u03bb1 = i] . (6)\nThen the value function satisfies the following recursive relation:\nV (p, k) = min(Q\u0304\u22171(p, k), Q \u2217 1(p, k)) = min ((\nmin i Q\u0304i1(p, k)\n) ,\nmin j\n( c+ cs1{j 6=k} + E[V (p \u2032, j)] ))\n(7)\nwhere p\u2032 is the belief state at next time-step, and the expectation is taken over the stochasticity in the next observation x. The optimal policy effectively divides the belief state space into a stopping region (Q\u0304\u2217 \u2264 Q\u2217) and a continuation region (Q\u0304\u2217 > Q\u2217), each of which further divided into subregions corresponding to alternative continuation and stopping actions. Note that the optimal decision policy is a stationary policy: the value function depends only on the belief state and observation location at the time the decision is to be taken, and not on time t per se.\nBellman\u2019s dynamic programming principle implies a numerical algorithm for computing the optimal policy: guess an initial setting V \u2032(p, k) of the value function (e.g., minimal stopping cost associated with each belief state p and observation location k), then iterate Eq. 7 until convergence, which yields the value function V (p, k) = V\u221e(p, k)."}, {"heading": "3 Case Study: Visual Search", "text": "In this section, we apply the active sensing model to a simple, three location visual search task, where we can compute the exact optimal policy (up to discretization of the state space), and compare its performance with the statistical policies [Butko and Movellan, 2010, Najemnik and Geisler, 2005]. The target and distractors differ in terms of the likelihood of observations received, when looking at them."}, {"heading": "3.1 C-DAC Policy", "text": "For simplicity, we assume that the observations are binary and Bernoulli distributed (iid conditioned on target and fixation locations):\np(x|s = i;\u03bbt = j) = 1{i=j}\u03b2x1 (1\u2212\u03b21)1\u2212x+1{i 6=j}\u03b2x0 (1\u2212\u03b20)1\u2212x\nThe difficulty of the task is determined by the discriminability between target and distractor, or the difference between \u03b21 and \u03b20. For simplicity, we assume that the only stopping action available is to choose the current fixated location: s\u0302(\u03c4 ;\u03bb\u03c4 = j) = j. To reduce the parameter space, we also set \u03b20 = 1 \u2212 \u03b21, which is a reasonable assumption stating that the distractor and target stimuli only differ in one way (e.g. opposing direction of motion when using random dots stimulus with the coherence of dots kept the same). In the following, we first present a brief description of the greedy MAP and the infomax algorithms, before moving on to model comparisons."}, {"heading": "3.2 Greedy MAP Policy", "text": "The greedy MAP algorithm [Najemnik and Geisler, 2005] suggests that agents should try to maximize the expected one-step look-ahead probability of finding the target. Thus, the reward function is:\nRg(pt, j) = Ext+1 [max i P (s = i|xt, xt+1,\u03bbt, \u03bbt+1 = j)]\n= Ext+1 [max i (pit+1)|xt+1, \u03bbt+1 = j]\nTo keep the notations consistent, we define the associated Q-factor, cost and policy as:\nQg(pt, j) = \u2212Rg(pt, j) V g(pt, j) = min\nj Qg(pt, j)\n\u03bbgt+1 = argmin j Qg(pt, j)"}, {"heading": "3.3 Infomax Policy", "text": "The infomax algorithm [Butko and Movellan, 2010] tries to maximize the information gained from each fixation, by minimizing the expected cumulative future entropy. Similar to [Butko and Movellan, 2010], we can define the Q-factors, cost and the policy as:\nQim(pt, j) = T\u2211 t\u2032=t+1 Ext\u2032 [H(pt\u2032)|xt\u2032 , \u03bbt+1 = j] V im(pt, j) = min j Qim(pt, j)\n\u03bbimt+1 = argmin j Qim(pt, j)\nwhere H(p) = \u2212 \u2211 i p\nilogpi is Shannon\u2019s entropy. Note that neither the original greedy MAP nor the infomax algorithm provide a principled answer as to when to stop searching and respond. They need to be augmented to stop once the maximum probability of any location containing the target exceeds a fixed threshold. We come back to the problem of how we set this threshold when we present comparison results."}, {"heading": "3.4 Model Comparison", "text": "Before we discuss the performance of different models in terms of \u201cbehavioral\u201d output, we first visually illustrate the decision policies (Fig. 1). The belief state p is represented by discretizing the two-dimensional belief state space (p1,p2) with m = 201 bins in each dimension (p3 = 1 \u2212 p1 \u2212 p2). Although for C-DAC the policy also depends on the current fixation location, we only show it for fixating the first location; the other representations being rotationally symmetric. In Fig. 1, the parameters used for the C-DAC policy are (c, cs, \u03b2) = (0.1, 0, 0.9), and for the statistical policies, (\u03b2, thresh) = (0.9, 0.8). Note that for this simple scenario with no switch cost, the infomax policy looks almost like the C-DAC policy \u2013 fixate the most likely location unless there is very strong evidence that the fixated location contains the target, in which case the observer should stop. The greedy MAP policy, on the other hand, looks completely different, and is in fact ambiguous in the sense that for a large set of belief states the policy does not give a unique next fixation location. We show one instance of this seemingly random policy, and note that there are regions where the policy suggests to look at either location 1 or 2 or 3 (corner regions speckled with green, orange and brown). Similarly, there are regions where the policy suggests to look at 1 or 2 (green+orange region). In fact, the performance of greedy MAP is so poor that we exclude it from the model comparisons below.\nFig. 2 shows the effects of how the C-DAC policy changes when different parameters of the task are changed. As seen in the figure, the stopping region expands if the cost of time increases (high c), intuitively this makes sense \u2013 if each time step is costlier then the observer should stop at a lower level of confidence, at the expense of higher error rate. Similarly, for the case when \u03b2 is smaller (high noise), stopping with a lower level of confidence makes sense \u2013 the value of each additional observation depends on how noisy the data is, the noisier the less worthwhile to continue observing,\nthus leading to a lower stopping criterion. Lastly, and arguably the most interesting case, is when there is an additional switch cost (added cs); this deters the algorithm from switching even when the belief in a given location has reduced below 1/3. In fact, this is the scenario where optimizing for behavioral objectives turns out to be truly beneficial, and although infomax can approximate the C-DAC policy when the switch cost is 0, it cannot do so when switch cost comes in to play.\nNext, we look at how these intuitions from the policy plots translate to output measures in terms of accuracy, response delay, and number of fixations. In order to set the stopping threshold for the infomax policy in the most generous/optimistic setting, we first run the C-DAC policy, and then set the threshold for infomax so that it matches the accuracy of C-DAC 1, while we compare the other output measures. We choose two scenarios: (1) no switch cost, (2) with switch cost. For all simulations, the algorithm starts with uniform prior (p = (1/3, 1/3, 1/3)) and initial fixation location 1, while the true target location is uniformly distributed. Fig. 3 shows the accuracy, number of time steps and number of switches for both scenarios. Confirming the intuition from the policy plots, the performance of infomax and C-DAC are comparable for cs = 0. However, when a switch cost is added, cs = 0.2, we see that although the accuracy is comparable by design, there is small improvement in search time of C-DAC, and a notable advantage in the number of switches. The behavior of the infomax policy does not adapt to the change in the behavioral cost function, thus incurring an overall higher cost. Algorithms like infomax that maximize abstract statistical objectives lack the inherent flexibility to adapt to changing behavioral goals or environmental constraints. Even for this simple visual search example, Infomax does not have a principled way of setting the stopping threshold, and we gave it the best-scenario outcome by adopting the stopping policy generated by C-DAC in different contexts.\n1Since a binary search is required to set this matching threshold, and the accuracy is sensitive w.r.t. this threshold, we settle on an approximate accuracy match for infomax that is comparable or lower than C-DAC."}, {"heading": "3.5 Approximate Control", "text": "Our model is formally a variant of POMDP (Partially Observable Markov Decision Process), or, more specifically, a Mixed Observability Markov Decision Process (MOMDP) [Ong et al., 2010, Araya-Lo\u0301pez et al., 2010], which differs from ordinary POMDP in that part of the state space is partly hidden (target location in our case) and partly observable (current fixation location in our case). In general, POMDPs are hard to solve since the decision made at each time step depends on all the past actions and observations, thus imposing enormous memory requirements. This is known as the curse of history, and is the first major hurdle towards any practical solution. An elegant way to alleviate this is to use belief states which serve as a sufficient statistic for the process history, thus requiring to maintain just a single distribution instead of the entire history. Converting a POMDP to a belief-state MDP is in fact a prevalent technique and the one we employ. However, this leads to another computational hurdle, known as the curse of dimensionality, since now we have a MDP with a continuous state-space, making tabular representation of value function infeasible. One way to work around the problem is to discretize the belief state space into a grid, where instead of finding the value function at all the points in the belief state simplex, we only do so for a finite number of grid points. The grid approximation, that we also use, has appealing performance guarantees which improve as the density of the grid is increased [Lovejoy, 1991]. To evaluate the value function at the points not in this set, we use some sort of interpolation technique (value at the nearest grid point, weighted average value at k-nearest grid point, etc.). However, although grid approximation may work for small state spaces, it does not scale well to larger, practical problems. For example, when used for the active sensing problem with k sensing locations, a uniform grid of size n has O(knk\u22121) complexity.\nAlthough there is a rich body of literature on approximate solutions of POMDP (e.g. [Powell, 2007, Lagoudakis and Parr, 2003, Kaplow, 2010]) tackling both general as well as application-specific approximations, most are inappropriate for dealing with the MOMDP problem such as the one encountered here. Furthermore, most of the POMDP approximation algorithms focus on discounted rewards and/or finitehorizon problems. Our formulation does not fall into these categories and thus require novel approximation schemes. We note that the Q-factors and the resulting value function are smooth and concave, making them amenable to low dimensional approximations. At each step, we find a low dimensional representation of the value function, and use that for the update step of the\nvalue iteration algorithm. Specifically, instead of recomputing the value function at each grid point, here we generate a large number of samples uniformly on the belief state space, compute a new estimate of the value function at those locations, and then extrapolate the value function to everywhere by improving its parametric fit.\nThe first low-dimensional approximation we consider is the Radial Basis Functions (RBF) representation:\n1. Generate M RBFs, centered at {\u00b5i}Mi=1, with fixed\n\u03c3: \u03c6(p) = 1 \u03c3(2\u03c0)k/2\ne ||p\u2212\u00b5i||\n2\n2\u03c32\n2. Generate m random points from belief space, p. 3. Initialize {V (pi)}mi=1 with the stopping costs. 4. Find minimum-norm w from: V (p) = \u03a6(p)w. 5. Generate new m random belief state points (p\u2032).\n6. Evaluate required V values using current w. 7. Update V (p\u2032) using value iteration. 8. Find a new w from V (p\u2032) = \u03a6(p\u2032)w.\n9. Repeat steps 5 through 8, until w converges.\nWhile we adopt a Gaussian kernel function, other constructs are possible and have been implemented in our problem without significant performance deviation (not shown), e.g. multiquadratic (\u03c6(p) = \u221a 1 + ||p\u2212 \u00b5i||2), inverse-quadratic(\u03c6(p) = (1 + ||p\u2212 \u00b5i||2)\u22121), thin plate spine (\u03c6(p) = ||p \u2212 \u00b5i||2ln||p\u2212 \u00b5i||), etc. [Buhmann, 2003].\nThe RBF approximation requires setting several parameters (number, mean, and variance of bases), which can be impractical for large problems, when there is little or no information available about the properties of the true value function. We thus also implement a nonparametric variation of the algorithm, whereby we use Gaussian Process Regression (GPR) [Williams and Rasmussen, 1996] to estimate the value function (step 4, 6 and 8). In addition, we also implement GPR\nwith hyperparameter learning (Automatic Relevance Determination, ARD), thus obviating the need to preset model parameters.\nThe approximations lead to considerable computational savings. The complexity of the RBF approximation is O(k(mM +M3)), for k sensing locations, m random points chosen at each step, and M bases. For the GPR approximation, the complexity is O(kN3), where N is the number of points used for regression. In practice, all the approximation algorithms we consider converge rapidly (under 10 iterations), though we do not have a proof that this holds for a general case.\nIn the simulations, the RBF approximate policy uses m = 1000 random point for each iteration, and M = 49 bases, uniformly placed in the belief simplex, with a unit variance. The GPR approximate policy uses a unit length scale, unit signal strength and a noisestrength of 0.1, with N = 200 random points used for regression. Fig. 4A shows the exact policy vs. the learned approximate policies for different approximations when the switch cost is 0, (c, cs, \u03b2) = (0.1, 0, 0.9). We notice that with handcrafted bases, RBF is a good approximation of the exact policy, whereas relaxing\nthe parametric form in GPR and subsequently learning the hyperparameters in GPR with ARD, leads to a slightly poorer but more robust non-parametric approximation. Similar observations can be made in Fig. 4B, for the environment with added switch cost, (c, cs, \u03b2) = (0.1, 0.1, 0.9). All the results are shown over a 201x201 grid. These faster yet robust approximations motivated us to apply our model to more complex problems. We investigate one such problem of visual search with peripheral vision next, and show how our model is fundamentally different from existing formulations such as infomax, even when the cost of effort is not considered."}, {"heading": "3.6 Visual Search with Peripheral Vision", "text": "In the very simple three-location visual search problem we considered above, we did not incorporate the possibility of peripheral vision, or the more general possibility that a sensor positioned in a particular location can have distance-dependent, degraded information about nearby locations as well. We therefore consider a simple example with peripheral vision (see Fig. 5B), whereby the observer can saccade to intermediate locations that give reduced information about either two (sensing locations on the edges of the triangle) or three (sensing location in the center) stimuli. This is motivated by experimental observations that humans not only fixate most probable target locations but sometimes also center-of-gravity locations that are intermediate among two or more target locations [Findley, 1982, Zelinsky et al., 1997].\nFormally, we need an acuity map, the notion that it is possible to gain information about stimuli peripheral to the fixation center (fovea), such that the quality of that information decays at greater spatial distance away from the fovea. For example, the task of Fig. 5B would require a continuation action space of 7 elements, L = {l1, l2, l3, l12, l23, l13, l123}, where the first\nthree actions correspond to fixating one of the three target locations, the next three to fixating midway between two target locations, and the last to fixating the center of all three. We parameterize the quality of peripheral vision by augmenting the observations to be three-dimensional, (x1, x2, x3), corresponding to the three simultaneously viewed locations. We assume that each xi is generated by a Bernoulli distribution favoring 1 if it is the target, and 0 if it is not, and its magnitude (absolute difference from 0.5) is greatest when observer directly fixates the stimulus, and smallest when the observer directly fixates one of the other stimuli. We use 4 parameters to characterize the observations (1 > \u03b21 > \u03b22 > \u03b23 > \u03b24 >= 0.5). So, when the agent is fixating one of the potential target locations (l1, l2 or l3), it gets an observation from the fixated location (parameter \u03b21 or 1\u2212\u03b21 depending on whether it is the target or a distractor), and observations from the non-fixated locations (parameter \u03b24 or 1 \u2212 \u03b24 depending on whether they are a target or a distractor). Similarly, for the midway locations (l12, l23 or l13), the observations are received for the closest locations (parameter \u03b22 or 1 \u2212 \u03b22 depending on whether they are a target or a distractor), and from the farther off location (parameter \u03b24 or 1 \u2212 \u03b24 depending on whether it is the target or a distractor). Lastly, for the center location (l123), the observations are made for all three locations (parameter \u03b23 or 1\u2212\u03b23 depending on whether they are a target or a distractor). Furthermore, since the agent can now look at locations that cannot be target, we relax the assumption that the agent must look at a particular location before choosing it, allowing the agent to stop at any location and declare the target."}, {"heading": "3.7 Model Comparison", "text": "We first present the policies, and, similar to our discussion of simple visual search task, we only show the C-DAC policy looking at the first location (l1) (the other fixation-dependent policies are rotationally symmetric). It is evident from Fig. 6 that now the C-DAC policy differs from the infomax policy even when no switch cost is considered, thus pointing to a more fundamental difference between the two. Note that for the parameters used here, C-DAC never chooses to look at the center l123, but it does so for other parameter settings (not shown). Infomax, however, never even looks at the actual potential locations, favoring only midway locations before declaring the target location.\nFor performance comparison in terms of behavioral output, we again investigate two scenarios: (1) no switch cost, (2) with switch cost. The threshold for infomax is set so that the accuracies are matched to facilitate fair comparison. For all simulations, the al-\ngorithm starts with uniform prior (p = (1/3, 1/3, 1/3)) and initial fixation at the center (location l123), while the true target location is uniformly distributed. Fig. 7 shows the accuracy, number of time steps, and number of switches for both scenarios. Now we notice that C-DAC outperforms infomax even when switch cost is not considered, in contrast to the simple task without peripheral vision (Fig. 3). Note however that C-DAC makes more switches for cs = 0, which makes sense since switches have no cost, and search time can potentially be reduced by allowing more switches. However, when we add a switch cost (cs = 0.005), C-DAC significantly reduces number of switches, whereas infomax lacks this adaptability to a changed environment."}, {"heading": "4 Discussion", "text": "In this paper, we proposed a POMDP plus Bayes riskminimization framework for active sensing, which optimizes behaviorally relevant objectives in expectation, such as speed, accuracy, and switching efficiency. We compared this C-DAC policy to the previously proposed infomax and greedy MAP policies. We found that greedy MAP performs very poorly, and although Infomax can approximate the optimal policy for some simple environments, it lacks intrinsic context sensitivity or flexibility. Specifically, for different environments, there is no principled way to set a decision threshold for either greedy MAP or Infomax, leading to higher costs, longer fixation durations, and larger number of switches in problem settings when those costs are significant. This performance difference and the advantage of the added flexibility provided by CDAC becomes even more profound when we consider a more general visual search problem with peripheral vision. The family of approximations that we present opens up the avenue for application of our model to complex, real world problems.\nThere have been several other related active sensing\nalgorithms that differ from C-DAC in their state representation, inference, control and/or approximation scheme. We briefly summarize some of these here. In [Darrell and Pentland, 1996], the problem of active gesture recognition is studied, by using historic state representation and nearest neighbor Q-function approximation. Sensing strategies for robots in RoboCup competition is studied in [Kwok and Fox, 2004], which uses states augmented with associated uncertainty and model-free Least Square Policy Iteration (LSPI) approximation [Lagoudakis and Parr, 2003]. Context dependent goals are considered in [Ji et al., 2007] and [Naghshvar and Javidi, 2010]. The former concentrates on multi-sensor multi-aspect sensing using Point Based Value Iteration (PBVI) approximation [Pineau et al., 2006]. The latter aims to provide conditions for reduction of an active sequential hypothesis testing problem to passive hypothesis testing. A Reinforcement Learning paradigm where reward is not dependent on information gain but on how close a saccade brings the target to the optical axis has also been proposed [Minut and Mahadevan, 2001]. Other control strategies like random search, sequential sweeping search, \u201cDrosophila-inspired\u201d search [Chung and Burdick, April 2007] and hierarchical POMDPs for visual action planning [Sridharan et al., 2010] have also been proposed. We choose infomax to compare our C-DAC policy against because, as a human-vision inspired model, it not only explains human fixation behavior on a variety of tasks, but also has cutting edge computer vision applications (e.g. the digital eye [Butko and Movellan, 2010]).\nA related problem domain, not typically studied as POMDP or MDP, is Multi-Armed Bandits (MAB) [Gittins, 1979]. The classical example of a MAB problem concerns with pulling levers (or playing arms) in a set of slot machines. The person gambling is unaware of the states and reward distribution of the levers, and has to figure out which lever to pull next in order to maximize the cumulative reward. Noting a correspondence between the ideas of pulling arms and fixating location, and between rewards and observations, the MAB framework seems to describe the active sensing problem. Concretely, given the locations fixated (arms played) so far, and the observations (rewards) received, how to choose which location to fixate (which arm to play) next. However, there are certain characteristics of the active sensing problem that make it difficult to study in a MAB framework as yet. Firstly, the problem is an instance of restless bandits [Whittle, 1988], where the state of an arm can change even when it is not played. In active sensing, the belief about a location being the target does change even when it is not fixated. Whittles index is a simple rule that assigns a value to each arm in a restless setting, and the\narm with the highest value is then played. The rule is asymptotically optimal only for a sub-class of problems (e.g. [Washburn and Schneider, 2008] and [Liu and Zhao, 2010]), but not optimal in general. Secondly, the states of the arms in the active sensing task are correlated (the elements of the belief-state have to add up to 1). There is some work on correlated arms for specific structure of correlation, like clustered arms [Pandey et al., 2007] and Gaussian process bandits [Dorard et al., 2009], but so far there is no general strategy for handling this scenario.\nActive learning is another related approach, with hypothesis testing as a sub-problem that is related to the problem of active sensing. The setting involves an unknown true hypothesis, and an agent that can perform queries providing information about the underlying hypothesis. The task is then to determine which query to perform next to optimally reduce the number of plausible hypothesis (version space). In active sensing however, although the belief about a hypothesis (target location) can become arbitrarily low, the number of plausible hypothesis does not reduce. This problem is investigated in [Golovin et al., 2010], and a near-optimal greedy solution is proposed along with performance guarantees. Besides the sub-optimality of the approach, the same test cannot be performed more than once (whereas in active sensing, one location can be fixated more than once). The lack of this provision stems from the fact that the noisy observations considered are actually deterministic with respect to a hidden noise parameter. Thus, as of yet it is hard to cast the active sensing problem in this framework.\nWe thus conclude that although there is a rich body of literature on related problems, as can be seen from the few examples we presented, our formulation is novel (to our best knowledge) in its goals and principled approach to the problem of active sensing. In general, the framework proposed here has the potential for not\nonly applications in visual search, but a host of other problems, ranging from active scene categorization to active foraging. The decision policies it generates are adaptive to the environment and sensitive to contextual factors. This flexibility and robustness to different environments makes the framework an appealing choice for a variety of active sensing applications."}], "references": [{"title": "A closer look at momdps", "author": ["Mauricio Araya-L\u00f3pez", "Vincent Thomas", "Olivier Buffet", "Fran\u00e7ois Charpillet"], "venue": "In Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "Araya.L\u00f3pez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Araya.L\u00f3pez et al\\.", "year": 2010}, {"title": "On the theory of dynamic programming", "author": ["R Bellman"], "venue": "PNAS, 38(8):716\u2013719,", "citeRegEx": "Bellman.,? \\Q1952\\E", "shortCiteRegEx": "Bellman.", "year": 1952}, {"title": "Strong supervision from weak annotation: Interactive training of deformable part models", "author": ["S Branson", "P Perona", "S Belongie"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "Branson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branson et al\\.", "year": 2011}, {"title": "Radial basis functions: theory and implementations, volume 12", "author": ["M.D. Buhmann"], "venue": "Cambridge university press,", "citeRegEx": "Buhmann.,? \\Q2003\\E", "shortCiteRegEx": "Buhmann.", "year": 2003}, {"title": "Infomax control of eyemovements", "author": ["N J Butko", "J R Movellan"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Butko and Movellan.,? \\Q2010\\E", "shortCiteRegEx": "Butko and Movellan.", "year": 2010}, {"title": "A decisionmaking framework for control strategies in probabilistic search", "author": ["Timothy H. Chung", "Joel W. Burdick"], "venue": "Intl. Conference on Robotics and Automation", "citeRegEx": "Chung and Burdick.,? \\Q2007\\E", "shortCiteRegEx": "Chung and Burdick.", "year": 2007}, {"title": "Active gesture recognition using partially observable markov decision processes", "author": ["T. Darrell", "A. Pentland"], "venue": "In Pattern Recognition,", "citeRegEx": "Darrell and Pentland.,? \\Q1996\\E", "shortCiteRegEx": "Darrell and Pentland.", "year": 1996}, {"title": "Gaussian process modelling of dependencies in multiarmed bandit problems", "author": ["L. Dorard", "D. Glowacka", "J. Shawe-Taylor"], "venue": "In Int. Symp. Op. Res,", "citeRegEx": "Dorard et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dorard et al\\.", "year": 2009}, {"title": "Global processing for saccadic eye movements", "author": ["J M Findley"], "venue": "Vision Research,", "citeRegEx": "Findley.,? \\Q1982\\E", "shortCiteRegEx": "Findley.", "year": 1982}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J C Gittins"], "venue": "J. Royal Stat. Soc.,", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "arXiv preprint arXiv:1010.3091,", "citeRegEx": "Golovin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2010}, {"title": "Bayesian surprise attracts human attention", "author": ["L Itti", "P Baldi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Itti and Baldi.,? \\Q2006\\E", "shortCiteRegEx": "Itti and Baldi.", "year": 2006}, {"title": "A saliency-based search mechanism for overt and covert shifts of visual attention", "author": ["L Itti", "C Koch"], "venue": "Vision Research,", "citeRegEx": "Itti and Koch.,? \\Q2000\\E", "shortCiteRegEx": "Itti and Koch.", "year": 2000}, {"title": "Nonmyopic multiaspect sensing with partially observable markov decision processes", "author": ["S. Ji", "R. Parr", "L. Carin"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Ji et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2007}, {"title": "Point-based POMDP solvers: Survey and comparative analysis", "author": ["R. Kaplow"], "venue": "PhD thesis, McGill University,", "citeRegEx": "Kaplow.,? \\Q2010\\E", "shortCiteRegEx": "Kaplow.", "year": 2010}, {"title": "Shifts in selective visual attention: towards the underlying neural circuitry", "author": ["C Koch", "S Ullman"], "venue": "Hum. Neurobiol.,", "citeRegEx": "Koch and Ullman.,? \\Q1985\\E", "shortCiteRegEx": "Koch and Ullman.", "year": 1985}, {"title": "Reinforcement learning for sensing strategies", "author": ["C. Kwok", "D. Fox"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Kwok and Fox.,? \\Q2004\\E", "shortCiteRegEx": "Kwok and Fox.", "year": 2004}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr.", "year": 2003}, {"title": "An information-theoretic framework for understanding saccadic behaviors", "author": ["Tai Sing Lee", "Stella Yu"], "venue": "In Advance in Neural Information Processing Systems,", "citeRegEx": "Lee and Yu.,? \\Q2000\\E", "shortCiteRegEx": "Lee and Yu.", "year": 2000}, {"title": "Indexability of restless bandit problems and optimality of whittle index for dynamic multichannel access", "author": ["K. Liu", "Q. Zhao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Liu and Zhao.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Zhao.", "year": 2010}, {"title": "Computationally feasible bounds for partially observed markov decision processes", "author": ["W.S. Lovejoy"], "venue": "Operations research,", "citeRegEx": "Lovejoy.,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy.", "year": 1991}, {"title": "A reinforcement learning model of selective visual attention", "author": ["S Minut", "S Mahadevan"], "venue": "In Proceedings of the Fifth International Conference on Autonomous Agents, Montreal,", "citeRegEx": "Minut and Mahadevan.,? \\Q2001\\E", "shortCiteRegEx": "Minut and Mahadevan.", "year": 2001}, {"title": "Active m-ary sequential hypothesis testing", "author": ["M. Naghshvar", "T. Javidi"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Naghshvar and Javidi.,? \\Q2010\\E", "shortCiteRegEx": "Naghshvar and Javidi.", "year": 2010}, {"title": "Optimal eye movement strategies in visual search", "author": ["J Najemnik", "W S Geisler"], "venue": "Nature, 434(7031):387\u2013", "citeRegEx": "Najemnik and Geisler.,? \\Q2005\\E", "shortCiteRegEx": "Najemnik and Geisler.", "year": 2005}, {"title": "Planning under uncertainty for robotic tasks with mixed observability", "author": ["Sylvie CW Ong", "Shao Wei Png", "David Hsu", "Wee Sun Lee"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Ong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2010}, {"title": "Multiarmed bandit problems with dependent arms", "author": ["S. Pandey", "D. Chakrabarti", "D. Agarwal"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Pandey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pandey et al\\.", "year": 2007}, {"title": "Anytime pointbased approximations for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "Powell.,? \\Q2007\\E", "shortCiteRegEx": "Powell.", "year": 2007}, {"title": "Planning to see: A hierarchical approach to planning visual actions on a robot using pomdps", "author": ["M Sridharan", "J Wyatt", "R Dearden"], "venue": "Artificial Intelligence,", "citeRegEx": "Sridharan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2010}, {"title": "Optimal policies for a class of restless multiarmed bandit scheduling problems with applications to sensor management", "author": ["R. Washburn", "M. Schneider"], "venue": "Journal of Advances in Information Fusion", "citeRegEx": "Washburn and Schneider.,? \\Q2008\\E", "shortCiteRegEx": "Washburn and Schneider.", "year": 2008}, {"title": "Restless bandits: activity allocation in a changing world", "author": ["P Whittle"], "venue": "J. App. Probability,", "citeRegEx": "Whittle.,? \\Q1988\\E", "shortCiteRegEx": "Whittle.", "year": 1988}, {"title": "Gaussian processes for regression", "author": ["C K I Williams", "C E Rasmussen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Rasmussen.,? \\Q1996\\E", "shortCiteRegEx": "Williams and Rasmussen.", "year": 1996}, {"title": "Yarbus. Eye Movements and Vision", "author": ["F A"], "venue": null, "citeRegEx": "A,? \\Q1967\\E", "shortCiteRegEx": "A", "year": 1967}, {"title": "Eye movements reveal the spatio-temporal dynamics of visual search", "author": ["G J Zelinsky", "R P Rao", "M M Hayhoe", "D H Ballard"], "venue": null, "citeRegEx": "Zelinsky et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zelinsky et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 4, "context": "Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko and Movellan, 2010] or", "startOffset": 127, "endOffset": 153}, {"referenceID": 23, "context": "one-step look-ahead accuracy [Najemnik and Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and sensor repositioning cost.", "startOffset": 29, "endOffset": 57}, {"referenceID": 23, "context": "Separately, in another active formulation, it has been proposed that saccades are chosen to maximize the greedy, one-step look-ahead probability of finding the target (greedy MAP), conditioned on self knowledge about visual acuity map [Najemnik and Geisler, 2005].", "startOffset": 235, "endOffset": 263}, {"referenceID": 1, "context": "Bellman\u2019s dynamic programming equation [Bellman, 1952] tells us that the problem is optimized if at each time point, the agent chooses the action associated with the lowest expected cost (the Q-factor for that action), given his current knowledge or belief state, pt.", "startOffset": 39, "endOffset": 54}, {"referenceID": 23, "context": "The greedy MAP algorithm [Najemnik and Geisler, 2005] suggests that agents should try to maximize the expected one-step look-ahead probability of finding the", "startOffset": 25, "endOffset": 53}, {"referenceID": 4, "context": "The infomax algorithm [Butko and Movellan, 2010] tries to maximize the information gained from each fixation, by minimizing the expected cumulative future entropy.", "startOffset": 22, "endOffset": 48}, {"referenceID": 4, "context": "Similar to [Butko and Movellan, 2010], we can define the Q-factors, cost and the policy as:", "startOffset": 11, "endOffset": 37}, {"referenceID": 20, "context": "of the grid is increased [Lovejoy, 1991].", "startOffset": 25, "endOffset": 40}, {"referenceID": 3, "context": "[Buhmann, 2003].", "startOffset": 0, "endOffset": 15}, {"referenceID": 31, "context": "We thus also implement a nonparametric variation of the algorithm, whereby we use Gaussian Process Regression (GPR) [Williams and Rasmussen, 1996] to estimate the value function (step 4, 6 and 8).", "startOffset": 116, "endOffset": 146}, {"referenceID": 6, "context": "In [Darrell and Pentland, 1996], the problem of active gesture recognition is studied, by using historic state representation and nearest neighbor Q-function approximation.", "startOffset": 3, "endOffset": 31}, {"referenceID": 16, "context": "Sensing strategies for robots in RoboCup competition is studied in [Kwok and Fox, 2004], which uses states augmented with associated uncertainty and model-free Least Square Policy Iteration (LSPI) ap-", "startOffset": 67, "endOffset": 87}, {"referenceID": 17, "context": "proximation [Lagoudakis and Parr, 2003].", "startOffset": 12, "endOffset": 39}, {"referenceID": 13, "context": "Context dependent goals are considered in [Ji et al., 2007] and [Naghshvar and Javidi, 2010].", "startOffset": 42, "endOffset": 59}, {"referenceID": 22, "context": ", 2007] and [Naghshvar and Javidi, 2010].", "startOffset": 12, "endOffset": 40}, {"referenceID": 26, "context": "The former concentrates on multi-sensor multi-aspect sensing using Point Based Value Iteration (PBVI) approximation [Pineau et al., 2006].", "startOffset": 116, "endOffset": 137}, {"referenceID": 21, "context": "A Reinforcement Learning paradigm where reward is not dependent on information gain but on how close a saccade brings the target to the optical axis has also been proposed [Minut and Mahadevan, 2001].", "startOffset": 172, "endOffset": 199}, {"referenceID": 28, "context": "trol strategies like random search, sequential sweeping search, \u201cDrosophila-inspired\u201d search [Chung and Burdick, April 2007] and hierarchical POMDPs for visual action planning [Sridharan et al., 2010] have also been proposed.", "startOffset": 176, "endOffset": 200}, {"referenceID": 4, "context": "the digital eye [Butko and Movellan, 2010]).", "startOffset": 16, "endOffset": 42}, {"referenceID": 9, "context": "A related problem domain, not typically studied as POMDP or MDP, is Multi-Armed Bandits (MAB) [Gittins, 1979].", "startOffset": 94, "endOffset": 109}, {"referenceID": 30, "context": "Firstly, the problem is an instance of restless bandits [Whittle, 1988], where the state of an arm can change even when it is not played.", "startOffset": 56, "endOffset": 71}, {"referenceID": 29, "context": "[Washburn and Schneider, 2008] and [Liu and Zhao, 2010]), but not optimal in general.", "startOffset": 0, "endOffset": 30}, {"referenceID": 19, "context": "[Washburn and Schneider, 2008] and [Liu and Zhao, 2010]), but not optimal in general.", "startOffset": 35, "endOffset": 55}, {"referenceID": 25, "context": "arms for specific structure of correlation, like clustered arms [Pandey et al., 2007] and Gaussian process bandits [Dorard et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 7, "context": ", 2007] and Gaussian process bandits [Dorard et al., 2009], but so far there is no general strategy for handling this scenario.", "startOffset": 37, "endOffset": 58}, {"referenceID": 10, "context": "This problem is investigated in [Golovin et al., 2010], and a near-optimal greedy solution is proposed along with performance guarantees.", "startOffset": 32, "endOffset": 54}], "year": 2013, "abstractText": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko and Movellan, 2010] or one-step look-ahead accuracy [Najemnik and Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and sensor repositioning cost. We simulate these algorithms on a simple visual search task to illustrate scenarios in which contextsensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the CDAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate a more complex search problem involving peripheral vision, and we notice that the performance advantage of C-DAC over generic statistical policies is even more evident in this scenario.", "creator": "LaTeX with hyperref package"}}}