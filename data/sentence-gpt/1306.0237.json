{"id": "1306.0237", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2013", "title": "Guided Random Forest in the RRF Package", "abstract": "Random Forest (RF) is a powerful supervised learner and has been popularly used in many applications such as bioinformatics. In this work I propose a new and enhanced RF called the guided random forest (GRF) for high-dimensional classification and feature selection. Similar to a feature selection method called guided regularized random forest (GRRF), GRF is built using the importance scores from an ordinary RF. The trees in GRRF are built sequentially, are highly correlated (which deteriorates accuracy performance as a classifier) and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in a distributed computing framework. Experiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), GRF outperforms RF on 8 data sets and 7 of them have significant differences at the 0.05 level. GRF uses, on average, only tens of features in the model, while RF uses thousands of features. Therefore, both accuracy and interpretability are significantly improved. I also found that, as a classifier, GRF outperforms GRRF on all the data sets and 7 of them have significant differences at the 0.05 level. The computational complexity of GRF is only about twice as much as an ordinary RF. GRF can be used in \"RRF\" v1.4, a package that also includes the regularized random forest methods. In this work I propose a new and enhanced RF called the guided random forest (GRF) for high-dimensional classification and feature selection. Similar to a feature selection method called guided regularized random forest (GRF), GRF is built using the importance scores from an ordinary RF. The trees in GRF are built sequentially, are highly correlated (which deteriorates accuracy performance as a classifier) and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in a distributed computing framework. Experiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), GRF outperforms RF on 8 data sets and 7 of them have significant differences at the 0.05 level. GRF uses, on average, only tens of features in the model, while RF uses thousands of features. Therefore, both accuracy and interpretability are significantly improved. I also found that, as a classifier, GRF outperforms GRRF on 8 data sets and 7 of them have significant differences at the 0", "histories": [["v1", "Sun, 2 Jun 2013 18:30:45 GMT  (14kb)", "https://arxiv.org/abs/1306.0237v1", null], ["v2", "Sat, 12 Oct 2013 03:56:07 GMT  (14kb)", "http://arxiv.org/abs/1306.0237v2", null], ["v3", "Mon, 18 Nov 2013 08:52:49 GMT  (13kb)", "http://arxiv.org/abs/1306.0237v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["houtao deng"], "accepted": false, "id": "1306.0237"}, "pdf": {"name": "1306.0237.pdf", "metadata": {"source": "CRF", "title": "Guided Random Forest in the RRF Package", "authors": ["Houtao Deng"], "emails": ["hdeng3@asu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n02 37\nv3 [\ncs .L\nG ]\n1 8\nN ov\n2 01\n3\nSimilar to a feature selection method called guided regularized random forest (GRRF), GRF is built using the importance scores from an ordinary RF. However, the trees in GRRF are built sequentially, are highly correlated and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in parallel.\nExperiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), RF applied to features selected by GRF outperforms RF applied to all features on 9 data sets and 7 of them have significant differences at the 0.05 level. Therefore, both accuracy and interpretability are significantly improved. GRF selects more features than GRRF, however, leads to better classification accuracy.\nNote in this work the guided random forest is guided by the importance scores from an ordinary random forest, however, it can also be guided by other methods such as human insights (by specifying \u03bbi).\nGRF can be used in \u201cRRF\u201d v1.4 (and later versions), a package that also includes the regularized random forest methods. Availability: The RRF package is freely distributed under GNU General Public License (GPL) and is available from CRAN (http://cran.r-project.org/), the official R package archive. Contact: hdeng3@asu.edu"}, {"heading": "1 INTRODUCTION", "text": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012). RF is able to handle mixed categorical and numerical features, multiple classes, are insensitive to the scale of features, and have been considered as a powerful supervised learner.\nRF can provide importance scores of features to understand the contribution of each feature. However, there can be a huge number of features for high-dimensional problems (all the gene data sets considered in our experiments have more than 1000 features), and it is challenging to investigate the importance scores from thousands of features. Therefore, it is desirable to develop a feature selection algorithm for RF. .\nThe guided regularized random forest (GRRF) proposed by Deng and Runger (2013) uses the importance scores from an RF built on the complete training data to complement the information gain in a local node. However, the trees in GRRF can be highly\ncorrelated and GRRF can not be built in parallel (Deng and Runger, 2013).\nThe guided random forest (GRF), proposed in this work, is a solution of the issues mentioned above. GRF is guided by the importance scores from an RF, and each tree in GRF is built independently from another tree. Experiments on 10 gene data sets show conclusive results that GRF uses many fewer features than RF, and RF applied to features selected by GRF is more accurate than RF."}, {"heading": "2 METHODS", "text": "Let gain(Xi) denote the Gini information gain of using a feature Xi to split a tree node. The key idea of GRF is weighting gain(Xi) using the importance scores from an RF.\ngainG(Xi) = \u03bbi gain(Xi) (1)\nwhere \u03bbi is calculated as\n\u03bbi = 1\u2212 \u03b3 + \u03b3 Impi\nImp\u2217 (2)\nwhere Impi is the importance score of Xi from an RF, Imp\u2217 is the maximum importance score, Impi\nImp\u2217 \u2208 [0, 1] is the normalized importance\nscore, and \u03b3 \u2208 [0, 1] controls the weight of the importance scores from RF. It can be seen that, features with smaller importance scores are penalized more in GRF, and the penalty increases as \u03b3 increases (GRF becomes RF when \u03b3 = 0). In this work I use the maximum penalty (i.e., \u03b3 = 1), in order to use a small number of features in GRF. So gainG(Xi) becomes\nImpi Imp\u2217 gain(Xi) (3)\nNote the key difference between GRF and GRRF is that the features used in previous trees have an impact on the current tree for GRRF, but does not have any impact for GRF. The features used in a GRRF model are expected to be relevant and non-redundant, while the features used in a GRF model are expected to be relevant, but not necessarily non-redundant."}, {"heading": "3 EXAMPLES AND RESULTS", "text": "Code 1 shows an example of using GRF (\u03b3 = 1) for feature selection. In the code, a classification data set with 500 features is simulated, and only 2 features are relevant to the class. While RF uses all the features and misclassifies 54 out of 250 instances, RF uses 196 features selected by GRF and misclassifies 34.\nc\u00a9 1\nCode 1. Feature Selection and Classification with GRF\nlibrary(RRF) # load the RRF package set.seed(1) # fix the random seed. # simulate classification data set X <- matrix(runif(500*500, min=-1, max=1), ncol=500) # class is only relevant to feature 1 and 21 Y <- (X[,1]) + (X[,21]) ix <- which(Y>quantile(Y, 1/2)); Y <- Y*0-1; Y[ix] <- 1 #assign class -1 and 1 #split data into training and testing sets trainIx <- 1:250 trainX <- X[trainIx,]; trainY <- Y[trainIx] testX <- X[-trainIx,]; testY <- Y[-trainIx] # build an ordinary RF on the training instances # note \u2019flagReg=1\u2019 results in a regularized RF RF <- RRF(trainX,flagReg=0, as.factor(trainY)) print(length(RF$feaSet)) #500 features used imp <- RF$importance[,\"MeanDecreaseGini\"] impRF <- imp/max(imp) # normalization # build a GRF with gamma = 1 # note the difference between GRF and RF is that # \u2019coefReg\u2019 is related to impRF in GRF, while # it is constant for all variables in RF. gamma <- 1 coefReg <- (1-gamma) + gamma*impRF GRF <- RRF(trainX,as.factor(trainY), flagReg=0,coefReg=coefReg) print(length(GRF$feaSet)) #196 features used GRF_RF <- RRF(trainX[,GRF$feaSet], flagReg=0, as.factor(trainY)) # test RF and GRF on the testing instances pred <- predict(RF,testX)#predict using RF # 54 instances misclassified print(length(which(pred != testY))) # predict using GRF\u2019s features pred <- predict(GRF_RF,testX[,GRF$feaSet]) # 34 instances misclassified print(length(which(pred != testY)))\nIn addition, I applied GRF-RF (RF applied to the feature subset selected by GRF), GRF, GRRF (\u03b3 = 0.1) and GRRF-RF (RF applied to the feature subset selected by GRRF), all with 1000 trees, to 10 gene data sets used in D\u0131\u0301az-Uriarte and De Andres (2006); Deng and Runger (2013). The references of the data sets are provided in a supplementary file to save space. I obtained the average error rates and average number of features for each method using the same procedure as Deng and Runger (2013), i.e., calculated from 100 replicates of training/testing splits with a ratio of 2:1. The results of RF and GRRF-RF are slightly different\nfrom the results of Deng and Runger (2013) due to randomness. Table 1 shows the average error rates of different methods. GRFRF outperforms RF on 9 data sets, 7 of them have significant differences at the 0.05 level. The advantage of GRF-RF over GRRF and GRRF-RF is also clear. GRF-RF also outperform GRF, and therefore applying RF to features selected by GRF is better than GRF as a classifier.\nTable 2 summarizes the data sets and shows the number of features used in different models. RF uses a subset of features in the model, and GRF uses a even smaller number of features in the model."}, {"heading": "4 CONCLUSIONS", "text": "The guided random forest (GRF) is proposed here for feature selection, particularly, for gene classification in this work. Experiments show that GRF-RF not only significantly outperforms RF in accuracy performance, but also uses many fewer features in the model. In this work I discuss the advantages of GRF for high-dimensional gene data sets. It may also be valuable to find other cases where GRF has advantages over other methods, with the option of tuning the parameter \u03b3 in Equation (2) (fixed as 1 here). Furthermore, in this work, \u03bbi is determined by the importance score of feature Xi from an ordinary random forest. However, \u03bbi can be specified by other ways too, e.g., F-score or human knowledge."}], "references": [{"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1), 5\u201332.", "citeRegEx": "Breiman,? 2001", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "Gene selection with guided regularized random forest", "author": ["H. Deng", "G. Runger"], "venue": "Pattern Recognition. to appear.", "citeRegEx": "Deng and Runger,? 2013", "shortCiteRegEx": "Deng and Runger", "year": 2013}, {"title": "Gene selection and classification of microarray data using random forest", "author": ["R. D\u0131\u0301az-Uriarte", "S. De Andres"], "venue": "BMC bioinformatics,", "citeRegEx": "D\u0131\u0301az.Uriarte and Andres,? \\Q2006\\E", "shortCiteRegEx": "D\u0131\u0301az.Uriarte and Andres", "year": 2006}, {"title": "Classification and regression by randomforest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News, 2(3), 18\u201322.", "citeRegEx": "Liaw and Wiener,? 2002", "shortCiteRegEx": "Liaw and Wiener", "year": 2002}, {"title": "Predicting in vitro drug sensitivity using random forests", "author": ["G. Riddick", "H. Song", "S. Ahn", "J. Walling", "D. Borges-Rivera", "W. Zhang", "H.A. Fine"], "venue": "Bioinformatics, 27(2), 220\u2013224.", "citeRegEx": "Riddick et al\\.,? 2011", "shortCiteRegEx": "Riddick et al\\.", "year": 2011}, {"title": "Predicting the lethal phenotype of the knockout mouse by integrating comprehensive genomic data", "author": ["Y. Yuan", "Y. Xu", "J. Xu", "R.L. Ball", "H. Liang"], "venue": "Bioinformatics, 28(9), 1246\u20131252. 2", "citeRegEx": "Yuan et al\\.,? 2012", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al.", "startOffset": 19, "endOffset": 57}, {"referenceID": 3, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al.", "startOffset": 19, "endOffset": 57}, {"referenceID": 4, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012).", "startOffset": 132, "endOffset": 173}, {"referenceID": 5, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012).", "startOffset": 132, "endOffset": 173}, {"referenceID": 1, "context": "However, the trees in GRRF can be highly correlated and GRRF can not be built in parallel (Deng and Runger, 2013).", "startOffset": 90, "endOffset": 113}, {"referenceID": 0, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012). RF is able to handle mixed categorical and numerical features, multiple classes, are insensitive to the scale of features, and have been considered as a powerful supervised learner. RF can provide importance scores of features to understand the contribution of each feature. However, there can be a huge number of features for high-dimensional problems (all the gene data sets considered in our experiments have more than 1000 features), and it is challenging to investigate the importance scores from thousands of features. Therefore, it is desirable to develop a feature selection algorithm for RF. . The guided regularized random forest (GRRF) proposed by Deng and Runger (2013) uses the importance scores from an RF built on the complete training data to complement the information gain in a local node.", "startOffset": 20, "endOffset": 857}, {"referenceID": 1, "context": "1) and GRRF-RF (RF applied to the feature subset selected by GRRF), all with 1000 trees, to 10 gene data sets used in D\u0131\u0301az-Uriarte and De Andres (2006); Deng and Runger (2013). The references of the data sets are provided in a supplementary file to save space.", "startOffset": 154, "endOffset": 177}, {"referenceID": 1, "context": "1) and GRRF-RF (RF applied to the feature subset selected by GRRF), all with 1000 trees, to 10 gene data sets used in D\u0131\u0301az-Uriarte and De Andres (2006); Deng and Runger (2013). The references of the data sets are provided in a supplementary file to save space. I obtained the average error rates and average number of features for each method using the same procedure as Deng and Runger (2013), i.", "startOffset": 154, "endOffset": 395}, {"referenceID": 1, "context": "1) and GRRF-RF (RF applied to the feature subset selected by GRRF), all with 1000 trees, to 10 gene data sets used in D\u0131\u0301az-Uriarte and De Andres (2006); Deng and Runger (2013). The references of the data sets are provided in a supplementary file to save space. I obtained the average error rates and average number of features for each method using the same procedure as Deng and Runger (2013), i.e., calculated from 100 replicates of training/testing splits with a ratio of 2:1. The results of RF and GRRF-RF are slightly different from the results of Deng and Runger (2013) due to randomness.", "startOffset": 154, "endOffset": 577}], "year": 2013, "abstractText": "Summary: Random Forest (RF) is a powerful supervised learner and has been popularly used in many applications such as bioinformatics. In this work we propose the guided random forest (GRF) for feature selection. Similar to a feature selection method called guided regularized random forest (GRRF), GRF is built using the importance scores from an ordinary RF. However, the trees in GRRF are built sequentially, are highly correlated and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in parallel. Experiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), RF applied to features selected by GRF outperforms RF applied to all features on 9 data sets and 7 of them have significant differences at the 0.05 level. Therefore, both accuracy and interpretability are significantly improved. GRF selects more features than GRRF, however, leads to better classification accuracy. Note in this work the guided random forest is guided by the importance scores from an ordinary random forest, however, it can also be guided by other methods such as human insights (by specifying \u03bbi). GRF can be used in \u201cRRF\u201d v1.4 (and later versions), a package that also includes the regularized random forest methods. Availability: The RRF package is freely distributed under GNU General Public License (GPL) and is available from CRAN (http://cran.r-project.org/), the official R package archive. Contact: hdeng3@asu.edu", "creator": "LaTeX with hyperref package"}}}