{"id": "1306.0271", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "KERT: Automatic Extraction and Ranking of Topical Keyphrases from Content-Representative Document Titles", "abstract": "We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for topical keyphrase generation and ranking. By shifting from the unigram-centric traditional methods of unsupervised keyphrase extraction to a phrase-centric approach, we are able to directly compare and rank phrases of different lengths. We construct a topical keyphrase ranking function which implements the four criteria that represent high quality topical keyphrases (coverage, purity, phraseness, and completeness) in terms of ranking. We have recently expanded our ranking methodology with the use of a new language, KERT (Keyphrase Extraction and Ranking by Topic), which includes multiple ways to obtain more detailed results in the analysis of topical keyphrases, and we have been able to implement a new language.\n\n\n\n\n\nThe keyphrase extraction functions consist of a single process. The process is based on a number of different procedures in which it involves performing keyword extraction. We also identify, in this case, that, in order to detect specific types of keyphrases, KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Extraction and Ranking by Topic), KERT (Keyphrase Ext", "histories": [["v1", "Mon, 3 Jun 2013 01:44:28 GMT  (50kb,D)", "http://arxiv.org/abs/1306.0271v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["marina danilevsky", "chi wang", "nihit desai", "jingyi guo", "jiawei han"], "accepted": false, "id": "1306.0271"}, "pdf": {"name": "1306.0271.pdf", "metadata": {"source": "CRF", "title": "KERT: Automatic Extraction and Ranking of Topical Keyphrases from Content-Representative Document Titles", "authors": ["Marina Danilevsky", "Chi Wang", "Nihit Desai", "Jingyi Guo", "Jiawei Han"], "emails": ["danilev1@illinois.edu,", "chiwang1@illinois.edu,", "nhdesai2@illinois.edu,", "jingyi@cs.umass.edu,", "hanj@cs.uiuc.edu"], "sections": [{"heading": "1 Introduction", "text": "Keyphrases have traditionally been defined as terms or phrases which summarize the topics in a document (Turney, 2000). Keyphrase extraction is an important step in many tasks, such as document summarization, clustering, and categorization (Manning and Schu\u0308tze, 1999). More recently, the definition has been expanded to include the notion of topical keyphrases - groups of keyphrases which summarize the topics in a given document, or document collection (Liu et al., 2010). Most existing work on keyphrase extraction identifies keyphrases from either individual documents or an entire text collection (Tomokiyo and Hurst, 2003; Liu et al., 2010). However, recently there has been some interest in working with documents consisting of very short text, such as a collection of tweets (Zhao et al., 2011), in order to summarize the document collection.\nOur framework also targets short texts - in particular, collections of content-representative titles. Document titles are content-representative if they may serve as concise descriptions of the content of the document. The words in a content-representative title can therefore be thought of as probabilistic priors for which words are the most likely to generate keyphrases describing the document. Scientific publications and newspaper articles generally have content-representative titles, whereas fiction books generally do not. As we address the task of representing the topics present in a document collection, content-representative titles are cleaner and more efficient to deal with than entire documents.\nMost current approaches to topic construction yield lists of unigrams to represent topics. However, it has long been known that unigrams account for only a small fraction of human-assigned index terms (Turney, 2000). Therefore, in order to construct high quality keyphrases for a given topic, it is important to provide n-gram keyphrases rather than unigram keywords. However, it is inappropriate to discard all unigrams when approaching this task. For instance, consider that the unigram \u2018classification\u2019 and the trigram \u2018support vector machines\u2019 are both high quality topical keyphrases for the Machine Learning topic in the domain of Computer Science. We should therefore be able to perform integrated ranking of mixed-length phrases in a natural way.\nSuch a ranking function must successfully represent human intuition for judging what constitutes a high quality topical keyphrase. We propose that this is best represented by four criteria: coverage, purity, phraseness, and completeness. As an example, consider the task of constructing and ranking\nar X\niv :1\n30 6.\n02 71\nv1 [\ncs .L\nG ]\n3 J\nun 2\nkeyphrases for various topics in Computer Science: \u2022 Coverage: A representative keyphrase for a topic should cover many documents within that topic. Example: \u2018information retrieval\u2019 has better coverage than \u2018cross-language information retrieval\u2019 in the Information Retrieval topic. \u2022 Purity: A phrase is pure in a topic if it is only frequent in documents belonging to that topic and not frequent in documents within other topics. Example: \u2018query processing\u2019 is more pure than \u2018query\u2019 in the Database topic. \u2022 Phraseness: A group of words should be combined together as a phrase if they co-occur significantly more often than the expected chance cooccurrence frequency, given that each term in the phrase occurs independently. Example: \u2018active learning\u2019 is a better phrase than \u2018learning classification\u2019 in the Machine Learning topic. \u2022 Completeness: A phrase is not complete if it is a subset of a longer phrase, in the sense that it rarely occurs in a title without the presence of the longer phrase. Example: \u2018support vector machines\u2019 is a complete phrase, whereas \u2018vector machines\u2019 is not because \u2018vector machines\u2019 is almost always accompanied by \u2018support\u2019.\nOur aim is to construct topics represented by a ranked list of keyphrases of various lengths where more highly ranked keyphrases are considered to be better index phrases for that topic. The main contributions of this work are: \u2022 We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for topical keyphrase generation and ranking. By altering the steps in the traditional methods of unsupervised keyphrase extraction, we can directly compare of phrases of different lengths, resulting in a natural integrated ranking of mixed-length keyphrases. \u2022We construct a topical keyphrase ranking function which implements the four criteria that intuitively represent high quality topical keyphrases (coverage, purity, phraseness, and completeness). \u2022We demonstrate the effectiveness of our approach on two collections of content-representative titles in the domains of Computer Science and Physics."}, {"heading": "2 Related Work", "text": "The state-of-the-art approaches to unsupervised keyphrase extraction have generally been graphbased, unigram-centric ranking methods, which first\nextract unigrams from text, rank them, and finally combine them into keyphrases. TextRank (Mihalcea and Tarau, 2004) constructs keyphrases from the top ranked unigrams in a document collection. Topical PageRank (Liu et al., 2010) splits the documents into topics and creates keyphrases from top ranked topical unigrams. Some previous methods have used clustering techniques on word graphs for keyphrase extraction (Liu et al., 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness. Barker and Cornacchia (2000) use natural language processing techniques to select noun phrases as keyphrases. Tomokiyo and Hurst (2003) take a language modeling approach, requiring a document collection with known topics as input and training a language model to define their ranking criteria.\nUnlike most of these methods which extract keyphrases from documents, we aim to extract keyphrases from a corpus of short texts. Zhao et al (2011) also uses short text - microblogging data from Twitter - but we work with contentrepresentative document titles.\nTopic modeling techniques such as PLSA (probabilistic latent semantic analysis) (Hofmann, 2001) and LDA (latent Dirichlet allocation) (Blei et al., 2003) take documents as input, model them as mixtures of different topics, and discover word distributions for each topic. Some previous work has developed topic modeling to discover topical phrases comprised of consecutive words (Wang et al., 2007). Our framework also uses topic modeling - an extension of LDA - to perform the initial word clustering into topics, but does not restrict phrases to only those word sequences explicitly found in the text.\nSince we aim to transform a collection of short texts into sets of ranked keyphrases, the top-K keyphrases for each topic may also serve as that topic\u2019s labels. Therefore our work is tangentially related to automatic topic labeling (Mei et al., 2007)."}, {"heading": "3 KERT Framework", "text": "A key aspect of our framework is that we do not follow the traditional unigram-centric approach of keyphrase extraction, where words are first extracted and ranked independently, and then combined to create phrases. Instead, we construct topical phrases immediately after clustering the unigrams. By shifting from a unigram-centric to a phrase-centric ap-\nproach, we are able to extract topical keyphrases and implement a ranking function that can directly compare keyphrases of different lengths, as we explain in Section 3.3.\nOur steps for topical keyphrase generation and ranking are as follows:\nStep 1. Cluster words in the document dataset into T foreground topics and one background topic, using background LDA.\nStep 2. Extract candidate keyphrases for each topic according to the word topic assignment.\nStep 3. Rank the keyphrases in each foreground topic z \u2208 1, . . . , k by integrating the criteria of Coverage, Purity, Phraseness, and Completeness.\nIn the subsequent sections we give detailed explanations of each step."}, {"heading": "3.1 Clustering Words using Topic Modeling", "text": "Content-representative titles exhibit several characteristics that guide our clustering approach: \u2022 Short and well-formed. Most titles are composed of a few key technical words and background words. Background words are found across titles belonging to different topics. \u2022 Ambiguous Unigrams. A single word (e.g. \u2018vector\u2019) may appear in different topics, while phrases (e.g. \u2018support vector machines\u2019) are overall less topic-ambiguous. \u2022Mix of Topics. A title may comprise a sparse mixture of different topics, in spite of its short length.\nLDA (Blei et al., 2003) and its extensions have been shown to be effective for modeling textual documents. We therefore use a modified LDA model which includes an additional background topic z = 0. Each title is modeled by a distribution over the foreground topics z = 1, . . . , k and the background topic. For each word in a title, we decide if it belongs to the background topic or one of the foreground topics, and then choose the word from the appropriate distribution.\nFormally, let \u03c6t denote the word distribution for topic t = 0, . . . , k. Let \u03b8d be the topic distribution of document d. Let \u03bb denote a Bernoulli distribution that chooses between the background topic and foreground topics. The generative process is as follows:\n1. Draw \u03c6t \u223c Dir(\u03b2), for t = 0, . . . , k.\n2. For each title d \u2208 D,\n(a) draw \u03b8d \u223c Dir(\u03b1).\n(b) for each word position i in d i. draw yd,i \u223c Bernoulli(\u03bb)\nii. if yd,i = 0, draw wd,i \u223c Multi(\u03c60), otherwise\nA. draw topic z \u223c \u03b8d B. draw wd,i \u223cMulti(\u03c6z)\nwhere \u03b1 and \u03b2 are Dirichlet prior parameters for \u03b8 and \u03c6 respectively.\nWe use a collapsed Gibbs sampling method for model inference. We iteratively sample the topic assignment zd,i for every word wd,i in each document until convergence. In traditional topic modeling tasks, the sampled topic assignments are mainly used to estimate the topic distribution \u03c6z . In our case, we are more interested in the topic assignments for the words in each title, because these values are the foundation of our topical keyphrase generation step, as described in the next section. We use the maximum a posteriori (MAP) principle to label each word: zd,i = argmaxzd,i=0,...,k P (zd,i|W )."}, {"heading": "3.2 Candidate Keyphrase Generation", "text": "There are two ways to discover keyphrases: either extract them from the text (sequences of words) which actually occur in the text, or to automatically construct them (Frank et al., 1999), an approach which is regarded as both more intelligent and more difficult (Hammouda et al., 2005; Manning and Schu\u0308tze, 1999).\nIn a dataset of content-representative titles, extracting phrases directly from the text is quite limiting as this approach is too sensitive to the caprices of various writing styles. For instance, consider that two computer science papers titles, one containing \u2018mining frequent patterns\u2019 and the other containing \u2018frequent pattern mining,\u2019 are clearly discussing the same topic, and should be treated as such. A keyphrase may also be separated by other words: \u2018mining top-k frequent closed patterns\u2019 also belongs to the topic of frequent pattern mining, in addition to incorporating secondary topics of top-k frequent patterns, and closed patterns. Therefore, we define a phrase to be an order-free set of words appearing in the same title, and must therefore construct our phrases.\nAfter the clustering step described in Section 3.1 is completed, each word wd,i in each title d has a topic label zd,i \u2208 {0, . . . , k}. If a set of words in a title d are assigned a common foreground topic t > 0,\nthese words may comprise a topical phrase in t (e.g. {\u2018frequent\u2019,\u2018pattern\u2019,\u2018mining\u2019} in the topic of \u2018Data Mining\u2019). If this set occurs in many titles with the topic label t, it is likely a good candidate keyphrase for that topic.\nWe use frequent pattern mining approaches to mine the candidate topical phrases. For each topic t, we construct a topic-t word set ptd = {wd,i|zd,i = t} consisting of the words with the topic label t for each document d. We unite all the topic-t word sets into the topic-t set Dt = {d|ptd 6= \u2205}. We may then mine frequent word sets from ptDt = {p t d|d \u2208 Dt} using any efficient pattern mining algorithm, such as FP-growth (Han et al., 2004). We require a good topical keyphrase to have enough topical support ft(p) > \u00b5 in order to filter out some coincidental co-occurrences (where ft(p) denotes the frequency of the word set p in topic t).\nWe thus define a candidate topical keyphrase for topic t to be a set of words p = {w1 . . . wn} which are simultaneously labeled with topic t in at least \u00b5 titles, as discovered by the frequent pattern mining step. We then move on to evaluating the quality of the candidate topical keyphrases in order to rank them within each topic."}, {"heading": "3.3 Ranking Measures and Function", "text": "As discussed in Section 1, a ranking function must compare topical keyphrases with respect to the four criteria of Coverage, Purity, Phraseness, and Completeness. This implies that the function should be able to directly compare keyphrases of mixed lengths, which we refer to as having the comparability property. For example, the keyphrases \u2018classification,\u2019 \u2018decision trees,\u2019 and \u2018support vector machines\u2019 should all be ranked highly in the integrated list of keyphrases for the Machine Learning topic, in spite of having different lengths.\nTraditional probabilistic modeling approaches such as language models or topic models do not have the comparability property. They can model the probability of seeing an n-gram given a topic, but the probabilities of n-grams with different lengths (unigrams, bigrams, etc) are not well comparable. These approaches simply find longer n-grams to have much smaller probability than shorter ones, because the probabilities of seeing every possible unigram sum up to 1, and so do the probabilities of seeing every possible bigram, trigram, etc. But the\ntotal number of possible n-grams grows following a power law (O(vn) where v is the vocabulary size), and therefore ranking functions based on these traditional approaches invariably favor short n-grams. While previous work has used various heuristics to correct this bias during post-processing steps, (Tomokiyo and Hurst, 2003; Zhao et al., 2011), our approach is cleaner and more principled.\nWe propose a different ranking model which exhibits the comparability property. The key idea is to represent the random event et(p) =\u2018seeing a phrase p in a random title with topic t\u2019. With this definition, the events of seeing n-grams of various lengths in different titles are no longer mutually exclusive, and therefore the probabilities no longer need to sum up to 1. Formally, the probability of et(p) is defined to be P (et(p)) =\nft(p) |Dt| . In the subsequent sections\nwe define our measures representing the 4 criteria of coverage, purity, phraseness, and completeness using quantities related to this probability."}, {"heading": "3.3.1 Coverage", "text": "A representative keyphrase for a topic should cover many documents within that topic. For example, \u2018information retrieval\u2019 has better coverage than \u2018cross-language information retrieval\u2019 in the topic of Information Retrieval. We directly quantify the coverage measure of a phrase as the probability of seeing a phrase in a random topic-t word set ptd \u2208 Dt:\n\u03c0covt (p) = P (et(p)) = ft(p)\n|Dt| (1)"}, {"heading": "3.3.2 Purity", "text": "A phrase is pure in topic t if it is only frequent in documents about topic t and not frequent in documents about other topics. For example, \u2018query processing\u2019 is a more pure keyphrase than \u2018query\u2019 in the Databases topic.\nWe measure the purity of a keyphrase by comparing the probability of seeing a phrase in the topic-t collection of word sets and the probability of seeing it in any other topic-t\u2032 collection (t\u2032 = 0, 1, . . . , k, t\u2032 6= t). A reference collection Dt,t\u2032 = Dt \u222aDt\u2032 is a mix of of topic-t and topic-t\u2032 titles. If there exists a topic t\u2032 such that the probability of et,t\u2032(p) =\u2018seeing a phrase p in a reference collection Dt,t\u2032\u2019 is similar or even larger than the probability of seeing p in Dt, the phrase p indicates confusion about topic t and t\u2032. The purity of a keyphrase compares the probability of seeing it in the topic-t collection and the maximal\nprobability of seeing it in any reference collection:\n\u03c0purt (p) = log P (et(p))\nmaxt\u2032 P (et,t\u2032(p)) (2)\n= log ft(p)\n|Dt| \u2212 logmax t\u2032\nft(p) + ft\u2032(p)\n|Dt,t\u2032 |"}, {"heading": "3.3.3 Phraseness", "text": "A group of words should be grouped into a phrase if they co-occur significantly more frequent than the expected co-occurrence frequency given that each word in the phrase occurs independently. For example, while \u2018active learning\u2019 is a good keyphrase as in the Machine Learning topic \u2018learning classification\u2019 is not, since the latter two words co-occur only because both of them are popular in the topic.\nWe therefore compare the probability of seeing a phrase p = {w1 . . . wn} and seeing the n words w1 . . . wn independently in topic-t documents:\n\u03c0phrt (p) = log P (et(p))\u220f\nw\u2208p P (et(w)) (3)\n= log ft(p)\n|Dt| \u2212 \u2211 w\u2208p log ft(w) |Dt|"}, {"heading": "3.3.4 Completeness", "text": "A phrase p is not complete if a longer phrase p\u2032 which contains p usually co-occurs with p. For example, \u2018vector machines\u2019 is not a complete phrase but \u2018support vector machines\u2019 is, because \u2019support\u2019 almost always accompanies \u2018vector machines\u2019.\nWe thus measure the completeness of a phrase p by examining the conditional probability of observing p\u2032 given p in a topic-t title:\n\u03c0comt (p) = 1\u2212max p\u2032%p P (et(p \u2032)|et(p)) (4)\n= 1\u2212max w P (et(p \u222a {w})|et(p)) = 1\u2212 maxw ft(p \u222a {w}) ft(p)"}, {"heading": "3.3.5 Combined Ranking Function", "text": "We combine these 4 measures into a comprehensive function for ranking a topical keyphrase:\nrt(p) = { 0 \u03c0comt \u2264 \u03b3 \u03c0covt [(1\u2212 \u03c9)\u03c0 pur t + \u03c9\u03c0 phr t ](p) o.w.\n(5)\nwhere \u03b3, \u03c9 \u2208 [0, 1] are two parameters. The completeness criterion is used as a filtering condition to remove incomplete phrases, where \u03b3 controls how aggressively we prune. \u03b3 = 0 corresponds to ignoring the criteria and retaining all maximal phrases, where no supersets have the same topical support. As \u03b3 approaches 1, more phrases will be filtered and only closed phrases (where no supersets are sufficiently frequent) will remain. The other three criteria then calculate the ranking score for the keyphrases which pass the completeness filter.\nThe coverage criterion is in some sense the most important, since a keyphrase with low coverage will be obviously of low quality. In rt(p), \u03c0covt (p) is a probability P (et(p)) and multiplies both \u03c0 pur t and \u03c0phrt . This is desirable because when P (et(p)) is small, phrase p will have low support, and thus the estimates of purity and phraseness will be unreliable and play a minor role.\nThe tradeoff between purity and phraseness is controlled by \u03c9. Both measures are log ratios on comparable scales, and can thus be balanced by a weighted summation. As \u03c9 increases we expect more topic-independent but common phrases to be ranked higher.\nThe ranking function can also be nicely explained in an information theoretic framework. In fact, the product of coverage and purity, \u03c0covt (p)\u03c0 pur t (p) = P (et(p)) log P (et(p))\nP (et,t\u2217 (p)) is equal to the pointwise KL-\ndivergence between the probability of et(p) and et,t\u2217(p). Pointwise KL-divergence is a distance measure between two probabilities that takes the absolute probability into consideration, and is more robust than pointwise mutual information when the relative difference between probabilities need to be supported by sufficiently high absolute probability. Likewise, the product of coverage and phraseness, \u03c0covt (p)\u03c0 phr t (p) is equivalent to pointwise KLdivergence between the probability of et(p) under different independence assumptions. Therefore, Eq. (5) can also be interpreted as a weighted summation of two pointwise KL-divergence metrics."}, {"heading": "4 Experiments", "text": "We use two collections of content-representative titles in our evaluation. The first - the DBLP dataset - is a set of titles of recently published computer science papers in the areas related to Databases, Data Mining, Information Retrieval, Machine Learning,\nand Natural Language Processing. These titles come from DBLP1, a bibliography website for computer science publications. The second collection - the arXiv dataset - is a sample of titles of physics papers published within the last decade, and labeled by their authors as belonging to the subfields of Optics, Fluid Dynamics, Atomic Physics, Instrumentation and Detectors, or Plasma Physics. This collection of titles comes from arXiv2, an online archive for electronic preprints of scientific papers.3\nBoth datasets were minimally pre-processed by removing all stopwords from the titles. After preprocessing, the DBLP dataset contained 33,313 titles consisting of 18,598 unique words, and the arXiv dataset contained 9,722 titles evenly sampled from the specified 5 physics subfields, and consisting of 9,648 unique words."}, {"heading": "4.1 DBLP Dataset Experiments", "text": "We use the DBLP dataset to evaluate the ability of our method to construct topical keyphrases that appear to be high quality to human judges, via a user study. We will first describe the methods we used for comparison, and then present a sample of the keyphrases actually generated by these methods and encountered by participants in the user study. We then explain the details of our user study, and present\n1http://www.dblp.org/ 2http://arxiv.org 3Both datasets will be online available\nquantitative results."}, {"heading": "4.1.1 Methods for Comparison", "text": "We use background LDA introduced in Section 3.1 for the word clustering step in order to create input for all the methods that we compare. We resort to a Newton-Raphson iteration method (Minka, 2000) to learn the hyperparameters, and empirically set \u03bb = 0.1, which leads to generally coherent results for our dataset.4\nTo evaluate the performance of KERT, we implemented several variations of the function, as well as two baseline functions. The baselines come from Zhao et al (2011), who focus on topical keyphrase extraction in microblogs, but claim that their method can be used for other text collections. We implement their two best performing methods: kpRelInt* and kpRel.5 We also construct variations of KERT where the keyphrase extraction steps are the same, but each of the four ranking criteria is ignored in turn. We refer to these versions as KERT\u2013cov, KERT\u2013pur, KERT\u2013phr, and KERT\u2013com, respectively.\n4The learned \u03b1 = 1.0 is smaller than the typical setting due to the nature of our short text, and \u03b2 = 0.07 is larger because in our dataset, different topics often share the same words.\n5Their main ranking function kpRelInt considers the heuristics of phrase interestingness and relevance. As their interestingness measure is represented by re-Tweets, a concept that is not appropriate to our dataset, we reimplement the interestingness measure to be the relative frequency of the phrase in the dataset instead, and we therefore refer to our reimplementation as kpRelInt*. kpRel considers only the relevance heuristic.\nThese variations nicely represent the possible settings for the parameters \u03b3 \u2208 [0, 1] and \u03c9 \u2208 [0, 1], which are described in Section 3.3.5. In KERT we set \u03b3 = \u03c9 = 0.5. KERT\u2013com sets \u03b3 = 0 to demonstrate what happens when we retain all maximal phrases. As \u03b3 approaches 1, more phrases will be filtered but a very small number of closed phrases (no supersets are frequent) will not be. KERT\u2013phr sets \u03c9 = 0 and KERT\u2013pur sets \u03c9 = 1, which demonstrates the tradeoff between ignoring phraseness for the sake of maximizing purity, and ignoring purity to optimize for phraseness, respectively."}, {"heading": "4.1.2 Qualitative Results", "text": "Table 1 shows the top 10 ranked topical keyphrases generated by each method for the topic of Machine Learning. kpRel and kpRelInt* yield very similar results, both clearly favoring unigrams. However, kpRel also ranks several keyphrases highly which are not very meaningful, such as \u2018learning classification\u2019 and \u2018selection learning.\u2019 Removing coverage from our ranking function yields the worst results, confirming the intuition that a high quality keyphrase must at minimum have good coverage. Without purity, the function favors bigrams and trigrams that all appear to be very meaningful, although several high quality unigrams such as \u2018learning\u2019 and \u2018classification\u2019 no longer appear. Removing phraseness, in contrast, yields meaningful unigrams but very few bigrams, and looks quite similar to the kpRelInt* baseline. Finally, without completeness, phrases such as \u2018support vector\u2019 and \u2018vector machines\u2019 are ranked highly, although they should not be, as both are sub-phrases of the high quality trigram \u2018support vector machines.\u2019"}, {"heading": "4.1.3 User Study and Quantitative Results", "text": "To quantitatively measure keyphrase quality, we invited people to judge the generated topical keyphrases generated by the different methods. Since the DBLP dataset generates topics in computer science, we recruited 10 computer science graduate students - who could thus be considered to be very knowledgeable judges - for a user study. We generated 5 topics from the DBLP dataset and found 4 of them were clearly interpretable as Machine Learning, Databases, Data Mining, and Information Retrieval.For each of the four topics, we retrieved the top 20 ranked keyphrases by each method. These keyphrases were gathered together per topic and pre-\nsented in random order, and users were asked to evaluate the quality of each keyphrase on a 5 point Likert scale.\nTo measure the performance of each method given the user study results, we adapt the nKQM@K measure (normalized keyphrase quality measure for top-K phrases) from (Zhao et al., 2011), which is itself a version of the nDCG metric from information retrieval (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002). We define nKQM@K for a method M using the topK generated keyphrases as:\nnKQM@K = 1\nT T\u2211 t=1\n\u2211K j=1 scoreaw(Mt,j) log2(j+1)\nIdealScoreK\nHere T is the number of topics, and Mt,j refers to the jth keyphrase generated by method M for topic t. Unlike in (Zhao et al., 2011), we have more than 2 judges, so we define scoreaw as the agreementweighted average score for the Mt,j keyphrase, which is a weighted mean of the judges\u2019 score by the weighted Cohen\u2019s \u03ba. This gives a higher value to a keyphrase with scores of (3,3,3) than to one with scores of (1,3,5), though the average score is identical. Finally, IdealScoreK is calculated using the scores of the top K keyphrases of all judged ones.\nTable 2 compares the performance across different methods. The top performances are clearly variations of KERT with different parameter settings. As expected KERT\u2013cov exhibits the worst performance. The baselines perform slightly better, and it is interesting to note that kpRel, which is smoothed purity, performs better than kpRelInt*, and even slightly better than KERT\u2013phr. This is because kpRelInt* adds in a measure of overall keyphrase coverage in the entire collection, which hurts rather than helps for this task. Removing completeness\nappears to have a very small negative effect, and we hypothesize this is because high-ranked incomplete keyphrases are relatively rare, though very obvious when they do occur (e.g. \u2018vector machines\u2019). KERT\u2013pur performs the best - which may reflect human bias towards longer phrases - with an improvement of at least 50% over the kpRelInt* baseline for all reported values of K."}, {"heading": "4.2 arXiv Dataset Experiments", "text": "We use the arXiv dataset, which contains labeled titles, to explore which method maximizes the mutual information between phrase-represented topics and titles. As the collection has 5 categories, we set T=5.\nFor each method, we do multiple runs for various values of K (the number of top-ranked phrases per topic considered), and calculate the mutual information MIK for that method as a function of K. To calculate MIK , we label each of the top K phrases in every topic with the topic in which it is ranked highest. We then check each paper title to see if it contains any of these top phrases. If so, we update the number of events \u201cseeing a topic t and category c\u201d for t = 1 . . . T , with the averaged count for all those labeled phrases contained in the title; otherwise we update the number of events \u201cseeing a topic t and category c\u201d for t = 1 . . . T uniformly, where c is the Primary Category label for the paper title in consideration. Finally, we compute the mutual information at K:\nMIK = \u2211 t,c p(t, c) log2 p(t, c) p(t)p(c)\nWe compare the baselines (kpRelInt* and kpRel), KERT , and variations of KERT where only coverage (KERTcov), only purity (KERTpur), and only coverage and purity (KERTcov+pur) are used in the ranking function. We feed them the same input by background LDA with the same parameter settings as discussed above. Figure 1 shows MIK for each method for a range of K.\nIt is clear that for MIK, coverage is more important than purity, since KERTpur is by far the worst performer. Both baselines perform nearly as well as KERTcov, and all are comfortably beaten by KERTcov+pur (> 20% improvement for K between 100 and 600), which uses our coverage and purity measure. It is interesting to note that adding in the phraseness and completeness measures yields\nno improvement in MIK. However, the experiments with the DBLP dataset demonstrate that these measures are very helpful in the eyes of expert judges. In contrast, while MIK is definitely improved with the addition of the purity measure, people prefer for it to be removed. Although we outperform other approaches in both evaluations, these observations show interesting differences between theory-based and human-based evaluation metrics."}, {"heading": "5 Conclusion", "text": "In this work we introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for the automatic extraction and ranking of topical keyphrases using collections of content-representative document titles. Unlike existing techniques, our phrasecentric approach is able to construct candidate topical keyphrases in such a way as to allow our ranking function to directly compare the quality of keyphrases of different lengths. Our method yields high quality topical keyphrases, with over 50% improvement over a baseline method according to human judgement and over 20% improvement according to mutual information. In the future we would like to further explore why human judgement appears to be consistently biased against the purity criteria, in contrast to quantitative metrics such as mutual information. We are also interested in extending our approach to working with longer texts."}], "references": [{"title": "Using noun phrase heads to extract document keyphrases", "author": ["Barker", "Cornacchia2000] Ken Barker", "Nadia Cornacchia"], "venue": "In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence: Advances in Artificial", "citeRegEx": "Barker et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Barker et al\\.", "year": 2000}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Domain-specific keyphrase extraction", "author": ["Frank et al.1999] Eibe Frank", "Gordon W. Paynter", "Ian H. Witten", "Carl Gutwin", "Craig G. Nevill-Manning"], "venue": "In Proceedings of the 16th international joint conference on Artificial intelligence - Volume", "citeRegEx": "Frank et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1999}, {"title": "Extracting key terms from noisy and multitheme documents", "author": ["Maxim Grinev", "Dmitry Lizorkin"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Grineva et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Grineva et al\\.", "year": 2009}, {"title": "Corephrase: keyphrase extraction for document clustering", "author": ["Diego N. Matute", "Mohamed S. Kamel"], "venue": "In Proceedings of the 4th international conference on Machine Learning and Data Mining in Pattern Recogni-", "citeRegEx": "Hammouda et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hammouda et al\\.", "year": 2005}, {"title": "Mining frequent patterns without candidate generation: A frequent-pattern tree approach", "author": ["Han et al.2004] Jiawei Han", "Jian Pei", "Yiwen Yin", "Runying Mao"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "Han et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Han et al\\.", "year": 2004}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["Thomas Hofmann"], "venue": "Mach. Learn.,", "citeRegEx": "Hofmann.,? \\Q2001\\E", "shortCiteRegEx": "Hofmann.", "year": 2001}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Clustering to find exemplar terms for keyphrase extraction", "author": ["Liu et al.2009] Zhiyuan Liu", "Peng Li", "Yabin Zheng", "Maosong Sun"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Foundations of statistical natural language processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Automatic labeling of multinomial topic models", "author": ["Mei et al.2007] Qiaozhu Mei", "Xuehua Shen", "ChengXiang Zhai"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Mei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2007}, {"title": "TextRank: Bringing Order into Texts", "author": ["Mihalcea", "Tarau2004] Rada Mihalcea", "Paul Tarau"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Estimating a dirichlet distribution", "author": ["Thomas P. Minka"], "venue": null, "citeRegEx": "Minka.,? \\Q2000\\E", "shortCiteRegEx": "Minka.", "year": 2000}, {"title": "A language model approach to keyphrase extraction", "author": ["Tomokiyo", "Hurst2003] Takashi Tomokiyo", "Matthew Hurst"], "venue": "In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment - Volume", "citeRegEx": "Tomokiyo et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tomokiyo et al\\.", "year": 2003}, {"title": "Learning algorithms for keyphrase extraction", "author": ["Peter D. Turney"], "venue": "Inf. Retr.,", "citeRegEx": "Turney.,? \\Q2000\\E", "shortCiteRegEx": "Turney.", "year": 2000}, {"title": "Topical n-grams: Phrase and topic discovery, with an application to information retrieval", "author": ["Wang et al.2007] Xuerui Wang", "Andrew McCallum", "Xing Wei"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Topical keyphrase extraction from twitter", "author": ["Zhao et al.2011] Wayne Xin Zhao", "Jing Jiang", "Jing He", "Yang Song", "Palakorn Achananuparp", "Ee-Peng Lim", "Xiaoming Li"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "Keyphrases have traditionally been defined as terms or phrases which summarize the topics in a document (Turney, 2000).", "startOffset": 104, "endOffset": 118}, {"referenceID": 16, "context": "However, recently there has been some interest in working with documents consisting of very short text, such as a collection of tweets (Zhao et al., 2011), in order to summarize the document collection.", "startOffset": 135, "endOffset": 154}, {"referenceID": 14, "context": "However, it has long been known that unigrams account for only a small fraction of human-assigned index terms (Turney, 2000).", "startOffset": 110, "endOffset": 124}, {"referenceID": 8, "context": "Some previous methods have used clustering techniques on word graphs for keyphrase extraction (Liu et al., 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness.", "startOffset": 94, "endOffset": 134}, {"referenceID": 3, "context": "Some previous methods have used clustering techniques on word graphs for keyphrase extraction (Liu et al., 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness.", "startOffset": 94, "endOffset": 134}, {"referenceID": 3, "context": ", 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness. Barker and Cornacchia (2000) use natural language processing techniques to select noun phrases as keyphrases.", "startOffset": 8, "endOffset": 160}, {"referenceID": 3, "context": ", 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness. Barker and Cornacchia (2000) use natural language processing techniques to select noun phrases as keyphrases. Tomokiyo and Hurst (2003) take a language modeling approach, requiring a document collection with known topics as input and training a language model to define their ranking criteria.", "startOffset": 8, "endOffset": 267}, {"referenceID": 6, "context": "Topic modeling techniques such as PLSA (probabilistic latent semantic analysis) (Hofmann, 2001) and LDA (latent Dirichlet allocation) (Blei et al.", "startOffset": 80, "endOffset": 95}, {"referenceID": 1, "context": "Topic modeling techniques such as PLSA (probabilistic latent semantic analysis) (Hofmann, 2001) and LDA (latent Dirichlet allocation) (Blei et al., 2003) take documents as input, model them as mixtures of different topics, and discover word distributions for each topic.", "startOffset": 134, "endOffset": 153}, {"referenceID": 15, "context": "Some previous work has developed topic modeling to discover topical phrases comprised of consecutive words (Wang et al., 2007).", "startOffset": 107, "endOffset": 126}, {"referenceID": 10, "context": "Therefore our work is tangentially related to automatic topic labeling (Mei et al., 2007).", "startOffset": 71, "endOffset": 89}, {"referenceID": 1, "context": "LDA (Blei et al., 2003) and its extensions have been shown to be effective for modeling textual documents.", "startOffset": 4, "endOffset": 23}, {"referenceID": 2, "context": "There are two ways to discover keyphrases: either extract them from the text (sequences of words) which actually occur in the text, or to automatically construct them (Frank et al., 1999), an approach which is regarded as both more intelligent and more difficult (Hammouda et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 4, "context": ", 1999), an approach which is regarded as both more intelligent and more difficult (Hammouda et al., 2005; Manning and Sch\u00fctze, 1999).", "startOffset": 83, "endOffset": 133}, {"referenceID": 5, "context": "We may then mine frequent word sets from ptDt = {p t d|d \u2208 Dt} using any efficient pattern mining algorithm, such as FP-growth (Han et al., 2004).", "startOffset": 127, "endOffset": 145}, {"referenceID": 16, "context": "While previous work has used various heuristics to correct this bias during post-processing steps, (Tomokiyo and Hurst, 2003; Zhao et al., 2011), our approach is cleaner and more principled.", "startOffset": 99, "endOffset": 144}, {"referenceID": 12, "context": "We resort to a Newton-Raphson iteration method (Minka, 2000) to learn the hyperparameters, and empirically set \u03bb = 0.", "startOffset": 47, "endOffset": 60}, {"referenceID": 12, "context": "We resort to a Newton-Raphson iteration method (Minka, 2000) to learn the hyperparameters, and empirically set \u03bb = 0.1, which leads to generally coherent results for our dataset.4 To evaluate the performance of KERT, we implemented several variations of the function, as well as two baseline functions. The baselines come from Zhao et al (2011), who focus on topical keyphrase extraction in microblogs, but claim that their method can be used for other text collections.", "startOffset": 48, "endOffset": 345}, {"referenceID": 16, "context": "To measure the performance of each method given the user study results, we adapt the nKQM@K measure (normalized keyphrase quality measure for top-K phrases) from (Zhao et al., 2011), which is itself a version of the nDCG metric from information retrieval (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002).", "startOffset": 162, "endOffset": 181}, {"referenceID": 16, "context": "Unlike in (Zhao et al., 2011), we have more than 2 judges, so we define scoreaw as the agreementweighted average score for the Mt,j keyphrase, which is a weighted mean of the judges\u2019 score by the weighted Cohen\u2019s \u03ba.", "startOffset": 10, "endOffset": 29}], "year": 2013, "abstractText": "We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for topical keyphrase generation and ranking. By shifting from the unigram-centric traditional methods of unsupervised keyphrase extraction to a phrase-centric approach, we are able to directly compare and rank phrases of different lengths. We construct a topical keyphrase ranking function which implements the four criteria that represent high quality topical keyphrases (coverage, purity, phraseness, and completeness). The effectiveness of our approach is demonstrated on two collections of contentrepresentative titles in the domains of Computer Science and Physics.", "creator": "LaTeX with hyperref package"}}}