{"id": "1306.0386", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration", "abstract": "Given a Markov Decision Process (MDP) with $n$ states and $m$ actions per state, we study the number of iterations needed by Policy Iteratio n (PI) algorithms to converge. We consider two variations of PI: Howard's PI that changes all the actions with a positive advantage, and Sim plex-PI that only changes one action with maximal advantage. We show that Howard's PI terminates after at most $ n(m-1) \\lceil \\frac{1}{1-\\gamma}\\log (\\frac{1}{1-\\gamma}) \\rceil $ iterations, improving by a factor $O(\\log n)$ a result by Hansen et al. (2013), while Simplex-PI terminates after at most $ n(m-1) \\lceil \\frac{n}{1-\\gamma} \\log (\\frac{n}{1-\\gamma})\\rceil $ iterations, improving by a factor 2 a result by Ye (2011). We then consider bounds that are independent of the discount factor $\\gamma$. When the MDP is deterministic, we show that Simplex-PI terminates after at most $ 2 n^2 m (m-1) \\lceil 2 (n-1) \\log n \\rceil \\lceil 2 n \\log n \\rceil = O(n^4 m^2 \\log^2 n) $ iterations, improving by a factor $O(n)$ a bound obtained by Post and Ye (2012). We generalize this result to general MDPs under some structural assumptions: given a measure of the maximal transient time $\\tau_t$ and the maximal time $\\tau_r$ to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most $ n^2 m (m-1) (\\lceil \\tau_r \\log (n \\tau_r) \\rceil +\\lceil \\tau_r \\log (n \\tau_t) \\rceil) \\lceil {\\tau_t} \\log (n (\\tau_t+1)) \\rceil = \\tilde O (n^2 \\tau_t \\tau_r m^2) $ iterations. We explain why similar results seem hard to derive for Howard's PI. Finally, under the additional (restrictive) assumption that the MDP is weakly-communicating, we show that Simplex-PI and Howard's PI terminate after at most $n(m-1) (\\lceil \\tau_t \\log n \\tau_t \\rceil + \\lceil \\tau_r \\log n \\tau_r \\rceil) =\\tilde O(nm (\\tau_t+\\tau_r))$ iterations. We", "histories": [["v1", "Mon, 3 Jun 2013 12:48:27 GMT  (29kb)", "https://arxiv.org/abs/1306.0386v1", null], ["v2", "Thu, 6 Jun 2013 14:14:54 GMT  (29kb)", "http://arxiv.org/abs/1306.0386v2", "Markov decision processes ; Dynamic Programming ; Analysis of Algorithms"], ["v3", "Mon, 24 Jun 2013 14:09:56 GMT  (30kb)", "http://arxiv.org/abs/1306.0386v3", "Markov decision processes ; Dynamic Programming ; Analysis of Algorithms"], ["v4", "Wed, 10 Feb 2016 09:09:49 GMT  (33kb)", "http://arxiv.org/abs/1306.0386v4", "Markov decision processes, Dynamic Programming, Analysis of Algorithms, Mathematics of Operations Research, INFORMS, 2016"]], "reviews": [], "SUBJECTS": "math.OC cs.AI cs.DM cs.RO", "authors": ["bruno scherrer"], "accepted": true, "id": "1306.0386"}, "pdf": {"name": "1306.0386.pdf", "metadata": {"source": "CRF", "title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration", "authors": ["Bruno Scherrer"], "emails": ["bruno.scherrer@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n03 86\nv4 [\nm at\nh. O\nC ]\n1 0\nFe b\n20 16\n(\nm 1\u2212\u03b3\nlog ( 1 1\u2212\u03b3 ))\niterations, improving by a factor O(log n) a result by Hansen et al (2013), while Simplex-PI terminates after at most O (\nnm 1\u2212\u03b3\nlog ( 1 1\u2212\u03b3 ))\niterations, improving by a factor O(log n) a result by Ye (2011). Under some structural properties of the MDP, we then consider bounds that are independent of the discount factor \u03b3: quantities of interest are bounds \u03c4t and \u03c4r\u2014uniform on all states and policies\u2014respectively on the expected time spent in transient states and the inverse of the frequency of visits in recurrent states given that the process starts from the uniform distribution. Indeed, we show that Simplex-PI terminates after at most O\u0303 ( n3m2\u03c4t\u03c4r )\niterations. This extends a recent result for deterministic MDPs by Post & Ye (2013), in which \u03c4t \u2264 1 and \u03c4r \u2264 n; in particular it shows that Simplex-PI is strongly polynomial for a much larger class of MDPs. We explain why similar results seem hard to derive for Howard\u2019s PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that both Howard\u2019s PI and Simplex-PI terminate after at most O\u0303(m(n2\u03c4t + n\u03c4r)) iterations."}, {"heading": "1 Introduction", "text": "We consider a discrete-time dynamic system whose state transition depends on a control, where the state space X is of finite size n. When at state i \u2208 {1, .., n}, the action is chosen from a set of admissible actions Ai \u2282 A, where the action space A is of finite size m, such that (Ai)1\u2264i\u2264n form a partition of A. The action a \u2208 Ai specifies the transition probability pij(a) = P(it+1 = j|it = i, at = a) to the next state j. At each transition, the system is given a reward r(i, a, j) \u2208 R where r is the instantaneous reward function. In this context, we look for a stationary deterministic policy1, that is a function \u03c0 : X \u2192 A that maps states into admissible actions (for all i, \u03c0(i) \u2208 Ai) that maximizes the expected discounted sum of rewards from any state i, called the value of policy \u03c0 at state i:\nv\u03c0(i) := E\n[\n\u221e \u2211\nk=0\n\u03b3kr(ik, ak, ik+1)\n\u2223 \u2223 \u2223 \u2223 \u2223 i0 = i, \u2200k \u2265 0, ak = \u03c0(ik), ik+1 \u223c P(\u00b7|ik, ak) ] ,\nwhere \u03b3 \u2208 (0, 1) is a discount factor. The tuple \u3008X, (Ai)i\u2208X , p, r, \u03b3\u3009 is called a Markov Decision Process (MDP) (Puterman, 1994; Bertsekas and Tsitsiklis, 1996), and the associated problem is known as stochastic optimal control.\nThe optimal value starting from state i is defined as\nv\u2217(i) := max \u03c0 v\u03c0(i).\n1Restricting our attention to stationary deterministic policies is not a limitation. Indeed, for the optimality criterion to be defined soon, it can be shown that there exists at least one stationary deterministic policy that is optimal (Puterman, 1994).\nFor any policy \u03c0, we write P\u03c0 for the n \u00d7 n stochastic matrix whose elements are pij(\u03c0(i)), and r\u03c0 for the vector whose components are \u2211\nj pij(\u03c0(i))r(i, \u03c0(i), j). The value functions v\u03c0 and v\u2217 can be seen as vectors on X . It is well known that v\u03c0 is the solution of the following Bellman equation:\nv\u03c0 = r\u03c0 + \u03b3P\u03c0v\u03c0,\nthat is v\u03c0 is a fixed point of the affine operator T\u03c0 : v 7\u2192 r\u03c0 +\u03b3P\u03c0v. It is also well known that v\u2217 satisfies the following Bellman equation:\nv\u2217 = max \u03c0 (r\u03c0 + \u03b3P\u03c0v\u2217) = max \u03c0 T\u03c0v\u2217\nwhere the max operator is taken componentwise. In other words, v\u2217 is a fixed point of the nonlinear operator T : v 7\u2192 max\u03c0 T\u03c0v. For any value vector v, we say that a policy \u03c0 is greedy with respect to the value v if it satisfies:\n\u03c0 \u2208 argmax \u03c0\u2032 T\u03c0\u2032v\nor equivalently T\u03c0v = Tv. With some slight abuse of notation, we write G(v) for any policy that is greedy with respect to v. The notions of optimal value function and greedy policies are fundamental to optimal control because of the following property: any policy \u03c0\u2217 that is greedy with respect to the optimal value v\u2217 is an optimal policy and its value v\u03c0\u2217 is equal to v\u2217.\nLet \u03c0 be some policy. For any policy \u03c0\u2032, we consider the quantity\na\u03c0 \u2032\n\u03c0 = T\u03c0\u2032v\u03c0 \u2212 v\u03c0\nthat measures the difference in value resulting from switching the first action to \u03c0\u2032 with respect to always using \u03c0; we shall call it the advantage of \u03c0\u2032 with respect to \u03c0. Furthermore, we call maximal advantage with respect to \u03c0 the componentwise best such advantage:\na\u03c0 = max \u03c0\u2032\na\u03c0 \u2032\n\u03c0 = Tv\u03c0 \u2212 v\u03c0 ,\nwhere the second equality follows from the very definition of the Bellman operator T . While the advantage a\u03c0 \u2032\n\u03c0 may have negative values, the maximal advantage a\u03c0 has only non-negative values. We call the set of switchable states of \u03c0 the set of states for which the maximal advantage with respect to \u03c0 is positive:\nS\u03c0 = {i, a\u03c0(i) > 0}.\nAssume now that \u03c0 is non-optimal (this implies that S\u03c0 is a non-empty set). For any non-empty subset Y of S\u03c0, we denote switch(\u03c0, Y ) a policy satisfying:\n\u2200i, switch(\u03c0, Y )(i) =\n{\nG(v\u03c0)(i) if i \u2208 Y \u03c0(i) if i 6\u2208 Y.\nThe following result is well known (see for instance Puterman (1994)).\nLemma 1. Let \u03c0 be some non-optimal policy. If \u03c0\u2032 = switch(\u03c0, Y ) for some non-empty subset Y of S\u03c0, then v\u03c0\u2032 \u2265 v\u03c0 and there exists at least one state i such that v\u03c0\u2032(i) > v\u03c0(i).\nThis lemma is the foundation of the well-known iterative procedure, called Policy Iteration (PI), that generates a sequence of policies (\u03c0k) as follows.\n\u03c0k+1 \u2190 switch(\u03c0k, Yk) for some set Yk such that \u2205 ( Yk \u2286 S\u03c0k .\nThe choice for the subsets Yk leads to different variations of PI. In this paper we will focus on two of them:\n\u2022 When for all iterations k, Yk = S\u03c0k , that is one switches the actions in all states with positive advantage with respect to \u03c0k, the above algorithm is known as Howard\u2019s PI; it can be seen then that \u03c0k+1 \u2208 G(v\u03c0k).\n\u2022 When for all iterations k, Yk is a singleton containing a state ik \u2208 argmaxi a\u03c0k(i), that is if we only switch one action in the state with maximal advantage with respect to \u03c0k, we will call it Simplex-PI2.\nSince it generates a sequence of policies with increasing values, any variation of PI converges to an optimal policy in a number of iterations that is smaller than the total number of policies. In practice, PI converges in very few iterations. On random MDP instances, convergence often occurs in time sub-linear in n. The aim of this paper is to discuss existing and provide new upper bounds on the number of iterations required by Howard\u2019s PI and Simplex-PI that are much sharper than mn.\nIn the next sections, we describe some known results\u2014see also Ye (2011) for a recent and comprehensive review\u2014about the number of iterations required by Howard\u2019s PI and Simplex-PI, along with some of our original improvements and extensions. For clarity, all proofs are deferred to the later sections."}, {"heading": "2 Bounds with respect to a fixed discount factor \u03b3 < 1", "text": "A key observation for both algorithms, that will be central to the results we are about to discuss, is that the sequences they generate satisfy some contraction property3. For any vector u \u2208 Rn, let \u2016u\u2016\u221e = max1\u2264i\u2264n|u(i)| be the max-norm of u. Let 1 be the vector of which all components are equal to 1.\nLemma 2 (e.g. Puterman (1994), proof in Section 5). The sequence (\u2016v\u2217\u2212v\u03c0k\u2016\u221e)k\u22650 built by Howard\u2019s PI is contracting with coefficient \u03b3.\nLemma 3 ((Ye, 2011), proof in Section 6). The sequence (1T (v\u2217 \u2212 v\u03c0k))k\u22650 built by Simplex-PI is contracting with coefficient 1\u2212 1\u2212\u03b3\nn .\nContraction is a widely known property for Howard\u2019s PI, and it was to our knowledge first proved by (Ye, 2011) for Simplex-PI; we provide simple proofs in this paper for the sake of completeness. While the first contraction property is based on the \u2016 \u00b7 \u2016\u221e-norm, the second can be equivalently expressed in terms of the \u2016 \u00b7 \u20161-norm defined by \u2016u\u20161 = \u2211n\ni=1 |u(i)|, since the vectors v\u2217 \u2212 v\u03c0k are non-negative and thus satisfy 1T (v\u2217 \u2212 v\u03c0k) = \u2016v\u2217 \u2212 v\u03c0k\u20161. Contraction has the following immediate consequence 4.\nCorollary 1. Let Vmax = max\u03c0 \u2016r\u03c0\u2016\u221e\n1\u2212\u03b3 be an upper bound on \u2016v\u03c0\u2016\u221e for all policies \u03c0. In order to get an\n\u01eb-optimal policy, that is a policy \u03c0k satisfying \u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 \u01eb, Howard\u2019s PI requires at most \u2308 log Vmax \u01eb\n1\u2212\u03b3\n\u2309\niterations, while Simplex-PI requires at most \u2308 n log nVmax \u01eb\n1\u2212\u03b3\n\u2309\niterations.\nThese bounds depend on the precision term \u01eb, which means that Howard\u2019s PI and Simplex-PI are weakly polynomial for a fixed discount factor \u03b3. An important breakthrough was recently achieved by Ye (2011) who proved that one can remove the dependency with respect to \u01eb, and thus show that Howard\u2019s PI and Simplex-PI are strongly polynomial for a fixed discount factor \u03b3.\nTheorem 1 (Ye (2011)). Simplex-PI and Howard\u2019s PI both terminate after at most\n(m\u2212 n)\n\u2308\nn\n1\u2212 \u03b3 log\n(\nn2\n1\u2212 \u03b3\n)\u2309\n= O\n(\nmn\n1\u2212 \u03b3 log\nn\n1\u2212 \u03b3\n)\niterations.\nThe proof is based on the fact that PI corresponds to the simplex algorithm in a linear programming formulation of the MDP problem. Using a more direct proof\u2014not based on linear programming arguments\u2014Hansen et al. (2013) recently improved the result by a factor O(n) for Howard\u2019s PI.\n2In this case, PI is equivalent to running the simplex algorithm with the highest-pivot rule on a linear program version of the MDP problem (Ye, 2011).\n3A sequence of non-negative numbers (xk)k\u22650 is contracting with coefficient \u03b1 if and only if for all k \u2265 0, xk+1 \u2264 \u03b1xk. 4For Howard\u2019s PI, we have: \u2016v\u2217\u2212v\u03c0k\u2016\u221e \u2264 \u03b3 k\u2016v\u2217\u2212v\u03c00\u2016\u221e \u2264 \u03b3 kVmax. Thus, a sufficient condition for \u2016v\u2217\u2212v\u03c0k\u2016\u221e < \u01eb\nis \u03b3kVmax < \u01eb, which is implied by k \u2265 log\nVmax \u01eb\n1\u2212\u03b3 >\nlog Vmax\n\u01eb log 1 \u03b3 . For Simplex-PI, we have \u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 \u2016v\u2217 \u2212 v\u03c0k\u20161 \u2264\n(\n1\u2212 1\u2212\u03b3 n )k \u2016v\u2217 \u2212 v\u03c00\u20161 \u2264 ( 1\u2212 1\u2212\u03b3 n )k nVmax, and the conclusion is similar to that for Howard\u2019s PI.\nTheorem 2 (Hansen et al. (2013)). Howard\u2019s PI terminates after at most\n(m+ 1)\n\u2308\n1\n1\u2212 \u03b3 log\n(\nn\n1\u2212 \u03b3\n)\u2309\n= O\n(\nm\n1\u2212 \u03b3 log\nn\n1\u2212 \u03b3\n)\niterations.\nOur first results, that are consequences of the contraction property of Howard\u2019s PI (Lemma 2) are stated in the following theorems.\nTheorem 3 (Proof in Section 7). Howard\u2019s PI terminates after at most\n(m\u2212 n)\n\u2308\n1\n1\u2212 \u03b3 log\n(\n1\n1\u2212 \u03b3\n)\u2309\n= O\n(\nm\n1\u2212 \u03b3 log\n1\n1\u2212 \u03b3\n)\niterations.\nTheorem 4 (Proof in Section 8). Simplex-PI terminates after at most\nn(m\u2212 n)\n(\n1 + 2\n1\u2212 \u03b3 log\n1\n1\u2212 \u03b3\n)\n= O\n(\nmn\n1\u2212 \u03b3 log\n1\n1\u2212 \u03b3\n)\niterations.\nBoth results are a factor O(log n) better than the previously known results provided by Hansen et al. (2013) and Ye (2011). These improvements boil down to the use of the \u2016 \u00b7 \u2016\u221e-norm instead of the \u2016 \u00b7 \u20161- norm at various points of the previous analyses. For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting. For Simplex-PI, however, the line of analysis is slightly trickier: it amounts to bound the improvement in value at individual states and requires a bit of bookkeeping; the technique we use is to our knowledge original.\nThe bound for Simplex-PI is a factor O(n) larger than that for Howard\u2019s PI5. However, since one changes only one action per iteration, each iteration has a complexity that is in a worst-case sense lower by a factor n: the update of the value can be done in time O(n2) through the Sherman-Morrisson formula, though in general each iteration of Howard\u2019s PI, which amounts to compute the value of some policy that may be arbitrarily different from the previous policy, may require O(n3) time. Thus, it is remarkable that both algorithms seem to have a similar complexity.\nThe linear dependency of the bound for Howard\u2019s PI with respect to m is optimal (Hansen, 2012, Chapter 6.4). The linear dependency with respect to n or m (separately) is easy to prove for Simplex-PI; we conjecture that Simplex-PI\u2019s complexity is proportional to nm, and thus that our bound is tight for a fixed discount factor. The dependency with respect to the term 11\u2212\u03b3 may be improved, but removing it is impossible for Howard\u2019s PI and very unlikely for Simplex-PI. Fearnley (2010) describes an MDP for which Howard\u2019s PI requires an exponential (in n) number of iterations for \u03b3 = 1 and Hollanders et al. (2012) argued that this holds also when \u03b3 is in the vicinity of 1. Though a similar result does not seem to exist for Simplex-PI in the literature, Melekopoglou and Condon (1994) consider four variations of PI that all switch one action per iteration, and show through specifically designed MDPs that they may require an exponential (in n) number of iterations when \u03b3 = 1."}, {"heading": "3 Bounds for Simplex-PI that are independent of \u03b3", "text": "In this section, we will describe some bounds that do not depend on \u03b3 but that will be based on some structural properties of the MDP. On this topic, Post and Ye (2013) recently showed the following result for deterministic MDPs.\nTheorem 5 (Post and Ye (2013)). If the MDP is deterministic, then Simplex-PI terminates after at most O(n3m2 log2 n) iterations.\n5Note that it was also the case in Corollary 1.\nGiven a policy \u03c0 of a deterministic MDP, states are either on cycles or on paths induced by \u03c0. The core of the proof relies on the following lemmas that altogether show that cycles are created regularly and that significant progress is made every time a new cycle appears; in other words, significant progress is made regularly.\nLemma 4 (Post and Ye (2013, Lemma 3.4)). If the MDP is deterministic, after O(n2m logn) iterations, either Simplex-PI finishes or a new cycle appears.\nLemma 5 (Post and Ye (2013, Lemma 3.5)). If the MDP is deterministic, when Simplex-PI moves from \u03c0 to \u03c0\u2032 where \u03c0\u2032 involves a new cycle, we have\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) \u2264\n(\n1\u2212 1\nn\n)\n1 T (v\u03c0\u2217 \u2212 v\u03c0).\nIndeed, these observations suffice to prove6 that Simplex-PI terminates after O(n2m2 log n1\u2212\u03b3 ). Completely removing the dependency with respect to the discount factor \u03b3\u2014the term in O(log 11\u2212\u03b3 )\u2014requires a careful extra work described in Post and Ye (2013), which incurs an extra term of order O(n log(n)). The main result of this section is to show how these results can be extended to a more general setting. While Ye (2011) reason on states that belong to paths and cycles induced by policies on deterministic MDPs, we shall consider their natural generalization for stochastic MDPs: transient states and recurrent classes induced by policies. Precisely, we are going to consider bounds\u2014uniform on all policies and states\u2014of the average time 1) spent in transient states and 2) needed to revisit states in recurrent classes. For any policy \u03c0 and state i, denote \u03c4\u03c0(i, t) the expected cumulative time spent in state i until time t\u22121 given than the process starts from the uniform distribution U on X and takes actions according to \u03c0:\n\u03c4\u03c0(i, t) = E\n[\nt\u22121 \u2211\nk=0\n1it=i | i0 \u223c U, at = \u03c0(it)\n]\n=\nt\u22121 \u2211\nk=0\nP(it = i | i0 \u223c U, at = \u03c0(it)),\nwhere 1 denotes the indicator function. In addition, consider the vector \u00b5\u03c0 onX providing the asymptotic frequency in all states given that policy \u03c0 is used and that the process starts from the uniform distribution U :\n\u2200i, \u00b5\u03c0(i) = lim t\u2192\u221e\n1 t \u03c4\u03c0(i, t).\nWhen the Markov chain induced by \u03c0 is ergodic, and thus admits a unique stationary distribution, \u00b5\u03c0 is equal to this very stationary distribution. However, our definition is more general in that policies may induce Markov chains with aperiodicity and/or multiple recurrent classes. For any state i that is transient for the Markov chain induced by \u03c0, it is well known that limt\u2192\u221e \u03c4\n\u03c0(i, t) < \u221e and \u00b5\u03c0(i) = 0. However, for any recurrent state i, we know that limt\u2192\u221e \u03c4\n\u03c0(i, t) = \u221e and \u00b5\u03c0(i) > 0; in particular, if i belongs to some recurrent class R, which is reached with probability q from the uniform distribution U , then q\n\u00b5\u03c0(i) is the expected time between two visits of the state i.\nWe are now ready to express the structural properties with which we can provide an extension of the analysis of Post and Ye (2013).\nDefinition 1. Let \u03c4t and \u03c4r be the smallest finite constants such that for all policies \u03c0 and states i,\nif i is transient for \u03c0, then lim t\u2192\u221e\n\u03c4\u03c0(i, t) \u2264 \u03c4t\nelse if i is recurrent for \u03c0, then 1\n\u00b5\u03c0(i) \u2264 \u03c4r .\nNote that for any finite MDP, these finite constants always exist. With Definition 1 in hand, we can generalize Lemmas 4-5 as follows.\nLemma 6. After at most (m\u2212n)\u2308n2\u03c4t log(n2\u03c4t)\u2309+n\u2308n2\u03c4t log(n2)\u2309 iterations either Simplex-PI finishes or a new recurrent class appears.\n6This can be done by using arguments similar those for Theorem 1 (see Ye (2011) for details).\nLemma 7. When Simplex-PI moves from \u03c0 to \u03c0\u2032 where \u03c0\u2032 involves a new recurrent class, we have\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) \u2264\n(\n1\u2212 1\nn\u03c4r\n)\n1 T (v\u03c0\u2217 \u2212 v\u03c0).\nFrom these generalized observations, we can deduce the following original result.\nTheorem 6 (Proof in Section 9). Simplex-PI terminates after at most\n[\nm\u2308n\u03c4r log(n 2\u03c4r)\u2309+ (m\u2212 n)\u2308n\u03c4r log(n 2\u03c4t)\u2309 ] [ (m\u2212 n)\u2308n2\u03c4t log(n 2\u03c4t)\u2309+ n\u2308n 2\u03c4t log(n 2)\u2309 ] = O\u0303 ( n3m2\u03c4t\u03c4r )\niterations.\nRemark 1. This new result extends the result obtained for deterministic MDPs by Post and Ye (2013) recalled in Theorem 5. In the deterministic case, it is easy to see that \u03c4t = 1 and \u03c4r \u2264 n. Then, while Lemma 6 is a strict generalization of Lemma 4, Lemma 7 provides a contraction factor that is slightly weaker than that of Lemma 5\u2014 (\n1\u2212 1 n2\n) instead of ( 1\u2212 1 n )\n\u2014, which makes the resulting bound provided in Theorem 6 a factor O(n) worse than that of Theorem 5. This extra term in the bound is the price paid for making the constant \u03c4r (and the vector \u00b5\u03c0) independent of the discount factor \u03b3, that is by presenting our result in a way that only depends on the dynamics of the underlying MDP. An analysis that would strictly generalizes that of Ye (2011) can be done under a variation of Definition 1 where the constants \u03c4t and \u03c4r depend on the discount factor 7 \u03b3.\nAn immediate consequence of the above result is that Simplex-PI is strongly polynomial for sets of MDPs that are much larger than the deterministic MDPs mentioned in Theorem 5.\nCorollary 2. For any family of MDPs indexed by n and m such that \u03c4t and \u03c4r are polynomial functions of n and m, Simplex-PI terminates after a number of steps that is polynomial in n and m."}, {"heading": "4 Similar results for Howard\u2019s PI?", "text": "One may then wonder whether similar results can be derived for Howard\u2019s PI. Unfortunately, and as briefly mentioned by Post and Ye (2013), the line of analysis developed for Simplex-PI does not seem to adapt easily to Howard\u2019s PI, because simultaneously switching several actions can interfere in a way such that the policy improvement turns out to be small. We can be more precise on what actually breaks in the approach we have described so far. On the one hand, it is possible to write counterparts of Lemmas 4 and 6 for Howard\u2019s PI (see Section 10 for proofs).\nLemma 8. If the MDP is deterministic, after at most n iterations, either Howard\u2019s PI finishes or a new cycle appears.\nLemma 9. After at most (m\u2212n)\u2308n2\u03c4t log(n2\u03c4t)\u2309+n\u2308n2\u03c4t log(n2)\u2309 iterations, either Howard\u2019s PI finishes or a new recurrent class appears.\nOn the other hand, we did not manage to adapt Lemma 5 nor Lemma 7. In fact, it is unlikely that a result similar to that of Lemma 5 will be shown to hold for Howard\u2019s PI. In a recent deterministic example due to Hansen and Zwick (2010) to show that Howard\u2019s PI may require at least \u2126(n2) iterations, new\n7 Define the following \u03b3-discounted variation of \u03c4\u03c0(i, t): \u03c4\u03c0\u03b3 (i, t) = E [ \u2211t\u22121 k=0 \u03b3k1it=i | i0 \u223c U, at = \u03c0(it) ]\n= \u2211t\u22121\nk=0 \u03b3kP(it = i | i0 \u223c U, at = \u03c0(it)) and \u03c4\u03c0\u03b3 (i) = limt\u2192\u221e \u03c4 \u03c0 \u03b3 (i, t). Assume that we have constants \u03c4 \u03b3 t , and \u03c4 \u03b3 r\nsuch that for every policy \u03c0, \u03c4\u03c0\u03b3 (i) \u2264 \u03c4 \u03b3 t if i is a transient state for \u03c0, and 1 (1\u2212\u03b3)\u03c4\u03c0\u03b3 (i) \u2264 \u03c4\u03b3r if i is recurrent for \u03c0. Then, one can derive a bound similar to that of Theorem 6 where \u03c4t and \u03c4r are respectively replaced by \u03c4 \u03b3 t and \u03c4 \u03b3 r n . At a more technical level, our analysis begins by removing the dependency with respect to \u03b3: Lemma 11, page 11, shows that for every policy \u03c0, \u03c4\u03c0\u03b3 (i) \u2264 \u03c4t if i is a transient state for \u03c0, and\n1 (1\u2212\u03b3)\u03c4\u03c0\u03b3 (i) \u2264 n\u03c4r if i is recurrent for \u03c0 (this is where we pay\nthe O(n) term because the upper bound is n\u03c4r instead of \u03c4 \u03b3 r ); we then follow the line of arguments originally given by Post and Ye (2013), though our more general setting induces a few technicalities (in particular in the second part of the proof of Lemma 13 page 13).\ncycles are created every single iteration but the sequence of values satisfies8 for all iterations k < n 2\n4 + n 4\nand states i,\nv\u2217(i)\u2212 v\u03c0k+1(i) \u2265\n[\n1\u2212\n(\n2\nn\n)k ]\n(v\u2217(i)\u2212 v\u03c0k(i)).\nContrary to Lemma 5, as k grows, the amount of contraction gets (exponentially) smaller and smaller. With respect to Simplex-PI, this suggests that Howard\u2019s PI may suffer from subtle specific pathologies. In fact, the problem of determining the number of iterations required by Howard\u2019s PI has been challenging for almost 30 years. It was originally identified as an open problem by Schmitz (1985). In the simplest\u2014 deterministic\u2014case, the complexity is still an open problem: the currently best-known lower bound is O(n2) (Hansen and Zwick, 2010), while the best known upper bound is O(m n\nn ) Mansour and Singh\n(1999); Hollanders et al. (2014). On the positive side, an adaptation of the line of proof we have considered so far can be carried out under the following assumption.\nAssumption 1. The state space X can be partitioned in two sets T and R such that for all policies \u03c0, the states of T are transient and those of R are recurrent.\nUnder this additional assumption, we can deduce the following original bounds.\nTheorem 7 (Proof in Section 11). If the MDP satisfies Assumption 1, then Howard\u2019s PI and Simplex-PI terminate after at most\n(m\u2212 n) ( \u2308n\u03c4r logn 2\u03c4r\u2309+ \u2308n 2\u03c4t logn 2\u03c4t\u2309 ) = O\u0303(mn(n2\u03c4t + n\u03c4r))\niterations.\nIt should however be noted that Assumption 1 is rather restrictive. It implies that the algorithms converge on the recurrent states independently of the transient states, and thus the analysis can be decomposed in two phases: 1) the convergence on recurrent states and then 2) the convergence on transient states (given that recurrent states do not change anymore). The analysis of the first phase (convergence on recurrent states) is greatly facilitated by the fact that in this case, a new recurrent class appears every single iteration (this is in contrast with Lemmas 4, 6, 8 and 9 that were designed to show under which conditions cycles and recurrent classes are created). Furthermore, the analysis of the second phase (convergence on transient states) is similar to that of the discounted case of Theorems 3 and 4. In other words, this last result sheds some light on the practical efficiency of Howard\u2019s PI and Simplex-PI, and a general analysis of Howard\u2019s PI is still largely open, and constitutes intriguing future work.\nThe following sections contains detailed proofs of Lemmas 2 and 3, Theorems 3, 4, and 6, Lemmas 8 and 9, and finally Theorem 7. Before we start, we provide a particularly useful identity relating the difference between the values of two policies \u03c0 and \u03c0\u2032 and the relative advantage a\u03c0 \u2032\n\u03c0 .\nLemma 10. For all pairs of policies \u03c0 and \u03c0\u2032,\nv\u03c0\u2032 \u2212 v\u03c0 = (I \u2212 \u03b3P\u03c0\u2032) \u22121a\u03c0\n\u2032\n\u03c0 = (I \u2212 \u03b3P\u03c0) \u22121(\u2212a\u03c0\u03c0\u2032).\nProof. This first identity follows from simple linear algebra arguments:\nv\u03c0\u2032 \u2212 v\u03c0 = (I \u2212 \u03b3P\u03c0\u2032) \u22121r\u03c0\u2032 \u2212 v\u03c0 {v\u03c0\u2032 = T\u03c0\u2032v\u03c0\u2032 \u21d4 v\u03c0\u2032 = (I \u2212 \u03b3P\u03c0\u2032) \u22121r\u03c0\u2032}\n= (I \u2212 \u03b3P\u03c0\u2032) \u22121(r\u03c0\u2032 + \u03b3P\u03c0\u2032v\u03c0 \u2212 v\u03c0) = (I \u2212 \u03b3P\u03c0\u2032) \u22121(T\u03c0\u2032v\u03c0 \u2212 v\u03c0).\nThe second identity follows by symmetry.\n8This MDP has an even number of states n = 2p. The goal is to minimize the long term expected cost. The optimal value function satisfies v\u2217(i) = \u2212pN for all i, with N = p2 + p. The policies generated by Howard\u2019s PI have values v\u03c0k (i) \u2208 (p N\u2212k\u22121, pN\u2212k). We deduce that for all iterations k and states i, v\u2217(i)\u2212v\u03c0k+1 (i)\nv\u2217(i)\u2212v\u03c0k (i) \u2265 1+p\n\u2212k\u22122\n1+p\u2212k = 1\u2212 p\n\u2212k\u2212p\u2212k\u22122\n1+p\u2212k \u2265\n1\u2212 p\u2212k(1 \u2212 p\u22122) \u2265 1\u2212 p\u2212k.\nWe will repeatedly use the following property: since for any policy \u03c0, the matrix (1\u2212\u03b3)(I\u2212\u03b3P )\u22121 = (1\u2212 \u03b3)\n\u2211\u221e t=0(\u03b3P\u03c0) t is a stochastic matrix (as a mixture of stochastic matrices), then\n\u2016(I \u2212 \u03b3P )\u22121\u2016\u221e = 1\n1\u2212 \u03b3 ,\nwhere \u2016 \u00b7 \u2016\u221e is the natural induced max-norm on matrices. Finally, for any vector/matrix A and any number \u03bb, we shall use the notation \u201cA \u2265 \u03bb\u201d (respectively \u201cA \u2264 \u03bb\u201d) for denoting the fact that \u201call the coefficients of A are greater or equal to (respectively smaller or equal to) \u03bb\u201d."}, {"heading": "5 Contraction property for Howard\u2019s PI (Proof of Lemma 2)", "text": "For any k, we have\nv\u03c0\u2217 \u2212 v\u03c0k = T\u03c0\u2217v\u03c0\u2217 \u2212 T\u03c0\u2217v\u03c0k\u22121 + T\u03c0\u2217v\u03c0k\u22121 \u2212 T\u03c0kv\u03c0k\u22121 + T\u03c0kv\u03c0k\u22121 \u2212 T\u03c0kv\u03c0k {\u2200\u03c0, T\u03c0v\u03c0 = v\u03c0}\n\u2264 \u03b3P\u03c0\u2217(v\u03c0\u2217 \u2212 v\u03c0k\u22121) + \u03b3P\u03c0k(v\u03c0k\u22121 \u2212 v\u03c0k) {T\u03c0\u2217v\u03c0k\u22121 \u2264 T\u03c0kv\u03c0k\u22121}\n\u2264 \u03b3P\u03c0\u2217(v\u03c0\u2217 \u2212 v\u03c0k\u22121). {Lemma 1 and P\u03c0k \u2265 0}\nSince v\u03c0\u2217 \u2212 v\u03c0k is non-negative, we can take the max-norm and get:\n\u2016v\u03c0\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 \u03b3\u2016v\u03c0\u2217 \u2212 v\u03c0k\u22121\u2016\u221e."}, {"heading": "6 Contraction property for Simplex-PI (Proof of Lemma 3)", "text": "The proof we provide here is very close to the one given by Ye (2011). We provide it here for completeness, and also because it resembles the proofs we will provide for the bounds that are independent of \u03b3.\nOn the one hand, using Lemma 10, we have for any k:\nv\u03c0k+1 \u2212 v\u03c0k = (I \u2212 \u03b3P\u03c0k+1) \u22121a\u03c0k+1\u03c0k\n\u2265 a\u03c0k+1\u03c0k , {(I \u2212 \u03b3P\u03c0k+1) \u22121 \u2212 I \u2265 0 and a\u03c0k+1\u03c0k \u2265 0}\nwhich implies, by left multiplying by the vector 1T , that\n1 T (v\u03c0k+1 \u2212 v\u03c0k) \u2265 1 Ta\u03c0k+1\u03c0k . (1)\nOn the other hand, we have:\nv\u03c0\u2217 \u2212 v\u03c0k = (I \u2212 \u03b3P\u03c0\u2217) \u22121a\u03c0\u2217\u03c0k {Lemma 10}\n\u2264 1\n1\u2212 \u03b3 max s a\u03c0k+1\u03c0k (s) {\u2016(I \u2212 \u03b3P\u03c0\u2217)\n\u22121\u2016\u221e = 1\n1\u2212 \u03b3 and max s a\u03c0k+1\u03c0k (s) = maxs,\u03c0 a\u03c0\u03c0k(s) \u2265 0}\n\u2264 1\n1\u2212 \u03b3 1 Ta\u03c0k+1\u03c0k , {\u2200x \u2265 0, maxs x(s) \u2264 1Tx}\nwhich implies that\n1 T a\u03c0k+1\u03c0k \u2265 (1 \u2212 \u03b3)\u2016v\u03c0\u2217 \u2212 v\u03c0k\u2016\u221e\n\u2265 1\u2212 \u03b3\nn 1 T (v\u03c0\u2217 \u2212 v\u03c0k). {\u2200x, 1 Tx \u2264 n\u2016x\u2016\u221e} (2)\nCombining Equations (1) and (2), we get:\n1 T (v\u03c0\u2217 \u2212 v\u03c0k+1) = 1 T (v\u03c0\u2217 \u2212 v\u03c0k)\u2212 1 T (v\u03c0k+1 \u2212 v\u03c0k) \u2264 1 T (v\u03c0\u2217 \u2212 v\u03c0k)\u2212 1\u2212 \u03b3\nn 1 T (v\u03c0\u2217 \u2212 v\u03c0k)\n=\n(\n1\u2212 1\u2212 \u03b3\nn\n)\n1 T (v\u03c0\u2217 \u2212 v\u03c0k)."}, {"heading": "7 A bound for Howard\u2019s PI when \u03b3 < 1 (Proof of Theorem 3)", "text": "Although the overall line or arguments follows from those given originally by Ye (2011) and adapted by Hansen et al. (2013), our proof is slightly more direct and leads to a better result.\nFor any k, we have:\n\u2212a\u03c0k\u03c0\u2217 = (I \u2212 \u03b3P\u03c0k)(v\u2217 \u2212 v\u03c0k) {Lemma 10}\n\u2264 v\u2217 \u2212 v\u03c0k . {v\u2217 \u2212 v\u03c0k \u2265 0 and P\u03c0k \u2265 0}\nBy the optimality of \u03c0\u2217, \u2212a \u03c0k \u03c0\u2217 is non-negative, and we can take the max-norm:\n\u2016a\u03c0k\u03c0\u2217\u2016\u221e \u2264 \u2016v\u2217 \u2212 v\u03c0k\u2016\u221e\n\u2264 \u03b3k\u2016v\u03c0\u2217 \u2212 v\u03c00\u2016\u221e {Lemma 2} = \u03b3k\u2016(I \u2212 \u03b3P\u03c00) \u22121(\u2212a\u03c00\u03c0\u2217)\u2016\u221e {Lemma 10} \u2264 \u03b3k\n1\u2212 \u03b3 \u2016a\u03c00\u03c0\u2217\u2016\u221e. {\u2016(I \u2212 \u03b3P\u03c00)\n\u22121\u2016\u221e = 1\n1\u2212 \u03b3 }\nBy definition of the max-norm, and as a\u03c00\u03c0\u2217 \u2264 0 (using again the fact that \u03c0\u2217 is optimal), there exists a state s0 such that \u2212a\u03c00\u03c0\u2217(s0) = \u2016a \u03c00 \u03c0\u2217 \u2016\u221e. We deduce that for all k,\n\u2212a\u03c0k\u03c0\u2217(s0) \u2264 \u2016a \u03c0k \u03c0\u2217 \u2016\u221e \u2264\n\u03b3k\n1\u2212 \u03b3 \u2016a\u03c00\u03c0\u2217\u2016\u221e =\n\u03b3k\n1\u2212 \u03b3 (\u2212a\u03c00\u03c0\u2217(s0)).\nAs a consequence, the action \u03c0k(s0) must be different from \u03c00(s0) when \u03b3k\n1\u2212\u03b3 < 1, that is for all values of k satisfying\nk \u2265 k\u2217 =\n\u2308\nlog 11\u2212\u03b3 1\u2212 \u03b3\n\u2309\n>\n\u2308\nlog 11\u2212\u03b3\nlog 1 \u03b3\n\u2309\n.\nIn other words, if some policy \u03c0 is not optimal, then one of its non-optimal actions will be eliminated for good after at most k\u2217 iterations. By repeating this argument, one can eliminate all non-optimal actions (there are at most n\u2212m of them), and the result follows."}, {"heading": "8 A bound for Simplex-PI when \u03b3 < 1 (Proof of Theorem 4)", "text": "At each iteration k, let sk be the state in which an action is switched. We have (by definition of Simplex-PI):\na\u03c0k+1\u03c0k (sk) = max\u03c0,s a\u03c0\u03c0k(s).\nStarting with arguments similar to those for the contraction property of Simplex-PI, we have on the one hand:\nv\u03c0k+1 \u2212 v\u03c0k = (I \u2212 \u03b3P\u03c0k+1) \u22121a\u03c0k+1\u03c0k {Lemma 10}\n\u2265 a\u03c0k+1\u03c0k , {(I \u2212 \u03b3P\u03c0k+1) \u22121 \u2212 I \u2265 0 and a\u03c0k+1\u03c0k \u2265 0}\nwhich implies that\nv\u03c0k+1(sk)\u2212 v\u03c0k(sk) \u2265 a \u03c0k+1 \u03c0k (sk). (3)\nOn the other hand, we have:\nv\u03c0\u2217 \u2212 v\u03c0k = (I \u2212 \u03b3P\u03c0\u2217) \u22121a\u03c0\u2217\u03c0k {Lemma 10}\n\u2264 1\n1\u2212 \u03b3 a\u03c0k+1\u03c0k (sk) {\u2016(I \u2212 \u03b3P\u03c0\u2217)\n\u22121\u2016\u221e = 1\n1\u2212 \u03b3 and a\u03c0k+1\u03c0k (sk) = maxs,\u03c0 a\u03c0\u03c0k(s) \u2265 0}\nwhich implies that\n\u2016v\u03c0\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 1\n1\u2212 \u03b3 a\u03c0k+1\u03c0k (sk). (4)\nWrite \u2206k = v\u03c0\u2217 \u2212 v\u03c0k . From Equations (3) and (4), we deduce that:\n\u2206k+1(sk) \u2264 \u2206k(sk)\u2212 (1\u2212 \u03b3)\u2016\u2206k\u2016\u221e =\n(\n1\u2212 (1\u2212 \u03b3) \u2016\u2206k\u2016\u221e \u2206k(sk)\n)\n\u2206k(sk).\nThis implies\u2014since \u2206k(sk) \u2264 \u2016\u2206k\u2016\u221e\u2014that\n\u2206k+1(sk) \u2264 \u03b3\u2206k(sk),\nbut also\u2014since \u2206k(sk) and \u2206k+1(sk) are non-negative and thus (\n1\u2212 (1\u2212 \u03b3)\u2016\u2206k\u2016\u221e\u2206k(sk)\n)\n\u2265 0\u2014that\n\u2016\u2206k\u2016\u221e \u2264 1\n1\u2212 \u03b3 \u2206k(sk).\nNow, write nk for the vector on the state space such that nk(s) is the number of times state s has been switched until iteration k (including k). Since by Lemma 1 the sequence (\u2206k)k\u22650 is non-increasing, we have\n\u2016\u2206k\u2016\u221e \u2264 1\n1\u2212 \u03b3 \u2206k(sk) \u2264\n\u03b3nk\u22121(sk)\n1\u2212 \u03b3 \u22060(sk) \u2264\n\u03b3nk\u22121(sk)\n1\u2212 \u03b3 \u2016\u22060\u2016\u221e. (5)\nAt any iteration k, let s\u2217k = argmaxs nk\u22121(s) be the state in which actions have been switched the most. Since at each iteration k, one of the n components of nk is increased by 1, we necessarily have\nnk\u22121(s \u2217 k) \u2265\n\u230a\nk \u2212 1\nn\n\u230b\n\u2265 k \u2212 n\nn . (6)\nWrite k\u2217 \u2264 k \u2212 1 for the last iteration when the state s\u2217k was updated, such that we have\nnk\u22121(s \u2217 k) = nk\u2217\u22121(sk\u2217). (7)\nSince (\u2016\u2206k\u2016\u221e)k\u22650 is nonincreasing (using again Lemma 1), we have\n\u2016\u2206k\u2016\u221e \u2264 \u2016\u2206k\u2217\u2016\u221e {k \u2217 \u2264 k \u2212 1}\n\u2264 \u03b3nk\u2217\u22121(sk\u2217 )\n1\u2212 \u03b3 \u2016\u22060\u2016\u221e {Equation (5)}\n= \u03b3nk\u22121(s\n\u2217 k)\n1\u2212 \u03b3 \u2016\u22060\u2016\u221e {Equation (7)}\n\u2264 \u03b3\nk\u2212n n\n1\u2212 \u03b3 \u2016\u22060\u2016\u221e. {Equation (6) and x 7\u2192 \u03b3x is decreasing}\nWe are now ready to finish the proof. By using arguments similar to those for Howard\u2019s PI, we have:\n\u2016a\u03c0k\u03c0\u2217\u2016\u221e \u2264 \u2016\u2206k\u2016\u221e \u2264 \u03b3\nk\u2212n n\n1\u2212 \u03b3 \u2016\u22060\u2016\u221e \u2264\n\u03b3 k\u2212n n\n(1 \u2212 \u03b3)2 \u2016a\u03c00\u03c0\u2217\u2016\u221e.\nIn particular, we can deduce from the above relation that as soon as \u03b3 k\u2212n n\n(1\u2212\u03b3)2 < 1, that is for instance\nwhen k > k\u2217 = n (\n1 + 21\u2212\u03b3 log 1 1\u2212\u03b3\n)\n, one of the non-optimal actions of \u03c00 cannot appear in \u03c0k. Thus,\nevery k\u2217 iterations, a non-optimal action is eliminated for good, and the result follows from the fact that there are at most n\u2212m non-optimal actions."}, {"heading": "9 A general bound for Simplex-PI (Proof of Theorem 6)", "text": "The proof we give here is strongly inspired by that for the deterministic case of Post and Ye (2013): the steps (a series of lemmas) are similar. There are mainly two differences. First, our arguments are more direct in the sense that we do not refer to linear programming, but only provide simple linear algebra arguments. Second, it is more general: for any policy \u03c0, we consider the set of transient states (respectively recurrent classes) instead of the set of path states (respectively cycles); it slightly complicates the arguments, the most complicated extension being the second part of the proof of the forthcoming Lemma 13.\nConsider the vector x\u03c0 = (I\u2212\u03b3PT\u03c0 ) \u22121\n1 that provides a discounted measure of state visitations along the trajectories induced by a policy \u03c0 starting from the uniform distribution U on the state space X :\n\u2200i \u2208 X, x\u03c0(i) = n \u221e \u2211\nt=0\n\u03b3tP(it = i | i0 \u223c U, at = \u03c0(it)).\nThis vector plays a crucial role in the analysis. For any policy \u03c0 and state i, we trivially have x\u03c0(i) \u2208 (\n1, n1\u2212\u03b3\n)\n. In the case of deterministic MDPs, Post and Ye (2013)) exploits the fact that x\u03c0(i) belongs\nto the set (1, n) when i is on path of \u03c0, while x\u03c0(i) belongs to the set ( 1 1\u2212\u03b3 , n 1\u2212\u03b3 ) when i is on a cycle of \u03c0. Our extension of their result to the case of general (stochastic) MDPs will rely on the following result. For any policy \u03c0, we shall write R(\u03c0) for the set of states that are recurrent for \u03c0.\nLemma 11. With the constants \u03c4t and \u03c4r of Definition 1, we have for every discount factor \u03b3,\n\u2200i 6\u2208 R(\u03c0), 1 \u2264 x\u03c0(i) \u2264 n\u03c4t (8)\n\u2200i \u2208 R(\u03c0), 1\n\u03c4r \u2264 (1\u2212 \u03b3)x\u03c0(i) \u2264 n. (9)\nProof. The fact that x\u03c0(i) belongs to ( 1, n1\u2212\u03b3 ) is obvious from the definition of x\u03c0. The upper bound on x\u03c0 on the transient states i follows from the fact that for any policy \u03c0,\n\u03c4t(i) \u2265 lim t\u2192\u221e\n\u03c4\u03c0(i, t)\n=\n\u221e \u2211\nk=0\nP(it = i | i0 \u223c U, at = \u03c0(it))\n\u2265 \u221e \u2211\nk=0\n\u03b3kP(it = i | i0 \u223c U, at = \u03c0(it))\n= 1\nn x\u03c0(i).\nLet us now consider the lower bound on (1 \u2212 \u03b3)x\u03c0(i) when i is a recurrent state of some policy \u03c0. In general, the asymptotic frequency \u00b5\u03c0 of \u03c0 does not necessarily satisfy \u00b5\u03c0TP\u03c0 = P\u03c0 because P\u03c0 may correspond to an aperiodic or reducible chain. To deal with this issue, we consider the Cesa\u0300ro mean\nQ\u03c0 = lim t\u2192\u221e\n1\nt\nt\u22121 \u2211\nk=0\n(P\u03c0) k\nthat is well-defined (Stroock, 2005, Section 3.2). It can be shown (Fritz et al., 1979, Proposition 3.5(a)) that Q\u03c0 = Q\u03c0P\u03c0 = P\u03c0Q\u03c0 = Q\u03c0Q\u03c0. This implies in particular that\n(1\u2212 \u03b3)Q\u03c0(I \u2212 \u03b3P\u03c0) \u22121 = (1 \u2212 \u03b3)\n\u221e \u2211\nk=0\n\u03b3kQ\u03c0(P\u03c0) k = (1\u2212 \u03b3)\n\u221e \u2211\nk=0\n\u03b3kQ\u03c0 = Q\u03c0. (10)\nThen, by using twice the fact that \u00b5\u03c0 = 1 n Q\u03c0 T 1, we can see that for all recurrent states i,\n1 \u03c4r \u2264 \u00b5\u03c0(i)\n=\n[\n1 n Q\u03c0 T 1\n]\n(i)\n=\n[\n1 n (1\u2212 \u03b3)(I \u2212 \u03b3P\u03c0 T )\u22121Q\u03c0 T 1\n]\n(i) {Equation (10)}\n= [ (1\u2212 \u03b3)(I \u2212 \u03b3P\u03c0 T )\u22121\u00b5\u03c0 ] (i) \u2264 [ (1\u2212 \u03b3)(I \u2212 \u03b3P\u03c0 T )\u221211 ] (i) {\u00b5\u03c0 \u2264 1}\n= (1\u2212 \u03b3)x\u03c0(i).\nFinally, a rewriting of Lemma 10 in terms of the vector x\u03c0 will be useful in the following proofs: for any pair of policies \u03c0 and \u03c0\u2032,\n1 T (v\u03c0\u2032 \u2212 v\u03c0) = x\u03c0\u2032 Ta\u03c0 \u2032 \u03c0 = x\u03c0 T (\u2212a\u03c0\u03c0\u2032). (11)\nWe are now ready to delve into the details of the arguments. As mentioned before, the proof is structured in two steps: first, we will show that recurrent classes are created often; then we will show that significant progress is made every time a new recurrent class appears."}, {"heading": "9.1 Part 1: Recurrent classes are created often", "text": "Lemma 12. Suppose one moves from policy \u03c0 to policy \u03c0\u2032 without creating any recurrent class. Let \u03c0\u2020 be the final policy before either a new recurrent class appears or Simplex-PI terminates. Then\n1 T (v\u03c0\u2020 \u2212 v\u03c0\u2032) \u2264\n(\n1\u2212 1\nn2\u03c4t\n)\n1 T (v\u03c0\u2020 \u2212 v\u03c0).\nProof. The arguments are similar to those for the proof of Theorem 4. On the one hand, we have:\n1 T (v\u03c0\u2032 \u2212 v\u03c0) \u2265 1 Ta\u03c0 \u2032 \u03c0 . (12)\nOn the other hand, we have\n1 T (v\u03c0\u2020 \u2212 v\u03c0) = x T \u03c0\u2020 a \u03c0\u2020 \u03c0 {Equation (11)}\n= \u2211\ns6\u2208R(\u03c0\u2020)\nx\u03c0\u2020(s)a \u03c0\u2020 \u03c0 (s) +\n\u2211\ns\u2208R(\u03c0\u2020)\nx\u03c0\u2020(s)a \u03c0\u2020 \u03c0 (s)\n\u2264 n2\u03c4t max s6\u2208R(\u03c0\u2020) a \u03c0\u2020 \u03c0 (s) +\nn2\n1\u2212 \u03b3 max s\u2208R(\u03c0\u2020) a \u03c0\u2020 \u03c0 (s). {Equations (8)-(9)}\nSince by assumption recurrent classes of \u03c0\u2020 are also recurrent classes of \u03c0, we deduce that for all s \u2208 R(\u03c0\u2020), \u03c0\u2020(s) = \u03c0(s), so that maxs\u2208R(\u03c0\u2020) a \u03c0\u2020 \u03c0 (s) = 0. Thus, the second term of the above r.h.s. is null and\n1 T (v\u03c0\u2020 \u2212 v\u03c0) \u2264 n 2\u03c4t max s a \u03c0\u2020 \u03c0 (s)\n\u2264 n2\u03c4t max s a\u03c0 \u2032 \u03c0 (s) {max s T\u03c0\u2032v\u03c0(s) = max s,\u03c0\u0303 T\u03c0\u0303v\u03c0(s)}\n\u2264 n2\u03c4t1 Ta\u03c0\n\u2032\n\u03c0 . {\u2200x \u2265 0, max s\nx(s) \u2264 1Tx} (13)\nCombining Equations (12) and (13), we get:\n1 T (v\u03c0\u2020 \u2212 v\u03c0\u2032) = 1 T (v\u03c0\u2020 \u2212 v\u03c0)\u2212 1 T (v\u03c0\u2032 \u2212 v\u03c0)\n\u2264\n(\n1\u2212 1\nn2\u03c4t\n)\n1 T (v\u03c0\u2020 \u2212 v\u03c0).\nLemma 13. While Simplex-PI does not create any recurrent class nor finishes,\n\u2022 either an action is eliminated from policies after at most \u2308n2\u03c4t log(n2\u03c4t)\u2309 iterations,\n\u2022 or a recurrent class is broken after at most \u2308n2\u03c4t log(n2)\u2309 iterations.\nProof. Let \u03c0 be the policy in some iteration. Let \u03c0\u2020 be the last policy before a new recurrent class appears, and \u03c0\u2032 a policy generated after k iterations from \u03c0. We shall prove that one of the two events stated of the lemma must happen. Since\n0 \u2264 1T (v\u03c0\u2020 \u2212 v\u03c0) {v\u03c0\u2020 \u2265 v\u03c0}\n= x\u03c0 T (\u2212a\u03c0\u03c0\u2020) {Equation (11)} = \u2211\ns6\u2208R(\u03c0)\nx\u03c0(s)(\u2212a \u03c0 \u03c0\u2020 (s)) +\n\u2211\nC\u2208R(\u03c0)\n\u2211\ns\u2208C\nx\u03c0(s)(\u2212a \u03c0 \u03c0\u2020 (s))\nthere must exist either a state s0 6\u2208 R(\u03c0) such that\nx\u03c0(s0)(\u2212a \u03c0 \u03c0\u2020 (s0)) \u2265\n1 n x\u03c0 T (\u2212a\u03c0\u03c0\u2020) \u2265 0. (14)\nor a recurrent class R0 such that\n\u2211\ns\u2208R0\nx\u03c0(s)(\u2212a \u03c0 \u03c0\u2020 (s)) \u2265\n1 n x\u03c0 T (\u2212a\u03c0\u03c0\u2020) \u2265 0. (15)\nWe consider these two cases separately below.\n\u2022 case 1: Equation (14) holds for some s0 6\u2208 R(\u03c0). Let us prove by contradiction that for k sufficiently big, \u03c0\u2032(s0) 6= \u03c0(s0): let us assume that \u03c0\u2032(s0) = \u03c0(s0). Then\n1 T (v\u03c0\u2020 \u2212 v\u03c0\u2032) \u2265 v\u03c0\u2020(s0)\u2212 v\u03c0\u2032(s0) {v\u03c0\u2020 \u2265 v\u03c0\u2032}\n= v\u03c0\u2020(s0)\u2212 T\u03c0\u2032v\u03c0\u2032(s0) {v\u03c0\u2032 = T\u03c0\u2032v\u03c0\u2032} \u2265 v\u03c0\u2020(s0)\u2212 T\u03c0\u2032v\u03c0\u2020(s0) {v\u03c0\u2020 \u2265 v\u03c0\u2032} = \u2212a\u03c0 \u2032\n\u03c0\u2020 (s0)\n= \u2212a\u03c0\u03c0\u2020(s0) {\u03c0(s0) = \u03c0 \u2032(s0)} \u2265 1\nn\u03c4t x\u03c0(s0)(\u2212a\n\u03c0 \u03c0\u2020 (s0)) {Equation (8)}\n\u2265 1\nn2\u03c4t x\u03c0\nT (\u2212a\u03c0\u03c0\u2020) {Equation (14)}\n= 1\nn2\u03c4t 1 T (v\u03c0\u2020 \u2212 v\u03c0). {Equation (11)}\nIf there is no recurrent class creation, the contraction property given in Lemma 12 implies that if \u03c0\u2032 is obtained after k = \u2308n2\u03c4t log(n2\u03c4t)\u2309 > log(n2\u03c4t)\nlog 1 1\u2212 1\nn2\u03c4t\niterations, then\n1 T (v\u03c0\u2020 \u2212 v\u03c0\u2032) <\n1\nn2\u03c4t 1 T (v\u03c0\u2020 \u2212 v\u03c0),\nand we get a contradiction. As a conclusion, we necessarily have \u03c0\u2032(s0) 6= \u03c0(s0).\n\u2022 case 2: Equation (15) holds for some R0 that is a recurrent class of \u03c0. Let us prove by contradiction that for k sufficiently big, R0 cannot be a recurrent class of \u03c0\n\u2032: let us thus assume that R0 is a recurrent class of \u03c0\u2032. Write T for the set of states that are transient for \u03c0 (formally, T = X\\R(\u03c0)). For any subset Y of the state space X , write PY\u03c0 for the stochastic matrix of which the i\nth row is equal to that of P\u03c0 if i \u2208 Y , and is 0 otherwise, and write 1Y the vectors of which the ith component is equal to 1 if i \u2208 Y and 0 otherwise.\nUsing the fact that PR0\u03c0 P T \u03c0 = 0, one can first observe that\n(I \u2212 \u03b3PR0\u03c0 )(I \u2212 \u03b3P T \u03c0 ) = I \u2212 \u03b3(P R0 \u03c0 + P T \u03c0 ),\nfrom which we can deduce that\n\u2200s \u2208 R0, [ 1T \u222aR0 T (I \u2212 \u03b3P\u03c0) \u22121 ] (s) = [ 1T \u222aR0 T (I \u2212 \u03b3(PR0\u03c0 + P T \u03c0 )) \u22121 ] (s)\n= [ 1T \u222aR0 T (I \u2212 \u03b3P T\u03c0 ) \u22121(I \u2212 \u03b3PR0\u03c0 ) \u22121 ] (s). (16)\nAlso, let s be an arbitrary state and s\u2032 be a state of R0. Since 1 T s (P T \u03c0 ) k(s\u2032) is the probability that the chain starting in s reaches s\u2032 for the first time after k iterations, then\n1 T s (I \u2212 \u03b3P T \u03c0 )\n\u22121(s\u2032) \u2264 \u221e \u2211\nt=0\n1 T s (P T \u03c0 ) i(s\u2032) \u2264 1.\nand therefore,\n\u2200s\u2032 \u2208 R0, 1T \u222aR0 T (I \u2212 \u03b3P T\u03c0 ) \u22121(s\u2032) \u2264 n. (17)\nWriting \u03b4 for the vector that equals \u2212a\u03c0\u03c0\u2020 on R0 and that is null everywhere else, we have\n\u2211\ns\u2208R0\nx\u03c0(s)(\u2212a \u03c0 \u03c0\u2020 (s))\n= \u2211\ns\u2208R0\n[(I \u2212 \u03b3PT\u03c0 ) \u22121 1](s)\u03b4(s)\n= \u2211\ns\u2208R0\n[(I \u2212 \u03b3PT\u03c0 ) \u22121 1T \u222aR0 ](s)\u03b4(s) { \u2200s \u2208 R0, [(I \u2212 \u03b3P T \u03c0 ) \u22121 1X\\(T \u222aR0)](s) = 0 }\n= \u2211\ns\n[(I \u2212 \u03b3PT\u03c0 ) \u22121 1T \u222aR0 ](s)\u03b4(s) {\u2200s 6\u2208 R0, \u03b4(s) = 0}\n= 1T \u222aR0 T (I \u2212 \u03b3P\u03c0) \u22121\u03b4 = 1T \u222aR0 T (I \u2212 \u03b3P T\u03c0 ) \u22121(I \u2212 \u03b3PR0\u03c0 ) \u22121\u03b4 {Equation (16)}\n= \u2211\ns\n[(I \u2212 \u03b3P T\u03c0 T )\u221211T \u222aR0 ](s)[(I \u2212 \u03b3P R0 \u03c0 ) \u22121\u03b4](s)\n= \u2211\ns\u2208R0\n[(I \u2212 \u03b3P T\u03c0 T )\u221211T \u222aR0 ](s)[(I \u2212 \u03b3P R0 \u03c0 ) \u22121\u03b4](s) {\u2200s 6\u2208 R0, \u03b4(s) = 0}\n= \u2211\ns\u2208R0\n[(I \u2212 \u03b3P T\u03c0 T )\u221211T \u222aR0 ](s)(v\u03c0\u2020(s)\u2212 v\u03c0(s)) {Lemma 10}\n\u2264 n1R0 T (v\u03c0\u2020 \u2212 v\u03c0). {Equation (17)}\n(18)\nWe assumed that R0 is also a recurrent class of \u03c0 \u2032, which implies 1R0 T v\u03c0 = 1R0 T v\u03c0\u2032 , and\n1 T (v\u03c0\u2020 \u2212 v\u03c0\u2032) \u2265 1R0 T (v\u03c0\u2020 \u2212 v\u03c0\u2032) {v\u03c0\u2020 \u2265 v\u03c0\u2032}\n= 1R0 T (v\u03c0\u2020 \u2212 v\u03c0) {1R0 T v\u03c0 = 1R0 T v\u03c0\u2032} \u2265 1\nn\n\u2211\ns\u2208R0\nx\u03c0(s)(\u2212a \u03c0 \u03c0\u2020 (s)) {Equation (18)}\n\u2265 1\nn2 x\u03c0\nT (\u2212a\u03c0\u03c0\u2020) {Equation (15)}\n= 1\nn2 1 T (v\u03c0\u2020 \u2212 v\u03c0). {Equation (11)}\nIf there is no recurrent class creation, the contraction property given in Lemma 12 implies that if \u03c0\u2032 is obtained after k = \u2308n2\u03c4t log(n2)\u2309 > log(n2)\nlog 1 1\u2212 1\nn2\u03c4t\niterations, then\n1 T (v\u03c0\u2020 \u2212 v\u03c0\u2032) <\n1\nn2 1 T (v\u03c0\u2020 \u2212 v\u03c0),\nand thus we get a contradiction. As a conclusion, R0 cannot be a recurrent class of \u03c0 \u2032.\nA direct consequence of the above result is Lemma 6 that we originally stated on page 5, and that we restate for clarity.\nLemma 6. After at most (m\u2212n)\u2308n2\u03c4t log(n2\u03c4t)\u2309+n\u2308n2\u03c4t log(n2)\u2309 iterations, either Simplex-PI finishes or a new recurrent class appears.\nProof. Before a recurrent class is created, at most n recurrent classes need to be broken and (m \u2212 n) actions to be eliminated, and the time required by these events is bounded thanks to the previous lemma."}, {"heading": "9.2 Part 2: A new recurrent class implies a significant step towards the optimal value", "text": "We now proceed to the second part of the proof, and begin by proving Lemma 7 (originally stated page 6).\nLemma 7. When Simplex-PI moves from \u03c0 to \u03c0\u2032 where \u03c0\u2032 involves a new recurrent class, we have\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) \u2264\n(\n1\u2212 1\nn\u03c4r\n)\n1 T (v\u03c0\u2217 \u2212 v\u03c0).\nProof. Let s0 be the state such that \u03c0 \u2032(s0) 6= \u03c0(s0). On the one hand, since \u03c0 \u2032 contains a new recurrent class R (necessarily containing s0), we have\n1 T (v\u03c0\u2032 \u2212 v\u03c0) = x\u03c0\u2032 Ta\u03c0 \u2032 \u03c0 {Equation (11)}\n= x\u03c0\u2032(s0)a\u03c0(s0) {Simplex-PI switches 1 action and a\u03c0(s0) = a\u03c0 \u2032 \u03c0 (s0)} \u2265 1\n(1\u2212 \u03b3)\u03c4r a\u03c0(s0). {Equation (9) with s0 \u2208 R(\u03c0\n\u2032)} (19)\nOn the other hand,\n\u2200s, v\u03c0\u2217(s)\u2212 v\u03c0(s) = [(I \u2212 \u03b3P\u03c0\u2217) \u22121a\u03c0\u2217\u03c0 ](s) {Lemma 10}\n\u2264 1\n1\u2212 \u03b3 a\u03c0(s0). {\u2016(I \u2212 \u03b3P\u03c0\u2217)\n\u22121\u2016\u221e \u2264 1\n1\u2212 \u03b3 and a\u03c0(s0) = max s,\u03c0\u0303 a\u03c0\u0303\u03c0(s) \u2265 0}\n(20)\nCombining these two observations, we obtain\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) = 1 T (v\u03c0\u2217 \u2212 v\u03c0)\u2212 1 T (v\u03c0\u2032 \u2212 v\u03c0)\n\u2264 1T (v\u03c0\u2217 \u2212 v\u03c0)\u2212 1\n(1 \u2212 \u03b3)\u03c4r a\u03c0(s0) {Equation (19)}\n\u2264 1T (v\u03c0\u2217 \u2212 v\u03c0)\u2212 1\n\u03c4r max s v\u03c0\u2217(s)\u2212 v\u03c0\u2032(s) {Equation (20)}\n\u2264\n(\n1\u2212 1\nn\u03c4r\n)\n1 T (v\u03c0\u2217 \u2212 v\u03c0). {\u2200x,\n1 n 1 Tx \u2264 max s x(s)}\nLemma 14. While Simplex-PI does not terminate,\n\u2022 either some non-optimal action is eliminated from recurrent states after at most \u2308n\u03c4r log(n2\u03c4r)\u2309 recurrent class creations,\n\u2022 or some non-optimal action is eliminated from policies after at most \u2308n\u03c4r log(n2\u03c4t)\u2309 recurrent class creations.\nProof. Let \u03c0 be the policy in some iteration and \u03c0\u2032 the policy generated after k iterations from \u03c0 (without loss of generality we assume \u03c0\u2032 6= \u03c0\u2217). Let s0 = argmaxs x\u03c0(s)(\u2212a\u03c0\u03c0\u2217(s)). We have\nx\u03c0(s0)(\u2212a \u03c0 \u03c0\u2217 (s0)) \u2265\n1 n x\u03c0 T (\u2212a\u03c0\u03c0\u2217) {\u2200x, 1 Tx \u2264 nmax s x(s)}\n= 1\nn 1 T (v\u03c0\u2217 \u2212 v\u03c0). {Equation (11)} (21)\nWe now consider two cases, respectively corresponding to s0 6\u2208 R(\u03c0) or s0 \u2208 R(\u03c0).\n\u2022 case 1: s0 6\u2208 R(\u03c0). Let us prove by contradiction that \u03c0\u2032(s0) 6= \u03c0(s0) if k is sufficiently large: let us assume that \u03c0\u2032(s0) = \u03c0(s0). Then, by using repeatedly the fact that for all \u03c0\u0303, a \u03c0\u0303 \u03c0\u2217\n\u2264 0 (by definition of the optimal policy \u03c0\u2217), we have:\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) = x\u03c0\u2032\nT (\u2212a\u03c0 \u2032\n\u03c0\u2217 ) {Equation (11)}\n\u2265 x\u03c0\u2032(s0)(\u2212a \u03c0\u2032\n\u03c0\u2217 (s0))\n\u2265 \u2212a\u03c0 \u2032\n\u03c0\u2217 (s0) {x\u03c0\u2032(s0) \u2265 1}\n= \u2212a\u03c0\u03c0\u2217(s0) {\u03c0(s0) = \u03c0 \u2032(s0)} \u2265 1\nn\u03c4t x\u03c0(s0)(\u2212a\n\u03c0 \u03c0\u2217 (s0)) {Equation (8)}\n\u2265 1\nn2\u03c4t 1 T (v\u03c0\u2217 \u2212 v\u03c0). {Equation (21)}\nAfter k = \u2308n\u03c4r logn2\u03c4t\u2309 > logn2\u03c4t\nlog 1 1\u2212 1\nn\u03c4r\nrecurrent classes are created, we have by the contraction\nproperty of Lemma 7 that\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) <\n1\nn2\u03c4t 1 T (v\u03c0\u2217 \u2212 v\u03c0)\nand we get a contradiction. As a conclusion, we have \u03c0\u2032(s0) 6= \u03c0(s0).\n\u2022 case 2: s0 \u2208 R(\u03c0). Let us prove by contradiction that \u03c0\u2032(s0) 6= \u03c0(s0) if s0 is recurrent for \u03c0\u2032 and k is sufficiently large: let us assume that \u03c0\u2032(s0) = \u03c0(s0) and s0 \u2208 R(\u03c0\n\u2032). Then, by using again the fact that for all \u03c0\u0303, a\u03c0\u0303\u03c0\u2217 \u2264 0, we have:\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) = x\u03c0\u2032\nT (\u2212a\u03c0 \u2032\n\u03c0\u2217 ) {Equation (11)}\n= \u2211\ns\nx\u03c0\u2032(s)(\u2212a \u03c0\u2032\n\u03c0\u2217 (s))\n\u2265 \u2211\ns\u2208R0\nx\u03c0\u2032(s)(\u2212a \u03c0\u2032\n\u03c0\u2217 (s))\n\u2265 1\n(1 \u2212 \u03b3)\u03c4r\n\u2211\ns\u2208R0\n(\u2212a\u03c0 \u2032\n\u03c0\u2217 (s)) {Equation (9)}\n\u2265 1\n(1 \u2212 \u03b3)\u03c4r (\u2212a\u03c0\n\u2032\n\u03c0\u2217 (s0))\n= 1\n(1 \u2212 \u03b3)\u03c4r (\u2212a\u03c0\u03c0\u2217(s0)) {\u03c0(s0) = \u03c0 \u2032(s0)}\n\u2265 1\nn\u03c4r x\u03c0(s0)(\u2212a\n\u03c0 \u03c0\u2217 (s0)) {x\u03c0(s0) \u2264\nn\n1\u2212 \u03b3 }\n\u2265 1\nn2\u03c4r 1 T (v\u03c0\u2217 \u2212 v\u03c0). {Equation (21)}\nAfter k = \u2308n\u03c4r logn2\u03c4r\u2309 > logn2\u03c4r\nlog 1 1\u2212 1\nn\u03c4r\nnew recurrent classes are created, we have by the contraction\nproperty of Lemma 7 that\n1 T (v\u03c0\u2217 \u2212 v\u03c0\u2032) <\n1\nn2\u03c4r 1 T (v\u03c0\u2217 \u2212 v\u03c0),\nand we get a contradiction. As a conclusion, we know that \u03c0\u2032(s0) 6= \u03c0(s0) if s0 is recurrent for \u03c0\u2032.\nWe are ready to conclude: At most, the (m\u2212n) non-optimal actions may need to be eliminated from all states; in addition, all actions may need to be eliminated from recurrent states (some optimal actions may only be used at transient states and thus also need to be eliminated from recurrent states). Overall, convergence can thus be obtained after at most a total of m\u2308n\u03c4r log(n2\u03c4r)\u2309 + (m \u2212 n)\u2308n\u03c4r log(n2\u03c4t)\u2309 recurrent class creations. The result follows from the fact that each class creation requires at most (m\u2212 n)\u2308n2\u03c4t log(n2\u03c4t)\u2309+ n\u2308n2\u03c4t log(n2)\u2309 iterations (cf. Lemma 6)."}, {"heading": "10 Cycle and recurrent classes creations for Howard\u2019s PI (Proofs", "text": "of Lemmas 8 and 9)\nLemma 8. If the MDP is deterministic, after at most n iterations, either Howard\u2019s PI finishes or a new cycle appears.\nProof. Consider a sequence of l generated policies \u03c01, \u00b7 \u00b7 \u00b7 , \u03c0l from an initial policy \u03c00 such that no new cycle appears. By induction, we have\nv\u03c0l \u2212 v\u03c0k = T\u03c0lv\u03c0l \u2212 T\u03c0lv\u03c0k\u22121 + T\u03c0lv\u03c0k\u22121 \u2212 T\u03c0kv\u03c0k\u22121 + T\u03c0kv\u03c0k\u22121 \u2212 T\u03c0kv\u03c0k {\u2200\u03c0, T\u03c0v\u03c0 = v\u03c0}\n\u2264 \u03b3P\u03c0l(v\u03c0l \u2212 v\u03c0k\u22121) + \u03b3P\u03c0k(v\u03c0k\u22121 \u2212 v\u03c0k) {T\u03c0lv\u03c0k\u22121 \u2264 T\u03c0kv\u03c0k\u22121}\n\u2264 \u03b3P\u03c0l(v\u03c0l \u2212 v\u03c0k\u22121) {Lemma 1 and P\u03c0k \u2265 0}\n\u2264 (\u03b3P\u03c0l) k(v\u03c0l \u2212 v\u03c00). {By induction on k}\n(22)\nSince the MDP is deterministic and has n states, (P\u03c0l) n will only have non-zero values on columns that correspond to R(\u03c0l). Furthermore, since no cycle is created, R(\u03c0l) \u2282 R(\u03c00), which implies that v\u03c0l(s)\u2212v\u03c00(s) = 0 for all s \u2208 R(\u03c0l). As a consequence, we have (P\u03c0l)\nn(v\u03c0l \u2212v\u03c00) = 0. By Equation (22), this implies that v\u03c0l = v\u03c0n . If l > n, then Howard\u2019s PI must have terminated.\nLemma 9. After at most (m\u2212n)\u2308n2\u03c4t log(n2\u03c4t)\u2309+n\u2308n2\u03c4t log(n2)\u2309 iterations, either Howard\u2019s PI finishes or a new recurrent class appears.\nProof. A close examination of the proof of Lemma 6, originally designed for Simplex-PI, shows that it applies to Howard\u2019s PI without any modification."}, {"heading": "11 A bound for Howard\u2019s PI and Simplex-PI under Assump-", "text": "tion 1 (Proof of Theorem 7)\nWe here consider that the state space is decomposed into 2 sets: T is the set of states that are transient under all policies, and R is the set of states that are recurrent under all policies. From this assumption, it can be seen that when running Howard\u2019s PI or Simplex-PI, the values and actions chosen on T have no influence on the evolution of the values and policies on R. So we will study the convergence of both algorithms in two steps: we will first bound the number of iterations to converge on R; we will then add the number of iterations for converging on T given that convergence has occurred on R.\nConvergence on the set R of recurrent states: Without loss of generality, we consider here that the state space is only made of the set of recurrent states.\nFirst consider Simplex-PI. If all states are recurrent, new recurrent classes are created at every iteration, and Lemma 7 holds. Then, in a way similar to the proof of Lemma 14, it can be shown that every \u2308n\u03c4r logn\n2\u03c4r\u2309 iterations, a non-optimal action can be eliminated. As there are at most (m \u2212 n) non-optimal actions, we deduce that Simplex-PI converges in at most (m \u2212 n)\u2308n\u03c4r logn2\u03c4r\u2309 iterations on R.\nConsider now Howard\u2019s PI. We can prove the following lemma.\nLemma 10. If the MDP satisfies Assumption 1 and all states are recurrent under all policies, Howard\u2019s PI generates policies (\u03c0k)k\u22650 that satisfy:\n1 T (v\u03c0\u2217 \u2212 v\u03c0k+1) \u2264\n(\n1\u2212 1\nn\u03c4r\n)\n1 T (v\u03c0\u2217 \u2212 v\u03c0k).\nProof. On the one hand, we have\n1 T (v\u03c0k+1 \u2212 v\u03c0k) = x\u03c0k+1 Ta\u03c0k+1\u03c0k {Equation (11)}\n= x\u03c0k+1 Ta\u03c0k {a \u03c0k+1 \u03c0k = a\u03c0k} \u2265 1\n(1 \u2212 \u03b3)\u03c4r 1 T a\u03c0k {Equation (9) and all states are recurrent}\n\u2265 1\n(1 \u2212 \u03b3)\u03c4r \u2016a\u03c0k\u2016\u221e. {\u2200x \u2265 0,1\nTx \u2265 \u2016x\u2016\u221e} (23)\nOn the other hand,\n1 T (v\u03c0\u2217 \u2212 v\u03c0k) = x\u03c0\u2217 T a\u03c0\u2217\u03c0k {Equation (11)}\n\u2264 x\u03c0\u2217 T a\u03c0k {a\u03c0k \u2265 a \u03c0\u2217 \u03c0k } \u2264 n\n1\u2212 \u03b3 \u2016a\u03c0k\u2016\u221e. {\n\u2211\ni\nx\u03c0\u2217(i) \u2264 n\n1\u2212 \u03b3 and a\u03c0k \u2265 0} (24)\nBy combining Equations (23) and (24), we obtain:\n1 T (v\u03c0\u2217 \u2212 v\u03c0k+1) = 1 T (v\u03c0\u2217 \u2212 v\u03c0k)\u2212 1 T (v\u03c0k+1 \u2212 v\u03c0k)\n\u2264\n(\n1\u2212 1\nn\u03c4r\n)\n1 T (v\u03c0\u2217 \u2212 v\u03c0k).\nThen, similarly to Simplex-PI, we can prove that after every \u2308n\u03c4r logn2\u03c4r\u2309 iterations a non-optimal action must be eliminated. And as there are at most (m \u2212 n) non-optimal actions, we deduce that Howard\u2019s PI converges in at most (m\u2212 n)\u2308n\u03c4r logn 2\u03c4r\u2309 iterations on R.\nConvergence on the set T of transient states: Consider now that convergence has occurred on the recurrent states R. A simple variation of the proof of Lemma 6/Lemma 9 (where we use the fact that we don\u2019t need to consider the events where recurrent classes are broken since recurrent classes do not evolve anymore) allows us to show that the extra number of iterations for both algorithms to converge on the transient states is at most (m\u2212 n))\u2308n2\u03c4t logn2\u03c4t\u2309, and the result follows.\nAcknowledgements.\nI would like to thank Ian Post for exchanges about the proof in Post and Ye (2013), Thomas Dueholm Hansen for noticing a flaw in a claimed result for deterministic MDPs in an earlier version, Romain Aza\u0308\u0131s for the reference on the Cesaro mean of stochastic matrices, and the reviewers and editor for their very careful feedback, who helped improve the paper overall, and the proof of Lemma 13 in particular."}], "references": [{"title": "Policy iteration for perfect information stochastic mean payoff games", "author": ["M. Akian", "S. Gaubert"], "venue": null, "citeRegEx": "Akian and Gaubert,? \\Q2013\\E", "shortCiteRegEx": "Akian and Gaubert", "year": 2013}, {"title": "Neurodynamic Programming", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific. Fearnley, J", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Worst-case Analysis of Strategy Iteration and the Simplex Method", "author": ["Berlin", "B. Huppert", "W. Willems"], "venue": "Ph.D. thesis,", "citeRegEx": "Berlin et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Berlin et al\\.", "year": 1979}, {"title": "turn-based stochastic games with a constant discount factor", "author": ["R. Hollanders", "J. Delvenne", "R. Jungers"], "venue": "J. ACM ,", "citeRegEx": "Hollanders et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hollanders et al\\.", "year": 2012}, {"title": "Improved bound on the worst case", "author": ["R. Hollanders", "B. Gerencs\u00e9r", "J. Delvenne", "R. Jungers"], "venue": "IEEE conference on Decision and control", "citeRegEx": "Hollanders et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hollanders et al\\.", "year": 2014}, {"title": "On the complexity of policy iteration", "author": ["Y. Mansour", "S. Singh"], "venue": "In UAI ,", "citeRegEx": "Mansour and Singh,? \\Q1999\\E", "shortCiteRegEx": "Mansour and Singh", "year": 1999}, {"title": "The simplex method is strongly polynomial for deterministic Markov decision", "author": ["Y. Ye"], "venue": "Markov decision processes. INFORMS Journal on Computing,", "citeRegEx": "I. and Ye,? \\Q2013\\E", "shortCiteRegEx": "I. and Ye", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "The tuple \u3008X, (Ai)i\u2208X , p, r, \u03b3\u3009 is called a Markov Decision Process (MDP) (Puterman, 1994; Bertsekas and Tsitsiklis, 1996), and the associated problem is known as stochastic optimal control.", "startOffset": 75, "endOffset": 123}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting.", "startOffset": 306, "endOffset": 331}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting. For Simplex-PI, however, the line of analysis is slightly trickier: it amounts to bound the improvement in value at individual states and requires a bit of bookkeeping; the technique we use is to our knowledge original. The bound for Simplex-PI is a factor O(n) larger than that for Howard\u2019s PI. However, since one changes only one action per iteration, each iteration has a complexity that is in a worst-case sense lower by a factor n: the update of the value can be done in time O(n) through the Sherman-Morrisson formula, though in general each iteration of Howard\u2019s PI, which amounts to compute the value of some policy that may be arbitrarily different from the previous policy, may require O(n) time. Thus, it is remarkable that both algorithms seem to have a similar complexity. The linear dependency of the bound for Howard\u2019s PI with respect to m is optimal (Hansen, 2012, Chapter 6.4). The linear dependency with respect to n or m (separately) is easy to prove for Simplex-PI; we conjecture that Simplex-PI\u2019s complexity is proportional to nm, and thus that our bound is tight for a fixed discount factor. The dependency with respect to the term 1 1\u2212\u03b3 may be improved, but removing it is impossible for Howard\u2019s PI and very unlikely for Simplex-PI. Fearnley (2010) describes an MDP for which Howard\u2019s PI requires an exponential (in n) number of iterations for \u03b3 = 1 and Hollanders et al.", "startOffset": 306, "endOffset": 1640}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting. For Simplex-PI, however, the line of analysis is slightly trickier: it amounts to bound the improvement in value at individual states and requires a bit of bookkeeping; the technique we use is to our knowledge original. The bound for Simplex-PI is a factor O(n) larger than that for Howard\u2019s PI. However, since one changes only one action per iteration, each iteration has a complexity that is in a worst-case sense lower by a factor n: the update of the value can be done in time O(n) through the Sherman-Morrisson formula, though in general each iteration of Howard\u2019s PI, which amounts to compute the value of some policy that may be arbitrarily different from the previous policy, may require O(n) time. Thus, it is remarkable that both algorithms seem to have a similar complexity. The linear dependency of the bound for Howard\u2019s PI with respect to m is optimal (Hansen, 2012, Chapter 6.4). The linear dependency with respect to n or m (separately) is easy to prove for Simplex-PI; we conjecture that Simplex-PI\u2019s complexity is proportional to nm, and thus that our bound is tight for a fixed discount factor. The dependency with respect to the term 1 1\u2212\u03b3 may be improved, but removing it is impossible for Howard\u2019s PI and very unlikely for Simplex-PI. Fearnley (2010) describes an MDP for which Howard\u2019s PI requires an exponential (in n) number of iterations for \u03b3 = 1 and Hollanders et al. (2012) argued that this holds also when \u03b3 is in the vicinity of 1.", "startOffset": 306, "endOffset": 1770}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting. For Simplex-PI, however, the line of analysis is slightly trickier: it amounts to bound the improvement in value at individual states and requires a bit of bookkeeping; the technique we use is to our knowledge original. The bound for Simplex-PI is a factor O(n) larger than that for Howard\u2019s PI. However, since one changes only one action per iteration, each iteration has a complexity that is in a worst-case sense lower by a factor n: the update of the value can be done in time O(n) through the Sherman-Morrisson formula, though in general each iteration of Howard\u2019s PI, which amounts to compute the value of some policy that may be arbitrarily different from the previous policy, may require O(n) time. Thus, it is remarkable that both algorithms seem to have a similar complexity. The linear dependency of the bound for Howard\u2019s PI with respect to m is optimal (Hansen, 2012, Chapter 6.4). The linear dependency with respect to n or m (separately) is easy to prove for Simplex-PI; we conjecture that Simplex-PI\u2019s complexity is proportional to nm, and thus that our bound is tight for a fixed discount factor. The dependency with respect to the term 1 1\u2212\u03b3 may be improved, but removing it is impossible for Howard\u2019s PI and very unlikely for Simplex-PI. Fearnley (2010) describes an MDP for which Howard\u2019s PI requires an exponential (in n) number of iterations for \u03b3 = 1 and Hollanders et al. (2012) argued that this holds also when \u03b3 is in the vicinity of 1. Though a similar result does not seem to exist for Simplex-PI in the literature, Melekopoglou and Condon (1994) consider four variations of PI that all switch one action per iteration, and show through specifically designed MDPs that they may require an exponential (in n) number of iterations when \u03b3 = 1.", "startOffset": 306, "endOffset": 1942}, {"referenceID": 3, "context": "In the simplest\u2014 deterministic\u2014case, the complexity is still an open problem: the currently best-known lower bound is O(n) (Hansen and Zwick, 2010), while the best known upper bound is O( n n ) Mansour and Singh (1999); Hollanders et al.", "startOffset": 194, "endOffset": 219}, {"referenceID": 3, "context": "In the simplest\u2014 deterministic\u2014case, the complexity is still an open problem: the currently best-known lower bound is O(n) (Hansen and Zwick, 2010), while the best known upper bound is O( n n ) Mansour and Singh (1999); Hollanders et al. (2014). On the positive side, an adaptation of the line of proof we have considered so far can be carried out under the following assumption.", "startOffset": 220, "endOffset": 245}], "year": 2016, "abstractText": "Given a Markov Decision Process (MDP) with n states and a total number m of actions, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal \u03b3-discounted policy. We consider two variations of PI: Howard\u2019s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard\u2019s PI terminates after at most O ( m 1\u2212\u03b3 log ( 1 1\u2212\u03b3 )) iterations, improving by a factor O(log n) a result by Hansen et al (2013), while Simplex-PI terminates after at most O ( nm 1\u2212\u03b3 log ( 1 1\u2212\u03b3 )) iterations, improving by a factor O(log n) a result by Ye (2011). Under some structural properties of the MDP, we then consider bounds that are independent of the discount factor \u03b3: quantities of interest are bounds \u03c4t and \u03c4r\u2014uniform on all states and policies\u2014respectively on the expected time spent in transient states and the inverse of the frequency of visits in recurrent states given that the process starts from the uniform distribution. Indeed, we show that Simplex-PI terminates after at most \u00d5 ( nm\u03c4t\u03c4r ) iterations. This extends a recent result for deterministic MDPs by Post & Ye (2013), in which \u03c4t \u2264 1 and \u03c4r \u2264 n; in particular it shows that Simplex-PI is strongly polynomial for a much larger class of MDPs. We explain why similar results seem hard to derive for Howard\u2019s PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that both Howard\u2019s PI and Simplex-PI terminate after at most \u00d5(m(n\u03c4t + n\u03c4r)) iterations.", "creator": "LaTeX with hyperref package"}}}