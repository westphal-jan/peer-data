{"id": "1306.0751", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2013", "title": "First-order Decomposition Trees", "abstract": "Lifting attempts to speed up probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the first-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree.\n\n\n\n\nThe results were presented in a three-part series entitled (3): Analysis of Probabilistic Equations for Positives of Non-Positive Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic Probabilistic", "histories": [["v1", "Tue, 4 Jun 2013 12:43:07 GMT  (1195kb,D)", "http://arxiv.org/abs/1306.0751v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nima taghipour", "jesse davis", "hendrik blockeel"], "accepted": true, "id": "1306.0751"}, "pdf": {"name": "1306.0751.pdf", "metadata": {"source": "CRF", "title": "First-Order Decomposition Trees", "authors": ["Nima Taghipour", "Jesse Davis", "Hendrik Blockeel"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Probabilistic logical modes (PLMs) combine elements of first-order logic with graphical models to succinctly model complex, uncertain, structured domains [5]. These domains often involve a large number of objects, making efficient inference a challenge. To address this, Poole [12] introduced the concept of lifted probabilistic inference, i.e., inference that exploits the symmetries in the model to improve efficiency. Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22]. While the relation between the propositional algorithms is well studied, we have far less insight into their lifted counterparts.\nThe performance of propositional inference, such as variable elimination [4, 14] or recursive conditioning [2], is characterized in terms of a corresponding tree decomposition of the model, and their complexity is measured based on properties of the decomposition, mainly its width. It is known that standard (propositional) inference has complexity exponential in the treewidth [2, 4]. This allows us to measure the complexity of various inference algorithms only based on the structure of the model and its given decomposition. Such analysis is typically done using a secondary structure for representing the decomposition of graphical models, such as decomposition trees (dtrees) [2].\nHowever, the existing notion of treewidth does not provide a tight upper bound for the complexity of lifted inference, since it ignores the opportunities that lifting exploits to improve efficiency. Currently, there exists no notion analogous to treewidth for lifted inference to analyze inference complexity based on the model structure. In this paper, we take a step towards filling these gaps.\nOur work centers around a new structure for specifying and analyzing a lifted solution to an inference problem, and makes the following contributions. First, building on the existing structure of dtrees for propositional graphical models, we propose the structure of First-Order dtrees (FO-dtrees) for PLMs. An FO-dtree represents both the decomposition of a PLM and the symmetries that lifting exploits for performing inference. Second, we show how to determine whether an FO-dtree has a lifted solution, from its structure alone. Third, we present a method to read a lifted solution (a sequence of lifted inference operations) from a liftable FO-dtree, just like we can read a propositional inference solution from a dtree. Fourth, we show how the structure of an FO-dtree determines the complexity\nar X\niv :1\n30 6.\n07 51\nv1 [\ncs .A\nI] 4\nJ un\n2 01\n3\nof inference using its corresponding solution. We formally analyze the complexity of lifted inference in terms of the novel, symmetry-aware notion of lifted width for FO-dtrees. As such, FO-dtrees serve as the first formal tool for finding, evaluating, and choosing among lifted solutions.1"}, {"heading": "2 Background", "text": "We use the term \u201cvariable\u201d in both the logical and probabilistic sense. We use logvar for logical variables and randvar for random variables. We write variables in uppercase and their values in lowercase. Applying a substitution \u03b8 = {s1 \u2192 t1, . . . , sn \u2192 tn} to a structure S means replacing each occurrence of si in S by the corresponding ti. The result is written S\u03b8."}, {"heading": "2.1 Propositional and first-order graphical models", "text": "Probabilistic graphical models such as Bayesian networks, Markov networks and factor graphs compactly represent a joint distribution over a set of randvars V = {V1, . . . , Vn} by factorizing the distribution into a set of local distribution. For example, factor graphs represent the distribution as a product of factors: Pr(V1, . . . , Vn) = 1Z \u220f \u03c6i(Vi), where \u03c6i is a potential function that maps each configuration of Vi \u2286 V to a real number and Z is a normalization constant. Probabilistic logical models use concepts from first-order logic to provide a high-level modeling language for representing propositional graphical models. While many such languages exist (see [5] for an overview), we focus on parametric factors (parfactors) [12] that generalize factor graphs.\nParfactors use parametrized randvars (PRVs) to represent entire sets of randvars. For example, the PRV BloodType(X), where X is a logvar, represents one BloodType randvar for each object in the domain of X (written D(X)). Formally, a PRV is of the form P (X)|C where C is a constraint consisting of a conjunction of inequalities Xi 6= t where t \u2208 D(Xi) or t \u2208 X. It represents the set of all randvars P (x) where x \u2208 D(X) and x satisfies C; this set is denoted rv(P (X)|C). A parfactor uses PRVs to compactly encode a set of factors. For example, the parfactor \u03c6(Smoke(X),Friends(X,Y ),Smoke(Y )) could encode that friends have similar smoking habits. It imposes a symmetry in the model by stating that the probability that, among two friends, both, one or none smoke, is the same for all pairs of friends, in the absence of any other information.\nFormally, a parfactor is of the form \u03c6(A)|C, where A = (Ai)ni=1 is a sequence of PRVs, C is a constraint on the logvars appearing inA, and \u03c6 is a potential function. The set of logvars occurring in A is denoted logvar(A). A grounding substitution maps each logvar to an object from its domain. A parfactor g represents the set of all factors that can be obtained by applying a grounding substitution to g that is consistent with C; this set is called the grounding of g, and is denoted gr(g). A parfactor model is a set G of parfactors. It compactly defines a factor graph gr(G) = {gr(g)|g \u2208 G}. Following the literature, we assume that the model is in a normal form, such that (i) each pair of logvars have either identical or disjoint domains, and (ii) for each pair of co-domain logvars X , X \u2032 in a parfactor \u03c6(A)|C, (X 6= X \u2032) \u2208 C. Every model can be written into this form in poly time [13]."}, {"heading": "2.2 Inference", "text": "A typical inference task is to compute the marginal probability of some variables by summing out the remaining variables, which can be written as: Pr(V \u2032) = \u2211V\\V\u2032 \u220f i \u03c6i(Vi). This is an instance of the\ngeneral sum-product problem [1]. Abusing notation, we write this sum of products as \u2211 V\\V\u2032 M(V).\nInference by recursive decomposition. Inference algorithms exploit the factorization of the model to recursively decompose the original problem into smaller, independent subproblems. This is achieved by a decomposition of the sum-product, according to a simple decomposition rule.\nDefinition 1 (The decomposition rule) Let P be a sum-product computation P : \u2211VM(V), and let M = {M1(V1), . . .Mk(Vk)} be a partitioning (decomposition) of M(V). Then, the decomposi-\n1Similarly to existing studies on propositional inference [2, 4], our analysis only considers the model\u2019s global structure, and makes no assumptions about its local structure.\ntion of P , w.r.t. M is an equivalent sum-product formula PM, defined as follows:\nPM : \u2211\nV\u2032\n[ (\u2211\nV\u20321\nM1(V1) ) . . . (\u2211\nV\u2032k\nMk(Vk) ) ]\nwhere V \u2032 = \u22c3i,j Vi \u2229 Vj , and V \u2032i = Vi \\ V \u2032. Most exact inference algorithms recursively apply this rule and compute the final result using topdown or bottom-up dynamic programming [1, 2, 4]. The complexity is then exponential only in the size of the largest sub-problem solved. Variable elimination (VE) is a bottom-up algorithm that computes the nested sum-product by repeatedly solving an innermost problem \u2211 V M(V,V \u2032) to eliminate V from the model. At each step, VE eliminates a randvar V from the model by multiplying the factors in M(V,V \u2032) into one and summing-out V from the resulting factor. Decomposition trees. A single inference problem typically has multiple solutions, each with a different complexity. A decomposition tree (dtree) is a structure that represents the decomposition used by a specific solution and allows us to determine its complexity [2]. Formally, a dtree is a rooted tree in which each leaf represents a factor in the model.2 Each node in the tree represents a decomposition of the model into the models under its child subtrees. Properties of the nodes can be used to determine the complexity of inference. Child(T ) refers to T \u2019s child nodes; rv(T ) refers to the randvars under T , which are those in its factor if T is a leaf and rv(T ) = \u222aT \u2032\u2208Child(T )rv(T \u2032) otherwise. Using these, the important properties of cutset, context, and cluster are defined as follows:\n\u2022 cutset(T ) = \u222a{T1,T2}\u2208child(T )rv(T1) \u2229 rv(T2) \\ acutset(T ), where acutset(T ) is the union of cutsets associated with ancestors of T .\n\u2022 context(T ) = rv(T ) \u2229 acutset(T ) \u2022 cluster(T ) = rv(T ), if T is a leaf; otherwise cluster(T ) = cutset(T ) \u222a context(T )\nFigure 1 shows a factor graph model, a dtree for it with its clusters, and the corresponding sumproduct factorization. Intuitively, the properties of dtree nodes help us analyze the size of subproblems solved during inference. In short, the time complexity of inference is O(n exp(w)) where n is the size (number of nodes) of the tree and w is its width, i.e., its maximal cluster size minus one."}, {"heading": "3 Lifted inference: Exploiting symmetries", "text": "The inference approach of Section 2.2 ignores the symmetries imposed by a PLM. Lifted inference aims at exploiting symmetries among a model\u2019s isomorphic parts. Two constructs are isomorphic if there is a structure preserving bijection between their components. As PLMs make assertions about whole groups of objects, they contain many isomorphisms, established by a bijection at the level of objects. Building on this, symmetries arise between constructs at different levels [11], such as between: randvars, value assignments to randvars, factors, models, or even sum-product problems. All exact lifted inference methods use two main tools for exploiting symmetries, i.e., for lifting:\n1. Divide the problem into isomorphic subproblems, solve one instance, and aggregate 2. Count the number of isomorphic configurations for a group of interchangeable variables\ninstead of enumerating all possible configurations. 2We use a slightly modified definition for dtrees, which were originally defined as full binary rooted trees.\nBelow, we show how these tools are used by lifted variable elimination (LVE) [3, 10, 12, 17, 18].\nIsomorphic decomposition: exploiting symmetry among subproblems. The first lifting tool identifies cases where the application of the decomposition rule results in a product of isomorphic sum-product problems. Since such problems all have isomorphic answers, we can solve one problem and reuse its result for all the others. In LVE, this corresponds to lifted elimination, which uses the operations of lifted multiplication and lifted sum-out on parfactors to evaluate a single representative problem. Afterwards, LVE also attempts to aggregate the result (compute their product) by taking advantage of their isomorphism. For instance, when the results are identical, LVE computes their product simply by exponentiating the result of one problem.\nExample 1. Figure 2 shows the model defined by \u03c6(F (X,Y ), F (Y,X))|X 6= Y , with D(X) = D(Y ) = {a, b, c, d}. The model asserts that the friendship relationship (F ) is likely to be symmetric. To sum-out the randvars F using the decomposition rule, we partition the ground factors into six groups of the form {\u03c6(F (x, y), F (y, x)), \u03c6(F (y, x), F (x, y))}, i.e., one group for each 2-subset {x, y} \u2286 {a, b, c, d}. Since no randvars are shared between the groups, this decomposes the problem into the product of six isomorphic sums \u2211 F (x,y),F (y,x) \u03c6(F (x, y), F (y, x)) \u00b7 \u03c6(F (y, x), F (x, y)). All six sums have the same result c (a scalar). Thus, LVE computes c only once (lifted elimination) and computes the final result by exponentiation as c6 (lifted aggregation).\nCounting: exploiting interchangeability among randvars. Whereas isomorphic decomposition exploits symmetry among problems, counting exploits symmetries within a problem, by identifying interchangeable randvars. A group of (k-tuples of) randvars are interchangeable, if permuting the assignment of values to the group results in an equivalent model. Consider a sumproduct subproblem \u2211 VM(V,V \u2032) that contains a set of n interchangeable (k-tuples of) randvars V = {(Vi1, Vi2, . . . Vik)}ni=1. The interchangeability allows us to rewrite V into a single counting randvar #[V], whose value is the histogram h = {(v1, n1), . . . , (vr, nr)}, where ni is the number of tuples with joint state vi. This allows us to replace a sum over all possible joint states of V with a sum over the histograms for #[V]. That is, we compute M(V \u2032) = \u2211mi=1 MUL(hi) \u00d7M(hi,V \u2032), where MUL(hi) denotes the number of assignments to V that yield the same histogram hi for #[V]. Since the number of histograms is O(nexp(k)), when n k, we gain exponential savings over enumerating all the possible joint assignments, whose number is O(exp(nk)). This lifting tool is employed in LVE by counting conversion, which rewrites the model in terms of counting randvars.\nExample 2. Consider the model defined by the parfactor \u03c6(S(X), S(Y ))|X 6= Y , which is\u220f i 6=j \u03c6(S(xi), S(xj)). The group of randvars {S(x1), . . . , S(xn)} are interchangeable here, since under any value assignment where nt randvars are true and nf randvars are false, the model evaluates to the same value \u03c6\u2032(nt, nf ) = \u03c6(t, t)nt.(nt\u22121) \u00b7 \u03c6(t, f)nt.nf \u00b7 \u03c6(f, t)nf .nt \u00b7 \u03c6(f, f)nf .(nf\u22121). By counting conversion, LVE rewrites this model into \u03c6\u2032(#X [S(X)])."}, {"heading": "4 First-Order decomposition trees", "text": "In this section, we propose the structure of FO-dtrees, which compactly represent a recursive decomposition for a PLM and the symmetries therein."}, {"heading": "4.1 Structure", "text": "An FO-dtree provides a compact representation of a propositional dtree, just like a PLM is a compact representation of a propositional model. It does so by explicitly capturing isomorphic decomposition, which in a dtree correspond to a node with isomorphic children. Using a novel node type, called a decomposition into partial groundings (DPG) node, an FO-dtree represents the entire set of\n\u03c61(P (xn)) \u03c62(A, P (xn))\n. . . . . .\n\u03c61(P (x1)) \u03c62(A, P (x1))\nTX\nTx1 Txn\n\u2200x\n\u03c61(P (x)) \u03c62(A, P (x))\nTx\nTX\n\u21d4 \u03c6(F (x, y), F (y, x)) \u03c6(F (y, x), F (x, y))\n\u2200{x, y} y = x TXY\nTxy\n(a) (b)\nFigure 3: (a) dtree (left) and FO-dtree (right) of Example 3; (b) FO-dtree of Example 1\nisomorphic child subtrees with a single representative subtree. To formally introduce the structure, we first show how a PLM can be decomposed into isomorphic parts by DPG.\nDPG of a parfactor model. The DPG of a parfactor g is defined w.r.t. a k-subset X = {X1, . . . , Xk} of its logvars that all have the same domain DX. For example, the decomposition used in Example 1, and shown in Figure 2, is the DPG of \u03c6(F (X,Y ), F (Y,X))|X 6= Y w.r.t. logvars {X,Y }. Formally,DPG(g,X) partitions the model defined by g into (|DX| k ) parts: one partGx for each k-subset x = {x1, . . . , xk} of the objects in DX. Each Gx in turn contains all k! (partial) groundings of g that can result from replacing (X1, . . . , Xk) with a permutation of (x1, . . . , xk). The key intuition behind DPG is that for any x,x\u2032 \u2286k DX, Gx is isomorphic to Gx\u2032 , since any bijection from x to x\u2032 yields a bijection from Gx to Gx\u2032 .\nDPG can be applied to a whole model G = {gi}mi=1, if G\u2019s logvars are (re-)named such that (i) only co-domain logvars share the same name, and (ii) logvars X appear in all parfactors.\nExample 3. Consider G = {\u03c61(P (X)), \u03c62(A,P (X))}. DPG(G, {X}) = {Gi}ni=1, where each group Gi = {\u03c61(P (xi)), \u03c62(A,P (xi))} is a grounding of G (w.r.t. X). FO-dtrees simply add to dtrees special nodes for representing DPGs in parfactor models.\nDefinition 2 (DPG node) A DPG node TX is a triplet (X,x, C), where X = {X1, . . . Xk} is a set of logvars with the same domain DX, x = {x1, . . . , xk} is a set of representative objects, and C is a constraint, such that for all i 6= j: xi 6= xj \u2208 C. We denote this node as \u2200x : C in the tree.\nA representative object is simply a placeholder for a domain object.3 The idea behind our FO-dtrees is to use TX to graphically indicate aDPG(G,X). For this, each TX has a single child distinguished as Tx, under which the model is a representative instance of the isomorphic models Gx in the DPG.\nDefinition 3 (FO-dtree) An FO-dtree is a rooted tree in which\n1. non-leaf nodes may be DPG nodes\n2. each leaf contains a factor (possibly with representative objects)\n3. each leaf with a representative object x is the descendent of exactly one DPG node TX = (X,x, C), such that x \u2208 x\n4. each leaf that is a descendent of TX has all the representative objects x, and\n5. for each TX with X = {X1, . . . , Xk}, Tx has k! children {Ti}k!i=1, which are isomorphic up to a permutation of the representative objects x.\nSemantics. Each FO-dtree defines a dtree, which can be constructed by recursively grounding its DPG nodes. Grounding a DPG node TX yields a (regular) node T \u2032X with (|DX| k ) children {Tx\u2192x\u2032 |x\u2032 \u2286k DX}, where Tx\u2192x\u2032 is the result of replacing x with objects x\u2032 in Tx. Example 4. Figure 3 (a) shows the dtree of Example 3 and its corresponding FO-dtree, which only has one instance Tx of all isomorphic subtrees Txi . Figure 3 (b) shows the FO-dtree for Example 1.\n3As such, it plays the same role as a logvar. However, we use both to distinguish between a whole group of randvars (a PRV P (X)), and a representative of this group (a representative randvar P (x))."}, {"heading": "4.2 Properties", "text": "Darwiche [2] showed that important properties of a recursive decomposition are captured in the properties of dtree nodes. In this section, we define these properties for FO-dtrees. Adapting the definitions of the dtree properties, such as cutset, context, and cluster, for FO-dtrees requires accounting for the semantics of an FO-dtree, which uses DPG nodes and representative objects. More specifically, this requires making the following two modifications (i) use a function Child\u03b8(T ), instead of Child(T ), to take into account the semantics of DPG nodes, and (ii) use a function \u2229\u03b8 that finds the intersection of two sets of representative randvars. First, for a DPG node TX = (X,x, C), we define: Child\u03b8(TX) = {Tx\u2192x\u2032 |x\u2032 \u2286k DX}. Second, for two sets A = {ai}ni=1 and B = {bi}ni=1 of (representative) randvars we define: A \u2229\u03b8 B = {ai|\u2203\u03b8 \u2208 \u0398 : ai\u03b8 \u2208 B}, with \u0398 the set of grounding substitutions to their representative objects. Naturally, this provides a basis to define a \u2018\\\u03b8\u2019 operator as : A \\\u03b8 B = A \\ (A \u2229\u03b8 B). All the properties of an FO-dtree are defined based on their corresponding definitions for dtrees, by replacing Child, \u2229, \\ with Child\u03b8, \u2229\u03b8, \\\u03b8. Interestingly, all the properties can be computed without grounding the model, e.g., for a DPG node TX , we can compute rv(TX) simply as rv(Tx)\u03b8\u22121X , with \u03b8\u22121X = {x\u2192 X}.4 Figure 4 shows examples of FO-dtrees with their node clusters.\nCounted FO-dtrees. FO-dtrees capture the first lifting tool, isomorphic decomposition, explicitly in DPG nodes. The second tool, counting, can be simply captured by rewriting interchangeable randvars in clusters of the tree nodes with counting randvars. This can be done in FO-dtrees similarly to the operation of counting conversion on logvars in LVE. We call such a tree a counted FO-dtree. Figure 5(a) shows an FO-dtree (left) and its counted version (right)."}, {"heading": "5 Liftable FO-dtrees", "text": "When inference can be performed using the lifted operations (i.e., without grounding the model), it runs in polynomial time in the domain size of logvars. Formally, this is called a domain-lifted inference solution [19]. Not all FO-dtrees have a lifted solution, which is easy to see since not\n4The only non-trivial property is cutset of DPG nodes. We can show that cutset(TX) excludes from rv(TX) \\ acutset(TX) only those PRVs for which X is a binding class of logvars [8, 19].\nall models are liftable [7], though each model has at least one FO-dtree.5 Fortunately, we can structurally identify the FO-dtrees for which we know a lifted solution.\nWhat models can the lifting tools handle? Lifted inference identifies isomorphic problems and solves only one instance of those. Similar to propositional inference, for a lifted method the difficulty of each sub-problem increases with the number of variables in the problem\u2013 those that appear in the clusters of FO-dtree nodes. When each problem has a bounded (domain-independent) number of those, the complexity of inference is clearly independent of the domain size. However, a subproblem can involve a large group of randvars\u2014 when there is a PRV in the cluster. While traditional inference is then intractable, lifting may be able to exploit the interchangeability among the randvars and reduce the complexity by counting. Thus, whether a problem has a lifted solution boils down to whether we can rewrite it such that it only contains a bounded (domain-independent) number of counting randvars and ground randvars. This requires the problem to have enough symmetries in it such that all the randvars V = V1, . . . Vn in each cluster can be divided into k groups of interchangeable (tuples of) randvars V1,V2, . . . ,Vk, where k is independent of the domain size.\nTheorem 1 A (non-counted) FO-dtree has a lifted inference solution if its clusters only consist of (representative) randvars and 1-logvar PRVs. We call such an FO-dtree a liftable tree.6\nProof sketch. Such a tree has a corresponding LVE solution: (i) each sub-problem that we need to solve in such a tree can be formulated as a (sum-out) problem on a model consisting of a parfactor with 1-logvar PRVs, and (ii) we can count-convert all the logvars in a parfactor with 1-logvar PRVs [10, 16], to rewrite all the PRVs into a (bounded) number of counting randvars.7"}, {"heading": "6 Lifted inference based on FO-dtrees", "text": "A dtree can prescribe the operations performed by propositional inference, such as VE [2]. In this section, we show how a liftable FO-dtree can prescribe an LVE solution for the model, thus providing the first formal method for symbolic operation selection in lifted inference.\nIn VE, each inference procedure can be characterized based on its elimination order. Darwiche [2] shows how we can read a (partial) elimination order from a dtree (by assigning elimination of each randvar to some tree node). We build on this result to read an LVE solution from a (non-counted) FO-dtree. For this, we assign to each node a set of lifted operations, including lifted elimination of PRVs (using multiplication and sum-out), and counting conversion and aggregation of logvars:\n\u2022 \u2211V : A PRV V is eliminated at anode T , if V \u2208 cluster(T ) \\ context(T ). \u2022 AGG(X): A logvar X is aggregated at a DPG node TX = (X,x, C), if (i) X \u2208 X, and\n(ii) X /\u2208 logvar(cluster(TX)). \u2022 #X : A logvar X is counted at TX, if (i) X \u2208 X, and (ii) X \u2208 logvar(cluster(TX)).\nA lifted solution can be characterized by a sequence of these operations. For this we simply need to order the operations according to two rules:\n1. If node T2 is a descendent of T1, and OPi is performed at Ti, then OP2 \u227a OP1. 2. For operations at the same node, aggregation and counting precede elimination.\nExample 5. From the FO-dtree shown in Figure 5 (a) we can read the following order of operations:\u2211 F (X,Y ) \u227a #Y \u227a \u2211 S(X) \u227a AGG(X) \u227a\u2211#Y [D(Y )], see Figure 5 (b)."}, {"heading": "7 Complexity of lifted inference", "text": "In this section, we show how to compute the complexity of lifted inference based on an FO-dtree. Just as the complexity of ground inference for a dtree is parametrized in terms of the tree\u2019s width, we define a lifted width for FO-dtrees and use it to parametrize the complexity of lifted inference.\n5A basic algorithm for constructing an FO-dtree for a PLM is presented in the appendix. 6Note that this only restricts the number of logvars in PRVs appearing in an FO-dtree\u2019s clusters, not PRVs\nin the PLM. For instance, all the liftable trees in this paper correspond to PLMs containing 2-logvar PRVs. 7For a more detailed proof, see the appendix.\nTo analyze the complexity, it suffices to compute the complexity of the operations performed at each node. Similar to standard inference, this depends on the randvars involved in the node\u2019s cluster: for each lifted operation at a node T , LVE manipulates a factor involving the randvars in cluster(T ), and thus has complexity proportional to O(|range(cluster(T ))|), where range denotes the set of possible (joint) values that the randvars can take on. However, unlike in standard inference, this complexity need not be exponential in |rv(cluster(T ))|, since the clusters can contain counting randvars that allow us to handle interchangeable randvars more efficiently. To accommodate this in our analysis, we define two widths for a cluster: a ground width wg , which is the number of ground randvars in the cluster, and a counting width, w#, which is the number of counting randvars in it. The cornerstone of our analysis is that the complexity of an operation performed at node T is exponential only in wg , and polynomial in the domain size with degree w#. We can thus compute the complexity of the entire inference process, by considering the hardest of these operations, and the number of operations performed. We do so by defining a lifted width for the tree.\nDefinition 4 (Lifted width) The lifted width of an FO-dtree T is a pair (wg, w#), where wg is the largest ground width among the clusters of T and and w# is the largest counting width among them.\nTheorem 2 The complexity of lifted variable elimination for a counted liftable FO-dtree T is:\nO(nT \u00b7 log n \u00b7 exp(wg) \u00b7 n(w#\u00b7r#)# ),\nwhere nT is the number of nodes in T , (wg, w#) is its lifted width, n (resp., n#) is the the largest domain size among its logvars (resp., counted logvars), and r# is the largest range size among its tuples of counted randvars.\nProof sketch. We can prove the theorem by showing that (i) the largest range size among clusters, and thus the largest factor constructed by LVE, is O(exp(wg) \u00b7n(w#\u00b7r#)), (ii) in case of aggregation or counting conversion, each entry of the factor is exponentiated, with complexity O(log n), and (iii) there are at most nT operations. (For a more detailed proof, see the appendix.)\nComparison to ground inference. To understand the savings achieved by lifting, it is useful to compare the above complexity to that of standard VE on the corresponding dtree, i.e., using the same decomposition. The complexity of ground VE is: O(nG \u00b7 exp(wg) \u00b7 exp(n#.w#)), where nG is the size of the corresponding propositional dtree. Two important observations are:\n1. The number of ground operations is linear in the dtree\u2019s size nG, instead of the FO-dtree\u2019s size nT (which is polynomially smaller than nG due to DPG nodes). Roughly speaking, lifting allows us to perform nT /nG of the ground operations by isomorphic decomposition.\n2. Ground VE, has a factor exp(n#.w#) in its complexity, instead of n w# # for lifted inference.\nThe latter is typically exponentially smaller. These speedups, achieved by counting, are the most significant for lifted inference, and what allows it to tackle high treewidth models."}, {"heading": "8 Conclusion", "text": "We proposed FO-dtrees, a tool for representing a recursive decomposition of PLMs. An FO-dtree explicitly shows the symmetry between its isomorphic parts, and can thus show a form of decomposition that lifted inference methods employ. We showed how to decide whether an FO-dtree is liftable (has a corresponding lifted solution), and how to derive the sequence of lifted operations and the complexity of LVE based on such a tree. While we focused on LVE, our analysis is also applicable to lifted search-based methods, such as lifted recursive conditioning [13], weighted firstorder model counting [21], and probabilistic theorem proving [6]. This allows us to derive an order of operations and complexity results for these methods, when operating based on an FO-dtree. Further, we can show the close connection between LVE and search-based methods, by analyzing their performance based on the same FO-dtree. FO-dtrees are also useful to approximate lifted inference algorithms, such as lifted blocked Gibbs sampling [22] and RCR [20], that attempt to improve their inference accuracy by identifying liftable subproblems and handling them by exact inference.\nAppendix\nIn this appendix, we provide proofs for the Theorem 1 and 2, and present a basic algorithm for constructing FO-dtrees for PLMs."}, {"heading": "A Proof of Theorem 1", "text": "Proof. Following the discussion in the paper, each subproblem arising during inference requires handling a parfactor involving the randvars and PRVs that appear at the cluster of the node. To prove that each of these problems are liftable (do not require us to ground the PRVs and deal with all their randvars directly), we need to show that the whole group of randvars in each cluster can be partitioned into m groups of interchangeable k-tuples of randvars, with m and k independent of the domain size. We prove this relying on the properties of counting randvars in PLMs, and the correctness of counting conversion in LVE [10, 16]. For simplicity, let us assume that there are no ground randvars in the cluster (the generalization to include ground randvars is trivial). Then the model can be written as a 1-logvar parfactor as follows:\n\u03c6(P11(X11), . . . P1,n1(X1,n1), . . . , Pm1(X11), . . . Pm,n1(Xm,nm)) |C, in which for each i \u2208 {1, . . . ,m}, all Xij are logvars from a distinct domain Di, and Pij is an PRV containing such a logvar\u2014note that for the same i some Xij (and some Pij) can have the same name, although the PRVs are distinct. Since no PRV contains more than one logvar we can count-convert all the logvars in this model. This merges all distinct PRVs Pij(Xi) into one counting randvar. As such, by applying counting conversion on all the logvars Xij of domain Di, we can rewrite in the model the group of PRVs Pi1(Xi1), . . . , Pi,ni(Xi,ni) into a counting randvar\n#Xi [P \u2032 i1(Xi), . . . , P \u2032 i,ki(Xi)]\nwhere P \u2032ij are the distinct predicates among Pij , that is:\n{P \u2032ij(Xi)}kij=1 = ni\u22c3\nj=1\nPij(Xi)\nAfter counting all the logvars the parfactor becomes of the form\n\u03c6\u2032 ( #X1 [P \u2032 11(X1), . . . , P \u2032 1,k1(X1)], . . . ,#Xm [P \u2032 m1(Xm), . . . , P \u2032 m,km(Xm)] )\nThis shows that the whole group of randvars in the model can be partitioned into m groups of interchangeable k-tuples of randvars\u2013 one group of tuples for each counting randvar. Note that here both k and m are independent of the domain size of the logvars: (i) m is the number of distinct domains among the logvars, and (ii) k can be no larger than the number of PRVs with a co-domain logvar in the model, that is, k \u2264 max{ki}i \u2264 max{ni}i. It is straight-forward to show that this also holds in the general case of a parfactor involving both 1-logvar and ground randvars."}, {"heading": "B Proof of Theorem 2", "text": "Proof. We prove the theorem by bounding the complexity of each lifted operation performed at each of the nT nodes of the tree. First consider a lifted elimination performed at some node T \u2032. The complexity of this operation is proportional to |range(cluster(T \u2032))|, as it needs to deal with a parfactor involving the (counting) randvars in the cluster. Each cluster is a group A = {A1, A2, . . . Aw\u2032g , \u03b31, \u03b32, . . . , \u03b3w\u2032#} of randvars Ai, and counting randvars \u03b3i = #Xi [Pi1(Xi), . . . , Pik(Xi)], where w \u2032 # \u2264 w#, and w\u2032g \u2264 wg .Thus\n|range(A)| = (\u220f\ni\n|range(Ai)| ) \u00b7 (\u220f\nj\n|range(\u03b3j)| ) .\nFor the first product, we have w\u2032g\u220f\ni=1\n|range(Ai)| = O(exp(wg)).\nMoreover, since for each counting randvar \u03b3i, |range(\u03b3i)| = O(nrii ), where ni is the domain size of Xi, and ri is the range size of the tuples of PRVs inside \u03b3i, for the second product we have\nw\u2032#\u220f\nj=1\n|range(\u03b3j)| = O((nr## )w#) = O(n (w#\u00b7r#) # )\nThese two show that |range(A)| = O(exp(wg) \u00b7 n(w#\u00b7r#)# )\nThis is the complexity of each lifted elimination step. Build on this we compute the complexity of the other two lifted operations, aggregation and counting conversion. For each of the |range(A)| entries in the parfactor, these two operations perform an exponentiation which has complexity O(log n), where n is the domain size of the logvar. As such, this has complexityO(log n \u00b7exp(wg) \u00b7n(w#\u00b7r#)# ). Since there at most one of each operation performed at each of the nT nodes, the complexity of entire inference is\nO(nT \u00b7 log n \u00b7 exp(wg) \u00b7 n(w#\u00b7r#)# )."}, {"heading": "C Finding corresponding FO-dtrees", "text": "In this section, we provide a simple algorithm that given a model G constructs a corresponding FOdtree. Our method works in a top-down manner according to a recursive decomposition of G using DPGs. We also briefly discuss possible extensions of this simple algorithm, which can transform it into a greedy algorithm for finding \u2018better\u2019 trees.\nWe construct the tree top-down according to a recursive decomposition of G, which also employs DPGs (Algorithm 1). At the beginning we have a single root node T with model G. According to a decomposition of G into {Gi}i we add the children Ti of T to the tree, and then recursively build each tree Ti for Gi. Under DPG nodes we represent only one instance of the children. DPGs allow us to decompose the model into partial groundings, and recursive application of this tool results in a ground model. This allows us to reduce the problem to finding a dtree for the ground model.\nFO-dtree(G) if G is ground\nreturn DTREE(G) if \u2203X that allows DPG TX \u2190 DPG-NODE(X,x, G) Gx = {G\u03b8|\u03b8 \u2208 \u0398x} T.ADDCHILD(FO-Dtree(Gx)) else: T \u2190 NEWNODE() choose logvars X that co-occur in G: (there is always at least one choice X = {Xi}) GX \u2190 {g|X \u2208 logvar(g)} G\u00acX \u2190 G \\GX T.ADDCHILDREN(FO-Dtree(GX),FO-Dtree(G\u00acX)) return T\nAlgorithm 1: A simple algorithm for finding a corresponding FO-dtree.\nExtension to a greedy method for finding FO-dtrees. The above is a simple algorithm that shows the existence of a FO-dtree for each model, by finding one possible FO-dtree. While it does not consider the quality of the found FO-dtree, it can be easily modified into an algorithm that greedily searches for better trees, by performing better DPGs. For this we need to make two changes in Algorithm 1: (1) rename the logvars such that the model allows for a DPG, instead of relying on the naming of logvars in the model, and (2) select among the possible DPGs based on some criteria.\nThe first change requires us to align the logvars in different parfactors before performing a DPG, that is to rename the logvars properly such that a subset of the logvars allow for DPG. This is a\nsimple generalization of finding an alignment between two parfactors, which is employed in lifted multiplication. This change allows us to consider all possible DPGs of the model in our search, without being restricted by the naming of logvars in the model. The second change allows us to consider the quality of different DPGs for selection among them. Here we give a score to possible DPGs, which is a greedy measure of the quality of their decomposition. For instance, we can simply consider the cutset size of the decomposition, or the size of its resulting clusters. A straightforward measure is comparing the lifted width of the resulting nodes, which takes into account also the opportunities exploited by counting. These two changes should be naturally incorporated into one module, which considers possible logvar re-namings (alignments) that enable some DPG, measures the quality of the corresponding DPGs, and selects among them. Search for alignments can be guided by considering the properties of logvars in the model [8, 19], and our result about computing properties of FO-dtree nodes based on the properties of logvars."}], "references": [{"title": "Solving #-SAT and Bayesian inference with backtracking search", "author": ["F. Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Recursive conditioning", "author": ["Adnan Darwiche"], "venue": "Artif. Intell.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Lifted first-order probabilistic inference", "author": ["Rodrigo de Salvo Braz", "Eyal Amir", "Dan Roth"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["Rina Dechter"], "venue": "Artif. Intell.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "An Introduction to Statistical Relational Learning", "author": ["Lise Getoor", "Ben Taskar", "editors"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Probabilistic theorem proving", "author": ["Vibhav Gogate", "Pedro Domingos"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Liftability of probabilistic inference: Upper and lower bounds", "author": ["Manfred Jaeger", "Guy Van den Broeck"], "venue": "In Proceedings of the 2nd International Workshop on Statistical Relational AI (StaRAI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Lifted inference seen from the other side : The tractable features", "author": ["Abhay Jha", "Vibhav Gogate", "Alexandra Meliou", "Dan Suciu"], "venue": "In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Counting belief propagation", "author": ["Kristian Kersting", "Babak Ahmadi", "Sriraam Natarajan"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Lifted probabilistic inference with counting formulas", "author": ["Brian Milch", "Luke S. Zettlemoyer", "Kristian Kersting", "Michael Haimes", "Leslie Pack Kaelbling"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Markov chains on orbits of permutation groups", "author": ["Mathias Niepert"], "venue": "In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "First-order probabilistic inference", "author": ["David Poole"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Towards completely lifted search-based probabilistic inference", "author": ["David Poole", "Fahiem Bacchus", "Jacek Kisynski"], "venue": "CoRR, abs/1107.4035,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Exploiting contextual independence in probabilistic inference", "author": ["David Poole", "Nevin Lianwen Zhang"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Lifted first-order belief propagation", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Generalized counting for lifted variable elimination", "author": ["Nima Taghipour", "Jesse Davis"], "venue": "In Proceedings of the 2nd International Workshop on Statistical Relational AI (StaRAI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Lifted variable elimination with arbitrary constraints", "author": ["Nima Taghipour", "Daan Fierens", "Jesse Davis", "Hendrik Blockeel"], "venue": "In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Completeness results for lifted variable elimination", "author": ["Nima Taghipour", "Daan Fierens", "Guy Van den Broeck", "Jesse Davis", "Hendrik Blockeel"], "venue": "In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "On the completeness of first-order knowledge compilation for lifted probabilistic inference", "author": ["Guy Van den Broeck"], "venue": "In Proceedings of the 24th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Lifted relax, compensate and then recover: From approximate to exact lifted probabilistic inference", "author": ["Guy Van den Broeck", "Arthur Choi", "Adnan Darwiche"], "venue": "In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Lifted probabilistic inference by first-order knowledge compilation", "author": ["Guy Van den Broeck", "Nima Taghipour", "Wannes Meert", "Jesse Davis", "Luc De Raedt"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "On lifting the gibbs sampling algorithm", "author": ["Deepak Venugopal", "Vibhav Gogate"], "venue": "In Proceedings of the 26th Annual Conference on Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Probabilistic logical modes (PLMs) combine elements of first-order logic with graphical models to succinctly model complex, uncertain, structured domains [5].", "startOffset": 154, "endOffset": 157}, {"referenceID": 11, "context": "To address this, Poole [12] introduced the concept of lifted probabilistic inference, i.", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 5, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 7, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 8, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 9, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 12, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 14, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 16, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 17, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 18, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 20, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 21, "context": "Various lifted algorithms have been proposed, mainly by lifting propositional inference algorithms [3, 6, 8, 9, 10, 13, 15, 17, 18, 19, 21, 22].", "startOffset": 99, "endOffset": 143}, {"referenceID": 3, "context": "The performance of propositional inference, such as variable elimination [4, 14] or recursive conditioning [2], is characterized in terms of a corresponding tree decomposition of the model, and their complexity is measured based on properties of the decomposition, mainly its width.", "startOffset": 73, "endOffset": 80}, {"referenceID": 13, "context": "The performance of propositional inference, such as variable elimination [4, 14] or recursive conditioning [2], is characterized in terms of a corresponding tree decomposition of the model, and their complexity is measured based on properties of the decomposition, mainly its width.", "startOffset": 73, "endOffset": 80}, {"referenceID": 1, "context": "The performance of propositional inference, such as variable elimination [4, 14] or recursive conditioning [2], is characterized in terms of a corresponding tree decomposition of the model, and their complexity is measured based on properties of the decomposition, mainly its width.", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "It is known that standard (propositional) inference has complexity exponential in the treewidth [2, 4].", "startOffset": 96, "endOffset": 102}, {"referenceID": 3, "context": "It is known that standard (propositional) inference has complexity exponential in the treewidth [2, 4].", "startOffset": 96, "endOffset": 102}, {"referenceID": 1, "context": "Such analysis is typically done using a secondary structure for representing the decomposition of graphical models, such as decomposition trees (dtrees) [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "While many such languages exist (see [5] for an overview), we focus on parametric factors (parfactors) [12] that generalize factor graphs.", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "While many such languages exist (see [5] for an overview), we focus on parametric factors (parfactors) [12] that generalize factor graphs.", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "Every model can be written into this form in poly time [13].", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "This is an instance of the general sum-product problem [1].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Then, the decomposiSimilarly to existing studies on propositional inference [2, 4], our analysis only considers the model\u2019s global structure, and makes no assumptions about its local structure.", "startOffset": 76, "endOffset": 82}, {"referenceID": 3, "context": "Then, the decomposiSimilarly to existing studies on propositional inference [2, 4], our analysis only considers the model\u2019s global structure, and makes no assumptions about its local structure.", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "Most exact inference algorithms recursively apply this rule and compute the final result using topdown or bottom-up dynamic programming [1, 2, 4].", "startOffset": 136, "endOffset": 145}, {"referenceID": 1, "context": "Most exact inference algorithms recursively apply this rule and compute the final result using topdown or bottom-up dynamic programming [1, 2, 4].", "startOffset": 136, "endOffset": 145}, {"referenceID": 3, "context": "Most exact inference algorithms recursively apply this rule and compute the final result using topdown or bottom-up dynamic programming [1, 2, 4].", "startOffset": 136, "endOffset": 145}, {"referenceID": 1, "context": "A decomposition tree (dtree) is a structure that represents the decomposition used by a specific solution and allows us to determine its complexity [2].", "startOffset": 148, "endOffset": 151}, {"referenceID": 10, "context": "Building on this, symmetries arise between constructs at different levels [11], such as between: randvars, value assignments to randvars, factors, models, or even sum-product problems.", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "Below, we show how these tools are used by lifted variable elimination (LVE) [3, 10, 12, 17, 18].", "startOffset": 77, "endOffset": 96}, {"referenceID": 9, "context": "Below, we show how these tools are used by lifted variable elimination (LVE) [3, 10, 12, 17, 18].", "startOffset": 77, "endOffset": 96}, {"referenceID": 11, "context": "Below, we show how these tools are used by lifted variable elimination (LVE) [3, 10, 12, 17, 18].", "startOffset": 77, "endOffset": 96}, {"referenceID": 16, "context": "Below, we show how these tools are used by lifted variable elimination (LVE) [3, 10, 12, 17, 18].", "startOffset": 77, "endOffset": 96}, {"referenceID": 17, "context": "Below, we show how these tools are used by lifted variable elimination (LVE) [3, 10, 12, 17, 18].", "startOffset": 77, "endOffset": 96}, {"referenceID": 1, "context": "Darwiche [2] showed that important properties of a recursive decomposition are captured in the properties of dtree nodes.", "startOffset": 9, "endOffset": 12}, {"referenceID": 18, "context": "Formally, this is called a domain-lifted inference solution [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "We can show that cutset(TX) excludes from rv(TX) \\ acutset(TX) only those PRVs for which X is a binding class of logvars [8, 19].", "startOffset": 121, "endOffset": 128}, {"referenceID": 18, "context": "We can show that cutset(TX) excludes from rv(TX) \\ acutset(TX) only those PRVs for which X is a binding class of logvars [8, 19].", "startOffset": 121, "endOffset": 128}, {"referenceID": 6, "context": "all models are liftable [7], though each model has at least one FO-dtree.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "Such a tree has a corresponding LVE solution: (i) each sub-problem that we need to solve in such a tree can be formulated as a (sum-out) problem on a model consisting of a parfactor with 1-logvar PRVs, and (ii) we can count-convert all the logvars in a parfactor with 1-logvar PRVs [10, 16], to rewrite all the PRVs into a (bounded) number of counting randvars.", "startOffset": 282, "endOffset": 290}, {"referenceID": 15, "context": "Such a tree has a corresponding LVE solution: (i) each sub-problem that we need to solve in such a tree can be formulated as a (sum-out) problem on a model consisting of a parfactor with 1-logvar PRVs, and (ii) we can count-convert all the logvars in a parfactor with 1-logvar PRVs [10, 16], to rewrite all the PRVs into a (bounded) number of counting randvars.", "startOffset": 282, "endOffset": 290}, {"referenceID": 1, "context": "A dtree can prescribe the operations performed by propositional inference, such as VE [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Darwiche [2] shows how we can read a (partial) elimination order from a dtree (by assigning elimination of each randvar to some tree node).", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "While we focused on LVE, our analysis is also applicable to lifted search-based methods, such as lifted recursive conditioning [13], weighted firstorder model counting [21], and probabilistic theorem proving [6].", "startOffset": 127, "endOffset": 131}, {"referenceID": 20, "context": "While we focused on LVE, our analysis is also applicable to lifted search-based methods, such as lifted recursive conditioning [13], weighted firstorder model counting [21], and probabilistic theorem proving [6].", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "While we focused on LVE, our analysis is also applicable to lifted search-based methods, such as lifted recursive conditioning [13], weighted firstorder model counting [21], and probabilistic theorem proving [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 21, "context": "FO-dtrees are also useful to approximate lifted inference algorithms, such as lifted blocked Gibbs sampling [22] and RCR [20], that attempt to improve their inference accuracy by identifying liftable subproblems and handling them by exact inference.", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "FO-dtrees are also useful to approximate lifted inference algorithms, such as lifted blocked Gibbs sampling [22] and RCR [20], that attempt to improve their inference accuracy by identifying liftable subproblems and handling them by exact inference.", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "We prove this relying on the properties of counting randvars in PLMs, and the correctness of counting conversion in LVE [10, 16].", "startOffset": 120, "endOffset": 128}, {"referenceID": 15, "context": "We prove this relying on the properties of counting randvars in PLMs, and the correctness of counting conversion in LVE [10, 16].", "startOffset": 120, "endOffset": 128}], "year": 2013, "abstractText": "Lifting attempts to speedup probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the first-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree.", "creator": "LaTeX with hyperref package"}}}