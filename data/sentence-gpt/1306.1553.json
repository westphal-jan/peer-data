{"id": "1306.1553", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2013", "title": "Direct Uncertainty Estimation in Reinforcement Learning", "abstract": "Optimal probabilistic approach in reinforcement learning is computationally infeasible. Its simplification consisting in neglecting difference between true environment and its model estimated using limited number of observations causes exploration vs exploitation problem. Uncertainty can be expressed in terms of a probability distribution over the space of environment models, and this uncertainty can be propagated to the action-value function via Bellman iterations, which are computationally insufficiently efficient though it can be used to estimate and estimate the probability distribution. The probabilistic approach is also probabilistic and provides predictions that are well-defined and can be extended to future research.\n\n\n\nIn conclusion, here are two main approaches that we take for consideration in reinforcement learning. One is that the two approaches have very common origins. They both allow for a simple probabilistic model to be used to evaluate and manipulate behavior of a group of different training groups. It is a very important one and a good one for training groups to understand the nature of learning. However, the probabilistic approach does not have all the features that are required for training, which is due to many of the features that the training groups are accustomed to. So, in order to take into account the complexity of a trained training group and the practical requirements for a training group, the training groups are required to evaluate, control, modify and use the same probabilistic model and train each other to understand the neural circuitry of each training group.\nThe main reason why it has been shown that the probabilistic model of reinforcement learning is a highly probabilistic model in the first place. But, as it was demonstrated, this probabilistic model is an extremely difficult and costly model. And it is quite important that the probabilistic model is able to distinguish between the best and the best and the worst and not be used to infer the best.\nFor example, in the above case, in the model above, the training group in the training group has a training group with a training group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with a learning group with", "histories": [["v1", "Thu, 6 Jun 2013 20:57:19 GMT  (130kb)", "http://arxiv.org/abs/1306.1553v1", "AGI-13 Workshop paper"], ["v2", "Tue, 25 Jun 2013 14:32:12 GMT  (57kb)", "http://arxiv.org/abs/1306.1553v2", "AGI-13 Workshop paper"]], "COMMENTS": "AGI-13 Workshop paper", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sergey rodionov", "alexey potapov", "yurii vinogradov"], "accepted": false, "id": "1306.1553"}, "pdf": {"name": "1306.1553.pdf", "metadata": {"source": "CRF", "title": "Direct Uncertainty Estimation in Reinforcement Learning", "authors": ["Sergey Rodionov", "Alexey Potapov", "Yurii Vinogradov"], "emails": ["potapov@aideus.com", "rodionov@aideus.com"], "sections": [{"heading": null, "text": "Direct Uncertainty Estimation in Reinforcement Learning\nSergey Rodionov1,2, Alexey Potapov1,3, Yurii Vinogradov3\n1AIDEUS, Russia 2Aix Marseille Universit\u00e9, CNRS, LAM (Laboratoire d'Astrophysique de Marseille) UMR\n7326, 13388, Marseille, France 3National Research University of Information Technology, Mechanics and Optics,\nSt. Petersburg, Russia {potapov,rodionov}@aideus.com\nOptimal probabilistic approach in reinforcement learning is computationally infeasible. Its simplification consisting in neglecting difference between true environment and its model estimated using limited number of observations causes exploration vs exploitation problem. Uncertainty can be expressed in terms of a probability distribution over the space of environment models, and this uncertainty can be propagated to the action-value function via Bellman iterations, which are computationally insufficiently efficient though. We consider possibility of directly measuring uncertainty of the action-value function, and analyze sufficiency of this facilitated approach."}, {"heading": "1 Introduction", "text": "Absence of prior knowledge about the environment (absence of its precise model) is naturally characterized by the intuitive notion of uncertainty. However, no generally accepted accurate formal description of this notion exists. Probability theory is the most traditional way of describing uncertainty, but adequate interpretation of probability itself isn\u2019t that clear. This can be seen from numerous paradoxes in probability theory. That\u2019s why some attempts of extending probability theory were made. The most well-known one is fuzzy set theory. However, fuzzy operations can be considered as probabilistic operations with some additional assumptions about operands (e.g. their independence), which allow simplifying their computation. The main difference here lies not in formalisms, but in their interpretation and usage.\nDifferences in interpretations frequently appear, because complex systems (e.g. verbal notions) are being analyzed. Some simple measure of uncertainty cannot be applied in such cases without being a part of some model of intelligence. Thus, consideration of this problem in the context of intelligent agents can be most appropriate. These agents can be put in environments of different classes, e.g. Markov environments or arbitrary computable environments. The latter are quite interesting since they possess maximum uncertainty. Exactly this case helps to reveal difficulties in classic probability theory, which are solved within algorithmic probability theory [1].\nOptimal solutions with similar structure exist both for Markov environments using classic probability and for computable environments using algorithmic probability without involving other formalisms.\nHowever, this solution is computationally intractable, since it consists in enumeration of all possible actions and all possible responses of the environment with estimation of probability of each possibility. Traditional reinforcement learning methods rely only on one most probable model of the environment ignoring all other models that makes them computationally very efficient. The same simplification can be applied also to such models as AIXI dealing with arbitrary environments, but it is insufficient for achieving efficient universal agents, so it has not been analyzed in detail. We also consider this simplification on the example of Markov environments assuming that the case of arbitrary environments mainly differs in the way of computing probabilities. Policies that use only one best model of the environment for prediction lack exploration in classic RL. This lack is compensated not by using many models, but by introducing specific exploratory strategies (simplest one is \u03b5-greedy). However, this results in the exploration vs exploitation dilemma. This dilemma is exactly the reason for introducing different measures of uncertainty in RL methods. Hence uncertainty is introduced as a technique for compensating negative effects caused by simplification of optimal probabilistic methods.\nIn this paper, we consider one specific approach of introducing and utilizing uncertainty in models of RL agents in Markov environments. This approach relies on direct empirical estimation of uncertainty for a \u201csplit\u201d action-value function Q(s, a, s'). It showed adequate results in solving the exploration vs exploitation dilemma meaning that uncertainty can indeed be considered as a \u201cheuristic\u201d that helps to greatly simplify the optimal solution without violating its important features."}, {"heading": "2 Related Works", "text": "Consider traditional settings for RL-agents [2]. Let an RL-agent is placed in a Markov environment defined by a state space S, a set of possible actions A, transition probabilities P(s' | s, a), ]1,0[: \u2192\u00d7\u00d7 SASP , and a reward function R(s, a, s'),\nR\u2192\u00d7\u00d7 SASR : , where R is a set of possible reward values. The value function V\u03c0(s) for some policy \u03c0 is calculated as the summed discounted\nfuture rewards\n\u239f\u239f \u23a0 \u239e \u239c\u239c \u239d \u239b \u03c0\u03b3= \u2211 +\u03c0 t ttt t t sssREsV )),(,()( 1 ,\nand it should be maximized over some policy space. The action-value function Q\u03c0(s, a) that depends also on the action a is usually introduced. The action-value function Q* for optimal strategy should satisfy the wellknown Bellman equation\n( )),(max),,(),( ** asQsasREasQ as \u2032\u2032\u03b3+\u2032= \u2032\u2032 .\nIf the true model of the environment including P and R is known, Q* can be estimated by dynamic programming. If such model isn\u2019t given, it seems natural to use some estimation of P and R, and this is done in most existing RL-methods (estimated P and R aren\u2019t necessarily used in the explicit form). Transition probabilities are simply replaced by frequencies. However, different values of transition probabilities are also possible. Indeed, if some action was used n times in some state, and it had i-th outcome ni times, the probability P(pi | ni, n) that real probability of i-th outcome given these observations equals pi can be calculated using Bayes\u2019 rule\n)()1()()|,(),|( i nn i n iiiiii pPpppPpnnPnnpP ii \u2212\u2212=\u221d . (1)\nOptimal Bayesian agent should account for these probabilities of probabilities and their updates after new hypothetical observations. However, their maintenance is computationally infeasible, and some simplifications are needed. For example, uncertainty in transition probabilities is represented by covariance matrices in [3]. That is, each transition probability is treated as normally distributed random variable. Uncertainty in R is also taken into account (this uncertainty is presented for s-a-s' transitions that have never taken place yet). The Bellman equation written for random variables is used to propagate uncertainty from P and R to Q using dynamic programming.\nIn [4], this approach is simplified by ignoring covariances (only dispersions are used). However, this method still requires propagation of uncertainty using Bellman iterations on each step. This is not too practical. One would like to update uncertainty of Q as easy as updates of Q are done in such methods as Q-learning or SARSA.\nAnother question is connected to adoption of uncertainty in selecting actions. Authors of [3] proposed to introduce some modified action-value function for uncertainty-aware policy improvement\n),(),(),( asQasQasQ \u03be\u03c3\u2212=\u2032 , (2)\nwhere \u03be is the parameter for balancing exploration and exploitation. Presence of the free parameter \u03be indicates that this is not a complete solution of the exploration vs exploitation dilemma.\nOther approaches to incorporating uncertainty in RL exist (see references in [3, 4]), but the approach mentioned above is the most appropriate for our objectives."}, {"heading": "3 Direct Estimation of Uncertainty of Q", "text": "Uncertainty propagation has clear sense, but it violates the main advantage of modelfree reinforcement learning. Is it possible to estimate uncertainty of Q without explicitly constructing environment models as it can be done while estimating Q itself? Or at least is it possible to avoid uncertainty propagation?\nOne can try evaluating variations of Q(s, a) for each pair (s, a) empirically. Indeed, changes of Q(s, a) can be caused by the lack of agent\u2019s knowledge. Thus, the agent should simply accumulate averaged Qt2(s, a) in addition to averaged Qt(s, a) for each (s, a). Unfortunately, this approach works only in deterministic environments. The\nreason is quite clear. Stochasticity of the environment causes persistent variations of Q values. Consider the following well-known update rule as an example\nQ(st, at) Q(st, at)+\u03b1[rt+\u03b3Q(st+1, at+1)\u2013Q(st, at)]. (3)\nObviously, even if optimal values Q*(s, a) are used as initial values, updates will cause variations ),|(),( assRasQ s \u2032\u03b1\u03c3\u221d\u03c3 \u2032 .\nThus, variations of Q(s, a) caused by imprecise knowledge of the environment and by intrinsic stochasticity of the environment should be separated (these two types of uncertainty of Q have entirely different meaning, but both of them can be expressed in terms of probability).\nTo do this, we introduce \u201csplit\u201d action-value functions Q(s, a, s'), which variations are caused only by uncertainty of the agent\u2019s knowledge, but not stochasticity of the environment. Indeed, this function can be updated in the same way as Q(s, a)\nQ(st, at, st+1) Q(st, at, st+1)+\u03b1[rt+\u03b3Q(st+1, at+1)\u2013Q(st, at, st+1)]. (4)\nSince rt will always be the same for certain s-a-s' transition, stochasticity of the environment will not cause variations of Q(s, a, s'). Unfortunately, the total action-value function Q(s, a) should be calculated as\n\u2211 \u2032 \u2032\u2032= s sasQassPasQ ),,(),|(),( . (5)\nThus, transition probabilities should be estimated in any case, but the uncertainty ),,( sasQ \u2032\u03c3 can be estimated empirically without its propagation via dynamic pro-\ngramming. Now let\u2019s consider more sensible way of adopting uncertainty in selecting action than (2). If it is assumed that Q-values are random variables, for which Bellman equation can be written, than it should be also assumed that such equations for selecting actions as\n),(maxarg)( asQs a =\u03c0\nare written for random variables making \u03c0(s) also a random variable. Since Q(s, a) has some estimated probability distribution (e.g. Gaussian distribution described by EQ(s, a) and \u03c3Q(s, a)), one can simply generate one sample of Q(s, a) for each a in accordance with corresponding distributions and find maximum among them to choose an action. This procedure will output realizations of the random variable arg max\na Q(s,a) . Thus, it is the correct extension of the reward-\nmaximization policy on the case of uncertain Q. However, uncertainty is estimated for Q(s, a, s') in our case, and uncertainty adoption should be slightly more complicated. Random values of Q(s, a, s') for each s' are sampled from corresponding distributions and their weighted average value is calculated as an instance of Q(s, a). Weights are uncertain probabilities in (5), which specific values are also randomly sampled in accordance with the distribution (1). The acceptance-rejection method is used here: pi are randomly sampled from [0, 1]; they\nare normalized (their sum should equal 1.0); then they are accepted if the value randomly chosen from the range [0, 1] is smaller than their likelihood (prior probabilities are assumed to be uniformly distributed).\nThe vaguest issue is accounting for transitions that haven\u2019t been encountered yet. One should estimate possible probabilities of such transitions and their Q-values. Our agent assumes that there is one unknown outcome for each action. Its probability can easily be estimated using (1) as nunknownunknown pnpP )1(),0|( \u2212\u221d . However, there is no information about Q(s, a, s') and its uncertainty for this hypothetical transition. This uncertainty is \u201cabsolute\u201d, i.e. there is no universal prior distribution of Q for arbitrary environment. One can consider the lottery environment, in which the agent has two actions corresponding to buying a ticket or not. Both probability of winning and possible prize are unknown. One can imagine two types of the environment with positive and negative expected reward for the first action (the second action has zero reward). Imagine that the agent has bought losing tickets for 10100 times. Should it stop? It is impossible to answer this question without any prior knowledge about the distribution of Q (or R). We should incorporate such prior knowledge about the real world into AGI. However, in this paper, we simply applied optimistic approach assuming that the largest possible reward is known. We also initialize Q-values with maximum value of Q in Q-learning methods that greatly encourages exploration even with small \u03b5."}, {"heading": "4 Experiments", "text": "In our experiments, we considered \u201clayered\u201d environments with the following structure. Zero level (l=0) has one state; all other levels (l=1 ...m) have n states per level. Single state on zero level (l=0), has n possible actions, and each of them leads with probability p=1.0 to corresponding state on l=1. Each state on the last level (l=m) has only one possible action, which leads to the single state on zero level with p=1.0. Each state on intermediate levels l=1...m\u20131 has k possible actions, each of which has two possible results leading to one of two states on l=i+1 (probabilities of possible outcomes of each action are chosen randomly). Here we used m=20, n=10, k=2.\nWe compared three algorithms: (1) ordinary Q-learning with \u03b5-greedy strategy, which is turned off after some large number of steps (when Q-function is learned); (2) modified Q-learning (we will refer to it as split-Q-learning) with the split actionvalue function Q(s, a, s') and also with disabling \u03b5-greedy strategy; (3) modified Qlearning with uncertain Q(s, a, s').\nIt should be pointed out that disabling \u03b5-greedy strategy is \u201cunfair\u201d, since we don\u2019t know for arbitrary environment, when exploration should be stopped (and in the case of non-stationary environments it shouldn\u2019t be stopped at all). Thus, rewards during exploration show performance of all methods, and rewards after terminating \u03b5-greedy strategy of the first two methods serve as the reference result for the third algorithm that continues exploration, because uncertainty is not reduced to zero.\nFig. 1 shows rewards (averaged over 10000 trial environments) obtained by these tree algorithms on each step.\nIt can be seen that split-Q-learning outperforms ordinary Q-learning. This result deserves independent study that goes beyond the topic of this paper. Discontinuity on the curves obtained for two first algorithms is obviously caused by turning off the \u03b5greedy strategy. Interestingly, rewards of ordinary Q-learning after this step are lower than that of split-Q-learning. The possible reason consists in constant updates of Qvalues using the latest rewards, which are random in stochastic environments (and thus Q-function oscillates). Indeed, this difference appears to be smaller for lower values of \u03b1 parameter, but smaller \u03b1 also causes slower convergence.\nUncertain split-Q-learning shows better performance in comparison with the \u03b5greedy strategy. One can try different values of \u03b5. Higher values will yield faster exploration and achievement of the optimal policy, but this policy will be \u201cpenalized\u201d by large number of random action (until turning off the \u03b5-greedy strategy). Smaller values of \u03b5 will converge slower (but quite fast in the case of optimistic agents) to higher level of rewards. In our experiments, uncertainty-based exploration outperformed the \u03b5-greedy strategy with both small and large values of \u03b5 meaning that it goes faster to higher level of rewards. This is equivalent to adaptive alteration of \u03b5 from initial high values to subsequent small values. Of course, it doesn\u2019t reach the maximum possible level of rewards (in comparison with the optimal policy with the turned off \u03b5-greedy strategy), because some uncertainty always remains.\nConclusion\nThe approach for direct empirical estimation of uncertainty in models of RL agents was introduced on the base of the split action-value function Q(s, a, s'). This approach doesn\u2019t require uncertainty propagation and can be computationally efficient. Pre-\nliminary experiments with this approach showed possibility of solving the exploration vs exploitation problem without considering full probability distribution over the space of all environment models.\nThe main issue for future investigations is connected with non-stationary environments, for which uncertainty should not converge to zero and should supply an appropriate level of explorations."}, {"heading": "Acknowledgements", "text": "This work was supported by the Ministry of Education and Science of the Russian Federation."}], "references": [{"title": "Algorithmic Probability, Heuristic Programming and AGI", "author": ["R. Solomonoff"], "venue": "Baum, E., Hutter, M., Kitzelmann, E. (eds). Advances in Intelligent Systems Research, vol. 10 (proc. 3 Conf. on Artificial General Intelligence), pp. 151\u2013157", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Uncertainty in Reinforcement Learning \u2014 Awareness, Quantisation, and Control", "author": ["D. Schneegass", "A. Hans", "S. Udluft"], "venue": "Robot Learning, S. Jabin (ed.), pp. 65\u201390", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient Uncertainty Propagation for Reinforcement Learning with Limited Data", "author": ["A. Hans", "S. Udluft"], "venue": "Proc. Int. Conf. on Artificial Neural Networks (ICANN 2009), Cyprus, Part I, LNCS 5768, pp. 70\u201379", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Exactly this case helps to reveal difficulties in classic probability theory, which are solved within algorithmic probability theory [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Consider traditional settings for RL-agents [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "For example, uncertainty in transition probabilities is represented by covariance matrices in [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "In [4], this approach is simplified by ignoring covariances (only dispersions are used).", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Authors of [3] proposed to introduce some modified action-value function for uncertainty-aware policy improvement", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Other approaches to incorporating uncertainty in RL exist (see references in [3, 4]), but the approach mentioned above is the most appropriate for our objectives.", "startOffset": 77, "endOffset": 83}, {"referenceID": 3, "context": "Other approaches to incorporating uncertainty in RL exist (see references in [3, 4]), but the approach mentioned above is the most appropriate for our objectives.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "The acceptance-rejection method is used here: pi are randomly sampled from [0, 1]; they", "startOffset": 75, "endOffset": 81}, {"referenceID": 0, "context": "0); then they are accepted if the value randomly chosen from the range [0, 1] is smaller than their likelihood (prior probabilities are assumed to be uniformly distributed).", "startOffset": 71, "endOffset": 77}], "year": 2013, "abstractText": "Optimal probabilistic approach in reinforcement learning is computationally infeasible. Its simplification consisting in neglecting difference between true environment and its model estimated using limited number of observations causes exploration vs exploitation problem. Uncertainty can be expressed in terms of a probability distribution over the space of environment models, and this uncertainty can be propagated to the action-value function via Bellman iterations, which are computationally insufficiently efficient though. We consider possibility of directly measuring uncertainty of the action-value function, and analyze sufficiency of this facilitated approach.", "creator": "PScript5.dll Version 5.2"}}}