{"id": "1306.2119", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2013", "title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "abstract": "We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/n^{1/2}). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. A generalization method (N = 10/2) will produce a simple convex function: A(1, 2) = (1, 2) = (0, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) = (1, 2) =", "histories": [["v1", "Mon, 10 Jun 2013 07:31:10 GMT  (130kb)", "http://arxiv.org/abs/1306.2119v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["francis r bach", "eric moulines"], "accepted": true, "id": "1306.2119"}, "pdf": {"name": "1306.2119.pdf", "metadata": {"source": "CRF", "title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "authors": ["Francis Bach"], "emails": ["francis.bach@ens.fr", "eric.moulines@enst.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n21 19\nv1 [\ncs .L\nG ]\n\u221a\nn). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing that they often outperform existing approaches.\n1 Introduction\nLarge-scale machine learning problems are becoming ubiquitous in many areas of science and engineering. Faced with large amounts of data, practitioners typically prefer algorithms that process each observation only once, or a few times. Stochastic approximation algorithms such as stochastic gradient descent (SGD) and its variants, although introduced more than 60 years ago [1], still remain the most widely used and studied method in this context (see, e.g., [2, 3, 4, 5, 6, 7]).\nWe consider minimizing convex functions f , defined on a Euclidean space H, given by f(\u03b8) = E [ \u2113(y, \u3008\u03b8, x\u3009) ]\n, where (x, y) \u2208 H \u00d7 R denotes the data and \u2113 denotes a loss function that is convex with respect to the second variable. This includes logistic and least-squares regression. In the stochastic approximation framework, independent and identically distributed pairs (xn, yn) are observed sequentially and the predictor defined by \u03b8 is updated after each pair is seen.\nWe partially understand the properties of f that affect the problem difficulty. Strong convexity (i.e., when f is twice differentiable, a uniform strictly positive lower-bound \u00b5 on Hessians of f) is a key property. Indeed, after n observations and with the proper step-sizes, averaged SGD achieves the rate of O(1/\u00b5n) in the strongly-convex case [5, 4], while it achieves only O(1/ \u221a n) in the nonstrongly-convex case [5], with matching lower-bounds [8, 9]. The main issue with strong convexity is that typical machine learning problems are high dimensional and have correlated variables so that the strong convexity constant \u00b5 is zero or very\nclose to zero, and in any case smaller than O(1/ \u221a n). This then makes the non-strongly convex methods better. In this paper, we aim at obtaining algorithms that may deal with arbitrarily small strong-convexity constants, but still achieve a rate of O(1/n).\nSmoothness plays a central role in the context of deterministic optimization. The known convergence rates for smooth optimization are better than for non-smooth optimization (e.g., see [10]). However, for stochastic optimization the use of smoothness only leads to improvements on constants (e.g., see [11]) but not on the rate itself, which remains O(1/ \u221a n) for non-strongly-convex problems.\nWe show that for the square loss and for the logistic loss, we may use the smoothness of the loss and obtain algorithms that have a convergence rate of O(1/n) without any strong convexity assumptions. More precisely, for least-squares regression, we show in Section 2 that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression this is achieved by a novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent (see Section 3). For these algorithms, we provide a non-asymptotic analysis of their generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing in Section 4 that they often outperform existing approaches.\n2 Constant-step-size least-mean-square algorithm\nIn this section, we consider stochastic approximation for least-squares regression, where SGD is often referred to as the least-mean-square (LMS) algorithm. The novelty of our convergence result is the use of the constant step-size with averaging, leading to O(1/n) rate without strong convexity.\n2.1 Convergence in expectation\nWe make the following assumptions:\n(A1) H is a d-dimensional Euclidean space, with d > 1.\n(A2) The observations (xn, zn) \u2208 H\u00d7H are independent and identically distributed.\n(A3) E\u2016xn\u20162 and E\u2016zn\u20162 are finite. Denote by H = E(xn \u2297 xn) the covariance operator from H to H. Without loss of generality, H is assumed invertible (by projecting onto the minimal subspace where xn lies almost surely). However, its eigenvalues may be arbitrarily small.\n(A4) The global minimum of f(\u03b8) = (1/2)E [ \u3008\u03b8, xn\u30092 \u2212 2\u3008\u03b8, zn\u3009 ] is attained at a certain \u03b8\u2217 \u2208 H. We denote by \u03ben = zn \u2212 \u3008\u03b8\u2217, xn\u3009xn the residual. We have E [ \u03ben ] = 0, but in general, it is not true\nthat E [ \u03ben \u2223 \u2223 xn ] = 0 (unless the model is well-specified).\n(A5) We study the stochastic gradient (a.k.a. least mean square) recursion defined as\n\u03b8n = \u03b8n\u22121 \u2212 \u03b3(\u3008\u03b8n\u22121, xn\u3009xn \u2212 zn) = (I \u2212 \u03b3xn \u2297 xn)\u03b8n\u22121 + \u03b3zn, (1)\nstarted from \u03b80 \u2208 H. We also consider the averaged iterates \u03b8\u0304n = (n+ 1)\u22121 \u2211n k=0 \u03b8k.\n(A6) There exists R > 0 and \u03c3 > 0 such that E [ \u03ben \u2297 \u03ben ] 4 \u03c32H and E ( \u2016xn\u20162xn \u2297 xn ) 4 R2H , where 4 denotes the the order between self-adjoint operators, i.e., A 4 B if and only if B \u2212 A is positive semi-definite.\nDiscussion of assumptions. Assumptions (A1-5) are standard in stochastic approximation (see, e.g., [12, 6]). Note that for least-squares problems, zn is of the form ynxn, where yn \u2208 R is the response to be predicted as a linear function of xn. We consider a slightly more general case than least-squares because we will need it for the quadratic approximation of the logistic loss in Section 3.1. Note that in assumption (A4), we do not assume that the model is well-specified.\nAssumption (A6) is true for least-square regression with almost surely bounded data, since, if \u2016xn\u20162 6 R2 almost surely, then E ( \u2016xn\u20162xn \u2297 xn ) 4 E ( R2xn \u2297 xn )\n= R2H ; a similar inequality holds for the output variables yn. Moreover, it also holds for data with infinite supports, such as Gaussians or mixtures of Gaussians (where all covariance matrices of the mixture components are lower and upper bounded by a constant times the same matrix). Note that the finite-dimensionality assumption could be relaxed, but this would require notions similar to degrees of freedom [13], which is outside of the scope of this paper.\nThe goal of this section is to provide a bound on the expectation E [ f(\u03b8\u0304n)\u2212 f(\u03b8\u2217) ]\n, that (a) does not depend on the smallest non-zero eigenvalue of H (which could be arbitrarily small) and (b) still scales as O(1/n).\nTheorem 1 Assume (A1-6). For any constant step-size \u03b3 < 1R2 , we have\nE [ f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) ] 6 1\n2n\n[ \u03c3 \u221a d\n1\u2212 \u221a \u03b3R2 +R\u2016\u03b80 \u2212 \u03b8\u2217\u2016\n1 \u221a\n\u03b3R2\n]2\n. (2)\nWhen \u03b3 = 1/(4R2), we obtain E [ f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) ] 6 2n\n[ \u03c3 \u221a d+R\u2016\u03b80 \u2212 \u03b8\u2217\u2016 ]2 .\nProof technique. We adapt and extend a proof technique from [14] which is based on nonasymptotic expansions in powers of \u03b3. We also use a result from [2] which studied the recursion in Eq. (1), with xn \u2297 xn replaced by its expectation H . See the appendix for details.\nOptimality of bounds. Our bound in Eq. (2) leads to a rate of O(1/n), which is known to be optimal for least-squares regression (i.e., under reasonable assumptions, no algorithm, even more complex than averaged SGD can have a better dependence in n) [15]. The term \u03c32d/n is also unimprovable.\nInitial conditions. If \u03b3 is small, then the initial condition is forgotten more slowly. Note that with additional strong convexity assumptions, the initial condition would be forgotten faster (exponentially fast without averaging), which is one of the traditional uses of constant-step-size LMS [16].\nSpecificity of constant step-sizes. The non-averaged iterate sequence (\u03b8n) is a homogeneous Markov chain; under appropriate technical conditions, this Markov chain has a unique stationary (invariant) distribution and the sequence of iterates (\u03b8n) converges in distribution to this invariant distribution; see [17, Chapter 17]. Denote by \u03c0\u03b3 the invariant distribution. Assuming that the Markov Chain is Harris recurrent, the ergodic theorem for Harris Markov chain shows that \u03b8\u0304n\u22121 = n\u22121 \u2211n\u22121\nk=0 \u03b8k converges almost-surely to \u03b8\u0304\u03b3 def = \u222b\n\u03b8\u03c0\u03b3(d\u03b8), which is the mean of the stationary distribution. Taking the expectation on both side of Eq. (1), we get E[\u03b8n]\u2212 \u03b8\u2217 = (I \u2212 \u03b3H)(E[\u03b8n\u22121]\u2212 \u03b8\u2217), which shows, using that limn\u2192\u221e E[\u03b8n] = \u03b8\u0304\u03b3 that H\u03b8\u0304\u03b3 = H\u03b8\u2217 and therefore \u03b8\u0304\u03b3 = \u03b8\u2217 since H is invertible. Under slightly stronger assumptions, it can be shown that\nlimn\u2192\u221e nE[(\u03b8\u0304n \u2212 \u03b8\u2217)2] = Var\u03c0\u03b3 (\u03b80) + 2 \u2211\u221e k=1 Cov\u03c0\u03b3 (\u03b80, \u03b8k) ,\nwhere Cov\u03c0\u03b3 (\u03b80, \u03b8k) denotes the covariance of \u03b80 and \u03b8k when the Markov chain is started from stationarity. This implies that limn\u2192\u221e nE[f(\u03b8\u0304n) \u2212 f(\u03b8\u2217)] has a finite limit. Therefore, this interpretation explains why the averaging produces a sequence of estimators which converges to the\nsolution \u03b8\u2217 pointwise, and that the rate of convergence of E[f(\u03b8n)\u2212 f(\u03b8\u2217)] is of order O(1/n). Note that for other losses than quadratic, the same properties hold except that the mean under the stationary distribution does not coincide with \u03b8\u2217 and its distance to \u03b8\u2217 is typically of order \u03b32 (see Section 3).\n2.2 Convergence in higher orders\nWe are now going to consider an extra assumption in order to bound the p-th moment of the excess risk and then get a high-probability bound. Let p be a real number greater than 1.\n(A7) There exists R > 0, \u03ba > 0 and \u03c4 > \u03c3 > 0 such that, for all n > 1, \u2016xn\u20162 6 R2 a.s., and\nE\u2016\u03ben\u2016p 6 \u03c4pRp and E [ \u03ben \u2297 \u03ben ] 4 \u03c32H, (3)\n\u2200z \u2208 H, E\u3008z, xn\u30094 6 \u03ba\u3008z,Hz\u30092. (4)\nThe last condition in Eq. (4) says that the kurtosis of the projection of the covariates xn on any direction z \u2208 H is bounded. Note that computing the constant \u03ba happens to be equivalent to the optimization problem solved by the FastICA algorithm [18], which thus provides an estimate of \u03ba. In Table 1, we provide such an estimate for the non-sparse datasets which we have used in experiments, while we consider only directions z along the axes for high-dimensional sparse datasets. For these datasets where a given variable is equal to zero except for a few observations, \u03ba is typically quite large. Adapting and analyzing normalized LMS techniques [19] to this set-up is likely to improve the theoretical robustness of the algorithm (but note that results in expectation from Theorem 1 do not use \u03ba). The next theorem provides a bound for the p-th moment of the excess risk.\nTheorem 2 Assume (A1-7). For any real p > 1, and for a step-size \u03b3 6 1/(12p\u03baR2), we have:\n( E \u2223 \u2223f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) \u2223 \u2223 p)1/p 6\np\n2n\n( 7\u03c4 \u221a d+R\u2016\u03b80 \u2212 \u03b8\u2217\u2016 \u221a 3 + 2\n\u03b3pR2\n)2\n. (5)\nFor \u03b3 = 1/(12p\u03baR2), we get: ( E \u2223 \u2223f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) \u2223 \u2223 p)1/p 6 p 2n (\n7\u03c4 \u221a d+ 6 \u221a \u03baR\u2016\u03b80 \u2212 \u03b8\u2217\u2016 )2 .\nNote that to control the p-th order moment, a smaller step-size is needed, which scales as 1/p.\nWe can now provide a high-probability bound; the tails decay polynomially as 1/(n\u03b412\u03b3\u03baR 2\n) and the smaller the step-size \u03b3, the lighter the tails.\nCorollary 1 For any step-size such that \u03b3 6 1/(12\u03baR2), any \u03b4 \u2208 (0, 1),\nP\n(\nf(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) > 1\nn\u03b412\u03b3\u03baR2\n[ 7\u03c4 \u221a d+R\u2016\u03b80 \u2212 \u03b8\u2217\u2016( \u221a 3 + \u221a 24\u03ba) ]2\n24\u03b3\u03baR2\n)\n6 \u03b4 . (6)\n3 Beyond least-squares: M-estimation\nIn Section 2, we have shown that for least-squares regression, averaged SGD achieves a convergence rate of O(1/n) with no assumption regarding strong convexity. For all losses, with a constant stepsize \u03b3, the stationary distribution \u03c0\u03b3 corresponding to the homogeneous Markov chain (\u03b8n) does always satisfy \u222b\nf \u2032(\u03b8)\u03c0\u03b3(d\u03b8) = 0, where f is the generalization error. When the gradient f \u2032 is linear (i.e., f is quadratic), then this implies that f \u2032( \u222b\n\u03b8\u03c0\u03b3(d\u03b8))=0, i.e., the averaged recursion converges pathwise to \u03b8\u0304\u03b3 = \u222b\n\u03b8\u03c0\u03b3(d\u03b8) which coincides with the optimal value \u03b8\u2217 (defined through f \u2032(\u03b8\u2217)=0). When the gradient f \u2032 is no longer linear, then \u222b f \u2032(\u03b8)\u03c0\u03b3(d\u03b8) 6= f \u2032( \u222b\n\u03b8\u03c0\u03b3(d\u03b8)). Therefore, for general M -estimation problems we should expect that the averaged sequence still converges at rate O(1/n) to the mean of the stationary distribution \u03b8\u0304\u03b3 , but not to the optimal predictor \u03b8\u2217. Typically, the\naverage distance between \u03b8n and \u03b8\u2217 is of order \u03b3 (see Section 4 and [20]), while for the averaged iterates that converge pointwise to \u03b8\u0304\u03b3 , it is of order \u03b3\n2 for strongly convex problems under some additional smoothness conditions on the loss functions (these are satisfied, for example, by the logistic loss [21]).\nSince quadratic functions may be optimized with rate O(1/n) under weak conditions, we are going to use a quadratic approximation around a well chosen support point, which shares some similarity with the Newton procedure (however, with a non trivial adaptation to the stochastic approximation framework). The Newton step for f around a certain point \u03b8\u0303 is equivalent to minimizing a quadratic surrogate g of f around \u03b8\u0303, i.e., g(\u03b8) = f(\u03b8\u0303) + \u3008f \u2032(\u03b8\u0303), \u03b8 \u2212 \u03b8\u0303\u3009 + 12 \u3008\u03b8 \u2212 \u03b8\u0303, f \u2032\u2032(\u03b8\u0303)(\u03b8 \u2212 \u03b8\u0303)\u3009. If fn(\u03b8) def = \u2113(yn, \u3008\u03b8, xn\u3009), then g(\u03b8) = Egn(\u03b8), with gn(\u03b8) = f(\u03b8\u0303) + \u3008f \u2032n(\u03b8\u0303), \u03b8 \u2212 \u03b8\u0303\u3009 + 12 \u3008\u03b8 \u2212 \u03b8\u0303, f \u2032\u2032n (\u03b8\u0303)(\u03b8 \u2212 \u03b8\u0303)\u3009; the Newton step may thus be solved approximately with stochastic approximation (here constant-step size LMS), with the following recursion:\n\u03b8n = \u03b8n\u22121 \u2212 \u03b3g\u2032n(\u03b8n\u22121) = \u03b8n\u22121 \u2212 \u03b3 [ f \u2032n(\u03b8\u0303) + f \u2032\u2032 n (\u03b8\u0303)(\u03b8n\u22121 \u2212 \u03b8\u0303) ] . (7)\nThis is equivalent to replacing the gradient f \u2032n(\u03b8n\u22121) by its first-order approximation around \u03b8\u0303. A crucial point is that for machine learning scenarios where fn is a loss associated to a single data point, its complexity is only twice the complexity of a regular stochastic approximation step, since, with fn(\u03b8) = \u2113(yn, \u3008xn, \u03b8\u3009), f \u2032\u2032n (\u03b8) is a rank-one matrix.\nChoice of support points for quadratic approximation. An important aspect is the choice of the support point \u03b8\u0303. In this paper, we consider two strategies: \u2013 Two-step procedure: for convex losses, averaged SGD with a step-size decaying at O(1/ \u221a n)\nachieves a rate (up to logarithmic terms) of O(1/ \u221a n) [5, 6]. We may thus use it to obtain a first decent estimate. The two-stage procedure is as follows (and uses 2n observations): n steps of averaged SGD with constant step size \u03b3 \u221d 1/\u221an to obtain \u03b8\u0303, and then averaged LMS for the Newton step around \u03b8\u0303. As shown below, this algorithm achieves the rate O(1/n) for logistic regression. However, it is not the most efficient in practice.\n\u2013 Support point = current average iterate: we simply consider the current averaged iterate \u03b8\u0304n\u22121 as the support point \u03b8\u0303, leading to the recursion:\n\u03b8n = \u03b8n\u22121 \u2212 \u03b3 [ f \u2032n(\u03b8\u0304n\u22121) + f \u2032\u2032 n (\u03b8\u0304n\u22121)(\u03b8n\u22121 \u2212 \u03b8\u0304n\u22121) ] . (8)\nAlthough this algorithm has shown to be the most efficient in practice (see Section 4) we currently have no proof of convergence. Given that the behavior of the algorithms does not change much when the support point is updated less frequently than each iteration, there may be some connections to two-time-scale algorithms (see, e.g., [22]). In Section 4, we also consider several other strategies based on doubling tricks.\nInterestingly, for non-quadratic functions, our algorithm imposes a new bias (by replacing the true gradient by an approximation which is only valid close to \u03b8\u0304n\u22121) in order to reach faster convergence (due to the linearity of the underlying gradients).\nRelationship with one-step-estimators. One-step estimators (see, e.g., [23]) typically take any estimator with O(1/n)-convergence rate, and make a full Newton step to obtain an efficient estimator (i.e., one that achieves the Cramer-Rao lower bound). Although our novel algorithm is largely inspired by one-step estimators, our situation is slightly different since our first estimator has only convergence rate O(n\u22121/2) and is estimated on different observations.\n3.1 Self-concordance and logistic regression\nWe make the following assumptions:\n(B1) H is a d-dimensional Euclidean space, with d > 1.\n(B2) The observations (xn, yn) \u2208 H\u00d7 {\u22121, 1} are independent and identically distributed.\n(B3) We consider f(\u03b8) = E [ \u2113(yn, \u3008xn, \u03b8\u3009) ]\n, with the following assumption on the loss function \u2113 (whenever we take derivatives of \u2113, this will be with respect to the second variable):\n\u2200(y, y\u0302) \u2208 {\u22121, 1} \u00d7 R, \u2113\u2032(y, y\u0302) 6 1, \u2113\u2032\u2032(y, y\u0302) 6 1/4, |\u2113\u2032\u2032\u2032(y, y\u0302)| 6 \u2113\u2032\u2032(y, y\u0302).\nWe denote by \u03b8\u2217 a global minimizer of f , which we thus assume to exist, and we denote by H = f \u2032\u2032(\u03b8\u2217) the Hessian operator at a global optimum \u03b8\u2217.\n(B4) We assume that there exists R > 0, \u03ba > 0 and \u03c1 > 0 such that \u2016xn\u20162 6 R2 almost surely, and\nE [ xn \u2297 xn ] 4 \u03c1E [ \u2113\u2032\u2032(yn, \u3008\u03b8\u2217, xn\u3009)xn \u2297 xn ] = \u03c1H, (9)\n\u2200z \u2208 H, \u03b8 \u2208 H, E [ \u2113\u2032\u2032(yn, \u3008\u03b8, xn\u3009)2\u3008z, xn\u30094 ] 6 \u03ba ( E [ \u2113\u2032\u2032(yn, \u3008\u03b8, xn\u3009)\u3008z, xn\u30092 ])2 . (10)\nAssumption (B3) is satisfied for the logistic loss and extends to all generalized linear models (see more details in [21]), and the relationship between the third derivative and second derivative of the loss \u2113 is often referred to as self-concordance (see [24, 25] and references therein). Note moreover that we must have \u03c1 > 4 and \u03ba > 1.\nA loose upper bound for \u03c1 is 1/ infn \u2113 \u2032\u2032(yn, \u3008\u03b8\u2217, xn\u3009) but in practice, it is typically much smaller (see Table 1). The condition in Eq. (10) is hard to check because it is uniform in \u03b8. With a slightly more complex proof, we could restrict \u03b8 to be close to \u03b8\u2217; with such constraints, the value of \u03ba we have found is close to the one from Section 2.2 (i.e., without the terms in \u2113\u2032\u2032(yn, \u3008\u03b8, xn\u3009)).\nTheorem 3 Assume (B1-4), and consider the vector \u03b6n obtained as follows: (a) perform n steps of averaged stochastic gradient descent with constant step size 1/2R2 \u221a n, to get \u03b8\u0303n, and (b) perform n step of averaged LMS with constant step-size 1/R2 for the quadratic approximation of f around \u03b8\u0303n. If n > (19 + 9R\u2016\u03b80 \u2212 \u03b8\u2217\u2016)4, then\nEf(\u03b6n)\u2212 f(\u03b8\u2217) 6 \u03ba3/2\u03c13d\nn (16R\u2016\u03b80 \u2212 \u03b8\u2217\u2016+ 19)4. (11)\nWe get an O(1/n) convergence rate without assuming strong convexity, even locally, thus improving on results from [21] where the the rate is proportional to 1/(n\u03bbmin(H)). The proof relies on selfconcordance properties and the sharp analysis of the Newton step (see appendix).\n4 Experiments\n4.1 Synthetic data\nLeast-mean-square algorithm. We consider normally distributed inputs, with covariance matrix H that has random eigenvectors and eigenvalues 1/k, k = 1, . . . , d. The outputs are generated from a linear function with homoscedastic noise with unit signal to noise-ratio. We consider d = 20 and the least-mean-square algorithm with several settings of the step size \u03b3n, constant or proportional to 1/ \u221a n. Here R2 denotes the average radius of the data, i.e., R2 = trH . In the left plot of Figure 1, we show the results, averaged over 10 replications.\nWithout averaging, the algorithm with constant step-size does not converge pointwise (it oscillates), and its average excess risk decays as a linear function of \u03b3 (indeed, the gap between each values of the constant step-size is close to log10(4), which corresponds to a linear function in \u03b3).\nWith averaging, the algorithm with constant step-size does converge at rate O(1/n), and for all values of the constant \u03b3, the rate is actually the same. Moreover (although it is not shown in the plots), the standard deviation is much lower.\nWith decaying step-size \u03b3n = 1/(2R 2 \u221a n) and without averaging, the convergence rate isO(1/ \u221a n),\nand improves to O(1/n) with averaging.\nLogistic regression. We consider the same input data as for least-squares, but now generates outputs from the logistic probabilistic model. We compare several algorithms and display the results in Figure 1 (middle and right plots).\nOn the middle plot, we consider SGD. Without averaging, the algorithm with constant step-size does not converge and its average excess risk reaches a constant value which is a linear function of \u03b3 (indeed, the gap between each values of the constant step-size is close to log10(4)). With averaging, the algorithm does converge, but as opposed to least-squares, to a point which is not the optimal solution, with an error proportional to \u03b32 (the gap between curves is twice as large).\nOn the right plot, we consider various variations of our Newton-approximation scheme. The \u201c2-step\u201d algorithm is the one for which our convergence rate holds (n being the total number of examples, we perform n/2 steps of averaged SGD, then n/2 steps of LMS). Not surprisingly, it is not the best in practice (in particular at n/2, when starting the constant-size LMS, the performance worsens temporarily). It is classical to use doubling tricks to remedy this problem while preserving convergence rates [26], this is done in \u201c2-step-dbl.\u201d, which avoids the previous erratic behavior.\nWe have also considered getting rid of the first stage where plain averaged stochastic gradient is used to obtain a support point for the quadratic approximation. We now consider only Newton-steps but change only these support points. We consider updating the support point at every iteration, i.e., the recursion from Eq. (8), while we also consider updating it every dyadic point (\u201cdbl.-approx\u201d). The last two algorithms perform very similarly and achieve the O(1/n) early. In all experiments on real data, we have considered the simplest variant (which corresponds to Eq. (8)).\n4.2 Standard benchmarks\nWe have considered 6 benchmark datasets which are often used in comparing large-scale optimization methods. The datasets are described in Table 1 and vary in values of d, n and sparsity levels. These are all finite binary classification datasets with outputs in {\u22121, 1}. For least-squares and logistic regression, we have followed the following experimental protocol: (1) remove all outliers (i.e., sample points xn whose norm is greater than 5 times the average norm), (2) divide the dataset in two equal parts, one for training, one for testing, (3) sample within the training dataset with replacement, for 100 times the number of observations in the training set (this corresponds to 100 effective passes; in all plots, a black dashed line marks the first effective pass), (4) compute averaged cost on training and testing data (based on 10 replications). All the costs are shown in log-scale, normalized to that the first iteration leads to f(\u03b80)\u2212 f(\u03b8\u2217) = 1.\nAll algorithms that we consider (ours and others) have a step-size, and typically a theoretical value that ensures convergence. We consider two settings: (1) one when this theoretical value is used, (2) one with the best testing error after one effective pass through the data (testing powers of 4 times the theoretical step-size).\nHere, we only consider covertype, alpha, sido and news, as well as test errors. For all training errors and the two other datasets (quantum, rcv1 ), see the appendix.\nLeast-squares regression. We compare three algorithms: averaged SGD with constant step-size, averaged SGD with step-size decaying as C/R2 \u221a n, and the stochastic averaged gradient (SAG) method which is dedicated to finite training data sets [27], which has shown state-of-the-art performance in this set-up1. We show the results in the two left plots of Figure 2 and Figure 3.\nAveraged SGD with decaying step-size equal to C/R2 \u221a n is slowest (except for sido). In particular, when the best constant C is used (right columns), the performance typically starts to increase significantly. With that step size, even after 100 passes, there is no sign of overfitting, even for the high-dimensional sparse datasets.\nSAG and constant-step-size averaged SGD exhibit the best behavior, for the theoretical stepsizes and the best constants, with a significant advantage for constant-step-size SGD. The non-sparse datasets do not lead to overfitting, even close to the global optimum of the (unregularized) training objectives, while the sparse datasets do exhibit some overfitting after more than 10 passes.\nLogistic regression. We also compare two additional algorithms: our Newton-based technique and \u201cAdagrad\u201d [7], which is a stochastic gradient method with a form a diagonal scaling2 that allows to reduce the convergence rate (which is still in theory proportional to O(1/ \u221a n)). We show results in the two right plots of Figure 2 and Figure 3. Averaged SGD with decaying step-size proportional to 1/R2 \u221a n has the same behavior than for least-squares (step-size harder to tune, always inferior performance except for sido). SAG, constant-step-size SGD and the novel Newton technique tend to behave similarly (good with theoretical step-size, always among the best methods). They differ notably in some aspects: (1) SAG converges quicker for the training errors (shown in the appendix) while it is a bit slower for the testing error, (2) in some instances, constant-step-size averaged SGD does underfit (covertype, alpha, news), which is consistent with the lack of convergence to the global optimum mentioned earlier, (3) the novel Newton approximation is consistently better.\nOn the non-sparse datasets, Adagrad performs similarly to the Newton-type method (often better in early iterations and worse later), except for the alpha dataset where the step-size is harder to tune (the best step-size tends to have early iterations that make the cost go up significantly). On\n1The original algorithm from [27] is considering only strongly convex problems, we have used the step-size of 1/16R2, which achieves fast convergence rates in all situations (see http://research.microsoft.com/en-us/um/cambridge/events/mls2013/downloads/stochastic gradient.pdf).\n2Since a bound on \u2016\u03b8\u2217\u2016 is not available, we have used step-sizes proportional to 1/ supn \u2016xn\u2016\u221e.\nsparse datasets like rcv1, the performance is essentially the same as Newton. On the sido data set, Adagrad (with fixed steps size, left column) achieves a good testing loss quickly then levels off, for reasons we cannot explain. On the news dataset, it is inferior without parameter-tuning and a bit better with. Adagrad uses a diagonal rescaling; it could be combined with our technique, early experiments show that it improves results but that it is more sensitive to the choice of step-size.\nOverall, even with d and \u03ba very large (where our bounds are vacuous), the performance of our algorithm still achieves the state of the art, while being more robust to the selection of the step-size: finer quantities likes degrees of freedom [13] should be able to quantify more accurately the quality of the new algorithms.\n5 Conclusion\nIn this paper, we have presented two stochastic approximation algorithms that can achieve rates of O(1/n) for logistic and least-squares regression, without strong-convexity assumptions. Our analysis reinforces the key role of averaging in obtaining fast rates, in particular with large step-sizes. Our work can naturally be extended in several ways: (a) an analysis of the algorithm that updates the support point of the quadratic approximation at every iteration, (b) proximal extensions (easy to implement, but potentially harder to analyze); (c) adaptive ways to find the constant-step-size; (d) step-sizes that depend on the iterates to increase robustness, like in normalized LMS [19], and (e) non-parametric analysis to improve our theoretical results for large values of d.\nAcknowledgements\nThis work was partially supported by the European Research Council (SIERRA Project 239993). The authors would like to thank Simon Lacoste-Julien and Mark Schmidt for discussions related to this work.\nIn the appendix we provide proofs of all three theorems, as well as additional experimental results (all training objectives, and two additional datasets quantum and rcv1 ).\nNotations. Throughout this appendix material, we are going to use the notation \u2016X\u2016p = [ E(\u2016X\u2016p) ]1/p for any random vector X and real number p > 1. By Minkowski\u2019s inequality, we have the triangle inequality \u2016X + Y \u2016p 6 \u2016X\u2016p + \u2016Y \u2016p whenever the expression makes sense.\nA Proof of Theorem 1\nWe first denote by \u03b7n = \u03b8n \u2212 \u03b8\u2217 \u2208 H the deviation to \u03b8\u2217. Since we consider quadratic functions, it satisfies a simplified recursion:\n\u03b7n = \u03b7n\u22121 \u2212 \u03b3(xn \u2297 xn)\u03b8n + \u03b3\u03ben = ( I \u2212 \u03b3xn \u2297 xn ) \u03b7n\u22121 + \u03b3\u03ben. (12)\nWe also consider \u03b7\u0304n = 1\nn+1 \u2211n k=0 \u03b7k = \u03b8\u0304n \u2212 \u03b8\u2217 the averaged iterate. We have f(\u03b8n) \u2212 f(\u03b8\u2217) =\n1 2 \u3008\u03b7n, H\u03b7n\u3009 and f(\u03b8\u0304n)\u2212 f(\u03b8\u2217) = 12 \u3008\u03b7\u0304n, H\u03b7n\u3009.\nThe crux of the proof is to consider the same recursion as Eq. (12), but replacing xn \u2297 xn by its expectation H (which is related to fixed design analysis in linear regression). This is of course only an approximation, and thus one has to study the remainder term; it happens to satisfy a similar recursion, on which we can apply the same technique, and so on. This proof technique is taken from [14]. Here we push it to arbitrary orders with explicit constants for averaged constant-step-size stochastic gradient descent.\nConsequences of assumptions. Note that Assumption (A6) implies that E\u2016xn\u20162 6 R2 (indeed, taking the trace of E ( \u2016xn\u20162xn \u2297 xn )\n4 R2H , we get E\u2016xn\u20164 6 R2E\u2016xn\u20162, and we always have by Cauchy-Schwarz inequality, E\u2016xn\u20162 6 \u221a E\u2016xn\u20164 6 R \u221a\nE\u2016xn\u20162). This then implies that trH 6 R2 and thus H 4 (trH)I 4 R2I. Thus, whenever \u03b3 6 1/R2, we have \u03b3H 4 I, for the order between positive definite matrices.\nWe denote by Fn the \u03c3-algebra generated by (x1, z1, . . . , xn, zn). Both \u03b8n and \u03b8\u0304n are Fnmeasurable.\nA.1 Two main lemmas\nThe proof relies on two lemmas, one that provides a weak result essentially equivalent (but more specific and simpler because the step-size is constant) to non-strongly-convex results from [6], and one that replaces xn\u2297xn by its expectation H in Eq. (12), which may then be seen as a non-asymptotic counterpart to the similar set-tup in [2].\nLemma 1 Assume (xn, \u03ben) \u2208 H\u00d7H are Fn-measurable for a sequence of increasing \u03c3-fields (Fn), n > 1. Assume that E[\u03ben|Fn\u22121] = 0, E [ \u2016\u03ben\u20162|Fn\u22121 ] is finite and E [( \u2016xn\u20162xn \u2297 xn ) |Fn\u22121 ] 4 R2H, with E [ xn \u2297 xn \u2223\n\u2223Fn\u22121] = H for all n > 1, for some R > 0 and invertible operator H. Consider the recursion \u03b1n = ( I \u2212 \u03b3xn \u2297 xn ) \u03b1n\u22121 + \u03b3\u03ben, with \u03b3R2 6 1. Then:\n(1\u2212 \u03b3R2)E [ \u3008\u03b1\u0304n\u22121, H\u03b1\u0304n\u22121\u3009 ] + 1\n2n\u03b3 E\u2016\u03b1n\u20162 6\n1\n2n\u03b3 \u2016\u03b10\u20162 +\n\u03b3\nn\nn \u2211\nk=1\nE\u2016\u03bek\u20162.\nProof. We follow the proof technique of [6] (which relies only on smoothness) and get:\n\u2016\u03b1n\u20162 = \u2016\u03b1n\u22121\u20162 + \u03b32\u2016\u03ben \u2212 (xn \u2297 xn)\u03b1n\u22121\u20162 + 2\u03b3\u3008\u03b1n\u22121, \u03ben \u2212 (xn \u2297 xn)\u03b1n\u22121\u3009\n6 \u2016\u03b1n\u22121\u20162 + { 2\u03b32\u2016\u03ben\u20162 + 2\u03b32\u2016(xn \u2297 xn)\u03b1n\u22121\u20162 } + 2\u03b3\u3008\u03b1n\u22121, \u03ben \u2212 (xn \u2297 xn)\u03b1n\u22121\u3009.\nBy taking expectations, we obtain:\nE [ \u2016\u03b1n\u20162|Fn\u22121 ] 6 \u2016\u03b1n\u22121\u20162 + 2\u03b32\u2016\u03ben\u20162 + 2\u03b32\u3008\u03b1n\u22121,E [ \u2016xn\u20162xn \u2297 xn ] \u03b1n\u22121\u3009 \u2212 2\u03b3\u3008\u03b1n\u22121, H\u03b1n\u22121\u3009 6 \u2016\u03b1n\u22121\u20162 + 2\u03b32\u2016\u03ben\u20162 + 2\u03b32R2\u3008\u03b1n\u22121, H\u03b1n\u22121\u3009 \u2212 2\u03b3\u3008\u03b1n\u22121, H\u03b1n\u22121\u3009 = \u2016\u03b1n\u22121\u20162 + 2\u03b32\u2016\u03ben\u20162 + 2\u03b32R2\u3008\u03b1n\u22121, H\u03b1n\u22121\u3009 \u2212 2\u03b3\u3008\u03b1n\u22121, H\u03b1n\u22121\u3009 6 \u2016\u03b1n\u22121\u20162 + 2\u03b32\u2016\u03ben\u20162 \u2212 2\u03b3(1\u2212 \u03b3R2)\u3008\u03b1n\u22121, H\u03b1n\u22121\u3009.\nBy taking another expectation, we get\nE\u2016\u03b1n\u20162 6 E\u2016\u03b1n\u22121\u20162 + 2\u03b32E\u2016\u03ben\u20162 \u2212 2\u03b3(1\u2212 \u03b3R2)E\u3008\u03b1n\u22121, H\u03b1n\u22121\u3009.\nThis leads to the desired result, because, by convexity, \u3008\u03b1\u0304n\u22121, H\u03b1\u0304n\u22121\u3009 6 1n \u2211n\u22121 k=0 \u3008\u03b1k, H\u03b1k\u3009.\nLemma 2 Assume \u03ben \u2208 H is Fn-measurable for a sequence of increasing \u03c3-fields (Fn), n > 1. Assume E[\u03ben|Fn\u22121] = 0, E [ \u2016\u03ben\u20162 ] is finite, and for all n > 1, E [ \u03ben \u2297 \u03ben ] 4 C. Consider the recursion \u03b1n = ( I \u2212 \u03b3H ) \u03b1n\u22121 + \u03b3\u03ben, with \u03b3H 4 I for some invertible H. Then:\nE[\u03b1n \u2297 \u03b1n] = (I \u2212 \u03b3H)n\u03b10 \u2297 \u03b10(I \u2212 \u03b3H)n + \u03b32 n \u2211\nk=1\n(I \u2212 \u03b3H)n\u2212kC(I \u2212 \u03b3H)n\u2212k, (13)\nE [ \u3008\u03b1\u0304n\u22121, H\u03b1\u0304n\u22121\u3009 ] 6 1\nn\u03b3 \u2016\u03b10\u20162 +\ntrCH\u22121\nn . (14)\nProof. The proof relies on the fact that cost functions are quadratic and our recursions are thus linear, allowing to obtain \u03b1n in closed form. The sequence (\u03b1n) satisfies a linear recursion, from which we get, for all n > 1:\n\u03b1n = (I \u2212 \u03b3H)n\u03b10 + \u03b3 n \u2211\nk=1\n(I \u2212 \u03b3H)n\u2212k\u03bek,\nwhich leads to the first result using classical martingale second moment expansions (which amount to considering \u03bei, i = 1, . . . , n independent, so that the variance of the sum is the sum of variances). Moreover, using the identity \u2211n\u22121\nk=0 (I \u2212 \u03b3H)k = ( I \u2212 (I \u2212 \u03b3H)n )(\n\u03b3H )\u22121 , we get:\n\u03b1\u0304n\u22121 = 1\nn\nn\u22121 \u2211\nk=0\n(I \u2212 \u03b3H)k\u03b10 + \u03b3\nn\nn\u22121 \u2211\nk=1\nk \u2211\nj=1\n(I \u2212 \u03b3H)k\u2212j\u03bej\n= 1\nn\n( I \u2212 (I \u2212 \u03b3H)n )( \u03b3H )\u22121 \u03b10 + \u03b3\nn\nn\u22121 \u2211\nk=1\nk \u2211\nj=1\n(I \u2212 \u03b3H)k\u2212j\u03bej\n= 1\nn\n( I \u2212 (I \u2212 \u03b3H)n )( \u03b3H )\u22121 \u03b10 + \u03b3\nn\nn\u22121 \u2211\nj=1\n( n\u22121 \u2211\nk=j\n(I \u2212 \u03b3H)k\u2212j )\n\u03bej\n= 1\nn\n( I \u2212 (I \u2212 \u03b3H)n )( \u03b3H )\u22121 \u03b10 + \u03b3\nn\nn\u22121 \u2211\nj=1\n( n\u22121\u2212j \u2211\nk=0\n(I \u2212 \u03b3H)k )\n\u03bej\n= 1\nn\n( I \u2212 (I \u2212 \u03b3H)n )( \u03b3H )\u22121 \u03b10 + \u03b3\nn\nn\u22121 \u2211\nj=1\n( I \u2212 (I \u2212 \u03b3H)n\u2212j )( \u03b3H )\u22121\n\u03bej .\nWe then get, using standard martingale square moment inequalities (which here also amount to considering \u03bei, i = 1, . . . , n independent, so that the variance of the sum is the sum of variances):\nE\u3008\u03b1\u0304n\u22121, H\u03b1\u0304n\u22121\u3009 = 1\nn\u03b3 \u3008\u03b10,\n[ I \u2212 (I \u2212 \u03b3H)n ]2( n\u03b3H )\u22121 \u03b10\u3009\n+ 1\nn2\nn\u22121 \u2211\nj=1\ntr ( I \u2212 (I \u2212 \u03b3H)n\u2212j )2 H\u22121C\n6 1\nn\u03b3 \u2016\u03b10\u20162 +\n1 n trH\u22121C,\nbecause for all u \u2208 [0, 1], (1\u2212(1\u2212u) n)2\nnu 6 1 (see Lemma 3 in Section A.6), and the second term is the sum of terms which are all less than trH\u22121C.\nNote that we may replace the term 1n\u03b3 \u2016\u03b10\u20162 by 1 n2\u03b32 \u3008\u03b10, H\u22121\u03b10\u3009, which is only interesting when\n\u3008\u03b10, H\u22121\u03b10\u3009 is small.\nA.2 Proof principle\nThe proof relies on an expansion of \u03b7n and \u03b7\u0304n\u22121 as polynomials in \u03b3 due to [14]. This expansion is done separately for the noise process (i.e., when assuming \u03b70 = 0) and for the noise-free process that depends only on the initial conditions (i.e., when assuming that \u03c3 = 0). The bounds may then be added.\nIndeed, we have \u03b7n = M n 1 \u03b70 + \u03b3 \u2211n k=1 M n k+1\u03bek, with M j i = (I \u2212 \u03b3xj \u2297 xj) \u00b7 \u00b7 \u00b7 (I \u2212 \u03b3xi \u2297\nxi) and M i\u22121 i = I, and thus \u03b7\u0304n = 1 n+1 \u2211n i=0\n[\nM i1\u03b70 + \u03b3 \u2211i k=1 M i k+1\u03bek\n]\n= 1n+1 \u2211n i=0 M i 1\u03b70 +\n\u03b3 \u2211n\nk=1\n(\n\u2211n i=k M i k+1\n)\n\u03bek, leading to\n\u2016H1/2\u03b7\u0304n\u2016p 6 \u2225 \u2225 \u2225\n\u2225\n1\nn+ 1\nn \u2211\ni=0\nM j1\u03b70\n\u2225 \u2225 \u2225 \u2225\np\n+\n\u2225 \u2225 \u2225 \u2225 \u03b3 n \u2211\nk=1\n( n \u2211\ni=k\nM ik+1\n)\n\u03bek\n\u2225 \u2225 \u2225 \u2225\np\n,\nfor any p > 2 for which it is defined: the left term depends only on initial conditions and the right term depends only on the noise process (note the similarity with bias-variance decompositions).\nA.3 Initial conditions\nIn this section, we assume that \u03ben is uniformly equal to zero, and that \u03b3R 2 6 1.\nWe thus have \u03b7n = (I \u2212 \u03b3xn \u2297 xn)\u03b7n\u22121 and thus \u2016\u03b7n\u20162 = \u2016\u03b7n\u22121\u20162 \u2212 2\u03b3\u3008\u03b7n\u22121, (xn \u2297 xn)\u03b7n\u22121\u3009+ \u03b32\u3008\u03b7n\u22121, (xn \u2297 xn)2\u03b7n\u22121\u3009.\nBy taking expectations (first given Fn\u22121, then unconditionally), we get: E\u2016\u03b7n\u20162 6 E\u2016\u03b7n\u22121\u20162 \u2212 2\u03b3E\u3008\u03b7n\u22121, H\u03b7n\u22121\u3009+ \u03b32R2E\u3008\u03b7n\u22121, H\u03b7n\u22121\u3009 using E\u2016xn\u20162xn \u2297 xn 4 R2H, 6 E\u2016\u03b7n\u22121\u20162 \u2212 \u03b3E\u3008\u03b7n\u22121, H\u03b7n\u22121\u3009 using \u03b3R2 6 1, from which we obtain, by summing from 1 to n and using convexity (note that Lemma 1 could be used directly as well):\nE\u3008\u03b7\u0304n\u22121, H\u03b7\u0304n\u22121\u3009 6 \u2016\u03b70\u20162 n\u03b3 .\nHere, it would be interesting to explore conditions under which the initial conditions may be forgotten at a rate O(1/n2), as obtained by [6] in the strongly convex case.\nA.4 Noise process\nIn this section, we assume that \u03b70 = \u03b80\u2212\u03b8\u2217 = 0 and \u03b3R2 6 1 (which implies \u03b3H 4 I). Following [14], we recursively define the sequences (\u03b7rn)n>0 for r > 0 (and their averaged counterparts \u03b7\u0304 r n):\n\u2013 The sequence (\u03b70n) is defined as \u03b7 0 0 = \u03b70 = 0 and for n > 1, \u03b7 0 n = (I \u2212 \u03b3H)\u03b70n\u22121 + \u03b3\u03ben.\n\u2013 The sequence (\u03b7rn) is defined from (\u03b7 r\u22121 n ) as \u03b7 r 0 = 0 and, for all n > 1:\n\u03b7rn = (I \u2212 \u03b3H)\u03b7rn\u22121 + \u03b3(H \u2212 xn \u2297 xn)\u03b7r\u22121n\u22121. (15)\nRecursion for expansion. We now show that the sequence \u03b7n \u2212 \u2211r i=0 \u03b7 i n then satisfies the following recursion, for any r > 0 (which is of the same type than (\u03b7n)):\n\u03b7n \u2212 r \u2211\ni=0\n\u03b7in = (I \u2212 \u03b3xn \u2297 xn) ( \u03b7n\u22121 \u2212 r \u2211\ni=0\n\u03b7rn\u22121\n)\n+ \u03b3(H \u2212 xn \u2297 xn)\u03b7rn\u22121. (16)\nIn order to prove Eq. (16) by recursion, we have, for r = 0,\n\u03b7n \u2212 \u03b70n = (I \u2212 \u03b3xn \u2297 xn)\u03b7n\u22121 \u2212 (I \u2212 \u03b3H)\u03b70n\u22121 = (I \u2212 \u03b3xn \u2297 xn)(\u03b7n\u22121 \u2212 \u03b70n\u22121) + \u03b3(H \u2212 xn \u2297 xn)\u03b70n\u22121,\nand, to go from r to r + 1:\n\u03b7n \u2212 r+1 \u2211\ni=0\n\u03b7in = (I \u2212 \u03b3xn \u2297 xn) ( \u03b7n\u22121 \u2212 r \u2211\ni=0\n\u03b7in\u22121\n)\n+ \u03b3(H \u2212 xn \u2297 xn)\u03b7rn\u22121\n\u2212(I \u2212 \u03b3H)\u03b7r+1n\u22121 \u2212 \u03b3(H \u2212 xn \u2297 xn)\u03b7rn\u22121\n= (I \u2212 \u03b3xn \u2297 xn) ( \u03b7n\u22121 \u2212 r+1 \u2211\ni=0\n\u03b7in\u22121\n)\n+ \u03b3(H \u2212 xn \u2297 xn)\u03b7r+1n\u22121.\nBound on covariance operators. We now show that we also have a bound on the covariance operator of \u03b7rn\u22121, for any r > 0 and n > 2:\nE [ \u03b7rn\u22121 \u2297 \u03b7rn\u22121 ] 4 \u03b3r+1R2r\u03c32I. (17)\nIn order to prove Eq. (17) by recursion, we get for r = 0:\nE [ \u03b70n\u22121 \u2297 \u03b70n\u22121 ]\n4 \u03b32\u03c32 n\u22121 \u2211\nk=1\n(I \u2212 \u03b3H)2n\u22122\u22122kH\n4 \u03b32\u03c32 ( I \u2212 (I \u2212 \u03b3H)2n\u22122 )( I \u2212 (I \u2212 \u03b3H)2 )\u22121 H\n= \u03b32\u03c32 ( I \u2212 (I \u2212 \u03b3H)2n\u22122 )( 2\u03b3H \u2212 \u03b32H2 )\u22121 H\n4 \u03b32\u03c32 ( I \u2212 (I \u2212 \u03b3H)2n\u22122 )( \u03b3H )\u22121 H 4 \u03b3\u03c32I.\nIn order to go from r to r + 1, we have, using Lemma 2 and the fact that \u03b7rk\u22121 and xk are independent:\nE [ \u03b7r+1n\u22121 \u2297 \u03b7r+1n\u22121 ]\n4 \u03b32E\n[ n\u22121 \u2211\nk=1\n(I \u2212 \u03b3H)n\u22121\u2212k(H \u2212 xk \u2297 xk)E [ \u03b7rk\u22121 \u2297 \u03b7rk\u22121 ] (H \u2212 xk \u2297 xk)(I \u2212 \u03b3H)n\u22121\u2212k ]\n4 \u03b3r+3R2r\u03c32E\n[ n\u22121 \u2211\nk=1\n(I \u2212 \u03b3H)n\u22121\u2212k(H \u2212 xk \u2297 xk)2(I \u2212 \u03b3H)n\u22121\u2212k ] using the result for r,\n4 \u03b3r+3R2r+2\u03c32 n\u22121 \u2211\nk=1\n(I \u2212 \u03b3H)2n\u22122\u22122kH using E(xk \u2297 xk \u2212H)2 4 E\u2016xk\u20162xk \u2297 xk 4 R2H,\n4 \u03b3r+2R2r+2\u03c32I.\nPutting things together. We may apply Lemma 1 to the sequence ( \u03b7n \u2212 \u2211r i=0 \u03b7 i n ) , to get\nE\n\u2329 \u03b7\u0304n\u22121 \u2212 r \u2211\ni=0\n\u03b7\u0304in\u22121, H ( \u03b7\u0304n\u22121 \u2212 r \u2211\ni=0\n\u03b7\u0304in\u22121 )\n\u232a\n6 1 1\u2212 \u03b3R2 \u03b3 n\nn \u2211\nk=2\nE\u2016(H \u2212 xk \u2297 xk)\u03b7rk\u22121\u20162\n6 1\n1\u2212 \u03b3R2 \u03b3 r+2\u03c32R2r+4.\nWe may now apply Lemma 2 to Eq. (15), to get, with a noise process \u03bern = (H \u2212 xn \u2297 xn)\u03b7r\u22121n\u22121 which is such that\nE [ \u03bern \u2297 \u03bern ] 4 \u03b3rR2r\u03c32H,\nE\u3008\u03b7\u0304rn\u22121, H\u03b7\u0304rn\u22121\u3009 6 1\nn \u03b3rR2rd\u03c32.\nWe thus get, using Minkowski\u2019s inequality (i.e., triangle inequality for the norms \u2016 \u00b7 \u2016p):\n( E\u3008\u03b7\u0304n\u22121, H\u03b7\u0304n\u22121\u3009 )1/2 6 ( 1\n1\u2212 \u03b3R2 \u03b3 r+2\u03c32R2r+4\n)1/2 + \u03c3 \u221a d\u221a n r \u2211\ni=0\n\u03b3i/2Ri\n6 ( 1\n1\u2212 \u03b3R2 \u03b3 r+2\u03c32R2r+4\n)1/2 + \u03c3 \u221a d\u221a n 1\u2212 ( \u221a \u03b3R2)r+1 1\u2212 \u221a \u03b3R2 .\nThis implies that for any \u03b3R2 < 1, we obtain, by letting r tend to +\u221e:\n( E\u3008\u03b7\u0304n\u22121, H\u03b7\u0304n\u22121\u3009 )1/2 6 \u03c3 \u221a d\u221a n\n1\n1\u2212 \u221a \u03b3R2 .\nA.5 Final bound\nWe can now take results from Appendices A.3 and A.4, to get\n( E\u3008\u03b7\u0304n\u22121, H\u03b7\u0304n\u22121\u3009 )1/2 6 \u03c3 \u221a d\u221a n\n1\n1\u2212 \u221a \u03b3R2 + \u2016\u03b70\u20162 n\u03b3 ,\nwhich leads to the desired result.\nA.6 Proof of Lemma 3\nIn this section, we state and prove a simple lemma.\nLemma 3 For any u \u2208 [0, 1] and n > 0, (1\u2212 (1 \u2212 u)n)2 6 nu.\nProof. Since u \u2208 [0, 1], we have, 1\u2212 (1\u2212 u)n 6 1. Moreover, n(1\u2212 u)n\u22121 6 n, and by integrating between 0 and u, we get 1 \u2212 (1 \u2212 u)n 6 nu. By multiplying the two previous inequalities, we get the desired result.\nB Proof of Theorem 2\nThroughout the proof, we use the notation forX \u2208 H a random vector, and p any real number greater than 1, \u2016X\u2016p = ( E\u2016X\u2016p )1/p\n. We first recall the Burkholder-Rosenthal-Pinelis (BRP) inequality [28, Theorem 4.1]. Let p \u2208 R, p > 2 and (Fn)n>0 be a sequence of increasing \u03c3-fields, and (xn)n>1 an adapted sequence of elements of H, such that E [ xn|Fn\u22121 ]\n= 0, and \u2016xn\u2016p is finite. Then, \u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n\u2225 \u2225 \u2225 \u2225 k \u2211\nj=1\nxj\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\np\n6 \u221a p\n\u2225 \u2225 \u2225 \u2225 n \u2211\nk=1\nE [ \u2016xk\u20162|Fk\u22121 ]\n\u2225 \u2225 \u2225 \u2225 1/2\np/2\n+ p\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n\u2016xk\u2016 \u2225 \u2225 \u2225\n\u2225\np\n(18)\n6 \u221a p\n\u2225 \u2225 \u2225 \u2225 n \u2211\nk=1\nE [ \u2016xk\u20162|Fk\u22121 ]\n\u2225 \u2225 \u2225 \u2225 1/2\np/2\n+ p\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n\u2016xk\u20162 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n.\nWe use the same notations than the proof of Theorem 1, and the same proof principle: (a) splitting the contributions of the initial conditions and the noise, (b) providing a direct argument for the initial condition, and (c) performing an expansion for the noise contribution.\nConsequences of assumptions. Note that by Cauchy-Schwarz inequality, assumption (A7) implies for all z, t \u2208 H, E\u3008z, xn\u30092\u3008t, xn\u30092 6 \u03ba\u3008z,Hz\u3009\u3008t,Ht\u3009. It in turn implies that for all positive semi-definite self-adjoint operators M,N , E\u3008xn,Mxn\u3009\u3008xn, Nxn\u3009 6 \u03ba tr(MH) tr(NH).\nB.1 Contribution of initial conditions\nWhen the noise is assumed to be zero, we have \u03b7n = (I \u2212 \u03b3xn \u2297 xn)\u03b7n\u22121 almost surely, and thus, since 0 4 \u03b3xn \u2297 xn 4 I, \u2016\u03b7n\u2016 6 \u2016\u03b70\u2016 almost surely, and\n\u2016\u03b7n\u20162 = \u2016\u03b7n\u22121\u20162 \u2212 2\u03b3\u3008\u03b7n\u22121, (xn \u2297 xn)\u03b7n\u22121\u3009+ \u03b32\u3008\u03b7n\u22121, (xn \u2297 xn)2\u03b7n\u22121\u3009 6 \u2016\u03b7n\u22121\u20162 \u2212 2\u03b3\u3008\u03b7n\u22121, (xn \u2297 xn)\u03b7n\u22121\u3009+ \u03b3\u3008\u03b7n\u22121, (xn \u2297 xn)\u03b7n\u22121\u3009\nusing \u2016xn\u20162 6 R2 and \u03b3R2 6 1, = \u2016\u03b7n\u22121\u20162 \u2212 \u03b3\u3008\u03b7n\u22121, (xn \u2297 xn)\u03b7n\u22121\u3009,\nwhich we may write as\n\u2016\u03b7n\u20162 \u2212 \u2016\u03b7n\u22121\u20162 + \u03b3\u3008\u03b7n\u22121, H\u03b7n\u22121\u3009 6 \u03b3\u3008\u03b7n\u22121, (H \u2212 xn \u2297 xn)\u03b7n\u22121\u3009 def= Mn.\nWe thus have:\nAn def = \u2016\u03b7n\u20162 + \u03b3\nn \u2211\nk=1\n\u3008\u03b7k\u22121, H\u03b7k\u22121\u3009 6 \u2016\u03b70\u20162 + n \u2211\nk=1\nMk.\nNote that we have\nE[M2n|Fn\u22121] 6 E [ \u03b32\u3008\u03b7n\u22121, (xn \u2297 xn)\u03b7n\u22121\u30092|Fn\u22121 ] 6 \u03b32R2\u2016\u03b70\u20162\u3008\u03b7n\u22121, H\u03b7n\u22121\u3009,\nand |Mn| 6 \u03b3\u2016\u03b70\u20162R2. We may now apply the Burkholder-Rosenthal-Pinelis inequality in Eq. (18), to get:\n\u2225 \u2225An \u2225 \u2225 p 6 \u2016\u03b70\u20162 +\n\u221a p\n\u2225 \u2225 \u2225 \u2225 n \u2211\nk=1\nE [ M2k |Fk\u22121 ]\n\u2225 \u2225 \u2225 \u2225 1/2\np/2\n+ p\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n|Mk| \u2225 \u2225 \u2225\n\u2225\np\n6 \u2016\u03b70\u20162 + \u03b3 \u221a p\n\u2225 \u2225 \u2225 \u2225 \u2016\u03b70\u20162R2 n \u2211\nk=1\n\u3008\u03b7k\u22121, H\u03b7k\u22121\u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n+ p\u03b3R2\u2016\u03b70\u20162\n6 \u2016\u03b70\u20162 + \u03b31/2R \u221a p\u2016\u03b70\u2016 \u2225 \u2225An \u2225 \u2225 1/2\np/2 + p\u03b3R2\u2016\u03b70\u20162\n6 \u2016\u03b70\u20162 + \u03b31/2R \u221a p\u2016\u03b70\u2016 \u2225 \u2225An \u2225 \u2225 1/2\np + p\u03b3R2\u2016\u03b70\u20162.\nWe have used above that (a) \u2211n k=1\u3008\u03b7k\u22121, H\u03b7k\u22121\u3009 6 An\u03b3 and that (b) \u2225 \u2225An \u2225 \u2225 p/2 6 \u2225 \u2225An \u2225 \u2225 p . This leads to (\n\u2225 \u2225An \u2225 \u2225 1/2 p \u2212 1 2 \u03b31/2R \u221a p\u2016\u03b70\u2016\n)2\n6 \u2016\u03b70\u20162 + 5p\n4 \u03b3R2\u2016\u03b70\u20162,\nwhich leads to \u2225\n\u2225An \u2225 \u2225 1/2 p \u2212 1 2 \u03b31/2R \u221a p\u2016\u03b70\u2016 = \u2016\u03b70\u2016\n\u221a\n1 + 5p\u03b3R2\n4 ,\n\u2016An \u2225 \u2225 p 6 \u2016\u03b70\u20162\n(\n2 + 5p\u03b3R2\n2 +\np\u03b3R2\n2\n)\n6 \u2016\u03b70\u20162(2 + 3p\u03b3R2).\nFinally, we obtain, for any p > 2\n\u2225 \u2225 \u2329 \u03b7\u0304n\u22121, H\u03b7\u0304n\u22121 \u232a\u2225 \u2225\np 6 \u2016\u03b70\u20162 n\u03b3 (2 + 3p\u03b3R2),\ni.e., by a change of variable p \u2192 p2 , for any p > 4, we get\n\u2225 \u2225H1/2\u03b7\u0304n\u22121 \u2225 \u2225\np 6\n\u2225 \u2225\u3008\u03b7\u0304n\u22121, H\u03b7\u0304n\u22121\u3009\u20161/2p/2 = \u2016\u03b70\u2016\u221a n\u03b3\n\u221a\n2 + 3p\n2 \u03b3R2.\nBy using monotonicity of norms, we get, for any p \u2208 [2, 4]:\n\u2225 \u2225H1/2\u03b7\u0304n\u22121 \u2225 \u2225\np 6\n\u2225 \u2225H1/2\u03b7\u0304n\u22121 \u2225 \u2225\n4 6 \u2016\u03b70\u2016\u221a n\u03b3 \u221a 2 + 6\u03b3R2 6 \u2016\u03b70\u2016\u221a n\u03b3 \u221a 2 + 3p\u03b3R2,\nwhich is also valid for p > 4. Note that the constants in the bound above could be improved by using a proof by recursion.\nB.2 Contribution of the noise\nWe follow the same proof technique than for Theorem 1 and consider the expansion based on the sequences (\u03b7rn)n, for r > 0. We need (a) bounds on \u03b7 0 n, (b) a recursion on the magnitude (in \u2016 \u00b7 \u2016p norm) of \u03b7rn and (c) a control of the error made in the expansions.\nBound on \u03b7\u03040n. We start by a lemma similar to Lemma 2 but for all moments. This will show a bound for the sequence \u03b7\u03040n.\nLemma 4 Assume \u03ben \u2208 H is Fn-measurable for a sequence of increasing \u03c3-fields (Fn), n > 1. Assume E[\u03ben|Fn\u22121] = 0, E [ \u2016\u03ben\u20162|Fn\u22121 ] is finite. Assume moreover that for all n > 1, E [\n\u03ben \u2297 \u03ben|Fn\u22121 ]\n4 C and \u2016\u03ben\u2016p 6 \u03c4R almost surely for some p > 2. Consider the recursion \u03b1n = ( I \u2212 \u03b3H )\n\u03b1n\u22121 + \u03b3\u03ben, with \u03b10 = 0 and \u03b3H 4 I. Let p \u2208 R, p > 2. Then:\n\u2016H1/2\u03b1\u0304n\u22121\u2016p 6 \u221a p\u221a n \u221a trCH\u22121 + \u221a \u03b3pR\u03c4\u221a n . (19)\nProof. We have, from the proof of Lemma 2:\n\u03b1\u0304n\u22121 = \u03b3\nn\nn\u22121 \u2211\nj=1\n( I \u2212 (I \u2212 \u03b3H)n\u2212j )( \u03b3H )\u22121\n\u03bej\n\u2016H1/2\u03b1\u0304n\u22121\u2016p 6 \u03b3\nn\n\u2225 \u2225 \u2225 \u2225\nn\u22121 \u2211\nj=1\n( I \u2212 (I \u2212 \u03b3H)n\u2212j )( \u03b3H )\u22121 H1/2\u03bej\n\u2225 \u2225 \u2225 \u2225\np\n6 \u03b3\nn\n\u2225 \u2225 \u2225 \u2225\nn\u22121 \u2211\nj=1\n\u03b2j\n\u2225 \u2225 \u2225 \u2225\np\n,\nwith \u03b2j = ( I \u2212 (I \u2212 \u03b3H)n\u2212j )( \u03b3H )\u22121 H1/2\u03bej . We have\nn\u22121 \u2211\nj=1\nE[\u2016\u03b2j\u20162|Fj\u22121] = n\u22121 \u2211\nj=1\ntrE [ \u03bej \u2297 \u03bej |Fj\u22121 ] H\n( I \u2212 (I \u2212 \u03b3H)n\u2212j \u03b3H )2\n6 \u03b3\u22122 n\u22121 \u2211\nj=1\nE [ \u3008\u03bej , H\u22121\u03bej\u3009|Fj\u22121 ] 6 n\u03b3\u22122 trCH\u22121,\nand\n\u2016\u03b2j\u2016p 6 \u03bbmax [( I \u2212 (I \u2212 \u03b3H)n\u2212j )( \u03b3H )\u22121 H1/2 ] \u2016\u03bej\u2016p\n6 \u03b3\u22121/2\u2016\u03bej\u2016p max u\u2208(0,1] 1\u2212 (1 \u2212 u)n\u2212j u1/2\n6 \u221a n\u2212 j\u221a \u03b3 \u2016\u03bej\u2016p 6 \u03c4R \u221a n\u221a \u03b3 ,\nusing Lemma 3 in Section A.6, and assumption (A7). Using Burkholder-Rosenthal-Pinelis inequality in Eq. (18), we then obtain\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n\u22121}\n\u2016H1/2\u03b1\u0304k\u2016 \u2225 \u2225 \u2225\n\u2225\np\n6 \u221a p\u221a n \u221a trCH\u22121 + p \u221a \u03b3\u221a n \u03c3R,\nleading to the desired result.\nBounds on \u03b70n. Following the same proof technique as above, we have\n\u03b70n = \u03b3\nn \u2211\nj=1\n(I \u2212 \u03b3H)n\u2212j\u03bej ,\nfrom which we get, for any positive semidefinite operator M such that trM = 1, using BRP\u2019s inequality:\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n\u2225 \u2225M1/2\u03b70k\u2016 \u2225 \u2225 \u2225\n\u2225\np\n6 \u221a p\u03b3\u03c3\n\u2225 \u2225 \u2225 \u2225 n \u2211\nj=1\ntrH(I \u2212 \u03b3H)n\u2212jM(I \u2212 \u03b3H)n\u2212j \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n+ p\u03b3\u03c4R\n6 \u221a p\u03b3\u03c3\n\u2225 \u2225 \u2225 \u2225 1\n\u03b3 trM\n\u2225 \u2225 \u2225 \u2225 1/2\np/2\n+ p\u03b3\u03c4R\n6 1\nR\n\u221a p\u03b3R2(\u03c3 + \u03c4 \u221a p\u03b3R2),\nleading to\nsup trM=1\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n\u2225 \u2225M1/2\u03b70k\u2016 \u2225 \u2225 \u2225\n\u2225\np\n6 1\nR\n\u221a p\u03b3R2(\u03c3 + \u03c4 \u221a p\u03b3R2). (20)\nRecursion on bounds on \u03b7rn. We introduce the following quantity to control the deviations of \u03b7 r n:\nAr = sup trM=1\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n\u2225 \u2225M1/2\u03b7rk \u2225 \u2225\n\u2225 \u2225 \u2225 \u2225\np\n.\nWe have from Eq. (20), A0 6 1 R\n\u221a p\u03b3R2(\u03c3 + \u03c4 \u221a p\u03b3R2).\nSince \u03b7rn = (I\u2212\u03b3H)\u03b7rn\u22121+\u03b3(H\u2212xn\u2297xn)\u03b7r\u22121n\u22121, for all n > 1, we have the closed form expression\n\u03b7rn = \u03b3\nn \u2211\nk=2\n(I \u2212 \u03b3H)n\u2212k(H \u2212 xk \u2297 xk)\u03b7r\u22121k\u22121,\nand we may use BRP\u2019s inequality in Eq. (18) to get, for any M such that trM = 1:\nAr 6 B + C,\nwith\nB = \u221a p\u03b3\n\u2225 \u2225 \u2225 \u2225 n \u2211\nk=2\n\u3008\u03b7r\u22121k\u22121,E [ (H \u2212 xk \u2297 xk)(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k(H \u2212 xk \u2297 xk) ] \u03b7r\u22121k\u22121\u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n6 \u221a p\u03b3\n\u2225 \u2225 \u2225 \u2225 n \u2211\nk=2\n\u3008\u03b7r\u22121k\u22121,E [ (xk \u2297 xk)(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k(xk \u2297 xk) ] \u03b7r\u22121k\u22121\u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\nusing E trN(H \u2212 xk \u2297 xk)M(H \u2212 xk \u2297 xk) 6 E trN(xk \u2297 xk)H(xk \u2297 xk),\n6 \u221a p\u03b3\n\u2225 \u2225 \u2225 \u2225 n \u2211\nk=2\n\u03ba\u3008\u03b7r\u22121k\u22121, H\u03b7r\u22121k\u22121\u3009 trH(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\nusing the kurtosis property,\n6 \u221a p\u03b3 \u221a \u03baAr\u22121 ( n \u2211\nk=2\ntrH(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k )1/2\nusing \u3008\u03b7r\u22121k\u22121, H\u03b7r\u22121k\u22121\u3009 6 sup k\u2208{1,...,n} \u3008\u03b7r\u22121k\u22121, H\u03b7r\u22121k\u22121\u3009,\n6 \u221a p\u03b3 \u221a \u03baRAr\u22121 ( 1\n\u03b3 trM\n)1/2\n= \u221a p\u03b3R2\u03baAr\u22121,\nand\nC = p\u03b3\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{2,...,n}\n\u3008\u03b7r\u22121k\u22121, (H \u2212 xk \u2297 xk)(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k(H \u2212 xk \u2297 xk)\u03b7r\u22121k\u22121\u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n6 p\u03b3\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{2,...,n}\n\u3008\u03b7r\u22121k\u22121, (xk \u2297 xk)(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k(xk \u2297 xk)\u03b7r\u22121k\u22121\u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n+p\u03b3\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{2,...,n}\n\u3008\u03b7r\u22121k\u22121, H(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212kH\u03b7r\u22121k\u22121\u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\nusing Minkowski\u2019s inequality,\n6 +p\u03b3\n( n \u2211\nk=2\nE [ \u3008\u03b7r\u22121k\u22121, (xk \u2297 xk)(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k(xk \u2297 xk)\u03b7r\u22121k\u22121\u3009p/2 ]\n)1/p\n+p\u03b3R2Ar\u22121\nbounding the supremum by a sum, and using H(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212kH 4 H2,\n6 p\u03b3\n( n \u2211\nk=2\nE [ \u3008\u03b7r\u22121k\u22121, xk\u3009p\u3008xk, (I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212kxk\u3009p/2 ]\n)1/p\n+ p\u03b3R2Ar\u22121\n6 p\u03b3RAr\u22121\n( n \u2211\nk=2\nE [ \u3008xk, (I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212kxk\u3009p/2 ]\n)1/p\n+ p\u03b3R2Ar\u22121\nby conditioning with respect to xk,\n6 p\u03b3RAr\u22121\n( n \u2211\nk=2\nE [ \u3008xk, (I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212kxk\u3009(R2 trM)p/2\u22121 ]\n)1/p\n+ p\u03b3R2Ar\u22121\nbounding \u3008xk, (I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212kxk\u3009 by R2 trM,\n6 p\u03b3R2Ar\u22121 + p\u03b3RAr\u22121\n( (R2)p/2\u22121 n \u2211\nk=2\ntrH(I \u2212 \u03b3H)n\u2212kM(I \u2212 \u03b3H)n\u2212k )1/p\n6 p\u03b3R2Ar\u22121 + p\u03b3RAr\u22121\n( (R2)p/2\u22121R2\u03b3\u22121 )1/p = p\u03b3R2Ar\u22121 + p(\u03b3R 2)1\u22121/pRAr\u22121.\nThis implies that Ar 6 A0 ( \u221a p\u03b3R2\u03ba+p\u03b3R2+p(\u03b3R2)1\u22121/p )r , which in turn implies, from Eq. (20),\nAr 6 \u221a p\u03b3R2(\u03c3 + \u03c4 \u221a p\u03b3R2) ( \u221a p\u03b3R2\u03ba+ p\u03b3R2 + p(\u03b3R2)1\u22121/p )r . (21)\nThe condition on \u03b3 will come from the requirement that \u221a p\u03b3R2\u03ba+ p\u03b3R2 + p(\u03b3R2)1\u22121/p < 1.\nBound on \u2016H1/2\u03b7\u0304rn\u22121\u2016. We have the closed-form expression:\n\u03b7\u0304rn\u22121 = \u03b3\nn\nn\u22121 \u2211\nj=2\nI \u2212 (I \u2212 \u03b3H)n\u2212j \u03b3H (H \u2212 xj \u2297 xj)\u03b7r\u22121j\u22121 ,\nleading to, using BRP\u2019s inequality in Eq. (18), similar arguments than in the previous bounds, (\nI \u2212 (I \u2212 \u03b3H)n\u2212j )2( \u03b3H )\u22122 H 4 H\u22121\u03b3 and ( I \u2212 (I \u2212 \u03b3H)n\u2212j )2( \u03b3H )\u22122\nH 4 n\u03b3 I:\n\u2016H1/2\u03b7\u0304rn\u22121\u2016p\n6 \u03b3 \u221a p\nn\n\u2225 \u2225 \u2225 \u2225\nn\u22121 \u2211\nj=2\n\u3008\u03b7r\u22121j\u22121 ,E [ (H \u2212 xj \u2297 xj) ( I \u2212 (I \u2212 \u03b3H)n\u2212j )2( \u03b3H )\u22122 H(H \u2212 xj \u2297 xj) ] \u03b7r\u22121j\u22121 \u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n+ \u03b3p\nn\n\u2225 \u2225 \u2225 \u2225\nsup j\u2208{2,...,n\u22121}\n\u3008\u03b7r\u22121j\u22121 , (H \u2212 xj \u2297 xj) ( I \u2212 (I \u2212 \u03b3H)n\u2212j )2( \u03b3H )\u22122 H(H \u2212 xj \u2297 xj)\u03b7r\u22121j\u22121 \u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n6 \u03b3 \u221a p\nn\n\u2225 \u2225 \u2225 \u2225\nn\u22121 \u2211\nj=2\n\u3008\u03b7r\u22121j\u22121 ,E [ (H \u2212 xj \u2297 xj)\u03b3\u22122H\u22121(H \u2212 xj \u2297 xj) ] \u03b7r\u22121j\u22121 \u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n+ \u03b3p\nn\n\u2225 \u2225 \u2225 \u2225\nsup j\u2208{2,...,n\u22121}\n\u3008\u03b7r\u22121j\u22121 , H n \u03b3 H\u03b7r\u22121j\u22121 \u3009\n\u2225 \u2225 \u2225 \u2225 1/2\np/2\n+ \u03b3p\nn\n\u2225 \u2225 \u2225 \u2225\nsup j\u2208{2,...,n\u22121}\n\u3008\u03b7r\u22121j\u22121 , (xj \u2297 xj) ( I \u2212 (I \u2212 \u03b3H)n\u2212j )2( \u03b3H )\u22122 H(xj \u2297 xj)\u03b7r\u22121j\u22121 \u3009 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n6\n\u221a p\nn\n\u2225 \u2225 \u2225 \u2225\nn\u22121 \u2211\nj=2\n\u03ba\u3008\u03b7r\u22121j\u22121 , H\u03b7r\u22121j\u22121 \u3009 trH\u22121H \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n+ \u221a \u03b3pR2\u221a n Ar\u22121\n+ \u03b3p\nn\n( n\u22121 \u2211\nj=2\nE\n[\n\u3008\u03b7r\u22121j\u22121 , xj\u3009p\u3008xj , ( I \u2212 (I \u2212 \u03b3H)n\u2212j )2( \u03b3H )\u22122 Hxj\u3009p/2 ])1/p\n6 \u221a p\u03bad\u221a n RAr\u22121 + \u221a \u03b3pR2\u221a n Ar\u22121\n+ \u03b3p\nn RAr\u22121\n( n\u22121 \u2211\nj=2\nE\n[\n\u3008xj , ( I \u2212 (I \u2212 \u03b3H)n\u2212j )2( \u03b3H )\u22122 Hxj\u3009p/2 ])1/p\n6 \u221a p\u221a n RAr\u22121( \u221a \u03b3pR2 + \u221a \u03bad) + \u03b3p n RAr\u22121 ( n\u22121 \u2211\nj=2\nd \u03b32 (n \u03b3 R2\n)p/2\u22121 )1/p\n6 \u221a p\u221a n RAr\u22121( \u221a \u03b3pR2 + \u221a \u03bad) + \u03b3p n RAr\u22121n 1/2\u03b3\u22121/2\u22121/pR1\u22122/pd1/p\n= \u221a p\u221a n RAr\u22121( \u221a \u03b3pR2 + \u221a \u03bad) + p\u221a n RAr\u22121(\u03b3R 2)1/2\u22121/pd1/p\n= \u221a p\u221a n RAr\u22121 [ \u221a \u03b3pR2 + \u221a \u03bad+ \u221a p(\u03b3R2)1/2\u22121/pd1/p ]\n6 \u221a p\u221a n \u221a p\u03b3R2(\u03c3 + \u03c4 \u221a p\u03b3R2)( \u221a \u03b3pR2 + \u221a \u03bad+ \u221a pd1/p(\u03b3R2)1/2\u22121/p )( \u221a p\u03b3R2\u03ba+ p\u03b3R2 + p(\u03b3R2)1\u22121/p )r\u22121 ,\nusing Eq. (21).\nWe may then impose a restriction on \u03b3R2, i.e., \u03b3R2 6 1\u03b1\u03bap with \u03b1 > 1. We then have\n\u221a p\u03b3R2\u03ba+ p\u03b3R2 + p(\u03b3R2)1\u22121/p 6 1\u221a \u03b1 + 1 \u03b1 + (\u03b1p)\u22121+1/p\n6 1\u221a \u03b1 + 1 \u03b1 + (\u03b1)\u22121(\u03b1p)1/p\n6 1\u221a \u03b1 + 1 \u03b1 + (\u03b1)\u22121(\u03b12)1/2 if \u03b1 > 2,\n= 1\u221a \u03b1 + 1 \u03b1 +\n\u221a\n2 \u03b1 .\nWith \u03b1 = 12, we obtain a bound of 0.781 6 810 above. This leads to the bound \u2016H1/2\u03b7\u0304rn\u22121\u2016p 6 \u221a p\u221a n \u221a p\u03b3R2 ( \u03c3 + \u03c4\u221a 12 \u221a \u03ba ) ( 1\u221a 12\u03ba + \u221a \u03bad+ d1/p \u221a p ( 1 12p )(1/2\u22121/p)/(1\u22121/p))( 8/10 )r\u22121\n6 \u221a p\u221a n \u221a p\u03b3R2\u03c4(1 + 1\u221a 12 ) \u221a \u03bad( 1\u221a 12 + 1 + sup p>2 \u221a p ( 1 12p )(1/2\u22121/p)/(1\u22121/p))( 8/10 )r\u22121\n6 \u221a p\u221a n \u221a p\u03b3R2\u03c4(1 + 1\u221a 12 ) \u221a \u03bad( 1\u221a 12 + 1 + \u221a 2 )( 8/10 )r\u22121\n6 7\n2 \u221a p\u221a n \u221a p\u03b3R2\u03c4 \u221a \u03bad ( 8/10 )r\u22121 . (22)\nBound on \u2016H1/2(\u03b7\u0304n\u22121 \u2212 \u2211r i=0 \u03b7\u0304 i n\u22121)\u2016p. From Eq. (16) and the fact that 0 4 I \u2212 \u03b3xn \u2297 xn 4 I almost surely, we get: \u2225\n\u2225 \u2225 \u2225 \u03b7n \u2212 r \u2211\ni=0\n\u03b7in\n\u2225 \u2225 \u2225 \u2225\np\n6\n\u2225 \u2225 \u2225 \u2225 \u03b7n\u22121 \u2212 r \u2211\ni=0\n\u03b7in\u22121\n\u2225 \u2225 \u2225 \u2225\np\n+ \u03b3 \u2225 \u2225(H \u2212 xn \u2297 xn)\u03b7rn\u22121 \u2225 \u2225\np\n6\n\u2225 \u2225 \u2225 \u2225 \u03b7n\u22121 \u2212 r \u2211\ni=0\n\u03b7in\u22121\n\u2225 \u2225 \u2225 \u2225\np\n+ \u03b3R \u2225 \u2225\u3008xn, \u03b7rn\u22121\u3009 \u2225 \u2225\np\n6\n\u2225 \u2225 \u2225 \u2225 \u03b7n\u22121 \u2212 r \u2211\ni=0\n\u03b7in\u22121\n\u2225 \u2225 \u2225 \u2225\np\n+ \u03b3R2Ar.\nThis implies that \u2225\n\u2225 \u2225 \u2225 H1/2(\u03b7\u0304n\u22121 \u2212 r \u2211\ni=0\n\u03b7\u0304in\u22121)\n\u2225 \u2225 \u2225 \u2225\np\n6 R\n\u2225 \u2225 \u2225 \u2225 \u03b7n \u2212 r \u2211\ni=0\n\u03b7in\n\u2225 \u2225 \u2225 \u2225\np\n6 n\u03b3R3Ar. (23)\nPutting things together. We get by combining Lemma 4 with Eq. (22) and Eq. (23) and then letting r tends to infinity,\n\u2016H1/2\u03b7\u0304rn\u22121\u2016p 6 r \u2211\ni=1\n\u2225 \u2225H1/2\u03b7\u0304in\u22121 \u2225 \u2225 p + \u2225 \u2225H1/2\u03b7\u03040n\u22121 \u2225 \u2225 p +\n\u2225 \u2225 \u2225 \u2225 H1/2(\u03b7\u0304n\u22121 \u2212 r \u2211\ni=0\n\u03b7\u0304in\u22121)\n\u2225 \u2225 \u2225 \u2225\np\n6\n{\n7\n2 \u221a p\u221a n \u221a p\u03b3R2\u03c4 \u221a \u03bad 1\u2212 (8/10)r 1\u2212 8/10 } + {\u221a pd\u03c3\u221a n + \u221a \u03b3pR\u03c4\u221a n } +O((8/10)r)\n6 \u221a pd\u03c3\u221a n + \u221a p\u221a n \u221a p\u03b3R2\u03c4 \u221a \u03bad(1 + 7 2 10 2 ) 6 \u221a pd\u03c3\u221a n + 18.5 \u221a p\u221a n \u221a p\u03b3R2\u03c4 \u221a \u03bad\n6 \u221a pd\u03c3\u221a n + 18.5\u221a 12 \u221a pd\u03c4\u221a n 6 \u221a pd(\u03c3 + 6\u03c4)\u221a n 6 7 \u221a pd\u03c4\u221a n .\nB.3 Final bound\nFor \u03b3 6 112\u03bapR2 , we obtain, from the last equations of Section B.1 and Section B.2,\n\u2016H1/2\u03b7\u0304rn\u22121\u2016p 6 7 \u221a pd\u03c4\u221a n + \u2016\u03b70\u2016\u221a n\u03b3 \u221a 2 + 3p\u03b3R2\n6 \u221a p\u221a n ( 7 \u221a d\u03c4 +R\u2016\u03b70\u2016 \u221a 3 + 2 \u03b3pR2 ) .\nMoreover, when \u03b3 = 112\u03bapR2 , we have:\n\u2016H1/2\u03b7\u0304rn\u22121\u2016p 6 7 \u221a pd\u03c4\u221a n + \u2016\u03b70\u2016\u221a n \u221a 12\u03bapR2 \u221a 2 + 1 4\n6 7 \u221a pd\u03c4\u221a n + 6R\u2016\u03b70\u2016\u221a n \u221a \u03bap = \u221a p\u221a n ( 7 \u221a d\u03c4 + 6 \u221a \u03baR\u2016\u03b80 \u2212 \u03b8\u2217\u2016 ) .\nB.4 Proof of Corollary 1\nWe have from the previous proposition, for \u03b3 6 112\u03bapR2 :\n( E \u2223 \u2223f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) \u2223 \u2223 p)1/p 6\n1\n2n\n(\u221a p [ 7\u03c4 \u221a d+R\u2016\u03b80 \u2212 \u03b8\u2217\u2016 \u221a 3 ] +R\u2016\u03b80 \u2212 \u03b8\u2217\u2016 \u221a 2\n\u03b3R2\n)2\n6 1\n2n\n(\u221a p +\u25b3\n\u221a\n1\n\u03b7\n)2\n,\nwith \u03b7 = 12\u03b3\u03baR2 6 1/p, and = 7\u03c4 \u221a d+R\u2016\u03b80 \u2212 \u03b8\u2217\u2016 \u221a 3 and \u25b3 = R\u2016\u03b80 \u2212 \u03b8\u2217\u2016 \u221a 24\u03ba.\nThis leads to, using Markov\u2019s inequality:\nP\n(\nf(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) > t\n2n\n)\n6\n(\u221a p +\u25b3 \u221a\n1/\u03b7\u221a t\n)2p\n.\nThis leads to, with p = 1\u03b7 ,\nP\n(\nf(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) > t\n2n\n)\n6\n(\n( +\u25b3)2 \u03b7t\n)1/\u03b7\n.\nThis leads to\nP\n(\nf(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) > t\n2n\n[ 7\u03c4 \u221a d+R\u2016\u03b80 \u2212 \u03b8\u2217\u2016( \u221a 3 + \u221a 24\u03ba) ]2 ) 6 (\n1\n12\u03b3\u03baR2t\n)1/(12\u03b3\u03baR2)\n. (24)\nThus the large deviations decay as power of t, with a power that decays as 1/(12\u03b3\u03baR2). If \u03b3 is small, the deviations are lighter.\nIn order to get the desired result, we simply take t = 112\u03b3\u03baR2 \u03b4 \u221212\u03ba\u03b3R2 .\nC Proof of Theorem 3\nThe proof relies mostly on properties of approximate Newton steps: \u03b81 def = \u03b8\u0303n is an approximate minimizer of f , and \u03b83 def = \u03b6n is an approximate minimizer of the associated quadratic problem.\nIn terms of convergence rates, \u03b81 will be (1/ \u221a n)-optimal, while \u03b83 will be (1/n)-optimal for the quadratic problem because of previous results on averaged LMS. A classical property is that a single Newton step squares the error. Therefore, the full Newton step should have an error which is the square of the one of \u03b81, i.e., O(1/n). Overall, since \u03b83 approaches the full Newton step with rate O(1/n), this makes a bound of O(1/n).\nIn Section C.1, we provide a general deterministic result on the Newton step, while in Section C.2, we combine with two stochastic approximation results, making the informal reasoning above more precise.\nC.1 Approximate Newton step\nIn this section, we study the effect of an approximate Newton step. We consider \u03b81 \u2208 H, the Newton iterate \u03b82 = \u03b81\u2212f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81), and an approximation \u03b83 of \u03b82. In the next proposition, we provide a bound on f(\u03b83)\u2212 f(\u03b8\u2217), under different conditions, whether \u03b81 is close to optimal for f , and/or \u03b83 is close to optimal for the quadratic approximation around \u03b81 (i.e., close to \u03b82). Eq. (25) corresponds to the least-favorable situations where both errors are small, while Eq. (26) and Eq. (27) consider cases where \u03b81 is sufficiently good. See proof in Section E.3. These three cases are necessary for the probabilistic control.\nProposition 1 (Approximate Newton step) Assume (B3-4), and \u03b81, \u03b82, \u03b83 \u2208 H such that f(\u03b81)\u2212 f(\u03b8\u2217) = \u03b51, \u03b82 = \u03b81 \u2212 f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81) and 12 \u3008\u03b83 \u2212 \u03b82, f \u2032\u2032(\u03b81)(\u03b83 \u2212 \u03b82)\u3009 = \u03b52. Then, if t2 = \u03b51\u03ba\u03c1,\nf(\u03b83)\u2212 f(\u03b8\u2217) 6 \u03b51 + \u221a 2\u03c1\u03b52e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51. (25)\nIf t = \u221a \u03b51\u03ba\u03c1 6 1/16, then\nf(\u03b83)\u2212 f(\u03b8\u2217) 6 57\u03ba\u03c1\u03b521 + 2 \u221a \u03c1\u03b52. (26)\nMoreover, if t = \u221a \u03b51\u03ba\u03c1 6 1/16 and \u03b52\u03ba\u03c1 6 1/16, then\nf(\u03b83)\u2212 f(\u03b8\u2217) 6 57\u03ba\u03c1\u03b521 + 12\u03b52. (27)\nNote that in the favorable situation in Eq. (26), we get error of the form O(\u03b521+\u03b52). It essentially suffices now to show that in our set-up, in a probabilistic sense to be determined, \u03b51 = O(1/ \u221a n) and \u03b52 = O(1/n), while controlling the unfavorable situations.\nC.2 Stochastic analysis\nWe consider the following two-step algorithm:\n\u2013 Starting from any initialization \u03b80, run n iterations of averaged stochastic gradient descent to get \u03b81,\n\u2013 Run from \u03b81 n steps of LMS on the quadratic approximation around \u03b81, to get \u03b83, which is an approximation of the Newton step \u03b82.\nWe consider the events\nA1 =\n{\nf(\u03b81)\u2212 f(\u03b8\u2217) 6 1 162 (\u03ba\u03c1)\u22121\n}\n=\n{\n\u03b51 6 1 162 (\u03ba\u03c1)\u22121\n}\nand\nA2 =\n{\n1 2 \u3008\u03b83 \u2212 \u03b82, f \u2032\u2032(\u03b81)(\u03b83 \u2212 \u03b82)\u3009 6 1 16 (\u03ba\u03c1)\u22121\n}\n=\n{\n\u03b52 6 1 16 (\u03ba\u03c1)\u22121\n}\n.\nWe denote by G1 the \u03c3-field generated by the first n observations (the ones used to define \u03b81). We have, by separating all events, i.e., using 1 = 1A11A2 + 1A11Ac2 + 1Ac1 :\nE [ f(\u03b83)\u2212 f(\u03b8\u2217) \u2223 \u2223G1 ]\n= E [ 1A11A2 ( f(\u03b83)\u2212 f(\u03b8\u2217) ) \u2223 \u2223G1 ] + E [ 1A11Ac2 ( f(\u03b83)\u2212 f(\u03b8\u2217) ) \u2223 \u2223G1 ] + E [ 1Ac 1 ( f(\u03b83)\u2212 f(\u03b8\u2217) ) \u2223 \u2223G1 ] 6 E [\n1A11A2 ( 57\u03ba\u03c1\u03b521 + 12\u03b52 )\u2223 \u2223G1 ] + E [ 1A11Ac2 ( 57\u03ba\u03c1\u03b521 + 2 \u221a \u03c1\u03b52 )\u2223 \u2223G1 ]\n+E [\n1Ac 1\n( \u03b51 + \u221a 2\u03c1\u03b52e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51 )\u2223 \u2223G1 ] using Prop. 1,\n6 57\u03ba\u03c1\u03b521 + 12E [ 1A1\u03b52 \u2223 \u2223G1 ] + E [ 1A11Ac2 ( 2 \u221a \u03c1\u03b52 )\u2223 \u2223G1 ]\n+1Ac 1\n(\n\u03b51 + E [\u221a \u03b52|G1 ] \u221a 2\u03c1e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51 )\n6 57\u03ba\u03c1\u03b521 + 12\u00d7 1A1E [ \u03b52 \u2223 \u2223G1 ] + 2 \u221a \u03c11A1 \u221a P(Ac2|G1) \u221a E [ \u03b52 \u2223 \u2223G1 ] using Cauchy-Schwarz inequality,\n+1Ac 1\n(\n\u03b51 + E [\u221a \u03b52|G1 ] \u221a 2\u03c1e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51 )\n= 57\u03ba\u03c1\u03b521 + 12\u00d7 1A1E [ \u03b52 \u2223 \u2223G1 ] + 2 \u221a \u03c11A1\n\u221a\nP({\u03b52 > 1\n16\u03ba\u03c1 |G1)\n\u221a\nE [ \u03b52 \u2223 \u2223G1 ]\n+1Ac 1\n(\n\u03b51 + E [\u221a \u03b52|G1 ] \u221a 2\u03c1e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51 )\n6 57\u03ba\u03c1\u03b521 + 12\u00d7 1A1E [ \u03b52 \u2223 \u2223G1 ] + 2 \u221a \u03c11A1 \u221a 16\u03ba\u03c1E(\u03b52|G1) \u221a E [ \u03b52 \u2223 \u2223G1 ] using Markov\u2019s inequality,\n+1Ac 1\n( \u03b51 + E [\u221a \u03b52|G1 ] \u221a 2\u03c1e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51 )\n= 57\u03ba\u03c1\u03b521 + 1A1E [ \u03b52 \u2223 \u2223G1 ]\n(12 + 2 \u221a \u03c1 \u221a 16\u03ba\u03c1)\n+1Ac 1\n(\n\u03b51 + E [\u221a \u03b52|G1 ] \u221a 2\u03c1e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51 ) . (28)\nWe now need to control \u03b52, i.e., the error made by the LMS algorithm started from \u03b81.\nLMS on the second-order Taylor approximation. We consider the quadratic approximation around \u03b81 \u2208 H, and write is as an expectation, i.e.,\ng(\u03b8) = f(\u03b81) + \u3008f \u2032(\u03b81), \u03b8 \u2212 \u03b81\u3009+ 1 2 \u3008\u03b8 \u2212 \u03b81, f \u2032\u2032(\u03b81)(\u03b8 \u2212 \u03b81)\u3009\n= f(\u03b81) + E [\u2329 \u2113\u2032(y, \u3008x, \u03b81\u3009)x, \u03b8 \u2212 \u03b81 \u232a] + 1\n2 E [\u2329 \u03b8 \u2212 \u03b81, \u2113\u2032\u2032(y, \u3008x, \u03b81\u3009)x \u2297 x(\u03b8 \u2212 \u03b81) \u232a]\n= f(\u03b81) + \u2329 E [ \u2113\u2032(y, \u3008x, \u03b81\u3009)x ] , \u03b8 \u2212 \u03b81 \u232a + 1\n2\n\u2329 \u03b8 \u2212 \u03b81,E [ \u2113\u2032\u2032(y, \u3008x, \u03b81\u3009)x \u2297 x ] (\u03b8 \u2212 \u03b81) \u232a .\nWe consider x\u0303n = \u221a \u2113\u2032\u2032(yn, \u3008xn, \u03b81\u3009)xn and z\u0303n = \u2212\u2113\u2032(yn, \u3008xn, \u03b81\u3009)xn, so that\ng(\u03b8) = f(\u03b81) + E\n[\n1 2 \u3008\u03b8 \u2212 \u03b81, x\u0303n\u30092 \u2212 \u3008z\u0303n, \u03b8 \u2212 \u03b81\u3009\n]\n.\nWe denote by \u03b82 = \u03b81 \u2212 f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81) the output of the Newton step, i.e., the global minimizer of g, and \u03be\u0303n = z\u0303n \u2212 \u3008\u03b82 \u2212 \u03b81, x\u0303n\u3009x\u0303n the residual.\nWe have E\u03be\u0303n = 0, E [ x\u0303n \u2297 x\u0303n ] = f \u2032\u2032(\u03b81), and, for any z \u2208 H:\n( E [ \u3008z, \u03be\u0303n\u3009 ]2)1/2 =\n(\nE [( \u2113\u2032\u2032(yn, \u3008xn, \u03b81\u3009)\u3008\u03b82 \u2212 \u03b81, xn\u3009+ \u2113\u2032(yn, \u3008xn, \u03b81\u3009 ) \u3008z, xn\u3009 ]2 )1/2\n6\n(\nE [( \u2113\u2032\u2032(yn, \u3008xn, \u03b81\u3009)\u3008\u03b82 \u2212 \u03b81, xn\u3009\u3008z, xn\u3009 ]2 )1/2 + ( E [ \u2113\u2032(yn, \u3008xn, \u03b81\u3009\u3008z, xn\u3009 ]2 )1/2\nusing the triangle inequality,\n6 \u221a \u03ba \u221a \u3008z, f \u2032\u2032(\u03b81)z\u3009\u3008\u03b82 \u2212 \u03b81, f \u2032\u2032(\u03b81)(\u03b82 \u2212 \u03b81)\u3009+ ( E [ \u3008z, xn\u3009 ]2 )1/2 6 \u221a \u03ba \u221a \u3008z, f \u2032\u2032(\u03b81)z\u3009\u3008\u03b82 \u2212 \u03b81, f \u2032\u2032(\u03b81)(\u03b82 \u2212 \u03b81)\u3009+ \u221a \u03c1 \u221a \u3008z, f \u2032\u2032(\u03b8\u2217)z\u3009\n6 \u221a \u3008z, f \u2032\u2032(\u03b81)z\u3009 [\u221a \u03ba\u2016f \u2032\u2032(\u03b81)\u22121/2f \u2032(\u03b81)\u2016+ \u221a \u03c1e \u221a \u03ba\u03c1d1/2 ] ,\nwhere we denote d21 = \u3008\u03b81\u2212\u03b8\u2217, H(\u03b81\u2212\u03b8\u2217)\u3009, and we have used assumption (B4) , |\u2113\u2032| 6 1 and Prop. 5 relating H and f \u2032\u2032(\u03b81). This leads to\nE [ \u03be\u0303n \u2297 \u03be\u0303n ] 4 [\u221a \u03ba\u2016f \u2032\u2032(\u03b81)\u22121/2f \u2032(\u03b81)\u2016+ \u221a \u03c1e \u221a \u03ba\u03c1d1/2 ]2 f \u2032\u2032(\u03b81).\nThus, we have:\n\u2013 E [ \u03be\u0303n \u2297 \u03be\u0303n ] 4 \u03c32f \u2032\u2032(\u03b81) with \u03c3 = \u221a \u03ba\u2016f \u2032\u2032(\u03b81)\u22121/2f \u2032(\u03b81)\u2016+ \u221a \u03c1e \u221a \u03ba\u03c1d1/2.\n\u2013 \u2016xn\u20162 6 R2/4 almost surely.\nWe may thus apply the previous results, i.e., Theorem 1, to obtain with the LMS algorithm a \u03b83 \u2208 H such that, with \u03b3 = 1R2 :\nE [ \u03b52|G1 ] 6 2\nn\n[\u221a d \u221a \u03ba\u2016f \u2032\u2032(\u03b81)\u22121/2f \u2032(\u03b81)\u2016+ \u221a d \u221a \u03c1e \u221a \u03ba\u03c1d1/2 + R\n2 \u2016f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81)\u2016\n]2\n6 2\nn\n[\u221a d \u221a \u03ba\u2016H\u22121/2f \u2032(\u03b81)\u2016e \u221a \u03ba\u03c1d1/2 + \u221a d \u221a \u03c1e \u221a \u03ba\u03c1d1/2 + R 2 e \u221a \u03ba\u03c1d1\u2016H\u22121f \u2032(\u03b81)\u2016 ]2\nusing Prop. 5 ,\n6 2\nn\n[\u221a d \u221a \u03ba( \u221a 3 + 2 \u221a \u03ba\u03c1\u03b51) \u221a \u03b51e \u221a \u03ba\u03c1d1/2 + \u221a d \u221a \u03c1e \u221a \u03ba\u03c1d1/2 + R 2 e \u221a \u03ba\u03c1d1 e \u221a \u03ba\u03c1d1 \u2212 1\u221a \u03ba\u03c1d1 \u2016\u03b81 \u2212 \u03b8\u2217\u2016 ]2\nusing Prop. 11 and Eq. (49) from Section E.3.\nThus, E [ \u03b52|G1 ] 6 2n\n[ R\u2016\u03b81 \u2212 \u03b8\u2217\u2016\u25b32(t) +\u25b33(t) \u221a d\u03c1 ]2 , with increasing functions\n\u25b32(t) = 1 2 e \u221a 3+t2t e \u221a 3+t2t \u2212 1\u221a 3 + t2t ,\n\u25b33(t) = [ ( \u221a 3 + 2t)t+ 1 ] e \u221a 3+t2t/2,\nwhich are such that \u25b32(t) 6 0.6 and \u25b33(t) 6 1.2 if t 6 1/16.\nWe then get from Eq. (28):\nE [ f(\u03b83)\u2212 f(\u03b8\u2217) \u2223 \u2223G1 ]\n6 57\u03ba\u03c1\u03b521 + 2\nn (12 + 2\n\u221a \u03c1 \u221a 16\u03ba\u03c1) [ 0.6R\u2016\u03b81 \u2212 \u03b8\u2217\u2016+ \u221a d\u03c11.2 ]2\n+1Ac 1\n(\n\u03b51 +\n\u221a\n2\nn\n( R\u2016\u03b81 \u2212 \u03b8\u2217\u2016\u25b32(t) +\u25b33(t) \u221a d\u03c1 ) \u221a 2\u03c1e \u221a 3+t2t/2 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) \u221a \u03b51 )\n6 57\u03ba\u03c1\u03b521 + 1\nn (12 + 8\n\u221a \u03c1 \u221a \u03ba\u03c1) ( 6d\u03c1+ 3\n2 R2\u2016\u03b81 \u2212 \u03b8\u2217\u20162\n)\n+1Ac 1\n(\u221a \u03b51 [ \u221a t\u221a \u03ba\u03c1 + \u221a \u03c1e \u221a 3+t2t( \u221a 3 + 2t) ] + \u221a 2 n ( R\u2016\u03b81 \u2212 \u03b8\u2217\u2016\u25b32(t) +\u25b33(t) \u221a d\u03c1 ) \u221a 2\u03c1e \u221a 3+t2t/2 )\n= 57\u03ba\u03c1\u03b521 + 12\nn (3 + 2\n\u221a \u03c1 \u221a \u03ba\u03c1) ( 2d\u03c1+ 1\n2 R2\u2016\u03b81 \u2212 \u03b8\u2217\u20162\n)\n+1Ac 1\n(\u221a \u03c1\u03b51 [\n\u221a t\n\u03c1 \u221a \u03ba + e\n\u221a 3+t2t( \u221a 3 + 2t) ] + \u221a 4\u03c1\nn \u25b32(t)e\n\u221a 3+t2t/2R\u2016\u03b81 \u2212 \u03b8\u2217\u2016+\n\u221a\n4\u03c1\nn \u25b33(t)e\n\u221a 3+t2t/2 \u221a\n\u03c1d\n)\n6 57\u03ba\u03c1\u03b521 + 12\nn (3 + 2\n\u221a \u03c1 \u221a \u03ba\u03c1) ( 2d\u03c1+ 1\n2 R2\u2016\u03b81 \u2212 \u03b8\u2217\u20162\n)\n+1Ac 1\n(\u221a \u03c1\u03b51\u25b34(t) +\n\u221a\n\u03c1 n R\u2016\u03b81 \u2212 \u03b8\u2217\u2016\u25b35(t) +\n\u221a\n\u03c1\nn\n\u221a \u03c1d\u25b36(t) ) , (29)\nwith (using \u03c1 > 4):\n\u25b34(t) = \u221a t\n4 + e\n\u221a 3+t2t( \u221a 3 + 2t) 6 5 exp(2t2)\n\u25b35(t) = 2e \u221a 3+t2t/2\u22062(t) 6 4 exp(3t 2) \u25b36(t) = 2e \u221a 3+t2t/2\u22063(t) 6 6 exp(3t 2).\nThe last inequalities may be checked graphically. By taking expectations and using E|XY Z| 6 (E|X |2)1/2(E|X |4)1/4(E|X |4)1/4, this leads to, from Eq. (29):\nE [ f(\u03b83)\u2212 f(\u03b8\u2217) ]\n6 57\u03ba\u03c1E [ \u03b521 ]\n+ 12\nn (3 + 2\n\u221a \u03c1 \u221a \u03ba\u03c1) ( 2d\u03c1+ 1\n2 E [ R2\u2016\u03b81 \u2212 \u03b8\u2217\u20162 ])\n+ \u221a \u03c1 \u221a\nP(Ac1)\n(\n( E [ \u03b521 ])1/4( E [ \u25b34(t) ]4)1/4\n+\n\u221a\n1\nn\n( E [ R4\u2016\u03b81 \u2212 \u03b8\u2217\u20164 ])1/4( E [ \u25b35(t) ]4)1/4 +\n\u221a\n1\nn\n\u221a\n\u03c1d ( E [ \u25b36(t) ]4)1/4\n)\n. (30)\nWe now need to use bounds on the behavior of the first n steps of regular averaged stochastic gradient descent.\nFine results on averaged stochastic gradient descent. In order to get error bounds on \u03b81, we run n steps of averaged stochastic gradient descent with constant-step size \u03b3 = 1/(2R2 \u221a n). We\nneed the following bounds from [21, Appendix E and Prop. 1]:\nE[(f(\u03b81)\u2212 f(\u03b8\u2217))2] 6 1\nn (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 +\n3 4 )2\nE[(f(\u03b81)\u2212 f(\u03b8\u2217))3] 6 1\nn3/2 (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 +\n3 2 )3\nE [ R4\u2016\u03b81 \u2212 \u03b8\u2217\u20164 ] 6 (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 3\n4 )2\nE [ R2\u2016\u03b81 \u2212 \u03b8\u2217\u20162 ] 6 R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 1\n4\nP\n[\nf(\u03b81)\u2212 f(\u03b8\u2217) > 1 162 (\u03ba\u03c1)\u22121\n]\n6 166(\u03ba\u03c1)3E [ f(\u03b81)\u2212 f(\u03b8\u2217) ]3 using Markov\u2019s inequality,\n6 166(\u03ba\u03c1)3 1\nn3/2 (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 +\n3 2 )3.\nHowever, we need a finer control of the deviations in order to bound quantities of the form e\u03b1\u03b51 .\nIn Section D, extending results from [21], we show in Prop. 3 that if \u03b1(10+2R 2\u2016\u03b80\u2212\u03b8\u2217\u20162)\u221a n 6 12e , then Ee\u03b1(f(\u03b81)\u2212f(\u03b8\u2217)) 6 1.\nPutting things together. From Eq. (30), we then get, if 6(10+2R 2\u2016\u03b80\u2212\u03b8\u2217\u20162)\u221a n 6 12e :\nE [ f(\u03b83)\u2212 f(\u03b8\u2217) ]\n6 57\u03ba\u03c1 1\nn (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 +\n3 4 )2 + 12 n (3 + 2 \u221a \u03c1 \u221a \u03ba\u03c1) ( 2d\u03c1+ 1 4 + 1 2 R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 )\n+ \u221a \u03c1\n\u221a\n166(\u03ba\u03c1)3 1\nn3/2 (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 +\n3 2 )3\n\u00d7 ( 5 ( 1\nn (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 +\n3 4 )2 )1/4 + 4\n\u221a\n1\nn\n( (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 3 4 )2 )1/4 + 6\n\u221a\n1\nn\n\u221a\n\u03c1d\n)\nUsing the notation D = (R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 32 ), we obtain:\nE [ f(\u03b83)\u2212 f(\u03b8\u2217) ]\n6 57\u03ba\u03c1 1\nn D2 +\n12\nn (3 + 2\n\u221a \u03c1 \u221a \u03ba\u03c1) ( 2d\u03c1+ D\n2\n)\n+ \u221a \u03c1\n\u221a\n166(\u03ba\u03c1)3 1\nn3/2 D3 \u00d7\n(\n5 ( 1\nn D2 )1/4 + 4\n\u221a\n1\nn\n( D2 )1/4 + 6\n\u221a\n1\nn\n\u221a\n\u03c1d\n)\n= 57\u03ba\u03c1 1\nn D2 +\n12\nn (3 + 2\n\u221a \u03c1 \u221a \u03ba\u03c1) ( 2d\u03c1+ D\n2\n)\n+ \u221a \u03c1163(\u03ba\u03c1)3/2 1\nn3/4 D3/2 \u00d7\n(\n5 1\nn1/4 D1/2 + 4\n1\nn1/2 D1/2 + 6 1\u221a n \u221a \u03c1d\n)\n6 \u03ba3/2\u03c12\nn\n[\n57\n4 D2 + 12(\n3\n16 +\n2 4 ) ( 2d\u03c1+ D 2 ) + 163D3/2 \u00d7 ( 5D1/2 + 4 1 n1/4 D1/2 + 6 1 n1/4 \u221a \u03c1d )]\nusing \u03c1 > 4 and \u03ba > 1,\n6 \u03ba3/2\u03c12D2\nn\n[\n57\n4 1 + 12(\n3\n16 +\n2 4 ) ( 2d\u03c1 4 9 + 1 2 2 3 ) + 163 \u00d7 ( 5 + 4 + 6 1 n1/4 \u221a 2\u221a 3 \u221a \u03c1d )] using D > 3 2 ,\n6 \u03ba3/2\u03c12D2\nn\n[\n36881 + 20067\n\u221a \u03c1d n1/4 + 17d\u03c1 ] 6 \u03ba3/2\u03c13d n 56965(R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 3 2 )2\n6 \u03ba3/2\u03c13d\nn (16R\u2016\u03b80 \u2212 \u03b8\u2217\u2016+ 19)4.\nThe condition 6(10+2R 2\u2016\u03b80\u2212\u03b8\u2217\u20162)\u221a n 6 12e is implied by n > (19 + 9R\u2016\u03b80 \u2212 \u03b8\u2217\u2016)4.\nD Higher-order bounds for stochastic gradient descent\nIn this section, we provide high-order bounds for averaged stochastic gradient for logistic regression. The first proposition gives a finer result than [21], with a simpler proof, while the second proposition is new.\nProposition 2 Assume (B1-4). Consider the stochastic gradient recursion \u03b8n = \u03b8n\u22121\u2212\u03b3\u2113\u2032(yn, \u3008\u03b8n\u22121, xn\u3009)xn and its averaged version \u03b8\u0304n\u22121. We have, for all real p > 1,\n\u2225 \u2225f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) \u2225 \u2225\np 6\n17\u03b3R2 2 ( \u221a p+ p\u221a n )2 + 1 \u03b3n \u2016\u03b80 \u2212 \u03b8\u2217\u20162 (31)\n\u2225 \u2225\u2016\u03b8n \u2212 \u03b8\u2217\u20162 \u2225 \u2225 p 6 17\u03b32R2n(\n\u221a p+ p\u221a n )2 + 2\u2016\u03b80 \u2212 \u03b8\u2217\u20162. (32)\nProof. Following [21], we have the recursion:\n2\u03b3 [ f(\u03b8n\u22121)\u2212 f(\u03b8\u2217) ] + \u2016\u03b8n \u2212 \u03b8\u2217\u20162 6 \u2016\u03b8n\u22121 \u2212 \u03b8\u2217\u20162 + \u03b32R2 +Mn,\nwith Mn = \u22122\u03b3\u3008\u03b8n\u22121 \u2212 \u03b8\u2217, f \u2032n(\u03b8n\u22121)\u2212 f \u2032(\u03b8n\u22121)\u3009.\nThis leads to\n2\u03b3nf\n(\n1\nn\nn \u2211\nk=1\n\u03b8k\u22121\n)\n\u2212 2\u03b3nf(\u03b8\u2217) + \u2016\u03b8n \u2212 \u03b8\u2217\u20162 6 An,\nwith An = \u2016\u03b80\u2212\u03b8\u2217\u20162+n\u03b32R2+ \u2211n k=1 Mk. Note that E(Mk|Fk\u22121) = 0 and |Mk| 6 4\u03b3R\u2016\u03b8k\u22121\u2212\u03b8\u2217\u2016 6 4\u03b3RA\n1/2 k\u22121 almost surely. We may now use BRP\u2019s inequality in Eq. (18) to get:\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{0,...,n} Ak\n\u2225 \u2225 \u2225 \u2225\np\n6 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 + n\u03b32R2 + \u221a p\n\u2225 \u2225 \u2225 \u2225 16\u03b32R2 n \u2211\nk=1\n\u2016\u03b8k\u22121 \u2212 \u03b8\u2217\u20162 \u2225 \u2225 \u2225\n\u2225\n1/2\np/2\n+p\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{1,...,n}\n4\u03b3R\u2016\u03b8k\u22121 \u2212 \u03b8\u2217\u2016 \u2225 \u2225 \u2225\n\u2225\np\n6 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 + n\u03b32R2 + \u221a p4\u03b3R \u221a n \u2225 \u2225 \u2225\n\u2225 sup k\u2208{0,...,n\u22121} Ak\n\u2225 \u2225 \u2225 \u2225 1/2\np/2\n+p4\u03b3R\n\u2225 \u2225 \u2225 \u2225\nsup k\u2208{0,...,n\u22121}\nA 1/2 k\n\u2225 \u2225 \u2225 \u2225\np\n6 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 + n\u03b32R2 + 4\u03b3R \u2225 \u2225 \u2225\n\u2225 sup k\u2208{0,...,n\u22121} Ak\n\u2225 \u2225 \u2225 \u2225 1/2\np/2\n(\u221a pn+ p ) .\nThus if B =\n\u2225 \u2225 \u2225 \u2225 supk\u2208{0,...,n}Ak \u2225 \u2225 \u2225 \u2225\np\n, we have\nB 6 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 + n\u03b32R2 + 4\u03b3RB1/2 (\u221a pn+ p ) .\nBy solving this quadratic inequality, we get:\n( B1/2 \u2212 2\u03b3R(\u221apn+ p) )2 6 \u2016\u03b80 \u2212 \u03b8\u2217\u20162 + n\u03b32R2 + 4\u03b32R2 (\u221a pn+ p )2\nB1/2 6 2\u03b3R( \u221a pn+ p) + \u221a \u2016\u03b80 \u2212 \u03b8\u2217\u20162 + n\u03b32R2 + 4\u03b32R2 (\u221a pn+ p )2\nB 6 8\u03b32R2( \u221a pn+ p)2 + 2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 2n\u03b32R2 + 8\u03b32R2 (\u221a pn+ p )2\n6 16\u03b32R2( \u221a pn+ p)2 + 2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 + 2n\u03b32R2 6 17\u03b32R2( \u221a pn+ p)2 + 2\u2016\u03b80 \u2212 \u03b8\u2217\u20162.\nThe previous statement leads to the desired result if p > 2. For p \u2208 [1, 2], we may bound it by the value at p = 2, and a direct calculation shows that the bound is still correct.\nProposition 3 Assume (B1-4). Consider the stochastic gradient recursion \u03b8n = \u03b8n\u22121\u2212\u03b3\u2113\u2032(yn, \u3008\u03b8n\u22121, xn\u3009)xn and its averaged version \u03b8\u0304n\u22121. If\n\u03b1e(10 + 2R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162)\u221a n 6 1 2 , then E exp ( \u03b1 [ f(\u03b8\u0304n\u22121)\u2212f(\u03b8\u2217) ]) 6 1.\nProof. Using that almost surely, \u2016\u03b8\u0304n\u22121 \u2212 \u03b8\u2217\u2016 6 \u2016\u03b80 \u2212 \u03b8\u2217\u2016 + n\u03b3R we obtain that almost surely f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) 6 R\u2016\u03b80 \u2212 \u03b8\u2217\u2016+ n\u03b3R2.\nMoreover, from the previous proposition, we have for p 6 n4 ,\n\u2225 \u2225f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) \u2225 \u2225\np 6\n17\u03b3R2\n2\n9 4 p+ 1 \u03b3n \u2016\u03b80 \u2212 \u03b8\u2217\u20162.\nFor \u03b3 = 1 2R2 \u221a n , we get:\n\u2225 \u2225f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) \u2225 \u2225\np 6 10p\u221a n + 2R2\u221a n \u2016\u03b80 \u2212 \u03b8\u2217\u20162,\nand\nf(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) 6 R\u2016\u03b80 \u2212 \u03b8\u2217\u2016+ \u221a n\n2 almost surely.\nThis leads to the bound valid for all p:\n\u2225 \u2225f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) \u2225 \u2225\np 6 p\u221a n (10 + 2R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162).\nWe then get\nE exp ( \u03b1 [ f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217) ]) = \u221e \u2211\nk=0\n\u03b1p p! E[|f(\u03b8\u0304n\u22121)\u2212 f(\u03b8\u2217)|p]\n6\n\u221e \u2211\nk=0\n\u03b1p\np!\npp\nnp/2\n( 10 + 2R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 )p\n6\n\u221e \u2211\nk=0\n\u03b1p 2(p/e)p pp np/2\n( 10 + 2R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 )p using Stirling\u2019s formula,\n6 1\n2\n\u221e \u2211\nk=0\n(e\u03b1)p\nnp/2\n( 10 + 2R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162 )p\n6 1\n2\n1 1\u2212 1/2 = 1 if \u03b1e(10 + 2R2\u2016\u03b80 \u2212 \u03b8\u2217\u20162)\u221a n 6 1 2 .\nE Properties of self-concordance functions\nIn this section, we review various properties of self-concordant functions, that will prove useful in proving Theorem 3. All these properties rely on bounding the third-order derivatives by secondorder derivatives. More precisely, from assumptions (B3-4), we have for any \u03b8, \u03b4, \u03b7 \u2208 H, where f (r)[\u03b41, . . . , \u03b4k] denotes the k-th order differential of f :\nf \u2032\u2032\u2032(\u03b8)[\u03b4, \u03b4, \u03b7] = E [ \u2113\u2032\u2032\u2032(yn, \u3008\u03b8, xn\u3009)\u3008\u03b4, xn\u30092\u3008\u03b7, xn\u3009 ]\n|f \u2032\u2032\u2032(\u03b8)[\u03b4, \u03b4, \u03b7]| 6 E [ \u2113\u2032\u2032(yn, \u3008\u03b8, xn\u3009)\u3008\u03b4, xn\u30092|\u3008\u03b7, xn\u3009| ]\n6\n\u221a\nE [ \u2113\u2032\u2032(yn, \u3008\u03b8, xn\u3009)2\u3008\u03b4, xn\u30094 ]\n\u221a\nE [ \u3008\u03b7, xn\u30092 ] using Cauchy-Schwarz,\n6 \u221a \u03ba\u03c1f \u2032\u2032(\u03b8)[\u03b4, \u03b4] \u221a \u3008\u03b7,H\u03b7\u3009 using the two assumptions.\nE.1 Global Taylor expansions\nIn this section, we derive global non-asymptotic Taylor expansions for self-concordant functions, which show that they behave similarly to like quadratic functions.\nThe following proposition shows that having a small excess risk f(\u03b8) \u2212 f(\u03b8\u2217) implies that the weighted distance to optimum \u3008\u03b8 \u2212 \u03b8\u2217, H(\u03b8 \u2212 \u03b8\u2217)\u3009 is small. Note that for quadratic functions, these two quantities are equal and that throughout this section, we always consider norms weighted by the matrix H (Hessian at optimum).\nProposition 4 (Bounding weighted distance to optimum from function values) Assume (B34). Then, for any \u03b8 \u2208 H:\n\u3008\u03b8 \u2212 \u03b8\u2217, H(\u03b8 \u2212 \u03b8\u2217)\u3009 6 3 [ f(\u03b8)\u2212 f(\u03b8\u2217) ] + \u03ba\u03c1 [ f(\u03b8)\u2212 f(\u03b8\u2217) ]2 . (33)\nProof. Let \u03d5 : t 7\u2192 f [ \u03b8\u2217 + t(\u03b8 \u2212 \u03b8\u2217) ] . Denoting d = \u221a \u3008\u03b8 \u2212 \u03b8\u2217, f \u2032\u2032(\u03b8\u2217)(\u03b8 \u2212 \u03b8\u2217)\u3009, we have:\n|\u03d5\u2032\u2032\u2032(t)| 6 E [ \u2113\u2032\u2032\u2032(y, \u3008\u03b8\u2217 + t(\u03b8 \u2212 \u03b8\u2217), x\u3009)|\u3008\u03b8 \u2212 \u03b8\u2217, x\u3009|3 ]\n6 E [ \u2113\u2032\u2032(y, \u3008\u03b8\u2217 + t(\u03b8 \u2212 \u03b8\u2217), x\u3009)\u3008\u03b8 \u2212 \u03b8\u2217, x\u30092 ]\u221a \u03ba\u03c1d = \u221a \u03ba\u03c1d\u03d5\u2032\u2032(t),\nfrom which we obtain \u03d5\u2032\u2032(t) > \u03d5\u2032\u2032(0)e\u2212 \u221a \u03ba\u03c1dt. Following [25], by integrating twice (and noting that \u03d5\u2032(0) = 0 and \u03d5\u2032\u2032(0) = d2), we get\nf(\u03b8) = \u03d5(1) > \u03d5(0) + \u03d5\u2032\u2032(0) 1\nS2d2 (\ne\u2212 \u221a \u03ba\u03c1d + \u221a \u03ba\u03c1d\u2212 1 )\n> f(\u03b8\u2217) + 1\n\u03ba\u03c1\n( e\u2212 \u221a \u03ba\u03c1d + \u221a \u03ba\u03c1d\u2212 1 ) .\nThus e\u2212 \u221a \u03ba\u03c1d + \u221a \u03ba\u03c1d\u2212 1 6 \u03ba\u03c1 [ f(\u03b8)\u2212 f(\u03b8\u2217) ] . The function \u03ba : u 7\u2192 e\u2212u + u \u2212 1 is an increasing bijection from R+ to itself. Thus this implies d 6 1\u221a\u03ba\u03c1\u03ba \u22121 ( \u03ba\u03c1 [ f(\u03b8) \u2212 f(\u03b8\u2217) ] ) . We show below that \u03ba\u22121(v) 6 \u221a 3v + v2, leading to the desired\nresult. The identity \u03ba\u22121(v) 6 \u221a 3v + v2 is equivalent to e\u2212u + u \u2212 1 > \u221a u2 + \u03b12 \u2212 \u03b1, for \u03b1 = 32 . It then suffices to show that 1 \u2212 e\u2212u > u\u221a u2+\u03b12 . This can be shown by proving the monotonicity of\nu 7\u2192 e\u2212u + u\u2212 1\u2212 \u221a u2 + \u03b12 + \u03b1, and we leave this exercise to the reader.\nThe next proposition shows that Hessians between two points which are close in weighted distance are close to each other, for the order between positive semi-definite matrices.\nProposition 5 (Expansion of Hessians) Assume (B3-4). Then, for any \u03b81, \u03b82 \u2208 H:\nf \u2032\u2032(\u03b81)e \u221a \u03ba\u03c1 \u221a \u3008\u03b82\u2212\u03b81,H(\u03b82\u2212\u03b81)\u3009 < f \u2032\u2032(\u03b82) < f \u2032\u2032(\u03b81)e \u2212\u221a\u03ba\u03c1 \u221a \u3008\u03b82\u2212\u03b81,H(\u03b82\u2212\u03b81)\u3009, (34)\n\u2225 \u2225f \u2032\u2032(\u03b81) \u22121/2f \u2032\u2032(\u03b82)f \u2032\u2032(\u03b81) \u22121/2 \u2212 I \u2225 \u2225\nop 6 e\n\u221a \u03ba\u03c1 \u221a\n\u3008\u03b82\u2212\u03b81,H(\u03b82\u2212\u03b81)\u3009 \u2212 1. (35)\nProof. Let z \u2208 H and \u03c8(t) = z\u22a4f \u2032\u2032(\u03b81 + t(\u03b82 \u2212 \u03b81))z. We have:\n|\u03c8\u2032(t)| = |f \u2032\u2032\u2032(\u03b81 + t(\u03b82 \u2212 \u03b81))[z, z, \u03b82 \u2212 \u03b81]| 6 f \u2032\u2032(\u03b81 + t(\u03b82 \u2212 \u03b81))[z, z] \u221a \u03ba\u03c1d = \u03c8(t) \u221a \u03ba\u03c1d,\nwith d12 = \u221a \u3008\u03b82 \u2212 \u03b81, H(\u03b82 \u2212 \u03b81)\u3009. Thus \u03c8(0)e \u221a \u03ba\u03c1d12t > \u03c8(t) > \u03c8(0)e\u2212 \u221a \u03ba\u03c1d12t. This implies, for t = 1, that f \u2032\u2032(\u03b81)e \u221a \u03ba\u03c1d12 < f \u2032\u2032(\u03b82) < f \u2032\u2032(\u03b81)e \u2212\u221a\u03ba\u03c1d12 ,\nwhich implies the desired results. \u2016 \u00b7 \u2016op denotes the operator norm (largest singular value).\nThe following proposition gives an approximation result bounding the first order expansion of gradients by the first order expansion of function values.\nProposition 6 (Expansion of gradients) Assume (B3-4). Then, for any \u03b81, \u03b82 \u2208 H and \u2206 \u2208 H:\n\u3008\u2206, f \u2032(\u03b82)\u2212 f \u2032(\u03b81)\u2212 f \u2032\u2032(\u03b81)(\u03b82 \u2212 \u03b81)\u3009 6 \u221a \u03ba\u03c1\u3008\u2206, H\u2206\u30091/2 [ f(\u03b82)\u2212 f(\u03b81)\u2212 \u3008f \u2032(\u03b81), \u03b82 \u2212 \u03b81\u3009 ] . (36)\nProof. Let \u03d5(t) = \u3008\u2206, f \u2032(\u03b81+ t(\u03b82 \u2212 \u03b81))\u2212 f \u2032(\u03b81)\u2212 tf \u2032\u2032(\u03b81)(\u03b82 \u2212 \u03b81)\u3009. We have \u03d5\u2032(t) = \u3008\u2206, f \u2032\u2032(\u03b81 + t(\u03b82 \u2212 \u03b81))(\u03b81 \u2212 \u03b82\u3009 \u2212 \u3008\u2206, f \u2032\u2032(\u03b81)(\u03b81 \u2212 \u03b82)\u3009 and |\u03d5\u2032\u2032(t)| = |f \u2032\u2032\u2032(\u03b81 + t(\u03b82 \u2212 \u03b81))[\u03b82 \u2212 \u03b81, \u03b82 \u2212 \u03b81,\u2206]| 6\u221a \u03ba\u03c1\u3008\u2206, H\u2206\u30091/2\u3008\u03b81 \u2212 \u03b82, f \u2032\u2032(\u03b81 + t(\u03b82 \u2212 \u03b81))(\u03b81 \u2212 \u03b82)\u3009. This leads to\n\u3008\u2206, f \u2032(\u03b82)\u2212 f \u2032(\u03b81)\u2212 f \u2032\u2032(\u03b81)(\u03b82 \u2212 \u03b81)\u3009 6 \u221a \u03ba\u03c1\u3008\u2206, H\u2206\u30091/2 [ f(\u03b82)\u2212 f(\u03b81)\u2212 \u3008f \u2032(\u03b81), \u03b82 \u2212 \u03b81\u3009 ] .\nNote that one may also use the bound\n|\u03d5\u2032(t)| 6 \u2016\u03b81 \u2212 \u03b82\u2016\u3008\u2206, f \u2032\u2032(\u03b81)2\u2206\u30091/2 [ et \u221a \u03ba\u03c1\u2016H1/2(\u03b81\u2212\u03b82)\u2016 \u2212 1 ] ,\nleading to\n\u3008\u2206, f \u2032(\u03b82)\u2212 f \u2032(\u03b81)\u2212 f \u2032\u2032(\u03b81)(\u03b82 \u2212 \u03b81)\u3009\n6 \u2016\u03b81 \u2212 \u03b82\u2016\u3008\u2206, f \u2032\u2032(\u03b81)2\u2206\u30091/2 e \u221a \u03ba\u03c1\u2016H1/2(\u03b81\u2212\u03b82)\u2016 \u2212 1\u2212\u221a\u03ba\u03c1\u2016H1/2(\u03b81 \u2212 \u03b82)\u2016\u221a\n\u03ba\u03c1\u2016H1/2(\u03b81 \u2212 \u03b82)\u2016 . (37)\nThe following proposition considers a global Taylor expansion of function values. Note that when \u03ba\u03c1\u3008\u03b82 \u2212 \u03b81, H(\u03b82 \u2212 \u03b81)\u3009 tends to zero, we obtain exactly the second-order Taylor expansion. For more details, see [25]. This is followed by a corrolary that upper bounds excess risk by distance to optimum (this is thus the other direction than Prop. 4).\nProposition 7 (Expansion of function values) Assume (B3-4). Then, for any \u03b81, \u03b82 \u2208 H and \u2206 \u2208 H:\nf(\u03b82)\u2212 f(\u03b81)\u2212 \u3008f \u2032(\u03b81), \u03b82 \u2212 \u03b81\u3009 6 \u3008\u03b82 \u2212 \u03b81, f \u2032\u2032(\u03b81)(\u03b82 \u2212 \u03b81)\u3009 e \u221a \u03ba\u03c1 \u221a \u3008\u03b82\u2212\u03b81,H(\u03b82\u2212\u03b81)\u3009 \u2212 1\u2212\u221a\u03ba\u03c1 \u221a\n\u3008\u03b82 \u2212 \u03b81, H(\u03b82 \u2212 \u03b81)\u3009 \u03ba\u03c1\u3008\u03b82 \u2212 \u03b81, H(\u03b82 \u2212 \u03b81)\u3009 . (38)\nProof. Let \u03d5(t) = f [ \u03b81 + t(\u03b82 \u2212 \u03b81) ] \u2212 f(\u03b81) \u2212 t\u3008f \u2032(\u03b81), \u03b82 \u2212 \u03b81\u3009. We have \u03d5\u2032(t) = \u3008f \u2032 [ \u03b81 + t(\u03b82 \u2212 \u03b81) ] , \u03b82 \u2212 \u03b81\u3009 \u2212 \u3008f \u2032(\u03b81), \u03b82 \u2212 \u03b81\u3009 and \u03d5\u2032\u2032(t) = \u3008\u03b82 \u2212 \u03b81, f \u2032\u2032 [ \u03b81 + t(\u03b82 \u2212 \u03b81) ]\n(\u03b82 \u2212 \u03b81)\u3009. Moreover, \u03d5\u2032\u2032\u2032(t) 6 \u221a \u03ba\u03c1\u03d5\u2032\u2032(t) \u221a \u3008\u03b82 \u2212 \u03b81, H(\u03b82 \u2212 \u03b81)\u3009, leading to \u03d5\u2032\u2032(t) 6 e \u221a \u03ba\u03c1t \u221a \u3008\u03b82\u2212\u03b81,H(\u03b82\u2212\u03b81)\u3009\u03d5\u2032\u2032(0). Integrating twice between 0 and 1 leads to the desired result.\nCorollary 2 (Excess risk) Assume (B3-4), and \u03b81 \u2208 H and \u03b82 = \u03b81 \u2212 f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81). Then\nf(\u03b8)\u2212 f(\u03b8\u2217) 6 e \u221a \u03ba\u03c1d \u2212\u221a\u03ba\u03c1d\u2212 1\n\u03ba\u03c1 , (39)\nwhere d = \u221a \u3008\u03b8 \u2212 \u03b8\u2217, H(\u03b8 \u2212 \u03b8\u2217)\u3009.\nProof. Applying Prop. 7 to \u03b82 = \u03b8 and \u03b81 = \u03b8\u2217, we get the desired result.\nThe following proposition looks at a similar type of bounds than Prop. 7; it is weaker when \u03b82 and \u03b81 are close (it does not converge to the second-order Taylor expansion), but stronger for large values (it does not grow exponentially fast).\nProposition 8 (Bounding function values with fewer assumptions) Assume (B3-4), and \u03b81, \u03b82 \u2208 H. Then\nf(\u03b82)\u2212 f(\u03b81) 6 \u221a \u03c1\u2016H1/2(\u03b81 \u2212 \u03b82)\u2016. (40)\nProof. Let \u03d5(t) = f(\u03b81 + t(\u03b82 \u2212 \u03b81))\u2212 f(\u03b81). We have |\u03d5\u2032(t)| = |E\u2113\u2032(yn, \u3008xn, \u03b81+ t(\u03b82 \u2212 \u03b81)\u3009)\u3008\u03b82 \u2212 \u03b81t, xn\u3009| 6 \u221a \u03c1\u2016H1/2(\u03b81 \u2212 \u03b82)\u2016. Integrating between 0 and 1 leads to the desired result.\nE.2 Analysis of Newton step\nSelf-concordance has been traditionally used in the analysis of Newton\u2019s method (see [29, 24]). In this section, we adapt classical results to our specific notion of self-concordance (see also [25]). A key quantity is the so-called \u201cNewton decrement\u201d at a certain point \u03b81, equal to \u3008f \u2032(\u03b81), f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81)\u3009, which governs the convergence behavior of Newton methods (this is the quantity which is originally shown to be quadratically convergent). In this paper, we consider a slightly different version where the Hessian is chosen to be the one at \u03b8\u2217, i.e., \u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009.\nThe following proposition shows how a full Newton step improves the Newton decrement (by taking a square).\nProposition 9 (Effect of Newton step on Newton decrement) Assume (B3-4), and \u03b81 \u2208 H and \u03b82 = \u03b81 \u2212 f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81). Then\n\u3008f \u2032(\u03b82), H\u22121f \u2032(\u03b82)\u3009 6 \u03ba\u03c1e2 \u221a \u03ba\u03c1d1\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u30092\n( e \u221a \u03ba\u03c1d12 \u2212\u221a\u03ba\u03c1d12 \u2212 1\n\u03ba\u03c1d212\n)2\n, (41)\nwhere d212 = \u3008\u03b82 \u2212 \u03b81, H(\u03b82 \u2212 \u03b81)\u3009 6 e \u221a \u03ba\u03c1d1\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009 and d1 = \u3008\u03b81 \u2212 \u03b8\u2217, H(\u03b81 \u2212 \u03b8\u2217)\u30091/2.\nProof. When applying the two previous propositions to the Newton step \u03b82 = \u03b81\u2212f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81), we get:\n\u3008\u2206, f \u2032(\u03b82)\u3009 6 \u221a \u03ba\u03c1\u3008\u2206, H\u2206\u30091/2\u3008f \u2032(\u03b81), f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81)\u3009\ne \u221a \u03ba\u03c1d12 \u2212\u221a\u03ba\u03c1d12 \u2212 1\n\u03ba\u03c1d212\n6 \u221a \u03ba\u03c1Se \u221a \u03ba\u03c1d1\u3008\u2206, H\u2206\u30091/2\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009\ne \u221a \u03ba\u03c1d12 \u2212\u221a\u03ba\u03c1d12 \u2212 1\n\u03ba\u03c1d212 .\nWe then optimize with respect to \u2206 to obtain the desired result.\nThe following proposition shows how the Newton decrement is upper bounded by a function of the excess risk.\nProposition 10 (Newton decrement) Assume (B3-4), and \u03b81 \u2208 H, then,\n\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009 6 ( 1\n2\n\u221a \u03ba\u03c1\u22061 +\n\u221a\nd21 + \u221a \u03ba\u03c1d1\u22061 + 1\n4 \u03ba\u03c1\u220621\n)2\n, (42)\nwith d1 = \u221a \u3008\u03b81 \u2212 \u03b8\u2217, H(\u03b81 \u2212 \u03b8\u2217)\u3009 and \u22061 = f(\u03b81)\u2212 f(\u03b8\u2217).\nProof. We may bound the Newton decrement as follows:\n\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009 6 \u3008f \u2032(\u03b81)\u2212H(\u03b81 \u2212 \u03b8\u2217), H\u22121f \u2032(\u03b81)\u3009+ \u3008H(\u03b81 \u2212 \u03b8\u2217), H\u22121f \u2032(\u03b81)\u3009 6 \u221a \u03ba\u03c1 [ f(\u03b81)\u2212 f(\u03b8\u2217) ] \u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u30091/2 + \u3008f \u2032(\u03b81), \u03b81 \u2212 \u03b8\u2217\u3009. (43)\nThis leads to\n\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009 6 ( 1\n2\n\u221a \u03ba\u03c1 [ f(\u03b81)\u2212 f(\u03b8\u2217) ] +\n\u221a\n\u3008f \u2032(\u03b81), \u03b81 \u2212 \u03b8\u2217\u3009+ 1\n4 \u03ba\u03c1\n[ f(\u03b81)\u2212 f(\u03b8\u2217) ]2 )2 .\nMoreover,\n\u3008f \u2032(\u03b81), \u03b81 \u2212 \u03b8\u2217\u3009 = \u3008H(\u03b81 \u2212 \u03b8\u2217), \u03b81 \u2212 \u03b8\u2217\u3009+ \u3008f \u2032(\u03b81)\u2212H(\u03b81 \u2212 \u03b8\u2217), \u03b81 \u2212 \u03b8\u2217\u3009 6 \u3008H(\u03b81 \u2212 \u03b8\u2217), \u03b81 \u2212 \u03b8\u2217\u3009+ \u221a \u03ba\u03c1\u3008\u03b81 \u2212 \u03b8\u2217, H(\u03b81 \u2212 \u03b8\u2217)\u30091/2 [ f(\u03b81)\u2212 f(\u03b8\u2217) ]\n6 d21 + \u221a \u03ba\u03c1d1 [ f(\u03b81)\u2212 f(\u03b8\u2217) ] .\nOverall, we get\n\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009\n6\n(\n1\n2\n\u221a \u03ba\u03c1 [ f(\u03b81)\u2212 f(\u03b8\u2217) ] +\n\u221a\nd21 + \u221a \u03ba\u03c1d1 [ f(\u03b81)\u2212 f(\u03b8\u2217) ] + 1\n4 \u03ba\u03c1\n[ f(\u03b81)\u2212 f(\u03b8\u2217) ]2 )2 .\nThe following proposition provides a bound on a quantity which is not the Newton decrement. Indeed, this is (up to the difference in the Hessians), the norm of the Newton step. This will be key in the following proofs.\nProposition 11 (Bounding gradients from unweighted distance to optimum) Assume (B34), and \u03b81 \u2208 H, then,\n\u2016H\u22121f \u2032(\u03b81)\u2016 6 e \u221a \u03ba\u03c1d1 \u2212 1\u221a \u03ba\u03c1d1 \u2016\u03b81 \u2212 \u03b8\u2217\u2016, (44)\nwith d1 = \u221a \u3008\u03b81 \u2212 \u03b8\u2217, H(\u03b81 \u2212 \u03b8\u2217)\u3009.\nProof. We have:\n\u2016H\u22121f \u2032(\u03b81)\u2016 6 \u2016H\u22121 [ f \u2032(\u03b81)\u2212H(\u03b81 \u2212 \u03b8\u2217) ] \u2016+ \u2016H\u22121 [ H(\u03b81 \u2212 \u03b8\u2217) ] \u2016\n6 \u2016\u03b81 \u2212 \u03b8\u2217\u2016 ( 1 + e \u221a \u03ba\u03c1d1 \u2212 1\u2212\u221a\u03ba\u03c1d1\u221a\n\u03ba\u03c1d1\n)\n.\nThe next proposition shows that having a small Newton decrement implies that the weighted distance to optimum is small.\nProposition 12 (Weighted distance to optimum) Assume (B3-4). If we have \u221a \u03ba\u03c1e \u221a \u03ba\u03c1d\u3008f \u2032(\u03b8), H\u22121f \u2032(\u03b8)\u30091/2 6 1 2 , with d = \u221a \u3008\u03b8 \u2212 \u03b8\u2217, H(\u03b8 \u2212 \u03b8\u2217)\u3009, then\nd 6 4e \u221a \u03ba\u03c1d\u3008f \u2032(\u03b8), H\u22121f \u2032(\u03b8)\u30091/2.\nProof. For any \u2206 \u2208 H such that \u3008\u2206, H\u2206\u3009 = 1, and t > 0, we have, following the same reasoning than for Prop. 7:\nf(\u03b8 + t\u2206) > f(\u03b8) + t\u3008\u2206, f \u2032(\u03b8)\u3009 + \u3008\u2206, f \u2032\u2032(\u03b8)\u2206\u3009e \u2212vt + vt\u2212 1\nv2\n> f(\u03b8) + \u3008\u2206, f \u2032\u2032(\u03b8)\u2206\u3009\nv2\n[\ne\u2212vt \u2212 1 + tv ( 1\u2212 s )\n]\nwith v = \u221a \u03ba\u03c1 \u221a \u3008\u2206, H\u2206\u3009 = \u221a\u03ba\u03c1 and\ns = v|\u3008\u2206, f \u2032(\u03b8)\u3009| \u3008\u2206, f \u2032\u2032(\u03b8)\u2206\u3009 6\n\u221a \u03ba\u03c1\u3008f \u2032(\u03b8), f \u2032\u2032(\u03b8)\u22121f \u2032(\u03b8)\u30091/2\n\u3008\u2206, f \u2032\u2032(\u03b8)\u2206\u30091/2 6 \u221a \u03ba\u03c1e \u221a \u03ba\u03c1d\u3008f \u2032(\u03b8), H\u22121f \u2032(\u03b8)\u30091/2.\nIt is shown in [25] that if s \u2208 [0, 1), then\ne\u22122s/(1\u2212s) + (1\u2212 s)2s(1\u2212 s)\u22121 \u2212 1 > 0.\nThis implies that if s 6 1/2, for t = 2 \u221a \u03ba\u03c1\u22121s\n1\u2212s , f(\u03b82 + t\u2206) > f(\u03b82). Thus,\nd = \u221a \u3008\u03b8 \u2212 \u03b8\u2217, H(\u03b8 \u2212 \u03b8\u2217)\u3009 6 t 6 4 \u221a \u03ba\u03c1 \u22121 s 6 4e \u221a \u03ba\u03c1d\u3008f \u2032(\u03b8), H\u22121f \u2032(\u03b8)\u30091/2. (45)\nNote that the quantity d appears twice in the result above.\nE.3 Proof of Prop. 1\nIn this section, we prove Prop. 1 using tools from self-concordance analysis. These tools are described in the previous Sections E.1 and E.2. In order to understand the proof, it is preferable to read these sections first.\nWe use the notation t2 = \u03ba\u03c1\u03b51. We then get d 2 1 def = \u3008\u03b81\u2212\u03b8\u2217, H(\u03b81\u2212\u03b8\u2217)\u3009 6 (3+ t2)\u03b51 from Prop. 4.\nProof of Eq. (25). We have, from Prop. 8,\nf(\u03b83)\u2212 f(\u03b8\u2217) 6 f(\u03b82)\u2212 f(\u03b8\u2217) + \u221a \u03c1\u2016H1/2(\u03b83 \u2212 \u03b82)\u2016\n6 f(\u03b82)\u2212 f(\u03b8\u2217) + \u221a 2\u03c1\u03b52e \u221a \u03ba\u03c1d1/2. (46)\nMoreover, we have, also from Prop. 8, f(\u03b82)\u2212 f(\u03b8\u2217) 6 \u221a \u03c1\u2016H1/2f \u2032\u2032(\u03b81)\u22121f \u2032(\u03b81)\u2016, and using Prop. 5, we get f(\u03b82)\u2212 f(\u03b8\u2217) 6 e \u221a \u03ba\u03c1d1 \u221a \u03c1\u2016H\u22121/2f \u2032(\u03b81)\u2016. (47)\nWe may now use Prop. 10 and use the bound:\n\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009 6 ( 1\n2\n\u221a \u03ba\u03c1\u03b51 +\n\u221a\n(3 + t2)\u03b51 + \u221a \u03ba\u03c1 \u221a (3 + t2)\u03b51\u03b51 + 1\n4 \u03ba\u03c1\u03b521\n)2\n6\n(\n1 2 t \u221a \u03b51 +\n\u221a\n(3 + t2)\u03b51 + t \u221a (3 + t2)\u03b51 + 1\n4 t2\u03b51\n)2\n=\n(\n1 2 t+\n\u221a\n(3 + t2) + t \u221a (3 + t2) + 1 4 t2 )2 \u03b51 def = 1(t) 2\u03b51. (48)\nA simple plot shows that for all t > 0,\n1(t) = 1\n2 t+\n\u221a\n(3 + t2) + t \u221a (3 + t2) + 1\n4 t2 6\n\u221a 3 + 2t. (49)\nCombining with Eq. (46) and Eq. (47), we get\nf(\u03b83)\u2212 f(\u03b8\u2217) 6 e \u221a 3+t2t\u221a\u03c1\u03b51( \u221a 3 + 2t) + \u221a 2\u03c1\u03b52e \u221a 3+t2t/2,\nwhich is exactly Eq. (25).\nProof of Eq. (26) and Eq. (27). For these two inequalities, the starting point is the same. Using Eq. (48) (i.e., the Newton decrement at \u03b81), we first show that the distances d12 and d2 are bounded. Using f \u2032\u2032(\u03b81) < e\u2212 \u221a \u03ba\u03c1d1H (Prop. 5):\nd212 6 e \u221a \u03ba\u03c1d1\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u3009 6 et \u221a 3+t2 1(t) 2\u03b51 def = 2(t) 2\u03b51,\nand thus\nd2 6 d1 + d12 6\n[\n\u221a\n3 + t2 + 2(t) ]\u221a \u03b51.\nNow, we can bound the Newton decrement at \u03b82, using Prop. 9:\n\u3008f \u2032(\u03b82), H\u22121f \u2032(\u03b82)\u3009 6 \u03ba\u03c1e2 \u221a \u03ba\u03c1d1\u3008f \u2032(\u03b81), H\u22121f \u2032(\u03b81)\u30092\n( e \u221a \u03ba\u03c1d12 \u2212\u221a\u03ba\u03c1d12 \u2212 1\n( \u221a \u03ba\u03c1d12)2\n)2\n6 \u03ba\u03c1\u03b521 2(t) 4\n(\net 2(t) \u2212 t 2(t)\u2212 1 (t 2(t))2 )2 def = 3(t)\u03ba\u03c1\u03b5 2 1.\nThus, using Prop. 12, if \u03ba\u03c1e2 \u221a \u03ba\u03c1d2\u3008f \u2032(\u03b82), H\u22121f \u2032(\u03b82)\u3009 6 t4e2t[ \u221a 3+t2+ 2(t)] 3(t) 6 1 4 , then\nd2 6 4e \u221a \u03ba\u03c1d2 \u221a 3(t)\u03ba\u03c1\u03b521 6 4e t[ \u221a 3+t2+ 2(t)] \u221a 3(t)\u03ba\u03c1\u03b521 def = 4(t) \u221a \u03ba\u03c1\u03b521.\nWe then have\nd3 = \u221a \u3008\u03b83 \u2212 \u03b8\u2217, H(\u03b83 \u2212 \u03b8\u2217)\u3009 6 \u221a \u3008\u03b83 \u2212 \u03b82, H(\u03b83 \u2212 \u03b82)\u3009+ \u221a \u3008\u03b82 \u2212 \u03b8\u2217, H(\u03b82 \u2212 \u03b8\u2217)\u3009 = d23 + d2 6 4(t) \u221a \u03ba\u03c1\u03b521 + \u221a 2\u03b52e t \u221a 3+t2/2\nd3 \u221a \u03ba\u03c1 6 4(t)t 2 + \u221a 2\u03b52\u03ba\u03c1e t \u221a 3+t2/2 6 4(t)t 2 + \u221a 2u2et \u221a 3+t2/2\nwhere \u03b52\u03ba\u03c1 6 u 2.\nWe now have two separate paths to obtain Eq. (26) and Eq. (27).\nIf we assume that \u03b52 is bounded, i.e., with t = 1/16 and u = 1/4, then, one can check computationally that we obtain d3 \u221a \u03ba\u03c1 6 0.41 and thus b = 0.576 below:\nf(\u03b83)\u2212 f(\u03b8\u2217) 6 e \u221a \u03ba\u03c1d3 \u2212\u221a\u03ba\u03c1d3 \u2212 1\n\u03ba\u03c1 using Prop. 2,\n6 d23 max \u03b1\u2208[0,0.41 e\u03b1 \u2212 \u03b1\u2212 1 \u03b12 6 0.576 ( 4(t) \u221a \u03ba\u03c1\u03b521 + \u221a 2\u03b52e t \u221a 3+t2/2 )2\n6 0.576(1 + 1/c) 4(t) 2\u03ba\u03c1\u03b521 + 2\u00d7 0.576(1 + c)et \u221a 3+t2\u03b52 6 57\u03ba\u03c1\u03b521 + 12\u03b52, with c = 8.1,\nwhich is exactly Eq. (27). If we only assume \u03b51 bounded, then we have (from the beginning of the proof):\nf(\u03b83)\u2212 f(\u03b8\u2217) 6 f(\u03b82)\u2212 f(\u03b8\u2217) + \u221a \u03c1\u2016H1/2(\u03b83 \u2212 \u03b82)\u2016\n6 57\u03ba\u03c1\u03b521 + \u221a 2\u03c1\u03b52e t \u221a 3+t2/2 6 57\u03ba\u03c1\u03b521 + 2 \u221a \u03c1\u03b52,\nbecause we may use the earlier reasoning with \u03b53 = 0. This is Eq. (26).\nIn Table 2, we describe the datasets we have used in experiments and where they were downloaded from.\nIn Figure 4, we provide similar results than in Section 4, for two additional datasets, quantum and rcv1, while in Figure 5, Figure 6 and Figure 7, we provide training objectives for all methods. We can make the following observations:\n\u2013 For non-sparse datasets, SAG manages to get the smallest training error, confirming the results of [27].\n\u2013 For the high-dimensional sparse datasets, constant step-size SGD is performing best (note that as shown in Section 3, it is not converging to the optimal value in general, this happens notably for the alpha dataset).\nReferences\n[1] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.\n[2] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855, 1992.\n[3] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Adv. NIPS, 2008.\n[4] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In Proc. ICML, 2007.\n[5] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.\n[6] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Adv. NIPS, 2011.\n[7] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2010.\n[8] A. S. Nemirovsky and D. B. Yudin. Problem complexity and method efficiency in optimization. Wiley & Sons, 1983.\n[9] A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of convex optimization. In Adv. NIPS, 2009.\n[10] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic Publishers, 2004.\n[11] G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2):365\u2013397, 2012.\n[12] H. J. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and applications. Springer-Verlag, second edition, 2003.\n[13] C. Gu. Smoothing spline ANOVA models. Springer, 2002.\n[14] R. Aguech, E. Moulines, and P. Priouret. On a perturbation approach for the analysis of stochastic tracking algorithms. SIAM J. Control and Optimization, 39(3):872\u2013899, 2000.\n[15] A. B. Tsybakov. Optimal rates of aggregation. In Proc. COLT, 2003.\n[16] O. Macchi. Adaptive processing: The least mean squares approach with applications in transmission. Wiley West Sussex, 1995.\n[17] S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Cambridge University Press, London, 2009.\n[18] A. Hyva\u0308rinen and E. Oja. A fast fixed-point algorithm for independent component analysis. Neural computation, 9(7):1483\u20131492, 1997.\n[19] N.J. Bershad. Analysis of the normalized lms algorithm with gaussian inputs. IEEE Transactions on Acoustics, Speech and Signal Processing, 34(4):793\u2013806, 1986.\n[20] A. Nedic and D. Bertsekas. Convergence rate of incremental subgradient algorithms. Stochastic Optimization: Algorithms and Applications, pages 263\u2013304, 2000.\n[21] F. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression. Technical Report 00804431, HAL, 2013.\n[22] V. S. Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291\u2013294, 1997.\n[23] A. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge Univ. press, 2000.\n[24] Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex programming. SIAM studies in Applied Mathematics, 1994.\n[25] F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384\u2013 414, 2010.\n[26] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proc. COLT, 2001.\n[27] N. Le Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets. In Adv. NIPS, 2012.\n[28] I. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, 22(4):pp. 1679\u20131706, 1994.\n[29] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2003."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/ \u221a", "creator": "LaTeX with hyperref package"}}}