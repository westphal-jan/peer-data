{"id": "1306.2158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2013", "title": "\"Not not bad\" is not \"bad\": A distributional account of negation", "abstract": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models. In summary, we propose a multidecene framework in which we propose a multidecene model to describe a multidecene representation of semantics, where the semantics of the same logic are expressed with a single operator. For the first time, our multidecene model is a robust and flexible representation of semantics, where all the rules on negation are contained in one or more monad-specific (i.e., one monad-specific) semantics. In addition to the monad-specific semantics, we propose the multidecene framework for general and general semantics for compositional semantics that are not currently under development, such as: (a) the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the negation of the", "histories": [["v1", "Mon, 10 Jun 2013 10:29:09 GMT  (25kb,D)", "http://arxiv.org/abs/1306.2158v1", "9 pages, to appear in Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality"]], "COMMENTS": "9 pages, to appear in Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karl moritz hermann", "edward grefenstette", "phil blunsom"], "accepted": false, "id": "1306.2158"}, "pdf": {"name": "1306.2158.pdf", "metadata": {"source": "CRF", "title": "\u201cNot not bad\u201d is not \u201cbad\u201d: A distributional account of negation", "authors": ["Karl Moritz Hermann", "Edward Grefenstette", "Phil Blunsom"], "emails": ["firstname.lastname@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Distributional models of semantics characterize the meanings of words as a function of the words they co-occur with (Firth, 1957). These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Schu\u0308tze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.\nDuring the past few years, research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences. This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.). Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell\nand Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al., 2012). On the non-compositional front, Erk and Pado\u0301 (2008) keep word vectors separate, using syntactic information from sentences to disambiguate words in context; likewise Turney (2012) treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition.\nThese compositional distributional approaches often portray themselves as attempts to reconcile the empirical aspects of distributional semantics with the structured aspects of formal semantics. However, they in fact only principally co-opt the syntax-sensitivity of formal semantics, while mostly eschewing the logical aspects.\nExpressing the effect of logical operations in high dimensional distributional semantic models is a very different task than in boolean logic. For example, whereas predicates such as \u2018red\u2019 are seen in predicate calculi as functions mapping elements of some set Mred to > (and all other domain elements to \u22a5), in compositional distributional models we give the meaning of \u2018red\u2019 a vector-like representation, and devise some combination operation with noun representations to obtain the representation for an adjective-noun pair. Under the logical view, negation of a predicate therefore yields a new truth-function mapping elements of the complement of Mred to > (and all other domain elements to\u22a5), but the effect of negation and other logical operations in distributional models is not so sharp: we expect the representation for \u201cnot red\u201d to remain close to other objects of the same domain of discourse (i.e. other colours) while being sufficiently different from the representation of \u2018red\u2019 in some manner. Exactly how textual logic ar X iv :1 30 6.\n21 58\nv1 [\ncs .C\nL ]\n1 0\nJu n\n20 13\nwould best be represented in a continuous vector space model remains an open problem.\nIn this paper we propose one possible formulation for a continuous vector space based representation of semantics. We use this formulation as the basis for providing an account of logical operations for distributional models. In particular, we focus on the case of negation and how it might work in higher dimensional distributional models. Our formulation separates domain, value and functional representation in such a way as to allow negation to be handled naturally. We explain the linguistic and model-related impacts of this mode of representation and discuss how this approach could be generalised to other semantic functions.\nIn Section 2, we provide an overview of work relating to that presented in this paper, covering the integration of logical elements in distributional models, and the integration of distributional elements in logical models. In Section 3, we introduce and argue for a tripartite representation in distributional semantics, and discuss the issues relating to providing a linguistically sensible notion of negation for such representations. In Section 4, we present matrix-vector models similar to that of Socher et al. (2012) as a good candidate for expressing this tripartite representation. We argue for the elimination of non-linearities from such models, and thus show that negation cannot adequately be captured. In Section 5, we present a short analysis of the limitation of these matrixvector models with regard to the task of modelling non-boolean logical operations, and present an improved model bypassing these limitations in Section 6. Finally, in Section 7, we conclude by suggesting future work which will extend and build upon the theoretical foundations presented in this paper."}, {"heading": "2 Motivation and Related Work", "text": "The various approaches to combining logic with distributional semantics can broadly be put into three categories: those approaches which use distributional models to enhance existing logical tools; those which seek to replicate logic with the mathematical constructs of distributional models; and those which provide new mathematical definitions of logical operations within distributional semantics. The work presented in this paper is in the third category, but in this section we will also pro-\nvide a brief overview of related work in the other two in order to better situate the work this paper will describe in the literature.\nVector-assisted logic The first class of approaches seeks to use distributional models of word semantics to enhance logic-based models of textual inference. The work which best exemplifies this strand of research is found in the efforts of Garrette et al. (2011) and, more recently, Beltagy et al. (2013). This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pado\u0301 (2008) to deal with issues polysemy and ambiguity.\nAs this class of approaches deals with improving logic-based models rather than giving a distributional account of logical function words, we view such models as orthogonal to the effort presented in this paper.\nLogic with vectors The second class of approaches seeks to integrate boolean-like logical operations into distributional semantic models using existing mechanisms for representing and composing semantic vectors. Coecke et al. (2010) postulate a mathematical framework generalising the syntax-semantic passage of Montague Grammar (Montague, 1974) to other forms of syntactic and semantic representation. They show that the parses yielded by syntactic calculi satisfying certain structural constraints can be canonically mapped to vector combination operations in distributional semantic models. They illustrate their framework by demonstrating how the truth-value of sentences can be obtained from the combination of vector representations of words and multilinear maps standing for logical predicates and relations. They furthermore give a matrix interpretation of negation as a \u2018swap\u2019 matrix which inverts the truth-value of vectorial sentence representations, and show how it can be embedded in sentence structure.\nRecently, Grefenstette (2013) showed that the examples from this framework could be extended to model a full quantifier-free predicate logic using tensors of rank 3 or lower. In parallel, Socher et al. (2012) showed that propositional logic can be learned using tensors of rank 2 or lower (i.e. only matrices and vectors) through the use of non-linear\nactivation functions in recursive neural networks. The work of Coecke et al. (2010) and Grefenstette (2013) limits itself to defining, rather than learning, distributional representations of logical operators for distributional models that simulate logic, and makes no pretense to the provision of operations which generalise to higher-dimensional distributional semantic representations. As for the non-linear approach of Socher et al. (2012), we will discuss, in Section 4 below, the limitations with this model with regard to the task of modelling logic for higher dimensional representations.\nLogic for vectors The third and final class of approaches is the one the work presented here belongs to. This class includes attempts to define representations for logical operators in high dimensional semantic vector spaces. Such approaches do not seek to retrieve boolean logic and truth values, but to define what logical operators mean when applied to distributional representations. The seminal work in this area is found in the work of Widdows and Peters (2003), who define negation and other logical operators algebraically for high dimensional semantic vectors. Negation, under this approach, is effectively a binary relation rather than a unary relation: it expresses the semantics of statements such as \u2018A NOT B\u2019 rather than merely \u2018NOT B\u2019, and does so by projecting the vector for A into the orthogonal subspace of the vector for B. This approach to negation is useful for vector-based information retrieval models, but does not necessarily capture all the aspects of negation we wish to take into consideration, as will be discussed in Section 3."}, {"heading": "3 Logic in text", "text": "In order to model logical operations over semantic vectors, we propose a tripartite meaning representation, which combines the separate and distinct treatment of domain-related and value-related aspects of semantic vectors with a domain-driven syntactic functional representation. This is a unification of various recent approaches to the problem of semantic representation in continuous distributional semantic modelling (Socher et al., 2012; Turney, 2012; Hermann and Blunsom, 2013).\nWe borrow from Socher et al. (2012) and others (Baroni and Zamparelli, 2010; Coecke et al., 2010) the idea that the information words refer to is of two sorts: first the semantic content of the\nword, which can be seen as the sense or reference to the concept the word stands for, and is typically modelled as a semantic vector; and second, the function the word has, which models the effect the word has on other words it combines with in phrases and sentences, and is typically modelled as a matrix or higher-order tensor. We borrow from Turney (2012) the idea that the semantic aspect of a word should not be modelled as a single vector where everything is equally important, but ideally as two or more vectors (or, as we do here, two or more regions of a vector) which stand for the aspects of a word relating to its domain, and those relating to its value.\nWe therefore effectively suggest a tripartite representation of the semantics of words: a word\u2019s meaning is modelled by elements representing its value, domain, and function, respectively.\nThe tripartite representation We argue that the tripartite representation suggested above allows us to explicitly capture several aspects of semantics. Further, while there may be additional distinct aspects of semantics, we argue that this is a minimal viable representation.\nFirst of all, the differentiation between domain and value is useful for establishing similarity within subspaces of meaning. For instance, the words blue and red share a common domain (colours) while having very different values. We hypothesise that making this distinction explicit will allow for the definition of more sophisticated and fine-grained semantics of logical operations, as discussed below. Although we will represent domain and value as two regions of a vector, there is no reason for these not to be treated as separate vectors at the time of comparison, as done by Turney (2012).\nThrough the third part, the functional representation, we capture the compositional aspect of semantics: the functional representation governs how a term interacts with its environment. Inspired by the distributional interpretation (Baroni and Zamparelli, 2010; Coecke et al., 2010) of syntactically-paramatrized semantic composition functions from Montogovian semantics (Montague, 1974), we will also assume the function part of our representation to be parametrized principally by syntax and domain rather than value. The intuition behind taking domain into account in addition to syntactic class being that all members of a domain largely interact with their environment\nin the same fashion.\nModeling negation The tripartite representation proposed above allows us to define logical operations in more detail than competing approaches. To exemplify this, we focus on the case of negation.\nWe define negation for semantic vectors to be the absolute complement of a term in its domain. This implies that negation will not affect the domain of a term but only its value. Thus, blue and not blue are assumed to share a common domain. We call this naive form of negation the inversion of a term A, which we idealise as the partial inversion Ainv of the region associated with the value of the word in its vector representation A.\nAdditionally, we expect negation to have a diminutive effect. This diminutive effect is best exemplified in the case of sentiment: good is more positive than not bad, even though good and bad are antonyms of each other. By extension not not good and not not not bad end up somewhere in the middle\u2014qualitative statements still, but void of any significant polarity. To reflect this diminutive effect of negation and double negation commonly found in language, we define the idealised diminutive negation \u00acA of a semantic vectorA as a scalar inversion over a segment of the value region of its representation with the scalar \u00b5 : 0 < \u00b5 < 1, as shown in Figure 1.\nAs we defined the functional part of our representation to be predominately parametrized by syntax and domain, it will remain constant under negation and inversion."}, {"heading": "4 A general matrix-vector model", "text": "Having discussed, above, how the vector component of a word can be partitioned into domain and value, we now turn to the partition between semantic content and function. A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation.\nModels using dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al. (2012). We use the class of recursive matrix-vector models as the basis for our investigation; for a detailed introduction see the MV-RNN model described in Socher et al. (2012).\nWe begin by describing composition for a general dual-space model, and apply this model to the notion of compositional logic in a tripartite representation discussed earlier. We identify the shortcomings of the general model and subsequently discuss alternative composition models and modifications that allow us to better capture logic in vector space models of meaning.\nAssume a basic model of compositionality for such a tripartite representation as follows. Each term is encoded by a semantic vector v capturing its domain and value, as well as a matrix M capturing its function. Thus, composition consists of two separate operations to learn semantics and function of the composed term:\nvp = fv(va,vb,Ma,Mb) (1)\nMp = fM (Ma,Mb)\nAs we defined the functional representation to be parametrized by syntax and domain, its composition function does not require va and vb as inputs, with all relevant information already being contained in Ma,Mb. In the case of Socher et al. (2012) these functions are as follows:\nMp =WM [ Ma Mb ] (2)\nvp = g ( Wv [ Mavb Mbva ]) (3)\nwhere g is a non-linearity."}, {"heading": "4.1 The question of non-linearities", "text": "While the non-linearity g could be equipped with greater expressive power, such as in the boolean\nlogic experiment in Socher et al. (2012)), the aim of this paper is to place the burden of compositionality on the atomic representations instead. For this reason we treat g as an identity function, and WM , Wv as simple additive matrices in this investigation, by setting\ng = I Wv =WM = [I I]\nwhere I is an identity matrix. This simplification is justified for several reasons.\nA simple non-linearity such as the commonly used hyperbolic tangent or sigmoid function will not add sufficient power to overcome the issues outlined in this paper. Only a highly complex nonlinear function would be able to satisfy the requirements for vector space based logic as discussed above. Such a function would defeat the point however, by pushing the \u201cheavy-lifting\u201d from the model structure into a separate function.\nFurthermore, a non-linearity effectively encodes a scattergun approach: While it may have the power to learn a desired behaviour, it similarly has a lot of power to learn undesired behaviours and side effects. From a formal perspective it would therefore seem more prudent to explicitly encode desired behaviour in a model\u2019s structure rather than relying on a non-linearity."}, {"heading": "4.2 Negation", "text": "We have outlined our formal requirements for negation in the previous section. From these requirements we can deduce four equalities, concerning the effect of negation and double negation on the semantic representation and function of a term. The matrices J\u00b5 and J\u03bd (illustrated in\nFigure 2) describe a partially scaled and inverted identity matrix, where 0 < \u00b5, \u03bd < 1.\nfv(not, a) = J\u00b5va (4) fM (not, a) \u2248Ma (5) fv(not, fv(not, a)) = J\u03bdJ\u00b5va (6) fM (not, fM (not, a)) \u2248Ma (7)\nBased on our assumption about the constant domain and interaction across negation, we can replace the approximate equality with a strict equality in Equations 5 and 7. Further, we assume that both Ma 6= I and Ma 6= 0, i.e. that A has a specific and non-zero functional representation. We make a similar assumption for the semantic representation va 6= 0.\nThus, to satisfy the equalities in Equations 4 through 7, we can deduce the values of vnot and Mnot as discussed below.\nValue and Domain in Negation Under the simplifications of the model explained earlier, we know that the following is true:\nfv(a, b) = g ( Wv [ Mavb Mbva ]) = I ([ I I ] [Mavb Mbva\n]) =Mavb +Mbva\nI.e. the domain and value representation of a parent is the sum of the two Mv multiplications of its children. The matrix Wv could re-weight this addition, but that would not affect the rest of this analysis.\nGiven the idea that the domain stays constant under negation and that a part of the value is inverted and scaled, we further know that these two equations hold:\n\u2200a \u2208 A : fv(not, a) = J\u00b5va \u2200a \u2208 A : fv(not, fv(not, a)) = J\u03bdJ\u00b5va\nAssuming that both semantic and functional representation across all A varies and is non-zero, these equalities imply the following conditions for the representation of not:\nMnot = J\u00b5 = J\u03bd\nvnot = 0\nThese two equations suggest that the term not has no inherent value (vnot = 0), but merely acts as a function, inverting part of another terms semantic representation (Mnot = J\u00b5).\nFunctional Representation in Negation We can apply the same method to the functional representation. Here, we know that:\nfM (a, b) =WM [ Ma Mb ] = [ I I ] [Ma Mb\n] =Ma +Mb\nFurther, as defined in our discussion of negation, we require the functional representation to remain unchanged under negation:\n\u2200a \u2208 A : fM (not, a) =Ma \u2200a \u2208 A : fM (not, fM (not, a)) =Ma\nThese requirements combined leave us to conclude that Mnot = 0. Combined with the result from the first part of the analysis, this causes a contradiction:\nMnot = 0\nMnot = J\u00b5\n=\u21d2 J\u00b5 = 0\nThis demonstrates that the MV-RNN as described in this paper is not capable of modelling semantic logic according to the principles we outlined. The fact that we would require Mnot = 0 further supports the points made earlier about the non-linearities and setting WM to [ I I ] . Even a specific WM and non-linearity would not be able to ensure that the functional representation stays constant under negation given a non-zero Mnot.\nClearly, any other complex semantic representation would suffer from the same issue here\u2014the failure of double-negation to revert a representation to its (diminutive) original."}, {"heading": "5 Analysis", "text": "The issue identified with the MV-RNN style models described above extends to a number of other models of vector spaced compositionality. It can be viewed as a problem of uninformed composition caused by a composition function that fails to account for syntax and thus for scope.\nOf course, identifying the scope of negation is a hard problem in its own right\u2014see e.g. the *SEM 2012 shared task (Morante and Blanco, 2012). However, at least for simple cases, we can deduce scope by considering the parse tree of a sentence:\nIf we consider the parse tree for this car is not blue, it is clear that the scope of the negation expressed includes the colour but not the car (Figure 3).\nWhile the MV-RNN model in Socher et al. (2012) incorporates parse trees to guide the order of its composition steps, it uses a single composition function across all steps. Thus, the functional representation of not will to some extent propagate outside of its scope, leading to a vector capturing something that is not blue, but also not quite a car.\nThere are several possibilities for addressing this issue. One possibility is to give greater weight to syntax, for instance by parametrizing the composition functions fv and fM on the parse structure. This could be achieved by using specific weight matrices Wv and WM for each possible tag. While the power of this approach is limited by the complexity of the parse structure, it would be better able to capture effects such as the scoping and propagation of functional representations.\nAnother approach, which we describe in greater detail in the next section, pushes the issue of propagation onto the word level. While both approaches could easily be combined, this second option is more consistent with our aim of avoiding the implicit encoding of logic into fixed model parameters in favour of the explicit encoding in model structure."}, {"heading": "6 An improved model", "text": "As we outlined in this paper, a key requirement for a compositional model motivated by formal semantics is the ability to propagate functional representations, but also to not propagate these representations when doing so is not semantically appropriate. Here, we propose a modification of the MV-RNN class of models that can capture this dis-\ntinction without the need to move the composition logic into the non-linearity.\nWe add a parameter \u03b1 to the representation of each word, controlling the degree to which its functional representation propagates after having been applied in its own composition step.\nThus, the composition step of the new model requires three equations:\nMp =WM\n[ \u03b1a\n\u03b1a+\u03b1b Ma\n\u03b1b \u03b1a+\u03b1b Mb\n] (8)\nvp = g ( Wv [ Mavb Mbva ]) (9)\n\u03b1p = max(\u03b1a, \u03b1b) (10)\nGoing back to the discussion of negation, this model has the clear advantage of being able to capture negation in the way we defined it. As fv(a, b) is unchanged, these two equations still hold:\nMnot = J\u00b5 = J\u03bd\nvnot = 0\nHowever, as fM (a, b) is changed, the second set of equations changes. We use Z as the \u03b1denominator (Z = \u03b1a + \u03b1B) for simplification:\nfM (a, b) =WM [ \u03b1a Z Ma \u03b1b Z Mb ] = [ I I ] [ \u03b1a Z Ma \u03b1b Z Mb\n] = \u03b1a Z Ma + \u03b1b Z Mb\nFurther, we still require the functional representation to remain constant under negation:\n\u2200a \u2208 A : fM (not, a) =Ma \u2200a \u2208 A : fM (not, fM (not, a)) =Ma\nThus, we can infer the following two conditions on the new model:\n\u03b1not Z\nMnot \u2248 0 \u03b1a Z Ma \u2248Ma\nFrom our previous investigation we already know that Mnot = J\u00b5 6= 0, i.e. that not has a nonzero functional representation. While this caused a contradiction for the original MV-RNN model,\nthe design of the improved model can resolve this issue through the \u03b1-parameter:\n\u03b1not = 0\nThus, we can use this modified MV-RNN model to represent negation according to the principles outlined in this paper. The result \u03b1not = 0 is in accordance with our intuition about the propagation of functional aspects of a term: We commonly expect negation to directly affect the things under its scope (not blue) by choosing their semantic complement. However, this behaviour should not propagate outside of the scope of the negation. A not blue car is still very much a car, and when a film is not good, it is still very much a film."}, {"heading": "7 Discussion and Further Work", "text": "In this paper, we investigated the capability of continuous vector space models to capture the semantics of logical operations in non-boolean cases. Recursive and recurrent vector models of meaning have enjoyed a considerable amount of success in recent years, and have been shown to work well on a number of tasks. However, the complexity and subsequent power of these models comes at the price that it can be difficult to establish which aspect of a model is responsible for what behaviour. This issue was recently highlighted by an investigation into recursive autoencoders for sentiment analysis (Scheible and Schu\u0308tze, 2013). Thus, one of the key challenges in this area of research is the question of how to control the power of these models. This challenge motivated the work in this paper. By removing non-linearities and other parameters that could disguise model weaknesses, we focused our work on the basic model design. While such features enhance model power, they should not be used to compensate for inherently flawed model designs.\nAs a prerequisite for our investigation we established a suitable encoding of textual logic. Distributional representations have been well explained on the word level, but less clarity exists as to the semantic content of compositional vectors. With the tripartite meaning representation we proposed one possible approach in that direction, which we subsequently expanded by discussing how negation should be captured in this representation.\nHaving established a suitable and rigorous system for encoding meaning in compositional vectors, we were thus able to investigate the repre-\nsentative power of the MV-RNN model. We focused this paper on the case of negation, which has the advantage that it does not require many additional assumptions about the underlying semantics. Our investigation showed that the basic MV-RNN model is incompatible with our notion of negation and thus with any textual logic building on this proposal.\nSubsequently, we analysed the reasons for this failure. We explained how the issue of negation affects the general class of MV-RNN models. Through the issue of double-negation we further showed how this issue is largely independent on the particular semantic encoding used. Based on this analysis we proposed an improved model that is able to capture such textual logic.\nIn summary, this paper has two key contributions. First, we developed a tripartite representation for vector space based models of semantics, incorporating multiple previous approaches to this topic. Based on this representation, the second contribution of this paper was a modified MV-RNN model that can capture effects such as negation in its inherent structure.\nIn future work, we would like to build on the proposals in this paper, both by extending our work on textual logic to include formulations for e.g. function words, quantifiers, or locative words. Similarly, we plan to experimentally validate these ideas. Possible tasks for this include sentiment analysis and relation extraction tasks such as in Socher et al. (2012) but also more specific tasks such as the *SEM shared task on negation scope and reversal (Morante and Blanco, 2012)."}, {"heading": "Acknowledgements", "text": "The first author is supported by the UK Engineering and Physical Sciences Research Council (EPSRC). The second author is supported by EPSRC Grant EP/I03808X/1."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Montague meets markov: Deep semantics with probabilistic logical form", "author": ["I. Beltagy", "C. Chau", "G. Boleda", "D. Garrette", "E. Erk", "R. Mooney."], "venue": "June.", "citeRegEx": "Beltagy et al\\.,? 2013", "shortCiteRegEx": "Beltagy et al\\.", "year": 2013}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata."], "venue": "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["J. Bos."], "venue": "Proceedings of the 2008 Conference on Semantics in Text Processing, pages 277\u2013286. Association for Computational Linguistics.", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Mathematical Foundations for a Compositional Distributional Model of Meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark."], "venue": "March.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "From distributional to semantic similarity", "author": ["J.R. Curran."], "venue": "Ph.D. thesis.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "A structured vector space model for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP \u201908, (October):897.", "citeRegEx": "Erk and Pad\u00f3.,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2008}, {"title": "A synopsis of linguistic theory 19301955", "author": ["J.R. Firth."], "venue": "Studies in linguistic analysis.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Integrating logical representations with probabilistic information using markov logic", "author": ["D. Garrette", "K. Erk", "R. Mooney."], "venue": "Proceedings of the Ninth International Conference on Computational Semantics, pages 105\u2013114. Association for Computational", "citeRegEx": "Garrette et al\\.,? 2011", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh."], "venue": "Proceedings of EMNLP, pages 1394\u20131404.", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni."], "venue": "Proceedings of the Tenth International Conference on Computational Semantics. Association for Compu-", "citeRegEx": "Grefenstette et al\\.,? 2013", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["E. Grefenstette."], "venue": "Proceedings of the Second Joint Conference on Lexical and Computational Semantics.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Explorations in automatic thesaurus discovery", "author": ["G. Grefenstette"], "venue": null, "citeRegEx": "Grefenstette.,? \\Q1994\\E", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["K.M. Hermann", "P. Blunsom."], "venue": "Proceedings of ACL, Sofia, Bulgaria, August. Association for Computational Linguistics.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "A unified sentence space for categorical distributionalcompositional semantics: Theory and experiments", "author": ["D. Kartsaklis", "M. Sadrzadeh", "S. Pulman."], "venue": "Proceedings of 24th International Conference on Computational Linguistics (COLING 2012):", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais."], "venue": "Psychological review.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata."], "venue": "Proceedings of ACL, volume 8.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in Distributional Models of Semantics", "author": ["J. Mitchell", "M. Lapata."], "venue": "Cognitive Science.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "English as a Formal Language", "author": ["R. Montague."], "venue": "Formal Semantics: The Essential Readings.", "citeRegEx": "Montague.,? 1974", "shortCiteRegEx": "Montague.", "year": 1974}, {"title": "SEM 2012 shared task: resolving the scope and focus of negation", "author": ["R. Morante", "E. Blanco."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task,", "citeRegEx": "Morante and Blanco.,? 2012", "shortCiteRegEx": "Morante and Blanco.", "year": 2012}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos."], "venue": "Machine learning, 62(1-2):107\u2013136.", "citeRegEx": "Richardson and Domingos.,? 2006", "shortCiteRegEx": "Richardson and Domingos.", "year": 2006}, {"title": "Cutting recursive autoencoder trees", "author": ["C. Scheible", "H. Sch\u00fctze."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Scheible and Sch\u00fctze.,? 2013", "shortCiteRegEx": "Scheible and Sch\u00fctze.", "year": 2013}, {"title": "Automatic word sense discrimination", "author": ["H. Sch\u00fctze."], "venue": "Computational linguistics, 24(1):97\u2013123.", "citeRegEx": "Sch\u00fctze.,? 1998", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning."], "venue": "Advances in Neural Information Processing Systems, 24:801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng."], "venue": "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing, pages 1201\u20131211.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Domain and function: A dualspace model of semantic relations and compositions", "author": ["P.D. Turney."], "venue": "Journal of Artificial Intelligence Research, 44:533\u2013 585.", "citeRegEx": "Turney.,? 2012", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "Word vectors and quantum logic: Experiments with negation and disjunction", "author": ["D. Widdows", "S. Peters."], "venue": "Mathematics of language, 8(141-154).", "citeRegEx": "Widdows and Peters.,? 2003", "shortCiteRegEx": "Widdows and Peters.", "year": 2003}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["F.M. Zanzotto", "I. Korkontzelos", "F. Fallucchi", "S. Manandhar."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 1263\u20131271. Associa-", "citeRegEx": "Zanzotto et al\\.,? 2010", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "Distributional models of semantics characterize the meanings of words as a function of the words they co-occur with (Firth, 1957).", "startOffset": 116, "endOffset": 129}, {"referenceID": 12, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 152, "endOffset": 186}, {"referenceID": 5, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 152, "endOffset": 186}, {"referenceID": 22, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 214, "endOffset": 229}, {"referenceID": 15, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 255, "endOffset": 282}, {"referenceID": 16, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 17, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 9, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 2, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 24, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.", "startOffset": 231, "endOffset": 279}, {"referenceID": 13, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.", "startOffset": 231, "endOffset": 279}, {"referenceID": 16, "context": "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 130, "endOffset": 207}, {"referenceID": 17, "context": "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 130, "endOffset": 207}, {"referenceID": 27, "context": "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 130, "endOffset": 207}, {"referenceID": 0, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 4, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 10, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 14, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 23, "context": ", 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al.", "startOffset": 55, "endOffset": 103}, {"referenceID": 13, "context": ", 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al.", "startOffset": 55, "endOffset": 103}, {"referenceID": 24, "context": ", 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al., 2012).", "startOffset": 54, "endOffset": 75}, {"referenceID": 0, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al., 2012). On the non-compositional front, Erk and Pad\u00f3 (2008) keep word vectors separate, using syntactic information from sentences to disambiguate words in context; likewise Turney (2012) treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition.", "startOffset": 44, "endOffset": 336}, {"referenceID": 0, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al., 2012). On the non-compositional front, Erk and Pad\u00f3 (2008) keep word vectors separate, using syntactic information from sentences to disambiguate words in context; likewise Turney (2012) treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition.", "startOffset": 44, "endOffset": 464}, {"referenceID": 23, "context": "In Section 4, we present matrix-vector models similar to that of Socher et al. (2012) as a good candidate for expressing this tripartite representation.", "startOffset": 65, "endOffset": 86}, {"referenceID": 3, "context": "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 103, "endOffset": 114}, {"referenceID": 20, "context": "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 142, "endOffset": 173}, {"referenceID": 5, "context": "The work which best exemplifies this strand of research is found in the efforts of Garrette et al. (2011) and, more recently, Beltagy et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 1, "context": "(2011) and, more recently, Beltagy et al. (2013). This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 27, "endOffset": 49}, {"referenceID": 1, "context": "(2011) and, more recently, Beltagy et al. (2013). This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 27, "endOffset": 308}, {"referenceID": 18, "context": "(2010) postulate a mathematical framework generalising the syntax-semantic passage of Montague Grammar (Montague, 1974) to other forms of syntactic and semantic representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 4, "context": "Coecke et al. (2010) postulate a mathematical framework generalising the syntax-semantic passage of Montague Grammar (Montague, 1974) to other forms of syntactic and semantic representation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Recently, Grefenstette (2013) showed that the examples from this framework could be extended to model a full quantifier-free predicate logic using tensors of rank 3 or lower.", "startOffset": 10, "endOffset": 30}, {"referenceID": 11, "context": "Recently, Grefenstette (2013) showed that the examples from this framework could be extended to model a full quantifier-free predicate logic using tensors of rank 3 or lower. In parallel, Socher et al. (2012) showed that propositional logic can be learned using tensors of rank 2 or lower (i.", "startOffset": 10, "endOffset": 209}, {"referenceID": 4, "context": "The work of Coecke et al. (2010) and Grefenstette (2013) limits itself to defining, rather than learning, distributional representations of logical operators for distributional models that simulate logic, and makes no pretense to the provision of operations which generalise to higher-dimensional distributional semantic representations.", "startOffset": 12, "endOffset": 33}, {"referenceID": 4, "context": "The work of Coecke et al. (2010) and Grefenstette (2013) limits itself to defining, rather than learning, distributional representations of logical operators for distributional models that simulate logic, and makes no pretense to the provision of operations which generalise to higher-dimensional distributional semantic representations.", "startOffset": 12, "endOffset": 57}, {"referenceID": 4, "context": "The work of Coecke et al. (2010) and Grefenstette (2013) limits itself to defining, rather than learning, distributional representations of logical operators for distributional models that simulate logic, and makes no pretense to the provision of operations which generalise to higher-dimensional distributional semantic representations. As for the non-linear approach of Socher et al. (2012), we will discuss, in Section 4 below, the limitations with this model with regard to the task of modelling logic for higher dimensional representations.", "startOffset": 12, "endOffset": 393}, {"referenceID": 26, "context": "The seminal work in this area is found in the work of Widdows and Peters (2003), who define negation and other logical operators algebraically for high dimensional semantic vectors.", "startOffset": 54, "endOffset": 80}, {"referenceID": 24, "context": "This is a unification of various recent approaches to the problem of semantic representation in continuous distributional semantic modelling (Socher et al., 2012; Turney, 2012; Hermann and Blunsom, 2013).", "startOffset": 141, "endOffset": 203}, {"referenceID": 25, "context": "This is a unification of various recent approaches to the problem of semantic representation in continuous distributional semantic modelling (Socher et al., 2012; Turney, 2012; Hermann and Blunsom, 2013).", "startOffset": 141, "endOffset": 203}, {"referenceID": 13, "context": "This is a unification of various recent approaches to the problem of semantic representation in continuous distributional semantic modelling (Socher et al., 2012; Turney, 2012; Hermann and Blunsom, 2013).", "startOffset": 141, "endOffset": 203}, {"referenceID": 0, "context": "(2012) and others (Baroni and Zamparelli, 2010; Coecke et al., 2010) the idea that the information words refer to is of two sorts: first the semantic content of the word, which can be seen as the sense or reference to the concept the word stands for, and is typically modelled as a semantic vector; and second, the function the word has, which models the effect the word has on other words it combines with in phrases and sentences, and is typically modelled as a matrix or higher-order tensor.", "startOffset": 18, "endOffset": 68}, {"referenceID": 4, "context": "(2012) and others (Baroni and Zamparelli, 2010; Coecke et al., 2010) the idea that the information words refer to is of two sorts: first the semantic content of the word, which can be seen as the sense or reference to the concept the word stands for, and is typically modelled as a semantic vector; and second, the function the word has, which models the effect the word has on other words it combines with in phrases and sentences, and is typically modelled as a matrix or higher-order tensor.", "startOffset": 18, "endOffset": 68}, {"referenceID": 21, "context": "We borrow from Socher et al. (2012) and others (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 0, "context": "(2012) and others (Baroni and Zamparelli, 2010; Coecke et al., 2010) the idea that the information words refer to is of two sorts: first the semantic content of the word, which can be seen as the sense or reference to the concept the word stands for, and is typically modelled as a semantic vector; and second, the function the word has, which models the effect the word has on other words it combines with in phrases and sentences, and is typically modelled as a matrix or higher-order tensor. We borrow from Turney (2012) the idea that the semantic aspect of a word should not be modelled as a single vector where everything is equally important, but ideally as two or more vectors (or, as we do here, two or more regions of a vector) which stand for the aspects of a word relating to its domain, and those relating to its value.", "startOffset": 19, "endOffset": 524}, {"referenceID": 25, "context": "Although we will represent domain and value as two regions of a vector, there is no reason for these not to be treated as separate vectors at the time of comparison, as done by Turney (2012).", "startOffset": 177, "endOffset": 191}, {"referenceID": 0, "context": "Inspired by the distributional interpretation (Baroni and Zamparelli, 2010; Coecke et al., 2010) of syntactically-paramatrized semantic composition functions from Montogovian semantics (Montague, 1974), we will also assume the function part of our representation to be parametrized principally by syntax and domain rather than value.", "startOffset": 46, "endOffset": 96}, {"referenceID": 4, "context": "Inspired by the distributional interpretation (Baroni and Zamparelli, 2010; Coecke et al., 2010) of syntactically-paramatrized semantic composition functions from Montogovian semantics (Montague, 1974), we will also assume the function part of our representation to be parametrized principally by syntax and domain rather than value.", "startOffset": 46, "endOffset": 96}, {"referenceID": 18, "context": ", 2010) of syntactically-paramatrized semantic composition functions from Montogovian semantics (Montague, 1974), we will also assume the function part of our representation to be parametrized principally by syntax and domain rather than value.", "startOffset": 96, "endOffset": 112}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation.", "startOffset": 102, "endOffset": 123}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation. Models using dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al.", "startOffset": 102, "endOffset": 348}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation. Models using dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al. (2012). We use the class of recursive matrix-vector models as the basis for our investigation; for a detailed introduction see the MV-RNN model described in Socher et al.", "startOffset": 102, "endOffset": 373}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation. Models using dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al. (2012). We use the class of recursive matrix-vector models as the basis for our investigation; for a detailed introduction see the MV-RNN model described in Socher et al. (2012). We begin by describing composition for a general dual-space model, and apply this model to the notion of compositional logic in a tripartite representation discussed earlier.", "startOffset": 102, "endOffset": 544}, {"referenceID": 23, "context": "In the case of Socher et al. (2012) these functions are as follows:", "startOffset": 15, "endOffset": 36}, {"referenceID": 23, "context": "logic experiment in Socher et al. (2012)), the aim of this paper is to place the burden of compositionality on the atomic representations instead.", "startOffset": 20, "endOffset": 41}, {"referenceID": 19, "context": "the *SEM 2012 shared task (Morante and Blanco, 2012).", "startOffset": 26, "endOffset": 52}, {"referenceID": 23, "context": "While the MV-RNN model in Socher et al. (2012) incorporates parse trees to guide the order of its composition steps, it uses a single composition function across all steps.", "startOffset": 26, "endOffset": 47}, {"referenceID": 21, "context": "This issue was recently highlighted by an investigation into recursive autoencoders for sentiment analysis (Scheible and Sch\u00fctze, 2013).", "startOffset": 107, "endOffset": 135}, {"referenceID": 19, "context": "(2012) but also more specific tasks such as the *SEM shared task on negation scope and reversal (Morante and Blanco, 2012).", "startOffset": 96, "endOffset": 122}, {"referenceID": 22, "context": "Possible tasks for this include sentiment analysis and relation extraction tasks such as in Socher et al. (2012) but also more specific tasks such as the *SEM shared task on negation scope and reversal (Morante and Blanco, 2012).", "startOffset": 92, "endOffset": 113}], "year": 2013, "abstractText": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models.", "creator": "TeX"}}}