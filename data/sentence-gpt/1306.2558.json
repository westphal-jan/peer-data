{"id": "1306.2558", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2013", "title": "The Effect of Biased Communications On Both Trusting and Suspicious Voters", "abstract": "In recent studies of political decision-making, apparently anomalous behavior has been observed on the part of voters, in which negative information about a candidate strengthens, rather than weakens, a prior positive opinion about the candidate. This behavior appears to run counter to rational models of decision making, and it is sometimes interpreted as evidence of non-rational \"motivated reasoning\". We consider scenarios in which this effect arises in a model of rational decision making which includes the possibility of deceptive information. In particular, we will consider a model in which there are two classes of voters, which we will call trusting voters and suspicious voters, and two types of information sources, which we will call unbiased sources and biased sources. In our model, new data about a candidate can be efficiently incorporated by a trusting voter, and anomalous updates are impossible; however, anomalous updates can be made by suspicious voters, if the information source mistakenly plans for an audience of trusting voters, and if the partisan goals of the information source are known by the suspicious voter to be \"opposite\" to his own. Our model is based on a formalism introduced by the artificial intelligence community called \"multi-agent influence diagrams\", which generalize Bayesian networks to settings involving multiple agents with distinct goals.\n\n\nIn the model of \"trust\" we take the probability that the candidate and its source are trustworthy. This is done by employing the assumption that the source is trustworthy, but the probability of the candidate's trustworthiness is very high. It is used in this model to assess the probability of the candidate's trustworthiness. In a hypothetical case, our model predicts that the candidate may be trustworthy, but the probability of the candidate being trustworthy is very high. We have an idea of the probability of the candidate being trustworthy. In a hypothetical case, a candidate who is trustworthy, in a situation where the candidate will have more trust than those who are trusted. This model uses the assumptions that were made in this model and their source, and they are usually used in models of trust that are not fully automated. The model relies on models that are often used to predict their expected behavior and are often used to predict their expected behavior. We have an idea of the probability of the candidate being trustworthy, and we have an idea of the probability of the candidate being trustworthy. In a hypothetical case, an agent whose objective is to improve the trustworthiness of the candidate is being trustworthy. We estimate that the expected behavior of the candidate in a given situation can be easily predicted if the candidate is trustworthy. In", "histories": [["v1", "Tue, 11 Jun 2013 15:45:11 GMT  (391kb,D)", "http://arxiv.org/abs/1306.2558v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["william w cohen", "david p redlawsk", "douglas pierce"], "accepted": false, "id": "1306.2558"}, "pdf": {"name": "1306.2558.pdf", "metadata": {"source": "CRF", "title": "The Effect of Biased Communications On Both Trusting and Suspicious Voters", "authors": ["William W. Cohen", "David P. Redlawsk"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Historically, political decision-making has been modeled in a number of ways. Models that propose rational decision making on the part of voters must account for the fact that voters frequently have difficulty in responding to factual surveys on political issues. One resolution to this difficulty is to model candidate evaluation as an online learning process, in which a tally representing candidate affect is incremented in response to external information [13], after which the information itself is discarded. However, in a number of recent studies of political decision-making, apparently anomolous behavior has been observed on the part of voters, in which negative information about a candidate k strengthens (rather than weakens) a prior positive opinion held about k [5, 20] .\nThis behavior appears to run counter to rational models of decision making, and it is sometimes interpreted as evidence of non-rational \u201cmotivated reasoning\u201d [11]. In motivated reasoning models, a voter will (1) evaluate the affect of new informationi.e., its positive or negative emotional charge, then (2) compare this to the affect predicted by current beliefs, and finally (3) react, where congruent information (i.e., information consistent with predicted affect) is processed quickly and easily, and incongruent\n1\nar X\niv :1\n30 6.\n25 58\nv1 [\ncs .A\nI] 1\ninformation is processed by a slower stop-and-think process. Stop-and-think processing may include steps such as counter-arguing, discounting the validity of the information, or bolstering existing affect by recalling previously-assimilated information [14, 20].\nSome evidence for the motivated reasoning hypothesis comes from hman-subject experiments using a dynamic process tracing environment (DPTE), in which data relevant to a mock election is presented as a dynamic stream of possibly relevant news items. In DPTE experiments, detailed hypotheses about political reasoning can be tested, for instance by varying the frequency and amount of incongruent information presented to voters in the mock election. Experimental evidence shows, for instance, that both political sophisticates and novices spend more time processing negative information about a liked candidate, and novices also spend longer processing positive information about a disliked candidate [20]. Most intriguingly, small to moderate amounts of incongruent information\u2014e.g., negative information about a liked candidate\u2014actually reinforce the prior positive view of the candidate [21].\nThis apparently anomalous effect\u2014whereby information has the inverse of the expected impact on a voter\u2014appears to be inconsistent with rational decision-making. In this paper, we show analytically that this \u201canomalous\u201d effect can occur in a model of rational decision making which includes the possibility of deceptive information. The model makes another interesting prediction: it justifies as computationally effective and efficient a heuristic of pretending to believe information from a possibly-deceptive source if that source\u2019s political preferences are the same as the voters.\nIn particular, we will consider a model in which there are two classes of voters, which we will call trusting voters and suspicious voters, and two types of information sources, which we will call unbiased sources and biased sources. Information from an unbiased source is modeled simply as data Dk that probabilistically inform a voter about a candidate k\u2019s positions. Trusting voters are voters that treat information about a candidate as coming from an unbiased source. We show that, in our model, new data about a candidate can be efficiently incorporated by a trusting voter, and anomalous updates (in which \u201cnegative\u201d information increases support) are impossible.\nBiased sources are information sources j who plan their communications with a goal in mind (namely, encouraging trusting voters to vote in a particular way). To do this, j will access some data Ck which is communicated to them only (not to voters directly) and release some possibly-modified version Bk of Ck, concealing the original Ck. Bk is chosen based on the utility to j of the probable effect of Bk on a trusting voter i.\nWe then introduce suspicious voters. Unlike trusting voters, who behave as if communications were from unbiased sources, suspicious voters explicitly model the goal-directed behavior of biased sources.\nThe behavior of rational suspicious voters depends on circumstances\u2014depending on the assumptions made, different effects are possible. If the partisan goals of the biased source j and a suspicious voter i are aligned, then a suspicious voter can safely act as if the information is correct\u2014i.e., perform the same updates as a naive voter. Intuitively, this is because j is choosing information Bk strategically to influence a naive voter to achieve j\u2019s partisan goals, and since i\u2019s goals are the same, it is strategically useful for i to \u201cplay along\u201d with the deception; this intutition can be supported rigorously in our model. If the partisan goals of j are unknown, then a rational suspicious voter i may discount or ignore the information Bk; again, this intuition can be made rigorous, if appropriate assumptions are made. Finally, if the partisan goals of j are known to be \u201copposite\u201d those of i, then a rational suspicious voter may display the \u201canomalous\u201d behavior discussed above: information Bk that would cause decreased support for a suspicious voter will cause increased support for i. Intuitively, this occurs because i recognizes that j may be attempting to decrease support for candidate k, and since i and j have \u201copposite\u201d partisan alignments, it is rational for i to instead increase support.\nIn short, in this model, a negative communication about k can have the effect opposite to one\u2019s initial expectation; however, the apparent paradox is not due to motivated reasoning, but simply to imprecise planning on the part of j. In particular, j\u2019s communication was planned by a biased source with the aim of influencing trusting voters, while in fact, i is a suspicious voter.\nBelow, we will first summarize related work, and then flesh out these ideas more formally. Our model will be based on a formalism introduced by the artificial intelligence community called \u201cmultiagent influence diagrams\u201d, which generalize Bayesian networks to settings involving multiple agents with\ndistinct goals."}, {"heading": "2 Related work", "text": "This work is inspired for recent work on motivated reasoning and hot cognition in political contexts (for recent overviews, see [9, 19]). There is strong experimental evidence that information processing of political information involves emotion, and there recent research has sought to either collect empirical evidence for [20, 5], and and build models that explicate [12], the mechanisms behind this phenomenon.\nThe models of this paper are not intended to dispute role of emotion in political decision making. Indeed, our models reflect situations in which one party deliberately withholds or distorts information to manipulate a second party, and introspection clearly suggests that such situations will typically invoke an emotional response. However, work in social learning (e.g., [22]) and information cascades (e.g., [23]) shows that behaviors (such as \u201cfollowing the herd\u201d) which appear to be driven by non-rational emotions may in fact be strategies that are lead to results that are evolutionarily desirable (if not always \u201crational\u201d from the individual\u2019s perspective.) Hence, the identification of emotional aspects to decision-making does not preclude rational-agent explanations; rather, it raises the question of why these mechanisms exist, what evolutionary pressures might cause them to arise, and whether or not those pressures are still relevant in modern settings. This paper makes a step toward these long-term goals by identifying cases in which behavior explainable by motivated reasoning models is also rational, for instance in the result of Theorem 2.\nThe explanation suggested here for anomalous, motivated-reasoning-like updating is based on a voter recognizing that a source may be biased, and correcting for that bias. While this explanation of anomalous updates is (to our knowledge) novel, it is certainly recognized that trust in the source of information is essential in political persuasion, and that a voter\u2019s social connextions strongly influences political decisionmaking (e.g., [1]). More generally, empirical studies of persuasion substantiate a role of confirmatory bias and prior beliefs [7], and show that in non-political contexts (e.g., in investing), sophisticated consumers of information adjust for credibility, while inexperienced agents under-adjust.\nThe results of this paper are also related to models of media choice\u2014for instance, research in which the implications of a presumed tendency of voters to seek confirmatory news is explored mathematically [4, 2, 8, 25]. Other analyses show why preferences for unbiased news lead to economic incentives to distort the news [3]. This paper does not address these issues, but does contribute by providing a rational-agent model for why such a confirmatory bias exists: in particular, Proposition 3 describes a strategy from using information from biased information sources with similar preferences as a voter. We notice that this strategy is both simple and computationally inexpensive, and might be preferable on these grounds to more complex strategies to \u201cde-noise\u201d biased information from sources with unknown preferences.\nA further connection is to formal work on \u201ctalk games\u201d, such as Crawford and Sobel\u2019s model of strategic communication [6]. In this model, a well-informed but possibly deceptive \u201csender\u201d sends a message to a \u201creceiver\u201d who (like our voter) takes an action that affects both herself and the sender. Variants of this model have explored cases in which information can only be withheld or disclosed, and disclosed information may be verified by the receiver [15]; cases where the receiver uses approximate \u201ccoarse\u201d reasoning [16]; and cases where there is a mixed population of strategic and naive recievers, all of whom obtain information from senders acting strategically [17].\nThe analysis goals in \u201ctalk games\u201d is different from the goals of this paper. whereas we investigate whether specific counter-intuitive observed behavior can arise in a plausible (not necessarily temporally stable) situation, this prior work primarily nalyzes the communication efficiency of a a system in equilibrium, Some of the results obtained for talk games are reminiscent of results shown here: for instance, Crawford and Sobel show that in equilibrium, signaling is more informative when the sender and reciever\u2019s preferences are more similar. However, other results are less intuitive: for instance, in some models there is no deception at equilibrium [17]. We note that while equilibria are convenient to analyze, there is no particular reason to believe that natural political discourse reaches an equilibria.\nGame theory has a long history in analyses of politics; in particular, writing in 1973, Shubik discusses possible applications of game theory to analysis of misinformation [24]. The tools used used in this\npaper arose from more recent work in artificial intelligence [10, 18], specifically analysis of multi-agent problem solving tasks, in which one agent explicitly models the goals and knowledge of another in settings involving probabilistic knowledge. One small contribution of this paper is introduction of a new set of mathematical techniques, which (to our knowledge) have not been previously used for analysis of political decision-making. We note however that while these tools are convenient, they are not absolutely necessary to obtain our results."}, {"heading": "3 Modeling trusting voters", "text": ""}, {"heading": "3.1 A model", "text": "Consider the influence diagram on the left-hand side of Figure 1. In this model i is a voter, and k is a candidate. A voter i has a preference Ti: for example, Ti might be a member of the set\ndom(T ) = {goodLiberal, evilLiberal, goodConservative, evilConservative}\nLikewise Tk is candidate k\u2019s actual position, which is also an element of dom(T ). Sik measures how similar two positions are. Yik is a measure of i\u2019s support for k, which is chosen by voter i to maximize i\u2019s utility. The utility Ui for voter i is a function of how appropriate Yik is given Sik. Finally, Dk is some data about k, generated probabilistically according to the value of Tk. As an example, this might be a statement made by k. The notation used in this diagram (and elsewhere below) is summarized in Table 1.\nThe shapes in the nodes in the diagrams indicate the type of the variable: diamonds for utilities, circles for random variables, and squares for decision variable, which an agent (in this case, voter i) will choose in order to maximize utility. The dotted arrow lines leading into a decision variable indicates information available when the decision is made. The arrows leading into a random variable node indicate \u201cparents\u201d\u2014variables on which the value of the variable is conditioned. This sort of diagram is called an influence diagram, and the general version we will use later (in which multiple agents may exist) is called a MAID (Multi-Agent Influence Diagram) [18, 10].\nMore precisely the model defines a probability distribution generated by this process:\n\u2022 Pick ti \u223c P (Ti), where P (Ti) is a prior on voter preferences. \u2022 Pick tk \u223c P (Tk), where P (Tk) is a prior on candidate positions. \u2022 Pick dk \u223c P (Dk|Tk = tk). Equivalently, we could let dk = fD(tk, D), where fD is a function and D is a random variable chosen independently of all other random variables in the model.\n\u2022 Pick sik \u223c P (Skij |Tk = tk, Tj = tj). Equivalently, we could let sik = fS(ti, tk, S), where fS is a function and S is a random variable, again chosen independently of all other random variables in the model.\n\u2022 Allow voter i to pick yik, based on a user-chosen probability distribution P\u03c4 (Yik|Sik = sik), or equivalently computed using fY (sik, Y ).\n\u2022 Pick utility ui from P (U |yik, sik)\u2014or equivalently, computed as ui = fU (yik, sik, U ). The user can choose any distribution P\u03c4 (Y |S), but we will henceforth assume that she will make the optimal choice\u2014i.e., the probability distribution P\u03c4 will be chosen by i to maximize the expected utility ui, where ui will be picked from P (U |yik, Sik)Pr(Sik)\u2014or equivalently computed as ui = \u2211 s\u2032 P (Sik = s\u2032) \u2217 fU (yik, s\u2032, U ). In this specific case, the conversion is based on the observation that\nPTrV(Yik = y|Sik = s) = P ( y = argmaxy\u2032 \u222b U fu(y \u2032, s, U )d U ) (1)\nand yields the Bayes network on the righthand side of Figure 1. Here Yik is simply a random variable conditioned on Sik, where the form of the dependency depends on Equation 1. Notice that the link from Sik to Yik is deterministic. We call this the trusting voter model, since voter i trusts the validity of the information Dk.\nMAIDs (and their single-agent variants, influence diagram networks) have a number of advantages as a formalism. They provide a compact and natural computational representation for situations which are otherwise complex to describe - in particular, situations in which agents have limited knowledge\nof the game structure, or mutually inconsistent beliefs, but act rationally in accordance with these beliefs. In particular, MAIDs relax the assumption usually made in Bayesian games that players\u2019 beliefs are consistent, and supports an explicit process in which one player can model another player\u2019s strategy. MAIDs also support an expressive structured representation for a player\u2019s beliefs. Together these features make them appropriate for modeling \u201cbounded rationality\u201d situations of the sort we consider here. Further discussion of MAIDs, and their formal relation to other formalisms for games and probability distributiobns, can be found elsewhere [10, 18].\nNext, we will explore some simplifications of Eq. 1) If fu is deterministic, then Eq. 1 simplifies to the following choice of yik given sik:\nyik = fy TrV(sik) = argmaxy\u2032fu(y \u2032, sik)\nIf only a distribution P (Sik = s) is known, then voter i\u2019s optimal strategy is to let yik = argmaxy\u2032 \u2211 s fu(y \u2032, s)P (Sik = s)\nIn any case, however, the model has similar properties: once we assume that i uses an optimal strategy, then the MAID becomes an ordinary Bayes net, defining a joint distribution over the variables Ti, Tk, Sik, and Yik. We will henceforth use P\nTrV to denote probabilities computed in this model, and reserve the non-superscripted P (A|B) and P (A) for conditional (respectively prior) probability distribution that are assumed to be available as background information.\nExample 1 To take a concrete example, igure 2 shows the conditional probability tables for a small example, where candidates and target positions have values like evilLiberal and goodConserv, and the value of a communication Ck is a name of something that a candidates might support (e.g.,motherhood or guns)."}, {"heading": "3.2 Implications of the model", "text": ""}, {"heading": "3.2.1 New information about a candidate is easy to process", "text": "It is obvious how to use the trusting voter model to compute Yik if Tk and Ti are known. However, a more reasonable situation is that Dk and Ti are known to i, but the true position of the candidate can only be inferred, indirectly, from Dk. Fortunately, using standard Bayes network computations, we can also easily compute a distribution over Yik given Dk.\nFirst, note that we can marginalize over Tk and compute P (Dk = d) = \u2211 t\u2032 P (Dk = d|t\u2032)P (Tk = t\u2032)\nand that using Bayes\u2019 rule\nP (Tk = t|Dk = d) = P (Dk = d|Tk = t)P (Tk = t) P (Dk = d) = P (Dk = d|Tk = t) P (Dk = d) P (Tk = t) (2)\nThis gives a simple rule for i to use in choosing her vote Yik given Dk:\nPTrV(Y = y|Ti = ti, Dk = dk) (3) = \u2211 s PTrV(Y = y|Sik = s) \u2211 t P (Sik = s|Ti = ti, Tk = t)P (Tk = t|Dk = dk) (4)\n= PTrV(Y = y|Sik)P (Sik|ti, Tk)P (Tk|dk)\nThe last line uses a simplified notation from the Bayes net community, where sums used to marginalize are omitted, and the event X = x is replaced with x when the variable X is clear from context.\nThis procedure can be extended easily to the case of multiple data items Dk,1, . . . , Dk,m about the candidate, each independently generated from P (Dk|Tk), as shown in Figure 2. It can be shown that\nP (Tk = t|dk,1, . . . , dk,m) = \u220f ` P (dk,`|Tk = t) P (dk,`) P (Tk = t)\nPut another way, we can define P (Tk = t|dk,1, . . . , dk,m) recursively as follows:\nP (Tk = t|dk,1, . . . , dk,m) = P (dk,m|Tk = t) PTrV(dk,m) \u00b7 P (Tk = t|dk,1, . . . , dk,m\u22121)\nHence, voter i can quickly update beliefs about Tk incrementally with each new piece of information dk,` and then (as before) use Equation 3 to update her vote.\nThis incremental update property is worth emphasizing\u2014while it may be complicated (if not computationally complex) to compute PTrV(Y |S), or it may be difficult for a voter to establish her preferences ti, absorbing new information in the trusting voter model is straightforward, and consists of two \u201cnatural\u201d steps: estimating the candidate\u2019s position Tk given the information dk,`; and then updating her support yik for candidate k, given the updated estimate of the candidate\u2019s position."}, {"heading": "3.2.2 Positive information increases support", "text": "In a number of recent studies of political decision-making, negative information about a candidate k has been observed to strengthen (rather than weaken) a voter\u2019s support for k. We will show that under fairly reasonable assumptions this effect can not occur with the trusting voter model. In particular, we will assume that support Yik increases monotonically with the similarity Sik of the candidate\u2019s position Tk and the voter\u2019s preference Ti.\nRecall that in the trusting voter model Yik is a deterministic function of Sik, defined as\nfY TrV(sik) = argmaxy\u2032 \u222b U fu(y \u2032, s, U )d U (5)\nIf fY TrV has the property that \u2200s1 > s2, fY TrV(s1) \u2265 fyTrV(s2) then we will say that i\u2019s support for k increases monotonically with sik. This is one assumption needed for our result.\nWe also need to precisely define \u201cnegative\u201d information. We say that dk is strictly negative about k to voter i if there is some partition of dom(Sik) into triples (a1, b1, \u03b41), . . . ,(am, bm, \u03b4m) so that\n\u2022 For every triple (a`, b`, \u03b4`), a` < b`, \u03b4` > 0, P (Sik = a`|dk) = P (Sik = a`) + \u03b4`, and P (Sik = b`|dk) = P (Sik = b`)\u2212 \u03b4`. In other words, learning Dk = dk shifts some positive probability mass \u03b4` from the larger similarity value b` to the smaller similarity value a`.\n\u2022 For all s \u2208 dom(Sik) that are not in any triple, P (sik = s|dk) = P (sik = s). In other words, the probability mass of values s not in any triples is unchanged.\nTo illustrate this, consider Figure 3, which illustrates a plausible example of strictly negative information. Strictly positive information is defined analogously.\nWe can now state formally the claim that negative information will reduce support. An analogous statement holds for strictly positive information.\nTheorem 1 Let EP [X] denote the expected value of X in probability distribution P . In a trusted voter model PTrV, if voter i\u2019s support for k increases monotonically with sik and dk is strictly negative about k to voter i, then EPTrV [Yik|Dk = dk] < EPTrV [Yik]. Proof. Let S\u2032 = dom(Sik) \u2212 {a1, b1, . . . , am, bm}, where the a`, b`\u2019s are the triples guaranteed by the strictly-negative property of dk.\nEPTrV [Yik|Dk = dk] = \u2211 y y \u00b7 PTrV(Yik = y|Dk = dk)\n= \u2211 s fy TrV(s)PTrV(Sik = s|Dk = dk)\n= \u2211 s\u2032\u2208S\u2032 fy TrV(s)PTrV(Sik = s|dk) + m\u2211 `=1 fy TrV(a`)P TrV(Sik = a`|dk) + m\u2211 `=1 fy TrV(b`)P TrV(Sik = b`|dk)\nLooking at the three terms of the final sum in turn, clearly\u2211 s\u2032\u2208S\u2032 fy TrV(s)PTrV(Sik = s|dk) = \u2211 s\u2032\u2208S\u2032 fy TrV(s)PTrV(Sik = s)\nand the last two terms can be written as\nm\u2211 `=1 fy TrV(a`)P TrV(Sik = a`|dk) + fyTrV(b`)PTrV(Sik = b`|dk)\n= m\u2211 `=1 fy TrV(a`) ( PTrV(Sik = a`) + \u03b4` ) + fy TrV(b`) ( PTrV(Sik = b`)\u2212 \u03b4` ) =\nm\u2211 `=1 fy TrV(a`)P TrV(Sik = a`) + fy TrV(b`)P TrV(Sik = b`) + \u03b4`(fy TrV(a`)\u2212 fyTrV(b`))\n\u2264 m\u2211 `=1 fy TrV(a`)P TrV(Sik = a`) + fy TrV(b`)P TrV(Sik = b`)\nwith the last step holding because (a) \u03b4` > 0 and (b) a` < b`. (Recall that from the monotonicity of fy, if a` < b` then fy TrV(a`) \u2264 fyTrV(b`)). Combining these gives that\nEPTrV [Yik|Dk = dk]\n= \u2211 s\u2032\u2208S\u2032 fy TrV(s)PTrV(Sik = s|dk) + m\u2211 `=1 fy TrV(a`)P TrV(Sik = a`|dk) + m\u2211 `=1 fy TrV(b`)P TrV(Sik = b`|dk)\n\u2264 \u2211 s\u2032\u2208S\u2032 fy TrV(s)PTrV(Sik = s) + m\u2211 `=1 fy TrV(a`)P TrV(Sik = a`) + fy TrV(b`)P TrV(Sik = b`)\n= \u2211\ns\u2208dom(Sik)\nfy TrV(s)PTrV(Sik = s) = EPTrV [Yik]\nThis concludes the proof."}, {"heading": "4 Modeling biased pundits and suspicious voters", "text": ""}, {"heading": "4.1 Biased pundits", "text": "We now consider a new model, as shown in Fig 4. In this model there is a pundit j, who, like a voter, has a target candidate position Tj . Pundit j observes a private datapoint Ck from candidate k\u2014perhaps based on a private communication or research\u2014and then publishes a \u201cbiased version\u201d Bk of Ck. However, Bk is chosen under the assumption that some trusting voter i will react to Bk as if it were Dk in the trusting voter model of Figure 1. Specifically, we assume that Bk will provoke a vote Yik according to the trusting voter model. The utility assigned by j to this outcome is a function of i\u2019s vote Yik, the similarity of Sjk of Tk to j\u2019s target Tj , and a \u201creputational cost\u201d Rjk, which is a function of Ck and Bk. For instance, Rjk might be zero if bk = ck, and otherwise some measure of how embarrassing it might be to j if his deception of replacing ck with bk were discovered.\nMore precisely the model defines a probability distribution generated by this process:\n\u2022 Pick tj \u223c P (Tj), where P (Tj) is a prior on pundit preferences. \u2022 Pick tk \u223c P (Tk), where P (Tk) is a prior on candidate positions. \u2022 Pick ck \u223c P (Dk|Tk = tk), or equivalently, ck = fD(tk, D). (Notice that we assume ck is chosen from\nthe same conditional distribution P (Dk|Tk) used in the trusting voter model to chose Dk\u2014we\u2019re using a different variable here to emphasize the different role it will play.)\n\u2022 Allow pundit j to pick bk, based some user-chosen distribution P\u03c3(Bk|Ck = ck). \u2022 Pick rik \u223c P (Rjk|Bk = bk, Ck = ck), or equivalently, rjk = fR(bk, ck, R). \u2022 Show bk to a trusting voter i, presenting it as a sample from P (Dk|Tk = tk), and allow user i to\npick yi according to the trusting voter model.\n\u2022 Pick ui \u223c P (Uj |Rjk = rjk, Sjk = sjk, Yik = yik), or equivalently, let uj = fU (rjk, sjk, yjk, U ).\nTo distinguish the two utility functions, we will henceforth use fU TrV for the utility function fU (s, y) used in the trusting voter model, and use fU BiP for the utility function fU (r, s, y) defined above. As below, we will assume pundit j will pick bk, based on available estimates of Sij and knowledge of Ck, to maximize the expected utility uj . This can be computed as\nuj = \u2211 r,s,y PTrV(Yik = y|Dk = bk)P (Sjk = s|Dk = ck)P (Rjk = r|bk = ck)fUBiP(r, s, y, U )\nwhere PTrV(Yik|Dk = bk) is estimated using the trusting voter model; P (Sjk|Dk = ck) is computed as in Section 3.2.1, using P (DCk|Tk) and P (Sjk|Tj , Tk); and P (Rjk = r|bk, ck) is computed using the given probability function of reputational cost r, as a function of the unbiased data ck and the biased version bk that is released.\nSince pundit j picks Bk to maximize utility, then as before, we can convert this MAID to a Bayes net. Specifically, Bk depends on the parents Ck and Sjk as follows.\nPBiP(Bk = b|Ck = c, Sjk = s) =\nP ( b = argmaxb\u2032 \u2211 r,y PTrV(Yik = y|Dk = bk)P (Rjk = r|b\u2032, ck) \u222b U fU BiP(r, s, y, U )d U )\nThis can be simplified, if we assume that fU BiP and fR are deterministic:\nPBiP(Bk = b|Ck = c, Sjk = s) = P ( b = argmaxb\u2032 \u2211 y PTrV(Yik = y|Dk = bk)fUBiP(fR(b\u2032, c), s, y) ) (6)"}, {"heading": "4.2 Suspicious voters", "text": "Finally, we introduce a model for a \u201csuspicious voter\u201d. Intuitively, this model is simple. As before, we assume that j chooses bk according to the biased pundit model of Eq. 6\u2014i.e., that j believes i to be a trusting voter. The suspicious voter will then attempt to reason with this correctly, and find the vote Yi maximizing i\u2019s true utility, given then information revealed by j\u2019s choice of bk. The MAID and Bayes net versions of this model are shown in Figure 5 and Figure 6 respectively, and probabilities computed in this model will be writted as P SuV.\nThis suspicious voter model does not have a simple closed-form solution for P SuV(S), as in the trusting voter model. The generative process for the suspicious voter model is identical to the biased pundit model, except that after bk is chosen according to Eq. 6, i will pick a value of Yik from a distribution P SuV(Yik|Bk = bk), which is chosen to maximize the expected value of ui. We will define\nP SuV(Yik = y|Sik = s) = P ( y = argmaxy\u2032 \u222b U fu(y \u2032, s, U )d U ) or, assuming determinacy,\nyik = fy SuV(sik) = argmaxy\u2032fu(y \u2032, sik)\nThis leads to a more complex inference problem for voters. Although the process of computing P (Sik|Ti, Tk) is unchanged, relative to the trusting voter model, a suspicious voter cannot estimate a distribution over Tk using dk, as in Eq. 2, because dk is not known. Instead i only has indirect evidence about dk in the form of bk.\nHowever, voter i can use this indirect evidence to compute P SuV(Tk|Bk = bk) = \u2211 c PTrV(Tk|Dk = c) \u00b7 PBiP(Bk = b|Ck = c) \u00b7 PTrV(Dk = c) (7)\nwhere PBiP(b|c) = \u2211 tj PBiP(b|c, tj)P (tj) is the probability, in the biased pundit model, of pundit j publishing Bk = b when Ck = c. We can now break the summation over c into two cases:\nP SuV(Tk|Bk = bk) = PTrV(Tk|Dk = b) \u00b7 PTrV(Dk = b) \u00b7 PBiP(Bk = b|Ck = b) (8)\n+ \u2211 c6=b PTrV(Tk|Dk = c) \u00b7 PTrV(Dk = c) \u00b7 PBiP(Bk = b|Ck = c) (9)\nIn the term in line 8, j is not altering the original input ck, so we say he is being accurate. In line 9, so we say that j is being deceptive. In order to exploit the information in bk using Eq 8-9 to optimize her utility, the suspicious voter must assess the probability of deception, and adjust inferences about candidate k accordingly\u2014a potentially difficult inference problem."}, {"heading": "4.3 Implications of the suspicious voter model", "text": "To simplify further analysis, we will make some additional assumptions.\n\u2022 We assume a deterministic reputational cost function fR in the biased pundit model. \u2022 We assume the reputational cost of being accurate is zero, i.e., that \u2200c, fR(c, c) = 0, and that the\nreputational cost of being deceptive is greater than zero, i.e., \u2200b 6= c, fR(b, c) > 0. \u2022 We assume deterministic utility functions fUTrV and fUBiP. \u2022 We assume the utility for pundits is the same as the utility for voters, minus the reputational cost\nof altering c to b\u2014i.e., that fU BiP(r, s, y) \u2261 fUTrV(s, y)\u2212 r\nGiven these assumptions, some observations can now be made.\nProposition 1 If the prior P (Tj) such that P BiP(b|c) = PBiP(b\u2032|c\u2032) for all communications b, b\u2032, c and c\u2032, then for all bk, P SuV(Tk|bk) = P SuV(Tk) = PTrV(Tk).\nIn other words, if PBiP(b|c) is constant, then a biased pundit\u2019s publications bk convey no information to i. This can be seen immediately by inspection of Eq. 7. Notice that requiring that PBiP(b|c) does not imply that any individual pundits simply publish information b uniformly at random, without regard to c\u2014instead, it says that if one averages over all pundits and considers \u2211 tj PBiP(b|c, tj)P (tj), then the cumulative probability of seeing any particular b is constant, and independent of c. More generally, one can make this observation.\nProposition 2 If the prior P (Tj) such that (1) P BiP(c|c) = \u03b1 for all c, and (2) PBiP(b|c) = PBiP(b\u2032|c) for all communications b 6= c and b\u2032 6= c\u2032, then for all bk,\nP SuV(Tk|bk) = \u03b1PTrV(Tk|bk) + (1\u2212 \u03b1)PTrV(Tk)\nIn other words, if all deceptions are equally likely, but publications are accurate with fixed probability \u03b1, then a suspicious voter\u2019s update to Tk is simply a mixture of her prior belief P SuV(Tk) = P TrV(Tk) and the belief a trusting voter would have, PTrV(Tk|bk), with mixing coefficient \u03b1. Again, this proposition can be be verified immediately by inspection of lines 8-9.\nA final observation is that when a biased pundit\u2019s preference tj is the same as a voter\u2019s preference ti, and this is known to both i and j, then even a suspicious voter will obtain high utility by simply believing j\u2019s publication bk. In particular i\u2019s utility from adopting the belief that D = bk is just as high as if i had observed ck itself.\nProposition 3 If tj = ti, and bk is a publication from j under the biased pundit model, then the expected utility to i of voting according to PTrV(Tk|Dk = bk) is at least as large as the expected utility to i of voting according to PTrV(Tk|Dk = ck).\nThis proposition seems plausible if we recall that bk was chosen to maximize the utility to j of i\u2019s belief in bk in the trusting voter model\u2014thus, since the utility to i is the same as the utility to j, it seems reasonable that adopting this belief is also useful to i. To establish it more formally, let us define EU`(b|c, ti) to be the expected utility to agent ` (either i or j), absent reputational costs, of having i adopt the belief in the trusting-voter model that Dk = b when in fact Dk = c, if Ti = ti. In other words, we define\nEU`(b|c, ti) \u2261 \u2211\ns`k,tk,yik\nPTrV(Tk = tk|Dk = c)P (S`k = s`k|t`, tk)PTrV(Yik = yik|Ti = ti, Dk = b)fTrVu(s`k, y)\nNote that the weighting in the factors PTrV(Tk = tk|Dk = c)PTrV(S`k = s`k|t`, tk) holds for both i and j, because here we care about the true distribution over Tk, as deduced from c. The weighting in the factor PTrV(Yik = y|Ti = ti, Dk = b) arises because for both i and j, utility is based on i\u2019s estimated support yik for k given i\u2019s known preference ti.\nIf we assume that ti = tj = t, then this simplifies to EUi(b|c, ti) = EUj(b|c, ti) = \u2211\ns,tk,yik\nPTrV(Tk = tk|Dk = c)PTrV(S = s|t, tk)PTrV(Yik = yik|Ti = t,Dk = b)fu(s, y)\nand hence we see that in this case, the functions for i and j are indeed the same. Since j has chosen b to maximize EU`(b|c) \u2212 fR(b, c), and fR is never negative, clearly EUj(b|c, ti) \u2265 EUj(b|c, ti), and so EUi(b|c, ti) \u2265 EUi(b|c, ti) as well.\nAs noted in Section 2, there are a number of papers analyzing media bias in which voters are assumed prefer \u201cgood news\u201d (i.e., new biased towards their favored candidates) leading to fragmentation and specialization as media companies differentiate by providing news biased for their readers. The results above suggest a rational reason for picking a news source j with the same partisan preferences as one\u2019s self: in particular, this sort of news is computionally easier to process. Similarly, biased sources with unknown preferences are \u201cless informative\u201d, in the sense that new information leads to changes in support (relative to unbiased sources, or well-aligned partisan sources)."}, {"heading": "4.4 Suspicious voters and \u201cirrationality\u201d", "text": "Finally, we address the question of whether suspicous voters can behave in the counter-intuitive manner discussed in the introduction\u2014whether information about the candidate k that is negative (as interpreted by a trusting voter) can increase support for a suspicous voter. We will show that this is possible.\nTheorem 2 It can be the case that information b will decrease i\u2019s support for k in the trusted voter model, and increase i\u2019s support for k in the suspicous voter model: i.e., it may be that EPTrV [Yik|Dk = b] < EPTrV [Yik] but EPSuV [Yik|Dk = b] > EPSuV [Yik].\nIntuitively, this happens when i believes strongly that j is being deceptive, and i has different candidate preferences from j. The theorem asserts the existence of such behavior, so we are at liberty to make additional assumptions in the proof (preferably, ones that could be imagined to hold in reality).\nIn the proof, we assume that j\u2019s preferences Tj are known to i. This is plausible since context may indicate, for instance, that j is a strong conservative. This does not affect the basic model, since we allow the case of an arbitrary prior on Tj .\nWe also assume that all information about candidates is either strictly positive for i and strictly negative for j, or else strictly negative for i and strictly positive for j. To see how this is possible, first imagine a hypercube-like space of candidate positions, as in Figure 3, and assume that Ti and Tj are on opposite corners of the cube. The cube might indicate, for example, positions on the environment, abortion, and increased military spending, with i preferring the liberal position on all three and j preferring the conservative positions. Then assume that all information indicates the probability of the candidate\u2019s position along these three axis; in this case the assumption is satisfied.\nGiven these two assumptions, the statement of the theorem holds. First, we need a slightly stronger version of Theorem 1. Informally, this states that if i has partial knowledge of b, but does know that b is strictly negative for i, then i\u2019s support for k will be weakened.\nCorollary 1 Suppose H is a probability distribution over items of information b, and also b is strictly negative for i for every b with non-zero probability in H. Let PTrV(Yik|H) denote\u2211\nb\nPTrV(Yik|Dk = b)PH(b).\nThen EPTrV [Yik|H] < EPTrV [Yik].\nProof. For any item s \u2208 dom(Sik), it is clear that\nP (Y |E) = \u2211 b P (Y |b)PH(b)\nand also, by marginalization over the (unrelated) variable H, we see that P (Y ) = \u2211 b P (Y )PH(b). Since for all b with non-zero probability under H we have that E[Y |b] < E[Y ], the result holds. This concludes the proof.\nWe can now prove Theorem 2. Proof. Suppose information b that is strictly negative for i is observed by i, and consider again the formula for P SuV(Tk|Bk = bk) given on lines 8 and 9. This shows that the suspicous voter will reason by cases. In one case, j is being accurate, and the change in probability for Tk is in the same direction as in the trusted voter model, and as noted above, this will lead to a belief update that decreases support for k. However, this change is down-weighted by the factor PBiP(Bk = b|Ck = b)PTrV(Dk = b), which can be interpreted as the probability that b was really observed times the probability that j chooses to report accurately. We will assume that PTrV(Dk = b) is very small, so that\nP SuV(Tk|Bk = bk) \u2248 \u2211 c6=b PTrV(Tk|Dk = c) \u00b7 PTrV(Dk = c) \u00b7 PBiP(Bk = b|Ck = c)\nIn this case, j is being deceptive. Reasoning is this case is complicated by the fact that i must consider all inputs c that could have observed, and compute the product of PTrV(Dk = c), the prior probability of c, and also P BiP(Bk = b|Ck = c), the probability of b being reported in place of c by j. However, since the preferences of i and j are opposite, the latter is quite informative: in particular, since b was deceptively chosen by j to be negative for i, then b must prefer that i give weaker support for k, implying that c is actually negative for j, and hence positive for i. This holds for every c that could have let to the deceptive report b. By Corollary 1, the net change in support for i in the suspicous voter model positive. This concludes the proof."}, {"heading": "5 Concluding Remarks", "text": "To summarize, we propose a model in which there are two classes of voters, trusting voters and suspicious voters, and two types of information sources, unbiased sources and biased sources. Information from an unbiased source is modeled simply as observations Dk that probabilistically inform a voter about a candidate k\u2019s positions, and trusting voters are voters that treat information about a candidate as coming from an unbiased source. We show that reasoning about new information is computationally easy for trusting voters, and that trusting voters behave intuitively: in particular, negative information about candidate k (according to a particular definition) will decrease support for k, and positive information will increase support.\nBiased sources are information sources j who plan their communications in order to encourage trusting voters to vote in a particular way). To do this, they report some possibly-modified version Bk of a private observation Ck, concealing the original Ck. In the model, Bk is chosen based on the utility to j of the probable effect of Bj on a trusting voter i.\nFinally, suspicious voters model the behavior of biased sources. In general this is complicated to do, however, some special cases lead to simple inference algorithms. For instance, under one set of\nassumptions, all information from biased sources can be ignored. In another set of assumptions, a suspicious voter will make the same sort of updates to her beliefs as a trusting voter, but simply make them less aggressively, discounting the information by a factor related to the probability of deception.\nAnother interesting tractible reasoning case for suspicious voters is when the biased information source j has the same latent candidate preferences as the voter i. In this case, suspicious voters can act the same way a trusting voter would\u2014even if the information acted on is false, it is intended to achieve a result that is desirable to i (as well as j).\nThese results are of some interest in light of the frequently-observed preference for partisan voters to collect information from similarly partisan information channels. The results suggest a possibly explanation for this, in terms of information content. The optimal way to process information from a possibly-deceptive unknown source, or a source with a known-to-be-different partisan alignment, is to either discount it, ignore it, or else employ complicated (and likely computationally complex) reasoning schemes. However, reports from partisan source with the same preferences as a voter can be acted on as if they were trusted\u2014even if the reports are actually deceptive.\nFinally, we show rigorously that a suspicious voter can, in some circumstances, increase support for a candidate k after receiving negative information about k. Specifically, information that would decrease support for a trusting voter might increase support for a suspicious voter\u2014if she believes the source has different candidate preferences, and if she believes the source is being deceptive. This behavior mimics behavior attributed elsewhere to \u201cmotivated reasoning\u201d, but does not arise from \u201cirrationality\u201d\u2014instead it is a result of the voters correct identification of, and compensation for, an ineffective attempt at manipulation on the half the information source.\nWe should note that this hypothesis does not suggest that emotion is not present in such situations\u2014 in fact, it seems likely that reports viewed as deceptive would indeed provoke strong emotional responses. It does suggest that some of the emotion associated with these counterintuitive updates may be associated with mechanisms that have an evolutionary social purpose, rather than being a result of some imperfect adaption of humans to modern life.\nIt seems plausible that additional effects can be predicted from the suspicious-voter model. For instance, although we have not made this conjecture rigorous, if a biased source j does not know i\u2019s political preferences, then deceptive messages b will likely tend to be messages that would be interpreted as negative by most voters. For instance, j might assert that the candidate violates some cultural norm, or holds an extremely unpopular political views. (Or , on the other hand, assert the candidate has a property that almost all voters agree is \u201cgood\u201d. Arguably, most information from deceptive partisan sources would be of this sort, rather than discussion of stands on widely-disagreed-on issues (like gay marriage or abortion in the US).\nAs they stand, however, the results do suggest a number of specific predictions about how information might be processed in a social setting. First, information provided by persons believed to have political alignments similar to voter i will be more easily assimilated, and have more effect on the view of i, than information provided by persons believed to have different political alignments. Second, information provided by persons with political alignments similar to voter i will be more assimilated in roughly the same speed (and with the same impact) as information from a believed-to-be-neutral source. Third, information provided by persons with political alignments different from voter i may lead to counterintuitive updates, while information from similarly-aligned sources or neutral sources will not. An important topic for future work would be testing these predictions, for instance using the DPTE methodology."}], "references": [{"title": "The social calculus of voting: Interpersonal, media, and organizational influences on presidential choices", "author": ["P.A. Beck", "R.J. Dalton", "S. Greene", "R. Huckfeldt"], "venue": "American Political Science Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Political polarization and the electoral effects of media bias", "author": ["D. Bernhardt", "S. Krasa", "M. Polborn"], "venue": "Journal of Public Economics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Unfairly balanced: Unbiased news coverage and information loss", "author": ["Jeremy Burke"], "venue": "In Annual Meeting of the American Political Science Association,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Introductions and requests: Rhetorical strategies that elicit online community response", "author": ["M. Burke", "E. Joyce", "T. Kim", "V. Anand", "R. Kraut"], "venue": "In Proceedings of the The third communities and technologies conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Voters, emotions, and memory", "author": ["Andrew J.W. Civettini", "David P. Redlawsk"], "venue": "Political Psychology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Strategic information transmission", "author": ["V. P Crawford", "J. Sobel"], "venue": "Econometrica: Journal of the Econometric Society, page 14311451,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Gentzkow. Persuasion: Empirical evidence", "author": ["M.S. DellaVigna"], "venue": "Annual Review of Economics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A spatial theory of media slant and voter choice", "author": ["J. Duggan", "C. Martinelli"], "venue": "The Review of Economic Studies,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Political communication faces the 21st century", "author": ["D.A. Graber", "J.M. Smith"], "venue": "Journal of Communication,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Multi-agent influence diagrams for representing and solving games", "author": ["D. Koller", "B. Milch"], "venue": "Games and Economic Behavior,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "The case for motivated reasoning", "author": ["Z Kunda"], "venue": "Psychological Bulletin,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "First steps toward a Dual-Process accessibility model of political beliefs, attitudes, and behavior. Feeling politics: Emotion in political information processing, page", "author": ["M. Lodge", "C. Taber", "C. Weber"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "An impression-driven model of candidate evaluation", "author": ["Milton Lodge", "Kathleen M. McGraw", "Patrick Stroh"], "venue": "The American Political Science Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Three Steps toward a Theory of Motivated Political Reasoning, pages 183\u2013213", "author": ["Milton Lodge", "Charles Taber"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Relying on the information of interested parties", "author": ["P. Milgrom", "J. Roberts"], "venue": "The RAND Journal of Economics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1986}, {"title": "Coarse thinking and persuasion", "author": ["S. Mullainathan", "J. Schwartzstein", "A. Shleifer"], "venue": "The Quarterly Journal of Economics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Naive audience and communication bias", "author": ["M. Ottaviani", "F. Squintani"], "venue": "International Journal of Game Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Networks of influence diagrams: A formalism for representing agents beliefs and decisionmaking processes", "author": ["A. Pfeffer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Feeling politics: Emotion in political information processing", "author": ["D. P Redlawsk"], "venue": "Palgrave Macmillan,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Hot cognition or cool consideration? testing the effects of motivated reasoning on political decision making", "author": ["David P. Redlawsk"], "venue": "The Journal of Politics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "The affective tipping point: Do motivated reasoners ever get it", "author": ["David P. Redlawsk", "Andrew J.W. Civettini", "Karen M. Emmerson"], "venue": "Political Psychology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Why copy others? insights from the social learning strategies tournament", "author": ["L. Rendell", "R. Boyd", "D. Cownden", "M. Enquist", "K. Eriksson", "M.W. Feldman", "L. Fogarty", "S. Ghirlanda", "T. Lillicrap", "K.N. Laland"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Experimental study of inequality and unpredictability in an artificial cultural", "author": ["M. J Salganik", "P. S Dodds", "D. J Watts"], "venue": "market. Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Game theory and political science", "author": ["Martin Shubik"], "venue": "Paper No. 351,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1973}, {"title": "Ideological media bias", "author": ["D.F. Stone"], "venue": "Journal of Economic Behavior & Organization,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "One resolution to this difficulty is to model candidate evaluation as an online learning process, in which a tally representing candidate affect is incremented in response to external information [13], after which the information itself is discarded.", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "However, in a number of recent studies of political decision-making, apparently anomolous behavior has been observed on the part of voters, in which negative information about a candidate k strengthens (rather than weakens) a prior positive opinion held about k [5, 20] .", "startOffset": 262, "endOffset": 269}, {"referenceID": 19, "context": "However, in a number of recent studies of political decision-making, apparently anomolous behavior has been observed on the part of voters, in which negative information about a candidate k strengthens (rather than weakens) a prior positive opinion held about k [5, 20] .", "startOffset": 262, "endOffset": 269}, {"referenceID": 10, "context": "This behavior appears to run counter to rational models of decision making, and it is sometimes interpreted as evidence of non-rational \u201cmotivated reasoning\u201d [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "Stop-and-think processing may include steps such as counter-arguing, discounting the validity of the information, or bolstering existing affect by recalling previously-assimilated information [14, 20].", "startOffset": 192, "endOffset": 200}, {"referenceID": 19, "context": "Stop-and-think processing may include steps such as counter-arguing, discounting the validity of the information, or bolstering existing affect by recalling previously-assimilated information [14, 20].", "startOffset": 192, "endOffset": 200}, {"referenceID": 19, "context": "Experimental evidence shows, for instance, that both political sophisticates and novices spend more time processing negative information about a liked candidate, and novices also spend longer processing positive information about a disliked candidate [20].", "startOffset": 251, "endOffset": 255}, {"referenceID": 20, "context": ", negative information about a liked candidate\u2014actually reinforce the prior positive view of the candidate [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "This work is inspired for recent work on motivated reasoning and hot cognition in political contexts (for recent overviews, see [9, 19]).", "startOffset": 128, "endOffset": 135}, {"referenceID": 18, "context": "This work is inspired for recent work on motivated reasoning and hot cognition in political contexts (for recent overviews, see [9, 19]).", "startOffset": 128, "endOffset": 135}, {"referenceID": 19, "context": "There is strong experimental evidence that information processing of political information involves emotion, and there recent research has sought to either collect empirical evidence for [20, 5], and and build models that explicate [12], the mechanisms behind this phenomenon.", "startOffset": 187, "endOffset": 194}, {"referenceID": 4, "context": "There is strong experimental evidence that information processing of political information involves emotion, and there recent research has sought to either collect empirical evidence for [20, 5], and and build models that explicate [12], the mechanisms behind this phenomenon.", "startOffset": 187, "endOffset": 194}, {"referenceID": 11, "context": "There is strong experimental evidence that information processing of political information involves emotion, and there recent research has sought to either collect empirical evidence for [20, 5], and and build models that explicate [12], the mechanisms behind this phenomenon.", "startOffset": 232, "endOffset": 236}, {"referenceID": 21, "context": ", [22]) and information cascades (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": ", [23]) shows that behaviors (such as \u201cfollowing the herd\u201d) which appear to be driven by non-rational emotions may in fact be strategies that are lead to results that are evolutionarily desirable (if not always \u201crational\u201d from the individual\u2019s perspective.", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": ", [1]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "More generally, empirical studies of persuasion substantiate a role of confirmatory bias and prior beliefs [7], and show that in non-political contexts (e.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "The results of this paper are also related to models of media choice\u2014for instance, research in which the implications of a presumed tendency of voters to seek confirmatory news is explored mathematically [4, 2, 8, 25].", "startOffset": 204, "endOffset": 217}, {"referenceID": 1, "context": "The results of this paper are also related to models of media choice\u2014for instance, research in which the implications of a presumed tendency of voters to seek confirmatory news is explored mathematically [4, 2, 8, 25].", "startOffset": 204, "endOffset": 217}, {"referenceID": 7, "context": "The results of this paper are also related to models of media choice\u2014for instance, research in which the implications of a presumed tendency of voters to seek confirmatory news is explored mathematically [4, 2, 8, 25].", "startOffset": 204, "endOffset": 217}, {"referenceID": 24, "context": "The results of this paper are also related to models of media choice\u2014for instance, research in which the implications of a presumed tendency of voters to seek confirmatory news is explored mathematically [4, 2, 8, 25].", "startOffset": 204, "endOffset": 217}, {"referenceID": 2, "context": "Other analyses show why preferences for unbiased news lead to economic incentives to distort the news [3].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "A further connection is to formal work on \u201ctalk games\u201d, such as Crawford and Sobel\u2019s model of strategic communication [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 14, "context": "Variants of this model have explored cases in which information can only be withheld or disclosed, and disclosed information may be verified by the receiver [15]; cases where the receiver uses approximate \u201ccoarse\u201d reasoning [16]; and cases where there is a mixed population of strategic and naive recievers, all of whom obtain information from senders acting strategically [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "Variants of this model have explored cases in which information can only be withheld or disclosed, and disclosed information may be verified by the receiver [15]; cases where the receiver uses approximate \u201ccoarse\u201d reasoning [16]; and cases where there is a mixed population of strategic and naive recievers, all of whom obtain information from senders acting strategically [17].", "startOffset": 224, "endOffset": 228}, {"referenceID": 16, "context": "Variants of this model have explored cases in which information can only be withheld or disclosed, and disclosed information may be verified by the receiver [15]; cases where the receiver uses approximate \u201ccoarse\u201d reasoning [16]; and cases where there is a mixed population of strategic and naive recievers, all of whom obtain information from senders acting strategically [17].", "startOffset": 373, "endOffset": 377}, {"referenceID": 16, "context": "However, other results are less intuitive: for instance, in some models there is no deception at equilibrium [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "Game theory has a long history in analyses of politics; in particular, writing in 1973, Shubik discusses possible applications of game theory to analysis of misinformation [24].", "startOffset": 172, "endOffset": 176}, {"referenceID": 9, "context": "paper arose from more recent work in artificial intelligence [10, 18], specifically analysis of multi-agent problem solving tasks, in which one agent explicitly models the goals and knowledge of another in settings involving probabilistic knowledge.", "startOffset": 61, "endOffset": 69}, {"referenceID": 17, "context": "paper arose from more recent work in artificial intelligence [10, 18], specifically analysis of multi-agent problem solving tasks, in which one agent explicitly models the goals and knowledge of another in settings involving probabilistic knowledge.", "startOffset": 61, "endOffset": 69}, {"referenceID": 17, "context": "This sort of diagram is called an influence diagram, and the general version we will use later (in which multiple agents may exist) is called a MAID (Multi-Agent Influence Diagram) [18, 10].", "startOffset": 181, "endOffset": 189}, {"referenceID": 9, "context": "This sort of diagram is called an influence diagram, and the general version we will use later (in which multiple agents may exist) is called a MAID (Multi-Agent Influence Diagram) [18, 10].", "startOffset": 181, "endOffset": 189}, {"referenceID": 9, "context": "Further discussion of MAIDs, and their formal relation to other formalisms for games and probability distributiobns, can be found elsewhere [10, 18].", "startOffset": 140, "endOffset": 148}, {"referenceID": 17, "context": "Further discussion of MAIDs, and their formal relation to other formalisms for games and probability distributiobns, can be found elsewhere [10, 18].", "startOffset": 140, "endOffset": 148}], "year": 2013, "abstractText": "In recent studies of political decision-making, apparently anomalous behavior has been observed on the part of voters, in which negative information about a candidate strengthens, rather than weakens, a prior positive opinion about the candidate. This behavior appears to run counter to rational models of decision making, and it is sometimes interpreted as evidence of non-rational \u201cmotivated reasoning\u201d. We consider scenarios in which this effect arises in a model of rational decision making which includes the possibility of deceptive information. In particular, we will consider a model in which there are two classes of voters, which we will call trusting voters and suspicious voters, and two types of information sources, which we will call unbiased sources and biased sources. In our model, new data about a candidate can be efficiently incorporated by a trusting voter, and anomalous updates are impossible; however, anomalous updates can be made by suspicious voters, if the information source mistakenly plans for an audience of trusting voters, and if the partisan goals of the information source are known by the suspicious voter to be \u201copposite\u201d to his own. Our model is based on a formalism introduced by the artificial intelligence community called \u201cmulti-agent influence diagrams\u201d, which generalize Bayesian networks to settings involving multiple agents with distinct goals.", "creator": "LaTeX with hyperref package"}}}