{"id": "1306.3525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2013", "title": "Approximation Algorithms for Bayesian Multi-Armed Bandit Problems", "abstract": "In this paper, we consider several finite-horizon Bayesian multi-armed bandit problems with side constraints which are computationally intractable (NP-Hard) and for which no optimal (or near optimal) algorithms are known to exist with sub-exponential running time. All of these problems violate the \"idling bandit property\", which assumes that the reward from the play of an arm is not contingent upon when the arm is played. Not only are index policies suboptimal in these contexts, there has been little analysis of such policies in these problem settings. We show that if we consider near-optimal policies, in the sense of approximation algorithms, then there exists (near) index policies. Conceptually, if we can find policies that satisfy an approximate version of the idling bandit property, namely, that the reward from the play of an arm depends on when the arm is played to within a constant factor, then we have an avenue towards solving these problems. However such an approximate version of the idling bandit property does not hold on a per-play basis and are shown to hold in a global sense. Clearly, such a property is not necessarily true o arbitrary single arm policies and finding such single arm policies is nontrivial. We show that by restricting the state spaces of arms we can find single arm policies and that these single arm policies can be combined into global (near) index policies where the approximate version of the idling bandit property is true in expectation. The number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability. One cannot look at the state spaces of arms in an exact sense of the magnitude of the desired state space. The problem solved by limiting the state spaces of arms is discussed in detail in this paper. Although the value of an approximate version of the idling bandit property in an approximate version of the idling bandit property does not hold on a per-play basis and are shown to hold in a global sense, these single arm policies have no effect on the overall state space. Further, we show that when the state spaces of arms are not met by the sum of the total number of different arm policies and the total number of distinct arm policies, we can find single arm policies in the global sense. Furthermore, our model shows that when a state space with a per-play basis, in which case a per-play state is met by the sum of the total number of different arm policies and the total number of distinct arm policies", "histories": [["v1", "Fri, 14 Jun 2013 22:24:29 GMT  (48kb)", "https://arxiv.org/abs/1306.3525v1", "arXiv admin note: text overlap witharXiv:1011.1161"], ["v2", "Wed, 17 Jul 2013 19:16:47 GMT  (83kb,D)", "http://arxiv.org/abs/1306.3525v2", "arXiv admin note: text overlap witharXiv:1011.1161"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1011.1161", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["sudipto guha", "kamesh munagala"], "accepted": false, "id": "1306.3525"}, "pdf": {"name": "1306.3525.pdf", "metadata": {"source": "CRF", "title": "Approximation Algorithms for Bayesian Multi-Armed Bandit Problems\u2217", "authors": ["Sudipto Guha", "Kamesh Munagala"], "emails": ["sudipto@cis.upenn.edu.", "kamesh@cs.duke.edu."], "sections": [{"heading": null, "text": "We present a general solution framework that yields constant factor approximation algorithms for all the above variants. Our framework proceeds by formulating a weakly coupled linear programming relaxation, whose solution yields a collection of compact policies whose execution is restricted to a single arm. These single-arm policies are made more structured to ensure polynomial time computability of the relaxation, and their execution is then carefully sequenced so that the resulting global policy is not only feasible, but also yields a constant approximation. We show that the relaxation can be solved using the same techniques as for computing index policies; in fact, the final policies we design are very close to being index policies themselves.\nConceptually, we find policies that satisfy an approximate version of the exchange property, namely, that the reward from a play does not depend on time of play to within a constant factor. However such a property does not hold on a per-play basis and only holds in a global sense: We show that by restricting the state spaces of the arms, we can find single arm policies that can be combined into global (near) index policies that satisfy the approximate version of the exchange property analysis in expectation. The number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability.\n\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42]. \u2020Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA 19104. sudipto@cis.upenn.edu. Research supported by NSF awards CCF-0644119, CCF-1117216. Part of this research was performed when the author was a visitor at Google. \u2021Department of Computer Science, Duke University, Durham, NC 27708-0129., kamesh@cs.duke.edu. Supported by an Alfred P. Sloan Research Fellowship, an award from Cisco, and by NSF grants CCF-0745761, CCF-1008065, and IIS-0964560.\nar X\niv :1\n30 6.\n35 25\nv2 [\ncs .D\nS] 1\n7 Ju\nl 2 01"}, {"heading": "1 Introduction.", "text": "In this paper, we consider the problem of iterated allocation of resources, when the effectiveness of a resource is uncertain a priori, and we have to make a series of allocation decisions based on past outcomes. Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].\nOf particular interest is the celebrated Multi-Armed Bandit (MAB) problem, where an agent decides on allocating resources between n competing actions (arms) with uncertain rewards and can only take one action at a time (play the arm). The play of the arm provides the agent both with some reward as well as some further information about the arm which causes the state of the arm to be updated. The objective of the agent is to play the arms (according to some specified constraints) for a time horizon T which maximizes sum of the expected rewards obtained in the T steps. The goal of the algorithm designer (and this paper) in this case is to provide the agent with a a decision policy. Formally, each arm i is equipped with a state space Si and a decision policy is a mapping from the current states of all the arms to an action, which corresponds to playing a subset of the arms. The goal is to use the description of the state spaces {Si} and the constraints as input, and output a decision policy which maximizes the objective of the agent. The running time of the algorithm that outputs such a decision policy, and the complexity of specifying the policy itself, should ideally be polynomial in \u2211 i |Si|, which is the complexity of specifying the input. In this regard, of particular interest are Index Policies where each arm is reduced to a single priority value and the priority values of the arms (indices) are combined using some scheduling algorithm. Index policies are significantly easier to implement and conceptually reduce the task of designing a decision policy to designing single arm policies. However optimum index policies exist only in limited settings, see [14, 32] for further discussion.\nIn recent years MAB problems are increasingly being used in situations where the possible alternative actions (arms) are machine generated and therefore the parameter n is significantly large in comparison to the optimization horizon T . This is in contrast to the historical development of the MAB literature which mostly considered few alternatives, motivated by applications such as medical treatments or hypothesis testing. While several of those applications considered large |Si|, many of those results relied on concentration of measure properties derived from the fact that n/T could be assumed to be vanishingly small in those applications.\nThe recent applications of MAB problems arise in advertising, content delivery, and route selection, where the arms correspond to advertisers, (possibly machine generated) webpages, and (machine generated) routes respectively, and the parameter n/T does not necessarily vanish. As a result, the computational complexity of optimization becomes an issue with large n. Moreover, even simple constraints such as budgets on rewards, concavity of rewards render the computation intractable (NP Hard) \u2014 and these recent applications are rife with such constraints. This fact forces us to rethink bandit formulations in settings which were assumed to be well understood in the absence of computational complexity considerations. One such general setting is the Bayesian Stochastic Multi-Armed Bandit formulation, which dates back to the results of [7, 18].\nFinite Horizon Bayesian Multi-armed Bandit Problem. We revisit the classical finitehorizon1 multi-armed bandit problem in the Bayesian setting. This problem forms the basic scaffolding for all the variants we consider in this paper, and is described in detail in Section 2. There is\n1We study the finite-horizon version instead of the more \u201cstandard\u201d discounted reward version for two reasons: It makes the presentation simpler and easier to follow, and furthermore, in many applications the discounted reward variant is used mainly as an approximation (in the limit) to the finite horizon variant. We note that taking the limit can create surprises with regard to polynomial time tractability.\na set of n independent arms. Arm i provides rewards in an i.i.d. fashion based on an parametrized distribution Di(\u03b8i), where the parameter \u03b8i is unknown. A prior distribution Di is specified over possible \u03b8i; the priors of different arms are independent. At each step, the decision policy can play a set of arms (as allowed by the constraints of the specific problem) and observe the outcome of only the set of arms that have been played. Based on the outcomes, the priors of the arms that were played in the previous step are updated using Bayes\u2019 rule to the corresponding posterior distribution. The objective (most often) is to maximize the expected sum of the rewards over the T time steps, where the expectation is taken over the priors as well as the outcome of each play.2\nThe process of updating the prior information for an arm can be succinctly encoded in a natural state space Si for arm i, where each state encodes the posterior distribution conditioned on a sequence of observations from plays made to that arm. At each state, the available actions (such as plays) update the posterior distribution by Bayes\u2019 rule, causing probabilistic transitions to other states, and yielding a reward from the observation of the play. This process therefore is a special case of the classic finite horizon multi-armed bandit problem [33]. The key property of the state space corresponding to Bayesian updating is that the rewards satisfy the martingale property: The reward at the current state is the same as the expected reward of the distribution over next states when a play is made in that state. The Bayesian MAB formulation is therefore a canonical example of the Martingale Reward Bandit considered in recent literature such as [30]3.\nIn this paper, we study several Bayesian MAB variants which are motivated by side-constraints which arise both from modern and historical applications. We briefly outline some of these constraints below, and discuss technical challenges that arise later on.\n(a) Arms can have budgets on how much reward can be accrued from them - this is a very natural constraint in most applications of the bandit setting.\n(b) The decision to stop playing an arm can be irrevocable due to high setup costs (or destruction) of the availability of the underlying action.\n(c) There could be a switching cost from one arm to another; these could be economically motivated and adversarial in nature. These could also arise from feasibility constraints on policies for instance, energy consumption in a sensor network to switch measurements from one node to the next, or for a robot to move physically to a different location.\n(d) Feedback about rewards can have a time-delay in arriving, for instance in pay-per-conversion auction mechanisms where information about a conversion arrives at a later time. These delays can be non-uniform.\n(e) The reward at a time-step can be a non-convex function of the set of arms played at that step. Consider for instance, when a person is shown multiple different advertisements, and the sale is attributed to the \u201cmost influential\u201d advertisement that was shown; or consider a situation where packets are sent across multiple links (arms) and delivering the packet twice does not give any additional reward.\n(f) An extreme example of (e) is the futuristic optimization where the the actions taken over the T steps are for exploration only. The goal of the optimization to maximize the reward of the action taken at the T + 1st step.\n2We also consider the situation where the T are used for pure exploration, and the goal is to optimize the expected reward for the T + 1st step only.\n3While most of the results in this paper translate to the larger class of Martingale Reward bandits, we focus the discussion on Bayesian Bandits in the interest of simplicity.\nViolation of Exchange Properties. While the constraints outlined above appear to be very different, they all outline a fundamental issue: They all violate the property that the reward of the arm also does not change depending of when it is scheduled for play. This property of exchange of plays is the most known application of the idling bandit property, which ensures that the state of an arm does not change while that arm is not being played. As a consequence an arm which has high reward on the current play, can be played immediately (\u201cexchanged\u201d with its later play) without any loss of reward. This ability of exchanging plays, is at the core of all existing analysis of stochastic MAB problems with provable guarantees [21, 14, 32]. In fact index policies provide the sharpest example of such an use of the exchangeable property. Contingent on this property, the scheduling decisions can be decoupled from the policy decisions \u2014 as a result the optimization problem can be posed without resorting to a \u201ctime indexed\u201d formulation.\nHowever this exchangeability of plays does not hold in all the problems we consider here. For example, in constraint (e), the reward is non-convex combination of the outcomes of simultaneous plays of different arms, the reward from the play of an arm is not just the function of the current play of the arm, because the other arms may have larger or smaller values. This is also obviously true if the goal is to maximize the reward of the T + 1st step, as in constraint (f). The same issue arises in constraint (d) \u2013 the information derived about an arm is not a function of the current play of the arm; since we may be receiving delayed information about that arm due to a previous play. A similar phenomenon occurs in constraints (c) and (b) where conditioned on the decision of switching to another arm, the (effective) reward from an arm changes even though we have not played the arm. For the above problems, the traditional and well understood index policies such as the Gittins index [34, 64] which are optimal for discounted infinite horizon settings, are either undefined or lead to poor performance bounds. Therefore natural questions that arise are: Can we design provably near optimal policies for these problems? Can such policies be shown to be almost (relaxations of) index policies? And perhaps more importantly: What new conceptual analysis idea can we bring to bear on these problems? Answering these questions is the focus of this paper."}, {"heading": "1.1 A Solution Recipe", "text": "Our main contribution is to present a general solution recipe for all the above constraints. At a high level, our approach uses linear programming of a type that is similar to the widely used weakly coupled relaxation for bandit problems4 or \u201cdecomposable\u201d linear program (LP) relaxation [14, 66]. This LP relaxation provides a collection of single-arm policies whose execution is confined to one bandit arm. Such policies, though not feasible, are desirable for two reasons: First, they are efficient to compute since the computation is restricted to one bandit arm (in contrast, note that the joint state space over all the arms is exponential in n); and secondly, they often naturally yield index policies. However, as mentioned before, prior results in this realm crucially require indexability, and fail to provide any guarantees for non-indexable bandit problems of the type we consider. Our recipe below provides a novel avenue for efficiently designing and analyzing feasible policies, albeit at the cost of optimality.\nSingle-arm Policies: The first step of the recipe is to consider the execution of a global policy restricted to a single arm, and express its actions only in terms of parameters related to that arm. This defines a single-arm policy. We identify a tractable state space for single arms, which is a function of the original state spaces Si, and reason that good single arm policies exist in that space. Though classic index policies are constructed using this approach [54, 66], our approach is fundamentally different: In a global policy, actions pertaining to a single arm\n4We owe the terminology \u201cweakly coupled\u201d to Adelman and Mersereau [1].\nneed not be contiguous in time \u2013 our single-arm policies make them contiguous. We show that the martingale property of the rewards enables us to do this step, and this will be the crux of the subsequent algorithm. In essence, though these policies are weaker in structure than previously studied policies, this lack of structure is critical in deriving our results.\nLinear Program: We write a linear programming relaxation, which finds optimal single-arm policies feasible for a small set of coupling constraints across the arms, where the coupling constraints describe how these single arm policies interact within a global policy. This step can either be a direct linear program; however, this is infeasible in many cases, and we show a novel application of the Lagrangian relaxation to not only solve this relaxation, but also analyze the approximation guarantee the relaxation produces.\nScheduling: The single-arm policies produced by the relaxation are not a feasible global policy. Our final step is to schedule the single arm policies into a globally feasible policy. This scheduling step needs ideas from analysis of approximation algorithms, and specially from the analysis of adaptivity gaps. We note that the feasibility of this step crucially requires the weak structure in the single-arm policies alluded to in the first item. The final policies we design are indeed very similar to (though not the same as) index policies; we highlight this aspect in Section 3.5.\nWe note that each of the above steps requires new ideas, some of which are problem specific. For instance, for the first step, it is not always obvious why the single-arm policies have polynomial complexity. In constraint (e), the reward depends on the specific set of K arms that are played at any step. This does not correspond naturally to any policy whose execution is restricted to a single arm. Similarly, in constraint (d), the state for a single arm depends on the time steps in the past where plays were made and the policy is awaiting feedback \u2013 this is exponential in the length of the delay. Our first contribution is to show that in each case, there are different state space for the policy which has size polynomial in \u2211 i |Si|, T , over which we can write a relaxed decision problem.\nSimilarly, the scheduling part has surprising aspects: For constraint (e), the LP relaxation does not even encode that we receive the max reward every step, and instead only captures the sum of rewards over time steps. Yet, we can schedule the single-arm policies so that the max reward at any step yields a good approximation. For in constraint (d), the single-arm policies wait different amounts of time before playing, and we show how to interleave these plays to obtain good reward.\nFinally, there are fundamental roadblocks, above and beyond the specific issues mentioned for each problem, that applies to almost every problem of this genre. There are very few techniques that provide good algorithm-independent upper bounds for these problems. One such approach has been linear programming. But there is a fundamental hurdle regarding the use of linear programs in the context of MAB problems.\nA linear program often (and for many of the cases described herein) accounts for the rewards globally after all observations have been revealed. In contrast, a policy typically has to adapt as the observations are revealed. The gap between these two accounting processes is termed as the adaptivity gap (an example of such in the context of scheduling can be found in [25]). The adaptivity gap is a central facet of bandit problems where the policies are adapt to the observable outcomes. We show that there exists policies for whom the adaptivity gap is at most a constant factor! Observe that showing this property also implies satisfying an approximate version of the exchange property, namely, up to a constant factor the reward from a play does not depend on when the play is made. Note that this property does not (can not) hold on a per play basis and is a global statement which holds in expectation."}, {"heading": "1.2 Problem Definitions, Results, and Roadmap", "text": "We now describe the specific problems we study in this paper, highlighting the algorithmic difficulty that arises therein. In terms of performance bounds, we show that the resulting policies have expected reward which is always within a fixed constant factor of the reward of the optimal policy, regardless of the parameters (such as n, T ) of the problem, and regardless of the nature of the priors Di of the arms. Such a multiplicative guarantee is termed as approximation ratio or factor5. We present constant factor approximation algorithms for all the problems we consider. The running times we achieve are small polynomial functions of T and \u2211 i |Si|, which is also the complexity of an explicit representation of the state space of each arm. As mentioned above, our running times are very close to the time taken to compute standard indices.\nApproximation algorithms are a natural avenue for designing policies, since fairly natural versions of the problems we consider can be shown to be NP-Hard. For instance, for constraint (e), where the per-step reward is the maximum observed value from the set of plays is NP-Hard because it generalizes computing a subset of k distributions that maximizes the expected maximum [35]. The bandit problem with switching costs (constraint c) is Max-SNP Hard6 since it generalizes an underlying combinatorial optimization problem termed orienteering [17]. For other problems, such as the constraints of irrevocable policies (constraint b), there are examples where no global policy satisfying the said constraint can achieve a reward within a small constant factor of the upper bound proved by some natural linear programming relaxation. In this case, and many others we provide alternate analysis of existing heuristics which have been shown to perform well in practice [30].\nWe now discuss the specific problems. Each of the problems involve budgets; however, these can be incorporated in the state space in a natural way (by not allowing states corresponding to larger rewards). In some cases, specially constraint (e), budgets pose more unusual challenges.\nIrrevocable Policies for Finite Horizon MAB Problem. This problem was formalized by Farias and Madan [30]. This is the Bayesian MAB problem described earlier where the policy plays at most K arms in a step and the objective is to maximize the expected reward over a horizon of T steps. Each of the arms have budgets on the total reward that can be accrued from that arm. There is an additional constraint that we do not revisit any arm that we have stopped playing. Using the algorithmic framework introduced in [38], Farias and Madan showed an 8 approximation algorithm which also works well in practice. In Section 3, we provide a better analysis argument (based on revisiting [38] and newer ideas) and that argument improves the result to a factor (2 + )-approximation for any > 0 in time O(( \u2211 i#Edges(Si)) log(nT/ )) where #Edges(Si) is the number of edges that define the statespace Si. The parameter #Edges(Si) is a natural representation of the sparsity of the input and thus we can view the algorithm to be almost linear time in that said measure. We also show that 2\u2212O( 1n) is the best possible bound against weakly coupled relaxations over single arm policies for n arms. Although this is not the main result in this paper, we present this problem first because the intuitive analysis idea is used throughout the paper and this problem has a right level of complexity for illustrative purposes.\nTraversal Dependent MAB Problems. It is widely acknowledged [12, 19] that the scenarios that call for the application of bandit problems typically have constraints/costs for switching arms. Banks and Sundaram [12] provide an illuminating discussion even for the discounted reward version, and highlight the technical difficulty in designing reasonable policies. Since the switching costs\n5Following standard convention, we will express this ratio as a number larger than 1, meaning that it will be an upper bound on how much larger the optimal reward can be compared to the reward generated by our policy.\n6There exists > 0 such that computing a 1 + approximation is NP Hard.\ncouple the arms together, it is not even clear how to define a weakly coupled system. These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32]. For instance [12, 11], consider a worker who has a choice of working in k firms. A priori, she has no idea of her productivity in each firm. In each time period, her wage at the current firm is an indicator of her productivity there, and partially resolves the uncertainty in the latter quantity. Her expected wage in the period, in turn, depends on the underlying (unknown) productivity value. At the end of each time step, she can change firms in order to estimate her productivity at different places. Her payoff every epoch is her wage at the firm she is employed in at that epoch. The added twist is that changing firms does not come for free, and incurs a cost to the worker. What should the strategy of the worker be to maximize her (expected) payoff? A similar problem can be formulated from the perspective of a firm trying out different workers with a priori unknown productivity to fill a post. On the other hand, the switching costs may be constraints imposed on the policies by physical considerations \u2013 for example an energy constrained robot moving to specific locations. The above discussion suggests two sets of problems \u2013 the Adversarial Order Irrevocable Policies for Finite Horizon MAB problem and the MAB with Metric Switching Costs problem. We address each of them below:\nAdversarial Order Irrevocable Policies for Finite Horizon MAB. In all these cases, the decision agent may not have the flexibility to choose an ordering of events and yet have to provide some algorithm with provable rewards. In this problem we are asked to design a collection of single arm policies subject to a total horizon of T and K plays at a time. After we design the policies, an adversary chooses the order in which we can play the arms irrevocably \u2013 if we quit playing an arm then we cannot return to playing that same arm. We then schedule the policies such that we maximize the expected reward. This problem naturally models a \u201cswitching cost\u201d behavior where the costs are induced by an adversary. To the best of our knowledge this problem had not been formulated earlier \u2013 in Section 4 we provide a (4 + )-approximate solution (when compared to the best possible order we could have received) that can be computed in time O(( \u2211\ni#Edges(Si)) log(nT/ )) as before. The main benefit of this problem is revealed in the next problem on metric switching costs.\nMAB with Metric Switching Costs. In this variant, the arms are located in a metric space with distance function `. If the previous play was for arm i and the next play is for arm j, the distance cost incurred is `ij . The total distance cost is at most L in all decision paths. The policy pays this cost whenever it switches the arm being played. If the budget L is exhausted, the policy must stop. This problem already models the more common \u201cswitching in-and-out\u201d costs using a star graph. To the best of our knowledge there was no prior theoretical results on the Bayesian multi-armed bandit with metric switching costs7. Observe that in absence of the metric assumption, the problem already encodes significantly difficult graph traversal problems and are unlikely to have bounded performance guarantees. In Section 4, we present a (4+ )-approximation8 (for any > 0) the Finite-horizon MAB problem with metric switching costs for K = 1. The result worsens to (4.5 + )-approximation for any K \u2265 2. This result uses the ideas from the adversarially ordered bandits and separates the hard combinatorial traversal decisions from the bandit decisions.\n7Except the conference paper [40], which is subsumed by this article. 8These result weakens if we use the 4-approximation in [17] instead of the 2 + approximation for the orienteering\nproblem in [22]. The constants become 16/3 + and 25/4 + for K = 1 and K \u2265 2 respectively.\nDelayed Feedback. In this variant, the feedback about a play (the observed reward) for arm i is obtained after some \u03b4i time steps. This notion was introduced by Anderson [5] in an early work in mid 1960s. Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28]. Recently, this issue of delays has been thrust to the fore due to the increasing application of iterative allocation problems in online advertising. As an example described in Agarwal etal in [2], consider the problem of presenting different websites/snippets to an user and induce the user to visit these web pages. Each pair of user and webpage define an arm. The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55]. There has been little formal analysis of delayed feedback and its impact on performance despite the fact that delays are ubiquitous.\nWe note that in the case of delayed feedback, even for a single arm, the decision policy needs to keep track of plays with outstanding feedback for that arm, so that the optimal single-arm decision policy has complexity which is exponential in the delay parameter \u03b4i. It is therefore not clear how to solve the natural weakly coupled LP efficiently. In a broader sense, the challenge in the delayed setting is not just to balance explorations along with the exploitation, but also to decide a schedule for the possible actions both for a single arm (due to delays) as well as across the arms. In Section 5, we show several constant factor approximations for this problem. We show that when maxi \u03b4i \u2264 \u221a T/50 then the approximation ratio does not exceed 3 even for playing K different arms (with additive rewards) simultaneously. The approximation ratio worsens but remains O(1) for maxi \u03b4i \u2264 T/(48 log T ). Interestingly the policies obtained from these algorithm satisfy the property for each single arm which is best summarized as: \u201cquit, exploit or double the exploration\u201d which is a natural policy in retrospect. Moreover the different algorithmic approaches expose the natural \u201cregimes\u201d of the delay parameter from small to large.\nNon-Convex Reward Functions. Typically, MAB problems have focused on either the reward from the played arm, or if multiple arms are played, the sum of the rewards of these arms. However, in many applications, the reward from playing multiple arms is some concave function of the individual rewards. Consider for instance charging advertisers based on tracking clicks made by a user before they make a conversion; in this context, the charge is made to the most influential advertiser on this path and is hence a non-linear function of the clicks made by the user. As another example, consider an fault tolerant network application wishing to choose send packets along K independent routes [3]. The network only has prior historical information about the success rate of each of the routes and wishes to simultaneously explore the present status of the routes as well as maximize the probability of delivery of the packet. This problem can be modeled as a multi-armed bandit problem where multiple arms are being played at a time step and the reward is a nonlinear (but concave) function of the rewards of the played arms. As a concrete example, we define the MaxMAB problem, where at most K arms can be played every step, and the reward at any step is the maximum of the observed values. This problem produces two interesting issues:\nFirst, if only the maximum value is obtained as a reward, should the budgets of the other arms be decreased? A case can be made for either an answer of \u201cyes\u201d (budgets indicate opportunity) which defines a All-Plays-Cost accounting or an answer of \u201cno\u201d (budgets indicate payouts of competing advertisers in a repeated bidding scenario) which defines Only-Max-Costs accounting.\nSecond, if multiple arms are played and their rewards are non-additive, does the modeling of how the feedback is received (One-at-a-Time or Simultaneous-Feedback) affect us? In Section 6,\nwe provide a (4 + )-approximation in the model where the budgets of all arms are decreased and the feedback is received one-at-a-time. We also show that these four variants (from the two issues) are related to each other and their respective optimum solutions do not differ by more than a O(1) factor. We show this by providing algorithms in a weaker model that achieve O(1) factor reward of the optimum in a correspondingly stronger model. From a policy perspective, the policies designed for this problem have the property that restricted to a single arm, the differences between small reward outcomes are ignored. Therefore the policy focuses on an almost index-like computation of only the large reward outcomes.\nFuture Utilization and Budgeted Learning. One of the more recent applications of MAB has been the Budgeted Learning type applications popularized by [50, 52, 58]. This application has also been hinted at in [7]. The goal of a policy here is to perform pure exploration for T steps. At the T + 1st step, the policy switches to pure exploitation and therefore the objective is to optimize the reward for the T +1st step. The simplest example of such an objective is to choose one arm so as to maximize the (conditional) expected value; more formally, given any policy \u03c0 we have a probability distribution over final (joint) state space. In each such (joint) state, the policy chooses the arm that maximizes the expected value \u2013 observe that this is a function g(\u03c0) of the chosen policy. The goal is to find the policy that maximizes the expected value of g(\u03c0) where the expectation is taken over all initial conditions and the outcomes of each play. Note that the non-linearity arises from the fact that different evolution paths correspond to different final choices. Natural extensions of such correspond to Knapsack type constraints and similar problems have been discussed in the context of power allocation (when the arms are channels) [24] and optimizing \u201cTCP friendly\u201d network utility functions [51]. In Section 7 we provide a (3 + )-approximation for the basic variant9."}, {"heading": "1.3 Related Work", "text": "Multi-armed bandit problems have been extensively studied since their introduction by Robbins in [56]. From that starting point the literature has diverged into a number of (often incomparable) directions, based on the objective and the information available. In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start. However, these results both require the reward rate to be large and large time horizon T compared the number of arms (that is, vanishing n/T ). In the application scenarios mentioned above, it will typically be the case that the number of arms is very large and comparable to the optimization horizon and the reward rates are low.\nMoreover almost all analysis of stochastic MAB in the literature has relied on the exchange property; these results depend on \u201cmoving plays/exchange arguments\u201d \u2014 if we can play arm i currently, we consider waiting or moving the play of the arm to a later time step without any loss. The exchange properties are required for defining submodularity as well as its extensions, such as sequence or adaptive submodularity [60, 37, 4]. The exchange property is not true in cases of irrevocability (since we cannot have a gap in the plays, so moving even a single play later creates a gap and is infeasible), delays (since if the next play of an arm is contingent on the outcome of the previous play, we cannot move the first play later arbitrarily because then we may make the second play before the outcome of the the first is known, which is now infeasible), or non-linear rewards (since the reward is explicitly dependent on the subset of plays made together). It may be appear\n9Subsuming the main result of the conference paper [38].\nthat we can avoid the issue by appealing to non-stochastic/adversarial MABs [9] \u2013 but they do not help in the presence of budget constraints which couples the action across various time steps. It is known that online convex analysis cannot be analyzed well in the presence of large number of arms and a \u201cstate\u201d that couples the time steps [29]. Budgets are one of the ways a \u201cstate\u201d is enforced. Delays and irrevocability also explicitly encode state. All future utilization objectives trivially encode state as well.\nThe problem considered in [61] is similar in name but is different from the MaxMAB problem we have posed herein \u2014 that paper maximizes the single maximum value seen across all the arms and across all the T steps. The authors of [36] show that several natural index policies for the budgeted learning problem are constant approximations using analysis arguments which are different from those presented here. The specific approximation ratios proved in [36] are improved upon by the results herein with better running times. The authors of [44] present a constant approximation for non-martingale finite horizon bandits; however, these problems require techniques that are orthogonal to those in this paper. The problems considered in [43] is an infinite horizon restless bandit problem. Though that work also uses a weakly coupled relaxation and its Lagrangian (as is standard for many MAB problems), the techniques we use here are different."}, {"heading": "2 Preliminaries: State Spaces, Budgets and Decision Policies", "text": "Recall the basic setting of the finite horizon Bayesian MAB problem. There is a set of n independent arms. Arm i provides rewards in an i.i.d. fashion from a parametrized distribution Di(\u03b8i). The parameter \u03b8i can be arbitrary. It is unknown at the outset, but a prior Di is specified over possible values of \u03b8i. The priors of different arms are independent. At each step, a decision policy can play a set of arms (as allowed by the constraints of the specific problem) and observe the outcome of only the set of arms that have been played. Based on the outcomes, the priors {Di} of the arms that were played in the previous step are updated using Bayes\u2019 rule to the corresponding posterior distributions. The objective of the decision policy (most often) is to maximize the expected sum of the rewards obtained over the T time steps, where the expectation is taken over the priors as well as the outcome of each play. We note that other objectives are possible; we also consider the case where the T plays are used for pure exploration, and the goal is to optimize the expected reward for the T + 1st play."}, {"heading": "2.1 State Space of an Arm and Martingale Rewards", "text": "Let \u03c3 denote a sequence of observed rewards, when k = |\u03c3| plays of arm i are made. Given these observations, the prior Di uniquely updates to a posterior Di\u03c3. Since the rewards are i.i.d., only the set (as opposed to sequence) of observations \u03c3 matters in uniquely defining the posterior distribution. Suppose Di(\u03b8) is a r valued distribution, a sequence of k plays can therefore yield( r+k k ) sets \u03c3. Each of these uniquely defines a posterior distribution Di\u03c3. We term each of these sets \u03c3 as a state of arm i. At any point in time, the arm i is in some state, and over a horizon of T steps, the number of possible states is O(T r). We denote the set of these states as Si, and a generic state as u \u2208 Si. Let the start state corresponding to the initial prior Di be denoted \u03c1i.\nGiven a posterior distribution D of arm i (corresponding to some state u \u2208 Si and set of observations \u03c3), the next play can be viewed as follows: A parameter \u03b8i is drawn from D, and a reward is observed from distribution Di(\u03b8i). Therefore, the posterior distribution D at state u also defines the distribution over observed rewards of the next play. If the observed reward is q, this modifies the state from u with set of observations \u03c3 to state v corresponding to the set \u03c3\u222a{q}. We define puv as the probability of this event conditioned on being at state u and executing a play.\nWe further define ru = E[Di(\u03b8)|Si = u] as the expected value of the observed reward of this play. We term this the reward of state u. Let #Edges(Si) be the number of non-zero transition edges in Si, i.e., puv 6= 0. We use the notation |Si| to denote the number of states and if the play in each state has at most d outcomes then #Edges(Si) \u2264 d|Si| \u2264 |Si|2. Observe that \u2211 i#Edges(Si) is a natural measure of input sparsity.\nCompact Representation and Martingale Property. At this point, we can forget about Bayesian updating (for the most part), and pretend we have a state space Si for arm i, where state u yields reward ru, and has transition probability matrix {puv}. The goal is to design a decision policy for playing the arms (subject to problem constraints) so that the expected reward over T steps is maximized. Since the prior updates satisfy Bayes\u2019 rule, it is a standard observation that ru = \u2211 v\u2208Si puvrv for all states u \u2208 Si. In other words, the state space of an arm satisfies the Martingale property on the rewards. We note that all our results except for the one in Section 5 hold for arbitrary state spaces satisfying the martingale property; however, we choose to present the results for case of Bayesian updates, since this is the canonical and most widely known application of the MAB problems.\nExample: Bernoulli Bandits. Consider a fixed but unknown distribution over two outcomes (success or failure of treatment, click or no click, conversion or no conversion). This is a Bernoulli(1, \u03b8) trial where \u03b8 is unknown. A family of prior distribution on \u03b8 is the Beta distribution; this is the conjugate prior of the Bernoulli distribution, meaning that the posterior distributions also lie within the same family. A Beta distribution with parameters \u03b10, \u03b11 \u2208 Z+, which we denote Beta(\u03b11, \u03b10) has p.d.f. of the form c\u03b8\u03b11\u22121(1\u2212\u03b8)\u03b10\u22121, where c is a normalizing constant. Beta(1, 1) is the uniform distribution. The distribution Beta(\u03b11, \u03b10) corresponds to the current (posterior) distribution over the possible values of \u03b8 after having observed (\u03b11\u2212 1) 1\u2019s and (\u03b10\u2212 1) 0\u2019s, starting from the belief that \u03b8 was uniform, distributed as Beta(1, 1).\nGiven the distribution u = Beta(\u03b11, \u03b10) as the prior, the expected value of \u03b8 is \u00b5u = \u03b11\n\u03b11+\u03b10 ,\nand this is also the expected value of the observed outcome of a play conditioned on this prior. Updating the prior on a sample is straightforward. On seeing a 1, the posterior (of the current sample, and the prior for the next sample) is v = Beta(\u03b11 + 1, \u03b10), and this event happens with probability \u00b5u. On seeing a 0, the new distribution is w = Beta(\u03b11, \u03b10 + 1) and this event happens with probability 1\u2212 \u00b5u.\nThe posterior density u = Beta(\u03b11, \u03b10) corresponds to state u \u2208 Si, and can be uniquely specified by the values (\u03b11, \u03b10). Therefore, ru = \u00b5u. If the arm is played in this state, the state evolves to v with probability puv = \u00b5u and to w with probability puw = 1\u2212 \u00b5u. As stated earlier, the hard case for our algorithms (and the case that is typical in practice) is when the input to the problem is a set of arms {i} with priors Di \u223c Beta(\u03b11i, \u03b10i) where \u03b10i \u03b11i which corresponds to a set of poor prior expectations of the arms. Observe that #Edges(Si) \u2264 2|Si| for this example."}, {"heading": "2.2 Modeling Budgets and Feedback", "text": "One ingredient in our problem formulations is natural budget constraints in individual arms. As we will see later, these constraints introduce non-trivial complications in designing the decision policies. There are three types of budget constraints we can consider.\nPlay Budget Model: This corresponds to the number of times an individual arm i can be played, typically denoted by Ti where Ti \u2264 T . In networking, each play attempts to add traffic to a\nroute and we may choose to avoid congestion. In online advertising, we could limit the number of impressions of a particular advertisement, often done to limit advertisement-blindness.\nAll-Plays-Cost Model The total reward obtained from arm i should be at most its budget Bi. The arm can be played further, but these plays do not yield reward.\nOnly-Max-Costs Model In MaxMab, the reward of only the arm that is maximum is used, and hence only this budget is depleted. In other words, there is a bound on the total reward achievable from an arm (call this Ai), but we only count reward from an arm when it was the maximum value arm at any step.\nObserve that the play budget and All-Plays-Cost models simply involve truncating the state space Si, so that an arm cannot be played if the number of plays or observations violates the budget constraint \u2013 this obviates the need to discuss this constraint explicitly in most sections. The Only-Max-Costs model is more complicated to handle, requiring expanding the state space to incorporate the depleted budget. We discuss that model only in Section 6.4. Note however that the All-Plays-Cost model is also nontrivial to encode in the context of delays in feedback, and we discuss that model in Section 5 further.\nModeling the feedback received is an equally important issue. We just alluded to delays in the feedback for the play of a single arm. However if multiple plays are being made simultaneously \u2013 the definition of simultaneity has interesting consequences. We can conceive two models:\nOne-at-a-Time Feedback: In this model even if we are allowed to play K > 1 arms in a single time slot, we make only one play a time and receive the feedback before making the play of a different arm in that same time slot.\nSimultaneous-Feedback: In this model we decide on a palette of at most K plays in a time slot and make them before receiving the result of any of the plays in that time slot.\nClearly, the One-at-a-Time model allows for more adaptive policies. However for additive rewards, where we sum up the rewards obtained from all the plays made in a time slot \u2013 this issue regarding the order of feedback is not very relevant. However in the case where we have subadditive rewards then the distinction between One-at-a-Time and Simultaneous-Feedback is clearly significant \u2013 specially in the All-Plays-Cost accounting. The distinction continues to hold in the Only-Max-Costs pays model as well, because we can adapt the set of arms to play to the outcome of the initial plays in the same time slot."}, {"heading": "2.3 Decision Policies and Running Times", "text": "A decision policy P is a mapping from the current state of the overall system to an action, which involves playing K arms in the basic version of the problem. This action set can be richer, as we discuss in subsequent sections. Observe that the state of the overall system involves all the knowledge available to the policy, including:\n1. The joint states {u \u2208 Si} of all the arms;\n2. The current time step, which also yields the remaining time horizon;\n3. The remaining budget of all the arms;\n4. Plays made in the past for which feedback is outstanding (in the context of delayed feedback).\nTherefore, the \u201cstate\u201d used by a decision policy could be much more complicated than the product space of Si, and one of our contributions in this paper is to simplify that state space and make it tractable.\nRunning Times. Our goal will be to compute decision policies in time that depends polynomially on T and \u2211 i |Si|, which is the sum of the sizes of the state spaces of all the arms. In the case of Bernoulli bandits discussed above, this would be polynomial in the number of arms n, and the time horizon T . Our running times and the description of the final policies will be comparable (in a favorable way) to the time required for dynamic programming to compute the standard index policies [14, 34]. This will become clear in Section 3; however, fine-tuning the running time is not the main focus of this paper. However our running times will often be linear in the natural input sparsity, that is, \u2211 i#Edges(Si)."}, {"heading": "2.3.1 Single Arm Policies", "text": "A single arm policy Pi for arm i is a policy whose execution (actions and rewards) are only confined to the state space of arm i. In other words, it has one of several possible actions at any step; in the basic version of the problem, the available actions at any step involve either (i) play the arm \u2013 if the arm is in state u \u2208 Si, this yields reward ru; or (ii) stop playing and exit. Furthermore, the actions of Pi only depend on the state variables restricted to arm i \u2013 the current state u \u2208 Si, remaining budget of arm i, the remaining time horizon, etc. In most situations we will eliminate states that cannot be reached in T steps (the horizon). Formally;\nDefinition 1. Let Si(T ) be the state space Si truncated to T steps. Let Ci(T ) describe all single-arm policies of arm i with horizon T . Each Pi \u2208 Ci(T ) is a (randomized) mapping from the current state of the arm to an action. The state of the system is captured by the current posterior u \u2208 Si(T ) and the remaining time horizon. Such a policy at any point in time has at two available actions: (i) Play the arm; or (ii) stop. Note that the states and actions are for the basic problem, so that variants will have more states/actions which will be described at the appropriate context.\nDefinition 2. Given a single-arm policy Pi let R(Pi) to be the expected reward and T (Pi) as the expected number of plays made. The expectation is taken over all randomizations within the policy as well as all outcomes of all plays.\nThe construction of single-arm policies will be non-trivial, as will be described in subsequent sections. One of the contributions of this paper is in the development of single-arm policies with succinct descriptions. Succinct descriptions also allow us to focus on key aspects of the policy and analyze simple and common modifications of the policies."}, {"heading": "3 Irrevocable Policies for Finite Horizon Bayesian MAB Problems", "text": "In this section we consider the basic finite horizon Bayesian MAB problem (as defined in Section 1) with the additional constraint of Irrevocability; that is, the plays for any arm are contiguous. Recently, Farias and Madan [30] provided an 8-approximation for this problem. Previously, constant factor approximations were provided in [36, 40] for the finite horizon MAB problem (not necessarily using irrevocable policies). The analysis herein provides a significant strengthening of all previous results on this problem. The contribution of this section is the introduction of a palette of basic techniques which are illustrative of all the (significantly more difficult) problems in this paper.\nIn term of specific results we show that given a solution of the standard LP formulation for this problem, a simple scheduling algorithm provides a 2 approximation (Theorem 4). Moreover we prove that this is the best possible result against the standard LP solution. We then show that we can compute a near optimal solution of the standard LP efficiently, that is, for any \u2208 (0, 1] we provide an algorithm that runs in O(( \u2211 i#Edges(Si)) log(nT/ )) time and provides a 2 + -approximate solution against the optimum solution of that standard LP (Theorem 7 and Corollary 8). In terms of techniques we introduce a compact LP representation and its analysis based on Lagrangians, which are used throughout the rest of the paper. Likewise we consider the technique of truncation in Section 3.2 \u2013 while this technique is similar to [30, Lemma 2] but our presentation uses a slightly different intuition that is also useful throughout this paper.\nRoadmap: We present the standard LP relaxation that bounds the value of the optimum solution OPT in Section 3.1; along with an interpretation of fractional solution of that LP in terms of single arm policies. We show how the standard LP can be represented in a compact form demonstrating the weak coupling. As stated, we discussion truncation in Section 3.2. In Section 3.3 we then present the 2-approximation result (Theorem 4) and the (family of) examples which demonstrate that the bound of 2 is tight. In Section 3.4 we then show how to achieve the (1 + )-approximation (Theorem 7 and Corollary 8) for the compact LP. We conclude in Section 3.5 with a comparison between the Lagrangian induced solutions and the Gittins Index."}, {"heading": "3.1 Linear Programming Relaxations and Single arm Policies", "text": "Consider the following LP, which has two variables wu and zu for each arm i and each u \u2208 Si(T ). Recall Si(T ) is the statespace Si truncated to a horizon of T steps. The variable wu corresponds to the probability (over all randomized decisions made by any algorithm as well as the random outcomes of all plays made by the same algorithm) that during the execution of arm i enters state u \u2208 Si. Variable zu corresponds to the probability of playing arm i in state u.\nLP1 = Max n\u2211 i=1 \u2211 u\u2208Si(T )\nruzu\u2211n i=1 \u2211 u\u2208Si zu \u2264 KT\u2211\nv\u2208Si zvpvu = wu \u2200i, u \u2208 Si(T ) \\ {\u03c1i} zu \u2264 wu \u2200u \u2208 Si(T ),\u2200i\nzu, wu \u2208 [0, 1] \u2200u \u2208 Si(T ),\u2200i\nWe use LP1 to refer to the value and (LP1) as the LP.\nClaim 1. Let OPT be the value of the optimum policy. Then OPT \u2264 LP1.\nProof. We show that the {w\u2217u, z\u2217u} as defined above, corresponding to a globally optimal policy P\u2217, are feasible for the constraints of the LP.\nThe first constraint follows by linearity of expectation: The expected time for which arm i is played by P\u2217 is Ti = \u2211 u\u2208Si(T ) z \u2217 u. Since at most K arms are played every step, we must have\u2211\ni Ti \u2264 KT , which is the first constraint. Note that Ti \u2264 T , since we have truncated Si to have only states corresponding to at most T observations.\nThe second constraint simply encodes that the probability of reaching a state u \u2208 Si(T ) precisely the probability with which it is played in some state v \u2208 Si(T ), times the probability pvu that it reaches u conditioned on that play. The constraint zu \u2264 wu simply captures that the event an arm is played in state u \u2208 Si(T ) only happens if the arm reaches this state.\nThe objective is precisely the expected reward of the optimal policy \u2013 recall that the reward of playing arm i in state u is ru. Hence, the LP is a relaxation of the optimal policy.\nA similar LP formulation was proposed for the multi-armed bandit problem by Whittle [66] and Bertsimas and Nino-Mora [54]; however, one key difference is that we ignore the time at which the arm is played in defining the LP variables.\nFrom Linear Programs to Single Arm Policies: The optimal solution to (LP1) clearly does not directly correspond to a feasible policy since the variables do not faithfully capture the joint evolution of the states of different arms \u2013 in particular, the optimum solution to (LP1) enforces the horizon T only in expectation over all decision paths, and not individually on each decision path. In this paper, we present such relaxations for each variant we consider, and develop techniques to solve the relaxation and convert the solution to a feasible policy while preserving a large fraction of the reward. Below, we present an interpretation of any feasible solution to (LP1), which also helps us compact representation subsequently.\nLet \u3008wu, xu, zu\u3009 denote any feasible solution to the (LP1). Assume w.l.o.g. that w\u03c1i = 1 for all i. Ignoring the first constraint of (LP1) for the time being, the remaining constraints encode a separate policy Pi for each arm as follows: Consider any arm i in isolation. The play starts at state \u03c1i. The arm is played with probability z\u03c1i , so that state u \u2208 Si(T ) is reached with probability z\u03c1ip\u03c1iu. Similarly, conditioned on reaching state u \u2208 Si(T ), with probability zu/wu, arm i is played. This yields a policy Pi for arm i which is described in Figure 1.\nFor policy Pi, it is easy to see by induction that if state u \u2208 Si(T ) is reached by the policy with probability wu, then state u \u2208 Si(T ) is reached and arm i is played with probability zu. Therefore, Pi satisfies all the constraints of (LP1) except the first. The first constraint simply encodes that the total expected number of plays made by all these single-arm policies {Pi} is at most KT .\nNote that any feasible solution of (LP1) defines a collection of single arm policies {Pi} and each Pi \u2208 Ci(T ) as defined in Definition 1. Using the notation in Definition 2, R(Pi) = \u2211 u\u2208Si(T ) ruzu\nand T (Pi) = \u2211\nu\u2208Si(T ) zu we can represent (LP1) as follows:\nLP1 = Max{Pi\u2208Ci(T )} {\u2211 i R(Pi) | \u2211 i T (Pi) \u2264 KT }"}, {"heading": "3.2 The Idea of Truncation", "text": "We now show that the time horizon of a single-arm policy on T steps can be reduced to \u03b2T for constant \u03b2 \u2264 1 while sacrificing a constant factor in the reward. We note that though the statement seems simple, this theorem only applies to single-arm policies and is not true for the global policy executing over multiple arms. The proof of this theorem uses the martingale structure of the rewards, and proceeds via a stopping time argument. A similar statement is proved in [30, Lemma\n2] using an inductive argument; however that exact proof would require modifications in the varied setting we are applying it and therefore we restate and prove the theorem.\nDefinition 3. Given a policy P, a decision path is a sequence of (observed reward, action) pairs encountered by the policy till stopping. Different decision paths are encountered by the policy with different probabilities, which depend on the observed rewards.\nRecall the notation R(P), T (P) introduced in Definition 2.\nTheorem 2. (The Truncation Theorem.) Consider an arbitrary single arm policy P executing over state space S defined by i.i.d. draws from a reward distribution D(\u03b8), where a prior distribution D is specified over the unknown parameter \u03b8. Suppose E[D(\u03b8)] \u2265 0 for all parameter choices \u03b8. Consider a different policy P \u2032 that executes the decisions of P but stops after making (at least) \u03b2 fraction of the plays on any decision path of P. Then (i) R(P \u2032) \u2265 \u03b2R(P) and (ii) T (P \u2032) \u2264 T (P).\nProof. We view the system as follows. The unknown reward distribution D(\u03b8) has mean \u00b5(\u03b8), and the initial prior D over \u03b8 specifies a prior distribution f(\u00b5) over possible \u00b5. Consider the execution of P conditioned on the event that the (unknown) mean is \u00b5, and denote this execution as P(\u00b5). Let R(P(\u00b5)), T (P(\u00b5)) denote the expected reward and the number of plays of the policy P conditioned on this event. We have R(P) = \u222b R(P(\u00b5))f(\u00b5)d\u00b5 and T (P) = \u222b T (P(\u00b5))f(\u00b5)d\u00b5.\nSuppose the execution P(\u00b5) reaches some state v \u2208 S on decision path q and decides to make a play there. Since the rewards are i.i.d. draws, the expected reward of the next play at v is \u00b5 regardless of the state. We charge the reward to the path q as follows: Regardless of the actual observation at state v corresponding to path q, we charge reward exactly equal to \u00b5 to q for making a play at state v. In other terms, we charge reward exactly \u00b5 whenever a play is made is any state, regardless of the actual observation. It is clear that the expected reward of any play is preserved by this accounting scheme, since the rewards are i.i.d. draws from a distribution with mean \u00b5.\nBy linearity of expectation, the above charging scheme means that for any decision path q, the expected reward is simply \u00b5 times the number of plays made on this path. More formally, suppose decision path q involves l(q) plays and that is taken by P(\u00b5) with probability g(q). Then\nR(P(\u00b5)) = \u2211 q \u00b5\u00d7 l(q)\u00d7 g(q) and T (P(\u00b5)) = \u2211 q l(q)\u00d7 g(q)\nFor any \u00b5, the policy P \u2032(\u00b5) encounters the same distribution over decision paths as P(\u00b5), except that on any decision path, the execution stops after making at least \u03b2 fraction of the plays. This means that for any decision path q of P(\u00b5), the execution P \u2032(\u00b5) makes at least \u03b2l(q) plays which yield at least \u03b2\u00b5l(q) expected reward. The theorem now follows by integrating over all q and \u00b5.\nGiven the equivalence of randomized policies and fractional solutions to the relaxation (LP1), the relaxation has a compact representation: Simply find one single-arm policy for each arm, so that the resulting ensemble of policies have expected number of plays at most KT , and maximum expected reward. This yields the following equivalent version:\nLP1 = Max{Pi\u2208Ci(T )} {\u2211 i R(Pi) | \u2211 i T (Pi) \u2264 KT }\nIn subsequent sections, wherever possible, we work with the compact LP formulation directly. We note that the single-arm policies in these sections have a richer set of actions, and operate on a richer state-space; nevertheless, the derivation of the LP relaxation will be nearly identical to that described in this section. We point out differences as and when appropriate.\n3.3 A 2-Approximation using Irrevocable Policies and a Tight Lower Bound\nRecall that the optimum solution of (LP1), as interpreted in Figure 1) will be collection of single arm policies {P\u2217i } such that LP1 \u2264 \u2211 iR(P\u2217i ) and \u2211 i T (P\u2217i ) \u2264 KT . Such a solution of (LP1) can be\nfound in polynomial time. Given a collection of single arm policies Pi which satisfy \u2211\ni T (Pi) \u2264 KT we introduce the final scheduling policy as shown in Figure 2.\nLemma 3. The policy outlined in Figure 2 provides an expected reward of at least 12 \u2211 iR(Pi).\nProof. Let the number of plays of arm i be Ti. We know E[Ti] = T (Pi) and \u2211\ni T (Pi) \u2264 KT . We start playing arm i after \u2211 j<i Tj plays (if the sum is less than T ); which means the remaining\nhorizon for Pi is T \u2212 min{T, 1K \u2211\nj<i Tj}. We apply the Truncation Theorem 2 with \u03b2 = 1 \u2212 1 T min{T, 1 K \u2211 j<i Tj} and the expected reward of Pi continuing from that starting point onward is(\n1\u2212 1T min{T, 1 K \u2211 j<i Tj} ) R(Pi). Note that this is a consequence of the independence of arm i\nfrom \u2211\nj<i Tj . Thus the total expected reward R is\nR = E \u2211 i 1\u2212 1 T min T, 1K\u2211 j<i Tj  R(Pi)  \u2265 E \u2211 i 1\u2212 1 KT \u2211 j<i Tj R(Pi) \n= \u2211 i 1\u2212 1 KT \u2211 j<i E[Tj ] R(Pi) = \u2211 i ( 1\u2212 \u2211 j<i T (Pj) KT ) R(Pi) (1)\nThe bound in Equation 1 is best represented pictorially; for example consider Figure 3. Equation 1 indicates that R is 1KT fraction of the shaded area in Figure 3, which contains the triangle of area 1 2KT \u2211 iR(Pi). Therefore R \u2265 1 2 \u2211 j R(Pj) and the lemma follows.\nTheorem 4. There exists a simple 2-approximation for the finite horizon Bayesian MAB problem (with budgets and arbitrary priors) using an irrevocable scheduling policy. Such a policy can be found in polynomial time (solving a linear program).\nProof. Given a collection of single arm policies {P\u2217i } which correspond to the optimum solution of (LP1), we apply Lemma 3 with Pi = P\u2217i ; since the collection {P\u2217i } is feasible, namely, \u2211 i T (P\u2217i ) \u2264\nKT . Therefore the expected reward is at least 12 \u2211 iR(P\u2217i ) \u2265 1 2LP1 and the theorem follows.\nTight example of the analysis. We show that the gap of the optimum policy and LP1 is a factor of 2 \u2212 O( 1n), even for unit length plays. Consider the following situation: We have two \u201ctypes\u201d of arms. The type I arm gives a reward 0 with probability a = 1/n and 1 otherwise. The type II arm always gives a reward 0. We have n independent arms. Each has an identical prior distribution of being type I with probability p = 1/n2 and type II otherwise. Set T = n.\nConsider the symmetric LP solution that allocates one play to each arm; if it observed a 1, it plays the arm for n steps. The expected number of plays made is n + O(1/n), and the expected reward is n\u00d7 1/n = 1. Therefore, LP1 \u2265 1\u2212O(1/n).\nConsider the optimum policy. We first observe that if the policy ever sees a reward 1 then the optimum policy has found one of the type II arms, and the policy will continue to play this arm for the rest of the time horizon. At any point of time before the time horizon, since T = n, there is always at least one arm which has not been played yet. Suppose the policy plays an arm and observe the reward 0, then the posterior probability of this arm being type II increased. So the optimum policy should not prefer this currently played arm over an unplayed arm. Thus the optimum policy would be to order the arms arbitrarily and make a single play on every new arm. If the outcome is 0, the policy quits, otherwise the policy keeps playing the arm for the rest of the horizon. The reward of the optimum policy can thus be bounded by \u2211T\u22121 x=0 ap(1 + (T \u2212 x\u2212 1)a) = pa2T (T + 1)/2 + (1\u2212 a)/n = 12 +O( 1 n). Thus the gap is a factor of 2\u2212O( 1 n)."}, {"heading": "3.4 Weak Coupling, Efficient Algorithms and Compact Representations", "text": "We outline how to solve (LP1) efficiently using a standard application of weak duality. Recall,\nLP1 = Max{Pi\u2208Ci(T )} {\u2211 i R(Pi) | \u2211 i T (Pi) \u2264 KT }\nWe take the Lagrangian of the coupling constraint to obtain:\nLPLag(\u03bb) = Max{Pi\u2208Ci(T )}\n{ KT\u03bb+\n\u2211 i (R(Pi)\u2212 \u03bbT (Pi))\n}\nNote now that there are no constraints connecting the arms, so that the optimal policy is obtained by solving LPLag(\u03bb) separately for each arm.\nDefinition 4. Let Qi(\u03bb) = MaxPi\u2208Ci(T )R(Pi) \u2212 \u03bbT (Pi) denote the optimal solution to LPLag(\u03bb) restricted to arm i, so that LPLag(\u03bb) = KT\u03bb + \u2211 iQi(\u03bb). Let Li(\u03bb) denote the corresponding optimal policy for arm i. As a convention if Qi(\u03bb) = 0 then we choose Li(\u03bb) to be the trivial policy which does nothing.\nLemma 5. For any \u03bb \u2265 0 we have LPLag(\u03bb) = \u03bbKT + \u2211\niQi(\u03bb) \u2265 LP1 \u2265 OPT .\nThe above Lemma is an easy consequence of weak duality. We now compute Qi(\u03bb).\nLemma 6. Qi(\u03bb) and the corresponding single-arm policy Li(\u03bb) (completely specifying an optimum solution of LPLag(\u03bb)) can be computed in time O(#Edges(Si)). For \u03bb \u2265 0, R(Li(\u03bb)), T (Li(\u03bb)) and Qi(\u03bb) are non-increasing as \u03bb increases.\nProof. We use a straightforward bottom up dynamic program over the DAG represented by Si(T ) which is the statespace Si restricted to a horizon T .\nLet Gain(u) to be the maximum of the objective of the single-arm policy conditioned on starting at u \u2208 Si(T ). If u has no children, then if we \u201cplay\u201d at node u, then Gain(u) = ru\u2212 \u03bb in this case. Stopping corresponds to Gain(u) = 0. Therefore we set Gain(u) = max{0, ru \u2212 \u03bb} in this case. If u had children, playing corresponds to a gain of ru \u2212 \u03bb+ \u2211 v puvGain(v). Therefore:\nGain(u) = Max { 0, ru \u2212 \u03bb+\n\u2211 v puvGain(v)\n}\nThe policy Pu constructed by the dynamic program rooted at u \u2208 Si(T ) satisfies the invariant R(Pu) = Gain(u) + \u03bbT (Pu). This immediately implies Gain(\u03c1i) = Qi(\u03bb). Note that we can ensure that Gain(u) = 0 is obtained by the trivial policy at u of doing nothing.\nMoreover the decision to play at \u03bb\u2032 also implies a decision to play at \u03bb < \u03bb\u2032. This trivially implies that T (Li(\u03bb)), R(Li(\u03bb)) is non-increasing as \u03bb increases. Likewise, observe that for every u \u2208 Si(T ), we have Gain(u) is nonincreasing as \u03bb increases, and therefore Qi(\u03bb) is nonincreasing.\nIn terms of the variables of the original (LP1), Qi(\u03bb) is defined as follows:\nQi(\u03bb) = Max n\u2211 i=1 \u2211 u\u2208Si(T )\n(ru \u2212 \u03bb)zu\u2211 v\u2208Si zvpvu = wu \u2200i, u \u2208 Si(T ) \\ {\u03c1i}\nzu \u2264 wu \u2200u \u2208 Si(T ),\u2200i zu, wu \u2208 [0, 1] \u2200u \u2208 Si(T ),\u2200i\n(2)\nThe solution presented in Lemma 6 shows that the optimum policy Li(\u03bb) satisfies zu = 0 (no play, or Gain(u) = 0) or zu = wu (play orGain(u) > 0); which are to expected using complementary slackness [14]. By a standard application of weak duality (see for instance [46]), a (1+ ) approximate solution to (LP1) can be obtained by taking a convex combination of the solutions to LPLag(\u03bb) for two values \u03bb\u2212 and \u03bb+; these can be computed by binary search. This yields the following.\nTheorem 7. In time O(( \u2211\ni#Edges(Si)) log(nT/ )), we can compute quantities a, \u03bb\u2212 and \u03bb+, where |\u03bb\u2212\u2212\u03bb+| \u2264 OPT/(KT ) and a fraction a \u2208 [0, 1) so that if P\u2217i denotes the single-arm policy that executes Li(\u03bb\u2212) with probability a and Li(\u03bb+) with probability (1\u2212 a), then these policies are feasible for (LP1) with objective at least OPT/(1 + ).\nProof. Observe that for \u03bb = 0, if we satisfy the constraint \u2211\ni T (Li(\u03bb)) \u2264 KT then the theorem is immediately true based on Lemmas 3.1 and 6 (setting = 0). So in the remainder we assume that \u2211 i T (Li(0)) > KT . Note that OPT \u2264 \u2211 iQi(0). Moreover, for all i, OPT \u2265 Qi(0) since the\noptimum can disregard all other arms. Let M = \u2211\niQi(0) and thus M \u2264 nOPT . Now if we set \u03bb = 2M then all Qi(\u03bb) = 0 because the penalty of \u03bb to the root node is larger than the total reward of the policy. Thus all Li(M) are the trivial null policy. In this case\u2211 i T (Li(M)) = 0 < KT .\nTherefore we can maintain the interval defined by the two numbers \u03bb\u2212 < \u03bb+ such that\u2211 i T (Li(\u03bb\u2212)) > KT and \u2211 i T (Li(\u03bb+)) \u2264 KT . Initially \u03bb\u2212 = 0 and \u03bb+ =\u221e. We can now perform a binary search and maintain the properties till we have \u03bb+ \u2212 \u03bb\u2212 \u2264 M/(2nKT ) \u2264 OPT/(2KT ). Since \u2211 i T (Li(\u03bb\u2212)) > KT \u2265 \u2211 i T (Li(\u03bb+)) there exists an unique a \u2208 [0, 1) such that\na ( T (Li(\u03bb\u2212)) ) + (1\u2212 a) ( T (Li(\u03bb+)) ) = KT\nNote that for such an a, we have \u2211\ni T (P\u2217i ) = a (T (Li(\u03bb\u2212))) + (1\u2212 a) (T (Li(\u03bb+))) = KT thereby satisfying the main constraint in the compact representation of LP1. Observe that for \u03bb \u2208 {\u03bb\u2212, \u03bb+},\n\u03bbKT + \u2211 i {R(Li(\u03bb)\u2212 \u03bbT (Li(\u03bb))} \u2265 OPT (3)\nusing the definition of Qi(\u03bb),Li(\u03bb) and Lemma 3.1. As a consequence, since R(P\u2217i ) = aR(Li(\u03bb\u2212)+ (1\u2212 a)R(Li(\u03bb+); we have:\nOPT \u2264 a [ \u03bb\u2212KT +\n\u2211 i { R(Li(\u03bb\u2212)\u2212 \u03bb\u2212T (Li(\u03bb\u2212))\n}] +\n(1\u2212 a) [ \u03bb+KT +\n\u2211 i { R(Li(\u03bb+)\u2212 \u03bb+T (Li(\u03bb+)) }] = (a\u03bb\u2212 + (1\u2212 a)\u03bb+)KT +\n\u2211 i R(P\u2217i )\u2212 \u03bb\u2212 \u2211 i T (P\u2217i )\u2212 (1\u2212 a)(\u03bb+ \u2212 \u03bb\u2212) \u2211 i T (Li(\u03bb+))\nand since \u2211\ni T (P\u2217i ) = KT the last equation rewrites to\nOPT \u2264 \u2211 i R(P\u2217i ) + (1\u2212 a)(\u03bb+ \u2212 \u03bb\u2212)\n[ KT \u2212\n\u2211 i T (Li(\u03bb+))\n]\nBut that implies OPT \u2264 \u2211 iR(P\u2217i ) + (1 \u2212 a)(\u03bb+ \u2212 \u03bb\u2212)KT \u2264 \u2211\niR(P\u2217i ) + OPT/2. Since for x \u2208 (0, 1] we have 11+x \u2264 1\u2212 x/2 we have OPT/(1 + ) \u2264 \u2211 iR(P\u2217i ).\nObserve that the initial size of the interval is 2M and the final size is at most M/(nKT ). Therefore the number of binary searches is log nKT = O(log(nT/ )) since k < n. The theorem follows.\nUsing Theorem 7, Lemma 3 and a rescaling of the following is immediate: Corollary 8. Given any \u2208 (0, 1], using O(( \u2211\ni#Edges(Si)) log(nT/ )) time we can compute a (2 + )-approximation to the finite horizon Bayesian MAB using irrevocable policies.\nFurther applications of Theorem 7: We prove a corollary which will be useful later; Corollary 9. If in time O(\u03c4) we can compute an c-approximation to \u2211\niQi(\u03bb,C) where Qi(\u03bb,C) is Qi(\u03bb) is defined by the maximizing system (2) with the additional constraint C over single the arm policy; then we can compute a (c + )-approximation for (LP1) for any \u2208 (0, 1], which satisfies \u2211 i T (Pi) = KT/c and the additional restriction over the same constraints C over the single arm policies in time O(\u03c4 log(nT/ )). Further the scheduling policy in Figure 2 now provides a 2c(c+ )2c\u22121 approximation to OPT (C) which is the optimum solution which obeys the constraint that\u2211 i T (Pi) \u2264 KT along with the single arm constraints C.\nProof. A relaxation of OPT (C) can be expressed as a mathematical program (C need not be linear constraints) where we have (LP1) with additional constraints C. However weak duality still holds and for any \u03bb \u2265 0,\n\u03bbKT + \u2211 i Qi(\u03bb,C) \u2265 OPT (C) (4)\nNow the proof of the corollary follows from replicating the proof of Theorem 7 with the following consequence of the approximation algorithm, instead of Equation (3),\n\u03bbKT + c  \u2211 i:Li(\u03bb) obeys C {R(Li(\u03bb)\u2212 \u03bbT (Li(\u03bb))}  \u2265 OPT (C) (5) which follows from equation 4 and the c- approximation of Qi(\u03bb,C). Observe that now if we choose \u03bb\u2212, \u03bb+ such that \u2211 i T (Li(\u03bb\u2212)) > KT/c \u2265 \u2211 i T (Li(\u03bb+)) then we can ensure that \u2211 i T (P\u2217i ) = KT/c. We still execute the policies as described in Figure 2, and the expected reward is at least (following the identical logic as in Lemma 3):\nR\u2032 \u2265 \u2211 i ( 1\u2212 \u2211 j<i T (Pj) KT ) R(Pi)\nPictorially, R\u2032 is at least 1KT times the area of the shaded triangle and the rectangle as shown in Figure 4, which amounts to (1\u2212 12c) \u2211 iR(Pi). Observe that c = 1 corresponds to the statement of Lemma 3. The overall approximation is therefore 2c(c+ )2c\u22121 which proves the theorem."}, {"heading": "3.5 Comparison to Gittins Index", "text": "The most widely used policy for the discounted version of the multi-armed bandit problem is the Gittins index policy [33]. Recall the single-arm policies constructed in Section 3.4. For arm i, consider the policy Li(\u03bb) corresponding to the value Qi(\u03bb). We can account for the reward of this policy as follows. Suppose any policy is given fixed reward \u03bb per play (so that if the expected number of plays is T (P ), the policy earns \u03bbT (P )). Then, the value Qi(\u03bb) is the optimal excess reward a policy can earn given these average values, since this is precisely maxP\u2208Ci(R(P )\u2212\u03bbT (P )). To solve our relaxation (LP1), we find \u03bb so that the total expected number of plays made by the single-arm policies for different i sums to T . The definition of Qi(\u03bb) can be generalized to an equivalent definition Qi(\u03bb, u) for policies whose start state is u \u2208 Si(T ) (instead of being the root\n\u03c1i). The Gittins index for u \u2208 Si(T ) can be defined as:\nGittins Index = \u03a0i(u) = max{\u03bb | Qi(\u03bb, u) > 0}\nIn other words, the Gittins index for state u is the maximum value of R(P )/T (P ) over policies restricted to starting at state u (and making at least one play), i.e., the maximum amortized perstep long term reward obtainable by playing at u. The Gittins index policy works as follows: At any time step, play the arm i whose current state u has largest index \u03a0i(u). For the discounted reward version of the problem, such an index policy yields the optimal solution. This is not true for the finite horizon version, and the other variants we consider below. Nevertheless, the starting point for our algorithms is the solution to (LP1), and as shown above, this has computational complexity similar to the computation of the Gittins index.\nIn contrast to the Gittins index, our policies are based on computing (as in Theorem 7) one global penalty \u03bb\u2217 across all arms by solving (LP1); consider the case \u03bb+ \u2248 \u03bb\u2217 \u2248 \u03bb\u2212 in Theorem 7. For this penalty, for each arm i and state u \u2208 Si(T ), the policy Li(\u03bb\u2217) makes a decision on whether to play or not play. We execute these decisions, and impose a fixed priority over arms to break ties in case multiple arms decide to play."}, {"heading": "4 Traversal Dependent Bayesian MAB Problems", "text": "In this section we consider how the constraints on a traversal of different bandit arms can affect the approximation algorithm. A concrete example of such traversal related constraint is the Bayesian MAB Problem with Switching Costs where there is a cost of switching between arms. Denote the cost of switching from arm i to arm j as `ij \u2208 Z+. The system starts at an arm i0. The goal is to maximize the expected reward subject to rigid constraints that (i) the total number of plays is at most T and (ii) the total switching cost is at most L on all decision paths. This problem has received significant attention, see the discussion in Section 1 and in [12] - however efficient solutions with provable bounds in the Bayesian setting has been elusive.\nA classic example of such a switching cost problem can be when the costs `ij define a distance metric, which is natural in most navigational settings and was considered earlier in [40]. Here we will provide a X approximation for that problem improving the 12-approximation provided in [40].\nHowever a strong motivation of this section is to continue developing general techniques for Bayesian MAB problems and therefore we take a slightly indirect route. We first consider a different problem: Finite Horizon Bayesian MAB using Arbitrary Order Irrevocable Policies. In this problem, once we have decided upon the set of arms to play then an adversary provides us with a specific order such that if we start playing an arm i then we cannot visit/revisit any arm before arm i in that said order10. We will provide an efficient (again near linear time in the input sparsity) Y -approximation for this problem. Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right. In addition, there are two key benefits of this approach.\n\u2022 First, the analysis technique for arbitrary (or adversarial) order will disentangle the decisions between the constraint associated with the traversal and the constraint associated with the finite horizon. Note that all these traversal problems encode a natural combinatorial optimization problem and are often MAX SNP Hard \u2014 and this disentanglement and isolation of the combinatorial difficulty produces natural policies.\n10This admits K > 1 cases. The adversary does not have the knowledge of the true reward values.\n\u2022 Second, because we use irrevocable policies, the switching cost is only relevant for the algorithm for the first transition from i to j. The proof presented herein will remain exactly the same if the second transition of i to j costs more or less than the first transition, as long as the costs are positive!\nRoadmap: We first discuss the Arbitrary Order Irrevocable Policy problem in Section 4.1. We then show in Section 4.2 how the analysis applies to the Bayesian MAB problem with Metric Switching costs; the analysis will extend beyond the metric assumption as long as a certain traversal type problem (Orienteering Problem) can be approximated to small factors in polynomial time."}, {"heading": "4.1 Arbitrary Order Irrevocable Policies for the Bayesian MAB Problem", "text": "The set up of this problem is similar to the Finite Horizon MAB problem using irrevocable policies discussed in Section 3. The only difference is that we cannot use the ordering of R(Pi)/T (Pi) as described in the scheduling policy in Figure 2 \u2013 instead we will have to use an arbitrary order after the arms and the corresponding policies Pi have been chosen, that is, the Step 2 is not performed. Note that the bound of LP1 remains a valid upper bound of this problem \u2013 but Lemma 3 does not apply explicitly and Theorem 7 is not useful implicitly. In what follows, we prove Theorem 10 which replaces them. The notation used will be the same as in Section 3.\nTheorem 10. The Finite Horizon Bayesian MAB Problem using arbitrary order irrevocable policies has a 4 approximation that can be found in polynomial time and a (4 + )-approximation in O(( \u2211\ni#Edges(Si)) log(nT/ )) time using the scheduling policy in Figure 5.\nProof. Using the exact same initial arguments as in Lemma 3 we arrive at the inequality 1 which states that the expected reward R\u2032 in this case satisfies:\nR\u2032 \u2265 \u2211 i\n( 1\u2212 \u2211 j<i T (P \u2032j) KT ) R(P \u2032i) \u2265 ( 1\u2212 \u2211 j T (P \u2032j) KT )\u2211 i R(P \u2032i)\nNow observe that R(P \u2032i) = \u03b1R(Pi) and T (P \u2032i) = \u03b1T (P \u2032i) and therefore R\u2032 \u2265 \u03b1(1 \u2212 \u03b1) \u2211\niR(Pi). Using an exact solution or Theorem 7, and setting \u03b1 = 12 the statement in this theorem follows.\nHowever note that there is a slack in the above analysis because in Step 1 in the policy in Figure 5, we could have found weakly coupled policies that take a combined horizon of 2KT . Balancing this\nslack will provide us with an alternate optimization which is useful if we cannot compute Qi(\u03bb) or (LP1) in a near optimally fashion. One such example is the Finite Horizon Bayesian MAB Problem with Metric Switching Costs, which we discuss next."}, {"heading": "4.2 Bayesian MAB Problem with Metric Switching Costs", "text": "For simplicity we also assume K = 1 in this section, that is, the system is allowed to play only one arm at a time and observe only the outcome of that played arm. We discuss the K > 1 at the end of the subsection. The system starts at an arm i0. A policy, given the outcomes of the actions so far (which decides the current states of all the arms), makes one of the following decisions (i) play the arm it is currently on; (ii) play a different arm (paying the distance cost to switch to that arm); or (iii) stop. Just as before, a policy obtains reward ru if it plays arm i in state u \u2208 Si. Any policy is also subject to rigid constraints that the total number of plays is at most T and the total distance cost is at most L on all decision paths. To begin with, we delete all arms j such that `i0j > L. No feasible policy can reach such an arm without exceeding the distance cost budget. Let OPT denote both the optimal solution as well as its expected reward."}, {"heading": "4.2.1 A (Strongly Coupled) Relaxation and its Lagrangian", "text": "We describe a sequence of relaxations to the optimal policy, culminating with a weakly coupled relaxation. A priori, it is not clear how to construct such a relaxation, since the switching cost constraint couples the arms together in an intricate fashion. We achieve weak coupling via the Lagrangian of a natural LP relaxation, which we show can be solved as a combinatorial problem called orienteering over single-arm policies.\nDefinition 5. Let C(L, T ) denote the set of policies on all the remaining arms, over a time horizon T , that can perform one of two actions: (1) Play current arm; or (2) Switch to different arm. Such policies have no constraints on the total number of plays, but are required to have distance cost L on all decision paths. Observe that if the constraint corresponding to the distance constraint is removed then P \u2208 C(L, T ) will decompose to {Pi \u2208 Ci(T )}, that is, the single arm policies Pi (which are the projections of P ) have at most a horizon of T (See Definition 1 for the definition of Ci(T )).\nGiven a policy P \u2208 C(L, T ) define the following quantities in expectation over the decision paths: Let R(P ) be the expected reward obtained by the policy and let T (P ) denote the expected number of plays made. Note that any policy P \u2208 C(L, T ) needs to have distance cost at most L on all decision paths. Consider the following optimization problem, which is still strongly coupled since C(L, T ) is the space of policies over all the arms, and not the space of single-arm policies:\n(M1) : MaxP\u2208C(L,T ) {R(P ) | T (P ) \u2264 T }\nProposition 1. OPT is feasible for (M1).\nProof. We have T (OPT ) \u2264 T ; since OPT \u2208 C(L, T ), this shows it is feasible for (M1).\nLet the optimum solution of (M1) be OPT \u2032 and the corresponding policy be P \u2217 such that R(P \u2217) = OPT \u2032 \u2265 OPT . Note that P \u2217 need not be feasible for the original problem, since it enforces the time horizon T only in expectation over the decision paths. We now consider the Lagrangian of the above for \u03bb > 0, and define the problem M2(\u03bb):\nDefinition 6. Let V (\u03bb) = MaxP\u2208C(L,T ) (R(P )\u2212 \u03bbT (P )). Let M2(\u03bb) be MaxP\u2208C(L,T )f\u03bb(P ). For a policy P \u2208 C(T ) let f\u03bb(P ) = \u03bbT +R(P )\u2212 \u03bbT (P ). Then,\nM2(\u03bb) : MaxP\u2208C(L,T ) f\u03bb(P ) = \u03bbT + MaxP\u2208C(L,T ) (R(P )\u2212 \u03bbT (P )) = \u03bbT + V (\u03bb)\nWe first relate OPT to the optimal value \u03bbT + V (\u03bb) of the problem M2(\u03bb).\nLemma 11. For any \u03bb \u2265 0, we have M2(\u03bb) = \u03bbT + V (\u03bb) \u2265 OPT .\nProof. This is simply weak duality: For the optimal policy P \u2217 to (M1), we have T (P \u2217) \u2264 T . Since this policy is feasible for M2(\u03bb) for any \u03bb \u2265 0, the claim follows.\nIn the Lagrangian formulation, if arm i is played in state u \u2208 Si, the expected reward obtained is ru \u2212 \u03bb. We re-iterate that the only constraint on the set of policies C is that the distance cost is at most L on all decision paths."}, {"heading": "4.2.2 Structure of M2(\u03bb)", "text": "The critical insight, which explicitly uses the fact that in the MAB the state of an inactive arm does not change and which allows weak coupling, is the following:\nLemma 12. For any \u03bb \u2265 0, given any P \u2208 C, there exists a P \u2032 \u2208 C that never revisits an arm that it has already played and switched out of, such that f\u03bb(P \u2032) \u2265 f\u03bb(P ).\nProof. We will use the fact that Si is finite in our proof. Suppose P \u2208 C revisits an arm. Consider the last point in time, denote this \u03b1, where the policy is at some arm i, makes a decision to switch to arm j and at some subsequent point in time on some decision path, revisits arm i. Note that after time \u03b1, the subsequent decision policy P\u03b1 satisfies the condition that no arm is revisited. Therefore, all plays of arm j are contiguous in time within the policy P\u03b1. Let P\u0303 denote the decision policy subsequent to exiting arm j; this policy can be different depending on the outcomes of the plays for arm j. However, since any such P\u0303 never revisits arm j, we can simply choose that policy P\u0303 \u2217 with maximum f\u03bb(P\u0303 ) and execute it regardless of the outcomes of the plays of arm j. This yields a new policy P \u2032 so that f\u03bb(P\n\u2032) \u2265 f\u03bb(P ), and furthermore, the distance cost of P \u2032 is at most L in all decision paths, so that P \u2032 is feasible. By the repeated application of this procedure, the decision policy P\u03b1 can be changed to the following form without decreasing f\u03bb(P ): The new policy makes an adaptive set of plays arm j; regardless of the outcomes of these plays, the policy switches to the same next arm k, again makes an adaptive set of plays; transitions to the same next arm l regardless of the outcome of the plays, and so on, without ever revisiting an arm. We will refer to such a policy as a \u201cpath\u201d over arms.\nRecall that the overall decision policy is at arm i just before time \u03b1. Suppose this arm is revisited in the path P\u03b1, so that an adaptive set of plays is made for this arm. We modify the policy to make these plays at time \u03b1, and then switch to arm j regardless of the outcome of these plays. Suppose the original path P\u03b1 visited arm i from arm k and subsequently switched from i to l regardless of the outcomes of the plays of i, the new policy switches from arm k directly to arm l. Since the states of arms that are not played never changes, this movement preserves the states of all the arms, and the moved plays for arm i are stochastically identical to the original plays. Further, the distance cost of the new policy is only smaller, since the cost of switching into and out of arm i is removed. This eliminates \u03b1 as the last time when the policy is at some arm which is subsequently revisited. By repeated application of the above procedure, the lemma follows.\nNote that the above is not true for policies restricted to be feasible for (M1). This is because the step where we use policy P\u0303 regardless of the outcome of the plays for arm j need not preserve the constraint T (P ) \u2264 T , since this depends on the number of plays made for arm j. The Lagrangian M2(\u03bb) makes the overall objective additive in the (new) objective values for each arm, with the only constraint being that the distance cost is preserved. Since this cost is preserved in each decision branch, it is preserved by using the best policy P\u0303 regardless of the outcome of the plays for j."}, {"heading": "4.2.3 Orienteering and the Final Algorithm", "text": "We now show that the optimal solution to M2(\u03bb) is a collection of single-arm policies connected via a combinatorial optimization problem termed orienteering.\nDefinition 7. In the orienteering problem [17, 13, 22], we are given a metric space G(V,E), where each node v \u2208 V has a reward ov. There is a start node s \u2208 V , and a distance bound L. The goal is to find that tour P starting at s, such that \u2211 v\u2208P ov is maximized, subject to the length of the tour being at most L. Observe that for any given any \u03b5 \u2208 (0, 1] we can discretize {ov|ov > 0} such that within a (1 + \u03b5) approximation we can assume that ov are integers which are at most O(n/\u03b5).\nAn immediate consequence of Lemma 12 is the following:\nCorollary 13. Define a graph G(V,E), where node i \u2208 V corresponds to arm i. The distance between nodes i and j is `ij, and the reward of node i is oi = Qi(\u03bb). The optimum solution V (\u03bb) of M2(\u03bb) is the optimal solution to the orienteering problem on G starting at node i0 and respecting rigid distance budget L.\nProof. Consider any n-arm policy P \u2208 C. By Lemma 12, the decision tree of the policy can be morphed into a sequence of \u201csuper-nodes\u201d, one for playing each arm, such that the decision about which arm to play next is independent of the outcomes for the current arm. The policy maximizing f\u03bb(P ) will therefore choose the best policy in Ci for each single arm as the \u201csuper-node\u201d(obtaining objective value precisely Qi(\u03bb)), and visit these subject to the constraint that the distance cost is at most L. This is precisely the orienteering problem on the graph defined above.\nTheorem 14. For any \u2208 (0, 1] the orienteering problem has a (2 + )-approximation that can be found in polynomial time [22]. The authors of [17] showed that any c-approximation for K = 1 case (where we choose a single tour) extends to an (c+ 1)-approximation for the K > 1 case where we choose K tours11.\nWe are now ready to present the main theorem for this application,\nTheorem 15. For the finite-horizon multi-armed bandit problem with metric switching costs, for K = 1 play at a time step there exists a polynomial time computable ordering of the arms and a policy for each arm, such that a solution which plays the arms using those fixed policies, in that fixed order without revisiting any arm, has reward at least 1/(4 + \u03b5) times that of the best adaptive policy, for any \u03b5 \u2208 (0, 1]. For K > 1 the the reward is at least 1/(4.5 + \u03b5).\nProof. We will use Corollary 9 and use the scheduling policy in Figure 5 setting \u03b1 = 1. Note that using Lemma 13, the constraints C are null! The traversal constraint does not imply any constraints on the internals of a single arm policy. However OPT (C) still has a rigid constraint of a traversal cost of at most L.\n11They also provided a 4 approximation for K = 1, along with bounds for a variety of other traversal problems.\nBased on the c(K) approximation algorithm and Corollary 9 we have a collection of policies {Pi} such that (i) \u2211 i T (Pi) = KT/c(K) and (ii) c(K) \u2211 iR(Pi) \u2265 OPT (K) where c(K) is the approximation factor determined by Theorem 14 and OPT (K) is the corresponding optimum solution (again under the rigid traversal cost L). Now observe that the reward R\u2032\u2032 we obtain satisfies:\nR\u2032\u2032 \u2265 ( 1\u2212 \u2211 j T (P \u2032j) KT )\u2211 i R(P \u2032i) \u2265 ( 1\u2212 1 c(K) ) 1 c(K) OPT (K)\nThe theorem follows using c(1) = 2 + and c(K) = 3 + where = \u03b5/12. Note that if c(K) \u2264 2 then we should have used \u03b1 = min{1, c(K)/2}.\nRemark: Note that the above technique is powerful enough to approximate any switching cost problem as long as the basic combinatorial problem is approximable. We note that there exists approximations for asymmetric distances (directed graphs with triangle inequality) and other traversal problems in [17]. Modifications to Theorem 15 will provide a solution for all such problems."}, {"heading": "5 Multi-armed Bandits with Delayed Feedback", "text": "In this variant of the MAB problem, if an arm i is played, the feedback about the reward outcome is available after \u03b4i time steps. Once again the budgets of the arms is encoded in the respective state spaces. In this section we assume that there are no switching or traversal dependent costs. Given the algorithms and analysis we propose for handling delayed feedback, such additional constraints can be handled using the ideas of the preceding sections.\nFrom an analysis standpoint, the idea of truncation is not (immediately) useful. This is because a policy can (and should) be \u201cback-loaded\u201d in the sense that if we consider a single arm, most of the plays are made towards the end of the horizon (possibly because the policy is confident that the reward of this arm is large and well separated from the alternatives). Therefore truncating the horizon (by any factor) may cause these good plays to be eliminated and as a consequence we would not have any guarantee on the expected reward. In what follows we will introduce two techniques that avoid the problem mentioned:\n(a) A Delay Free Simulation; where at some point of time in the policy, the delays become irrelevant.\n(b) A Block Compaction Strategy; where the plays of a policy are moved earlier in time.\nOf course, both of these ideas lose optimality, but interestingly that loss of optimality can be bounded. These two techniques are similar, they both increase the number of plays, and yet are useful in very different regimes. The delay free simulation idea, by itself gives us a (2(1+ )+32(y+ y2))-approximation for any \u2208 (0, 1] where y = maxi \u03b4i/ \u221a T . This implies that when the delay is small, the result is very close to the finite horizon MAB result discussed in Section 3 except that we are not using irrevocable policies. For maxi \u03b4i \u2264 \u221a T/50 this approximation ratio is less than 3. However this idea ceases to be less useful by itself as maxi \u03b4i increases. The block compaction strategy (in conjunction with the delay free simulation) allows an O(1) approximation even when \u03b4i are large as long as 21(2\u03b4i + 1)(1 + log \u03b4i) \u2264 T for all i.\nNote that it is not immediate how to construct good policies for maxi \u03b4i is a non-trivial constant (say 100) or even the larger regime without losing factors proportional to maxi \u03b4i. Thus it is doubly interesting that these two regimes naturally emerge from the analysis of approximation factors \u2013 these two regimes expose two different scheduling ideas. We come a full circle from the discussion in Ehrenfeld [27], where the explicit connections between stopping rules for delayed feedback in the two bandit setting and scheduling policies were considered.\nThe Overall Plan and Roadmap: We will bound the optimum using single arm policies. In both regimes of maxi \u03b4i, instead of solving the relaxations, we first show that there exists a subclass of policies which (i) have much more structure, (ii) preserve the approximation bound with respect to the globally optimum policies and (iii) are easy to find. We first present the basic notation and definitions in Section 5.1. We then discuss the regime where maxi \u03b4i \u2264 \u221a T/10 in Section 5.2 and prove the relevant approximation bound in Lemma 20, under the assumption that we can find the suitable policies. In the interest of the presentation we do not immediately show how to compute the policies but we discuss the regime where \u03b4i are larger (but 21(2\u03b4i + 1)(1 + log \u03b4i) \u2264 T for all i) next in Section 5.3 and prove the approximation bound in Lemma 22, under the same assumption that we can find the suitable policies. Finally we show how to compute the policies in polynomial time, using a linear program in Section 5.4. That linear program can also be solved to a (1 + )-approximation efficiently using ideas in the previous sections. As a consequence,\nTheorem 16. We can approximate the finite horizon Bayesian MAB problem with delayed feedback to a 2(1 + ) + 32(y + y2) approximation where y = maxi \u03b4i/ \u221a T for any > 0 in polynomial time.\nIf the delays are small constants or o( \u221a T ) then this result implies a (2+o(1))-approximation which seamlessly extends Theorem 4. For maxi \u03b4i \u2264 T/(48 log T ) we provide a O(1) approximation in polynomial time."}, {"heading": "5.1 Weakly Coupled Relaxation and Block Structured Policies", "text": "Single arm policies: In the case of delayed feedback, describing a single-arm policy is more complicated. This policy is now a (randomized) mapping from the current state of the arm to one of the following actions: (i) make a play; (ii) wait some number of steps (less or equal to T ), so that when the result of a previous play is known, the policy changes state; (iii) wait a few steps and make a play (without extra information); or (iv) quit.\nDefinition 8. Let Ci(T, \u03b4i) be the set of all single-arm policies over a horizon of T steps.\nAs in Section 3, we will use the LP system (LPDelay) to bound of the reward of the best collection of single-arm policies - the goal will be to find one policy per arm so that the total expected number of plays is at most T and the expected reward is maximized. This is expressed by the following relaxation:\nLPDelay = Max{Pi\u2208Ci(T,\u03b4i)} {\u2211 i R(Pi) | \u2211 i T (Pi) \u2264 T } The state of the system is now captured by not only the current posterior u \u2208 Si, but also the plays with outstanding feedback and the remaining time horizon. Note that the state encodes plays with outstanding feedback, and this has size 2\u03b4i , which is exponential in the input. Therefore, it is not even clear how to solve (LPDelay) in polynomial time, and we use (LPDelay) as an initial formulation for the purposes of an upper bound.\nDefinition 9. A single-arm policy is said to be Block Structured if the policy executes in phases of size (2\u03b4i + 1). At the start of each phase (or block), the policy makes at most \u03b4i + 1 consecutive plays. The policy then waits for the rest of the block in order to obtain feedback on these plays, and then moves to the next block. A block is defined to be full if exactly \u03b4i + 1 plays are made in it. Let the class of Block Structured policies for arm i over a horizon T be Cbi (T, \u03b4i).\nWe first show that all single-arm policies can be replaced with block structured policies while violating the time horizon by a constant factor. The idea behind this proof is simple \u2013 we simply insert delays of length \u03b4i after every chunk of plays of length \u03b4i.\nLemma 17. Any policy P \u2208 Ci(T, \u03b4i) can be converted it to a Block Structured policy P \u2032 \u2208 Cbi (2T, \u03b4i) such that R(P) \u2264 R(P \u2032) and T (P \u2032) \u2264 T (P) (note that the horizon increases).\nProof. We can assume that the policy makes a play at the very first time step, because we can eliminate any wait without any change of behavior of the policy.\nConsider the actions of P for the first \u03b4i + 1 steps, the result of any play in these steps is not known before all these plays are made. An equivalent policy P \u2032 simulates P for the first \u03b4i+1 steps, and then waits for \u03b4i steps, for a total of 2\u03b4i + 1 steps. This ensures that P \u2032 knows the outcome of the plays before the next block begins.\nNow consider the steps from \u03b4i + 2 to 2\u03b4i + 3 of P. As P is executed, it makes some plays possibly in an adaptive fashion based on the outcome of the plays in the previous \u03b4i + 1 steps, but not on the current \u03b4i + 1 steps. P \u2032 however knows the outcome of the previous plays, and can simulate P for these \u03b4i + 1 steps and then wait for \u03b4i steps again. It is immediate to observe that P \u2032 can simulate P at the cost of increasing the horizon by a factor of 2\u03b4i+1\u03b4i+1 < 2. Observe that in each block of 2\u03b4i + 1, P \u2032 can make all the plays consecutively at the start of the block without any change in behavior. The budgets are also respected in this process. This proves the lemma.\nWe now show how to find a set of block structured policies {Pi} such that each has horizon at most 2T and together satisfy the following properties \u2211 iR(Pi) \u2265 LPDelay and \u2211 i T (Pi) \u2264 KT . Observe that by Lemma 17, such policies exist.\nLemma 18. We find a collection of policies {Pi|Pi \u2208 Cbi (2T, \u03b4i)} such that \u2211\niR(Pi) \u2265 LPDelay/(1+ ) and \u2211 i T (Pi) \u2264 KT for any \u2208 (0, 1] in time polynomial in \u2211 i Si, T (using a linear program\nor a fast approximate solution for such).\nWe relegate the proof of Lemma 18 to Section 5.4 and continue with the algorithmic development."}, {"heading": "5.2 Delay Free Simulations and Small Delays", "text": "In this section we discuss the case when the delays are small in comparison to the horizon, i.e., maxi \u03b4i \u2264 \u221a T/50. We next introduce the delay free simulation idea. Let r = \u221a T/(2 maxi \u03b4i) and \u03b3 = 4r(maxi \u03b4i) 2/T . The choice of these parameters will become clear shortly.\nDefinition 10. Define a policy Pi to be in a delay-free mode at time t if at time t the policy Pi makes a play, but uses the outcome of a play made at time t\u2212 \u03b4i (or earlier). The outcome of the current play is available at time t+ \u03b4i and can be used subsequently.\nObserve that the truncation idea of Theorem 2 applies to a policy once it is in a delay free mode.\nDefinition 11. Define a policy Pi to be a (\u03b3T, T )-horizon policy if the policy is block structured on all decision paths till a horizon of \u03b3T and subsequently makes at most T plays in a delay free mode. A pictorial depiction of a (\u03b3T, T )-horizon policy is shown if Figure 6.\nLemma 19. Given any block structured policy P \u2208 Cbi (2T, \u03b4i) we can construct a (\u03b3T, T )-horizon policy P \u2032 such that R(P) \u2264 R(P \u2032) and T (P \u2032) \u2264 (1 + 1r )T (P).\nProof. Consider the first time the policy P makes r\u03b4i plays on some decision path. Define a new policy P \u2032 as follows: It is identical to P till the end of the block that contains the r\u03b4i-th play and then it makes \u03b4i additional plays. Let these plays be z(1), z(2), . . . , z(\u03b4i). Note that P was not making any plays and waiting for the end of the block. After this point, consider the t-th play made by P where t \u2264 \u03b4i \u2013 the policy P \u2032 makes the play but simply uses the outcome of z(t) which is\nknown by now. The outcome of the t-th play is not known (or used) and would only be used for the t+ \u03b4i-th play. Note that since the outcome of two plays are from the same underlying distribution we can couple the outcomes to be identical as well. If P decides to stop execution, then P \u2032 stops execution as well. Clearly, P \u2032 makes \u03b4i additional plays than P on any decision path. Since P made at least r\u03b4i plays by then, this shows that T (P \u2032) \u2264 (1 + 1r )T (P). Since the execution of P \u2032 is coupled play-by-play with the execution of P, it is clear that R(P) \u2264 R(P \u2032). Observe that once P \u2032 has switched to this \u201cdelay free mode\u201d then P \u2032 can now simply ignore the blocks, but would make at most T plays. This switch to a delay free mode must occur within the first r\u03b4i blocks since each block must have one play \u2013 and given that the blocks account for 2\u03b4i + 1 units of time, the switch occurs within an horizon of r\u03b4i(2\u03b4i + 1) \u2264 4r\u03b42i \u2264 \u03b3T .\nUsing the (\u03b3T, T )-horizon policy: Using Lemma 18 we can find a collection of block structured policies {Pi} such that \u2211 iR(Pi) \u2265 LPDelay/(1 + ) and \u2211 i T (Pi) \u2264 KT for any > 0.\nUsing Lemma 19 we now have a collection of (\u03b3T, T )-horizon single arm policies {P \u2032i} such that\u2211 iR(P \u2032i) \u2265 LPDelay/(1+ ) and \u2211 i T (P \u2032i) \u2264 (1+ 1 r )KT . But before such, consider the following scheduling policy given in Figure 7.\nLemma 20. The expected reward R of the scheduling policy in Figure 7 is at least LPDelay/(2(1+ ) + 32(y + y2)) where y = maxi \u03b4i/ \u221a T . For maxi \u03b4i \u2264 \u221a T/50 the approximation ratio does not exceed 3 for suitably small \u2208 (0, 0.1].\nProof. Again let Tj be the actual number of plays of a policy for arm j. Observe that the policy for arm i does not affect the policy for arm j if j < i in the order defined in Step 2 in the policy shown in Figure 7. Now consider the policy P \u2032\u2032i . If a policy does not start at or before time (1 \u2212 \u03b3)T we disregard its entire reward (see the pictorial representation of the accounting in Figure 6) because the policy will not finish executing the entire block structured part. Now consider the policy P \u2032\u2032i . Observe that the delay free part of policy P \u2032\u2032 is truncated by a factor at most 1T ( T \u2212 1KT \u2211 j<i Tj \u2212 \u03b3T ) . Note that the policy P \u2032\u2032i may start before all the plays in \u2211 j<i Tj\nare made, because some of those policies were waiting for feedback. Therefore the contribution from P \u2032\u2032i conditioned on T1, T2, . . . , Ti\u22121 is at least in expectation\n1\nT T \u2212 1 K \u2211 j<i Tj \u2212 \u03b3T R(P \u2032\u2032i ) Again note that the actual reward will be higher because the contribution cannot be negative \u2013 whereas we are ignoring the case when T \u2264 1K \u2211 j<i Tj + \u03b3T . Moreover the contribution from the first \u03b3T steps is always present. However the above accounting suffices for our bound. The expected reward (summed over all i and the respective conditionings removed exactly as in the proof of Lemma 3) we get:\nR \u2265 1 T \u2211 i T \u2212 1 K \u2211 j<i T (P \u2032\u2032j )\u2212 \u03b3T R(P \u2032\u2032i ) \u2265 (1\u2212 \u03b3)\u2211 i ( 1\u2212 \u2211 j<i T (P \u2032\u2032j ) KT (1\u2212 \u03b3) ) R(P \u2032\u2032i )\nBut since \u2211\ni T (P \u2032\u2032i ) \u2264 KT (1\u2212 \u03b3), by exact same argument as in Lemma 3 we have:\nR \u2265 (1\u2212 \u03b3)1 2 \u2211 i R(P \u2032\u2032i ) \u2265 (1\u2212 \u03b3) 1 2 \u03b1 \u2211 i R(P \u2032i) \u2265 (1\u2212 \u03b3)\u03b1 2(1 + ) LPDelay (6)\nUsing \u03b1 = 1\u2212\u03b31+1/r , \u03b3 \u2264 1 4 and \u2264 1the approximation ratio of LPDelay/R is at most\n2(1 + )(1 + 1r )\n(1\u2212 \u03b3)2 \u2264 2(1 + )(1 + 1/r)(1 + 2\u03b3) \u2264 2(1 + ) + 8 r + 8\u03b3 + 8\u03b3 r\nSince \u03b3 = 4r(maxi \u03b4i) 2/T the above ratio is 2(1 + ) + 32(maxi \u03b4i)\n2 T + 8 ( 1 r + 4r(maxi \u03b4i) 2 T ) which is\nminimized at r = \u221a T/(2 maxi \u03b4i) giving an approximation ratio of 2(1 + ) + 32(y + y\n2) where y = maxi \u03b4i/ \u221a T which proves the Lemma.\nLemma 20 also shows that for large maxi \u03b4i the ratio grows as O ( (maxi \u03b4i) 2\nT\n) , which indicates why\nother ideas are needed. We now discuss the idea of block compaction."}, {"heading": "5.3 Block Compaction and Larger Delays", "text": "In this section we assume that T \u2265 21(2\u03b4i + 1)(1 + log \u03b4i) for all i. Let \u03b3 = 13 , \u03c1 = 13. The next lemma states that we can also shrink the horizon by increasing the number of plays \u2014 which is intuitively equivalent to moving the plays forward in the policy.\nLemma 21. For any \u03c1 > 1 given a policy Pi \u2208 Cbi (2T, \u03b4i), then there exists a (\u03b3T, T )-horizon policy (as defined in Lemma 19) P \u2032i such that R(Pi) \u2264 R(P \u2032i) and T (P \u2032i) \u2264 \u03c1T (Pi).\nProof. Consider the execution of Pi. Without loss of generality we can assume that all blocks in this policy have at most \u03b4i/\u03c1 plays, otherwise the policy can transition to the delay free mode increasing the plays by a factor of \u03c1. The total number of plays in the delay free mode can be at most T . In the remainder of the proof we will consider the prefix of the policy where no block has more than \u03b4i/\u03c1 plays. We consider the execution of the original policy, and show a coupled execution in the new policy P \u2032i so that if on a particular decision path, Pi used (#b) blocks, then the number of blocks on the same decision path in P \u2032 is b(#b)/7c+ 1 + log \u03b4i. Observe that since (#b) is at most 2T/(2\u03b4i + 1); the time horizon required for b(#b)/7c+ 1 + log \u03b4i blocks is at most\n( (#b)\n7 + 1 + log \u03b4i\n) (2\u03b4i + 1) < 2T\n7 + 3\u03b4i(1 + log \u03b4i) < \u03b3T \u2212\n( T\n21 \u2212 3\u03b4i(1 + log \u03b4i)\n) \u2264 \u03b3T\nfor the setting of \u03b3 = 1/3 and the assumption on T \u2265 21(2\u03b4i + 1)(1 + log \u03b4i) for all i. Therefore proving that the number of blocks is b(#b)/7c+ 1 + log \u03b4i is sufficient to prove the lemma.\nWe group blocks of the policy Pi into size classes; size class s has blocks whose number of plays lies in [2s, 2s+1). We couple the executions as follows: Consider the execution of Pi and define the new policy P\u0302i as follows.\nSuppose there were x plays in Pi in the first block, and the size class of this block is s. Then P\u0302i makes \u03c1x = 13x plays in this block. The policy P\u0302i uses the the outcomes of the extra 12x plays made in the first block to simulate the behavior of P \u2032i as below:\n(a) At any point of time P\u0302i has an excess z number of plays which it has made, and knows the outcome but has not utilized the outcome yet. It also has a multi-set S of types which it can simulate. Initially z = 12x and S = {s, s, s, s, s, s}. Observe that we will maintain the invariant that z is larger than the number of plays required to play the all types of blocks in S.\n(b) Suppose the next block according to the decisions made by Pi (and simulated by P\u0302i) makes x\u2032 plays corresponding to type s\u2032.\n(i) If s\u2032 \u2208 S then we use the first x\u2032 outcomes we already know and set z \u2190 z \u2212 x\u2032 and S \u2190 S\u2212{s\u2032} (removing just one copy of s\u2032 from the multiset). Note that we do not make any plays and do not suffer the 2\u03b4i + 1 time steps for this block.\n(ii) If s\u2032 6\u2208 S then we make \u03c1x\u2032 = 13x\u2032 plays. We now again use the first x\u2032 of the outcomes \u2013 if z \u2264 x\u2032 then we use all the old z outcomes and use x\u2032 \u2212 z new outcomes. Otherwise if x\u2032 < z then we use the x\u2032 old outcomes. In either case the total number of excess outcomes we have stored is z+ 12x\u2032. We set z \u2190 z+ 12x\u2032 and S \u2190 S\u222a{s\u2032, s\u2032, s\u2032, s\u2032, s\u2032, s\u2032}.\nObserve that we are maintaining the invariant that the outcomes of the plays are used only after they are known and in first-in first-out fashion. Moreover making a play corresponding to x plays of type s will definitely generate 12x extra plays which can be used to simulate the next 6 blocks of type s in the decision path irrespective of when those blocks will occur. Observe that since the underlying reward distribution is fixed, these plays will be stochastically identical in both policies Pi, P\u0302i. Moreover it it easy to see that for any decision path in Pi with (#b) blocks, the corresponding path in P\u0302i has b(#b)/7c+ 1 + log \u03b4i blocks \u2013 where the division by 7 arises from the current block (of x plays) and 6 other blocks of type s (at most 2x plays each). The additive term of 1 + log \u03b4i corresponds to the total number of size types and arises from the fact that we may not have any blocks of a particular type s on the decision path. Observe that this is a worst case bound and holds for any decision paths.\nTherefore we can immediately say that in P\u0302i either we will quit the policy within a horizon of \u03b3T or would have started to play in a delay free manner.\nThe proof requires one final observation \u2013 P\u0302i already uses the plays in a first-in first-out fashion and respects the block boundaries. It differs from a (\u03b3T, T )-horizon policy in that it does not use all the information it already has. However then there exists a block structured policy that uses the information and maximizes the reward while maintaining the same number of plays! This block structured policy is the policy P \u2032i.\nWe now observe that there exists (\u03b3T, T )-horizon policies P \u2032i such that \u2211\niR(P \u2032i) \u2265 LPDelay and\u2211 i T (P \u2032i) \u2264 \u03c1KT . As we will see in section 5.4, for any \u2208 (0, 1] we can find (\u03b3T, T )-horizon\npolicies {P \u2032i} that satisfy \u2211 iR(P \u2032i) \u2265 LPDelay/(1 + ) and \u2211\ni T (P \u2032i) \u2264 \u03c1KT . We can now apply the scheduling policy in Figure 7 (with changes to Steps 1 and 3) setting \u03b1 = (1\u2212 \u03b3)/\u03c1. The next lemma is immediate:\nLemma 22. Suppose T \u2265 21(2\u03b4i + 1)(1 + log \u03b4i) for all \u03b4i. Consider the policy which is identical to the policy in Figure 7 except that in Step 1 we find (\u03b3T, T )-horizon policies {P \u2032i} that satisfy\u2211\niR(P \u2032i) \u2265 LPDelay/(1 + ) and \u2211\ni T (P \u2032i) \u2264 \u03c1KT ; and in Step 3 we set \u03b1 = (1 \u2212 \u03b3)/\u03c1. This new scheduling policy gives an expected reward R\u2032 which is at least LPDelay/(119(1 + )).\nProof. This proof is identical to the proof of Lemma 20 up to Equation (6) in concluding\nR\u2032 \u2265 (1\u2212 \u03b3)\u03b1 2(1 + ) LPDelay\nThe rest of the lemma follows from \u03b3 = 1/3 and \u03b1 = (1\u2212 \u03b3)/\u03c1 = 2/39.\nThe interesting aspect of Lemma 22 is that the approximation factor remains O(1) even when 21(2\u03b4i + 1)(1 + log \u03b4i) \u2248 T which is rather large delays."}, {"heading": "5.4 Constructing Block Structured Policies: Proof of Lemma 18", "text": "We now show how to find a set of block structured policies {Pi} such that each has horizon at most 2T and together satisfy the following properties \u2211 iR(Pi) \u2265 LPDelay and \u2211 i T (Pi) \u2264 KT . Observe that by Lemma 17, such policies exist. Such a policy for arm i operates in blocks of length 2\u03b4i + 1, and over a horizon of 2T steps. In each block, it makes at most \u03b4i/\u03c1 plays, and then waits\nfor the feedback of these plays. For any state \u03c3 of arm i, the policy decides on the number ` of consecutive plays made in this state. Define the following quantities. These quantities are an easy computation from the description of Si, and the details are omitted.\n1. Let ri(\u03c3, `) denote the expected reward obtained when ` consecutive plays are made at state \u03c3, and feedback is obtained at the very end.\n2. Let pi(\u03c3, \u03c3 \u2032, `) denote the probability that if the state at the beginning of a block is \u03c3, and `\nplays are made in the block, the state at the beginning of the next block is \u03c3\u2032.\nWe will formulate an LP to find a collection of randomized block-structured policies Pi. For each i, define the following variables over the decision tree of the policy Pi and the system (LPD2).\n\u2022 xi\u03c3: the probability that the state for arm i at the start of a block is \u03c3. \u2022 yi\u03c3`: probability that Pi makes 0 \u2264 ` \u2264 \u03b4i consecutive plays starting at state \u03c3.\nLPD2 = Max \u2211 i,\u03c3,`\nri(\u03c3, `)yi\u03c3`\u2211 \u03c3 xi\u03c3 \u2264 1 \u2200i\u2211 ` yi\u03c3` \u2264 xi\u03c3 \u2200i, \u03c3\u2211\n\u03c3,` yi\u03c3`pi(\u03c3, \u03c3 \u2032, `) = xi\u03c3\u2032 \u2200i, \u03c3\u2032\u2211\ni,\u03c3,` `yi\u03c3` \u2264 KT xi\u03c3, yi\u03c3` \u2265 0 \u2200i, \u03c3, `\nThe number of variables is at most \u2211 i |Si|\u03b4i which is polynomial in T and \u2211\ni |Si|. We have the following LP relaxation, which simply encodes finding one randomized well-structured policy per arm so that the expected number of plays made is at most T . The system (LPD2) returns a collection of single arm policies; the policies are interpreted in Figure 8. The interpretation is almost the same as in Figure 1 except that there ` = 1.\nObserve that as a consequence of Lemma 17, LPD2 \u2265 LPDelay. Lemma 18 follows.\n6 MaxMAB: Bandits with Non-linear Objectives\nIn MaxMAB at each step, the decision policy can play at most K arms but the reward obtained is the maximum of the values that are observed. In other words, the policy plays at most K arms each step, but is only allowed to choose the arm with the maximum observed value and obtain its reward. Note that the states of all of the arms which are played evolve to their respective posteriors, since all these arms were observed. The reward is clearly nonlinear (nonadditive) in the plays \u2013 and several issues arise immediately.\n(a) Handling Budgets: As discussed in Section 2.2 we have a modeling choice in terms of how budgets are handled. It is easy to conceive of two separate application scenarios. In the first application scenario, every arm that is played expends a budget that it its observed value \u2013 this is the All-Plays-Cost model as discussed in Section 2.2. This is natural in scenarios where the observed value correlates with the effort spent by the alternative. This case can easily be handled by truncating the space Si so that states u \u2208 Si corresponding to total observations larger than Bi are discarded \u2013 and this is the model we have discussed in all the previous sections since we considered linear (additive) rewards (or K = 1 for which avoids this issue).\nIn the second application scenario only the arm with the maximum observed value is charged its budget \u2013 which is the Only-Max-Costs model as discussed in Section 2.2. This accounting is natural in many strategic scenarios. Note that in this budget model, if we have a good arm then we can easily resolve the priors of K \u2212 1 other arms at a time while still getting the reward of the good arm. When the good arm is exhausted, then we have much more information about the other arms. Therefore the policies corresponding to these arms will only exhaust the budget deeper into the horizon. This runs counter to the intuition of the truncation! Yet, we can show that up to a O(1) factor the difference in accounting does not matter.\n(b) Simultaneous or One-at-a-time feedback: Given that we are playing at most K arms then again it is not difficult to conceive of two separate application scenarios. In the first application scenario, at each time slot we decide on the set of arms to play and then make the plays simultaneously \u2013 we then receive the respective feedbacks and finally choose the reward for the slot. This is the Simultaneous-Feedback model. In the second scenario we make the plays one-ata-time and get immediate feedback. We refer to this model as One-at-a-Time feedback model. Therefore in this scenario we can choose a reward even before we have played all the potential set of arms and move to the next time slot (only the arms which have been played gets updated to their corresponding posteriors). Clearly the second model allows for more powerful and more adaptive policies in the All-Plays-Cost budget model.\nIt may appear that in the Only-Max-Costs model, this feedback issue is not relevant \u2013 because once the set of arms are fixed in a single time slot there is no need to stop (choose reward) and move to the next time slot before observing all the values. However in the one-at-a-time feedback model the set of arms that can potentially be played in a time slot changes as the outcomes of the plays in that time slot are obtained. Therefore the one-at-a-time feedback model is more powerful even in the Only-Max-Costs budget model. The feedback and budget issues are two different and orthogonal modeling choices.\nRoadmap: We prove the following results.\n(i) First we consider the observed value budget model. We provide a factor (4+ )-approximation for the one-at-a-time feedback model in Section 6.1. We then show that the algorithm can be modified to work in the Simultaneous-Feedback model while providing an O(1)-factor reward of the One-at-a-Time feedback model in Section 6.2.\n(ii) We consider the Only-Max-Costs budget model in Section 6.4 and show that there exists a policy uses the All-Plays-Cost accounting for budgets over a subset of the states in Si and is yet within O(1)-factor of the best One-at-a-Time feedback policy in the Only-Max-Costs model over Si. The results extend to the Simultaneous-Feedback model as in Section 6.2 with an appropriate loss of approximation.\nIn terms of techniques, we introduce the basic ideas and single arm policies in Section 6.1. We then introduce a \u201cthrottling\u201d notion in the scheduling of the different policies in Section 6.2. We also show how this notion extends to handle concave functions other than the maximum, for example knapsack type constraints. Finally in Section 6.4 we show that there exists a subset of states S \u2032i \u2286 Si for each arm such that if we restrict ourselves to S \u2032i then the Simultaneous-Feedback and All-Plays-Cost accounting suffices for a O(1)-approximation to the optimum One-at-a-Time feedback policy with Only-Max-Costs accounting.\n6.1 One-at-a-Time Feedback and All-Plays-Cost Models\nIn this section we assume that the budget model is the All-Plays-Cost model, that it, the total value of all observations that can be derived from an arm is bounded. This can be easily incorporated in the statespace Si as in the previous sections. We now consider the One-at-a-Time feedback model.\nSingle Arm Policies: In this case, the single-arm policy for arm i takes one of several possible actions (possibly in a randomized fashion) for each possible state of arm i at any time step: (i) Stop execution; (ii) Play the arm but do not choose it; (iii) Play the arm and choose it if the observed value from a play is q, and obtain reward q; and (iv) Do not play the arm, i.e., wait for the next time step.\nDefinition 12. For any single-arm policy Pi, let N(q,Pi) denote the expected number of times the policy chooses arm i (hence obtaining its reward) and the reward is q. Let R\u0302(Pi) = \u2211 q qN(q,Pi); the total expected reward of the policy. Recall T (Pi) is the expected number of plays of the policy.\nThe goal of the following relaxation will be to find a (randomized) single-arm policy Pi for each arm so that two constraints are respected: The expected number of times any arm is chosen is at most T (since we allow only one arm to be chosen per time step); and the expected number of plays made is at most KT (since there are at most K plays per step). Consider the following LP:\nLPMaxMab = Max{Pi} {\u2211 i \u2211 q qN(q,Pi) | \u2211 i \u2211 q N(q,Pi) \u2264 T and \u2211 i T (Pi) \u2264 KT }\nLemma 23. LPMaxMab is an upper bound on OPT , the value of the optimal policy for the one-at-a-time feedback model.\nProof. Consider the (randomized) single-arm policies obtained by viewing the execution of the optimal policy restricted to each arm i. It is easy to check via linearity of expectation that these policies are feasible for the constraint in LPMaxMab, since the expected number of choices made is at most T , and the expected number of plays made is at most KT . Further, the objective value of these policies is precisely OPT . We first consider the relaxation to a single constraint \u2211\ni (\u2211 qN(q,Pi) + T (Pi)/K ) \u2264 2T and then\ntaking the Lagrangian. The result is:\nLagMaxMab(\u03bb) = MaxPolicies {Pi}\n{ 2T\u03bb+\n\u2211 i (\u2211 q (q \u2212 \u03bb)N(q,Pi)\u2212 \u03bb K T (Pi) )}\n= 2T\u03bb+ \u2211 i MaxPolicies {Pi} (\u2211 q (q \u2212 \u03bb)N(q,Pi)\u2212 \u03bb K T (Pi) ) = 2T\u03bb+ Q\u0302i\nLemma 24. There exists a collection of optimum single arm policies {Pi} for LagMaxMab(\u03bb) which (a) do not wait (use the operation (iv) in the definition of single arm policies); (b) always choose the arm when the observed value is q > \u03bb and obtain q\u2212\u03bb reward; and (c) can be computed by dynamic programming in time O( \u2211 i#Edges(Si)).\nProof. LagMaxMab(\u03bb) has no constraints connecting the arms and has a separable objective, thus the presence or absence of other arms is immaterial to arm a i. Therefore condition (a) follows \u2013 there is no advantage to waiting.\nGiven a policy Pi, if we alter the policy to always choose q if q > \u03bb then the objective can only increase, and therefore (b) follows.\nPart (c) follows from the same style of bottom up dynamic programming as in the proof of Lemma 6. Recall that Xiu corresponds to the posterior distribution of reward of arm i if it is observed in state u \u2208 Si. Then if the gain of a node u is defined by\ngain(u) = max 0, \u2211 q>\u03bb Pr[Xiu = q](q \u2212 \u03bb) \u2212 \u03bb K + \u2211 v puvgain(v)  where the \u2211 v puvgain(v) term is 0 for all leaf nodes (since they have no children). Note Q\u0302i =\ngain(\u03c1i) where \u03c1i is the root of Si.\nThe next theorem follows exactly using the same arguments as in the proof of Theorem 7. Theorem 25. In time O(( \u2211\ni#Edges(Si)) log(nT/ )), we can compute randomized policies {Pi} such that R\u0302(Pi) \u2265 LPMaxMab/(1 + ) and \u2211 i (\u2211 qN(q,Pi) + T (Pi)/K ) \u2264 2T .\nThe next lemma is the key insight of the algorithm. Lemma 26. If the reward of a policy Pi is defined as R\u0302(Pi) = \u2211 q>\u03bb qN(q,Pi) or R\u0303(Pi) = \u2211\nq>\u03bb(q\u2212 \u03bb)N(q,Pi) for any \u03bb, then the Truncation theorem (Theorem 2) applies.\nProof. The proof of Theorem 2 used a path by path argument. That argument is valid in this case as well. Recall that Xiu corresponds to the posterior distribution of reward of arm i if it is observed in state u \u2208 Si. By Bayes\u2019 rule, it is easy to check that Pr[Xiu = q] itself is a Martingale over the state space Si.\nDefinition 13. Given a random variable X define Tail(X,\u03bb) = \u2211\nq\u2265\u03bb Pr[X = q] \u00b7 q. Define Excess(X,\u03bb) = \u2211 q>\u03bb Pr[X = q] \u00b7 (q \u2212 \u03bb).\nIt is easy to check that both Tail(Xiu, \u03bb),Excess(Xiu, \u03bb) are martingales over Si. For the case R\u0302(Pi) consider the execution of Pi conditioned on some choice of the underlying distribution Di drawn from Di. Suppose the policy plays for t steps on some decision path. At each step, the reward obtained is drawn i.i.d. from Di, and so the expected reward is Tail(Di, \u03bb) (which is drawn from a non-negative distribution), and this quantity is independent of rewards obtained in previous steps. Therefore, the expected reward is t \u00b7 Tail(Di, \u03bb). If the policy is restricted to execute for \u03b2T steps, this reduces the length of this decision path by at most factor 1/\u03b2, so that the expected reward obtained is at least \u03b2t \u00b7Tail(Di, \u03bb). Taking expectation over all decision paths and the choice of Di proves the claim.\nThe case of R\u0303(Pi) follows from the exact same argument applied to Excess(Di, \u03bb).\nRemark: Observe that different policies in {Pi} can have different \u03bb since Pi is a randomized policy based on \u03bb+ or \u03bb\u2212. However the important aspect of Lemma 26 is that it holds for any \u03bb.\nThe Final Scheduling: The algorithm is presented in Figure 9.\nTheorem 27. The expected reward of the final policy is at least LPMaxMab/(4(1 + )).\nProof. The proof is near identical to the proof of Theorem 4. Observe that\u2211 i (\u2211 q N(q,P \u2032i) + T (P \u2032i)/K ) = T\nand the horizon of the policy P \u2032i can be truncated to T \u2212Yi where Yi is the number of slots in which P \u2032i is not allowed to play. Again if Tj is the number of steps of policy P \u2032j and nj is the number of steps where policy P \u2032j chose a reward then\nYi \u2264 \u2211 j<i ( nj + Tj K ) =\u21d2 E[Yi] \u2264 \u2211 j<i (\u2211 q N(q,P \u2032j) + T (P \u2032j)/K )\nwhich immediately implies that using Lemma 26 the expected reward is at least\u2211 i E [ 1\u2212 Yi T ] R\u0302(P \u2032i) \u2265 1 2 \u2211 i R\u0302(P \u2032i) \u2265 1 4 \u2211 i R\u0302(Pi)\nusing the argument identical to the proof of Lemma 3. The theorem follows.\n6.2 Simultaneous Plays and Simultaneous-Feedback Model\nWe now show that we can make simultaneous plays; receive simultaneous feedback on all the arms that are played in a single time step; and still obtain a reward which is an O(1) factor of LPMaxMab. We use the following lemma;\nLemma 28. For \u03b2, \u03b1 \u2208 (0, 1] in time O(( \u2211\ni#Edges(Si)) log 2(nT/ \u2032)) we can find a collection of single arm policies {Li} such that (a) Each Li has a horizon of at most \u03b2T ; (b) \u2211\ni T (Li) \u2264 \u03b1KT ; (c) \u2211 i \u2211 qN(q,Li) \u2264 \u03b1\u03b2T ; and (d) \u2211 i \u2211 q qN(q,Li) \u2265 \u03b1\u03b2LPMaxMab/(1 + ).\nProof. Consider the Lagrangian relaxation of just the first constraint with multiplier \u03bb(1);\nLagFirstMax = Max{Pi}\n{ \u03bb(1)T +\n\u2211 i \u2211 q (q \u2212 \u03bb(1))N(q,Pi) | \u2211 i T (Pi) \u2264 KT\n}\nObserve that each of the policies in the optimum solution of LagFirstMax will always choose an arm for the reward if the observed value q is larger than \u03bb(1). Therefore we can use the same solution idea as Theorem 7 and get a collection of policies {P \u2032i} such that \u2211 i T (P \u2032i) \u2264 KT and \u2211 q>\u03bb(1)(q\u2212\n\u03bb(1))N(q,P \u2032i) \u2265 (LagFirstMax \u2212 \u03bb(1)T )/(1 + \u2032) in time O(( \u2211\ni#Edges(Si)) log(nT/ \u2032)). Observe that this will involve using a second multiplier \u03bb(2) and choosing between \u03bb+(2) and \u03bb\u2212(2) and the policies P \u2032i will be randomized.\nWe now perform (i) truncate P \u2032i to a horizon of \u03b2T (denote these truncated policies as P \u2032\u2032i ) and then (ii) for each i let Pi be the policy P \u2032\u2032i with probability \u03b1 \u2264 1 and otherwise be the null policy. Observe that we now have {P\u0302i} such that (note we are using Lemma 26 on R\u0303(P \u2032i) here):\n(a) Each P\u0302i has a horizon of at most \u03b2T ; (b) \u2211\ni T (P\u0302i) \u2264 \u03b1KT and\n(c) \u2211\ni \u2211 q>\u03bb1 (q \u2212 \u03bb(1))N(q, P\u0302i) \u2265 \u03b1\u03b2(LagFirstMax\u2212 \u03bb(1)T )/(1 + \u2032).\nNow we observe that any feasible solution that satisfies (a)\u2013(c) for \u03bb(1) continues to be a feasible solution for \u03bb\u2032(1) < \u03bb(1). We can now apply the reasoning of Corollary 9 and have a collection of policies Li that would satisfy the conditions (a),(b), and the following:\u2211\ni \u2211 q N(q,Li) = \u03b1\u03b2T/(1 + \u2032) < \u03b1\u03b2T and \u2211 i \u2211 q qN(q,Li) \u2265 \u03b1\u03b2LagFirstMax/(1 + \u2032)2\nThe lemma follows by setting \u2032 = /3 and observing that we will be computing the policies Pi at most O(log(nT/ \u2032)) due to the binary search.\nDefinition 14. Let \u03bbi be the \u03bb(1) \u2208 {\u03bb+, \u03bb\u2212} chosen for Li.\n6.3 The Final Scheduling of {Li}\nThe scheduling policy for the Simultaneous-Feedback model is given in Figure 10. Lemma 29. The policy Li executes to completion with probability at least ( 1\u2212 3(1+\u03b2)\u03b11\u2212\u03b2 ) on all its decision paths.\nProof. To bound the probability of this event, assume arm i is placed last in the ordering, and only marked Current after all other policies are marked Finished - this only reduces the probability that Li executes to completion on any of its decision paths. For the arms j 6= i, observe that at each step, either at least K arms are played, or sufficiently many arms are played so that the sum of the probabilities that the observed values exceed the respective \u03bbj values is at least 1 3 . Therefore, at each step, if S is the set of arms marked Current, Zjt is an indicator variable denoting whether arm i is played, and Wjqt is an indicator variable denoting whether arm i is observed in state q, we must have:\n\u2211 j\u2208S \u2211 q>\u03bbj Wjqt + Zjt K  \u2265 1 3\nIf Y denotes the random time for which policies for arms j 6= i execute, by linearity of expectation:\nE[Y ]\n3 \u2264 E \u2211 t \u2211 j \u2211 q>\u03bbj Wjqt + Zjt K  = E \u2211 j 6=i \u2211 q>\u03bbj N(q,Qj) + T (Qj) K  \u2264 \u03b1\u03b2T + \u03b1T Therefore, E[Y ] \u2264 3(1 + \u03b2)\u03b1T so that Pr[Y \u2264 (1\u2212 \u03b2)T ] \u2265 ( 1\u2212 3(1+\u03b2)\u03b11\u2212\u03b2 ) by Markov\u2019s inequality.\nIn this event, Li executes to completion, since its horizon is at most \u03b2T due to truncation.\nLemma 30. Consider the event j \u2208 S0 and j is marked Finished. In this event, suppose we count the contribution from this arm to the overall objective only when it is the maximum played arm. Then, its expected contribution is at least 13 times the value of policy Lj.\nProof. We have \u2211\ni Pr[Xiu \u2265 \u03bbi] \u2264 2 3 whenever the arm j is played simultaneously with any other\narm. Since the other arms are independent of arm j, if j is observed at q \u2265 \u03bb then with probability at least 13 all other arms are observed to be less than \u03bb. The lemma follows.\nCombining Lemmas 28, 29, and 30, we observe that with constant probability Lj executes to completion on all its decision paths. Using linearity of expectation the combined simultaneous play policy has expected value at least \u03b1\u03b23(1+ ) ( 1\u2212 3(1+\u03b2)\u03b11\u2212\u03b2 ) times LPMaxMab. Setting \u03b1 = (1\u2212 \u03b2)/(6(1 + \u03b2)) and \u03b2 = \u221a\n2\u2212 1 the approximation ratio is less than 210 for suitable . However even though the approximation factor is large, the important aspect is that we are using the stronger bound for an LP which captures the one-at-a-time model. This is summarized in the next theorem.\nTheorem 31. In time O(( \u2211\ni#Edges(Si)) log 2(nT/ )) we can find a policy that executes in the\nSimultaneous-Feedback model and provides an O(1)-approximation to the optimum policy in the One-at-a-Time feedback model.\n6.4 MaxMAB in the Only-Max-Costs Model\nNow consider the case where each arm has a budget Bi and the budget of an arm i is depleted only if arm i has the largest observed value (and the decrease in the budget is that observed value). We discuss the one-at-a-time feedback model for this problem. The extension to the simultaneous play follows from a discussion analogous to Section 6.2 with appropriate loss of constants.\nIn this Only-Max-Costs scenario, the state u \u2208 Si does not have sufficient information to encode which plays so far resulted in the arm being chosen (and hence the budget being depleted). Therefore, the state in the single-arm dynamic program also needs to separately encode the budget spent so far. This leads to difficulty in extending the results for the non-budgeted case. As a concrete hindrance, we note that the characterizations in Lemmas 24 and 28 are no longer true for the Lagrangian, since the policy might decide not to choose an arm with q > \u03bb (and deplete its budget) if it hopes that larger values of q will be subsequently observed. As a consequence of these, the analysis for the budgeted case is significantly more complicated. Nevertheless, we show that an approximately optimal solution to the Lagrangian has structure similar to Lemma 24 and hence, the scheduling policy in Figure 9 yields a constant approximation.\nDefinition 15. The policy Pi is considered p-feasible if the total reward from choices made on any decision path is at most Bi.\nAs in the previous section, define the following linear program, where the goal is to find a collection of p-feasible randomized single-arm policies Pi. As before, the policy can play the arm; observe the reward, and decide to choose the arm; or stop execution. The optimum value is bounded above by the LP relaxation below.\nMax-Pays-Mab = Maxp-feasible {Pi} {\u2211 i \u2211 q qN(q,Pi) | \u2211 i \u2211 q N(q,Pi) \u2264 T and \u2211 i T (Pi) \u2264 KT }\nObserve that the system Max-Pays-Mab differs from LPMaxMab in just the definition of the feasible policies. To simplify the remainder of the presentation, we begin with the assumption (but it is removed later).\n(A1) All reward values q are powers of two. This assumption loses a factor of 2 in the approximation ratio, since we can round q down to powers of 2 while symbolically maintaining their distinction in the prior updates.\nWe show the following surprising Lemma:\nLemma 32. Suppose we are given a p-feasible policy Ppi and a cost of choosing a reward of \u03bb(1); then there exists a policy Pi in the All-Plays-Cost model which satisfies T (Pi) \u2264 T (Ppi ) and\u2211\nq\n(q \u2212 \u03bb(1))N(q,Pi) \u2265 1\n4 \u2211 q (q \u2212 \u03bb(1))N(q,Ppi )\nunder assumption (A1).\nProof. Define q to be small if q \u2208 [\u03bb(1), 2\u03bb(1)) and large otherwise. Since q values are powers of 2, there is only one small value of q; call this value qs. Consider the policy Ppi . Either half the objective value \u2211 q(q \u2212 \u03bb(1))N(q,P p i ) is achieved by choosing qs (Case 1) or by choosing large q > qs (Case 2). The policy Pi will be different depending on the two cases \u2014 the algorithm need\nnot know which case applies; we will solve both cases and choose the better solution. In Case 1, we set \u03bd = \u03bb(1), and Case 2 set \u03bd = min{q|q \u2265 2\u03bb(1)}.\nModify policy Ppi as follows: In the new policy Pi, whenever a reward observed is at least \u03bd choose the arm even if the original policy had not chosen it. Note that we immediately have T (Pi) \u2264 T (Ppi ) since we are not affecting the play decisions.\nWe now bound the value of the new policy by considering every decision path of the original policy by fixing a decision path and consider the contribution to the objective \u2211 q(q\u2212\u03bb(1))N(q,Pi) in this decision path.\nCase 1: Suppose choosing qs contributes more than half of the value \u2211 q(q\u2212\u03bb(1))N(q,P p i ). In this case, \u03bd = \u03bb(1). Consider just the contribution of qs \u2212 \u03bb(1) on any decision path, and suppose qs is chosen k times, so that kqs \u2264 Bi. If the modified policy Pi chooses this q for k\u2032 times where k\u2032 \u2265 k times, then its value clearly dominates the contribution to Ppi . Otherwise, suppose Pi chooses qs some k\u2032 < k times, then the choices of the remaining q must have exhausted the budget. Since per unit q, the policy generates value at least 1\u2212 \u03bb(1)qs , a knapsack argument shows that the value generated in the modified policy on all q is at least the value generated by the original policy on the qs \u2212 \u03bb(1) values:\nValue of Pi on the path \u2265 Bi (\n1\u2212 \u03bb(1) qs\n) \u2265 kqs ( 1\u2212 \u03bb(1)\nqs\n) = k(qs \u2212 \u03bb(1))\nNote that contribution from qs in Ppi to the path is k(qs \u2212 \u03bb(1)). Case 2: Suppose the large q values contribute to at least half of the value \u2211 q(q \u2212 \u03bb(1))N(q,P p i ). In this case, we have \u03bd = min{q|q \u2265 2\u03bb(1)}. On any decision path, consider just the contribution of large q\u2212\u03bb(1). Note that q\u2212\u03bb(1) \u2265 q/2 for such q, so that in the worst case, the new policy exhausts its budget and contributes Bi/2 to the objective while the original policy could have contributed Bi. If the new policy does not exhaust its budget, it must match the value of the original policy on large q.\nSince we lose a factor two in splitting the analysis into two cases, and a factor of 2 within the second case. Therefore, the new policy is a 4 approximation and satisfies the properties in the statement.\nThe next Lemma follows immediately: Lemma 33. For \u03b2, \u03b1 \u2208 (0, 1] in time O(( \u2211\ni#Edges(Si)) log 2(nT/ \u2032)) we can find a collection\nof single arm policies {Li} in the All-Plays-Cost model such that (a) Each Li has a horizon of at most \u03b2T ; (b) \u2211 i T (Li) \u2264 \u03b1KT ; (c) \u2211 i \u2211 qN(q,Li) \u2264 \u03b1\u03b2T/4; and (d) \u2211 i \u2211 q qN(q,Li) \u2265 \u03b1\u03b2Max-Pays-Mab/(4(1 + )), under the condition (A1). The condition (A1) can be removed if we relax condition (d) to \u2211 i \u2211 q qN(q,Li) \u2265 \u03b1\u03b2Max-Pays-Mab/(8(1 + )).\nProof. The proof is identical to the proof of Lemma 28 except in the stage where we find the policies P \u2032i, where we can only guarantee\u2211\nq>\u03bb(1)\n(q \u2212 \u03bb(1))N(q,P \u2032i) \u2265 1\n4 (LagFirstMax\u2212 \u03bb(1)T )/(1 + \u2032)\nas a consequence of Lemma 32. Note that to apply Lemma 32 we need to use two different dynamic programming solutions \u2014 one for considering all values larger than qs and one for all values larger or equal to 2qs. Observe that qs is fixed when we know \u03bb(1).\nWe are ready to prove the main result of this subsection.\nTheorem 34. In time O(( \u2211\ni#Edges(Si)) log 2(nT/ )) we can find a policy that executes in\nthe All-Plays-Cost model and provides an O(1)-approximation to the optimum policy in the Only-Max-Costs model, where both are measured in the One-at-a-Time feedback model.\nIn the same running time we can find a policy that executes in the conjunction of All-Plays-Cost and Simultaneous-Feedback models and guarantees an O(1)-approximation to the Only-Max-Costs accounting in the One-at-a-Time feedback model. Therefore the optimum solutions in all the possible four models are within O(1) factor of each other.\nProof. The first part of the theorem follows from finding policies {Li} as described in Lemma 33 setting \u03b1 = 1, \u03b2 = 1 and observing that \u2211 i (\u2211 qN(q,Li) + T (Li)/K ) \u2264 5T/4. We can now run the scheduling policy as described in Figure 9 using the probability 4/5 in Step 1. It is easy to observe that the reward of the combined policy is at least 25 \u2211 i \u2211 q qN(q,Li). This is at least Max-Pays-Mab/(10(1+ )) under the assumption (A1) and Max-Pays-Mab/(20(1+ )) otherwise. Observe that the policy we execute is in the All-Plays-Cost model.\nFor the second part we again use Lemma 33 and run the scheduling algorithm in Figure 10 and\nthe expected reward is \u03b1\u03b224(1+ ) ( 1\u2212 3(1+\u03b2/4)\u03b11\u2212\u03b2 ) which is O(1) for suitable \u03b1, \u03b2. The last part of the theorem now follows."}, {"heading": "7 Future Utilization and Budgeted Learning", "text": "In this section, we consider the budgeted learning problem. We assume K = 1 arms can be played any step. The goal of a policy here is to perform pure exploration for T steps. At the T + 1st step, the policy switches to pure exploitation and therefore the objective is to optimize the expected reward for the T + 1st play. More formally, given any policy P , each decision path yields a final (joint) state space. In each such final state, choose that arm i whose final state u \u2208 Si has maximum reward ru; this is the reward the policy obtains in this decision path. The goal is to find the policy that maximizes the expected reward obtained, where the expectation is taken over the execution of the policy. We provide a (3 + )-approximation for the problem in time O(( \u2211\ni#Edges(Si)) log(nT/ )) time. The result holds for adversarially ordered versions, where an adversary specifies the order in which we can execute the policies without returning to an arm. The result extends to a 4 + -approximation under metric traversal constraints.\nSingle arm policies. Consider the execution of a policy P , and focus on some arm i. The execution for this arm defines a single arm policy Pi, with the following actions available in any state u \u2208 Si: (i) Make a play; (ii) Choose the arm as final, and obtain reward ru; or (iii) Quit.\nDefinition 16. Given a single arm policy Pi for arm i let I(Pi) be the probability that arm i is chosen as final. Let R\u0303(Pi) be the expected reward obtained from events when the arm is chosen as final \u2013 note that this is different from previous sections. As before T (Pi) is the number of expected plays made by the policy Pi.\n7.1 A Weakly Coupled Relaxation and a (3 + )-Approximate Solution\nWe define the following weakly coupled formulation:\nLPBud = MaxFeasible {Pi} {\u2211 i R\u0303(Pi) \u2223\u2223\u2223\u2223\u2223\u2211 i (I(Pi) + T (Pi)/T ) \u2264 2 }\nLemma 35. LPBud is an upper bound for the reward of the optimal policy, OPT .\nProof. Consider the (randomized) single-arm policies obtained by viewing the execution of the optimal policy restricted to each arm i. It is easy to check via linearity of expectation that these policies are feasible for the constraint in LPMaxMab, since the expected number of choices made on any decision path is at most one, and the expected number of plays made is at most T . Further, the objective value of these policies is precisely OPT .\nIn what follows we show that we can construct a feasible policy which has an objective value LPBud/3. This proof will be similar to the argument in Section 4. Consider the Lagrangian:\nBudLag = MaxFeasible {Pi}\n{ 2\u03bb+\n\u2211 i ( R\u0303(Pi)\u2212 \u03bbI(Pi)\u2212 \u03bbT (Pi)/T )} = 2\u03bb+ \u2211 i Q\u0303i(\u03bb)\nwhere Q\u0303i(\u03bb) is the maximum value of R\u0303(Pi)\u2212 \u03bbI(Pi)\u2212 \u03bbT (Pi)/T for a feasible policy Pi. Immediately it follows from weak duality that (compare Lemma 11 and Lemma 6);\nLemma 36. Q\u0303i(\u03bb) can be computed by a dynamic program in time O(#Edges(Si)). For the optimum policy Pi(\u03bb) we have R\u0303(P ) = Q\u0303i(\u03bb) + \u03bb(I(Pi(\u03bb)) + T (Pi(\u03bb))/T ). Moreover I(Pi(\u03bb)) + T (Pi(\u03bb))/T is non-increasing in \u03bb.\nProof. Let Gain\u2032(u) to be the maximum of the objective of the single-arm policy conditioned on starting at u \u2208 Si. We perform a bottom up dynamic programming over Si. If u has no children, then if we \u201cchoose the node as a final answer\u201d at node u, then Gain\u2032(u) = ru \u2212 \u03bb in this case. Stopping and not doing anything corresponds to Gain\u2032(u) = 0. Playing the arm at node u will rule out either of the outcomes, and in this case Gain\u2032(u) = \u2212 \u03bbT + \u2211 v puvGain \u2032(v). In summary:\nGain\u2032(u) = max { 0, ru \u2212 \u03bb, \u2212 \u03bb\nT + \u2211 v puvGain \u2032(v)\n}\nWe have Gain\u2032(\u03c1i) = Q\u0303i(\u03bb). The running time follows from inspection. Observe that R\u0303(Pi(\u03bb)) = Q\u0303i(\u03bb) + \u03bb(I(Pi(\u03bb)) + T (Pi(\u03bb))/T ) is maintained for every subtree corresponding to the optimum policy starting at u \u2208 Si. Note that increasing \u03bb decreases the contribution of the latter two terms which corresponds to nonincreasing I(P ), T (P ).\nLemma 37. 2\u03bb + \u2211\ni Q\u0303i(\u03bb) \u2265 LPBud. Moreover for any constant \u03b4 > 0, we can choose a \u03bb\u2217 in polynomial time so that for the resulting collection of policies {P \u2217i } which achieve the respective Q\u0303i(\u03bb \u2217) we have: (1) \u03bb\u2217T \u2265 (1\u2212 \u03b4)LPBud/3 and (2) \u2211 i Q\u0303i(\u03bb \u2217) \u2265 \u03bb\u2217.\nProof. The first part follows from weak duality. For the second part, our approach is similar to that in Theorem 8. Observe that at very large \u03bb we have Q\u0303i(\u03bb) = 0 for all i and therefore \u2211 i Q\u0303i(\u03bb) < \u03bb.\nAt \u03bb = 0 we easily have \u2211\ni Q\u0303i(\u03bb) \u2265 \u03bb. we can now perform a binary search such that we have two values of \u03bb\u2212, \u03bb+ maintaining the invariants that \u2211 i Q\u0303i(\u03bb \u2212) \u2265 \u03bb\u2212 and \u2211 i Q\u0303i(\u03bb +) < \u03bb+ respectively. Note that it suffices to ensure \u03bb+ \u2212 \u03bb\u2212 \u2264 \u03b4\u03bb\u2212/3; where we can use \u03bb\u2217 = \u03bb\u2212. The lemma follows from that fact that 2\u03bb+ \u2211 i Q\u0303i(\u03bb) \u2265 LPBud."}, {"heading": "7.2 An Amortized Accounting", "text": "Consider the single-arm policy Pi(\u03bb\u2217) that corresponds to the value Q\u0303i(\u03bb). Pi(\u03bb\u2217) performs one of three actions for each state u \u2208 Si: (i) Play the arm; (ii) choose the arm as the final answer and stop or (iii) Quit. For this policy, R\u0303(Pi(\u03bb \u2217)) = Q\u0303i(\u03bb \u2217) +\u03bb\u2217(I(Pi(\u03bb\u2217)) +T (Pi(\u03bb\u2217))/T ). This implies that the reward R(Pi(\u03bb\u2217)) of this policy can be amortized, so that for state u \u2208 Si, the reward is collected as follows:\n1. An upfront reward of Q\u0303i(\u03bb \u2217) when the play for the arm initiates at the root \u03c1 \u2208 Si. 2. A reward of \u03bb\u2217 for choosing the arm in any state u and stopping.\n3. A reward of \u03bb\u2217/T for playing the arm in u \u2208 Si.\nNote that this accounting is not true if the policy P\u2217i (\u03bb) is executed incompletely, for instance, if it is terminated prematurely."}, {"heading": "7.3 The Final Algorithm", "text": "We now present the overall algorithm in Figure 11. The next lemma is the crux of the entire analysis, and follows the amortized accounting argument. The hurdle with using the argument directly is that when the horizon is exhausted, the currently executing single-arm policy is not executed completely. We first pretend it executed completely, violating the horizon.\nLemma 38. Suppose Final(\u03bb\u2217) completely executes the single-arm policy that it is executing at time T , before stopping. (Observe that this policy is not feasible.) The expected reward of this (infeasible) policy is at least LPBud/(3 + ).\nProof. Consider three stopping conditions: (a) The policy has visited all the arms and have no further arms to play, (b) the policy choose an arm, or (c) the policy continue past T steps.\nBy the amortized argument, in case (a) the contribution to the reward using the amortized accounting is \u2211 i Q\u0303(\u03bb\n\u2217) which is at least \u03bb\u2217 by Lemma 37. In case (b), the contribution is again \u03bb\u2217. In case (c) the contribution is T \u00b7 \u03bb\u2217/T = \u03bb\u2217. Thus in all cases the contribution is at least \u03bb\u2217 and the Lemma follows (setting \u03b4 \u2264 /3 in Lemma 37).\nTheorem 39. The policy Final(\u03bb\u2217) achieves a reward of at least LPBud/(3 + ).\nProof. The policy Final(\u03bb\u2217) differs from that in Lemma 38 as follows: At the T + 1st step, instead of continuing executing the current policy Pj(\u03bb\u2217) for some arm j in state u \u2208 Sj (and continuing the amortized accounting), Final(\u03bb\u2217) simply chooses the arm j as the final answer. Let the remainder of the policy which was not executed be P .\nBy the martingale property of the rewards, choosing the arm j at this state u would contribute to the objective at least R\u0303(P ) because P may not choose the arm at all and have 0 contribution in some evolutions. Therefore, stopping single-arm policy Pj(\u03bb\u2217) prematurely (when the horizon is exhausted) and choosing it as final yields at least as much reward as executing it completely. Therefore, the reward of Final(\u03bb\u2217) is at least the reward of the infeasible policy analyzed in Lemma 38, and the theorem follows.\nThe next corollary follows by inspection based on the arguments leading up to Theorem 39. Corollary 40. If we can solve \u2211\ni Q\u0303i(\u03bb) to an \u03b1 approximation then we would have an (\u03b1+ 2 + ) approximation for budgeted learning.\nObserve that the Lagrangian formulation in BudLag would still satisfy Lemma 12, and as a consequence we immediately have a factor (\u03b1 + 2 + )-approximation algorithm for the variant of the budgeted learning problem where we have a metric switching cost and a total bound of L on the total switching cost, following the \u03b1 = 2 + (or possibly other) approximation algorithms for the Orienteering problem discussed in Section 4. We can also solve the budgeted learning problem with r additional packing (knapsack type) constraints to within (r + 2 + )-approximation."}, {"heading": "8 Conclusions", "text": "In this paper, we have shown that for several variants of the finite horizon multi-armed bandit problem, we can formulate and solve weakly coupled LP relaxations, and use the solutions of these relaxations to devise feasible decision policies whose reward is within a fixed constant factor of the optimal reward. This provides analytic justification for using such relaxations to guide policy design in practice, and the resulting policies are comparable in complexity to standard index policies.\nThe main open questions posed by our work is to improve the performance bounds for large delays in the delayed feedback model or the results for the Simultaneous-Feedback model for the MaxMAB problem. Observe that in the latter case we are providing comparisons against the strong upper bounds of One-at-a-Time model. This would require new techniques or a lower upper bound. This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44]. It would also be interesting to characterize the class of bandit problems for which weakly coupled relaxations provide constant factor approximations."}, {"heading": "Acknowledgments", "text": "We thank Martin Pa\u0301l for several helpful discussions."}], "references": [{"title": "Relaxations of weakly coupled stochastic dynamic programs", "author": ["D. Adelman", "A.J. Mersereau"], "venue": "Operations Research, 56(3):712\u2013727,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Explore/exploit schemes for web content optimization", "author": ["D. Agarwal", "B.-C. Chen", "P. Elango"], "venue": "Proceedings of ICDM, pages 1\u201310,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A measurement-based analysis of multihoming", "author": ["A. Akella", "B. Maggs", "S. Seshan", "A. Shaikh", "R. Sitaraman"], "venue": "Proceedings of SIGCOMM, pages 353\u2013364,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximizing sequence-submodular functions and its application to online advertising", "author": ["S. Alaei", "A. Malekian"], "venue": "CoRR abs/1009.4153,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequential analysis with delayed observations", "author": ["T.W. Anderson"], "venue": "Journal of the American Statistical Association, 59(308):1006\u20131015,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1964}, {"title": "The search for optimality in clinical trials", "author": ["P. Armitage"], "venue": "International Statistical Review, 53(1):15\u201324,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1985}, {"title": "Bayes and minmax solutions of sequential decision problems", "author": ["K.J. Arrow", "D. Blackwell", "M.A. Girshick"], "venue": "Econometrica, 17:213\u2013244,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1949}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, 47(2-3):235\u2013256,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM J. Comput., 32(1):48\u201377,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Automated experiment-driven management of (database) systems", "author": ["S. Babu", "N. Borisov", "S. Duan", "H. Herodotou", "V. Thummala"], "venue": "Proc. of HotOS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Denumerable-armed bandits", "author": ["J.S. Banks", "R.K. Sundaram"], "venue": "Econometrica, 60(5):1071\u2013 1096,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Switching costs and the gittins index", "author": ["J.S. Banks", "R.K. Sundaram"], "venue": "Econometrica, 62(3):687\u2013694,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Approximation algorithms for deadline-tsp and vehicle routing with time-windows", "author": ["N. Bansal", "A. Blum", "S. Chawla", "A. Meyerson"], "venue": "STOC, pages 166\u2013174,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Dynamic Programming and Optimal Control", "author": ["D. Bertsekas"], "venue": "Athena Scientific, second edition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Conservation laws, extended polymatroids and multi-armed bandit problems: A unified polyhedral approach", "author": ["D. Bertsimas", "J. Nino-Mora"], "venue": "Math. of Oper. Res., 21(2):257\u2013306,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Restless bandits, linear programming relaxations, and a primal-dual index heuristic", "author": ["D. Bertsimas", "J. Ni\u00f1o-Mora"], "venue": "Oper. Res., 48(1):80\u201390,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Approximation algorithms for orienteering and discounted-reward tsp", "author": ["A. Blum", "S. Chawla", "D.R. Karger", "T. Lane", "A. Meyerson", "M. Minkoff"], "venue": "SIAM J. Comput., 37(2):653\u2013670,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "On sequential designs for maximizing the sum of n observations", "author": ["R.N. Bradt", "S.M. Johnson", "S. Karlin"], "venue": "Ann. Math. Statist., 27(4):1060\u20131074,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1956}, {"title": "Optimal learning and experimentation in bandit problems", "author": ["M. Brezzi", "T.-L. Lai"], "venue": "Journal of Economic Dynamics and Control, 27(1):87\u2013108,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "J. ACM, 44(3):427\u2013485,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Prediction, Learning and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Improved algorithms for orienteering and related problems", "author": ["C. Chekuri", "N. Korula", "M. P\u00e1l"], "venue": "SODA \u201908: Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms, pages 661\u2013670,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequential decision for a binomial parameter with delayed observations", "author": ["S.C. Choi", "V.A. Clark"], "venue": "Biometrics, 26(3):411\u2013420,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1970}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & sons,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1991}, {"title": "Approximating the stochastic knapsack problem: The benefit of adaptivity", "author": ["B.C. Dean", "M.X. Goemans", "J. Vondrak"], "venue": "Math. of Operations Research, 33(4):945\u2013964,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Reflective control for an elastic cloud appliation: An automated experiment workbench", "author": ["A. Demberel", "J. Chase", "S. Babu"], "venue": "Proc. of HotCloud,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "On a scheduling problem in sequential analysis", "author": ["S. Ehrenfeld"], "venue": "The Annals of Mathematical Statistics, 41(4):1206\u20131216,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1970}, {"title": "The two-armed bandit with delayed responses", "author": ["S.G. Eick"], "venue": "The Annals of Statistics, 16(1):254\u2013 264,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1988}, {"title": "Risk-sensitive online learning", "author": ["E. Even-Dar", "M.J. Kearns", "J.W. Vaughan"], "venue": "ALT, pages 199\u2013213,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "The irrevocable multiarmed bandit problem", "author": ["V.F. Farias", "R. Madan"], "venue": "Operations Research, 59(2):383\u2013399,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Online convex optimization in the bandit setting: Gradient descent without a gradient", "author": ["A. Flaxman", "A. Kalai", "H.B. McMahan"], "venue": "Annual ACM-SIAM Symp. on Discrete Algorithms,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-Armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": "Wiley,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "D.M. Jones"], "venue": "Progress in statistics (European Meeting of Statisticians),", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1972}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "D.M. Jones"], "venue": "Progress in statistics (European Meeting of Statisticians),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1972}, {"title": "How to probe for an extreme value", "author": ["A. Goel", "S. Guha", "K. Munagala"], "venue": "ACM Transactions of Algorithms, 7(1):12:1\u201320,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "The ratio index for budgeted learning, with applications", "author": ["A. Goel", "S. Khanna", "B. Null"], "venue": "Proceedings of SODA,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive submodular optimization under matroid constraints", "author": ["D. Golovin", "A. Krause"], "venue": "CoRR abs/1101.4450,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximation algorithms for budgeted learning problems", "author": ["S. Guha", "K. Munagala"], "venue": "Proc. of STOC,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Sequential design of experiments via linear programming", "author": ["S. Guha", "K. Munagala"], "venue": "CoRR, arxiv:0805.0766,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-armed bandits with metric switching costs", "author": ["S. Guha", "K. Munagala"], "venue": "Proceedings of ICALP,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Approximate indexability and bandit problems with concave rewards and delayed feedback", "author": ["S. Guha", "K. Munagala"], "venue": "Proceedings of APPROX-RANDOM,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterated allocations with delayed feedback", "author": ["S. Guha", "K. Munagala", "M. P\u00e1l"], "venue": "Manuscript, available at CoRR http://arxiv.org/abs/1011.1161,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximation algorithms for restless bandit problems", "author": ["S. Guha", "K. Munagala", "P. Shi"], "venue": "J. ACM, 58(1):3:1\u201350,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximation algorithms for correlated knapsacks and non-martingale bandits", "author": ["A. Gupta", "R. Krishnaswamy", "M. Molinaro", "R. Ravi"], "venue": "Proc. of FOCS,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Profiling, what-if analysis, and cost-based optimization of mapreduce programs", "author": ["H. Herodotou", "S. Babu"], "venue": "Proc. of VLDB,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and lagrangian relaxation", "author": ["K. Jain", "V.V. Vazirani"], "venue": "J. ACM, 48(2):274\u2013296,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2001}, {"title": "Job-search and the theory of turnover", "author": ["B. Jovanovich"], "venue": "J. Political Economy, 87:972990,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1979}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Proc. of 16th Conf. on Computational Learning Theory,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6:4\u201322,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1985}, {"title": "Budgeted learning of naive-bayes classifiers", "author": ["D. Lizotte", "O. Madani", "R. Greiner"], "venue": "Proceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence (UAI-03), pages 378\u201338,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimization flow control, i: Basic algorithm and convergence", "author": ["S. Low", "D.E. Lapsley"], "venue": "IEEE/ACM Transactions on Networks, 7(6), December", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1999}, {"title": "Active model selection", "author": ["O. Madani", "D.J. Lizotte", "R. Greiner"], "venue": "UAI \u201904: Proc. 20th Conf. on Uncertainty in Artificial Intelligence, pages 357\u2013365,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2004}, {"title": "Job-search and labor market analysis", "author": ["D. Mortensen"], "venue": "O. Ashenfelter and R. Layard, editors, Handbook of Labor Economics, volume 2, page 849919. North Holland, Amsterdam,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1985}, {"title": "Restless bandits, partial conservation laws and indexability", "author": ["J. Ni\u00f1o-Mora"], "venue": "Adv. in Appl. Probab., 33(1):76\u201398,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-UAV dynamic routing with partial observations using restless bandits allocation indices", "author": ["J.L. Ny", "M. Dahleh", "E. Feron"], "venue": "Proceedings of the 2008 American Control Conference,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin American Mathematical Society, 55:527\u2013535,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1952}, {"title": "A two-armed bandit theory of market pricing", "author": ["M. Rothschild"], "venue": "J. Economic Theory, 9:185202,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1974}, {"title": "Active learning in discrete input spaces", "author": ["J. Schneider", "A. Moore"], "venue": "34th Interface Symp.,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2002}, {"title": "Adaptive treatment assignment methods and clinical trials", "author": ["R. Simon"], "venue": "Biometrics, 33(4):743\u2013 749,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1977}, {"title": "An online algorithm for maximizing submodular functions", "author": ["M.J. Streeter", "D. Golovin"], "venue": "NIPS, pages 1577\u20131584,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2008}, {"title": "An asymptotically optimal algorithm for the max k-armed bandit problem", "author": ["M.J. Streeter", "S.F. Smith"], "venue": "Proceedings of AAAI,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2006}, {"title": "On sequential decision problems with delayed observations", "author": ["Y. Suzuki"], "venue": "Annals of the Institute of Statistical Mathematics, 18(1):229\u2013267,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1966}, {"title": "Tuple routing strategies for distributed eddies", "author": ["F. Tian", "D.J. DeWitt"], "venue": "Proc. of VLDB,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2003}, {"title": "A short proof of the Gittins index theorem", "author": ["J.N. Tsitsiklis"], "venue": "Annals of Appl. Prob., 4(1):194\u2013 199,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1994}, {"title": "Sequential Analysis", "author": ["A. Wald"], "venue": "Wiley, New York,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1947}, {"title": "Restless bandits: Activity allocation in a changing world", "author": ["P. Whittle"], "venue": "Appl. Prob., 25(A):287\u2013 298,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1988}], "referenceMentions": [{"referenceID": 37, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 101, "endOffset": 105}, {"referenceID": 39, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 117, "endOffset": 121}, {"referenceID": 40, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 138, "endOffset": 142}, {"referenceID": 38, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 181, "endOffset": 189}, {"referenceID": 41, "context": "\u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42].", "startOffset": 181, "endOffset": 189}, {"referenceID": 64, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 40, "endOffset": 44}, {"referenceID": 55, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 20, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 31, "context": "Since the seminal contributions of Wald [65] and Robbins [56], a vast literature, including both optimal and near optimal solutions, has been developed, see references in [14, 21, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 13, "context": "However optimum index policies exist only in limited settings, see [14, 32] for further discussion.", "startOffset": 67, "endOffset": 75}, {"referenceID": 31, "context": "However optimum index policies exist only in limited settings, see [14, 32] for further discussion.", "startOffset": 67, "endOffset": 75}, {"referenceID": 6, "context": "One such general setting is the Bayesian Stochastic Multi-Armed Bandit formulation, which dates back to the results of [7, 18].", "startOffset": 119, "endOffset": 126}, {"referenceID": 17, "context": "One such general setting is the Bayesian Stochastic Multi-Armed Bandit formulation, which dates back to the results of [7, 18].", "startOffset": 119, "endOffset": 126}, {"referenceID": 32, "context": "This process therefore is a special case of the classic finite horizon multi-armed bandit problem [33].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "The Bayesian MAB formulation is therefore a canonical example of the Martingale Reward Bandit considered in recent literature such as [30]3.", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "This ability of exchanging plays, is at the core of all existing analysis of stochastic MAB problems with provable guarantees [21, 14, 32].", "startOffset": 126, "endOffset": 138}, {"referenceID": 13, "context": "This ability of exchanging plays, is at the core of all existing analysis of stochastic MAB problems with provable guarantees [21, 14, 32].", "startOffset": 126, "endOffset": 138}, {"referenceID": 31, "context": "This ability of exchanging plays, is at the core of all existing analysis of stochastic MAB problems with provable guarantees [21, 14, 32].", "startOffset": 126, "endOffset": 138}, {"referenceID": 33, "context": "For the above problems, the traditional and well understood index policies such as the Gittins index [34, 64] which are optimal for discounted infinite horizon settings, are either undefined or lead to poor performance bounds.", "startOffset": 101, "endOffset": 109}, {"referenceID": 63, "context": "For the above problems, the traditional and well understood index policies such as the Gittins index [34, 64] which are optimal for discounted infinite horizon settings, are either undefined or lead to poor performance bounds.", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "At a high level, our approach uses linear programming of a type that is similar to the widely used weakly coupled relaxation for bandit problems4 or \u201cdecomposable\u201d linear program (LP) relaxation [14, 66].", "startOffset": 195, "endOffset": 203}, {"referenceID": 65, "context": "At a high level, our approach uses linear programming of a type that is similar to the widely used weakly coupled relaxation for bandit problems4 or \u201cdecomposable\u201d linear program (LP) relaxation [14, 66].", "startOffset": 195, "endOffset": 203}, {"referenceID": 53, "context": "Though classic index policies are constructed using this approach [54, 66], our approach is fundamentally different: In a global policy, actions pertaining to a single arm We owe the terminology \u201cweakly coupled\u201d to Adelman and Mersereau [1].", "startOffset": 66, "endOffset": 74}, {"referenceID": 65, "context": "Though classic index policies are constructed using this approach [54, 66], our approach is fundamentally different: In a global policy, actions pertaining to a single arm We owe the terminology \u201cweakly coupled\u201d to Adelman and Mersereau [1].", "startOffset": 66, "endOffset": 74}, {"referenceID": 0, "context": "Though classic index policies are constructed using this approach [54, 66], our approach is fundamentally different: In a global policy, actions pertaining to a single arm We owe the terminology \u201cweakly coupled\u201d to Adelman and Mersereau [1].", "startOffset": 237, "endOffset": 240}, {"referenceID": 24, "context": "The gap between these two accounting processes is termed as the adaptivity gap (an example of such in the context of scheduling can be found in [25]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 34, "context": "For instance, for constraint (e), where the per-step reward is the maximum observed value from the set of plays is NP-Hard because it generalizes computing a subset of k distributions that maximizes the expected maximum [35].", "startOffset": 220, "endOffset": 224}, {"referenceID": 16, "context": "The bandit problem with switching costs (constraint c) is Max-SNP Hard6 since it generalizes an underlying combinatorial optimization problem termed orienteering [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 29, "context": "In this case, and many others we provide alternate analysis of existing heuristics which have been shown to perform well in practice [30].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "This problem was formalized by Farias and Madan [30].", "startOffset": 48, "endOffset": 52}, {"referenceID": 37, "context": "Using the algorithmic framework introduced in [38], Farias and Madan showed an 8 approximation algorithm which also works well in practice.", "startOffset": 46, "endOffset": 50}, {"referenceID": 37, "context": "In Section 3, we provide a better analysis argument (based on revisiting [38] and newer ideas) and that argument improves the result to a factor (2 + )-approximation for any > 0 in time O(( \u2211 i#Edges(Si)) log(nT/ )) where #Edges(Si) is the number of edges that define the statespace Si.", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "It is widely acknowledged [12, 19] that the scenarios that call for the application of bandit problems typically have constraints/costs for switching arms.", "startOffset": 26, "endOffset": 34}, {"referenceID": 18, "context": "It is widely acknowledged [12, 19] that the scenarios that call for the application of bandit problems typically have constraints/costs for switching arms.", "startOffset": 26, "endOffset": 34}, {"referenceID": 11, "context": "Banks and Sundaram [12] provide an illuminating discussion even for the discounted reward version, and highlight the technical difficulty in designing reasonable policies.", "startOffset": 19, "endOffset": 23}, {"referenceID": 56, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 132, "endOffset": 136}, {"referenceID": 46, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 52, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 11, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 171, "endOffset": 183}, {"referenceID": 10, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 234, "endOffset": 242}, {"referenceID": 31, "context": "These constraints/costs can be motivated by strategic considerations and even be adversarial: Pricesetting under demand uncertainty [57], decision making in labor markets [47, 53, 12], and resource allocation among competing projects [11, 32].", "startOffset": 234, "endOffset": 242}, {"referenceID": 11, "context": "For instance [12, 11], consider a worker who has a choice of working in k firms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 10, "context": "For instance [12, 11], consider a worker who has a choice of working in k firms.", "startOffset": 13, "endOffset": 21}, {"referenceID": 39, "context": "Except the conference paper [40], which is subsumed by this article.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "These result weakens if we use the 4-approximation in [17] instead of the 2 + approximation for the orienteering problem in [22].", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "These result weakens if we use the 4-approximation in [17] instead of the 2 + approximation for the orienteering problem in [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "This notion was introduced by Anderson [5] in an early work in mid 1960s.", "startOffset": 39, "endOffset": 42}, {"referenceID": 61, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 54, "endOffset": 62}, {"referenceID": 22, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 54, "endOffset": 62}, {"referenceID": 26, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 5, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 58, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 27, "context": "Since then, though there have been additional results [62, 23], a theoretical guarantee on adaptive decision making under delayed observations has been elusive, and the computational difficulty in obtaining such has been commented upon in [27, 6, 59, 28].", "startOffset": 239, "endOffset": 254}, {"referenceID": 1, "context": "As an example described in Agarwal etal in [2], consider the problem of presenting different websites/snippets to an user and induce the user to visit these web pages.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 116, "endOffset": 119}, {"referenceID": 62, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 196, "endOffset": 204}, {"referenceID": 44, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 196, "endOffset": 204}, {"referenceID": 25, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 243, "endOffset": 247}, {"referenceID": 54, "context": "The delay can also arise from batched updates and systems issues, for instance in information gathering and polling [2], adaptive query processing [63], experiment driven management and profiling [10, 45], reflective control in cloud services [26], or unmanned aerial vehicles [55].", "startOffset": 277, "endOffset": 281}, {"referenceID": 2, "context": "As another example, consider an fault tolerant network application wishing to choose send packets along K independent routes [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 49, "context": "One of the more recent applications of MAB has been the Budgeted Learning type applications popularized by [50, 52, 58].", "startOffset": 107, "endOffset": 119}, {"referenceID": 51, "context": "One of the more recent applications of MAB has been the Budgeted Learning type applications popularized by [50, 52, 58].", "startOffset": 107, "endOffset": 119}, {"referenceID": 57, "context": "One of the more recent applications of MAB has been the Budgeted Learning type applications popularized by [50, 52, 58].", "startOffset": 107, "endOffset": 119}, {"referenceID": 6, "context": "This application has also been hinted at in [7].", "startOffset": 44, "endOffset": 47}, {"referenceID": 23, "context": "Natural extensions of such correspond to Knapsack type constraints and similar problems have been discussed in the context of power allocation (when the arms are channels) [24] and optimizing \u201cTCP friendly\u201d network utility functions [51].", "startOffset": 172, "endOffset": 176}, {"referenceID": 50, "context": "Natural extensions of such correspond to Knapsack type constraints and similar problems have been discussed in the context of power allocation (when the arms are channels) [24] and optimizing \u201cTCP friendly\u201d network utility functions [51].", "startOffset": 233, "endOffset": 237}, {"referenceID": 55, "context": "Multi-armed bandit problems have been extensively studied since their introduction by Robbins in [56].", "startOffset": 97, "endOffset": 101}, {"referenceID": 48, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 7, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 20, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 59, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 60, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 19, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 47, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 30, "context": "In context of theoretical results, one typical goal [49, 8, 21, 60, 61, 20, 48, 31] has been to assume that the agent has absolutely no knowledge of \u03c1i (model free assumption) and then the task has been to minimize the \u201cregret\u201d or the lost reward, that is comparing the performance to an omniscient policy that plays the best arm from the start.", "startOffset": 52, "endOffset": 83}, {"referenceID": 59, "context": "The exchange properties are required for defining submodularity as well as its extensions, such as sequence or adaptive submodularity [60, 37, 4].", "startOffset": 134, "endOffset": 145}, {"referenceID": 36, "context": "The exchange properties are required for defining submodularity as well as its extensions, such as sequence or adaptive submodularity [60, 37, 4].", "startOffset": 134, "endOffset": 145}, {"referenceID": 3, "context": "The exchange properties are required for defining submodularity as well as its extensions, such as sequence or adaptive submodularity [60, 37, 4].", "startOffset": 134, "endOffset": 145}, {"referenceID": 37, "context": "It may be appear Subsuming the main result of the conference paper [38].", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "that we can avoid the issue by appealing to non-stochastic/adversarial MABs [9] \u2013 but they do not help in the presence of budget constraints which couples the action across various time steps.", "startOffset": 76, "endOffset": 79}, {"referenceID": 28, "context": "It is known that online convex analysis cannot be analyzed well in the presence of large number of arms and a \u201cstate\u201d that couples the time steps [29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 60, "context": "The problem considered in [61] is similar in name but is different from the MaxMAB problem we have posed herein \u2014 that paper maximizes the single maximum value seen across all the arms and across all the T steps.", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "The authors of [36] show that several natural index policies for the budgeted learning problem are constant approximations using analysis arguments which are different from those presented here.", "startOffset": 15, "endOffset": 19}, {"referenceID": 35, "context": "The specific approximation ratios proved in [36] are improved upon by the results herein with better running times.", "startOffset": 44, "endOffset": 48}, {"referenceID": 43, "context": "The authors of [44] present a constant approximation for non-martingale finite horizon bandits; however, these problems require techniques that are orthogonal to those in this paper.", "startOffset": 15, "endOffset": 19}, {"referenceID": 42, "context": "The problems considered in [43] is an infinite horizon restless bandit problem.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "Our running times and the description of the final policies will be comparable (in a favorable way) to the time required for dynamic programming to compute the standard index policies [14, 34].", "startOffset": 184, "endOffset": 192}, {"referenceID": 33, "context": "Our running times and the description of the final policies will be comparable (in a favorable way) to the time required for dynamic programming to compute the standard index policies [14, 34].", "startOffset": 184, "endOffset": 192}, {"referenceID": 29, "context": "Recently, Farias and Madan [30] provided an 8-approximation for this problem.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "Previously, constant factor approximations were provided in [36, 40] for the finite horizon MAB problem (not necessarily using irrevocable policies).", "startOffset": 60, "endOffset": 68}, {"referenceID": 39, "context": "Previously, constant factor approximations were provided in [36, 40] for the finite horizon MAB problem (not necessarily using irrevocable policies).", "startOffset": 60, "endOffset": 68}, {"referenceID": 0, "context": "u\u2208Si(T ) ruzu \u2211n i=1 \u2211 u\u2208Si zu \u2264 KT \u2211 v\u2208Si zvpvu = wu \u2200i, u \u2208 Si(T ) \\ {\u03c1i} zu \u2264 wu \u2200u \u2208 Si(T ),\u2200i zu, wu \u2208 [0, 1] \u2200u \u2208 Si(T ),\u2200i", "startOffset": 108, "endOffset": 114}, {"referenceID": 65, "context": "A similar LP formulation was proposed for the multi-armed bandit problem by Whittle [66] and Bertsimas and Nino-Mora [54]; however, one key difference is that we ignore the time at which the arm is played in defining the LP variables.", "startOffset": 84, "endOffset": 88}, {"referenceID": 53, "context": "A similar LP formulation was proposed for the multi-armed bandit problem by Whittle [66] and Bertsimas and Nino-Mora [54]; however, one key difference is that we ignore the time at which the arm is played in defining the LP variables.", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "u\u2208Si(T ) (ru \u2212 \u03bb)zu \u2211 v\u2208Si zvpvu = wu \u2200i, u \u2208 Si(T ) \\ {\u03c1i} zu \u2264 wu \u2200u \u2208 Si(T ),\u2200i zu, wu \u2208 [0, 1] \u2200u \u2208 Si(T ),\u2200i (2)", "startOffset": 92, "endOffset": 98}, {"referenceID": 13, "context": "The solution presented in Lemma 6 shows that the optimum policy Li(\u03bb) satisfies zu = 0 (no play, or Gain(u) = 0) or zu = wu (play orGain(u) > 0); which are to expected using complementary slackness [14].", "startOffset": 198, "endOffset": 202}, {"referenceID": 45, "context": "By a standard application of weak duality (see for instance [46]), a (1+ ) approximate solution to (LP1) can be obtained by taking a convex combination of the solutions to LPLag(\u03bb) for two values \u03bb\u2212 and \u03bb+; these can be computed by binary search.", "startOffset": 60, "endOffset": 64}, {"referenceID": 32, "context": "The most widely used policy for the discounted version of the multi-armed bandit problem is the Gittins index policy [33].", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "This problem has received significant attention, see the discussion in Section 1 and in [12] - however efficient solutions with provable bounds in the Bayesian setting has been elusive.", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "A classic example of such a switching cost problem can be when the costs `ij define a distance metric, which is natural in most navigational settings and was considered earlier in [40].", "startOffset": 180, "endOffset": 184}, {"referenceID": 39, "context": "Here we will provide a X approximation for that problem improving the 12-approximation provided in [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 31, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 46, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 52, "context": "Since switching costs are often used to model economic interactions in the Bandit setting, as in [12, 32, 47, 53], the adversarially ordered traversal problem is an interesting subproblem in its own right.", "startOffset": 97, "endOffset": 113}, {"referenceID": 16, "context": "In the orienteering problem [17, 13, 22], we are given a metric space G(V,E), where each node v \u2208 V has a reward ov.", "startOffset": 28, "endOffset": 40}, {"referenceID": 12, "context": "In the orienteering problem [17, 13, 22], we are given a metric space G(V,E), where each node v \u2208 V has a reward ov.", "startOffset": 28, "endOffset": 40}, {"referenceID": 21, "context": "In the orienteering problem [17, 13, 22], we are given a metric space G(V,E), where each node v \u2208 V has a reward ov.", "startOffset": 28, "endOffset": 40}, {"referenceID": 21, "context": "For any \u2208 (0, 1] the orienteering problem has a (2 + )-approximation that can be found in polynomial time [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 16, "context": "The authors of [17] showed that any c-approximation for K = 1 case (where we choose a single tour) extends to an (c+ 1)-approximation for the K > 1 case where we choose K tours11.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "We note that there exists approximations for asymmetric distances (directed graphs with triangle inequality) and other traversal problems in [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 26, "context": "We come a full circle from the discussion in Ehrenfeld [27], where the explicit connections between stopping rules for delayed feedback in the two bandit setting and scheduling policies were considered.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 2, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 1, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 2, "context": "Throttling: Find a subset S\u2032 such that \u2211 s\u2208S\u2032 Pr[Xisus \u2265 \u03bbis ] \u2208 [ 1 3 , 2 3 ] and schedule these arms.", "startOffset": 65, "endOffset": 78}, {"referenceID": 14, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 102, "endOffset": 114}, {"referenceID": 53, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 102, "endOffset": 114}, {"referenceID": 15, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 102, "endOffset": 114}, {"referenceID": 43, "context": "This could involve formulation of more strongly coupled LPs, for instance, polymatroidal formulations [15, 54, 16] or time-indexed formulations [44].", "startOffset": 144, "endOffset": 148}], "year": 2013, "abstractText": "In this paper, we consider several finite-horizon Bayesian multi-armed bandit problems with side constraints. These constraints include metric switching costs between arms, delayed feedback about observations, concave reward functions over plays, and explore-then-exploit models. These problems do not have any known optimal (or near optimal) algorithms in sub-exponential running time; several of the variants are in fact computationally intractable (NP-Hard). All of these problems violate the exchange property that the reward from the play of an arm is not contingent upon when the arm is played. This separation of scheduling and accounting of the reward is critical to almost all known analysis techniques, and yet it does not hold even in fairly basic and natural setups which we consider here. Standard index policies are suboptimal in these contexts, there has been little analysis of such policies in these settings. We present a general solution framework that yields constant factor approximation algorithms for all the above variants. Our framework proceeds by formulating a weakly coupled linear programming relaxation, whose solution yields a collection of compact policies whose execution is restricted to a single arm. These single-arm policies are made more structured to ensure polynomial time computability of the relaxation, and their execution is then carefully sequenced so that the resulting global policy is not only feasible, but also yields a constant approximation. We show that the relaxation can be solved using the same techniques as for computing index policies; in fact, the final policies we design are very close to being index policies themselves. Conceptually, we find policies that satisfy an approximate version of the exchange property, namely, that the reward from a play does not depend on time of play to within a constant factor. However such a property does not hold on a per-play basis and only holds in a global sense: We show that by restricting the state spaces of the arms, we can find single arm policies that can be combined into global (near) index policies that satisfy the approximate version of the exchange property analysis in expectation. The number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability. \u2217This paper presents a unified version of results that first appeared in three conferences: STOC \u201907 [38], ICALP \u201909 [40], and APPROX \u201913 [41], and subsumes unpublished manuscripts [39, 42]. \u2020Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA 19104. sudipto@cis.upenn.edu. Research supported by NSF awards CCF-0644119, CCF-1117216. Part of this research was performed when the author was a visitor at Google. \u2021Department of Computer Science, Duke University, Durham, NC 27708-0129., kamesh@cs.duke.edu. Supported by an Alfred P. Sloan Research Fellowship, an award from Cisco, and by NSF grants CCF-0745761, CCF-1008065, and IIS-0964560. ar X iv :1 30 6. 35 25 v2 [ cs .D S] 1 7 Ju l 2 01 3", "creator": "LaTeX with hyperref package"}}}