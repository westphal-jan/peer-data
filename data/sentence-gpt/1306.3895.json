{"id": "1306.3895", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2013", "title": "On-line PCA with Optimal Regrets", "abstract": "We carefully investigate the on-line version of PCA, where in each trial a learning algorithm plays a k-dimensional subspace, and suffers the compression loss on the next instance when projected into the chosen subspace. In this setting, we analyze two popular on-line algorithms, Gradient Descent (GD) and Exponentiated Gradient (EG). We show that both algorithms are essentially optimal in the worst-case scenario, while both algorithms can perform very well in the worst-case scenario in each case.\n\n\n\nIn this example, Gradient Descent outperforms Gradient Descent and shows a much better performing performance on all of the trials in each case.\nIn this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient Descent. In this example, Gradient Descent outperforms Gradient", "histories": [["v1", "Mon, 17 Jun 2013 15:29:00 GMT  (47kb)", "https://arxiv.org/abs/1306.3895v1", null], ["v2", "Fri, 9 May 2014 05:28:39 GMT  (47kb)", "http://arxiv.org/abs/1306.3895v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiazhong nie", "wojciech kotlowski", "manfred k warmuth"], "accepted": false, "id": "1306.3895"}, "pdf": {"name": "1306.3895.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jiazhong Nie", "Wojciech Kot", "Manfred K. Warmuth"], "emails": ["niejiazhong@cse.ucsc.edu", "manfred@cse.ucsc.edu", "vwkotlowski@cs.put.poznan.pl"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n38 95\nv2 [\ncs .L\nG ]\n9 M\nay 2\nKeywords: Online learning, regret bounds, expert setting, k-sets, PCA, Gradient Descent and Matrix Exponentiated Gradient algorithms."}, {"heading": "1 Introduction", "text": "In Principal Component Analysis (PCA), the n-dimensional data is projected / compressed onto a k-dimensional subspace so that the total quadratic compression loss is minimized. The problem of (centered) PCA is equivalent to finding the eigenvectors of the k largest eigenvalues of the covariance matrix. Here the data points xt are arbitrary unit vectors in R\nn and the instances of the PCA problem are the outer products xtx \u22a4 t . The covariance matrix C = \u2211 t xtx \u22a4 t is the sum of the instances. In this paper we consider the online version of centered PCA [16], where in each trial t = 1, . . . , T , the algorithm chooses (based on the previously observed\n\u22c6 The first and the third authors were supported by the NSF grant IIS-0917397. The second author was supported by the Fundation for Polish Science under the Homing Plus Program, co-financed by the European Regional Development Fund.\npoints x1, . . . ,xt\u22121) a subspace of dimension k described by a projection matrix P t of rank k. Then a next point xt (or instance xtx \u22a4 t ) is revealed and the algorithm incurs the \u201ccompression loss\u201d:\n\u2016xt \u2212 P txt\u201622 = tr((I \u2212 P t)xtx\u22a4t ). (1)\nThe goal is to obtain an online algorithm whose cumulative loss over trials t = 1, . . . , T is close to the cumulative loss of the best rank k projection matrix chosen in hindsight after seeing all T instances. The difference between the cumulative losses of the algorithm and the best off-line comparator is called the regret.\nThere are two main families of algorithms in online learning: The Gradient Descent (GD)[4,19] family which is based on regularizing with the squared Euclidean distance, and the Exponentiated Gradient (EG)[10] family which use the relative entropy as their regularization. The first family leads to additive updates of the parameter vector/matrix. When there are no constraints on the parameter space, then the parameter vector/matrix is a linear combination of the instances. However when there are constraints, then after the update the parameter is projected onto the constraints (via a Bregman projection w.r.t. the squared Euclidean distance). As we shall discuss in the conclusions (Section 4), projections w.r.t. inequality constraints introduce all kinds of subtle problems for GD. The second family leads to multiplicative update algorithms. For that family the non-negativity constraints on the parameters are already enforced and less projections are needed.\nIn [16], a matrix version of the multiplicative update was applied to PCA, whose regret bound is logarithmic in the dimension n. This algorithm is based on regularizing with the quantum relative entropy and is called the Matrix Exponentiated Gradient (MEG) algorithm [?]. Beginning with some of the early work on linear regression [10], it is known that multiplicative updates are especially useful when the instances are dense. In the matrix context this means that the symmetric positive semi-definite instance matrix Xt \u2208 Rn\u00d7n processed at trial t has maximum eigenvalue of say one. However in the PCA context, the instance matrices are the outer products, i.e. Xt = xtx \u22a4 t . Such instances (also called dyads) are sparse in the sense that their trace norm is one, independent of the dimension n of the instance matrix. Thus, one may suspect that MEG is not able to fully exploit the sparsity of the instance matrices. On the other hand for linear regression, GD is known to have the advantage when the instances are sparse and consistently with that, when GD is used for PCA, then its regret is bounded by a term that is independent of the dimension of the instances. The advantage of GD in the sparse case is also supported by a general survey of Mirror Descent algorithms (to which GD and MEG belong) for the case when the loss vectors (which have negative components) lie in certain symmetric norm balls [14].\nSurprisingly, the situation is quite different for PCA: We show that MEG achieves the same regret bound as GD for online PCA (despite the sparseness of the instance matrices) and the regret bounds for both algorithms are within a constant factor of our new lower bound that holds for any online PCA algorithm.\nThis surprising performance of MEG comes from the fact that the losses in the PCA case are restricted to be non-negative, and therefore our results are qualitatively different from the cases studied in [14] where loss vectors are within a p\u2212norm ball, i.e. symmetric around zero.\nActually, there are two kinds of regret bounds in the literature: bounds expressed as a function of the time horizon T and bounds that depend on an upper bound on the loss of the best comparator (which we call a loss budget following [?]). In typical applications for PCA, there exists a low dimensional subspace which captures most of the variance in the data and guarding against the worstcase loss that grows with T is not useful. We can show that when considering regret bounds as a function of a loss budget, MEG is optimal and strictly better than GD by a factor of \u221a k. This suggests that the multiplicative updates algorithm is the best choice for prediction problems, in which the parameters are mixture of projection matrices and the losses are non-negative. Note that by upper bounds on the regret, we mean upper bounds for particular algorithms. However, the matching lower bounds are always proved against any algorithm that solves the problem.\nRelated work and our contribution: The comparison of the GD and MEG algorithms has quite an extensively history (see, e.g. [10,18,15,14]). It is simplest to compare algorithms in the case when the loss is linear. Linear losses are the least convex losses and in the regret bounds, convex losses are often approximated by first-order Taylor approximations [19] which are linear, and the gradient of the loss functions as the loss vector. Note that in this case the assumptions on the gradient of the loss are typically symmetric.\nIn the case when the parameter space and the space of loss vectors are convex and symmetric, the regret bounds are as expected: EG is optimal when the parameter space is 1-norm bounded and the loss vectors are infinity norm bounded, and GD is optimal when the both spaces are 2-norm bounded [15,14]. However, none of the previous work exploits the special PCA setup, where the loss matrices (here the instances) are non-negative and sparse (see (1)). In this paper we carefully study this case.\nWe also made significant technical progress on the lower bounds. Previous lower bounds focused on the non-sparse case [16,12]. Lower bounds were proved as a function of a loss budget. In this paper we prove lower bounds as a function of time. These lower bounds harbor the budget case as a special case.\nFor the time dependent case, lower bounds were previously shown for the expert setting [6,3,1]. However, these lower bounds rely on the Central Limit Theorem and only hold in the limit (as T, n \u2192 \u221e). In contrast our lower bounds use a probabilistic bounding argument for the minimum of n random variables and the resulting bounds are non-asymptotic.\nIn summary, our contribution consists of proving tighter regret bounds for two online PCA algorithms, as well as proving lower bounds on the regret of any algorithm for online PCA. From that we get the following conclusions: MEG\u2019s and GD\u2019s regret bounds are independent of the dimension n of the problem and\nare tight within a constant factor when the time horizon T is fixed, which implies that both algorithms are essentially optimal in this case. If we fix the loss budget instead, MEG remains optimal, while GD is proved to be suboptimal.\nFurthermore, for a generalization of the PCA setting to the dense instance case, we improve the known regret bound significantly by switching from a loss version to a gain version of MEG. It turns out that MEG is optimal in the dense setting as well, whereas GD is not.\nOutline of the paper: In Section 2 we describe the MEG and GD algorithms and prove regret bounds for them. In Section 3 we prove lower bounds for both the sparse and the dense setting. We conclude with an open problem about the Incremental Off-line version of GD."}, {"heading": "2 The online algorithms", "text": "The GD and MEG algorithms are both examples of the Mirror Descent algorithm [14]. Mirror Descent updates its parameters by minimizing a trade-off between a divergence to the parameter at the end of the last trial and the loss on the current single instance, followed by a projection into the parameter set. The divergence is always a Bregman divergence. In Machine Learning these updates were discovered in [10,8]. If we choose the quantum relative entropy as the Bregman divergence, then we get the matrix version of a multiplicative update algorithm known as Matrix Exponentiated Gradient algorithm (here denoted as MEG). Similarly, the squared Frobenius norm results in an additive update algorithm known as Gradient Descent (GD).3\nSparse and dense instances: We call a symmetric positive semi-definite matrix sparse iff its trace norm (sum of the eigenvalues) is at most one. Note that the instance matrices in our online PCA setup are sparse since they are outer products of unit vectors. We also generalize our subspace learning problem to dense instance matrices, which are symmetric positive semi-definite matrices with maximum eigenvalues at most one."}, {"heading": "2.1 The MEG algorithms", "text": "In the online PCA problem, the algorithm predicts at trial t with a projection matrix P t of rank k and incurs the compression loss \u2016xt\u2212P txt\u201622 upon receiving the next point xt. This loss is equivalent to the linear loss tr ((I \u2212 P t)Xt), where Xt = xtx \u22a4 t is the instance matrix. Actually, I \u2212 P t is a complementary projection matrix which has rank m = n\u2212 k. Since\ntr ((I \u2212 P t)Xt) = tr (Xt)\u2212 tr (P tXt) , 3 We avoided the name \u201cMatrix\u201d Gradient Descent, since Gradient Descent updates are motivated by regularizing with the squared Euclidean distance and the Frobenius norm of a matrix is simply the Euclidean norm of \u201cvectorized\u201d matrix.\nthere are always two versions of the algorithms: one that produces projection matrices of rank m = n\u2212 k and minimizes the compression loss tr ((I \u2212 P t)Xt) and one that produces projection matrices of rank k and maximizes the gain tr (P tXt) (or minimizes \u2212tr (P tXt)). As we shall see, for the MEG algorithm the loss and the gain versions (referred to as Loss MEG and Gain MEG throughout the paper) are different, whereas for GD, both versions collapse to the same algorithm.\nWe allow the algorithms to choose their projection matrix at random. Thus the algorithms maintain a mixture of projection matrices of rank k or m = n\u2212k, respectively, as their parameter matrix W t. These mixtures are generalized density matrices, which are symmetric, positive definite matrices with eigenvalues upper bounded by 1, and trace equal to k or m, respectively [16]. We use Wk and Wm to denote the parameter space of all such matrices, respectively. Now we define the update of the Loss MEG and Gain MEG as follows:\nLoss MEG: W t+1 = argmin W\u2208Wm (\u2206(W ,W t) + \u03b7 tr(WXt)) ,\nGain MEG: W t+1 = argmin W\u2208Wk\n(\u2206(W ,W t)\u2212 \u03b7 tr(WXt)) ,\nwhere \u2206(W ,W \u2032) = tr ( W (logW \u2212 logW \u2032 ) is the quantum relative entropy, and \u03b7 > 0 is a learning rate. Also tr(WXt) for W \u2208 Wm is the expected loss in trial t of the random projection matrix of rank m drawn from the mixture summarized by W \u2208 Wm. Similarly, tr(WXt) for W \u2208 Wk is the expected gain in trial t of the random projection matrix of rank k drawn from the mixture summarized by W \u2208 Wk, Note that the loss version of MEG corresponds to the original MEG algorithm developed in [16], where it was shown to have the following regret bound:\nregretLoss MEG \u2264 \u221a 2Bm log n\nm +m log\nn m . (2)\nThis bound holds for any sequence of instance matrices (dense as well as sparse) for which the total compression loss of the best rank k subspace does not exceed the loss budget B. With a similar analysis, the regret of Gain MEG can be bounded by\nregretGain MEG \u2264 \u221a 2kG log n\nk .\nThis bound holds for any sequence of instance matrices (dense as well as sparse) for which the total gain of the best rank k subspace does not exceed the gain budget G.\nBudget dependent upper bounds on the regret always lead to time dependent regret bounds (as exploited in the proof of the below Theorem). Note that for PCA, the gain bound G is usually much larger than the loss bound B and therefore Gain MEG is not that useful for PCA. However as we shall see for dense instances, Gain MEG is sometimes better than Loss MEG. Incidentally, for lower bounds on the regret the implication is reversed in that time dependent regret bounds imply budget dependent regret bounds.\nTheorem 1. For sparse instance sequences of length T , the regret of the Loss MEG and Gain MEG algorithms is upper bounded by:\nregretLoss MEG \u2264 m \u221a 2T\nn log\nn m +m log n m \u2264 \u221a 2kmT n + k (3)\nregretGain MEG \u2264 \u221a 2kT log n\nk .\nSimilarly, for dense instances, the following regret bounds hold:\nregretLoss MEG \u2264 m \u221a 2T log n\nm +m log\nn\nm\nregretGain MEG \u2264 k \u221a 2T log n\nk .\nProof. The best rank k subspace picks k eigendirections of the covariance matrix C = \u2211T t=1 Xt with the largest eigenvalues. Hence the total compression loss equals the sum of the smallest m eigenvalues of C. If \u03bb1, . . . , \u03bbn denote the eigenvalues of C then:\nn\u2211\ni=1\n\u03bbi = tr(C) = T\u2211\nt=1\ntr (Xt) \u2264 { T for sparse instances,\nTn for dense instances.\nThis implies that the total compression loss of the comparator is upper bounded by Tmn and Tm, respectively. Plugging these values into (2) results in the bounds for Loss MEG. The second inequality in (3) follows from\nm log n\nm = m log\n( 1 + k\nm\n) \u2264 k.\nFor the regret bounds of Gain MEG, we use the fact that G is upper bounded by T when instances are sparse and upper bounded by kT when the instances are dense. \u2293\u2294\nNote that in light of previous results for MEG, it is actually surprising that the regret bound (3) for Loss MEG with sparse instances is independent of the dimension n of the problem.\nWe now discuss in detail which version of MEG has a better regret bound for the dense instance case. We claim that this depends on the value of k, the dimension of the chosen subspace. Consider the ratio of the regret bounds of Loss MEG over Gain MEG. When T \u2265 k, then we can ignore the m log nm term in the Loss MEG bound since this term is at most k. In this case the ratio becomes:\n\u0398\n(\u221a k2\nm2 ln nk ln nm\n) .\nWhen k \u2264 n2 , ln nm = ln(1+ km ) = \u0398( km ), and the ratio simplifies to \u0398 (\u221a ln n k\nn k\n) .\nTherefore, when nk grows, the regret bound for the Loss MEG is less than the regret bound for the Gain MEG. Similarly, when k \u2265 n2 , the ratio becomes \u0398 (\u221a n m\nln n m\n) and the regret bound for the Gain MEG is better in this case."}, {"heading": "2.2 The GD algorithm", "text": "In this section we consider the GD algorithm (see e.g. [5,19]) which is motivated by the squared Frobenius norm (The loss and gain versions are the same in this case and we use the loss version below):\nW t+1 = argmin W\u2208Wm\n( 1 2\u2016W \u2212W t\u2016 2 F + \u03b7 tr(WXt) ) .\nThis algorithm is simple and a time dependent regret bound has been proved for arbitrary convex losses [19,14]. By applying this bound to PCA we obtain:\nregretGD \u2264 ( max 1\u2264t\u2264T \u2016Xt\u2016F )\u221a T \u2016W 1 \u2212W c\u20162F = ( max 1\u2264t\u2264T \u2016Xt\u2016F )\u221a mkT n ,\nwhere W c is any comparator in the parameter space Wm. For sparse instances, \u2016Xt\u2016F = \u221a tr(XtX \u22a4 t )\u22641, the regret is bounded by \u221a mkT/n = \u221a (n\u2212 k)kT/n. This is the same as the regret bound for Loss MEG (3) except for an additional\u221a 2 factor bound for the Loss MEG. When instances are dense, \u2016Xt\u2016F \u2264 n,\nresulting in regret bound of \u221a mkT . To see that this bound is worse than the MEG bound for dense instances, we can consider the ratio of the regret bound for GD over the regret bound for the appropriate version of MEG. It is easy to check that when k \u2264 n2 , the ratio is \u0398( \u221a m k\nln( 2m k\n) ), and when k \u2265 n2 , the ratio is\n\u0398(\n\u221a k m\nln( 2k m\n) ). In both case, the regret bound for MEG is better by more than a\nconstant factor. We now conclude this section by investigating budget bounds for GD. Since GD achieves the same time horizon dependent regret bound as Loss MEG, we first conjectured that this is also the case for budget dependent regret bounds. However, this is not true: we will now show in remainder of this section a k \u221a B lower bound on the regret of GD for instance sequences with budget B. Since Loss MEG has regret at most \u221a kB in this case, this lower bound shows that GD is suboptimal by a factor of \u221a k.\nIt suffices to prove the lower bound on a restricted data set. As already observed in [16], the PCA problem has the m-set problem as a special case. In this problem, all instance matrices are diagonal (i.e. the eigenvectors are the standard basis vectors) and therefore the algorithm can restrict itself to choosing subspaces that are subsets of the standard basis vectors. In other words, PCA collapses to learning a subset of m = n \u2212 k experts. The algorithm chooses a subset of m out of n experts in each trial, the loss of all experts is given as a\nvector \u2113 \u2208 [0, 1]n, and the loss of a set is the total loss of all experts in the set. The algorithm maintains uncertainty over the m-sets by means of a weight vector w \u2208 [0, 1]n, such that \u2211n i=1 wi = m. We denote the set of all such weight vectors as Sm. The above GD algorithm for PCA specializes to the following algorithm for learning sets:\nGradient Descent step: w\u0302t+1 = wt \u2212 \u03b7\u2113t Projection step: wt+1 = argminw\u2208Sm \u2016w \u2212 w\u0302t+1\u20162.\n(4)\nThe projection step is a convex optimization problem with inequality constraints and can be analyzed using its KKT conditions. We only describe the projection step in two cases needed for the lower bound. Let wt = (w\n1, \u00b7 \u00b7 \u00b7 , wn) be the weight of GD at trial t. Our lower bound is for the sparse case. In the set problem this means that the loss vectors \u2113t are standard basis vectors. Let \u2113t = eit . In the simplest case, the descent step decreases the weight of expert i by \u03b7 and the projection step adds \u03b7n to all n weights so that the total weight remains m:\nw\u0302t+1=(w 1,. . ., wi\u2212\u03b7,. . ., wn),wt+1= ( w1+ \u03b7\nn ,. . ., wi\u2212(n\u2212 1)\u03b7 n ,. . ., wn+ \u03b7 n\n) . (5)\nTwo problems may happen with the additive adjustment: wi \u2212 \u03b7 + \u03b7n might be negative or some of the weights wj + \u03b7 n might be larger than 1. The projection step is slightly more involved when this happens. In our lower bound, we only need the following additional case:\nwt = ( w1, . . . , wi\u22121\ufe38 \ufe37\ufe37 \ufe38\nall \u2264 1 \u2212 \u03b7 n\n, wi\ufe38\ufe37\ufe37\ufe38 <n\u22121\nn \u03b7 , 1\u2212 \u03b4, . . . , 1\u2212 \u03b4\ufe38 \ufe37\ufe37 \ufe38 for \u03b4< wi n\n) .\nOne can show that in this case the projection sets wi to 0, it sets the n\u2212i weights of size 1\u2212 \u03b4 to 1, and it adds w\ni\u2212(n\u2212i)\u03b4 i\u22121 to the first i\u2212 1 weights which are not\ncapped. That is, in this case the projections produces the following updated weight vector:\nwt+1 = ( w1+\nwi \u2212 (n\u2212 i)\u03b4 i\u2212 1 , . . . , w i\u22121+ wi \u2212 (n\u2212 i)\u03b4\ni\u2212 1 , 0\ufe38\ufe37\ufe37\ufe38 capped at 0 , 1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 capped at 1\n) . (6)\nNote that the total weight of the projected weight vector wt+1 is again m. Now we are ready to give our regret lower bound for GD.\nTheorem 2. For any k \u2264 n/2 and any learning rate \u03b7, there is a sparse loss sequence for online PCA which has budget B and forces the GD algorithm (4) to incur regret at least \u2126(k \u221a B).\nProof. W.l.o.g., assume all the experts have the same initial weight m/n = (n \u2212 k)/n \u2265 1/2. Call the first k experts bad experts, the (k + 1)st expert the faulty expert and the last m\u2212 1 experts good experts. Let \u03b7\u2032 = min{\u03b7, 1}.\nThe loss sequence consists of two phases. We will show that the algorithm suffers loss at least\u2126(k\u03b7 ) in the first phase and essentially loss at leastB+\u2126(kB\u03b7)\nin the second phase. The optimum trade-off between these two term give the lower bound.\nMore precisely, in the first phase unit losses are given to bad experts and in each trial the algorithm suffers the current weight of the chosen bad expert. The phase ends when each of the good experts and the faulty expert have weights at least 1 \u2212 \u03b7\u20324m . To show that the algorithm suffers loss at least \u2126(k\u03b7 ) in this phase, first notice that for a particular bad expert, its weight decreases by at most n\u22121n \u03b7 when it receives a unit of loss and increases in all of the other trials (see (5) and (6)). So when it receives loss for the s-th time, its weight is lower bounded by\nmax\n{ m\nn \u2212 (s\u2212 1)n\u2212 1 n \u03b7, 0\n} \u2265 max { 1\n2 \u2212 (s\u2212 1)\u03b7, 0\n} (7)\nThe sum of (7) with s = 1, 2, . . . is a lower bound on the loss of the algorithm when this particular bad expert incurred a unit of loss. Note that (7) is the term of an arithmetic series that is capped from below by zero. One can show that as long as there is a constant gap between the first and last term of the summation, then the sum of these terms is at least \u2126( 1\u03b7 ). In our case, this gap is at least 1/4 since the first term (initial weight) is at least 1/2 and the last term, upper bounded by this bad expert\u2019s weight after phase one, is less than\nm\ufe38\ufe37\ufe37\ufe38 sum of the weights\nof all experts\n\u2212 m(1\u2212 \u03b7 \u2032\n4m )\n\ufe38 \ufe37\ufe37 \ufe38 sum of the weights\nof faulty and good experts\n\u2264 1 4 .\nSince we have k bad experts, we obtained a \u2126(k\u03b7 ) lower bound on the loss of the algorithm during the first phase.\nWe now describe the second phase which lasts for B rounds, where each round consists of several trials. At the beginning of each round, the faulty expert receives one unit of loss. Its weight after the gradient descent step is at most 1\u2212\u03b7 and after the projection step it can be shown to be at most max{1\u2212\u03b7/2, 0} (see (6)). Notice that after the first trial of each round, all good experts will have weight 1 since they are at least 1\u2212 \u03b7\u20324m after phase one. In the following trials of this round, unit losses are given to bad experts until the faulty expert recovers its weight to at least 1 \u2212 \u03b7 \u2032\n4m . Since all the weights of good experts are capped at 1, the re-balancing of weights only occurs between the faulty and the bad experts. This means that in each trial, the faulty expert can only recover at most 1/(k + 1) of the loss incurred by the algorithm in this trial. Thus we can lower bound the loss of algorithm in a given round as follows:\n1\u2212 \u03b7 \u2032\n4m +(k+1)\n( 1\u2212 \u03b7 \u2032\n4m \u2212max\n{ 1\u2212\u03b7 2 , 0 }) \u2265 1\u2212 \u03b7 \u2032 4m + k + 1 4 min{\u03b7, 1} = 1+\u2126(k\u03b7\u2032).\nAfter B such rounds, algorithm suffers loss at least B +\u2126(kB\u03b7\u2032): When \u03b7 \u2265 1, this is B + \u2126(kB) and when \u03b7 \u2264 1, then summing up the bounds of the two\nphases, gives an \u2126(k/\u03b7)+B+\u2126(Bk\u03b7) lower bound on the loss of the algorithm. The latter is minimized at \u03b7 = \u0398(1/ \u221a B) and for this choice of \u03b7, the algorithm suffers loss at least B+\u2126(k \u221a B). The theorem now follows, since the best off-line m-set for the loss sequence consists of the faulty expert, which suffers total loss B, and all m\u2212 1 good experts, which incur no loss. \u2293\u2294"}, {"heading": "3 Lower bounds and optimal algorithms", "text": "In the previous section, we showed a lower bound on the regret of GD as a function of the budget B of the sequence. In this section we show regret bounds for any algorithm that solves the problem. In particular, we show regret lower bounds for online PCA and its generalization to the dense instance case. As argued in Section 2.2, it suffices to prove our lower bounds for the m-set problem which is the vector version of online PCA and its generalization to dense instances. We prove lower bound on the minimax regret, i.e. the minimum worst case regret any algorithm can achieve against the best set:\nmin alg. A with wt \u2208 Sm max sparse/dense loss seq. \u21131...T of length T\nR(A, \u21131...T ) loss of alg. A - loss of best set\non loss sequence \u21131...T\n.\nRecall that Sm were vectors in [0, 1] n of total weight m that represent mixtures of sets of size m. Our lower bounds will match the uppers bounds on the regret of MEG (within constant factors) that we proved in the previous section for both online PCA and its generalization to dense instances. The lower bounds rely on the following probabilistic bounding technique for the minimax regret:\nmin alg. A with wt \u2208 Sm max loss seq. \u21131...T of length T R(A, \u21131...T ) \u2265 min alg. A with wt \u2208 Sm E\u21131...T\u223cP [ R(A, \u21131...T ) ]\n= min alg. A with wt \u2208 Sm\nE\u21131...T\u223cP [ LA ]\u2212 EL\u223cP [ LC ] , (8)\nwhere P is any distribution on loss sequences, and LA and LC are the cumulative losses of the algorithm and the best m-set, respectively."}, {"heading": "3.1 Lower bounds for PCA with sparse instances", "text": "Recall that for the vectorized version of PCA, the loss vectors \u2113t are restricted to be standard basis vectors. We start with the following technical lemma for two experts.\nLemma 1. Let p \u2208 [0, 1] be such that p \u2264 1/4 and Tp \u2265 1/2. Assume that in a two expert game, one of the experts is randomly chosen to suffer one unit of loss with probability 2p in each trial, and with probability 1\u2212 2p none of the experts suffers any loss. Then, after T independent trials,\nE [ Loss of the winner ] \u2264 Tp\u2212 c \u221a Tp,\nfor a constant c independent of T and p.\nDue to the space limit, we omit the proof of this lemma. We are now ready to prove a lower bound for PCA. We first consider the case when k \u2264 n2 .\nTheorem 3. For T \u2265 k and k \u2264 n2 , in the T trial online PCA problem with sparse instances, any online algorithm suffers worst case regret at least \u2126( \u221a kT ).\nProof. At each trial, a randomly chosen expert out of the first 2k experts receives a unit of loss. To show an upper bound on the loss of the comparator, we group these 2k experts into k pairs and notice that the losses of each expert pair have a joint distribution as described in Lemma 1 with p = 12k . Hence, the expected loss\nof the winner in each pair is at most T/2k\u2212 c \u221a T/k, and the total expected loss for the k winners from all k pairs is upper bounded by T/2\u2212 c \u221a kT . Since the last n\u22122k experts are loss-free, this is also an upper bound on the expected loss of the comparator, because the comparator will pick n\u22122k loss-free experts and k best experts among the remaining 2k experts. On the other hand, since losses are generated independently between trials, any online algorithm suffers loss at least T/2. Taking the difference between two bounds concludes the proof. \u2293\u2294 Noting that m \u221a ln(n/m)T/n) \u2264 m \u221a (k/m)T/n \u2264 \u221a kT , the lower bound in Theorem 3 matches the upper bound of Loss MEG algorithm in Theorem 1 for the case k \u2264 n2 . For the case k \u2265 n2 , we need the following lemma, which is a generalization of Lemma 1 to n experts. In the proof we upper bound the minimum loss of the experts by the loss of the winner of a tournament among the experts. The tournament winner does not necessarily have the lowest loss. However as we shall see later, its expected loss is close enough to the expected loss of the best expert to make this bounding techniques useful for obtaining lower bounds on the regret.\nLemma 2. Choose any n, S and T , such that n = 2S and S divides T . If the loss sequence of length T is generated from a distribution P, such that: \u2013 at each trial t, the distribution of losses on n experts is exchangeable, \u2013 the distribution of losses is i.i.d. between trials,\nthen,\nE [ Minimum loss of n experts in T trials ]\n\u2264 S E [ Loss of the winner among two experts in T/S trials ] .\nProof. Due to the space limit, we only sketch the proof. The key idea in the proof is to upper bound the loss of the best expert by the loss of the expert that wins a tournament with S rounds. In each round, the experts are paired and compared against their partners, using the sum of their losses in the next T/S consecutive trails. The winner of each local pair competition survives to the next round. The winners are again paired and winners among those continue with the tournament until one expert is left from the original n = 2S experts. We call this expert the tournament winner. The expected loss of the best expert is upper bounded by the expected loss of the tournament winner, which curiously enough\nequals the number of rounds times the expected loss of the two expert case:\nE [ Minimum loss of all n experts in all T trials ]\n\u2264 E [\nLoss of the tournament winner in the\nS rounds tournament among the n = 2S experts\n]\nsince expectations sum =\n\u2211\nrounds 1 \u2264 s \u2264 S\nE [ Loss of tournament winner in round s ]\ni.i.d. loss btw. trials = S E [ Loss of the tournament winner in one round ]\ndef. of local tournament = S E [ Loss of winner among two experts in T/S trials ] .\nThe last equality would be trivial if the distribution P on the sequence of loss vectors was i.i.d. between experts. An additional argument is needed to show the equality with the weaker assumption of exchangeability. \u2293\u2294 We now consider the uncommon case when k \u2265 n2 : Theorem 4. For T \u2265 n log2(n/m) and k \u2265 n2 , in the T trial online PCA problem with sparse instances, any online algorithm suffers worst case regret at least \u2126(m \u221a ln(n/m)T/n).\nProof. At each trial, a randomly chosen expert out of n experts receives a unit of loss. To show an upper bound on the loss of the comparator, we partition the n experts into m groups and notice that the losses of the n/m experts in each group are exchangeable. By applying Lemma 2 to each group, we obtain:\nE [ Loss of the winner in a given group in T trials ]\n\u2264 log2( n\nm ) E\n[ Loss of winner of two experts in\nT\nlog2( n m )\ntrials ] . (9)\nWe bound the last expectation that deals with the 2 experts case by applying Lemma 1 with p = 1/n and T/ log2(n/m). This lets us replace the expectation by the upper bound\nT log2(n/m)n \u2212 \u221a\nT\nlog2(n/m)n .\nPlugging this into (9) gives a T/n \u2212 \u221a log2(n/m)T/n upper bound on the expected loss of a winner in a given group. We upper bound the expected loss of the comparator by the total loss of m winners from the m groups, which in expectation is at most mT/n\u2212m \u221a log2(n/m)T/n.\nFinally the loss of the algorithm is bounded as follows. Since every expert suffers loss 1/n in expectation at each trail and losses are i.i.d. between trials, any online algorithm suffers loss at least mT/n. This concludes the proof. \u2293\u2294\nCombining this lower bound with the upper bounds proved in Section 2.1 on the regret of Loss MEG for the sparse instance case results in the following corollary:\nCorollary 1. For online PCA with sparse instances, the regret \u0398(m \u221a\nT ln(n/m) n )\nof Loss MEG is within a constant factor of the minimax regret."}, {"heading": "3.2 Lower bound for PCA with dense instances", "text": "The following lower bound again employs Lemma 2 which was proved using a tournament.\nTheorem 5. For T \u2265 log2( dmin{k,m} ), in the T trial online PCA problem with dense instances, any online algorithm suffers worst case regret at least\n\u2126(m \u221a ln(n/m)T ) when m \u2264 n\n2 or \u2126(k\n\u221a ln(n/k)T ) when m \u2265 n\n2 .\nProof. The proof is similar to the proof of Theorem 4, except that at each trial, unit losses are independently given to all the experts with probability 12 . For such a distribution over losses, any algorithm suffers cumulative loss at least mT/2 in expectation. We now upper bound the comparator\u2019s expected loss by distinguishing two cases: When m \u2264 n/2, we group the experts into m groups and upper bound the comparator loss using the m winners, one from each of the groups. This gives an mT/2\u2212 cm \u221a ln(n/m)T upper bound, and results in a \u2126(m \u221a\nln(n/m)T ) lower bound for the regret. When m \u2265 n/2, we group the experts into k groups and consider a loser out of each group, i.e. the expert which suffers the largest loss in each group. One can flip around the content of Lemma 2 to show that the loser\u2019s loss in a group of n/k experts is lower bounded by T/2 + c \u221a ln(n/k)T , so that the expected loss of all k losers is lower bounded by kT/2+ ck \u221a ln(n/k)T . The claimed regret bounds now follows from the fact that the cumulative loss of the comparator is upper bounded by the total expected loss of all experts (nT/2) minus the total expected loss of all k losers. This completes the sketch proof. \u2293\u2294\nCombining this lower bound with the upper bounds on the regret of Loss MEG and Gain MEG for dense instance case proved in Section 2 gives a following corollary, which basically states that the Loss MEG is optimal for m \u2264 n2 while the Gain MEG is optimal for m \u2265 n2 .\nCorollary 2. Consider online PCA with dense instances.\n\u2013 When m \u2264 n2 , the regret \u0398(m \u221a\nT log nm ) of Loss MEG is within a constant factor of the minimax regret. \u2013 When m \u2265 n2 , the regret \u0398(k \u221a T log nk ) of Gain MEG is within a constant\nfactor of the minimax regret.\nMinimax regret for sequences with a budget. One can also show the minimax regret for a prediction game in which the budget B is fixed, rather than the time horizon T . In this case, no matter if the instances are dense or sparse, we get the following corollary establishing the optimality of Loss MEG:\nCorollary 3. Let B \u2265 m log2 nm . For online PCA with both sparse and dense instances, the regret \u0398( \u221a m ln(n/m)B) of Loss MEG is within a constant factor the minimax budget regret.\nProof. Since the instance matrices have eigenvalues bounded by one, the minimax regret is upper bounded by O( \u221a m ln(n/m)B), the regret bound of the Loss MEG algorithm given in (2), Section 2.1. On the other hand, we now reason that for any algorithm we can construct a sparse instance sequence of budget B incurring regret at least \u2126( \u221a m ln(n/m)B). This instance sequence is constructed via Theorem 3 and Theorem 4: For any algorithm, these theorems provide a sparse instance sequence of length T with regret at least \u2126(m \u221a\nT ln(n/m) n ).\nWe apply these theorems with T = nmB \u2265 n log2 nm . Since the produced sequence is sparse and has length nmB, its budget is at most B. Finally plugging T = nmB into the regret bounds guaranteed by the theorems results in the regret\n\u2126( \u221a m ln(n/m)B). \u2293\u2294"}, {"heading": "4 Conclusion", "text": "We showed in this paper that GD is non-optimal for various problems. However, our lower bounds are for the Mirror Descent version of GD that trades off the loss on the last example with a divergence to the last capped parameter matrix. There is an alternate algorithm: the Incremental Off-line [2] or Follow the Perturbed Leader algorithm [9] that in its motivation trades off the total loss on all examples against the divergence to the initial distribution. Note that both versions follow their update with a projection into the parameter space. We conjecture that the Incremental Off-line version of GD is strictly better than the commonly studied Mirror Descent version. The advantage of processing all examples versus just the last one has now shown up in a number of different contexts: in Boosting it led to better algorithms [17] and it also was crucial for obtaining a kernelizable online PCA algorithm [13]. When there are only equality constraints and the loss is linear, then the two versions of the algorithm are provably the same (See e.g. [7]). However when there are inequality constraints that are not enforced by the divergence, then the projection steps of the Mirror Descent version of the algorithm \u201cforgets\u201d information about the past examples whenever the algorithm runs into the boundaries of the inequality constraints.\nMore concretely, we conjecture that the Incremental Off-line version of GD has the optimal budget regret bound for online PCA (as Mirror Descent MEG does which enforces the non-negativity constraints with its divergence). If this conjecture is true, then this would be the first case where there is a provable gap between processing just the last versus all past examples. If the conjecture is false, then Mirror Descent MEG is truly better than both versions of GD. Both outcomes would be in important step forward in our understanding of online algorithms. Note that our k \u221a B lower bound for GD specifically exploits the forgetting effect and consequently only applies to the Mirror Descent version of the GD algorithm."}], "references": [{"title": "A stochastic view of optimal regret through minimax duality", "author": ["J. Abernethy", "A. Agarwal", "P.L. Bartlett", "A. Rakhlin"], "venue": "COLT", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["K.S. Azoury", "M.K. Warmuth"], "venue": "Machine Learning 43(3), 211\u2013246", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "J. ACM 44(3),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Worst-case quadratic loss bounds for prediction using linear functions and gradient descent", "author": ["N. Cesa-Bianchi", "P.M. Long", "M.K. Warmuth"], "venue": "IEEE Trans. Neural Netw. Learning Syst. 7(3), 604\u2013619", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Worst-case quadratic loss bounds for prediction using linear functions and gradient descent", "author": ["N. Cesa-Bianchi", "P.M. Long", "M.K. Warmuth"], "venue": "IEEE Trans. Neural Netw. Learning Syst. 7(3), 604\u2013619", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning permutations with exponential weights", "author": ["D.P. Helmbold", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research 10, 1705\u20131736", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Tracking the best linear predictor", "author": ["M. Herbster", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research 1, 281\u2013309", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient algorithms for online decision problems", "author": ["A.T. Kalai", "S. Vempala"], "venue": "J. Comput. Syst. Sci. 71(3), 291\u2013307", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Inf. Comput. 132(1), 1\u201363", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Combining strategies efficiently: high-quality decisions from conflicting advice", "author": ["W.M. Koolen"], "venue": "Ph.D. thesis, Institute of Logic, Language and Computation (ILLC), University of Amsterdam", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Hedging structured concepts", "author": ["W.M. Koolen", "M.K. Warmuth", "J. Kivinen"], "venue": "COLT. pp. 93\u2013105", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Online kernel pca with entropic matrix updates", "author": ["D. Kuzmin", "M.K. Warmuth"], "venue": "ICML. pp. 465\u2013472", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "On the universality of online mirror descent", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "NIPS. pp. 2645\u20132653", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex games in banach spaces", "author": ["K. Sridharan", "A. Tewari"], "venue": "Proceedings of the 23nd Annual Conference on Learning Theory (COLT)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension", "author": ["M.K. Warmuth", "D. Kuzmin"], "venue": "Journal of Machine Learning Research 9, 2287\u20132320", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Totally corrective boosting algorithms that maximize the margin", "author": ["M.K. Warmuth", "J. Liao", "G. R\u00e4tsch"], "venue": "ICML. pp. 1001\u20131008", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Leaving the span", "author": ["M.K. Warmuth", "S.V.N. Vishwanathan"], "venue": "COLT. pp. 366\u2013381", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "Fawcett, T., Mishra, N. (eds.) ICML. pp. 928\u2013936. AAAI Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 15, "context": "In this paper we consider the online version of centered PCA [16], where in each trial t = 1, .", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "There are two main families of algorithms in online learning: The Gradient Descent (GD)[4,19] family which is based on regularizing with the squared Euclidean distance, and the Exponentiated Gradient (EG)[10] family which use the relative entropy as their regularization.", "startOffset": 87, "endOffset": 93}, {"referenceID": 18, "context": "There are two main families of algorithms in online learning: The Gradient Descent (GD)[4,19] family which is based on regularizing with the squared Euclidean distance, and the Exponentiated Gradient (EG)[10] family which use the relative entropy as their regularization.", "startOffset": 87, "endOffset": 93}, {"referenceID": 9, "context": "There are two main families of algorithms in online learning: The Gradient Descent (GD)[4,19] family which is based on regularizing with the squared Euclidean distance, and the Exponentiated Gradient (EG)[10] family which use the relative entropy as their regularization.", "startOffset": 204, "endOffset": 208}, {"referenceID": 15, "context": "In [16], a matrix version of the multiplicative update was applied to PCA, whose regret bound is logarithmic in the dimension n.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Beginning with some of the early work on linear regression [10], it is known that multiplicative updates are especially useful when the instances are dense.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "The advantage of GD in the sparse case is also supported by a general survey of Mirror Descent algorithms (to which GD and MEG belong) for the case when the loss vectors (which have negative components) lie in certain symmetric norm balls [14].", "startOffset": 239, "endOffset": 243}, {"referenceID": 13, "context": "This surprising performance of MEG comes from the fact that the losses in the PCA case are restricted to be non-negative, and therefore our results are qualitatively different from the cases studied in [14] where loss vectors are within a p\u2212norm ball, i.", "startOffset": 202, "endOffset": 206}, {"referenceID": 9, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 17, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 13, "context": "[10,18,15,14]).", "startOffset": 0, "endOffset": 13}, {"referenceID": 18, "context": "Linear losses are the least convex losses and in the regret bounds, convex losses are often approximated by first-order Taylor approximations [19] which are linear, and the gradient of the loss functions as the loss vector.", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "In the case when the parameter space and the space of loss vectors are convex and symmetric, the regret bounds are as expected: EG is optimal when the parameter space is 1-norm bounded and the loss vectors are infinity norm bounded, and GD is optimal when the both spaces are 2-norm bounded [15,14].", "startOffset": 291, "endOffset": 298}, {"referenceID": 13, "context": "In the case when the parameter space and the space of loss vectors are convex and symmetric, the regret bounds are as expected: EG is optimal when the parameter space is 1-norm bounded and the loss vectors are infinity norm bounded, and GD is optimal when the both spaces are 2-norm bounded [15,14].", "startOffset": 291, "endOffset": 298}, {"referenceID": 15, "context": "Previous lower bounds focused on the non-sparse case [16,12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 11, "context": "Previous lower bounds focused on the non-sparse case [16,12].", "startOffset": 53, "endOffset": 60}, {"referenceID": 5, "context": "For the time dependent case, lower bounds were previously shown for the expert setting [6,3,1].", "startOffset": 87, "endOffset": 94}, {"referenceID": 2, "context": "For the time dependent case, lower bounds were previously shown for the expert setting [6,3,1].", "startOffset": 87, "endOffset": 94}, {"referenceID": 0, "context": "For the time dependent case, lower bounds were previously shown for the expert setting [6,3,1].", "startOffset": 87, "endOffset": 94}, {"referenceID": 13, "context": "The GD and MEG algorithms are both examples of the Mirror Descent algorithm [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "In Machine Learning these updates were discovered in [10,8].", "startOffset": 53, "endOffset": 59}, {"referenceID": 7, "context": "In Machine Learning these updates were discovered in [10,8].", "startOffset": 53, "endOffset": 59}, {"referenceID": 15, "context": "These mixtures are generalized density matrices, which are symmetric, positive definite matrices with eigenvalues upper bounded by 1, and trace equal to k or m, respectively [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "Similarly, tr(WXt) for W \u2208 Wk is the expected gain in trial t of the random projection matrix of rank k drawn from the mixture summarized by W \u2208 Wk, Note that the loss version of MEG corresponds to the original MEG algorithm developed in [16], where it was shown to have the following regret bound:", "startOffset": 238, "endOffset": 242}, {"referenceID": 4, "context": "[5,19]) which is motivated by the squared Frobenius norm (The loss and gain versions are the same in this case and we use the loss version below):", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "[5,19]) which is motivated by the squared Frobenius norm (The loss and gain versions are the same in this case and we use the loss version below):", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "This algorithm is simple and a time dependent regret bound has been proved for arbitrary convex losses [19,14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 13, "context": "This algorithm is simple and a time dependent regret bound has been proved for arbitrary convex losses [19,14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 15, "context": "As already observed in [16], the PCA problem has the m-set problem as a special case.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "vector l \u2208 [0, 1], and the loss of a set is the total loss of all experts in the set.", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "The algorithm maintains uncertainty over the m-sets by means of a weight vector w \u2208 [0, 1], such that \u2211n i=1 wi = m.", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "Recall that Sm were vectors in [0, 1] n of total weight m that represent mixtures of sets of size m.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "Let p \u2208 [0, 1] be such that p \u2264 1/4 and Tp \u2265 1/2.", "startOffset": 8, "endOffset": 14}, {"referenceID": 1, "context": "There is an alternate algorithm: the Incremental Off-line [2] or Follow the Perturbed Leader algorithm [9] that in its motivation trades off the total loss on all examples against the divergence to the initial distribution.", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "There is an alternate algorithm: the Incremental Off-line [2] or Follow the Perturbed Leader algorithm [9] that in its motivation trades off the total loss on all examples against the divergence to the initial distribution.", "startOffset": 103, "endOffset": 106}, {"referenceID": 16, "context": "The advantage of processing all examples versus just the last one has now shown up in a number of different contexts: in Boosting it led to better algorithms [17] and it also was crucial for obtaining a kernelizable online PCA algorithm [13].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "The advantage of processing all examples versus just the last one has now shown up in a number of different contexts: in Boosting it led to better algorithms [17] and it also was crucial for obtaining a kernelizable online PCA algorithm [13].", "startOffset": 237, "endOffset": 241}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "We carefully investigate the online version of PCA, where in each trial a learning algorithm plays a k-dimensional subspace, and suffers the compression loss on the next instance when projected into the chosen subspace. In this setting, we give regret bounds for two popular online algorithms, Gradient Descent (GD) and Matrix Exponentiated Gradient (MEG). We show that both algorithms are essentially optimal in the worst-case when the regret is expressed as a function of the number of trials. This comes as a surprise, since MEG is commonly believed to perform sub-optimally when the instances are sparse. This different behavior of MEG for PCA is mainly related to the non-negativity of the loss in this case, which makes the PCA setting qualitatively different from other settings studied in the literature. Furthermore, we show that when considering regret bounds as a function of a loss budget, MEG remains optimal and strictly outperforms GD. Next, we study a generalization of the online PCA problem, in which the Nature is allowed to play with dense instances, which are positive matrices with bounded largest eigenvalue. Again we can show that MEG is optimal and strictly better than GD in this setting.", "creator": "LaTeX with hyperref package"}}}