{"id": "1306.5554", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2013", "title": "Correlated random features for fast semi-supervised learning", "abstract": "This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, XNV applies multiview regression using Canonical Correlation Analysis (CCA) on unlabeled data to bias the regression towards useful features. It has been shown that, if the views contains accurate estimators, CCA regression can substantially reduce variance with a minimal increase in bias. Random views are justified by recent theoretical and empirical work showing that regression with random features closely approximates kernel regression, implying that random views can be expected to contain accurate estimators. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. In fact, in the recent study, we identified a significant reduction in error in the CCA regression when the average accuracy of any data collection was significantly higher than the average expected accuracy for all datasets in our dataset (Figure 7). Our results provide evidence that the most important step in improving predictive performance is to improve CCA performance and to reduce the variance of its estimation in models by applying this technique. In addition, the CCA is widely used in the research and development of large-scale machine learning and deep learning models. Therefore, the current approach to optimizing machine learning models is to consider the number of possible steps in the algorithm to optimize a machine learning model, such as the average accuracy of the model to that of its model. Second, XNV optimizes the statistical power used to estimate the accuracy of an algorithm using the number of variables that are normally included in the analysis. The algorithms are usually implemented using the assumption that these variables are always included in the analysis, but are used in the analysis. The analysis is also possible if the estimated accuracy of a model is a significant factor. Finally, our findings suggest that the optimization of model performance is best applied when the data collection is performed with an approximation of the model to that of its model. This is particularly important when comparing a model to a particular dataset (where the probability of performing the optimization of the model is less than half the probability that an estimate of a model is not well defined). Finally, our findings show that, if the predictions of the model are not accurate, the predictions of the model will be significantly overestimated", "histories": [["v1", "Mon, 24 Jun 2013 09:49:08 GMT  (327kb)", "https://arxiv.org/abs/1306.5554v1", "15 pages, 3 figures, 6 tables"], ["v2", "Tue, 5 Nov 2013 11:28:33 GMT  (639kb)", "http://arxiv.org/abs/1306.5554v2", "15 pages, 3 figures, 6 tables"]], "COMMENTS": "15 pages, 3 figures, 6 tables", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["brian mcwilliams", "david balduzzi", "joachim m buhmann"], "accepted": true, "id": "1306.5554"}, "pdf": {"name": "1306.5554.pdf", "metadata": {"source": "CRF", "title": "Correlated random features for fast semi-supervised learning", "authors": ["Brian McWilliams"], "emails": ["brian.mcwilliams@inf.ethz.ch", "david.balduzzi@inf.ethz.ch", "jbuhmann@inf.ethz.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 6.\n55 54\nv2 [\nst at\n.M L"}, {"heading": "1 Introduction", "text": "As the volume of data collected in the social and natural sciences increases, the computational cost of learning from large datasets has become an important consideration. For learning non-linear relationships, kernel methods achieve excellent performance but na\u0131\u0308vely require operations cubic in the number of training points.\nRandomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2]. Random features have been introduced to approximate kernel machines when the number of training examples is very large, rendering exact kernel computation intractable. Among several different approaches, the Nystro\u0308m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].\nA second problem arising with large datasets concerns obtaining labels, which often requires a domain expert to manually assign a label to each instance which can be very expensive \u2013 requiring significant investments of both time and money \u2013 as the size of the dataset increases. Semi-supervised learning aims to improve prediction by extracting useful structure from the unlabeled data points and using this in conjunction with a function learned on a small number of labeled points.\nContribution. This paper proposes a new semi-supervised algorithm for regression and classification, Correlated Nystro\u0308m Views (XNV), that addresses both problems simultaneously. The method\nconsists in essentially two steps. First, we construct two \u201cviews\u201d using random features. We investigate two ways of doing so: one based on the Nystro\u0308m method and another based on random Fourier features (so-called kitchen sinks) [2, 6]. It turns out that the Nystro\u0308m method almost always outperforms Fourier features by a quite large margin, so we only report these results in the main text.\nThe second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views. Intuitively, if both views contain accurate estimators, then penalizing uncorrelated features reduces variance without increasing the bias by much. Recent theoretical work by Bach [5] shows that Nystro\u0308m views can be expected to contain accurate estimators.\nWe perform an extensive evaluation of XNV on 18 real-world datasets, comparing against a modified version of the SSSL (simple semi-supervised learning) algorithm introduced in [10]. We find that XNV outperforms SSSL by around 10-15% on average, depending on the number of labeled points available, see \u00a73. We also find that the performance of XNV exhibits dramatically less variability than SSSL, with a typical reduction of 30%.\nWe chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11]. However, since SSSL does not scale up to large sets of unlabeled data, we modify SSSL by introducing a Nystro\u0308m approximation to improve runtime performance. This reduces runtime by a factor of \u00d71000 on N = 10, 000 points, with further improvements as N increases. Our approximate version of SSSL outperforms kernel ridge regression (KRR) by > 50% on the 18 datasets on average, in line with the results reported in [10], suggesting that we lose little by replacing the exact SSSL with our approximate implementation.\nRelated work. Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14]. Our algorithm builds on an elegant proposal for multi-view regression introduced in [7]. Surprisingly, despite guaranteeing improved prediction performance under a relatively weak assumption on the views, CCA regression has not been widely used since its proposal \u2013 to the best of our knowledge this is first empirical evaluation of multi-view regression\u2019s performance. A possible reason for this is the difficulty in obtaining naturally occurring data equipped with multiple views that can be shown to satisfy the multi-view assumption. We overcome this problem by constructing random views that satisfy the assumption by design."}, {"heading": "2 Method", "text": "This section introduces XNV, our semi-supervised learning method. The method builds on two main ideas. First, given two equally useful but sufficiently different views on a dataset, penalizing regression using the canonical norm (computed via CCA), can substantially improve performance [7]. The second is the Nystro\u0308m method for constructing random features [1], which we use to construct the views."}, {"heading": "2.1 Multi-view regression", "text": "Suppose we have data T = ( (x1, y1), . . . , (xn, yn) ) for xi \u2208 RD and yi \u2208 R, sampled according to joint distribution P (x, y). Further suppose we have two views on the data\nz(\u03bd) : RD \u2212\u2192 H(\u03bd) = RM : x 7\u2192 z(\u03bd)(x) =: z(\u03bd) for \u03bd \u2208 {1, 2}.\nWe make the following assumption about linear regressors which can be learned on these views. Assumption 1 (Multi-view assumption [7]). Define mean-squared error loss function \u2113(g,x, y) = (g(x) \u2212 y)2 and let loss(g) := EP \u2113(g(x), y). Further let L(Z) denote the space of linear maps from a linear space Z to the reals, and define:\nf (\u03bd) := argmin g\u2208L(H(\u03bd)) loss(g) for \u03bd \u2208 {1, 2} and f := argmin g\u2208L(H(1)\u2295H(2)) loss(g).\nThe multi-view assumption is that\nloss ( f (\u03bd) ) \u2212 loss(f) \u2264 \u01eb for \u03bd \u2208 {1, 2}. (1)\nIn short, the best predictor in each view is within \u01eb of the best overall predictor.\nCanonical correlation analysis. Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables. CCA finds bases for the two sets of variables such that the correlation between projections onto the bases are maximized. The first pair of canonical basis vectors, ( b (1) 1 ,b (2) 1 ) is found by solving:\nargmax b(1),b(2)\u2208RM\ncorr ( b(1)\u22a4z(1),b(2)\u22a4z(2) ) . (2)\nSubsequent pairs are found by maximizing correlations subject to being orthogonal to previously found pairs. The result of performing CCA is two sets of bases, B(\u03bd) = [ b (\u03bd) 1 , . . . ,b (\u03bd) M ] for\n\u03bd \u2208 {1, 2}, such that the projection of z(\u03bd) onto B(\u03bd) which we denote z\u0304(\u03bd) satisfies\n1. Orthogonality: ET [ z\u0304 (\u03bd)\u22a4 j z\u0304 (\u03bd) k ] = \u03b4jk , where \u03b4jk is the Kronecker delta, and 2. Correlation: ET [ z\u0304 (1)\u22a4 j z\u0304 (2) k ] = \u03bbj \u00b7 \u03b4jk where w.l.o.g. we assume 1 \u2265 \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0.\n\u03bbj is referred to as the jth canonical correlation coefficient.\nDefinition 1 (canonical norm). Given vector z\u0304(\u03bd) in the canonical basis, define its canonical norm as\n\u2016z\u0304(\u03bd)\u2016CCA :=\n\u221a\u221a\u221a\u221a D\u2211\nj=1\n1\u2212 \u03bbj \u03bbj\n( z\u0304 (\u03bd) j )2 .\nCanonical ridge regression. Assume we observe n pairs of views coupled with real valued labels{ z (1) i , z (2) i , yi }n i=1 , canonical ridge regression finds coefficients \u03b2\u0302 (\u03bd) = [ \u03b2\u0302 (\u03bd) 1 , . . . , \u03b2\u0302 (\u03bd) M ]\u22a4 such that\n\u03b2\u0302 (\u03bd)\n:= argmin \u03b2\n1\nn\nn\u2211\ni=1\n( yi \u2212 \u03b2 (\u03bd) \u22a4z\u0304 (\u03bd) i )2 + \u2016\u03b2(\u03bd)\u20162CCA. (3)\nThe resulting estimator, referred to as the canonical shrinkage estimator, is\n\u03b2\u0302 (\u03bd) j =\n\u03bbj\nn\nn\u2211\ni=1\nz\u0304 (\u03bd) i,j yi. (4)\nPenalizing with the canonical norm biases the optimization towards features that are highly correlated across the views. Good regressors exist in both views by Assumption 1. Thus, intuitively, penalizing uncorrelated features significantly reduces variance, without increasing the bias by much. More formally:\nTheorem 1 (canonical ridge regression, [7]). Assume E[y2|x] \u2264 1 and that Assumption 1 holds. Let f (\u03bd)\n\u03b2\u0302 denote the estimator constructed with the canonical shrinkage estimator, Eq. (4), on training\nset T , and let f denote the best linear predictor across both views. For \u03bd \u2208 {1, 2} we have\nET [loss(f (\u03bd)\n\u03b2\u0302 )]\u2212 loss(f) \u2264 5\u01eb+\n\u2211M j=1 \u03bb 2 j\nn\nwhere the expectation is with respect to training sets T sampled from P (x, y).\nThe first term, 5\u01eb, bounds the bias of the canonical estimator, whereas the second, 1n \u2211\n\u03bb2j bounds the variance. The \u2211 \u03bb2j can be thought of as a measure of the \u201cintrinsic dimensionality\u201d of the unlabeled data, which controls the rate of convergence. If the canonical correlation coefficients decay sufficiently rapidly, then the increase in bias is more than made up for by the decrease in variance."}, {"heading": "2.2 Constructing random views", "text": "We construct two views satisfying Assumption 1 in expectation, see Theorem 3 below. To ensure our method scales to large sets of unlabeled data, we use random features generated using the Nystro\u0308m method [1].\nSuppose we have data {xi}Ni=1. When N is very large, constructing and manipulating the N \u00d7 N Gram matrix [K]ii\u2032 = \u3008\u03c6(xi), \u03c6(xi\u2032 )\u3009 = \u03ba(xi,xi\u2032) is computationally expensive. Where here, \u03c6(x) defines a mapping from RD to a high dimensional feature space and \u03ba(\u00b7, \u00b7) is a positive semi-definite kernel function.\nThe idea behind random features is to instead define a lower-dimensional mapping, z(xi) : RD \u2192 R\nM through a random sampling scheme such that [K]ii\u2032 \u2248 z(xi) \u22a4z(xi\u2032 ) [6, 15]. Thus, using random features, non-linear functions in x can be learned as linear functions in z(x) leading to significant computational speed-ups. Here we give a brief overview of the Nystro\u0308m method, which uses random subsampling to approximate the Gram matrix.\nThe Nystro\u0308m method. Fix an M \u226a N and randomly (uniformly) sample a subset M = {x\u0302i}Mi=1 of M points from the data {xi}Ni=1. Let K\u0302 denote the Gram matrix [K\u0302]ii\u2032 where i, i\n\u2032 \u2208 M. The Nystro\u0308m method [1, 3] constructs a low-rank approximation to the Gram matrix as\nK \u2248 K\u0303 := N\u2211\ni=1\nN\u2211\ni\u2032=1\n[\u03ba(xi, x\u03021), . . . , \u03ba(xi, x\u0302M )] K\u0302 \u2020 [\u03ba(xi\u2032 , x\u03021), . . . , \u03ba(xi\u2032 , x\u0302M )] \u22a4 , (5)\nwhere K\u0302\u2020 \u2208 RM\u00d7M is the pseudo-inverse of K\u0302. Vectors of random features can be constructed as\nz(xi) = D\u0302 \u22121/2V\u0302\u22a4 [\u03ba(xi, x\u03021), . . . , \u03ba(xi, x\u0302M )] \u22a4 ,\nwhere the columns of V\u0302 are the eigenvectors of K\u0302 with D\u0302 the diagonal matrix whose entries are the corresponding eigenvalues. Constructing features in this way reduces the time complexity of learning a non-linear prediction function from O(N3) to O(N) [15].\nAn alternative perspective on the Nystro\u0308m approximation, that will be useful below, is as follows. Consider integral operators\nLN [f ](\u00b7) := 1\nN\nN\u2211\ni=1\n\u03ba(xi, \u00b7)f(xi) and LM [f ](\u00b7) := 1\nM\nM\u2211\ni=1\n\u03ba(xi, \u00b7)f(xi), (6)\nand introduce Hilbert space H\u0302 = span {\u03d5\u03021, . . . , \u03d5\u0302r} where r is the rank of K\u0302 and the \u03d5\u0302i are the first r eigenfunctions of LM . Then the following proposition shows that using the Nystro\u0308m approximation is equivalent to performing linear regression in the feature space (\u201cview\u201d) z : X \u2192 H\u0302 spanned by the eigenfunctions of linear operator LM in Eq. (6):\nProposition 2 (random Nystro\u0308m view, [3]). Solving\nmin w\u2208Rr\n1\nN\nN\u2211\ni=1\n\u2113(w\u22a4z(xi), yi) + \u03b3\n2 \u2016w\u201622 (7)\nis equivalent to solving\nmin f\u2208H\u0302\n1\nN\nN\u2211\ni=1\n\u2113(f(xi), yi) + \u03b3\n2 \u2016f\u20162H\u03ba. (8)\n2.3 The proposed algorithm: Correlated Nystro\u0308m Views (XNV)\nAlgorithm 1 details our approach to semi-supervised learning based on generating two views consisting of Nystro\u0308m random features and penalizing features which are weakly correlated across views. The setting is that we have labeled data {xi, yi}ni=1 and a large amount of unlabeled data {xi} N i=n+1.\nStep 1 generates a set of random features. The next two steps implement multi-view regression using the randomly generated views z(1)(x) and z(2)(x). Eq. (9) yields a solution for which unimportant\nAlgorithm 1 Correlated Nystro\u0308m Views (XNV).\nInput: Labeled data: {xi, yi}ni=1 and unlabeled data: {xi} N i=n+1\n1: Generate features. Sample x\u03021, . . . , x\u03022M uniformly from the dataset, compute the eigendecompositions of the sub-sampled kernel matrices K\u0302(1) and K\u0302(2) which are constructed from the samples 1, . . . ,M and M + 1, . . . , 2M respectively, and featurize the input:\nz(\u03bd)(xi) \u2190 D\u0302 (\u03bd),\u22121/2V\u0302(\u03bd)\u22a4 [\u03ba(xi, x\u03021), . . . , \u03ba(xi, x\u0302M )] \u22a4 for \u03bd \u2208 {1, 2}.\n2: Unlabeled data. Compute CCA bases B(1), B(2) and canonical correlations \u03bb1, . . . , \u03bbM for the two views and set z\u0304i \u2190 B(1)z(1)(xi). 3: Labeled data. Solve\n\u03b2\u0302 = argmin \u03b2\n1\nn\nn\u2211\ni=1\n\u2113 ( \u03b2\u22a4z\u0304i, yi ) + \u2016\u03b2\u20162CCA + \u03b3\u2016\u03b2\u2016 2 2 . (9)"}, {"heading": "Output: \u03b2\u0302", "text": "features are heavily downweighted in the CCA basis without introducing an additional tuning parameter. The further penalty on the \u21132 norm (in the CCA basis) is introduced as a practical measure to control the variance of the estimator \u03b2\u0302 which can become large if there are many highly correlated features (i.e. the ratio 1\u2212\u03bbj\u03bbj \u2248 0 for large j). In practice most of the shrinkage is due to the CCA norm: cross-validation obtains optimal values of \u03b3 in the range [0.00001, 0.1].\nComputational complexity. XNV is extremely fast. Nystro\u0308m sampling, step 1, reduces the O(N3) operations required for kernel learning to O(N). Computing the CCA basis, step 2, using standard algorithms is in O(NM2). However, we reduce the runtime to O(NM) by applying a recently proposed randomized CCA algorithm of [16]. Finally, step 3 is a computationally cheap linear program on n samples and M features.\nPerformance guarantees. The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15]. Recent work of Bach [5] provides theoretical guarantees on the quality of Nystro\u0308m estimates in the fixed design setting that are relevant to our approach.1\nTheorem 3 (Nystro\u0308m generalization bound, [5]). Let \u03be \u2208 RN be a random vector with finite variance and zero mean, y = [y1, . . . , yN ]\n\u22a4, and define smoothed estimate y\u0302kernel := (K + N\u03b3I)\u22121K(y + \u03be) and smoothed Nystro\u0308m estimate y\u0302Nystro\u0308m := (K\u0303 + N\u03b3I)\u22121K\u0303(y + \u03be), both computed by minimizing the MSE with ridge penalty \u03b3. Let \u03b7 \u2208 (0, 1). For sufficiently large M (depending on \u03b7, see [5]), we have\nEME\u03be [ \u2016y \u2212 y\u0302Nystro\u0308m\u2016 2 2 ] \u2264 (1 + 4\u03b7) \u00b7 E\u03be [ \u2016y \u2212 y\u0302kernel\u2016 2 2 ]\nwhere EM refers to the expectation over subsampled columns used to construct K\u0303.\nIn short, the best smoothed estimators in the Nystro\u0308m views are close to the optimal smoothed estimator. Since the kernel estimate is consistent, loss(f) \u2192 0 as n \u2192 \u221e. Thus, Assumption 1 holds in expectation and the generalization performance of XNV is controlled by Theorem 1.\nRandom Fourier Features. An alternative approach to constructing random views is to use Fourier features instead of Nystro\u0308m features in Step 1. We refer to this approach as Correlated Kitchen Sinks (XKS) after [2]. It turns out that the performance of XKS is consistently worse than XNV, in line with the detailed comparison presented in [3]. We therefore do not discuss Fourier features in the main text, see \u00a7SI.3 for details on implementation and experimental results.\n1Extending to a random design requires techniques from [17].\n2.4 A fast approximation to SSSL\nThe SSSL (simple semi-supervised learning) algorithm proposed in [10] finds the first s eigenfunctions \u03c6i of the integral operator LN in Eq. (6) and then solves\nargmin w\u2208Rs\nn\u2211\ni=1\n  s\u2211\nj=1\nwj\u03c6k(xi)\u2212 yi\n  2\n, (10)\nwhere s is set by the user. SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10]. It also has good generalization guarantees under reasonable assumptions on the distribution of eigenvalues of LN . However, since SSSL requires computing the full N \u00d7 N Gram matrix, it is extremely computationally intensive for large N . Moreover, tuning s is difficult since it is discrete.\nWe therefore propose SSSLM , an approximation to SSSL. First, instead of constructing the full Gram matrix, we construct a Nystro\u0308m approximation by sampling M points from the labeled and unlabeled training set. Second, instead of thresholding eigenfunctions, we use the easier to tune ridge penalty which penalizes directions proportional to the inverse square of their eigenvalues [18].\nAs justification, note that Proposition 2 states that the Nystro\u0308m approximation to kernel regression actually solves a ridge regression problem in the span of the eigenfunctions of L\u0302M . As M increases, the span of L\u0302M tends towards that of LN [15]. We will also refer to the Nystro\u0308m approximation to SSSL using 2M features as SSSL2M . See experiments below for further discussion of the quality of the approximation."}, {"heading": "3 Experiments", "text": "Setup. We evaluate the performance of XNV on 18 real-world datasets, see Table 1. The datasets cover a variety of regression (denoted by R) and two-class classification (C) problems. The sarcos dataset involves predicting the joint position of a robot arm; following convention we report results on the 1st, 5th and 7th joint positions.\nThe SSSL algorithm was shown to exhibit state-of-the-art performance over fully and semisupervised methods in scenarios where few labeled training examples are available [10]. However, as discussed in \u00a72.2, due to its computational cost we compare the performance of XNV to the Nystro\u0308m approximations SSSLM and SSSL2M .\nWe used a Gaussian kernel for all datasets. We set the kernel width, \u03c3 and the \u21132 regularisation strength, \u03b3, for each method using 5-fold cross validation with 1000 labeled training examples. We trained all methods using a squared error loss function, \u2113(f(xi), yi) = (f(xi)\u2212yi)2, with M = 200 random features, and n = 100, 150, 200, . . . , 1000 randomly selected training examples.\n2Taken from the UCI repository http://archive.ics.uci.edu/ml/datasets.html 3Taken from http://www.causality.inf.ethz.ch/activelearning.php 4Taken from http://www.dcc.fc.up.pt/\u02dcltorgo/Regression/DataSets.html 5Taken from http://www.gaussianprocess.org/gpml/data/\nRuntime performance. The SSSL algorithm of [10] is not computationally feasible on large datasets, since it has time complexity O(N3). For illustrative purposes, we report run times6 in seconds of the SSSL algorithm against SSSLM and XNV on three datasets of different sizes.\nruntimes bank8 cal housing sylva SSSL 72s 2300s - SSSL2M 0.3s 0.6s 24s XNV 0.9s 1.3s 26s\nFor the cal housing dataset, XNV exhibits an almost 1800\u00d7 speed up over SSSL. For the largest dataset, sylva, exact SSSL is computationally intractable. Importantly, the computational overhead of XNV over SSSL2M is small.\nGeneralization performance. We report on the prediction performance averaged over 100 experiments. For regression tasks we report on the mean squared error (MSE) on the testing set normalized by the variance of the test output. For classification tasks we report the percentage of the test set that was misclassified.\nThe table below shows the improvement in performance of XNV over SSSLM and SSSL2M (taking whichever performs better out of M or 2M on each dataset), averaged over all 18 datasets. Observe that XNV is considerably more accurate and more robust than SSSLM .\nXNV vs SSSLM/2M n = 100 n = 200 n = 300 n = 400 n = 500 Avg reduction in error 11% 16% 15% 12% 9% Avg reduction in std err 15% 30% 31% 33% 30%\nThe reduced variability is to be expected from Theorem 1.\nTable 2 presents more detailed comparison of performance for individual datasets when n = 200, 400. The plots in Figure 1 shows a representative comparison of mean prediction errors for several datasets when n = 100, . . . , 1000. Error bars represent one standard deviation. Observe that XNV almost always improves prediction accuracy and reduces variance compared with SSSLM and SSSL2M when the labeled training set contains between 100 and 500 labeled points. A complete set of results is provided in \u00a7SI.1.\nDiscussion of SSSLM . Our experiments show that going from M to 2M does not improve generalization performance in practice. This suggests that when there are few labeled points, obtaining a\n6Computed in Matlab 7.14 on a Core i5 with 4GB memory.\nmore accurate estimate of the eigenfunctions of the kernel does not necessarily improve predictive performance. Indeed, when more random features are added, stronger regularization is required to reduce the influence of uninformative features, this also has the effect of downweighting informative features. This suggests that the low rank approximation SSSLM to SSSL suffices.\nFinally, \u00a7SI.2 compares the performance of SSSLM and XNV to fully supervised kernel ridge regression (KRR). We observe dramatic improvements, between 48% and 63%, consistent with the results observed in [10] for the exact SSSL algorithm.\nRandom Fourier features. Nystro\u0308m features significantly outperform Fourier features, in line with observations in [3]. The table below shows the relative improvement of XNV over XKS:\nXNV vs XKS n = 100 n = 200 n = 300 n = 400 n = 500 Avg reduction in error 30% 28% 26% 25% 24% Avg reduction in std err 36% 44% 34% 37% 36%\nFurther results and discussion for XKS are included in the supplementary material."}, {"heading": "4 Conclusion", "text": "We have introduced the XNV algorithm for semi-supervised learning. By combining two randomly generated views of Nystro\u0308m features via an efficient implementation of CCA, XNV outperforms the prior state-of-the-art, SSSL, by 10-15% (depending on the number of labeled points) on average over 18 datasets. Furthermore, XNV is over 3 orders of magnitude faster than SSSL on medium sized datasets (N = 10, 000) with further gains as N increases. An interesting research direction is to investigate using the recently developed deep CCA algorithm, which extracts higher order correlations between views [19], as a preprocessing step.\nIn this work we use a uniform sampling scheme for the Nystro\u0308m method for computational reasons since it has been shown to perform well empirically relative to more expensive schemes [20]. Since CCA gives us a criterion by which to measure the important of random features, in the future we aim to investigate active sampling schemes based on canonical correlations which may yield better performance by selecting the most informative indices to sample.\nAcknowledgements. We thank Haim Avron for help with implementing randomized CCA and Patrick Pletscher for drawing our attention to the Nystro\u0308m method."}, {"heading": "SI.2 Comparison with Kernel Ridge Regression", "text": "We compare SSSLM and XNV to kernel ridge regression (KRR). The table below reports the percentage improvement in mean error of both of these methods against KRR, averaged over the 18 datasets according to the experimental procedure detailed in \u00a73. Parameters \u03c3 (kernel width) and \u03b3 (ridge penalty) for KRR were chosen by 5-fold cross validation. We observe that both SSSLM and XNV far outperform KRR, by 50 \u2212 60%. Importantly, this shows our approximation to SSSL far outperforms the fully supervised baseline.\nSSSLM and XNV vs KRR n = 100 n = 200 n = 300 n = 400 n = 500 Avg reduction in error for SSSLM 48% 52% 56% 58% 60% Avg reduction in error for XNV 56% 62% 63% 63% 63%"}, {"heading": "SI.3 Random Fourier features", "text": "Random Fourier features are a method for approximating shift invariant kernels [6], i.e. where \u03ba(xi,xi\u2032 ) = \u03ba(xi \u2212 xi\u2032 ). Such a kernel function can be represented in terms of its inverse Fourier transform as \u03ba(xi \u2212 xi\u2032 ) = \u222b RD P (\u03c9)ej\u03c9 \u22a4(xi\u2212xi\u2032). P (\u03c9) is the Fourier transform of \u03ba which is guaranteed to be a proper probability distribution and so for real-valued features \u03ba(xi,xi\u2032) can be equivalently interpreted as E\u03c9 [ z(xi) \u22a4z(xi\u2032 ) ]\nwhere z(xi) = 1\u221a2 cos(\u03c9 \u22a4xi + b) . Replacing\nthe expectation by the sample average leads to a scheme for constructing random features. In particular, a Gaussian kernel of width \u03c3 has a Fourier transform which is also Gaussian. Sampling \u03c9m \u223c N (0, 2\u03c3ID) and bm \u223c Unif [\u2212\u03c0, \u03c0], we can then construct features whose inner product approximates this kernel as zi = 1\u221aM [ cos(\u03c9\u22a41 xi + b1), . . . , cos(\u03c9 \u22a4 Mxi + bM ) ] .\nIt was recently shown how both random Fourier features the Nystro\u0308m approximation could be cast in the same framework [3]. A major difference between the methods lies in the sampling scheme employed. Random Fourier features are constructed in a data independent fashion which makes them extremely cheap to compute. Nystro\u0308m features are constructed in a data dependent way which leads to improved performance but, in the case of semi-supervised learning, more expensive since we need to evaluate the approximate kernel for all unlabeled points we wish to use.\nAlgorithm 2 details Correlated Kitchen Sinks (XKS). This algorithm generates random views using the random Fourier features procedure in step 1. Steps 2 and 3 proceed exactly as in Algorithm 1.\nAlgorithm 2 Correlated Kitchen Sinks (XKS).\nInput: Labeled data: {xi, yi}ni=1 and unlabeled data: {xi} N i=n+1\n1: Generate features. Draw \u03c91, . . . \u03c92K i.i.d. from P and featurize the input:\nz (1) i \u2190 [\u03c6(xi;\u03c91), . . . , \u03c6(xi;\u03c9M )] ,\nz (2) i \u2190 [\u03c6(xi;\u03c9M+1), . . . , \u03c6(xi;\u03c92M )] .\n2: Unlabeled data. Compute CCA bases B(1), B(2) and canonical correlations \u03bb1, . . . , \u03bbM for the two views and set z\u0304i \u2190 B(1)z (1) i . 3: Labeled data. Solve\n\u03b2\u0302 = min \u03b2\n1\nn\nn\u2211\ni=1\n\u2113 ( \u03b2\u22a4z\u0304i, yi ) + \u2016\u03b2\u20162CCA + \u03b3\u2016\u03b2\u2016 2 2 . (11)"}, {"heading": "Output: \u03b2\u0302", "text": "It can be shown that, with sufficiently many features, views constructed via random Fourier features contain good approximations to a large class of functions with high probability, see main theorem of [2]. We do not provide details, since XKS is consistently outperformed by XNV in practice.\nSI.4 Complete XKS results\nFor completeness we report on the performance of the XKS algorithm. We use the same experimental setup as in Section 3. We compare the performance of XKS against a linear machine learned using M and 2M random Fourier features respectively.\nTable 4 shows the performance improvement of XKS over RFFM/2M , averaged across the 18 datasets. Table 6 compares the prediction error and standard deviation for each of the datasets individually. Figure 3 shows the performance across the full range of values of n for all datasets. The relative performance of XKS against RFFM and RFF2M follows the same trend seen in Section 3, suggesting that CCA-based regression consistently improves on regression across single and joint views.\nFinally, Table 5 compares the performance of correlated Nystro\u0308m features against correlated kitchen sinks. XNV typically outperforms XKS on 16 out of 18 datasets; with XKS only ever outperforming XNV on bank8, house and orange. Since XNV almost always outperformsXKS, we only discuss Nystro\u0308m features in the main text."}], "references": [{"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C Williams", "M Seeger"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A Rahimi", "B Recht"], "venue": "In Adv in Neural Information Processing Systems (NIPS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "ZH: Nystr\u00f6m Method vs Random Fourier Features: A Theoretical and Empirical Comparison", "author": ["T Yang", "YF Li", "M Mahdavi", "R Jin", "Zhou"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A Gittens", "MW Mahoney"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Sharp analysis of low-rank kernel approximations", "author": ["F Bach"], "venue": "COLT", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["A Rahimi", "B Recht"], "venue": "In Adv in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Multi-view Regression Via Canonical Correlation Analysis", "author": ["S Kakade", "DP Foster"], "venue": "In Computational Learning Theory (COLT)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Relations between two sets of variates", "author": ["H Hotelling"], "venue": "Biometrika", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1936}, {"title": "Canonical Correlation Analysis: An Overview with Application to Learning Methods", "author": ["DR Hardoon", "S Szedmak", "J Shawe-Taylor"], "venue": "Neural Comp", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound", "author": ["M Ji", "T Yang", "B Lin", "R Jin", "J Han"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M Belkin", "P Niyogi", "V Sindhwani"], "venue": "JMLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A Blum", "T Mitchell"], "venue": "COLT", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Multiview clustering via Canonical Correlation Analysis", "author": ["K Chaudhuri", "SM Kakade", "K Livescu", "K Sridharan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Multi-view predictive partitioning in high dimensions", "author": ["B McWilliams", "G Montana"], "venue": "Statistical Analysis and Data Mining", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning", "author": ["P Drineas", "MW Mahoney"], "venue": "JMLR", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Efficient Dimensionality Reduction for Canonical Correlation Analysis", "author": ["H Avron", "C Boutsidis", "S Toledo", "A Zouzias"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "An Analysis of Random Design Linear Regression", "author": ["D Hsu", "S Kakade", "T Zhang"], "venue": "COLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A Risk Comparison of Ordinary Least Squares vs Ridge Regression", "author": ["PS Dhillon", "DP Foster", "SM Kakade", "LH Ungar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Deep Canonical Correlation Analysis", "author": ["G Andrew", "R Arora", "J Bilmes", "K Livescu"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Randomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2].", "startOffset": 185, "endOffset": 191}, {"referenceID": 1, "context": "Randomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2].", "startOffset": 185, "endOffset": 191}, {"referenceID": 0, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 156, "endOffset": 161}, {"referenceID": 3, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 156, "endOffset": 161}, {"referenceID": 4, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 156, "endOffset": 161}, {"referenceID": 1, "context": "We investigate two ways of doing so: one based on the Nystr\u00f6m method and another based on random Fourier features (so-called kitchen sinks) [2, 6].", "startOffset": 140, "endOffset": 146}, {"referenceID": 5, "context": "We investigate two ways of doing so: one based on the Nystr\u00f6m method and another based on random Fourier features (so-called kitchen sinks) [2, 6].", "startOffset": 140, "endOffset": 146}, {"referenceID": 6, "context": "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.", "startOffset": 74, "endOffset": 80}, {"referenceID": 8, "context": "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.", "startOffset": 74, "endOffset": 80}, {"referenceID": 4, "context": "Recent theoretical work by Bach [5] shows that Nystr\u00f6m views can be expected to contain accurate estimators.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "We perform an extensive evaluation of XNV on 18 real-world datasets, comparing against a modified version of the SSSL (simple semi-supervised learning) algorithm introduced in [10].", "startOffset": 176, "endOffset": 180}, {"referenceID": 9, "context": "We chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "We chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "Our approximate version of SSSL outperforms kernel ridge regression (KRR) by > 50% on the 18 datasets on average, in line with the results reported in [10], suggesting that we lose little by replacing the exact SSSL with our approximate implementation.", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].", "startOffset": 139, "endOffset": 146}, {"referenceID": 13, "context": "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].", "startOffset": 139, "endOffset": 146}, {"referenceID": 6, "context": "Our algorithm builds on an elegant proposal for multi-view regression introduced in [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "First, given two equally useful but sufficiently different views on a dataset, penalizing regression using the canonical norm (computed via CCA), can substantially improve performance [7].", "startOffset": 184, "endOffset": 187}, {"referenceID": 0, "context": "The second is the Nystr\u00f6m method for constructing random features [1], which we use to construct the views.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Assumption 1 (Multi-view assumption [7]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables.", "startOffset": 31, "endOffset": 37}, {"referenceID": 8, "context": "Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables.", "startOffset": 31, "endOffset": 37}, {"referenceID": 6, "context": "More formally: Theorem 1 (canonical ridge regression, [7]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "To ensure our method scales to large sets of unlabeled data, we use random features generated using the Nystr\u00f6m method [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "The idea behind random features is to instead define a lower-dimensional mapping, z(xi) : R \u2192 R M through a random sampling scheme such that [K]ii\u2032 \u2248 z(xi) z(xi\u2032 ) [6, 15].", "startOffset": 164, "endOffset": 171}, {"referenceID": 14, "context": "The idea behind random features is to instead define a lower-dimensional mapping, z(xi) : R \u2192 R M through a random sampling scheme such that [K]ii\u2032 \u2248 z(xi) z(xi\u2032 ) [6, 15].", "startOffset": 164, "endOffset": 171}, {"referenceID": 0, "context": "The Nystr\u00f6m method [1, 3] constructs a low-rank approximation to the Gram matrix as", "startOffset": 19, "endOffset": 25}, {"referenceID": 2, "context": "The Nystr\u00f6m method [1, 3] constructs a low-rank approximation to the Gram matrix as", "startOffset": 19, "endOffset": 25}, {"referenceID": 14, "context": "Constructing features in this way reduces the time complexity of learning a non-linear prediction function from O(N) to O(N) [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "(6): Proposition 2 (random Nystr\u00f6m view, [3]).", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "However, we reduce the runtime to O(NM) by applying a recently proposed randomized CCA algorithm of [16].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 3, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 4, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 14, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 4, "context": "Recent work of Bach [5] provides theoretical guarantees on the quality of Nystr\u00f6m estimates in the fixed design setting that are relevant to our approach.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "1 Theorem 3 (Nystr\u00f6m generalization bound, [5]).", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "For sufficiently large M (depending on \u03b7, see [5]), we have", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "We refer to this approach as Correlated Kitchen Sinks (XKS) after [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "It turns out that the performance of XKS is consistently worse than XNV, in line with the detailed comparison presented in [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 16, "context": "Extending to a random design requires techniques from [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "4 A fast approximation to SSSL The SSSL (simple semi-supervised learning) algorithm proposed in [10] finds the first s eigenfunctions \u03c6i of the integral operator LN in Eq.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "Second, instead of thresholding eigenfunctions, we use the easier to tune ridge penalty which penalizes directions proportional to the inverse square of their eigenvalues [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "As M increases, the span of L\u0302M tends towards that of LN [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "The SSSL algorithm was shown to exhibit state-of-the-art performance over fully and semisupervised methods in scenarios where few labeled training examples are available [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "The SSSL algorithm of [10] is not computationally feasible on large datasets, since it has time complexity O(N).", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "We observe dramatic improvements, between 48% and 63%, consistent with the results observed in [10] for the exact SSSL algorithm.", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "Nystr\u00f6m features significantly outperform Fourier features, in line with observations in [3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "An interesting research direction is to investigate using the recently developed deep CCA algorithm, which extracts higher order correlations between views [19], as a preprocessing step.", "startOffset": 156, "endOffset": 160}], "year": 2013, "abstractText": "This paper presents Correlated Nystr\u00f6m Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude.", "creator": "LaTeX with hyperref package"}}}