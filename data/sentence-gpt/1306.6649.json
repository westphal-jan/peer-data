{"id": "1306.6649", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2013", "title": "Measurements of collective machine intelligence", "abstract": "Independent from the still ongoing research in measuring individual intelligence, we anticipate and provide a framework for measuring collective intelligence. Collective intelligence refers to the idea that several individuals can collaborate in order to achieve high levels of intelligence. We present thus some ideas on how the intelligence of a group can be measured and simulate such tests.\n\n\n\nThe results are based on a range of research methods, such as the Human Intelligence Scale (HSS) and Human Intelligence Scale (HIA), which is a measurement of a group of individuals on the basis of their individual intelligence. Each of these methods is presented in the full paper:\nFigure 5.\nThe following examples are from the paper on Individual Intelligence Scale (HIA) and Human Intelligence Scale (HIA). The HIA and HIA are representative of individual traits which are typically measured by group.\nFigure 6.\nA) Individual Intelligence Scale (HIA) is a measure of individual intelligence that is not measured by group. In the first example the HIA is only measured by group. The HIA measures the overall intelligence in different ways, such as by measuring individual intelligence.\nFigure 7.\nThe human intelligence scale is not representative of individual intelligence. In the second example the HIA is only measured by group. In the third example the HIA is only measured by group. In the fourth example the HIA is only measured by group. In the fifth example the HIA is only measured by group.\nFigure 8.\nThe Human Intelligence Scale (HIA) is the same as the Human Intelligence Scale (HIA), but with different levels of intelligence, from two levels of intelligence.\nThe HIA, for example, can be compared with the Human Intelligence Scale (HIA), using multiple statistical tests that can be used to simulate a collective intelligence.\nThe Human Intelligence Scale (HIA) is a measure of individual intelligence that is not measured by group. In the fourth example the HIA is only measured by group. In the fifth example the HIA is only measured by group. In the sixth example the HIA is only measured by group. In the fifth example the HIA is only measured by group. In the fifth example the HIA is only measured by group. In the sixth example the HIA is only measured by group.\nThe Human Intelligence Scale (HIA) is a measure of individual intelligence that is not measured by group. In the fourth example the HIA is only measured by group. In the fifth example the HIA is only measured", "histories": [["v1", "Thu, 27 Jun 2013 20:10:45 GMT  (1152kb,D)", "http://arxiv.org/abs/1306.6649v1", "78 pages"]], "COMMENTS": "78 pages", "reviews": [], "SUBJECTS": "cs.AI cs.MA", "authors": ["michel halmes"], "accepted": false, "id": "1306.6649"}, "pdf": {"name": "1306.6649.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "Master in Computer Engineering\nMaster Thesis\nMeasurements of collective machine intelligence\nStudent: Michel Halmes Universite\u0301 Libre de Bruxelles\nSupervisor: Prof. Jose\u0301\nHerna\u0301ndez-Orallo\nJuly 1, 2013\nar X\niv :1\n30 6.\n66 49\nv1 [\ncs .A\nI] 2\n7 Ju\nn 20\n13\nAcknowledgment\nI would like to thank my supervisor Prof. Jose Hernandez-Orallo for having supported my master thesis during the whole year. I thank you for the time you considered to this project and for your constructive comments which directed my work.\nFinally, I would also like to thank the many people which made out of my Erasmus year in Valencia an unforgettable time.\nAbstract\nIntelligence is a fairly intuitive concept of our everyday life. As usually ac-knowledged in psychometrics, \u201cintelligence is the ability measured by intelligence tests\u201d. However, defining what exactly intelligence tests should measure is less obvious. During the last decade, computer scientists have attempted to provide a formal definition of intelligence. There seems now to be the tendency that intelligence should make reference to the formalism provided by the field of algorithmic information theory. Yet, a consensus is far from being reached.\nIndependent from the still ongoing research in measuring individual intelligence, we anticipate and provide a framework for measuring collective intelligence. Collective intelligence refers to the idea that several individuals can collaborate in order to achieve high levels of intelligence. We present thus some ideas on how the intelligence of a group can be measured and simulate such tests. We will however focus here on groups of artificial intelligence agents (i.e., machines). We will explore how a group of agents is able to choose the appropriate problem and to specialize for a variety of tasks. This is a feature which is an important contributor to the increase of intelligence in a group (apart from the addition of more agents and the improvement due to common decision making).\nOur results reveal some interesting results about how (collective) intelligence can be modeled, about how collective intelligence tests can be designed and about the underlying dynamics of collective intelligence. As it will be useful for our simulations, we provide also some improvements of the threshold allocation model originally used in the area of swarm intelligence but further generalized here.\nKeywords: collective intelligence, machine intelligence tests, task allocation models, problem specialization, swarm intelligence, universal psychometrics, joint decision making, multi-task evaluation\nContents\nAcknowledgment 1\nAbstract 2"}, {"heading": "1 Goal statement and overview 6", "text": "1.1 Goal statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"}, {"heading": "2 Intelligence and intelligence tests 8", "text": "2.1 Psychometrics, IQ tests and comparative cognition . . . . . . . . 8 2.2 The Turing test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3 Inductive inference . . . . . . . . . . . . . . . . . . . . . . . . . . 10"}, {"heading": "3 Collective intelligence 14", "text": "3.1 A few words on social intelligence . . . . . . . . . . . . . . . . . . 14 3.2 The wisdom of the crowds . . . . . . . . . . . . . . . . . . . . . . 15 3.3 Collective intelligence systems . . . . . . . . . . . . . . . . . . . . 16 3.4 Approaches to collective intelligence design . . . . . . . . . . . . 17\n3.4.1 Reinforcement learning . . . . . . . . . . . . . . . . . . . 17 3.4.2 Market-based mechanisms . . . . . . . . . . . . . . . . . . 17 3.4.3 Swarm intelligence . . . . . . . . . . . . . . . . . . . . . . 18\n3.5 Results from research on human collective intelligence . . . . . . 20"}, {"heading": "4 An approach based on abstracted intelligence, vote aggregation and task allocation 21", "text": "4.1 Conceptualization of \u201cintelligence\u201d . . . . . . . . . . . . . . . . . 22\n4.1.1 Abstraction from information processing capabilities: Item response functions . . . . . . . . . . . . . . . . . . . . . . 22\n4.1.2 Aggregation of a group\u2019s abilities: Voting systems . . . . 24 4.2 Introducing social aspects . . . . . . . . . . . . . . . . . . . . . . 26 4.3 Factors of interest . . . . . . . . . . . . . . . . . . . . . . . . . . 29"}, {"heading": "5 Experiments 30", "text": "5.1 Voting on one problem . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.1.1 Homogeneous group of agents . . . . . . . . . . . . . . . . 30 5.1.2 The impact of adding low performing agents . . . . . . . 32 5.1.3 The impact of adding high performing agents . . . . . . . 35\n5.2 Simplified version of the allocation model with several problems . 35 5.2.1 Imposing an appropriate allocation: specialized agents . . 36\n5.2.2 A simplified allocation model . . . . . . . . . . . . . . . . 37 5.2.3 Simulation with specialized agents and the simplified al-\nlocation model . . . . . . . . . . . . . . . . . . . . . . . . 37 5.3 Standard version of the allocation model with specialized agents 38\n5.3.1 Allocation model testing and improvements . . . . . . . . 38 5.3.2 Remark on dynamic environments . . . . . . . . . . . . . 42\n5.4 Use of the problems\u2019 difficulties in the allocation task . . . . . . . 43 5.4.1 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.4.2 Should the group be provided with a measure of difficulty? 44 5.4.3 Uniform problem weighting . . . . . . . . . . . . . . . . . 50 5.5 Use of the agents\u2019 ability in the voting process . . . . . . . . . . 52 5.5.1 Should the group be provided with a measure of ability? . 52 5.6 Imitating agents . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.6.1 Random imitation . . . . . . . . . . . . . . . . . . . . . . 54 5.6.2 Best imitation . . . . . . . . . . . . . . . . . . . . . . . . 55"}, {"heading": "6 Analysis of the results 58", "text": "6.1 Modeling of intelligence . . . . . . . . . . . . . . . . . . . . . . . 58 6.2 Collective intelligence tests . . . . . . . . . . . . . . . . . . . . . 58\n6.2.1 How to introduce a social dimension into collective intelligence tests . . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.2.2 Dynamic environments . . . . . . . . . . . . . . . . . . . . 59 6.2.3 Which information to provide? . . . . . . . . . . . . . . . 59\n6.3 Collective decision making . . . . . . . . . . . . . . . . . . . . . . 59 6.3.1 The dynamics of odd and even number of agents . . . . . 59 6.3.2 The importance of voting systems . . . . . . . . . . . . . 60 6.3.3 Independence of votes . . . . . . . . . . . . . . . . . . . . 60 6.4 Allocation models . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.4.1 Avoiding stimulus divergence . . . . . . . . . . . . . . . . 61 6.4.2 Additional terms in the stimulus update rule . . . . . . . 61 6.4.3 Adaptation of the threshold update rule . . . . . . . . . . 61"}, {"heading": "7 Discussions for future work 63", "text": "7.1 Use of agent ability in the resource allocation process . . . . . . . 63 7.2 Single peaked response functions . . . . . . . . . . . . . . . . . . 64 7.3 Different types of problems . . . . . . . . . . . . . . . . . . . . . 65 7.4 Intelligence affecting the allocation capability . . . . . . . . . . . 65 7.5 Asymptotic performance worse that random . . . . . . . . . . . . 66"}, {"heading": "8 Conclusion 68", "text": "Bibliography & Appendices 69\nBibliography 69"}, {"heading": "A Appendices 74", "text": "A.1 Proof of equation (5.3) . . . . . . . . . . . . . . . . . . . . . . . . 74 A.2 Description of the implementation . . . . . . . . . . . . . . . . . 75\nA.2.1 Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\nA.2.2 Principal routine . . . . . . . . . . . . . . . . . . . . . . . 76\nChapter 1\nGoal statement and overview\nThere have been some works which have studied the contribution of collective decisions making (e.g., multi-classifier systems and decision theory) and some other works which have focused on optimal allocation (e.g., swarm intelligence and resource allocation). In this work, we analyze both things together, as they are contributors to the observed increase in collective intelligence in many real systems (e.g., brains, societies, biology). Thus, we will explore group performance according to joint decision making, task allocation, several degrees of intelligence (or ability) and agent specialization."}, {"heading": "1.1 Goal statement", "text": "The goal of this master thesis is to experiment with collective machine intelligence abstraction models, so as to observe some interesting phenomena in the intellectual capabilities of several, collaborating machines. Put differently, we would like to analyze the dynamics behind a group of AI agents solving collectively a problem requiring \u201csome minimum amount of intelligence1\u201d. The results from such a study might be very useful for developing collective intelligence tests, when combined methods for measuring individual intelligence.\nQuestions which are interesting in this context are for instance:\n\u2022 Can the intelligence of a group be significantly higher than the intelligence of the (most intelligent) agents in the group (with positive influence due to collaboration)?\n\u2022 Can it be smaller (with negative influence through disturbance)?\n\u2022 How should the agents aggregate their skills? Which joint decision making system is the most suitable? Is it beneficial to provide the agents with a measure of their relative intelligence?\n\u2022 If several tasks must be performed simultaneously, how can the group allocate its resources?\n1 As there is no consensus on a formal definition of intelligence, we use the term in an abstract way to represent any cognitive or problem solving capability\nChapter 1. Goal statement and overview 1.2 Overview\nThe approach will be mainly experimental as based on computer simulations, using models of collective behavior based on swarm intelligence, task allocation and problem specialization."}, {"heading": "1.2 Overview", "text": "We will start defining the concept of intelligence, and present the main approaches and related formalisms for measuring it (chapter 2).\nThen we will present the concept of collective intelligence (chapter 3). We will mostly focus on collective intelligence for machines. However, related concepts for humans such as the wisdom of the crowds and social intelligence will also be mentioned.\nIn chapter 4 we explain the approach used to simulate a collective intelligence test. We will hence explain how we model the test problems and how we introduce social aspects into the test. Thereafter, we perform some experiments (chapter 5). We have several test setups, each of which will be explained first, and then analyzed and interpreted.\nIn chapter 6 we analyze the results from a more integrated perspective. In chapter 7 we explain some additional setups which might be interesting for future work. Finally we conclude by looking at what objectives have been met and provide the take-away message from this report.\nChapter 2\nIntelligence and intelligence tests\nIn this chapter we will briefly define the concept of intelligence; both from the point of view of psychology and computer science."}, {"heading": "2.1 Psychometrics, IQ tests and comparative cog-", "text": "nition\nThe concept of intelligence is fairly intuitive for all of us. It describes the ability of subjects \u2013 humans, but also animals or machines \u2013 to perform cognitive tasks. However, what exactly intelligence is, and which cognitive tasks precisely reflect intelligence, is less clear.\nThe most commonly accepted measure of intelligence is the so-called Intelligence Quotient (IQ). This is in fact the normalized test score achieved in an IQ test. Hence, following Boring [5], \u201cintelligence is the ability measured by the IQ test\u201d.\nIQ tests are designed by psychologists; more precisely those working in the field of psychometrics. Typically, the IQ test measures abstract reasoning capabilities. The problems of the test are mostly related to abilities such as verbal comprehension, word fluency, number facility, spatial visualization, associative memory, perceptual speed, reasoning, and induction. In 1904, the psychologist Spearman [53] discovered a positive correlation across the performances in these different tasks. He called the common factor in intelligence the general factor g. Consequently, the intelligence related to a specific type of task was denoted s. He argued hence that intelligence test should reflect the g-factor as it reflects the general ability of an individual to perform any cognitive task.\nIQ tests are indeed fairly successful in discriminating humans according to who will be more or less successful in performing cognitive task encountered in real life. However, a problem of the tests is that they are too anthropocentric. It is thus badly suited for evaluating animals or machines. Moreover, some test problems of the IQ tests are those which are frequently faced by (adult) humans in real live and at which they are thus good at. As a result of this, it cannot be said that IQ tests reflect what might be understood as \u201cuniversal intelligence\u201d,\nChapter 2. Intelligence and intelligence tests 2.2 The Turing test\ni.e., a valid concept for any kind of individual. A further discussion of what a universal test is and why IQ tests are not universal can be found in [16] and [22].\nThe field of comparative cognition extends intelligence beyond humans to animals. An important challenge of this discipline is that no human language or gestures can be used to provide the instructions for the test. To overcome this, rewards \u2013 mostly in the form of food \u2013 are used to incentivize the animals to achieve the highest score possible, hence to reveal its true \u201cintelligence\u201d. These advancements have already provided us with interesting insights about the cognitive abilities of animals. (see for instance [51])"}, {"heading": "2.2 The Turing test", "text": "Also, with respect to the evaluation of machine intelligence, some advancement has been made. The Turing Test expresses how far a computer is able to resemble a human. It is named after its famous inventor who was very concerned with machine intelligence already in 1950. Turing [57] asked for instance whether computers would one day be able to \u201cthink\u201d. He was convinced that one day they would be able to do so. As the definition of \u201cthinking\u201d is again fairly abstract, he imagined the following test which he initially called it the imitation game. In this test a human judge engages a written conversation with a human and a machine without knowing who is who. If in the future a machine would be build such that the judge cannot clearly distinguish the machine from the human, Turing argued that the machine could be said to\u201cthink\u201d and that it had attained the intelligence of humans.\nOver time, other versions of the Turing test have been invented so as to compare machines to humans. Some milestones of artificial intelligence could clearly be set this way. In 1997, a chess machine named deep blue beat the Russian chess master Garry Kasparov [45]. In 2011, an IBM project called Watson beat humans in the game Jeopardy! [17].\nSeveral tasks which initially allowed distinguishing machines from humans have over time lost their discriminative power. Currently, this can still be done using the CAPTCHA1 tests [59], where distorted letters and numbers have to be identified in a picture. This test is now omnipresent and is successfully used to avoid for instance the automated creation of email addresses by non intelligent machines. Yet, it is only a matter of time until this task can also be performed by machines.\nIt is hence becoming more and more difficult to distinguish humans from machines based on their performance in specific cognitive tasks. However, a good performance at the Turing test does not imply higher intelligence. This is related to the Chinese Room argument brought forward by Searle et al. [49]. In his explanation, an operator disposing of a Chinese input-output table could maintain a seemingly intelligent conversation in Chinese, without actually being familiar with this language. And it has indeed been shown that a fairly easy algorithm could actually maintain a human-like conversation, without actually understanding the content of the conversation [55].\nMoreover, the Turing Test is again too anthropocentric, as the reference object of intelligence is human. The Turing Test can thus also not be used as\n1Completely Automated Public Turing test to tell Computers and Humans Apart\nChapter 2. Intelligence and intelligence tests 2.3 Inductive inference\nuniversal intelligence test. Computer scientists have hence made efforts to provide a universal definition of what intelligence actually is. Their current approach uses concepts of algorithmic information theory, which will be explained next."}, {"heading": "2.3 Inductive inference", "text": "A recent approach has been to use inductive inference for designing intelligence test, hence for to measuring and defining intelligence [24, 20, 37]. In inductive inference tests, the evaluee \u2013 a human, an animal or a machine \u2013 has to observe a non-random sequence of characters x = {x1, x2, . . . , xt}. Typically, but not necessarily, those characters are assumed to be bits. The evaluee has then to learn the underlying pattern of this sequence and start predicting the next symbol xt+1. When from a certain moment on all predictions are correct, we say that the evaluee has learned to predict the sequence.\nThe advantage of using inductive inference is that we dispose of a formalism to describe it mathematically. This formalism stems from algorithmic information theory developed by Kolmogorov [32], Chaitin [9] and Solomonoff [52]. Let us explain the related concepts and how they might intervene in evaluating intelligence.\nIn order to evaluate intelligence, the evaluee must of course be tested on several sequences. As one might intuitively understand, there are sequences which are more difficult to predict than others. For instance the sequence {111 . . .} is fairly easy to predict, while {010011000111 . . .} is already more difficult. A mathematical formalization of a sequence\u2019s \u201cdifficulty\u201d is its Kolomorow Complexity. The Kolomorow Complexity of a sequence x \u2013 actually the amount of information contained in it \u2013 is given by the size of the smallest program q on a Universal Turing Machine U so that the latter generates this sequence on output [39]:\nKU (x) = min q |q| : U(q) = x (2.1)\nThe definition of a Universal Turing Machine (UTM) is a Turing Machine that can simulate any other Turing machine if previously fed with the appropriate program.\nOne can show that this definition is actually independent (to an extend) of the Turing Machine which is used. The difference of the complexity as measured on two distinct machines is a constant independent of x: KU1(x) = KU2(x) + O(1). This is because any Universal Turing Machine can be simulated by another with program of fixed length. This is known as the invariance theorem.\nThe Kolomorov complexity is often also referred to as the Minimum Description Length (MDL) [61, 60, 48], which is the shortest string, which taken as an algorithm produces x.\nIntelligence as defined here is hence the ability of predicting a non-random sequence depending on its (Kolomorov) complexity. Herna\u0301ndez-Orallo and Minaya-Collado [24] have designed a test based on inductive sequence prediction. They show that their scores are actually closely related to the IQ. The advantage of this approach over an IQ test is however that we do not rely on\nChapter 2. Intelligence and intelligence tests 2.3 Inductive inference\nsome arbitrarily defined test score. Instead, there is now a well defined mathematical concept behind it.\nThe use of the Kolomorov complexity shows that the idea of inductive sequence prediction is actually strongly related to that of compression. The underlying task behind sequence prediction is to find the shortest description behind a sequence, which is nothing else than compressing it. We know that a totally random sequence cannot be predicted. In accordance to this, information theory tells us that it can also not be compressed. Herna\u0301ndez-Orallo and Minaya-Collado [24] refer hence to \u201cintelligence as the ability of compression\u201d, although they argue that this direct connection needs to be further refined and developed (and led beyond inductive inference [21]).\nThe use of compression (and its mathematical counterpart, i.e. Kolomorov complexity) solves another potential problem with the use of prediction as a measure of intelligence. Consider the sequence x = {2, 4, 6, 8}. Typically one would predict the next number to appear as being 10, because the kth item of the sequence is given by 2k. However, the polynomial 2k4\u221220k3+70k2\u221298k+48 follows also the same initial pattern [37]. According to this, the next number to predict would be 58. An intelligence test would however interpret 10 as the correct answer. This is due to Occams\u2019s Razor principle: \u201cIf there are alternative explanations for a phenomenon, then \u2013 all other things being equal \u2013 we should select the simplest one\u201d. The origin of this principle is rather philosophical. Yet, algorithmic information theory provides a mathematical justification to it.\nSolomonoff [52] defined the a priori probability that on any input on a Universal Turing Machine U appears the string x. He considers for this the set of all programs which on output provide x. The un-normalized prior of x is given by:\nPU (x) = \u2211\nq:U(q)=x\n2\u2212|p| (2.2)\nThis definition is again independent of the UTM which is considered. It is easy to see that this probability is dominated by the shortest description, hence that of length KU (x):\nPU (x) = O ( 2\u2212KU (x) )\n(2.3)\nThis is also known as universal distribution. Thus by selecting the shortest description, one selects actually the most likely one. This justifies Occams\u2019s Razor principle, which is consequently also called Minimum Description Length (MDL) principle.\nAn advantage is also that a measure of intelligence base on prediction/compression overcomes the Chinese Room argument[15]. As there exist an infinite number of sequences to predict, a look-up table cannot be used. Instead, prediction must actually be based on understanding the pattern underlying the sequence.\nA disadvantage of using the Kolomorov complexity to evaluate intelligence is that it can typically not be computed in finite time (due to the Halting problem). However, there exist computable approximations of it (e.g., the Levin\u2019s Kt [38]).\nThe Kolomorov complexity is a very powerful tool, which can be used beyond simply describing the complexity of a sequence. By analogy, it can also used as a measure for the complexity of a whole testing environment \u00b5. The complexity of\nChapter 2. Intelligence and intelligence tests 2.3 Inductive inference\nthis environment is simply the length of the shortest input to a UTM simulating the latter. This idea and its use for evaluating intelligence was pioneered by Dobrev [12, 13]. It was then further elaborated in a more elegant way by Legg and Hutter [36] using Markov Decision Processes and reinforcement learning.\nA testing environment is typically described as a stochastic (Markov decision) process in which at each time step the evaluee takes an observation ot from an observation space O and receives a reward rt form a reward space R. The evaluee will then chose an action at from an action space A. The probability of the couple otrt \u2013 which is called the perception \u2013 depends on all previous actions, observations and rewards: \u00b5(otrt|o1r1a1o2r2a2...ot\u22121rt\u22121at\u22121). In the case of an inductive sequence prediction test, the observation is simply the last symbol xt, the reward might be chosen at 1 and 0 depending on whether or not this symbol was correctly predicted. The action is then to select the next symbol. Using this formalism more complex environments can be described. Herna\u0301ndez-Orallo et al. [25] discuss how an environment with several evaluees can be constructed. This is for instance useful for the design of adversarial prediction problems, where one evaluee has to predict a sequence generated by the other, as in [28, 26].\nLegg and Hutter [37] use the here presented formalism from algorithmic information theory to design a universal measure of intelligence. Let ri,\u00b5t be the reward of evaluee i in environment \u00b5 at round t. The expected cumulative reward of this evaluee in this environment is the defined as:\n\u03a6\u0304(\u00b5, i) := E ( \u221e\u2211 t=1 ri,\u00b5t ) (2.4)\nThere is a problem with the convergence of this series, which is however resolved here by supposing that the agent has a finite life and/or the environment can only provide a finite total reward.\nThe proposed measure of universal intelligence is actually the average of this expected cumulative reward in all environments \u00b5 from the environment space E , weighted by the prior probability of this environment PU (\u00b5) = 2\u2212KU (\u00b5):\n\u03a5\u0304(i) := \u2211 \u00b5\u2208E 2\u2212KU (\u00b5)\u03a6\u0304(\u00b5, i) (2.5)\nThis measure is called universal not because it uses the universal probability distribution, but rather because it could be applied to any type of evaluee: humans, animals and machines. It is still dependent on the considered UTM. From the invariance theorem we know however that another UTM would give the same result.\nEven though the use of the universal distribution seems intuitive, it might be criticized here. The universal distribution gives a very high weight to the simplest problems. There are also other problems to turn the above measure into an intelligence test. Some of these issues are addressed in [23, 27]. One recurrent one is the use of a universal distribution to choose among environments instead of an actual measure of difficulty. As put forward by Herna\u0301ndez-Orallo et al. [25] and as we will discuss furthermore (see e.g., 5.4.3 and 7.2), it might very well be that intelligence is not a monotonic phenomenon in the problem\u2019s complexity i.e., a very intelligent being might actually underperform less intelligent ones\nChapter 2. Intelligence and intelligence tests 2.3 Inductive inference\non very easy problems. In order to take this into account, Herna\u0301ndez-Orallo et al. [25] suggest for instance to define a minimum complexity of the problems. Alternatively, intelligence might be defined as the maximum complexity of problems at which the performance differs significantly from random.\nChapter 3\nCollective intelligence\nIn the previous chapter we have considered intelligence from an individual point of view; both for humans, animals and machines. For humans however, it is very restrictive to consider their intelligence in an environment in which they are alone. Multiple aspects of what one might consider as \u201cintelligence\u201d relate to the interaction with other humans. Soon in the development of intelligence tests, this idea emerged under the name of social intelligence.\nAlso for machines the same holds, yet to a lesser extent. In Artificial Intelligence the idea emerged that a group of less intelligent agents can actually outperform one very intelligent agent. For this to be true, the group must of course put its resources together. The agents must hence interact. The study and design of interacting artificial intelligence systems is called multi-agent systems or collective intelligence.\nFor the sake of completeness we will briefly discuss social intelligence. Thereafter we discuss collective intelligence. Both notions must however not be confused. Social intelligence reflects the individual ability of a human to interact with other human beings. Collective intelligence relates to interactions of artificial intelligence agents. Yet, it refers rather to the resulting intelligence of the group rather than the capacity of the individual agent to interact with others. Nonetheless, as we will see, the more these agents have \u201csocial\u201d abilities, the higher will also be the resulting collective intelligence."}, {"heading": "3.1 A few words on social intelligence", "text": "The idea of social intelligence [8] was first mentioned by Thorndike [56]. He distinguished three facets of intelligence: the ability to understand and manage ideas (abstract intelligence), concrete objects (mechanical intelligence) and people (social intelligence).\nMany authors have since then discussed social aspects of intelligence. Vernon [58] defines it as \u201cthe ability to get along with people in general, social techniques or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into temporary moods or underlying personality traits of stranger\u201d.\nSoon, the first social intelligence tests emerged. The first of such tests was the George Washington Social Intelligence Test (GWSIT) [44]. This test is\nChapter 3. Collective intelligence 3.2 The wisdom of the crowds\ncomposed of a number of subtests, which are combined to provide an aggregated index of social intelligence. Aspects which are measured are:\n\u2022 Judgment of social situations\n\u2022 Memory of names and faces\n\u2022 Observation of human behavior\n\u2022 Recognition of the mental states behind words\n\u2022 Recognition of mental states from facial expression\n\u2022 Social information\n\u2022 Sense of humor\nHowever, this test soon came under criticism as it correlated much with abstract intelligence tests [29]. Hence some authors (e.g., [62]) argued that \u201csocial intelligence is nothing else than general intelligence applied to the social domain\u201d. Yet, we would like to have a test which measures abilities distinct from cognitive abilities. Therefore many other tests of social intelligence have been proposed.\nO\u2019Sullivan et al. [47] for instance developed a test which seems to withstand such criticism [50]. Their test represents the \u201csocial ability to judge people with respect to feelings, motives, thoughts, intentions, attitudes, or other psychological dispositions which might affect an individual\u2019s social behavior \u201d. They define six cognitive abilities:\nCognition of behavioral units: the ability to identify the internal mental states of individuals\nCognition of behavioral classes: the ability to group together other people\u2019s mental states on the basis of similarity\nCognition of behavioral relations: the ability to interpret meaningful connections among behavioral acts\nCognition of behavioral systems: the ability to interpret sequences of social behavior\nCognition of behavioral transformations: the ability to respond flexibly in interpreting changes in social behavior\nCognition of behavioral implications: the ability to predict what will happen in an interpersonal situation."}, {"heading": "3.2 The wisdom of the crowds", "text": "Let us leave social intelligence aside now for a moment and consider collective intelligence. As mentioned in the introduction of this chapter, the idea behind the latter is that a group of less intelligent agents can be more intelligent than one very intelligent agent. The principle applies however equally to humans, which is usually referred as the wisdom of the crowds. One of the first of having\nChapter 3. Collective intelligence 3.3 Collective intelligence systems\nexploited this principle was Francis Galton [18]. In 1907, Galton visited a fair, at which there was a contest whose participants had to guess the weight of an ox. The closest guess would win a prize. Galton managed to get his hands on the people\u2019s votes after the contest. Most participants\u2019 vote differed a lot from the real weight. Some participants estimated far below and others far above the ox\u2019s weight. Out of the 800 participants, nobody guessed the correct value. However, when Galton computed the median vote \u2013 which he referred to as the vox populi, the voice of the people \u2013, he found out that it was actually in a 0.8% range of the real weight. His conclusion was exactly what we refer to as the wisdom of the crowds.\nThe wisdom of the crowds is present in very important applications of real life. The efficiency of markets is based exactly on this principle. On financial markets, participants make their bids for an asset. This is nothing else than providing their estimate its value. In most cases those bids will not reflect the correct market value. Yet, the errors made by the many bidders compensate each other and finally the market price will reflect the intrinsic value of the asset."}, {"heading": "3.3 Collective intelligence systems", "text": "In artificial intelligence, Collective intelligence [66] commonly refers to multiagent systems (i.e., a large distributed collection of computational processes) with no centralized communication or control. This means that there is no \u201cmaster agent\u201d which manages the other agents. Instead, each agent is autonomous and takes its own decisions. Typically, each agent is very simple. The \u201ccollective intelligence\u201d results from the cooperation and coordination of a large number of agents. In many cases, the agents are identical, but this is actually not necessary.\nTogether, the group intends to maximize a common objective, the world utility function. The difficulty when designing collective intelligence systems is to design the individual behavior(s) and objective(s) of the agents so that the world utility is maximized.\nHow difficult the collective intelligence design task is can be illustrated with the tragedy of the commons which might arise when a group of agents acting individually intents to maximize a world utility function.\nThe tragedy of the commons refers to a social dilemma that arises in the exploitation of common pool resources, i.e. resources which are used by several individuals, as for instance nature, public defense, security, and sometimes even information goods. It was first discussed by Garret Hardin [19]. He uses as example for such a resource medieval land tenure in Europe. This land was already at that time called the commons as it was accessible to everybody. Every farmer could freely let its cattle grass on these grounds. Hardin showed that this led to over-grassing of the commons. This is because letting one more cow grass on the commons brings actually a benefit \u2013 under the form of more milk \u2013 to its owner, while is has a negative impact on other farmers as the common resource \u2013 the grass on the commons \u2013 becomes more scarce and diminishes their production of milk. In other words, exploiting the commons has actually negative externalities to society. The dilemma behind this is that most common pool resources will be overexploited, even though - or rather\nChapter 3. Collective intelligence 3.4 Approaches to collective intelligence design\nbecause - each individual acts rationally, i.e. it takes into account only its own costs and benefits. The negative externalities to all the other individuals are not considered. It would nevertheless be socially desirable to take them into account.\nSuch a phenomenon might of course also arise in multi-agent systems. So as to avoid the tragedy of the commons, the agents might hence not be too \u201cselfish\u201d and must focus on the world utility. Here one understands again how important cooperation and coordination among the agents are.\nBefore presenting different approaches of collective intelligence let us make an important remark about the form the agents take. As one can imagine, the agents might take the form of isolated physical entities. A typical example of this would be a group of robots interaction with each other. However, it is not necessary that the agents are physically isolated from each other. One talks also about collective intelligence when the agents are actually embedded in the same computational unit. This is for instance the case when collective intelligence is used for optimization algorithms. In this case the agents are represented by several instances of simple algorithmic objects. Most examples of such algorithms stem from the field of swarm intelligence which we discuss below."}, {"heading": "3.4 Approaches to collective intelligence design", "text": "There exist several approaches on how to design collective intelligence systems. We will briefly cite a few examples here."}, {"heading": "3.4.1 Reinforcement learning", "text": "A first approach is to use Reinforcement Learning (RL). In this approach, one would define the reward so that it is partially composed of an individual utility and of the world utility. Yet in many cases, RL is not well suited due to the big size of the action-policy space [66]. We will hence not further discuss this approach."}, {"heading": "3.4.2 Market-based mechanisms", "text": "As mentioned above, markets are a perfect example for an application of the wisdom of the crowds (\u201chuman collective intelligence\u201d). One approach to design collective intelligence systems is to represent the problem to be solved by the group as a market.\nAs an example, let us briefly explain here the approach taken by Campos et al. [7]. In this paper, a group of paint-booths \u2013 the agents \u2013 have to paint trucks coming out of an assembly line. The goal \u2013 i.e., world function \u2013 is to minimize the total makespan of the trucks. The trucks have to be painted in colors depending on the customer orders. Each paint booth disposes of all colors. Yet, switching from one color to another requires an additional time to flush out the old paint and fill the booth with the appropriate color. Hence the number of color switches should be minimized.\nCampos et al. [7] present a market-based approach, where each agent i makes a bid to paint truck j. The truck will be assigned to the highest bidder. The\nChapter 3. Collective intelligence 3.4 Approaches to collective intelligence design\nbid of agent i for truck j is given by:\nBi(j) = P \u00b7 wj (1 + C \u00b7 e(i, j))\n\u2206TL (3.1)\nwhere wj is the priority of truck j and \u2206T is the time it would take to paint truck j. The value of e(i, j) is 1 if the last truck in the queue of booth i matches the color truck j needs to be painted, and 0 if taking on truck j requires a paint flush. The parameters P ,C, and L are used to set the relative importance of the three components wj , e(i, j) and \u2206T , respectively.\nThe advantage of the market-based approach is that it can revert to extensive knowledge from economics, such as the theory of general equilibrium and game theory, or more precisely auction theory.\nThe disadvantage of such an approach is that mechanisms based on rational individualistic agents can frequently be defeated by other more sophisticated algorithms. This is because they are not based on cooperation between the agents and are hence prone to market failures such as the tragedy of the commons explained above. And indeed, Campos et al. [7] also present an ant-algorithm which defeats the market-based version. Ant-based algorithms belong to the family of biologically inspired swarm intelligence systems, which we will discuss next. The ant-algorithm used by Campos et al. [7] is the same as the one we present in 4.2."}, {"heading": "3.4.3 Swarm intelligence", "text": "Swarm intelligence is \u201cany attempt to design algorithms or distributed problemsolving devices inspired by the collective behavior of social insect colonies and other animal societies\u201d [3]. Such social insects can for instance be ants, termites, bees, or even birds and fishes.\nSocial insects are suited for the design of collective intelligence systems as most of them are based on collective behavior without centralized control. They are organized in large populations of self-organized, simple agents.\nMost importantly however, one can be inspired from social insects as they use simple but effective models of communication. More precisely, social insects \u2013 and hence also the agents of swarm intelligence systems \u2013 communicate in two ways. First, there is direct communication with nearby insects. This communication can be established via physical contact (antennation), visual contact, chemical contact, etc.\nSecond, there is indirect communication via the environment itself. More precisely, there is indirect communication when one insect modifies the environment and the other responds to the new environment at a later time. This is called stigmergy. For ants and termites for instance this is done via pheromones. While walking, ants and termites deposit pheromones on the ground. Other ants will follow this pheromone trail with a high probability.\nLet us illustrate via a simple example how stigmergy can be useful for the group. Suppose the following experiment: close to an ant nest appears a new food source. There are two ways to get to this food source, one short and one long path. It is observed that in the beginning both ways are taken with equal probability. Yet, after a short moment, the colony will use the shortest path almost exclusively. The reason behind is that while walking to the food source and back to the nest again, the ants release pheromones on the ground,\nChapter 3. Collective intelligence 3.4 Approaches to collective intelligence design\nwhich other ants (and themselves) tend follow. Yet, the amounts of pheromones will rapidly become higher on the shortest path for two reasons. First \u2013 the time reason \u2013, as the shortest path can be crossed in a shorter amount of time therefore the ants having crossed the shortest path are available earlier to place their pheromones again. Second \u2013 the distance reason \u2013, given (initially) the same share of ants on both paths, the density of pheromones distributed will be lower on the longest path (as the density of ants on it is also lower). Soon as the share of ants become higher on the shortest path, the difference in the pheromone density will even be reinforced by the fact that the pheromones evaporate and must constantly be renewed.\nBy inspiring oneself from social insects one can design different kinds of collective intelligence algorithms. It is for instance possible to solve the shortest path problem similarly as explained before on a simplified network with only two possible paths. How exactly both ways of communication are translated in the algorithm is entirely at the discretion of the algorithm designer.\nIn the example of the shortest path one would create a colony of artificial ants (the agents) which build iteratively random paths through the network from the start to the goal. The paths they build depend however on the (artificial) pheromone on the edges; the more pheromones on an edge the more likely an ant will take this edge. These pheromones are represented by a so-called stigmergic variable \u03c4ij , which represents the amount of pheromones on the edge between the nodes i and j. After each round the pheromones are partially evaporated and the ants deposit some new pheromones.\nAs mentioned, with artificial ants some improvements can be made with respect to real ants. For instance, instead of having the ants distributing the same amount of pheromones on the edges they have taken on their paths, this amount of pheromones might depend on the quality of the build path, as expressed by the inverse of the path\u2019s total distance (i.e., the world utility to be minimized). Also one can allow only the ants which have built the best paths to update the pheromones. The information in the pheromones \u03c4ij can also be complemented with heuristic information \u03b7ij such as the inverse of the distance between node i and j, \u03b7ij =\n1 dij . Swarm intelligence algorithms can also be combined with\nlocal search algorithms. However, for the shortest path problem, ant colony algorithms have always lower performance than algorithms such as Dijkstra. Yet, for NP-hard problems \u2013 for which no exact algorithm exists \u2013 ant-colony algorithms algorithms (and swarm intelligence algorithms in general) represent useful meta-heuristics. Examples of NP-hard problems for which ant-colony algorithms have been successfully developed are for instance the traveling salesman problem [14], routing problems [6], scheduling problems [69] or partitioning [35] problems,\nNot only for optimization algorithms, but also for collective intelligence systems with physically separated agents, social insects can be an inspiring source for designers. The corresponding field concerned with designing groups of collaborating robots is called swarm robotics. We will not enter the details on how one can build such a swarm of robots. Instead, we will illustrate some real life application of such collective intelligence systems.\nSeveral research projects have started to assess the use of swarm robots for search and rescue (SAR) tasks after disasters [43, 10, 31]. The advantage of using robots for SAR tasks is of course that the lives of human rescuers are not\nChapter 3. Collective intelligence 3.5 Results from research on human collective intelligence\nput into danger by sending them into dangerous environment. However, many simple robots seem more promising than one very sophisticated one. This is because a collective intelligence system has multiplied resources and hence not one single point of failure. If one robot fails \u2013 which is likely in environments after a disaster \u2013 the performance of the group will barely be affected. Also one can use different types of robots; each type specialized in on specific task. Flying robots for instance are more suited for searching large areas, while ground based robots are more suited for moving and transporting objects."}, {"heading": "3.5 Results from research on human collective", "text": "intelligence\nAs mentioned in the introduction, the aim of this report will be to study the dynamics behind artificial collective intelligence. This is a novelty in the academic research. Yet, similar studies have already been conducted with groups of humans.\nThe main conclusions of such research are basically identical [42, 67, 68]. When a group of humans faces a cognitive task, its performance is only very weakly correlated with the average or even the maximum intelligence of its members. Instead, social aspects of the group members explained the group\u2019s results. A good communication and collaboration among the member is more important than the IQ of the members.\nA first important factor is the social sensitivity of the group members. In groups with high collective intelligence, the members had high social sensitivity for each other: they paid attention to each other and asked questions. Also in such groups, the members performed very well in social intelligence test where it came down to reading the others\u2019 emotions.\nMoreover, in groups with high collective intelligence, the turn-taking was distributed equally. Groups in which the conversation was dominated by only a few individuals are typically underperforming.\nSurprisingly the share of women in the group correlates positively with the collective intelligence. This is because women tend to be socially more sensitive. However, some gender diversity is still advantageous.\nAlso, the way in which rewards (payments) are distributed in the group (even punishing those that underperform), as well as the use of have been sugested. One very interesting study is Kosinski et al. [33], who use a so-called crowdsource platform \u2013 a platform allowing employers to connect to several job seekers who will execute small tasks against a little reward \u2013 to evaluate collective intelligence. They use this platform to solve IQ tests and show that even small groups can do better than 99% of the population.\nTo put a long story short: Social intelligence plays an important role in collective intelligence. How well a group communicates and cooperates is important for its performance, perhaps more than the individual intelligence of its members. Our collective intelligence test which we are attempting to design for machines must take this into account. Finally, there is an important issue in collective intelligence which is present in any social organization. Groups need to handle a diversity of situations. A way to cope with this variety is by member specialization, originated and reinforced by an appropriate task allocation.\nChapter 4"}, {"heading": "An approach based on", "text": "abstracted intelligence, vote aggregation and task allocation\nAs mentioned in the goal statement (1.1), the broad context of this master thesis is to develop an intelligence test for groups of AI agents. Such a quest is of course by far too ambitious, especially given the state of advancement in the field of research on tests of individual intelligence. Therefore the objective of this master thesis will only be to provide some insights on the dynamics of collective intelligence, which might contribute to the development of such a test in the future.\nTwo aspects are critical for the development of a coherent collective intelligence test:\n1. As we have seen in 2.3, the research field analyzing intelligence in a formal way is tending more and more in the direction of defining intelligence as being the capability of processing information; more precisely compression. The intelligence test for collectives should hence be based on similar concepts as well. We will however go a step further and make an abstraction of which problem precisely the agents are facing.\n2. As we have concluded in 3.5, the collective intelligence of a group results from social aspects between its members. Ideally the test should reflect \u201csocial\u201d aspects of collective intelligence. A group of very intelligent, but uncooperative agents can certainly be used to resolve some complex (information processing) tasks. Yet, in this case one cannot talk about \u201ccollective\u201d intelligence. Collective intelligence always refers to some notion of collaboration. Our test should take this into account.\nWe will discuss both issues in the following.\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.1 Conceptualization of \u201cintelligence\u201d"}, {"heading": "4.1 Conceptualization of \u201cintelligence\u201d", "text": "As mentioned (2.3), we have some answers for the first issue, that is, about how one can conceptualize \u201cintelligence\u201d. The notion of intelligence must be related to the capability of processing/compressing information. The Kolmogorov complexity provides us with tools to derive a mathematical concept of difficulty, which can eventually be used for measuring intelligence. Yet, estimating the Kolmogorov complexity of objects is in itself fairly complicated (due to the halting problem). Also we would like to analyze any ability and not only intelligence.\nTherefore, we will make an abstraction of the kind of test-problem we use to evaluate the intelligence/capabilities of the group. We will henceforth not talk anymore about \u201cintelligence\u201d, but rather about \u201cability\u201d. This way, what we are actually modeling could be any kind of test (also, for instance a physical skill). Thus, the here defined approach is assumed to be independent from any method of measuring intelligence and can easily be adapted to future developments in this area.\nWe will first explain how we make an abstraction of the ability we are measuring. Thereafter, we will explain how one can aggregate the abilities of a group of agents."}, {"heading": "4.1.1 Abstraction from information processing capabilities: Item response functions", "text": "We will mathematically conceptualize abilities making reference to the field of item response theory (IRT) [40]. In IRT, probabilities are assigned about how a person responds to an item; here a test (which is actually a set of items). It is used in psychometrics, where it is useful for instance to provide a framework to analyze how well an assessment works so as to interpret the results and to refine it.\nFor this, an item response function is defined. We will present briefly here the three parameter logistic model [40]. In this model, the probability that a person succeeds on an item is given by:\nP (\u03b1) = c+ 1\u2212 c\n1 + exp [\u2212 a(\u03b1\u2212 b)] (4.1)\nwhere \u03b1 is the person\u2019s ability parameter and a, b and c are the item parameters. They have the following interpretation:\na: The discrimination (scale, slope) represents the maximum slope of the response function (with respect to the person\u2019s ability \u03b1). In other words it reflects whether or not less able persons have indeed a lower chance of succeeding similarly to a very able person in the test.\nb: The difficulty (item location), p(b) = 1+c2 , represents the half-way point between the minimum (c) and and the maximum (1) probability of succeeding. It is also the point where the slope is maximized: P \u2032(b) = a \u00b7 1\u2212c4\nc: The pseudo-guessing, (chance, asymptotic minimum), reflects the probability that a person with infinitely low ability guesses the correct answer: P (\u2212\u221e) = c\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.1 Conceptualization of \u201cintelligence\u201d\nWe will follow a similar approach, which we will adapt to our machine intelligence context. First of all, we will henceforth not talk anymore about persons, but \u201cagents\u201d. We will define some function P\u03bb(\u03b1), which reflects the probability that an agent with some ability (intelligence) \u03b1 finds the correct solution to a binary test problem with difficulty \u03bb. A binary test problem has only two answer possibilities; e.g. true/false or 0/1. Such problems are for instance two class classification or binary sequence prediction. We also assure that both answers are equally likely a priori.\nWe will use here the function:\nP\u03bb(\u03b1) = 1\n2 +\n1 1 + exp [\n2\u03bb \u03b1 ] , (4.2) which is shown in figure 4.1 1. We will henceforth refer to P\u03bb(\u03b1) as being the (expected) accuracy.\nThe function has been designed so as to ensure that every agent has a 100% chance to find the solution of a problem of no difficulty (\u03bb = 0). Thereafter, the probability of correctly solving the problem decreases monotonically with \u03bb. Yet, it decreases slower the higher \u03b1 is. As the difficulty of the problems increases, we suppose that the probability of solving the task converges to 50%, i.e., the answer becomes random. In fact, letting the probability go down to 0% (in the limit) would be less realistic. In this case, one would have to invert the provided answer in order to obtain an agent whose accuracy increases with the problem\u2019s difficulty. Yet, we discuss in our suggestion for future work what might happen when this probability goes below 50% (See 7.5 below).\nThe use of a function P\u03bb(\u03b1) is of course a generalization. The capability of an agent to solve a problem depends of course on the type of problem \u03c0\n1 P\u03bb(\u03b1) = 1 2\n( 1 + exp [ \u22122\u03bb \u03b1 ]) yields similar results (yet the slopes are steeper)\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.1 Conceptualization of \u201cintelligence\u201d\n(e.g. sequence prediction, text recognition, speech recognition,. . . ). Typically one would rather define a probability function P\u03bb(\u03c0, \u03b1). Yet, we will make abstraction of this additional (and difficult to conceptualize) parameter.\nThis simplification can be justified in two ways. First, we might suppose that the considered problem is the \u201cuniversal\u201d problem (or a set of all problems) allowing to measure \u201cintelligence\u201d as it might be defined in future research. Second, the function P\u03bb(\u03b1) might be considered as an average over different kinds of problems.\nOne should note that \u03b1 cannot really be taken as an absolute measure of abilities/intelligence. Yet, if \u03b1i < \u03b1i\u2032 \u21d2 \u2200\u03bb : P\u03bb(\u03b1i) < P\u03bb(\u03b1i\u2032), then \u03b1 can at least be used as a relative measure of abilities/intelligence. The function P\u03bb(\u03b1) considered here in equation (4.2) has this property because it is monotonically non-decreasing for \u03b1. Yet, as this is a fairly strong assumption we will also consider other types of functions P\u03bb(\u03b1) in our suggestions for future work (see 7.2)."}, {"heading": "4.1.2 Aggregation of a group\u2019s abilities: Voting systems", "text": "As we are talking about collective intelligence here, we are considering not only one, but a group of n > 1 agents of abilities \u03b1 = {\u03b11, . . . , \u03b1n}. We will henceforth refer to the group, the set of agents, as N . The individual agents are referred to by i. So as to measure the collective ability/intelligence resulting from the aggregation of the groups, a joint decision making system is required. One of the most general configurations of such a system is for instance a voting scheme.\nThe group will be faced with an evaluation problem of difficulty \u03bb (e.g., the prediction of a non-random series). Each agent will provide its answer ri to the problem (as determined by P\u03bb(\u03b1)). The group might then use an absolute majority voting system to determine its answer. The group will hence correctly solve the problem if more than 50% of the agents have solved it correctly2. In the case where exactly 50% of the individuals solve the problem correctly \u2013 and consequently the other 50% provide the wrong answer \u2013 the answer of the group is undetermined and will be drawn from a random coin flip.\nThe literature about multi-classifier systems [34] provides us with some theoretical results about which performance might be expected from such a majority voting system. Suppose that all agents have the same ability \u03b1, hence the same P\u03bb(\u03b1). Suppose also that the votes are independent of each other, which is somewhat restrictive here, as we cannot really talk about \u201ccollective\u201d ability. In this case, the probability that the group solves correctly the problem (its accuracy) is given by:\nPmaj = n\u2211 k=bn/2c+1 ( n k ) P\u03bb(\u03b1) k (1\u2212 P\u03bb(\u03b1))n\u2212k (4.3)\nYet, when the agents have different abilities, hence different P\u03bb(\u03b1), we can only give an upper and lower bound of the group\u2019s accuracy. For this, we first have to order the individuals by their individual accuracy \u2013 which is in our case\n2 Supposing that we are in a case of a problem with a binary answer (0/1)\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.2 Conceptualization of \u201cintelligence\u201d\nthe same as sorting them by their abilities:\nP\u03bb(\u03b11) \u2264 P\u03bb(\u03b12) \u2264 . . . \u2264 P\u03bb(\u03b1n)). (4.4)\nDefining k = bn/2c + 1, the bounds on the group\u2019s accuracy are given by [34]:\nmaxPmaj = min {1,\u03a3(k),\u03a3(k \u2212 1), . . . ,\u03a3(1)} (4.5)\nwhere \u03a3(m) = 1\nm n\u2212k+m\u2211 i=1 P\u03bb(\u03b1i)\nminPmaj = max {0, \u03be(k), \u03be(k \u2212 1), . . . , \u03be(1)}\nwhere \u03be(m) = 1\nm n\u2211 i=k\u2212m+1 P\u03bb(\u03b1i)\u2212 n\u2212 k m\nWhether the group is closer to the upper or to the lower bound depends on the complementarities of the agents. The upper bound can only be reached if the bad performance of some agents on some problems is systematically compensated by the good performance of other agents on these problems. On the opposite, if all agents perform similarly good or bad on the same problems, the lower bound is approached.\nMajority voting systems are however not the only possible voting system. An interesting alternative are weighted voting systems where the better agent will typically receive a higher weight. Again, the literature about multi-classifier systems [34] provide us with some theoretical results about weighted voting systems. First, it can be expected, that the performance of the weighted version of the voting system performs better than the unweighted version. Within the different weighting schemes which might be used, weights proportional to log P\u03bb(\u03b1i)1\u2212P\u03bb(\u03b1i) provide the best results.\nWe will hence define three voting systems for our experiments:\n1. A majority weighting system where each agent has the same weight.\n2. A weighting system taking into account the abilities of the agents. We use hence a weight proportional to \u03b1.\n3. The optimal weighting system using weights proportional to log P\u03bb(\u03b1i)1\u2212P\u03bb(\u03b1i) .\nOf course, so as to obtain a complete measure of the collective ability, we should observe the group\u2019s performance on a set of problems M with different difficulties \u03bb and aggregate the results using a weighted average:\n\u03a6\u0304(M, n) := \u2211 j\u2208M wM(\u03bbj)\u03a6\u0304(j, n), (4.6)\nwhere wM(\u03bbj) is a weighting function (thus \u2211 j\u2208M wM(\u03bbj) = 1) and \u03a6\u0304(j, n) := \u03a6\u0304(\u03b11, . . . , \u03b1n;\u03bbj) is the (empirically measured) average score/accuracy of the group on problem j with difficulty \u03bbj . As mentioned in 2.3, the current approach would be to express the difficulty by the Kolmogorov Complexity, \u03bbj \u221d KU (j). Consequently, the weight would be represented by a universal distribution wM(\u03bbj) := 2\n\u2212KU (j). We have however already expressed our doubts about the use of a universal distribution in 2.3 and will hence not further discuss this for now and leave it over to 5.4.3.\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.2 Introducing social aspects"}, {"heading": "4.2 Introducing social aspects", "text": "We have discussed how to conceptualize intelligence as an abstract ability. We have done so using item response functions as abstraction, and voting systems as aggregation of abilities/intelligence. Yet, this aggregation is without any form of interaction, cooperation or specialization among the agents. The second issue is hence about taking social aspects into account. How to include social aspects in machine intelligence tests has until now been mostly unaddressed by the academic literature. The only exeption is a multi-agent extension of the individual intelligence tests performed in Insa-Cabrera et al. [30], where several cooperation and competition settings are studied. In our work, we are concerned about collective intelligence, and we will focus on how tasks are allocated.\nTesting on several problems\nAs we mentioned in section 3.5, one way in which a group of agents can improve performance over an individual is by a good allocation of tasks, mostly of this leads to specialization. Accordingly, instead of having to solve only one problem, the group faces several (m \u2264 n) problems of difficulties \u03bb = {\u03bb1, \u03bb2, . . . \u03bbm}. We will henceforth refer to the set of problem as M. The individual problems are referred to by j.\nEach of the n agents can assign itself to only one of the problems at the same time. However, it can switch from one problem to another after each round. Among all the agents assigned to the same problem, the solution provided by the group will be determined via the aforementioned voting system. This way we require the group to coordinate/cooperate so as to distribute themselves among the various problems.\nMoreover, the problems have different levels of difficulty \u03bb. Thus, ideally, more and/or the most able agents should work on the most difficult problems.\nTask allocation using the threshold model\nIn order to make the group work, an algorithm must be proposed which allows the agents to allocate themselves to a problem without using a centralized decision maker (so that we can truly talk about \u201ccollective\u201d intelligence). One such algorithm is the dynamic task allocation algorithm inspired from division of labor observed with social insects such as ants ([4, 2, 41]). More precisely, it is inspired from an ant type called the Pheidole genus. As observed by Wilson [65], in such ant colonies two distinct types of ants are present. Minors are occupied with day-to-day tasks such as breeding. Majors take care of rather exceptional tasks such as defense. Wilson [65] observed however that if minors were retrieved from the colony, majors will consequently also perform the former\u2019s task.\nBonabeau et al. [1] showed that the division of labor/task allocation observed for the Pheidole genus, could easily be modeled trough a so-called threshold model.\nIn this model, a so-called stimulus Sj associated to the task j is used. It represents the urge of performing a task, which must be translated into assigning more agents to a problem. For the real ant colony, the stimulus corresponds to some pheromones emitted by the ants. In our case Sj might be a measure\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.2 Introducing social aspects\ninversely related to the average performance over the last few rounds of the considered problem.\nAlso we define some response thresholds \u03b8ij associated by each agent i to each of the tasks j. This threshold reflects the matter of size above which the stimulus Sj should be so that the agent i will allocate itself to the corresponding task j. More precisely the probability that the later happens is given by the following sigmoid function:\nProb(i 7\u2192 j) = S2j\nS2j + \u03b8 2 ij\n, (4.7)\nSuch a sigmoid function can be seen as a continuous step (threshold) function. It is also used in other domains of computer science such as artificial neural networks. It is represented in figure 4.2. The squares are taken to give the function a faster transition from the lower to the higher probabilities.\nIn the real ant colony, thresholds are fixed and depend only on the type of ant. Minors have low threshold and will hence perform the breeding task most of the time. Majors have a high threshold and will only perform this task if the pheromones indicate a high urge to do so.\nWhen being assigned to a problem, the agent will quit this problem with a probability p per round and then select another one (or the same) to allocate itself using the previous rule. The expected number of rounds/steps spent on each problem before considering a switch is thus 1p , as the latter follows a geometric distribution.\nBoth the thresholds and the stimuli vary over time. In the typical model, the stimulus is updated as follows:\nSj \u2190 Sj + \u03b4 \u2212 \u03b2 nj n , (4.8)\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.2 Introducing social aspects\nwhere nj is the number of active agents on task j, n is the total number of agents, \u03b4 is the increase in stimulus intensity per time unit, and \u03b2 is a scale factor measuring the importance of the resources currently allocated to the problem. The model tends therefore to distribute the workforce uniformly across the tasks.\nWe however adapt this \u201cstandard\u201d stimulus update rule to the situation in which it will be used. We can for instance make the stimuli dependent on the average performance of the last few time steps, in a way that the stimulus is higher when the group performed relatively badly on this problem. More precisely, the average performance \u03a8\u0304j we use is represented by an exponentially moving average.\nThe exponential moving average [63] is obtained by attributing decreasing weights to the performances situated far in the past. The update rule for this average over N rounds is:\n\u03a8\u0304j \u2190 \u03c9N\u03a8j + (1\u2212 \u03c9N )\u03a8\u0304j , (4.9)\nwhere 0 < \u03c9N = 2\nN+1 < 1. Of course, this average does not depend on a fixed number of rounds. It depends on by far more than N rounds. Yet, more recent performance has a higher influence than that further in the past. More precisely, in the way the weight \u03c9N is defined, 86% of the total weight is attributed to the N most recent rounds.\nAlso we include a factor 0 < \u03b6 < 1 before the stimulus in order to prevent it to diverge. The rule becomes:\nSj \u2190 \u03b6Sj + \u03b4 \u2212 \u03b2 nj n \u2212 \u03b2\u2032\u03a8\u0304j (4.10)\nIn addition, the tendency to distribute the agents uniformly among the problems must be put into doubt given the different complexities of the problems. We will hence test whether the term\nnj n is useful in our case (see 5.3.1).\nAs mentioned, for the real ants the thresholds are fixed. In the standard model [4, 2, 41], the thresholds are updated in a way so as to avoid unnecessary switching from one task to another. This means that once an agent is attributed to task/problem j, its corresponding threshold will decrease, while that associated to all other tasks increases:\n\u03b8ij \u2190 \u03b8ij \u2212 \u03be (4.11) \u03b8ik \u2190 \u03b8ik + \u03c6 \u2200k 6= j,\nwhere \u03be and \u03c6 are the learning and forgetting coefficients, respectively. The values of \u03b8ij are bounded from above and below: \u03b8ij \u2208 [\u03b8min, \u03b8max].\nParametrizing this model might seem very difficult. Yet, we can inspire from Bonabeau et al. [2] to get a first idea of the parameters\u2019 matter of size. Finally, we propose the following parameter values:\n\u03b8min = 1, \u03b8max = 100, \u03b2 = m\n2 , \u03b2\u2032 = 4, \u03b4 = 4, (4.12)\np = 0.5, \u03be = 2, \u03c6 = 0.5, \u03b6 = 0.98, \u03c9N = 0.33\nAs can be observed, the coefficient \u03b2 corresponding to the share of allocated agents is dependent on the number of problems m. This is because the share of allocated agents is in itself dependent on this number. Typically, its matter of\nChapter 4. An approach based on abstracted intelligence, vote aggregation and task allocation\n4.3 Factors of interest\nsize is nj n \u2248 1 m . We set therefore \u03b2 = m 2 in order to make the whole term \u03b2 nj n independent of m. The threshold allocation model is well suited for our need of a general example algorithm to perform the agent-problem allocation task. It is inspired from social insects and includes hence the communication methods typically used in the swarm intelligence design approach for collective intelligence systems. Some variables such as the stimuli and the model parameters are shared among the agents, which stands for some social interaction between them. Also, it is well described in the academic literature. As we will further highlight in 5.3.2, the model is also able to allocate the agents appropriately in a changing (i.e., dynamic) environment. Moreover, the model can easily be adapted to our specific experimental needs."}, {"heading": "4.3 Factors of interest", "text": "As we mentioned in the introduction, we are interested in evaluating collective behavior where some of the following features (or all of them) are considered:\nNumber of agents: the number of agents is naturally one of the most important features to be considered.\nAgents abilities: the degree of competence of the agents (and their diversity) is key.\nNumber and variety of tasks: we could have considered one task at a time which has to be solved by all the agents. While this will be one of the considered settings, we will explore the much richer problem of the group having to solve several tasks at a time. This implies allocation and, if tasks are different, specialization.\nAgent specialization: if different tasks are used, should we have specialized agents?\nCollective decision making methods: if there are more agents than tasks, then some agents will collaborate on the same task. Collective decision making policies will then be key here.\nOverall, this is the first work which studies the problem of collective decision making and collective task allocation together, in the context of performance evaluation.\nChapter 5\nExperiments\nGiven the approach defined in the previous chapter, we will perform some experiments so as to observe some interesting dynamics in collective intelligence as announced in the goal statement (1.1)."}, {"heading": "5.1 Voting on one problem", "text": "First of all, we will run the simulations using only one problem. This way we exclude the impact of the resource allocation algorithm and focus on the design of the voting system. What we would like to observe is how the group performs as compared to the average performance of the agents when facing the problem alone. A first thing to observe is the performance of a perfectly homogeneous group. As we will show in a theoretical way, we expect that the group can achieve any level of performance, given that the agents perform only slightly better than random, that there are a sufficiently high number of agents and that the agents\u2019 votes are independent. Yet, when the group is composed of agents with very different levels of intelligence we might observe other, more surprising results. We will for instance simulate a situation where one very intelligent agent interacts with many less intelligent agents (and vice versa)."}, {"heading": "5.1.1 Homogeneous group of agents", "text": "To start with, we will imagine a very simple setup. There will be only one problem to be solved by various agents with the same ability \u03b1.\nOne can expect that the more agents work on the same problem, the better the accuracy of the group will be. More precisely, in a homogeneous group of agents, the stochastic process composed of the (independent) answers provided follows a binomial distribution: r1, r2, . . . rn \u223c B(n, P\u03bb(\u03b1))\nHence the probability that among the n provided answers, k are correct is given by (see equation (4.3))\nProb(k) =\n( n\nk\n) P\u03bb(\u03b1) k(1\u2212 P\u03bb(\u03b1))n\u2212k (5.1)\nThe average share of individuals providing the correct answer will be P\u03bb(\u03b1) > 50% with a variance of \u03c32 = P\u03bb(\u03b1)(1\u2212P\u03bb(\u03b1))n . There exists always a number of\nChapter 5. Experiments 5.1 Voting on one problem\nagents n, so that the global accuracy is higher than any desired level. In fact, suppose that we would like to achieve an accuracy higher than 1\u2212 \u03b3. For this, the lower bound of the accuracy\u2019s one-sided confidence interval with confidence 1\u2212 \u03b3 must be above 50%. In other words, we must be 1\u2212 \u03b3 percent sure that the majority provides the right answer. Using the normal approximation1 of the binomial distribution (n & 30), this can be expressed as:\nP\u03bb(\u03b1)\u2212 z\u03b3\n\u221a (P\u03bb(\u03b1)(1\u2212 P\u03bb(\u03b1))\nn > 50% (5.2)\nwhere z\u03b3 is the \u03b3th quantile of the normal distribution. There is always an n so that this is verified.\nWe will choose here a setup where the difficulty and the ability are in the same matter of size. This way we ensure that the agents are neither too good, nor too bad for the problem. More precisely we choose: \u03bb = \u03b1 = 12.\nFigure 5.1 illustrates the explained phenomenon. We plot the average accuracy of the group Pn over 30000 votes for a group composed from n = 1 to n = 10 agents. As expected the accuracy increases as the number of agents increases.\nYet, one unexpected phenomenon can be observed. It seems that the accuracy of a group with an even number of agents is similar to that of the group with one agent less (i.e., the nearest and lower odd number). In order to get an intuition of what is happening, we will compare the accuracy of a group with one and two agents. Previously, we will simplify our notations: we define p := P\u03bb(\u03b1) and q := 1 \u2212 P\u03bb(\u03b1). The accuracy of one agent is of course equal to P1 = p. The accuracy of a group of two agents is composed of two terms:\n1 The normal approximation of the binomial can be used, as we are in a process of independent and identically distributed random variables.\n2The accuracy P\u03bb(\u03b1) of the agents is hence 61.9%\nChapter 5. Experiments 5.1 Voting on one problem\nP2 = p 2 + 122pq. The first term reflects the probability that both agents are right; the second term the probability that only one of both agents is correct and that the coin flip necessary to determine the group\u2019s (random) answer is correct. This accuracy can be simplified to P2 = p \u00b7 (p + q) = p. Hence both accuracies are the same, P1 = P2\n3. We demonstrate that this is true for any couple of an even number of agents and the nearest, lower odd number of agents. Actually what we state here is that:\nP2n = P2n\u22121 \u2200n \u2208 N (5.3)\nThis is proven in appendix A.1. We have hence observed a first interesting dynamics in collective intelligence. In this simple setting, increasing the number of agents on a problem will increase the performance of the group. This is important to be considered when designing a collective intelligence test. Yet, in this particular case, one must make the distinction between an even and an odd number of agents working on a problem. It happens frequently \u2013 as we will confirm as well for heterogeneous groups here below \u2013 that going from an odd number of agents to the nearest superior even number does not increase the performance as much as going from an even number of agents to the nearest superior odd number.\nAs we could understand from the developments above, this is due to the fact that a group with an even number of agents might find itself in a situation of an undetermined voting result, forcing it to use a coin flip to determine its answer. This has a negative impact on its performance and puts it at a disadvantage as compared to a group with an odd number of agents. This should also be considered when designing and interpreting the results of a collective machine intelligence test."}, {"heading": "5.1.2 The impact of adding low performing agents", "text": "We will now study a different setup with one problem. The aim here is to test the three voting systems we have defined: majority voting, voting weighted by \u03b1 and using the optimal weight. In the previous point the voting systems all boiled down to majority (i.e., unweighted) voting as all agents were identical. We must hence put ourselves in a case of a heterogeneous group.\nWe will study a group of very distinct agents; some have a very high ability4, which provides them with an accuracy close to 100%. Others have a very low one which makes their decision quasi-random. We will then observe how each of the joint decision making system is impacted when more and more agents with very low abilities are added. More precisely we define our unique problem as having a difficulty of \u03bb = 10. Our group always includes one \u201cgood\u201d agent with ability \u03b1 = 205. Then we successively add \u201cbad\u201d agents with ability \u03b1 = 56. The upper graph in figure 5.2 shows the average accuracy of the group Pn over 50\u2019000 votes as a function of the number of bad agents for the three voting systems. The same experiment is repeated in the lower graph with \u03b1 = 8 7 for the bad agents. We will mainly discuss the upper graph.\n3 Similarly, P3 = p3 + 3p2q and P4 = p4 + 4p3q + 1 2 6p2q2 = p2 \u00b7 (p+ q) \u00b7 (p+ 3q) = P3 4 As compared to the difficulty 5 The accuracy of such agents is 76.9% 6 The accuracy of such agents is 51.8% 7 The accuracy of such agents is 57.6%\nChapter 5. Experiments 5.1 Voting on one problem\nA first thing we observe is that the performance of the majority voting system is significantly inferior to the performance of both weighted systems. It is also not surprising that the optimal weighting system has the best performance. Our results are hence coherent with what we know already form multi-classifier systems [34].\nIt can also be observed that the performance of the absolute weighting scheme decreases with the number of bad agents in the group. Yet, this decrease stagnates when adding more and more bad agents. This is because two distinct phenomena are underlying here. Adding underperforming agents is bad in the beginning as the weight of the high performing agent decreases and more weight is given to agents whose performance is close to random. At the same\nChapter 5. Experiments 5.1 Voting on one problem\ntime, we know that a group of agents with an individual accuracy only slightly above 50% can achieve any level of group accuracy given a high enough number of group members. Hence there is a compensating effect which increases the performance when we add bad agents. We know also that when the number of agents goes to infinity, the performance will increase up to 100% for all three voting systems.\nA similar, yet less pronounced effect of performance decrease is observed for the system weighted by \u03b1. The system using the optimal weight seems completely unaffected by this phenomenon. It provides constant performance independently of the number of bad agents. This is due to the fact that the accuracy of the bad agents is so close to random that the weight associated to their votes is nearly zero.\nIn the lower graph of figure 5.2 we see that the performance decreases (or respectively stagnation) depends only on the very low performance of the bad agents. When this ability increases, the agents contribute positively in all three voting systems.\nWhat might surprise us here is that we observe again a difference between an even and odd number of (bad) agents in the group. In the unweighted version we observe that including an even number of bad agents provides superior performance than an odd number. To see what happens we compare the situation when only one bad agent is present with that of a group including two bad agents. If only one bad agent is included, it happens frequently that the bad agent votes for the bad answer and the good agent for the good one. More precisely if one considers that the good agent is perfect (i.e., 100% accuracy) and that the bad agent is fully random (i.e., 50% accuracy), this situation occurs in exactly one case out of two. As the weights of both agents are identical, a coin flip must be used to determine the answer. When two bad agents are present, undetermined votes will never occur as there are in total an odd number of agents. Here it happens frequently that both agents have opposite votes and the good agent will provide the good answer. For one perfect and two random agents this will happen in 50% of the cases. Only in 25% of the cases the bad agents will both vote for the bad answer and outvote the correct answer of the good agent. The bad agents will hence \u201ccancel\u201d each other out and disturb the good agent less.\nIn the version weighted by \u03b1 we observe that the performance of an odd number of bad agents is similar to the nearest lower even number of agents, P2n\u22121 = P2n. Put differently, an even number of (all) agents is similar to the nearest lower odd number of agents. This is the phenomenon we observed and explained already in the previous point 5.1.1 for a homogenous group. We explained that this phenomenon is due to the fact that the performance of an even number of agents is reduced due to the necessity of using a coin flip in the case of an undetermined vote. Here we show that it also applies to a heterogeneous group. Yet, this is only possible as the ability \u2013 hence the weight \u2013 of the good agent \u03b1 = 20 is a multiple of the bad agents\u2019 ability \u03b1 = 5. Hence an undetermined vote is possible, however only for more than 205 = 4 bad agents in the group. In the lower graph of figure 5.2 this is not the case anymore. Here we observe only the just explained effect that an odd number of bad agents disturb the good agent more.\nChapter 5. Experiments 5.2 Simplified version of the allocation model with several\nproblems"}, {"heading": "5.1.3 The impact of adding high performing agents", "text": "Figure 5.3 shows the result for a similar experiment as the previous one. However here, we add good instead of bad agents. Trivially the performance increases for all three systems and converges to 100% when we add more good agents. The performance of the weighted systems is superior or equal to the unweighted version. Both weighted versions have similar performances. Which exact weight is used is thus not of a great importance here. The only role of the weighting system is here to annihilate the influence that bad agents can have on the vote outcome.\nWhat might surprise us is that the performance of a group including an even number of good agents is identical for all voting systems. This is because when the good agents disagree about the correct answer, the bad agent decides about the outcome of the group, whether the system is weighted or unweighted.\nLet us shortly also discuss what happens when the system includes an odd number of good agents. In the unweighted version it might happen that a coin flip is needed to determine the group\u2019s vote. Again, the one bad agent is more able to disturb the good ones. In the weighted version however a coin flip is never necessary."}, {"heading": "5.2 Simplified version of the allocation model", "text": "with several problems\nIn the previous section we analyzed the voting systems by working on only one problem. In this section we introduce for the first time a setup with several problems in order to verify the correct functioning of the allocation model and get familiar with it. However we will not immediately use the allocation model as defined in 4.2. We will first use a simplified allocation model. Also we will modify our setup so as to impose an \u201coptimal\u201d allocation. This is to verify that\nChapter 5. Experiments 5.2 Simplified version of the allocation model with several\nproblems\nour model is indeed capable of \u201ccorrectly\u201d allocating the agents. We will first discuss the latter point and then explain how we simplify the model."}, {"heading": "5.2.1 Imposing an appropriate allocation: specialized agents", "text": "Until now we have not discussed how the agents should be \u201coptimally\u201d allocated. The question is hence how a group should allocate itself so as to maximize its performance. First of all, we need to define what we understand as performance measure. As mentioned above in equation (4.6), the group\u2019s performance can be expressed as:\n\u03a6\u0304(M, n) := \u2211 j\u2208M wM(\u03bbj)\u03a6\u0304(j, n)\nA difficulty with this expression consists in defining the \u201cright\u201d weight wM(\u03bbj), as it is currently still an open question in individual intelligence tests (see 2.3). There exist an infinite number of possibilities to define this function.\nHence, we will make an abstraction of the aggregated measure of performance. Instead we define a more intuitive notion of the \u201coptimal\u201d allocation. What we will do is to associate one problem j\u0302i to each agent i, such that the agent performs systematically better, say because the agent is a \u201cspecialist\u201d in this problem. We will refer to agents which have been associated to a specific problem as \u201cspecialized\u201d agents.\nMore precisely, what we will do is to increase the ability \u03b1 for the agents\u2019 associated problems. Therefore, our abilities do not depend only on the agent i anymore, but also on the considered problem j. Say that \u03b1i is the \u201ctypical\u201d ability as we have used it until now. Then the ability on the associated/specialized problem is just a multiple of this ability, i.e., \u03b1ij\u0302i := 3\u03b1i, while the ability on all other problems remains the same \u03b1ij := \u03b1i.\nThis can be represented in the form of a matrix:\nProblem 1 2 3 4\nA ge\nn t 1 3\u03b1i \u03b1i \u03b1i \u03b1i 2 3\u03b1i \u03b1i \u03b1i \u03b1i 3 \u03b1i 3\u03b1i \u03b1i \u03b1i 4 \u03b1i \u03b1i 3\u03b1i \u03b1i 5 \u03b1i \u03b1i 3\u03b1i \u03b1i 6 \u03b1i \u03b1i \u03b1i 3\u03b1i\nOne can understand that if the ability on an associated problem if sufficiently higher, the agent should be allocated to this problem8 for most reasonable weighting functions. We have hence not proven that allocating each agent to its associated problem is optimal; more precisely, we cannot talk about optimality as we have not defined a weighting function. Yet, it is mostly \u201cappropriate\u201d to allocate an agent to the problems on which it performs systematically\n8 A necessary condition for this is for instance that each problem has at least one associated agent. Otherwise this problem would be unserved and the group would hence achieve a performance of 0% accuracy. Allocating at least one (un-specialized) agent would result in performance of at least 50%\nChapter 5. Experiments 5.2 Simplified version of the allocation model with several\nproblems\nbetter. We will henceforth not talk anymore about the associated problem, but about the \u201cappropriate\u201d one.\nOf course, the table above can even be generalized to many other kinds of situations one would like to simulate, simply by modifying the pattern of \u03b1ij . One could for instance suppose that each agent has not only one but several appropriate problems."}, {"heading": "5.2.2 A simplified allocation model", "text": "Our model as it has been defined now is very complex and includes many parameters. We need hence to simplify it. So as to eliminate a few parameters we will first work with a model, where the response thresholds \u03b8ij are static. In order to be mostly allocated to its appropriate problem, the response threshold of agent i takes a low value on its appropriate problem and a high value on all others.\nWe will also simplify the stimulus update rule. In order to eliminate at least one parameter, we will not consider the term \u03b2\nnj n anymore. We have already put\nthe usefulness of this term into doubt previously (4.2). The update rule depends hence only on the average performance \u03a8\u0304j of the problem, which depends in turn on the appropriateness of the allocation. As a result, the simplified update rule is:\nSj \u2190 \u03b6Sj + \u03b4 \u2212 \u03b2\u2032\u03a8\u0304j (5.4)\nConcretely, this means that the stimulus to allocate more agents to a problem depends only on its current performance, and not on the share of agents allocated to it. More precisely, the lower the performance, the higher will be the stimulus to allocate more agents to the problem."}, {"heading": "5.2.3 Simulation with specialized agents and the simplified allocation model", "text": "Figure 5.4 shows the percentage of the (discrete) time the agents are allocated to their appropriate problem \u2013 henceforth noted by %Approp \u2013 as a function of the ratio \u03b8noApprop \u03b8Approp , that is, the quotient of the threshold of a non-appropriate problems and that of the appropriate problem9. More precisely, the values shown here correspond to an average over 20 runs of a simulation with 1000 time steps. We will simulate two agents, each specialized in one distinct of the two problems.\nThis percentage of correct allocation is an interesting value to consider as it can be seen as a proxy for agent\u2019s performance independent of the problems\u2019 difficulties. As explained, the agents has a higher ability (3\u03b1) on its appropriate problem, which is the reason why it is better to allocate it mostly to this specific problem. Hence, the average ability of the agent on its problems increases with this percentage: \u03b1\u0304 = 3\u03b1%Approp + \u03b1(1\u2212%Approp).\nOf course when \u03b8noApprop \u03b8Approp = 1, the agents are allocated randomly, that is 50% of the time to their appropriate problem and 50% to their non appropriate. Yet, as this ratio increases, the share of times the agents are appropriately allocated increases as well and becomes significantly different form a random allocation.\n9 The exact values have been chosen so that: \u03b8noApprop + \u03b8Approp = \u03b8max\nChapter 5. Experiments 5.3 Standard version of the allocation model with specialized\nagents\n\u03b8Approp\nFor instance when \u03b8noApprop is three times bigger than \u03b8Approp, the agents are on average correctly allocated in about 80% of the time.\nOur allocation model works hence as expected and the agents are allocated according to the thresholds."}, {"heading": "5.3 Standard version of the allocation model with", "text": "specialized agents\nIn this section, we will use the standard version of the threshold model so as to verify that the model is able to provide us with coherent values of the thresholds. We will still impose the appropriate allocation by specializing each agent to the problem. We will then adapt our model until we achieve the appropriate allocation. This way, we are sure that our parameter setting is correct. We will also discuss the introduction of the term \u03b2\nnj n into the stimulus update rule."}, {"heading": "5.3.1 Allocation model testing and improvements", "text": "The graph in figure 5.5 shows the share of correct allocations for the four the different settings we will discuss next. The values shown correspond to an average over 50 runs of a simulation with 1000 time steps. The simulations are made on 2 problems with difficulty \u03bb = 10. The specialized agents have an ability \u03b1 = 810.\n10 The accuracy of such agents is thus 57.6% on a non appropriate problem and 80.3% on their appropriate one\nChapter 5. Experiments 5.3 Standard version of the allocation model with specialized\nagents\n(a) Standard model with 2 agents\nWe use a setup with two specialized agents on two problems (the same as in the previous section). As we can observe, with the standard setting of the allocation model, the allocation does not differ from random. The agents are allocated 50% of the time to one of the problems. As we have seen before, our allocation model works correctly once the threshold of the appropriate problem is lower than that of the non-appropriate problems. There must be a problem with our threshold update model in equation (4.11), which we repeat for convenience:\n\u03b8ij \u2190 \u03b8ij \u2212 \u03be \u03b8ik \u2190 \u03b8ik + \u03c6 \u2200k 6= j,\nwhere j is the problem agent i has been allocated to. Immediately we realize that nothing allows our thresholds to be systematically lower on the appropriate problem. The model has been designed to decrease the threshold on the problem the agent is currently allocated to, in order to avoid some unnecessary switching. Yet, this does not imply that the agent is also allocated to its appropriate problem.\nChapter 5. Experiments 5.3 Standard version of the allocation model with specialized\nagents\n(b) Model depending on the individual performance with 2 agents\nWe must hence introduce a new feature to correct this. One might adapt the model so that it generates lower values for the appropriate problem. Yet, this would somewhat be \u201ctoo easy\u201d as the agents might for instance not know which is the problem they are specialized to i.e., the problem they are good at. We will consider here something we have not taken into account until now: the individual performance of the agent so far on each of the problems. We will estimate this as the exponentially moving average (see equation (4.9)) of the accuracy over the last N = 100 answers provided of agent i on this problem j, noted \u03a8\u0304ij . In figure 5.6 one can observe that this empirical estimate of the accuracy converges indeed rapidly to the underlying accuracy P\u03bbj (\u03b1i).\nWe will use this measure so as to ensure that the agents have the tendency to be allocated to problems which they are good at and to be kept away from problems at which they are bad at. The model becomes:\n\u03b8ij \u2190 \u03b8ij \u2212 \u03be \u00b7 \u03a8\u03042ij (5.5) \u03b8ik \u2190 \u03b8ik + \u03c6\n\u03a8\u03042ik \u2200k 6= j\nThe value of the threshold to which the agent is currently allocated to will thus be decreased faster, depending on how good the past performance on this problem was. At the same time problems to which the agents are currently not allocated, and on which they have performed relatively badly, will see their threshold increase faster. We take the square of our individual performance measure \u03a8\u03042ij so as to reinforce this effect, that is to have a better distinction between the higher values (close to 100%) and the intermediate values (close to random i.e., 50%) of \u03a8\u0304ij .\nThe result is shown in the bar (b) of the figure 5.5. Here we achieve a correct allocation in 70% of the cases which distinguishes significantly from random. We\nChapter 5. Experiments 5.3 Standard version of the allocation model with specialized\nagents\nobtain hence the desired allocation. Also intuitively, it seems natural to take the individual performance into account to allocate the agents.\n(c) Model depending on the individual performance with 4 agents\nBar (c) in figure 5.5 shows the share of correct allocations when we use the previous model with four agents. One can observe that the fact of including more agents actually increases the likelihood that the agents are correctly allocated. This might be surprising in the first moment.\nYet, there is reasonable explanation for this. If only two agents are working on the two problems, it happens frequently that both are allocated to the same problem. The other problem is hence un-served/abandoned which implies that the group\u2019s accuracy on it is at 0%. In a perfectly random allocation this happens actually 50% of the time. As a consequence of the resulting performance drop, the associated stimulus will increase rapidly and force an agent to be allocated to it. As the related threshold is still lower for the agent who has just left the problem, the latter will be reallocated to it. This phenomenon reduces hence the possibility that both agents exchange their problems.\nHowever, when four agents work on the two problems, the likelihood that all agents will work on the same is actually reduced. In a random allocation, this happens in 12.5% of the cases. This way a disequilibrium in the number of allocated agents might persist for a longer period of time. The agents can thus explore other problems. In the case that they perform relatively well on another problem they might allocate themselves permanently to it. Another agent \u2013 actually the least performing \u2013 will sooner or later leave this problem and in turn allocate itself to a problem at which it is better. This way a group of more agents is better in allocating itself in a more optimal way.\n(d) Model depending on the individual performance and share of allocated agents with 2 agents\nIn equation (4.10), we defined a stimulus update rule depending on the share of all agents which are allocated to a problem: Sj \u2190 Sj +\u03b4\u2212\u03b2 njn \u2212\u03b2\n\u2032\u03a8\u0304j . Initially, we put the term \u03b2\nnj n into doubt as it would result in a uniform distribution\namong the problems which might be undesirable. Yet, we see in bar (d) of the figure 5.5 that introducing this term \u2013 yet with a smaller parameter than the one used for the problem\u2019s performance (\u03b2 = 1 < \u03b2\u2032 = 4)11 - actually increases the likelihood that the two agents are allocated correctly. This might seem again very surprising.\nThe explanation of this phenomenon is however also related to that of the previous point. In fact, the inclusion of this term makes it easier for the agents to exchange their problems. As previously explained, when only two agents are working on two problems it happens frequently that the agents are allocated to the same problem. It happens then that the agent who joined this problem at last is reallocated to its initial problem as its threshold for the latter is still lower. Yet, with the inclusion of the term\nnj n this issue is better resolved. In\n11 We have m = 2, hence \u03b2 = m 2 = 1. Typically it makes no sense to compare just the\nparameters. In fact, one has to compare the while terms \u03b2 nj n and \u03b2\u2032\u03a8\u0304j . Yet in our case nj n and \u03a8\u0304j have the same matter of size - they are closely located at 50%. Hence a comparison makes sense here.\nChapter 5. Experiments 5.3 Standard version of the allocation model with specialized\nagents\nthe case where both agents are on the same problem this term equals one on this problem, and zero on the unserved problem. This means that the stimulus for the unserved problem will decrease. At the same time, the stimulus for the served problem will decrease. This implies that both agents \u2013 not only the one who joined last \u2013 might go over to the unserved problem. Consequently, this mechanism makes it more likely that the agents exchange their problems so as to allocate themselves appropriately.\nThe inclusion of this term is hence not designed to ensure a uniform allocation of the agents. Rather its role is weaker through the lower associated parameter. It should be included in order to better handle situations in which some problem have no allocated agents and/or all agents are allocated to only one problem."}, {"heading": "5.3.2 Remark on dynamic environments", "text": "In the previous subsection we improved our allocation model and verified that it performs reasonably well i.e., the specialized agents are mostly allocated to their appropriate problem. Of course, our goal is not to achieve of the best performing allocation model. Yet, having a model which performs reasonably well is important to gain some insights from our experiments.\nIt might be interesting to put forward one important aspect of our model: it is dynamic. In the static version of our model we saw that we could get arbitrarily close to a 100% appropriate allocation by fixing a high value of\n\u03b8noApprop \u03b8Approp .\nIn the dynamic version, this is not possible as the agents do not know which problem they are specialized on. They have to find it out by exploring the different problems. The fact of exploring regularly problems to which they should not be allocated reduces hence also the group\u2019s performance.\nYet, the dynamic version has one important advantage over its static equivalent: It is adapted to changing environments. Suppose for instance that after a few time steps, the difficulties of the problems change or that the agents are specialized on another problem. Such a dynamic environment could moreover be a very interesting feature of the testing environment for collective intelligence. It might be that two groups perform similarly in a static environment, but the one performs better in a dynamic environment. One might therefore conclude that this group is \u201cmore intelligent/able\u201d than the other. A dynamic model is able to reallocate the agent adequately.\nIn figure 5.7, we perform the following experiment: We use again two specialized agents and two problems. We let the simulation run over 2500 time steps. After half the time \u2013 indicated by the red line \u2013 we exchange the association/specialization of the agents. On the y-axis we indicate for each agent whether it is allocated to problem 0 or problem 1. Yet, so as to smooth this curve the moving average over 20 periods is plotted. The blue agent is initially specialized on problem 0 and the green agent to problem 1.\nWe observe that there is first a phase of intense switching from one problem to the other, i.e., exploration. Then the agents become rapidly more and more allocated to their appropriate problem, i.e., exploitation. Yet, we observe still some attempts of problem switching. When the problem associations change, these attempts become more and more successful and last over a longer time. Finally, the agents completely allocate themselves to their new appropriate problem.\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nWe have hence verified that our model has indeed this property of reacting correctly to a dynamic environment. This indicates that our parameters are chosen in a way that the thresholds and stimuli do not converge to extreme values which would make problem switching impossible."}, {"heading": "5.4 Use of the problems\u2019 difficulties in the allo-", "text": "cation task\nUntil now we have implicitly supposed that the agents do not know which of the problems are the easiest and which are the most difficult. Differently put, the agents did not use any information about the problems\u2019 difficulties \u03bb to allocate themselves. This will change now and the difficulties \u03bb become publicly known. We will modify the stimulus update rule in order to allow a higher share of allocation\nnj n and to maintain a systematically higher stimulus on the most\ndifficult problems. So as to simplify the setup \u2013 yet without any loss of generality \u2013 we will suppose that there exist three levels of difficulty: \u03bbl < \u03bbm < \u03bbh. In the threshold model we might let the parameters of the stimuli update become dependent on the difficulty:\nSj \u2190 \u03b6Sj + \u03b4\u03bbj \u2212 \u03b2\u03bbj nj n \u2212 \u03b2\u2032\u03a8\u0304j (5.6)\nFirst, we will choose \u03b2\u03bbl > \u03b2\u03bbm > \u03b2\u03bbh . This implies that a higher share of agents can be allocated to the difficult problem without that the stimulus decreases significantly. The opposite holds for the easy problem to which we want to allocate the least share of our resources possible. Also we choose \u03b4\u03bbl < \u03b4\u03bbm < \u03b4\u03bbh in order to force the stimulus to be systematically higher on the difficult problem, and vice versa.\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nThe advantage of this approach is that we simulate a fairly general situation in a simplified way. Phenomena which will appear on three problems are typically the same as those on four or more. Also we can independently defined the three values of \u03b4\u03bbj and \u03b2\u03bbj , without explicitly defining the function which links the difficulty \u03bbj to these parameters; \u03b4(\u03bbj) and \u03b2(\u03bbj). However, for every triple of the of values for \u03b4\u03bbj and \u03b2\u03bbj , there exist functions such that \u03b4\u03bbj = \u03b4(\u03bbj) and \u03b2\u03bbj = \u03b2(\u03bbj), respectively."}, {"heading": "5.4.1 Simulation", "text": "We perform the following experiment. Our group includes 12 homogeneous agents with ability \u03b1 = 5. They have to be allocated to 3 problems with difficulties \u03bbl = 3;\u03bbm = 5;\u03bbh = 6. Hence an individual agent would achieve an accuracy of 73.1%, 61.9% and 58.3% respectively.\nThe graphs in figure 5.8 shows the results of this experiment over 30 runs of a simulation with 1000 time steps. The upper graph shows the average group accuracy for the three problems. In thelowere we show also the average number of agents allocated to each of the three problems.\nWe analyze four different cases of parameter settings. Case (a) serves as a reference where all parameters have the same values on the three problems. We use the typical parameter values \u03b4 = 4 and \u03b2 = m2 = 1.5. In the other cases multiply the parameters we want to increase (\u03b2\u03bbl and \u03b4\u03bbh) by a factor fhigh > 1. Similarly we multiply the parameters we want to decrease (\u03b2\u03bbh and \u03b4\u03bbl) by a factor flow < 1.\nWhat one can observe is that we produce indeed the desired effect. We observe in the lower graph that as our parameters become more distinct on the problems, more agents are retrieved from the easy problem to be allocated to the most difficult one. As a consequence the performance on the former decreases considerably, while that on the latter increases \u2013 due to its high difficulty \u2013 only slowly. As we can see in (c) and (d), the parameters can even be chosen so as to achieve superior performance on the more difficult problems. Being able to achieve superior performance on the most difficult problems might be an important ability of the group, as a test might give higher importance to these problems.\nAgain, how to weight the importance of task difficulty is an open question. We will get back to this issue in subsection 5.4.3.\n5.4.2 Should the group be provided with a measure of difficulty?\nWe conclude thus that providing the agents with a measure of difficulty can indeed help them to allocate their resources better among the problems. This is especially true if we consider that a test might value some problems more than others; whether the most difficult or rather the easiest. Typically, the group must be informed about how important each problem is. This would be done by providing a weighting function which we noted wM(\u03bbj) in equation (4.6).\nYet, we will argue that not providing the group with this information might actually be more reasonable. This is for two reasons. First, it is unclear which measure exactly should be provided to the group. More precisely, a measure of difficulty must always be relative to the agent. Second, an intelligent group does\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nnot need this information and can generate/explore all necessary information itself. We will discuss each argument in turn.\nAny agent independent difficulty measure is ambiguous\nLet us start with the first argument. Until now, our measure of difficulty was given by the parameter \u03bb. As our response function is monotonically decreasing\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nthis parameter represents a valid relative measure of difficulty. Mathematically this means that \u2200\u03bbl < \u03bbh,\u2200\u03b1 : P\u03b1(\u03bbl) < P\u03b1(\u03bbh). Yet, this is true for any monotonically increasing function applied to \u03bb (e.g. log \u03bb, \u03bb2, e\u03bb). In the specific case of our simulation our measure of difficulty is well defined given the response function P\u03bb(\u03b1) from equation (4.2). Yet, our response function P\u03bb(\u03b1) is just one arbitrary choice among an infinite number of other possible functions. Thus one cannot give a \u201cuniversal\u201d definition of difficulty which one should provide to the group.\nMost importantly however, there exist many cases where \u03bb (and its monotonic transformations) are not even a relative measure of difficulty. As mentioned, we have chosen P\u03bb(\u03b1) to be a monotonic function. Yet, there is reason to believe that there exist cases where this is not true anymore. In 7.2, we will present a model of single peaked response function. In this model each agent has a specific range of \u03bb at which he is good at. Yet, for problems with a significantly higher, and even lower values of \u03bb, the agent performs significantly worse. In this example \u03bb is definitely not an acceptable (relative) measure of the problems difficulty.\nAlso using the formalism we have defined so far, we can easily generate situations where \u03bb ceases to be a valid measure of difficulty which would allow the group to define its allocation priorities. In 5.2.1 we considered specialized agents to whom we have assigned an appropriate problem at which they were good at. Also in such a case \u03bb ceased being an acceptable (relative) measure of difficulty. This is because the agent\u2019s accuracy P\u03bb(\u03b1) does not depend on \u03bb exclusively anymore but also whether or not the agent is specialized on the considered problem.\nConsider again the setup with 12 homogeneous agents we have just used in our simulations in 5.4.1. Let us however introduce specialized agents here. More precisely, nine agents are specialized on the most difficult one (\u03bbh = 6) and the remaining three are associated to the problem with intermediate difficulty (\u03bbm = 5). In its typical parameter setting \u2013 thus using the unmodified stimulus update rule in (4.10) \u2013, the group performs best on the problems with the highest \u03bb. More precisely the average group accuracies are: Pn(\u03bbl) = 83.9%; Pn(\u03bbm) = 85.25%; Pn(\u03bbh) = 87.0%. Hence \u03bb is not an adequate measure of difficulty for the group.\nThe problem with the difficulty measure \u03bb is that it is agent independent. It does hence not take into account phenomena such as non-monotonic response functions and agent specialization.\nTo sum up, we have shown, first, that there is no universal definition of measure of difficulty, and second, that there exist cases in which providing such a measure would be misleading for the group. A valid measure of difficulty must be specific to an agent.\nThe environment itself provides sufficient information about the problems\u2019 \u201cdifficulties\u201d\nThe second and supposedly most important argument why the tester should not provide a measure of difficulty is that the agents should actually find out which problems are the most difficult for them. Actually this could make the test even more discriminative. An intelligent group might rapidly find out which problems require more resources, while a less intelligent group would struggle\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nperceiving the test environment correctly. To show this, we will illustrate that the group can actually distribute its resources according to the problems\u2019 difficulties without receiving any indication about it. Let us define for this an objective. In case (b) of figure 5.9 we see that the group achieves nearly uniform performance on all problems. As this is an easily verifiable criterion, let us suppose that this is the group\u2019s objective. So as to attain this objective, the group must be aware of the problems\u2019 respective difficulty.\nFigure 5.9 shows different attempts to uniformize the groups\u2019 performance. We state the average performance on all three problems over 30 runs with 1000 rounds. As a comparison, case (a) shows the performances with the typical parameter setting. Also case (b) relates to the performance which we have achieved by adapting the stimuli update parameter knowing the values of \u03bb, as in case (b) of figure 5.8.\nIn our first attempt \u2013 shown in (c) \u2013 we give more emphasis to the problem\u2019s performance in our stimulus update rule. This is done simply by giving a higher value to the parameter \u03b2\u2032 associated to the average performance over the last few steps on the problems \u03a8\u0304j . In our second attempt \u2013 shown in (d) \u2013 we use the square of this performance estimate \u03a8\u03042j . This way we give even more emphasis to the differences which might coexist between the problems performances. In (e) we make the values of \u03b4 (the constant term) and \u03b2 (the parameter associated to the share of agents allocated) dependent on the group\u2019s performance. Similar to our approach taken in 5.4.1, the value of \u03b4 should be high on difficult problems and \u03b2 should be low on difficult problems. We use \u03a8\u0304j as a proxy for the problem\u2019s difficulty. Therefore we divide the former, and multiply the latter by \u03a8\u0304j .\nIn all three cases we have empirically determined the parameter setting which makes the group\u2019s performance as uniform as possible on all three problems12.\nBefore discussing the results of our, one should note that the performance on all problems is inferior to that using the standard parameter setting. This is surprising as the performance decrease on one problem should be compensated by an increase on another one. The reason behind this is that by modifying our parameters in order to achieve a more uniform allocation, we have decreased the importance of the term reflecting the share of allocated agents (\u03b2\nnj n ). As a\nconsequence it happens more frequently that the problems are abandoned and achieve a performance of 0%. We confirm hence again the interpretation we have given to this term in 5.3.1.\nWe observe that our attempts to control the performance across the problems achieve only mediocre results. The reason behind this is that we use the wrong estimate for the problems difficulty: \u03a8\u0304j . First, as \u03a8\u0304j reflects the very recent performance on the problem, it is a very fluctuating measure. Also this measure depends strongly on the number and abilities of the currently allocated agents and hence not only on the problem\u2019s difficulty. Suppose that at one moment the group achieves uniform performance on all problems over a few rounds. Hence, \u03a8\u0304j is the same for all problems and does not reflect the difficulty properly anymore. In this case, the only term in the stimulus update rule which will differ across the problems is the share of allocated agents,\nnj n . As the performance is\nuniform, we know that this share must be higher on the most difficult problems.\n12 As expressed by the difference between the maximum and minimum performance.\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nThis will again force the group to allocate itself with a uniform number of agent across all problems. Agents will thus be taken away from the most difficult problem in favor of the easiest problems, which will in turn result in a nonuniform performance. Consequently, using \u03a8\u0304j as difficulty measure, a uniform performance cannot be a stable equilibrium of the allocation .\nSimilarly to what we argue above with respect to \u03bbj as a measure of difficulty, the biggest default of \u03a8\u0304j is that it does not reflect the difficulty as it is perceived by each of the agents. As we will further discuss below, it might be that each agent perceives the difficulty of a problem differently. It is for instance possible that one agent is good at a problem where another agent performs rather badly. However, for another problem the opposite might hold. Hence the difficulty is a very subjective measure and this must be taken into account. A measure reflecting the aggregate performance of several agents, such\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nas \u03a8\u0304j , is not acceptable. What we need now is a better measure of the problems\u2019 difficulties, which can be easily estimated by the agents and which reflects their individual difficulty with the problem. In 5.3.1 above we introduced \u03a8\u0304ij , which is the exponentially moving average of agent i\u2019s accuracy on problem j. As mentioned, this measure converges rapidly to the underlying accuracy P\u03bbj (\u03b1i). Considering what we just said about a \u201cuniversal\u201d definition of problem difficulty, this measure is acceptable as such, even though \u2013 or rather because \u2013 it depends on the considered agent. This measure13 has a precise interpretation and takes specific characteristic of the agent (e.g., specialization in some types of problems) into account.\nNext we need to aggregate the agent-dependent measure of difficulty \u03a8\u0304ij , into another measure which reflects the group\u2019s perception of the problem\u2019s difficulty, i.e., its priority to allocate its resources to it. This might be expressed as a generalized mean of \u03a8\u0304ij over all agents. The generalized mean over all agents is given by:\n\u3008\u03a8\u0304ij\u3009p = ( n\u2211 i=1 \u03a8\u0304pij ) 1 p\n(5.7)\nwhere p \u2208 R. Using the generalized mean, many types of aggregations can be expressed and all of them make sense.\nFor p = \u2212\u221e, the generalized mean is equals to the minimum of the terms \u3008\u03a8\u0304ij\u3009\u2212\u221e = mini \u03a8\u0304ij . We would hence use the accuracy of the worst agent on this problem as the group\u2019s measure of difficulty. For p = +\u221e, the generalized mean is equals to the maximum of the terms \u3008\u03a8\u0304ij\u3009+\u221e = maxi \u03a8\u0304ij . We would hence use the accuracy of the best agent on this problem as the group\u2019s measure of difficulty.\nFor all other values of p, the generalized mean lies between these two values \u3008\u03a8\u0304ij\u3009\u2212\u221e \u2264 \u3008\u03a8\u0304ij\u3009p \u2264 \u3008\u03a8\u0304ij\u3009+\u221e. It is increasing with p : \u2200pl < ph : \u3008\u03a8\u0304ij\u3009pl \u2264 \u3008\u03a8\u0304ij\u3009ph . The usual mean is of course obtained when p = 1. For p = 0, we obtain the geometric mean, which is more dependent on the lower values of \u03a8\u0304ij , hence the least performing agents.\nThe question is now which value of p the group should choose in order to properly aggregate the individual difficulties. As we will briefly show here, in some situations, \u3008\u03a8\u0304ij\u3009p should rather be close to the minimum individual accuracy, while in others it should be close to the maximum. Suppose for instance that all agents perform equally well on all problems except that there is one problem where only one of the agents performs well. Definitely the latter problem is the most difficult for the group and a measure close to the minimum accuracy reflects the group\u2019s ideal allocation priorities. Suppose now that all agents perform equally well on all problems except that there is one problem, where one of the agents performs extremely well. Definitely the latter problem is the least difficult for the group and a measure close to the maximum accuracy reflects the group\u2019s ideal allocation priorities.\nWhich value of p should be chosen depends thus on the specific case (i.e., the form of P\u03bb(\u03b1) and which other factor (such as specializations) intervene in this function, the agents and the problems present). We will however not discuss\n13Note that \u03a8\u0304ij expresses rather the \u201ceasiness\u201d of a problem, hence its inverse is the corresponding measure of difficulty\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nhow exactly p should be chosen. This is left over to the \u201cintelligence\u201d of the group. Anyway in our case all values of p provide us with the same mean as the group is homogeneous and thus \u03a8\u0304ij is the same for each agent.\nIn case (f) of figure 5.9, we use \u3008\u03a8\u0304ij\u3009p as a difficulty measure and we see that we achieve nearly uniform performance across the problems and this without the performance loss seen in the previous cases. Choosing different values for the parameter of stimulus update rule, it has been verified that the group could also achieve superior performance on the most difficult problems, similarly to what we observe in figure 5.8 when the values of \u03bb were public.\nWe have thus illustrated how the group can explore the environment in order to find out which problems are the most difficult and which deserve hence more resources. Of course, in general, this is not a trivial task for the group as it requires a high level of communication between the agents. Therefore it can \u2013 and we argue that it should \u2013 be used as a part of the intelligence test. The group should not be provided with a measure of difficulty, but find this out for itself."}, {"heading": "5.4.3 Uniform problem weighting", "text": "We just saw that by providing a measure of difficulty \u2013 as expressed here by the response function parameter \u03bb \u2013, but also by computing on its own a measure of difficulty \u3008\u03a8\u0304ij\u3009p, the group could adapt its allocation model so as to take into account that some problems require more resources than others. More precisely, more difficult problems might be considered more important by the tester and might hence receive a higher weight wM(\u03bbj) we used in our expression (4.6) of the group\u2019s performance:\n\u03a6\u0304(M, n) := \u2211 j\u2208M wM(\u03bbj)\u03a6\u0304(j, n)\nActually this weighting scheme wM(\u03bbj) will again be important here and needs some further discussion.\nWe just claimed that the tester should not provide the group with a measure of the problems difficulty \u03bbj . Yet, the importance of each problem, i.e., the weighting scheme wM(\u03bbj) must be provided to the group. This is because in order to achieve the highest score possible, the group must know how its results on the individual problems inM will be aggregated into one single performance measure \u03a6\u0304(M, n).\nHowever, providing wM(\u03bbj) to the group, while requiring that it has to find its own estimates of the \u03bbj makes no sense. Actually, wM(\u03bbj) contains a lot of information about the problem\u2019s difficulty. Let us take the example where the tester attributes more weight to the most difficult problems. In this case the function wM(\u03bbj) is simply a (normalized) monotonically increasing function of \u03bbj . Yet, as we said before any monotonically increasing function applied to a (relative) measure of difficulty (suppose that it is valid in the specific case considered) is again a measure of difficulty. Hence, if we claim that the tester might not provide \u03bbj , then it can also not provide wM(\u03bbj).\nAs a consequence the weighting of the problems importance must be uniform, as it is the only (non random) weighting scheme which provides no information about the problems difficulties. And there are actually some reasons which speak in favor of uniform weighting.\nChapter 5. Experiments 5.4 Use of the problems\u2019 difficulties in the allocation task\nA first argument is related to what we said about dynamic versus static environments. In 5.3.2 we argued that dynamic environments are more interesting for testing, as two groups might perform similarly in a static, but differently in a dynamic environment. An interesting type of dynamic environments is of course those where the problems\u2019 difficulties change over time and where the agents have to reallocate themselves accordingly. If one would weight the problems according to their difficulty, one would have to re-weight them after the difficulties change. Consequently, one would have to communicate the weight change to the group. This is of course be a perfect indication to the group that the environment has changed. Based on this indication, a group could hence adapt itself to the new environment and perform well, even though it would have performed badly without this indication. Choosing a uniform weighting is therefore useful for disguise to the group any change in a dynamic environment.\nAlso we argued above that there exists \u2013 at the moment \u2013 no \u201cuniversal\u201d definition of the problems difficulty14. Hence the weighting function is also undefined. If the exact weights cannot be defined, a uniform weighting is an easy way to avoid this difficulty.\nMoreover, it seems reasonable to suppose that an \u201cintelligent\u201d group performs well at a variety of problems. More precisely, it should perform well on difficult problems, but also on easier ones. A less intelligent group will only perform well at the easy problems. A uniform weighting system reflects this requirement, while it is still able to discriminate groups of different abilities.\nHowever, one might argue that non-uniform weights are still required for a test in order to discriminate properly. Uniform weighting is badly suited in a situation where a very intelligent agent performs badly on an easy problem and is thus outperformed by a less intelligent one. An explanation of this is for instance that the easy problem is actually \u201ctoo easy\u201d for the intelligent agent. In 7.2 below we present an example illustrating such a case. Suppose that our set of problems M contains one easy and one difficult problem. Hence, the intelligent and the less intelligent agent perform well on one of both problems. In this case, a test using a uniform weight might not discriminate both agents or even reveal the wrong agent as being the most intelligent. More weight must definitely be given to the most difficult problem. Yet, a higher importance can also be given to a specific type of problem by including more instances of it into the set of problemsM. By including more instances of the most discriminative problems into M, a uniform weighting can still be maintained.\nOne might also criticize the use of uniform weighting by the fact that it diverges from the current approach taken in individual intelligence measurements. As we have explained in 2.3, the universal measure of (individual) intelligence in equation (2.5) \u2013 as proposed by Legg and Hutter [37] \u2013 weights the problems using a universal distribution. Following this approach, one would use wM = 2\n\u2212KU (j) as the most appropriate weight. The measure of performance on the set of problems M is thus given by: \u03a8\u0304(M, n|wM = 2\u2212KU (j)) :=\u2211 j\u2208M 2\n\u2212KU (j)\u03a6\u0304(j, n) . An open question is now how one can aggregate this performance measure over several problem setsM in order to obtain a universal measure of collective intelligence \u03a5\u0304 . Another problem with this approach is that it is subject to the criticism we put already forward in 2.3. The easiest\n14For intelligence tests, a difficulty measure derived from the Kolmogorov complexity might become this definition\nChapter 5. Experiments 5.5 Use of the agents\u2019 ability in the voting process\nproblems receive the highest weight. However, uniform weighting is still coherent with an approach based on algorithmic information theory. Again we use Kolmogorov Complexity for our collective intelligence measure. Yet, instead of expressing the complexity of the individual problems in M, one can actually express the complexity of the whole set M, which is in fact nothing else than the definition of our testing environment, previously denoted by \u00b5. The complexity KU (M) is simply the shortest input to the UTM U , which simulates all problems inM. As mentions, the next step is to define a set E of different environments \u2013 thus problems sets \u2013 on which the group will be evaluated. Note then by \u03a6\u0304(M, n|wM = 1m ) =\u2211 j\u2208M 1 m \u03a6\u0304(j, n), the performance measure using a uniform weighting. Then similar to the definition of universal measure of individual intelligence in (2.5), one could express a measure of universal collective intelligence as:\n\u03a5\u0304(n) = \u2211 M\u2208E 2\u2212KU (M)\u03a6\u0304(M, n|wM = 1m ) (5.8)\nHowever, we are still not convinced of the use of a universal distribution. As a preferable measure of collective intelligence we propose for instance the average complexity of the testing environment on which the group performs significantly better than random (where the \u201cmargin\u201d above random performance should be a parameter).\nAs the space E , and even the set of environments M with a specific complexity KU (M) = K, is infinite, a difficulty of such an approach is however to develop a test which is executable in a finite time. Some ideas about how this might be done, can be found in Herna\u0301ndez-Orallo and Dowe [23]."}, {"heading": "5.5 Use of the agents\u2019 ability in the voting pro-", "text": "cess\nIn the previous section we discussed how the group could use information about the problems\u2019 difficulties and whether this measure should actually be provided to the group. We will now do the same with the ability measure \u03b1. This information might be used in two ways. First, it can be used in the voting process and second, in the allocation process. We will leave the latter point for future work and only discuss the first here.\nIn the voting process, \u03b1 might be used to give more weight to the most intelligent agents. In 4.1.2 we have defined three weighting systems: a majority voting system, a weighting system proportional to \u03b1 and the optimal weighting system proportional to log P\u03bb(\u03b1i)1\u2212P\u03bb(\u03b1i) . We have already made an extensive discussion about these weighting systems in 5.1."}, {"heading": "5.5.1 Should the group be provided with a measure of ability?", "text": "There is however one question, which is still open: In how far are the agents able to find out about their ability themselves or must it be provided by the tester (assuming that it is actually known, which is not trivial)? Our answer to this question is similar to that of the previous point 5.4.2 where we argued\nChapter 5. Experiments 5.6 Imitating agents\nthat the tester should not provide any information about the problems\u2019 difficulties. Similarly, we argue that the group should not be provided with an ability measure. And the reasons are actually identical to that of the previous point.\nFirst of all, there exists (currently) no valid measure of intelligence. In the case of a monotonically increasing response function our parameter \u03b1 can at least be used as a relative measure of ability: \u2200\u03b1l < \u03b1h,\u2200\u03bb : P\u03bb(\u03b1l) < P\u03bb(\u03b1h). Yet, any monotonically increasing transformation of \u03b1 is also a valid measure. In most cases, the response function might however not even be a relative measure of ability due to phenomena such as agent specialization or different types of problems, in which case the response function is not monotonic anymore.\nSecond, the agents can find out \u2013 while exploring the environment \u2013 how well the agents perform on each of the problems. Again \u03a8\u0304ij , the exponential moving average of agent i\u2019s accuracy on problem j contains the necessary information. This measure depends on both, the agent and the considered problem. Therefore, it takes into account any specific phenomenon we have just mentioned (specialization, problem types). As explained, \u03a8\u0304ij is an empirical measure of P\u03bbj (\u03b1i). And as we know, weights proportional to log P\u03bbj (\u03b1i\n1\u2212P\u03bbj (\u03b1i) provide the op-\ntimal results. The group can thus simply use weights proportional to log \u03a8\u0304ij 1\u2212\u03a8\u0304ij in order to use the best weighting system possible.\nWe have verified in figure 5.6 that \u03a8\u0304ij varies indeed closely P\u03bbj (\u03b1i), without of course converging perfectly to it. In order to verify that the estimate \u03a8\u0304ij can successfully be used in the weighting scheme, let us consider a heterogeneous group of seven agents with abilities \u03b1 = {1, 2, 3, 4, 5, 6, 7} on a problem with difficulty15 \u03bb = 5. Then the average group accuracy over 50 runs with 1000 rounds equals 73.1% using the underlying value of the agents\u2019 accuracy P\u03bbj (\u03b1i). Using the empirical value \u03a8\u0304ij , the accuracy drops only very slightly to 72.8%. This is to compare to an average group accuracy of 68.1% when majority voting is used. We conclude that \u03a8\u0304ij can indeed be used as a proxy for P\u03bbj (\u03b1i) in the weighting scheme.\nDetermining the right measure of agents ability to be used for voting is hence fairly easy and should also be manageable by the \u201cless intelligent\u201d groups. No indication must/should be provided by the tester.\nAgain, it should be noted that using an empirical value \u03a8\u0304ij is well suited for dynamical environments. Suppose that for one reason or another, the agent\u2019 accuracy changes over time. Then \u03a8\u0304ij will slowly take this into account and the voting weights are adapted accordingly."}, {"heading": "5.6 Imitating agents", "text": "Until now we have supposed that the agents provide their answer independently. This might be considered as a very restrictive hypothesis when studying \u201ccollective\u201d intelligence. Typically one would expect that cooperation should result in positive interactions and increase the performance of the group.\nWe will now analyze a form of cooperation, which might possibly result in negative interactions. Up to now the agents\u2019 answers have followed a binomial\n15 The individual accuracies of the agents are thus 50.0%, 50.7%, 53.4%, 57.6%, 61.9%, 65.9% and 69.3% respectively.\nChapter 5. Experiments 5.6 Imitating agents\nprobability distribution, which implies that the answers are independent from each other. This will change now.\nFirst, we will suppose that the n agents make their decisions one after each other (sequentially), and not simultaneously as previously supposed. We will then suppose that the agents are somewhat \u201clazy\u201d and inspire themselves from the answers provided previously. As before, the first agent will provide its (binary) response according to r1 \u223c P\u03bb(\u03b11). The following agent will inspire itself partially from the previous one, so that: r2 \u223c (1\u2212 \u03c7)P\u03bb(\u03b12) + \u03c7r1, where 0 < \u03c7 < 1 is the imitation rate. Similarly, the lth agent to decide provides an answer inspired by all previous answers:\nrl \u223c (1\u2212 \u03c7)P\u03bb(\u03b1i) + \u03c7\nl \u2212 1 l\u22121\u2211 i=1 ri (5.9)\nOne might expect that the resulting performance of the group is actually worse, even though \u2013 or rather \u2013 because the agents are actually collaborating/interacting. As explained in 5.1.1 for a homogeneous group of agents, the (independent) answers provided follow a binomial distribution r1, r2, . . . rn \u223c B(n, P\u03bb(\u03b1)). The average share of agents providing the correct answer will be the same as the accuracy of the agents P\u03bb(\u03b1) > 50%. Yet, the variance of this average share is \u03c32 = P\u03bb(\u03b1)(1\u2212P\u03bb(\u03b1))n . Thus the higher the number of agents n, the higher is indeed the likelihood that the majority of agents votes for the right answer.\nHowever, when we introduce imitating agents \u2013 hence when the answers provided are not independent \u2013 this is not given anymore. Take the extreme case where \u03c7 = 1. In this case the answer provided by all agents is that of the first agent. The average accuracy is the same as previously. However, its variance is that of the Bernoulli distribution (so the same as for one, here the first agent): \u03c32 = P\u03bb(\u03b1)(1 \u2212 P\u03bb(\u03b1)). Therefore, we cannot use the law of big numbers so as to increase the group\u2019s accuracy.\nWe have hence shown analytically that some specific form of collaboration/imitation will actually reduce the performance, at least in the particular case of homogeneous and fully imitating agents. Through our simulations we would like to generalize this result to other cases. For this we will work on one problem only. We will observe what happens when the agents are heterogeneous and not fully imitating \u03c7 < 1. It is also interesting to observe what happens if the abilities \u03b1i are public knowledge. In this case, each agent could wait for the answer of the more able/intelligent agents and inspire itself from their answers. One might believe that such a setting will result in superior performance of the group, as the vote of the best agents receive a higher importance. Yet, given the previous explanations stating that the benefits of the law of big numbers are reduced, it is believed that this is actually detrimental to performance."}, {"heading": "5.6.1 Random imitation", "text": "In figure 5.10 we show the average group performance (over 30 runs of 1000 time steps) for imitating agents which provide their answers (hence imitate) in a random order. The test setup is the same as in the previous section 5.5. That is, seven agents of ability \u03b1 = {1, 2, 3, 4, 5, 6, 7} work on a problem of\nChapter 5. Experiments 5.6 Imitating agents\ndifficulty \u03bb = 516. A majority voting system is used. As can be observed, the performance decreases with the imitation rate. As explained this is due to the loss of independence of the individual votes, which increases the variance of the share of correct votes. When \u03c7 = 1, the answer provided by the group is the one provided by the first agent. As the latter is selected at random, the group\u2019s accuracy is the average of the individual accuracies which is 58.4% in our case."}, {"heading": "5.6.2 Best imitation", "text": "Let us now see what happens when the agents provide the answers in the order of their abilities; that is, from the most accurate to the least accurate. The agents are hence imitating the better agents. On the same setup as above, the performance increases, yet only slightly. Again, when \u03c7 = 1 the group\u2019s accuracy is that of the agent having provided its answer at first, which is here the best agent having an accuracy of 69.3%.\nAs explained above, there are actually two underlying effects at work here when the agents start imitating more and more. First, there is the increase of the answers\u2019 dependence, reducing the likelihood that a majority will vote for the right answer. Second, more importance is given to the decisions made by the best agents, which should increase the group\u2019s accuracy. Which of both effects is dominant depends actually on the specific setup which has been chosen. In our case, the accuracy of the best agent (69.3%) is actually superior to the group using majority voting without imitation (68.1%). It was thus to expect that the groups accuracy would increase when \u03c7 goes from 0 to 1. However,\n16 The individual accuracies of the agents are thus 50.0%, 50.7%, 53.4%, 57.6%, 61.9%, 65.9% and 69.3% respectively.\nChapter 5. Experiments 5.6 Imitating agents\nit is not necessarily \u2013 actually rarely \u2013 the case that the group\u2019s performance is lower than that of the best agent. It happened in our case as we are in presence of a very heterogeneous group with very good and very bad agents. When the bad agents do not imitate the good agents, their performance is very close to random and \u201cdisturb\u201d the good agent (as discussed in 5.1.2), which has a very negative impact on the group\u2019s performance. The latter might hence be increased when the agents start imitation the better agents. Imitation contributes hence positively here by increasing the weight of the good agents and to decrease that of the bad agents in a heterogeneous group.\nHowever, if the group is less heterogeneous \u2013 i.e., the individual accuracies are more similar \u2013 it is less important to increase the importance of the good agent and imitation is actually detrimental to performance. This is what can be observed in figure 5.11b, where we have replaced the bad (i.e. disturbing) agents by some better ones \u03b1 = {4, 4, 4, 4, 5, 6, 7}17. In this case, imitation has a detrimental impact on the group\u2019s performance as the independence of the answers is more beneficial than having the agents imitating the answers of the better agents.\nWe can thus confirm that imitation, and also the imitation of the better agents, is most generally detrimental to performance. More precisely, this statement is true precisely for groups which are not excessively heterogeneous and/or sufficiently big so that the performance of the group with independent votes is higher than that of the best agent. Giving more importance to the most accurate agents should not be achieved by imitation, but rather by weighting. Actually, this means also that the independence of the agent\u2019s answers is not just a restrictive hypothesis of our report, but rather a beneficial property for the group\u2019s performance.\nOur results are in coherence with those of others. Orle\u0301an [46] investigates the dynamics of a group where some agents are truly informed and others are pure imitators of the informed agents (their vote is determined through the majority vote of the informed agents). He shows that for a very small share of imitators, the latter\u2019s individual performance is actually superior to that of the individual informed agents. Also the imitators do not impact the collective performance negatively as they simply replicate/multiply the answers of the informed agents. However, he shows that when the share of imitators in the population is above a certain threshold, the individual performance of the imitators, but also that of the group decreases strongly as more agents start imitating.\n17 The accuracies of the agents are hence now 57.6% for the first four agents 61.9%, 65.9% and 69.3% for the three remaining.\nChapter 5. Experiments 5.6 Imitating agents\nChapter 6\nAnalysis of the results"}, {"heading": "6.1 Modeling of intelligence", "text": "Our work represents a step forward on how one can model (collective) intelligence. For this we used the approach defined by item response theory by stating a response function in equation (4.2):\nP\u03bb(\u03b1) = 1\n2 +\n1 1 + exp ( 2 \u03bb\u03b1 )\nThis approach was at the same time very simple, but still quite general. First of all, the model depends on two parameters only; one \u2013 the ability \u03b1 \u2013 characterizing the agent and the other \u2013 the difficulty \u03bb \u2013 characterizing the problem. Moreover, this function is monotonically decreasing with \u03bb and monotonically increasing with \u03b1. Also, it converges to an accuracy of 50% as the problem gets more and more difficult.\nAll these properties however could easily be modified. In 7.2 we will see that with a slight modification in the previous function, one can also model single peaked functions. We argue however that the approach taken in 5.2.1 is actually even more general. In the latter section we defined \u2013 instead of a constant ability for each agent \u2013 an ability matrix \u03b1ij . Many specific situations could be modeled this way. We discussed the case where the agents are specialist in one problem 5.2.3 or one specific type of problem (see 7.3). As mentioned, also non-monotonicity could be modeled this way, without even modifying the response function. In 7.5 we will briefly discuss agents with accuracies lower than random."}, {"heading": "6.2 Collective intelligence tests", "text": "Our approach has also allowed us to get some ideas about how collective intelligence test might be designed.\nChapter 6. Analysis of the results 6.3 Collective decision making"}, {"heading": "6.2.1 How to introduce a social dimension into collective intelligence tests", "text": "One innovative advancement of our approach deals with how to test social aspects of collective intelligence. We proposed that the group should not be tested on only one, but several problems at the same time. Therefore, the group must be able to organize itself so as to allocate its resources across all problems. As we could understand from our experiments, allocating the agents appropriately across the problems is not a trivial task. Each agent might for instance be good at a different problem. Allocating the agents in a way so that each agent is allocated to a problem at which he is good is not simple. Most important is however that the allocation task forces the group to communicate \u2013 for instance about the agent\u2019s abilities and the problem\u2019s difficulties \u2013, which is an important aspect of collective intelligence."}, {"heading": "6.2.2 Dynamic environments", "text": "We have also argued in 5.3.2 that dynamic environments \u2013 i.e., environments in which for instance the problems\u2019 difficulties change over time \u2013 are more difficult and might be exploited to discriminate groups. Two groups might perform similarly on static problems, yet one of them might be better in a dynamic environment than the other."}, {"heading": "6.2.3 Which information to provide?", "text": "We have also provided an answer to which information should be provided to the group. We have argued that as few information as possible should be provided to the group. We have discussed here about whether the problem\u2019s difficulties (see 5.4) or the agent\u2019s abilities (see 5.5) should be provided to the group. None of these measures should be provided. This is because there exists no \u201cuniversal\u201d definition of the ability or the difficulty. Any measure of agent ability must be specific to a problem and any measure of problem difficulty must be specific to an agent. Moreover, the environment provides enough information about these measures. However, so as to exploit these measures successfully, the agents must be able to communicate about them. Hence letting the agents find out about their abilities and the problems difficulties and to exploit this information collectively, can actually be used as a part of the test. Again the importance of communication appears here, which is \u2013 as mentioned \u2013 an important aspect of collective intelligence."}, {"heading": "6.3 Collective decision making", "text": "One of the most important issues in collective performance of a group is how the group makes a joint decision (for one problem or for several). We have observed several phenomena."}, {"heading": "6.3.1 The dynamics of odd and even number of agents", "text": "We have explained on a theoretical basis that when one increases the number of agents on a problem, the performance of the group should increase. However,\nChapter 6. Analysis of the results 6.3 Collective decision making\nwe observed that this increase is typically different when one goes from an even number of agents to an odd number than vice versa.\nOne reason for this is that in a group with an even number of agents it might arise that the vote is undetermined and that a random flip must be used to decide about the group\u2019s vote. We have shown analytically that in a homogenous group using majority voting, an odd number of agents perform as well as the group with the nearest even number of agents."}, {"heading": "6.3.2 The importance of voting systems", "text": "We have also shown the importance of the voting system. We have shown that in a majority voting system, the performance of good agents could significantly be hampered by the presence of bad agents. Yet, we know that there exists one and only one optimal voting system, in which each agent whose answer is not random contributes positively to the group\u2019s performance. In fact, the optimal weighting system is proportional to log P\u03bbj (\u03b1i)\n1\u2212P\u03bbj (\u03b1i) . This holds also for agents\nwhose performance is actually worse than random as we will discuss in 7.5. As we have shown in 5.4.2, all the ingredients of this system \u2013 the individual accuracy on the problem P\u03bbj (\u03b1i) \u2013 can be easily be estimated by the group."}, {"heading": "6.3.3 Independence of votes", "text": "In 5.6 we have analyzed what might happen if we drop the hypothesis of independent votes. We modeled this by supposing that the agents are imitating each other. This can be seen as a form of cooperation/communication between the agents. As we have shown, even when each agent actually imitates the better agent, one can typically (in cases of fairly homogenous and/or sufficiently big groups) expect that the performance of the group will decrease as the agents\u2019 imitation rate increases. The independence of votes might hence be considered as a strength of the group.\nShowing this has some interesting consequences for one of the most important collective intelligence systems: financial systems. The systems are supposed to be efficient as many agents make their decisions independently from each other. Each market participant makes errors, yet these errors tend to compensate by the fact that the average buying or selling decision of a big number of market participants leads to a price which reflect at each moment the intrinsic value of an asset. Nevertheless, one might criticize the hypothesis that the market participants act independently from each other. In fact, traders base their decisions frequently (not only on their own analysis, but also) on the trading behavior of others, mainly the most successful traders. Also, most traders base their decisions on statements made by big institutions such as rating agencies. If the independence of market decisions is not given anymore, this efficiency mechanism goes out of force as we have shown.\nThis has already been advanced by others, yet in a less formal way. De Keuleneer [11] talking about the over-reliance on rating agencies puts it like this:\n\u201cWe know that a market system based on a multitude of decisions, with plenty of trials and errors, mistakes and successes, will produce a better allocation of capital than a centrally planned mechanism,\nChapter 6. Analysis of the results 6.4 Allocation models\nwhere a few \u201cexperts\u201d make all decisions. The latter may look attractive in theory but fails in practice. In fact experts routinely make bad decisions, because they can be wrong and they can be corrupted, and bad decisions applied globally then lead to massive misallocation of capital and dramatic failures.\u201d\nShowing that in systems where agents inspire themselves from others are less performing \u2013 also when the inspiration comes from the best agents \u2013 has some important implications for the regulation of financial systems. Policy makers should take measures so as to ensure that market decisions become more independent. They should for instance make sure that trades can be made anonymously at the stock exchange and also that the involved amount of money is not revealed. Of course, the over-reliance on rating agencies must be reduced."}, {"heading": "6.4 Allocation models", "text": "The task allocation problem is another important issue in collective intelligence. Our approach has shown some interesting modification of the threshold allocation model which might also inspire other users of this model."}, {"heading": "6.4.1 Avoiding stimulus divergence", "text": "We have proposed a feature avoiding the divergence of the stimulus. We included a factor \u03b6 < 1 in the update rule in (4.10):\nSj \u2190 \u03b6Sj + \u03b4 \u2212 \u03b2 nj n \u2212 \u03b2\u2032\u03a8\u0304j\nIt is surprising that this simple, but effective feature has to our knowledge not been mentioned previously."}, {"heading": "6.4.2 Additional terms in the stimulus update rule", "text": "We have also shown that the stimulus update rule can be adapted with some problem specific terms. In (4.10) we included a term \u03a8\u0304j reflecting the group\u2019s recent performance on the problem. Yet, we showed also that the term \u03b2\nnj n\nshould always remain in the model. This term can (in the standard model) be interpreted as the term forcing the group to allocate somewhat uniformly across the problems. Yet, complemented with other terms (and possibly in combination with a lower associated parameter) its purpose is rather to avoid extreme allocations where one (or more) problems are unserved and/or all agents are allocated to one problem.\nIn 5.4.1, we showed also that it might be useful to transform the parameters of the update rule into functions of other parameters, here a measure of difficulty."}, {"heading": "6.4.3 Adaptation of the threshold update rule", "text": "Finally our experiments suggest that including additional parameters in the threshold update rule might also improve the performance of our allocation model. In equation (5.5), we assured that the agents would typically be allocated\nChapter 6. Analysis of the results 6.4 Allocation models\nto problems they are good at, by making the rule dependent on the agent\u2019s individual performance on the problems:\n\u03b8ij \u2190 \u03b8ij \u2212 \u03be \u00b7 \u03a8\u03042ij \u03b8ik \u2190 \u03b8ik + \u03c6\n\u03a8\u03042ik \u2200k 6= j\nChapter 7\nDiscussions for future work\nIn this chapter we will discuss further ideas for experiments, which can be performed in the context of the here explained approach."}, {"heading": "7.1 Use of agent ability in the resource alloca-", "text": "tion process\nIn 5.5 we discussed the use of an ability measure in the voting process. This information might however also be used to allocate the agents among the problems. More precisely, we will have to allocate the agents to problems whose difficulty \u03bb are suited to the agent\u2019s level of intelligence \u03b1. We will present here an idea about how this might be done. This means again that the difficulties \u03bb of the problems have to be public knowledge.\nTo do this, we can inspire ourselves from the mail retrieval problem. In the latter, a group of mailman has to retrieve the post from various cities. For this we use the typical threshold model to allocate the mailman to the cities. Yet, an additional piece of information is taken into account: the distance dz(i)j between the current city z(i) of mailman i, and other cities j which might be served next. This information is important as the distance is proportional to the time the mailmen are occupied traveling from one city to another.\nThe probability for the mailman i to serve city j next should be higher for the nearer the city is. Following the approach of Bonabeau et al. [2], we can incorporate the distance in our allocation model so that the probability of allocation becomes inversely proportional to it:\nProb(i 7\u2192 j) = S2j\nS2j + \u00b5\u03b8 2 ij + \u03bdd 2 z(i)j\n(7.1)\nThis means that each mailman will have a preferred zone of cities which will be served by him. Applied to our allocation problem, we first have to define for each agent a \u201cpreferred zone\u201d of problems. So as to not waste our resources, the problem should neither be too easy (in which case the agent would better be allocated to a more difficult problem), nor too difficult (in which case the agent does not really contribute to solving the problem and should be allocated to an easier one). Keeping this in mind, it can be observed in figure 4.1 that\nChapter 7. Discussions for future work 7.2 Single peaked response functions\nan intermediate level of accuracy is achieved when \u03bbj \u2248 \u03b1i. We will therefore attempt to associate the problems in a way so as to respect this relation in the best way possible. We will use the \u201cdistance function\u201d: dij = |\u03b1i \u2212 \u03bbj |.\nFor the mail retrieval problem, the threshold updates are also modified. For a mailman i, the threshold is not only lowered for city j he is currently serving, but also \u2013 yet to a lesser extent \u2013 for the neighboring cities N(j):\n\u03b8ij \u2190 \u03b8ij \u2212 \u03be0 (7.2) \u03b8in \u2190 \u03b8in \u2212 \u03be1, \u2200n \u2208 N(j), \u03be0 > \u03be1 \u03b8ik \u2190 \u03b8ik + \u03c6, k 6= j, k /\u2208 N(j)\nThis implies that the mailmen are more responsive to the stimuli \u2013 i.e., the need to retrieve the post \u2013 from a nearby city.\nApplied to our allocation problems we want our agents to be more responsive to the stimuli \u2013 i.e., the need to allocate more agents \u2013 from problems with similar difficulties to the current problem. We could define two problems as being neighbors if their difficulties \u03bb do not differ by more than 10%."}, {"heading": "7.2 Single peaked response functions", "text": "Until now we have supposed that the capability of correctly solving a problem (more precisely the corresponding probability P\u03bb(\u03b1)) is a monotonically decreasing function of the difficulty \u03bb for each agent. Yet, for instance it might be that a very intelligent agent struggles solving very easy problems (say that they are \u201ctoo easy\u201d).\nThere are indeed real life examples of problems on which such a phenomenon arises. More precisely there are tests on which children perform better than adults. One such test is shown below in figure 7.1. The question is: in which direction is the bus driving? Most pre-school children answer this test correctly: The bus is driving to the left1. Most adults however are not able to provide the correct answer, also after a long period of consideration. The justification \u2013 which is also provided by the pre-school children \u2013 why the bus is heading leftwards is that one cannot see the entrance door which one could see if the bus would drive to the right.\n1The discussion is made for countries where traffic is right-hand. The answer is opposite for left-hand traffic countries, such as the United Kingdom or Australia.\n2Source: www.sharpbrains.com/blog/2007/02/24/exercise-your-brains-visual-logic-brain-teaser\nChapter 7. Discussions for future work 7.4 Different types of problems\nIt might therefore be interesting to analyze what happens if P\u03bb(\u03b1) is a single peaked function, i.e., each agent has a specific degree of difficulty \u03bb at which it performs best. An example of such a function might be for instance:\nP\u03bb(\u03b1) = 1\n2 +\n0.9\n1 + exp [ 2 ( \u03bb\u2212 \u03b1 \u03b1 )2] (7.3)\nwhich is plotted in figure 7.23. Here, our measure of intelligence \u03b1 plays two roles. First, the response function reaches its maximum for \u03bb = \u03b1, and reflects thus in some sense the capability of solving difficult problems. Second, \u03b1 reflects how big the variety of problems is the agent can solve as represented by the \u201cvariance\u201d of the function P\u03bb(\u03b1). It is worth mentioning that \u03b1 is now not even a relative measure of intelligence, even though it does still reflect the \u201cintelligence\u201d of the agents.\nWhat we are actually doing here is in somewhat a continuous version of the agent-problem association/specialization we presented in 5.2.1. For this we used a problem specific ability matrix \u03b1ij . Yet, defining an ability matrix is more general than modifying the response function P\u03bb(\u03b1). More precisely, by defining a problem specific ability matrix \u03b1ij one can achieve \u2013 for each couple agents-problem \u2013 the same accuracies P\u03b1ij (\u03bbj) which could be achieved by modifying the response function P\u03b1i(\u03bbj) with constant abilities \u03b1i for each agent. We have already discussed what happens when an agent is specialized to one specific problem in 5.3.1. We will therefore leave the setup explained here for future work.\nNote that for the defined transfer function, a group using the allocation model inspired from the mailman defined in 7.1 would work very well, as this model tends to allocate the agents to problems in a way that \u03b1i \u2248 \u03bbj ."}, {"heading": "7.3 Different types of problems", "text": "It would also be interesting to investigate what happens if we introduce several types of problems, \u03c0. Each agent i has then not only one ability \u03b1i, but a vector of abilities \u03b1i(\u03c0j). Now we see that this is a generalization of the ability matrix \u03b1ij we defined for the specialized agents in 5.2.1. Hence, we will not discuss this further here and leave it over for future work."}, {"heading": "7.4 Intelligence affecting the allocation capabil-", "text": "ity\nUntil now we have assumed that the capability of solving the problem does not affect the task allocation algorithm. Yet, this is a strong hypothesis. It might very well be that very intelligent individuals are also better able to allocate themselves to the most appropriate problem. One might therefore test the impact of making the allocation algorithm dependent on the parameter \u03b1. More\n3P\u03bb(\u03b1) = 1 2 \u2217 [ 1 + 0.9 \u2217 exp [ 2 ( \u03bb\u2212\u03b1 \u03b1 )2]] yields similar results\nChapter 7. Discussions for future work 7.5 Asymptotic performance worse that random\nprecisely, we could add a random noise \u03b7(\u03b1i) to the probabilities of switching to another problem as defined by the threshold model:\nProb(i 7\u2192 j) = S2j\nS2j + \u03b8 2 ij\n\u2217 \u03b7(\u03b1i) (7.4)\nThe size of this random noise \u201cperturbation\u201d is inversely proportional to the intelligence \u03b1i of the agent. This reflects that less intelligent agents are less able to \u201cunderstand\u201d the task allocation algorithm and are hence less precise in determining the correct probabilities. As an example of the noise we might use a log-normal distribution with \u03c3 = 1\u03b1 and \u00b5 = 0. The use of a log-normal distribution ensures that the probabilities remain positive. For high values of \u03b1 the perturbation factor is close to one and the probabilities of the threshold model are barely modified. Yet, for the lower values of \u03b1 the assignments become more random."}, {"heading": "7.5 Asymptotic performance worse that random", "text": "Until now we have supposed that the function P\u03bb(\u03b1) tends asymptotically to 50% \u2013 the performance of a random classifier \u2013 as the problems get more difficult. Yet, one might imagine a situation in which this probability becomes even worse than random (especially for humans). It might thus be interesting to analyze what happens when P\u03bb(\u03b1) becomes lower than 50% for problems which are too difficult for the agent.\nTypically one would expect that this decreases the group\u2019s performance when, for instance, majority voting is used. As we have discussed it in 5.1, bad agents are able to disturb good agents. For agents having an accuracy below 50% this is even more true.\nIt should however be noted that a group using the optimal weighting scheme is actually positively affected by agents having an accuracy P\u03bb(\u03b1) < 50%. In\nChapter 7. Discussions for future work 7.5 Asymptotic performance worse that random\nfact, their associated weights log P\u03bbj (\u03b1i\n1\u2212P\u03bbj (\u03b1i) will be negative, which is actually\nequivalent to inverting their vote.\nChapter 8\nConclusion\nIn chapter 1 we stated our goal as being to observe some interesting dynamics in collective intelligence and investigate how it might be tested. These goals have been met. Yet, our proposals for future work show that there is still much work to do.\nWhat distinguishes our approach from that of others is that we investigate simultaneously the use of task allocation and joint decision making systems.\nOur approach has brought advancements in several areas. First, we have brought ideas about how (collective) intelligence can be modeled. Second, we brought some ideas about how one can perform collective intelligence tests. Our simulations of such tests have provided us with some insights about their underlying dynamics. Then, we have also gained insights about collective intelligence, not just when tested, but more in general. Fourth, our proposals for future work might inspire some future analysis. Finally, for our experiments, we have refined the threshold allocation model and hence shown that it might be adapted to more complex situations.\nThe results of our approach can be used in several ways. First of all, they can be used by others who want to implement task allocation and/or joint decision making systems. Most importantly however, this piece of research should be understood as a first attempt to to develop collective intelligence tests. It complements the ongoing research for individual intelligence tests. We hope that this work will inspire further research in this area.\nThe big question which remains is whether what we are actually looking fore exists. Is it possible to measure (collective) intelligence and give a mathematical definition of it? Or is intelligence a vague concept such as \u201cconsciousness\u2019 or even \u201cbeauty\u201d, which has been created by humans? It might very well be that every notion of intelligence must be specific to a particular (set of) problems. And even if in the future a universal definition of intelligence might be found for individuals, this report shows that extending it to groups brings some considerable challenges."}, {"heading": "Appendix A", "text": "Appendices\nA.1 Proof of equation (5.3)\nWhat we prove here is the following: Accuracy of a group with an even number of agents is similar to that of the group with the nearest and lower odd number of agents:\nP2n = P2n\u22121 \u2200n \u2208 N\nLet us start by an arbitrary even number of agents represented by 2n : n \u2208 N:\nP2n = 2n\u2211 k=b2n/2c+1\ufe38 \ufe37\ufe37 \ufe38\n=n+1\n( 2n\nk\n) pkq2n\u2212k + 1 2 \u00b7 ( 2n n ) pnqn (A.1)\nWhere the first term represents again the probability of all vote combinations which give a majority to the correct answer and the second term represents the probability that a coin flip is needed and successful. In the first term we will make a change to the index so as to start the sum in zero:k\u2032 = k \u2212 n\u2212 1. Also the factor in the second term can be simplified:\n1 2 \u00b7 ( 2n n ) = 1 2 \u00b7 (2n)! n! \u00b7 n! = 1 2 \u00b7 2n \u00b7 (2n\u2212 1) \u00b7 \u00b7 \u00b7 1 n(n\u2212 1) \u00b7 \u00b7 \u00b7 1 \u00b7 n! = (2n\u2212 1)! (n\u2212 1)! \u00b7 n! = ( 2n\u2212 1 n\u2212 1 ) (A.2)\nHence we get:\nP2n = n\u22121\u2211 k\u2032=0 ( 2n k\u2032 + n+ 1 ) pk \u2032+n+1qn\u2212k \u2032\u22121 + ( 2n\u2212 1 n\u2212 1 ) pnqn (A.3)\nWe will now express the group accuracy for the corresponding odd number 2n\u2212 1:\nP2n\u22121 = 2n\u22121\u2211 k=b(2n\u22121)/2c+1\ufe38 \ufe37\ufe37 \ufe38\n=n\n( 2n\u2212 1 k ) pkq2n\u2212k\u22121 (A.4)\nSo as to let appear identical terms as in the expression of P2n, we will apply a mathematical trick here. In our previous example we saw that a factor p+q = 1\nChapter A. Appendices A.2 Description of the implementation\ncould be isolated in the accuracy expression of an even number of agents. Hence we will multiply P2n\u22121 by the same factor:\n(p+ q)P2n\u22121 = 2n\u22121\u2211 k=n ( 2n\u2212 1 k ) pk+1q2n\u2212k\u22121 + 2n\u22121\u2211 k=n ( 2n\u2212 1 k ) pkq2n\u2212k (A.5)\nAs before, we will modify the index of the sums. We would like the terms to have the same exponents. Hence, for the first sum we will apply k\u2032 = k \u2212 n and for the second we will use k\u2032 = k \u2212 n\u2212 1:\nP2n\u22121 = n\u22121\u2211 k\u2032=0 ( 2n\u2212 1 k\u2032 + n ) pk \u2032+n+1qn\u2212k \u2032\u22121 + n\u22122\u2211 k\u2032=0 ( 2n\u2212 1 k\u2032 + n+ 1 ) pk \u2032+n+1qn\u2212k \u2032\u22121(A.6)\n+ ( 2n\u2212 1 n\u2212 1 ) pnqn\nThe last term corresponds to the term with the negative index k\u2032 = \u22121 which appears in the second sum. Next we will use Pascal\u2019s rule ( n k ) = ( n\u22121 k\u22121 ) + ( n\u22121 k ) to group the corresponding terms of both sums:\nP2n\u22121 = ( 2n\u2212 1 2n\u2212 1 ) p2n+ n\u22122\u2211 k\u2032=0 ( 2n k\u2032 + n+ 1 ) pk \u2032+n+1qn\u2212k \u2032\u22121 (A.7)\n+ ( 2n\u2212 1 n\u2212 1 ) pnqn\nThe first term corresponds to the last term of the first sum (k\u2032 = n \u2212 1 ) having no equivalent in the second one. Yet, this term can again be re-injected into the sum:\nP2n\u22121 = n\u22121\u2211 k\u2032=0 ( 2n k\u2032 + n+ 1 ) pk \u2032+n+1qn\u2212k \u2032\u22121 + ( 2n\u2212 1 n\u2212 1 ) pnqn (A.8)\nWhich shows that P2n = P2n\u22121. What we have actually done, is to prove the formula (4.3) known from multi-classifier systems.\nA.2 Description of the implementation\nSo as to perform the previously described experiments, a program has been implemented in Java. We will very briefly describe this implementation here. We will start by describing the involved classes. Thereafter the main idea of the code will be explained.\nA.2.1 Classes\nOur implementation contains four classes:\nAgent\nThis class represents the agents. One important attribute is for instance the ability \u03b1 of the agent. The main role of this class is to implement the functions\nChapter A. Appendices A.2 Description of the implementation\nnecessary to solve the allocation problem using the threshold model. So as to specify the behavior of the agents, several enum classes are used. One specifies for instance the joint decision taking system which is used by the agents: Absolute voting or weighted voting, either by their \u03b1 or using the optimal weighting scheme log P\u03bb(\u03b1)1\u2212P\u03bb(\u03b1) . Another specifies which probabilistic rule for problem Prob(i 7\u2192 j) is used. This can either be the standard version, the version with a log normal noise depending on \u03b1 or the one stemming from the mailman problem. Finally, we also need to specify which type of response function is used. There is the standard, monotonically decreasing and a specialized, single peaked version. Also there are two versions where the agents are specialized to a problem, one with a simplified, static threshold update rule and another with the standard update rule. Finally, there are also two versions of lazy agents, one where the agents imitate the better agents and another where the order of decision taking is randomized.\nProblem\nThis class represents the problems. An important attribute is for instance the difficulty \u03bb of the agent. The main role of this class is to implement the functions necessary to evaluate the agents on the problem they have chosen. Again, problems have distinct behaviors, depending on how the stimulus update rule is implemented. Here we distinguish between a version with and without a term related to the share of agents specialized to the problem. Also we distinguish a version where the parameters of the update rule depend on the difficulty if we decide that this information is known to the agents.\nTest\nThis class represents the test, in which the agents have to face various problems. It has been implemented using a singleton pattern, i.e., there exists only one instance of this class. It implements the principal routine presented below.\nRun\nThis class implements the main function. So as to run the implementation, one parameter has to be provided: the name of an XML file responsible for importing the specifications of the run. An example of such a specification file can be found in figure A.1.\nFigure A.1: Example of an XML specification file\nA.2.2 Principal routine\nThe execution of the implementation is divided into five steps, of which the last four are executed in a loop:\nInitialization During the initialization phase, all instances of the classes will be initialized as specified in the XML file.\nChapter A. Appendices A.2 Description of the implementation\nLoop\nSwitch problems The agents select a new problem according to the threshold model.\nEvaluate All agents have to provide their answer to the problem they have chosen and to vote the group\u2019s answer.\nUpdate All agents update their thresholds \u03b8ij and all problems update their stimuli Sj .\nReport So as to be able to analyze the results of our experimental phase, after each round the classes Agent and Problem will both print some descriptive statistics about their state into a text file. Both files can then be imported into Matlab so as to analyze them. In Matlab we dispose also of method to launch the implementation repeatedly under various specifications. Thisis very useful for our experiments, for instance in order to make comparisons between different specifications and to obtain statistically significant results.\nThe Java environment of Matlab provides also the necessary functions to make automated modifications of the parameters specified in the XML file."}], "references": [{"title": "Quantitative study of the fixed threshold model for the regulation of division of labour in insect societies", "author": ["Eric Bonabeau", "Guy Theraulaz", "Jean-Louis Deneubourg"], "venue": "Proceedings of the Royal Society of London. Series B: Biological Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Adaptive task allocation inspired by a model of division of labor in social insects", "author": ["Eric Bonabeau", "Andrej Sobkowski", "Guy Theraulaz", "Jean-Louis Deneubourg"], "venue": "Biocomputing and emergent computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Swarm intelligence: from natural to artificial systems", "author": ["Eric Bonabeau", "Marco Dorigo", "Guy Theraulaz"], "venue": "Number 1. OUP USA,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Inspiration for optimization from social insect", "author": ["Eric Bonabeau", "Marco Dorigo", "Guy Theraulaz"], "venue": "behaviour. Nature,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Intelligence as the tests test it", "author": ["E.G. Boring"], "venue": "New Republic,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1923}, {"title": "Applying the ant system to the vehicle routing problem", "author": ["Bernd Bullnheimer", "Richard F Hartl", "Christine Strauss"], "venue": "In Meta-Heuristics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Dynamic scheduling and division of labor in social insects", "author": ["Mike Campos", "Eric Bonabeau", "Guy Theraulaz", "Jean-Louis Deneubourg"], "venue": "Adaptive Behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Personality and social intelligence", "author": ["Nancy Cantor", "John F Kihlstrom"], "venue": "Prentice-Hall Englewood Cliffs, NJ,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "On the length of programs for computing finite binary sequences", "author": ["Gregory J Chaitin"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1966}, {"title": "Urban search and rescue robots: from tragedy to technology", "author": ["Angela Davids"], "venue": "Intelligent Systems, IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Problem with the rating agencies is how they are used", "author": ["Eric De Keuleneer"], "venue": "Financial Times: www.ft.com/cms/s/0/", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Ai - what is this? a definition of artificial intelligence. PC Magazine Bulgaria (in Bulgarian, English version at http: // www. dobrev", "author": ["D. Dobrev"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Formal definition of artificial intelligence", "author": ["Dimiter Dobrev"], "venue": "International Journal of Information Theories and Applications,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Ant colonies for the travelling salesman problem. BioSystems", "author": ["Marco Dorigo", "Luca Maria Gambardella"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "A computational extension to the Turing Test", "author": ["D.L. Dowe", "A.R. Hajek"], "venue": "Proceedings of the 4th Conference of the Australasian Cognitive Science Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "IQ tests are not for machines", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "yet. Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Watson: Beyond Jeopardy", "author": ["David Ferrucci", "Anthony Levas", "Sugato Bagchi", "David Gondek", "Erik Mueller"], "venue": "Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "The tragedy of the commons", "author": ["Hardin Garrett"], "venue": "Science, 162(3859):1243\u2013", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1968}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "National Institute of Standards and Technology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Universal psychometrics: Measuring cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "M.V. Hern\u00e1ndez-Lloreda"], "venue": "Cognitive Systems Research, (to appear),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo", "David L Dowe"], "venue": "Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A formal definition of intelligence based on an intensional variant of algorithmic complexity", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo", "Neus Minaya-Collado"], "venue": "In Proceedings of International Symposium of Engineering of Intelligent Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo", "David L Dowe", "Sergio Espa\u00f1a-Cubillo", "M Victoria Hern\u00e1ndez-Lloreda", "Javier Insa-Cabrera"], "venue": "In Artificial general intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Turing tests with turing machines", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo", "Javier Insa", "David L Dowe", "Bill Hibbard"], "venue": "In The Alan Turing Centenary Conference, Turing-100, Manchester,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Bias and no free lunch in formal measures of intelligence", "author": ["B. Hibbard"], "venue": "Journal of Artificial General Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Adversarial sequence prediction", "author": ["Bill Hibbard"], "venue": "Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "The measurement of social intelligence", "author": ["Thelma Hunt"], "venue": "Journal of Applied Psychology,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1928}, {"title": "On measuring social intelligence: experiments on competition and cooperation", "author": ["Javier Insa-Cabrera", "Jos\u00e9-Luis Benacloch-Ayuso", "Jos\u00e9 Hern\u00e1ndez- Orallo"], "venue": "In Artificial General Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Robocup rescue: Search and rescue in large-scale disasters as a domain for autonomous agents research", "author": ["Hiroaki Kitano", "Satoshi Tadokoro", "Itsuki Noda", "Hitoshi Matsubara", "Tomoichi Takahashi", "Atsuhi Shinjou", "Susumu Shimada"], "venue": "In Systems, Man, and Cybernetics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Three approaches to the definition of the concept quantity of information", "author": ["Andrei Nikolaevich Kolmogorov"], "venue": "Problemy peredachi informatsii,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1965}, {"title": "Crowd iq: Measuring the intelligence of crowdsourcing platforms", "author": ["Michal Kosinski", "Yoram Bachrach", "Gjergji Kasneci", "Jurgen Van-Gael", "Thore Graepel"], "venue": "In Proceedings of the 3rd Annual ACM Web Science Conference,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["Ludmila I Kuncheva"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "A colony of ant-like agents for partitioning in VLSI technology", "author": ["Pascale Kuntz", "Paul Layzell", "Dominique Snyers"], "venue": "In Proceedings of the Fourth European Conference on Artificial Life,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1997}, {"title": "A universal measure of intelligence for artificial agents", "author": ["S. Legg", "M. Hutter"], "venue": "In Intl Joint Conf on Artificial Intelligence, IJCAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds and Machines,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Universal sequential search problems", "author": ["Leonid A Levin"], "venue": "Problemy Peredachi Informatsii,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1973}, {"title": "Vitanyi. An introduction to Kolmogorov complexity and its applications", "author": ["Ming Li", "Paul MB"], "venue": "Springer-Verlag, 3rd edition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1993}, {"title": "Statistical theories of mental test scores, volume 47", "author": ["Frederic M Lord", "Melvin R Novick", "Allan Birnbaum"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1968}, {"title": "Task allocation via self-organizing swarm coalitions in distributed mobile sensor network", "author": ["Kian Hsiang Low", "Wee Kheng Leow", "Marcelo H Ang"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1999}, {"title": "Social savvy boosts the collective intelligence of groups", "author": ["Greg Miller"], "venue": "Science, 330(6000):22\u201322,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "The cooperation of swarm-bots: Physical interactions in collective robotics", "author": ["Francesco Mondada", "Luca Maria Gambardella", "Dario Floreano", "Stefano Nolfi", "J-L Deneuborg", "Marco Dorigo"], "venue": "Robotics & Automation Magazine, IEEE,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2005}, {"title": "Are you socially intelligent", "author": ["Fred August Moss", "Thelma Hunt"], "venue": "Scientific American,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1927}, {"title": "Kasparov vs. Deep Blue: Computer chess comes of age", "author": ["Monty Newborn", "Monroe Newborn"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1997}, {"title": "The evolution of imitation", "author": ["Andr\u00e9 Orl\u00e9an"], "venue": "In The Economics of Networks,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1998}, {"title": "Measurements of social intelligence", "author": ["Maureen O\u2019Sullivan", "Joy Paul Guilford", "Richard deMille"], "venue": "Reports from the Psychological Laboratory,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1965}, {"title": "A universal prior for integers and estimation by minimum description length", "author": ["J. Rissanen"], "venue": "Annals of Statistics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1983}, {"title": "Minds, brains, and programs", "author": ["John R Searle"], "venue": "Behavioral and brain sciences,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1980}, {"title": "Social intelligence: A concept in search of data", "author": ["Luke A Shanley", "Ronald E Walker", "Jeanne M Foley"], "venue": "Psychological Reports,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1971}, {"title": "Shettleworth. Cognition, evolution, and behavior", "author": ["J Sara"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2009}, {"title": "A formal theory of inductive inference. part I", "author": ["Ray J Solomonoff"], "venue": "Information and control,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1964}, {"title": "General intelligence\u201d, objectively determined and measured", "author": ["Charles Spearman"], "venue": "The American Journal of Psychology,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1904}, {"title": "Intelligence, information processing, and analogical reasoning", "author": ["Robert J Sternberg"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1977}, {"title": "Artificial stupidity", "author": ["John Sundman"], "venue": "Salon, Feb,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2003}, {"title": "Some characteristics of the good judge of personality", "author": ["Philip E Vernon"], "venue": "The Journal of Social Psychology,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1933}, {"title": "Telling humans and computers apart automatically or how lazy cryptographers do AI", "author": ["Louis von Ahn", "Manuel Blum", "John Langford"], "venue": "Computer Science Department,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2002}, {"title": "Statistical and Inductive Inference by Minimum Message Length", "author": ["C.S. Wallace"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2005}, {"title": "An information measure for classification", "author": ["C.S. Wallace", "D.M. Boulton"], "venue": "Computer Journal,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1968}, {"title": "The measurement and appraisal of adult intelligence", "author": ["David Wechsler"], "venue": "Academic Medicine,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1958}, {"title": "The relation between caste ratios and division of labor in the ant genus pheidole (hymenoptera: Formicidae)", "author": ["Edward O Wilson"], "venue": "Behavioral Ecology and Sociobiology,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1984}, {"title": "An introduction to collective intelligence", "author": ["David H Wolpert", "Kagan Tumer"], "venue": "arXiv preprint cs/9908014,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 1999}, {"title": "What makes a team smarter? More women", "author": ["Anita Woolley", "Thomas Malone"], "venue": "Harvard Business Review,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2011}, {"title": "Evidence for a collective intelligence factor in the performance of human", "author": ["Anita Williams Woolley", "Christopher F Chabris", "Alex Pentland", "Nada Hashmi", "Thomas W Malone"], "venue": "groups. Science,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Hence, following Boring [5], \u201cintelligence is the ability measured by the IQ test\u201d.", "startOffset": 24, "endOffset": 27}, {"referenceID": 51, "context": "In 1904, the psychologist Spearman [53] discovered a positive correlation across the performances in these different tasks.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "A further discussion of what a universal test is and why IQ tests are not universal can be found in [16] and [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "A further discussion of what a universal test is and why IQ tests are not universal can be found in [16] and [22].", "startOffset": 109, "endOffset": 113}, {"referenceID": 49, "context": "(see for instance [51])", "startOffset": 18, "endOffset": 22}, {"referenceID": 43, "context": "In 1997, a chess machine named deep blue beat the Russian chess master Garry Kasparov [45].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "In 2011, an IBM project called Watson beat humans in the game Jeopardy! [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 55, "context": "Currently, this can still be done using the CAPTCHA tests [59], where distorted letters and numbers have to be identified in a picture.", "startOffset": 58, "endOffset": 62}, {"referenceID": 47, "context": "[49].", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "And it has indeed been shown that a fairly easy algorithm could actually maintain a human-like conversation, without actually understanding the content of the conversation [55].", "startOffset": 172, "endOffset": 176}, {"referenceID": 22, "context": "A recent approach has been to use inductive inference for designing intelligence test, hence for to measuring and defining intelligence [24, 20, 37].", "startOffset": 136, "endOffset": 148}, {"referenceID": 18, "context": "A recent approach has been to use inductive inference for designing intelligence test, hence for to measuring and defining intelligence [24, 20, 37].", "startOffset": 136, "endOffset": 148}, {"referenceID": 35, "context": "A recent approach has been to use inductive inference for designing intelligence test, hence for to measuring and defining intelligence [24, 20, 37].", "startOffset": 136, "endOffset": 148}, {"referenceID": 30, "context": "This formalism stems from algorithmic information theory developed by Kolmogorov [32], Chaitin [9] and Solomonoff [52].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "This formalism stems from algorithmic information theory developed by Kolmogorov [32], Chaitin [9] and Solomonoff [52].", "startOffset": 95, "endOffset": 98}, {"referenceID": 50, "context": "This formalism stems from algorithmic information theory developed by Kolmogorov [32], Chaitin [9] and Solomonoff [52].", "startOffset": 114, "endOffset": 118}, {"referenceID": 37, "context": "The Kolomorow Complexity of a sequence x \u2013 actually the amount of information contained in it \u2013 is given by the size of the smallest program q on a Universal Turing Machine U so that the latter generates this sequence on output [39]: KU (x) = min q |q| : U(q) = x (2.", "startOffset": 228, "endOffset": 232}, {"referenceID": 57, "context": "The Kolomorov complexity is often also referred to as the Minimum Description Length (MDL) [61, 60, 48], which is the shortest string, which taken as an algorithm produces x.", "startOffset": 91, "endOffset": 103}, {"referenceID": 56, "context": "The Kolomorov complexity is often also referred to as the Minimum Description Length (MDL) [61, 60, 48], which is the shortest string, which taken as an algorithm produces x.", "startOffset": 91, "endOffset": 103}, {"referenceID": 46, "context": "The Kolomorov complexity is often also referred to as the Minimum Description Length (MDL) [61, 60, 48], which is the shortest string, which taken as an algorithm produces x.", "startOffset": 91, "endOffset": 103}, {"referenceID": 22, "context": "Hern\u00e1ndez-Orallo and Minaya-Collado [24] have designed a test based on inductive sequence prediction.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Hern\u00e1ndez-Orallo and Minaya-Collado [24] refer hence to \u201cintelligence as the ability of compression\u201d, although they argue that this direct connection needs to be further refined and developed (and led beyond inductive inference [21]).", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "Hern\u00e1ndez-Orallo and Minaya-Collado [24] refer hence to \u201cintelligence as the ability of compression\u201d, although they argue that this direct connection needs to be further refined and developed (and led beyond inductive inference [21]).", "startOffset": 228, "endOffset": 232}, {"referenceID": 35, "context": "However, the polynomial 2k\u221220k+70k\u221298k+48 follows also the same initial pattern [37].", "startOffset": 80, "endOffset": 84}, {"referenceID": 50, "context": "Solomonoff [52] defined the a priori probability that on any input on a Universal Turing Machine U appears the string x.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "An advantage is also that a measure of intelligence base on prediction/compression overcomes the Chinese Room argument[15].", "startOffset": 118, "endOffset": 122}, {"referenceID": 36, "context": ", the Levin\u2019s Kt [38]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "This idea and its use for evaluating intelligence was pioneered by Dobrev [12, 13].", "startOffset": 74, "endOffset": 82}, {"referenceID": 12, "context": "This idea and its use for evaluating intelligence was pioneered by Dobrev [12, 13].", "startOffset": 74, "endOffset": 82}, {"referenceID": 34, "context": "It was then further elaborated in a more elegant way by Legg and Hutter [36] using Markov Decision Processes and reinforcement learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "[25] discuss how an environment with several evaluees can be constructed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "This is for instance useful for the design of adversarial prediction problems, where one evaluee has to predict a sequence generated by the other, as in [28, 26].", "startOffset": 153, "endOffset": 161}, {"referenceID": 24, "context": "This is for instance useful for the design of adversarial prediction problems, where one evaluee has to predict a sequence generated by the other, as in [28, 26].", "startOffset": 153, "endOffset": 161}, {"referenceID": 35, "context": "Legg and Hutter [37] use the here presented formalism from algorithmic information theory to design a universal measure of intelligence.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "Some of these issues are addressed in [23, 27].", "startOffset": 38, "endOffset": 46}, {"referenceID": 25, "context": "Some of these issues are addressed in [23, 27].", "startOffset": 38, "endOffset": 46}, {"referenceID": 23, "context": "[25] and as we will discuss furthermore (see e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] suggest for instance to define a minimum complexity of the problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The idea of social intelligence [8] was first mentioned by Thorndike [56].", "startOffset": 32, "endOffset": 35}, {"referenceID": 54, "context": "Vernon [58] defines it as \u201cthe ability to get along with people in general, social techniques or ease in society, knowledge of social matters, susceptibility to stimuli from other members of a group, as well as insight into temporary moods or underlying personality traits of stranger\u201d.", "startOffset": 7, "endOffset": 11}, {"referenceID": 42, "context": "The first of such tests was the George Washington Social Intelligence Test (GWSIT) [44].", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "However, this test soon came under criticism as it correlated much with abstract intelligence tests [29].", "startOffset": 100, "endOffset": 104}, {"referenceID": 58, "context": ", [62]) argued that \u201csocial intelligence is nothing else than general intelligence applied to the social domain\u201d.", "startOffset": 2, "endOffset": 6}, {"referenceID": 45, "context": "[47] for instance developed a test which seems to withstand such criticism [50].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[47] for instance developed a test which seems to withstand such criticism [50].", "startOffset": 75, "endOffset": 79}, {"referenceID": 60, "context": "In artificial intelligence, Collective intelligence [66] commonly refers to multiagent systems (i.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "It was first discussed by Garret Hardin [19].", "startOffset": 40, "endOffset": 44}, {"referenceID": 60, "context": "Yet in many cases, RL is not well suited due to the big size of the action-policy space [66].", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] present a market-based approach, where each agent i makes a bid to paint truck j.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] also present an ant-algorithm which defeats the market-based version.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] is the same as the one we present in 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Swarm intelligence is \u201cany attempt to design algorithms or distributed problemsolving devices inspired by the collective behavior of social insect colonies and other animal societies\u201d [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 13, "context": "Examples of NP-hard problems for which ant-colony algorithms have been successfully developed are for instance the traveling salesman problem [14], routing problems [6], scheduling problems [69] or partitioning [35] problems, Not only for optimization algorithms, but also for collective intelligence systems with physically separated agents, social insects can be an inspiring source for designers.", "startOffset": 142, "endOffset": 146}, {"referenceID": 5, "context": "Examples of NP-hard problems for which ant-colony algorithms have been successfully developed are for instance the traveling salesman problem [14], routing problems [6], scheduling problems [69] or partitioning [35] problems, Not only for optimization algorithms, but also for collective intelligence systems with physically separated agents, social insects can be an inspiring source for designers.", "startOffset": 165, "endOffset": 168}, {"referenceID": 33, "context": "Examples of NP-hard problems for which ant-colony algorithms have been successfully developed are for instance the traveling salesman problem [14], routing problems [6], scheduling problems [69] or partitioning [35] problems, Not only for optimization algorithms, but also for collective intelligence systems with physically separated agents, social insects can be an inspiring source for designers.", "startOffset": 211, "endOffset": 215}, {"referenceID": 41, "context": "Several research projects have started to assess the use of swarm robots for search and rescue (SAR) tasks after disasters [43, 10, 31].", "startOffset": 123, "endOffset": 135}, {"referenceID": 9, "context": "Several research projects have started to assess the use of swarm robots for search and rescue (SAR) tasks after disasters [43, 10, 31].", "startOffset": 123, "endOffset": 135}, {"referenceID": 29, "context": "Several research projects have started to assess the use of swarm robots for search and rescue (SAR) tasks after disasters [43, 10, 31].", "startOffset": 123, "endOffset": 135}, {"referenceID": 40, "context": "The main conclusions of such research are basically identical [42, 67, 68].", "startOffset": 62, "endOffset": 74}, {"referenceID": 61, "context": "The main conclusions of such research are basically identical [42, 67, 68].", "startOffset": 62, "endOffset": 74}, {"referenceID": 62, "context": "The main conclusions of such research are basically identical [42, 67, 68].", "startOffset": 62, "endOffset": 74}, {"referenceID": 31, "context": "[33], who use a so-called crowdsource platform \u2013 a platform allowing employers to connect to several job seekers who will execute small tasks against a little reward \u2013 to evaluate collective intelligence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "We will mathematically conceptualize abilities making reference to the field of item response theory (IRT) [40].", "startOffset": 107, "endOffset": 111}, {"referenceID": 38, "context": "We will present briefly here the three parameter logistic model [40].", "startOffset": 64, "endOffset": 68}, {"referenceID": 32, "context": "The literature about multi-classifier systems [34] provides us with some theoretical results about which performance might be expected from such a majority voting system.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "Defining k = bn/2c + 1, the bounds on the group\u2019s accuracy are given by [34]:", "startOffset": 72, "endOffset": 76}, {"referenceID": 32, "context": "Again, the literature about multi-classifier systems [34] provide us with some theoretical results about weighted voting systems.", "startOffset": 53, "endOffset": 57}, {"referenceID": 28, "context": "[30], where several cooperation and competition settings are studied.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "One such algorithm is the dynamic task allocation algorithm inspired from division of labor observed with social insects such as ants ([4, 2, 41]).", "startOffset": 135, "endOffset": 145}, {"referenceID": 1, "context": "One such algorithm is the dynamic task allocation algorithm inspired from division of labor observed with social insects such as ants ([4, 2, 41]).", "startOffset": 135, "endOffset": 145}, {"referenceID": 39, "context": "One such algorithm is the dynamic task allocation algorithm inspired from division of labor observed with social insects such as ants ([4, 2, 41]).", "startOffset": 135, "endOffset": 145}, {"referenceID": 59, "context": "As observed by Wilson [65], in such ant colonies two distinct types of ants are present.", "startOffset": 22, "endOffset": 26}, {"referenceID": 59, "context": "Wilson [65] observed however that if minors were retrieved from the colony, majors will consequently also perform the former\u2019s task.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "[1] showed that the division of labor/task allocation observed for the Pheidole genus, could easily be modeled trough a so-called threshold model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "In the standard model [4, 2, 41], the thresholds are updated in a way so as to avoid unnecessary switching from one task to another.", "startOffset": 22, "endOffset": 32}, {"referenceID": 1, "context": "In the standard model [4, 2, 41], the thresholds are updated in a way so as to avoid unnecessary switching from one task to another.", "startOffset": 22, "endOffset": 32}, {"referenceID": 39, "context": "In the standard model [4, 2, 41], the thresholds are updated in a way so as to avoid unnecessary switching from one task to another.", "startOffset": 22, "endOffset": 32}, {"referenceID": 1, "context": "[2] to get a first idea of the parameters\u2019 matter of size.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "Our results are hence coherent with what we know already form multi-classifier systems [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 35, "context": "5) \u2013 as proposed by Legg and Hutter [37] \u2013 weights the problems using a universal distribution.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "Some ideas about how this might be done, can be found in Hern\u00e1ndez-Orallo and Dowe [23].", "startOffset": 83, "endOffset": 87}, {"referenceID": 44, "context": "Orl\u00e9an [46] investigates the dynamics of a group where some agents are truly informed and others are pure imitators of the informed agents (their vote is determined through the majority vote of the informed agents).", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "De Keuleneer [11] talking about the over-reliance on rating agencies puts it like this:", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "[2], we can incorporate the distance in our allocation model so that the probability of allocation becomes inversely proportional to it:", "startOffset": 0, "endOffset": 3}], "year": 2013, "abstractText": "Intelligence is a fairly intuitive concept of our everyday life. As usually acknowledged in psychometrics, \u201cintelligence is the ability measured by intelligence tests\u201d. However, defining what exactly intelligence tests should measure is less obvious. During the last decade, computer scientists have attempted to provide a formal definition of intelligence. There seems now to be the tendency that intelligence should make reference to the formalism provided by the field of algorithmic information theory. Yet, a consensus is far from being reached. Independent from the still ongoing research in measuring individual intelligence, we anticipate and provide a framework for measuring collective intelligence. Collective intelligence refers to the idea that several individuals can collaborate in order to achieve high levels of intelligence. We present thus some ideas on how the intelligence of a group can be measured and simulate such tests. We will however focus here on groups of artificial intelligence agents (i.e., machines). We will explore how a group of agents is able to choose the appropriate problem and to specialize for a variety of tasks. This is a feature which is an important contributor to the increase of intelligence in a group (apart from the addition of more agents and the improvement due to common decision making). Our results reveal some interesting results about how (collective) intelligence can be modeled, about how collective intelligence tests can be designed and about the underlying dynamics of collective intelligence. As it will be useful for our simulations, we provide also some improvements of the threshold allocation model originally used in the area of swarm intelligence but further generalized here.", "creator": "LaTeX with hyperref package"}}}