{"id": "1307.1275", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2013", "title": "Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice", "abstract": "This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing three-level representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge. Finally, we predict the success of the training scheme with a 100% positive trend of an improvement. We report the conclusion of this challenge in the Official Contest for International Student Computing and International Student Computing Awards.", "histories": [["v1", "Thu, 4 Jul 2013 11:10:45 GMT  (261kb)", "http://arxiv.org/abs/1307.1275v1", "6 pages, 1 figure, Presented at the Workshop on Representation Learning, ICML 2013"]], "COMMENTS": "6 pages, 1 figure, Presented at the Workshop on Representation Learning, ICML 2013", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["fangxiang feng", "ruifan li", "xiaojie wang"], "accepted": false, "id": "1307.1275"}, "pdf": {"name": "1307.1275.pdf", "metadata": {"source": "META", "title": "Constructing Hierarchical Image-tags Bimodal Representations  for Word Tags Alternative Choice", "authors": ["Fangxiang Feng", "Ruifan Li", "Xiaojie Wang"], "emails": ["f.fangxiang@gmail.com", "rfli@bupt.edu.cn", "xjwang@bupt.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "The multi-modal learning challenge of ICML 2013 aims at developing a predictive system for word tags using bimodal data: images and texts. Specifically, the data used in this contest contains two groups: the Small ESP Game Dataset (von Ahn & Dabbish, 2004)\nPresented at the ICML Workshop on Representation Learning, Atlanta, Georgia, USA, 2013. Copyright 2013 by the author(s).\nfor training created by Luis von Ahn and the manually labeled dataset for test by Ian Goodfellow. In the rest of this paper, we refer to these two datasets as ESP and GF, respectively. The ESP consists of 100,000 labeled images with tags. The GF consists of 1000 test examples come in triples: an image, and two annotations, i.e. a correct description and an incorrect one. The GF is further evenly divided into public test set and private test set. The performance of the predictions is evaluated based on the accuracy at predicting which of the two descriptions fits the image better. Below we describe some important properties of the these two datasets:\n\u2022 Some statistical differences exist between these two datasets. The images in ESP have a variety of sizes, while the test images are 300-pixel long on the larger dimension.\n\u2022 For each image in GF, the incorrect description is always the correct description of one other test image.\nThis paper describes our solution to handle the above challenges. Our approach treats an image and its tag words as a pair of data for the same hidden object and endeavors to model the similar representations between these two types of descriptions.\nThe following sections describe our solution in detail. The architecture of our approach is outlined in section 2. Then the three consecutive stages for constructing representations are described in sections 3, 4, and 5, successively. Section 6 introduces our strategies for choosing word tags. Section 7 shows our experimental results."}, {"heading": "2. System Architecture", "text": "The main idea of our solution is that we endeavor to construct hierarchical representations of bimodal data for choosing the word tags. In the training phase, we represent the data using three consecutive stages. In the first stage, the low-level representations for these two types of data are obtained respectively. For images, the features released by the contest organizer, extracted by four descriptors in MPEG-7, and images gist features are combined to form the level-1 representations. For tag words, the typical bag-of-words model is used for level-1 representations. In the second stage, the level-1 representations for image and tag words are distilled to form the level-2 representation using two stacked Restricted Boltzmann Machines (RBMs), respectively. In the third stage, we propose a quasi-Siamese auto-encoder for learning the level-3 similar representations of these bimodal data. The details of this network are given in section 5.\nThe architecture of our solution is shown in Figure 1. In this figure, three sub-figures show the three stages for representation constructions. The digits in the boxes are the numbers of neurons used for each layer except the two boxes for images and tag words. The detailed description of each stage is presented in the following sections.\nIn the test phase, a new pair of data (an image and one of its tags) is given to the three-stage modules, which can obtain the similarity/dissimilarity between the pair of data. By comparing the dissimilarity of two tags with the image, the tag with smaller dissimilarity is chosen as the alternative. For this task, a particular strategy is utilized to improve the accuracy."}, {"heading": "3. Obtaining Level 1 Representations", "text": "Because of the bimodal nature of this competition, we represent our input data from two perspectives: image and text. For image representation we adopt three types of features: the features from contest organizer, the MPEG-7, and gist descriptors. The contest organizer released some extracted image features with 816 dimensions. We remove the invalid 408 all-zero dimensions, reducing the size of features from 816 to 408.\nBesides that, we use MPEG-7 and gist descriptors. One part of MPEG-7 is a standard for visual descriptors. We use four different visual descriptors defined in MPEG-7 for image representations: Color Layout (CL), Color Structure (CS), Edge Histogram (EH), and Scalable Color (SC). CL is based on spatial distribution of colors. It is obtained applying the DCT transformation. We used 192 coefficients. CS is based on color distribution and local spatial structure of the color. We used the 256 coefficients form. EH is based on spatial distribution of edges (fixed 80 coefficients). SC is based on the color histogram in HSV color space encoded by a Haar transform. We used the form of 256 coefficients. The software module based on the MPEG-7 Reference Software, available at http://www.cs.bilkent.edu.tr/~bilmdg/ bilvideo-7/Software.html, permits obtaining all four different descriptors. Thus, we extract the features of MPEG-7 with the size of 784.\nGist represents the dominant spatial structure of a scene by a set of perceptual dimensions, including naturalness, openness, roughness, expansion, and ruggedness. These perceptual dimensions can be\nestimated using spectral and coarsely localized information. In our experiments, we use the package from http://people.csail.mit.edu/torralba/ code/spatialenvelope/ for image gist descriptor. From all the three groups of features, each image can be represented as a vector of 1704 dimensions. Among them, the first 408 dimensions stand for features provided by the organizer, the middle 784 dimensions for MPEG-7, and the last 512 dimensions for gist features.\nFor tags representation we use bag-of-words model. A dictionary of 4000 high-frequency words is built from all the tag words of ESP. Then, each word in one image tag can be represented as a multinomial variable. Conveniently, the 1-of-4000 coding scheme is adopted. Thus, each tag can be represented as a vector with 4000 1/0 elements, in which each element stands for whether the tag word is in the dictionary or not. For tag words of an image in the dictionary, they are encoded as 1s, and vice versa."}, {"heading": "4. Learning Level 2 Representations", "text": "In the second stage, we use RBMs to construct the level-2 representations. Those level-1 representations obtained in the first stage for images and tag words have different properties. That is, the level-1 representations of images have real values and those of tag words have multiple 1/0 values. We model these two types of data by different variants of RBMs: GaussianBernoulli RBM and Replicated softmax, respectively. Below, we describe some key points of those learning machines."}, {"heading": "4.1. Restricted Boltzmann Machines", "text": "RBM (Smolensky, 1986) is an undirected graphical model with stochastic binary units in visible layer and hidden layer but without connections between units within these two layers. Given that there are n visible units v and m hidden units h, and each unit is distributed by Bernoulli distribution with logistic activation function \u03c3(x) = 1/(1 + exp(\u2212x)), we then define a joint probabilistic distribution of visible units v and hidden units h\np(v,h) = 1\nZ exp (\u2212E(v,h)) (1)\nin which, Z is the normalization constant and E(v,h) is the energy function defined by the configurations of all the units as\nE(v,h) = \u2212 n\u2211 i=1 m\u2211 j=1 wijvihj \u2212 n\u2211 i=1 bivi \u2212 m\u2211 j=1 cjhj (2)\nBy maximizing the log-likelihood of input data, we can\nlearn the parameters of an RBM. This can be achieved by gradient decent. And the weights updates using\n\u2206wij = \u00b7 \u2202log p(v)\n\u2202wij = \u00b7\n( \u3008vihj\u3009data \u2212 \u3008vihj\u3009model ) (3)\nin which, is the learning rate, and \u3008\u00b7\u3009 is the operator of expectation with the corresponding distribution denoted by the subscript. The activation of visible units and hidden units can be infered by the following two equations\np(hj = 1|v) = \u03c3 ( cj +\nn\u2211 i=1 wijvi\n) (4)\np(vi = 1|h) = \u03c3 bi + n\u2211 j=1 wijhj  (5) in which, \u03c3(\u00b7) is the logistic activation function."}, {"heading": "4.2. Modeling Real-valued Data", "text": "We model the real-valued data using Gaussian RBM, which is an extension of the binary RBM replacing the Bernoulli distribution with Gaussian distribution for the visible data (Welling et al., 2004). The energy function of different configurations of visible units and hidden ones are E(v,h) = \u2212 n\u2211 i=1 m\u2211 j=1 wij vi \u03c3i hj+ n\u2211 i=1 (vi \u2212 bi)2 2\u03c32 \u2212 m\u2211 j=1 cjhj\n(6)\nThe gradient of the log-likelihood function is:\n\u2202log p(v)\n\u2202wij = \u2329 vi \u03c3i hj \u232a data \u2212 \u2329 vi \u03c3i hj \u232a model\n(7)\nUsually, we set the variances \u03c32 = 1 for all visible units."}, {"heading": "4.3. Modeling Count Data", "text": "For the count data, we use Replicated Softmax Model (Salakhutdinov & Hinton, 2009) for modeling this sparse vectors. The energy function of the sate configurations is defined as follows\nE(v,h) = \u2212 n\u2211 i=1 m\u2211 j=1 wijvihj \u2212 n\u2211 i=1 bivi \u2212M m\u2211 j=1 cjhj\n(8) where M is the total number of words in a document. Note that this replicated softmax model can be interpreted as an RBM model that uses a single visible multinomial unit with support 1, . . . ,K which is sampled M times. That is, for each document we create\na separate RBM with as many softmax units as there are words in the document.\nWe can efficiently learn all the model by using the Contrastive Divergence approximation (CD) (Hinton, 2002).\nIn our solution, for each modality we stack two RBMs to learn the level-2 representations. These two-layer stacked RBMs can be trained by greedy layer-wise method (Hinton et al., 2006; Bengio et al., 2007)."}, {"heading": "5. Learning Level 3 Representations", "text": "In the third stage, we propose a quasi-Siamese autoencoder for bimodal representations. The Siamese architecture of neural networks is originally proposed for signature verification (Bromley et al., 1993). The network takes a pair of signature patterns either from the same person or not as inputs. The loss function is simultaneously optimized by minimizing a dissimilarity metric when this pair of signatures is from the same person, and maximizing this dissimilarity metric when they belong to different persons. And the simple distance for approximating the \u201dsemantic\u201d distance in the input space is obtained by mapping these two patterns using the same nonlinear sub-networks. Incorporated by deep learning, the Siamese architecture has been successfully applied to face recognition (Chopra et al., 2005), dimensionality reduction (Salakhutdinov & Hinton, 2007), and speech recognition (Chen & Salman, 2011). However, these Siamese neural networks are used for one single modality. The inputs to these networks are either images or speech representations.\nRecent advances in multimodal deep learning have seen a trend to learn a joint representation by fusing different modalities (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012). (Man et al., 2012) suggests that information from different sensory channels converges somewhere in the brain to possibly form modalityinvariant representations. Motivated by this, we propose a quasi-Siamese neural network for bimodal learning. Below we describe the details.\nThe quasi-Siamese has two sub-networks with the same architecture but different parameters. And these two networks are connected by some predefined compatibility measure. This network is shown in Figure 1. By designing a proper loss function from energy-based learning (Yann LeCun & Huang, 2006), we can learn the similar representations for these two bimodalities.\nFormally, we denote the mapping from the inputs of these two sub-networks to the code layers as f(p;Wf )\nand g(q;Wg), in which, f for image modality and g for text modality; W denotes the weight parameters in these two sub-networks. And the subscript in the weights W denotes the corresponding modality. We define the compatibility measure between ith pair of image pi and the given tag words qi as\nC(pi, qi;Wf ,Wg) = \u2016f(pi;Wf )\u2212 g(qi;Wg)\u20161 (9)\nwhere \u2016\u00b7\u2016 is the L1 norm.\nTo learn the similar representations of these two modalities for one object, we come up with a loss function given input pi, qi, and a binary indicator I with respect to the inputs, where I = 1 if the tag words qi is for the image qi, and I = 0 otherwise. To simplify the notation we group the network parameters Wf ,Wg as \u0398. As the result, we define the loss function on any pair of inputs as\nL(pi, qi, I; \u0398) =\u03b1 (LI(pi; \u0398) + LT (qi; \u0398)) + (1\u2212 \u03b1)LC(pi, qi, I; \u0398)\n(10)\nwhere\nLI(pi; \u0398) = \u2016pi \u2212 p\u0302i\u201622 (11a)\nLT (qi; \u0398) = \u2016qi \u2212 q\u0302i\u201622 (11b) LC(pi, qi, I; \u0398) = IC2 + (1\u2212 I) exp(\u2212\u03bbC) (11c)\nHere, \u2016\u00b7\u20162 is the L2 norm. LI and LT are the losses caused by data reconstruction errors for the given inputs (an image and its tag words) of two subnets. While LC(pi, qi, I; \u0398) are contrastive losses incurred by whether the image and tag words are compatibility or not in two different situations indicated by I. \u03bb in (11c) is a constant determinated by the upper bound of C(pi, qi; \u0398) on all training data. \u03b1(0 < \u03b1 < 1) in the total loss function (11) is a parameter used to trade off between two groups of objectives, reconstruction losses and compatibility losses.\nThe learning for quasi-Siamese auto-encoder can be performed by standard back-propagation algorithm."}, {"heading": "6. Choosing Alternatives", "text": "By obtaining the hierarchical three-level representations, the model is prepared for choosing alternatives. We have two strategies: a general strategy and a dataspecific strategy. The general strategy is direct. To be specific, a pair of image pi and one of its tag words qi are taken into the network, the compatibility LC(pi, qi) between these two modalities can be calculated by equation (11c). Then, replacing the tag words qi with the other one q\u0303i, we can compute the other compatibility LC(pi, q\u0303i) between the image pi and the\nother tag words q\u0303i. Finally, the tag words with larger compatibility are chosen as the correct tag for that image. Although this general strategy is applicable, we figure out another more accurate strategy by considering characteristics of data.\nThe data-specific strategy is based on one observation. To emphasize, for each image in GF the incorrect description is always the correct description of one other test image, which have been described in section 1. That means there exist loops among the tag words of some images. For example, three images pa, pb, and pc, and their tag words form the tuples (pa, qa, q\u0303a), (pb, qb, q\u0303b), and (pc, qc, q\u0303c), respectively. There exist links among the six tag words of the three images. That is, for the six tag words we have either {q\u0303a = pb, q\u0303b = pc, q\u0303c = pa} or {q\u0303a = pc, q\u0303c = pb, q\u0303b = pa}. And, once the tag words for an image is determined, the link can de resolved. We could find out the image with maximum discrepancy of compatibility between its two tag-words simply by\narg max i\u2208link\n(LC(pi, qi, I)\u2212 LC(pi, q\u0303i, I))2 (12)\nfor all images in one link.\nTo summary, we first find out all links in the test images. And then for each link we look for the most deterministic matching pair. Consequently, the set of images in the link are all resolved."}, {"heading": "7. Experiments and Final Results", "text": "In this section, we report our experimental details and their results. In all our experiments, we only use the datasets ESP and GF provided by the organizer, though additional datasets can be used for training this model. Descriptions and some characteristics of the datasets ESP and GF have been given in section 1. We publish our implementation code at https:// github.com/FangxiangFeng/deepnet, which is based on Nitish Srivastava\u2019s DeepNet library.\nThe ESP dataset has only the correct tag words for each image. Therefore, we need to generate an incorrect counterpart for word tags of each image in this dataset. This can be achieved by randomly choosing one from all the correct tag words of the rest images, while ensuring that each of the tag words occurs only one time.\nIn the training phase, the level-1, level-2, level-3 representations are extracted consecutively. The settings for learning level-1 representations has been described in section 3. In learning level-2 representations, we construct two stacked RBMs with the neurons configurations 1704-1024-1024 and 4000-1024-1024 for images\nand tag words, respectively. In learning level-3 representations, the quasi-Siamese auto-encoder with neurons configurations 1024-512-1024 both for images and tag words bimodal data, in which the free parameters \u03b1 and \u03bb is set to 0.5 and 0.2, respectively. Additionally, we encourage the sparsity of the representations at all layers in the overall system.\nWe use both general and data-specific strategies described in section 6. To fulfill the computation of AUC (Area Under an ROC Curve), we express the dissimilarity of an image between its one tag words as a probability P (pi). We use Euclidean distances, though other metrics could be adopted. More specifically, P (pi) is expressed as\nP (pi) = (LC(pi, qi, I))2\n(LC(pi, qi, I))2 + (LC(pi, q\u0303i, I))2 (13)\nWhen the general strategy was adopted, we obtain the AUC with 0.87533; while the data-specific strategy can achieve the AUC with 100%, as in Table 1.\nThe public leaderboard show the scores achieved by all the 21 contesters. There are three teams also achieved score with 1.00000. And the fifth rank achieved score with 0.72979. The private leaderboard show the scores in the final test. The first two ranks teams RBM and MMDL achieved score with 1.00000. The first five ranks in leaderboard in public and private tests are listed in Table 2. Note that the last row in this table has two team names, the first for public test and the other for private test."}, {"heading": "8. Discussion and Conclusions", "text": "Our results show that the solution is effective for this task. We believe that the strategy applied in choosing the alternatives is important. And in moderate representations are enough for make an accurate choice. For this reason, we did not tune the parameters very carefully and the learning cycles are reduced to speed up the overall learning process.\nIn conclusion, we construct a hierarchically bimodal representation and data-specific strategy for word tag alternative choice. These bimodal representations are obtained by three-stage extractions. In the first stage, the level-1 representations are achieved by extracting from images and texts using typical methods. In the second stage, the level-2 representations are learned by two consecutive RBMs for each modality. In the third stage, a quasi-Siamese auto-encoder is proposed for learning the level-3 representations. When choosing alternatives, we endeavor to find the maximum discrepancy among a link of images from an observation of the data characteristics."}, {"heading": "Acknowledgments", "text": "We thank the organizers for organizing this interesting competition. We also thank Nitish Srivastava for sharing his DeepNet library. Some part of the work was supported by National Sciences Foundation of China (No. 61273365) and the Fundamental Research Funds for the Central Universities (No. 2013RC0304)."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Signature verification using a \u201dSiamese\u201d time delay neural network", "author": ["J. Bromley", "I. Guyon", "Y. Le Cun", "E. Saeckinger", "R. Shah"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Extracting speaker-specific information with a regularized Siamese deep network", "author": ["K. Chen", "A. Salman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chen and Salman,? \\Q2011\\E", "shortCiteRegEx": "Chen and Salman", "year": 2011}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "In Proceedings of the 2005 IEEE", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Comput.,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Sight and sound converge to form modality-invariant representations in temporoparietal cortex", "author": ["K. Man", "J.T. Kaplan", "A. Damasio", "K. Meyer"], "venue": "Journal of Neuroscience,", "citeRegEx": "Man et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Man et al\\.", "year": 2012}, {"title": "Multimodal deep learning", "author": ["J Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2007}, {"title": "Replicated softmax: an undirected topic model", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1. chapter Information processing in dynamical systems: foundations of harmony theory, pp. 194\u2013281", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Multimodal learning with deep Boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srivastava and Salakhutdinov,? \\Q2012\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov", "year": 2012}, {"title": "Labeling images with a computer game", "author": ["L. von Ahn", "L. Dabbish"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Ahn and Dabbish,? \\Q2004\\E", "shortCiteRegEx": "Ahn and Dabbish", "year": 2004}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Welling et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2004}, {"title": "Predicting structured data. chapter A Tutorial on Energy-Based Learning, pp. 1\u201359", "author": ["Yann LeCun", "Sumit Chopra", "Huang", "Fu-Jie"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "RBM (Smolensky, 1986) is an undirected graphical model with stochastic binary units in visible layer and hidden layer but without connections between units within these two layers.", "startOffset": 4, "endOffset": 21}, {"referenceID": 13, "context": "We model the real-valued data using Gaussian RBM, which is an extension of the binary RBM replacing the Bernoulli distribution with Gaussian distribution for the visible data (Welling et al., 2004).", "startOffset": 175, "endOffset": 197}, {"referenceID": 4, "context": "We can efficiently learn all the model by using the Contrastive Divergence approximation (CD) (Hinton, 2002).", "startOffset": 94, "endOffset": 108}, {"referenceID": 5, "context": "These two-layer stacked RBMs can be trained by greedy layer-wise method (Hinton et al., 2006; Bengio et al., 2007).", "startOffset": 72, "endOffset": 114}, {"referenceID": 0, "context": "These two-layer stacked RBMs can be trained by greedy layer-wise method (Hinton et al., 2006; Bengio et al., 2007).", "startOffset": 72, "endOffset": 114}, {"referenceID": 1, "context": "The Siamese architecture of neural networks is originally proposed for signature verification (Bromley et al., 1993).", "startOffset": 94, "endOffset": 116}, {"referenceID": 3, "context": "Incorporated by deep learning, the Siamese architecture has been successfully applied to face recognition (Chopra et al., 2005), dimensionality reduction (Salakhutdinov & Hinton, 2007), and speech recognition (Chen & Salman, 2011).", "startOffset": 106, "endOffset": 127}, {"referenceID": 7, "context": "Recent advances in multimodal deep learning have seen a trend to learn a joint representation by fusing different modalities (Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).", "startOffset": 125, "endOffset": 179}, {"referenceID": 6, "context": "(Man et al., 2012) suggests that information from different sensory channels converges somewhere in the brain to possibly form modalityinvariant representations.", "startOffset": 0, "endOffset": 18}], "year": 2013, "abstractText": "This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing threelevel representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge.", "creator": "LaTeX with hyperref package"}}}