{"id": "1307.2982", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2013", "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing", "abstract": "There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 11 Jul 2013 05:52:21 GMT  (572kb,D)", "http://arxiv.org/abs/1307.2982v1", null], ["v2", "Sun, 15 Dec 2013 02:36:21 GMT  (303kb,D)", "http://arxiv.org/abs/1307.2982v2", null], ["v3", "Fri, 25 Apr 2014 01:31:55 GMT  (306kb,D)", "http://arxiv.org/abs/1307.2982v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.DS cs.IR", "authors": ["mohammad norouzi", "ali punjani", "david j fleet"], "accepted": false, "id": "1307.2982"}, "pdf": {"name": "1307.2982.pdf", "metadata": {"source": "CRF", "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing", "authors": ["Mohammad Norouzi", "Ali Punjani", "David J. Fleet"], "emails": ["norouzi@cs.toronto.edu", "alipunjani@cs.toronto.edu", "fleet@cs.toronto.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014Binary codes, Hamming distance, nearest neighbor search, multi-index hashing.\nF"}, {"heading": "1 INTRODUCTION", "text": "THere has been growing interest in representing im-age data and feature descriptors in terms of compact binary codes, often to facilitate fast near neighbor search and feature matching in vision applications (e.g., [2], [7], [30], [31], [33], [19]). Binary codes are storage efficient and comparisons require just a small number of machine instructions. Millions of binary codes can be compared to a query in less than a second. But the most compelling reason for binary codes is their use as direct indices (addresses) into a hash table, yielding a dramatic increase in search speed compared to an exhaustive linear scan (e.g., [35], [29], [24]).\nNevertheless, using binary codes as direct hash indices is not necessarily efficient. To find near neighbors one needs to examine all hash table entries (or buckets) within some Hamming ball around the query. The problem is that the number of such buckets grows near-exponentially with the search radius. Even with a small search radius, the number of buckets to examine is often larger than the number of items in the database, and hence slower than linear scan. Recent papers on binary codes mention the use of hash tables, but resort to linear scan when codes are longer than 32 bits (e.g., [33], [29], [20], [24]). Not surprisingly, code lengths are often significantly longer than 32 bits in order to achieve satisfactory retrieval performance (e.g., see Fig. 5).\nThis paper presents a new algorithm for exact knearest neighbor (kNN) search on binary codes that\n\u2022 M. Norouzi is with the Department of Computer Science, Univeristy of Toronto. E-mail: norouzi@cs.toronto.edu \u2022 A. Punjani is with the Department of Computer Science, Univeristy of Toronto. E-mail: alipunjani@cs.toronto.edu \u2022 D.J. Fleet is with the Department of Computer Science, Univeristy of Toronto. E-mail: fleet@cs.toronto.edu\nis dramatically faster than exhaustive linear scan. This has been an open problem since the introduction of hashing techniques with binary codes. Our new multiindex hashing algorithm exhibits sub-linear search times, is storage efficient, and straightforward to implement. Empirically, on databases of up to 1B codes we find that multi-index hashing is hundreds of times faster than linear scan. Extrapolation suggests that the speedup gain grows quickly with database size beyond 1B codes."}, {"heading": "1.1 Background: Problem and Related Work", "text": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30]. Sometimes the binary codes are generated directly as feature descriptors for images or image patches, such as BRIEF or FREAK [2], [6], [7], and sometimes binary corpora are generated by discrete similarity-preserving mappings from highdimensional data. Most such mappings are designed to preserve Euclidean distance (e.g., [11], [20], [28], [31], [35]). Others focus on semantic similarity (e.g., [24], [30], [29], [33]). Our concern in this paper is not the algorithm used to generate the codes, but rather with fast search in Hamming space.1\nWe address two related search problems in Hamming space. Given a dataset of binary codes, H \u2261 {hi}ni=1, the first problem is to find the k codes in H that are closest in Hamming distance to a given query, i.e., kNN search in Hamming distance. The 1-NN problem in Hamming space was called the Best Match problem by Minsky and Papert [22]. They observed that there are no obvious approaches significantly better than exhaustive search, and asked whether such approaches might exist.\n1. There do exist several other promising approaches to fast approximate NN search on large real-valued image features (e.g., [3], [17], [23], [5]). Nevertheless, we restrict our attention in this paper to compact binary codes and exact search.\nar X\niv :1\n30 7.\n29 82\nv1 [\ncs .C\nV ]\n1 1\nJu l 2\n01 3\nThe second problem is to find all codes in a dataset H that are within a fixed Hamming distance of a query, sometimes called the Approximate Query problem [13], or Point Location in Equal Balls (PLEB) [15]. A binary code is an r-neighbor of a query code, denoted g, if it differs from g in r bits or less. We define the r-neighbor search problem as: find all r-neighbors of a query g from H.\nOne way to tackle r-neighbor search is to use a hash table populated with the binary codes h \u2208 H, and examine all hash buckets whose indices are within r bits of a query g (e.g., [33]). For binary codes of q bits, the number of distinct hash buckets to examine is\nL(q, r) = r\u2211 z=0 ( q z ) . (1)\nAs shown in Fig. 1 (top), L(q, r) grows very rapidly with r. Thus, this approach is only practical for small radii or short code lengths. Some vision applications restrict search to exact matches (i.e., r = 0) or a small search radius (e.g., [14], [34] ), but in most cases of interest the desired search radius is larger than is currently feasible (e.g., see Fig. 1 (bottom)).\nOur work is inspired in part by the multi-index hashing results of Greene, Parnas, and Yao [13]. Building on the classical Turan problem for hypergraphs, they construct a set of over-lapping binary substrings such that any two codes that differ by at most r bits are guaranteed to be identical in at least one of the constructed substrings. Accordingly, they propose an exact method for finding all r-neighbors of a query using multiple hash tables, one for each substring. At query time, candidate r-neighbors are found by using query substrings as indices into their corresponding hash tables. As explained below, while run-time efficient, the main drawback of their approach is the prohibitive storage required for the requisite number of hash tables. By comparison, the method we propose requires much less storage, and is only marginally slower in search performance.\nWhile we focus on exact search, there also exist algorithms for finding approximate r-neighbors ( -PLEB), or approximate nearest neighbors ( -NN) in Hamming distance. One example is Hamming Locality Sensitive Hashing [15], [10], which aims to solve the (r, )- neighbors decision problem: determine whether there exists a binary code h \u2208 H such that \u2016h \u2212 g\u2016H \u2264 r, or whether all codes in H differ from g in (1 + )r bits or more. Approximate methods are interesting, and the approach below could be made faster by allowing misses. Nonetheless, this paper will focus on the exact search problem.\nThis paper proposes a data-structure that applies to both kNN and r-neighbor search in Hamming space. We prove that for uniformly distributed binary codes of q bits, and a search radius of r bits when r/q is small, our query time is sub-linear in the size of dataset. We also demonstrate impressive performance on real-world datasets. To our knowledge this is the first practical datastructure solving exact kNN in Hamming distance."}, {"heading": "2 MULTI-INDEX HASHING", "text": "Our approach is a form of multi-index hashing. Binary codes from the database are indexed m times into m different hash tables, based on m disjoint binary substrings. Given a query code, entries that fall close to the query in at least one such substring are considered neighbor candidates. Candidates are then checked for validity using the entire binary code, to remove any non-r-neighbors. To be practical for large-scale datasets, the substrings must be chosen so that the set of candidates is small, and storage requirements are reasonable. We also require that all true neighbors will be found.\nThe key idea here stems from the fact that, with n binary codes of q bits, the vast majority of the 2q possible buckets in a full hash table will be empty, since 2q n. It seems expensive to examine all L(q, r) buckets within r bits of a query, since most of them contain no items. Instead, we merge many buckets together (most of which are empty) by marginalizing over different dimensions of the Hamming space. The downside is that these larger\nbuckets are not restricted to the Hamming volume of interest around the query. Hence not all items in the merged buckets are r-neighbors of the query, so we need to cull any candidate that is not a true r-neighbor."}, {"heading": "2.1 Substring Search Radii", "text": "In more detail, each binary code h, comprising q bits, is partitioned into m disjoint substrings, h(1), . . . ,h(m), each of length bq/mc or dq/me bits. For convenience in what follows, we assume that q is divisible by m, and that the substrings comprise contiguous bits. The key idea rests on the following statement: When two binary codes h and g differ by r bits or less, then, in at least one of their m substrings they must differ by at most br/mc bits. This leads to the first proposition:\nProposition 1: If \u2016h \u2212 g\u2016H \u2264 r, where \u2016h \u2212 g\u2016H denotes the Hamming distance between h and g, then\n\u2203 1 \u2264 z \u2264 m s.t. \u2016h(z) \u2212 g(z)\u2016H \u2264 r\u2032 , (2)\nwhere r\u2032 = br/mc. Proof of Proposition 1 follows straightforwardly from the Pigeonhole Principle. That is, suppose that the Hamming distance between each of the m substrings is strictly greater than r\u2032. Then, \u2016h \u2212 g\u2016H \u2265 m (r\u2032 + 1). Clearly, m (r\u2032 + 1) > r, since r = mr\u2032 + a for some a where 0 \u2264 a < m, which contradicts the premise.\nThe significance of Proposition 1 derives from the fact that the substrings have only q/m bits, and that the required search radius in each substring is just r\u2032 = br/mc. For example, if h and g differ by 3 bits or less, and m = 4, at least one of the 4 substrings must be identical. If they differ by at most 7 bits, then in at least one substring they differ by no more than 1 bit; i.e., we can search a Hamming radius of 7 bits by searching a radius of 1 bit on each of 4 substrings. More generally, instead of examining L(q, r) hash buckets, it suffices to examine L(q/m, r\u2032) buckets in each of m substring hash tables.\nWhile it suffices to examine all buckets within a radius of r\u2032 in all m hash tables, we next show that it is not always necessary. Rather, it is often possible to use a radius of just r\u2032 \u2212 1 in some of the m substring hash tables while still guaranteeing that all r-neighbors of g will be found. In particular, with r = mr\u2032 + a, where 0 \u2264 a < m, to find any item within a radius of r on q-bit codes, it suffices to search a+1 substring hash tables to a radius of r\u2032, and the remaining m\u2212(a+1) substring hash tables up to a radius of r\u2032\u22121. Without loss of generality, since there is no order to the substring hash tables, we search the first a + 1 hash tables with radius r\u2032, and all remaining hash tables with radius r\u2032 \u2212 1. Proposition 2: If ||h\u2212 g||H \u2264 r = mr\u2032 + a, then\n\u2203 z \u2264 a+ 1 s.t. ||h(z) \u2212 g(z)||H \u2264 r\u2032 (3a) OR\n\u2203 z > a+ 1 s.t. ||h(z) \u2212 g(z)||H \u2264 r\u2032 \u2212 1 . (3b)\nAlgorithm 1: Building m substring hash tables. Binary code dataset: H = {hi}ni=1 for j = 1 to m do\nInitialize jth hash table for i = 1 to n do\nInsert h(j)i into j th hash table\nend for end for\nTo prove Proposition 2, we show that when (3a) is false, (3b) must be true. If (3a) is false, then it must be that a < m\u2212 1, since otherwise a = m\u2212 1, in which case (3a) and Proposition 1 are equivalent. If (3a) is false, it also follows that h and g differ in each of their first a + 1 substrings by r\u2032+1 or more bits. Thus, the total number of bits that differ in the first a + 1 substrings is at least (a+1)(r\u2032+1). Because ||h\u2212 g||H \u2264 r, it also follows that the total number of bits that differ in the remaining m\u2212 (a+1) substrings is at most r\u2212 (a+1)(r\u2032+1). Then, using Proposition 1, the maximum search radius required in each of the remaining m\u2212 (a+ 1) substring hash tables is\u230a r \u2212 (a+1)(r\u2032+1) m\u2212 (a+1) \u230b = \u230a mr\u2032 + a\u2212 (a+1)r\u2032 \u2212 (a+1) m\u2212 (a+1)\n\u230b = \u230a r\u2032 \u2212 1\nm\u2212 (a+1) \u230b = r\u2032 \u2212 1 , (4)\nand hence Proposition 2 is true. Because of the near exponential growth in the number of buckets for large search radii, the smaller substring search radius required by Proposition 2 is significant.\n2.2 Multi-Index Hashing for r-neighbor Search\nIn a pre-processing step, given a dataset of binary codes, one hash table is built for each of the m substrings, as outlined in Algorithm 1. At query time, given a query g with substrings {g(j)}mj=1, we search the jth substring hash table for entries that are within a Hamming distance of br/mc or br/mc \u2212 1 of g(j), as prescribed by (3). By doing so we obtain a set of candidates from the jth substring hash table, denoted Nj(g). According to the propositions above, the union of the m sets, N (g) = \u22c3 j Nj(g), is necessarily a superset of the rneighbors of g. The last step of the algorithm computes the full Hamming distance between g and each candidate in N (g), retaining only those codes that are true r-neighbors of g. Algorithm 2 outlines the r-neighbor retrieval procedure for a query g.\nThe search cost depends on the number of lookups (i.e., the number of buckets examined), and the number of candidates tested. Not surprisingly there is a natural trade-off between them. With a large number of lookups one can minimize the number of extraneous candidates. By merging many buckets to reduce the number of lookups, one obtains a large number of candidates to\nAlgorithm 2: r-Neighbor Search for Query g . Query substrings: {g(j)}mj=1 Substring radius: r\u2032 = br/mc, and a = r \u2212mr\u2032 for j = 1 to a+ 1 do\nLookup r\u2032-neighbors of g(j) from jth hash table end for for j = a+ 2 to m do\nLookup (r\u2032-1)-neighbors of g(j) from jth hash table end for Remove all non r-neighbors from the candidate set.\ntest. In the extreme case with m = q, substrings are 1 bit long, so we can expect the candidate set to include almost the entire database.\nNote that the idea of building multiple hash tables is not novel in itself (e.g., see [13], [15]). However previous work relied heavily on exact matches in substrings. Relaxing this constraint is what leads to a more effective algorithm."}, {"heading": "3 PERFORMANCE ANALYSIS", "text": "We next develop an analytical model of search performance to help address two key questions: (1) How does search cost depend on substring length, and hence the number of substrings? (2) How do run-time and storage complexity depend on database size, code length, and search radius?\nTo help answer these questions we exploit a wellknown bound on the sum of binomial coefficients [9]; i.e., for any 0 < \u2264 12 and \u03b7 \u2265 1.\nb \u03b7c\u2211 \u03ba=0 ( \u03b7 \u03ba ) \u2264 2H( ) \u03b7 , (5)\nwhere H( ) \u2261 \u2212 log2 \u2212 (1\u2212 ) log2(1\u2212 ) is the entropy of a Bernoulli distribution with probability .\nIn what follows, n continues to denote the number of q-bit database codes, and r is the Hamming search radius. Let m denote the number of hash tables, and let s denote the substring length s = q/m. Hence, the maximum substring search radius becomes r\u2032 = br/mc = bs r/qc. As above, for the sake of model simplicity, we assume q is divisible by m.\nWe begin by formulating an upper bound on the number of lookups. First, the number of lookups in Algorithm 3 is bounded above by the product of m, the number of substring hash tables, and the number of hash buckets within a radius of bs r/qc on substrings of length s bits. Accordingly, using (5), if the search radius is less than half the code length, r \u2264 q/2 , then the total number of lookups is given by\nlookups(s) = m bs r/qc\u2211 z=0 ( s z ) \u2264 q s 2H(r/q)s . (6)\nClearly, as we decrease the substring length s, thereby increasing the number of substrings m, exponentially fewer lookups are needed.\nTo analyze the expected number of candidates per bucket, we consider the case in which the n binary codes are uniformly distributed over the Hamming space. In this case, for a substring of s bits, for which a substring hash table has 2s buckets, the expected number of items per bucket is n/2s. The expected size of the candidate set therefore equals the number of lookups times n/2s.\nThe total search cost per query is the cost for lookups plus the cost for candidate tests. While these costs will vary with the code length q and the way the hash tables are implemented, we find that, to a reasonable approximation, the costs of a lookup and a candidate test are similar (when q \u2264 256). Accordingly, we model the total search cost per query, for retrieving all r-neighbors, in units of the time required for a single lookup, as\ncost(s) = ( 1 + n\n2s ) q s bsr/qc\u2211 k=0 ( s k ) , (7)\n\u2264 ( 1 + n\n2s ) q s 2H(r/q)s . (8)\nIn practice, database codes will generally not be uniformly distributed, nor are uniformly distributed codes ideal for multi-index hashing. Indeed, the cost of search with uniformly distributed codes is relatively high since the search radius increases as the density of codes decreases. Rather, the uniform distribution is primarily a mathematical convenience that facilitates the analysis of run-time and storage complexity, thereby providing some insight into the effectiveness of the approach and how one might choose an effective substring length."}, {"heading": "3.1 Optimal Substring Length", "text": "As noted above in Sec. 2.2, finding a good substring length is critical to the efficiency of multi-index hashing. When the substring length is too large or too small the approach will not be effective. To find a good substring length, we first note that, dividing cost(s) in Eqn. (7) by q has no effect on the optimal s, denoted s\u2217. Accordingly, one can view s\u2217 as a function of two quantities, namely the number of items, n, and the search ratio r/q.\nFigure 2 plots cost as a function of substring length s, for 240-bit codes, different database sizes n, and different search radii (expressed as a fraction of the code length q). Dashed curves depict cost(s) in (7) while solid curves of the same color depict the upper bound in (8). The tightness of the bound is evident in the plots, as are the quantization effects of the upper range of the sum in (7). The small circles in Fig. 2 (top) depict cost when all quantization effects are included, and hence it is only shown at substring lengths that are integer divisors of the code length.\nFig. 2 (top) shows cost for search radii equal to 5%, 15% and 25% of the code length, with n = 109 in all cases. One striking property of these curves is that the cost is persistently minimal in the vicinity of s = log2 n,\nindicated by the vertical line close to 30 bits. This behavior remains consistent over a wide range of database sizes.\nFig. 2 (bottom) shows the dependence of cost on s for databases with n = 106, 109, and 1012, all with r/q = 0.25 and q = 128 bits. In this case we have laterally displaced each curve by \u2212 log2 n; notice how this aligns the minima close to 0. These curves suggest that, over a wide range of conditions, cost is minimal for s = log2 n. For this choice of the substring length, the expected number of items per substring bucket, i.e., n/2s, reduces to 1. As a consequence, the number of lookups is equal to the expected number of candidates. Interestingly, this choice of substring length is similar to that of Greene et al. [13]."}, {"heading": "3.2 Run-Time Complexity", "text": "Choosing s in the vicinity of log2 n provides a characterization of retrieval run-time complexity. When s = log2 n, the upper bound on the number of lookups (6) also becomes a bound on the number candidates. In particular, if we substitute log2 n for s in (8), then we find the following upper bound on the cost, now as a function\nof database size, code length, and the search radius:\ncost(s) \u2264 2 q log2 n nH(r/q) . (9)\nThus, for a uniform distribution over binary codes, if we choose m such that s \u2248 log2 n, the expected query time complexity is O(q nH(r/q)/log2 n). For a small ratio of r/q this is sub-linear in n. For example, if r/q \u2264 .11, then H(.11) < .5, and the run-time complexity becomes O(b \u221a n/log2 n). That is, the search time increases with the square root of the database size when the search radius is approximately 10% of the code length. For r/q \u2264 .06, this becomes O(b 3 \u221a n/log2 n). The time complexity with respect to q is not as important as that with respect to n since q is not expected to vary significantly in most applications."}, {"heading": "3.3 Storage Complexity", "text": "The storage complexity of our multi-index hashing algorithm is asymptotically optimal. To store the full database of binary codes requires O(nq) bits. For each of m hash tables, we also need to store n unique identifiers to the database items. This allows one to identify the retrieved items and fetch their full codes; this requires an additional O(mn log2 n) bits. In sum, the storage required is O(nq+mn log2 n). When bq/ log2 nc \u2264 m \u2264 dq/ log2 ne, as is suggested above, this storage cost reduces to O(nq + n log2 n). Here, the n log2 n term does not cancel as m \u2265 1, but in most interesting cases q > log2 n.\nWhile the storage cost for our multi-index hashing algorithm is linear in nq, the related multi-index hashing algorithm of Greene et al. [13] entails storage complexity that is super-linear in n. To find all r-neighbors, for a given search radius r, they construct m = O(r2sr/q) substrings of length s bits per binary code. Their suggested substring length is also s = log2 n, so the number of substring hash tables becomes m = O(rnr/q), each of which requires O(n log2 n) in storage. As a consequence for large values of n, even with small r, this technique requires a prohibitive amount of memory to store the hash tables.\nOur approach is more memory-efficient than that of [13] because we do not enforce exact equality in substring matching. In essence, instead of creating all of the hash tables off-line, and then having to store them, we flip bits of each substring at run-time and implicitly create some of the substring hash tables on-line. This increases run-time slightly, but greatly reduces storage costs.\n4 k-NEAREST NEIGHBOR SEARCH To use the above multi-index hashing in practice, one must specify a Hamming search radius r. For many tasks, the value of r is chosen such that queries will, on average, retrieve k near neighbors. Nevertheless, as expected, we find that for many hashing techniques and different sources of visual data, the distribution of binary\ncodes is such that a single search radius for all queries will not produce similar numbers of neighbors.\nFigure 3 depicts empirical distributions of search radii needed for 10-NN and 1000-NN on three sets of binary codes obtained from 1B SIFT descriptors [18], [21]. In all cases, for 64 and 128-bit codes, and for hash functions based on LSH [4] and MLH [24], there is a substantial variance in the search radius. This suggests that binary codes are not uniformly distributed over the Hamming space. As an example, for 1000-NN in 64-bit LSH codes, more than 10% of the queries require a search radius of 10 bits or larger, while for about 10% of the queries it can be 5 or smaller. Also evident from Fig. 3 is the growth in the required search radius as one moves from 64-bit codes to 128 bits, and from 10-NN to 1000-NN.\nA fixed radius for all queries would produce too many neighbors for some queries, and too few for others. It is therefore more natural for many tasks to fix the number of required neighbors, i.e., k, and let the search radius depend on the query. Fortunately, our multi-index hashing algorithm is easily adapted to accommodate query-dependent search radii.\nGiven a query, one can progressively increase the Hamming search radius per substring, until a specified number of neighbors is found. For example, if one examines all r\u2032-neighbors of a query\u2019s substrings, from which more than k candidates are found to be within a Hamming distance of (r\u2032 + 1)m \u2212 1 bits (using the full codes for validation), then it is guaranteed that k-nearest neighbors have been found. Indeed, if all kNNs of a query g differ from g in r bits or less, then Propositions 1 and 2 above provide guanantees all such neighbors will be found if one searches the substring hash tables with the prescribed radii.\nIn our experiments, we follow this progressive increment of the search radius until we can find kNN in the guaranteed neighborhood of a query. This approach, given in Algorithm 3, is helpful because it uses a specific\nAlgorithm 3: kNN Search with Query g . Query substrings: {g(i)}mi=1 Initialize sets: Nd = \u2205, for 0 \u2264 d \u2264 q Initialize integers: r\u2032 = 0, a = 0, r = 0 repeat\nAssert: Full radius of search is r = mr\u2032 + a . Lookup buckets in the (a+1)th substring hash table that differ from g(a+1) in exactly r\u2032 bits. For each candidate found, measure full Hamming distance, and add items with distance d to Nd. a\u2190 a+ 1 if a \u2265 m then a\u2190 0 r\u2032 \u2190 r\u2032 + 1 end if r \u2190 r + 1\nuntil \u2211r\u22121 d=0 |Nd| \u2265 k (i.e., k r-neighbors are found)\nsearch radius for each query depending on the distribution of codes in that neighborhood."}, {"heading": "5 EXPERIMENTS", "text": "Experiments are run on two different architectures. The first is a mid- to low-end 2.3Ghz dual quad-core AMD Opteron processor, with 2MB of L2 cache, and 128GB of RAM. The second is a high-end machine with a 2.9Ghz dual quad-core Intel Xeon processor, 20MB of L2 cache, and 128GB of RAM. The difference in the size of the L2 cache has a major impact on the run-time of linear scan, since the effectiveness of linear scan depends greatly on L2 cache lines. With roughly ten times the L2 cache, linear scan on the Intel platform is roughly twice as fast as on the AMD machines. By comparison, multi-index hashing does not have a serial memory access pattern and so the cache size does not have such a pronounced effect. Actual run-times for multi-index hashing on the Intel and AMD platforms are within 20% of one another.\nBoth linear scan and multi-index hashing were implemented in C++ and compiled with identical compiler flags. To accommodate the large size of memory footprint required for 1B codes, we used the libhugetlbfs package and Linux Kernel 3.2.0 to allow the use of 2MB page sizes. Further details about the implementations are given in the Appendix. Finally, despite the existence of multiple cores, all experiments are run on a single core to simplify run-time measurements.\nThe memory requirements for multi-index hashing are described in detail in the Appendix. We currently require approximately 27GB for multi-index hashing with 1B 64- bit codes, and approximately twice that for 128-bit codes. Fig. 4 shows how the memory footprint depends on the database size for linear scan and multi-index hashing. As explained in the Appendix, and demonstrated in Fig. 4 the memory requirements of multi-index hashing grow linearly in the database size, as does linear scan. While we use a single computer in our experiments, one could implement a distributed version of multi-index hashing on computers with much less memory by placing each substring hash table on a separate computer."}, {"heading": "5.1 Datasets", "text": "We consider two well-known large-scale vision corpora: 80M Gist descriptors from 80 million tiny images [32] and 1B SIFT features from the BIGANN dataset [18]. SIFT vectors [21] are 128D descriptors of local image structure in the vicinity of feature points. Gist features [27] extracted from from 32 \u00d7 32 images capture global image structure in 384D vectors. These two feature types cover a spectrum of NN search problems in vision from feature to image indexing.\nWe use two similarity-preserving mappings to create datasets of binary codes, namely, binary angular Locality Sensitive Hashing (LSH) [8], and Minimal Loss Hashing (MLH) [24], [25]. LSH is considered a baseline random projection method, closely related to cosine similarity. MLH is a state-of-the-art learning algorithm that, given a set of similarity labels, finds an optimal mapping by\nminimizing a loss function over pairs or triplets of binary codes.\nBoth the 80M Gist and 1B SIFT corpora comprise three disjoint sets, namely, a training set, a base set for populating the database, and a test query set. Using a random permutation, Gist descriptors are divided into a training set with 300K items, a base set of 79 million items, and a query set of size 104. The SIFT corpus comes with 100M for training, 109 in the base set, and 104 test queries.\nFor LSH we subtract the mean, and pick a set of coefficients from the standard normal density for a linear projection, followed by quantization. For MLH the training set is used to optimize hash function parameters [25]. After learning is complete, we remove the training data and use the resulting hash function with the base set to create the database of binary codes. With two image corpora (SIFT and Gist), up to three code lengths (64, 128, and 256 bits), and two hashing methods (LSH and MLH), we obtain several datasets of binary codes with which to evaluate our multi-index hashing algorithm. Note that 256-bit codes are only used with LSH and SIFT vectors.\nFigure 5 shows Euclidean NN recall rates for kNN search on binary mappings of 1M and 1B SIFT descriptors. In particular, we plot the fraction of Euclidean 1st nearest neighbors found, by kNN in 64-bit and 128- bit LSH [8] and MLH [25] binary codes. As expected 128-bit codes are more accurate, and MLH outperforms LSH. Since multi-index hashing solves exact kNN in Hamming distance, the approximation is due to quantization by the hash function. To preserve the similarity structure in the original SIFT descriptors, it seems useful to use longer codes, and exploit data-dependant hash functions such as MLH. Interestingly, as described below, the speedup factors of multi-index hashing on MLH codes are better than those for LSH."}, {"heading": "5.2 Results", "text": "Each experiment below involves 104 queries, for which we report the average run-time. Our implementation of the linear scan baseline searches 60 million 64-bit codes in just under one second on the AMD machine. On the Intel machine it examines over 80 million 64-bit codes per second. This is remarkably fast compared to\nEuclidean NN search with 128D SIFT vectors. The speed of linear scan is in part due to memory caching, without which it would be much slower. Run-times for linear scan on other datasets, on both architectures, are given in Tables 1 and 2.\n5.3 Multi-Index Hashing vs. Linear Scan\nTables 1 and 2 shows run-time per query for the linear scan baseline, along with speed-up factors of multi-index hashing for different kNN problems and nine different datasets. Despite the remarkable speed of linear scan, the multi-index hashing implementation is hundreds of times faster. For example, the multi-index hashing method solves the exact 1000-NN for a dataset of 1B 64- bit codes in about 50 ms, well over 300 times faster than linear scan (see Table 1). Performance on 1-NN and 10- NN are even more impressive. With 128-bit MLH codes, multi-index hashing executes the 1NN search task over 1000 times faster than the linear scan baseline.\nThe run-time of linear scan does not depend on the number of neighbors, nor on the underlying distribution of binary codes. The run-time for multi-index hashing, however, depends on both factors. In particular, as the desired number of NNs increases, the Hamming radius\nof the search also increases (e.g., see Fig. 3). This implies longer run-times for multi-index hashing. Indeed, notice that going from 1-NN to 1000-NN on each row of the tables shows a decrease in the speed-up factors.\nThe multi-index hashing run-time also depends on the distribution of binary codes. Indeed, one can see from Table 1 that MLH code databases yield faster run times than the LSH codes; e.g., for 100-NN in 1B 128-bit codes the speed-up for MLH is 353\u00d7 vs 208\u00d7 for LSH. Fig. 3 depicts the histograms of search radii needed for 1000- NN with 1B 128-bit MLH and LSH codes. Interestingly, the mean of the search radii for MLH codes is 19.9 bits, while it is 19.8 for LSH. While the means are similar the variance are not; the standard deviations of the search radii for MLH and LSH are 4.0 and 5.0 respectively. The longer tail of the distribution of search radii for LSH plays an important role in the expected run-time. In fact, queries that require relatively large search radii tend to dominate the average query cost.\nIt is also interesting to look at the multi-index hashing run-times as a function of n, the number of binary codes in the database. To that end, Fig. 6 and 7 depict runtimes for linear scan and multi-index kNN search on the AMD machine. The left two figures in each show different vertical scales (since the behavior of multi-index\nkNN and linear scan are hard to see at the same scale). The right-most panels show the same data on log-log axes. First, it is clear from these plots that multi-index hashing is much faster than linear scan for a wide range of dataset sizes and k. Just as importantly, it is evident from the log-log plots that as we increase the database size, the speedup factors improve. The dashed lines on the log-log plots depict \u221a n (up to a scalar constant). The similar slope of multi-index hashing curves with the square root curves show that multi-index hashing exhibits sub-linear query time, even for the empirical, non-uniform distributions of codes."}, {"heading": "5.4 Direct lookups with a single hash table", "text": "An alternative to linear scan and multi-index hashing is to hash the entire codes into a single hash table, and then use direct hashing with each query. As suggested in the introduction and Fig. 1, although this approach avoids the need for any candidate checking, it may require a prohibitive number of lookups. Nevertheless, for sufficiently small code lengths or search radii, it may be effective in practice.\nRather than implement this directly, given the complexity associated with efficiently implementing folded hash tables, we instead consider the empirical number of lookups one would need, as compared to the number of items in the database. If the number of lookups is vastly greater than the size of the dataset one can readily\nconclude that linear scan is likely to be as fast or faster than direct indexing into a single hash table.\nFortunately, the statistics of neighborhood sizes and required search radii for kNN tasks are available from the multi-index hashing experiments reported above. One can use the substring search radii to compute the required radii on the full codes (e.g., see Fig. 3). For each radius one can compute the number of lookups that would be required to find all neighbors from a single hash table, for each query. Summed over a dataset, this provides an indication of the expected run-time.\nFigure 9 shows the total number of lookups required for 1-NN and 1000-NN tasks on 64- and 128-bit codes (from LSH on SIFT) using a single hash table. They are plotted as a function of the size of the dataset, from 104 to 109 items. For comparison, the plots also show the number of database items, and the number of lookups that were needed for multi-index hashing (as reported in 1 above for the AMD machine). Note that Fig. 9 has\nlogarithmic scales. It is evident that with a single hash table the number of lookups is almost always several orders of magnitude than there are items in the dataset. And not surprisingly, this is also several orders of magnitude more lookups than required for multi-index hashing. Although the relative speed of a lookup operation compared to a candidate check (as used in linear scan) is not known precisely, there are a few important considerations. Linear scan has an exactly serial memory access pattern and so can make very efficient use of cache, whereas lookups in a hash table are inherently random. Furthermore, in any plausible implementation of a single hash table for 64 bit or longer codes, there will be some penalty for collision detection.\nAs illustrated in Fig. 9, the only cases where a single hash table might potentially be more efficient than linear scan are with very small codes (64 bits or less), with a large dataset (1 billion items or more), and a small search distances (e.g., for 1-NN). In all other cases, linear scan requires orders of magnitude fewer operations. With any code length longer than 64 bits, a single hash table approach is completely infeasable to run, requiring upwards of 15 orders of magnitude more operations than linear scan for 128-bit codes."}, {"heading": "5.5 Substring Optimization", "text": "The substring hash tables used above have been formed by simply dividing the full codes into disjoint but consecutive sequences bits. For LSH and MLH, this is equivalent to randomly assigning bits to substrings.\nIt natural to ask whether further gains in efficiency are possible by optimizing this assignment. In particular, by carefully assigning bits to substrings one may be able to maximize the discriminability of the different substrings. In other words, while the radius of substring searches and hence the number of lookups is determined by the desired search radius on the full codes, and will remain fixed, by optimizing the assignment of bits to substrings one might be able to reduce the number of candidates one needs to validate.\nTo explore this idea we considered a simple method for which substrings are assigned bits one at a time in a greedy fashion, based on correlations between bits. In particular, of those bits not yet assigned, the next substring is assigned the bit that minimizes the maximum correlation between that bit and all other bits already assigned to that substring. Initialization also occurs in a greedy manner: A random bit is assigned to the first substring, after which the first bit to substring j is that which is maximally correlated with the first bit of substring j \u2212 1. This approach significantly decreases the correlation between bits within a single substring, which should make the distribution of substrings more uniform, and thereby lower the number of candidates within a given search radius. Arguably an even better approach would be to maximize the entropy of the\nentries within each substring hash table, thereby making the distribution of substrings as uniform as possible. This entropic approach is, however, left to future work.\nThe results obtained with the correlation-based algorithm show that optimizing substrings can provide overall run-time reductions on the order of 20% against consecutive substrings for some cases. Table 3 displays the improvements achieved by optimizing substrings for different codes lengths and different values of k. Fig. 10 shows the run-time performance of optimized substrings."}, {"heading": "6 CONCLUSION", "text": "This paper describes a new algorithm for exact nearest neighbor search on large-scale datasets of binary codes. The algorithm is a form of multi-index hashing that has provably sub-linear run-time behavior for uniformly distributed codes. It is storage efficient and easy to implement. We show empirical performance on datasets of binary codes obtained from 1 billion SIFT, and 80 million Gist features. With these datasets we find that, for 64-bit and 128-bit codes, our new multi-index hashing implementation is often more than two orders of magnitude faster than a linear scan baseline.\nWhile the basic algorithm is developed in this paper there are several interesting avenues for future research. For example our preliminary research shows that log2 n is a good choice for the substring length, and it should be possible to formulate a sound mathematical basis for this choice. The assignment of bits to substrings was shown to be important above, however the algorithm used for\nthis assignment is clearly suboptimal. It is also likely that different substring lengths might be useful for the different hash tables.\nWhile the current paper concerns exact nearestneighbor tasks, it would also be interesting to consider approximate methods based on the same multi-index hashing framework. Indeed there are several ways that one could find approximate rather than the exact nearest neighbors for a given query. For example, one could stop at a given radius of search, even though k items may not have been found. Alternatively, one might search until a fixed number of unique candidates have been found, even though all substring hash tables have not been inspected to the necessary radius, Such approximate algorithms have the potential for even greater efficiency, and would be the most natural methods to compare to most existing methods which are approximate, such as binary LSH. That said, such comparisons are more difficult than for exact methods since one must taken into account not only the storage and run-time costs, but also some measure of the cost of errors (usually in terms of recall and precision).\nFinally, recent results have shown that for many datasets in which the binary codes are the result of some form of vector quantization, an asymmetric Hamming distance is attractive [12], [17]. In such methods, rather than converting the query into a binary code, one directly compares a real-valued query to the database of binary codes. The advantage is that the quantization noise entailed in converting the query to a binary string is avoided and one can more accurately using distances\nin the binary code space to approximate the desired distances in the feature space of the query. One simple way to do this is to use multi-index hashing and then only use an asymmetric distance when culling candidates. The potential for more interesting and effective methods is yet another promising anvenue for future work."}, {"heading": "APPENDIX A IMPLEMENTATION DETAILS", "text": "Our implementation of multi-index hashing is available at [1]. As described above, the algorithm builds hash tables on disjoint s-bit substrings of the codes to find a set of neighbor candidates. A key component is how to implement and store the hash tables. Three ways of implementing hash tables are described below, the last of which is our method of choice.\nDirect Address Tables: When substring length is small (e.g., s \u2264 32) one can explicitly allocate memory for 2s buckets and store all the data points associated with each substring in its corresponding bucket. A straightforward implementation of this approach, stores a table of 2s pointers to the bucket data structures (e.g., linked lists or a resizable arrays). On a 64-bit machine, however, pointers are 8 bytes so just storing an empty hash table for s = 32 requires 32GB. This approach was used in [26], and hence their storage requirement was large.\nFolded Hash Tables: When s is large, explicit memory allocation with 2s buckets is not possible. Instead, one could use folded hash tables, for which the binary substring indices are hashed into smaller hash tables e.g., by taking the indices modulo a prime number. This approach is storage efficient, but slower as it requires a method for collision handling. We do not use folded hash tables since the longest substring we need is 32 bits.\nSparse Direct Address Tables: One can also consider more efficient implementations of direct address tables. Instead of storing a table of 2s pointers, one can group hash buckets into subsets of, for example, 32 elements. Then, each bucket group uses a 32-bit binary vector to encode which buckets are empty or not (at least one data point). We then need only one pointer per group, since data points associated with any of the buckets in a group are stored within a single resizable array, ordered based on their bucket index. For fast access, we store the index of the beginning and the end of the part of the resizable array corresponding to each non-empty bucket. This approach is much more memory-efficient than the simple direct address tables; e.g., for s = 32, and bucket groups of size 32, an empty hash table requires just 1.5GB. Also, accessing any bucket of the hash table with this approach has a worst case O(1) run-time complexity. Memory Requirements: We use sparse, direct-address hash tables with bucket groups of size 32. We store one 64-bit pointer for each hash table bucket group, and a 32-bit binary vector to encode empty and non-empty buckets within the group; this entails 2(s\u22125) \u00b7 (8 + 4) bytes for an empty s-bit hash table, or 1.5GB when s = 32. For\neach non-empty bucket group we use a resizable array to store IDs of data points. The bookkeeping for this resizable array entails 3 32-bit integers.\nIn our experiments, most bucket groups have at least one non-empty bucket, so we have to pay the cost of a resizable array for almost all bucket groups. Taking this into account the total storage for an s-bit hash table becomes 2(s\u22125) \u00b7 24 bytes (3GB for s = 32). For each non-empty bucket within a bucket group, we store a 32-bit integer to indicate the index of the beginning of the part of the resizable array corresponding to that bucket. The number of non-empty buckets is bounded by mmin(n, 2s), where m is the number of hash tables. Thus we need an extra mmin(n, 2s) \u00b7 4 bytes. For each data point per hash table we store an ID to reference the full binary code; each ID is 4 bytes as the size of datasets n \u2264 232; this yields a total of 4mn bytes. Finally, storing the full binary codes requires nms/8 bytes, because q = ms.\nThe total cost is m2(s\u22125) \u00b7 24+mmin(n, 2s) \u00b7 4+4mn+ nms/8 bytes (for s = log2 n, this is O(nq)). For 1B 64- bit codes, and m = 2 hash tables (32 bits each), this is 28GB. For 128-bit and 256-bit codes our implementation requires 57GB and 113GB. Note that the last two terms (for IDs and codes) are irreducible, but the first term can be reduced in a memory efficient implementation. Duplicate Candidates: When retrieving candidates from the m substring hash tables, it is inevitable that some codes are found multiple times. To detect such duplicates, and discard them, we allocate a bit-string with n bits. Every time a candidate is found, we check the corresponding bit and discard the candidate if it is marked as a duplicate. Otherwise we retain the candidate. Before each query we clear the bit array, which in theory requires O(n) but negligible in practice. Hamming Distance: To compare a query and a candidate, used in both multi-index search and linear scan, we compute the Hamming distance on the full q-bit codes, with one xor operation for every 64 bits followed by a pop count to tally the ones. We used the built-in GCC function __builtin_popcount. Number of Substrings: The number of substrings we use is determined with a hold-out validation set of database entries. From that set we estimate the running time of the algorithm for different choices of m, and select the m that yields the best results. Our experiments show that this empirical value for m is typically very close to q / log2 n. Translation Lookaside Buffer and Huge Pages: Modern processors have an on-chip cache that holds a lookup table of memory addresses, used to map virtual memory addresses to physical memory addresses for each running process. Typically, memory is split into 4KB pages, and a process that allocates memory is given pages by the operating system. The Translation Lookaside Buffer (TLB) keeps track of these pages. For processes like multiindex hashing that have a large memory footprint (tens\nof GB), the number of pages quickly overtakes the size of the TLB (typically about 1500 entries). For processes that require random memory access, this means that almost every memory access produces a TLB miss - the requested address is in a page not cached in the TLB, and this TLB entry is then fetched from slow RAM before the requested page itself can be found and accessed. This bottleneck severely slows down memory access and causes significant volatility in run-times for memoryaccess intensive process.\nTo remedy this situation, the libhugetlbfs library can be used in Linux. This library allows the operating system to allocate Huge Pages (2MB each) rather than regular 4KB pages. This reduces the number of pages used by a process with a large footprint, and hence the frequency of TLB misses and the speed of memory access. It also reduces measurement volatility. The increase in speed of multi-index hashing results reported here compared to those in [26] are attributed to libhugetlbfs."}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was financially supported in part by NSERC Canada, the GRAND Network Centre of Excellence, and the Canadian Institute for Advanced Research (CIFAR). The authors would also like to thank Mohamed Aly, Rob Fergus, Ryan Johnson, Abbas Mehrabian, and Pietro Perona for useful discussions about this work.\nREFERENCES [1] https://github.com/norouzi/mih/. [2] A. Alahi, R. Ortiz, and P. Vandergheynst. Freak: Fast retina\nkeypoint. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2012. [3] M. Aly, M. Munich, and P. Perona. Distributed kd-trees for retrieval from very large image collections. In Proc. British Machine Vision Conference, 2011. [4] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Communications of the ACM, 51(1):117\u2013122, 2008. [5] A. Babenko and V. Lempitsky. The inverted multi-index. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2012. [6] A. Bergamo, L. Torresani, and A. Fitzgibbon. Picodes: Learning a compact code for novel-category recognition. In Proc. Advances in Neural Information Processing Systems, volume 24, 2011. [7] M. Calonder, V. Lepetit, C. Strecha, and P. Fua. Brief: Binary robust independent elementary features. In Proc. European Conference on Computer Vision, page 778792, 2010. [8] M. Charikar. Similarity estimation techniques from rounding algorithms. In ACM Symposium on Theory of Computing. ACM, 2002. [9] J. Flum and M. Grohe. Parameterized Complexity Theory. Springer, 2006. [10] A. Gionis, P. Indyk, R. Motwani, et al. Similarity search in high dimensions via hashing. In Proc. Int. Conf. Very Large Databases, pages 518\u2013529, 1999. [11] Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2011. [12] A. Gordo and F. Perronnin. Asymmetric distances for binary embeddings. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 729\u2013736, 2011. [13] D. Greene, M. Parnas, and F. Yao. Multi-index hashing for information retrieval. In IEEE Symposium on Foundations of Computer Science, pages 722\u2013731, 1994. [14] J. He, R. Radhakrishnan, S.-F. Chang, and C. Bauer. Compact hashing with joint optimization of search accuracy and time. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2011.\n[15] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In ACM Symposium on Theory of Computing, pages 604\u2013613, 1998. [16] H. Je\u0301gou, M. Douze, and C. Schmid. Hamming embedding and weak geometric consistency for large scale image search. In Proc. European Conference on Computer Vision, volume I, pages 304\u2013317, 2008. [17] H. Je\u0301gou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Trans. PAMI, 33(1):117\u2013128, 2011. [18] H. Je\u0301gou, R. Tavenard, M. Douze, and L. Amsaleg. Searching in one billion vectors: re-rank with source coding. In IEEE Acoustics, Speech and Signal Processing, pages 861\u2013864. IEEE, 2011. [19] D. Kuettel, M. Guillaumin, and V. Ferrari. Segmentation propagation in imagenet. In Proc. European Conference on Computer Vision, 2012. [20] B. Kulis and T. Darrell. Learning to hash with binary reconstructive embeddings. In Proc. Advances in Neural Information Processing Systems, volume 22, 2009. [21] D. G. Lowe. Distinctive image features from scale-invariant keypoints. Int. Journal of Computer Vision, 60(2):91\u2013110, 2004. [22] M. Minsky and S. Papert. Perceptrons. MIT Press, 1969. [23] M. Muja and D. Lowe. Fast approximate nearest neighbors with\nautomatic algorithm configuration. In International Conference on Computer Vision Theory and Applications, 2009. [24] M. Norouzi and D. J. Fleet. Minimal loss hashing for compact binary codes. In Proc. International Conference on Machine Learning, 2011. [25] M. Norouzi, D. J. Fleet, and R. Salakhutdinov. Hamming space metric learning. In Proc. Advances in Neural Information Processing Systems, volume 25, 2012. [26] M. Norouzi, A. Punjani, and D. Fleet. Fast search in hamming space with multi-index hashing. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2012. [27] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3):145\u2013175, 2001. [28] M. Raginsky and S. Lazebnik. Locality-sensitive binary codes from shift-invariant kernels. In Proc. Advances in Neural Information Processing Systems, volume 22, 2009. [29] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 2009. [30] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose estimation with parameter-sensitive hashing. In Proc. IEEE International Conference on Computer Vision, volume 2, 2003. [31] C. Strecha, A. Bronstein, M. Bronstein, and P. Fua. LDAHash: improved matching with smaller descriptors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(1):66\u201378, 2012. [32] A. Torralba, R. Fergus, and W. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Trans. PAMI, 30(11):1958\u20131970, 2008. [33] A. Torralba, R. Fergus, and Y. Weiss. Small codes and large image databases for recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2008. [34] J. Wang, , S. Kumar, and S. Chang. Sequential projection learning for hashing with compact codes. In Proc. International Conference on Machine Learning, 2010. [35] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Proc. Advances in Neural Information Processing Systems, volume 21, 2008."}], "references": [{"title": "Freak: Fast retina keypoint", "author": ["A. Alahi", "R. Ortiz", "P. Vandergheynst"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed kd-trees for retrieval from very large image collections", "author": ["M. Aly", "M. Munich", "P. Perona"], "venue": "Proc. British Machine Vision Conference,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Communications of the ACM, 51(1):117\u2013122,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "The inverted multi-index", "author": ["A. Babenko", "V. Lempitsky"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Picodes: Learning a compact code for novel-category recognition", "author": ["A. Bergamo", "L. Torresani", "A. Fitzgibbon"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 24,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Brief: Binary robust independent elementary features", "author": ["M. Calonder", "V. Lepetit", "C. Strecha", "P. Fua"], "venue": "Proc. European Conference on Computer Vision, page 778792,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "ACM Symposium on Theory of Computing. ACM,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Parameterized Complexity Theory", "author": ["J. Flum", "M. Grohe"], "venue": "Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proc. Int. Conf. Very Large Databases,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Asymmetric distances for binary embeddings", "author": ["A. Gordo", "F. Perronnin"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 729\u2013736,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-index hashing for information retrieval", "author": ["D. Greene", "M. Parnas", "F. Yao"], "venue": "IEEE Symposium on Foundations of Computer Science, pages 722\u2013731,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Compact hashing with joint optimization of search accuracy and time", "author": ["J. He", "R. Radhakrishnan", "S.-F. Chang", "C. Bauer"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "ACM Symposium on Theory of Computing, pages 604\u2013613,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Hamming embedding and weak geometric consistency for large scale image search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Proc. European Conference on Computer Vision, volume I, pages 304\u2013317,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "IEEE Trans. PAMI, 33(1):117\u2013128,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Searching in one billion vectors: re-rank with source coding", "author": ["H. J\u00e9gou", "R. Tavenard", "M. Douze", "L. Amsaleg"], "venue": "IEEE Acoustics, Speech and Signal Processing, pages 861\u2013864. IEEE,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Segmentation propagation in imagenet", "author": ["D. Kuettel", "M. Guillaumin", "V. Ferrari"], "venue": "Proc. European Conference on Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 22,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. Journal of Computer Vision, 60(2):91\u2013110,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Perceptrons", "author": ["M. Minsky", "S. Papert"], "venue": "MIT Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1969}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D. Lowe"], "venue": "International Conference on Computer Vision Theory and Applications,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "Proc. International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Hamming space metric learning", "author": ["M. Norouzi", "D.J. Fleet", "R. Salakhutdinov"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 25,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["M. Norouzi", "A. Punjani", "D. Fleet"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision, 42(3):145\u2013175,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 22,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "Proc. IEEE International Conference on Computer Vision, volume 2,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "LDAHash: improved matching with smaller descriptors", "author": ["C. Strecha", "A. Bronstein", "M. Bronstein", "P. Fua"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(1):66\u201378,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W. Freeman"], "venue": "IEEE Trans. PAMI, 30(11):1958\u20131970,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["J. Wang", "S. Kumar", "S. Chang"], "venue": "In Proc. International Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Proc. Advances in Neural Information Processing Systems, volume 21,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 28, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": ", [2], [7], [30], [31], [33], [19]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": ", [35], [29], [24]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [35], [29], [24]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": ", [35], [29], [24]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": ", [33], [29], [20], [24]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": ", [33], [29], [20], [24]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": ", [33], [29], [20], [24]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": ", [33], [29], [20], [24]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 117, "endOffset": 120}, {"referenceID": 14, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 155, "endOffset": 158}, {"referenceID": 17, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 180, "endOffset": 184}, {"referenceID": 28, "context": "Nearest neighbor (NN) search on binary codes is used for image search [28], [33], [35], matching local features [2], [7], [16], [31], image classification [6], object segmentation [19], and parameter estimation [30].", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "Sometimes the binary codes are generated directly as feature descriptors for images or image patches, such as BRIEF or FREAK [2], [6], [7], and sometimes binary corpora are generated by discrete similarity-preserving mappings from highdimensional data.", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "Sometimes the binary codes are generated directly as feature descriptors for images or image patches, such as BRIEF or FREAK [2], [6], [7], and sometimes binary corpora are generated by discrete similarity-preserving mappings from highdimensional data.", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "Sometimes the binary codes are generated directly as feature descriptors for images or image patches, such as BRIEF or FREAK [2], [6], [7], and sometimes binary corpora are generated by discrete similarity-preserving mappings from highdimensional data.", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": ", [11], [20], [28], [31], [35]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": ", [24], [30], [29], [33]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": ", [24], [30], [29], [33]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": ", [24], [30], [29], [33]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": ", [24], [30], [29], [33]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "The 1-NN problem in Hamming space was called the Best Match problem by Minsky and Papert [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": ", [3], [17], [23], [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 15, "context": ", [3], [17], [23], [5]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": ", [3], [17], [23], [5]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": ", [3], [17], [23], [5]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 16, "context": "Binary codes with 64 and 128 bits were obtained by random projections (LSH) from the SIFT descriptors [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "The second problem is to find all codes in a dataset H that are within a fixed Hamming distance of a query, sometimes called the Approximate Query problem [13], or Point Location in Equal Balls (PLEB) [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "The second problem is to find all codes in a dataset H that are within a fixed Hamming distance of a query, sometimes called the Approximate Query problem [13], or Point Location in Equal Balls (PLEB) [15].", "startOffset": 201, "endOffset": 205}, {"referenceID": 31, "context": ", [33]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": ", [14], [34] ), but in most cases of interest the desired search radius is larger than is currently feasible (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": ", [14], [34] ), but in most cases of interest the desired search radius is larger than is currently feasible (e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Our work is inspired in part by the multi-index hashing results of Greene, Parnas, and Yao [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "One example is Hamming Locality Sensitive Hashing [15], [10], which aims to solve the (r, )neighbors decision problem: determine whether there exists a binary code h \u2208 H such that \u2016h \u2212 g\u2016H \u2264 r, or whether all codes in H differ from g in (1 + )r bits or more.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "One example is Hamming Locality Sensitive Hashing [15], [10], which aims to solve the (r, )neighbors decision problem: determine whether there exists a binary code h \u2208 H such that \u2016h \u2212 g\u2016H \u2264 r, or whether all codes in H differ from g in (1 + )r bits or more.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": ", see [13], [15]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": ", see [13], [15]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "We next develop an analytical model of search performance to help address two key questions: (1) How does search cost depend on substring length, and hence the number of substrings? (2) How do run-time and storage complexity depend on database size, code length, and search radius? To help answer these questions we exploit a wellknown bound on the sum of binomial coefficients [9]; i.", "startOffset": 378, "endOffset": 381}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] entails storage complexity that is super-linear in n.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Our approach is more memory-efficient than that of [13] because we do not enforce exact equality in substring matching.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Shown are histograms of the search radii that are required to find 10-NN and 1000-NN, for 64 and 128-bit code from LSH [4], and 128-bit codes from MLH [24], based on 1B SIFT descriptors [18].", "startOffset": 119, "endOffset": 122}, {"referenceID": 22, "context": "Shown are histograms of the search radii that are required to find 10-NN and 1000-NN, for 64 and 128-bit code from LSH [4], and 128-bit codes from MLH [24], based on 1B SIFT descriptors [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Shown are histograms of the search radii that are required to find 10-NN and 1000-NN, for 64 and 128-bit code from LSH [4], and 128-bit codes from MLH [24], based on 1B SIFT descriptors [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "Figure 3 depicts empirical distributions of search radii needed for 10-NN and 1000-NN on three sets of binary codes obtained from 1B SIFT descriptors [18], [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 19, "context": "Figure 3 depicts empirical distributions of search radii needed for 10-NN and 1000-NN on three sets of binary codes obtained from 1B SIFT descriptors [18], [21].", "startOffset": 156, "endOffset": 160}, {"referenceID": 2, "context": "In all cases, for 64 and 128-bit codes, and for hash functions based on LSH [4] and MLH [24], there is a substantial variance in the search radius.", "startOffset": 76, "endOffset": 79}, {"referenceID": 22, "context": "In all cases, for 64 and 128-bit codes, and for hash functions based on LSH [4] and MLH [24], there is a substantial variance in the search radius.", "startOffset": 88, "endOffset": 92}, {"referenceID": 30, "context": "We consider two well-known large-scale vision corpora: 80M Gist descriptors from 80 million tiny images [32] and 1B SIFT features from the BIGANN dataset [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "We consider two well-known large-scale vision corpora: 80M Gist descriptors from 80 million tiny images [32] and 1B SIFT features from the BIGANN dataset [18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 19, "context": "SIFT vectors [21] are 128D descriptors of local image structure in the vicinity of feature points.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "Gist features [27] extracted from from 32 \u00d7 32 images capture global image structure in 384D vectors.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "We use two similarity-preserving mappings to create datasets of binary codes, namely, binary angular Locality Sensitive Hashing (LSH) [8], and Minimal Loss Hashing (MLH) [24], [25].", "startOffset": 134, "endOffset": 137}, {"referenceID": 22, "context": "We use two similarity-preserving mappings to create datasets of binary codes, namely, binary angular Locality Sensitive Hashing (LSH) [8], and Minimal Loss Hashing (MLH) [24], [25].", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "We use two similarity-preserving mappings to create datasets of binary codes, namely, binary angular Locality Sensitive Hashing (LSH) [8], and Minimal Loss Hashing (MLH) [24], [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "Recall rates for BIGANN dataset [18] (1M and 1B subsets) obtained by kNN on 64- and 128-bit MLH and LSH codes.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "For MLH the training set is used to optimize hash function parameters [25].", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "In particular, we plot the fraction of Euclidean 1 nearest neighbors found, by kNN in 64-bit and 128bit LSH [8] and MLH [25] binary codes.", "startOffset": 108, "endOffset": 111}, {"referenceID": 23, "context": "In particular, we plot the fraction of Euclidean 1 nearest neighbors found, by kNN in 64-bit and 128bit LSH [8] and MLH [25] binary codes.", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Finally, recent results have shown that for many datasets in which the binary codes are the result of some form of vector quantization, an asymmetric Hamming distance is attractive [12], [17].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "Finally, recent results have shown that for many datasets in which the binary codes are the result of some form of vector quantization, an asymmetric Hamming distance is attractive [12], [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 24, "context": "This approach was used in [26], and hence their storage requirement was large.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "The increase in speed of multi-index hashing results reported here compared to those in [26] are attributed to libhugetlbfs.", "startOffset": 88, "endOffset": 92}], "year": 2017, "abstractText": "There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is straightforward to implement and storage efficient. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speed-ups over a linear scan baseline for datasets of up to one billion codes of 64, 128, or 256 bits.", "creator": "LaTeX with hyperref package"}}}