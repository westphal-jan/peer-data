{"id": "1307.4879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2013", "title": "Says who? Automatic Text-Based Content Analysis of Television News", "abstract": "We perform an automatic analysis of television news programs, based on the closed captions that accompany them. Specifically, we collect all the news broadcasted in over 140 television channels in the US during a period of six months. The resulting results reflect an increase in the number of hours viewers broadcast during the period of the broadcast, including those from television broadcast stations and TV programming. We also offer our most recent analysis of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 18 Jul 2013 09:37:45 GMT  (1392kb)", "http://arxiv.org/abs/1307.4879v1", null], ["v2", "Thu, 29 Aug 2013 08:12:30 GMT  (524kb,D)", "http://arxiv.org/abs/1307.4879v2", "In the 2013 workshop on Mining Unstructured Big Data Using Natural Language Processing, co-located with CIKM 2013"]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["carlos castillo", "gianmarco de francisci morales", "marcelo mendoza", "nasir khan"], "accepted": false, "id": "1307.4879"}, "pdf": {"name": "1307.4879.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n48 79\nv1 [\ncs .C\nL ]\n1 8\nJu l 2\n01 3\nWe begin by segmenting, processing, and annotating the closed captions automatically. Next, we focus on the analysis of their linguistic style and on mentions of people. We present a series of key insights about news providers, about people in the news, and about the biases that can be uncovered by automatic means. These insights are contrasted by looking at the data from multiple points of view, including qualitative assessment."}, {"heading": "1. INTRODUCTION", "text": "Television is a dominant source of news today, wielding an enormous influence over many aspects of our life. The ascent of the Web has caused a significant drop in newspaper and radio audiences, but television remains the number one source for news in the US [1].\nWe analyze the closed captions of newscasts, which are provided by the news networks themselves. By using these streams of text, we study how to characterize each news network, each person-type named entity mentioned in the news (newsmaker), and the relationship between news networks and newsmakers (e.g., the biases of networks in the coverage of news related to a person). To the best of our knowledge, this work is the first to perform text analysis of content in television news with such broad goals and in such an ambitious scale.\nWe propose a pipeline of tools to process the input stream of data. While the specific choice of each tool is taskdependent, the pipeline itself and its components represent a minimum number of necessary steps to extract useful information from the data. Any other data mining task that uses closed captions, such as text segmentation, classification, or clustering, can build upon a pre-processing pipeline similar to the one described here.\n2013.\nOur pipeline filters out non-news programs, segments the captions into sentences, detects named entities (specifically people), applies a part-of-speech tagger to find words and qualifiers used together with each entity, and labels automatically each sentence with an overall sentiment score.\nThese tools extract a set of measurable dimensions from the text, which we employ to tackle the following tasks:\n\u2022 characterize news providers and news programs in terms of style, news coverage and timeliness (Section 4);\n\u2022 characterize newsmakers with respect to their popularity, profession and similarity to other people (Section 5);\n\u2022 characterize biases and framing in the coverage of newsmakers from different providers (Section 6)."}, {"heading": "2. RELATED WORK", "text": "One of the oldest references on mining television content is a DARPA-sponsored workshop in 1999 with a topic detection and tracking challenge [2]. Higher-level applications have been emerging in recent years. For example, Henzinger et al. [11] describe a system for finding web pages related to television content, and test different methods to synthesize a web search query from a television transcript. Oger et al. [21] classify videos based on a transcription obtained from speech recognition. Xu et al. [29] describe a system to rate the credibility of information items on television by looking at how often the same image is described in a similar way by more than one news source.\nInformation from closed captions has also been combined with other data sources. Lin and Hauptmann [14] present a system for video classification based on closed captions as well as content-based attributes of the video. Misra et al. [16] also combine closed captions and multimedia content to improve video segmentation. Shamma et al. [25] align the closed captions during a live event (a presidential debate) with social media reactions. Gibbon et al. [7] combine closed captions with real-time television audience measures to detect ads \u2013 which typically are accompanied by sudden drops in viewership (\u201czapping\u201d).\nQuantitative analysis of media. Several research groups have used quantitative evidence to, among other tasks, measure the amount of bias of different news sources. For instance, D\u2019Alessio and Allen [4] study gatekeeping, coverage, and statement bias by annotating content from magazines and television in the US. They find no substantial bias on magazines in the US, and a small coverage bias on US television. Schoenbach et al. [24] study Dutch and German television and observe that top leaders such as chancellors or\nprime ministers get a substantially larger number of mentions than the second most mentioned politicians. Morris and Francia [19] focus on US party conventions, and notice that what people hear on the news is mostly commentary, with limited amount of quotes from the convention participants. Moreover, they find large differences in the sentiment polarity with which candidates are treated.\nMost content analysis of news reports is based on a timeconsuming process of manual coding; automatic methods are less frequently used. Groseclose and Milyo [9] use an indirect measure of television bias by manually counting references to think tanks in each network, and then by scoring each think tank on a left-right political spectrum by automatically counting their appearances in congressional records of speeches by politicians of different political leaning. Flaounas et al. [6] study news articles available on the web, and analyze the prevalence of different topics and the distribution of readability and subjectivity scores.\nTopics and perspectives. The PhD thesis of Lin [13] and related papers (e.g. [15]), introduce a joint probabilistic model of the topics and the perspectives from which documents and sentences are written. The parameters of the model are learned from a training corpus for which the ideological perspectives are known. In contrast, the methods we study on this paper are unsupervised.\nLin et al. process news videos using content-based methods. First, news boundaries are detected automatically using visual clues, such as scene and background changes. Second, each news segment is annotated with concepts such as sky, person, outdoor, etc. which are inferred automatically from shots in the video. This approach is effective for settings in which closed captions are not available."}, {"heading": "3. DATA AND TOOLS", "text": "We use closed captions provided by a software company that develops second-screen experiences, i.e., applications that display extra information about a TV show on a smartphone or tablet. The same data is also publicly available from the Internet Archive1, which recently opened a TV search service and whose archives date back to 2009. Our dataset consist of all the closed captions from January 2012 to June 2012 for about 140 channels. On average, each TV channel generates \u2248 2MB of text every day.\nEach channel is mapped to the conventional name of its network; for some networks we have data from the East Coast and West Coast of the US, and denote them as, e.g., ABC-w and ABC-e. The three major news networks according to the number of self-declared regular viewers [1] are Fox News (21%), CNN (16%) and MSNBC (11%).\nWe obtain in advance a television schedule with the title of each program, the channel on which it will be aired, the beginning and ending times, and a category and subcategory. We consider all the programs of type newscast, and four sub-categories: general news, sports news, entertainment news and business news, which we abbreviate in the remainder of the paper as gen, spt, ent, and biz, respectively. Given that one channel may have more than one type of program (e.g., Fox Business has a majority of business news, but also broadcasts general news), we define a news provider as a combination of network and genre, e.g., Fox Business[biz] and Fox Business[gen].\n1http://archive.org/details/tv"}, {"heading": "3.1 Text pre-processing", "text": "The closed captions are streams of plain text that we process through a series of steps. First, to segment the text stream into sentences we use a series of heuristics which include detecting a change of speaker, conventionally signaled by a text marker (\u201c>>\u201d), using the presence of full stops, and using time-based rules. We remark that there exist methods to join sentences into passages [26, 16], but for our analysis we use single sentences as basic units of content, and we only group them when they match to the same news item, as described in Section 3.2.\nSecond, we recognize and extract named entities by using a named entity tagger that works in two steps: entity resolution [30] and\u201caboutness\u201d ranking [22]. We focus on the person type in the remainder of this paper, and whenever we find a given entity in the closed captions of a news provider, we count a mention of that person by the provider.\nThird, we apply the Stanford NLP tagger [28] to perform part-of-speech tagging and dependency parsing.2 As a last step of the text pre-processing, we apply sentiment analysis to each sentence by using SentiStrength [27].\nExample. A brief example can illustrate the main parts of our text pre-processing. The input data is similar to this:\n[1339302660.000] WHAT MORE CAN YOU ASK FOR? [1339302662.169] >> THIS IS WHAT NBA [1339302663.203] BASKETBALL IS ABOUT.\nThe TV channel maps to a network name (CNN), and the time is used to look-up in the programming guide to determine the type (CNN World Sports, which is about sports news). Hence, this news provider is identified as CNN[spt]. Finally, the text is tagged to generate the following output:\nWhat/WP more/JJR can/MD you/PRP ask/VB for/IN ?/. This/DT is/VBZ what/WDT NBA/NNP [entity: National_Basketball_ Association] basketball/NN is/VBZ about/IN ./."}, {"heading": "3.2 News matching", "text": "We match the processed captions to recent news stories, which are obtained from a major online news aggregator. Captions are matched in the same genre, e.g., sentences in sports are matched to online news in the sports section of the news aggregator. News in the website that are older than three days are ignored. The matching task is the same as the one described by Henzinger et al. [11], but the approach is based on supervised learning rather than web searches.3 More details can be found in [removed for double-blind]. The matching is performed in two steps. In the first step, a per-genre classification model trained on thousands of examples labeled by editors is applied. In this model, the two classes are \u201csame story\u201d and \u201cdifferent story\u201d and each example consists of a sentence, a news story, and a class label. The features for the classifier are computed from each sentence-story pair by applying the named entity tagger described in the previous section on both elements of the pair, and then by looking at entity co-occurrences. The models are fine-tuned to have high precision.\nIn the second step, recall is improved by aggregating multiple sentence-level matchings that occur in a short time period to form a \u201cqualified matching\u201d. 2 Further details on the tag set are available in the manual http://nlp.stanford.edu/software/dependencies_manual.pdf 3Citation to the report describing this subsystem omitted due to double-blind constraints."}, {"heading": "4. NEWS PROVIDERS", "text": "In this section we examine news providers, and try to answer the following research questions:\nQ1: Styles and genres. Are there stylistic attributes of newscasts that correlate with the genre of each provider?\nQ2: Coverage. Do major networks have more coverage of news events compared to minor ones?\nQ3: Timeliness. To what extent \u201cbreaking\u201d a news story depends on covering a large number of stories?"}, {"heading": "4.1 Styles and genres", "text": "We apply FABIA [12] to obtain a soft bi-clustering of news providers according to the words they use more frequently (we ignore for now named entities, which are considered in Section 5). The output is shown in Table 1, where we have included descriptive providers and words for each cluster.\nMost of the cohesive clusters are \u201cpure\u201d, i.e., they have a single genre. Additionally, the three most popular networks are clustered together in the sixth cluster. Among the nonpure clusters, the third one is the most interesting. E[ent], an entertainment news service, and CNN Headln[gen] cluster around descriptive words such as wearing, love, and looks, terms strongly related to the world of entertainment news. While E[ent] is expected to use such words, its similarity to CNN Headln[gen] can be attributed to at least two factors: (i) a deliberate stylistic and content choice made to compete with other fast-paced headline-oriented providers, and (ii) intrinsic aspects of the headline format, which is less formal, less deep, and short in nature, which inevitably leads to a more superficial coverage.\nFinally, the grouping of business and sports providers at the bottom of Table 1 is also of interest. They use similar polysemic terms such as beats, scoring, wins, loses. The use of a shared terminology stems from the common nature of competition associated with both sports and business.\nPart-of-speech classes and dependencies. We use partof-speech and dependency tags to analyze the differences in style among providers. A previous work done on online news observed that, e.g., adjectives are more commonly used in fashion and art articles [6]. Table 2 summarizes the main linguistic categories found in our dataset.\nWe represent each provider as a distribution over linguistic categories (e.g., number of verbs, number of adjectives), and apply hierarchical agglomerative clustering with euclidean distance to this representation. Figure 1 shows the resulting clustering of the top-30 providers with most mentions.\nThe clustering presents three clear super-groups: sports news on the left, entertainment news in the middle, and general and business news on the right. Thus, while business providers share their vocabulary with sports providers, their style is closer to general providers. Fox News and MSNBC are often considered antagonistic organizations with polarizing conservative and liberal views. However, from the perspective of style they are similar, and also similar to CNN. Therefore, the three most popular networks are similar both in their vocabulary and style. One outlier is PBS, essentially a public broadcaster whose style is quite different from the major networks. Finally, both KRON and NBC (which are affiliates and share several programs) show stylistic similarities to entertainment providers even when broadcasting general news.\nNext we proceed to aggregate the linguistic categories at the level of genres. Figure 2 presents the results, where we have also included the type of dependency found.\nIn a large number of cases for business providers, the dependency parser cannot extract the correct dependency (label \u201c/dep\u201d), while for entertainment providers the incidence is very small. A possible interpretation of this difference may be due to a different complexity of phrases. As observed by Flaounas et al. [6] for online news, politics, environment, science, and business use a more complex language; sports, arts, fashion and weather a simpler one. The analysis of other variables in our data seems to support this hypothesis. A typical sentence has 9.1 words in sports news, 9.0 words in entertainment news, 11.5 in general news, and 13.2 in business news. The Fog readability index [10] (which estimates the years of formal education needed to understand a text on a first reading) is 6.7 for sports, 7.2 for entertainment, 9.1 for general, and 9.4 for business.\nFor the other linguistic categories, entertainment has the largest relative prevalence of NN/poss (singular common noun, possession modifier, such as \u201cKristen Bell struggled to work with her fiance\u0301\u201d), sports has the largest value for NN/appos (singular common noun, appositional modifier, such as \u201cKevin Love\u2019s 51 points, a Minnesota Timberwolves team record\u201d), and general news has the largest value for NNP/nn (singular proper noun, compound modifier, such as \u201cPresident Obama is refocusing his campaign\u201d).\nSentiments. We analyze the distribution of sentiments expressed by each provider on each caption. The result is shown in Figure 3, in which we have included the number of negative words and positive words, as well as the distribution of sentiment scores. The bulk of sentiments on news seem to range from neutral to positive. All of the seven most positive providers are of the sports genre. CNN Headln[ent] is an outlier in at least two senses: it is the most negative and polarized provider, and it has many sentimentloaded words (e.g. it has more sentiment-loaded words than CNN Headln[gen], even when its constitutes the minority of programs in CNN Headln). This can be attributed to the \u201cattention-grabbing\u201d needs of the headlines format in the case of entertainment news."}, {"heading": "4.2 Coverage", "text": "In this section and in the next one, we make use of the news matchings described in Section 3.2. A provider covers a story if it has at least one qualified matching for it.\nWhen measuring coverage, we have to consider that some news stories are more prominent than others. We denote by prominence the fraction of providers of a given genre that covers a story, so a story has prominence 1.0 if it is covered by all the providers of a genre \u2013 which is quite rare.\nFigure 4(a) shows the probability that a provider of general news covers a story for different levels of prominence. Some providers such as NBC, Fox News, CBS and CNN Headln, offer more extensive news coverage than others. This wider selection of stories is likely due to having access to a larger pool of resources (e.g., employees, budget, affiliates) compared to the other general news providers. This result is expected, however the data also suggests two other relevant findings. NBC and CNN Headln seem to have a non-trivial coverage (\u2248 0.3\u22120.4) of relatively niche stories (prominence \u2248 0.2), content that is not covered by Fox News. However, Fox News has a wider coverage of stories having a prominence of \u2248 0.4 and over, which means it reports on a higher number of stories than either NBC or CNN.\nThis result profiles two distinct editorial approaches to covering the news, stemming from two distinct editorial agendas. NBC \u2013 which is often associated with a liberal agenda \u2013 chooses to spend resources to cover niche topics and issues, while Fox News \u2013 favored by conservatives \u2013 prefers to cover mostly wide-reaching, mainstream stories.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.2 0.4 0.6 0.8 1\nP ro\nba bi\nlit y\nof c\nov er\nin g\nProminence of a story (fraction of providers that mention it)\nNBC-w NBC-e Fox News CBS-e CNN Headln CBS-w Fox-w MSNBC KRON-w CNN CW-e PBS-w Fox Business PBS-e CW-w BBCa CW-w LinkTV KRON-e Lifetime Sleuth\n(a) Probability that a provider of general news covers a story as a function of its prominence.\n0\n200\n400\n600\n800\n1000\n1200\n0 0.2 0.4 0.6 0.8 1\nN um\nbe r\nof s\nto rie\ns\nProminence of a story (fraction of providers that mention it)\nNBC-w NBC-e Fox News CBS-e CNN Headln CBS-w Fox-w MSNBC KRON-w CNN CW-e PBS-w Fox Business PBS-e CW-w BBCa CW-w LinkTV KRON-e Lifetime Sleuth\n(b) Distribution of prominence for general news stories. The distribution is bimodal.\nFigure 4: Relationship between coverage and prominence for different providers (best seen in color).\nFigure 4(b) also shows coverage on general news, this time from the point of view of the distribution across different levels of prominence. The distribution is clearly bimodal, with the first mode around 3, and the second one around 14. Most news are covered by just a handful of providers, while a few manage to catch the attention of many providers. Fox Business has a large share of unique stories, probably due to the introduction of more niche stories from the business domain, even in their general news programs. In addition, a person watching NBC, KRON, and to some extent CNN Headln has a relatively higher chance to be exposed to news that are not shown by many other providers."}, {"heading": "4.3 Timeliness and duration", "text": "In this section we examine how different providers cover a story over time. The life cycle of a prominent news story is exemplified by Figure 5, which depicts the coverage of an abuse probe involving US marines during two days on January 2012. Each dot represents a matching of this news story with a provider.\nProviders are sorted by earliest matching: in this case, Fox News[gen], CNN Headln[gen] and MSNBC[gen] are the first to broadcast the story, within minutes of each other.\nThere are two time-dependent variables of interest that we can measure. The most important one is how often a provider is among the first ones to broadcast a story (i.e., \u201cbreaks\u201da story). The second is how long the providers keep broadcasting a given story.\nTimeliness. Given that many news networks operate in cycles of 60 minutes, we consider that a provider \u201cbreaks\u201d a story if it is among the ones that broadcast the story within the first 60 minutes since the first story matching. Figure 6 plots providers general news along two axes: how many qualified story matchings they provide and how many of those correspond to \u201cbreaking\u201d a story.\nMost of the providers lie along the diagonal (with a ratio of \u2248 1/10), with the exception of Fox Business and NBC. While these providers do not cover as many stories as CNN and Fox News, they are clearly better at breaking stories.\nDuration. We define the duration of a story as the interval between the first and the last matching. This interval is bounded by the lifetime of the stories, which is three days. Table 3 reports the average duration per provider.\nThe longest duration is found in sports providers, followed by business, then general ones. Indeed, a game that occurs over the week-end can be commented during several days. For the major general news providers, the typical duration of a story is from 8 to 12 hours."}, {"heading": "5. NEWSMAKERS", "text": "This section presents an analysis of newsmakers, i.e., people who appear in news stories on TV. We consider the following research questions:\nQ4: People and news. Which persons are more prominently mentioned on television news, and how?\nQ5: Newsmakers by profession. Are there observable differences in the way different professions are covered and portrayed by news programs?\nQ6: Newsmaker groups. To what extent can automatic text analysis identify groups of similar newsmakers?"}, {"heading": "5.1 People in the news", "text": "The number of mentions of people in the news follows a power law,4 as shown in Figure 7(a). Providers of sports news are the most prolific in terms of number of mentions of people (ESPN[spt], NFL[spt], ESN[spt], and NBA[spt]), directly followed by the three largest general news providers (Fox[gen], MSNBC[gen], and CNN[gen]). The high number of mentions of people in sports news is likely due to the coverage of team sports, where many athletes in competition are mentioned during a short period.\nThe distribution of number of mentions per person is also very skewed,5 as shown in Figure 7(b). The three most mentioned persons in our dataset are the current US president Barack Obama, the Republican Party politician Mitt Romney, and the basketball player LeBron James.\nDiversity. We also analyze the diversity of language associated with the top-30 persons with most mentions by using entropy as a proxy. The entropy of the vocabulary used when mentioning them is between 8 and 10 bits per word, which is in line with recent studies on English language [17]. When plotting entropy vs. number of mentions in lin-log scale (omitted for brevity), we observe 5 outliers with the largest entropy/ log(mentions) ratio: Ronald Reagan, Jimmy Carter, Bill Clinton, Hillary Clinton, and George W. Bush; all former US presidents except Hillary Clinton, current Secretary of State and former First Lady. A relatively larger entropy of vocabulary in mentions suggests a wider variety of topics, views and/or opinions when referring to a person.\n4 The curve fits a shifted Weibull distribution (\u03b1 = 0.48, \u03b2 = 20, 690, \u03b3 = 69) with a K.-S. statistic of 0.059. 5 The curve fits a Burr distribution (a generalization of the log-logistic distribution, parameters K = 1.8079, \u03b1 = 0.7139, \u03b2 = 407.35) with a K.-S. statistic of 0.025."}, {"heading": "5.2 Newsmakers by profession", "text": "The named entity tagger we use [30] resolves entities to Wikipedia pages, thus allowing us to obtain more information about people from those pages. We scrape data from Wikipedia infoboxes to categorize newsmakers according to professional areas and activities, and obtain a coverage of 98.2% of the mentions in our data. Table 4 shows the 24 most mentioned professions. The table spans a large range of prominence, from the most mentioned profession having more than 400k mentions to the last one shown in the table having only 3k.\nThe five most prominent professions in the news are basketball player, football player, Republican Party politician, Democratic Party politician, and musician. The rest of the list is dominated by other sportspeople, artists, and entertainers. The relative prominence of different sports probably varies during the year: our observation period covers the key games of the basketball and football season, but only the initial part of the baseball season. Further down the list, businesspeople appear at the 11th place, journalists at the 14th place, and government officials at position 19.\nConcentration of mentions per profession. The distribution of mentions per person in each profession varies substantially. Politicians and government officials are represented by a few key individuals who attract the majority of mentions, which is consistent with the findings of Schoenbach et al. [24]. Our dataset spans across the US presidential campaign period, which may cause mentions to be even more concentrated around the top candidates. A high level of concentration of mentions is observed also in businesspersons, dominated by Warren Buffett, and in individual sports such as golf, tennis, and wrestling.\nWith the exception of soccer (in which two-thirds of the mentions go to Lionel Messi and David Beckham), in team sports we observe that mentions are distributed across a wider range of athletes: this is the case for basketball, football, baseball, and hockey. Another profession in which the distribution of mentions is comparatively less skewed is acting, with George Clooney (the most mentioned actor during this period) attracting a mere 4% of mentions. Indeed, in recent years an increasing fragmentation of viewership of films has been reported6 with large Hollywood films facing increasing competition from TV films, dramas, and series.\nSentiments. We first focus on individuals and select those that have at least 10k mentions. The persons most associated with negative words are: Osama bin Laden (\u22120.92), Whitney Houston (who passed away during the observation period, \u22120.25), and George W. Bush (\u22120.21).\n6 http://www.nytimes.com/2012/10/29/movies/hollywood-seeks-toslow-cultural-shift-to-tv.html\nThe most associated with positive words are three football stars: Andrew Luck7 (1.7), Eli Manning (0.24) and Peyton Manning (0.11).\nIn terms of professions, Democratic Party politicians get a more negative treatment than Republican Party politicians. We observe that while the former are incumbent in the US government, the latter were undergoing their presidential primary during the first four months of our study. At each primary or caucus (there were tens of them) a number of winners or groups of winners were declared.\nOverall, the most positive average score (0.62) is attained by professional wrestlers. Note that Dwayne \u201cThe Rock\u201d Johnson and \u201980s popular culture icon Hulk Hogan also have an important career as entertainers, with\u201cThe Rock\u201dstaging a much-publicized come back in early 2012. The second most positive sentiment (0.11) is attained by journalists, thus indicating that they often refrain from criticizing or speaking in negative terms on air about their colleagues.\n7 This is likely to be to some extent, but not entirely, an artifact of \u201cluck\u201d being in the dictionary of the sentiment analysis software used."}, {"heading": "5.3 Automatic clustering of newsmakers", "text": "In Section 4.1 we showed that the distribution of linguistic classes can be used to cluster news providers. We attempt the same with newsmakers, however, the resulting clustering (omitted due to space constraints) contains a mixture of homogeneous and heterogeneous groups. To a large extent, this can be explained by variations across providers, which we treat more in-depth in Section 6.\nTable 5 shows a clustering based on linguistic attributes for each of the top providers per genre. Interestingly, entertainment and sports programs tend to conflate all politicians in one cluster, whereas business and general providers tend to separate them. For instance, CNN Headln[gen] generates a clear cluster with all the primary candidates, and Fox News[gen] separates primary candidates from final presidential candidates. This would suggest that Fox News has a more nuanced coverage of Republican Party politics than the other networks. Along the same lines, Fox Business[biz] refers to a mixture of entertainment people (George Clooney, Kim Kardashian) and sports people (LeBron James, Kobe Bryant) using a similar style, while E[ent] exhibit stylistic differences in the way it speaks about male celebrities (George Clooney, Justin Bieber) and female ones (Lindsay Lohan, Britney Spears): E displays a greater nuance in covering celebrity news.\nIn general, these differences suggest that providers are more discerning when covering people in their area of expertise, than when speaking about people outside it.\nTensor decomposition. We further explore the stylistic relationship between newsmakers and news providers via a multi-way analysis of the principal components extracted from newsmaker-tag-provider triads. Figure 8 shows the result of projecting a three-way interaction model on two dimensions while fixing the linguistic tags.\nThis model is obtained by a three-way decomposition [8], which estimates a core tensor of the data. This technique is a natural extension of principal component analysis (PCA) for two-dimensional matrices. We find that the dimensionality of the data is 3 (for the newsmakers dimension), 2 (for the linguistic dimension), and 3 (for the providers dimensions), and the tensor decomposition achieves a 78% accuracy in reconstructing the original data.\nThe first component neatly separates football from basketball players, which are the two most prominent professions in our dataset. The sport-specific providers NBA[spt] and NFL[spt] appear near the axes, as naturally they cover more in depth their main sport. Generalist sports news providers such as ESPN News[spt] and ESN2[spt] appear towards the top-right corner, while ESPN News[spt] seems to have a slight bias towards basketball.\nThe second component clearly separates sportspeople from politicians (the second most mentioned area in our dataset), together with the providers that mention each the most."}, {"heading": "6. FRAMING AND BIAS", "text": "In an often-cited definition, Entman [5] defines framing as selecting \u201csome aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.\u201d\nEditors must frame for the benefit of their audience, in order to maximize the information they obtain during their attention windows. Bias, on the other hand, corresponds to presenting a partial perspective on facts [23], or being perceived as being partial [20]. Viewers are likely to perceive bias that is opposite to their partisan or ideological views [18]: liberals tend to find a conservative bias in media, while conservatives tend to find the opposite [3].\nIn this section we analyze how different news providers frame different people. Specifically, we focus on the following research questions:\nQ7: Positive vs negative coverage. Are there significant differences in the sentiment polarity used by different providers when mentioning a given person?\nQ8: Outliers. Are there people who are treated differently in a given provider compared to the rest?\nQ9: Professionals or entertainers. Can we identify athletes or politicians that appear in the news because of their celebrity status, independently of their professional merit?"}, {"heading": "6.1 Positive vs negative coverage per provider", "text": "Table 6 shows the average sentiment of the four most mentioned people (two politicians and two sportsmen) across the providers with the largest number of mentions of people. Previous works based on manual coding have observed clear variations in the coverage of different candidates by the major networks [19]. Our automatic analysis confirms this finding: while Obama and Romney are treated equally by CNN and MSNBC, CNN Headln and Fox News give Romney a more positive treatment. An even larger difference favoring Romney is exhibited by Fox Business.\nWith respect to sports news, we notice an interesting phenomenon. NBA, specialized in basketball, speaks more positively about the football player (Tim Tebow); conversely, NFL, specialized in football, speaks more positively about the basketball player (LeBron James). On average, Tim Tebow receives a more positive coverage than LeBron James, who among other things is still criticized for a team transfer in 2010, and according to USA Today became in 2012 one of the most disliked athletes in the US.8\n8http://usat.ly/xgiJ25"}, {"heading": "6.2 Outlier analysis", "text": "We perform an analysis of outliers based on stylistic attributes (table omitted due to space limitations), and we find that the top outliers in each provider tend to include people that are famous in areas not usually covered by the provider, similarly to Table 5.\nWe show outliers by vocabulary in Table 7. For each provider, we present the people whose distribution of words in sentences mentioning them differs the most from the background distribution.\nWith the exception of Fox News[gen], other general news providers use a more specific vocabulary when speaking about current US president Barack Obama. Interestingly, for ESPN News[spt] and KRON[ent] the most significant outliers (Junior Seau and Whitney Houston, respectively) passed away during the observation period."}, {"heading": "6.3 Athletes, politicians ... or entertainers?", "text": "Sportspeople in general are often in the news for their off-field behavior more than their performance on the field. Other times, the media focuses their attention on them in a way that emphasizes their celebrity status as a media figure over their professional accomplishments. In Figure 9(a), we display the number of mentions in sports stories vs the number of mentions in general and entertainment news stories for the most mentioned sportspeople. Those above the diagonal are mentioned more in entertainment and general stories than other equally-famous people in sports news.\nFor instance, DwyaneWade, Kobe Bryant and Tom Brady are mentioned roughly the same number of times in sports news stories, however Tom Brady has a much larger presence in general and entertainment news \u2013 for instance, he has done guest appearances in comedy show Saturday Night Life and cartoons The Simpsons and Family Guy.9\nWe draw a similar graph for politicians in Figure 9(b). Arnold Schwarzenegger, Donald Trump,10 John F. Kennedy, Sarah Palin, and to some extent Hillary Clinton have a relatively strong presence in entertainment news. We note that California governor Arnold Schwarzenegger still has a career as an actor, Donald Trump appeared in a reality show, and Sarah Palin was a former beauty pageant.\n9 http://en.wikipedia.org/wiki/Tom_Brady#Personal_life\n10 During the period of observation his appearances in the news were\nmostly due to his participation in the Republican primaries and later endorsement of Mitt Romney."}, {"heading": "7. CONCLUSIONS", "text": "New domains provide both new challenges for machine learning methods and new insights to be drawn from the data. Closed captions data for television programs, now available from the Internet Archive, brings the television domain within the realm of data mining research. As already happened with the Web, we expect that mining this new source of data will provide a variety of interesting results.\nWe outlined the main results of an ambitious study on this large collection of closed captions, focusing on the domain of news. We demonstrated the richness of this dataset by analyzing several aspects such as the relationship between genres and styles, coverage and timeliness, and sentiments and biases when mentioning people in the news. The pipeline proposed in this paper breaks the stream of text into sentences, extracts entities, annotates the text with part-ofspeech tags and sentiment scores, and finally finds matching online news articles. These steps provide a basic building block for any future task that leverages closed caption data, e.g., stream segmentation in topically homogeneous and semantically meaningful parts, classification and clustering of television news or characterization of people and events.\nFuture work. This work is the first to apply automatic content analysis of text to closed captions of news on television, however it represents only the first step in this research direction. From an application point of view one important question is still unanswered: what is the best use of the information we can automatically extract? What is the\u201ckiller\u201d application? Several other questions about what is possible to extract automatically still remain to be investigated. For instance, can we automatically detect racism in networks? Is mass media proactive, in the sense that an editorial agenda drives the nature of news output? Or is it reactive, with the nature of the target audience driving the news output?\nAutomatic content analysis cannot replace human judgment, but allows researchers to answer questions using evidence that is orders of magnitude larger than the one obtained by manual coding. Computational methods can bring scientific rigor to the subjective world of media studies."}, {"heading": "8. REFERENCES", "text": "[1] P. R. Center. In Changing News Landscape, Even Television\nis Vulnerable. Trends in News Consumption., Sep. 2012. [2] C. Cieri, D. Graff, M. Liberman, N. Martey, S. Strassel, et al.\nThe TDT-2 text and speech corpus. In Proc. of DARPA Broadcast News Workshop, pages 57\u201360, 1999. [3] K. Coe, D. Tewksbury, B. J. Bond, K. L. Drogos, R. W. Porter, A. Yahn, and Y. Zhang. Hostile news: Partisan use and perceptions of cable news programming. J. of Comm., 58(2):201\u2013219, 2008. [4] D. D\u2019Alessio and M. Allen. Media bias in presidential elections: a meta-analysis. J. of Comm., 50(4):133\u2013156, 2000. [5] R. M. Entman. Framing: Toward clarification of a fractured paradigm. J. of Comm., 43(4):51\u201358, 1993. [6] I. Flaounas, O. Ali, T. Lansdall, T. De Bie, N. Mosdell, J. Lewis, and N. Cristianini. Research methods in the age of digital journalism. Digital Journalism, 1(1):102\u2013116, 2013. [7] D. Gibbon, Z. Liu, E. Zavesky, D. Paul, D. F. Swayne, R. Jana, and B. Shahraray. Combining content analysis of television programs with audience measurement. In Proc. of CCNC, pages 754\u2013758. IEEE, 2012. [8] P. Giordani, H. Kiers, and M. Del Ferraro. ThreeWay: An R package for three-way component analysis, 2012. [9] T. Groseclose and J. Milyo. A measure of media bias. Quarterly Journal of Economics, 120(4):1191\u20131237, 2005.\n[10] R. Gunning. The technique of clear writing. McGraw-Hill New York, 1952. [11] M. Henzinger, B.-W. Chang, B. Milch, and S. Brin. QueryFree news search. WWWJ, 8(2):101\u2013126, 2005. [12] S. Hochreiter, U. Bodenhofer, M. Heusel, A. Mayr, A. Mitterecker, A. Kasim, T. Khamiakova, S. Van Sanden, D. Lin, W. Talloen, L. Bijnens, H. W. H. Go\u0308hlmann, Z. Shkedy, and D.-A. Clevert. FABIA: factor analysis for bicluster acquisition. Bioinformatics, 26(12):1520\u20131527, 2010. [13] W.-H. Lin. Identifying Ideological Perspectives in Text and Video. PhD thesis, CMU, Oct. 2008. [14] W. H. Lin and A. Hauptmann. News video classification using SVM-based multimodal classifiers and combination strategies. In Proc. of ACM MM, pages 323\u2013326, 2002. [15] W. H. Lin, T. Wilson, J. Wiebe, and A. Hauptmann. Which side are you on?: identifying perspectives at the document and sentence levels. In Proc. of CoNLL, CoNLL-X \u201906, pages 109\u2013116. ACL, 2006. [16] H. Misra, F. Hopfgartner, A. Goyal, P. Punitha, and J. Jose. TV news story segmentation based on semantic coherence and content similarity. In Proc. of MMM, volume 5916 of LNCS, pages 347\u2013357, 2010. [17] M. Montemurro and D. Zanette. Universal entropy of word ordering across linguistic families. PLoSOne, 6(5), 2011. [18] J. S. Morris. Slanted objectivity? perceived media bias, cable news exposure, and political attitudes. Social Science Quarterly, 88(3):707\u2013728, 2007. [19] J. S. Morris and P. L. Francia. From network news to cable commentary: The evolution of television coverage of the party conventions. In State of the Parties Conference, 2005. [20] D. Niven. Objective evidence on media bias: Newspaper coverage of congressional party switchers. Journalism & Mass Communication Quarterly, 80(2):311\u2013326, 2003. [21] S. Oger, M. Rouvier, and G. Linares. Transcription-based video genre classification. In Proc. of ICASSP, pages 5114\u2013 5117. IEEE, 2010. [22] D. Paranjpe. Learning document aboutness from implicit user feedback and document structure. In Proc. of CIKM, pages 365\u2013374. ACM Press, 2009. [23] S. Reese and P. Shoemaker. Mediating the message: Theories of influence on mass media content. Journalism Quarterly, 74:2, 1996. [24] K. Schoenbach, J. De Ridder, and E. Lauf. Politicians on TV news: Getting attention in dutch and german election campaigns. European Journal of Political Research, 39(4): 519\u2013531, 2001. [25] D. A. Shamma, L. Kennedy, and E. F. Churchill. Tweet the debates: understanding community annotation of uncollected sources. In Proc. of WSM, pages 3\u201310. ACM, 2009. [26] A. F. Smeaton, H. Lee, N. E. O\u2019Connor, S. Marlow, and N. Murphy. TV news story segmentation, personalisation and recommendation. In AAAI Spring Symp. on Intelligent Multimedia Knowledge Management, 2003. [27] M. Thelwall, K. Buckley, G. Paltoglou, D. Cai, and A. Kappas. Sentiment strength detection in short informal text. JASIST, 61(12):2544\u20132558, 2010. [28] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of NAACL, pages 173\u2013180. ACL, 2003. [29] L. Xu, Q. Ma, and M. Yoshikawa. Credibility-oriented ranking of multimedia news based on a material-opinion model. In Proc. of WAIM, pages 290\u2013301, 2011. [30] Y. Zhou, L. Nie, O. Rouhani-Kalleh, F. Vasile, and S. Gaffney. Resolving surface forms to Wikipedia topics. In Proc. of COLING, pages 1335\u20131343. ACL, 2010."}], "references": [{"title": "Changing News Landscape, Even Television is Vulnerable", "author": ["P.R. Center"], "venue": "Trends in News Consumption.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "et al", "author": ["C. Cieri", "D. Graff", "M. Liberman", "N. Martey", "S. Strassel"], "venue": "The TDT-2 text and speech corpus. In Proc. of DARPA Broadcast News Workshop, pages 57\u201360", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Hostile news: Partisan use and perceptions of cable news programming", "author": ["K. Coe", "D. Tewksbury", "B.J. Bond", "K.L. Drogos", "R.W. Porter", "A. Yahn", "Y. Zhang"], "venue": "J. of Comm., 58(2):201\u2013219", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Media bias in presidential elections: a meta-analysis", "author": ["D. D\u2019Alessio", "M. Allen"], "venue": "J. of Comm.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Framing: Toward clarification of a fractured paradigm", "author": ["R.M. Entman"], "venue": "J. of Comm., 43(4):51\u201358", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Research methods in the age of digital journalism", "author": ["I. Flaounas", "O. Ali", "T. Lansdall", "T. De Bie", "N. Mosdell", "J. Lewis", "N. Cristianini"], "venue": "Digital Journalism, 1(1):102\u2013116", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining content analysis of television programs with audience measurement", "author": ["D. Gibbon", "Z. Liu", "E. Zavesky", "D. Paul", "D.F. Swayne", "R. Jana", "B. Shahraray"], "venue": "Proc. of CCNC, pages 754\u2013758. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "and M", "author": ["P. Giordani", "H. Kiers"], "venue": "Del Ferraro. ThreeWay: An R package for three-way component analysis", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A measure of media bias", "author": ["T. Groseclose", "J. Milyo"], "venue": "Quarterly Journal of Economics, 120(4):1191\u20131237", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "The technique of clear writing", "author": ["R. Gunning"], "venue": "McGraw-Hill New York", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1952}, {"title": "Query- Free news search", "author": ["M. Henzinger", "B.-W. Chang", "B. Milch", "S. Brin"], "venue": "WWWJ, 8(2):101\u2013126", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "FABIA: factor analysis for bicluster acquisition", "author": ["S. Hochreiter", "U. Bodenhofer", "M. Heusel", "A. Mayr", "A. Mitterecker", "A. Kasim", "T. Khamiakova", "S. Van Sanden", "D. Lin", "W. Talloen", "L. Bijnens", "H.W.H. G\u00f6hlmann", "Z. Shkedy", "D.-A. Clevert"], "venue": "Bioinformatics, 26(12):1520\u20131527", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying Ideological Perspectives in Text and Video", "author": ["W.-H. Lin"], "venue": "PhD thesis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "News video classification using SVM-based multimodal classifiers and combination strategies", "author": ["W.H. Lin", "A. Hauptmann"], "venue": "Proc. of ACM MM, pages 323\u2013326", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Which side are you on?: identifying perspectives at the document and sentence levels", "author": ["W.H. Lin", "T. Wilson", "J. Wiebe", "A. Hauptmann"], "venue": "Proc. of CoNLL, CoNLL-X \u201906, pages 109\u2013116. ACL", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "TV news story segmentation based on semantic coherence and content similarity", "author": ["H. Misra", "F. Hopfgartner", "A. Goyal", "P. Punitha", "J. Jose"], "venue": "Proc. of MMM, volume 5916 of LNCS, pages 347\u2013357", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Universal entropy of word ordering across linguistic families", "author": ["M. Montemurro", "D. Zanette"], "venue": "PLoSOne, 6(5)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Slanted objectivity? perceived media bias", "author": ["J.S. Morris"], "venue": "cable news exposure, and political attitudes. Social Science Quarterly, 88(3):707\u2013728", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "From network news to cable commentary: The evolution of television coverage of the party conventions", "author": ["J.S. Morris", "P.L. Francia"], "venue": "State of the Parties Conference", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Objective evidence on media bias: Newspaper coverage of congressional party switchers", "author": ["D. Niven"], "venue": "Journalism & Mass Communication Quarterly, 80(2):311\u2013326", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Transcription-based video genre classification", "author": ["S. Oger", "M. Rouvier", "G. Linares"], "venue": "Proc. of ICASSP, pages 5114\u2013 5117. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning document aboutness from implicit user feedback and document structure", "author": ["D. Paranjpe"], "venue": "Proc. of CIKM, pages 365\u2013374. ACM Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Mediating the message: Theories of influence on mass media content", "author": ["S. Reese", "P. Shoemaker"], "venue": "Journalism Quarterly, 74:2", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "Politicians on TV news: Getting attention in dutch and german election campaigns", "author": ["K. Schoenbach", "J. De Ridder", "E. Lauf"], "venue": "European Journal of Political Research, 39(4): 519\u2013531", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Tweet the debates: understanding community annotation of uncollected sources", "author": ["D.A. Shamma", "L. Kennedy", "E.F. Churchill"], "venue": "Proc. of WSM, pages 3\u201310. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "N", "author": ["A.F. Smeaton", "H. Lee"], "venue": "E. O\u2019Connor, S. Marlow, and N. Murphy. TV news story segmentation, personalisation and recommendation. In AAAI Spring Symp. on Intelligent Multimedia Knowledge Management", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Sentiment strength detection in short informal text", "author": ["M. Thelwall", "K. Buckley", "G. Paltoglou", "D. Cai", "A. Kappas"], "venue": "JASIST, 61(12):2544\u20132558", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proc. of NAACL, pages 173\u2013180. ACL", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Credibility-oriented ranking of multimedia news based on a material-opinion model", "author": ["L. Xu", "Q. Ma", "M. Yoshikawa"], "venue": "Proc. of WAIM, pages 290\u2013301", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Resolving surface forms to Wikipedia topics", "author": ["Y. Zhou", "L. Nie", "O. Rouhani-Kalleh", "F. Vasile", "S. Gaffney"], "venue": "Proc. of COLING, pages 1335\u20131343. ACL", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The ascent of the Web has caused a significant drop in newspaper and radio audiences, but television remains the number one source for news in the US [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "One of the oldest references on mining television content is a DARPA-sponsored workshop in 1999 with a topic detection and tracking challenge [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 10, "context": "[11] describe a system for finding web pages related to television content, and test different methods to synthesize a web search query from a television transcript.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] classify videos based on a transcription obtained from", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] describe a system to rate the credibility of information items on television by looking at how often the same image is described in a similar way by more than one news source.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Lin and Hauptmann [14] present a system for video classification based on closed captions as well as content-based attributes of the video.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "[16] also combine closed captions and multimedia content to improve video segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] align the closed captions during a live event (a presidential debate) with social media reactions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] combine closed captions with real-time television audience measures to detect ads \u2013 which typically are accompanied by sudden drops in viewership (\u201czapping\u201d).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "For instance, D\u2019Alessio and Allen [4] study gatekeeping, coverage, and statement bias by annotating content from magazines and television in the US.", "startOffset": 34, "endOffset": 37}, {"referenceID": 23, "context": "[24] study Dutch and German television and observe that top leaders such as chancellors or", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Morris and Francia [19] focus on US party conventions, and notice that what people hear on the news is mostly commentary, with limited amount of quotes from the convention participants.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "Groseclose and Milyo [9] use an indirect measure of television bias by manually counting references to think tanks in each network, and then by scoring each think tank on a left-right political spectrum by", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "[6] study news articles available on the web, and analyze the prevalence of different topics and the distribution of readability and subjectivity scores.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "The PhD thesis of Lin [13] and related papers (e.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "[15]), introduce a joint probabilistic model of the topics and the perspectives from which documents and sentences are written.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The three major news networks according to the number of self-declared regular viewers [1] are Fox News (21%), CNN (16%) and MSNBC (11%).", "startOffset": 87, "endOffset": 90}, {"referenceID": 25, "context": "We remark that there exist methods to join sentences into passages [26, 16], but for our analysis we use single sentences as basic units of content, and we only group them when they match to the same news item, as described in Section 3.", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "We remark that there exist methods to join sentences into passages [26, 16], but for our analysis we use single sentences as basic units of content, and we only group them when they match to the same news item, as described in Section 3.", "startOffset": 67, "endOffset": 75}, {"referenceID": 29, "context": "Second, we recognize and extract named entities by using a named entity tagger that works in two steps: entity resolution [30] and\u201caboutness\u201d ranking [22].", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "Second, we recognize and extract named entities by using a named entity tagger that works in two steps: entity resolution [30] and\u201caboutness\u201d ranking [22].", "startOffset": 150, "endOffset": 154}, {"referenceID": 27, "context": "Third, we apply the Stanford NLP tagger [28] to perform part-of-speech tagging and dependency parsing.", "startOffset": 40, "endOffset": 44}, {"referenceID": 26, "context": "to each sentence by using SentiStrength [27].", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "[11], but the approach is based on supervised learning rather than web searches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We apply FABIA [12] to obtain a soft bi-clustering of news providers according to the words they use more frequently (we ignore for now named entities, which are considered in Section 5).", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": ", adjectives are more commonly used in fashion and art articles [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "[6] for online news, politics, envi-", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The Fog readability index [10]", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "The entropy of the vocabulary used when mentioning them is between 8 and 10 bits per word, which is in line with recent studies on English language [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 29, "context": "The named entity tagger we use [30] resolves entities to Wikipedia pages, thus allowing us to obtain more information about people from those pages.", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "This model is obtained by a three-way decomposition [8], which estimates a core tensor of the data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "In an often-cited definition, Entman [5] defines framing as selecting \u201csome aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described.", "startOffset": 37, "endOffset": 40}, {"referenceID": 22, "context": "Bias, on the other hand, corresponds to presenting a partial perspective on facts [23], or being perceived as being partial [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "Bias, on the other hand, corresponds to presenting a partial perspective on facts [23], or being perceived as being partial [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "Viewers are likely to perceive bias that is opposite to their partisan or ideological views [18]: liberals tend to find a conservative bias in media, while conservatives tend to find the opposite [3].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Viewers are likely to perceive bias that is opposite to their partisan or ideological views [18]: liberals tend to find a conservative bias in media, while conservatives tend to find the opposite [3].", "startOffset": 196, "endOffset": 199}, {"referenceID": 18, "context": "Previous works based on manual coding have observed clear variations in the coverage of different candidates by the major networks [19].", "startOffset": 131, "endOffset": 135}], "year": 2013, "abstractText": "We perform an automatic analysis of television news programs, based on the closed captions that accompany them. Specifically, we collect all the news broadcasted in over 140 television channels in the US during a period of six months. We begin by segmenting, processing, and annotating the closed captions automatically. Next, we focus on the analysis of their linguistic style and on mentions of people. We present a series of key insights about news providers, about people in the news, and about the biases that can be uncovered by automatic means. These insights are contrasted by looking at the data from multiple points of view, including qualitative assessment.", "creator": "gnuplot 4.6 patchlevel 1"}}}