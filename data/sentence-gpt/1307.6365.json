{"id": "1307.6365", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2013", "title": "Time-Series Classification Through Histograms of Symbolic Polynomials", "abstract": "Time-series classification has attracted considerable research attention due to the various domains where time-series data are observed, ranging from medicine to econometrics. Traditionally, the focus of time-series classification has been on short time-series data composed of a unique pattern with intraclass pattern distortions and variations, while recently there have been attempts to focus on longer series composed of various local patterns. This study presents a novel method which can detect local patterns in long time-series via fitting local polynomial functions of arbitrary degrees. The coefficients of the polynomial functions are converted to symbolic words via equivolume discretizations of the coefficients' distributions. The symbolic polynomial words enable the detection of similar local patterns by assigning the same words to similar polynomials. Moreover, a histogram of the frequencies of the words is constructed from each time-series' bag of words. Each row of the histogram enables a new representation for the series and symbolize the existence of local patterns and their frequencies. Experimental evidence demonstrates outstanding results of our method compared to the state-of-art baselines, by exhibiting the best classification accuracies in all the datasets and having statistically significant improvements in the absolute majority of experiments.\n\n\n\n\n\n\n\nThe results of this study suggest that the pattern similarity and its occurrence in the long time-series is highly variable and could be related to the effect of the time-series and its frequency. However, as the time series was defined using a formula that does not represent a random function, this result is not uniform. This means that the number of terms, time series and the length of time series can differ significantly from the data, but the data are not uniform as shown in the table below.\n\nThe analysis of the long time series is summarized in two parts. The first part explains the relation between the term and its occurrence and the frequency and its occurrence as well as the frequency. The second part provides the basis for the concept of the characteristic of a given variable. This method also applies to the duration of the series. The second part explains the similarity of the term with the duration as well as its occurrence as the frequency of the corresponding term. The third part explains the relation between the time series and the frequency of the corresponding term.\nThe main study, in contrast, shows the relationship between the frequency and the frequency of the corresponding term. The first part explains the relation between the frequency and the frequency of the corresponding term. The second part reveals the relation between", "histories": [["v1", "Wed, 24 Jul 2013 10:07:50 GMT  (7692kb,D)", "http://arxiv.org/abs/1307.6365v1", null], ["v2", "Thu, 25 Jul 2013 03:40:27 GMT  (7692kb,D)", "http://arxiv.org/abs/1307.6365v2", "Submitted to IEEE ICDM 2013"], ["v3", "Wed, 31 Jul 2013 10:58:02 GMT  (0kb,I)", "http://arxiv.org/abs/1307.6365v3", "Potential conflict with conference dual submission rule"], ["v4", "Mon, 23 Dec 2013 22:26:35 GMT  (5380kb,D)", "http://arxiv.org/abs/1307.6365v4", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB cs.LG", "authors": ["josif grabocka", "martin wistuba", "lars schmidt-thieme"], "accepted": false, "id": "1307.6365"}, "pdf": {"name": "1307.6365.pdf", "metadata": {"source": "CRF", "title": "Time-Series Classification Through Histograms of Symbolic Polynomials", "authors": ["Josif Grabocka", "Martin Wistuba", "Lars Schmidt-Thieme"], "emails": ["schmidt-thieme}@ismll.uni-hildesheim.de"], "sections": [{"heading": null, "text": "Time-Series Classification Through Histograms of Symbolic Polynomials\nJosif Grabocka, Martin Wistuba, Lars Schmidt-Thieme Information Systems and Machine Learning Lab\nUniversity of Hildesheim {josif, wistuba, schmidt-thieme}@ismll.uni-hildesheim.de\nAbstract\u2014Time-series classification has attracted considerable research attention due to the various domains where timeseries data are observed, ranging from medicine to econometrics. Traditionally, the focus of time-series classification has been on short time-series data composed of a unique pattern with intraclass pattern distortions and variations, while recently there have been attempts to focus on longer series composed of various local patterns. This study presents a novel method which can detect local patterns in long time-series via fitting local polynomial functions of arbitrary degrees. The coefficients of the polynomial functions are converted to symbolic words via equivolume discretizations of the coefficients\u2019 distributions. The symbolic polynomial words enable the detection of similar local patterns by assigning the same words to similar polynomials. Moreover, a histogram of the frequencies of the words is constructed from each time-series\u2019 bag of words. Each row of the histogram enables a new representation for the series and symbolize the existence of local patterns and their frequencies. Experimental evidence demonstrates outstanding results of our method compared to the state-of-art baselines, by exhibiting the best classification accuracies in all the datasets and having statistically significant improvements in the absolute majority of experiments.\nI. INTRODUCTION\nClassification of time series is an important domain of machine learning due to the widespread occurrence of timeseries data in real-life applications. Measurements conducted in time are frequently encountered in diverse domains ranging from medicine and econometrics up to astronomy. Therefore, time series has attracted considerable research interest in the last decade and a myriad of classification methods have been introduced.\nMost of the existing literature on time-series classification focuses on classifying short time series, that is series which mainly incorporate a single long pattern. The research problem within this family of time-series data is the detection of pattern distortions and other types of intra-class pattern variations. Among other successful techniques in this category, the nearest neighbor classifier equipped with a similarity metric called Dynamic Time Warping (DTW) has been shown to perform well in a large number of datasets [1].\nNevertheless, few studies [2]\u2013[4] have been dedicated towards the classification of time-series data which is long and composed of many local patterns. Avoiding the debate on the categorization of the words short and long, we would like to emphasize the differences between those two types of data with a clarification analogy. For instance, assume we would like to measure the similarity between two strings (i.e. words or timeseries of characters). In such a case, string similarity metrics\nwill scan the characters of those two strings, in a sequence, and detect the degree of matchings/similarities. In contrast, if we are searching for similarities between two books, then a character by character similarity is meant to fail because the arrangement of words in different books is never exactly the same. Text mining approaches which compute bags of words occurring in a text and then utilize histograms of the occurrences of each word, have been recently applied to the time-series domain by the metaphor of computing \u201dbags of patterns\u201d [2], [4].\nThis paper presents a novel method to classify long time series composed of local patterns occurring in an unordered fashion and by varying frequencies. Our principle relies on detecting local polynomial patterns which are extracted in a sliding window approach, hence fitting one polynomial to each sliding window segment. As will be detailed in Section III-C we propose a quick technique to fit sliding window content which has a linear run-time complexity.\nOnce the polynomial coefficients of each sliding window content are computed, then we convert those coefficients into symbolic forms, (i.e. alphabet words). The motivation for calling the method Symbolic Polynomial arises from that procedure. Such discretization of polynomial coefficients, in the form of words, allows the detection of similar patterns by converting close coefficient values into the same literal word. In addition, the words computed from the time series allow the construction of a dictionary and a histogram of word frequencies, which enables an efficient representation of local patterns.\nWe utilize an equivolume discretization of the distributions of the polynomial coefficients to compute the symbolic words, as will be explained in Section III-D. Threshold values are computed to separate the distribution into equal volumes and each volume is assigned one alphabet letter. Consequently, each polynomial coefficient is assigned to the region its value belongs to, and is replaced by the region\u2019s character. Ultimately, the word of a polynomial is the concatenation of the characters of each polynomial coefficient merged together. The words of each time series are then stored in a separate \u2019bag\u2019. A dictionary is constructed with each word appearing at least once in the dataset and a histogram is initialized with each row representing a time series and each column one of the words in the dictionary. Finally, the respective frequencies of words are updated for each time series and the rows of the histogram are the new representation of the original time series. Such a representation offers a powerful mean to reflect which patterns (i.e. symbolic polynomial words) and how often they\nar X\niv :1\n30 7.\n63 65\nv1 [\ncs .A\nI] 2\n4 Ju\nl 2 01\n3\noccur in a series (i.e. the frequency value in each histogram cell).\nThe novelty of our method, compared to state-of-art approaches [3], [4] which utilize constant functions to express local patterns, relies on offering an expressive technique to represent patterns as polynomial of arbitrary degrees. Furthermore, we present a fitting algorithm which can compute the polynomial coefficients for a sliding window segment in linear time, therefore our method offers superior expressiveness without compromising run-time complexity.\nEmpirical results conducted on datasets from the health care domain, demonstrate qualitative improvements compared to the state-of-art techniques in terms of classification accuracy, as is detailed in Section IV-D. We experimented with the datasets from [4], and in order to widen the variety of data we introduce three new datasets. Our method wins in all four datasets with a clear statistically significance in three of them, proving the validity of our method. We add experiments regarding the running time of the method and we show that our method is practically fast and feasible."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Time-Series Representations", "text": "In order to understand the regularities embedded inside time-series, a large number of researchers have invested efforts into deriving and discovering time series representations. The ultimate target of representation methods is to encapsulate the regularities of time-series patterns by omitting the intrinsic noise. Discrete Fourier transforms have attempted to represent repeating series structures as a sum of sinusoidal signals [5]. Similarly, wavelet transformations approximate a timeseries via orthonormal representations in the form of wavelets [6]. However, such representations perform best under the assumption that series contain frequently repeating regularities and little noise which is not strictly the case in real-life applications. Singular Value Decomposition is a dimensionality reduction technique which has also been applied to extract latent dimensionality information of a series [7], while supervised decomposition techniques have aimed at incorporating class information into the low-rank data learning [8].\nIn addition to those approaches, researchers have been also focused on preserving the original form of the time series without transforming them to different representations. Nevertheless, the large number of measurement points negatively influence the run-time of algorithms. Attempts to shorten time series by preserving their structure started by linearly averaging chunks of series points. Those chunks are converted to a single mean value and the concatenation of means create a short form known as a Piecewise Constant Approximation [9]. A more sophisticated technique operates by converting the mean values into symbolic form into a method called Symbolic Aggregate Approximation, denoted shortly as SAX [10], [11]. SAX enables the conversion of time-series values into a sequence of symbols and offers the possibility to semantically interpret series segments. Further sophistication of lower bounding techniques have advanced the representation method towards efficient indexing and searching [12], enabling large scale mining of time series [13]. Nonlinear approximations of the series segments have also been proposed. For instance\nleast squares approximation of time series via orthogonal polynomials have been proposed for segmentation purposes in a hybrid sliding/growing window scenario [14]. Throughout this paper we will propose a novel representation technique based on the utilization of polynomial functions of an arbitrary degree to approximate sliding windows of a time series. Our method brings novelty in converting the coefficients into literal representations, while the ultimate form is the frequency of the literal words constructed per each sliding window."}, {"heading": "B. Time-Series Similarity Metrics", "text": "The time-series community has invested considerable efforts in understanding the notion of similarity among series. Time series patterns exhibit high degrees of intra and inter class variation, which is found in forms of noisy distortions, phase delays, frequency differences and signal scalings. Therefore, accurate metrics to evaluate the distance among two series play a crucial role in terms of clustering and classification accuracy. Euclidean distance, commonly known as the L2 norm between vectors, is a fast metric which compares the offset of every pair of points from two series, belonging to the same time stamp index. Despite being a fast metric of linear run-time complexity, the Euclidean distance is not directly designed to detect pattern variations. A popular metric called Dynamic Time Warping (DTW) overcomes the deficiencies of the Euclidean distance by allowing the detection of relative time indexes belonging to similar series regions. DTW achieves highly competitive classification accuracies and is regarded as a strong baseline [1]. Even though DTW is slow in the original formulation having a quadratic run-time complexity, still recent techniques involving early pruning and lower bounding have utilized DTW for fast large scale search [15].\nOther techniques have put emphasis on the need to apply edit distance penalties for assessing the similarity between time series [16], [17]. Such methods are inspired by the edit distance principle of strings which counts the number of atomic operations needed to convert a string to the other. In the context of time series the analogy is extended to the sum of necessary value changes needed for an alignment. Other approaches have put emphasis on detecting the longest common subsequence of series, believing in the assumption that time series have a fingerprint segment which is the most determinant with respect to classification [18]. Detection of similarities in a streaming time-series scenarios motivated attempts to handle scaling and shifting in the temporal and amplitude aspects [19]."}, {"heading": "C. Time-Series Classification", "text": "During the past decade, most time series classification attempts have targeted the classification of short time series. Nevertheless, the definition of shortness might lead to ambiguous understandings, therefore we would dedicate some room for further clarifying our definition. For instance, assume a time-series dataset representing outer shapes of two different tree leaves, hence a binary classification. Such series include one single pattern each and do not exceed hundreds of data points, therefore we define them simply short. On the other hand assume we have an imaginary large dataset containing concatenated outer shapes of all the leaves of a forest. Then our imaginary task is to compare different forests for classifying which continent the forest is located in. In the second case, the\ntime series are a combination of many local patterns occurring at varying frequencies and positions. This category of datasets is hereafter defined as long time series.\n1) Classifying Short Time Series: Classification of short time series has gathered considerable attraction in the literature. Among the initial pioneer methods and still one of the best performing ones is the nearest neighbor classifier accompanied by the DTW distance metrics, which constitute a hard-to-beat baseline [1]. Other powerful nonlinear classifiers like the Support Vector Machines have been tweaked to operate over time series, partially because originally the kernel functions are not designed for invariant pattern detection and partially because DTW is not a positive semi-definite kernel [20]. Therefore the creation of positive semi-definite kernels like the Gaussian elastic metric kernel arose [21]. Another approach proposed to handle variations by inflating the training set and creating new distorted instances from the original ones [22].\n2) Classifying Long Time Series: The classification of long time series focuses on long signals which are composed of one or more types of patterns appearing in unpredicted order and frequencies. Principally, the classification of those series has been mainly conducted by detecting the inner patterns and computing statistics over them. For instance, underlying series patterns have been expressed as the motifs and the difference between the motif frequencies has been utilized [2]. Other approaches have explored the conversion of each sliding window segment into a literal word constructed by piecewise constant approximations and the SAX method [3], [4]. The words belonging to each time series are gathered in a \u2019bag\u2019 and a histogram of the words is constructed. The histogram vectors are the new representations of the time series. Such a technique has been shown to be rotation-invariant, because the occurrence of a pattern is not related to its position [4]. In contrast to the existing work, our novel study introduces an expressive histogram formulation based on literal words build from local pattern detection via polynomial approximations. Our model ensures scalability by computing in linear run-time."}, {"heading": "III. PROPOSED METHOD", "text": ""}, {"heading": "A. Preamble Definitions", "text": "1) Alphabet: An alphabet is an ordered set of distinct symbols and is denoted by \u03a3. The number of symbols in an alphabet is called the size of the alphabet and denoted by \u03b1 = |\u03a3|. For illutration purposes we will utilize the Latin variant for the English language composed of the set of character symbols \u03a3 = (A,B,C, . . . , Y, Z).\n2) Word: A word w \u2208 \u03a3\u2217 from an alphabet is defined as a sequence of symbols, therefore one sequence out of the set of possible sequences of arbitrary length l, defined as the Kleene star \u03a3\u2217 := \u222a\u221el=0\u03a3l. For instance CACB is a word from the English alphabet having length four.\n3) Polynomial: A polynomial of degree d having coefficients \u03b2 \u2208 Rd+1, is defined as a sum of terms known as monomials. Each monomial is a multiplication of a coefficient times the a power of the predictor value X \u2208 RN , as shown in Equation 1. The polynomial can also be written as a linear dot product in case we introduce a new predictor variable\nZ \u2208 RN\u00d7(d+1) which is composed of all the powers of the original predictor variable X .\nY\u0302 = d\u2211 j=0 \u03b2jX j = Z\u03b2, (1)\nwhere Z := [X0, X1, X2, . . . , Xd]\n4) Time Series: A time series of length N is an ordered sequence of numerical values and denoted by S \u2208 RN . The special characteristics of time-series data compared to plain vector instances is the high degree of correlation that close-by values have in the sequence. A time-series dataset containing M instances is denoted as T \u2208 RM\u00d7N , assuming time series of a dataset have the same length.\n5) Sliding Window Segment: A sliding window segment is an uninterrupted subsequence of S having length n denoted by St,n \u2208 Rn. The time index t represents the starting point of the series, while the index n the length of the sliding window, i.e. St,n = [St, St+1, St+2, . . . , St+n\u22121]. The total number of sliding window segments of size n for a series of length N is N \u2212n, in case we slide the window by incrementing the start index t by one index at a time."}, {"heading": "B. Proposed Principle", "text": "The principle proposed in this study is to detect local patterns in a time series via computing local polynomials. The polynomials offer a superior mean to detect local patterns compared to constant or linear models, because they can perceive information like the curvature of a sub-series. Furthermore, in case of reasonably sized sliding windows the polynomials can approximate the underlying series segment without over-fitting. In this paper, we demonstrate that the polynomial fitting for the sliding window scenario can be computed in linear runtime. Once the local polynomials are computed, we propose a novel way to utilize the polynomial coefficients for computing the frequencies of the patterns. The polynomial coefficients are converted to alphabet words via an equivolume discretization approach. Such a conversion from real valued coefficients to short symbolic words allows for the translation of similar polynomials to the same word, therefore similar patterns can be detected. We call such words symbolic polynomials. The words belonging to the time series are collected in a large \u2019bag\u2019 of words, (implemented as a list), then a histogram is created by summing up the frequency of occurrence for each words. Each row of a histogram encapsulates the word frequencies of time series, (i.e. frequencies of local patterns). A histogram row is the new representation of the time series and is used as a vector instance for classification."}, {"heading": "C. Local Polynomial Fitting", "text": "Our method operates by sliding a window throughout a time-series and computing the polynomial coefficients in that sliding window segment. The segment of time series inside the sliding window is normalized before being approximated to a mean 0 and deviation of 1. The incremental step for sliding a window is one, so that every subsequence is scanned. Computing the coefficients of a polynomial regression is conducted by minimizing the least squares error between the\npolynomial estimate and the true values of the sub-series. The objective function is denoted by L and is shown in Equation 2. The task is to fit a polynomial to approximate the real values Y of the time series window of length n, previously denoted as St,n.\nL(Y, Y\u0302 ) = ||Y \u2212 Z\u03b2||2 (2) Y := [St, St+1, St+2, . . . St+n\u22121]\nInitially, the predictors are the time indexes X = [0, 1, . . . , n\u2212 1] and they are converted to the linear regression form by introducing a variable Z \u2208 Rn\u00d7(d+1) as shown below in Equation 3.\nZ =  00 01 . . . 0d 10 11 . . . 1d ... ... . . . ... (n\u2212 2)0 (n\u2212 2)1 . . . (n\u2212 2)d\n(n\u2212 1)0 (n\u2212 1)1 . . . (n\u2212 1)d\n (3)\nThe solution of the least square system is conducted by solving the first derivative with respect the polynomial coefficients \u03b2 as presented in Equation 4.\n\u2202L(Y, Y\u0302 )\n\u2202\u03b2 = 0 leads to \u03b2 =\n( ZTZ )\u22121 ZTY (4)\nA typical solution of a a polynomial fitting is provided in Figure 1. On the left plot we see an instantiation of a sliding window fitting. The sliding window of size 120 is shown in the left plot, while the fitting of the segment inside the sliding window segment is scaled up on the right plot. Please note that inside the sliding window the time is set relative to the sliding window frame from 0 to 119. The series of Figure 1 is a segment from the GAITPD dataset.\nSince the relative time inside each sliding window is between 0 and n\u22121, then the predictors Z are the same for all the sliding windows of all time series. Consequently, we can pre-compute the term P = ( ZTZ )\u22121 ZT in the beginning of the program and use the projection matrix P to compute the\npolynomial coefficients \u03b2 of the local segment Y as \u03b2 = PY . Algorithm 1 describes the steps needed to compute all the polynomial coefficients of the sliding windows (starting at t) of every time series (indexed by i) in the dataset. For every time series we collect all the polynomial coefficients in a bag, denoted as \u03a6(i). The outcome of the fitting process are the bags of all time series \u03a6. Please note that the complexity of fitting a polynomial to a sliding window is linear and the overall algorithm has a complexity of O(M \u00b7 d \u00b7n \u00b7N), which considering d << N, d << M and n << N , means linear run-time complexity in terms of N and M , that is O(M \u00b7N).\nAlgorithm 1 Polynomial Fitting of a Time-Series Dataset\nRequire: Dataset T \u2208 RM\u00d7N , Sliding window size n, Polynomial degree d Ensure: \u03a6 \u2208 RM\u00d7(N\u2212n)\u00d7(d+1) 1: P \u2190 ( ZTZ )\u22121 ZT\n2: for i \u2208 {1 . . .M} do 3: \u03a6(i) \u2190 \u2205 4: for t \u2208 {1 . . . N \u2212 n} do 5: Y \u2190 [S(i)t , S (i) t+1, . . . S (i) t+n\u22121] 6: \u03b2 \u2190 PY 7: \u03a6(i) \u2190 \u03a6(i) \u222a {\u03b2} 8: end for 9: end for\n10: return (\u03a6(i))i=1,...,M"}, {"heading": "D. Converting Coefficients To Symbolic Words", "text": "The next step of our study is to convert the computed polynomial coefficients \u03a6 from Algorithm 1 into words. The principle of conversion is to transform each of the d + 1 coefficient of every \u03b2 of \u03a6 to one symbol. Therefore, the extracted words have lengths of d + 1 symbols. For each of the \u03b2 values of the polynomial coefficients we construct the histogram distribution and divide it into regions of equal volume as shown in Figure 2. In the image we have divided the histogram into as many regions as the alphabet size (\u03b1 = 4) we would like to utilize. Such a process is called an equivolume discretization. The thresholds between the regions are named quantile points and are defined in the figure as yellow lines. Dividing the histogram into \u03b1 many regions is equivalent to sorting the coefficient values and choosing the threshold values\ncorresponding to indexes multiple of 1\u03b1 . For instance, dividing the histogram into 4 regions for an alphabet of size 4 requires thresholds values corresponding to indexes at 14 , 2 4 , 3 4 of the total number of values, which means that the each region has 25% of the values. Formally, let us define a sorted list of the j-th coefficient values regarding all window segments as Bj \u2190 sort ({ \u03b2j | \u03b2 \u2208 \u03a6(i), i = 1, . . . ,M }) and let the size of this sorted list be sj \u2190 |Bj |. Then the (\u03b1\u22121) many threshold values are defined as \u00b5jk \u2190 B j\nbsj k\u03b1c ,\u2200k \u2208 {1, . . . \u03b1 \u2212 1} and\n\u00b5j\u03b1 \u2190\u221e.\nAlgorithm 2 Convert Polynomial Coefficients to Words Require: Polynomial Coefficients \u03a6, Alphabet Size \u03b1 Ensure: W \u2208 RM\u00d7(N\u2212n)\u00d7(d+1)\n1: {Compute the thresholds} 2: for j \u2208 {0 . . . d} do 3: Bj \u2190 sort ({ \u03b2j | \u03b2 \u2208 \u03a6(i), i = 1, . . . ,M }) 4: sj \u2190 |Bj | 5: \u00b5j\u03b1 \u2190\u221e 6: for k \u2208 {1 . . . \u03b1\u2212 1} do 7: \u00b5jk \u2190 B j\nbsj k\u03b1c 8: end for 9: end for\n10: {Convert the coefficients to words} 11: \u03a3\u2190 {A,B, . . . , Y, Z} 12: for i \u2208 {1 . . .M} do 13: W (i) \u2190 \u2205 14: for \u03b2 \u2208 \u03a6(i) do 15: w \u2190 \u2205 16: for j \u2208 {0 . . . d} do 17: k \u2190 argmaxk\u2208{1,...,\u03b1} \u03b2j < \u00b5 j k 18: w \u2190 w \u25e6 \u03a3k 19: end for 20: W (i) \u2190W (i) \u222a {w} 21: end for 22: end for 23: return (W (i))i=1,...,M\nAlgorithm 2 describes the conversion of polynomial coef-\nficients to symbolic form, i.e. words. The first phase computes the threshold values \u00b5jk to discretize the distribution of each coefficient in an equivolume fashion. The second phase processes all the coefficients \u03b2 of time series sliding windows and converts each individual coefficient to a character c, depending on the position of the \u03b2 values with respect to the threshold values. The concatenation operator is denoted by the symbol \u25e6. The characters are concatenated into words w and stored in bags of words W . The complexity of this algorithm is also linear in terms of N and M . In case a linear search is used for finding the symbol index k, then the complexity is O(M \u00b7N \u00b7\u03b1), while a binary search reduces the complexity to O(M \u00b7N \u00b7log(\u03b1)). Yet, please note that in practice \u03b1 << N and \u03b1 << M , therefore the complexity is translated to O(M \u00b7N)."}, {"heading": "E. Populating the Histogram", "text": "Once we have converted our polynomial coefficients and converted them to words, the next step is to convert the words into a histogram of word frequencies, as depicted in Figure 3. The steps of the histogram population are clarified by Algorithm 3. The first step is to build a dictionary D, which is a set of each word that appears in any time series at least\nonce. Then we create a histogram H with as many rows as time-series and as many columns as there are words in the dictionary. The initial values of the histogram cells are 0. Each cell indicate a positive integer which semantically represent how many times does a word (column index) appear in a time series (row index). The algorithm iterates over all the words of a series and increases the frequency of occurrence of that word in the histogram.\nOnce the histogram is populated, then each row of the histogram denotes a vector containing the frequencies of the dictionary words for the respective time series. Practically the row represent what local patterns (i.e. words) exist in a series and how often they appear. For instance Figure 4 presents instances from the GAITPD dataset belonging to two types of patients in a binary classification task, healthy patients (blue) and Parkinson\u2019s Disease patients (red). In the left plot we show the original time series while on the right plot the histogram rows containing the polynomial words versus their frequencies. The parameters leading to the histogram for the GAITPD dataset are n = 100, \u03b1 = 4, d = 7. As can be inspected the original time-series offer little direct opportunity to distinguish one class from the other and the series look alike. Moreover the Euclidean distance of adjacent series in the figure show that the Euclidean classifier would mistakenly classify the third\nAlgorithm 3 Populate the Histogram Require: Word bags W Ensure: Histogram H\n1: {Build the dictionary} 2: Ordered set dictionary D \u2190 \u2205 3: for w \u2208W (i),\u2200i \u2208 {1 . . .M} do 4: if w /\u2208 D then 5: D \u2190 D \u222a {w} 6: end if 7: end for 8: {Build the histogram} 9: H \u2190 {0}i=1,...,M j=1,...,|D| 10: for w \u2208W (i),\u2200i \u2208 {1 . . .M} do 11: Find j with Dj = w 12: Hi,j \u2190 Hi,j + 1 13: end for 14: return H\ninstance of the blue class. In contrast the histograms are much more informative and it is possible to observe frequencies of local patterns which allow the discrimination of one class from the other. A complete distance matrix between blue B and red R instances is shown in Table I. As can be seen our histogram\nrepresentations result in perfect accuracy in terms of nearest neighbor classification (bold), while the original series result in 2 errors."}, {"heading": "F. Comparison To Other Methods", "text": "The closest method comparable in nature to ours is the approach which builds histograms from SAX words [3], [4]. However the SAX words are build from locally constant approximations which in general are less expressive than the polynomials of our approach. Figure 5 demonstrate the deficiencies of the locally constant approximation in detecting the curvatures of a sliding window sub-series. In the experiment of Figure 5, we used an alphabet of size four and utilized the classical quantile threshold for SAX, being values {\u22120.67, 0, 0.67}. Please note that the series are shown by black dots and represent normalized window segments. We have fitted both a constant model and our polynomial model to the series data. Assume we want to have a four character SAX word for each of the sliding windows segment. As can be easily seen the SAX words for both segments are \u2019ABCD\u2019. On the other hand, referring to the coefficient values of Figure 2, we can see that the symbolic polynomial word belonging to polynomial y(x) = 0.25x3 is \u2019CCBC\u2019, while the polynomial word belonging to y(x) = \u22120.05x3 + 1.1x is \u2019BCDC\u2019. As we can see our method can accurately distinguish the difference between those patterns, while the SAX method averages the content and looses information about their curvatures. We would like to point out clearly that our method is more expressive by needing the same complexity, in this case both methods use four characters. In terms of run-time, SAX needs only one pass through the data, so from the algorithmic complexity point of view both methods have same algorithmic complexity of O(M \u00b7N), i.e. number of series by their length. The complexity of our method has an multiplicative constant, which is the degree of the polynomial as shown in Algorithm 2, however d << M, d << N . Finally, as will be shown in Section IV-D, the classification results of our method are much better than SAX histograms."}, {"heading": "G. Classifier", "text": "The classifier that we are going to use is the nearest neighbor method, which is a strong classifier in time-series classification [1] and is used by state-of-art methods [4]. After converting the original time series into pattern frequency representations, in the form of rows of a histogram matrix, then each row will be treated as a vector instance. The nearest neighbor will utilize the Euclidean distance to compute the difference between histogram rows."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": ""}, {"heading": "A. Descriptions of Datasets", "text": "All the experiments are based on datasets retrieved from Physionet, a repository of complex physiological signals primarily from the health care domain [23]. Our first dataset, ECG2, represents time series from the domain of Electrocardiography belonging to five different sources [4]. The other datasets shown below were not used before in the realm of time-series classification, so we processed and adopted them for a classification task. Being the first paper to introduce them for time-series classification, we are dedicating some lines to explain the preparation of the datasets.\n\u2022 GAITPD: The dataset contains measures of walking patterns (aka gait) from 93 patients with idiopathic Parkinson\u2019s Disease and 73 healthy (control) patients [24]. Measurements of the force underneath each foot was recorded for each patient via 8 sensors. We selected the total force of either feet as two independent series. The recorded gait time series were divided into equilength segments of 4000 measurement points. Finally we created a binary classification dataset by separating healthy and Parkinson\u2019s patients.\n\u2022 RATBP: Hypertension caused by salt consumption was tested over two types of rats, the Dahl salt sensitive (SS) rats and the consomic SS.13BN rats\n[25]. Both high-salt and low-salt diets were provided to a total of 15 rats throughout a certain period of time. The amount of salt in the diet was alternated for each rat and the blood pressure series of each rat were measured. We segmented the blood pressure time series into equilength chunks of 2000 point measurements and utilized a binary categorization scheme by separating low-salt series from high-salt series.\n\u2022 NESFDB: Postural sway refers to the oscillations of the body while standing. A group of 27 individuals of young and old age groups were voluntarily tested for postural sway behaviors [26]. Vibrating elements were implanted in the shoes of each volunteer beneath the fore foot and heel. Simulations on the patients were conducted by emitting low-pass filtered vibrational noise. A reflective marker was placed on the shoulders of the individuals and the sway was converted from a video recording into a time series describing the displacement of the marker. We utilized the series derived from both front and lateral video recordings as two instances. The time series were divided into two groups of sways, sway measurements with vibrational stimulus and measurements without stimulus.\nThe statistics of each dataset in terms of the number of instances, the length of each time series and the number of classes is summarized in Table II. Please note that all the instances within one dataset have the same length."}, {"heading": "B. Baselines", "text": "Let us name our method as SymPol, meaning Symbolic Polynomials and refer to our method with the abbreviation form in the remaining sections. In order to evaluate the performance of SymPol, we compare against the following three baselines.\n1) BSAX: refers to the method of constructing bags of SAX words from time series through a sliding window approach. The words occurring in the bags are used to populate a histogram of frequencies [4]. A nearest neighbor method is applied to classify the histogram instances by treating the histogram rows as the new time-series representation. Comparing against this classifier will give chance to understand the benefit of polynomial approximation compared to constant models and will provide evidences on the state-of-art quality of the results.\n2) ENN: is the classical nearest neighbor classifier with the Euclidean L2 loss as the distance metric. It operates over the whole time series, without segmenting the series for local patterns. The comparison against the plain nearest neighbor will show whether the detection of local patterns has more advantage than comparing the whole long series.\n3) DTWNN: differs from the Euclidean nearest neighbor classifier in defining a new distance metric for the comparison of two time series and performs well in time-series classification [1]. Dynamic Time Warping (DTW) operates by creating a matrix with all the possible warping paths, (i.e. alignment of pairs of indexes from two series), and selects the warping alignment with the smallest overall possible distance. DTW compares a full series without segmentations similarly to the Euclidean version of the nearest neighbor. Such comparison\nwill both identify the benefits of the segmentation and also the benefits of local polynomials against global warping alignments."}, {"heading": "C. Reproducibility", "text": "Two different type of experiments were conducted in our study. The first empirical evidence focuses on the accuracy of our method with respect to classification of time series. The second experiment will analyze the computational run time of the methods. All the experiments were computed in a five folds cross-validation experimental setup. The timeseries instances of each dataset were divided into 5 sets. In a circular fashion (repeated five times) each different set was once selected as the testing set, while the remaining four were used for training. Among the four sets used for training, one of them was selected as a validation set and the remaining three left as training. As a summary, all the combination of parameters were evaluated on the validation set and learned on the three training set, while the parameter values giving the smallest errors on the validation were selected. Those parameter values were finally evaluated over the testing set (learning from the three training sets) to report the final error rate.\nA grid search mechanism was selected for searching the hyperparameter values. Our method SymPol requires the tuning of three parameters, the size of the sliding window n, the size of the alphabet \u03b1 and the degree of the polynomials d. The size of the sliding window was selected among the range of n \u2208 {100, 200, 300, 400}, while the size of the alphabet was picked from \u03b1 \u2208 {4, 6, 8}. Lastly the degree of the polynomial was picked to be one of d \u2208 {1, 2, 3, 4, 5, 6, 7, 8}.\nSimilarly, the baseline named BSAX also requires the fitting of three hyperparameters. The length of a SAX word, denoted |w|, was selected from the range of {2, 3, 4, 5, 6, 7, 8, 9}, while the size of the alphabet was selected among the values {4, 6, 8}. The size of the sliding window is selected from a range of {100, 200, 300, 400}, however those values were rounded to fit the length of a sax word. For instance if the length of a Sax word is 3, then the size of the sliding window was rounded from 100 to 102 in order for the sliding window to be equally divisible into three chunks. The hyperparameter values found in our experiments are shown in Table IV, with ranges of multiple values due to different parameter searches per each different validation set."}, {"heading": "D. Results", "text": "The classification accuracy results of our experiments are presented in Table III. For our method SymPol and all the baselines we show the mean and the standard deviation of the five fold cross-validation experiments as described in the setup section. The smallest error rate is highlighted in bold.\nTABLE III: Error Rate Results\nDataset SymPol BSAX ENN DTWNN \u00b5 (mean) \u03c3 (st.dev.) \u00b5 (mean) \u03c3 (st.dev.) \u00b5 (mean) \u03c3 (st.dev.) \u00b5 (mean) \u03c3 (st.dev.)\nECG2 0.0000 0.0000 0.0080 0.0098 0.5480 0.0240 0.2120 0.0160 GAITPD 0.0238 0.0083 0.0548 0.0120 0.3924 0.0211 0.2468 0.0206 RATBP 0.1333 0.0272 0.1889 0.0111 0.4389 0.0272 0.3333 0.0994 NESFDB 0.4310 0.0212 0.4405 0.0395 0.4929 0.0208 0.5440 0.0356\nTABLE V: Run Time Results (seconds)\nAs can be clearly seen our method demonstrates an unrivaled superiority. SymPol wins in all the four datasets and has a clear statistical significance in three of them ECG2, GAITPD, RATBP. Our method performs perfectly in the ECG2 dataset by having 100% classification accuracy. In addition, SymPol reduces the error on the GAITPD dataset by 57% with respect the closest baseline, while on the RATBP dataset the error is reduced by 29%.\nThe second type of results represent the running times of the algorithms and is shown in Table V. As can be clearly seen the Euclidean distance on the original dataset is the fastest method, which is a natural behavior because no processing is done over series to extract histograms. The BSAX method is the next in terms of speed due to the computational advantage of the constant model which requires only one pass over the data. SymPol is positively positioned in terms of run time. As already analyzed before, the algorithmic complexity is comparable to the BSAX except for an additional constant, which is the polynomial degree. In datasets like ECG2, GAITPD and NESFDB the execution times are bigger by a small constant factor of two. The runtime constant in the RATBP dataset is higher because the hyperparameter search resulted to require a degree of 7. As a summary, we can clearly see that the method is practically very feasible and fast in terms of run time and is close even to techniques that require only a single scan over\nthe time-series values."}, {"heading": "E. Hyper-parameter Search Sensitivity", "text": "As presented in Section IV-C our hyperparameter search technique is the grid search, where we scan for all the possible combination of one parameter\u2019s values to all the possible values of other parameters. As Figure 6 shows, the error rate is nonlinear with respect to the parameter values of the method. Therefore, a grid search mechanism is practically suitable, because gradient based methods would have resulted in local optima while nonlinear optimization techniques would require much more computations than the grid. As can be seen in the plot, the grid search could successfully detect the global optimum in the region denoted by a mark."}, {"heading": "V. CONCLUSION", "text": "In this study we presented a novel method to classify long time series, which are composed of local patterns. Local polynomial approximations are computed in a sliding window approach for each normalized segment under the sliding window. The computed polynomial coefficients are converted to symbolic forms (i.e. literal words) via an equivolume discretization procedure. Thresholds for the distribution of the values of each coefficient are determined to split the\ncoefficient\u2019s histogram into equal regions and each region is assigned an alphabet symbol. In a second step all the polynomial coefficients are transformed into characters by locating them within the threshold values of the histogram and assigning the region symbol. The final literal representation of a polynomial is a word composed of the concatenation of each coefficient\u2019s character, in the order of the coefficient\u2019s monomial degrees. Once the bags of words are computed then a histogram is populated with the frequencies of each word in a time series. We presented a linear time technique to compute the polynomial approximation of a sliding window segment, while the overall method has a run time complexity which is linear in terms of the series points.\nThe classification accuracy of the nearest neighbor method utilizing the histogram rows that our method computed was compared against the performance of three baselines. Our method won all the experiments, most of them with a statistically significant margin. Furthermore, empirical results demonstrate that our method has a practically feasible running time performance, comparable even to the fastest methods which require a single scan over the time series."}], "references": [{"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proc. VLDB Endow., vol. 1, no. 2, pp. 1542\u20131552, Aug. 2008. [Online]. Available: http://dl.acm.org/citation.cfm?id=1454159.1454226", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Motif-based classification of time series with bayesian networks and svms", "author": ["K. Buza", "L. Schmidt-Thieme"], "venue": "GfKl, ser. Studies in Classification, Data Analysis, and Knowledge Organization, A. Fink, B. Lausen, W. Seidel, and A. Ultsch, Eds. Springer, 2008, pp. 105\u2013 114.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding structural similarity in time series data using bag-of-patterns representation", "author": ["J. Lin", "Y. Li"], "venue": "Proceedings of the 21st International Conference on Scientific and Statistical Database Management, ser. SSDBM 2009. Berlin, Heidelberg: Springer-Verlag, 2009, pp. 461\u2013477. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-02279-1 33", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Rotation-invariant similarity in time series using bag-of-patterns representation", "author": ["J. Lin", "R. Khade", "Y. Li"], "venue": "J. Intell. Inf. Syst., vol. 39, no. 2, pp. 287\u2013315, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast subsequence matching in time-series databases", "author": ["C. Faloutsos", "M. Ranganathan", "Y. Manolopoulos"], "venue": "Proceedings of the 1994 ACM SIGMOD international conference on Management of data, ser. SIGMOD \u201994. New York, NY, USA: ACM, 1994, pp. 419\u2013429. [Online]. Available: http://doi.acm.org/10.1145/191839.191925", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Efficient time series matching by wavelets", "author": ["K.-P. Chan", "A.-C. Fu"], "venue": "15th International Conference on Data Engineering, Proceedings 1999, 1999, pp. 126\u2013133.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Singular-value decomposition approach to time series modelling", "author": ["J.A. Cadzow", "B. Baseghi", "T. Hsu"], "venue": "Communications, Radar and Signal Processing, IEE Proceedings F, vol. 130, no. 3, pp. 202\u2013210, 1983.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1983}, {"title": "Classification of sparse time series via supervised matrix factorization", "author": ["J. Grabocka", "A. Nanopoulos", "L. Schmidt-Thieme"], "venue": "AAAI, J. Hoffmann and B. Selman, Eds. AAAI Press, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Dimensionality reduction for fast similarity search in large time series databases", "author": ["E.J. Keogh", "K. Chakrabarti", "M.J. Pazzani", "S. Mehrotra"], "venue": "Knowl. Inf. Syst., vol. 3, no. 3, pp. 263\u2013286, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Experiencing sax: a novel symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "L. Wei", "S. Lonardi"], "venue": "Data Min. Knowl. Discov., vol. 15, no. 2, pp. 107\u2013144, Oct. 2007. [Online]. Available: http://dx.doi.org/10.1007/s10618-007-0064-z", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Saxually explicit images: Finding unusual shapes", "author": ["L. Wei", "E. Keogh", "X. Xi"], "venue": "Proceedings of the Sixth International Conference on Data Mining, ser. ICDM \u201906. Washington, DC, USA: IEEE  Computer Society, 2006, pp. 711\u2013720. [Online]. Available: http: //dx.doi.org/10.1109/ICDM.2006.138", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "isax: indexing and mining terabyte sized time series", "author": ["J. Shieh", "E. Keogh"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD \u201908. New York, NY, USA: ACM, 2008, pp. 623\u2013631. [Online]. Available: http://doi.acm.org/10.1145/1401890.1401966", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "isax 2.0: Indexing and mining one billion time series", "author": ["A. Camerra", "T. Palpanas", "J. Shieh", "E. Keogh"], "venue": "Proceedings of the 2010 IEEE International Conference on Data Mining, ser. ICDM \u201910. Washington, DC, USA: IEEE Computer Society, 2010, pp. 58\u201367. [Online]. Available: http://dx.doi.org/10.1109/ICDM.2010.124", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Online segmentation of time series based on polynomial least-squares approximations", "author": ["E. Fuchs", "T. Gruber", "J. Nitschke", "B. Sick"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 12, pp. 2232\u20132245, Dec. 2010. [Online]. Available: http://dx.doi.org/10.1109/TPAMI.2010.44", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["T. Rakthanmanon", "B. Campana", "A. Mueen", "G. Batista", "B. Westover", "Q. Zhu", "J. Zakaria", "E. Keogh"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, ser. KDD 2012. New York, NY, USA: ACM, 2012, pp. 262\u2013270. [Online]. Available: http://doi.acm.org/10.1145/2339530.2339576", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "On the marriage of lp-norms and edit distance", "author": ["L. Chen", "R. Ng"], "venue": "Proceedings of the Thirtieth international conference on Very large data bases - Volume 30, ser. VLDB \u201904. VLDB Endowment, 2004, pp. 792\u2013803. [Online]. Available: http://dl.acm.org/citation.cfm? id=1316689.1316758", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust and fast similarity search for moving object trajectories", "author": ["L. Chen", "M.T. \u00d6zsu", "V. Oria"], "venue": "Proceedings of the 2005 ACM SIGMOD international conference on Management of data, ser. SIGMOD \u201905. New York, NY, USA: ACM, 2005, pp. 491\u2013502. [Online]. Available: http://doi.acm.org/10.1145/1066157.1066213", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Discovering similar multidimensional trajectories", "author": ["M. Vlachos", "G. Kollios", "D. Gunopulos"], "venue": "Data Engineering, 2002. Proceedings. 18th International Conference on, 2002, pp. 673\u2013684.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Spade: On shapebased pattern detection in streaming time series", "author": ["Y. Chen", "M. Nascimento", "B.-C. Ooi", "A. Tung"], "venue": "Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, 2007, pp. 786\u2013795.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Support vector machines and dynamic time warping for time series", "author": ["S. Gudmundsson", "T.P. Runarsson", "S. Sigurdsson"], "venue": "IJCNN. IEEE, 2008, pp. 2772\u20132776.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Time series classification using support vector machine with gaussian elastic metric kernel", "author": ["D. Zhang", "W. Zuo", "D. Zhang", "H. Zhang"], "venue": "Pattern Recognition (ICPR), 2010 20th International Conference on, 2010, pp. 29\u201332.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Invariant timeseries classification", "author": ["J. Grabocka", "A. Nanopoulos", "L. Schmidt-Thieme"], "venue": "ECML/PKDD (2), ser. Lecture Notes in Computer Science, P. A. Flach, T. D. Bie, and N. Cristianini, Eds., vol. 7524. Springer, 2012, pp. 725\u2013740.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals", "author": ["A.L. Goldberger", "L.A.N. Amaral", "L. Glass", "J.M. Hausdorff", "P.C. Ivanov", "R.G. Mark", "J.E. Mietus", "G.B. Moody", "C.-K. Peng", "H.E. Stanley"], "venue": "Circulation, vol. 101, no. 23, pp. e215\u2013e220, 2000 (June 13), circulation Electronic Pages: http://circ.ahajournals.org/cgi/content/full/101/23/e215 PMID:1085218; doi: 10.1161/01.CIR.101.23.e215.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Rhythmic auditory stimulation modulates gait variability in parkinson\u2019s disease.", "author": ["J. Hausdorff", "J. Lowenthal", "T. Herman", "L. Gruendlinger", "C. Peretz", "N. Giladi"], "venue": "Eur J Neurosci,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Identifying physiological origins of baroreflex dysfunction in salt-sensitive hypertension in the dahl ss rat.", "author": ["S.M. Bugenhagen", "A.W. Cowley", "D.A. Beard"], "venue": "Physiol Genomics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Vibrating insoles and balance control in elderly", "author": ["A. Priplata", "J. Niemi", "J. Harry", "L. Lipsitz", "J. Collins"], "venue": "people.\u201d Lancet,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Among other successful techniques in this category, the nearest neighbor classifier equipped with a similarity metric called Dynamic Time Warping (DTW) has been shown to perform well in a large number of datasets [1].", "startOffset": 213, "endOffset": 216}, {"referenceID": 1, "context": "Nevertheless, few studies [2]\u2013[4] have been dedicated towards the classification of time-series data which is long and composed of many local patterns.", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "Nevertheless, few studies [2]\u2013[4] have been dedicated towards the classification of time-series data which is long and composed of many local patterns.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "Text mining approaches which compute bags of words occurring in a text and then utilize histograms of the occurrences of each word, have been recently applied to the time-series domain by the metaphor of computing \u201dbags of patterns\u201d [2], [4].", "startOffset": 233, "endOffset": 236}, {"referenceID": 3, "context": "Text mining approaches which compute bags of words occurring in a text and then utilize histograms of the occurrences of each word, have been recently applied to the time-series domain by the metaphor of computing \u201dbags of patterns\u201d [2], [4].", "startOffset": 238, "endOffset": 241}, {"referenceID": 2, "context": "The novelty of our method, compared to state-of-art approaches [3], [4] which utilize constant functions to express local patterns, relies on offering an expressive technique to represent patterns as polynomial of arbitrary degrees.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "The novelty of our method, compared to state-of-art approaches [3], [4] which utilize constant functions to express local patterns, relies on offering an expressive technique to represent patterns as polynomial of arbitrary degrees.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "We experimented with the datasets from [4], and in order to widen the variety of data we introduce three new datasets.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "Discrete Fourier transforms have attempted to represent repeating series structures as a sum of sinusoidal signals [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "Similarly, wavelet transformations approximate a timeseries via orthonormal representations in the form of wavelets [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "Singular Value Decomposition is a dimensionality reduction technique which has also been applied to extract latent dimensionality information of a series [7], while supervised decomposition techniques have aimed at incorporating class information into the low-rank data learning [8].", "startOffset": 154, "endOffset": 157}, {"referenceID": 7, "context": "Singular Value Decomposition is a dimensionality reduction technique which has also been applied to extract latent dimensionality information of a series [7], while supervised decomposition techniques have aimed at incorporating class information into the low-rank data learning [8].", "startOffset": 279, "endOffset": 282}, {"referenceID": 8, "context": "Those chunks are converted to a single mean value and the concatenation of means create a short form known as a Piecewise Constant Approximation [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "A more sophisticated technique operates by converting the mean values into symbolic form into a method called Symbolic Aggregate Approximation, denoted shortly as SAX [10], [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "A more sophisticated technique operates by converting the mean values into symbolic form into a method called Symbolic Aggregate Approximation, denoted shortly as SAX [10], [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "Further sophistication of lower bounding techniques have advanced the representation method towards efficient indexing and searching [12], enabling large scale mining of time series [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Further sophistication of lower bounding techniques have advanced the representation method towards efficient indexing and searching [12], enabling large scale mining of time series [13].", "startOffset": 182, "endOffset": 186}, {"referenceID": 13, "context": "For instance least squares approximation of time series via orthogonal polynomials have been proposed for segmentation purposes in a hybrid sliding/growing window scenario [14].", "startOffset": 172, "endOffset": 176}, {"referenceID": 0, "context": "DTW achieves highly competitive classification accuracies and is regarded as a strong baseline [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "Even though DTW is slow in the original formulation having a quadratic run-time complexity, still recent techniques involving early pruning and lower bounding have utilized DTW for fast large scale search [15].", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "Other techniques have put emphasis on the need to apply edit distance penalties for assessing the similarity between time series [16], [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Other techniques have put emphasis on the need to apply edit distance penalties for assessing the similarity between time series [16], [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "Other approaches have put emphasis on detecting the longest common subsequence of series, believing in the assumption that time series have a fingerprint segment which is the most determinant with respect to classification [18].", "startOffset": 223, "endOffset": 227}, {"referenceID": 18, "context": "Detection of similarities in a streaming time-series scenarios motivated attempts to handle scaling and shifting in the temporal and amplitude aspects [19].", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "Among the initial pioneer methods and still one of the best performing ones is the nearest neighbor classifier accompanied by the DTW distance metrics, which constitute a hard-to-beat baseline [1].", "startOffset": 193, "endOffset": 196}, {"referenceID": 19, "context": "Other powerful nonlinear classifiers like the Support Vector Machines have been tweaked to operate over time series, partially because originally the kernel functions are not designed for invariant pattern detection and partially because DTW is not a positive semi-definite kernel [20].", "startOffset": 281, "endOffset": 285}, {"referenceID": 20, "context": "Therefore the creation of positive semi-definite kernels like the Gaussian elastic metric kernel arose [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "Another approach proposed to handle variations by inflating the training set and creating new distorted instances from the original ones [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 1, "context": "For instance, underlying series patterns have been expressed as the motifs and the difference between the motif frequencies has been utilized [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Other approaches have explored the conversion of each sliding window segment into a literal word constructed by piecewise constant approximations and the SAX method [3], [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "Other approaches have explored the conversion of each sliding window segment into a literal word constructed by piecewise constant approximations and the SAX method [3], [4].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "Such a technique has been shown to be rotation-invariant, because the occurrence of a pattern is not related to its position [4].", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "The closest method comparable in nature to ours is the approach which builds histograms from SAX words [3], [4].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "The closest method comparable in nature to ours is the approach which builds histograms from SAX words [3], [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "The classifier that we are going to use is the nearest neighbor method, which is a strong classifier in time-series classification [1] and is used by state-of-art methods [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "The classifier that we are going to use is the nearest neighbor method, which is a strong classifier in time-series classification [1] and is used by state-of-art methods [4].", "startOffset": 171, "endOffset": 174}, {"referenceID": 22, "context": "All the experiments are based on datasets retrieved from Physionet, a repository of complex physiological signals primarily from the health care domain [23].", "startOffset": 152, "endOffset": 156}, {"referenceID": 3, "context": "Our first dataset, ECG2, represents time series from the domain of Electrocardiography belonging to five different sources [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 23, "context": "\u2022 GAITPD: The dataset contains measures of walking patterns (aka gait) from 93 patients with idiopathic Parkinson\u2019s Disease and 73 healthy (control) patients [24].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "A group of 27 individuals of young and old age groups were voluntarily tested for postural sway behaviors [26].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "The words occurring in the bags are used to populate a histogram of frequencies [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "3) DTWNN: differs from the Euclidean nearest neighbor classifier in defining a new distance metric for the comparison of two time series and performs well in time-series classification [1].", "startOffset": 185, "endOffset": 188}], "year": 2017, "abstractText": "Time-series classification has attracted considerable research attention due to the various domains where timeseries data are observed, ranging from medicine to econometrics. Traditionally, the focus of time-series classification has been on short time-series data composed of a unique pattern with intraclass pattern distortions and variations, while recently there have been attempts to focus on longer series composed of various local patterns. This study presents a novel method which can detect local patterns in long time-series via fitting local polynomial functions of arbitrary degrees. The coefficients of the polynomial functions are converted to symbolic words via equivolume discretizations of the coefficients\u2019 distributions. The symbolic polynomial words enable the detection of similar local patterns by assigning the same words to similar polynomials. Moreover, a histogram of the frequencies of the words is constructed from each time-series\u2019 bag of words. Each row of the histogram enables a new representation for the series and symbolize the existence of local patterns and their frequencies. Experimental evidence demonstrates outstanding results of our method compared to the state-of-art baselines, by exhibiting the best classification accuracies in all the datasets and having statistically significant improvements in the absolute majority of experiments.", "creator": "LaTeX with hyperref package"}}}