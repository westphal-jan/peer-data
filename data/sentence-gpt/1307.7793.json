{"id": "1307.7793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jul-2013", "title": "Multi-dimensional Parametric Mincuts for Constrained MAP Inference", "abstract": "In this paper, we propose novel algorithms for inferring the Maximum a Posteriori (MAP) solution of discrete pairwise random field models under multiple constraints. We show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly. These multiple solutions enable us to even deal with `soft constraints' (higher order penalty functions). Moreover, we propose two practical variants of our algorithm to solve problems with hard constraints. We also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation. Experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods.", "histories": [["v1", "Tue, 30 Jul 2013 03:02:44 GMT  (1255kb,D)", "http://arxiv.org/abs/1307.7793v1", "19 pages"]], "COMMENTS": "19 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["yongsub lim", "kyomin jung", "pushmeet kohli"], "accepted": false, "id": "1307.7793"}, "pdf": {"name": "1307.7793.pdf", "metadata": {"source": "CRF", "title": "Multi-dimensional Parametric Mincuts for Constrained MAP Inference", "authors": ["Yongsub Lim", "Kyomin Jung", "Pushmeet Kohli"], "emails": ["yongsub@kaist.ac.kr", "kyomin@kaist.edu", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Markov Random Fields (MRF) is an undirected graphical model, which has been extensively studied and used in various fields, including statistical physics [11], and computer vision [19]. It represents interdependency of discrete random variables as a graph over which a probabilistic space is defined. Computing the solution which has the maximum probability under the random field, or Maximum a Posteriori (MAP) inference is NP-hard in general. However, a number of subclasses of MRFs have been isolated for which the problem can be solved in polynomial time [2]. Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem. Such algorithms are widely used for various problems in machine learning and computer vision [38, 17]. Since MAP inference in an MRF is equivalent to minimizing the corresponding energy function1, in what follows, we will explain these problems in terms of energy minimization.\nIn many real world problems, the values of certain statistics of the desired solution may be available as prior knowledge. For instance, in the case of foreground-background image segmentation, we may know the approximate shape and/or size of the object being segmented, and thus might want to find the most probable segmentation that has a particular area (number of foreground pixels) and boundary length (number of discontinuities). Another example is community detection in a network [8] where we may know the number of nodes belonging to each community. Such scenarios result in constraints in the solution space, and MAP inference becomes a constrained energy minimization problem, which is generally NP-hard even if the unconstrained version is polynomial time solvable.\n\u2217yongsub@kaist.ac.kr \u2020kyomin@kaist.edu \u2021pkohli@microsoft.com 1Energy of a labelling is the negative logarithm of its posterior probability.\nar X\niv :1\n30 7.\n77 93\nv1 [\ncs .L\nG ]\n3 0\nJu l 2\nEnergy minimization under the above-mentioned statistics constraints results in a challenging optimization problem. However, recent work in computer vision has shown that this problem can be handled efficiently using the parametric mincuts [14] which allow simultaneous computation of exact solutions for some constraint instances. Although the parametric mincuts provide a general framework to deal with constrained energy minimization, they can only handle one linear equality constraint.\nFor minimizing energy functions under multiple constraints, a number of continuous relaxation based methods have been proposed in the literature. For instance, linear relaxation approaches were adopted to handle bounding-box and connectivity constraints defined on the labelling [18, 24]. Further, Klodt and Cremers [12] proposed a convex relaxation framework to deal with moment constraints. Continuous relaxation based methods have also been used for constrained discrete optimization, and can handle multiple inequality constraints. All the above-mentioned methods suffer from following basic limitations: they only handle linear constraints, and the solution involves rounding of the solution of the relaxed problem which may introduce large errors."}, {"heading": "1.1 Our contribution", "text": "In this paper we show how the constrained discrete optimization problem associated with constrained MAP inference can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and propose an algorithm that isolates all constraint instances for which the problem can be solved exactly. This leads to densely many minimizers, each of which is, optimal under distinct constraint instance. These minimizers can be used to compute good approximate solutions of problems with soft constraints (enforced with a higher order term in the energy).\nOur algorithm works by exploiting the Lagrangian dual of the minimization problem, and requires an oracle which can compute values of the Lagrangian dual efficiently. A graph-cut algorithm [3] is a popular example of such an oracle. In fact, our algorithm generalizes the (one-dimensional) parametric mincuts [5, 14] to multiple-dimensions. In contrast to the parametric mincuts [5], our algorithm can deal with multiple constraints simultaneously, including some non-linear constraints (as we show in the paper). This extension allows our algorithm to be used as a technique for multi-dimensional sampling e.g. to obtain different segmentation results for image segmentation as done in [4].\nWe propose two variants of our algorithm to efficiently deal with the problem of performing MAP Inference under hard constraints. The first variant computes the maximum of the dual and outputs its corresponding primal solution as an approximation of the constrained minimization. The primal is computed using selective oracle calls, leading to fast computation time. The other variant combines the first variant with our multidimensional parametric mincuts algorithm to deal with problems with soft-constraints, which allows to find a solution closer to a desired one via additional search.\nOur method is quite general and can be applied to any constrained discrete optimization problems whose Lagrangian dual value is efficiently computable. Examples include submodular function minimization with constraints such as the balanced minimum cut problem, and constrained shortest path problems. Further, in contrast to traditional continuous relaxation based methods, our technique can easily handle complicated soft constraints.\nIn Section 5, we demonstrate that our algorithms compute solutions very close to the ground truth compared with these continuous relaxation based methods on the foreground-background image segmentation problem."}, {"heading": "1.2 Related work", "text": "A number of methods have been proposed to obtain better labelling solutions by inferring the MAP solution from a restricted domain of solutions which satisfy some constraints. Among them, solutions to image labelling problems which have a particular distribution of labels [36] or satisfy a topological property like connectivity [32] have been widely studied.\nMore specifically, for the problem of foreground-background image segmentation, most probable segmentations under the label count constraint have been shown to be closer to the ground truth [14, 20]. Another\nexample is the silhouette constraint which has been used for the problem of 3D reconstruction [13, 29]. This constraint ensured that a ray emanating from any silhouette pixel must pass through at least one voxel which belongs to the \u2018object\u2019.\nRecently, dual decomposition has been proposed for constrained MAP inference [7, 37]. Gupta et al. [7] dealt with cardinality-based clique potentials and developed both exact and approximate algorithms. Also Woodford et al. [37] studied a problem involving marginal statistics such as the area constraint especially with convex penalties, and showed that the proposed method improves quality of solutions for various computer vision problems.\nMAP inference under constraints are also applied to combinatorial optimization such as the balanced metric labelling. For this problem, Naor and Schwartz [23] obtained an O( lnn )-approximate algorithm where each label is assigned to at most min [ O(ln k) 1\u2212 , `+ 1 ] (1 + )` variables/nodes."}, {"heading": "2 Setup and preliminaries", "text": ""}, {"heading": "2.1 Energy minimization", "text": "Markov Random Fields (MRF) defined on a graph G = (V,E) is a probability distribution where every vertex u \u2208 V has a corresponding random variable xu taking a value from the finite label set L. The probability distribution is defined as Pr(x) \u221d exp(\u2212f(x)) where x = (xu), and the corresponding energy function f is in the following form:\nf(x) = \u2211 c\u2208CG \u03c6c(xc), (1)\nwhere CG is the set of cliques in G and \u03c6c is a potential defined over the clique c. The MAP problem is to find an assignment x\u2217 \u2208 L|V | which has the maximum probability, and is equivalent to minimizing the corresponding energy function f . In general it is NP-hard to minimize f , but it is known that if f is submodular, it can be minimized in polynomial time. Especially, if f is a pairwise submodular energy function defined on binary variables, which considers only cliques of size up to 2, i.e.\nf(x) = \u2211 u\u2208V \u03c6u(xu) + \u2211 (u,v)\u2208E \u03c6uv(xu, xv), (2)\nit can be efficiently minimized by solving a equivalent st-mincut problem [15]. Such f is widely used in machine learning and computer vision [9, 20]."}, {"heading": "2.2 Energy minimization with constraints", "text": "Energy minimization with constraints is to compute the solution x\u2217 minimizing an energy function among x\u2019s satisfying given constraints. A typical example of constraints is the label count constraint \u2211 i xi = b where xi \u2208 {0, 1}. In this paper, we consider the following energy minimization with multiple constraints.\nmin x\u2208{0,1}n\n{f(x) : hi(x) = bi, 1 \u2264 i \u2264 m} , (3)\nwhere x \u2208 {0, 1}n, m is a constant, and for 1 \u2264 i \u2264 m, hi : {0, 1}n \u2192 R and bi \u2208 R. In (3), each constraint hi(x) = bi encodes distinct prior knowledge on a desired solution. For convenience, we denote (h1(x), . . . , hm(x)) by H(x).\nLet us consider the following Lagrangian dual g : Rm \u2192 R of f , which is widely used for discrete optimization [16, 30].\ng(\u03bb) = min x\u2208{0,1}n L(x, \u03bb), (4)\nwhere L(x, \u03bb) = f(x) + \u03bbT (H(x)\u2212 b). (5)\nNote that g is defined over a continuous space while f is defined over a discrete space. As in the continuous minimization, maximizing g over \u03bb \u2208 Rm provides a lower bound for (3). Now we define the characteristic set, which is the collection of minimizers of (5) over all \u03bb \u2208 Rm.\nDefinition 1 (Characteristic Set). The Characteristic Set is defined by \u03c7g = \u22c3\n\u03bb\u2208Rm argmin x\u2208{0,1}n L(x, \u03bb). (6)\nLemma 1. Let x\u2217 \u2208 \u03c7g and b\u2217 = H(x\u2217). Then f(x\u2217) = minx\u2208{0,1}n {f(x) : H(x) = b\u2217} [6].\nProof. Suppose that x\u0302 satisfies that H(x\u0302) = b\u2217. It implies \u03bbT (H(x\u0302)\u2212 b\u2217) = \u03bbT (H(x\u2217)\u2212 b\u2217) for any \u03bb \u2208 Rm. Since x\u2217 \u2208 \u03c7g, L(x\u2217, \u03bb) \u2264 L(x\u0302, \u03bb) for some \u03bb \u2208 Rm. Thus, from (5), f(x\u2217) \u2264 f(x\u0302).\nIn this paper, we develop a novel algorithm to compute the characteristic set \u03c7g. We will show that if the dual g(\u03bb) is efficiently computable for any fixed \u03bb \u2208 Rm, for example, when L(x, \u03bb) is submodular on x, our algorithm computes \u03c7g by evaluating g(\u03bb) for poly(|\u03c7g|) number of \u03bb \u2208 Rm. One implication of \u03c7g is\ng(\u03bb) = min x\u2208{0,1}n L(x, \u03bb) = min x\u2208\u03c7g L(x, \u03bb), (7)\nmeaning that minx\u2208{0,1}n L(x, \u03bb) indeed depends on a much smaller set \u03c7g. Note that \u03c7g does not depend on the constraint instance b, thus, in the remaining of the paper, we regard b = 0 unless there is explicit specification. In Section 5, we will show that |\u03c7g| is polynomially bounded in n for many constraints corresponding to useful statistics of the solution. Through experiments, we will show that |\u03c7g| is densely many among all possible constraint instances by an example of image segmentation.\nNote that if we can compute minimizers of (3) for densely many constraint instances b, we can obtain a good approximate solution for the following soft-constrained problem with any global penalty function \u03c1.\nmin x\u2208{0,1}n\n{ f(x) + \u03c1(H(x)\u2212 b\u0302) } . (8)\nIn (8), b\u0302 encodes our prior knowledge on a solution, and examples of \u03c1 include \u2016 \u00b7 \u2016`p and sigmoid functions. This soft-constrained optimization has been widely used in terms of lasso regularization and ridge regression, and also in computer vision [17, 31]."}, {"heading": "2.3 Generalization", "text": "Although we describe our method for problems involving pseudo-Boolean2 objective functions, there is a class of multi-label functions to which our method can be applied. For instance, the results of [26] show transformation of any multi-label submodular functions of order up to 3 to a pairwise submodular one, meaning that it can be solved by the graph-cut algorithm. This enables us to handle the following type of constraints, which is analogue of linear constraints in binary cases: for each j \u2208 L,\nhj(x) = \u2211 i\u2208V aij\u03b4xi;j = bj , (9)\nwhere \u03b4xi,j is Kronecker delta function. Our method is also applicable to any constrained combinatorial optimization problems whose g(\u03bb) is efficiently computable. We will discuss it more in detail in Section 5.2.\n2Real-valued functions defined over boolean vectors {0, 1}n."}, {"heading": "3 Computing the Characteristic Set", "text": ""}, {"heading": "3.1 Algorithm description", "text": "In this section, we describe our algorithm that computes the characteristic set \u03c7g. We assume that for a given set S = \u220fm i=1[Ni,Mi] where Ni,Mi \u2208 R for all i, there is an oracle to compute the Lagrangian dual g efficiently for any \u03bb \u2208 S. For simplicity of explanation, we assume S = [\u2212M,M ]m for some M > 0. We denote the oracle call by\nO(\u03bb) = argmin x\u2208{0,1}n L(x, \u03bb). (10)\nEssentially, our algorithm iteratively decides the \u03bb\u2019s in S for which the oracle will be called. Later we prove that the number of oracle calls in our algorithm to compute \u03c7g is polynomial in |\u03c7g|.\nWe first define the following, which has a central role in our algorithm.\nDefinition 2 (Induced dual of g on X). Let g : Rm \u2192 R be the Lagrangian dual of f , and X \u2286 {0, 1}n. The induced dual gX of g is defined by\ngX(\u03bb) = min x\u2208X L(x, \u03bb). (11)\nFrom the definition of \u03c7g, note that g = g{0,1}n = g\u03c7g . For each x \u2208 {0, 1}n, we define a hyperplane Px by\nPx = {(\u03bb, z) \u2208 Rm+1 : \u03bb \u2208 Rm, z = L(x, \u03bb)}. (12)\nFor (\u03bb, z) \u2208 Px, we use the notation so that Px(\u03bb) = z. For convenience, we will denote any v \u2208 Rm+1 by (\u03bbv, zv), where \u03bbv \u2208 Rm is the first m coordinates of v and zv \u2208 R is the (m+ 1)-th coordinate of v. Since {0, 1}n is finite and each x \u2208 {0, 1}n corresponds to a hyperplane in (m + 1)-dimension, g consists of the boundary of the upper polytope of (4). Then \u03c7g corresponds to the collection of m-dimensional facets of this polytope.\nTo compute \u03c7g, we will recursively update a structure called the skeleton of gX defined below. Intuitively, the skeleton of gX is the collection of vertices and edges of the polytope corresponding to gX .\nDefinition 3 (Proper convex combination). Given x, x1, . . . , xk \u2208 R`, x is a proper convex combination of {xi : 1 \u2264 i \u2264 k} if x = \u2211k i=1 \u03b1ixi for some \u03b1 \u2208 (0, 1)k with \u2211k i=1 \u03b1i = 1.\nDefinition 4 (Skeleton of gX over S). For a given induced dual gX : Rm \u2192 R, let \u0393X(S) = {q \u2208 Rm+1 : \u03bbq \u2208 S, zq \u2264 gX(\u03bbq)}, and for u, v \u2208 \u0393X(S), e(u, v) \u2286 \u0393X(S) is the line segment connecting u and v. The skeleton of gX is GgX = (VgX , EgX ) satisfying the followings.\n\u2022 VgX = {v \u2208 \u0393X(S) : if v is a proper convex combination of U \u2286 \u0393X(S), then U = {v}}.\n\u2022 EgX = {e(u, v) : u, v \u2208 VgX , and if y \u2208 e(u, v) is a proper convex combination of W \u2286 \u0393X(S), then W \u2286 e(u, v)} \u222a {e(u, v) : u \u2208 VgX , \u03bbu \u2208 {\u2212M,M}m, v = (\u03bbu,\u2212\u221e)}.\nOur algorithm runs by updating X \u2286 \u03c7g and GgX iteratively. If a new minimizer x \u2208 {0, 1}n is computed by the oracle call, it is inserted to X and the algorithm computes GgX = (VgX , EgX ). Then, the algorithm determines new \u03bb\u2019s for which the oracle will be called from the new vertices added to VgX . We prove in Theorem 1 that at the end of the algorithm, X = \u03c7g.\nInitially, the algorithm begins with X = {x0} where x0 is the output of the oracle call for any arbitrary \u03bb0 \u2208 {\u2212M,M}m. The inittial skeleton G = (V, E) is given by V = {v1, . . . , v2m} \u2282 Rm+1 where {\u03bbvi : 1 \u2264 i \u2264 2m} = {\u2212M,M}m and zvi = Px0(\u03bbvi) for 1 \u2264 i \u2264 2m; and E = EgX . Note that G = GgX , i.e. the skeleton of gX . This initialization is denoted by InitSkeleton() and it returns X and G.\nIn each iteration with the skeleton GgX = (VgX , EgX ), the algorithm chooses any vertex v \u2208 VgX , and checks whether zv = gX(\u03bbv) using the oracle call for \u03bbv. If zv = gX(\u03bbv), we confirm that zv = g(\u03bbv) and v \u2208 Vg. If not, xv /\u2208 X computed from the oracle satisfies Pxv (\u03bbv) = g(\u03bbv) < zv. Then, the algorithm computes a new skeleton GgX\u222a{xv} as explained below.\nAlgorithm 1: DualSearch\nInput: Oracle O Output: X\n1 (X,G)\u2190 InitSkeleton() 2 Give V an arbitrary order 3 foreach v \u2208 V in the order do 4 xv = O(\u03bbv) 5 if Pxv (\u03bbv) < zv then 6 X = X \u222a {xv} 7 Append V+ = {u \u2208 Pxv \u2229 e : e \u2208 E , e 6\u2286 Pxv} to V in arbitrary order 8 Remove V\u2212 = {u \u2208 V : zu > Pxv (\u03bbu)} from V 9 E\u2212 = {e(u1, u2) \u2208 E : u1 \u2208 V\u2212 or u2 \u2208 V\u2212}\n10 E+ = {e(u1, u3) : \u2203 e(u1, u2) \u2208 E\u2212, u3 = e(u1, u2) \u2229 Pxv} 11 E = E \u222a ConvEdge(V+) \u222a E+ \u2212 E\u2212 12 end\n13 end\nLet X \u2032 = X \u222a{xv}. To compute GgX\u2032 , geometrically we cut GgX by Pxv . This can be done by finding the set V\u2212 of skeleton vertices of gX strictly above Pxv , and finding the set V+ of all intersection points between Pxv and EgX . Then, V\u2212 is removed from VgX , and V+ is added to VgX . Lastly, the set of edges of the convex hull of V+, which is denoted by ConvEdge(V+), is added to EgX 3. Then, the updated GgX is GgX\u2032 . Due to the concavity of gX , we can compute all the above sets by the depth or breadth first search starting from v. Algorithm 1 describes the whole procedure.\nExample of execution We explain the running process of DualSearch with a toy example. Let us consider an energy function f(x1, x2) = x1 + x2, and two constraints h1 and h2 defined as follows.\nh1(x1, x2) = x1 \u2212 x2, (13) h2(x1, x2) = 2|x1 \u2212 x2|. (14)\nHere, we set M = 2. Initially, the algorithm computes a minimizer x(0) = (1, 0) for \u03bb(0) = (\u22122,\u22122). Then the initial V becomes {(\u22122,\u22122,\u22125), (\u22122, 2, 3), (2,\u22122,\u22121), (2, 2, 7)}, which is shown in Figure 1(a). At this point, X = {x(0)}. Let (\u22122, 2, 3) \u2208 V be chosen in the next iteration, and for that vertex, the new minimizer x(1) \u2208 \u03c7g is found. This updates both X = {x(0), x(1)} and the skeleton as shown in Figure 1(b). In the following iterations, (2, 2, 7), (2,\u22122,\u22121) and (\u22122, 0.5, 0) are chosen, but for those vertices, there is no new minimizer; that is, for those vertices, a minimizer is either x(1) = (1, 0) or x(2) = (0, 0). The skeleton at this point is shown in Figure 1(c). Next, (2,\u22121.5, 0) is chosen, and the new minimizer x(2) = (0, 1) is computed so that X is updated by {x(0), x(1), x(2)}. This changes the skeleton as in Figure 1(d)."}, {"heading": "3.2 Correctness of the algorithm", "text": "In what follows, we analyze the correctness and query complexity of DualSearch. All proofs are provided in Section A.\nLemma 2. At the end of each iteration of DualSearch, G = GgX .\nLemma 2 states that when DualSearch terminates, G is the skeleton of an induced dual gX where X is the output of the algorithm. It remains to show that the computed X is indeed the characteristic set \u03c7g.\n3 For a given V+, ConvEdge(V+) can be computed, for example, by [1]. In general, for given (m + 1)-dimensional points, a convex hull algorithm outputs a set of m dimensional facets of the convex hull. Then, we can obtain the edges of the convex hull by recursively applying the algorithm to every computed facets.\nTheorem 1. When DualSearch terminates with X, X = \u03c7g.\nFrom Lemma 2 and Theorem 1, the following holds.\nCorollary 1. When DualSearch terminates, G = G\u03c7g .\nNow we analyze the query complexity. At each iteration, the algorithm uses exactly one oracle call. Then, either one new v \u2208 Vg is identified if Line 5 of Algorithm 1 is not satisfied, or one new x \u2208 \u03c7g is obtained if Line 5 is satisfied. Using these facts, we prove the following theorem.\nTheorem 2. The number of oracle calls in DualSearch is |Vg|+ |\u03c7g|.\nRecall that each x \u2208 \u03c7g corresponds to a facet of the (m + 1)-dimensional convex polytope of g. Since each vertex is determined as the intersection of at least (m + 1) facets, at the end of our algorithm, |Vg| is bounded by O(|\u03c7g|m+1). Thus, the query complexity becomes O (poly(|\u03c7g|))."}, {"heading": "4 Algorithms for a specific constraint instance", "text": "In this section, we propose two variants of DualSearch to compute an approximate solution for a specific constraint instance. The first one is called DualMax, and the second one is AdaptSearch which combines DualMax and DualSearch. While DualSearch essentially does not need prior knowledge, DualMax and AdaptSearch explicitly use a given prior knowledge b\u0302 for more efficient computation.\n4.1 DualMax\nGiven b\u0302 \u2208 Rm, this algorithm finds the maximum of the dual g, which provides a lower bound of (3). If a corresponding minimizer of (3) is in \u03c7g, this algorithm finds that minimizer efficiently. Even though the corresponding minimizer is not in \u03c7g, the algorithm finds a lower bound of the minimum, which is a good approximate solution as shown in Section 5.1.2.\nThe main difference of DualMax from DualSearch is the vertex set appended to V in Line 7 of Algorithm 1. At each iteration, DualMax calls the oracle for the maximum of the current induced dual. While DualSearch appends all vertices in V+, DualMax only appends one vertex v \u2208 V+ where zv \u2265 zu for all u \u2208 V+. Then zv becomes the maximum of the induced dual for the next iteration. Since the (induced) dual is concave, such a local search on S enables us to eventually find the maximum of the dual. The following is the modification of DualSearch to obtain DualMax.\n\u2022 The initial vertex set is changed to V \u2032 = {v} where zv \u2265 zu for all u \u2208 V where V is the ordinary initial skeleton vertex.\n\u2022 Line 7 of Algorithm 1 is changed to \u201cappend to V the one vertex v \u2208 V+ such that zv \u2265 zu for all u \u2208 V+\u201d.\nThen, the following Lemma holds, and the proof is provided in Appendix.\nLemma 3. When DualMax terminates, for the last v\u2217 for which the oracle is called, zv\u2217 = g(\u03bbv\u2217) = max\u03bb g(\u03bb).\nNote that DualMax uses far fewer oracle calls than DualSearch, which leads to fast computation of the maximum value of g and the corresponding primal solution. The cutting plane method [6] can do the same computation as DualMax, and DualMax can be understood as one implementation of the cutting plane method. While the cutting plane method computes the maximum of the dual by linear programming with computed hyperplanes at each time, DualMax computes it by keeping and updating the skeleton of the dual.\nNow, we suggest a way for DualMax to deal with inequality constraints by inserting a slack variable. For a given problem with inequality constraints, we first transform the problem to one with equality constraints, and apply the algorithm to the transformed problem. Let us consider the following problem.\nmin x\n{ f(x) : b\u0304\u2212 k \u2264 H(x) \u2264 b\u0304 } , (15)\nwhere k \u2208 Rm, and the inequality is the coordinatewise inequality. The inequality gap contains our prior knowledge, i.e. b\u0302i \u2208 [b\u0304i\u2212ki, b\u0304i]. First we transform the problem to a problem with equality constraints using a slack variable y \u2208 Rm as follows.\nmin x,y\n{ f\u0302(x, y) : H(x) + y = b\u0304 } , (16)\nwhere y \u2208 \u220fm i=1[0, ki], and f\u0302(x, y) = f(x). Let us consider the following Lagrangian.\nL\u0302(x, y, \u03bb) = f\u0302(x, y) + \u03bbT (H(x) + y \u2212 b\u0304). (17)\nFor a minimizer (x\u2217, y\u2217) of L\u0302 for a fixed \u03bb, it always holds that y\u2217i = 0 for \u03bbi > 0, y \u2217 i = ki for \u03bbi < 0, and y\u2217 can be any number in [0, ki] for \u03bbi = 0. Hence, y \u2217 only depends on \u03bb. Then, the dual g\u0302(\u03bb) of f\u0302(x, y) becomes g\u0302(\u03bb) = min\nx\n{ f(x) + \u03bbT (H(x) + y\u2217 \u2212 b\u0304) } . (18)\nNote that max\u03bb g\u0302(\u03bb) is a lower bound of (15). Since y \u2217 is determined only by \u03bb, g\u0302(\u03bb) can be computed by the same oracle for g(\u03bb). Now, we obtain the following lemma.\nLemma 4. Let (x\u2217, y\u2217) be such that L\u0302(x\u2217, y\u2217, \u03bb\u2217) = g\u0302(\u03bb\u2217) for some \u03bb\u2217 \u2208 S, and b\u2217 = H(x\u2217) + y\u2217. Then f(x\u2217) = minx {f(x) : b\u2217 \u2212 k \u2264 H(x) \u2264 b\u2217}.\nProof. Assume (x\u0302, y\u0302) satisfying H(x\u0302) + y\u0302 = b\u2217. It implies that \u03bbT (H(x\u0302) + y\u0302\u2212 b\u2217) = \u03bbT (H(x\u2217) + y\u2217\u2212 b\u2217) for any \u03bb \u2208 Rm. Then, g\u0302(\u03bb\u2217) = L\u0302(x\u2217, y\u2217, \u03bb\u2217) \u2264 L(x\u0302, y\u0302, \u03bb\u2217). Finally, f\u0302(x\u2217, y\u2217) \u2264 f\u0302(x\u0302, y\u0302), and by the definition of f\u0302 , f(x\u2217) \u2264 f(x\u0302) holds.\nHence, we can solve (15) by the same manner as in the equality case. Inequality constraints make DualMax more widely applicable because we may not know the exact statistics of a desired solution in practice.\n4.2 AdaptSearch\nDualSearch is a very effective algorithm because it finds minimizers for all \u03bb \u2208 S. But in general we do not know where good solutions are found, and thus we should use a large search region S, which leads to slow running time. On the other hand, while DualMax efficiently finds the maximum of the dual for a specific b\u0302, it may be difficut to determine b\u0302 for equality constraints in practice. Even if we use inequality constraints to deal with the uncertainty, as inequality gap gets larger, the accuracy of DualMax gets lower. To overcome these drawbacks, we propose a hybrid algorithm, called AdaptSearch, to combine advantages of DualMax and DualSearch, which runs as follows.\nStep 1 Let our prior knowledge b\u0302 be given, and S \u2282 Rm be a large search region. Step 2 Run DualMax on S with inequality constraint b\u0302 \u2212 k\u2212 \u2264 H(x) \u2264 b\u0302 + k+ for moderately small\nk\u2212, k+ > 0. Let b\u2217 be the constraint instance for which the dual maximum is computed.\nStep 3 Run DualMax again on S with equality constraint H(x) = b\u2217. Then, we obtain \u03bb\u2217 at which DualMax computes the maximum of the dual.\nStep 4 Run DualSearch for a small search region \u220fm i=1[\u03bb \u2217 i \u2212 \u03b1i, \u03bb\u2217i + \u03b1i] where \u03b1i \u2265 0, and let X\u2217 be the\noutput of DualSearch.\nStep 5 Output a solution among X\u2217 that minimizes the soft-constrained objective.\nNote that in AdaptSearch, we can also use the cutting plane method instead of DualMax. In general, any convex search region is adoptable in Step 4, but we observed from extensive experiments that small constants \u03b1i are enough to obtain a good solution. We will show in Section 5 that AdaptSearch computes better solutions than DualMax and runs quite fast."}, {"heading": "5 Applications", "text": ""}, {"heading": "5.1 Labelling problems in computer vision", "text": "In computer vision, a number of problems can be reduced to labelling problems, including image segmentation, 3D-reconstruction, and stereo. Our constrained energy minimization algorithms can be applied to those problems, for instance, when we may have knowledge on the volume of a reconstructed object for 3D-reconstruction or on the number of pixels belonging to an object for image segmentation. In this section, we show how our algorithms are applied to the foreground-background (fg-bg) image segmentation problem.\nThe fg-bg image segmentation problem is to divide a given image to foreground (object) and background. This can be done by labelling all pixels such that 1 is assigned to foreground pixels and 0 is assigned to background pixels. For this problem, one popular approach is to consider an image as a grid graph in which each node has four neighbours, and minimize an energy function f of the form (2), which is submodular. The unary terms of the function encode how likely each pixel belongs to the foreground or background, while the pairwise terms encode the smoothness of the boundary of the object being segmented. However, in general, a minimizer of (2) is not the ground truth, and it has been shown that imposing statistics on a desired solution can improve segmentation results [12].\nBelow, we describe some linear constraints that have been successfully used in computer vision. \u2022 Size: \u2211 i\u2208V xi = b where b \u2208 R [20, 35, 36].\n\u2022 Mean: \u2211 i\u2208V cixi\u2211 i\u2208V xi\n= b where b \u2208 R2 and ci = (vi, hi) \u2208 R2 denotes the vertical and horizontal coordinates of a pixel i, respectively [12].\n\u2022 Cov.: \u2211 i\u2208V (vi\u2212\u00b5v)(hi\u2212\u00b5h)xi\u2211 i\u2208V xi\n= b where b \u2208 R and (\u00b5v, \u00b5h) \u2208 R2 denotes the mean center of the object [12].\nWe can define the variance constraints for the vertical and horizontal coordinates in a similar way to the covariance constraint.\nIn many scenarios, researchers are interested in ensuring that the boundary of the object in the segmentation has a particular length. This length can be measured by counting the number of pairs of adjacent variables having different labels and described by \u2211 (i,j)\u2208E |xi \u2212 xj | = b where b \u2208 R. For this boundary constraint, the search region S may be restricted to a subregion of R\u00d7 [K,\u221e] where K \u2264 0 is the smallest real number ensuring L(x, \u03bb) submodular for all \u03bb \u2208 S. Figure 2 shows improvement of segmentation results by imposing the above constraints.\n5.1.1 Query complexity of DualSearch\nRecall that the query complexity of DualSearch is polynomial in |\u03c7g|. Note that |\u03c7g| is upper bounded by the number C of all possible constraint instances. For all the constraints above, we can show C = O(poly(n)). For example, for the size constraint, C = n, and for the boundary length constraint, C \u2264 2n because G is a grid graph. Let us consider the mean constraint, and let b\u0302 \u2208 R2 be obtained from our prior knowledge. Then, the Lagrangian is as follows:\nL(x, \u03bb) = f(x) + \u03bbT (\u2211 i (ci \u2212 b\u0302)xi ) , (19)\nwhere ci \u2208 Z2 is bounded by the size of row and column of the image. Hence, the numbers of possible values of \u2211 i cixi and b\u0302 \u2211 i xi are O(n\n2) and O(n), respectively, which leads to C = O(n3). By a similar analysis, we can show C = O(n3) for the covariance and variance constraints. If we consider multiple constraints simultaneously, C is bounded by multiplication of the upper bound of each constraint. Hence, |\u03c7g| = O(poly(n)) for any combination of the constraints above."}, {"heading": "5.1.2 Experiments", "text": "First we did experiments for the size and boundary constraints, and used the following Lagrangian. L(x, \u03bb) = f(x) + \u03bb1 \u2211 i\u2208V xi + \u03bb2 \u2211 (i,j)\u2208E |xi \u2212 xj |. (20)\nTable 1 reports the summary of results of DualSearch for 12 images with size 120 \u00d7 120. DualSearch produces minimizers for a very large number of constraint instances. One implication is that for any given constraint instance, DualMax and AdaptSearch can compute a minimizer with very close constraint instance to the original one. Figure3 shows an example of a skeleton projected onto two dimensional \u03bb space that is computed with (20) for a 12\u00d7 12 image.\nFigure 4 shows experimental results of DualMax and AdaptSearch. For AdaptSearch, we used a soft constraint with a square penalty function for the size and the boundary length constraint, that is, \u03b71( \u2211 i xi \u2212 b\u03021)2 and \u03b72( \u2211 (i,j)\u2208E |xi \u2212 xj | \u2212 b\u03022)2. We chose \u03b71 = 1 and \u03b72 = 100 with which segmentation results generally show less error. Also for the first running of DualMax, we used the inequality gap k+, k\u2212 of \u00b110% of b\u0302, and b\u03021, b\u03022 were obtained from the ground truth. The small search region to apply DualSearch is used with \u03b11 = \u03b12 = 1, except for the first image with \u03b11 = \u03b12 = 0.3.\nWe also compared our algorithms with LP [18] and QP [12] relaxation based methods. Table 2 shows that DualMax is faster and more accurate compared with both methods. Since LP and QP cannot handle higher order both-side constrained inequality constraints unlike our algorithms, we used linear constraints introduced previously. Segmentation results are provided in Appendix."}, {"heading": "5.2 Combinatorial optimization", "text": "Submodular minimization Our method can also be used for constrained submodular function minimization (SFM). SFM is known to be polynomial time solvable and a number of studies have considered SFM under specific constraints such as vertex cover and size constraints [10, 22]. In contrast to previous work, we provide a framework for dealing with multiple general constraints. Our method can not only deal with any linear constraint, but can also handle some higher order constraints which ensure that the dual is computable. For instance, as shown in the previous section, any submodular constraint hi(x) can be handled with restricted \u03bbi \u2265 0.\nShortest path problem The restricted shortest path problem is a widely studied constrained version in which each edge has an associated delay in addition to its length. A path is feasible if its total delay is less than some threshold D [21]. This is a linear constraint \u2211 i dixi \u2264 D where di is the delay of edge i. Another\nnatural constraint for the shortest path problem is to drop some k nodes among a given set of m nodes. For instance, we may want to design a tour that should contain k cities among m cities. Indeed, this becomes a Hamiltonian path problem when k = m = n. As in the project selection problem, we may partition n cities to r groups, and try to visit ki number of cities from each group where 1 \u2264 i \u2264 r. Note that all constraints above are linear so that our method can be applied.\nProject selection problem Given a set P of projects, a profit function q : P \u2192 R, and a prerequisite relation R \u2286 P\u00d7P , this problem is to find projects maximizing the total profit while satisfying a prerequisite relation. This is also known as the maximal closure problem and can be solved in polynomial time by transforming it to a st-mincut problem [25]. In practice, P may be represented by sets P1, . . . , Pm that may overlap, and we may want to select ki projects from Pi for 1 \u2264 i \u2264 m. This can be formulated using linear constraints ki = u T i x where ui is an indicator which projects belong to Pi. This enables the use of our method to solve the constrained project selection problem."}, {"heading": "6 Conclusions", "text": "This paper proposes novel algorithms to deal with the multiple constrained MAP inference problem. Our algorithm AdaptSearch is able to generate high-quality candidate solutions in a short time (see Figure 4) and enables handling of problems with very high order potential functions. We believe it would have a significant impact on the solution of many labelling problems encountered in computer vision and machine learning. As future work, we intend to analyze the use of our algorithms for enforcing statistics in problems encountered in various domains of machine learning."}, {"heading": "A Proofs", "text": "In this section, we provide the proofs omitted in the main body. We use notations G(t), V+(t), V\u2212(t) and X(t) to indicate V+,V\u2212,G and X at the end of the t-th iteration in lgorithm 1, respectively. Also we denote a vertex chosen in Line 3 at the t-th iteration by v, and \u0393X(S) by \u0393X without S. Note that initially G(0) = GgX by the definition of InitSkeleton().\nA.1 Proof of Lemma 2\nLemma 2 is proved by the following Lemma 5 and Lemma 6.\nLemma 5. Assume that G(t\u2212 1) = GgX(t\u22121) . Then V(t) = VgX(t) .\nProof. (=\u21d2) Let u \u2208 V(t). First assume that u is also in V(t \u2212 1). Suppose that u /\u2208 VgX(t) . There is Q \u2286 \u0393X(t) and Q \u2229 {u} = \u2205 so that u is a proper convex combination of Q. Note that \u0393X(t) \u2286 \u0393X(t\u22121). Thus, u is a proper convex combination of Q over \u0393X(t\u22121). It is a contradiction to u \u2208 V(t\u2212 1).\nAssume that u /\u2208 V(t\u22121). Then, u \u2208 V+(t), implying that u \u2208 e(u1, u2) \u2208 E(t\u22121) and u \u2208 Pxv . Suppose that there is Q \u2286 \u0393X(t) and Q\u2229{u} = \u2205 so that u is a proper convex combination of Q. Since Q * e(u1, u2) and \u0393X(t) \u2286 \u0393X(t\u22121), it is a contradiction to the definition of E(t\u2212 1).\n(\u21d0=) Let u \u2208 VgX(t) . Suppose that u /\u2208 V(t) but u \u2208 V(t \u2212 1), which means that u \u2208 V\u2212. Then, zu > gX(t)(\u03bbu), a contradiction to u \u2208 VgX(t) . Suppose that u /\u2208 V(t) nor u /\u2208 V(t \u2212 1). Then, there is Q \u2282 \u0393X(t\u22121) and Q \u2229 {u} = \u2205 so that u is a proper convex combination of Q. Note that Q * Pxv because u \u2208 VgX(t) . Since zu \u2264 Pxv (\u03bbu), at least one of Q is strictly below Pxv , and let Q\u2212 be the set of such elements of Q. Since u \u2208 VgX(t) , at least one of Q is strictly above Pxv , and let Q+ be the set of such elements of Q. Let P be the set of intersections of e(q\u2212, q+) and Pxv where q\n\u2212 \u2208 Q\u2212 and q+ \u2208 Q+. Suppose that u is strictly below Pxv , then u is a proper convex combination of P and Q\n\u2212 \u2282 \u0393X(t), implying a contradiction. So u \u2208 Pxv . Suppose that |P | > 1, then u is a proper convex combination of P \u2282 \u0393X(t), which is a contradiction. Thus, |P | = 1 and u \u2208 Pxv . Then since u is on some edge e(w1, w2) \u2208 E(t \u2212 1) and u \u2208 Pxv , u \u2208 V+(t) by the algorithm so that u is present in V(t), which is a contradiction.\nLemma 6. Assume that G(t\u2212 1) = GgX(t\u22121) . Then E(t) = EgX(t) .\nProof. (=\u21d2) Let e(u,w) \u2208 E(t). Assume that e(u,w) is added by E+ so that w is the intersection of e(u, u\u2032) \u2208 E(t\u2212 1) and Pxv . Suppose that there is Q \u2282 \u0393X(t), and Q * e(u,w) so that for some p \u2208 e(u,w), p is a proper convex combination of Q. Since \u0393X(t) \u2286 \u0393X(t\u22121), Q \u2286 \u0393X(t\u22121). Also since Q \u2282 \u0393X(t) and Q * e(u,w), Q * e(u, u\u2032). It is a contradiction to p \u2208 e(u, u\u2032) \u2208 E(t\u2212 1).\nAssume that e(u,w) \u2208 E(t\u2212 1). Since \u0393X(t) \u2286 \u0393X(t\u22121), no p \u2208 e(u,w) is a proper convex combination of Q \u2208 \u0393X(t) and Q * e(u,w). Thus, e(u,w) \u2208 EgX(t) due to u,w \u2208 V(t) = VgX(t) by Lemma 5.\nAssume that e(u,w) is added by ConvEdge(V+(t)). Suppose that there is Q \u2282 \u0393X(t) and Q * e(u,w) so that for some p \u2208 e(u,w), p is a proper convex combination of Q. Since p \u2208 e(u,w) \u2208 ConvEdge(V+(t)) \u2282 Pxv , Q \u2282 \u0393X(t) \u2229 Pxv . Since e(u,w) is an edge of the convex hull of V+(t), any p \u2208 e(u,w) cannot be a proper convex combination of Q, which is a contradiction.\n(\u21d0=) Let e(u,w) \u2208 EgX(t) . Suppose that e(u,w) /\u2208 E(t). If u \u2208 V\u2212 or w \u2208 V\u2212, it is a contradiction to e(u,w) \u2208 EgX(t) \u2282 2\u0393X(t) . If both u,w \u2208 V+(t), by the definition of ConvEdge(V+), and the fact that e(u,w) /\u2208 E(t), e(u,w) /\u2208 EgX(t) , which is a contradiction. Therefore one of u,w belongs to V(t \u2212 1) \u2229 V(t), and the other belongs to V+(t). Without loss of generality, let u \u2208 V(t \u2212 1) \u2229 V(t) and w \u2208 V+(t), then u must be strictly below Pxv . Then, e(u,w) /\u2208 E(t \u2212 1). There is Q \u2208 \u0393X(t\u22121) so that Q * e(u,w) and for some p \u2208 e(u,w), p is a proper convex combination of Q. If all q \u2208 Q are strictly above Pxv , p is also strictly above Pxv . If for all q \u2208 Q, zq \u2264 Pxv (\u03bbq), Q \u2282 \u0393X(t), implying a contradiction to e(u,w) \u2208 EgX(t) . Thus, at least one of Q is strictly below Pxv , and let Q \u2212 be the set of such elements of Q. Also at least one of Q is\nstrictly above Pxv , and let Q + be the set of such elements of Q. Let P be the set of intersections of e(q\u2212, q+) and Pxv where q \u2212 \u2208 Q\u2212 and q+ \u2208 Q+. If |P | > 1 or p /\u2208 P , p is a proper convex combination of Q\u2212 and P . Thus, |Q\u2212| = |Q+| = 1 and p \u2208 Pxv . This holds for all p \u2208 e(u,w), which means that e(u,w) \u2208 Pxv . This is a contradiction to the fact that u is strictly below Pxv .\nA.2 Proof of Theorem 1\nProof. Let G = (V, E) be the skeleton at the end of DualSearch. It holds that X \u2286 \u03c7g for each iteration by the algorithm. Suppose that there is x\u2217 \u2208 \u03c7g\\X when the algorithm terminates. Then, there is \u03bb\u2217 \u2208 S such that Px\u2217(\u03bb \u2217) < Px(\u03bb \u2217) for every x \u2208 X. Also there is x\u0302 \u2208 X such that Px\u0302(\u03bb\u2217) = gX(\u03bb\u2217). Then, (\u03bb\u2217, Px\u0302(\u03bb \u2217)) can be represented as a convex combination of V \u2032 \u2282 V such that Px\u0302(\u03bbv) = gX(\u03bbv) for all v \u2208 V \u2032. By a property of the algorithm, Px\u2217(\u03bbv) \u2265 Px\u0302(\u03bbv) = gX(\u03bbv) = g(\u03bbv) for each v \u2208 V \u2032. Since \u03bb\u2217 is a convex combination of {\u03bbv : v \u2208 V \u2032}, we have Px\u2217(\u03bb\u2217) \u2265 Px\u0302(\u03bb\u2217). This implies a contradiction to Px\u2217(\u03bb \u2217) < Px\u0302(\u03bb \u2217).\nA.3 Proof of Theorem 2\nProof. For each iteration, there is exactly one oracle call. Let C be a set of confirmed vertices u, that is, zu = g(\u03bbu). Note that at each iteration, either |C| or |X| increases by one, depending on whether Pxv (\u03bbv) < zv, and confirmed vertices are never removed from V. When the algorithm terminates, |X| = |\u03c7g| by Theorem 1 and |C| = |Vg| by Corollary 1. Thus, the algorithm uses |Vg|+ |\u03c7g| number of oracle calls.\nA.4 Proof of Lemma 3\nProof. First we prove that for every iteration, a chosen v satisfies that zv = max\u03bb gX(\u03bb). It initially holds by the definition of v. Assume that at the (t\u2212 1)-th iteration, the statement holds. Let v be chosen in the t-th iteration. If Pxv (\u03bbv) \u2265 zv, there is no change on X and G so that the statement holds, and the algorithm terminates. When Pxv (\u03bbv) < zv, let v\u0302 \u2208 V+ be such that zv\u0302 \u2265 zu for all u \u2208 V+. Suppose that there is v\u0304 \u2208 V(t) such that zv\u0304 > zv\u0302 and zv\u0304 is the maximum value of gX(t). Note that zv is the maximum value of gX(t\u22121) by the assumption. Suppose zv\u0304 = zv, and let P \u2286 V(t \u2212 1) be the set such that for every p \u2208 P , zp = zv. Since v\u0304 \u2208 V(t), zv\u0304 \u2264 Pxv (\u03bbv\u0304) and zv > Pxv (\u03bbv). Then, some edge between two vertices of P should intersect with Pxv due to the concavity of gX(t), and let v\n\u2032 be the intersection. Then, zv = zv\u2032 = zv\u0302, which is a contradiction to zv\u0304 > zv\u0302.\nIn GX(t\u22121), since v\u0304 is not the maximum, and by the concavity of gX(t\u22121), there is at least one edge e(v\u0304, v\u0304\u2032) where v\u0304\u2032 \u2208 V(t\u22121) such that zv\u0304\u2032 > zv\u0304. In order that zv\u0304 becomes the maximum of gX(t), v\u0304\u2032 should not belong to V(t), implying that zv\u0304\u2032 > Pxv (\u03bbv\u0304\u2032). Then, there is intersection q of Pxv and e(v\u0304, v\u0304\u2032), implying that zq \u2265 zv\u0304. If zq > zv\u0304, it is a contradiction to the fact that zv\u0304 is the maximum value of gX(t) due to q \u2208 V(t). If zq = zv\u0304, it means that zv\u0304 \u2208 V+ and thus zv\u0304 = zv\u0302, which is a contradiction to zv\u0304 > zv\u0302.\nWe have proved that when the algorithm terminates, zv = max\u03bb gX(\u03bb) \u2265 max\u03bb g(\u03bb). Let the last v be v\u2217. In the last iteration, zv\u2217 = Pxv\u2217 (\u03bb \u2217) = g(\u03bb\u2217) \u2264 max\u03bb g(\u03bb).\nB Segmentation results for Table 2"}], "references": [{"title": "The quickhull algorithm for convex hulls", "author": ["C.B. Barber", "D.P. Dobkin", "H. Huhdanpaa"], "venue": "ACM Transactions on Mathematical Software (TOMS)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Pseudo-boolean optimization", "author": ["E. Boros", "P. Hammer"], "venue": "Discrete Applied Mathematics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "CPMC: Automatic object segmentation using constrained parametric min-cuts", "author": ["J. Carreira", "C. Sminchisescu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["G. Gallo", "M. Grigoriadis", "R. Tarjan"], "venue": "SIAM J. on Comput", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Efficient inference with cardinality-based clique potentials", "author": ["R. Gupta", "A.A. Diwan", "S. Sarawagi"], "venue": "Z. Ghahramani (ed.) ICML, ACM International Conference Proceeding Series,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Community detection as an inference problem", "author": ["M.B. Hastings"], "venue": "Phys. Rev. E 74,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Transformation of general binary MRF minimization to the first-order case", "author": ["H. Ishikawa"], "venue": "PAMI 33,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Submodular function minimization under covering constraints", "author": ["S. Iwata", "K. Nagano"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A convex framework for image segmentation with moment constraints", "author": ["A. Klodt", "D. Cremers"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Integration of multiview stereo and silhouettes via convex functionals on convex domains", "author": ["K. Kolev", "D. Cremers"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Application of parametric maxflow in computer vision", "author": ["V. Kolmogorov", "Y. Boykov", "C. Rother"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Minimizing non-submodular functions with graph cuts - a review", "author": ["V. Kolmogorov", "C. Rother"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "MRF optimization via dual decomposition: message-passing revisited", "author": ["N. Komodakis", "N. Paragios", "G. Tziritas"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Graph cut based inference with co-occurrence statistics", "author": ["L. Ladicky", "C. Russell", "P. Kohli", "P. Torr"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Image segmentation with a bounding box prior", "author": ["V. Lempitsky", "P. Kohli", "C. Rother", "T. Sharp"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Markov random filed models in computer vision", "author": ["S.Z. Li"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Energy minimization under constraints on label counts", "author": ["Y. Lim", "K. Jung", "P. Kohli"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "A simple efficient approximation scheme for the restricted shortest path problem", "author": ["D.H. Lorenz", "D. Raz"], "venue": "Operations Research Letters 28,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Size-constrained submodular minimization through minimum norm base", "author": ["K. Nagano", "Y. Kawahara", "K. Aihara"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Balanced metric labeling", "author": ["J. Naor", "R. Schwartz"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Global connectivity potentials for random field models", "author": ["S. Nowozin", "C. Lampert"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Maximal closure of a graph and applications to combinatorial problems", "author": ["J.C. Picard"], "venue": "Management Science", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1976}, {"title": "Exact inference in multi-label CRFs with higher order cliques", "author": ["S. Ramalingam", "P. Kohli", "K. Alahari", "P. Torr"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "High resolution matting via interactive trimap segmentation", "author": ["C. Rhemann", "C. Rother", "A. Rav-Acha", "T. Sharp"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "grabcut\u201d: interactive foreground extraction using iterated graph cuts", "author": ["C. Rother", "V. Kolmogorov", "A. Blake"], "venue": "ACM Trans. Graph", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "Multi-view reconstruction using photo-consistency and exact silhouette constraints: A maximum-flow formulation", "author": ["S. Sinha", "M. Pollefeys"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Parallel and distributed graph cuts by dual decomposition", "author": ["P. Strandmark", "F. Kahl"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Random field model for integration of local information and global information", "author": ["T. Toyoda", "O. Hasegawa"], "venue": "PAMI 30,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Graph cut based image segmentation with connectivity priors", "author": ["S. Vicente", "V. Kolmogorov", "C. Rother"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "MAP estimation via agreement on trees: message-passing and linear programming", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "MAP estimation, linear programming and belief propagation with convex free energies", "author": ["Y. Weiss", "C. Yanover", "T. Meltzer"], "venue": "UAI", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (MAP-MRF)", "author": ["T. Werner"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "A global perspective on MAP inference for low-level vision", "author": ["O. Woodford", "C. Rother", "V. Kolmogorov"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "A global perspective on map inference for low-level vision", "author": ["O.J. Woodford", "C. Rother", "V. Kolmogorov"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}], "referenceMentions": [{"referenceID": 16, "context": "Markov Random Fields (MRF) is an undirected graphical model, which has been extensively studied and used in various fields, including statistical physics [11], and computer vision [19].", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "However, a number of subclasses of MRFs have been isolated for which the problem can be solved in polynomial time [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 31, "context": "Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem.", "startOffset": 88, "endOffset": 92}, {"referenceID": 30, "context": "Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem.", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem.", "startOffset": 146, "endOffset": 149}, {"referenceID": 14, "context": "Such algorithms are widely used for various problems in machine learning and computer vision [38, 17].", "startOffset": 93, "endOffset": 101}, {"referenceID": 6, "context": "Another example is community detection in a network [8] where we may know the number of nodes belonging to each community.", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "However, recent work in computer vision has shown that this problem can be handled efficiently using the parametric mincuts [14] which allow simultaneous computation of exact solutions for some constraint instances.", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "For instance, linear relaxation approaches were adopted to handle bounding-box and connectivity constraints defined on the labelling [18, 24].", "startOffset": 133, "endOffset": 141}, {"referenceID": 21, "context": "For instance, linear relaxation approaches were adopted to handle bounding-box and connectivity constraints defined on the labelling [18, 24].", "startOffset": 133, "endOffset": 141}, {"referenceID": 9, "context": "Further, Klodt and Cremers [12] proposed a convex relaxation framework to deal with moment constraints.", "startOffset": 27, "endOffset": 31}, {"referenceID": 2, "context": "A graph-cut algorithm [3] is a popular example of such an oracle.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "In fact, our algorithm generalizes the (one-dimensional) parametric mincuts [5, 14] to multiple-dimensions.", "startOffset": 76, "endOffset": 83}, {"referenceID": 11, "context": "In fact, our algorithm generalizes the (one-dimensional) parametric mincuts [5, 14] to multiple-dimensions.", "startOffset": 76, "endOffset": 83}, {"referenceID": 4, "context": "In contrast to the parametric mincuts [5], our algorithm can deal with multiple constraints simultaneously, including some non-linear constraints (as we show in the paper).", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "to obtain different segmentation results for image segmentation as done in [4].", "startOffset": 75, "endOffset": 78}, {"referenceID": 33, "context": "Among them, solutions to image labelling problems which have a particular distribution of labels [36] or satisfy a topological property like connectivity [32] have been widely studied.", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "Among them, solutions to image labelling problems which have a particular distribution of labels [36] or satisfy a topological property like connectivity [32] have been widely studied.", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "More specifically, for the problem of foreground-background image segmentation, most probable segmentations under the label count constraint have been shown to be closer to the ground truth [14, 20].", "startOffset": 190, "endOffset": 198}, {"referenceID": 17, "context": "More specifically, for the problem of foreground-background image segmentation, most probable segmentations under the label count constraint have been shown to be closer to the ground truth [14, 20].", "startOffset": 190, "endOffset": 198}, {"referenceID": 10, "context": "example is the silhouette constraint which has been used for the problem of 3D reconstruction [13, 29].", "startOffset": 94, "endOffset": 102}, {"referenceID": 26, "context": "example is the silhouette constraint which has been used for the problem of 3D reconstruction [13, 29].", "startOffset": 94, "endOffset": 102}, {"referenceID": 5, "context": "Recently, dual decomposition has been proposed for constrained MAP inference [7, 37].", "startOffset": 77, "endOffset": 84}, {"referenceID": 34, "context": "Recently, dual decomposition has been proposed for constrained MAP inference [7, 37].", "startOffset": 77, "endOffset": 84}, {"referenceID": 5, "context": "[7] dealt with cardinality-based clique potentials and developed both exact and approximate algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "[37] studied a problem involving marginal statistics such as the area constraint especially with convex penalties, and showed that the proposed method improves quality of solutions for various computer vision problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "For this problem, Naor and Schwartz [23] obtained an O( lnn )-approximate algorithm where each label is assigned to at most min [ O(ln k) 1\u2212 , `+ 1 ] (1 + )` variables/nodes.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "it can be efficiently minimized by solving a equivalent st-mincut problem [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "Such f is widely used in machine learning and computer vision [9, 20].", "startOffset": 62, "endOffset": 69}, {"referenceID": 17, "context": "Such f is widely used in machine learning and computer vision [9, 20].", "startOffset": 62, "endOffset": 69}, {"referenceID": 13, "context": "Let us consider the following Lagrangian dual g : R \u2192 R of f , which is widely used for discrete optimization [16, 30].", "startOffset": 110, "endOffset": 118}, {"referenceID": 27, "context": "Let us consider the following Lagrangian dual g : R \u2192 R of f , which is widely used for discrete optimization [16, 30].", "startOffset": 110, "endOffset": 118}, {"referenceID": 14, "context": "This soft-constrained optimization has been widely used in terms of lasso regularization and ridge regression, and also in computer vision [17, 31].", "startOffset": 139, "endOffset": 147}, {"referenceID": 28, "context": "This soft-constrained optimization has been widely used in terms of lasso regularization and ridge regression, and also in computer vision [17, 31].", "startOffset": 139, "endOffset": 147}, {"referenceID": 23, "context": "For instance, the results of [26] show transformation of any multi-label submodular functions of order up to 3 to a pairwise submodular one, meaning that it can be solved by the graph-cut algorithm.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "3 For a given V+, ConvEdge(V+) can be computed, for example, by [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "However, in general, a minimizer of (2) is not the ground truth, and it has been shown that imposing statistics on a desired solution can improve segmentation results [12].", "startOffset": 167, "endOffset": 171}, {"referenceID": 17, "context": "\u2022 Size: \u2211 i\u2208V xi = b where b \u2208 R [20, 35, 36].", "startOffset": 33, "endOffset": 45}, {"referenceID": 32, "context": "\u2022 Size: \u2211 i\u2208V xi = b where b \u2208 R [20, 35, 36].", "startOffset": 33, "endOffset": 45}, {"referenceID": 33, "context": "\u2022 Size: \u2211 i\u2208V xi = b where b \u2208 R [20, 35, 36].", "startOffset": 33, "endOffset": 45}, {"referenceID": 9, "context": "\u2022 Mean: \u2211 i\u2208V cixi \u2211 i\u2208V xi = b where b \u2208 R and ci = (vi, hi) \u2208 R denotes the vertical and horizontal coordinates of a pixel i, respectively [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": ": \u2211 i\u2208V (vi\u2212\u03bcv)(hi\u2212\u03bch)xi \u2211 i\u2208V xi = b where b \u2208 R and (\u03bcv, \u03bch) \u2208 R denotes the mean center of the object [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "Table 1: Results of DualSearch on 12 images from [27] each of which has the size 120\u00d7 120.", "startOffset": 49, "endOffset": 53}, {"referenceID": 25, "context": "The values are averaged over 6 images from [28] of size 321\u00d7 481.", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "We also compared our algorithms with LP [18] and QP [12] relaxation based methods.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "We also compared our algorithms with LP [18] and QP [12] relaxation based methods.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "SFM is known to be polynomial time solvable and a number of studies have considered SFM under specific constraints such as vertex cover and size constraints [10, 22].", "startOffset": 157, "endOffset": 165}, {"referenceID": 19, "context": "SFM is known to be polynomial time solvable and a number of studies have considered SFM under specific constraints such as vertex cover and size constraints [10, 22].", "startOffset": 157, "endOffset": 165}, {"referenceID": 18, "context": "A path is feasible if its total delay is less than some threshold D [21].", "startOffset": 68, "endOffset": 72}, {"referenceID": 25, "context": "The used images are from [28].", "startOffset": 25, "endOffset": 29}, {"referenceID": 22, "context": "This is also known as the maximal closure problem and can be solved in polynomial time by transforming it to a st-mincut problem [25].", "startOffset": 129, "endOffset": 133}], "year": 2013, "abstractText": "In this paper, we propose novel algorithms for inferring the Maximum a Posteriori (MAP) solution of discrete pairwise random field models under multiple constraints. We show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly. These multiple solutions enable us to even deal with \u2018soft constraints\u2019 (higher order penalty functions). Moreover, we propose two practical variants of our algorithm to solve problems with hard constraints. We also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation. Experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods.", "creator": "LaTeX with hyperref package"}}}