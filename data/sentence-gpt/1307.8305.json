{"id": "1307.8305", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2013", "title": "The Planning-ahead SMO Algorithm", "abstract": "The sequential minimal optimization (SMO) algorithm and variants thereof are the de facto standard method for solving large quadratic programs for support vector machine (SVM) training. In this paper we propose a simple yet powerful modification. The main emphasis is on an algorithm improving the SMO step size by planning-ahead. The theoretical analysis ensures its convergence to the optimum. Experiments involving a large number of datasets were carried out to demonstrate the superiority of the new algorithm. These results are of significant interest in a large subset of the field and can be applied to other programs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 31 Jul 2013 12:38:20 GMT  (25kb)", "http://arxiv.org/abs/1307.8305v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tobias glasmachers"], "accepted": false, "id": "1307.8305"}, "pdf": {"name": "1307.8305.pdf", "metadata": {"source": "CRF", "title": "The Planning-ahead SMO Algorithm", "authors": ["Tobias Glasmachers"], "emails": ["tobias.glasmachers@ini.rub.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n83 05\nv1 [\ncs .L\nG ]\n3 1\nJu l 2\nKeywords: Sequential Minimal Optimization, Quadratic Programming, Support Vector Machine Training"}, {"heading": "1 Introduction", "text": "Training a support vector machine (SVM) for binary classification is usually accomplished through solving a quadratic program. Assume we are given a training dataset (x1, y1), . . . , (x\u2113, y\u2113) composed of inputs xi \u2208 X and binary labels yi \u2208 {\u00b11}, a positive semi-definite Mercer kernel function k : X\u00d7X \u2192 R on the input space X and a regularization parameter value C > 0. Then the dual SVM training problem is given by\nmaximize f(\u03b1) = yT\u03b1\u2212 1\n2 \u03b1TK\u03b1 (1)\ns.t.\n\u2113\u2211\ni=1\n\u03b1i = 0 (equality constraint)\nand Li \u2264 \u03b1i \u2264 Ui \u2200 1 \u2264 i \u2264 \u2113 (box constraint)\nfor \u03b1 \u2208 R\u2113. Here, the vector y = (y1, . . . , y\u2113) T \u2208 R\u2113 is composed of the labels, the positive semi-definite kernel Gram matrix K \u2208 R\u2113\u00d7\u2113 is given by Kij = k(xi, xj) and the lower and upper bounds are Li = min{0, yiC} and Ui = max{0, yiC}. We denote the feasible region byR and the set of optimal points by R\u2217. On R\u2217 the objective function f attains its maximum denoted by f\u2217 = max{f(\u03b1) |\u03b1 \u2208 R}.\nSolving this problem up to a sufficient accuracy seems to scale roughly quadratic in \u2113 in practice [7]. This relatively bad scaling behavior is one of the major drawbacks of SVMs in general as the number \u2113 of training examples may easily range to hundreds of thousands or even millions in today\u2019s pattern recognition problems."}, {"heading": "2 State of the Art SMO Algorithm", "text": "The sequential minimal optimization (SMO) algorithm [13] is an iterative decomposition algorithm [12] using minimal working sets of size two. This size is minimal to keep the current solution feasible. The algorithm explicitly exploits the special structure of the constraints of problem (1) and shows very good performance in practice. For each feasible point \u03b1 \u2208 R we define the index sets\nIup(\u03b1) ={i \u2208 {1, . . . , \u2113} | \u03b1i < Ui}\nIdown(\u03b1) ={i \u2208 {1, . . . , \u2113} | \u03b1i > Li} .\nThe canonical form of the SMO algorithm (using the common Karush-KuhnTucker (KKT) violation stopping condition) can be stated as follows:\nAlgorithm 1: General SMO Algorithm\nInput: feasible initial point \u03b1(0), accuracy \u03b5 \u2265 0 compute the initial gradient G(0) \u2190 \u2207f(\u03b1(0)) = y \u2212K\u03b1(0) set t \u2190 1 do\n1 select a working set B(t) 2 solve the sub-problem induced by B(t) and \u03b1(t\u22121), resulting in \u03b1(t) 3 compute the gradient\nG(t) \u2190 \u2207f(\u03b1(t)) = G(t\u22121) \u2212K ( \u03b1(t) \u2212 \u03b1(t\u22121) )\n4 stop if( max { G\n(t) i \u2223\u2223 i \u2208 Iup(\u03b1(t)) } \u2212min { G (t) j \u2223\u2223 j \u2208 Idown(\u03b1(t)) }) \u2264 \u03b5\nset t \u2190 t+ 1 loop;\nIf no additional information are available the initial solution is chosen to be \u03b1(0) = (0, . . . , 0)T resulting in the initial gradient G(0) = \u2207f(\u03b1(0)) = y which can be computed without any kernel evaluations.\nIt is widely agreed that the working set selection policy is crucial for the overall performance of the algorithm. This is because starting from the initial solution the SMO algorithm generates a sequence (\u03b1(t))t\u2208N of solutions which is determined by the sequence of working sets (B(t))t\u2208N. We will briefly discuss some concrete working set selection policies later on.\nFirst we will fix our notation. In each iteration the algorithm selects a working set of size two. In this work we will consider (ordered) tuples instead of sets for a number of reasons. Of course we want our tuples to correspond to sets of cardinality two. Therefore a working set B is of the form (i, j) with i 6= j. Due to its wide spread we will stick to the term working set\ninstead of tuple as long as there is no ambiguity. Whenever we need to refer\nto the corresponding set, we will use the notation B\u0302 = (\u0302i, j) := {i, j}. For a tuple B = (i, j) we define the direction vB = ei \u2212 ej where en is the n-th unit vector of R\u2113. This direction has a positive component for \u03b1i and a negative component for \u03b1j . We will restrict the possible choices such that the current point \u03b1 can be moved in the corresponding direction vB without immediately leaving the feasible region. This is equivalent to restricting i to Iup(\u03b1) and j to Idown(\u03b1). We collect the allowed working sets in a point \u03b1 in the set B(\u03b1) = Iup(\u03b1)\u00d7Idown(\u03b1)\\{(n, n) | 1 \u2264 n \u2264 \u2113}. With this notation a working set selection policy returns some B(t) \u2208 B(\u03b1(t\u22121)).\nThe sub-problem induced by the working set B(t) solved in step 2 in iteration t is defined as\nmaximize f(\u03b1(t)) = yT\u03b1(t) \u2212 1\n2 (\u03b1(t))TK\u03b1(t)\ns.t. \u2113\u2211\ni=1\n\u03b1 (t) i = 0 (equality constraint)\nLi \u2264 \u03b1 (t) i \u2264 Ui for i \u2208 B\u0302 (t) (box constraint)\nand \u03b1 (t) i = \u03b1 (t\u22121) i for i 6\u2208 B\u0302 (t) .\nThat is, we solve the quadratic program as good as possible while keeping all variables outside the current working set constant. We can incorporate the equality constraint into the parameterization \u03b1(t) = \u03b1(t\u22121) + \u00b5(t)vB(t) and arrive at the equivalent problem\nmaximize lt\u00b5 (t) \u2212\n1 2 Qtt(\u00b5 (t))2\ns.t. L\u0303t \u2264 \u00b5 (t) \u2264 U\u0303t\nfor \u00b5(t) \u2208 R with\nQtt =Kii \u2212 2Kij +Kjj = v T B(t) KvB(t)\nlt = \u2202f\n\u2202\u03b1i (\u03b1(t\u22121))\u2212\n\u2202f\n\u2202\u03b1j (\u03b1(t\u22121)) = vT B(t) \u2207f(\u03b1(t\u22121))\nL\u0303t =max{Li \u2212 \u03b1 (t\u22121) i , \u03b1 (t\u22121) j \u2212 Uj} U\u0303t =min{Ui \u2212 \u03b1 (t\u22121) i , \u03b1 (t\u22121) j \u2212 Lj}\nand the notation B(t) = (i, j). This problem is solved by clipping the Newton step \u00b5\u2217 = lt/Qtt to the bounds:\n\u00b5(t) = max { min { lt Qtt , U\u0303t } , L\u0303t } . (2)\nFor \u00b5(t) = lt/Qtt we call the SMO step free. In this case the SMO step coincides with the Newton step in direction vB(t) . Otherwise the step is said to hit the box boundary.\nRecently it has been observed that the SMO step itself can be used for working set selection resulting in so called second order algorithms [2, 5].\nWe can formally define the gain of a SMO step as the function gB(\u03b1) which computes the difference f(\u03b1\u2032) \u2212 f(\u03b1) of the objective function before and after a SMO step with starting point \u03b1 on the working set B, resulting in \u03b1\u2032. For each working set B this function is continuous and piecewise quadratic (see [5]). Then these algorithms greedily choose a working set B(t) promising the largest functional gain gB(t)(\u03b1\n(t\u22121)) = f(\u03b1(t))\u2212f(\u03b1(t\u22121)) by heuristically evaluating a subset of size O(\u2113) of the possible working sets B(\u03b1(t\u22121)).\nFan et al. [2] propose to choose the working set according to\ni = argmax\n{ \u2202f\n\u2202\u03b1n (\u03b1)\n\u2223\u2223\u2223\u2223 n \u2208 Iup(\u03b1) }\nj = argmax { g\u0303(i,n)(\u03b1) \u2223\u2223\u2223 n \u2208 Idown(\u03b1) \\ {i} }\n(3)\nwith g\u0303B(\u03b1) = 1\n2\n(vTB\u2207f(\u03b1)) 2\nvTBKvB \u2208 R\u22650 \u222a {\u221e}\nwhere g\u0303B(\u03b1) is an upper bound on the gain which is exact if and only if the step starting from \u03b1 with working set B is not constrained by the box.1 Note that in this case the Newton step \u00b5\u2217 = (vTB\u2207f(\u03b1))/(v T BKvB) in direction vB is finite and we get the alternative formulation\ng\u0303B(\u03b1) = 1\n2 (vTBKvB)(\u00b5 \u2217)2 . (4)\nThis formula can be used to explicitly compute the exact SMO gain gB(\u03b1) by plugging in the clipped step size (2) instead of the Newton step \u00b5\u2217.\nThe stopping condition in step 4 checks if the Karush-Kuhn-Tucker (KKT) conditions of problem (1) are fulfilled with the predefined accuracy \u03b5. List et al. [9] have shown that this is a meaningful stopping criterion. The accuracy \u03b5 is usually set to 0.001 in practice.\nSMO is a specialized version of the more general decomposition algorithm which imposes the weaker condition |B(t)| \u2264 q \u226a \u2113 on the working set size. The main motivation for decomposition is that in each step only the rows of the kernel matrix K which correspond to the working set indices are needed. Therefore the algorithm works well even if the whole matrix K does not fit into the available working memory. The SMO algorithm has the advantage over decomposition with larger working sets that the sub-problems in step 2 can be solved very easily. Because of its minimal working set size the algorithm makes less progress in a single iteration compared to larger working sets. On the other hand single iterations are faster. Thus, there is a trade-off between the time per iteration and the number of iterations needed to come close enough to the optimum. The decisive advantage of SMO in this context is that it can take its decisions which working set B (corresponding to the optimization direction vB) to choose more frequently between its very fast iterations. This strategy has proven beneficial in practice.\nIn elaborate implementations the algorithm is accompanied by a kernel cache and a shrinking heuristic [7]. The caching technique exploits the\n1The software LIBSVM [2] sets the denominator of g\u0303B(\u03b1) to \u03c4 = 10 \u221212\n> 0 whenever it vanishes. This way the infinite value is avoided. However, this trick was originally designed to tackle indefinite problems.\nfact that the SMO algorithm needs the rows of the kernel matrix which correspond to the indices in the current working set B(t). The kernel cache uses a predefined amount of working memory to store rows of the kernel matrix which have already been computed. Therefore the algorithm needs to recompute only those rows from the training data evaluating the possibly costly kernel function which have not been used recently. The shrinking heuristic removes examples from the problem that are likely to end up at the box boundaries in the final solution. These techniques perfectly cooperate and result in an enormous speed up of the training process. We will later use the fact that the most recently used rows of the kernel matrix K are available from the cache.\nThe steps 1, 3, and 4 of the SMO optimization loop take O(\u2113) operations, while the update 2 of the current solution is done in constant time.\nThere has not been any work on the improvement of step 2 of Algorithm 1. Of course, it is not possible to considerably speed up a computation taking O(1) operations, but we will see in the following how we may replace the optimal (greedy) truncated Newton step with other approaches."}, {"heading": "3 Behavior of the SMO Algorithm", "text": "We want to make several empirical and theoretical statements about the overall behavior of the SMO algorithm. This includes some motivation for the algorithm presented later. We start with theoretical results.\nIt is well known that the algorithm converges to an optimum for a number of working set selection strategies. Besides convergence proofs for important special cases [8, 15, 5, 3] proof techniques for general classes of selection policies have been investigated [6, 10, 1].\nChen et al. [1] have shown that under some technical conditions on problem (1) there exists t0 such that no SMO step ends up at the box bounds for iterations t > t0. For these iterations the authors derive a linear convergence rate. However, the prerequisites exclude the relevant case that the optimum is not isolated. Upper bounds for t0 are not known, and in experiments the algorithm rarely seems to enter this stage.\nFrom an empirical point of view we can describe the qualitative behavior of SMO roughly as follows. In the first iterations, starting from the initial solution \u03b1(0) = (0, . . . , 0)T , many steps move variables \u03b1i to the lower or upper bounds Li and Ui. After a while, these steps become rare and most iterations are spent on a relatively small number of variables performing free steps. In this phase the shrinking heuristics removes most bounded variables from the problem. Then working set selection, gradient update and stopping condition need to be computed only on the relatively small set of active variables, leading to extremely fast iterations.\nMany common benchmark problems and real world applications are simple in the sense that the algorithm performs only a number of iterations comparable to the number of examples. In this case only very few variables (if any) are changed many times. This indicates that there are only very few free support vectors or that the dependencies between variables are weak, making the optimization easy. On the other hand, for harder problems the\nalgorithm spends most of its iterations on free SMO steps to resolve complicated dependencies between the free variables. In fact, in some cases we can observe large blocks of iterations spent on a small number of variables. Due to its finite number of optimization directions the SMO algorithm is prone to oscillate while compensating the second order cross terms of the objective function. This oscillatory behavior observed in case of difficult problems is the main motivation for the consideration presented in the next section."}, {"heading": "4 Planning Ahead", "text": "Without loss of generality we consider the iteration t = 1 in this section. Assume we are given the current working set B(1) = (i(1), j(1)) and for some reason we already know the working set B(2) = (i(2), j(2)) to be selected in the next iteration. In addition, we presume that the solutions of both sub-problems involved are not at the bounds. That is, we can simply ignore the box constraints. From equation (4) we know with \u00b5(1) = l1/Q11 and \u00b5(2) = l2/Q22 that both free steps together result in the gain\ng2-step := f(\u03b1(2))\u2212 f(\u03b1(0)) = 1\n2 Q11(\u00b5\n(1))2 + 1\n2 Q22(\u00b5\n(2))2 . (5)\nUnder the assumption that we already know the working set B(2) we can of course precompute the second step. To stress this point of view we introduce the quantities\nwt = \u2202f\n\u2202\u03b1i(t) (\u03b1(0))\u2212\n\u2202f\n\u2202\u03b1j(t) (\u03b1(0)) = vB(t)\u2207f(\u03b1 (0)) for t \u2208 {1, 2}\nwhich only depend on \u03b1(0) and are thus known in iteration t = 1. We rewrite\nl1 = w1 l2 = w2 \u2212Q12\u00b5 (1)\nwith Q12 = Q21 = Ki(1)i(2) \u2212Ki(1)j(2) \u2212Kj(1)i(2) +Kj(1)j(2) = v T B(1) KvB(2) .\nThen we can express the step size\n\u00b5(2) = l2/Q22 = w2/Q22 \u2212Q12/Q22\u00b5 (1) (6)\nin these terms. The above notation suggests the introduction of the 2 \u00d7 2 matrix\nQ = ( Q11 Q21 Q12 Q22 )\nwhich is symmetric and positive semi-definite. If we drop the assumption that both steps involved are Newton steps the computation of the gain is more complicated:\ng2-step(\u00b5(1), \u00b5(2)) := f(\u03b1(2))\u2212 f(\u03b1(0)) = ( w1 w2 )T ( \u00b5(1) \u00b5(2) ) \u2212 1 2 ( \u00b5(1) \u00b5(2) )T Q ( \u00b5(1) \u00b5(2) )\nPlugging everything in, and in particular substituting \u00b5(2) according to eq. (6) we express the gain as a function of the single variable \u00b5(1) resulting in\ng2-step(\u00b5(1)) = \u2212 1 2 \u00b7 det(Q) Q22 (\u00b5(1))2 + Q22w1 \u2212Q12w2 Q22 \u00b5(1) + 1 2 \u00b7 w22 Q22 . (7)\nFor \u00b5(1) = w1/Q11 we obtain the gain computed in (5), but of course the maximizer of the quadratic function (7) will in general differ from this value. Thus, under the above assumption that we already know the next working set B(2) we can achieve a better functional gain by computing the optimal step size\n\u00b5(1) = Q22w1 \u2212Q12w2\ndet(Q) (8)\nwhere we again assume that we do not hit the box constraints which are dropped. It is easy to incorporate the constraints into the computation, but this has two drawbacks in our situation: First, it leads to a large number of different cases, and second it complicates the convergence proof. Further, dropping the constraints will turn out to be no restriction, as the algorithms resulting from these considerations will handle the box constrained case separately. Figure 1 illustrates the resulting step. We call the step \u00b5(1) \u00b7vB(1) the planning-ahead step, because we need to simulate the current and the next step in order to determine the step size \u00b5(1). Analogously we refer to \u00b5(1) as the planning-ahead step size.\nJust like the usual SMO update this step can be computed in constant time, that is, independent of the problem dimension \u2113. However, the kernel values of an up to 4 \u00d7 4 principal minor of the kernel Gram matrix K are needed for the computation, in contrast to a 2 \u00d7 2 minor for the standard SMO update step.\nNote the asymmetry of the functional gain as well as the optimal step size w.r.t. the iteration indices 1 and 2. We control the length of the first step, which of course influences the length of the second step. The asymmetry results from the fact that the second step is greedy in contrast to the first one. The first step is optimal given the next working set B(2) and planning one step ahead, while the second step is optimal in the usual sense of doing a single greedy step without any planning-ahead.\nAnother interesting property is that for \u00b5(1) 6\u2208 [0, 2 l1/Q11] the first step actually results in a decay of the dual objective, that is, f(\u03b1(1)) < f(\u03b1(0)), see Figure 2. Nevertheless, such steps can be extremely beneficial in some situations, see Figure 1. Of course, by construction both planned-ahead steps together result in an increase of the objective function, which is even maximized for the given working sets."}, {"heading": "5 Algorithms", "text": "In this section we will turn the above consideration into algorithms. We will present a first simple version and a refinement which focuses on the\nconvergence of the overall algorithm to an optimal solution. These modifications are all based on the SMO Algorithm 1. Thus, we will only state replacements for the working set selection step 1 and the update step 2.\nIn the previous section it is left open how we can know the choice of the working set in the forthcoming iteration. If we try to compute this working set given a step size \u00b5, it turns out that we need to run the working set selection algorithm. That is, the next working set depends on the current step size and it takes linear time to determine the working set for a given step size. This makes a search for the optimal combination of step size and working set impractical. We propose a very simple heuristic instead. For two reasons we suggest to reuse the previous working set: First, the chance that the corresponding kernel evaluations are cached is highest for this working set. Second, as already stated in section 3, the SMO algorithm sometimes tends to oscillate within a small number of variables. Figure 1 gives a low-dimensional example. Now, if we are in a phase of oscillation, the previous working set is a very good candidate for planning-ahead.\nThese considerations result in a new algorithm. It differs from the SMO algorithm only in step 2. The basic idea is to use the previous working set for planning-ahead. However, we revert to the standard SMO step if the previous step was used for planning-ahead or the planned steps are not free. This proceeding is formalized in Algorithm 2, which is a replacement for\nstep 2 of Algorithm 1.\nAlgorithm 2: Modification of step 2 of the SMO Algorithm\nif previous iteration performed a SMO step (eq. (2)) then\nCompute the planning-ahead step size \u00b5 = Q22w1\u2212Q12w2det(Q) (eq. (8)) assuming B(t\u22121) as the next working set if the current or the planned step ends at the box boundary then\nperform a SMO step (eq. (2)) else\nperform the step of size \u00b5 as planned end\nelse perform a SMO step (eq. (2))\nend\nAs already indicated in section 4 the algorithm uses planning-ahead only if both steps involved do not hit the box boundaries. This means that we need to check the box constraints while planning ahead, and we turn to the standard SMO algorithm whenever the precomputed steps become infeasible. Thus, there is no need to incorporate the box constraints into the planning-ahead step size given in equation (8).\nThe algorithm works well in experiments. However, it is hard to prove\nits convergence to an optimal solution for a number of reasons. The main difficulty involved is that we can not prove the strict increase of the objective function for the planning-ahead step, even if we additionally consider the subsequent iteration. Therefore we additionally replace the working set selection step 1 of Algorithm 1 with Algorithm 3.\nAlgorithm 3: Modification of step 1 of the SMO Algorithm\nInput: \u03b7 \u2208 (0, 1) Input: \u00b5(t\u22121): step size of the previous iteration t\u2212 1 Input: \u00b5\u2217: Newton step size of the previous iteration t\u2212 1 if previous step resulted from planning-ahead then\n// standard selection, see equation (3) i(t) \u2190 argmax{ \u2202f\u2202\u03b1n (\u03b1 (t\u22121)) | n \u2208 Iup(\u03b1 (t\u22121))} j(t) \u2190 argmax{g\u0303(i(t) ,n)(\u03b1 (t\u22121)) | n \u2208 Idown(\u03b1 (t\u22121)) \\ {i}}\nB(t) \u2190 (i(t), j(t)) else\nif 1\u2212 \u03b7 \u2264 \u00b5(t\u22121)/\u00b5\u2217 \u2264 1 + \u03b7 then\n// selection with additional candidate B(t\u22122) i(t) \u2190 argmax{ \u2202f\u2202\u03b1n (\u03b1 (t\u22121)) | n \u2208 Iup(\u03b1 (t\u22121))} j(t) \u2190 argmax{g\u0303(i(t),n)(\u03b1 (t\u22121)) | n \u2208 Idown(\u03b1 (t\u22121)) \\ {i}} B(t) \u2190 (i(t), j(t)) if g\u0303B(t\u22122)(\u03b1 (t\u22121)) > g\u0303B(t)(\u03b1 (t\u22121)) then\nB(t) \u2190 B(t\u22122)\nend\nelse\n// selection with additional candidate B(t\u22122) // based on g instead of g\u0303 i(t) \u2190 argmax{ \u2202f\u2202\u03b1n (\u03b1 (t\u22121)) | n \u2208 Iup(\u03b1 (t\u22121))} j(t) \u2190 argmax{g(i(t),n)(\u03b1 (t\u22121)) | n \u2208 Idown(\u03b1 (t\u22121)) \\ {i}} B(t) \u2190 (i(t), j(t)) if gB(t\u22122)(\u03b1 (t\u22121)) > gB(t)(\u03b1 (t\u22121)) then\nB(t) \u2190 B(t\u22122)\nend\nend\nend\nAt a first glance this algorithm looks more complicated than it is. The selection basically ensures that the planning-ahead step and the next SMO step together have a positive gain: Recall that for \u00b5(t\u22121)/\u00b5\u2217 \u2208 [1\u2212 \u03b7, 1 + \u03b7] the planning step itself makes some progress, see Figure 2. The following SMO step has always positive gain. Now consider the case that the planningstep does not make a guaranteed progress, that is, \u00b5(t\u22121)/\u00b5\u2217 6\u2208 [1\u2212 \u03b7, 1+ \u03b7]. The planned double-step gain (7) is by construction lower bounded by the Newton step gain. Thus, if the previous working set is reused in the following iteration the total gain is positive. Now the usage of the SMO gain g instead of the Newton step gain g\u0303 for working set selection ensures that this gain can only increase if another working set is actually selected in the step following planning-ahead. Thus, both steps together have positive gain in any case. In the following we will arbitrarily fix \u03b7 = 0.9. Thus, we will not consider \u03b7\nas a free hyper-parameter of Algorithm 3. It obviously makes sense to provide the working set which was used for planning-ahead as a candidate to the working set selection algorithm. As explained above, this property together with the usage of the SMO gain function g instead of the approximation g\u0303 ensures positive gain of the doublestep. Of course, positive gain is not sufficient to show the convergence to an optimal point. The following section is devoted to the convergence proof.\nAlthough planning-ahead is done in constant time, it takes considerably longer than the computation of the Newton step. For simple problems where planning-ahead does not play a role because most steps end up at the box bounds the unsuccessful planning steps can unnecessarily slow down the algorithm. As discussed in section 3 this is mainly the case at the beginning of the optimization. We introduce the following simple heuristic: If the previous iteration was a free SMO step, then we perform planning ahead, otherwise we perform another SMO step. Thus, we use the previous SMO step as a predictor for the current one. Algorithm 4 captures this idea.\nAlgorithm 4: Modification of step 2 of the SMO Algorithm. The only difference compared to Algorithm 2 is the first condition that the SMO step must be free.\nif previous iteration performed a free SMO step then\nCompute the planning-ahead step size \u00b5 = Q22w1\u2212Q12w2det(Q) (eq. (8)) assuming B(t\u22121) as the next working set if the current or the planned step ends at the box boundary then\nperform a SMO step (eq. (2)) else\nperform the step of size \u00b5 as planned end\nelse perform a SMO step (eq. (2)) end\nThe SMO algorithm with modified steps 1 and 2 as defined in Algorithms 4 and 3, respectively, will be referred to as the planning-ahead SMO (PASMO) algorithm in the following. For completeness, we state the complete PA-SMO algorithm at the end of the paper.\nIt is not really clear whether the consideration of the working set B(t\u22121) for planning ahead is a good choice. In fact, it would be good to know whether this choice has a critical impact on the performance. To evaluate this impact we need to introduce a variant of PA-SMO. Because the planning-step takes only constant time we could afford to perform N > 1 planning steps with the working sets B(t\u2212n) for 1 \u2264 n \u2264 N and choose the step size with the largest double-step gain. In this case we should also provide these sets to the working set selection algorithm as additional candidates. We call this variant the multiple planning-ahead algorithm using the N > 1 most recent working sets."}, {"heading": "6 Convergence of the Method", "text": "In this section we will show that the PA-SMO algorithm converges to the optimum f\u2217 of problem (1). First we introduce some notation and make some definitions.\nDefinition 1. We say that a function s : R\\R\u2217 \u2192 R has property (\u2217) if it is positive and lower semi-continuous. We extend this definition to functions s : R \u2192 R which are positive and lower semi-continuous on R \\ R\u2217. We say that a function h has property (\u2217\u2217) if there exists a function s with property (\u2217) which is a lower bound for h on R \\R\u2217.\nRecall two important properties of a lower semi-continuous function s: First, if s(\u03b1) > 0 then there exists an open neighborhood U of \u03b1 such that s(\u03b1) > 0 for all \u03b1 \u2208 U . Second, a lower semi-continuous function attains its minimum on a (non-empty) compact set. The gap function\n\u03c8(\u03b1) = max { vTB\u2207f(\u03b1) \u2223\u2223\u2223B \u2208 B(\u03b1) } \u2208 R\nwill play a central role in the following. Note that this function is used in the stopping condition in step 4 of Algorithm 1 because it is positive on R \\R\u2217 and zero or negative on R\u2217.\nLemma 1. The function \u03c8 has property (\u2217).\nProof. On R we introduce the equivalence relation \u03b11 \u223c \u03b12 \u21d4 B(\u03b11) = B(\u03b12) and split the feasible region into equivalence classes, denoted by [\u03b1]. Obviously, \u03c8 is continuous on each equivalence class [\u03b1]. Now, the topological boundary \u2202[\u03b1] of a class [\u03b1] is the union of those classes [\u03b1\u2032] with B([\u03b1\u2032]) \u2282 B([\u03b1]). Because the argument of the maximum operation in the definition of \u03c8 is a subset of B([\u03b1]) on the boundary the maximum can only drop down. Thus, \u03c8 is lower semi-continuous. Further, it is well known that \u03c8 is positive for non-optimal points.\nWe collect all possible working sets in\nB = { (i, j) \u2223\u2223\u2223 i, j \u2208 {1, . . . , \u2113} and i 6= j }\nand write the working set selection (3) as a map W : R \u2192 B. With this fixed working set selection we consider the Newton step gain\ng\u0303W : R \u2192 R \u22650 \u222a {\u221e}, g\u0303W (\u03b1) = g\u0303W (\u03b1)(\u03b1)\nas a function of \u03b1, in contrast to the family of functions with variable working set defined in eq. (4).\nLemma 2. There exists \u03c3 > 0 such that the function \u03d5(\u03b1) = \u03c3 (\u03c8(\u03b1))2 is a lower bound for g\u0303W on R \\ R \u2217. Thus, the Newton step gain g\u0303W has property (\u2217\u2217).\nProof. Of course, \u03d5 inherits property (\u2217) from \u03c8. We split R\\R\u2217 = M \u222aN into disjoint subsets\nM = { \u03b1 \u2208 R \\R\u2217 \u2223\u2223\u2223 vW (\u03b1) 6\u2208 ker(K) } N = { \u03b1 \u2208 R \\R\u2217 \u2223\u2223\u2223 vW (\u03b1) \u2208 ker(K) }\nand introduce the constants\n\u03c31 = max { vTBKvB \u2223\u2223\u2223 B \u2208 B }\n\u03c32 = min { vTBKvB \u2223\u2223\u2223 B \u2208 B with vTBKvB > 0 } .\nOn M we have vTW (\u03b1)KvW (\u03b1) > 0. We proceed in two steps. First we\ndefine the gap of the working set W (\u03b1)\n\u03c8W : R \u2192 R, \u03b1 7\u2192 v T W (\u03b1)\u2207f(\u03b1) .\nThen we make use of a result from [2], where the bound \u03c8W (\u03b1) \u2265 \u221a\n\u03c32/\u03c31 \u03c8(\u03b1) is derived in section 3. From the definition of g\u0303B(\u03b1) applied to the working set B(\u03b1) we get the inequality\ng\u0303W (\u03b1) = 1\n2\n(vTW (\u03b1)\u2207f(\u03b1)) 2\nvTW (\u03b1)KvW (\u03b1) \u2265\n1\n2\u03c31 (\u03c8W (\u03b1))\n2\nresulting in the desired lower bound with\n\u03c3 = 1\n2\u03c31 (\u221a \u03c32 \u03c31 )2 = \u03c32 2\u03c321 > 0 .\nOn N the situation is much simpler. For vTW (\u03b1)\u2207f(\u03b1) = 0 we can not make any progress on the working set W (\u03b1) which contradicts \u03b1 6\u2208 R\u2217. Thus we have vTW (\u03b1)\u2207f(\u03b1) 6= 0 which implies g\u0303W (\u03b1) = \u221e. This is because the quadratic term of the objective function in direction vW (\u03b1) vanishes and the function increases linearly. Of course we then have g\u0303W (\u03b1) \u2265 \u03c3(\u03c8(\u03b1)) 2 for \u03b1 \u2208 N .\nIn contrast to [2] there is no need to use an artificial lower bound \u03c4 > 0 for vanishing quadratic terms in this proof. With the properties of \u03d5 at hand it is straight forward to prove the following theorem:\nTheorem 1. Consider the sequence (\u03b1(t))t\u2208N in R with (f(\u03b1 (t))t\u2208N) monotonically increasing. Let there exist a constant c > 0 and an infinite set Tc \u2282 N such that the steps from \u03b1 (t\u22121) to \u03b1(t) have the property\nf(\u03b1(t))\u2212 f(\u03b1(t\u22121)) \u2265 c \u00b7 g\u0303W (\u03b1 (t\u22121))\nfor all t \u2208 Tc. Then we have lim t\u2192\u221e\nf(\u03b1(t)) = f\u2217.\nProof. Because of the compactness of R there exists a convergent subsequence (\u03b1(t\u22121))t\u2208T\u0303 for some T\u0303 \u2282 Tc. We will denote its limit point by \u03b1(\u221e). Assume the limit point is not optimal. Then \u03d5(\u03b1(\u221e)) > 0 by property (\u2217). The lower semi-continuity of \u03d5 implies \u03d5(\u03b1) > 0 for all \u03b1 in an open neighborhood U \u2032 of \u03b1(\u221e). We choose a smaller open neighborhood U of \u03b1(\u221e) such that its closure U is contained in U \u2032. Again by lower semi-continuity \u03d5 attains its minimum m > 0 on U . There is t0 such that \u03b1\n(t) \u2208 U for all t \u2208 T\u0303 with t > t0. Then we have\nf(\u03b1(\u221e)) \u2265 f(\u03b1(t0)) + \u2211\nt\u2208T\u0303 ,t>t0\nf(\u03b1(t))\u2212 f(\u03b1(t\u22121))\n\u2265 f(\u03b1(t0)) + \u2211\nt\u2208T\u0303 ,t>t0\nc \u00b7 g\u0303(\u03b1(t\u22121))\n\u2265 f(\u03b1(t0)) + \u2211\nt\u2208T\u0303 ,t>t0\nc \u00b7 \u03d5(\u03b1(t\u22121))\n\u2265 f(\u03b1(t0)) + \u2211\nt\u2208T\u0303 ,t>t0\nc \u00b7m = \u221e > f\u2217\nwhich is a contradiction. Thus, \u03b1(\u221e) is optimal.\nWith the additional assumption that infinitely many SMO steps end up free we can use Theorem 1 to show the convergence of Algorithm 1 to an optimal solution. This was already proven in [2] without this assumption.\nCorollary 1. Consider the sequence (\u03b1(t))t\u2208N in R with (f(\u03b1 (t)))t\u2208N monotonically increasing. If there are infinitely many t such that the step from \u03b1(t\u22121) to \u03b1(t) is a free SMO step with working set (3) then we have lim\nt\u2192\u221e f(\u03b1(t)) =\nf\u2217.\nProof. For a free SMO step we have f(\u03b1(t)) \u2212 f(\u03b1(t\u22121)) = g\u0303(\u03b1(t\u22121)). Thus we can simply apply the above theorem with c = 1.\nThe allurement of this approach is that we do not need any assumption on the steps which differ from free SMO steps as long as the objective function does not decrease. This is an ideal prerequisite to tackle the convergence of hybrid algorithms which need to distinguish qualitatively different branches, like for example the PA-SMO algorithm. Consequently, the following lemma will be helpful when applying the above results to PA-SMO.\nLemma 3. Consider two iterations t and t + 1 of the PA-SMO algorithm where planning-ahead is active in iteration t. The double-step gain g2-step = f(\u03b1(t+1))\u2212 f(\u03b1(t\u22121)) is then lower bounded by (1\u2212 \u03b72) \u00b7 g\u0303W (\u03b1 (t\u22121)).\nProof. Let \u00b5\u2217 denote the Newton step size in iteration t and let g\u0303\u2217 = g\u0303W (\u03b1\n(t\u22121)) be the gain achieved by this (possibly infeasible) step. Just like in Algorithm 3 we distinguish two cases:\n1. The step size \u00b5(t) satisfies 1\u2212 \u03b7 \u2264 \u00b5(t)/\u00b5\u2217 \u2264 1 + \u03b7: We write the gain in iteration t in the form ( 2\u00b5(t)/\u00b5\u2217\u2212 (\u00b5(t)/\u00b5\u2217)2 ) \u00b7 g\u0303\u2217,\nsee Figure 2. Together with the strict increase of the objective function in iteration t+ 1 we get g2-step \u2265 (1\u2212 \u03b72) \u00b7 g\u0303\u2217.\n2. The step size \u00b5(t) satisfies \u00b5(t)/\u00b5\u2217 6\u2208 [1\u2212 \u03b7, 1 + \u03b7]: By construction the planned ahead gain (7) is lower bounded by g\u0303\u2217\n(see Section 4). The planning-step assumes that the working set B(t\u22121) is selected in iteration t+ 1. However, another working set may actually be chosen. Because Algorithm 3 uses the SMO gain gB(\u03b1\n(t)) for working set selection the gain may only improve due to the choice of B(t+1) 6= B(t\u22121). Therefore g2-step is even lower bounded by g\u0303\u2217. With 1\u2212 \u03b72 \u2264 1 the desired bound follows.\nThe first case seems to complicate things unnecessarily. Further, it reduces the progress by a factor of 1 \u2212 \u03b72. We could simply skip the second if-condition in Algorithm 3 and in all cases turn to the else-part. From a purely mathematical point of view this is clearly true. However, the usage of the exact gain gB(\u03b1) instead of g\u0303B(\u03b1) is an unfavorable choice for working set selection in practice. For performance reasons we want to allow the algorithm to use the working set selection objective g\u0303B(\u03b1) as often as possible. Thus we have to cover case 1 in Lemma 3, too.\nTheorem 2. Let \u03b1(t) denote the sequence of feasible points produced by the PA-SMO algorithm starting from \u03b1(0) and working at perfect accuracy \u03b5 = 0. Then the algorithm either stops in finite time with an optimal solution or produces an infinite sequence with lim\nt\u2192\u221e f(\u03b1(t)) = f\u2217.\nProof. Because the algorithm checks the exact KKT conditions the finite stopping case is trivial. For the infinite case we distinguish two cases. If the sequence contains only finitely many steps which are planning ahead then there exists t0 > 0 such that in all iterations t > t0 the algorithm coincides with Algorithm 1 and the convergence proof given in [2] holds. Otherwise there exists an infinite sequence (tn)n\u2208N of planning steps. Now there are at least two possibilities to apply the above results. An easy one is as follows: From Lemma 3 we obtain a constant c = 1\u2212\u03b72 such that Theorem 1 implies the desired property. Alternatively we can argue that the double-step gain is non-negative by Lemma 3. Algorithm 4 ensures that the SMO steps in iterations tn \u2212 1, n \u2208 N just before the planning-ahead steps are free. Then we can apply Corollary 1. However, the second variant of the proof does not hold if we replace Algorithm 4 by Algorithm 2.\nAs already noted above Theorem 1 and Corollary 1 resolve the separate handling of different cases by the algorithm in a general manner. In the case of an infinite sequence of planning-ahead steps the proof does not consider the other iterations at all. This technique is similar to the convergence proof for the Hybrid Maximum-Gain second order algorithm presented in [5] which needs to cover different cases to ensure convergence, too."}, {"heading": "7 Experiments", "text": "The main emphasis of the experiments is to compare the PA-SMO algorithm with the standard (greedy) SMO algorithm. The most recent LIBSVM\nversion 2.84 implements Algorithm 1. For comparison, we implemented the modifications described in Algorithm 3 and Algorithm 4 directly into LIBSVM.\nNote that in the first iteration starting from \u03b1(0) = (0, . . . , 0)T the components yi = \u00b11 of the gradient \u2207f(\u03b1\n(0)) = y take only two possible values. The absolute values of these components are equal and they all point into the box. Therefore the working set selection algorithm could select any i(1) \u2208 Iup(\u03b1\n(0)) as the first index, because the gradient components of all indices are maximal. Thus, there is a freedom of choice for the first iteration. LIBSVM arbitrarily chooses i(1) = max(Iup(\u03b1\n(0))). Of course, this choice influences the path taken by the optimization. Experiments indicate that this choice can have a significant impact on the number of iterations and the runtime of the algorithm. Now, on a fixed dataset, an algorithm may appear to be superior to another one just because it is lucky to profit more from the asymmetry than the competitor. To reduce random effects, we created 100 random permutations of each dataset. All measurements reported are mean values over these 100 permutations. Because the permutations were drawn i.i.d. we can apply standard significance tests to our measurements.\nWe collected a set of 22 datasets for the performance comparison. For the 13 benchmark datasets from [14] we merged training and test sets. The artificial chess-board problem [4] was considered because it corresponds to quadratic programs which are very difficult to solve for SMO-type decomposition algorithms. Because this problem is described by a known distribution, we are in the position to sample datasets of any size from it. We arbitrarily fixed three datasets consisting of 1, 000, 10, 000, and 100, 000 examples. Six more datasets were taken from the UCI benchmark repository [11]: The datasets connect-4, king-rook-vs-king, and tic-tac-toe are extracted from games, while ionosphere, spambase, and internet-ads stem from real world applications.\nIn all experiments we use the Gaussian kernel\nk(xi, xj) = exp(\u2212\u03b3 \u2016xi \u2212 xj\u2016 2)\nwith the single kernel parameter \u03b3 > 0. The complexity control parameter C and the kernel parameter \u03b3 were selected with grid search on the crossvalidation error to ensure that the parameters are in a regime where the resulting classifiers generalize reasonably well, see Table 1. All experiments were carried out on a Xeon 3 GHz CPU running Fedora Linux."}, {"heading": "7.1 Results", "text": "We performed 100 runs (corresponding to the 100 permutations) per dataset for both algorithms and measured the runtime and the number of iterations. The results are summarized in Table 2.\nThere is a clear trend in these results. For some datasets the PA-SMO algorithm significantly outperforms the SMO algorithm, while for other datasets there is no significant difference. Most important, PA-SMO performs in no case worse than standard SMO.\nThe number of iterations is significantly reduced in nearly all cases. This result is not surprising. It basically means that the algorithm works as ex-\npected. However, early iterations working on the whole problem take much longer than late iterations after shrinking has more or less identified the interesting variables. Therefore it is natural that the number of iterations is only a weak indicator for the runtime. The runtime of the PA-SMO algorithm is usually slightly reduced in the mean. This difference is significant in 5 cases. However, the striking argument for the algorithm is that it never performs worse than the standard SMO algorithm.\nAlthough both algorithms use the same stopping condition the dual objective values achieved slightly varies. A careful check of these values reveals that the PA-SMO algorithm consistently achieves better solutions (paired Wilcoxon rank sum test, p = 0.05) for all datasets but chess-board-100000. Thus, the speed up is not a trivial effect of reduced solution quality. The tests reveal that the contrary is the case, that is, the new algorithm outputs better solutions in less time."}, {"heading": "7.2 Influence of Planning-Ahead vs. Working Set Selection", "text": "It is interesting to look a little bit behind the scenes. Recall that we changed two parts of the SMO algorithm. The truncated Newton step was replaced by the planning-ahead Algorithm 4 and the working set selection was modified accordingly by Algorithm 3. It is possible to use the second modification without the first one, but hardly vice versa. Therefore, we ran the SMO algorithm with the modified working set selection but without planning ahead to get a grip on the influence of these changes on the overall performance. That is, we made sure that the algorithm selects the working set used two iterations ago if it is a feasible direction and maximizes the Newton step gain g\u0303. While the results of the comparison to standard SMO were completely ambiguous, the PA-SMO algorithm turned out to be clearly superior. Thus, the reason for the speed up of PA-SMO is not the changed working set selection, but planning-ahead."}, {"heading": "7.3 Planning-Ahead Step Sizes", "text": "To understand how planning-ahead is really used by the algorithm we measured the quantity \u00b5/\u00b5\u2217 \u2212 1, that is, the size of the planning-ahead step relative to the Newton step. For free SMO steps this quantity is always 0, for larger steps it is positive, for smaller steps negative, and for steps in the opposite direction it is even smaller than\u22121. We present some representative histograms in Figure 3. These histograms reveal that most planning-steps are only slightly increased compared to the Newton step size, but there are cases where the algorithm chooses a step which is enlarged by a factor of several thousands. However, very few steps are reduced or even reversed, if any.\nObviously the step size histograms are far from symmetric. Therefore it is natural to ask whether a very simple increase of the Newton step size can be a good strategy. By heretically using\n\u00b5(t) = max { min { 1.1 \u00b7\nlt Qtt , U\u0303t\n} , L\u0303t }\ninstead of equation (2) we still achieve 1 \u2212 0.12 = 99% of the SMO gain in each iteration (see Figure 2) and avoid the drawback of the more complicated computations involved when planning-ahead. Further, this strategy can be implemented into an existing SMO solver in just a few seconds. Experiments indicate that it is surprisingly successful, no matter if the original working set selection or Algorithm 3 is used. For most simple problems it performs as good as the much more refined PA-SMO strategy. However, for the extremely difficult chess-board problem this strategy performs significantly worse."}, {"heading": "7.4 Multiple Planning-Ahead", "text": "We now turn to the variant of the PA-SMO algorithm which uses more than one recent working set for planning-ahead. This variant, as explained at the\nend of section 5, plans ahead with multiple candidate working sets. Further, these working sets are additional candidates for the working set selection. We can expect that the number of iterations decreases the more working sets are used this way. However, the computations per iteration of course increase, such that too many working sets will slow the entire algorithm down. Thus, there is a trade-off between the number of iterations and the time needed per iterations. Now the interesting question is whether there is a uniform best number of working sets for all problems.\nWe performed experiments with the 2, 3, 5, 10, and 20 most recent working sets. It turned out that the strategies considering the most recent two or three working sets perform comparable to standard PA-SMO, and even slightly better. For 5, and more drastically, for 10 or 20 working set evaluations the performance drops, see Figure 4. This result makes clear that we do not lose much when completely ignoring the multiple working set selection strategy, and at the same time we stay at the safe side. Therefore it seems reasonable to stick to the standard form of the PA-SMO algorithm. On the other hand we can get another small improvement if the two or three most recent working sets are considered."}, {"heading": "8 Conclusion", "text": "We presented the planning-ahead SMO algorithm (PA-SMO), which is a simple yet powerful improvement of SMO. At a first glance it is surprising that the truncated Newton step used in all existing variants of the SMO algorithm can be outperformed. This becomes clear from the greedy character of decomposition iterations. The experimental evaluation clearly shows the benefits of the new algorithm. As we never observed a decrease in performance, we recommend PA-SMO as the default algorithm for SVM training. PA-SMO is easy to implement based on existing SMO solvers. Due to the guaranteed convergence of the algorithm to an optimal solution the method is widely applicable. Further, the convergence proof introduces a general technique to address the convergence of hybrid algorithms."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "The sequential minimal optimization (SMO) algorithm and variants<lb>thereof are the de facto standard method for solving large quadratic<lb>programs for support vector machine (SVM) training. In this paper<lb>we propose a simple yet powerful modification. The main emphasis<lb>is on an algorithm improving the SMO step size by planning-ahead.<lb>The theoretical analysis ensures its convergence to the optimum. Ex-<lb>periments involving a large number of datasets were carried out to<lb>demonstrate the superiority of the new algorithm.", "creator": "LaTeX with hyperref package"}}}