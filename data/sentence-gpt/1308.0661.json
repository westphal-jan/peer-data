{"id": "1308.0661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2013", "title": "A Comparison of Named Entity Recognition Tools Applied to Biographical Texts", "abstract": "Named entity recognition (NER) is a popular domain of natural language processing. For this reason, many tools exist to perform this task. Amongst other points, they differ in the processing method they rely upon, the entity types they can detect, the nature of the text they can handle, and their input/output formats. This makes it difficult for a user to select an appropriate NER tool for a specific situation. In this article, we try to answer this question in the context of biographic texts. For this matter, we first constitute a new corpus by annotating Wikipedia articles. We then select publicly available, well known and free for research NER tools for comparison: Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply them to our corpus, assess their performances and compare them. When considering overall performances, a clear hierarchy emerges: Stanford has the best results, followed by LingPipe, Illionois and OpenCalais. However, a more detailed evaluation performed relatively to entity types and article categories highlights the fact their performances are diversely influenced by those factors. This complementarity opens an interesting perspective regarding the combination of these individual tools in order to improve performance.", "histories": [["v1", "Sat, 3 Aug 2013 05:57:48 GMT  (245kb)", "http://arxiv.org/abs/1308.0661v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["samet atda\\u{g}", "vincent labatut"], "accepted": false, "id": "1308.0661"}, "pdf": {"name": "1308.0661.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["samet2@gmail.com,", "vlabatut@gsu.edu.tr)."], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIdentifying and categorizing strings of text into different classes is a process defined as named entity recognition (NER) [1]. Strictly speaking, a named entity is a group of consecutive words found in a sentence, and representing concepts such as persons, locations, organizations, objects, etc. For instance, in the sentence \u201cMartin Schulz of Germany has been the president of the European Parliament since January 2012\u201d, \u201cMartin Schulz\u201d, \u201cGermany\u201d and \u201cEuropean Parliament\u201d are person, location and organization entities, respectively. Note there is no real consensus on the various types of entities. Although those are not exactly named entities, NER tools sometimes also handle numeric entities such as amounts of money, distances, or percentages, and hybrid entities such as dates (e.g. \u201cJanuary 2012\u201d in the previous sentence).\nRecognizing and extracting such data is a fundamental task and a core process of the natural language processing field (NLP), mainly for two reasons. First, NER is used directly in many applied research domains [1]. For instance, proteins and genes can be considered as named entities, and many works in medicine focus on the analysis of scientific articles to find out hidden relationships between them, and drive experimental research [2]. But NER is also used as a preprocessing step by more advanced NLP tools, such as relationship or information extraction [3]. As a result, a number of tools have been developed to perform NER.\nS. Atdag and V. Labatut are with the Department of Computer Science of the Galatasaray University, Istanbul, 34349, Turkey (e-mail: samet2@gmail.com, vlabatut@gsu.edu.tr).\nNER tools differ in many ways. First, the methods they rely upon range from completely manually specified systems (e.g. grammar rules) to fully automatic machine-learning processes, not to mention hybrids approaches combining both. Second, they do not necessarily handle the same classes of entities. Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8]. Fourth, some are implemented as libraries [6], and some take various other forms such as Web services [9]. Fifth, the data outputted by NER tools can take various forms, usually programmatic objects for libraries and text files for the others. There is no standard for files containing NERprocessed text, so output files can vary a lot from one tool to the other. Sixth, tools reach different levels of performance. Moreover, their accuracy can vary depending on the considered type of entity, class of text, etc.\nBecause of all these differences, comparing existing NER tools in order to identify the more suitable to a specific application is a very difficult task. And it is made even harder by two other factors. First, most tools require the user to specify certain configuration settings, like choosing a dictionary. This leads to a large number of possible combinations, each one potentially corresponding to very different behaviors and performances. Second, in order to perform a reliable assessment, one needs an appropriate corpus of annotated texts. This directly depends on the nature of the application domain, and on the types of entities targeted by the user. It is not always possible to find such a dataset, and if none exist, then it must be designed manually, which is a long and difficult operation.\nThe work we present here constitutes a preliminary step in a larger research project, consisting in extracting spatiotemporal events from biographical texts. For this purpose, we need first to select an efficient NER tool. As mentioned earlier, there is no entity-annotated corpus for this usage, and the comparison of NER tools is difficult. To solve this problem, we constituted an appropriate corpus based on a selection of Wikipedia pages and developed a platform automating the comparison of NER tools. It includes a variant of classic performance measures we proposed to best fit our needs. Both the corpus and tool are publicly available under open licenses1. We then used our platform to compare the most popular free NER tools, and discuss their results.\nThe rest of this article is organized as follows. In the next section, we briefly review the selected NER tools. In section III we present the methods we used to evaluate their performance. We propose a new set of measures allowing to take partial matches into account. In section IV, we describe the corpus we created for this work and compare it to the\n1 http://bit.gsu.edu.tr/compnet\nexisting ones. We then present and discuss, in section V, the performances obtained by the NER tools on these data. We conclude by highlighting the main points of our work, and discuss how it can be extended."}, {"heading": "II. EXISTING NER TOOLS", "text": "As mentioned before, many methods and tools were designed for named entity recognition. It is not possible to list them all here, but one can distinguish three main families [10]: hand-made rule-based methods, machine learning-based methods, and hybrid methods. The first use manually constructed finite state patterns [11]; the second treat NER as a classification process [10], and the third are a mix of those two approaches. We used three criteria for selecting appropriate NER tools. First, it must be publicly and freely available. Second, we favor proven tools, already wellestablished in the NER community. Last, due to our goal of finally identifying spatiotemporal events, we focused on tools able to handle at least Person, Location and Organization entities.\nIn the end, we selected four different tools. Stanford Named Entity Recognizer (SNER) [6] is based on linear chain conditional random fields. Illinois Named Entity Tagger (INET) [4] relies on several supervised learning methods: hidden Markov models, multilayered neural networks and other statistical methods. Alias-i LingPipe (LIPI) [12] uses \ud835\udc5b- gram character language models, trained through hidden Markov models and conditional random field methods. Several pre-trained models for the English language are provided with these three tools. Moreover, they all take the form of Java applications. The last selected tool differs in this point, since OpenCalais (OCWS) [9] is a Web service. Both LingPipe and OpenCalais are general tools, able to handle various other NLP tasks besides NRE. Moreover, both are commercial tools, but free licenses are available for academic use."}, {"heading": "III. EVALUATION METHODS", "text": "For a given text, the output of a NER tool is a list of entities and their associated types, and the ground truth takes the exact same form. In order to assess the tool performance, one basically wants to compare both lists. Different approaches can be used for this purpose, depending on the goal and context [1]. In this section, we first review the traditional approach, and then propose a variant adapted to our own context."}, {"heading": "A. Traditional Evaluation", "text": "The traditional evaluation relies on a set of counts classically used in classification: True Positive (TP), False Positive (FP) and False Negative (FN) counts. Those are used to process two distinct measures: Precision and Recall [13]. Precision is defined as the ratio \ud835\udc47\ud835\udc43 (\ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc43)\u2044 . It corresponds to the proportion of detected entities which are correct. Recall is defined as \ud835\udc47\ud835\udc43 (\ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc41)\u2044 . It is the proportion of real entities which were correctly detected. Both measures are complementary, in the sense they are related to type I (false alarm) and type II (miss) errors, respectively.\nComparing the estimated and actual lists of entities can be performed according to two distinct axes: spatial (position of the entities in the text) and typical (types of the entities). In\nterms of spatial performance, a TP is an actual entity whose position was correctly identified by the tool. A FP refers to an expression considered by the tool as an entity, but which does not appear as such in the ground truth. A FN is an actual entity the tool was not able to detect. Figure 1 presents an example of text extracted from Wikipedia and annotated. It contains 10 actual entities represented in boxes, and 9 estimated ones characterized by wavy underlines. In terms of exact matches, there are 5 TP (Victor Charles Goldbloom, Montreal, Selwyn House, McGill University, New York), 4 FP (Canada, MD, Dr.Goldbloom, Medical Center) and 5 FN (Alton Goldbloom, Annie Ballon, Lower Canada College, Goldbloom, Columbia Presbyterian Medical Center). This leads to a Precision of 0.56 and a Recall of 0.50.\nThe interpretation of the counts is different when assessing the typical performance. TP correspond to entities whose type was correctly estimated. Due to the NER process, they consequently also correspond to entities whose position was identified at least partially correctly. FP are expressions considered by the tool as entities, but whose type was incorrectly selected, or which are not actual entities. FN are actual entities for which the tool selected the wrong type, or no type at all [1]. As an example, Table I contains the types of the entities from Figure 1. We count 7 TP (rows 1, 2, 5, 7, 9, 10 and 11 in Table I), 2 FP (rows 6 and 8) and 3 FN (rows 3, 4 and 6). Based on these counts, we get a Precision of 0.78 ands a Recall of 0.70.\nRecall and Precision can then be combined, for example using the F-Measure, in order to get a single score. Some authors even combine spatial and typical performances to get a single overall, somewhat easier to interpret, value. One of our goals with this work is to characterize the behavior of NER tools on biographical texts. To our opinion, combining the various aspects of the tool performance will result in a loss of very relevant information. To avoid this, we want to keep separated measures for space and types. For types, we\ndecided to process Precision and Recall independently for each type. This allows assessing if the performance of a tool varies depending on the entity type. For instance, let us focus on Person entities from Table I. We count 2 TP (rows 1 and 9), 1 FP (row 8) and 2 FN (rows 3 and 4). For this specific type, we therefore get a Precision of 0.67 and a Recall of 0.50. For the spatial performance, we want to clearly distinguish partial and full matches. For this matter, in the next subsection we define variants of the traditional measures."}, {"heading": "B. Considering Partial Matches", "text": "The traditional approach used to assess spatial performance requires a complete match in order to count a TP: the boundaries of the estimated and actual entities must be exactly the same. However, in practice it is also possible to obtain partial matches [1], i.e. an estimated entity which intersects with an actual entity, but whose boundaries do not perfectly match. For example, in Figure 1 Lower Canada College is an actual entity, but the estimation only includes the word Canada. A partial match represents a significant piece of information: the NER tool detected something, even if it was not exactly the expected entity. Completely ignoring this fact seems a bit too strict to us. Moreover, in a later stage of our project, we will aim at developing a method to efficiently combine the findings of several NER tools, in order to improve the overall performance. From this perspective, it is important to consider the information represented by partial matches, and this is why we present an extension of the existing measures.\nFor this purpose, we propose alternative counts one can substitute to the previously presented ones. First, we need to count the Partial Matches (PM), i.e. the cases where the estimated entity contains only a part of the actual one. We consequently also need to consider the cases where the NER tool totally ignores the actual entity: we call this a Complete Miss (CM). The sum of PM and CM is equal to what was previously called FN. Another situation arises when the detected entities corresponds to no actual entity at all. We call this a Wrong Hit (WH). The sum of PM and WM is equal to FP. Finally, the last relevant case happens when we have a Full Match (FM). It exactly corresponds to a FP, but we decided to use a different name to define a consistent terminology. In the example from Figure 1, we have 5 FM (the entities previously considered as TP), 3 PM (Lower Canada College, Dr.Goldbloom, Columbia Presbyterian Medical Center), 1 WH (MD) and 2 CM (Alton Goldbloom and Annie Ballon).\nWe use our new counts to adapt the Precision and Recall measures. Regarding the numerator, we now have two different possibilities: FM or PM (instead of TP). For the Precision denominator, we need the total number of estimated entities, which amounts to \ud835\udc39\ud835\udc40 + \ud835\udc43\ud835\udc40 + \ud835\udc4a\ud835\udc3b (and not \ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc43 anymore). For the Recall denominator, we use the total number of actual entities, which is \ud835\udc39\ud835\udc40 + \ud835\udc43\ud835\udc40 + \ud835\udc36\ud835\udc40 (and not \ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc41 anymore). We therefore obtain two kinds of Precision, which we coin Full Precision (\ud835\udc39\ud835\udc40 (\ud835\udc39\ud835\udc40 + \ud835\udc43\ud835\udc40 + \ud835\udc4a\ud835\udc3b)\u2044 ) and Partial Precision (\ud835\udc43\ud835\udc40 (\ud835\udc39\ud835\udc40 + \ud835\udc43\ud835\udc40 + \ud835\udc4a\ud835\udc3b)\u2044 ). Similarly, we have two kinds of Recall, called Full Recall (\ud835\udc39\ud835\udc40 (\ud835\udc39\ud835\udc40 + \ud835\udc43\ud835\udc40 + \ud835\udc36\ud835\udc40)\u2044 ) and Partial Recall (\ud835\udc43\ud835\udc40 (\ud835\udc39\ud835\udc40 + \ud835\udc43\ud835\udc40 + \ud835\udc36\ud835\udc40)\u2044 ). Additionally, a Total Precision (resp. Recall) can be obtained by summing\nFull and Partial Precisions (resp. Recalls). In the example of Figure 1, we get \ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc39 = 0.56 and \ud835\udc43\ud835\udc5f\ud835\udc52\ud835\udc43 = 0.33, so the Total Precision is 0.89. For the Recall, we have \ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc39 = 0.50 and \ud835\udc45\ud835\udc52\ud835\udc50\ud835\udc43 = 0.30, resulting in a Total Recall of 0.80."}, {"heading": "IV. DESCRIPTION OF THE CORPUS", "text": "NER requires big amounts of data for both training and testing the tools. Most studies use some standard corpora generally designed for conferences or competitions, whereas some commercial tools are provided with their own data [12]. The New York Times Annotated Corpus [14] is a popular resource, constituted of manually annotated articles published in this journal. However, the access is conditional to the payment of a fee, and we decided to focus on freely available tools in this work. The Message Understanding Conference [15] proposed various corpora for NER. However, not all of them are freely available, and those which are focus on texts very different from biographies (terrorist reports, airplane crashes, etc.). The National Institute of Standards and Technology designed a NER corpus based on newswires [16], but it is not accessible from the web anymore. The Conference on Computational Natural Language Learning constituted NER corpora in 2002 and 2003, the latter in English [17]. However, all the articles are related to news, not biographies, and their access is commercial. In [18], four different corpora were constituted from emails, for the purpose of NER assessment. However, only Person entities were annotated. The Automatic Content Extraction corpora [19] are based on newswires, and their access requires to pay a fee.\nDue to the absence of a corpus meeting our needs and purpose, we designed a new one, specifically to assess NER tools on biographical texts. We first extracted more than 300 biographical articles from Wikipedia. We then cleaned and annotated 247 of them by hand. The corpus contains a total of 21364 annotated Person, Location, Organization and Date entities, as detailed in Table II.\nThe texts concern people from six categories of interest: Politics, Science, Military, Art, Sports, and other activities (medicine, law, etc.). The distribution of articles over categories is given in Table III. Note there are more politicians (from the 19th and 20th centuries) because the final goal of our spatiotemporal event extraction project primarily concerns this population."}, {"heading": "V. RESULTS AND DISCUSSIONS", "text": "We applied the NER tools described in section II on our corpus from section IV, using the measures presented in section III to assess their performance. The values obtained for the measures are displayed in Table IV and Table V, for spatial and typical evaluation, respectively. For NER tools proposing several pretrained models, we present only those\nhaving obtained the best performance. In order to study in details the behavior of the tested NER tools, we processed their performance not only for the whole corpus, but also by entity type and by article category."}, {"heading": "A. Overall Performance", "text": "Let us first consider the overall performances. From a spatial perspective (Table IV), there is a clear hierarchy between the tools. When considering the total measures, i.e. the sum of full and partial measures, SNER comes second for Precision (0.88) and first for Recall (0.93). Moreover, the part of partial matches in these results is very low. LIPI has the third Precision (0.81) and the second Recall (0.89), but the part of partial matches is much higher.\nINET is fourth for Precision (0.79) and third for Recall (0.78), and the share of partial matches are even more important (more than one third of the total performance). Note the fact the balance between full and partial matches changes from one tool to the other shows it is a relevant criterion for performance assessment. We manually examined the texts annotated by INET and found out this high level of partial matches has two main causes. First, many organization names include a location or a person name. INET tends to focus on them, rather than on the larger expression corresponding to the organization name. For example, in the expression Toronto's Consulate General of the Netherlands, INET detects the locations (Toronto and Netherlands). Second, INET has trouble detecting person names which include more than two words.\nAll previous three tools reach very comparable values for both measures. However, this is not the case for OCWS. This tool has the best Precision (0.91) but by far the worst Recall (0.61), with the smallest proportions of partial matches. The unbalance between the two measures means that OCWS is almost always right when detecting an entity, but also that it misses a lot of them.\nWith regards to the overall typical performances (Table V), the same hierarchy emerges between the tools. SNER has the second Precision (0.89) and the first Recall (0.92). It is followed by LIPI with the third Precision (0.82) and second Recall (0.88). INET reaches the fourth Precision (0.80) and third Recall (0.78). These values mean those tools perform relatively well, and are able to appropriately classify most entities. Moreover, their performances are balanced, which is not the case of OCWS. Exactly like for the spatial evaluation, we see OCWS reach the first Precision (0.91) but the last Recall (0.61). In words, on the one hand most of the entities\nit recognizes are correctly classified, but on the other hand it fails to correctly classify almost half the reference entities of the corpus."}, {"heading": "B. Performance by Entity Type", "text": "Let us now comment the performances by entity type. For the spatial assessment, as shown in Table IV, SNER performs above its overall level when dealing with Person and Location entities (especially for the former). However, its performances are under it when it comes to Organizations: full match-based measures decrease, while partial matchbased ones increase. The total measures stay relatively constant, though. An analysis of the annotated texts shows SNER has some difficulties in two cases, which mainly concern organizations. First, it tends to detect a full name followed by its abbreviation, such as in Partido Liberaci\u00f3n Nacional (PLN), as a single entity. Second, it sometimes splits names containing many words. For instance, in the phrase Dr. Isa\u00edas \u00c1lvarez Alfaro, it detects Isa\u00edas and \u00c1lvarez Alfaro as separate names. Finally, although it is less marked than for INET, SNER also sometimes mistakes person or location names in organization names. Regarding the typical performance, Person and Location entities are also slightly better handled: the former in terms of Precision and the latter in terms of Recall.\nConcerning Person entities, the spatial performances of INET are very similar to the overall ones. For locations, the total precision decreases (due to less partial matches), whereas the recall increases (due to more full matches). In other words, INET is better as rejecting incorrect locations. For organizations, the total measures are similar to the overall level, but the share of partial matches is much higher. This means INET does not miss more Organization entities (compared to other types), but it has trouble precisely identifying their limits. In terms of typical performance, INET is clearly better on persons, both in terms of Precision and Recall. For locations, we can make the same observations than for the spatial performance, i.e. lower Precision and higher Recall compared to overall values.\nFor OCWS, compared to the overall results, we get similar values for locations, whereas those obtained for persons are slightly higher, and slightly lower for organizations. For all types, we observe the behavior already noticed at the overall level: Precision is high, comparable to the best other tools, whereas Recall is extremely low. A manual analysis of the annotated texts revealed OCWS has trouble handling acronyms, which mainly represent organizations in our corpus. In terms of typical performance,\nTABLE IV. SPATIAL PERFORMANCE BY ENTITY TYPE AND ARTICLE CATEGORY\nSNER INET OCWS LIPI\nFP PP FR PR FP PP FR PR FP PP FR PR FP PP FR PR Overall 0.78 0.10 0.83 0.10 0.53 0.26 0.52 0.26 0.81 0.10 0.55 0.06 0.64 0.17 0.70 0.19\nType Person 0.87 0.05 0.89 0.05 0.56 0.28 0.54 0.27 0.87 0.07 0.56 0.04 0.79 0.11 0.81 0.12 Location 0.78 0.06 0.89 0.07 0.56 0.13 0.67 0.16 0.80 0.08 0.52 0.05 0.58 0.14 0.75 0.18 Organization 0.66 0.19 0.71 0.20 0.48 0.32 0.43 0.29 0.74 0.14 0.54 0.10 0.47 0.29 0.49 0.30\nCategory Art 0.71 0.12 0.77 0.13 0.63 0.17 0.66 0.18 0.77 0.11 0.51 0.08 0.59 0.19 0.60 0.19 Military 0.75 0.15 0.80 0.16 0.50 0.24 0.47 0.23 0.75 0.16 0.44 0.09 0.64 0.20 0.58 0.18 Politics 0.77 0.12 0.80 0.12 0.47 0.33 0.45 0.32 0.80 0.10 0.45 0.06 0.66 0.18 0.63 0.17 Science 0.77 0.13 0.80 0.13 0.61 0.21 0.57 0.20 0.82 0.12 0.46 0.07 0.63 0.20 0.64 0.20 Sports 0.85 0.07 0.85 0.08 0.46 0.26 0.41 0.23 0.92 0.05 0.44 0.03 0.65 0.24 0.60 0.22 Others 0.73 0.15 0.76 0.15 0.57 0.26 0.55 0.25 0.80 0.12 0.49 0.07 0.56 0.25 0.56 0.25\nPerson entities are also more accurately classified, and the tool is slightly better at not misclassifying organizations.\nThe performance of LIPI is much better on persons than overall, for both Precision and Recall: this is true for both spatial and typical measures. For locations, we observe a decrease in Full Precision and an increase in Full Recall, also for both spatial and typical results. Our interpretation is that LIPI detects more incorrect locations, but misses less correct ones. For organizations, there is a clear decrease, in terms of both Precision and Recall, with a larger part of partial matches. This last observation can be explained by the fact LIPI tends to merge consecutive organizations."}, {"heading": "C. Performance by Article Category", "text": "Certain article categories have an effect on the performance of certain tools. When considering SNER, there is no effect for the categories Military, Politics and Science. However, Art and Others lead to slightly lower performances, in terms of both space and types. On the contrary, the spatial performance is much higher than the overall level for Sports (and it is also true of the typical performance, at a lesser degree). This appears to be due to the fact the sport-related biographies generally contain a lot of person names, such as team-mates, opponent, coaches, etc. SNER is particularly good at recognizing person names, which is why its performances are higher for this category. Art-related articles contain many titles of artworks, which are generally confusing for NER tools: they often mistake them for organization names.\nFor INET, we observe a clear spatial performance increase for Art articles, which means it is not concerned by the previous observation. The performance is slightly better for Science and Sports, in the sense the proportion of full matches gets higher for both Precision and Recall (the total performance staying approximately equal). On the contrary, the values are lower for Military, Politics and Sports. One difficulty with military texts is the detection of army units (e.g. 2nd Stryker Cavalry Regiment) as organizations. In terms of typical performance, the differences are strongly marked only for Art and Others, positively, and for Sport, negatively. So in Art articles, INET is better than usual, not only at identifying the limits of entities, but also at classifying them, whereas it is the opposite for Sport.\nIn terms of spatial performance, OCWS is not very sensitive to categories: the observed performances are very similar to the overall ones. The Sports category constitutes an exception though: total Precision stays the same, but the full\nPrecision clearly increases, meaning OCWS is able to detect entities limits more accurately. This is certainly due to the presence of more person names, as already stated for SNER: OCWS gets its best performance on this entity type. The typical performances are more contrasted. The tool is clearly better on Science articles, for which its Recall is almost at the level of the other tools (0.63). On the contrary, the Recall is very low for Art, Others and especially Military (0.05). For the latter, it incorrectly classifies (or fail to detect) almost all the actual entities.\nLike OCWS, the spatial performance of LIPI is not much affected by the article categories. For the Others category though, we observe a behavior opposite to that of OCWS for Sports: total Precision and Recall stay approximately constant, but the part of partial matches increases. It is difficult to interpret this observation, since this category corresponds to heterogeneous article themes. For the typical categories, we observe small variations. The classification is slightly better on Sports and slightly worse on Art."}, {"heading": "D. General Comments", "text": "Several interesting conclusions can be drawn from our results and observations. First, even if the overall performances seem to indicate SNER as the best tool, it is difficult to rank them when considering the detailed performances. This puts in relief the fact single measures might be insufficient to properly assess the quality of NER tools and compare them. The different aspects we considered all proved to be useful to characterize the tools in a relevant way: partial matches, entity types, article categories.\nAs a related point, it turns out NER tools are affected by these factors in different ways. This is also why they are difficult to rank: none of them is the best on every type and category. As a consequence, these tools can be considered as complementary. For instance, if we consider types, then SNER is the best at recognizing persons. OCWS can be trusted when it recognizes locations and organizations, however is prone to missing a lot of them. On the contrary, LIPI is very good at not missing locations, but also incorrectly detect a lot of them. The differences are not as obvious for article categories, but this information can still be useful, e.g. SNER is much reliable when processing Sports articles. A set of voting rules could be manually derived to take advantage of these observations, or an automatic approach could be used, such as the training of a standard classifier, in order to combine outputs of individual NER tools, and increase the performance of the overall NER\nTABLE V. TYPICAL PERFORMANCE RESULTS BY ENTITY TYPE AND ARTICLE CATEGORY\nSNER INET OCWS LIPI\nPrecision Recall Precision Recall Precision Recall Precision Recall Overall 0.88 0.93 0.80 0.78 0.91 0.61 0.82 0.88\nType Person 0.93 0.95 0.84 0.82 0.94 0.68 0.91 0.94 Location 0.85 0.97 0.71 0.85 0.88 0.60 0.73 0.94 Organization 0.85 0.92 0.80 0.74 0.88 0.69 0.76 0.80\nCategory\nArt 0.83 0.92 0.80 0.86 0.90 0.34 0.78 0.81 Military 0.91 0.96 0.81 0.78 0.96 0.05 0.85 0.77 Politics 0.89 0.92 0.80 0.77 0.91 0.53 0.84 0.81 Science 0.90 0.93 0.82 0.80 0.94 0.63 0.83 0.85 Sports 0.93 0.93 0.73 0.64 0.96 0.51 0.89 0.83 Others 0.87 0.91 0.83 0.81 0.92 0.19 0.81 0.80\nsystem."}, {"heading": "VI. CONCLUSION", "text": "In this article, we focus on the problem of selecting an appropriate named entity recognition (NER) tool for biographic texts. Many NER tools exist, most of them based on generic approaches able to handle any kind of text. So, their performances on these specific data need to be compared in order to make a choice. However, existing corpora are not constituted of biographies. For this reason, we designed our own one and applied a selection of publicly available NER tools on it: Stanford NER [6], Illinois NET [4], OpenCalais WS [9] and LingPipe [12]. In order to highlight the importance of partial matches, we evaluated their performance using custom measures allowing to take them into account. Our results show a clear hierarchy between the tested tools: first Stanford NER, then LingPipe, Illinois NET and finally OpenCalais. The latter obtains particularly low Recall scores. When studying the detail of these performances, it turns out they are not uniform over entity types and article categories. Moreover, clear differences exist between tools in this regard. A tool like OpenCalais, which performs apparently much lower than the others (on these data), is still of interest because it can be good on niches, and therefore complete an otherwise better performing tool such as Stanford NER.\nOur contribution includes four points. The first one is the constitution of a biographic corpus. It is based on articles of the English version of Wikipedia. We manually annotated 247 texts to explicitly highlight Person, Organization, Location, and Date entities. The second point is the definition of performance measures allowing to take partial matches into account. For this purpose, we modified the Precision, Recall and F-Measure traditionally used in text mining. The third point is the implementation of a platform allowing to benchmark NER tools. It is general enough to be easily extensible to other NER tools, corpora and performance measures. Our corpus and platform are both freely available online. The last point concerns the application of this platform to the comparison of four popular and publicly available NER tools.\nThis work can be extended in several ways. First, the size of the corpus could be increased, in order to get more significant results. This would also allow using a part of the corpus for training, and therefore obtain classifiers possibly more adapted to process biographies than the general ones we used here. However, article annotation is a very difficult and time-costly task. Second, the benchmark could involve more NER tools, so that the results reflect more completely the possible choices of the end user. Finally, the comparison we conducted here showed individual NER tools perform diversely on bibliographic texts. Their results are influenced by factors such as entity types and article categories. Moreover, they are not affected in the same way: some are better at recognizing locations, others at organizations, etc. Those tools can therefore be considered as complementary. Combining their outputs by giving them more or less importance depending on these factors seems like a promising way of improving the global NER performance. This could be achieved by defining a set of voting rules based on the observations we made during this study, or by training a classifier."}, {"heading": "ACKNOWLEDGMENT", "text": "The first version of our platform was developed by Yasa\nAkbulut, and the first version of our corpus was constituted by Burcu K\u00fcpelio\u011flu. Both tasks were conducted in our research team."}], "references": [{"title": "Semi-Supervised Named Entity Recognition: Learning to Recognize 100 Entity Types with Little Supervision", "author": ["D. Nadeau"], "venue": "Ottawa- Carleton Institute for Computer Science, School of Information Technology and Engineering, University of Ottawa, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "GENETAG: a tagged corpus for gene/protein named entity recognition", "author": ["L. Tanabe", "N. Xie", "L.H. Thom", "W. Matten", "W.J. Wilbur"], "venue": "BMC Bioinformatics, vol. 6, p. S3, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "A Review of Relation Extraction", "author": ["N. Bach", "S. Badaskar"], "venue": "Literature review for Language and Statistics II, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "13th Conference on Computational Natural Language Learning, 2009, pp. 147-155.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Named Entity Work in IMPACT", "author": ["F. Landsbergen"], "venue": "presented at the IMPACT Final Conference 2011, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "43rd Annual Meeting on ACL, 2005, pp. 363-370.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "BANNER: an executable survey of advances in biomedical named entity recognition", "author": ["R. Leaman", "G. Gonzalez"], "venue": "Pacific Symposium on Biocomputing, vol. 13, pp. 652-663, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Assigning geographical scopes to Web pages", "author": ["B. Martins", "M. Chaves", "M.J. Silva"], "venue": "LNCS, vol. 3408, pp. 564-567, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Named Entity Recognition Approaches", "author": ["A. Mansouri", "L. Suriani Affendey", "A. Mamat"], "venue": "International Journal of Computer Science and Network Security, vol. 8, pp. 339-344, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Named entity recognition using an HMMbased chunk tagger", "author": ["G.D. Zhou", "J. Su"], "venue": "40th Annual Meeting on ACL, 2001, pp. 473- 480.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Introduction to the CoNLL-2002 shared task: language-independent named entity recognition", "author": ["E.F.T.K. Sang"], "venue": "6th Conference on Natural language Learning, 2002, pp. 1-4.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "The New York Times Annotated Corpus", "author": ["E. Sandhaus"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Message Understanding Conference- 6: a brief history", "author": ["R. Grishman", "B. Sundheim"], "venue": "16th conference on Computational linguistics, 1996, pp. 466-471.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "The Automatic Content Extraction (ACE) Program: Tasks, Data, and Evaluation", "author": ["G. Doddington", "A. Mitchell", "M. Przybocki", "L. Ramshaw", "S. Strassel", "R. Weischedel"], "venue": "4th International Conference on Language Resources and Evaluation, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to the CoNLL-2003 shared task: language-independent named entity recognition", "author": ["E.F.T.K. Sang", "F. de Meulder"], "venue": "7th conference on Natural Language Learning, 2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Extracting personal names from email: applying named entity recognition to informal text", "author": ["E. Minkov", "R.C. Wang", "W.W. Cohen"], "venue": "HLT/EMNLP, 2005, pp. 443-450.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Identifying and categorizing strings of text into different classes is a process defined as named entity recognition (NER) [1].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "First, NER is used directly in many applied research domains [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "For instance, proteins and genes can be considered as named entities, and many works in medicine focus on the analysis of scientific articles to find out hidden relationships between them, and drive experimental research [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 2, "context": "But NER is also used as a preprocessing step by more advanced NLP tools, such as relationship or information extraction [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 63, "endOffset": 68}, {"referenceID": 4, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 63, "endOffset": 68}, {"referenceID": 5, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 63, "endOffset": 68}, {"referenceID": 6, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 7, "context": "Third, some are generic and can be applied to any type of text [4-6], when others focus only on a specific domain such as biomedicine [7] or geography [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "Fourth, some are implemented as libraries [6], and some take various other forms such as Web services [9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "It is not possible to list them all here, but one can distinguish three main families [10]: hand-made rule-based methods, machine learning-based methods, and hybrid methods.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "The first use manually constructed finite state patterns [11]; the second treat NER as a classification process [10], and the third are a mix of those two approaches.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "The first use manually constructed finite state patterns [11]; the second treat NER as a classification process [10], and the third are a mix of those two approaches.", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "Stanford Named Entity Recognizer (SNER) [6] is based on linear chain conditional random fields.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "Illinois Named Entity Tagger (INET) [4] relies on several supervised learning methods: hidden Markov models, multilayered neural networks and other statistical methods.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Different approaches can be used for this purpose, depending on the goal and context [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "Those are used to process two distinct measures: Precision and Recall [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "FN are actual entities for which the tool selected the wrong type, or no type at all [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "However, in practice it is also possible to obtain partial matches [1], i.", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "The New York Times Annotated Corpus [14] is a popular resource, constituted of manually annotated articles published in this journal.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "The Message Understanding Conference [15] proposed various corpora for NER.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "The National Institute of Standards and Technology designed a NER corpus based on newswires [16], but it is not accessible from the web anymore.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "The Conference on Computational Natural Language Learning constituted NER corpora in 2002 and 2003, the latter in English [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "In [18], four different corpora were constituted from emails, for the purpose of NER assessment.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "For this reason, we designed our own one and applied a selection of publicly available NER tools on it: Stanford NER [6], Illinois NET [4], OpenCalais WS [9] and LingPipe [12].", "startOffset": 117, "endOffset": 120}, {"referenceID": 3, "context": "For this reason, we designed our own one and applied a selection of publicly available NER tools on it: Stanford NER [6], Illinois NET [4], OpenCalais WS [9] and LingPipe [12].", "startOffset": 135, "endOffset": 138}], "year": 2013, "abstractText": "Named entity recognition (NER) is a popular domain of natural language processing. For this reason, many tools exist to perform this task. Amongst other points, they differ in the processing method they rely upon, the entity types they can detect, the nature of the text they can handle, and their input/output formats. This makes it difficult for a user to select an appropriate NER tool for a specific situation. In this article, we try to answer this question in the context of biographic texts. For this matter, we first constitute a new corpus by annotating 247 Wikipedia articles. We then select 4 publicly available, well known and free for research NER tools for comparison: Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply them to our corpus, assess their performances and compare them. When considering overall performances, a clear hierarchy emerges: Stanford has the best results, followed by LingPipe, Illionois and OpenCalais. However, a more detailed evaluation performed relatively to entity types and article categories highlights the fact their performances are diversely influenced by those factors. This complementarity opens an interesting perspective regarding the combination of these individual tools in order to improve performance.", "creator": "Acrobat PDFMaker 11 pour Word"}}}