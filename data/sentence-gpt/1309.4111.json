{"id": "1309.4111", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2013", "title": "Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel", "abstract": "Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposed inspired variations on the algorithm that artificially inflate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the \"star shape\" in the eigenvectors--a common feature of empirical networks--can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models. The authors conclude that the clustering algorithm, for instance, allows us to \"improve statistical performance\" in networks in which the average of the degrees of the covariance can be adjusted to fit all of the clusters in our cluster.", "histories": [["v1", "Mon, 16 Sep 2013 20:47:51 GMT  (75kb,D)", "http://arxiv.org/abs/1309.4111v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["tai qin", "karl rohe"], "accepted": true, "id": "1309.4111"}, "pdf": {"name": "1309.4111.pdf", "metadata": {"source": "CRF", "title": "REGULARIZED SPECTRAL CLUSTERING UNDER THE DEGREE-CORRECTED STOCHASTIC BLOCKMODEL", "authors": [], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Our lives are embedded in networks\u2013social, biological, communication, etc.\u2013 and many researchers wish to analyze these networks to gain a deeper understanding of the underlying mechanisms. Some types of underlying mechanisms generate communities (aka clusters or modularities) in the network. As machine learners, our aim is not merely to devise algorithms for community detection, but also to study the algorithm\u2019s estimation properties, to understand if and when we can make justifiable inferences from the estimated communities to the underlying mechanisms. Spectral clustering is a fast and popular technique for finding communities in networks. Several previous authors have studied the estimation properties of spectral clustering under various statistical network models (McSherry [3], Dasgupta et al. [4], Coja-Oghlan and Lanka [5], Ames and Vavasis [6], Rohe et al. [7], Sussman et al. [8] and Chaudhuri et al. [1]). Recently, Chaudhuri et al. [1] and Amini et al. [2] proposed two inspired ways of artificially inflating the node degrees in ways that provide statistical regularization to spectral clustering.\nThis paper examines the statistical estimation performance of regularized spectral clustering under the Degree-Corrected Stochastic Blockmodel (DC-SBM), an extension of the Stochastic Blockmodel (SBM) that allows for heterogeneous degrees (Holland and Leinhardt [9], Karrer and Newman [10]). The SBM and the DC-SBM are closely related to the planted partition model and the extended planted partition model, respectively. We extend the previous results in the following ways:\n(a) In contrast to previous studies, this paper studies the regularization step with a canonical version of spectral clustering that uses k-means. The results do not require any assumptions on the minimum expected node degree; instead, there is a threshold demonstrating that higher degree nodes are easier to cluster. This threshold is a function of the leverage scores that have proven essential in other contexts, for both graph algorithms and network data analysis (see Mahoney [11] and references therein). These are the first results that relate leverage scores to the statistical performance of spectral clustering.\nResearch of TQ is supported by NSF Grant DMS-0906818 and NIH Grant EY09946. Research of KR is supported by grants from WARF and NSF grant DMS-1309998.\nar X\niv :1\n30 9.\n41 11\nv1 [\nst at\n.M L\n] 1\n6 Se\np 20\n13\n(b) This paper provides more guidance for data analytic issues than previous approaches. First, the results suggest an appropriate range for the regularization parameter. Second, our analysis gives a (statistical) model-based explanation for the \u201cstar-shaped\u201d figure that often appears in empirical eigenvectors. This demonstrates how projecting the rows of the eigenvector matrix onto the unit sphere (an algorithmic step proposed by Ng et al. [12]) removes the ancillary effects of heterogeneous degrees under the DC-SBM. Our results highlight when this step may be unwise.\nPreliminaries: Throughout, we study undirected and unweighted graphs or networks. Define a graph as G(E, V ), where V = {v1, v2, . . . , vN} is the vertex or node set and E is the edge set. We will refer to node vi as node i. E contains a pair (i, j) if there is an edge between node i and j. The edge set can be represented by the adjacency matrix A \u2208 {0, 1}n\u00d7n. Aij = Aji = 1 if (i, j) is in the edge set and Aij = Aji = 0 otherwise. Define the diagonal matrix D and the normalized Graph Laplacian L, both elements of RN\u00d7N , in the following way:\nDii = \u2211 j Aij , L = D \u22121/2AD\u22121/2.\nThe following notations will be used throughout the paper: || \u00b7 || denotes the spectral norm, and || \u00b7 ||F denotes the Frobenius norm. For two sequence of variables {xN} and {yN}, we say xN = \u03c9(yN ) if and only if yN/xN = o(1). \u03b4(.,.) is the indicator function where \u03b4x,y = 1 if x = y and \u03b4x,y = 0 if x 6= y."}, {"heading": "2. The Algorithm: Regularized Spectral Clustering (RSC)", "text": "For a sparse network with strong degree heterogeneity, standard spectral clustering often fails to function properly (Amini et al. [2], Jin [13]). To account for this, Chaudhuri et al. [1] proposed the regularized graph Laplacian that can be defined as\nL\u03c4 = D \u22121/2 \u03c4 AD \u22121/2 \u03c4 \u2208 RN\u00d7N\nwhere D\u03c4 = D + \u03c4I for \u03c4 \u2265 0. The spectral algorithm proposed and studied by Chaudhuri et al. [1] divides the nodes into two random subsets and only uses the induced subgraph on one of those random subsets to compute the spectral decomposition. In this paper, we will study the more traditional version of spectral algorithm that uses the spectral decomposition on the entire matrix (Ng et al. [12]). Define the regularized spectral clustering (RSC) algorithm as follows:\n(1) Given input adjacency matrix A, number of clusters K, and regularizer \u03c4 , calculate the regularized graph Laplacian L\u03c4 . (As discussed later, a good default for \u03c4 is the average node degree.) (2) Find the eigenvectors X1, ..., XK \u2208 RN corresponding to the K largest eigenvalues of L\u03c4 . Form X = [X1, ..., XK ] \u2208 RN\u00d7K by putting the eigenvectors into the columns. (3) Form the matrix X\u2217 \u2208 RN\u00d7K from X by normalizing each of X\u2019s rows to have unit length. That is, project each row of X onto the unit sphere of RK (X\u2217ij = Xij/( \u2211 j X 2 ij) 1/2). (4) Treat each row of X\u2217 as a point in RK , and run k-means with K clusters. This creates K non-overlapping sets V1, ..., VK whose union is V. (5) Output V1, ..., VK . Node i is assigned to cluster r if the i\u2019th row of X \u2217 is assigned to Vr.\nThis paper will refer to \u201cstandard spectral clustering\u201d as the above algorithm with L replacing L\u03c4 . These spectral algorithms have two main steps: 1) find the principal eigenspace of the (regularized) graph Laplacian; 2) determine the clusters in the low dimensional eigenspace. Later, we will study RSC under the Degree-Corrected Stochastic Blockmodel and show rigorously how regularization helps to maintain cluster information in step (a) and why normalizing the rows of X helps\nin step (b). From now on, we use X\u03c4 and X \u2217 \u03c4 instead of X and X \u2217 to emphasize that they are related to L\u03c4 . Let X i \u03c4 and [X \u2217 \u03c4 ] i denote the i\u2019th row of X\u03c4 and X \u2217 \u03c4 .\nThe next section introduces the Degree-Corrected Stochastic Blockmodel and its matrix formulation."}, {"heading": "3. The Degree-Corrected Stochastic Blockmodel (DC-SBM)", "text": "In the Stochastic Blockmodel (SBM), each node belongs to one of K blocks. Each edge corresponds to an independent Bernoulli random variable where the probability of an edge between any two nodes depends only on the block memberships of the two nodes (Holland and Leinhardt [9]). The formal definition is as follows.\nDefinition 3.1. For a node set {1, 2, ..., N}, let z : {1, 2, ..., N} \u2192 {1, 2, ...,K} partition the N nodes into K blocks. So, zi equals the block membership for node i. Let B be a K \u00d7 K matrix where Bab \u2208 [0, 1] for all a, b. Then under the SBM, the probability of an edge between i and j is Pij = Pji = Bzizj for any i, j = 1, 2, ..., n. Given z, all edges are independent.\nOne limitation of the SBM is that it presumes all nodes within the same block have the same expected degree. The Degree-Corrected Stochastic Blockmodel (DC-SBM) (Karrer and Newman [10]) is a generalization of the SBM that adds an additional set of parameters (\u03b8i > 0 for each node i) that control the node degrees. Let B be a K \u00d7K matrix where Bab \u2265 0 for all a, b. Then the probability of an edge between node i and node j is \u03b8i\u03b8jBzizj , where \u03b8i\u03b8jBzizj \u2208 [0, 1] for any i, j = 1, 2, ..., n. Parameters \u03b8i are arbitrary to within a multiplicative constant that is absorbed into B. To make it identifiable, Karrer and Newman [10] suggest imposing the constraint that, within each block, the summation of \u03b8i\u2019s is 1. That is, \u2211 i \u03b8i\u03b4zi,r = 1 for any block label r. Under this constraint, B has explicit meaning: If s 6= t, Bst represents the expected number of links between block s and block t and if s = t, Bst is twice the expected number of links within block s. Throughout the paper, we assume that B is positive definite.\nUnder the DC-SBM, define A , EA. This matrix can be expressed as a product of the matrices, A = \u0398ZBZT\u0398,\nwhere (1) \u0398 \u2208 RN\u00d7N is a diagonal matrix whose ii\u2019th element is \u03b8i and (2) Z \u2208 {0, 1}N\u00d7K is the membership matrix with Zit = 1 if and only if node i belongs to block t (i.e. zi = t).\n3.1. Population Analysis. Under the DC-SBM, if the partition is identifiable, then one should be able to determine the partition from A . This section shows that with the population adjacency matrix A and a proper regularizer \u03c4 , RSC perfectly reconstructs the block partition.\nDefine the diagonal matrix D to contain the expected node degrees, Dii = \u2211\nj Aij and define D\u03c4 = D + \u03c4I where \u03c4 \u2265 0 is the regularizer. Then, define the population graph Laplacian L and the population version of regularized graph Laplacian L\u03c4 , both elements of RN\u00d7N , in the following way:\nL = D\u22121/2A D\u22121/2, L\u03c4 = D \u22121/2 \u03c4 A D \u22121/2 \u03c4 . Define DB \u2208 RK\u00d7K as a diagonal matrix whose (s, s)\u2019th element is [DB]ss = \u2211\ntBst. A couple lines of algebra shows that [DB]ss = Ws is the total expected degrees of nodes from block s and that Dii = \u03b8i[DB]zizi . Using these quantities, the next Lemma gives an explicit form for L\u03c4 as a product of the parameter matrices.\nLemma 3.2. (Explicit form for L\u03c4 ) Under the DC-SBM with K blocks with parameters {B, Z,\u0398}, define \u03b8\u03c4i as:\n\u03b8\u03c4i = \u03b82i\n\u03b8i + \u03c4/Wzi = \u03b8i Dii Dii + \u03c4 .\nLet \u0398\u03c4 \u2208 Rn\u00d7n be a diagonal matrix whose ii\u2019th entry is \u03b8\u03c4i . Define BL = D \u22121/2 B BD \u22121/2 B , then L\u03c4 can be written\nL\u03c4 = D \u2212 1 2 \u03c4 A D \u2212 1 2 \u03c4 = \u0398 1 2 \u03c4 ZBLZ T\u0398 1 2 \u03c4 .\nRecall that A = \u0398ZBZT\u0398. Lemma 3.2 demonstrates that L\u03c4 has a similarly simple form that separates the block-related information (BL) and node specific information (\u0398\u03c4 ). Notice that if \u03c4 = 0, then \u03980 = \u0398 and L = D \u2212 1 2 A D\u2212 1 2 = \u0398 1 2ZBLZ T\u0398 1 2 . The next lemma shows that L\u03c4 has rank K and describes how its eigen-decomposition can be expressed in terms of Z and \u0398.\nLemma 3.3. (Eigen-decomposition for L\u03c4 ) Under the DC-SBM with K blocks and parameters {B, Z,\u0398}, L\u03bb has K positive eigenvalues. The remaining N \u2212 K eigenvalues are zero. Denote the K positive eigenvalues of L\u03c4 as \u03bb1 \u2265 \u03bb2 \u2265 ... \u2265 \u03bbK > 0 and let X\u03c4 \u2208 RN\u00d7K contain the eigenvector corresponding to \u03bbi in its i\u2019th column. Define X \u2217\u03c4 to be the row-normalized version of X\u03c4 , similar to X\u2217\u03c4 as defined in the RSC algorithm in Section 2. Then, there exists an orthogonal matrix U \u2208 RK\u00d7K depending on \u03c4 , such that\n(1) X\u03c4 = \u0398 1 2 \u03c4 Z(ZT\u0398\u03c4Z)\n\u22121/2U (2) X \u2217\u03c4 = ZU , Zi 6= Zj \u21d4 ZiU 6= ZjU , where Zi denote the i\u2019th row of the membership matrix\nZ.\nThis lemma provides four useful facts about the matrices X\u03c4 and X \u2217\u03c4 . First, if two nodes i and j belong to the same block, then the corresponding rows of X\u03c4 (denoted as X i\u03c4 and X j \u03c4 )\nboth point in the same direction, but with different lengths: ||X i\u03c4 ||2 = ( \u03b8\u03c4i\u2211\nj \u03b8 \u03c4 j \u03b4zj ,zi\n)1/2. Second,\nif two nodes i and j belong to different blocks, then X i\u03c4 and X j \u03c4 are orthogonal to each other. Third, if zi = zj then after projecting these points onto the sphere as in X \u2217\u03c4 , the rows are equal: [X \u2217\u03c4 ] i = [X \u2217\u03c4 ] j = Uzi . Finally, if zi 6= zj , then the rows are perpendicular, [X \u2217\u03c4 ]i \u22a5 [X \u2217\u03c4 ]j . Figure 1 illustrates the geometry of X\u03c4 and X \u2217\u03c4 when there are three underlying blocks. Notice that running k-means on the rows of X \u2217\u03bb (in right panel of Figure 1) will return perfect clusters.\nNote that if \u0398 were the identity matrix, then the left panel in Figure 1 would look like the right panel in Figure 1; without degree heterogeneity, there would be no star shape and no need for a projection step. This suggests that the star shaped figure often observed in data analysis stems from the degree heterogeneity in the network."}, {"heading": "4. Regularized Spectral Clustering with the Degree Corrected model", "text": "This section bounds the mis-clustering rate of Regularized Spectral Clustering under the DCSBM. The section proceeds as follows: Theorem 4.1 shows that L\u03c4 is close to L\u03c4 . Theorem 4.2 shows that X\u03c4 is close to X\u03c4 and that X\u2217\u03c4 is close to X \u2217 \u03c4 . Finally, Theorem 4.4 shows that the output from RSC with L\u03c4 is close to the true partition in the DC-SBM (using Lemma 3.3).\nTheorem 4.1. (Concentration of the regularized Graph Laplacian) Let G be a random graph, with independent edges and pr(vi \u223c vj) = pij. Let \u03b4 be the minimum expected degree of G, that is \u03b4 = mini Dii. For any > 0, if \u03b4 + \u03c4 > 3 lnN + 3 ln(4/ ), then with probability at least 1\u2212 ,\n(1) ||L\u03c4 \u2212L\u03c4 || \u2264 4 \u221a 3 ln(4N/ )\n\u03b4 + \u03c4 .\nRemark: This theorem builds on the results of Chung and Radcliffe [14] and Chaudhuri et al. [1] which give a seemingly similar bound on ||L\u2212L || and ||D\u22121\u03c4 A\u2212D\u22121\u03c4 A ||. However, the previous papers require that \u03b4 \u2265 c lnN , where c is some constant. This assumption is not satisfied in a large proportion of sparse empirical networks with heterogeneous degrees. In fact, the regularized graph Laplacian is most interesting when this condition fails, i.e. when there are several nodes with very\nlow degrees. Theorem 4.1 only assumes that \u03b4 + \u03c4 > 3 lnN + 3 ln(4/ ). This is the fundamental reason that RSC works for networks containing some nodes with extremely small degrees. It shows that, by introducing a proper regularizer \u03c4 , ||L\u03c4 \u2212 L\u03c4 || can be well bounded, even with \u03b4 very small. Later we will show that a suitable choice of \u03c4 is the average degree.\nThe next theorem bounds the difference between the empirical and population eigenvectors (and their row normalized versions) in terms of the Frobenius norm.\nTheorem 4.2. Let A be the adjacency matrix generated from the DC-SBM with K blocks and parameters {B, Z,\u0398}. Let \u03bb1 \u2265 \u03bb2 \u2265 ... \u2265 \u03bbK > 0 be the only K positive eigenvalues of L\u03c4 . Let X\u03c4 and X\u03c4 \u2208 RN\u00d7K contain the top K eigenvectors of L\u03c4 and L\u03c4 respectively. Define m = mini{min{||Xi\u03c4 ||2, ||X i\u03c4 ||2}} as the length of the shortest row in X\u03c4 and X\u03c4 . Let X\u2217\u03c4 and X \u2217\u03c4 \u2208 RN\u00d7K be the row normalized versions of X\u03c4 and X\u03c4 , as defined in step 3 of the RSC algorithm.\nFor any > 0 and sufficiently large N , assume that\n(a)\n\u221a K ln(4N/ )\n\u03b4 + \u03c4 \u2264 1 8 \u221a 3 \u03bbK , (b) \u03b4 + \u03c4 > 3 lnN + 3 ln(4/ ),\nthen with probability at least 1\u2212 , the following holds,\n(2) ||X\u03c4 \u2212X\u03c4O||F \u2264 c0 1\n\u03bbK\n\u221a K ln(4N/ )\n\u03b4 + \u03c4 , and ||X\u2217\u03c4 \u2212X \u2217\u03c4 O||F \u2264 c0\n1\nm\u03bbK\n\u221a K ln(4N/ )\n\u03b4 + \u03c4 .\nThe proof of Theorem 4.2 can be found in the supplementary materials. Next we use Theorem 4.2 to derive a bound on the mis-clustering rate of RSC. To define \u201cmisclustered\u201d, recall that RSC applies the k-means algorithm to the rows of X\u2217\u03c4 , where each row is a point in RK . Each row is assigned to one cluster, and each of these clusters has a centroid from k-means. Define C1, . . . , Cn \u2208 RK such that Ci is the centroid corresponding to the i\u2019th row of X\u2217\u03c4 . Similarly, run k-means on the rows of the population eigenvector matrix X \u2217 \u03c4 and define the population centroids C1, . . . , Cn \u2208 RK . In essence, we consider node i correctly clustered if Ci is closer to Ci than it is to any other Cj for all j with Zj 6= Zi.\nThe definition is complicated by the fact that, if any of the \u03bb1, . . . , \u03bbK are equal, then only the subspace spanned by their eigenvectors is identifiable. Similarly, if any of those eigenvalues are close together, then the estimation results for the individual eigenvectors are much worse that for the estimation results for the subspace that they span. Because clustering only requires estimation of the correct subspace, our definition of correctly clustered is amended with the rotation OT \u2208 RK\u00d7K , the matrix which minimizes \u2016X\u2217\u03c4OT \u2212 X \u2217\u03c4 \u2016F . This is referred to as the orthogonal Procrustes problem and [15] shows how the singular value decomposition gives the solution.\nDefinition 4.3. If CiOT is closer to Ci than it is to any other Cj for j with Zj 6= Zi, then we say that node i is correctly clustered. Define the set of mis-clustered nodes:\nM = {i : \u2203j 6= i, s.t.||CiOT \u2212 Ci||2 > ||CiOT \u2212 Cj ||2}.(3) The next theorem bounds the mis-clustering rate |M |/N .\nTheorem 4.4. (Main Theorem) Suppose A \u2208 RN\u00d7N is an adjacency matrix of a graph G generated from the DC-SBM with K blocks and parameters {B, Z,\u0398}. Let \u03bb1 \u2265 \u03bb2 \u2265 ... \u2265 \u03bbK > 0 be the K positive eigenvalues of L\u03c4 . Define M , the set of mis-clustered nodes, as in Definition 4.3. Let \u03b4 be the minimum expected degree of G. For any > 0 and sufficiently large N , assume (a) and (b) as in Theorem 4.2. Then with probability at least 1\u2212 , the mis-clustering rate of RSC with regularization constant \u03c4 is bounded,\n(4) |M |/N \u2264 c1 K ln(N/ )\nNm2(\u03b4 + \u03c4)\u03bb2K .\nRemark 1 (Choice of \u03c4): The quality of the bound in Theorem 4.4 depends on \u03c4 through three terms: (\u03b4 + \u03c4), \u03bbK , and m. Setting \u03c4 equal to the average node degree balances these terms. In essence, if \u03c4 is too small, there is insufficient regularization. Specifically, if the minimum expected degree \u03b4 = O(lnN), then we need \u03c4 \u2265 c( ) lnN to have enough regularization to satisfy condition (b) on \u03b4 + \u03c4 . Alternatively, if \u03c4 is too large, it washes out significant eigenvalues.\nTo see that \u03c4 should not be too large, note that\n(5) C = (ZT\u0398\u03c4Z) 1/2BL(Z T\u0398\u03c4Z) 1/2 \u2208 RK\u00d7K\nhas the same eigenvalues as the largest K eigenvalues of L\u03c4 (see supplementary materials for details). The matrix ZT\u0398\u03c4Z is diagonal and the (s, s)\u2019th element is the summation of \u03b8 \u03c4 i within block s. If EM = \u03c9(N lnN) where M = \u2211\niDii is the sum of the node degrees, then \u03c4 = \u03c9(M/N) sends the smallest diagonal entry of ZT\u0398\u03c4Z to 0, sending \u03bbK , the smallest eigenvalue of C, to zero.\nThe trade-off between these two suggests that a proper range of \u03c4 is (\u03b1EMN , \u03b2 EM N ), where 0 < \u03b1 < \u03b2 are two constants. Keeping \u03c4 within this range guarantees that \u03bbK is lower bounded by some constant depending only on K. In simulations, we find that \u03c4 = M/N (i.e. the average node degree) provides good results. The theoretical results only suggest that this is the correct rate. So, one could adjust this by a multiplicative constant. Our simulations suggest that the results are not sensitive to such adjustments.\nRemark 2 (Thresholding m): Mahoney [11] (and references therein) shows how the leverage scores of A and L are informative for both data analysis and algorithmic stability. For L, the leverage score of node i is ||Xi||22, the length of the ith row of the matrix containing the top K eigenvectors. Theorem 4.4 is the first result that explicitly relates the leverage scores to the statistical performance of spectral clustering. Recall that m2 is the minimum of the squared row lengths in X\u03c4 and X\u03c4 , that is the minimum leverage score in both L\u03c4 and L\u03c4 . This appears in the denominator of (4). The leverage scores in L\u03c4 have an explicit form\n||X i\u03c4 ||22 = \u03b8\u03c4i\u2211\nj \u03b8 \u03c4 j \u03b4zj ,zi\n.\nSo, if node i has small expected degree, then \u03b8\u03c4i is small, rendering ||X i\u03c4 ||2 small. This can deteriorate the bound in Theorem 4.4. The problem arises from projecting Xi\u03c4 onto the unit sphere for a node i with small leverage; it amplifies a noisy measurement. Motivated by this intuition, the next corollary focuses on the high leverage nodes. More specifically, let m\u2217 denote the threshold. Define S to be a subset of nodes whose leverage scores in L\u03c4 and X\u03c4 , ||X i\u03c4 || and ||Xi\u03c4 || exceed the threshold m\u2217: S = {i : ||X i\u03c4 || \u2265 m\u2217, ||Xi\u03c4 || \u2265 m\u2217}. Then by applying k-means on the set of vectors {[X\u2217\u03c4 ]i, i \u2208 S}, we cluster these nodes. The following corollary bounds the mis-clustering rate on S.\nCorollary 4.5. Let N1 = |S| denote the number of nodes in S and define M1 = M \u2229 S as the set of mis-clustered nodes restricted in S. With the same settings and assumptions as in Theorem 4.4, let \u03b3 > 0 be a constant and set m\u2217 = \u03b3/ \u221a N . If N/N1 = O(1), then by applying k-means on the set of vectors {[X\u2217\u03c4 ]i, i \u2208 S}, we have with probability at least 1\u2212 , there exist constant c2 independent of , such that\n(6) |M1|/N1 \u2264 c2 K ln(N1/ )\n\u03b32(\u03b4 + \u03c4)\u03bb2K .\nIn the main theorem (Theorem 4.4), the denominator of the upper bound contains m2. Since we do not make a minimum degree assumption, this value potentially approaches zero, making the bound useless. Corollary 4.5 replaces Nm2 with the constant \u03b32, providing a superior bound when there are several small leverage scores.\nIf \u03bbK (the Kth largest eigenvalue of L\u03c4 ) is bounded below by some constant and \u03c4 = \u03c9(lnN), then Corollary 4.5 implies that |M1|/N1 = op(1). The above thresholding procedure only clusters the nodes in S. To cluster all of the nodes, define the thresholded RSC (t-RSC) as follows:\n(a) Follow step (1), (2), and (3) of RSC as in section 2.1. (b) Apply k-means with K clusters on the set S = {i, ||Xi\u03c4 ||2 \u2265 \u03b3/ \u221a N} and assign each of them\nto one of V1, ..., VK . Let C1, ..., CK denote the K centroids given by k-means. (c) For each node i /\u2208 S, find the centroid Cs such that ||[X\u2217\u03c4 ]i\u2212Cs||2 = min1\u2264t\u2264K ||[X\u2217\u03c4 ]i\u2212Ct||2.\nAssign node i to Vs. (d) Output V1, ...VK .\nRemark 3 (Applying to SC): Theorem 4.4 can be easily applied to the standard SC algorithm under both the SBM and the DC-SBM by setting \u03c4 = 0. In this setting, Theorem 4.4 improves upon the previous results for spectral clustering.\nDefine the four parameter Stochastic Blockmodel SBM(p, r, s,K) as follows: p is the probability of an edge occurring between two nodes from the same block, r is the probability of an out-block linkage, s is the number of nodes within each block, and K is the number of blocks.\nBecause the SBM lacks degree heterogeneity within blocks, the rows of X within the same block already share the same length. So, it is not necessary to project Xi\u2019s to the unit sphere. Under the four parameter model, \u03bbK = (K[r/(p\u2212 r)] + 1)\u22121 (Rohe et al. [7]). Using Theorem 4.4, with p and r fixed and p > r, and applying k-means to the rows of X, we have\n|M |/N = Op ( K2 lnN\nN\n) .(7) If K = o( \u221a\nN lnN ), then |M |/N \u2192 0 in probability. This improves the previous results that required\nK = o(N1/3) (Rohe et al. [7]). Moreover, it makes the results for spectral clustering comparable to the results for the MLE in Choi et al. [16]."}, {"heading": "5. Simulation and Analysis of Political Blogs", "text": "This section compares five different methods of spectral clustering. Experiment 1 generates networks from the DC-SBM with a power-law degree distribution. Experiment 2 generates networks from the standard SBM. Finally, the benefits of regularization are illustrated on an empirical network from the political blogosphere during the 2004 presidential election (Adamic and Glance [17]).\nThe simulations compare (1) standard spectral clustering (SC), (2) RSC as defined in section 2, (3) RSC without projecting X\u03c4 onto unit sphere (RSC wp), (4) regularized SC with thresholding (t-RSC), and (5) spectral clustering with perturbation (SCP) (Amini et al. [2]) which applies SC to the perturbed adjacency matrix Aper = A + a11\nT . In addition, experiment 2 compares the performance of RSC on the subset of nodes with high leverage scores (RSC on S) with the other 5 methods. We set \u03c4 = M/N , threshold parameter \u03b3 = 1, and a = M/N2 except otherwise specified.\nExperiment 1. This experiment examines how degree heterogeneity affects the performance of the spectral clustering algorithms. The \u0398 parameters (from the DC-SBM) are drawn from the power law distribution with lower bound xmin = 1 and shape parameter \u03b2 \u2208 {2, 2.25, 2.5, 2.75, 3, 3.25, 3.5}. A smaller \u03b2 indicates to greater degree heterogeneity. For each fixed \u03b2, thirty networks are sampled. In each sample, K = 3 and each block contains 300 nodes (N = 900). Define the signal to noise ratio to be the expected number of in-block edges divided by the expected number of out-block edges. Throughout the simulations, the SNR is set to three and the expected average degree is set to eight.\nThe left panel of Figure 2 plots \u03b2 against the misclustering rate for SC, RSC, RSC wp, t-RSC, SCP and RSC on S. Each point is the average of 30 sampled networks. Each line represents one method. If a method assigns more than 95% of the nodes into one block, then we consider all nodes to be misclustered. The experiment shows that (1) if the degrees are more heterogeneous (\u03b2 \u2264 3.5), then regularization improves the performance of the algorithms; (2) if \u03b2 < 3, then RSC and t-RSC outperform RSC wp and SCP, verifying that the normalization step helps when the degrees are highly heterogeneous; and, finally, (3) uniformly across the setting of \u03b2, it is easier to cluster nodes with high leverage scores.\nExperiment 2. This experiment compares SC, RSC, RSC wp, t-RSC and SCP under the SBM with no degree heterogeneity. Each simulation has K = 3 blocks and N = 1500 nodes. As in the previous experiment, SNR is set to three. In this experiment, the average degree has three different settings: 10, 21, 30. For each setting, the results are averaged over 50 samples of the network.\nThe right panel of Figure 2 shows the misclustering rate of SC and RSC for the three different values of the average degree. SCP, RSC wp, t-RSC perform similarly to RSC, demonstrating that under the standard SBM (i.e. without degree heterogeneity) all spectral clustering methods perform comparably. The one exception is that under the sparsest model, SC is less stable than the other methods.\nAnalysis of Blog Network. This empirical network is comprised of political blogs during the 2004 US presidential election (Adamic and Glance [17]). Each blog has a known label as liberal or conservative. As in Karrer and Newman [10], we symmetrize the network and consider only the largest connected component of 1222 nodes. The average degree of the network is roughly 15. We apply RSC to the data set with \u03c4 ranging from 0 to 30. In the case where \u03c4 = 0, it is standard Spectral Clustering. SC assigns 1144 out of 1222 nodes to the same block, failing to detect the ideological partition. RSC detects the partition, and its performance is insensitive to the \u03c4 . With \u03c4 \u2208 [1, 30], RSC misclusters (80\u00b1 2) nodes out of 1222.\nIf RSC is applied to the 90% of nodes with the largest leverage scores (i.e. excluding the nodes with the smallest leverage scores), then the misclustering rate among these high leverage nodes is\n44/1100, which is almost 50% lower. This illustrates how the leverage score corresponding to a node can gauge the strength of the clustering evidence for that node relative to the other nodes.\nWe tried to compare these results t the regularized algorithm in [1]. However, because there are several very small degree nodes in this data, the values computed in step 4 of the algorithm in [1] sometimes take negative values. Then, step 5 (b) cannot be performed."}, {"heading": "6. Discussion", "text": "In this paper, we give theoretical, simulation, and empirical results that demonstrate how a simple adjustment to the standard spectral clustering algorithm can give dramatically better results for networks with heterogeneous degrees. Our theoretical results add to the current results by studying the regularization step in a more canonical version of the spectral clustering algorithm. Moreover, our main results require no assumptions on the minimum node degree. This is crucial because it allows us to study situations where several nodes have small leverage scores; in these situations, regularization is most beneficial. Finally, our results demonstrate that choosing a tuning parameter close to the average degree provides a balance between several competing objectives.\nAcknowledgements. Thanks to Sara Fernandes-Taylor for helpful comments. Research of TQ is supported by NSF Grant DMS-0906818 and NIH Grant EY09946. Research of KR is supported by grants from WARF and NSF grant DMS-1309998."}, {"heading": "Appendix A. Proof for Section 3", "text": "A.1. Proof of Lemma 3.2.\nProof. Recall that Dii = \u03b8i[DB]zi and [\u0398\u03c4 ]ii = \u03b8i Dii\nDii+\u03c4 . The ij\u2019th element of L\u03c4 :\n[L\u03c4 ]ij = Aij\u221a (Dii + \u03c4)(Djj + \u03c4) = \u03b8i\u03b8jBzizj\u221a DiiDjj\n\u221a Dii\nDii + \u03c4 Djj Djj + \u03c4\n= Bzizj\u221a\n[DB]zi [DB]zj\n\u2217 \u221a\n[\u0398\u03c4 ]ii[\u0398\u03c4 ]jj .\nHence,\nL\u03c4 = \u0398 1 2 \u03c4 ZBLZ T\u0398 1 2 \u03c4 .\nA.2. Proof of Lemma 3.3.\nProof. Let C = (ZT\u0398\u03c4Z) 1/2BL(Z T\u0398\u03c4Z) 1/2. If \u03b8i > 0, i = 1, ..., N , then C 0 since B 0 by assumption. Let \u03bb1 \u2265 ... \u2265 \u03bbK > 0 be the eigenvalues of C. Let \u039b \u2208 RK\u00d7K be a diagonal matrix with its ss\u2019th element to be \u03bbs. Let U \u2208 RK\u00d7K be an orthogonal matrix where its s\u2019th column is the eigenvector of C corresponding \u03bbs, s = 1, ...,K. By eigen-decomposition, we have C = U\u039bU T . Define X\u03c4 = \u0398 1 2 \u03c4 Z(ZT\u0398\u03c4Z) \u22121/2U , then\nX T\u03c4 X\u03c4 = U T (ZT\u0398\u03c4Z) \u22121/2(ZT\u0398\u03c4Z)(Z T\u0398\u03c4Z) \u22121/2U = UTU = I.\nOn the other hand,\nX\u03c4\u039bX T \u03c4 = \u0398 1 2 \u03c4 Z(Z T\u0398\u03c4Z) \u22121/2C(ZT\u0398\u03c4Z) \u22121/2ZT\u0398 1 2 \u03c4 = \u0398 1 2 \u03c4 ZBLZ T\u0398 1 2 \u03c4 = L\u03c4 .\nHence, \u03bbs, s = 1, ...,K are L\u03c4 \u2019s positive eigenvalues and X\u03c4 contains L\u03c4 \u2019s eigenvectors corresponding to its nonzero eigenvalues. For part 2, notice that ||X i\u03c4 ||2 = ( [\u0398\u03c4 ]ii\n[ZT\u0398\u03c4Z]zizi )1/2, then\n[X \u2217\u03c4 ] i = X i\u03c4 ||X i\u03c4 ||2\n= ( [\u0398\u03c4 ]ii [ZT\u0398\u03c4Z]zizi )1/2ZiU\n||X i\u03c4 ||2 = ZiU.\nTherefore, X \u2217\u03c4 = ZU ."}, {"heading": "Appendix B. Proof for Section 4", "text": "B.1. Proof of Theorem 4.1.\nProof. We extend the proof of Theorem 2 in Chung and Radcliffe [14] to the case of regularized graph laplacian. Let H = D \u22121/2 \u03c4 AD \u22121/2 \u03c4 . Then ||L\u03c4 \u2212L\u03c4 || \u2264 ||H \u2212L\u03c4 ||+ ||L\u03c4 \u2212H||. We bound the two terms separately. For the first term, we apply the concentration inequality for matrix:\nLemma B.1. Let X1, X2, ..., Xm be independent random N \u00d7 N Hermitian matrices. Moreover, assunme that ||Xi \u2212 E(Xi)|| \u2264 M for all i, and put v2 = || \u2211 var(Xi)||. Let X = \u2211 Xi. Then for any a > 0,\npr(||X \u2212 E(X)|| \u2265 a) \u2264 2N exp ( \u2212 a 2\n2v2 + 2Ma/3\n) .\nNotice that ||H \u2212L\u03c4 || = D\u22121/2\u03c4 (A\u2212A )D\u22121/2\u03c4 . Let Eij \u2208 RN\u00d7N be the matrix with 1 in the ij and ji\u2019th positions and 0 everywhere else. Let\nXij = D \u22121/2 \u03c4 ((Aij \u2212 pij)Eij)D\u22121/2\u03c4\n= Aij \u2212 pij\u221a\n(Dii + \u03c4)(Djj + \u03c4) Eij .\nH \u2212 L\u03c4 = \u2211 Xij . Then we can apply the matrix concentration theorem on {Xij}. By similar argument as in [14], we have\n||Xij || \u2264 [(Dii + \u03c4)(Djj + \u03c4)]\u22121/2 \u2264 1\n\u03b4 + \u03c4 , v2 = ||\n\u2211 E(X2ij)|| \u2264 1\n\u03b4 + \u03c4 . Take a = \u221a\n3 ln(4N/ ) \u03b4+\u03c4 . By assumption \u03b4 + \u03c4 > 3 lnN + 3 ln(4/ ), it implies a < 1. Applying\nLemma B.1, we have pr(||H \u2212L\u03c4 || \u2265 a) \u2264 2N exp ( \u2212\n3 ln(4N/ ) \u03b4+\u03c4\n2/(\u03b4 + \u03c4) + 2a/[3(\u03b4 + \u03c4)] ) \u2264 2N exp(\u22123 ln(4N/ )\n3 )\n\u2264 /2.\nFor the second term, first we apply the two sided concentration inequality for each i, (see for example Chung and Lu [18, chap. 2])\npr(|Dii \u2212Dii| \u2265 \u03bb) \u2264 exp{\u2212 \u03bb2 2Dii }+ exp{\u2212 \u03bb 2 2Dii + 2 3\u03bb }\nLet \u03bb = a(Dii + \u03c4), where a is the same as in the first part.\npr(|Dii \u2212Dii| \u2265 a(Dii + \u03c4)) \u2264 exp{\u2212 a2(Dii + \u03c4)2 2Dii }+ exp{\u2212 a 2(Dii + \u03c4)2\n2Dii + 2 3a(Dii + \u03c4)\n}\n\u2264 2 exp{\u2212 a 2(Dii + \u03c4)2\n(2 + 23a)(Dii + \u03c4) }\n\u2264 2 exp{\u2212a 2(Dii + \u03c4)\n3 }\n\u2264 2 exp{\u2212 ln(4N/ )(Dii + \u03c4) \u03b4 + \u03c4 } \u2264 2 exp{\u2212 ln(4N/ )} \u2264 /2N.\n||D\u22121/2\u03c4 D1/2\u03c4 \u2212 I|| = maxi| \u221a Dii + \u03c4\nDii + \u03c4 \u2212 1| \u2264 maxi|\nDii + \u03c4 Dii + \u03c4 \u2212 1|.\npr(||D\u22121/2\u03c4 D1/2\u03c4 \u2212 I|| \u2265 a) \u2264 pr(maxi| Dii + \u03c4\nDii + \u03c4 \u2212 1| \u2265 a)\n\u2264 pr(\u222ai{|(Dii + \u03c4)\u2212 (Dii + \u03c4)| \u2265 b(Dii + \u03c4)}) \u2264 /2.\nNote that ||L\u03c4 || \u2264 1, therefore, with probability at least 1\u2212 /2, we have\n||L\u03c4 \u2212H|| = ||D\u22121/2\u03c4 AD\u22121/2\u03c4 \u2212D\u22121/2\u03c4 AD\u22121/2\u03c4 ||\n= ||L\u03c4 \u2212D\u22121/2\u03c4 D1/2\u03c4 L\u03c4D1/2\u03c4 D\u22121/2\u03c4 || = ||(I \u2212D\u22121/2\u03c4 D1/2\u03c4 )L\u03c4D1/2\u03c4 D\u22121/2\u03c4 + L\u03c4 (I \u2212D1/2\u03c4 D\u22121/2\u03c4 )||\n\u2264 ||D\u22121/2\u03c4 D1/2\u03c4 \u2212 I||||D\u22121/2\u03c4 D1/2\u03c4 ||+ ||D\u22121/2\u03c4 D1/2\u03c4 \u2212 I|| \u2264 a2 + 2a.\nCombining the two part, we have that with probability at least 1\u2212 ,\n||L\u03c4 \u2212L\u03c4 || \u2264 a2 + 3a \u2264 4a, where a = \u221a\n3 ln(4N/ ) \u03b4+\u03c4 .\nB.2. Proof of Theorem 4.2.\nProof. First we apply a lemma from McSherry [3]:\nLemma B.2. For any matrix A, let PA denotes the projection onto the span of A\u2019s first K left sigular vectors. Then PAA is the optimal rank K approximation to A in the following sense. For any rank K matrix X, ||A\u2212 PAA|| \u2264 ||L\u2212X||. Further, for any rank K matrix B,\n(8) ||PAA\u2212B||2F \u2264 8K||A\u2212B||2.\nLet W \u2208 RK\u00d7K be a diagonal matrix that contains the K largest eigenvalues of L\u03c4 , w1 \u2265 w2 \u2265 ... \u2265 wK . Let \u039b \u2208 RK\u00d7K be the diagonal matrix that contains all positive eigenvalues of L\u03c4 . Take A = L\u03c4 and B = L\u03c4 in Lemma B.2. then PL\u03c4L\u03c4 = X\u03c4WX T \u03c4 and the previous inequality can be rewritten as\n||PL\u03c4L\u03c4 \u2212L\u03c4 ||2F = ||X\u03c4WXT\u03c4 \u2212X\u03c4\u039bX T\u03c4 ||2F \u2264 8K||L\u03c4 \u2212L\u03c4 ||2. Then we apply a modified version of the Davis-Kahan theorem (Rohe et al. [7]) to L\u03c4 .\nProposition B.3. Let S \u2282 R be an interval. Denote X\u03c4 as an orthonormal matrix whose column space is equal to the eigenspace of L\u03c4 corresponding to the eigenvalues in \u03bbS(L\u03c4 ) (more formally, the column space of X\u03c4 is the image of the spectral projection of L\u03c4 induced by \u03bbS(L\u03c4 )). Denote by X\u03c4 the analogous quantity for PL\u03c4L\u03c4 . Define the distance between S and the spectrum of L\u03c4 outside of S as\n\u2206 = min{|\u03bb\u2212 s|;\u03bb eigenvalue of L\u03c4 , \u03bb 6\u2208 S, s \u2208 S}. if X\u03c4 and X\u03c4 are of the same dimension, then there is an orthogonal matrix O, that depends on X\u03c4 and X\u03c4 , such that\n||X\u03c4 \u2212X\u03c4O||2F \u2264 2||PL\u03c4L\u03c4 \u2212L\u03c4 ||2F\n\u22062 .\nTake S = (\u03bbK/2, 2), then \u2206 = \u03bbK/2. By assumption (a) \u221a K ln(4N/ ) \u03b4+\u03c4 \u2264 1 8 \u221a 3 \u03bbK , we have that\nwhen N is sufficiently large, with probability at least 1\u2212 , |\u03bbK \u2212 wK | \u2264 ||L\u03c4 \u2212L\u03c4 || \u2264 4 \u221a 3 ln(4N/ )\n\u03b4 + \u03c4 \u2264 \u03bbK/2.\nHence wK \u2208 S. X and X are of the same dimension.\n||X\u03c4 \u2212X\u03c4O||F \u2264 \u221a\n2||PL\u03c4L\u03c4 \u2212L\u03c4 ||F \u2206\n\u2264 2 \u221a\n2||PL\u03c4L\u03c4 \u2212L\u03c4 ||F \u03bbK\n\u2264 8 \u221a K||L\u03c4 \u2212L\u03c4 ||\n\u03bbK\n\u2264 C \u03bbK\n\u221a K ln(4N/ )\n\u03b4 + \u03c4 .\nholds for C = 32 \u221a\n3 with probability at least 1\u2212 . For part 2, note that for any i,\n||[X\u2217\u03c4 ]i \u2212 [X \u2217\u03c4 ]iO||2 \u2264 ||Xi\u03c4 \u2212X i\u03c4 O||2\nmin{||Xi\u03c4 ||2, ||X i\u03c4 ||2} ,\nWe have that\n||X\u2217\u03c4 \u2212X \u2217\u03c4 O||F \u2264 ||X\u03c4 \u2212X\u03c4O||F\nm ,\nwhere m = mini{min{||Xi\u03c4 ||2, ||X i\u03c4 ||2}}.\nB.3. Proof of Theorem 4.4.\nProof. Recall that the set of misclustered nodes is defined as:\nM = {i : \u2203j 6= i, s.t.||CiOT \u2212 Ci||2 > ||CiOT \u2212 Cj ||2}.\nNote that Lemma 3.3 implies that the population centroid corresponding to i\u2019th row of X \u2217\u03c4\nCi = ZiU.\nSince all population centroids are of unit length and are orthogonal to each other, a simple calculation gives a sufficient condition for one observed centroid to be closest to the population centroid:\n||CiOT \u2212 Ci||2 < 1/ \u221a 2\u21d2 ||CiOT \u2212 Ci||2 < ||CiOT \u2212 Cj ||2 \u2200Zj 6= Zi.\nDefine the following set of nodes that do not satisfy the sufficient condition,\nU = {i : ||CiOT \u2212 Ci||2 \u2265 1/ \u221a 2}.\nThe mis-clustered nodes M \u2208 U . Define Q \u2208 RN\u00d7K , where the i\u2019th row of Q is Ci, the observed centroid of node i from k-means. By definition of k-means, we have\n||X\u2217\u03c4 \u2212Q||2 \u2264 ||X\u2217\u03c4 \u2212X \u2217\u03c4 O||2.\nBy triangle inequality,\n||Q\u2212 ZUO||2 = ||Q\u2212X \u2217\u03c4 O||2 \u2264 ||X\u2217\u03c4 \u2212Q||2 + ||X\u2217\u03c4 \u2212X \u2217\u03c4 O||2 \u2264 2||X\u2217\u03c4 \u2212X \u2217\u03c4 O||2.\nWe have with probability at least 1\u2212 , |M | N \u2264 |U | N = 1 N \u2211 i\u2208U 1\n\u2264 2 N \u2211 i\u2208U ||CiOT \u2212 Ci||22 = 2\nN \u2211 i\u2208U ||Ci \u2212 ZiUO||22\n\u2264 2 N ||Q\u2212 ZUO||2F \u2264 8 N ||X\u2217\u03c4 \u2212X \u2217\u03c4 O||2F \u2264 c1 K ln(N/ )\nNm2(\u03b4 + \u03c4)\u03bb2K .\nTai Qin\nE-mail: qin@stat.wisc.edu\nKarl Rohe\nE-mail: karlrohe@stat.wisc.edu"}], "references": [{"title": "Spectral clustering of graphs with general degrees in the extended planted partition model", "author": ["K. Chaudhuri", "F. Chung", "A. Tsiatas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Pseudo-likelihood methods for community detection in large sparse networks", "author": ["Arash A Amini", "Aiyou Chen", "Peter J Bickel", "Elizaveta Levina"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": "In Foundations of Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Spectral analysis of random graphs with skewed degree distributions", "author": ["Anirban Dasgupta", "John E Hopcroft", "Frank McSherry"], "venue": "In Foundations of Computer Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Finding planted partitions in random graphs with general degree distributions", "author": ["Amin Coja-Oghlan", "Andr\u00e9 Lanka"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Convex optimization for the planted k-disjointclique problem", "author": ["Brendan PW Ames", "Stephen A Vavasis"], "venue": "arXiv preprint arXiv:1008.2814,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Spectral clustering and the high-dimensional stochastic blockmodel", "author": ["K. Rohe", "S. Chatterjee", "B. Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1878}, {"title": "A consistent adjacency spectral embedding for stochastic blockmodel graphs", "author": ["D.L. Sussman", "M. Tang", "D.E. Fishkind", "C.E. Priebe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Stochastic blockmodels: First steps", "author": ["P.W. Holland", "S. Leinhardt"], "venue": "Social networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1983}, {"title": "Stochastic blockmodels and community structure in networks", "author": ["Brian Karrer", "Mark EJ Newman"], "venue": "Physical Review E,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Randomized algorithms for matrices and data. Advances in Machine Learning and Data Mining for Astronomy", "author": ["Michael W Mahoney"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Fast network community detection by score", "author": ["Jiashun Jin"], "venue": "arXiv preprint arXiv:1211.5803,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "On the spectra of general random graphs", "author": ["Fan Chung", "Mary Radcliffe"], "venue": "the electronic journal of combinatorics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A generalized solution of the orthogonal procrustes problem", "author": ["Peter H Sch\u00f6nemann"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1966}, {"title": "Stochastic blockmodels with a growing number of classes", "author": ["D.S. Choi", "P.J. Wolfe", "E.M. Airoldi"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "The political blogosphere and the 2004 us election: divided they blog", "author": ["Lada A Adamic", "Natalie Glance"], "venue": "In Proceedings of the 3rd international workshop on Link discovery,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "[1] and Amini et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] proposed inspired variations on the algorithm that artificially inflate the node degrees for improved statistical performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Several previous authors have studied the estimation properties of spectral clustering under various statistical network models (McSherry [3], Dasgupta et al.", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "[4], Coja-Oghlan and Lanka [5], Ames and Vavasis [6], Rohe et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4], Coja-Oghlan and Lanka [5], Ames and Vavasis [6], Rohe et al.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "[4], Coja-Oghlan and Lanka [5], Ames and Vavasis [6], Rohe et al.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "[7], Sussman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] and Chaudhuri et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] and Amini et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] proposed two inspired ways of artificially inflating the node degrees in ways that provide statistical regularization to spectral clustering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This paper examines the statistical estimation performance of regularized spectral clustering under the Degree-Corrected Stochastic Blockmodel (DC-SBM), an extension of the Stochastic Blockmodel (SBM) that allows for heterogeneous degrees (Holland and Leinhardt [9], Karrer and Newman [10]).", "startOffset": 262, "endOffset": 265}, {"referenceID": 9, "context": "This paper examines the statistical estimation performance of regularized spectral clustering under the Degree-Corrected Stochastic Blockmodel (DC-SBM), an extension of the Stochastic Blockmodel (SBM) that allows for heterogeneous degrees (Holland and Leinhardt [9], Karrer and Newman [10]).", "startOffset": 285, "endOffset": 289}, {"referenceID": 10, "context": "This threshold is a function of the leverage scores that have proven essential in other contexts, for both graph algorithms and network data analysis (see Mahoney [11] and references therein).", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "[12]) removes the ancillary effects of heterogeneous degrees under the DC-SBM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2], Jin [13]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[2], Jin [13]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "[1] proposed the regularized graph Laplacian that can be defined as L\u03c4 = D \u22121/2 \u03c4 AD \u22121/2 \u03c4 \u2208 RN\u00d7N where D\u03c4 = D + \u03c4I for \u03c4 \u2265 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] divides the nodes into two random subsets and only uses the induced subgraph on one of those random subsets to compute the spectral decomposition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Each edge corresponds to an independent Bernoulli random variable where the probability of an edge between any two nodes depends only on the block memberships of the two nodes (Holland and Leinhardt [9]).", "startOffset": 199, "endOffset": 202}, {"referenceID": 0, "context": "Let B be a K \u00d7 K matrix where Bab \u2208 [0, 1] for all a, b.", "startOffset": 36, "endOffset": 42}, {"referenceID": 9, "context": "The Degree-Corrected Stochastic Blockmodel (DC-SBM) (Karrer and Newman [10]) is a generalization of the SBM that adds an additional set of parameters (\u03b8i > 0 for each node i) that control the node degrees.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Then the probability of an edge between node i and node j is \u03b8i\u03b8jBzizj , where \u03b8i\u03b8jBzizj \u2208 [0, 1] for any i, j = 1, 2, .", "startOffset": 91, "endOffset": 97}, {"referenceID": 9, "context": "To make it identifiable, Karrer and Newman [10] suggest imposing the constraint that, within each block, the summation of \u03b8i\u2019s is 1.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "Remark: This theorem builds on the results of Chung and Radcliffe [14] and Chaudhuri et al.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "[1] which give a seemingly similar bound on ||L\u2212L || and ||D\u22121 \u03c4 A\u2212D\u22121 \u03c4 A ||.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "This is referred to as the orthogonal Procrustes problem and [15] shows how the singular value decomposition gives the solution.", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "Remark 2 (Thresholding m): Mahoney [11] (and references therein) shows how the leverage scores of A and L are informative for both data analysis and algorithmic stability.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Finally, the benefits of regularization are illustrated on an empirical network from the political blogosphere during the 2004 presidential election (Adamic and Glance [17]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 1, "context": "[2]) which applies SC to the perturbed adjacency matrix Aper = A + a11 T .", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "This empirical network is comprised of political blogs during the 2004 US presidential election (Adamic and Glance [17]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "As in Karrer and Newman [10], we symmetrize the network and consider only the largest connected component of 1222 nodes.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "With \u03c4 \u2208 [1, 30], RSC misclusters (80\u00b1 2) nodes out of 1222.", "startOffset": 9, "endOffset": 16}, {"referenceID": 0, "context": "We tried to compare these results t the regularized algorithm in [1].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "However, because there are several very small degree nodes in this data, the values computed in step 4 of the algorithm in [1] sometimes take negative values.", "startOffset": 123, "endOffset": 126}], "year": 2013, "abstractText": "Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently, Chaudhuri et al. [1] and Amini et al. [2] proposed inspired variations on the algorithm that artificially inflate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the \u201cstar shape\u201d in the eigenvectors\u2013a common feature of empirical networks\u2013can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models.", "creator": "LaTeX with hyperref package"}}}