{"id": "1311.2838", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2013", "title": "A PAC-Bayesian bound for Lifelong Learning", "abstract": "Lifelong learning focuses on the idea of retaining and reusing the knowledge learned from observed tasks for solving a new but related task. In this work we present a PAC-Bayesian bound on the generalization error in the lifelong learning framework. Our result gives a theoretical justification of how exploring the data from several related tasks makes it possible to automatically learn prior knowledge and how a prior that performs well on sufficiently many tasks guarantees good learning performance on new tasks from the same environment with high probability. The task-revised learning framework allows us to design a new learning framework that will enable it to take the training of our original participants in the same task.\n\n\n\n\n\nIntroduction\n\nIn our previous paper the learning framework for a particular task was described in the context of an automatic learning paradigm that is developed to optimize learning accuracy and improves learning accuracy with high probability. This model allows for a simple learning model, which, in turn, may help our neural models adapt to different tasks. Our previous work explored several ways in which learning the task has a specific target area (for example, the learning of the task has a target area, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for example, for", "histories": [["v1", "Tue, 12 Nov 2013 17:05:04 GMT  (10kb)", "https://arxiv.org/abs/1311.2838v1", null], ["v2", "Sat, 10 May 2014 10:45:51 GMT  (448kb,D)", "http://arxiv.org/abs/1311.2838v2", "to appear at ICML 2014"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["anastasia pentina", "christoph h lampert"], "accepted": true, "id": "1311.2838"}, "pdf": {"name": "1311.2838.pdf", "metadata": {"source": "CRF", "title": "A PAC-Bayesian Bound for Lifelong Learning", "authors": ["Anastasia Pentina", "Christoph H. Lampert"], "emails": ["APENTINA@IST.AC.AT", "CHL@IST.AC.AT"], "sections": [{"heading": null, "text": "In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods."}, {"heading": "1. Introduction", "text": "Today, many problems can be solved equally well or better by machine learning algorithms as by humans. However, these algorithms typically require large amount of training data to achieve acceptable results, whereas humans are able to learn new concepts from just a few examples. Presumably this difference comes from the fact that most machine learning systems are trained from scratch for each task at hand, whereas humans exploit context and knowledge they acquired previously while solving other tasks.\nThis observation motivates research on transfer learning: how can information from previously learned tasks be used for solving new tasks? Several scenarios of how this question can be formalized have been identified. Here, we discuss some that are most relevant in the context of super-\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nvised learning. As general setup, one assumes that one or more learning tasks have been observed, typically in form of labeled training sets. The methods then differ in how this information is meant to be used. In the multitask setting (Caruana, 1997), the goal is simply to perform well on all of the tasks. In domain adaptation (Bridle & Cox, 1990), the goal is to perform well on a new task for which only unlabeled or very few labeled data samples are observed. Finally, in lifelong learning or learning to learn (Thrun & Mitchell, 1995), the goal of the learner is to perform well on future tasks, for which so far no data has been observed. In this work we focus on the third setting.\nLifelong Learning. For lifelong learning to make sense, one must assume a relation between the observed tasks and the future tasks. To formalize this, Baxter (2000) introduced the notion of a task environment as a set of possible tasks that might need to be solved at some time. The observed tasks are sampled randomly from the environment according to an unknown task distribution. In this setting, Baxter also provided the first theoretical guarantees by proving generalization bounds in the framework of VC theory (Vapnik, 1998). After this work, however, progress on the theoretical understanding of lifelong machine learning slowed down. Many algorithms for transfer learning were developed and found empirically to work well in many cases. However, except for a few exception, such as (Maurer, 2009; Maurer et al., 2013), their theoretical justifications are so far not well understood.\nIn this work, we aim at making progress on the theoretical justifications of lifelong learning. In Section 2 we prove a general PAC-Bayesian generalization bound for lifelong learning that allows quantifying the relation between the expected loss on a future learning task to the average loss on the observed tasks. In contrast to Baxter\u2019s results, our bound has the advantage that its value depends on the representation of the data and on the learning algorithm used to solve the tasks. This makes it possible to interpret the bound as a quality measure of the transferred information. Therefore, by optimizing the measure we obtain principled\nar X\niv :1\n31 1.\n28 38\nv2 [\nst at\n.M L\n] 1\n0 M\nay 2\n01 4\nalgorithms for lifelong learning.\nIn Sections 2.1 and 2.2 we demonstrate this process in two cases: assuming that the solutions to all tasks can be represented by a single parameter vector plus small task-specific perturbation (Evgeniou & Pontil, 2004), we obtain an algorithm that resembles previously proposed methods for regularizing the weight vectors of future tasks using linear combinations of weight vectors of previous tasks, such as (Yang et al., 2007; Aytar & Zisserman, 2011).\nAn alternative assumption is that the solution vectors to tasks can differ significantly, but that they all lie in a common feature subspace of low dimension. In this setting, our bound provides an algorithm in which the observed tasks are used to identify the most promising subspace of features, such that learning for future tasks needs to take place only within the reduced feature space. This procedure is related to existing methods for representation and dictionary learning, e.g. (Argyriou et al., 2008; Kumar & Daume\u0301 III, 2012), which have also successfully been applied in the lifelong learning setting (Ruvolo & Eaton, 2013). In a case of linear regression, Maurer (2009) used this assumption to prove a generalization bound in the PAC framework, by using the concept of environment of tasks from Baxter (2000) and Rademacher complexity. Similar results were obtained in (Maurer et al., 2013) in the case of sparsity constraints.\nThe PAC-Bayesian framework. For the convenience of readers who are not familiar with the PAC-Bayesian framework, we introduce the most relevant concepts from the literature here. For more details, see (Langford, 2005; Seeger, 2003; Catoni, 2007).\nPAC-Bayesian theory studies the properties of randomized predictors, called Gibbs predictors. Formally, let X be an input set, Y an output set and H \u2282 {h : X \u2192 Y} a set of prediction functions (hypotheses). For any probability distribution P over H , the Gibbs predictor associated with P is the stochastic predictor that for any x \u2208 X randomly samples a hypothesis h \u223c P and then returns h(x).\nAssume that we are given a set S = {(x1, y1), . . . , (xm, ym)} of i.i.d. samples from an unknown probability distribution D over X \u00d7 Y . For any loss function, ` : Y \u00d7 Y \u2192 [0, 1], let er(Q) denote the expected loss of the Gibbs classifier associated with Q, i.e. er(Q) = Eh\u223cQE(x,y)\u223cD `(h(x), y), and let e\u0302r(Q) denote the expected empirical loss, i.e. e\u0302r(Q) = Eh\u223cQ 1 m \u2211m i=1 `(h(xi), yi). It is then possible to prove generalization bounds such as the following: with probability at least 1\u2212 \u03b4 (over the sampling of S) we have for all distributions Q over H (McAllester, 1999)\ner(Q)\u2264 e\u0302r(Q)+\n\u221a KL(Q||P ) + log 1\u03b4 + logm+ 2\n2m\u2212 1 , (1)\nwhere P is a reference or prior distribution over H that must be chosen before observing the samples S, and KL(Q||P ) is the Kullback-Leibler divergence, i.e. a measure how different Q is from P . As such, the bound resembled the typical trade-off in regularized risk minimization between the training loss and a regularizer (Vapnik, 1998).\nInequality (1) is uniform with respect to Q, so it holds regardless of which Q we choose. In particular, we can choose it after seeing S, and for this reason Q is typically referred to as posterior distribution in this context.\nChoosing Q such that it minimizes the right hand side of the bound we obtain a Gibbs predictor that can be expected to be a good choice for the learning task at hand, since its expected loss is controlled by a hopefully small quantity. While the inequality (1) holds regardless the agreement between the data distribution and the prior distribution P , the value of the right hand side of the bound strongly depends on the choice of P . Therefore, one would prefer a prior that allows learning a posterior that is at the same time close to the prior (KL(Q||P ) is small) and shows good performance on the training set (e\u0302r(Q) is small)."}, {"heading": "2. PAC-Bayesian Lifelong Learning", "text": "To develop a PAC-Bayesian theory of lifelong learning we adopt the concept of a task environment from Baxter (2000). We assume an unknown set of possible tasks T , all of which share the same input space X , output space Y , hypothesis set H and loss function ` : Y \u00d7 Y \u2192 [0, 1]. The lifelong learning system (which we will call an agent) observes n tasks t1, . . . , tn that are sampled i.i.d. from T according to some unknown distribution over tasks. For each task ti the agent observes a training set Si = {(xi1, yi1), . . . , (ximi , yimi)} that is sampled i.i.d. according to the task\u2019s unknown data distribution Di. To solve individual tasks, the agent makes use of an arbitrary but fixed learning algorithm, i.e. a deterministic procedure that, given a training set S and a form of prior knowledge P , outputs a posterior distribution Q = Q(S, P ) over H . The agent makes predictions using Gibbs predictor associated with Q. Staying within a PAC-Bayesian setting we assume that the prior knowledge, P , is encoded in a probability distribution over H . For concrete examples of the above setting see Sections 2.1 and 2.2.\nThe goal of the agent is to use the information contained in the observed tasks to identify prior knowledge that will cause as good as possible performance on new (so far unobserved) tasks from the same environment. This setting is strictly harder than multi-task learning or domain adaptation, since no data for the future tasks to be solved is available at the time the agent makes its decision. In particular, previously developed techniques for learning priors are\nnot directly applicable: first, note that we cannot use ordinary generalization bounds, such as (1), to identify optimal priors, since they only hold uniformly in Q if the prior is chosen independently from the training set. Catoni (2007) derived an expression for the overall \u201dbest\u201d prior, i.e. the distribution resulting in the smallest possible bound value. However, it is generally of a non-parametric form and uncomputable without full information about the data distribution. Parrado-Herna\u0301ndez et al. (2012) showed that priors can be learned by splitting the available training data into two parts, one for learning a prior, one for learning the predictor. This, however, requires training data for the task at hand, which is not available in the lifelong setting.\nOur first contribution in this work is the insight that one should treat the prior P itself as a random variable. Let P be an initial distribution over all possible priors, which we call hyperprior in concordance with the Bayesian nomenclature. For learning a prior the agent uses the observed tasks to adjust its original hyperprior into a hyperposterior distribution Q over the set of priors. This randomized setting allows us to follow a PAC-Bayesian path analogous to classical results. We will obtain a bound that requires a fixed hyperprior P but that holds uniformly with respect to the hyperposterior Q. The hyperposterior that minimizes the bound will provide us with the most promising distribution from which to obtain priors for future tasks.\nFormally, the goal of the agent is to find Q that minimizes the expected loss er(Qt) of a randomly sampled new task t with training set St and prior P sampled fromQ. We write\ner(Q) = E(t,St)EP\u223cQ er(Qt(St, P )), (2)\nwhere Qt(St, P ) is the posterior obtained by training the learning algorithm with prior P and training sample St. We call this quantity the transfer risk.\nWe cannot compute er(Q) because the distributions over the tasks and the tasks\u2019 data are both unknown. However, we can approximate it by its empirical counterpart, based on n observed tasks\ne\u0302r(Q) = 1 n \u2211n i=1 EP\u223cQ e\u0302r(Qi(Si, P )), (3)\nwhich we call empirical multi-task risk.\nOur main result is a theorem that bounds the difference between the two quantities defined above.\nTheorem 1 For any \u03b4 > 0 the following inequality holds with probability at least 1 \u2212 \u03b4 (over the training samples {S1, . . . , Sn}) for all hyperposterior distributions Q\ner(Q) \u2264 e\u0302r(Q) + 1\u221a n\n( KL(Q\u2016P) + 1\n8 \u2212 log \u03b4 2\n) (4)\n+ 1\nn \u221a m\u0304 KL((Q, Qn)\u2016(P, Pn)) + 1\u221a m\u0304\n( 1\n8 \u2212 1 n log \u03b4 2\n)\nwhere (Q, Qn) = Q \u00d7 \u220fn i=1Qi denotes the distribution in which we first sample P according to Q and then use it and the data Si to produce a posterior Qi for each task ti. (P, Pn) = P \u00d7 \u220fn i=1 P denotes the distribution in which we sample P according to P and use it as a posterior for all tasks. m\u0304 = ( 1 n \u2211n i=1 1 mi )\u22121 is the harmonic mean of the sample sizes.\nProof To prove Theorem 1 we introduce an intermediate quantity that can be seen as an expected multi-task risk\ne\u0303r(Q) = E P\u223cQ\n1\nn \u2211n i=1 E h\u223cQi E (x,y)\u223cDi `(h(x), y). (5)\nFirst we will bound the uncertainty on the task environment level by bounding the difference between transfer error, er(Q), and expected multi-task error, e\u0303r(Q). Then we will bound the uncertainty within observed tasks by bounding the difference between expected multi-task error, e\u0303r(Q), and its empirical approximation, e\u0302r(Q). Our main tool in both cases will be the following lemma.\nLemma 2 Let f be a random variable taking values in A and let X1, . . . , Xl be l independent random variables with each Xk distributed according to \u00b5k over the set Ak. For functions gk : A \u00d7 Ak \u2192 [ak, bk], k = 1 . . . l, let \u03bek(f) = EXk\u223c\u00b5k gk(f,Xk) for any fixed value of f . Then for any fixed distribution \u03c0 on A and any \u03bb, \u03b4 > 0 the following inequality holds with probability at least 1\u2212\u03b4 (over sampling X1, . . . , Xl) for all distributions \u03c1 over A\nE f\u223c\u03c1 \u2211l k=1 \u03bek(f)\u2212 E f\u223c\u03c1 \u2211l k=1 gk(h,Xk) \u2264\n1\n\u03bb\n( KL(\u03c1||\u03c0) + \u03bb 2\n8 \u2211l k=1 (bk \u2212 ak)2 \u2212 log \u03b4 ) . (6)\nFor the proof of this lemma, see the Appendix A.\nIn order to bound the difference between er(Q) and e\u0303r(Q) we treat each task t with the corresponding training sample St as a random variable and apply Lemma 2. Formally, we set \u03c1 = Q, \u03c0 = P , Xk = (tk, Sk), l = n, f = P and gk(f,Xk) = 1n Eh\u223cQk E (x,y)\u223cDk l(h(x), y) and apply Lemma 2 with \u03bb = \u221a n. Since ak = 0 and bk = 1n we obtain with probability at least 1\u2212 \u03b4/2 that for all Q\ner(Q) \u2264 e\u0303r(Q) + 1\u221a n\n( KL(Q||P) + 1\n8 \u2212 log \u03b4 2\n) . (7)\nTo bound the difference between e\u0303r(Q) and e\u0302r(Q) we apply Lemma 2 to the union of all training samples S\u2032 = \u22c3n i=1 Si. We set \u03c1 = (Q, Qn), \u03c0 = (P, Pn),\nXk = (xij , yij), l = \u2211 mi, f = (P, h1, . . . , hn) and gk(f,Xk) = 1\nnmi `(hi(xij), yij). In this setting ak = 0\nand bk = 1/(nmi), Lemma 2 with \u03bb = n \u221a m\u0304 yields that with probability at least 1\u2212 \u03b4/2 for all Q\ne\u0303r(Q) \u2264 e\u0302r(Q) + 1 n \u221a m\u0304 KL((Q, Qn)||(P, Pn))\n+ 1 8 \u221a m\u0304 \u2212 1 n \u221a m\u0304 log \u03b4 2 . (8)\nNow (4) follows by a union bound from (7) and (8).\nTo get a better understanding of Theorem 1, we rewrite (4) in the following way:\ner(Q) \u2264 e\u0302r(Q) + (\n1\u221a n + 1 n \u221a m\u0304\n) KL(Q\u2016P) (9)\n+ 1\nn \u221a m\u0304 n\u2211 i=1 E P\u223cQ KL(Qi(Si, P )\u2016P ) + const(n, m\u0304, \u03b4).\nWe see that the bound contains two types of complexity terms that correspond to two levels of our model: KL(Q\u2016P) belongs to the level of task environment in general, while each KL(Qi(Si, P )\u2016P ) corresponds specifically to the i-th task.\nTo better understand their roles, we look at the following limit cases: when the agent has access to sufficiently many tasks (n \u2192 \u221e) but tasks come with a finite amount of data (m\u0304 is finite), the first complexity term converges to 0 as 1/ \u221a n. The second complexity term converges to an average KL-divergence over tasks and may therefore remain non-zero. This means that observing many task gives the agent full knowledge about the task environment, but it cannot overcome the uncertainty within each task. In the opposite case, if the agent observes unlimited data for each tasks, but only for a finite number of tasks (m\u0304\u2192\u221e, n is finite), the second complexity term converges to 0 as 1/ \u221a m\u0304, while the first one does not, so there is still uncertainty on the task environment level. Only when both comes together, sufficiently many tasks and sufficient amounts of data per task, it is guaranteed that the empirical multi-task risk e\u0302r(Q) converges to the transfer risk er(Q).\nA second important aspect of Theorem 1 is that the bound (4) consists only of observable quantities. Therefore, we can treat it as a quality measure for hyperposteriors Q. By minimizing it, we obtain a hyperposterior distribution over priors that is adjusted to the particular environment of learning tasks. Since the bound holds uniformly with respect to Q, the guarantees of Theorem 1 also hold for the resulting learned hyperposterior, so we can expect priors sampled according to the learned hyperposterior to work well even for future tasks.\nIn the following sections, we discuss two instantiations of this procedure and show how they relate to previous work on transfer learning."}, {"heading": "2.1. Parameter Transfer", "text": "Let X \u2282 Rd and H be a set of linear predictors: h(x) = \u3008w, x\u3009 if Y = R or h(x) = sign\u3008w, x\u3009 if Y = {\u22121, 1}, where w \u2208 Rd is a weight vector. One of the common assumptions in multitask or lifelong learning is that the weight vectors for different tasks are only minor variations of an unknown prototypical vector (Evgeniou & Pontil, 2004). It can be captured by regularizing the distance to this vector (Aytar & Zisserman, 2011; Yang et al., 2007):\nw\u0302 = arg min w\n( \u2016w\u2212wpr\u20162+ C\nm m\u2211 j=1 (yj\u2212\u3008w, xj\u3009)2 ) , (10)\nwherewpr is some function of weight vectors of previously observed tasks, e.g. just their average, wpr = 1n \u2211n i=1 wi.\nTheorem 1 allows us to learn an \u201coptimal\u201d wpr from the data, instead of fixing the rule for computing it. For this, we choose P = N (wP , Id) and Q = N (wQ, Id), i.e. unit variance normal distributions with means wP and wQ, respectively. The mean wP is a random variable distributed first according to the hyperprior distribution, P , which we set as N (0, \u03c3Id) and later according to the hyperposterior, Q, which we model as Q = N (wQ, Id). The task of the learning consists of identifying the best wQ.\nAs underlying learning algorithm we use Equation (10) with regularizer centered at a prior vector. For any wP and training set S = {(xi, yi)i=1,...,m} the posterior, Q(S, P ) = N (wQ, Id), is given by\nwQ = argmin ( \u2016w\u2212wP \u20162+ C\nm m\u2211 j=1 (yj\u2212\u3008w, xj\u3009)2 ) . (11)\nThis has the closed form solution wQ = (m C Id+XX >)\u22121(m C wP+XY ) = AwP+b, (12)\nwhere X is the matrix with columns x1, . . . , xm, Y is a column of labels (y1, . . . , ym)>, A = ( Id + C mXX >)\u22121 and b = CmAXY .\nComputing the complexity terms from (9) we obtain\nKL(Q\u2016P) = \u2016wQ\u2016 2 2\u03c3 + d 2\n( log \u03c3 + 1 \u03c3 \u2212 1 ) and\nE P\u223cQ KL(Qi(Si, P )\u2016P ) = E wP\u223cQ\n\u2016(Ai \u2212 Id)wP + bi\u20162\n2\n= 1\n2\n( \u2016(Ai \u2212 Id)wQ + bi\u20162 + tr(Ai \u2212 Id)2 ) . (13)\nWe insert Equations (13) into the inequality (9) and obtain\n\u2200wQ er(wQ) \u2264 e\u0302r(wQ) + \u221a nm\u0304+ 1\n2\u03c3n \u221a m\u0304 \u2016wQ\u20162\n+ 1\n2n \u221a m\u0304 n\u2211 i=1 \u2016(Ai \u2212 Id)wQ + bi\u20162 + const . (14)\nThe last thing we have to specify is the loss function `. We consider two options: first, the binary classification setting with 0/1 loss: `(y1, y2) = Jy1 6= y2K. In this case the expected empirical error of the Gibbs classifier is given by the following expression (Germain et al., 2009; Langford & Shawe-Taylor, 2002)\ne\u0302r(wQ)= 1\nn n\u2211 i=1 1 mi mi\u2211 j=1 \u03a6  yijx>ij(AiwQ + bi)\u221a x>ij(Id+AiA > i )xij , (15) where \u03a6(z) = 12 ( 1\u2212erf( z\u221a 2 ) ) and erf(z) = 2\u221a \u03c0 \u222b z 0 e\u2212t 2 dt is the Gauss error function.\nFor a practical algorithm, one typically would prefer a bound on the loss of a deterministic classifier rather than of the stochastic Gibbs classifier. For 0/1-loss Theorem 1 provides this, since the Gibbs error is at most twice smaller than the expected error of the classifier defined by AiwQ + bi (McAllester, 2003; Laviolette & Marchand, 2007). Inserting (15) in (14) and multiplying the left hand side by 12 we obtain the following inequality:\n\u2200wQ 1\n2 E (t,St) E (x,y)\u223cDt [y 6= sign\u3008AtwQ + bt, x\u3009] \u2264 (16)\n\u221a nm\u0304+ 1\n2\u03c3n \u221a m\u0304 \u2016wQ\u20162 +\n1\n2n \u221a m\u0304 n\u2211 i=1 \u2016(Ai \u2212 Id)wQ + bi\u20162\n+ 1\nn n\u2211 i=1 1 mi mi\u2211 j=1 \u03a6  yijx>ij(AiwQ + bi)\u221a x>ij(Id +AiA > i )xij + const . For regression tasks, we consider the case of truncated squared loss, `(y1, y2) = min{(y1 \u2212 y2)2, 1} (the truncation is necessary to fulfill the condition of a bounded loss function). Since `(y1, y2) \u2264 (y1 \u2212 y2)2, we can substitute e\u0302r(wQ) in (14) by the empirical error of the Gibbs predictor with squared loss without violating the inequality. This error differs from the error of the predictor that is defined by AiwQ + bi only by a constant that does not depend on wQ. An elementary calculation shows that for truncated squared loss `, as in the case of 0/1 loss, the error of Gibbs predictor is at least one half of the expected error of the predictor defined by AiwQ + bi. Therefore in this case we obtain a result similar to the inequality (16)\n\u2200wQ 1\n2 E (t,St) E (x,y)\u223cDt min{(y \u2212 \u3008AtwQ + bt, x\u3009)2, 1} \u2264\n\u221a nm\u0304+ 1\n2\u03c3n \u221a m\u0304 \u2016wQ\u20162 +\n1\n2n \u221a m\u0304 n\u2211 i=1 \u2016(Ai \u2212 Id)wQ + bi\u20162\n+ 1\nn n\u2211 i=1 1 mi mi\u2211 j=1 (yij\u2212\u3008AiwQ+bi, xij\u3009)2 + const . (17)\nMinimizing the right hand side of (16) or (17) with respect to wQ, we obtain a data-dependent hyperposterior that induces prior distributions that are adjusted optimally (in the sense of the bound) to the task environment."}, {"heading": "2.2. Representation Transfer", "text": "A second assumption commonly made in multitask or lifelong learning is that the weight vectors for all tasks lie in low-dimensional subspace. Theorem 1 also allows us to learn such a subspace in a principled way.\nWe again assume that X \u2282 Rd and H is a set of linear predictors. We represent k-dimensional subspaces of Rd by d \u00d7 k matrices with orthogonal columns, i.e. elements of the Stiefel manifold Vd,k. As hyperprior, we want all subspaces to be equally likely, so we set P to the uniform distribution over Vd,k (Downs, 1972)\npP(B) = 1\nC0 for any B \u2208 Vd,k, (18)\nwhere C0 = 0F1( 12d, 0). As hyperposterior, Q, we want a distribution that concentrates its probability mass around a specific subspace, M . We choose a special case of Langevin distribution, D(Ik,M),\npQ(B) = 1\nC1 exp(tr(M>B)) for any B \u2208 Vd,k, (19)\nwhere C1 = 0F1( 12d, 1 4M >M). The only free parameter is M \u2208 Vd,k, i.e. a d \u00d7 k matrix with M>M = Ik that represents the \u201dmost promising subspace\u201d. Equation (19) can be interpreted as an analog of the Gaussian distribution on Vd,k, with mode M and unit variance. In the special case of k = 1, it reduces to the better known Von Mises distribution on the unit circle (Downs, 1972).\nAs in the previous section we use Gaussian distributions for prior and posterior, but defined only within the subspaces sampled from P orQ. For the prior, P , we choose a Gaussian with zero mean and variance \u03c3Ik. The posterior, Q, is a shifted Gaussian with variance \u03c3Ik and mean wQ in the same subspace. As in the previous section we use ridge regression as learning algorithm, but again only within the subspace determined by the prior,\nwQ = argmin w\n( \u2016w\u20162+ C\nm m\u2211 i=1\n(yi\u2212\u3008w,B>xi\u3009)2 ) , (20)\nwhere B is the matrix representing the subspace, such that B>x is the projected representation of the training data in this subspace.\nTo obtain an objective function for learning M , we first compute the complexity terms in the bound (9). KL(Q\u2016P) is a constant independent of M : P is uniform, so\nKL(Q\u2016P) depends only on the differential entropy of Q. This itself is a constant independent of the parameter matrix1. Furthermore, we have KL(Qi(Si, P )\u2016P ) = 1 2\u03c3\u2016wi(B)\u2016\n2, where B is the representation of the selected subspace. In combination, we get the following bound\ner(M) \u2264 e\u0302r(M) + 1 2\u03c3n \u221a m\u0304 n\u2211 i=1 E B\u223cD(Ik,M) \u2016wi(B)\u20162\n+const = 1\nn n\u2211 i=1 E B\u223cQ { e\u0302r(wi(B)) + 1 2\u03c3 \u221a m\u0304 \u2016wi(B)\u20162 } ,\n+ const . (21)\nwhere wi(B) = Cmi\n( Ik +\nC mi B>XiX > i B )\u22121\nB>XiYi. We see that a representation,M , can be considered promising for future tasks, if itself as well as the subspaces close to it allow classification with small loss and small weight vector norm (i.e. large margin) for all observed tasks."}, {"heading": "3. Experiments", "text": "In this section, we demonstrate how learning priors distributions by minimizing the bounds (16), (17) and (21) can improve prediction performance in real prediction tasks. To position our results with respect to previous work on parameter and representation transfer, we compare to adaptive ridge regression (ARR), i.e. Equation (10) the prior wpr set to the average of the weight vectors from the observed tasks, and with the ELLA algorithm (Ruvolo & Eaton, 2013) that learns a subspace representation using structured sparsity constraints, also with squared loss. We also report results for ordinary ridge regression without any knowledge transfer.\nWe perform experiments on three public datasets:\nLand Mine Detection (Xue et al., 2007). This dataset consists of 14820 data points. For each data point there are 9 features extracted from radar images and a binary label 0 or 1 corresponding to landmine or clutter. We also add a bias term, resulting in d = 10 features. Data points are collected from 29 geographical regions and we treat each region as a binary classification task.\nLondon School Data. This is a regression dataset, containing exam scores of 15362 students from 139 schools. Each student is described by 4 school-specific, 3 student-specific features and a year of examination. We use the same procedure as in (Argyriou et al., 2008; Kumar & Daume\u0301 III, 2012; Ruvolo & Eaton, 2013) to encode them in a set of binary features. We also add a bias term, so the final data\n1For any M \u2208 Vd,k there exits an orthogonal matrix L \u2208 Rd\u00d7d such that LM = J = {\u03b4ij} \u2208 Rd\u00d7k. Therefore if B \u223c D(Ik,M), than LB \u223c D(Ik, LM) = D(Ik, J). So, the entropy of D(Ik,M) is equal to the entropy of D(Ik, J) for any M .\ndimensionality is d = 28. Each school constitutes a task.\nAnimals with Attributes Dataset (Lampert et al., 2013). This dataset contains 30475 images from 50 classes. Each image comes with a 2000-dimensional feature vector, that we reduced to 100 dimensions using PCA. We l2-normalize the resulting feature vectors and add a bias term. We select the largest class, collie, and form 49 binary classification tasks, each of them is a classification of collie versus one of the remaining classes. For each task we use 2% of the data (approximately 20 images) available for collie class and the same amount of images from the another task, such that data between different tasks does not overlap."}, {"heading": "3.1. Parameter Transfer", "text": "We first perform experiments on prior learning in the setup of parameter transfer, as described in Section 2.1, calling the resulting algorithm Prior Learning with Gaussian hyperprior (PL-G). For the classification tasks (Landmine and Animals), we optimize the bound (16). To do so we replace \u03a6 by its convex relaxation, \u03a6cvx(z) = 12 \u2212 z\u221a 2\u03c0\n, if z \u2264 0 and \u03a6cvx(z) = \u03a6(z) otherwise, and use the conjugate gradient method for finding the minimum.\nFor the regression tasks (Schools) we first divide labels (examination scores) by their maximum value. This allows us to assume that the squared loss will not exceed 1. We optimize (17), and due to the squared loss, the problem has a closed form solution:\nwQ =\u2212 ( D + \u221a nm\u0304+ 1\n\u03c3n \u221a m\u0304 Id (22)\n+ 1\nn \u221a m\u0304 n\u2211 i=1 A\u2032>i A \u2032 i )\u22121( c+ 1 n \u221a m\u0304 n\u2211 i=1 A\u2032>i bi ) ,\nwhere A\u2032i=Ai \u2212 Id, D = 2\nn n\u2211 i=1 1 mi A>i XiX > i Ai, (23)\nc> = 2\nn n\u2211 i=1 1 mi ( C mi Y >i X > i A > i XiX > i Ai \u2212 Y >i X>i Ai ) .\nTo make results comparable with the baseline algorithms, we report the squared error multiplied by the squared value of the maximum examination score."}, {"heading": "3.2. Representation Transfer", "text": "In a second set of experiments, we implement the idea of representation transfer from Section 2.2, calling the algorithm Prior Learning with Langevin hyperprior (PL-L).\nAs in the case of parameter transfer, we use 0/1 loss to measure the quality in classification tasks. For the regression task we apply the same scaling procedure as discussed in Section 3.1 and use truncated squared loss. Both of these loss functions can be upper-bounded by the standard\nsquared loss, which we do to obtain tractable expressions for the right hand side of the Inequality (21). To be able to optimize the expression (21) numerically, we approximate it by replacing all expectations over Q by their values at its mode, M . Furthermore, we replace the error of any Gibbs predictor by the error of the deterministic predictor defined by the mode of the posterior distribution, wi(M). The result is a quadratic optimization problem over the Stiefel manifold, which we solve using gradient descent with curvilinear search (Wen & Yin, 2013)."}, {"heading": "3.3. Evaluation procedure", "text": "To get reliable estimates of the transfer risk, we repeat the following experimental procedure 100 times for each dataset and calculate the mean prediction errors and standard errors of the mean.\nIn each experiment, we set aside a subset of tasks as unobserved (9 in Landmines, 39 in Schools, 9 in Animals). These are not used during any part of training, but only to evaluate the methods on \u201dfuture\u201d tasks. Of the remaining tasks we use different fractions to measure the effect of a different number of observed tasks. The algorithms described in Section 3.1 and 3.2 and ARR have one free parameter, the regularization strength C \u2208 {10\u22123, . . . , 103}. We choose this using 3-fold cross-validation in the following way. We split the data of each task into three parts: we\nuse the first third of all tasks jointly to learn a prior. To evaluate this prior, we then train individual predictors using the second part of the data, and test their quality on the third part. For the ELLA algorithm, we use the same procedure to set the regularization strength \u00b5, the remaining parameters we leave at their default values. For the baseline, we set the regularization using ordinary 3-fold cross-validation."}, {"heading": "3.4. Results", "text": "The results of the experiments on all three datasets are shown in Figure 1. Since classes in Landmine dataset are unbalanced, for this problem we report the value of area under the ROC curve (AUC, bigger value means better prediction). Tasks in the Animals dataset are balanced, so for them we report the standard mean 0/1 error. Since the dataset was too large for the subspace methods, we only report results for the parameter transfer techniques. For the experiment on Schools dataset we report the mean squared error (MSE, smaller values mean better prediction).\nAs a first observation, Figure 1 confirms the findings of previous work that better prediction can be achieved by transferring information from related tasks. Overall, it shows that PL-G and PL-L are comparable to the existing, manually designed, techniques. Given sufficiently many tasks, they are able to improve the prediction accuracy over the baseline. As an illustration of the hyperprior concept, we\nshow results for PL-G with two different values for the Gaussian hyperprior variance (Figures 1(a), 1(b), 1(c)). For \u03c3 = 1, the adaption pursues in a very conservative way and many tasks are needed to find a reliable hyperposterior. With \u03c3 = 10, convergence is faster, and PL-G achieves results comparable with ARR or even slightly better. For practical tasks, the hyperprior should possibly be chosen by model selection.\nThe results for representation transfer (Figures 1(d) and 1(e)) show that the improvements achieved by PL-L are comparable to the ELLA algorithm. As in the case of parameter transfer, we show results for two different values of the Gaussian prior variance: \u03c3 = 1 and \u03c3 = 10. While for the Landmine dataset (Figure 1(d)), there is no significant difference in the performance for different values of parameters k and \u03c3, for the Schools dataset (Figure 1(e)) the choice of these parameters plays a bigger role. We see that the improvements of PL-L with \u03c3 = 10 are almost the same as the one achieved by ELLA, while for \u03c3 = 1 they are smaller. This might be the effect of too strict hyperparameters that cause the method to be more conservative than necessary. Another possible reason for the difference in accuracy is that ELLA makes additional sparsity assumption, which PL-L does not."}, {"heading": "4. Conclusion", "text": "In this work we studied lifelong learning from a theoretical perspective. Our main result is a generalization bound in a PAC-Bayesian framework (Theorem 1). On the one hand, the bound is very general, allowing us to recover two existing principles for transfer learning as special cases: the transfer of classifier parameters, and the transfer of subspaces/representations. On the other hand, the bound consists only of observable quantities, such that it can be used to derive principled algorithms for lifelong learning that achieve results comparable with existing manually designed methods.\nA further use of the bound we see is in using it to study the implicit assumptions of possible learning methods. For example, a method obtained by means of a unimodal hyperposterior will require all tasks to be related to each other. In future work, we plan to explore the potential of integrating more realistic assumptions, such as hierarchical or multimodal hyperposteriors. A second interesting direction will be to relax the condition that tasks are sampled i.i.d. from an environment, e.g. into the direction of learning tasks of continuously improving difficulty (Bengio et al., 2009).\nAcknowledgements. We thank Shai Ben-David, Olivier Catoni and Emilie Morvant for helpful discussions. This work was in parts funded by the European Research Council under the European Union\u2019s Seventh Framework\nProgramme (FP7/2007-2013)/ERC grant agreement no 308036."}, {"heading": "A. Proof of Lemma 2", "text": "In the proof we will make use of Hoeffding\u2019s Lemma:\nLemma 3 (Hoeffding, 1963) Let X be a real-valued random variable such that Pr(X \u2208 [a, b]) = 1 and let \u03be = E{X}. Then\nE [ e\u03bb(\u03be\u2212X) ] \u2264 e \u03bb2(b\u2212a)2 8 . (24)\nWe will also require the following property of the Kullback-Leibler divergence that holds for any \u03bb > 0 and can be proved by convex duality (Seeger, 2003):\nE f\u223cQ g(f) \u2264 1 \u03bb\n( KL(Q\u2016P ) + log E\nf\u223cP e\u03bbg(f)\n) . (25)\nWe now prove Lemma 2. First, we apply (25) to g(f) =\u2211l k=1 \u03bek(f)\u2212 \u2211l k=1 gk(f,Xk), obtaining\nE f\u223c\u03c1\n( l\u2211\nk=1\n\u03bek(f)\u2212 l\u2211\nk=1\ngk(f,Xk)\n) \u2264\n1\n\u03bb\n( KL(\u03c1\u2016\u03c0) + log E\nf\u223c\u03c0 e\u03bbg(f)\n) . (26)\nNote, that\ne\u03bbg(f) = l\u220f k=1 exp(\u03bb(\u03bek(f)\u2212 gk(f,Xk))), (27)\nsince for any fixed f the factors are independent. This allows us to apply Hoeffding\u2019s Lemma 3 to each factor:\nE X1\u223c\u00b51 \u00b7 \u00b7 \u00b7 E Xl\u223c\u00b5l\ne\u03bbg(f) \u2264 exp (\u03bb2\n8 \u2211l k=1 (bk \u2212 ak)2 ) .\n(28) By taking the expectation over f \u223c \u03c0 we obtain\nE f\u223c\u03c0 E X1\u223c\u00b51 \u00b7 \u00b7 \u00b7 E Xl\u223c\u00b5l\ne\u03bbg(f)\u2264exp (\u03bb2\n8 \u2211l k=1 (bk\u2212ak)2 ) .\n(29) Since \u03c0 is fixed and does not depend onX1, . . . , Xl, we can exchange the order of expectations. By applying Markov\u2019s inequality with respect to expectations overX1, . . . , Xl we obtain that with probability at least 1\u2212 \u03b4:\nlog E f\u223c\u03c0\ne\u03bbg(f) \u2264 \u03bb 2\n8 \u2211l k=1 (bk \u2212 ak)2 \u2212 log \u03b4. (30)\nWe obtain (2) by combining (30) and (26)."}], "references": [{"title": "Convex multi-task feature learning", "author": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Tabula rasa: Model transfer for object category detection", "author": ["Aytar", "Yusuf", "Zisserman", "Andrew"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Aytar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Aytar et al\\.", "year": 2011}, {"title": "A model of inductive bias learning", "author": ["Baxter", "Jonathan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Baxter and Jonathan.,? \\Q2000\\E", "shortCiteRegEx": "Baxter and Jonathan.", "year": 2000}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "RecNorm: Simultaneous normalisation and classification applied to speech recognition", "author": ["Bridle", "John S", "Cox", "Stephen J"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Bridle et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bridle et al\\.", "year": 1990}, {"title": "Multitask learning", "author": ["Caruana", "Rich"], "venue": "Machine Learning,", "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "PAC-Bayesian Supervised Classification (The Thermodynamics of Statistical Learning)", "author": ["Catoni", "Olivier"], "venue": "Monograph Series of the Institute of Mathematical Statistics. IMS,", "citeRegEx": "Catoni and Olivier.,? \\Q2007\\E", "shortCiteRegEx": "Catoni and Olivier.", "year": 2007}, {"title": "Regularized multi\u2013task learning", "author": ["Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "In International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "PAC-Bayesian learning of linear classifiers", "author": ["Germain", "Pascal", "Lacasse", "Alexandre", "Laviolette", "Fran\u00e7ois", "Marchand", "Mario"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "Germain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2009}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Kumar", "Abhishek", "Daum\u00e9 III", "Hal"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["Lampert", "Christoph H", "Nickisch", "Hannes", "Harmeling", "Stefan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "Lampert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2013}, {"title": "Tutorial on practical prediction theory for classification", "author": ["Langford", "John"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Langford and John.,? \\Q2005\\E", "shortCiteRegEx": "Langford and John.", "year": 2005}, {"title": "PAC-Bayes and margins", "author": ["Langford", "John", "Shawe-Taylor"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Langford et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2002}, {"title": "PAC-Bayes risk bounds for stochastic averages and majority votes of sample-compressed classifiers", "author": ["Laviolette", "Fran\u00e7ois", "Marchand", "Mario"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Laviolette et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Laviolette et al\\.", "year": 2007}, {"title": "Transfer bounds for linear feature learning", "author": ["Maurer", "Andreas"], "venue": "Machine Learning,", "citeRegEx": "Maurer and Andreas.,? \\Q2009\\E", "shortCiteRegEx": "Maurer and Andreas.", "year": 2009}, {"title": "Sparse coding for multitask and transfer learning", "author": ["Maurer", "Andreas", "Pontil", "Massimiliano", "RomeraParedes", "Bernardino"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Maurer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maurer et al\\.", "year": 2013}, {"title": "PAC-Bayesian model averaging", "author": ["McAllester", "David"], "venue": "In Workshop on Computational Learning Theory (COLT),", "citeRegEx": "McAllester and David.,? \\Q1999\\E", "shortCiteRegEx": "McAllester and David.", "year": 1999}, {"title": "Simplified PAC-Bayesian margin bounds", "author": ["McAllester", "David"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "McAllester and David.,? \\Q2003\\E", "shortCiteRegEx": "McAllester and David.", "year": 2003}, {"title": "PAC-Bayes bounds with data dependent priors", "author": ["Parrado-Hern\u00e1ndez", "Emilio", "Ambroladze", "Amiran", "ShaweTaylor", "John", "Sun", "Shiliang"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Parrado.Hern\u00e1ndez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parrado.Hern\u00e1ndez et al\\.", "year": 2012}, {"title": "ELLA: An efficient lifelong learning algorithm", "author": ["Ruvolo", "Paul", "Eaton", "Eric"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "Ruvolo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ruvolo et al\\.", "year": 2013}, {"title": "Bayesian Gaussian Process Models: PAC-Bayesian Generalization Error Bounds and Sparse Approximations", "author": ["Seeger", "Matthias W"], "venue": "PhD thesis, University of Edinburgh,", "citeRegEx": "Seeger and W.,? \\Q2003\\E", "shortCiteRegEx": "Seeger and W.", "year": 2003}, {"title": "Lifelong robot learning", "author": ["Thrun", "Sebastian", "Mitchell", "Tom M"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Thrun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1995}, {"title": "A feasible method for optimization with orthogonality constraints", "author": ["Wen", "Zaiwen", "Yin", "Wotao"], "venue": "Mathematical Programming,", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Multi-task learning for classification with Dirichlet process priors", "author": ["Xue", "Ya", "Liao", "Xuejun", "Carin", "Lawrence", "Krishnapuram", "Balaji"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}, {"title": "Cross-domain video concept detection using adaptive SVMs", "author": ["Yang", "Jun", "Yan", "Rong", "Hauptmann", "Alexander G"], "venue": "In International Conference on Multimedia,", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 16, "context": "However, except for a few exception, such as (Maurer, 2009; Maurer et al., 2013), their theoretical justifications are so far not well understood.", "startOffset": 45, "endOffset": 80}, {"referenceID": 25, "context": "2 we demonstrate this process in two cases: assuming that the solutions to all tasks can be represented by a single parameter vector plus small task-specific perturbation (Evgeniou & Pontil, 2004), we obtain an algorithm that resembles previously proposed methods for regularizing the weight vectors of future tasks using linear combinations of weight vectors of previous tasks, such as (Yang et al., 2007; Aytar & Zisserman, 2011).", "startOffset": 387, "endOffset": 431}, {"referenceID": 0, "context": "(Argyriou et al., 2008; Kumar & Daum\u00e9 III, 2012), which have also successfully been applied in the lifelong learning setting (Ruvolo & Eaton, 2013).", "startOffset": 0, "endOffset": 48}, {"referenceID": 16, "context": "Similar results were obtained in (Maurer et al., 2013) in the case of sparsity constraints.", "startOffset": 33, "endOffset": 54}, {"referenceID": 0, "context": "(Argyriou et al., 2008; Kumar & Daum\u00e9 III, 2012), which have also successfully been applied in the lifelong learning setting (Ruvolo & Eaton, 2013). In a case of linear regression, Maurer (2009) used this assumption to prove a generalization bound in the PAC framework, by using the concept of environment of tasks from Baxter (2000) and Rademacher complexity.", "startOffset": 1, "endOffset": 195}, {"referenceID": 0, "context": "(Argyriou et al., 2008; Kumar & Daum\u00e9 III, 2012), which have also successfully been applied in the lifelong learning setting (Ruvolo & Eaton, 2013). In a case of linear regression, Maurer (2009) used this assumption to prove a generalization bound in the PAC framework, by using the concept of environment of tasks from Baxter (2000) and Rademacher complexity.", "startOffset": 1, "endOffset": 334}, {"referenceID": 19, "context": "Parrado-Hern\u00e1ndez et al. (2012) showed that priors can be learned by splitting the available training data into two parts, one for learning a prior, one for learning the predictor.", "startOffset": 0, "endOffset": 32}, {"referenceID": 25, "context": "It can be captured by regularizing the distance to this vector (Aytar & Zisserman, 2011; Yang et al., 2007):", "startOffset": 63, "endOffset": 107}, {"referenceID": 8, "context": "In this case the expected empirical error of the Gibbs classifier is given by the following expression (Germain et al., 2009; Langford & Shawe-Taylor, 2002)", "startOffset": 103, "endOffset": 156}, {"referenceID": 24, "context": "Land Mine Detection (Xue et al., 2007).", "startOffset": 20, "endOffset": 38}, {"referenceID": 0, "context": "We use the same procedure as in (Argyriou et al., 2008; Kumar & Daum\u00e9 III, 2012; Ruvolo & Eaton, 2013) to encode them in a set of binary features.", "startOffset": 32, "endOffset": 102}, {"referenceID": 11, "context": "Animals with Attributes Dataset (Lampert et al., 2013).", "startOffset": 32, "endOffset": 54}, {"referenceID": 3, "context": "into the direction of learning tasks of continuously improving difficulty (Bengio et al., 2009).", "startOffset": 74, "endOffset": 95}], "year": 2014, "abstractText": "Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far. In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.", "creator": "LaTeX with hyperref package"}}}