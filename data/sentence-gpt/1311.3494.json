{"id": "1311.3494", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2013", "title": "Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation", "abstract": "Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where \\emph{any} algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints? In this paper, we describe how a single set of results implies positive answers to the above, for a variety of settings. We discuss the general use of these constraints to define an efficient machine learning model in the following chapter: In our previous paper, we discuss the possible implementation of multiple learning models using the most common models. In this series, we describe two general approaches to understanding the general use of a multi-learning model in the following section. In this series, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal implementation of multiple learning models using the most common models. In this section, we discuss the optimal", "histories": [["v1", "Thu, 14 Nov 2013 13:21:15 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v1", null], ["v2", "Wed, 5 Feb 2014 14:55:06 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v2", null], ["v3", "Thu, 6 Feb 2014 06:23:28 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v3", null], ["v4", "Tue, 6 May 2014 10:56:31 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v4", "New and improved results compared to previous version"], ["v5", "Wed, 21 May 2014 18:35:13 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v5", null], ["v6", "Tue, 28 Oct 2014 13:25:09 GMT  (139kb,D)", "http://arxiv.org/abs/1311.3494v6", "Full version of NIPS 2014 paper"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1311.3494"}, "pdf": {"name": "1311.3494.pdf", "metadata": {"source": "CRF", "title": "Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation", "authors": ["Ohad Shamir"], "emails": ["ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Information constraints play a key role in statistical learning. Of course, the main constraint is the availability of only a finite data set, from which the learner is expected to generalize. However, many problems currently researched in machine learning can be characterized as learning with additional information constraints, arising from the manner in which the learner may interact with the data. Some examples include:\n\u2022 Communication constraints in Distributed Learning: There has been much work in recent years on learning when the training data is distributed among several machines (with [11, 2, 23, 36, 23, 25, 14, 28] being just a few examples). Since the machines may work in parallel, this potentially allows significant computational speed-ups. On the flip side, communication rates between machines is typically much slower than their processing speeds, and a major challenge has been to perform these learning tasks with minimal communication.\n\u2022 Memory constraints: The standard implementation of many common learning tasks require memory which is super-linear in the size of the data. For example, principal component analysis (PCA) requires us to estimate eigenvectors of the data covariance matrix, whose size is quadratic in the data dimension and can be prohibitive for high-dimensional data. Another example is kernel learning, which requires manipulation of the Gram matrix, whose size is\nar X\niv :1\n31 1.\n34 94\nv1 [\ncs .L\nG ]\n1 4\nN ov\nquadratic in the number of data points. There has been considerable effort in reducing the memory footprint of such algorithms (e.g. [32, 4, 41, 38]).\n\u2022 Online learning constraints: The need for fast and scalable learning algorithms has popularised the use of online algorithms, which work by sequentially going over the training data, and incrementally updating a (usually small) state vector. Well-known special cases include gradient descent and mirror descent algorithms (see e.g. [39, 40]). The requirement of sequentially passing over the data can be seen as a type of information constraint, whereas the small state these algorithms often maintain can be seen as another type of memory constraint.\n\u2022 Partial-information constraints: A common situation in machine learning practice is when the available data is corrupted, has missing features, or is otherwise partially accessible. There has also been considerable interest in online learning with partial information, where the learner only gets partial feedback on his performance. This has been used to model various problems in web advertising, routing and multiclass learning. Perhaps the most well-known case is the multi-armed bandits problem [15, 7, 6], with many other variants being developed, such as contextual bandits [29, 30] and combinatorial bandits [18]. There is also considerable research on more general partial-information models, such as partial monitoring [15, 10].\nAlthough these examples come from very different domains, they all share the common feature of information constraints on how the learning algorithm can interact with the training data. In some specific cases (most notably, multi-armed bandits, and very recently, in the context of certain distributed protocols [42]) we can even formalize the price we pay for these constraints, in terms of degraded sample complexity or regret guarantees. However, we currently lack a general informationtheoretic framework, which directly quantifies how such constraints can impact performance. For example, are there cases where any online algorithm, which goes over the data one-by-one, must have a worse sample complexity than (say) empirical risk minimization? Are there situations where a small memory requirement provably degrades the learning performance? Can one quantify how a constraint of getting only a few bits from each example affects our ability to learn? To the best of our knowledge, there are currently no generic tools which allow us to answer such questions.\nIn this paper, we make a first step in developing such a framework. We consider a general class of learning processes, characterized only by information-theoretic constraints on how they may interact with the data (and independent of any specific learning problem semantics). As special cases, these include online algorithms, certain types of distributed algorithms, as well as algorithms with small memory footprint. We identify cases where any such algorithm must be worse than what can be attained without information constraints (for example, performing empirical risk minimization).\nAs corollaries of these generic results, we establish several new results for specific learning problems:\n\u2022 We prove that for some learning and estimation problems - in particular, sparse PCA and sparse covariance estimation in Rd - no online algorithm can attain statistically optimal performance with less than \u2126\u0303(d2) memory. To the best of our knowledge, this is the first formal example of a memory/ statistical performance trade-off. This is also interesting in light of recent work (e.g. [32]), which proposed O(d) memory algorithms for other variants of PCA.\n\u2022 We show that for similar types of problems, no distributed algorithm (which is based on a non-interactive or round-robin protocol) can attain optimal performance with less than \u2126(d2) communication per machine.\n\u2022 We provide a substantial generalization of existing lower bounds for online learning with partial information. For example, in the context of multi-armed bandits, we prove the following: Consider any online algorithm which is allowed to retain any b bits out of the d-dimensional loss vector every round. Then the number of rounds required to get non-trivial regret is at least \u2126\u0303(d/b) times larger than for a full-information algorithm. This includes existing bandit lower bounds, but is also applicable to more general settings such as attribute efficient learning, semi-bandit feedback and partial monitoring.\n\u2022 We demonstrate the existence of simple learning problems where any stochastic optimization algorithm with state linear in the dimension (e.g. gradient descent or mirror descent) cannot be statistically optimal.\nOur paper is structured as follows. We begin with a survey of some related work, followed by a formal introduction of our framework in Sec. 2. Our main results are presented in Sec. 3, and their application to various learning problems are discussed in Sec. 4. To aid readability, we defer the presentation of our fully generic (and technical) results to Sec. 5, from which the main results in Sec. 3 are derived as corollaries. We end with a discussion in Sec. 6. Proofs of all our results are presented in the appendices."}, {"heading": "Related Work", "text": "In stochastic optimization, there has been much work on lower bounds for sequential algorithms, starting from the seminal work of Yudin and Nemirovsky [34], and including more recent works such as [1]. [37] also consider such lower bounds from a more general information-theoretic perspective. However, these results all hold in an oracle model, where data is assumed to be made available in a very specific form (such as a stochastic gradient estimate). As already pointed out in [34], this does not directly translate to the more common setting, where we are given a dataset and wish to run a simple sequential optimization procedure. Indeed, recent works exploited this gap to get improved algorithms using more sophisticated oracles, such as the availability of prox-mappings [35]. Moreover, we are not aware of cases where these lower bounds indicate a gap between the attainable performance of any sequential algorithm and batch learning methods (such as empirical risk minimization, possibly with regularization).\nIn the context of distributed learning and statistical estimation, information-theoretic lower bounds have been recently shown in the pioneering work [42]. The authors focus on a setting where there are communication budget constraints on different machines, and identify cases where these constraints affect the statistical performance. Our results (in the context of distributed learning) are very similar in spirit, but there are two important differences. First, their lower bounds pertain to parametric estimation in Rd, and are non-trivial when the budget size per machine is much smaller than d. However, for many natural applications, sending O(d) bits is not considered overly expensive1. In contrast, our basic results pertain to simpler detection problems, and lead to nontrivial lower bounds in the natural regime where the budget size is the same or even larger than\n1For example, [43] recently proposed a communication-efficient algorithm for distributed stochastic optimization, which relies on a single distributed averaging of d-dimensional vectors.\nthe size of the parameter vector (e.g. as much as quadratic in the context of sparse PCA). The second difference is that their work focuses mostly on non-interactive distributed protocols, while we address a more general class, which also includes information-constrained algorithms beyond distributed learning.\nAs mentioned earlier, there are well-known lower bounds for multi-armed bandit problems and other online learning with partial-information settings. These lower bounds are usually tight and demonstrate a degradation in performance compared to full-information algorithms. However, these lower bounds are specific to the partial information feedback considered. For example, the standard multi-armed bandit lower bound [7] pertain to a setting where we can view a single coordinate of the loss vector, but doesn\u2019t apply as-is when we can view more than one coordinate (as in semibandit feedback [27, 5] or bandits with side-information [31]), or receive a different type of partial feedback (such as in partial monitoring [17]).\nThe inherent limitations of streaming and distributed algorithms, including memory and communication constraints, have been extensively studied within theoretical computer science (e.g. [3, 8, 20, 33, 9]). These include quite powerful and general tools, which also apply to algorithms beyond those considered here (such as distributed algorithms with interactive protocols). However, to the best of our knowledge, these results do not apply to statistical learning tasks, where the data is assumed to be drawn i.i.d. from some underlying distribution."}, {"heading": "2 Information-Constrained Protocols", "text": "We begin with a few words about notation. We use bold-face letters (e.g. x) to denote vectors, with xj denoting the j-th coordinate. In particular, we let ej \u2208 Rd denote the vector which is 1 in coordinate j and zero everywhere else. In general, upper-case letters denote random variables. We use the standard asymptotic notation O(\u00b7),\u2126(\u00b7) to hide constants, and O\u0303(\u00b7), \u2126\u0303(\u00b7) to hide constants and logarithmic factors. loge(\u00b7) refers to the natural logarithm, and log(\u00b7) to the base-2 logarithm.\nThe main focus of our paper is to study settings where there is a gap between the statistically optimal performance which can be attained based on i.i.d. training data, and the best possible performance attained by algorithms with information constraints. In particular, we study a generic type of information-constrained protocol, defined as follows:\nDefinition 1 ((b, n,m) Protocol). Given access to mn i.i.d. instances from some distribution, an algorithm is a (b, n,m) protocol if it has the following form, for arbitrary functions fk returning an output of at most b bits, and an arbitrary function f :\n\u2022 For k = 1, . . . ,m\n\u2013 Let Xk be an i.i.d. sample of n instances \u2013 Compute message W k = fk(X k,W 1,W 2, . . .W k\u22121)\n\u2022 Return W = f(W 1, . . . ,Wm)\nNote that the functions are completely arbitrary and can also be randomized. A crucial assumption however is that the size of the messages W k are constrained to be only b bits. This will form the information constraints mentioned earlier. Our results can be generalized to allow the size of W k to vary across k, and even to be chosen in a data-dependent manner.\nExamples of (b, n,m) protocols include:\n\u2022 Online learning algorithms: The data is streamed one-by-one or in mini-batches of size n, with mn instances overall. An algorithm sequentially updates its state based on a b-dimensional vector extracted from each example/batch (such as a gradient or gradient average), and returns a final result after all data is processed. This includes most gradient-based algorithms we are aware of. Also, it includes distributed versions of these algorithms (such as parallelizing the mini-batch processing step [24, 21]).\n\u2022 Algorithms for online learning with partial information: d-dimensional data sampled i.i.d., with n = 1 and m = T instances overall. Each step the algorithm can output b bits based on an instances, where b d.\nWe will also consider the following special (but still quite general) case:\nDefinition 2 (Bounded-memory (b, n,m) Protocol). A given algorithm is a bounded-memory (b, n,m) protocol if it is a (b, n,m) protocol, and for each k = 1 . . .m, W k = fk(X\nk,W k\u22121), and W = f(W 1, . . . ,Wm) = f(Wm) (that is, fk and f depend only on the message from the last iteration).\nIntuitively, bounded-memory protocols cannot simply accumulate information about previous data seen, and are forced to maintain a state W k of at most b bits which is incrementally updated.\nExamples of bounded-memory (b, n,m) protocols include:\n\u2022 Stochastic optimization algorithms which maintain a state vector of size b. For example, for linear predictors, most single-pass gradient-based algorithms maintain a state whose size is proportional to the size of the parameter vector that is being optimized.\n\u2022 Non-interactive and round-robin distributed algorithms: There are m machines and each machine receives an independent sample Xk of size n. It then sends a message W k = fk(X\nk) (which here depends only onXk). A centralized server then combines the messages to compute an output f(W 1 . . .Wm). This includes for instance the AVGM and Fast-KRR algorithms proposed in [43, 44] for distributed stochastic optimization. A round-robin variant of the above is when there are m machines, and one-by-one, each machine k broadcasts some information W k to the other machines, which depends onXk as well as previous messages sent by machines 1, 2, . . . , (k \u2212 1).\nIn our work, we contrast the performance attainable by such protocols, to constraint-free protocols which are allowed to interact with the available mn sampled instances in any manner."}, {"heading": "3 Main Results", "text": "All our results are based on a simple \u201chide-and-seek\u201d statistical estimation problem, for which we show a strong gap between the attainable performance of (b, n,m) protocols and constraint-free protocols. In the next section, we show how some well-known learning and statistical estimation tasks can be reduced to these simple problems.\nWe will consider two variants of this problem, with different applications. Our first problem, parameterized by a dimension d, bias \u03c1, and sample size mn, is defined as follows:\nDefinition 3 (Hide-and-seek Problem 1). Consider the set of distributions {PrJ(\u00b7)}dJ=1 over {\u22121,+1}d defined as PrJ(x) = 2 \u2212d+1 (1 2 + \u03c1xJ ) . Given an i.i.d. sample of mn instances generated from PrJ(\u00b7), where J is unknown, detect J .\nIn words, PrJ(\u00b7) corresponds to picking all coordinates other than J to be \u00b11 uniformly at random, and picking coordinate J independently to be +1 with probability ( 1 2 + \u03c1 ) , and \u22121 with\nprobability ( 1 2 \u2212 \u03c1 ) . It is easily verified that this creates instances with zero-mean coordinates, except coordinate J whose expectation is 2\u03c1. The goal is to detect J based on an empirical sample. We now present our first main result, which shows that for this hide-and-seek problem, there is a large regime where detecting J is information-theoretically possible, but any (b, n,m) protocol will fail to do so with high probability.\nTheorem 1. Consider hide-and-seek problem 1, and suppose \u03c1 = \u221a c/mn for some parameter\nc \u2264 min { m 54 , m 27 loge(2dn/b) } . Then for any estimate J\u0303 of J returned by any (b, n,m) protocol, there exists some J such that\nPrJ(J\u0303 = J) \u2264 1\nlog(d) +\n6mb\nd .\nHowever, given mn samples, if J\u0303 is the coordinate with the highest empirical average, then PrJ(J\u0303 = J) \u2265 1\u2212 2d exp ( \u221212c ) .\nThis theorem implies that in a regime where mb (the total number of bits in the messages written by the protocol after viewing each set of instances) is smaller than d, and as long as \u02dc\u2126(1) \u2264 c \u2264 O\u0303(m), then any (b, n,m) protocol will fail to detect J with high probability, even though a simple constraint-free protocol (just using the plug-in estimate for the expectation of each coordinate) succeeds with exponentially high probability.\nThe proof of the theorem follows from the more general Thm. 6 presented later, and the derivation appears in Appendix C.1. However, the technical details may obfuscate the proof\u2019s simple intuition, which we now turn to explain.\nThe upper bound just follows from the fact that the expectation of any bounded random variable can be estimated to accuracy O( \u221a 1/mn), hence detecting the biased coordinate is feasible when\n\u03c1 = \u2126( \u221a\n1/mn). However, in a setting where the instances are given to us one-by-one or in batches, this requires us to maintain an estimate for all d coordinates. Unfortunately, when we can only output a small number b of bits based on each instance/batch, we cannot accurately update the estimate for all d coordinates. For example, we can provide some information on all d coordinates, but then the information we can provide per coordinate (and in particular, the biased coordinate J) will be very small. Alternatively, we can provide accurate information on O(b) coordinates, but we don\u2019t know which coordinate is the important coordinate J , hence we are likely to \u201cmiss\u201d it. In fact, the proof relies on showing that no matter what, a (b, n,m) protocol cannot provide more than b/d bits of information (in expectation) on coordinate J . As there are only m rounds overall, the amount of information conveyed on coordinate J is at most mb/d. If this is much smaller than 1, there is insufficient information to accurately detect J .\nFrom a more information-theoretical viewpoint, one can view this as a result on the information transmission rate of a channel as illustrated in figure 1. In this channel, the message J is sent through one of d independent binary symmetric channels (corresponding to the j-th coordinate in the data instances). Even though W constitutes b bits, the information on J it can convey is no larger than b/d in expectation, since it doesn\u2019t \u201cknow\u201d which of the channels to decode.\nThm. 1 applies to any (b, n,m) protocol, memory-bounded or not. A stronger result is possible if we consider bounded-memory protocols. Whereas Thm. 1 theorem required mb d to be nontrivial, the next theorem applies already when cb d. Since c \u2264 O\u0303(m) always (and in general, we think of c as a reasonably large constant), the result is only stronger. To simplify the presentation, we will use asymptotic notation O\u0303(\u00b7), \u2126\u0303(\u00b7), which hides constants and logarithmic factors. We provide a precise re-statement of the theorem as part of the proof.\nTheorem 2. Consider hide-and-seek problem 1. Suppose \u03c1 = \u221a c/mn for some parameter 0.02 \u2264 c \u2264 O\u0303(m), and suppose that b = O(d) (where the asymptotic notation hides suitable constants and logarithmic factors). Then for any estimate J\u0303 of J returned by any bounded-memory (b, n,m) protocol, there exists some J such that\nPrJ(J\u0303 = J) \u2264 1\nlog(d) + O\u0303\n( cb\nd\n) .\nHowever, given mn samples, if J\u0303 is the coordinate with the highest empirical average, then PrJ(J\u0303 = J) \u2265 1\u2212 2d exp ( \u221212c ) .\nThe theorem is in fact a corollary of the more general Thm. 7 presented later, and the derivation appears in Appendix C.2. It is based on the following simple observation: Any bounded-memory (b, n,m) protocol is also a bounded-memory ( b, \u03ban, \u2308 m \u03ba \u2309) for any positive integer \u03ba \u2264 m. This is because given a a batch of \u03ban instances, we can always split it into \u03ba segments of n instances each, feed the segments one by one to our (b, n,m) protocol, and output the final message after m such segments are processed, ignoring any remaining instances (note that the bounded-memory assumption is crucial here). As a result, we can apply Thm. 1 to ( b, \u03ban, \u2308 m \u03ba \u2309) protocols, and choose the value of \u03ba which provides the strongest possible bound. For some of the applications we consider, hide-and-seek problem 1 isn\u2019t appropriate, and we\nwill need the following alternative problem, which is again parameterized by a dimension d, bias \u03c1, and sample size mn:\nDefinition 4 (Hide-and-seek Problem 2). Consider the set of distributions {PrJ(\u00b7)}dJ=1 over {\u2212ej ,+ej}dj=1, defined as\nPrJ(ej) = { 1 2d j 6= J 1 2d + \u03c1 d j = J PrJ(\u2212ej) = { 1 2d j 6= J 1 2d \u2212 \u03c1 d j = J .\nGiven an i.i.d. sample of mn instances generated from PrJ(\u00b7), where J is unknown, detect J .\nIn words, PrJ(\u00b7) corresponds to picking \u00b1ej where j is chosen uniformly at random, and the sign is chosen uniformly if j 6= J , and to be positive (resp. negative) with probability 12 + \u03c1 (resp. 1 2 \u2212 \u03c1) if j = J . It is easily verified that this creates sparse instances with zero-mean coordinates, except coordinate J whose expectation is 2\u03c1/d.\nWe now present a result analogous to Thm. 2 for this new hide-and-seek problem. Again to simplify the presentation, we will use asymptotic notation to hide constants and logarithmic factors, but provide a precise re-statement of the theorem as part of the proof.\nTheorem 3. Consider hide-and-seek problem 2. Suppose \u03c1 = \u221a cd/mn for some parameter c such that 0.02 \u2264 c \u2264 O\u0303(m), and suppose that \u03c1 \u2264 O\u0303(1) (where the asymptotic notation hides suitable constants and logarithmic factors). Then for any estimate J\u0303 of J returned by any bounded-memory (b, n,m) protocol, there exists some J such that\nPrJ(J\u0303 = J) \u2264 1\nlog(d) + O\u0303\n( cb\nd\n) .\nHowever, given mn samples, if J\u0303 is the coordinate with the highest empirical average, then PrJ(J\u0303 = J) \u2265 1\u2212 d exp ( \u221213c ) .\nThe proof appears in Appendix C.3. The theorem implies that in a rather broad regime, we attain a strong gap between what can be obtained information-theoretically, and using any boundedmemory (b, n,m) protocol. It is also a possible to get a version of Thm. 1 to this hide-and-seek problem, but we omit it as we currently don\u2019t have an immediate application."}, {"heading": "4 Applications", "text": ""}, {"heading": "4.1 Online Learning with Partial Information", "text": "Consider the standard multi-armed bandits setting, defined as a game over T rounds, where each round t a loss vector `t \u2208 [0, 1]d is chosen, and the learner (without knowing `t) needs to pick an action jt from a fixed set {1, . . . , d}, after which the learner suffers loss `t(jt). The goal of the learner is to minimize the regret in hindsight to the best fixed action, \u2211T t=1 `t(jt) \u2212 minj `t(j). Crucially, the learner never gets to see `t, but only `t(jt). The following theorem is a simple corollary of Thm. 1, and we sketch the proof in Appendix D.1\nTheorem 4. Suppose log(d) \u2265 4. For any algorithm which can view only b bits of each loss vector, there is a distribution over loss vectors `t \u2208 [0, 1]d with the following property: If the loss vectors are sampled i.i.d. from this distribution, then minj E[ \u2211T t=1 `t(jt) \u2212 \u2211T t=1 `t(j)] \u2265 \u2126\u0303(T ) as long as T \u2264 O(d/b).\nAs a result, we get that for any algorithm for online learning with any partial information feedback model (where b bits are extracted from each d-dimensional loss vector), it is impossible to get sub-linear regret guarantees in less than \u2126(d/b) rounds. In contrast, full-information algorithms (e.g. Hedge [26]) can get sublinear regret in O(log(d)) rounds.\nWhen b = O(1), this bound matches (up to logarithmic factors) a standard lower bound for multi-armed bandits, in terms of the relation2 of d and T . However, it\u2019s actually much more general, since the b bits extracted are arbitrary and don\u2019t need to correspond to the bandit feedback model (where we view a single coordinate). These bits may even be chosen by the learner in a datadependent manner. For example, we immediately get an \u2126(d/k) lower bound when we are allowed to view k coordinates instead of 1, corresponding to (say) the semi-bandit feedback model [18], or the side-observation model of [31] with a fixed upper bound k on the number of side-observations. In partial monitoring [17], we get a \u2126(d/k) lower bound where k is the logarithm of the feedback matrix width. In attribute efficient learning (e.g. [19]), a simple reduction implies an \u2126(d/k) sample complexity lower bound when we are constrained to view at most k features of each example. In general, for any current or future feedback model which corresponds to getting a limited number of bits from the loss vector, our results imply a non-trivial lower bound."}, {"heading": "4.2 Sparse PCA and Sparse Covariance Estimation", "text": "The sparse PCA problem [45] is defined as follows: We are given an i.i.d. sample of vectors x \u2208 Rs, and we assume that there is some direction, corresponding to some sparse vector v (of cardinality at most k), such that the variance E[(v>x)2] along that direction is larger than at any other direction. Our goal is to find that direction.\nWe will focus here on a simple variant of this problem, where the maximizing direction v is assumed to be 2-sparse, i.e. there are only 2 non-zero coordinates vi, vj . In that case, E[(v>x)2] = v21E[x21]+v22E[x22]+2v1v2E[xixj ]. Following previous work (e.g. [12]), we even assume that E[x2i ] = 1 for all i, in which case the sparse PCA problem reduces to detecting a coordinate pair (i, j), i 6= j for which xi, xj are maximally correlated. A special case is a sparse covariance estimation problem [13, 16], where we assume that E[xixj ] = 0 except for some pair (i, j) which we need to detect.\nFor this problem, we can show the following gap between the attainable performance of any bounded-memory (b, n,m) protocol, and a simple plug-in estimator without information constraints. Again, we use asymptotic notation to hide explicit constants and logarithmic factors.\nTheorem 5. Consider the class of 2-sparse PCA (or covariance estimation) problems in s > 2 dimensions as described above, and suppose that the sample size mn is \u2126\u0303(cs2) for some positive parameter c \u2208 [0.02, O\u0303(m)]. Consider all distributions such that:\n\u2022 E[x2i ] = 1 for all i.\n\u2022 There is a pair of distinct coordinates (I, J) such that |E[xIxJ ]| \u2265 max(i,j)6=(I,J),i<j |E[xixj ]|+\u03c4 for some \u03c4 > 0.\n\u2022 \u03c4 is sufficiently large so that for any i < j, if x\u0303ixj is the empirical average of xixj, then Pr ( |x\u0303ixj \u2212 E[xixj ]| \u2265 \u03c4\n2\n) \u2264 2 exp(\u2212c/3)\n2Based on a conjectured improvement of our results, it should also be possible to prove a \u2126\u0303( \u221a\n(d/b)T ) regret lower bound, which holds for any d, T and any algorithm extracting b bits from the loss vector. See Sec. 6 for more details."}, {"heading": "Then the following holds:", "text": "\u2022 Let (I\u0303 , J\u0303) = arg max(i,j) | x\u0303ixj |. Then for any distribution, Pr((I\u0303 , J\u0303) = (I, J)) \u2265 1 \u2212 s2 exp (\u2212c/3)\n\u2022 For any estimate (I\u0303 , J\u0303) of (I, J) returned by any bounded-memory (b, n,m) protocol, there exists a distribution such that Pr((I\u0303 , J\u0303) = (I, J)) \u2264 1 log(s2\u2212s)\u22121 + O\u0303 ( cb s2 ) .\nThe proof appears in Appendix D.2. Intuitively, it uses a reduction of the sparse PCA problem to the setting considered in Thm. 3. For a suitably large constant c, the theorem implies that a simple constraint-free protocol will succeed with high probability in detecting (I, J), while any boundedmemory (b, n,m) protocol will fail with high probability, unless b = \u2126\u0303(s2). This means that to get the optimal statistical performance, the memory used by the protocol must scale quadratically with the dimension s.\nTo the best of our knowledge, this is the first result which explicitly shows that memory constraints must incur a statistical cost for a standard estimation problem. It is interesting that sparse PCA was also shown recently to be affected by computational constraints on the algorithm\u2019s runtime [12]. Finally, as an interesting contrast, we note that recent work [32] proposed a linear-memory algorithm for PCA under a different setting, where the relevant directions are not necessarily sparse and the data distribution is Gaussian.\nA special case of bounded-memory (b, n,m) protocols are non-interactive (or round-robin) distributed algorithms over m machines. Thus, Thm. 5 holds also for such distributed algorithms, implying that they cannot match the performance of the simple plug-in method unless each machine communicates b = \u2126\u0303(s2) bits."}, {"heading": "4.3 Stochastic Optimization", "text": "Thm. 5, which deals with a detection problem, can be turned to a result about stochastic optimization, where our goal is to minimize a function of the form minw\u2208W Ezf(w, z) (for some domainW) given access to i.i.d. samples of z from some domain Z. For example, consider the simple ridge regression problem\nmin w \u2016w\u20162 + E(x,y)\n[ (\u3008w,x\u3009 \u2212 y)2 ] .\nThis is equivalent to minimizing w> ( I + E[xx>] ) w \u2212 2E[yx>]w.\nWe can consider a family of distributions where E[yx>] is fixed, and E[xx>] is zero outside the main diagonal, except at some unknown coordinates (I, J) where it is positively biased. This means that detecting (I, J) will allow us to attain zero error for this optimization problem, and this reduces to the sparse PCA setting of Thm. 5. Thus, it is possible to show that a constraint-free protocol can detect (I, J) and attain zero error with high probability, whereas any bounded-memory (b, n,m) protocol will fail and attain some positive error with high probability.\nThis example, however, requires distributional assumptions. Many algorithms for stochastic optimization are analyzed under a distribution-free setting, where any distribution over the examples is allowed, and our goal is to attain the minimax optimal rate over all distributions. We\nnow turn to demonstrate a simple (although not necessarily realistic) stochastic optimization problem, where information-constrained protocols attains a suboptimal rate even without distributional assumptions.\nSuppose we wish to minimize a stochastic function of the form f((w,v);Z) = w>Zv, where Z is a random matrix in [\u22121,+1]s\u00d7s and w,v range over all vectors in the simplex (i.e. wi, vi \u2265 0 and \u2211d i=1wi = \u2211d i=1 vi = 1). If E[Z] has a minimal element at location (I, J), then E[f((w,v);Z)] is minimized at w = eI ,v = eJ , and attains a value equal to that minimal element. Therefore, a simple concentration of measure argument implies that given mn samples Z1, Z2, . . . from any distribution, a constraint-free protocol which computes their average Z\u0304 and returns w = eI ,v = eJ for (I, J) = arg min(I,J) Z\u0304I,J satisfies f((w,v);Z)\u2212minw,v E [f((w,v);Z)] \u2264 O\u0303(1/ \u221a mn) with high probability. Thus, the optimization error rate attained with a constraint-free protocol is O\u0303(1/ \u221a mn).\nNow, consider any (b, n,m) protocol with b = O(s). This corresponds, for instance, to first order-optimization algorithms which use gradients (whose size is linear in the dimension s), or non-interactive distributed algorithms with linear communication budget. We argue that the optimization error of any such protocol can be as large as \u2126\u0303(1/ \u221a n), which is much larger than the O\u0303(1/ \u221a mn) rate attainable with constraint-free methods. To see why, consider the case where Z \u2208 {\u22121,+1}s\u00d7s with probability 1, the coordinates are chosen independently, and E[Z] is zero except some coordinate (I, J) where it equals \u2212\u03c1. This reduces to the setting discussed in Thm. 1 and Thm. 2, with d = s2. In particular, for \u03c1 as large as \u2126\u0303( \u221a 1/n), (b, n,m) protocols can fail to detect\n(I, J) with constant probability. Since getting optimization error much smaller than \u03c1 = \u2126\u0303( \u221a\n1/n) reduces to detecting (I, J), the attainable optimization error of any such protocol can be as large as \u2126\u0303( \u221a 1/n).\nInterestingly, this example also establishes a gap between first-order optimization algorithms (which use only gradient information from each instance), and second-order optimization algorithms (which may also use Hessians). Since the Hessian of f((w,v);Z) corresponds to Z, a second-order algorithm can implement the plug-in estimate above and attain the statistically optimal rate. In contrast, any first-order algorithm will attain an inferior rate.\n5 Generic Bounds for (b, n,m) Protocols\nRecall that in Sec. 3, we discussed two simple \u201chide-and-seek\u201d problems (definitions 3 and 4), each corresponding to a particular collection of distributions {PrJ(\u00b7)}dJ=1, with the goal of detecting J . In this section, we describe two results (one for (b, n,m) protocols and one for bounded-memory (b, n,m) protocols), which can be applied to any set of distributions {PrJ(\u00b7)}dJ=1 satisfying certain conditions, including potentially others than those discussed in this paper. In fact, the theorems of Sec. 3 are obtained as corollaries. We present them here, as they may prove useful in studying information constraints in other learning and statistical estimation problems.\nGenerically, we consider a set of distributions {PrJ(\u00b7)}dJ=1 supported on Ad where A is a finite set (such as {\u22121,+1} or {\u22121, 0,+1}), and the problem of detecting J given mn i.i.d. samples from PrJ(\u00b7). We first provide a generic bound for any (b, n,m) protocol (possibly non-memory-bounded). The result assumes we can find another \u201creference\u201d distribution Pr0(\u00b7) over the same domain as {PrJ(\u00b7)}, which is \u201cclose\u201d to all of them in an appropriately defined sense. In the result, we use H0(\u00b7) to denote the entropy function measured in bits (see Appendix E) with respect to Pr0.\nTheorem 6. Let {PrJ(\u00b7)}dJ=1 and Pr 0(\u00b7) be a set of distributions over a product domain Ad (where A is a finite set) with the following properties:\n1. \u2200 J, {xj}j 6=J , PrJ({xj}j 6=J |xJ) = Pr0({xj}j 6=J |xJ).\n2. \u2211d\nJ=1H 0(xJ)\u2212H0(x) \u2264 \u03b2 for some \u03b2 \u2265 0. 3. \u2200 J, xJ , Pr0(xJ) > 0, and in particular log (\n1 minJ,xJ Pr 0(xJ )\n) \u2264 \u03b3 for some \u03b3 > 0\n4. For some \u00b5 > 0, maxJ E0 [ log2e ( PrJ (xJ )\nPr0(xJ )\n)] \u2264 \u00b52 and 2 maxJ,xJ \u2223\u2223\u2223loge (PrJ (xJ )Pr0(xJ ))\u2223\u2223\u2223 \u2264 max{\u00b5, n\u00b52} 5. \u00b5 is sufficiently small so that 6n\u00b52 \u2264 1 and 3n\u00b52 loge ( 2dn\u03b3 b+\u03b2 ) \u2264 1.\nThen for any (b, n,m) protocol, there exists some J such that if the protocol is presented with i.i.d. samples from PrJ(\u00b7), and returns an estimate J\u0303 of J , then\nPrJ(J\u0303 = J) \u2264 1\nlog(d) + 6m\nb+ \u03b2\nd .\nThe formal proof appears in Appendix A. To understand the theorem\u2019s conditions, it is useful to consider the \u201chide-and-seek\u201d problem of definition 3, where PrJ(\u00b7) are supported on {\u22121,+1}d, and corresponds to choosing each coordinate independently to be \u00b11 with equal probability, except coordinate J which equals +1 with probability 12 + \u03c1 for some bias \u03c1. Also, we let Pr\n0(\u00b7) be the uniform distribution over {\u22121,+1}d.\nRoughly speaking, condition 1 is that given xJ , the distribution of the other coordinates does not depend on J , and condition 2 requires the coordinates to be approximately independent. This is certainly satisfied, for instance, when the coordinates are chosen independently. Condition 3 requires no value in location J to have too small a probability. Condition 4 requires PrJ(\u00b7) and Pr0(\u00b7) to be similar (as quantified by \u00b52), and condition 5 requires \u00b52 to be sufficiently small compared to n (up to logarithmic factors). For the \u201chide-and-seek\u201d problem of definition 3, it can be shown that \u03b2 = 0, \u03b3 = 1, and \u00b52 = 6\u03c12 where \u03c1 is the bias term (this is formally shown as part of proving Thm. 1).\nIn the case of bounded-memory protocols, the result can be strengthened as follows:\nTheorem 7. Suppose the conditions of Thm. 6 still hold, when we replace condition 4 and 5 with\n1. loge ( 2dmn\u03b3 b+\u03b2 ) \u2265 2, and 2 maxJ,xJ \u2223\u2223\u2223loge (PrJ (xJ )Pr0(xJ ))\u2223\u2223\u2223 \u2264 16 loge( 2dmn\u03b3b+\u03b2 ) 2. For some \u00b5 > 0, maxJ E0 [ log2e ( PrJ (xJ )\nPr0(xJ )\n)] \u2264 \u00b52.\n3. \u00b52 is such that 6mn\u00b52 loge ( 2dmn\u03b3 b+\u03b2 ) \u2208 [2,m].\nThen for any bounded-memory (b, n,m) protocol, there exists some J such that if the protocol is presented with i.i.d. samples from PrJ(\u00b7), and returns an estimate J\u0303 of J , then\nPrJ(J\u0303 = J) \u2264 1\nlog(d) + 54 loge\n( 2dmn\u03b3\nb+ \u03b2\n) mn\u00b52(b+ \u03b2)\nd\nThe formal proof appears in Appendix B. As discussed after Thm. 2, the crucial observation here is that any bounded-memory (b, n,m) protocol is also a bounded-memory ( b, \u03ban, \u2308 m \u03ba \u2309) protocol\nfor any positive integer \u03ba \u2264 m. Therefore, we can apply Thm. 6 for ( b, \u03ban, \u2308 m \u03ba \u2309) protocols, and choose the value of \u03ba which provides the strongest possible bound."}, {"heading": "6 Discussion and Open Questions", "text": "In this paper, we investigated cases where a generic type of information-constrained algorithm has strictly inferior statistical performance compared to constraint-free algorithms. As special cases, we demonstrated such gaps for first-order online and distributed optimization algorithms (e.g. in the context of sparse PCA and covariance estimation), and derived regret lower bounds for online learning with partial information, depending only on the number of bits received at each round. We believe these results form a first step in a fuller understanding of how information constraints affect learning ability in a statistical setting.\nThere are several immediate questions left open. One question is whether our bounds for (b, n,m) protocols in Thm. 1 (and its generalized counter-part in Thm. 6) can be improved. We conjecture this is true, and that the bound can be as good as the one for memory-bounded protocols in Thm. 2 (and Thm. 7 respectively3.) This would allow us, for instance, to recover a stronger version of Thm. 4 for online learning with partial information, implying a regret lower bound of \u2126\u0303( \u221a\n(d/b)/T ) for any number of rounds T . A second open question is whether there are convex and distribution-free stochastic optimization problems, for which online or distributed algorithms are provably inferior to constraint-free algorithms. The results discussed in Subsection 4.3 imply such cases under either distributional assumptions, or for non-convex (but still easily solvable) problems. Due to the large current effort in developing scalable algorithms for convex learning problems, this would establish that one must pay a statistical price for using such memory-and-time efficient algorithms.\nA third open question is whether the results for non-interactive (or round-robin) distributed algorithms can be extended to interactive algorithms, where the different machines can communicate over several rounds. As mentioned earlier, there is a rich literature on the communication complexity of interactive distributed algorithms within theoretical computer science, but we don\u2019t know how to \u201cimport\u201d these results to a statistical setting based on i.i.d. data.\nMore generally, there is much work remaining in extending the results here to other learning problems or other information constraints."}, {"heading": "A Proof of Thm. 6", "text": "The proof in this section will require some additional notation:\n\u2022 Let Xkj denote the vector of values of the j-th coordinate in Xk (the n instances given to the (b, n,m) protocol in iteration k).\n\u2022 To simplify the presentation, we will abuse notation and use the same letter to denote both a random variable and its possible values. For example, the notation \u2211 XkJ denotes a sum over\nall possible instantiations of XkJ .\n\u2022 We use E0, H0 and I0 to denote expectation, entropy and mutual-information respectively with respect to the probability measure Pr0(\u00b7).\nAlso, the proof requires use of several standard quantities and results from information theory \u2013 see Appendix E for more details."}, {"heading": "A.1 Technical Lemmas", "text": "Lemma 1. For any joint distribution p(w, j) and distribution q(w), it holds that\nEj\u223cpDkl(p(w|j)||p(w)) \u2264 Ej\u223cpDkl(p(w|j)||q(w))"}, {"heading": "Proof.", "text": "Ej\u223cpDkl(p(w|j)||q(w)) = \u2211 w,j p(w, j) log p(w, j) p(j)q(w) = \u2211 w,j p(w, j) log ( p(w, j) p(w)p(j) p(w) q(w) ) = \u2211 w,j p(w, j) log p(w, j) p(w)p(j) + \u2211 w,j p(w, j) log p(w) q(w)\n= Ej\u223cpDkl(p(w|j)||p(w)) + \u2211 w p(w) log p(w) q(w) = Ej\u223cpDkl(p(w|j)||p(w)) +Dkl(p(w)||q(w)) \u2265 Ej\u223cpDkl(p(w|j)||p(w)),\nwhere the last step is due to relative entropy being non-negative.\nLemma 2. For any non-negative random variables Q,D, such that Pr(D \u2264 b) = 1, and any positive c, E[QD] \u2264 c E[D] + b ( cPr(Q > c) + \u222b \u221e z=c Pr(Q > z)dz ) .\nProof. We use the standard fact that if Y is a non-negative random variable, E[Y ] = \u222b\u221e z=0 Pr(Y > z)dz. We have\nE[QD] = E[1Q\u2264cQD] + E[1Q>cQD] \u2264 c E[D] + b E[1Q>cQ] = c E[D] + b \u222b \u221e z=0 Pr(1Q>cQ > z)dz\n= c E[D] + b (\u222b c\nz=0 Pr(1Q>cQ > z)dz + \u222b \u221e z=c Pr(1Q>cQ > z)dz ) = c E[D] + b ( cPr(Q > c) +\n\u222b \u221e z=c Pr(Q > z)dz )\nLemma 3. Suppose that for some \u00b5 > 0, it holds that\nmax J\nE0 [ log2e ( PrJ(xJ)\nPr0(xJ)\n)] \u2264 \u00b52 and 2 max\nJ,xJ \u2223\u2223\u2223\u2223loge(PrJ(xJ)Pr0(xJ) )\u2223\u2223\u2223\u2223 \u2264 max{\u00b5, n\u00b52}\nThen for any J and any z \u2265 exp(1),\nPr0 ( PrJ(X k J )\nPr0(XkJ ) > z\n) \u2264 z\u22121/(3n\u00b52).\nProof. Since the choice of J, k don\u2019t matter, let us drop them and denote xi = X k i,J . The data is sampled i.i.d., so the left hand side in the stated inequality equals Pr0 ( Pr(x1 . . . xn)\nPr0(x1 . . . xn) > z\n) = Pr0 ( \u220fn i=1 Pr(xi)\u220fn i=1 Pr 0(xi) > z ) = Pr0 ( n\u2211 i=1 loge ( Pr(xi) Pr0(xi) ) > loge(z) ) .\nThe sum is over i.i.d. random variables (as each is a function of xi), and their expectation are non-positive, since E0 [ loge ( Pr(xi)\nPr0(xi)\n)] = \u2212Dkl ( Pr0(xi)||Pr(xi) ) and relative entropy is always non-negative. Therefore, we can upper bound the above by\nPr0 ( n\u2211 i=1 ( loge ( Pr(xi) Pr0(xi) ) \u2212 E0 [ loge ( Pr(xi) Pr0(xi) )]) > loge(z) ) . (1)\nNow the sum is over i.i.d. and zero-mean random variables. We consider two cases:\n\u2022 Suppose that n\u00b52 \u2264 \u00b5. Then the magnitude of each random variable is at most 2 maxxi \u2223\u2223\u2223loge ( Pr(xi)Pr0(xi))\u2223\u2223\u2223 \u2264\n\u00b5. Applying Hoeffding\u2019s inequality to Eq. (1), we can upper bound it by\nexp ( \u22122 log 2 e(z)\nn\u00b52\n) \u2264 exp ( \u22122 loge(z)\nn\u00b52\n) = z\u22122/(n\u00b5 2),\nwhere we used the assumption that loge(z) \u2265 1. \u2022 Suppose that n\u00b52 > \u00b5. Then the magnitude of each random variable is at most 2 maxxi \u2223\u2223\u2223loge ( Pr(xi)Pr0(xi))\u2223\u2223\u2223 \u2264\nn\u00b52, and has second moment of at most E0 [(\nloge\n( Pr(xi)\nPr0(xi)\n) \u2212 E0 [ loge ( Pr(xi)\nPr0(xi)\n)])2] \u2264 E0 [ log2e ( Pr(xi)\nPr0(xi)\n)] \u2264 \u00b52,\nApplying the well-known Bernstein inequality to Eq. (1), we can upper bound it by\nexp ( \u2212 log 2 e(z)\n2n\u00b52 + 2n\u00b52 loge(z)/3\n) \u2264 exp ( \u2212 log 2 e(z)\n2n\u00b52 loge(z) + 2n\u00b5 2 loge(z)/3\n) \u2264 exp ( \u2212 loge(z)\n3n\u00b52\n) ,\nwhere we used the assumption that loge(z) \u2265 1. This bound equals z\u22121/(3n\u00b5 2).\nTaking the larger bound among these two cases, the result follows."}, {"heading": "A.2 A Key Lemma", "text": "The following key lemma quantifies how the message W k is incapable of capturing information of all the different columns Xk1 , . . . , X k d , ultimately leading to our strong lower bounds.\nLemma 4. Suppose that \u2211d\nJ=1H 0(XkJ )\u2212H0(Xk) \u2264 \u03b2. Then\nEJ [ I0(W k;XkJ |W k\u221211 ) ] \u2264 b+ \u03b2\nd ,\nwhere EJ [\u00b7] denote the uniform distribution over the coordinate J . Proof. To simplify the presentation, we drop the k superscript W k and Xk, and denote W k\u221211 as W\u0302 . We have\nEJ [ I0(W ;XJ |W\u0302 ) ] = 1\nd d\u2211 J=1 I0(W ;XJ |W\u0302 ) = 1 d d\u2211 J=1 ( H0(XJ |W\u0302 )\u2212H0(XJ |W, W\u0302 ) ) .\nUsing the fact that \u2211d\nJ=1H 0(XJ |W, W\u0302 ) \u2265 H0(X1 . . . , Xd|W, W\u0302 ) = H0(X|W, W\u0302 ), this is at most\n1\nd\n( d\u2211\nJ=1\nH0(XJ |W\u0302 )\u2212H0(X|W, W\u0302 )\n)\n= 1\nd\n( d\u2211\nJ=1\nH0(XJ |W\u0302 )\u2212 ( H0(X|W\u0302 )\u2212 I0(X;W |W\u0302 )\n))\n= 1 d I0(X;W |W\u0302 ) + 1 d\n( d\u2211\nJ=1\nH0(XJ |W\u0302 )\u2212H0(X|W\u0302 ) ) (2)\nThe entropies H0 in this expression are measured with respect to the fixed distribution Pr0(\u00b7), and XJ refers to a fresh sample drawn independently of any previous messages W\u0302 . Therefore, we can drop the conditioning on W\u0302 , and get\nd\u2211 J=1 H0(XJ |W\u0302 )\u2212H0(X|W\u0302 ) = d\u2211 J=1 H0(XJ)\u2212H0(X) \u2264 \u03b2.\nAlso, since W is only allowed to contain b bits, then I0(X;W |W\u0302 ) = H0(W |W\u0302 )\u2212H0(W |X, W\u0302 ) \u2264 H0(W |W\u0302 ) \u2264 b. Plugging in these last two observations into Eq. (2), the result follows."}, {"heading": "A.3 Proof of Thm. 6", "text": "We now turn to prove the theorem itself. The overall strategy is as follows: We consider a randomized setting where J is picked uniformly at random, and let Pr(\u00b7) = 1d \u2211d J=1 PrJ(\u00b7) denote the joint data distribution over this randomized choice of J . We will focus on lower bounding H(J |J\u0303). To do so, note that I(J\u0303 ; J) = H(J)\u2212H(J |J\u0303) = log(d)\u2212H(J |J\u0303). Moreover, since we have the Markov chain J\u0303 \u2212Wm1 \u2212 J , the data-processing inequality tells us that I(J\u0303 ; J) \u2264 I(Wm1 ; J), so\nH(J |J\u0303) = H(J)\u2212 I(J\u0303 ; J) \u2265 log(d)\u2212 I(Wm1 ; J). (3)\nTherefore, it\u2019s enough to get an upper bound on I(Wm1 ; J), which will be the goal of most of the proof below. Once we have a satisfying lower bound on H(J |J\u0303), a simple application of Fano\u2019s inequality allows us to upper bound the probability of correctly detecting J .\nStage 1: Algebraic Manipulations\nBy definition, I(Wm1 ; J) = EJ [Dkl(Pr(Wm1 )||Pr(Wm1 ))]. Using Lemma 1 and the chain rule for relative entropy, it follows that\nI(Wm1 ; J) \u2264 EJ [ Dkl(Pr(W m 1 |J)||Pr0(Wm1 )) ] =\nm\u2211 k=1 EWk\u221211 [ EJ [ Dkl ( PrJ(W k|W k\u221211 )||Pr 0(W k|W k\u221211 ) )]] . (4)\nLet us focus on a particular choice of k and values W k\u221211 . To simplify the presentation, we drop the k superscript from the message W k and i.i.d. data Xk, and denote the previous messages W k\u221211 as W\u0302 . Thus, we consider the quantity\nEJ [ Dkl ( PrJ(W |W\u0302 )||Pr0(W |W\u0302 ) )] , (5)\nWe rewrite PrJ(W |W\u0302 ) as follows: PrJ(W |W\u0302 ) = \u2211 X PrJ(X|W\u0302 )PrJ(W |X, W\u0302 ).\nSince the data X is independent of previous messages W\u0302 ; W is independent of J given the dataset X and previous messages W\u0302 ; and Pr(W |X, W\u0302 ) = Pr0(W |X, W\u0302 ) for a fixed choice of X, W\u0302 , we can rewrite this as\nPrJ(W |W\u0302 ) = \u2211 X PrJ(X)Pr 0(W |X, W\u0302 )\nWe can write the above as\u2211 XJ PrJ(XJ) \u2211 {Xj}j 6=J PrJ ({Xj}j 6=J |XJ) Pr0 ( W \u2223\u2223\u2223XJ , {Xj}j 6=J , W\u0302) (6)\nBy condition 1 in the theorem statement and the i.i.d. sample assumption,\nPrJ({Xj}j 6=J |XJ) = n\u220f i=1 PrJ({Xi,j}j 6=J |Xi,J) = n\u220f i=1 Pr0({Xi,j}j 6=J |Xi,J) = Pr0({Xj}j 6=J |XJ),\nsubstituting this into Eq. (6) and using the fact that {Xj}j 6=J is a fresh sample independent of previous messages W\u0302 , we get\u2211\nXJ\nPrJ(XJ) \u2211\n{Xj}j 6=J\nPr0 ({Xj}j 6=J |XJ) Pr0 ( W \u2223\u2223\u2223XJ , {Xj}j 6=J , W\u0302)\n= \u2211 XJ PrJ(XJ) \u2211 {Xj}j 6=J Pr0 ( {Xj}j 6=J \u2223\u2223\u2223XJ , W\u0302)Pr0 (W \u2223\u2223\u2223XJ , {Xj}j 6=J , W\u0302) = \u2211 XJ PrJ(XJ)Pr 0(W |XJ , W\u0302 )\nTherefore, we can write Eq. (5) as\nEJ Dkl \u2211\nXJ\nPr0(W |XJ , W\u0302 )PrJ(XJ)  \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 Pr0(W |W\u0302 )  = EJ\n\u2211 W \u2211 XJ Pr0(W |XJ , W\u0302 )PrJ(XJ)  log \u2211XJ Pr0(W |XJ , W\u0302 )PrJ(XJ) Pr0(W |W\u0302 )  . To upper bound this equation, we note that trivially, Pr0(W |W\u0302 ) = \u2211 XJ\nPr0(W |W\u0302 )PrJ(XJ). Plugging this and using the log-sum inequality, we get\nEJ \u2211 W \u2211 XJ Pr0(W |XJ , W\u0302 )PrJ(XJ)  log \u2211XJ Pr0(W |XJ , W\u0302 )PrJ(XJ)\u2211 XJ Pr0(W |W\u0302 )PrJ(XJ)  \u2264 EJ\n\u2211 W,XJ Pr0(W |XJ , W\u0302 )PrJ(XJ) log Pr0(W |XJ , W\u0302 )PrJ(XJ) Pr0(W |W\u0302 )PrJ(XJ)  = EJ\n\u2211 W,XJ Pr0(W |XJ , W\u0302 )PrJ(XJ) log Pr0(W |XJ , W\u0302 ) Pr0(W |W\u0302 )  = EJ\n\u2211 XJ PrJ(XJ)Dkl ( Pr0(W |XJ , W\u0302 )||Pr0(W |W\u0302 ) )\nSince we assume Pr0(XJ) > 0 for all XJ , this equals\nEJ \u2211 XJ Pr0(XJ) PrJ(XJ) Pr0(XJ) Dkl ( Pr0(W |XJ , W\u0302 )||Pr0(W |W\u0302 ) ) = EJ [ E0XJ [ PrJ(XJ)\nPr0(XJ) Dkl\n( Pr0(W |XJ , W\u0302 )||Pr0(W |W\u0302 ) )]] (7)\nStage 2: Expectation Decomposition\nAt this stage, we are not far from our desired result. If the PrJ (XJ ) Pr0(XJ ) term in the expression above wasn\u2019t there, then we would get\nEJ [ E0XJ [ Dkl ( Pr0(W |XJ , W\u0302 )||Pr0(W |W\u0302 ) )]] = EJ [ I0 ( W ;XJ |W\u0302 )] ,\nand a good lower bound would follow by applying the key Lemma 4. However, since the PrJ (XJ ) Pr0(XJ ) term is there, we need to proceed more delicately. The key idea in what follows is that PrJ (XJ ) Pr0(XJ ) is small with high probability, hence can be effectively bounded. First, we can crudely upper bound the relative entropy term in Eq. (7) as follows:\nDkl ( Pr0(W |XJ , W\u0302 )||Pr0(W |W\u0302 ) ) \u2264 max\nW,XJ log\n( Pr0(W |XJ , W\u0302 )\nPr0(W |W\u0302 )\n)\n= max W,XJ log\n( Pr0(W |XJ , W\u0302 )\u2211\nXJ Pr0(XJ)Pr 0(W |XJ , W\u0302 )\n) \u2264 max\nW,XJ log\n( Pr0(W |XJ , W\u0302 )\nPr0(XJ)Pr 0(W |XJ , W\u0302 )\n)\n= log\n( 1\nminJ,XJ Pr 0(XJ) ) Since the sample is i.i.d., this equals\nn log\n( 1\nminJ,X1,J Pr 0(X1,J)\n) ,\nwhich is at most n\u03b3 by assumption 3 in the theorem statement. Therefore, applying Lemma 2 to Eq. (7), whereQ is PrJ (XJ ) Pr0(XJ ) (seen as a random function overXJ), D isDkl ( Pr0(W |XJ , W\u0302 )||Pr0(W |W\u0302 ) ) (seen as a random function over XJ), and the expectation is E0XJ , we get the upper bound\nEJ [ cE0X [ Dkl ( Pr0(W |XJ , W\u0302 )||Pr0(W |W\u0302 ) )] +n\u03b3 ( c Pr0 ( PrJ(XJ)\nPr0(XJ) > c\n) + \u222b \u221e z=c Pr0 ( PrJ(XJ) Pr0(XJ) > z ) dz )] for any c. In particular, let us pick c = exp(1). Using Lemma 3 (which is justified by assumption 4 in the theorem statement) and slightly simplifying, we get\nexp(1) EJ [I0(W ;XJ |W\u0302 )] + n\u03b3 ( exp ( 1\u2212 1\n3n\u00b52\n) + \u222b \u221e z=exp(1) z\u22121/(3n\u00b5 2)dz ) . (8)\nSolving the integral and assuming 3n\u00b52 \u2264 12 (which follows from assumption 5 in the theorem statement), we get\nexp(1) EJ [I0(W ;XJ |W\u0302 )] + n\u03b3 ( exp ( 1\u2212 1\n3n\u00b52\n) +\n1 1\n3n\u00b52 \u2212 1\nexp ( 1\u2212 1\n3n\u00b52\n))\n= exp(1) EJ [I0(W ;XJ |W\u0302 )] + n\u03b3\n1\u2212 3n\u00b52 exp\n( 1\u2212 1\n3n\u00b52 ) \u2264 exp(1) ( EJ [I0(W ;XJ |W\u0302 )] + 2n\u03b3 exp ( \u2212 1\n3n\u00b52 )) Using the key Lemma 4 (which is justified by assumption 2 in the theorem statement), we get the bound\nexp(1)\n( b+ \u03b2\nd + 2n\u03b3 exp\n( \u2212 1\n3n\u00b52\n)) .\nAssumption 5 in the theorem statement states that 3n\u00b52 loge ( 2dn\u03b3 b+\u03b2 ) \u2264 1, in which case it\u2019s easy to verify that 2n\u03b3 exp(\u22121/(3n\u00b52)) \u2264 b+\u03b2d , and therefore the expression above is at most\n2 exp(1) b+ \u03b2 d \u2264 6b+ \u03b2 d .\nTo summarize all we did so far, we attained the following upper bound on Eq. (5):\nEJ [ Dkl ( PrJ(W |W\u0302 )||Pr0(W |W\u0302 ) )] \u2264 6b+ \u03b2\nd .\nNow, recall that the left hand side is a term in Eq. (4) for a particular k and choice of W k\u221211 . So plugging this bound back in Eq. (4), we get I(Wm1 ; J) \u2264 6m b+\u03b2 d . Therefore, as discussed at the beginning of the proof (see Eq. (3)):\nH(J |J\u0303) = log(d)\u2212 I(J\u0303 ; J) \u2265 log(d)\u2212 I(Wm1 ; J) \u2265 log(d)\u2212 6m b+ \u03b2\nd .\nStage 3: Converting Entropy to Probability of Error\nLet Pe = Pr(J\u0303 6= J), and recall that J is considered as picked uniformly at random. By Fano\u2019s inequality and the lower bound we obtained,\n1 + Pe log(d) \u2265 H(Pe) + Pe log(d\u2212 1) \u2265 H(J |J\u0303) \u2265 log(d)\u2212 6m b+ \u03b2\nd ,\nand therefore\nPe \u2265 1\u2212 1 log(d) \u2212 6m b+ \u03b2 d log(d) \u2265 1\u2212 1 log(d) \u2212 6mb+ \u03b2 d\nSince Pe = Pr(J\u0303 6= J) = EJ [PrJ(J\u0303 6= J)], it follows that there exist some deterministic choice of J for which PrJ(J\u0303 6= J) is lower bounded as above. Since PrJ(J\u0303 = J) = 1 \u2212 PrJ(J\u0303 6= J), the result follows."}, {"heading": "B Proof of Thm. 7", "text": "As discussed in the text following the theorem statement, the crucial observation is that any bounded-memory (b, n,m) protocol is also a bounded-memory ( b, \u03ban, \u2308 m \u03ba \u2309) for any positive integer \u03ba \u2264 m. Which \u03ba should we pick? The bound in Thm. 6 will improve as \u03ba increase, so we will pick it to be the largest possible so that Thm. 6 still applies. Specifically, let\n\u03ba =  1 3n\u00b52 loge ( 2dmn\u03b3 b+\u03b2 )  .\nNote that this choice is indeed valid: By assumption 3 in our theorem statement, 3n\u00b52 loge(2dmn\u03b3/(b+ \u03b2)) \u2264 12 , so \u03ba \u2265 1, and also 3n\u00b5\n2 loge(2dmn\u03b3/(b+ \u03b2)) \u2265 1/m, so \u03ba \u2264 m. We now wish to apply Thm. 6 for ( b, \u03ban, \u2308 m \u03ba \u2309) protocols, so we need to check that replacing n with \u03ban doesn\u2019t violate any of the conditions of Thm. 6. To do so, it will be convenient to use the following inequalities (which follow from condition 3 in our theorem statement, the definition of \u03ba, and the fact that bac \u2265 12a for any a \u2265 2):\n1\n3n\u00b52 loge ( 2dmn\u03b3 b+\u03b2 ) \u2265 \u03ba \u2265 1 6n\u00b52 loge ( 2dmn\u03b3 b+\u03b2 ) . (9) Conditions 1,2 and 3 in Thm. 6 are still valid since they do not depend on n. As to condition 4,\nthe first part is implied by condition 2 in our theorem statement, and the second part holds since\n\u03ban\u00b52 \u2265 1\n6n\u00b52 loge ( 2dmn\u03b3 b+\u03b2 )n\u00b52 = 1 6 loge ( 2dmn\u03b3 b+\u03b2 ) , and we assume this upper bounds 2 maxJ,xJ\n\u2223\u2223\u2223loge (Pr(xJ |J)Pr0(xJ ) )\u2223\u2223\u2223 in condition 1 of our theorem statement. The first part of condition 5 in Thm. 6 is satisfied since\n6\u03ban\u00b52 \u2264 6n\u00b5 2\n3n\u00b52 loge ( 2dmn\u03b3 b+\u03b2 ) = 2 loge ( 2dmn\u03b3 b+\u03b2 ) , and condition 1 in our theorem statement states that loge ( 2dmn\u03b3 b+\u03b2 ) \u2265 2. The second part of condition 5 in Thm. 6 is satisfied since 3\u03ban\u00b52 loge(2dmn\u03b3/(b + \u03b2)) \u2264 1 by definition of \u03ba. So all the conditions are satisfied.\nAs a result, we can apply Thm. 6 for ( b, \u03ban, \u2308 m \u03ba \u2309) protocols, and get that for some choice of J ,\nPrJ(J\u0303 6= J) \u2265 1\u2212 1\nlog(d) \u2212 6 \u2308m \u03ba \u2309 b+ \u03b2 d . (10)\nPlugging in the lower bound on \u03ba from Eq. (9), we have\nPrJ(J\u0303 6= J) \u2265 1\u2212 1\nlog(d) \u2212 6\n\u2308 6mn\u00b52 loge ( 2dmn\u03b3\nb+ \u03b2\n)\u2309 b+ \u03b2\nd .\nFinally, we note that by condition 3 in the theorem statement, 6mn\u00b52 loge ( 2dmn\u03b3 b+\u03b2 ) \u2265 2, and since dae \u2264 32a for any a \u2265 2, we can lower bound the above by\n1\u2212 1 log(d)\n\u2212 6 (\n9mn\u00b52 loge\n( 2dmn\u03b3\nb+ \u03b2\n)) b+ \u03b2\nd ,\nfrom which the result follows."}, {"heading": "C Proofs of Main Results", "text": ""}, {"heading": "C.1 Proof of Thm. 1", "text": "The proof of the lower bound follows by applying Thm. 6 to the distribution we consider. We will use the reference distribution Pr0(\u00b7) which is uniform over {\u22121,+1}d.\nWe need to check that all conditions of Thm. 6 are fulfilled. Condition 1 is easy to verify: Conditioned on xJ , the distribution of the other coordinates are uniform under both PrJ(\u00b7) and Pr0(\u00b7).\nAs to the other conditions, we need to check what values of \u03b2, \u03b3, \u00b52 would satisfy them: \u2022 Calculating \u03b2: Pr0(\u00b7) is a uniform distribution over {\u22121,+1}d, hence \u2211d\nJ=1H 0(xJ) = H 0(x) and we can take \u03b2 = 0\n\u2022 Calculating \u03b3: We can take \u03b3 = 1, since Pr0(xJ) = 1/2 for all xJ .\n\u2022 Calculating \u00b52: We have\nmax J\nE0 [ log2e ( PrJ(xJ)\nPr0(xJ)\n)] = 1\n2 log2e\n( 1/2 + \u03c1\n1/2\n) + 1\n2 log2e\n( 1/2\u2212 \u03c1\n1/2\n) = 1\n2\n( log2e (1 + 2\u03c1) + log 2 e (1\u2212 2\u03c1) ) ,\nwhich can be shown to be at most 6\u03c12 for any \u03c1 \u2264 14 (which we may assume since c \u2264 m/54 and \u03c1 = \u221a c/mn. Moreover,\n2 max J,xJ \u2223\u2223\u2223\u2223loge(PrJ(xJ)Pr0(xJ) )\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223loge(1/2\u2212 \u03c11/2 )\u2223\u2223\u2223\u2223 = \u2212 loge(1\u2212 2\u03c1), which is at most 3\u03c1 for \u03c1 \u2264 15 . Therefore, by picking \u00b5\n2 = 9\u03c12, condition 4 of Thm. 6 is satisfied.\nUsing these observations, a straightforward calculation reveals that Thm. 6 holds as long as the technical conditions in our theorem statement are fulfilled. Thus, the bound of Thm. 6 applies, and using the values of \u00b52 and \u03c1, we get\nPrJ(J\u0303 = J) \u2264 1\nlog(d) +\n6mb\nd\nas required. It remains to prove the upper bound. Each coordinate has magnitude at most 1, so by Hoeffding\u2019s inequality, the probability of its empirical average deviating from its mean by at least \u03c1 is at most 2 exp(\u2212mn\u03c12/2). By a union bound, with probability at least 1 \u2212 2d exp(\u2212mn\u03c12/2) = 1\u2212 2d exp(\u2212c/2), the empirical average of all coordinates will deviate from their respective means by less than \u03c1. If this occurs, we can detect J simply by computing the empirical average and picking the estimate J\u0303 to be the coordinate with the highest empirical average."}, {"heading": "C.2 Proof of Thm. 2", "text": "First, we re-state the theorem in a more precise manner, without asymptotic notation. Theorem. Consider hide-and-seek problem 1. Suppose \u03c1 = \u221a c/mn for some parameter c, and assume\nloge\n( 2dmn\nb\n) \u2265 2 , \u221a c\nmn \u2264 1 36 loge ( 2dmn b ) , 2 \u2264 36 loge(2dmnb ) c \u2264 m\nThen for any estimate J\u0303 of J returned by any bounded-memory (b, n,m) protocol, there exists some J such that\nPrJ(J\u0303 = J) \u2264 1\nlog(d) + loge\n( 2dmn\nb\n) 324cb\nd .\nHowever, given mn samples, if J\u0303 is the coordinate with the highest empirical average, then PrJ(J\u0303 = J) \u2265 1\u2212 2d exp ( \u221212c ) .\nProof. The proof of the upper bound is identical to the one in Thm. 1. The proof of the lower bound follows by applying Thm. 7 to the distribution we consider. We use the reference distribution Pr0(\u00b7) which is uniform over {\u22121,+1}d.\nWe need to check that all conditions of Thm. 7 are fulfilled. This includes conditions 1,2,3 of Thm. 6 on the data distribution, but we have already verified that they hold with \u03b2 = 0, \u03b3 = 1 when we proved Thm. 1. As to the conditions in Thm. 7, conditions 1 and 3 follows from our assumptions and the fact that for any \u03c1 \u2264 14 ,\n2 max J,xJ \u2223\u2223\u2223\u2223loge(PrJ(xJ)Pr0(xJ) )\u2223\u2223\u2223\u2223 \u2264 \u22122 log(1\u2212 2\u03c1) \u2264 6\u03c1 = 6\u221a cmn.\nCondition 2 in Thm. 7 holds with \u00b52 = 6\u03c12 (we have already performed this calculation in the proof of Thm. 1).\nThus, we may apply Thm. 7, and get the upper bound\nPrJ(J\u0303 = J) \u2264 1\nlog(d) + 54 loge\n( 2dmn\nb\n) mn\u00b52b\nd .\nSubstituting \u00b52 = 6\u03c12 = 6c/mn, the result follows."}, {"heading": "C.3 Proof of Thm. 3", "text": "First, we will re-state the theorem in a more precise manner, without asymptotic notation. Theorem. Suppose \u03c1 = \u221a cd/mn for some parameter c. Assume\nloge\n( 2dmn log(2d)\nb+ 2\n) \u2265 2 , \u221a cd\nmn \u2264 1\n36 loge\n( 2dmn log(2d)\nb+2\n) , 2 \u2264 36 loge(2dmn log(2d)b+ 2 ) c \u2264 m\nThen for any estimate J\u0303 of J returned by any bounded-memory (b, n,m) protocol, there exists some J such that\nPrJ(J\u0303 = J) \u2264 1\nlog(d) + loge\n( 2dmn log(2d)\nb+ 2\n) 324c(b+ 2)\nd ,\nHowever, given mn samples, if J\u0303 is the coordinate with the highest empirical average, then PrJ(J\u0303 = J) \u2265 1\u2212 d exp ( \u221213c ) .\nProof. The proof lower bound follows by applying Thm. 7 to the distribution we consider. We will use the reference distribution Pr0(\u00b7) which is uniform over {\u2212ej ,+ej}dj=1.\nWe need to check that condition 1,2,3 of Thm. 6 hold, as well as the additional conditions in Thm. 7.\nCondition 1 in Thm. 6 is easy to verify: once xJ is fixed, the conditional distribution on the other coordinates is the same under both PrJ(\u00b7) and Pr0(\u00b7).\nAs to the other conditions, we need to check what values of \u03b2, \u03b3, \u00b52 would satisfy them:\n\u2022 Calculating \u03b2: For our distribution, using the definition of the entropy function, it is easily verified that H0(x) = log(d) + 1 and\nH0(xJ) = 1\nd log(2d) +\n( 1\u2212 1\nd\n) log ( d\nd\u2212 1\n) \u2264 1\nd (log(d) + 1) + log\n( 1 + 1\nd\u2212 1 ) \u2264 1\nd (log(d) + 1) +\n1\nd\u2212 1 .\nAs a result, d\u2211\nJ=1\nH0(xJ)\u2212H0(x) \u2264 d\nd\u2212 1 \u2264 2,\nso we can take \u03b2 = 2.\n\u2022 Calculating \u03b3: We can take \u03b3 = log(2d), since Pr0(xJ) \u2265 1/2d for all xJ .\n\u2022 Calculating \u00b52: We have\nmax J\nE0 [ log2e ( PrJ(xJ)\nPr0(xJ)\n)] = 1\n2d log2e\n( 1/2d+ \u03c1/d\n1/2d\n) + 1\n2d log2e\n( 1/2d\u2212 \u03c1/d\n1/2d\n) + 0\n= 1\n2d\n( log2e(1 + 2\u03c1) + log 2 e(1\u2212 2\u03c1) ) ,\nwhich can be shown to be at most 6\u03c12/d for any \u03c1 \u2264 14 (which is implied by the assumptions in the theorem statement). Therefore, we can take \u00b52 = 6\u03c12/d.\nAlso, using similar techniques, we have for any \u03c1 \u2264 14 that\n2 max J,xJ \u2223\u2223\u2223\u2223loge(PrJ(xJ)Pr0(xJ) )\u2223\u2223\u2223\u2223 \u2264 \u22122 log(1\u2212 2\u03c1) \u2264 6\u03c1 = 6 \u221a cd mn\nUsing these observations, a straightforward calculation reveals that Thm. 7 holds as long as the technical conditions in the theorem statement are fulfilled. Thus, the lower bound of Thm. 7 applies, and using the values of \u00b52 and \u03c1, we get\nPrJ(J\u0303 6= J) \u2265 1\u2212 1\nlog(d) \u2212 54 loge\n( 2dmn log(2d)\nb+ 2\n) m\u00b52(b+ \u03b2)\nd\n= 1\u2212 1 log(d)\n\u2212 54 loge ( 2dmn log(2d)\nb+ 2\n) 6mn\u03c12(b+ 2)\nd2\n= 1\u2212 1 log(d)\n\u2212 loge ( 2dmn log(2d)\nb+ 2\n) 324c(b+ 2)\nd\nas required. It remains to prove the upper bound. Any coordinate xj has magnitude at most 1 and E[x2j ] \u2264 1d with respect to any PrJ(\u00b7). Applying Bernstein\u2019s inequality, the probability of its empirical average deviating from its mean by at least \u03c1/d is at most\n2 exp \u2212 mn\u03c12 d (\n2 + 2\u03c13\n)  = 2 exp \u2212 c 2 + 23 \u221a cd mn  . Since the theorem conditions imply mn \u2265 cd, and trivially mn \u2265 1, this can be upper bounded by 2 exp(\u2212c/3). Thus, by a union bound, with probability at least 1\u2212 2d exp(\u2212c/3), the empirical average of all coordinates will deviate from their respective means by less than \u03c1/d. If this occurs, we can detect J simply by computing the empirical average and picking the estimate J\u0303 to be the coordinate with the highest empirical average."}, {"heading": "D Proof of Results from Sec. 4", "text": ""}, {"heading": "D.1 Proof of Thm. 4", "text": "Consider the set of distributions PrJ(\u00b7) over {0, 1}d, where each coordinate is chosen independently and uniformly, except coordinate J which equals 0 with probability 12 + \u03c1, where \u03c1 =\u221a\nmin {\n1 54 , 1 27 loge(2d/b)\n} = \u2126\u0303(1). Clearly, the coordinate j which minimizes E[`t(j)] is j = J . More-\nover, if at round t the learner chooses some jt 6= J , then E[`t(jt) \u2212 `t(J)] = \u03c1 \u2265 \u2126\u0303(1). Thus, to have E[ \u2211T t=1 `t(jt) \u2212 \u2211T t=1 `t(J)] < \u03c1T (for some small ) requires that the expected number of rounds where jt 6= J is at most T . By Markov\u2019s inequality, this means that the probability of J not being the most-commonly chosen coordinate is at most 2 .\nAs a result, we can use such an algorithm to detect J with probability of error at most 2 for arbitrarily small . However, by4 Thm. 1, for any (b, 1, T ) protocol, there is some J such that the protocol would correctly detect J with probability at most 1log(d) + 6Tb d , which is at most (say) 3 4 by the theorem\u2019s assumptions. For sufficiently small , we get a contradiction, hence the result follows."}, {"heading": "D.2 Proof of Thm. 5", "text": "The upper bound follows from the concentration of measure assumption on x\u0303ixj , and a union bound, which implies that\nPr ( \u2200i < j, |x\u0303ixj \u2212 E[xixj ]| \u2264 \u03c4\n2\n) \u2265 1\u2212 s(s\u2212 1) exp ( \u2212 c\n3\n) \u2265 1\u2212 s2 exp ( \u2212 c\n3\n) .\nIf this event occurs, then picking (I\u0303 , J\u0303) to be the coordinates with the largest empirical mean would indeed succeed in detecting (I, J), since |E[xIxJ ]| \u2265 |E[xixj ]|+ \u03c4 for all (i, j) 6= (I, J).\nThe lower bound is based on a reduction to the setting discussed in Thm. 3. Specifically, let PrI,J(\u00b7) (for coordinates I < J) be a distribution over the set { \u221a s 2\u03c31ei+ \u221a s 2\u03c32ej}(i<j),(\u03c31,\u03c32)\u2208{\u22121,+1}2 ,\n4The theorem discusses the case where the distribution is over {\u22121,+1}d, and coordinate J has a slight positive bias, but it\u2019s easily seen that the same result holds for the setting discussed here.\ndefined as follows for any i, j such that i < j, any \u03c31, \u03c32, and for \u03c1 = \u221a cs(s\u22121) 2mn :\nPr I,J\n(\u221a s\n2 \u03c31ei +\n\u221a s\n2 \u03c32ej\n) =  1 2s(s\u22121) (i, j) 6= (I, J) 1+2\u03c1 2s(s\u22121) (i, j) = (I, J), \u03c31 = \u03c32 1\u22122\u03c1\n2s(s\u22121) (i, j) = (I, J), \u03c31 = \u2212\u03c32\nIn words, this distribution assigns equal probability for any pair of coordinates i < j, and picks the signs \u03c31, \u03c32 uniformly at random unless (i, j) = (I, J), in which the signs are positively correlated. It is easy to verify that for all i, E[x2i ] = 1, and for all i < j, E[xixj ] = 0 unless (i, j) = (I, J), in which case E[xixj ] = 2\u03c1/(s\u2212 1) = \u221a 2cs (s\u22121)mn . Furthermore, we have E[(xixj) 2] = s2(s\u22121) , and |xixj | is always at most s/2. Therefore, by Bernstein\u2019s inequality, if x\u0303ixj is the empirical average of xixj (over mn samples), then\nPr ( |x\u0303ixj \u2212 E[xixj ]| \u2265 \u03c1\ns\u2212 1\n) \u2264 2 exp \u2212 mn\u03c12 (s\u2212 1)2 ( s s\u22121 + s 3(s\u22121)\u03c1 ) \n= 2 exp \u2212 mn cs(s\u22121)2mn s(s\u2212 1) ( 1 + 13 \u221a cs(s\u22121) 2mn )  = 2 exp \u2212 c 2 ( 1 + 13 \u221a cs(s\u22121) 2mn )  ,\nwhich is at most 2 exp(\u2212c/3) using the assumption that mn is sufficiently large compared to cs2. Thus, we see that the family of distributions {PrI,J(\u00b7)}I<J match the conditions in the theorem statement, when we take \u03c4 = 2\u03c1/(s\u2212 1).\nNow, let us consider the distribution over s \u00d7 s matrices induced by xx>, where x is sampled according to PrI,J(\u00b7), and in particular the distribution on the s(s\u22121)2 entries above the main diagonal. It is easily seen to be equivalent to a distribution which picks one entry (i, j) uniformly at random, and assigns it to be { \u2212 s2 ,+ s 2 } with equal probability, unless (i, j) = (I, J) in which case the positive value is picked with probability 12 + \u03c1, and the negative value with probability 1 2 \u2212 \u03c1. Since there are s(s\u22121) 2 entries, we can apply Thm. 3 with d = s(s\u22121) 2 and for the bias term\n\u03c1 = \u221a\ncd mn = \u221a cs(s\u22121) 2mn . With this choice, the conditions of Thm. 3 are implied by the conditions\nof our theorem, and we get that for any bounded-memory (b, n,m) protocol, there is some (I, J) which will be detected only with probability at most\n1\nlog(d) + O\u0303\n( cb\nd\n) =\n1\nlog(s2 \u2212 s)\u2212 1 + O\u0303\n( cb\ns2 ) as required."}, {"heading": "E Basic Results in Information Theory", "text": "The proof of Thm. 6 makes extensive use of quantities and basic results from information theory. We briefly review here the technical results relevant for our paper. A more complete introduction may be found in [22]. Following the settings considered in the paper, we will focus only on discrete distributions taking values on a finite set.\nGiven a random variable X taking values in a domain X , and having a distribution function p(\u00b7), we define its entropy as\nH(X) = \u2211 x\u2208X p(x) log2(1/p(x)) = EX log(1/p(x)).\nIntuitively, this quantity measures the uncertainty in the value ofX. This definition can be extended to joint entropy of two (or more) random variables, e.g. H(X,Y ) = \u2211 x,y p(x, y) log(1/p(x, y)), and to conditional entropy\nH(X|Y ) = \u2211 y p(y) \u2211 x p(x|y) log(1/p(x|y)).\nFor a particular value y of Y , we have H(X|Y = y) = \u2211 x p(x|y) log(1/p(x|y))\nIt is possible to show that \u2211n\nj=1H(Xi) \u2265 H(X1, . . . , Xn), with equality when X1, . . . , Xn are independent. Also, H(X) \u2265 H(X|Y ) (i.e. conditioning can only reduce entropy). A simple consequence of the data-processing inequality states that for any Markov chain X\u2212Y \u2212Z, it holds that H(X|Z) \u2264 H(X|Y ). Informally, further processing of Y can only increase our uncertainty about the variable X.\nMutual information I(X;Y ) is defined as\nI(X;Y ) = H(X)\u2212H(X|Y ) = H(Y )\u2212H(Y |X) = \u2211 x,y p(x, y) log ( p(x, y) p(x)p(y) ) .\nIntuitively, this measures the amount of information each variable carries on the other one, or in other words, the reduction in uncertainty on one variable given we know the other. Since entropy is always positive, we immediately get I(X;Y ) \u2264 min{H(X), H(Y )}. As for entropy, one can define the conditional mutual information between random variables X,Y given some other random variable Z as\nI(X;Y |Z) = \u2211 z p(z) \u2211 x,y p(x, y|z) log ( p(x, y|z) p(x|z)p(y|z) ) .\nThe chain rule for mutual information states that\nI(X1 . . . Xn;Y ) = n\u2211 i=1 I(Xi;Y |X1 . . . Xi\u22121).\nFinally, we define the relative entropy between two distributions p, q on the same set as\nDkl(p||q) = \u2211 x p(x) log ( p(x) q(x) ) .\nIt is possible to show that relative entropy is always non-negative. Also, it is easily verified that I(X;Y ) = \u2211 y p(y) Dkl(pX(\u00b7|y)||pX(\u00b7)),\nwhere pX is the distribution of the random variable X. An important inequality we use in the context of relative entropy calculations is the log-sum inequality. This inequality states that for any nonnegative ai, bi,(\u2211 i ai ) log \u2211 i ai\u2211 i bi \u2264 \u2211 i ai log ai bi ."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Many machine learning approaches are characterized by information constraints on how<lb>they interact with the training data. These include memory and sequential access constraints<lb>(e.g. fast first-order methods to solve stochastic optimization problems); communication con-<lb>straints (e.g. distributed learning); partial access to the underlying data (e.g. missing features<lb>and multi-armed bandits) and more. However, currently we have little understanding how such<lb>information constraints fundamentally affect our performance, independent of the learning prob-<lb>lem semantics. For example, are there learning problems where any algorithm which has small<lb>memory footprint (or can use any bounded number of bits from each example, or has certain<lb>communication constraints) will perform worse than what is possible without such constraints?<lb>In this paper, we describe how a single set of results implies positive answers to the above, for<lb>a variety of settings.", "creator": "LaTeX with hyperref package"}}}