{"id": "1311.7184", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2013", "title": "Using multiple samples to learn mixture models", "abstract": "In the mixture models problem it is assumed that there are $K$ distributions $\\theta_{1},\\ldots,\\theta_{K}$ and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same $K$ underlying distributions, but with different mixing weights in our set, and with more information on the parameters and parameters of the hidden distributions, we can find that a similar distribution is indeed observed.\n\nThe distribution model problem\nWhen an isolated distribution comes into play, a solution to the problem is to provide a combination model with the information collected from a mixture of the two distributions (i.e., for example, with an \u03b1 and \u03b2 = 0.005), where the distribution of the two distributions is determined by a pair of separate distributions. The distributions of a mixture are then split into one group by one, and all the distributions are distributed.\nThe analysis\nIn the analysis of the distribution model, we use two different sets of variables called \"hirch\", which are related to different distributions of the same distribution. We use each variable as the input in our sample and extract the distributions from the sample. For example, if each distribution is represented by two pairs of two, then the distribution of the two distributions is then divided into two sets:\n[b] The output of a mixture of two distribution distributions is then divided into two sets:\n[c] The output of a mixture of two distributions is then divided into two sets:\n[d] This is the same distribution as the output of a mixture of two distributions:\n[e] This is the same distribution as the output of a mixture of two distributions:\n[f] This is the same distribution as the output of a mixture of two distributions:\n[g] This is the same distribution as the output of a mixture of two distributions:\n[h] This is the same distribution as the output of a mixture of two distributions:\n[i] This is the same distribution as the output of a mixture of two distributions:\n[j] This is the same distribution as the output of a mixture of two distributions:\n[k] The output of a mixture of two distributions is then divided into two sets:\n[m] This is the same distribution as the output of a mixture of two distributions:", "histories": [["v1", "Thu, 28 Nov 2013 01:36:49 GMT  (284kb,D)", "http://arxiv.org/abs/1311.7184v1", "Published in Neural Information Processing Systems (NIPS) 2013"]], "COMMENTS": "Published in Neural Information Processing Systems (NIPS) 2013", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["jason d lee", "ran gilad-bachrach", "rich caruana"], "accepted": true, "id": "1311.7184"}, "pdf": {"name": "1311.7184.pdf", "metadata": {"source": "CRF", "title": "Using multiple samples to learn mixture models", "authors": ["Jason Lee", "Ran Gilad-Bachrach"], "emails": ["jdl17@stanford.edu", "rang@microsoft.com", "rcaruana@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The mixture model has been studied extensively from several directions. In one setting it is assumed that there is a single sample, that is a single collection of instances, from which one has to recover the hidden information. A line of studies on clustering theory, starting from [?] has proposed to address this problem by finding a projection to a low dimensional space and solving the problem in this space. The goal of this projection is to reduce the dimension while preserving the distances, as much as possible, between the means of the underlying distributions. We will refer to this line as MM (Mixture Models). On the other end of the spectrum, Topic modeling (TM), [?, ?], assumes multiple samples (documents) that are mixtures, with different weights of the underlying distributions (topics) over words. Comparing the two lines presented above shows some similarities and some differences. Both models assume the same generative structure: a point (word) is generated by first choosing the distribution \u03b8i using the mixing weights and then selecting a point (word) according to this distribution. The goal of both models is to recover information about the generative model (see [?] for more on that). However, there are some key differences:\n(a) In MM, there exists a single sample to learn from. In TM, each document is a mixture of the topics, but with different mixture weights.\n(b) In MM, the points are represented as feature vectors while in TM the data is represented as a word-document co-occurrence matrix. As a consequence, the model generated by TM cannot assign words that did not previously appear in any document to topics.\n\u2217Work done while the author was an intern at Microsoft Resaerch\nar X\niv :1\n31 1.\n71 84\nv1 [\nst at\n.M L\n] 2\n8 N\nov 2\n(c) TM assumes high density of the samples, i.e., that the each word appears multiple times. However, if the topics were not discrete distributions, as is mostly the case in MM, each \"word\" (i.e., value) would typically appear either zero or one time, which makes the co-occurrence matrix useless.\nIn this work we try to close the gap between MM and TM. Similar to TM, we assume that multiple samples are available. However, we assume that points (words) are presented as feature vectors and the hidden distributions may be continuous. This allows us to solve problems that are typically hard in the MM model with greater ease and generate models that generalize to points not in the training data which is something that TM cannot do."}, {"heading": "1.1 Definitions and Notations", "text": "We assume a mixture model in which there are K mixture components \u03b81, . . . , \u03b8K defined over the space X. These mixture components are probability measures over X. We assume that there are M mixture models (samples), each drawn with different mixture weights \u03a61, . . . ,\u03a6M such that \u03a6j = (\u03c6j1, . . . , \u03c6 j K) where all the weights are non-negative and sum to 1. Therefore, we have M different probability measures D1, . . . , DM defined over X such that for a measurable set A and j = 1, . . . ,M we have Dj(A) = \u2211 i \u03c6 j i\u03b8i (A). We will denote by \u03c6min the minimal value of \u03c6 j i .\nIn the first part of this work, we will provide an algorithm that given samples S1, . . . , SM from the mixtures D1, . . . , DM finds a low-dimensional embedding that preserves the distances between the means of each mixture. In the second part of this work we will assume that the mixture components have disjoint supports. Hence we will assume that X = \u222ajCj such that the Cj \u2019s are disjoint and for every j, \u03b8j(Cj) = 1. Given samples S1, . . . , SM , we will provide an algorithm that finds the supports of the underlying distributions, and thus clusters the samples."}, {"heading": "1.2 Examples", "text": "Before we dive further in the discussion of our methods and how they compare to prior art, we would like to point out that the model we assume is realistic in many cases. Consider the following example: assume that one would like to cluster medical records to identify subtypes of diseases (e.g., different types of heart disease). In the classical clustering setting (MM), one would take a sample of patients and try to divide them based on some similarity criteria into groups. However, in many cases, one has access to data from different hospitals in different geographical locations. The communities being served by the different hospitals may be different in socioeconomic status, demographics, genetic backgrounds, and exposure to climate and environmental hazards. Therefore, different disease sub-types are likely to appear in different ratios in the different hospital. However, if patients in two hospitals acquired the same sub-type of a disease, parts of their medical records will be similar. Another example is object classification in images. Given an image, one may break it to patches, say of size 10x10 pixels. These patches may have different distributions based on the object in that part of the image. Therefore, patches from images taken at different locations will have different representation of the underlying distributions. Moreover, patches from the center of the frame are more likely to contain parts of faces than patches from the perimeter of the picture. At the same time, patches from the bottom of the picture are more likely to be of grass than patches from the top of the picture. In the first part of this work we discuss the problem of identifying the mixture component from multiple samples when the means of the different components differ and variances are bounded. We focus on the problem of finding a low dimensional embedding of the data that preserves the distances between the means since the problem of finding the mixtures in a low dimensional space has already been address (see, for example [?]). Next, we address a different case in which we assume that the support of the hidden distributions is disjoint. We show that in this case we can identify the supports of each distribution. Finally we demonstrate our approaches on toy problems. The proofs of the theorems and lemmas\nappear in the appendix. Table 1 summarizes the applicability of the algorithms presented here to the different scenarios.\n1.3 Comparison to prior art\nare designed to address.\nThere are two common approaches in the theoretical study of the MM model. The method of moments [?, ?, ?] allows the recovery of the model but requires exponential running time and sample sizes. The other approach, to which we compare our results, uses a two stage approach. In the first stage, the data is projected to a low dimensional space and in the second stage the association of points to clusters is recovered. Most of the results with this approach assume that the mixture components are Gaussians. Dasgupta [?], in a seminal paper, presented the first result in this line.\nHe used random projections to project the points to a space of a lower dimension. This work assumes that separation is at least \u2126(\u03c3max \u221a n). This result has been improved in a series of papers. Arora and Kannan [?] presented algorithms for finding the mixture components which are, in most cases, polynomial in n and K. Vempala and Wang [?] used PCA to reduce the required separation to \u2126 ( \u03c3maxK1/4 log 1/4 ( n/\u03c6min )) . They use PCA to project on the first K principal components, however, they require the Gaussians to be spherical. Kanan, Salmasian and Vempala [?] used similar spectral methods but were able to improve the results to require separation of only c\u03c3maxK\n2/3 /\u03c62min. Chaud-\nhuri [?] have suggested using correlations and independence between features under the assumption that the means of the Gaussians differ on many features. They require separation of \u2126 ( \u03c3max \u221a K log(K\u03c3max logn/\u03c6min) ) , however they assume that the Gaussians\nare axis aligned and that the distance between the centers of the Gaussians is spread across \u2126 (K\u03c3max logn/\u03c6min) coordinates. We present a method to project the problem into a space of dimension d\u2217 which is the dimension of the affine space spanned by the means of the distributions. We can find this projection and maintain the distances between the means to within a factor of 1 \u2212 . The different factors, \u03c3max, n and will affect the sample size needed, but do not make the problem impossible. This can be used as a preprocessing step for any of the results discussed above. For example, combining with [?] yields an algorithm that requires a separation of only \u2126 ( \u03c3max \u221a d\u2217 ) \u2264 \u2126 ( \u03c3max \u221a K ) . However, using [?] will result in separation requirement\nof \u2126 ( \u03c3max \u221a K log (K\u03c3max log d\u2217/\u03c6min) ) . There is also an improvement in terms of the\nvalue of \u03c3max since we need only to control the variance in the affine space spanned by the means of the Gaussians and do not need to restrict the variance in orthogonal directions, as long as it is finite. Later we also show that we can work in a more generic setting where the distributions are not restricted to be Gaussians as long as the supports of the distributions are disjoint. While the disjoint assumption may seem too strict, we note that the results presented above make very similar assumptions. For example, even if the required separation is \u03c3maxK1/2 then if we look at the Voronoi tessellation around the centers of the Gaussians, each cell will contain at least 1 \u2212 (2\u03c0)\u22121K3/4 exp (\u2212K/2) of the mass of the Gaussian. Therefore, when K is large, the supports of the Gaussians are almost disjoint."}, {"heading": "2 Projection for overlapping components", "text": "In this section we present a method to use multiple samples to project high dimensional mixtures to a low dimensional space while keeping the means of the mixture components\nAlgorithm 1 Multi Sample Projection (MSP)"}, {"heading": "Inputs:", "text": "Samples S1, . . . , Sm from mixtures D1, . . . , Dm Outputs: Vectors v\u03041, . . . , v\u0304m\u22121 which span the projected space Algorithm:\n1. For j = 1, . . . ,m let E\u0304j be the mean of the sample Sj 2. For j = 1, . . . ,m\u2212 1 let v\u0304j = E\u0304j \u2212 E\u0304j+1 3. return v\u03041, . . . , v\u0304m\u22121\nwell separated. The main idea behind the Multi Sample Projection (MSP) algorithm is simple. Let \u00b5i be the mean of the i\u2019th component \u03b8i and let Ej be the mean of the j\u2019th mixture Dj . From the nature of the mixture, Ej is in the convex-hull of \u00b51, . . . , \u00b5K and hence in the affine space spanned by them; this is demonstrated in Figure 1. Under mild assumptions, if we have sufficiently many mixtures, their means will span the affine space spanned by \u00b51, . . . , \u00b5K . Therefore, the MSP algorithm estimates the Ej \u2019s and projects to the affine space they span. The reason for selecting this sub-space is that by projecting on this space we maintain the distance between the means while reducing the dimension to at most K \u2212 1. The MSP algorithm is presented in Algorithm 1. In the following theorem we prove the main properties of the MSP algorithm. We will assume that X = Rn, the first two moments of \u03b8j are finite, and \u03c32max denotes maximal variance of any of the components in any direction. The separation of the mixture components is minj 6=j\u2032 \u2016\u00b5j \u2212 \u00b5j\u2032\u2016. Finally, we will denote by d\u2217 the dimension of the affine space spanned by the \u00b5j \u2019s. Hence, d\u2217 \u2264 K\u2212 1. Theorem 1. MSP Analysis Let Ej = E [Dj ] and let vj = Ej \u2212 Ej+1. Let Nj = |Sj |. The following holds for MSP:\n1. The computational complexity of the MSP algorithm is n \u2211M j=1Nj + 2n (m\u2212 1)\nwhere n is the original dimension of the problem.\n2. For any > 0, Pr [ supj \u2225\u2225Ej \u2212 E\u0304j\u2225\u2225 > ] \u2264 n\u03c32max 2 \u2211j 1Nj . 3. Let \u00b5\u0304i be the projection of \u00b5i on the space spanned by v\u03041, . . . , v\u0304M\u22121 and assume that \u2200i, \u00b5i \u2208 span {vj}. Let \u03b1ij be such that \u00b5i = \u2211 j \u03b1 i jvj and let A = maxi\n\u2211\u2223\u2223\u03b1ij\u2223\u2223 then with probability of at least 1\u2212 n\u03c3 2max 2 \u2211 j 1 Nj\nPr [ max i,i\u2032 |\u2016\u00b5i \u2212 \u00b5i\u2032\u2016 \u2212 \u2016\u00b5\u0304i \u2212 \u00b5\u0304i\u2032\u2016| > ] \u2264 4n\u03c3 2maxA2 2 \u2211 j 1 Nj .\ndemonstrated here by the red line.\nThe MSP analysis theorem shows that with large enough samples, the projection will maintain the separation between the centers of the distributions. Moreover, since this is a projection, the variance in any direction cannot increase. The value of A measures the complexity of the setting. If the mixing coefficients are very different in the different samples then A will be small. However, if the mixing coefficients are very similar, a larger sample is required. Nevertheless, the size of the sample needed is polynomial in the parameters of the problem. It is also apparent that with large enough samples, a good projection will be found, even with\nlarge variances, high dimensions and close centroids. A nice property of the bounds presented here is that they assume only bounded first and second moments. Once a projection to a low dimensional space has been found, it is possible to find the clusters using approaches presented in section 1.3. However, the analysis of the MSP algorithm assumes that the means of E1, . . . , EM span the affine space spanned by \u00b51, . . . , \u00b5K . Clearly, this implies that we require that m > d\u2217. However, when m is much larger than d\u2217, we might end-up with a projection on too large a space. This could easily be fixed since in this case, E\u03041, . . . , E\u0304m will be almost co-planar in the sense that there will be an affine space of dimension d\u2217 that is very close to all these points and we can project onto this space."}, {"heading": "3 Disjoint supports and the Double Sample Clustering (DSC) algorithm", "text": "In this section we discuss the case where the underlying distributions have disjoint supports. In this case, we do not make any assumption about the distributions. For example, we do not require finite moments. However, as in the mixture of Gaussians case some sort of separation between the distributions is needed, this is the role of the disjoint supports. We will show that given two samples from mixtures with different mixture coefficients, it is possible to find the supports of the underlying distributions (clusters) by building a tree of classifiers such that each leaf represents a cluster. The tree is constructed in a greedy fashion. First we take the two samples, from the two distributions, and reweigh the examples such that the two samples will have the same cumulative weight. Next, we train a classifier to separate between the two samples. This classifier becomes the root of the tree. It also splits each of the samples into two sets. We take all the examples that the classifier assign to the label +1(\u22121), reweigh them and train another classifier to separate between the two samples. We keep going in the same fashion until we can no longer find a classifier that splits the data significantly better than random. To understand why this algorithm works it is easier to look first at the case where the mixture distributions are known. If D1 and D2 are known, we can define the L1 distance between them as L1 (D1, D2) = supA |D1 (A) \u2013D2 (A)|.1 It turns out that the supremum is attained by a set A such that for any i, \u00b5i (A) is either zero or one. Therefore, any inner node in the tree splits the region without breaking clusters. This process proceeds until all the points associated with a leaf are from the same cluster in which case, no classifier can distinguish between the classes. When working with samples, we have to tolerate some error and prevent overfitting. One way to see that is to look at the problem of approximating the L1 distance between D1 and D2 using samples S1 and S2. One possible way to do that is to define L\u03021 = supA \u2223\u2223\u2223\u2223A\u2229S1\u2223\u2223S1\u2223\u2223 \u2212 A\u2229S2\u2223\u2223S2\u2223\u2223 \u2223\u2223\u2223\u2223. However, this estimate is almost surely going to be 1 if the underlying distributions are absolutely continuous. Therefore, one has to restrict the class from which A can be selected to a class of VC dimension small enough compared to the sizes of the samples. We claim that asymptotically, as the sizes of the samples increase, one can increase the complexity of the class until the clusters can be separated. Before we proceed, we recall a result of [?] that shows the relation between classification and the L1 distance. We will abuse the notation and treat A both as a subset and as a classifier. If we mix D1 and D2 with equal weights then\nerr (A) = D1 (X \\A) +D2 (A) = 1\u2212D1 (A) +D2 (A) = 1\u2212 (D1 (A)\u2212D2 (A)) .\nTherefore, minimizing the error is equivalent to maximizing the L1 distance. 1the supremum is over all the measurable sets.\nAlgorithm 2 Double Sample Clustering (DSC)"}, {"heading": "Inputs:", "text": "\u2022 Samples S1, S2 \u2022 A binary learning algorithm L that given samples S1, S2 with weights w1, w2 finds a classifier h and an estimator e of the error of h. \u2022 A threshold \u03c4 > 0."}, {"heading": "Outputs:", "text": "\u2022 A tree of classifiers"}, {"heading": "Algorithm:", "text": "1. Let w1 = 1 and w2 = |S1|/|S2| 2. Apply L to S1 & S2 with weights w1 & w2 to get the classifier h and estimator e. 3. If e \u2265 12 \u2212 \u03c4 ,\n(a) return a tree with a single leaf. 4. else\n(a) For j = 1, 2, let S+j = {x \u2208 Sj s.t. h (x) > 0} (b) For j = 1, 2, let S\u2212j = {x \u2208 Sj s.t. h (x) < 0} (c) Let T+ be the tree returned by the DSC algorithm applied to S+1 and S + 2 (d) Let T\u2212 be the tree returned by the DSC algorithm applied to S\u22121 and S \u2212 2 (e) return a tree in which c is at the root node and T\u2212 is its left subtree and T+ is its right subtree\nThe key observation for the DSC algorithm is that if \u03c61i 6= \u03c62i , then a set A that maximizes the L1 distance betweenD1 andD2 is aligned with cluster boundaries (up to a measure zero).\nFurthermore, A contains all the clusters for which \u03c61i > \u03c62i and does not contain all the clusters for which \u03c61i < \u03c62i . Hence, if we split the space to A and A\u0304 we have few clusters in each side. By applying the same trick recursively in each side we keep on bisecting the space according to cluster boundaries until subspaces that contain only a single cluster remain. These sub-spaces cannot be further separated and hence the algorithm will stop. Figure 2 demonstrates this idea. The following lemma states this argument mathematically: Lemma 1. If Dj = \u2211 i \u03c6 j i\u03b8i then\n1. L1 (D1, D2) \u2264\u2211 i max ( \u03c61i \u2212 \u03c62i , 0 ) .\n2. If A\u2217 = \u222ai:\u03c61 i >\u03c62 i Ci then D1 (A\u2217)\u2212 D2 (A\u2217) = \u2211 i max ( \u03c61i \u2212 \u03c62i , 0 ) .\n3. If \u2200i, \u03c61i 6= \u03c62i and A is such that D1 (A)\u2212D2 (A) = L1 (D1, D2) then \u2200i, \u03b8i (A\u2206A\u2217) = 0.\nWe conclude from Lemma 1 that if D1 and D2 were explicitly known and one could have found a classifier that best separates between the distributions, that classifier would not break clusters as long as the mixing coefficients\nare not identical. In order for this to hold when the separation is applied recursively in the DSC algorithm it suffices to have that for every I \u2286 [1, . . . ,K] if |I| > 1 and i \u2208 I then\n\u03c61i\u2211 i\u2032\u2208I \u03c6 1 i\u2032 6= \u03c6 2 i\u2211 i\u2032\u2208I \u03c6 2 i\u2032\nto guarantee that at any stage of the algorithm clusters will not be split by the classifier (but may be sections of measure zero). This is also sufficient to guarantee that the leaves will contain single clusters. In the case where data is provided through a finite sample then some book-keeping is needed. However, the analysis follows the same path. We show that with samples large enough, clusters are only minimally broken. For this to hold we require that the learning algorithm L separates the clusters according to this definition: Definition 1. For I \u2286 [1, . . . ,K] let cI : X 7\u2192 {\u00b11} be such that cI(x) = 1 if x \u2208 \u222ai\u2208ICi and cI(x) = \u22121 otherwise. A learning algorithm L separates C1, . . . , CK if for every , \u03b4 > 0 there exists N such that for every n > N and every measure \u03bd over X\u00d7{\u00b11} with probability 1\u2212 \u03b4 over samples from \u03bdn:\n1. The algorithm L returns an hypothesis h : X 7\u2192 {\u00b11} and an error estimator e \u2208 [0, 1] such that |Prx,y\u223c\u03bd [h (x) 6= y]\u2212 e| \u2264\n2. h is such that \u2200I, Pr\nx,y\u223c\u03bd [h (x) 6= y] < Pr x,y\u223c\u03bd [cI (x) 6= y] + .\nBefore we introduce the main statement, we define what it means for a tree to cluster the mixture components: Definition 2. A clustering tree is a tree in which in each internal node is a classifier and the points that end in a certain leaf are considered a cluster. A clustering tree -clusters the mixture coefficient \u03b81, . . . , \u03b8K if for every i \u2208 1, . . . ,K there exists a leaf in the tree such that the cluster L \u2286 X associated with this leaf is such that \u03b8i (L) \u2265 1 \u2212 and \u03b8i\u2032 (L) < for every i\u2032 6= i.\nTo be able to find a clustering tree, the two mixtures have to be different. The following definition captures the gap which is the amount of difference between the mixtures. Definition 3. Let \u03a61 and \u03a62 be two mixture vectors. The gap, g, between them is\ng = min {\u2223\u2223\u2223\u2223 \u03c61i\u2211\ni\u2032\u2208I \u03c6 1 i\u2032 \u2212 \u03c6\n2 i\u2211\ni\u2032\u2208I \u03c6 2 i\u2032 \u2223\u2223\u2223\u2223 : I \u2286 [1, . . . ,K] and |I| > 1} . We say that \u03a6 is b bounded away from zero if b \u2264 mini \u03c6i. Theorem 2. Assume that L separates \u03b81, . . . , \u03b8K , there is a gap g > 0 between \u03a61 and \u03a62 and both \u03a61 and \u03a62 are bounded away from zero by b > 0. For every \u2217, \u03b4\u2217 > 0 there exists N = N ( \u2217, \u03b4\u2217, g, b,K) such that given two random samples of sizes N < n1, n2 from the two mixtures, with probability of at least 1\u2212 \u03b4\u2217 the DSC algorithm will return a clustering tree which \u2217-clusters \u03b81, . . . , \u03b8K when applied with the threshold \u03c4 = g/8."}, {"heading": "4 Empirical evidence", "text": "We conducted several experiments with synthetic data to compare different methods when clustering in high dimensional spaces. The synthetic data was generated from three Gaussians with centers at points (0, 0) , (3, 0) and (\u22123,+3). On top of that, we added additional dimensions with normally distributed noise. In the first experiment we used unit variance for all dimensions. In the second experiment we skewed the distribution so that the variance in the other features is 5. Two sets of mixing coefficients for the three Gaussians were chosen at random 100 times by selecting three uniform values from [0, 1] and normalizing them to sum to 1. We generated\ntwo samples with 80 examples each from the two mixing coefficients. The DSC and MSP algorithm received these two samples as inputs while the reference algorithms, which are not designed to use multiple samples, received the combined set of 160 points as input. We ran 100 trials. In each trial, each of the algorithms finds 3 Gaussians. We then measure the percentage of the points associated with the true originating Gaussian after making the best assignment of the inferred centers to the true Gaussians. We compared several algorithms. K-means was used on the data as a baseline. We compared three low dimensional projection algorithms. Following [?] we used random projections as the first of these. Second, following [?] we used PCA to project on the maximal variance subspace. MSP was used as the third projection algorithm. In all projection algorithm we first projected on a one dimensional space and then applied K-means to find the clusters. Finally, we used the DSC algorithm. The DSC algorithm uses the classregtree function in MATLAB as its learning oracle. Whenever K-means was applied, the MATLAB implementation of this procedure was used with 10 random initial starts. Figure 3(a) shows the results of the first experiment with unit variance in the noise dimensions. In this setting, the Maximal Variance method is expected to work well since the first two dimensions have larger expected variance. Indeed we see that this is the case. However, when the number of dimensions is large, MSP and DSC outperform the other methods; this corresponds to the difficult regime of low signal to noise ratio. In 12800 dimensions, MSP outperforms Random Projections 90% of the time, Maximal Variance 80% of the time, and K-means 79% of the time. DSC outperforms Random Projections, Maximal Variance and K-means 84%, 69%, and 66% of the time respectively. Thus the p-value in all these experiments is < 0.01. Figure 3(b) shows the results of the experiment in which the variance in the noise dimensions is higher which creates a more challanging problem. In this case, we see that all the reference methods suffer significantly, but the MSP and the DSC methods obtain similar results as in the previous setting. Both the MSP and the DSC algorithms win over Random Projections, Maximal Variance and K-means more than 78% of the time when the dimension is 400 and up. The p-value of these experiments is < 1.6\u00d7 10\u22127."}, {"heading": "5 Conclusions", "text": "The mixture problem examined here is closely related to the problem of clustering. Most clustering data can be viewed as points generated from multiple underlying distributions or generating functions, and clustering can be seen as the process of recovering the structure of or assignments to these distributions. We presented two algorithms for the mixture problem that can be viewed as clustering algorithms. The MSP algorithm uses multiple samples to find a low dimensional space to project the data to. The DSC algorithm builds a clustering tree assuming that the clusters are disjoint. We proved that these algorithms work under milder assumptions than currently known methods. The key message in this work is that when multiple samples are available, often it is best not to pool the data into one large sample, but that the structure in the different samples can be leveraged to improve clustering power."}, {"heading": "A Supplementary Material", "text": "Here we provide detailed analysis of the results presented in the paper that could not fit due to space limitations."}, {"heading": "A.1 Proof of Lemma 1", "text": "Proof. 1. Let A be a measurable set let Ai = A \u2229 Ci. D1 (A)\u2212D2 (A) = \u2211 i D1 (Ai)\u2212D2 (Ai)\n= \u2211 i \u03c61i \u03b8i (Ai)\u2212 \u03c62i \u03b8i (Ai)\n= \u2211 i \u03b8i (Ai) ( \u03c61i \u2212 \u03c62i ) \u2264\n\u2211 i max ( \u03c61i \u2212 \u03c62i , 0 ) .\n2. Let A\u2217 = \u222ai:\u03c61 i >\u03c62 i Ci then\nD1 (A\u2217)\u2212D2 (A\u2217) = \u2211\ni:\u03c61 i >\u03c62 i\n\u03b8i (Ci) ( \u03c61i \u2212 \u03c62i ) =\n\u2211 i max ( \u03c61i \u2212 \u03c62i , 0 ) .\n3. Assume that \u2200i, \u03c61i 6= \u03c62i and A is such that D1 (A) \u2212 D2 (A) = L1 (D1, D2). As before let Ai = A \u2229 Ci then\nD1 (A)\u2212D2 (A) = \u2211 i \u03b8i (Ai) ( \u03c61i \u2212 \u03c62i ) .\nIf exists i such that \u03b8i (A\u2206A\u2217) 6= 0 then there could be two cases. If i\u2217 is such that \u03c61i\u2217 > \u03c6 2 i\u2217 then \u03b8i\u2217 (A\u2217) = 1 hence \u03b8i\u2217 (A) < 1. Therefore,\nD1 (A)\u2212D2 (A) \u2264 \u2211 i \u03b8i (Ai) max ( \u03c61i \u2212 \u03c62i , 0 ) <\n\u2211 i max ( \u03c61i \u2212 \u03c62i , 0 ) which contradicts the assumptions. In the same way, if i\u2217 is such that \u03c61i\u2217 < \u03c62i\u2217 then \u03b8i\u2217 (A\u2217) = 0 hence \u03b8i\u2217 (A) > 0. Therefore,\nD1 (A)\u2212D2 (A) \u2264 \u2211 i \u03b8i (Ai) max ( \u03c61i \u2212 \u03c62i , 0 ) + \u03b8i\u2217 (Ai\u2217) ( \u03c61i \u2212 \u03c62i ) <\n\u2211 i \u03b8i (Ai) max ( \u03c61i \u2212 \u03c62i , 0 )"}, {"heading": "A.2 Proof of MSP Analysis theorem", "text": "Proof. of Theorem 1\n1. The computational complexity is straight forward. The MSP algorithm first computes the expected value for each of the samples. For every sample this takes nNj . Once the expected values were computed, computing each of the v\u0304j vector is 2n operations.\n2. Recall that Dj = \u2211 i \u03c6 j i\u03b8i. We can rewrite it as\nDj = K\u2211 i=1 \u03c6ji\u00b5i + K\u2211 i=1 \u03c6ji (\u03b8i \u2212 \u00b5i) = Ej + K\u2211 i=1 \u03c6ji (\u03b8i \u2212 \u00b5i) .\nNote that for every i, (\u03b8i \u2212 \u00b5i) has a zero mean and variance bounded by \u03c32max. Since \u03c6ji \u2265 0 and \u2211 i \u03c6 j i = 1 then the measure \u2211K i=1 \u03c6 j i (\u03b8i \u2212 \u00b5i) has zero mean and variance bounded by \u03c32max. Hence, Dj is a measure with mean Ej and variance bounded by \u03c32max. Since E\u0304j is obtained by averaging Nj instances, we get, from Chebyshev\u2019s inequality combined with the union bound that\nPr [\u2225\u2225Ej \u2212 E\u0304j\u2225\u2225 > ] \u2264 n\u03c32max\nNj 2 .\nSince there are m estimators, E\u03041, . . . , E\u0304m, using the union bound we obtain Pr [ sup j \u2225\u2225Ej \u2212 E\u0304j\u2225\u2225 > ] \u2264\u2211 j n\u03c32max Nj 2 .\n3. Recall that \u00b5\u0304i is the projection of \u00b5i on the space V\u0304 = span (v\u03041, . . . , v\u0304m\u22121). Therefore, \u00b5\u0304i = arg minv\u2208V\u0304 (\u2016\u00b5i \u2212 v\u2016) . Since \u00b5i \u2208 span {v1, . . . , vm\u22121} then \u00b5i = \u2211 \u03b1ijvj .\n\u2016\u00b5i \u2212 \u00b5\u0304i\u2016 \u2264 \u2225\u2225\u2225\u2211\u03b1jvj \u2212\u2211\u03b1j v\u0304j\u2225\u2225\u2225\n\u2264 \u2211 |\u03b1|j \u2016vj \u2212 v\u0304j\u2016 .\nHence, if A = maxi \u2211\u2223\u2223\u03b1ij\u2223\u2223 then with probability of at least 1\u2212 n\u03c32max 2 \u2211j 1nj\nmax i \u2016\u00b5i \u2212 \u00b5\u0304i\u2016 \u2264 A .\nFurthermore, max i,i\u2032 |\u2016\u00b5i \u2212 \u00b5i\u2032\u2016 \u2212 \u2016\u00b5\u0304i \u2212 \u00b5\u0304i\u2032\u2016| \u2264 2 A .\nIt is possible to improve upon the bounds presented here. We can get sharper bounds on the probability of success in several ways:\n1. If we assume that the sample space is bounded we can use Bernstein\u2019s inequality instead of Chebyshev\u2019s inequality\n2. If we assume that the covariance matrix is diagonal we can replace the union bounded with better concentration of measure bounds\n3. If we assume that the distributions are Gaussians we can use tail bounds on these distribution instead of Chebyshev\u2019s inequality\nTo simplify the presentation, we do not derive the bounds for these specific conditions."}, {"heading": "A.3 Proof of Theorem 2", "text": "The analysis we use several lemmas. To simplify the notation we define the following assumptions: Assumption 1. The gap between \u03a61 and \u03a62 is g > 0. Assumption 2. Both \u03a61 and \u03a62 are bounded away from zero by b > 0. Definition 4. For the set A, we say that the i\u2019th cluster is \u03b3-big if \u03b8i (A) \u2265 1\u2212 \u03b3 and we say that it is \u03b3-small if \u03b8i (A) \u2264 \u03b3.\nAssumption 3. For the set A all clusters are either \u03b3-big or \u03b3-small and there exists at least one \u03b3-big cluster. Assumption 4. The classifier h and the estimator e are such that\u2223\u2223\u2223\u2223 Prx,y\u223c\u03bd [h (x) 6= y]\u2212 e\n\u2223\u2223\u2223\u2223 \u2264 and\n\u2200I, Pr x,y\u223c\u03bd [h (x) 6= y] < Pr x,y\u223c\u03bd [cI (x) 6= y] +\nwhere \u03bd is a measure on X \u00d7 {\u00b11} such that the measure of B \u2286 X \u00d7 {\u00b11} is\n\u03bd (B) = 12\n( D1 ({x \u2208 A : (x, 1) \u2208 B})\nD1 (A) + D2 ({x \u2208 A : (x,\u22121) \u2208 B}) D2 (A)\n) .\nUsing these assumptions we turn to prove the lemmas. Lemma 2. Under assumptions 2 and 3, if I = { i : \u03c6 1 i\u2211\ni\u2032 \u03c61 i\u2032 \u03b8i\u2032 (A)\n> \u03c62i\u2211\ni\u2032 \u03c62 i\u2032 \u03b8i\u2032 (A)\n} , there\nare more than a single \u03b3-big cluster and \u03b3 < min (b/2, gb/k+3) then\nPr x,y\u223c\u03bd [cI (x) 6= y] \u2264 1 2\n( 1\u2212 g (1\u2212 \u03b3) + 3K\u03b3\nb ) where \u03bd is as defined in assumption 4. Moreover, the set I contains at least one \u03b3-big cluster but does not contain all the \u03b3-big clusters.\nProof. Let J be the set of \u03b3 big clusters. From the definition of \u03b3 we have that\nPr x,y\u223c\u03bd [cI (x) 6= y] = 1 2 (\u2211 i/\u2208I \u03c61i \u03b8i\u2032 (A)\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) + \u2211 i\u2208I \u03c62i \u03b8i\u2032 (A)\u2211 i\u2032 \u03c6 2 i\u2032\u03b8i\u2032 (A) )\n= 12\n( 1\u2212\n\u2211 i\u2208I ( \u03c61i\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) \u2212 \u03c6 2 i\u2211 i\u2032 \u03c6 2 i\u2032\u03b8i\u2032 (A) ) \u03b8i\u2032 (A) )\n\u2264 12\n( 1\u2212\n\u2211 i\u2208I\u2229J ( \u03c61i\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) \u2212 \u03c6 2 i\u2211 i\u2032 \u03c6 2 i\u2032\u03b8i\u2032 (A) ) \u03b8i\u2032 (A) )\n\u2264 12\n( 1\u2212\n\u2211 i\u2208I\u2229J ( \u03c61i\u2211 i\u2032\u2208J \u03c6 1 i\u2032 + \u03b3 \u2212 \u03c6 2 i\u2211 i\u2032\u2208J \u03c6 2 i\u2032 \u2212 \u03b3 ) \u03b8i\u2032 (A) ) .\nDue to assumption 2 \u03c61i\u2211\ni\u2032\u2208J \u03c6 1 i\u2032 + \u03b3\n= \u03c6 1 i\u2211\ni\u2032\u2208J \u03c6 1 i\u2032 \u2212 \u03b3\u03c6\ni i(\u2211\ni\u2032\u2208J \u03c6 1 i\u2032 ) (\u2211 i\u2032\u2208J \u03c6 1 i\u2032 + \u03b3 ) \u2265 \u03c6 1 i\u2211\ni\u2032\u2208J \u03c6 1 i\u2032 \u2212 \u03b3\u2211 i\u2032\u2208J \u03c6 1 i\u2032 + \u03b3\n\u2265 \u03c6 1 i\u2211\ni\u2032\u2208J \u03c6 1 i\u2032 \u2212 \u03b3 b . (1)\nSince \u03b3 < b/2 we have \u03c62i\u2211\ni\u2032\u2208J \u03c6 2 i\u2032 \u2212 \u03b3\n= \u03c6 2 i\u2211\ni\u2032\u2208J \u03c6 2 i\u2032\n+ \u03b3\u03c6 2 i(\u2211\ni\u2032\u2208J \u03c6 2 i\u2032 ) (\u2211 i\u2032\u2208J \u03c6 2 i\u2032 \u2212 \u03b3 ) \u2264 \u03c6 2 i\u2211\ni\u2032\u2208J \u03c6 2 i\u2032 + \u03b3\u2211 i\u2032\u2208J \u03c6 2 i\u2032 \u2212 \u03b3\n\u2264 \u03c6 2 i\u2211\ni\u2032\u2208J \u03c6 2 i\u2032\n+ 2\u03b3 b . (2)\nTherefore,\nPr x,y\u223c\u03bd [cI (x) 6= y] \u2264 1 2\n( 1\u2212\n\u2211 i\u2208I\u2229J ( \u03c61i\u2211 i\u2032\u2208J \u03c6 1 i\u2032 + \u03b3 \u2212 \u03c6 2 i\u2211 i\u2032\u2208J \u03c6 2 i\u2032 \u2212 \u03b3 ) \u03b8i (A) )\n\u2264 12\n( 1\u2212\n\u2211 i\u2208I\u2229J ( \u03c61i\u2211 i\u2032\u2208J \u03c6 1 i\u2032 \u2212 \u03c6 2 i\u2211 i\u2032\u2208J \u03c6 2 i\u2032 \u2212 3\u03b3 b ) \u03b8i (A) )\n\u2264 12\n( 1\u2212\n\u2211 i\u2208I\u2229J ( \u03c61i\u2211 i\u2032\u2208J \u03c6 1 i\u2032 \u2212 \u03c6 2 i\u2211 i\u2032\u2208J \u03c6 2 i\u2032 ) \u03b8i (A) + 3K\u03b3 b )\n\u2264 12\n( 1\u2212\n\u2211 i\u2208I\u2229J g\u03b8i (A) + gb/k+3 3K\u03b3 b\n)\n\u2264 12\n( 1\u2212 g (1\u2212 \u03b3) + 3K\u03b3\nb\n) .\nNote that we have used the fact that I \u2229 J is not empty. Otherwise, note that since \u03b3 < 1/2 and from (1) and (2)\n0 = \u2211 i\u2208I ( \u03c61i \u03b8i\u2032 (A)\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) \u2212 \u03c6 2 i \u03b8i\u2032 (A)\u2211 i\u2032 \u03c6 2 i\u2032\u03b8i\u2032 (A) ) \u2212 \u2211 i/\u2208I ( \u03c62i \u03b8i\u2032 (A)\u2211 i\u2032 \u03c6 2 i\u2032\u03b8i\u2032 (A) \u2212 \u03c6 1 i \u03b8i\u2032 (A)\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) )\n\u2264 \u03b3 \u2211 i\u2208I ( \u03c61i\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) ) \u2212 (1\u2212 \u03b3) \u2211 i\u2208J ( \u03c62i\u2211 i\u2032 \u03c6 2 i\u2032\u03b8i\u2032 (A) \u2212 \u03c6 1 i\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) ) \u2264 \u03b3K 12b (1\u2212 \u03b3) \u2212 (1\u2212 \u03b3) \u2211 i\u2208J ( g \u2212 3\u03b3 b\n) \u2264 \u03b3K\nb \u2212 2 (1\u2212 \u03b3)\n( g \u2212 3\u03b3\nb ) \u2264 \u03b3K b \u2212 ( g \u2212 3\u03b3 b\n) = \u03b3K + 3\nb \u2212 g .\nHowever, since \u03b3 < gb/k+3 we obtain \u03b3K+3b \u2212 g < 0 which is a contradiction. Therefore, I \u2229 J is not empty. In the same way we can see that I\u0304 \u2229 J is not empty as well.\nLemma 3. Under assumptions 1, 2, 3 and 4 if \u03b3 < min (b/2, gb/3) then\n\u2200i, \u03b8i (A \u2229 h) , \u03b8i ( A \u2229 h\u0304 ) /\u2208 [ \u03b3 + 2\ng \u2212 3\u03b3b , 1\u2212 \u03b3 \u2212 2 g \u2212 3\u03b3b\n] .\nMoreover, if I = { i : \u03c6 1 i\u2211\ni \u03c61 i \u03b8i(A)\n> \u03c62i\u2211\ni \u03c62 i \u03b8i(A) } then \u03b8i (A \u2229 h) \u2265 1 \u2212 \u03b3 \u2212 2 g\u2212 3\u03b3b iff i is a\n\u03b3-big cluster in A and i \u2208 I. Proof. Recall that for every set B, Dj (B) = \u2211 i \u03c6 j i\u03b8i (B). By using h both as a function and as a subset of X we have\nPr x,y\u223c\u03bd [h (x) 6= y] = 12 ( D1 (A \\ h) D1 (A) + D2 (h) D2 (A) ) = 12 ( 1\u2212 ( D1 (h \u2229A) D1 (A) \u2212 D2 (h \u2229A) D2 (A)\n)) = 12 ( 1\u2212 (\u2211 i \u03c6 1 i \u03b8i (h \u2229A)\u2211 i \u03c6 1 i \u03b8i (A) \u2212 \u2211 i \u03c6 2 i \u03b8i (h \u2229A)\u2211 i \u03c6 2 i \u03b8i (A)\n)) = 12 ( 1\u2212 \u2211 i ( \u03c61i\u2211 i\u2032 \u03c6 1 i\u2032\u03b8i\u2032 (A) \u2212 \u03c6 2 i\u2211 i\u2032 \u03c6 2 i\u2032\u03b8i\u2032 (A) ) \u03b8i (h \u2229A) )\nLet I = { i : \u03c6 1 i\u2211\ni \u03c61 i \u03b8i(A)\n> \u03c62i\u2211\ni \u03c62 i \u03b8i(A)\n} . It follows that cI = arg minh Prx,y\u223c\u03bd [h (x) 6= y]\nand hence Pr\nx,y\u223c\u03bd [cI (x) 6= y] \u2264 Pr x,y\u223c\u03bd [h (x) 6= y] < Pr x,y\u223c\u03bd [cI (x) 6= y] + .\nTherefore,\n> Pr x,y\u223c\u03bd [h (x) 6= y]\u2212 Pr x,y\u223c\u03bd [cI (x) 6= y]\n= 12 (\u2211 i (Ii\u2208I\u03b8i (A)\u2212 \u03b8i (h \u2229A)) ( \u03c61i\u2211 i \u03c6 1 i \u03b8i (A) \u2212 \u03c6 2 i\u2211 i \u03c6 2 i \u03b8i (A) ))\n\u2265 12 maxi (Ii\u2208I\u03b8i (A)\u2212 \u03b8i (h \u2229A)) ( \u03c61i\u2211 i \u03c6 1 i \u03b8i (A) \u2212 \u03c6 2 i\u2211 i \u03c6 2 i \u03b8i (A) ) Let J = {j : \u03b8j (A) > 1\u2212 \u03b3} then\n\u03c61i\u2211 i \u03c6 1 i \u03b8i (A) \u2212 \u03c6 2 i\u2211 i \u03c6 2 i \u03b8i (A) \u2265 \u03c6 1 i\u2211 j\u2208J \u03c6 1 j + \u2211 j\u2208J \u03c6 1 j\u03b3 \u2212 \u03c6 2 i\u2211 j\u2208J \u03c6 2 j (1\u2212 \u03b3)\n\u2265 \u03c6 1 i\u2211\nj\u2208J \u03c6 1 j + \u03b3\n\u2212 \u03c6 2 i\u2211\nj\u2208J \u03c6 2 j \u2212 \u03b3\nand hence \u2265 12 maxi (Ii\u2208I\u03b8i (A)\u2212 \u03b8i (h \u2229A)) ( \u03c61i\u2211 j\u2208J \u03c6 1 j + \u03b3 \u2212 \u03c6 2 i\u2211 j\u2208J \u03c6 2 j \u2212 \u03b3 ) or put otherwise\nmax i\n(Ii\u2208I\u03b8i (A)\u2212 \u03b8i (h \u2229A)) \u2264 2 mini ( \u03c61 i\u2211\nj\u2208J \u03c61 j +\u03b3 \u2212 \u03c6\n2 i\u2211\nj\u2208J \u03c62 j \u2212\u03b3 ) . (3) Note that since \u03b3 < b/2 from (1) and (2) it follows that\nmin i\n( \u03c61i\u2211\nj\u2208J \u03c6 1 j + \u03b3\n\u2212 \u03c6 2 i\u2211\nj\u2208J \u03c6 2 j \u2212 \u03b3\n) \u2265 min\ni ( \u03c61i\u2211 j\u2208J \u03c6 1 j \u2212 \u03c6 2 i\u2211 j\u2208J \u03c6 2 j ) \u2212 3\u03b3 b\n\u2265 g \u2212 3\u03b3 b .\nNote that g\u2212 3\u03b3/b > 0 since \u03b3 < gb/3. If i is such that \u03b8i (A) < \u03b3 then clearly \u03b8i (h \u2229A) < \u03b3. However, if \u03b8i (A) > 1\u2212 \u03b3 and i \u2208 I then from (3) we have that\n\u03b8i (h \u2229A) \u2265 \u03b8i (A)\u2212 2\ng \u2212 3\u03b3b .\n\u2265 1\u2212 \u03b3 \u2212 2 g \u2212 3\u03b3b\nIf however, \u03b8i (A) > 1\u2212 \u03b3 but i /\u2208 I then we can repeat the same argument for h\u0304 to get\n\u03b8i ( h\u0304 \u2229A ) \u2265 1\u2212 \u03b3 \u2212 2\ng \u2212 3\u03b3b and thus\n\u03b8i (h \u2229A) \u2264 \u03b3 + 2\ng \u2212 3\u03b3b .\nLemma 4. Under assumptions 2,3 and 4, if there is only a single \u03b3-big cluster then e \u2265 12 \u2212 \u03b3 ( 1 \u03b3 + b (1\u2212 \u03b3) + 1 1\u2212 \u03b3 + 1 b+ \u03b3 ) \u2212\nProof. Let i\u2217 be such that \u03b8i\u2217 is the single \u03b3-big cluster. For j = 1, 2\nDj (Ci\u2217 \u2229A) Dj (A) = \u03c6 j i\u2217\u03b8i\u2217 (A)\u2211 i \u03c6 j i\u03b8i (A)\n= 1 1 + \u2211 i6=i\u2217 \u03c6j i \u03b8i(A)\n\u03c6j i\u2217\u03b8i\u2217 (A)\n\u2265 11 + \u03b3b(1\u2212\u03b3) .\nFor any h,\u2223\u2223\u2223\u2223Dj (A \u2229 h)Dj (A) \u2212 \u03b8i\u2217 (h) \u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223Dj ((A \\ Ci\u2217) \u2229 h)Dj (A) \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223Dj (A \u2229 Ci\u2217 \u2229 h)Dj (A) \u2212 \u03b8i\u2217 (h) \u2223\u2223\u2223\u2223\n\u2264 \u03b3 \u03b3 + b (1\u2212 \u03b3) + \u2223\u2223\u2223\u2223\u2223\u03c6ji\u2217\u03b8i\u2217 (A \u2229 h)Dj (A) \u2212 \u03b8i\u2217 (h) \u2223\u2223\u2223\u2223\u2223\n\u2264 \u03b3 \u03b3 + b (1\u2212 \u03b3) + \u2223\u2223\u2223\u2223\u2223\u03c6 j i\u2217\u03b8i\u2217 ( A\u0304 ) Dj (A) \u2223\u2223\u2223\u2223\u2223+ \u03b8i\u2217 (h) \u2223\u2223\u2223\u2223\u2223 \u03c6ji\u2217Dj (A) \u2212 1 \u2223\u2223\u2223\u2223\u2223 \u2264 \u03b3\n\u03b3 + b (1\u2212 \u03b3) + \u03b3 1\u2212 \u03b3 + \u03b8i \u2217 (h)\n( 1\u2212 \u03c6 j i\u2217\n\u03c6ji\u2217 + \u03b3 ) \u2264 \u03b3\n\u03b3 + b (1\u2212 \u03b3) + \u03b3 1\u2212 \u03b3 + \u03b3 b+ \u03b3 .\nTherefore,\u2223\u2223\u2223\u2223D1 (A \u2229 h)D1 (A) \u2212 D2 (A \u2229 h)D2 (A) \u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223D1 (A \u2229 h)D1 (A) \u2212 \u03b8i\u2217 (h) \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223D2 (A \u2229 h)D2 (A) \u2212 \u03b8i\u2217 (h) \u2223\u2223\u2223\u2223\n\u2264 2\u03b3 (\n1 \u03b3 + b (1\u2212 \u03b3) + 1 1\u2212 \u03b3 + 1 b+ \u03b3\n) .\nHence\ne \u2265 Pr x,y\u223c\u03bd [h (x) 6= y]\u2212\n= 12 \u2212 1 2 \u2223\u2223\u2223\u2223D1 (A \u2229 h)D1 (A) \u2212 D2gb/3 (A \u2229 h)D2 (A) \u2223\u2223\u2223\u2223\u2212\n\u2265 12 \u2212 \u03b3 ( 1 \u03b3 + b (1\u2212 \u03b3) + 1 1\u2212 \u03b3 + 1 b+ \u03b3 ) \u2212 .\nLemma 5. Under assumptions 2,3 and 4, if there are t > 1 clusters which are \u03b3-big for \u03b3 < min (b/2, gb/k+3) then\ne \u2264 12\n( 1\u2212 g (1\u2212 \u03b3) + 3K\u03b3\nb\n) + 2\nand the split induced by h will have at least one \u03b3 + 2 g\u2212 3\u03b3b big cluster in each side of the split.\nProof. Let I = { i : \u03c6 1 i\u2211\ni \u03c61 i \u03b8i(A)\n> \u03c62i\u2211\ni \u03c62 i \u03b8i(A)\n} then from Lemma 2\nPr x,y\u223c\u03bd [cI (x) 6= y] \u2264 1 2\n( 1\u2212 g (1\u2212 \u03b3) + 3K\u03b3\nb ) and I contains at least one \u03b3-big cluster but does not contain all the \u03b3 big clusters. From Assumption 4 it follows that\ne \u2264 12\n( 1\u2212 g (1\u2212 \u03b3) + 3K\u03b3\nb\n) + 2 .\nMoreover, from Lemma 3 a cluster is \u03b3 + 2 g\u2212 3\u03b3b -big if and only if it is \u03b3-big in A and it is in I. Since I contains a \u03b3-big cluster than A \u2229 h contains a \u03b3 \u2212 2\ng\u2212 3\u03b3b -cluster. However, since\nI does not contain all the \u03b3-big clusters, there is a \u03b3 + 2 g\u2212 3\u03b3b -cluster in A \u2229 h\u0304 as well.\nWe are now ready to prove the main theorem.\nProof. of Theorem 2 Let < min (\ng2b 160K , bg 8K , g2b 4K(K+3) , \u2217g 4K ) and let \u03b3\u0302l = 4 lg then for every l \u2264 K we have\n3\u03b3\u0302l b < g 2 , \u03b3\u0302l < min ( b 2 , gb K + 3 ) .\nLet \u03b4 = \u03b4 \u2217\n4K and N1 be the size of the samples needed for L to return an , \u03b4 good hypothesis. Let N = max (\n4N1 b , 2 b2 log 1 \u03b4\n) .\nNote the following, if A is a set that contains at least one \u03b3-big cluster, for \u03b3 \u2264 \u03b3\u0302K then with probability 1\u2212 \u03b4, a sample of N points from \u03b3 contains atleast N1 points from A. To see that, note that\n\u03b3 \u2264 4 K g < 4K g \u00b7 gb8K \u2264 gb 2 \u2264 1 2 .\nSince each cluster is bounded away from zero by b > 0, the expected number points in A out of a sample of size N is at least (1\u2212 \u03b3) bN \u2265 bN/2. From Hoeffding\u2019s inequality, we have that for N \u2265 2b2 log 1 \u03b4 with a probability of 1 \u2212 \u03b4 there will be at least bN/4 points in A. Since N \u2265 4N1/b, we conclude that with a probability of at least 1 \u2212 \u03b4, there will be atleast N1 points from A in a random set of N points. Therefore, with probability 1\u2212 \u03b4/2, in the first 2K calls for the learning algorithm L, for which there was at least one \u03b3-big cluster in the leaf, there were at least N1 points to train the algorithm from, provided that \u03b3 \u2264 \u03b3\u0302K . Hence, with probability 1 \u2212 \u03b4, Assumption 4 holds for the first 2K call to the learning algorithm, provided that \u03b3 \u2264 \u03b3\u0302K . Next we will show that as long as Assumption 4 holds, the DSC algorithm will make at most 2K calls to L and all leafs will have at least one \u03b3-big cluster for \u03b3 \u2264 \u03b3\u0302K . We prove that by induction on the depth of the generated tree. Initially, we have a single leaf with all the points and hence all the clusters are 0-big hence the assumption clearly works. From Lemma 3 it follows that if all the clusters were \u03b3-big or \u03b3-small, and indeed Assumption 4 holds, then the new leafs that are generated by splitting on h will have only \u03b3\u2032-big or \u03b3\u2032-small clusters for \u03b3\u2032 \u2264 \u03b3 + 2\ng\u2212 3\u03b3b . Note that if \u03b3 < gb/6 then g \u2212 3\u03b3/b > g/2 hence \u03b3\u2032 \u2264 \u03b3 + 4 /g.\nFrom Lemma 4 and Lemma 5 it follows that every time the DSC algorithm calls the L algorithm, assuming that Assumption 4 holds for this call, one of two things happen, either the algorithm finds a non-trivial split of the clusters, which happens whenever there is more\nthan a single big cluster, in which case e \u2264 12 ( 1\u2212 g (1\u2212 \u03b3) + 3K\u03b3b ) + 2 or otherwise, if\nthere is only a single big cluster, e \u2265 12 \u2212 \u03b3 ( 1 \u03b3+b(1\u2212\u03b3) + 1 1\u2212\u03b3 + 1 b+\u03b3 ) \u2212 . Note that if\n\u03b3 \u2264 min (1/2 , 4K /g) then\n1 2 \u2212 \u03b3\n( 1\n\u03b3 + b (1\u2212 \u03b3) + 1 1\u2212 \u03b3 + 1 b+ \u03b3 ) \u2212 \u2265 12 \u2212 4K g ( 2 b + 2 + 1 b ) \u2212\n\u2265 12 \u2212 K gb\n( 12 + 8b+ gb\nK\n)\n> 1 2 \u2212 20K gb .\nSince, < g2b/160K, if there was only a single cluster and Assumption 4 holds then e > 12\u2212 g 8 . However, if there were multiple big clusters then\ne \u2264 12\n( 1\u2212 g (1\u2212 \u03b3) + 3K\u03b3\nb ) + 2 \u2264 12 \u2212 g 4 .\nHence, assuming that Assumption 4 holds for all splits then the algorithm will split every leaf that contain multiple big clusters and will not split any leaf that contain a single big cluster. Therefore, after K \u2212 1 splits, all the leafs will have a single big cluster. For each of the K leaf, the DSC algorithm will call L once to determine that it contains a single cluster and hence the number of calls to L will be at most 2K \u2212 1 and in each call all the clusters are either \u03b3-big or \u03b3-small for \u03b3 \u2264 \u03b3K = 4K /g. And therefore, with probability 1\u2212 \u03b4\u2217, the DSC algorithm will return a \u03b3K clustering tree. Since\n\u03b3K = 4 K g \u2264\n4K \u2217g\n4K g = \u2217 ,\nthis is an \u2217 clustering tree."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "In the mixture models problem it is assumed that there are K distributions \u03b81, . . . , \u03b8K and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data.", "creator": "LaTeX with hyperref package"}}}