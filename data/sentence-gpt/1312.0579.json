{"id": "1312.0579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2013", "title": "SpeedMachines: Anytime Structured Prediction", "abstract": "Structured prediction plays a central role in machine learning applications from computational biology to computer vision. These models require significantly more computation than unstructured models, and, in many applications, algorithms may need to make predictions within a computational budget or in an anytime fashion. In this work we propose an anytime technique for learning structured prediction that, at training time, incorporates both structural elements and feature computation trade-offs that affect test-time inference, and the time-dependent inference of such predictions. Using a combination of structural elements, we describe the effects of machine learning and machine learning algorithms on learning decision-making, which can be generalized in more detail in an in-depth review of this paper.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 2 Dec 2013 20:26:41 GMT  (849kb,D)", "http://arxiv.org/abs/1312.0579v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexander grubb", "daniel munoz", "j", "rew bagnell", "martial hebert"], "accepted": false, "id": "1312.0579"}, "pdf": {"name": "1312.0579.pdf", "metadata": {"source": "CRF", "title": "SpeedMachines: Anytime Structured Prediction", "authors": ["Alexander Grubb", "Daniel Munoz", "J. Andrew Bagnell"], "emails": ["agrubb@cmu.edu", "dmunoz@cs.cmu.edu", "dbagnell@cs.cmu.edu", "hebert@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "In real-world applications, we are often forced to trade-off between accurate predictions and the computation time needed to make them. In many problems, structured prediction algorithms are necessary to obtain accurate predictions; however, this class of techniques is typically more computationally demanding over simpler locally independent predictions. Furthermore, under limited computational resources we may forced to make a prediction after a limited and unknown amount of time. Therefore, we require an approach that is efficient while also capable of returning a sensible prediction when requested at any time. Furthermore, as the inference procedure is given more time, we should expect the predictive performance to also increase.\n\u2217agrubb@cmu.edu \u2020dmunoz@cs.cmu.edu \u2021dbagnell@cs.cmu.edu \u00a7hebert@cs.cmu.edu\nar X\niv :1\n31 2.\n05 79\nv1 [\ncs .L\nG ]\nThe contribution of this work is an algorithm for making anytime structured predictions. As detailed in the following sections, our approach accounts for both inference and feature computation times while automatically trading-off cost for accuracy, and vice versa, in order to maximize predictive performance with respect to computation time. Although our approach is applicable towards a variety of different structured prediction problems, we analyze its efficacy on the challenging problem of scene understanding in computer vision. Our experiments demonstrate that we can learn an efficient, anytime predictor whose classification performance improves towards state-of-the-art while automatically selecting what features to compute and where to compute them with respect to time."}, {"heading": "1.1 Related Work", "text": "A canonical approach for incorporating computation time during learning is a cascade of feed-forward modules, where each module becomes more sophisticated but also more computationally expensive the further it is down the cascade [26]. One drawback of the cascade approach is that the procedure is trained for a specific sequence length and is not suited for interruption. For example, stopping predictions after one module in a long cascade will generally perform much worse than a predictor that was trained in isolation. Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.\nIn the structured setting, Jiang et al. [13] proposed a technique for reinforcement learning that incorporates a user specified speed/accuracy trade-off distribution, and Weiss and Taskar [27] proposed a cascaded analog for structured prediction where the solution space is is iteratively refined/pruned over time. In contrast, we are focused on learning a structured predictor with interruptible, anytime properties which is also trained to balance both the structural and feature computation times during the inference procedure. Recent work in computer vision and robotics [23, 5] has similarly investigated techniques for making approximate inference in graphical models more efficient via a cascaded procedure that iteratively prunes subregions in the scene to analyze. We similarly incorporate such structure selection in our approach; however, we also account for feature computation time and avoid the early hard commitments required with a cascaded approach. This allows for early predictions to be as accurate as possible, as they do not have to be made conservatively as in the cascade approach."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Structured Prediction", "text": "In the structured prediction setting, we are given inputs x \u2208 X and associated structured outputs y \u2208 Y. The goal is to learn a function f : X \u2192 Y that minimizes some risk R[f ], typically evaluated pointwise over the inputs:\nR[f ] = EX [l(f(x))]. (1)\nWe will further assume that each input and output pair has some underlying structure, such as the graph structure of graphical models, that can be utilized to predict portions of the output locally. Let j index these structural elements. We then assume that a final structured output y can be represented as a variable length vector (y1, . . . , yJ), where each element yj lies in some vector space yj \u2208 Y \u2032. For example, these outputs could be the probability distribution over class labels for each pixel in an image, or distributions of part-of-speech labels for each word in a sentence. Similarly we can compute some features xj representing the portion of the input which corresponds to a given output, such as features computed over a neighborhood around a pixel in an input image.\nAs another example, consider labeling tasks such as part-of-speech tagging. In this domain, we are given a set of input sentences X , and for each word j in a given sentence, we want to output a vector y\u0302j \u2208 RK containing the scores with respect to each of the K possible part-of-speech labels for that word. This sequence of vectors for each word is the complete structured prediction y\u0302. An example loss function for this domain would be the multiclass log-loss, averaged over words, with respect to the ground truth parts-of-speech.\nAlong with the encoding of the problem, we also assume that the structure can be used to reduce the scope of the prediction problem, as in graphical models. One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20]. In order to model the contextual relationships among the outputs, these iterative approaches commonly perform a sequence of predictions, where each update relies on previous predictions made across the structure of the problem.\nLet N(j) represent the locally connected elements of j, such as the locally connected factors of a node j in a typical graphical model. For a given node j, the predictions over the neighboring nodes y\u0302N(j) can then be used to update the prediction for that node. For example, in the character recognition task, the predictions for neighboring characters can influence the prediction for a given character, and be used to update and improve the accuracy of that prediction.\nIn the iterative decoding approach a predictor \u03c6 is iteratively used to update different elements y\u0302j of the final structured output:\ny\u0302j = \u03c6(xj , y\u0302N(j)). (2)\nA complete policy then consists of a strategy for selecting which elements of the structured output to update, coupled with the predictor for updating the\ngiven outputs. Typical approaches include randomly selecting elements to update, iterating over the structure in a fixed ordering, or simultaneously updating all predictions at all iterations. As shown by Ross et al. [20], this iterative decoding approach can is equivalent to message passing approaches used to solve graphical models, where each update encodes a single set of messages passed to one node in the graphical model."}, {"heading": "2.2 Anytime Prediction", "text": "In traditional boosting, the goal is to learn a function f(x) = \u2211 t \u03b1tht(x), (3)\nwhich is additively built from a set of weaker predictors h \u2208 H that minimizes some risk functional arg minf\u2208F R[f ]. Minimizing this functional can be viewed as performing gradient descent in function space [18, 8]. Assuming the loss function is of the form given in (Eq. 1), the functional gradient, \u2207 = \u2207fR[f ], is a function of the form\n\u2207(x) = \u2202l(f(x)) \u2202f(x) . (4)\nThe weak predictor h that best minimizes the projection error of the functional gradient is selected at each iteration:\nht = arg max h\u2208H\n\u3008\u2207R[ft\u22121], h\u3009, (5)\n\u03b1t = arg min \u03b1\u2208R\nR[ft\u22121 + \u03b1ht]. (6)\nMinimizing this projection error can be equivalently performed through the least squares minimization\nht = arg min h\u2208H\nEX [ \u2016\u2207(x)\u2212 h(x)\u20162 ] . (7)\nExtending this framework, Grubb and Bagnell [12] introduce an anytime prediction method that modifies the standard boosting criterion to automatically trade-off the loss of a weak predictor, h, with its cost c(h) \u2208 R+. They do this by using a cost-greedy selection criterion that selects the weak predictor which gets the best improvement in loss per unit cost,\nht, \u03b1t = arg max h\u2208H,\u03b1\u2208R R [ft\u22121]\u2212R [ft\u22121 + \u03b1h] c(h) . (8)\nHence, for some fixed number of iterations T , the total cost of the learned function is c(fT ) = \u2211T t=1 c(ht). The learned predictor can easily adjust to any new budget of costs by evaluating the sequence until the budget is exhausted. Grubb and Bagnell prove that this SpeedBoost algorithm updates the resulting predictions at an increasing sequence of budgets that is competitive with any other sequence which uses the same weak predictors for a wide range of budgets [12]."}, {"heading": "3 Anytime Structured Prediction", "text": ""}, {"heading": "3.1 Weak Structured Predictors", "text": "We adapt the SpeedBoost framework to the structured prediction setting by learning an additive structured predictor. To accomplish this, we will adapt the policy-based iterative decoding approach to use an additive policy instead of one which replaces previous predictions.\nIn the iterative decoding described previously, recall that we have two components, one for selecting which elements to update, and another for updating the predictions of the given elements. Let St be the set of components selected for updating at iteration t. For current predictions yt we can re-write the policy for computing the predictions at the next iteration of the iterative decoding procedure as:\ny\u0302t+1j =\n{ \u03c6(xj , y\u0302 t N(j)) if j \u2208 S t\ny\u0302tj otherwise . (9)\nThe additive version of this policy instead uses weak predictors h, each of which maps both the input data and previous structured output to a more refined structured output, h : X \u00d7 Y \u2192 Y:\ny\u0302t+1 = y\u0302t + h(x, y\u0302t). (10)\nWe can build a weak predictor h which performs the same actions as the previous replacement policy by considering weak predictors with two parts: a function hS which selects which structural elements to update, and a predictor hP which runs on the selected elements and updates the respective pieces of the structured output.\nThe selection function hS takes in an input x and previous prediction y\u0302 and outputs a set of structural nodes S = {j1, j2, . . .} to update. For each structural element selected by hS, the predictor hP takes the place of \u03c6 in the previous policy, taking (xj , y\u0302N(j)) and computing an update for the prediction y\u0302j .\nReturning to the part-of-speech tagging example, possible selection functions would select different chunks of the sentence, either individual words or multiword phrases using some selection criteria. Given the set of selected elements, a prediction function would take each selected word or phrase and update the predicted distribution over the part-of-speech labels using the features for that word or phrase.\nUsing these elements we can write the weak predictor h, which produces a structured output (h(\u00b7)1, . . . , h(\u00b7)J), as\nh(x, y\u0302t)j =\n{ hP(xj , y\u0302 t N(j)) if j \u2208 hS(x, y\u0302)\n0 otherwise , (11)\nor alternatively we can write this using an indicator function:\nh(x, y\u0302t)j = 1(j \u2208 hS(x, y\u0302t))hP(xj , y\u0302tN(j)). (12)\nThe adapted cost model for this weak predictor is then simply the sum of the cost of evaluating both the selection function and the prediction function,\nc(h) = c(hS) + c(hP). (13)"}, {"heading": "3.2 Selecting Weak Predictors", "text": "In order to use the greedy improvement-per-unit-time selection strategy used by SpeedBoost in (Eq. 8), we need to be able to complete the projection operation over the H. We assume that we are given a fixed set of possible selection functions, HS, and a set of L learning algorithms, {Al}Ll=1, where A : D \u2192 HP generates a predictor given a training set D. In practice, these algorithms are generated by varying the complexities of the algorithm, e.g., depths in a decision tree predictor.\nGiven a fixed selection function hS and current predictions y\u0302, we can build a dataset appropriate for training weak predictors hP as follows. In order to minimize the projection error in (Eq. 7) for a predictor h of the form in (Eq. 12), it can be shown that this reduces to finding the prediction function hP that minimizes\narg min hP\u2208HP\nEX  \u2211 j\u2208hS(x,y\u0302) \u2225\u2225\u2207(x)j \u2212 hP(xj , y\u0302N(j))\u2225\u22252  , (14)\nwhere\n\u2207(x)j = \u2202l(f(x))\n\u2202f(x)j , (15)\nthe gradient of the loss with respect to the partial structured prediction y\u0302j . This optimization problem is equivalent to minimizing weighted least squares error over the dataset\nD = \u22c3 x \u22c3 j\u2208hS(x,y\u0302) {(\u03c8j ,\u2207(x)j)}, (16)\n= gradient(f, hS), (17)\nwhere \u03c8j = \u03c8(xj , y\u0302N(j)) is the feature descriptor for the given structural node, and \u2207(x)j is its target. In order to model contextual information, \u03c8 is drawn from both the raw features xj for the given element and the previous locally neighboring predictions y\u0302N(j).\nAlgorithm 1 summarizes the StructuredSpeedBoost algorithm for anytime structured prediction. It enumerates the candidate selection functions, hS, creates the training dataset defined by (Eq. 17), and then generates a candidate prediction function hP using each weak learning algorithm. For all the pairs of candidates, it uses the SpeedBoost criteria to select the most cost efficient pair, and then repeats.\nAlgorithm 1 StructuredSpeedBoost\nGiven: objective R, cost function c, set of selection functions HS, set of L learning algorithms {Al}Ll=1, number of iterations T , initial function f0. for t = 1, . . . , T do H\u2217 = \u2205 for hS \u2208 HS do\nCreate dataset D = gradient(ft\u22121, hS) (Eq. 17) for A \u2208 {A1, . . . ,AL} do\nTrain hP = A(D) Define h from hS and hP (Eq. 12) H\u2217 = H\u2217 \u222a {h}\nend for end for ht, \u03b1t = arg maxh\u2208H\u2217,\u03b1\u2208R R[ft\u22121]\u2212R[ft\u22121+\u03b1h] c(h)\nft = ft\u22121 + \u03b1tht end for"}, {"heading": "3.3 Handling Limited Training Data", "text": "In the presence of limited training data, training the prediction function hP using previous predictions y\u0302 can lead to overfitting. In practice, this can be alleviated by adapting the concept of stacking [28], which has demonstrated to be useful in other structured prediction work [3, 19]. Conceptually, the idea is that we do not want to use the same f we are currently learning to generate y\u0302 for use in the next boosting iteration. Concretely, we can instead split our entire training set \u2126 into two disjoint subsets, \u2126 = A\u222aB, A\u2229B = \u2205. At training time, we learn three separate structured predictors f, fA, fB over datasets \u2126, A,B, respectively. Let hP, h A P , h B P be the prediction functions for the structured predictors f, f\nA, fB , respectively. When training hP over x \u2208 \u2126, the predictions y\u0302 are generated from held-out predictions: for x \u2208 A, y\u0302 = fB(x), and for x \u2208 B, y\u0302 = fA(x). Now we need to train fA and fB in the same hold-out manner: when training hAP over x \u2208 A, y\u0302 = fB(x), and when training hBP over x \u2208 B, y\u0302 = fA(x). This interleaving process is solely done at training time to induce less-overfit predictions y\u0302 when training structured predictor f . Since f is trained over all the training data, we use solely its prediction at test-time and discard fA and fB . In practice, we follow this procedure using 10 folds instead of just two."}, {"heading": "4 Anytime Scene Understanding", "text": ""}, {"heading": "4.1 Background", "text": "In addition to part-of-speech tagging in natural language processing, scene understanding in computer vision is another important and challenging structured prediction problem. The de facto approach to this problem is with random field based models [15, 11, 17], where the random variables in the graph represent the\nobject category for a region/patch in the image. While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.\nInspired by the hierarchical representation used in the state-of-the-art scene understanding technique from Munoz et al. [19], we apply StructuredSpeedBoost to the scene understanding problem by reasoning over differently sized regions in the scene. In the following, we briefly review the hierarchical inference machine (HIM) approach from [19] and then describe how we can perform an anytime prediction whose structure is similar in spirit."}, {"heading": "4.2 Hierarchical Inference Machines", "text": "HIM parses the scene using a hierarchy of segmentations, as illustrated in Fig. 1. By incorporating multiple different segmentations, this representation addresses the problem of scale ambiguity in images. Instead of performing (approximate) inference on a large random field defined over the regions, inference is broken down into a sequence of predictions. As illustrated in Fig. 1, a predictor f is associated with each level in the hierarchy that predicts the probability distribution of classes/objects contained within each region. These predictions are then used by the subsequent predictor in the next level (in addition to features derived from the image statistics) to make refined predictions on the finer regions; and the process iterates. By passing class distributions between predictors, contextual information is modeled even though the segmentation at any particular level may be incorrect. We note that while Fig. 1 illustrates a topdown sequence over the hierarchy, in practice, the authors iterate up and down the hierarchy which we also do in our comparison experiments."}, {"heading": "4.3 Speedy Inference Machines", "text": "While HIM decomposes the structured prediction problem into an efficient sequence of predictions, it is not readily suited for an anytime prediction. First, the final predictions are generated when the procedure terminates at the leaf nodes in the hierarchy. Hence, interrupting the procedure before then would\nresult in final predictions over coarse regions that may severely undersegment the scene. Second, the amount of computation time at each step of the procedure is invariant to the current performance. Because the structure of the sequence is predefined, the inference procedure will predict multiple times on a region as it traverses over the hierarchy, even though there may be no room for improvement. Third, the input to each predictor in the sequence is a fixed feature descriptor for the region. Because these input descriptors must be precomputed for all regions in the hierarchy before the inference process begins, there is a fixed initial computational cost. In the following, we describe how StructuredSpeedBoost addresses these three problems three problems for anytime scene understanding."}, {"heading": "4.3.1 Interruptible Prediction", "text": "In order to address the first issue, we learn an additive predictor f which predicts a per-pixel classification for the entire image at once. In contrast to HIM whose multiple predictors\u2019 losses are measured over regions, we train a single predictor whose loss is measured over pixels. Concretely, given per-pixel ground truth distributions pj \u2208 RK , we wish to optimize per-pixel, cross-entropy risk for all pixels in the image\nR[f ] = EX \u2212\u2211 j \u2211 k pjk log q(f(x))jk  , (18) where\nq(y)jk = exp(yjk)\u2211 k\u2032 exp(yjk\u2032) , (19)\ni.e., the probability of the k\u2019th class for the j\u2019th pixel. Using (Eq. 12), the probability distribution associated with each pixel is then dependent on 1) the pixels to update, selected by hS, and 2) the value of the predictor hP evaluated on those respective pixels. The definition of these functions are defined in the following subsections."}, {"heading": "4.3.2 Structure Selection and Prediction", "text": "In order to account for scale ambiguity and structure in the scene, we can similarly integrate multiple regions into our predictor. By using a hierarchical segmentation of the scene that produces many segments/regions, we can consider each resulting region or segment of pixels S in the hierarchy as one possible set of outputs to update. Intuitively, there is no need to update regions of the image where the predictions are correct at the current inference step. Hence, we want to update the portion of the scene where the predictions are uncertain, i.e., have high entropy H. To achieve this, we use a selector function that selects\nregions that have high average per-pixel entropies in the current predictions,\nhS(x, y\u0302) = S \u2223\u2223\u2223\u2223\u2223\u2223 1|S| \u2211 j\u2208S H(q(y\u0302)j) > \u03b8  , (20) for some fixed threshold \u03b8. In practice, the set of predictors HS used at training time is created from a diverse set of thresholds \u03b8.\nAdditionally, we assume that the features \u03c8j used for each pixel in a given selected region are drawn from the entire region, so that if a given scale is selected features corresponding to that scale are used to update the selected pixels. For a given segment S, call this feature vector \u03c8S .\nGiven the above selector function, we use (Eq. 14) to find the next best predictor function, as in Algorithm 1, optimizing\nh\u2217P = arg min hP \u2211 S\u2208hS(x,y\u0302) \u2211 j\u2208S \u2016\u2207(x)j \u2212 hP(\u03c8S)\u20162. (21)\nBecause all pixels in a given region use the same feature vector, this reduces to the weighted least squares problem:\nh\u2217P = arg min hP \u2211 S\u2208hS(x,y\u0302) |S|\u2016\u2207S \u2212 hP(\u03c8S)\u20162. (22)\nwhere \u2207S = Ej\u2208S [\u2207(x)j ] = Ej\u2208S [pj \u2212 q(y\u0302)j ]. In words, we find a vector-valued regressor hP with minimal weighted least squares error between the difference in ground truth and predicted per-pixel distributions, averaged over each selected region/segment, and weighted by the size of the selected region. This is an intuitive update that places large weight to updating large regions."}, {"heading": "4.3.3 Dynamic Feature Computation", "text": "In the scene understanding problem, a significant computational cost during inference is often feature descriptor computation. To this end, we utilize the SpeedBoost cost model (Eq. 13) to automatically select the most computationally efficient features.\nThe features used in this application, drawn from previous work [11, 16] and detailed in the following section, are computed as follows. First, a set of base feature descriptors are computed from the input image data. In many applications it is useful to quantize these base feature descriptors and pool them together to form a set of derived features [2]. We follow the soft vector quantization approach in [2] to form a quantized code vector by computing distances to multiple cluster centers in a dictionary.\nThis computation incurs a fixed cost for 1) each group of features with a common base feature, and 2) an additional, smaller fixed cost for each actual feature used. In order to account for these costs, we use an additive model similar to Xu et al. [29]. Formally, let \u03c6 \u2208 \u03a6 be the set of features and \u03b3 \u2208 \u0393 be the set of feature groups, and c\u03c6 and c\u03b3 be the cost for computing derived feature \u03c6 and the base feature for group \u03b3, respectively. Let \u03a6(f) be the set of features used by predictor f and \u0393(f) the set of its used groups. Given a current predictor ft\u22121, its group and derived feature costs are then just the costs of any new group and derived features and have not previously been computed:\nc\u0393(hP) = \u2211\n\u03b3\u2208\u0393(hP)\\\u0393(ft\u22121)\nc\u03b3 , (23)\nc\u03a6(hP) = \u2211\n\u03c6\u2208\u03a6(hP)\\\u03a6(ft\u22121)\nc\u03c6. (24)\nThe total cost model in (Eq. 13) can then be derived using the sum of the feature costs and group costs as\nc(h) = c(hs) + c(hP) (25)\n= S + P + c\u0393(hP) + c\u03a6(hP), (26)\nwhere S and P are small fixed costs for evaluating a selection and prediction function, respectively.\nIn order to generate hP with a variety of costs, we use a modified regression tree that penalizes each split based on its potential cost, as in [29]. This approach augments the least-squares regression tree impurity function with a cost\nregularizer:\nED [ wD\u2016yD \u2212 hP(xD)\u20162 ] + \u03bb (c\u0393(hP) + c\u03a6(hP)) , (27)\nwhere \u03bb regularizes the cost. In addition to (Eq. 25), training regression trees with different values of \u03bb, enables StructuredSpeedBoost to automatically select the most cost-efficient predictor."}, {"heading": "5 Experimental Analysis", "text": ""}, {"heading": "5.1 Setup", "text": "We evaluate performance metrics between SIM and HIM on the 1) Stanford Background Dataset (SBD) [10], which contains 8 classes, and 2) Cambridge Video Dataset (CamVid) [1], which contains 11 classes; we follow the same training/testing evaluation procedures as originally described in the respective papers. As shown in Table 1, we note that HIM achieves state-of-the-art performance and these datasets and analyze the computational tradeoffs when compared with SIM. Since both methods operate over a region hierarchy of the scene, we use the same segmentations, features, and regression trees (weak predictors) for a fair comparison."}, {"heading": "5.1.1 Segmentations", "text": "We construct a 7-level segmentation hierarchy by recursively executing the graph-based segmentation algorithm (FH) [7] with parameters\n\u03c3 = 0.25, c = 102 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].\nThese values were qualitatively chosen to generate regions at different resolutions."}, {"heading": "5.1.2 Features", "text": "A region\u2019s feature descriptor is composed of 5 feature groups (\u0393): 1) region boundary shape/geometry/location (SHAPE) [11], 2) texture (TXT), 3) local binary patterns (LBP), 4) SIFT over intensity (I-SIFT), 5) SIFT separately over colors R, G, and B (C-SIFT). The last 4 are derived from per-pixel descriptors for which we use the publicly available implementation from [16].\nComputations for segmentation and features are shown in Table 2; all times were computed on an Intel i7-2960XM processor. The SHAPE descriptor is computed solely from the segmentation boundaries and is efficient to compute. The remaining 4 feature group computations are broken down into the per-pixel descriptor (base) and the average-pooled vector quantized codes (derived), where each of the 4 groups are quantized separately with a dictionary\nsize of 150 elements/centers using k-means. For a given pixel descriptor, \u03c5, its code assignment to cluster center, \u00b5i, is derived from its squared L2 distance di(\u03c5) = \u2016\u03c5 \u2212 \u00b5i\u201622. Using the soft code assignment from [2], the code is defined\nas max(0, zi(\u03c5)), where\nzi(\u03c5) = Ej [dj(\u03c5)]\u2212 di(\u03c5) (28) = Ej [\u2016\u00b5j\u20162]\u2212 2\u3008Ej [\u00b5j ], \u03c5\u3009 \u2212 (\u2016\u00b5i\u20162 \u2212 2\u3008\u00b5i, \u03c5\u3009). (29)\nNote that the expectations are indepndent from the query descriptor v, hence the i\u2019th code can be computed independently and enables selective computation for the region. The resulting quantized pixel codes are then averaged within each region. Thus, the costs to use these derived features are dependent if the pixel descriptor has already been computed or not. For example, when the weak learner first uses codes from the I-SIFT group, the cost incurred is the time to compute the I-SIFT pixel descriptor plus the time to compute distances to each specified center."}, {"heading": "5.2 Analysis", "text": "In Fig. 4 we show which cluster centers, from each of the four groups, are being selected by SIM as the inference time increases. We note that efficient SHAPE descriptor is chosen on the first iteration, followed by the next cheapest de-\nscriptors TXT and I-SIFT. Although LBP is cheaper than C-SIFT, the algorithm ignored LBP because it did not improve prediction wrt cost.\nIn Fig. 2, we compare the classification performance of SIM and several other algorithms with respect to inference time. We consider HIM as well as two variants which use a limited set of the 4 feature groups (only TXT and TXT & I-SIFT); these SIM and HIM models were executed on the same computer. We also compare to the reported performances of other techniques and stress that these timings are reported from different computing configurations. The single anytime predictor generated by our anytime structured prediction approach is competitive with all of the specially trained, standalone models without requiring any of the manual analysis necessary to create the different fixed models.\nIn Fig. 3, we show the progress of the SIM algorithm as it processes a scene from each of the datasets. Over time, we see the different structural nodes (regions) selected by the algorithm as well as improving classification."}, {"heading": "6 Conclusion", "text": "We proposed a technique for structured prediction with anytime properties. Our approach is based under the boosting framework that automatically incorporates new learners to our predictor that best increases performance with respect to efficiency in terms of both feature and inference computation times. We demonstrated the efficacy of our approach on the challenging task of scene\nunderstanding in computer vision and achieved state-of-the-art performance classifications with improved efficiency over previous work."}], "references": [{"title": "Segmentation and recognition using structure from motion point clouds", "author": ["Gabriel J. Brostow", "Jamie Shotton", "Julien Fauqueur", "Roberto Cipolla"], "venue": "In ECCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Honglak Lee", "Andrew Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Stacked sequential learning", "author": ["William W. Cohen", "Vitor Carvalho"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Search-based structured prediction. MLJ", "author": ["Hal Daume III", "John Langford", "Daniel Marcu"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "On-line semantic perception using uncertainty", "author": ["Roderick de Nijs", "Sebastian Ramos", "Gemma Roig", "Xavier Boix", "Luc Van Gool", "Kolja Kuhnlenz"], "venue": "In IROS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Learning hierarchical features for scene labeling", "author": ["Clement Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "In T-PAMI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Efficient graph-based image segmentation", "author": ["Pedro F. Felzenszwalb", "Daniel P. Huttenlocher"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Active classification based on value of classifier", "author": ["Tianshi Gao", "Daphne Koller"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["Stephen Gould", "Richard Fulton", "Daphne Koller"], "venue": "In ICCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Multi-class segmentation with relative location", "author": ["Stephen Gould", "Jim Rodgers", "David Cohen", "Gal Elidan", "Daphne Koller"], "venue": "prior. IJCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["Alexander Grubb", "J. Andrew Bagnell"], "venue": "In AISTATS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learned prioritization for trading off accuracy and speed", "author": ["Jiarong Jiang", "Adam Teichert", "Hal Daume III", "Jason Eisner"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Timely object recognition", "author": ["Sergey Karayev", "Tobias Baumgartner", "Mario Fritz", "Trevor Darrell"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Discriminative random fields", "author": ["Sanjiv Kumar", "Martial Hebert"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Global Structured Models towards Scene Understanding", "author": ["Lubor Ladicky"], "venue": "PhD thesis,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "What, where & how many? combining object detectors and crfs", "author": ["Lubor Ladicky", "Paul Sturgess", "Karteek Alahari", "Chris Russell", "Philip H.S. Torr"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classifiers", "author": ["L. Mason", "J. Baxter", "P.L. Bartlett", "M. Frean"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Stacked hierarchical labeling", "author": ["Daniel Munoz", "J. Andrew Bagnell", "Martial Hebert"], "venue": "In ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["S. Ross", "D. Munoz", "M. Hebert", "J.A. Bagnell"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Combining appearance and structure from motion features for road scene understanding", "author": ["Paul Sturgess", "Karteek Alahari", "Lubor Ladicky", "Philip H.S. Torr"], "venue": "In BMVC,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Scalable cascade inference for semantic image segmentation", "author": ["Paul Sturgess", "Lubor Ladicky", "Nigel Crook", "Philip H.S. Torr"], "venue": "In BMVC,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Superparsing: Scalable nonparametric image parsing with superpixels", "author": ["Joseph Tighe", "Svetlana Lazebnik"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Auto-context and its application to high-level vision tasks and 3d brain image segmentation", "author": ["Zhuowen Tu", "Xiang Bai"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Robust real-time face detection", "author": ["Paul A. Viola", "Michael J. Jones"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Structured prediction cascades", "author": ["David Weiss", "Ben Taskar"], "venue": "In AISTATS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Stacked generalization", "author": ["David H. Wolpert"], "venue": "Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1992}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Zhixiang Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "1 Related Work A canonical approach for incorporating computation time during learning is a cascade of feed-forward modules, where each module becomes more sophisticated but also more computationally expensive the further it is down the cascade [26].", "startOffset": 245, "endOffset": 249}, {"referenceID": 8, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 11, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 28, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 13, "context": "Recent works [9, 12, 29, 14] have investigated techniques for learning locally independent predictors which balance feature computation time and inference time during learning; however, they are not immediately amenable to the structured prediction scenario.", "startOffset": 13, "endOffset": 28}, {"referenceID": 12, "context": "[13] proposed a technique for reinforcement learning that incorporates a user specified speed/accuracy trade-off distribution, and Weiss and Taskar [27] proposed a cascaded analog for structured prediction where the solution space is is iteratively refined/pruned over time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[13] proposed a technique for reinforcement learning that incorporates a user specified speed/accuracy trade-off distribution, and Weiss and Taskar [27] proposed a cascaded analog for structured prediction where the solution space is is iteratively refined/pruned over time.", "startOffset": 148, "endOffset": 152}, {"referenceID": 22, "context": "Recent work in computer vision and robotics [23, 5] has similarly investigated techniques for making approximate inference in graphical models more efficient via a cascaded procedure that iteratively prunes subregions in the scene to analyze.", "startOffset": 44, "endOffset": 51}, {"referenceID": 4, "context": "Recent work in computer vision and robotics [23, 5] has similarly investigated techniques for making approximate inference in graphical models more efficient via a cascaded procedure that iteratively prunes subregions in the scene to analyze.", "startOffset": 44, "endOffset": 51}, {"referenceID": 2, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 3, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 24, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 20, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 19, "context": "One common approach to generating predictions on these structures is to use a policy-based or iterative decoding approach, instead of probabilistic inference over a graphical model [3, 4, 25, 21, 20].", "startOffset": 181, "endOffset": 199}, {"referenceID": 19, "context": "[20], this iterative decoding approach can is equivalent to message passing approaches used to solve graphical models, where each update encodes a single set of messages passed to one node in the graphical model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Minimizing this functional can be viewed as performing gradient descent in function space [18, 8].", "startOffset": 90, "endOffset": 97}, {"referenceID": 7, "context": "Minimizing this functional can be viewed as performing gradient descent in function space [18, 8].", "startOffset": 90, "endOffset": 97}, {"referenceID": 11, "context": "Extending this framework, Grubb and Bagnell [12] introduce an anytime prediction method that modifies the standard boosting criterion to automatically trade-off the loss of a weak predictor, h, with its cost c(h) \u2208 R.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Grubb and Bagnell prove that this SpeedBoost algorithm updates the resulting predictions at an increasing sequence of budgets that is competitive with any other sequence which uses the same weak predictors for a wide range of budgets [12].", "startOffset": 234, "endOffset": 238}, {"referenceID": 27, "context": "In practice, this can be alleviated by adapting the concept of stacking [28], which has demonstrated to be useful in other structured prediction work [3, 19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 2, "context": "In practice, this can be alleviated by adapting the concept of stacking [28], which has demonstrated to be useful in other structured prediction work [3, 19].", "startOffset": 150, "endOffset": 157}, {"referenceID": 18, "context": "In practice, this can be alleviated by adapting the concept of stacking [28], which has demonstrated to be useful in other structured prediction work [3, 19].", "startOffset": 150, "endOffset": 157}, {"referenceID": 14, "context": "The de facto approach to this problem is with random field based models [15, 11, 17], where the random variables in the graph represent the", "startOffset": 72, "endOffset": 84}, {"referenceID": 10, "context": "The de facto approach to this problem is with random field based models [15, 11, 17], where the random variables in the graph represent the", "startOffset": 72, "endOffset": 84}, {"referenceID": 16, "context": "The de facto approach to this problem is with random field based models [15, 11, 17], where the random variables in the graph represent the", "startOffset": 72, "endOffset": 84}, {"referenceID": 18, "context": "Figure 1: Hierarchical Inference Machines [19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 18, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 20, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 5, "context": "While random fields provide a clean interface between modeling and inference, recent works [25, 19, 21, 6] have demonstrated alternative approaches that achieve equivalent or improved performances with the additional benefit of a simple, efficient, and modular inference procedure.", "startOffset": 91, "endOffset": 106}, {"referenceID": 18, "context": "[19], we apply StructuredSpeedBoost to the scene understanding problem by reasoning over differently sized regions in the scene.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "In the following, we briefly review the hierarchical inference machine (HIM) approach from [19] and then describe how we can perform an anytime prediction whose structure is similar in spirit.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "8 HIM [19] 92.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "1 [6] 95.", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": "4 [21] - - - - - - - - - 78.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "5 HIM [19] 83.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "9 [5] 59 75 93 84 45 90 53 27 0 55 21 54.", "startOffset": 2, "endOffset": 5}, {"referenceID": 16, "context": "0 [17]\u2020 81.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "The features used in this application, drawn from previous work [11, 16] and detailed in the following section, are computed as follows.", "startOffset": 64, "endOffset": 72}, {"referenceID": 15, "context": "The features used in this application, drawn from previous work [11, 16] and detailed in the following section, are computed as follows.", "startOffset": 64, "endOffset": 72}, {"referenceID": 1, "context": "In many applications it is useful to quantize these base feature descriptors and pool them together to form a set of derived features [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "We follow the soft vector quantization approach in [2] to form a quantized code vector by computing distances to multiple cluster centers in a dictionary.", "startOffset": 51, "endOffset": 54}, {"referenceID": 28, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In order to generate hP with a variety of costs, we use a modified regression tree that penalizes each split based on its potential cost, as in [29].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "1 Setup We evaluate performance metrics between SIM and HIM on the 1) Stanford Background Dataset (SBD) [10], which contains 8 classes, and 2) Cambridge Video Dataset (CamVid) [1], which contains 11 classes; we follow the same training/testing evaluation procedures as originally described in the respective papers.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "1 Setup We evaluate performance metrics between SIM and HIM on the 1) Stanford Background Dataset (SBD) [10], which contains 8 classes, and 2) Cambridge Video Dataset (CamVid) [1], which contains 11 classes; we follow the same training/testing evaluation procedures as originally described in the respective papers.", "startOffset": 176, "endOffset": 179}, {"referenceID": 6, "context": "1 Segmentations We construct a 7-level segmentation hierarchy by recursively executing the graph-based segmentation algorithm (FH) [7] with parameters \u03c3 = 0.", "startOffset": 131, "endOffset": 134}, {"referenceID": 0, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 1, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 4, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 9, "context": "25, c = 10 \u00d7 [1, 2, 5, 10, 50, 200, 500], k = [30, 50, 50, 100, 100, 200, 300].", "startOffset": 13, "endOffset": 40}, {"referenceID": 10, "context": "2 Features A region\u2019s feature descriptor is composed of 5 feature groups (\u0393): 1) region boundary shape/geometry/location (SHAPE) [11], 2) texture (TXT), 3) local binary patterns (LBP), 4) SIFT over intensity (I-SIFT), 5) SIFT separately over colors R, G, and B (C-SIFT).", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "The last 4 are derived from per-pixel descriptors for which we use the publicly available implementation from [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "Using the soft code assignment from [2], the code is defined", "startOffset": 36, "endOffset": 39}], "year": 2013, "abstractText": "Structured prediction plays a central role in machine learning applications from computational biology to computer vision. These models require significantly more computation than unstructured models, and, in many applications, algorithms may need to make predictions within a computational budget or in an anytime fashion. In this work we propose an anytime technique for learning structured prediction that, at training time, incorporates both structural elements and feature computation trade-offs that affect test-time inference. We apply our technique to the challenging problem of scene understanding in computer vision and demonstrate efficient and anytime predictions that gradually improve towards state-of-the-art classification performance as the allotted time increases.", "creator": "LaTeX with hyperref package"}}}