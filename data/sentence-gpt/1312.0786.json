{"id": "1312.0786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2013", "title": "Image Representation Learning Using Graph Regularized Auto-Encoders", "abstract": "We consider the problem of image representation for the tasks of unsupervised learning and semi-supervised learning. In those learning tasks, the raw image vectors may not provide enough representation for their intrinsic structures due to their highly dense feature space. To overcome this problem, the raw image vectors should be mapped to a proper representation space which can capture the latent structure of the original data and represent the data explicitly for further learning tasks such as clustering, clustering, network building, and other tasks.\n\n\n\nThe basic goal of this approach is to minimize any problem with image representations for the task. The data should be encoded as either 'image representation' or 'sustaining', with each object represented as a bit of an array of blocks. The output should include the image vector and an input vector (an array of blocks representing all elements), representing the image image of the image as a binary representation.\nAn image representation with an image vector containing an image vector can be scaled to 1 \u00d7 25 (or as much as 10), with all inputs representing the image as an array of blocks representing all elements, while only an image vector with the input vector (the input vector of a block representing all elements) can be scaled to 1 \u00d7 25 (or as much as 10).\nThe first stage of our approach is to store a single image vector in a list, where all nodes are equal or equal (this may be the simplest way of storing the image vectors).\nTo represent the resulting data, an image vector must be contained within the input vector. This representation must contain one of three blocks. The final block of the image vector must contain two blocks.\nIn order to illustrate the importance of this approach, we created the image vector as a binary representation of all the images associated with the image vector. This image vector must contain three blocks. The final block of the image vector must contain two blocks. The final block of the image vector must contain two blocks. The final block of the image vector must contain two blocks.\nAfter a single image vector can be encoded as a bit of an array, the first block of the image vector must contain the entire image vector of the input vector. The second block of the image vector must contain two blocks. This output should be as large as 8.5 MB.\nThe input vector must contain two blocks. The first block of the image vector must contain three blocks. The second block of the image vector must contain two blocks. The second block of the image vector must contain two blocks.", "histories": [["v1", "Tue, 3 Dec 2013 11:59:57 GMT  (17kb)", "https://arxiv.org/abs/1312.0786v1", "9pages"], ["v2", "Wed, 19 Feb 2014 11:13:57 GMT  (18kb)", "http://arxiv.org/abs/1312.0786v2", "9pages"]], "COMMENTS": "9pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yiyi liao", "yue wang", "yong liu"], "accepted": false, "id": "1312.0786"}, "pdf": {"name": "1312.0786.pdf", "metadata": {"source": "CRF", "title": "Image Representation Learning Using Graph Regularized Auto-Encoders", "authors": ["Yiyi Liao", "Yue Wang", "Yong Liu"], "emails": ["yongliu@iipc.zju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n07 86\nv2 [\ncs .L\nG ]\n1 9\nFe b"}, {"heading": "1 Introduction", "text": "Although the dense original image can provide intuitive visual representation, it is well known that this representation may cover the hidden semantic patterns which need to be recognized by those image based learning tasks. On the other side, the performance of machine learning methods is also strongly dependent on the corresponding data representation on which they are applied. Thus image representation becomes a fundamental problem in visual analysis. Given an image data matrix X \u2208 Rm\u00d7n, each column of X corresponding to an image, the image representation is to find a representation function H = f(X)(H \u2208 Rl\u00d7n), which can extract useful information from X . And each column vector of H is the representation of an image in this concept space.\nAt the perspective of dimension reduction, the learned representation should have a lower dimension than the original one, i.e. l < m, and express the property of data better at the same time. The former is directive, while the later, usually be measured by the performance of clustering H . In this paper, we aim on this dimension reduction problem. One of the usual frameworks to model this problem is an optimization problem minimizing a cost shown as\nC = \u03a6(X,H) +\u03a8(H) (1)\nwhere the first term measures the approximation of H to X , while the second term, constrains the representation space.\nIn this paper, we propose an implementation of (1) based on deep learning and manifold, called graph regularized auto-encoder (GAE). The choice of \u03a6 is graph regularizer, which constrains the H to have the similar local geometry of original data space X . This is motivated by the property that a manifold resembles Euclidean space near each point (Wiki). Regard X as a manifold, then a neighborhood of each x has Euclidean property, which we want to be kept in H . However, whether this can be achieved depends on the choice of f , which maps X to H . It should have enough expressive power to map the original space to the constrained representation space. So we choose deep\nnetwork to achieve better performance beyond the existing many interesting linear functions with its nonlinearity. It is also expected that many recent successes on deep learning based approaches in supervised tasks [9, 22] can be extended to the context of unsupervised ones.\nThe remainder of this paper is organized as follows: In section 2, we will give a brief review of auto-encoder based representation learning and the related works; Section 3 will introduce our graph regularized auto-encoder for image representation learning tasks including both unsupervised conditions and semi-supervised conditions. Extensive experimental results on clustering are presented in Section 4. Finally, we provide a conclusion and future works in Section 5."}, {"heading": "2 Background", "text": "Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network. Given a data set X = {x1, ..., xn} \u2208 Rm\u00d7n, each column of X is a sample vector. H \u2208 Rl\u00d7n is a feature representation of the original data set X by an encoder function H = f\u03b8(X). Normally, l < m, and H can be regarded as a low dimensional representation (or subspace) of the original data set X . And another feature mapping function, which is called decoder, maps from feature space back into input space, thus producing a reconstruction Q = q\u03b8(H). A reconstruction error function L(X,Q), which is also called loss function, is defined, and the set of parameters \u03b8 of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input, i.e. attempting to incur the lowest possible reconstruction error of L(X,Q) 1.\nThe most commonly used forms for the decoder and encoder are affine mappings, optimally followed by a non-linearity as:\nH = f\u03b8(X) = sf (bH +WHX) (2)\nQ = q\u03b8(H) = sq(bQ +WQX) (3)\nsf and sq are the encoder and decoder activation functions, e.g. non-linear functions of sigmoid and hyperbolic tangent or linear identify function etc. Then the set of parameters is \u03b8 = {WH , bH ,WQ, bQ}, and the problem is formally presented as follows:\n\u03b8\u0302 = arg min\u03b8L(X, q\u03b8(f\u03b8(X))) (4)\nFormula (4) can be easily solved by the stochastic gradient descent based backpropagation approaches. The auto-encoders are also able to support multiple layers, e.g in Hinton\u2019s work [7], which train the encoder network (from X to Hi, i is the number of the layer) one-by-one using the Restricted Boltzamann Machines and the decoder layers of the network are formed by the inverse of the trained encoder layers, such as WH = (WQ)T in one layer auto-encoder.\nThere are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18]. It is pointed out that the sparse penalty used in sparse auto-encoder will tend to make only few input configurations can have a low reconstruction error [16], which may hurt the numerical optimization of parameters. The other two kinds of regularized auto-encoders are regarded to make the representation as insensitive as possible with respect to changes in input, which is commonly useful in supervised learning condition, however, it may not provide positive impacts in unsupervised and semi-supervised conditions.\nPrevious studies have also shown that the locally invariant idea [6] will play an important role in the image representation, especially for those tasks of unsupervised learning and semi-supervised learning. There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings. However, these methods are all linear, which may not provide enough expressive power to find a representation space that can preserve the local geometry.\nThere are some similar works on graph regularized neural network architecture. [14] proposed a graph regularizer that constrains the similarity between consecutive frames, which shows the human knowledge can be applied in this term. In [13], a graph constrains the data points belonging to the\n1Normally, the loss function is defined as the Euclidian distance of the two data set, that is \u2016 X \u2212Q \u20162.\nsame label is proposed. In this work, the deep network is trained, and then minimizes only the graph regularizer using this network. Both these works use the correct graph regularizer since it is built using the correct supervised or human knowledge information.\nAnother work similar to us is [6], in which a convolutional neural network is applied to minimize a graph regularizer. In our work, we minimize a combined cost (1) using a fully connected network. The reason is that, the graph in the unsupervised tasks is not completely correct (pair of data point belonging to the different label actually), an introduction of first term in (1) can act as a regularizer avoiding fitting of wrong information. Besides, we do not introduce a mechanism that pulls apart discriminative pairs."}, {"heading": "3 Graph Regularized Auto-Encoder", "text": "The problem is to design the second term \u03a8(H) in (1) to constrain the representation space. In this section, we introduce our geometrical regularization that implement locally invariance in unsupervised and semi-supervised learning."}, {"heading": "3.1 Single Layer Auto-Encoder Regularized with Graph", "text": "In our Graph regularized Auto-Encoder (GAE), both the decoder and the encoder use sigmoid as their activation functions. Denote sigmoid function as S(x) = 1/(1 + e\u2212x). Then the encoder and decoder can be presented as follow:\nH = f\u03b8(X) = S(WHX + bH) (5)\nQ = q\u03b8(H) = S(WQH + bQ) (6)\nAs the representation should discover the latent structure of the original data, the geometrical structure of the data will be the ideal latent structure in representation learning especially in unsupervised or semi-supervised learning. A reasonable assumption is that if two data points xi, xj are close in the intrinsic geometry of the data distribution, then their corresponding representations, hi, hj , should be also close to each other. This assumption is usually referred to as local invariance assumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.\nIn manifold learning, the local property of the data space are preserved in the reduced representation. But most algorithms considering this problem is linear. In our GAE, we introduce a locality preserved constraint to the nonlinear auto-encoder to better reflect its nature of manifold. Based on formula (4)-(6), we optimize the auto-encoder regularized with graph as follows\n\u03b8\u0302 = argmin(\u2016X \u2212Q\u20162 + \u03bbtr(HGHT )) (7)\nwhere \u03bb is a coefficient of the training algorithm, tr(HGHT ) is the term of graph regularizer, tr(\u00b7) denotes the trace of a matrix, and G is a graph coding the local property of the original data X . Denote vij as the weight for the locality between data sample xi and xj , and their corresponding representations are hi and hj . Based on the local invariance assumption, the regularization that requires the points in subspace keeping the same geometrical structure as the original data can be presented as the following weighted formula.\n\u2211\ni\n\u2211\nj\nvij\u2016hi \u2212 hj\u2016 2\n= \u2211\ni\nhTi \u2211\nj\nvijhi + \u2211\nj\nhTj \u2211\ni\nvijhj \u2212 2 \u2211\ni\n\u2211\nj\nhivijhj\n=tr(HD1H T ) + tr(HD2H T )\u2212 2tr(HVHT )\n=tr(HGHT )\n(8)\nwhere vij are entries of V , and V is the weight matrix, D1 nd D2 are diagonal form with D1,ii = \u2211\nj vij and D2,jj = \u2211\ni vij , and G = D1+D2\u2212 2V . With this constraint, the local property of the data in original space will be preserved after auto-encoder mapping. The matrix G has significant expressive power of the structure in the original data space. It is calculated from the weight matrix V , whose design will be introduced in section 3.3.\nFormula (7) can be solved by the Broyden-Fletcher-Goldfarb-Shanno (BFGS) training algorithm. In our GAE, the weight-tying constraint is not used, which means our GAE does not require WH = (WQ) T ."}, {"heading": "3.2 Multiple-layer Auto-Encoders Regularized with Graph", "text": "The auto-encoder was proposed in the context of the neural network, which is later applied to train the deep structure of networks to obtain better expressive power. In data representation, the idea of representing with multiple layers still works. Thus the representation of the original data can be presented with one layer of mapping, as well as multiple-layer mapping. We also implement the locally invariant constraint into the multiple layer auto-encoders by adding the graph regularized terms.\nAs training all the layers simultaneous in multiple-layer auto-encoders may be stacked, in our multiple-layer GAE, we train the multiple-layer GAE layer-by-layer. We use Hi to denote the data representation of the ith layer, and its corresponding decoder is denoted as Qi. The input data of the ith layer is the data representation of i\u2212 1 th layer 2. That is:\nHi = f\u03b8i(Hi\u22121) = S(WHiHi\u22121 + bHi) (9)\nQi = q\u03b8i(Hi) = S(WQiHi + bQi) (10)\nHere, \u03b8i = {WHi ,WQi , bHi , bQi}, and then the objective function of the ith layer of the GAE is,\n\u03b8\u0302i = argmin(\u2016Hi\u22121 \u2212Qi\u2016 2 + \u03bbtr(HiGi\u22121H T i )) (11)\nWhere Gi\u22121 is the graph regularizer generated from data Hi\u22121.\nThen the Multiple-Layer GAE (ML-GAE) algorithm can be given as follow:\nAlgorithm 1: ML-GAE Input: X , total layer number j Output: WH1 , bH1 , ...,WHj , bHj\n1 for i = 1 to j do 2 Solve \u03b8i by formula (11), and obtain the ith layer of data representation Hi; 3 end"}, {"heading": "3.3 Graph Regularizer Design", "text": "As mentioned above, the performance of date representation regularized with graph mainly lies on the design of the weight matrix V since it encodes the local invariance information of the data space. In this section, we will focus on the weight matrix design of supervised learning and unsupervised learning in the context of data representation with auto-encoders."}, {"heading": "3.3.1 Unsupervised Learning", "text": "In unsupervised learning, the label for the data is unavailable. We can only obtain the structure of the data from the local property of the data samples. There are three kinds of weights employed in our GAE, which are introduced as follows,\n\u2022 KNN-graph: It first constructs the k-nearest neighbor sets for each data sample. If xi lies in xj\u2019s k-nearest neighbor set, the weights vij is set as the distance between these two data samples, that is exp(\u2212\u2016xi \u2212 xj\u2016), otherwise vij is set to zero.\n\u2022 \u01eb-graph: It first constructs the \u01eb-neighbor sets for each data sample, the data sample xi\u2019s \u01eb-neighbor set contains all the data samples whose distances to xi are less than \u01eb. If xi lies in xj \u2019s \u01eb-nearest neighbor set, the weights vij is set as the distance between these two data samples, that is exp(\u2212\u2016xi \u2212 xj\u2016), otherwise vij is set to zero.\n2If i = 1, then the input data is the original data set X , and the training process is back to single layer GAE.\n\u2022 l1-graph: The weight setting is considered to resolve the following optimization problem,\nvij = argmin \u2016xi \u2212 \u2211\nj=1...n,j 6=i\nvijxj\u2016+ \u03bb \u2211\nj=1...n,j 6=i\n|vij |"}, {"heading": "3.3.2 Semi-supervised Learning", "text": "In semi-supervised learning, the data labels are partially available, which brings some ground truth information to the estimation of the data representation. In our GAE, the graph regularizer design for semi-supervised learning task is similar to the unsupervised learning task. We first construct the k-nearest neighbor sets or \u01eb-neighbor sets for the whole data set, and set the weight vij to zero if xi and xj are not neighbors. For the condition that xi and xj are neighbors, the weights are calculated as follow:\nvij =\n{\nexp(\u2212\u2016xi \u2212 xj\u2016) either xi or xj is unlabeled 1 xi, xj have the same label 0 xi, xj have different labels\n(12)\nAs mentioned before, the weights in the graph are computed by exp(\u2212\u2016xi \u2212 xj\u2016), which is a value less than 1 but larger than 0. Since the labeled data will provide ground truth information, the weights of two samples with the same labels are directly set to 1. And the weights between two samples with different labels are directly set to 0.\nThe graph constraint constructed with formula (12) is called semi-graph regularizer. Apparently, the marginal value 0 and 1 give the most confident level of the similarity since their corresponding are labeled. With this semi-graph regularizer, both labeled and unlabeled data samples are regarded fairly."}, {"heading": "4 Experimental Results", "text": "In this section, comparison experiments are carried out to demonstrate the performance of our proposed method in tasks of both unsupervised learning and semi-supervised learning. To evaluate image representations learned by different methods quantitatively, the k-means clustering is applied to the representations learned by different methods. Two metrics, the normalized mutual information(MI) and accuracy(AC) are used to measure the clustering performance. For fair comparison, the dimension of learned representation through all algorithms are set to be the same.\nThe normalized mutual information(MI) is given in [4], which is a normalized measure to evaluate how similar two sets of clusters are. The accuracy (AC) is used to measure the percentage of correct labels compared to the ground truth label provided by the data set. Specifically, given a data sample xi with clustered label and ground truth label ci and gi, the accuracy is defined as\nAC =\n\u2211\ni \u03b4(gi,map(ci))\nn\nwhere n is total number of samples, \u03b4(a, b) is delta function, which outputs 1 when a = b and outputs 0 otherwise, map(c) is the permutation mapping function that maps each clustered label ci to the best label provided by the data set. This function is implemented using the code published by [4]. For the normalized mutual information, denote C and C\u2032 as the set of clusters obtained from the ground truth and our algorithm. We first compute the mutual information as follows,\nMI(C,C\u2032) = \u2211\nci\u2208C,c \u2032 j \u2208C\u2032\np(ci, c \u2032 j) log\np(ci, c \u2032 j)\np(ci)p(c\u2032j)\nwhere p(ci) and p(c\u2032j) are the probabilities that a sample selected from the data set that belong to cluster ci and c\u2032j , p(ci, c \u2032 j) is the probability that a sample selected from the data set that belong to both ci and c\u2032j . Then, the normalized mutual information can be computed as\nMI(C,C\u2032) = MI(C,C\u2032)\nmax(H(C), H(C\u2032))\nwhere H(C) and H(C\u2032) are the entropy of C and C\u2032. When MI = 1, the two clusters are identical. when MI = 0, the two clusters are independent.\nThe experimental results are all average value of multiple times of random experiment. We set that the reduced representation of different dimension reduction techniques share the same dimensions. The data sets employed for the experiments including: ORL[21], Yale and COIL20, whose statistics are shown in table 1."}, {"heading": "4.1 Variants Comparison", "text": "Fine-tuning[7] of a pre-trained deep network can sometimes improve the performance in many supervised learning tasks. However, in unsupervised learning, the weight matrix built on Euclidean distance may include some wrong information, i.e. samples with different labels may be connected. We can compute the error rate as the ratio of wrong connections and the total connections.\nWe construct a 2 layer auto-encoder, and implement layer-wise pre-training based on the method in section3.2. Then we fine-tune the deep auto-encoder with the single graph regularization, i.e. only the second term of (11). The input data is chosen from COIL20. It has 20 classes. In this experiment, we select 8 classes in random for comparison. So the unsupervised weight matrix is also kind of random based on the data set. Experiment result in table 2 shows that when the weight matrix is constructed with no error, the performance will be promoted with fine-tuning. However, when the weight matrix contains wrong connection, then the result turns out to be worse.\nWe also conduct an experiment that pre-train the GAE without reconstruction error but only with graph regularization which show in the last two rows in table 2. The result is interesting that the deep architecture even cannot learn a meaningful representation. It may give some insights on the reconstruction error term.\nAs a result, in the next two sections, we train our deep auto-encoders using layer-wise pre-training with both reconstruction error and graph regularization."}, {"heading": "4.2 Experiments in Unsupervised Learning", "text": "For unsupervised learning task, all samples have no labels. So they are directly fed into the algorithm for measure evaluation. The dimension of the reduced representation is set to the number of classes in the input data set. The methods used for comparison including:\n\u2022 k-means: this is the baseline method which simply performs clustering in the original feature space.\n\u2022 PCA: Principal Components Analysis. This is the classical linear technique for dimensional reduction.\n\u2022 GAE: Graph Auto-Encoder (2 layers). It is a contribution of this paper that introduces the graph constraint to the auto-encoder. In the experiments, our GAE employs the KNN graph. There are two coefficients, k for the number of neighbors in the KNN graph and \u03bb for the intensity of the graph regularizer. They are all selected by the grid based search.\n\u2022 SAE: Sparse+Auto-encoder(2 layers)[15]. The sparse constraint is equipped to the autoencoder, which is a very common constraint choice in the field of auto-encoder. The formula is given as follows\n\u03b8\u0302 = argmin \u2016X \u2212 X\u0302\u20162 + \u03b7 \u2211\nj\nKL(\u03c1|\u03c1j) (13)\nwhere \u03c1 is the user defined sparsity coefficient, \u03c1j is the average response of the jth hidden unit for the whole dataset. The penalty term can make the hidden response more sparse. The coefficient \u03b7 is also selected by grid based search.\n\u2022 GNMF: Graph regularized Nonnegative Matrix Factorization. It is proposed in [4]. It is a combination of nonnegative constraint and locally invariant constraint. The graph parameter settings are similar to the GAE, which employ the KNN graph and the coefficients k and \u03bb are selected by the optimal grid search.\nThe results for whole datasets are shown in table 3, 4, 5. To randomize the experiments, we carry out evaluation with different cluster classes. For each given number of the cluster classes, we random choose the cluster classes from the whole datasets for 5 times. One can see that graph regularized auto-encoder achieves the best performance. Although the GNMF and GAE are employed the same encodes on the locally invariant information, the auto-encoder which implement the nonlinear sigmoid scaling on the deep structure will performance better than the nonnegative matrix factorization based approach."}, {"heading": "4.3 Experiments in Semi-supervised Learning", "text": "For semi-supervised learning task, a small part of the samples are labeled. In this experiment, these labeled samples are selected in random. For COIL20, 10% samples are labeled in each class, so there are 7 labeled samples in each class. For ORL and Yale, 20% are labeled, then 2 samples are labeled in each class 3. Still referring to the unsupervised learning experiments, the dimension of the learned representation is equal to the number of classes in the data set. The comparison methods used in this experiments including:\n\u2022 CNMF: constrained NMF. It is proposed in [12]. In their framework, the samples with the same label are required to have the same representation in the reduced space. There is no user defined parameters either.\n\u2022 SGAE: Semi-Graph regularized Auto-Encoder (2 layers). It is a representation learning algorithm proposed in this paper consisting of the auto-encoder regularized by the semigraph regularizer presented in section 3.3.2. The parameters include the intensity of the graph constraint, \u03bb and the number of neighbors in the KNN graph, k. Similarly to the GAE in unsupervised learning experiment, these two parameters are selected by optimal grid search.\nThe clustering results on all classes of the datasets are shown in table 6, 7, 8. Similarly to the randomize experiment of the unsupervised learning, we also conduct the randomize experiment\n3Here one labeled sample is meaningless to both CNMF and SGAE, so we label 20% of the samples in each class.\nfor semi-supervised learning on different number of classes. The classes for the experiment are also randomly sampled from the whole datasets with 5 times, and the average clustering results are shown in rightmost column of the table. It can be found that the semi-graph regularized auto-encoder gives a significant improvement of performance compared to the constrained NMF. The reason may be that the CNMF only utilizes the labeled data while ignoring the geometric structure hidden in the unlabeled data. When it comes to the semi-graph regularized auto-encoder, all the information from labeled and unlabeled data are all considered. As we expected, semi-graph regularized auto-encoder achieves better performance compared to the unsupervised clustering results in table 3, 4, 5."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a novel graph regularized auto-encoder, which can learn a locally invariant representation of the images for both unsupervised and semi-supervised learning tasks. In unsupervised learning, our approach trains the image representation by an multiple-layer auto-encoder regularized with the graph, which encodes the locally neighborhood relationships of the original data. And in semi-supervised learning, the graph regularizer used in our auto-encoders is extended to semi-graph regularizer, which adds the penalty and reward obtained from the labeled data points to the locally neighborhood weight matrix. Experimental results on image clustering show our method provides better performance comparing with the stat-of-the-art approaches. The further work may focus on investigating the affections of the parameter settings in GAE and the impacts of the deep structure is also a possible future work."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "NIPS, pages 585\u2013591,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, 7:2399\u20132434,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1548\u20131560,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems 22, pages 646\u2013654.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR \u201906, pages 1735\u20131742, Washington, DC, USA,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, 313(5786):504 \u2013 507,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["G.E. Hinton", "R.S. Zemel"], "venue": "NIPS, pages 3\u201310,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1106\u20131114.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "ICML, pages 536\u2013543,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Constrained nonnegative matrix factorization for image representation", "author": ["H. Liu", "Z. Wu", "D. Cai", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1299\u20131311,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep supervised t-distributed embedding", "author": ["M.R. Min", "L. Maaten", "Z. Yuan", "A.J. Bonner", "Z. Zhang"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 791\u2013798,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 737\u2013744. ACM,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["C. Poultney", "S. Chopra", "Y.L. Cun"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y.-L. Boureau", "Y. LeCun"], "venue": "J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1185\u20131192. MIT Press, Cambridge, MA,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C.S. Poultney", "S. Chopra", "Y. LeCun"], "venue": "NIPS, pages 1137\u20131144,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "Proceedings of the 2011 European conference on Machine learning and knowledge discovery in databases - Volume Part II, ECML PKDD\u201911, pages 645\u2013660, Berlin, Heidelberg,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Contractive Auto-Encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Applications of Computer Vision, 1994., Proceedings of the Second IEEE Workshop on, pages 138\u2013142. IEEE,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Convolutional-recursive deep learning for 3d object classification", "author": ["R. Socher", "B. Huval", "B.P. Bath", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS, pages 665\u2013673,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "A connection between score matching and denoising autoencoders", "author": ["P. Vincent"], "venue": "Neural Comput., 23(7):1661\u20131674, July", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning, ICML \u201908, pages 1096\u20131103, New York, NY, USA,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res., 11:3371\u20133408, Dec.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "It is also expected that many recent successes on deep learning based approaches in supervised tasks [9, 22] can be extended to the context of unsupervised ones.", "startOffset": 101, "endOffset": 108}, {"referenceID": 21, "context": "It is also expected that many recent successes on deep learning based approaches in supervised tasks [9, 22] can be extended to the context of unsupervised ones.", "startOffset": 101, "endOffset": 108}, {"referenceID": 6, "context": "Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network.", "startOffset": 13, "endOffset": 22}, {"referenceID": 7, "context": "Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network.", "startOffset": 13, "endOffset": 22}, {"referenceID": 2, "context": "Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network.", "startOffset": 13, "endOffset": 22}, {"referenceID": 6, "context": "g in Hinton\u2019s work [7], which train the encoder network (from X to Hi, i is the number of the layer) one-by-one using the Restricted Boltzamann Machines and the decoder layers of the network are formed by the inverse of the trained encoder layers, such as WH = (WQ) in one layer auto-encoder.", "startOffset": 19, "endOffset": 22}, {"referenceID": 16, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 10, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 4, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 9, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 75, "endOffset": 90}, {"referenceID": 23, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 116, "endOffset": 128}, {"referenceID": 25, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 116, "endOffset": 128}, {"referenceID": 24, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 116, "endOffset": 128}, {"referenceID": 18, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 159, "endOffset": 167}, {"referenceID": 17, "context": "There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], denoising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": "It is pointed out that the sparse penalty used in sparse auto-encoder will tend to make only few input configurations can have a low reconstruction error [16], which may hurt the numerical optimization of parameters.", "startOffset": 154, "endOffset": 158}, {"referenceID": 5, "context": "Previous studies have also shown that the locally invariant idea [6] will play an important role in the image representation, especially for those tasks of unsupervised learning and semi-supervised learning.", "startOffset": 65, "endOffset": 68}, {"referenceID": 19, "context": "There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings.", "startOffset": 137, "endOffset": 140}, {"referenceID": 13, "context": "[14] proposed a graph regularizer that constrains the similarity between consecutive frames, which shows the human knowledge can be applied in this term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In [13], a graph constrains the data points belonging to the", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Another work similar to us is [6], in which a convolutional neural network is applied to minimize a graph regularizer.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "This assumption is usually referred to as local invariance assumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.", "startOffset": 70, "endOffset": 80}, {"referenceID": 19, "context": "This assumption is usually referred to as local invariance assumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.", "startOffset": 70, "endOffset": 80}, {"referenceID": 1, "context": "This assumption is usually referred to as local invariance assumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.", "startOffset": 70, "endOffset": 80}, {"referenceID": 3, "context": "The normalized mutual information(MI) is given in [4], which is a normalized measure to evaluate how similar two sets of clusters are.", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "This function is implemented using the code published by [4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 20, "context": "The data sets employed for the experiments including: ORL[21], Yale and COIL20, whose statistics are shown in table 1.", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "Fine-tuning[7] of a pre-trained deep network can sometimes improve the performance in many supervised learning tasks.", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "\u2022 SAE: Sparse+Auto-encoder(2 layers)[15].", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "It is proposed in [4].", "startOffset": 18, "endOffset": 21}, {"referenceID": 11, "context": "It is proposed in [12].", "startOffset": 18, "endOffset": 22}], "year": 2014, "abstractText": "It is an important task to learn a representation for images which has low dimension and preserve the valuable information in original space. At the perspective of manifold, this is conduct by using a series of local invariant mapping. Inspired by the recent successes of deep architectures, we propose a local invariant deep nonlinear mapping algorithm, called graph regularized auto-encoder (GAE). The local invariant is achieved using a graph regularizer, which preserves the local Euclidean property from original space to the representation space, while the deep nonlinear mapping is based on an unsupervised trained deep auto-encoder. This provides an alternative option to current deep representation learning techniques with its competitive performance compared to these methods, as well as existing local invariant methods.", "creator": "LaTeX with hyperref package"}}}