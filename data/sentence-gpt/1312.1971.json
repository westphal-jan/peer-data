{"id": "1312.1971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2013", "title": "Modeling Suspicious Email Detection using Enhanced Feature Selection", "abstract": "The paper presents a suspicious email detection model which incorporates enhanced feature selection. In the paper we proposed the use of feature selection strategies along with classification technique for terrorists email detection. The presented model focuses on the evaluation of machine learning algorithms such as decision tree (ID3), logistic regression, Na\\\"ive Bayes (NB), and Support Vector Machine (SVM) for detecting emails containing suspicious content. In the literature, various algorithms achieved good accuracy for the desired task. However, the results achieved by those algorithms can be further improved by using appropriate feature selection mechanisms. We have identified the use of a specific feature selection scheme that improves the performance of the existing algorithms. We are interested in investigating whether the current algorithm performs the best or best job with best performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 6 Dec 2013 19:25:33 GMT  (323kb)", "http://arxiv.org/abs/1312.1971v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sarwat nizamani", "nasrullah memon", "uffe kock wiil", "panagiotis karampelas"], "accepted": false, "id": "1312.1971"}, "pdf": {"name": "1312.1971.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["saniz@mmmi.sdu.dk).", "memon@mmmi.sdu.dk)", "ukwiil@mmmi.sdu.dk)", "pkarampelas@hauniv.us)."], "sections": [{"heading": null, "text": "model which incorporates enhanced feature selection. In the paper we proposed the use of feature selection strategies along with classification technique for terrorists email detection. The presented model focuses on the evaluation of machine learning algorithms such as decision tree (ID3), logistic regression, Na\u00efve Bayes (NB), and Support Vector Machine (SVM) for detecting emails containing suspicious content. In the literature, various algorithms achieved good accuracy for the desired task. However, the results achieved by those algorithms can be further improved by using appropriate feature selection mechanisms. We have identified the use of a specific feature selection scheme that improves the performance of the existing algorithms.\nIndex Terms\u2014Decision tree, feature selection, logistic\nregression, Naive Bayes, SVM.\nI. INTRODUCTION\nEmail is the most popular way of communication of this era. It provides an easy and reliable method of communication. Email messages can be sent to an individual or groups. A single email can spread among millions of people within few moments. Nowadays, most individuals even cannot imagine the life without email. For those reasons, email has become a widely used medium for communication of terrorists as well. A great number of researchers [1],[2],[3],[4]focused in the area of counterterrorism after the disastrous events of 9/11 trying to predict terrorist plans from suspicious communication. This also motivated us to contribute in this area.\nIn this paper, we have applied data mining techniques to detect suspicious emails, i.e., an email that alerts of upcoming terrorist events. We have applied decision tree (ID3) [5], Na\u00efve Bayes [6], logistic regression [7], and SVM [7] algorithms, emphasizing initially on feature space creation, then applying various feature selection techniques by selecting a subset of the original feature space.\nFeature selection involves the choice of a feature subset\nManuscript received on April 11, 2012.\nSarwat Nizamani is at Maersk McKinney Moller Institute, University\nof Southern, Denmark. (email: saniz@mmmi.sdu.dk).\nNasrullah Memon is at Maersk McKinney Moller Institute,\nUniversity of Southern, Denmark. (email: memon@mmmi.sdu.dk)\nUffe Kock Wiil is at Maersk McKinney Moller Institute, University\nof Southern, Denmark. (email: ukwiil@mmmi.sdu.dk)\nPanagiotis Karampelas is at Hellenic American University,\nManchester, NH, USA (email: pkarampelas@hauniv.us).\ndataset with various classifiers and various feature s election schemes. We also observed that for a specific classifier in choice of feature selection, an appropriate evaluator should be used with an appropriate search method. In the original feature space, we have used some keywords and some indicators. For example, if domain specific keywords are found with suspicious indicators in an email message, it is classified as suspicious, whereas the occurrence of domain specific keywords without the presence of suspicious indicators in an email, it is not classified as suspicious. With the selection of proper features, the accuracy of decision tree, SVM, Na\u00efve Bayes and logistic regression is improved. We suggest the use of a specific feature selection scheme with a particular evaluator and an appropriate search method. In the paper, we have conducted experiments on our dataset using state of art classifiers and supervised feature selection methods. The choice of feature selection involves the selection of an evaluator and a search method. For our experiments, we applied CfsSubsetEval, ChiSquare, InfoGain, GainRatio and ConsistencySubsetEval evaluators and BestFirst, GreedyStepWise and Ranker search methods. The results show that ConsistensySubsetEval with GreedyStepWise search methods improves the performance of three out of four classifiers.\nWe have developed an email dataset containing suspicious emails, because there is no benchmark dataset available in the domain. Some emails in the dataset are taken from some open emails released by press concerning Mumbai attack [9]. Few of the emails are real emails of 9/11 incident which are also used by authors [1]. We also added some dummy emails resembling to terrorist emails. The dataset consists of 45% suspicious emails and 55% non-suspicious emails. Some examples from the dataset are given in Appendix.\nThe paper is organized as follows: Section II describes related work, whereas Section III explains various classification algorithms used for experimentation. Section IV discusses problem statement while Section V elaborates the proposed methodology. Section VI illustrates our experimental results and Section VII concludes the paper with future work."}, {"heading": "II. RELATED WORK", "text": "The research in the area of email analysis usually focuses on two areas namely: email traffic analysis and email content analysis. A lot of research has been conducted for Email traffic analysis [10],[11]. An email traffic analysis system manipulates the traffic part of the email to investigate the unusual behavior\n[11] of suspicious individual. The traffic part of an email includes To, Carbon copy (Cc), Blind Carbon copy (BCc) and the Date fields. Email content analysis [11], [1], [22] on the other hand is the study of the unstructured part of the email such as the subject and body. Keila and Skillicorn [11] have investigated on the Enron [13]data set which contains email communications among employees of an organization who were involved in the collapse of the organization. The authors [1] have applied ID3 algorithm to detect suspicious emails by using keyword base approach and by applying rules. They have not used any information regarding the context of the identified keywords in the emails. S. Appavu & R. Rajaram [2] have applied association rule mining to detect suspicious emails with the additional benefits of classifying the (suspicious in terms of terror plots) emails further into specialized classes such as suspicious alert or suspicious info. This system decides whether the email can be classified as suspicious alert in the presence of suspicious keyword in the future tense otherwise only it is classified as suspicious info. The authors [14],[15] incorporated feature selection strategies along with classification systems. According to [15], by using feature selection methods one can improve the accuracy, applicability, and understandability of the learning process. Selvakuberan et al. [14]have applied filtered feature selection methods [16] on web page classification; according to their results the evaluator CfsSubsetEval yields better performance with search methods BestFirst, Ranker search, and Forward selection. Pineda-Bautista et al. [17] proposed a method for selecting the subset of features for each class in multi-class classification task. The classifiers that were used by the authors were Naive Baye's (NB) [6], k-Nearest Neighbors (k-NN) [17], C4.5 [19], and MultiLayer Perceptron (MLP). The authors trained the classifier for each class separately by using only the features of that particular class. Durant and Smith [20]have emphasized the use of a feature selection method for achieving accuracy of sentiment classification. They proposed to apply CfsSubsetEval with the BestFirst search method.\nThe ID3 (a type of decision tree) [21], algorithm is mostly used for email classification and content analysis systems [1],[2]. In the classification experiments for email spam filtering [21], decision tree classifiers outperform the other classifiers like SVM [23], neural networks [24], and others. SVM have also been applied for content extraction in the terrorism domain [25]. The method focuses on the context along with keywords. In general terms, features are collection of patterns on which the classification task is performed. Indeed, selecting an optimal set of features is generally difficult, both theoretically and empirically [25]. For the classification tasks, the proper strategy for feature selection is of utmost importance as emphasized in [27]. In this paper, we compare the evaluation of state of art decision tree, logistic regression, SVM, and Na\u00efve Bayes [6] algorithms first without explicitly selecting features and then with various feature selection approaches. Our experiments show that choosing the most appropriate feature selection method can significantly improve the performance accuracy of existing state of the art classification algorithms for detection of suspicious emails."}, {"heading": "III. CLASSIFICATION ALGORITHMS", "text": "A. Decision Tree\nA decision tree [5],[19] consists of two types of nodes, namely; internal and external. Internal nodes correspond to attributes selected by decision tree algorithm for making decision at specific level of hierarchy. The branches coming out from these internal nodes are the values of that attribute. The attribute at top level of hierarchy in the tree has more power of classifying the instances of different classes. The external nodes in the tree correspond to the decision classes. Decision tree classifiers have some advantages over other classifiers, i.e., it is simple to build, its generated rules are easily interpretable by human and it is an inductive algorithm. Its accuracy can be very high, if an adequate training set is provided. There have been many decision tree algorithms developed through the time. Iterative Dochotomiser3 (ID3) [5] has remained the choice of data mining research community for many years. Beside its salient it has also some restrictions, i.e., ID3 can only deal with categorical attributes , it cannot handle missing values and it is not incremental. This led to the development of C4.5 [19] algorithm which can address the restrictions of ID3.\nB. Naive Bayes (NB)\nNa\u00efve Bayes [6] is a generative classification method that is based on Bayes theorem. It calculates the prior probabilities of each class and probabilities of each attribute in each class. It assumes that the probabilities of each attribute are independent of each other. At the time of classification it uses the prior probabilities of each class and the probabilities of the observed attributes. The class with highest probability is assigned to the instance being classified.\nC. Support Vector Machine (SVM)\nSVM is a discriminative supervised machine learning technique of classification. SVM applies Vapnik\u2019s statistical learning theory [7] to train classifiers. SVM has some salient features for which it has been considered as state of art in the classification tasks. SVM has been used for text classification, hand written digit detection and many other classification tasks. Some of its unique features are: it can work well in a very high dimensional feature space, it uses only a subset of original training set to make decision boundary called support vectors and it is also suitable for non-linearly separable data (it uses kernel trick). Author [28] has described a number of features that explain why SVM is ideal for text classification tasks.\nD. Logistic Regression\nLogistic Regression [7] belongs to the generalized linear model category of statistical models. It can predict a discrete outcome from a set of variables that may be categorical, numerical, continuous or dichotomous.\nIV. PROBLEM STATEMENT\nThe problem under consideration is to identify emails that contain suspicious contents indicating future terrorism events. We consider the task of suspicious email detection as a classification task. We start with a training set T = {e1, e2, e3\u2026 em} and class labels isSuspicious = {Yes, No}. Each email is given a label. The purpose is to formulate a model that learns from the training set and is able to classify a new email sample as either suspicious or non-suspicious.\nWe cannot deny the importance of email that is a major source of communication among most individuals and organizations, including terrorists and terrorist organizations. From this major source of communication, we can potentially locate evidence of future terrorist events. We propose a methodology to find clues about such events through email communication before those events take place. The proposed system first extracts useful features from the email body. If such features present in a certain combination, the email is marked as suspicious and the evidence of a potential future terrorist event is captured. If some features i.e., keywords are present but not others i.e., suspicious indicators, it may just be an email discussing past events, maybe condemning the events and so on. In the paper, we extracted the features (that are suspicious) along with the context. For example: an email body contains the message text as: \u201cAll the true Muslims condemn the terrorist attacks of 9/11.\u201d In the sentence, the keywords \u201cterrorist\u201d and \u201cattack\u201d are used but they do not indicate a future attack due to the presence of another feature \u201ccondemn.\u201d In the following section we present the methodology for problem under consideration."}, {"heading": "V. PROPOSED METHODOLOGY", "text": "Our proposed methodology uses machine learning techniques to detect the suspicious emails. It evaluates the performance of four classifiers with feature selection strategies. The algorithm for proposed methodology is given in the Fig. 1.\nThe algorithm described in Fig. 1, illustrates the way suspicious email detection model works. In the algorithm, the variable FS is an array of 10 feature selection strategies and has the values as initialized for various feature selection schemes. WFS stands for Without Feature Selection, CFS-BFS stands for CfsSubSetEval and Best First Search method, CFS-GSS stands CfsSubSetEval and Greedy Stepwise Search method, CFS-RS stands for CfsSubSetEval and Rank Search method, CSE-BFS stands for ConsistencySubsetEval and BestFirst Search method, CSE-GSS stands for ConsistencySubsetEval and GreedyStepwise Search method while CSE-RS stands for ConsistencySubsetEval and Rank Search method, IG-R stands for InfoGain and Ranker search method, GR-R stands for GainRatio evaluation and Ranker search method where Chi-R stands for ChiSquare evaluation and Ranker search methods.\nAs the significance of feature selection strategy in the task of email classification has been identified, the next subsection discusses it analytically. In the following section, the system architecture is described to clarify how the feature selection presented is incorporated with the classifiers."}, {"heading": "A. Feature Selection", "text": "Feature selection is a way to select a subset of the original feature space. The number of features in the space affects the computation time and also the accuracy of the classifier. The key idea behind feature selection is to search a feasible subset of features by evaluating them, through some evaluators [14]. In this paper we focus on proper feature selection by which we could achieve relatively better performance of the required task even with the existing algorithms. Feature F is defined as a vector of K and I and it is the original feature space:\nF = {K1, K2,\u2026\u2026Kn,I1, I2,\u2026\u2026In} (1)\nK is a vector of n keywords and I is a vector of indicators. Among the indicators some indicators make the email suspicious and some make the email non suspicious:\nI = Is + In (2)\nisSuspicious is a function over K and I:\nisSuspicious (K,I) = \u201cYes\u201d, if (K=1 and Is = 1) isSuspicious (K,I) = \u201cYes\u201d, if (K =1 and Is = 1 and In = 1)\nisSuspicious (K,I) = \u201cNo\u201d, if (K=1 and In = 1) isSuspicious (K,I) = \u201cNo\u201d, if (K=1 and Is = 0 and In = 1) (3)\nIn the proposed approach we have not only used the terrorism domain keywords as features but also certain indicators such as the word 'condemn' as presented in the previous example. If a keyword is used in combination with a non-suspicious indicator, then it is not an indication of an upcoming terrorist event.\nFor our task, we have applied a supervised filtered feature selection method [16] because our task is a typical supervised machine learning classification task.\nIn the feature selection process we have applied\nCfsSubsetEval, ConsistenceySubsetEval, InfoGain, GainRatio, and ChiSquare evaluators and BestFirst, GreedyStepwise and Ranker search methods. We have applied various combinations of evaluators and search methods and applied them on each of the four state of the art classifiers."}, {"heading": "B. System Architecture", "text": "The strategy used in the proposed system, for feature selection and generation is the one that it is kept outside the main classification engine, which contains different classification algorithms. This means that the feature selection is applied before classification. We used the well known open source data mining tool WEKA (Waikato Environment for Knowledge Analysis) [29]software for our feature selection and classification purposes.\nInitially a text message is given as input and then feature selection strategies are applied. Extracted features are recorded in the ARFF file format which is the WEKA specific form of a Comma Separated Values (CSV) file. WEKA expects the input file to be in ARFF format. The communication between WEKA and our proposed system takes place with the help of data file exchanges. The original feature space is generated separately from WEKA with the help of a rule execution engine. The rest of the feature selection methods are applied manually from WEKA\u2019s filter feature selection. The email content is inspected and the rules are derived from the keywords and indicators in corresponding repositories. The rule execution engine executes these rules and the results of rule execution are used by the feature function factory to formulate the corresponding feature functions. The derived feature functions are kept in a feature function repository for future reuse. These features are applied on the decision tree, Na\u00efve Bayes, SVM, and Logistic Regression classifiers to classify emails as suspicious or non suspicious. The described system architecture is shown in Fig. 2."}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "Our experiments were conducted using the steps defined in the algorithm given in Fig. 1. For conducting the experiments as mentioned before we used WEKA data mining tool. We have developed the terror email dataset from various sources like news groups. We also have used some emails that are used by authors [1],[2]. Some other emails are also dummy emails. The reason for developing such a dataset is because to the best of our knowledge, there is no such benchmark email dataset available in the counterterrorism domain.\nFor evaluating our experimental results we have used 10 fold cross validation. In this method the dataset is divided into 10\nsubsets and the algorithm runs in ten passes. In each pass one subset is used for testing and the rest nine of them are used as training sets. In each pass a new test set is selected and finally the average accuracy is returned. The accuracy A is measured as\n \n p\ni\niac p A 1\n1 (4)\nwhere ac is the accuracy of correctly classified emails in pass i and p is the total number of passes. The experimental results show that the accuracy of the suspicious email detection task not only depends on the classifier itself but also on the feature selection strategy. Firstly, we conducted the experiments without using any feature selection strategy. This resulted in a relatively poor accuracy of the four algorithms. The results can be observed in Table II. Secondly, the experiments were conducted by applying feature selection strategies on each of the classification algorithms. Finally, the accuracies of the algorithms were compared with and without various feature selection methods. The results which are illustrated in TableII showed the highest accuracy. The results highlighted using bold face illustrates the second highest accuracy.\nThe accuracy of the logistic regression algorithm has been increased from 69.64% to 83.92%. Performance of the decision tree algorithm has been increased from 78.57% to 83.92%. The Na\u00efve Bayes algorithm increased its performance from 69.64% to 78.57%. Finally, the SVM algorithm increased its performance from 73.21% to 80.35%.\nThe effect of each feature selection method on each of the algorithms, i.e. ID3, Naive Bayes, logistic regression and SVM is depicted in the graph Figures 3, 4, 5 and 6 respectively. It can also be observed from the results that an appropriate feature selection strategy greatly affects the performance of the logistic regression algorithm. ID3 is the best among them when no feature selection method is applied and also achieves maximum performance with feature selection. From the experiments it can be observed that with the feature selection method ConsistencySubsetEval (evaluator) and GreedyStepwise (search method) three of the four classifiers achieved the maximum performance \u2013 except the Na\u00efve Bayes. Using feature selection methods like ChiSquare, InfoGain, and GainRatio with the Ranker search method resulted in the same accuracy of all the classifiers as without any feature selection method applied.\nFor the sake of space in graphs in Figures 3, 4, 5 and 6 we\nassign label to each feature selection scheme in Table I."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "In this paper, we have presented suspicious email detection strategies using various classifiers and different feature selection methods. We concluded that in the specific task, the decision tree algorithm (ID3) outperformed the rest of the state of the art classifiers as Na\u00efve Bayes, SVM, and logistic regression. After applying the appropriate feature selection strategy, the logistic regression algorithm also gave the maximum performance together with the decision tree algorithm. We also concluded that a feature selection strategy using ConsistencySubsetEval (as evaluator) and GreedyStepwise (as search method) achieves the maximum performance gain in terms of accuracy. In the future, we plan to also apply classifier based feature selection method for the specific task. We also plan to apply feature selection method on boosting algorithm for suspicious email detection task. At the moment, the experiments are conducted in relatively small dataset but in the future, we are planning to construct a larger dataset from terrorist statements derived from news groups, blogs, forums and terrorist websites.\nFig. 4. Impact of feature selection in Naive Bayes\nFig. 5. Impact of feature selection in logistic regression\nREFERENCES\n[1] S. Appavu, R. Rajaram, (2007). Suspicious email detection via decision tree: a data mining approach. Journal of Computing and Information Technology -CIT 15 .pp. 161-169. [2] S. Appavu, R. Rajaram, \"Association rule mining for suspicious email detection: a data mining approach\u201d, In Proc. of the IEEE International Conference on Intelligence and Security Informatics, New Jersey, USA, 2007, pp. 316-323. [3] S. Appavu and R. Rajaram, (2008). Learning to classify threatening e-mail. Int. Journal of Artificial Intelligence and Soft Computing. 1(1). pp. 39-51. [4] J. Allanach, H. Tu, S. Singh, P. Willet and K. Pattipati, \u201cDetecting, tracking and counteracting t errorist networks via Hidden Markov Model\u201c , IEEE Aerospace Conference, 2004. [5] J. R.Quinlan, (1986). Induction of Decision Trees, Machine Learning. 1(1). pp. 81-106, , Kluwer Academic Publishers, Boston. [6] A. McCallum, K. Nigam,(2003) . A comparison of event models for Na\u00efve Bayes text classification. Journal of Machine Learning Research. pp. 1265-1287. [7] T. M. Mitchel, Machine Learning, 2nd ed. 2010. ch 1. pp. 1-17. [8] V. Vapnik, The Nature of Statistical Theory, 1995. Springer, New York. [9] http://www.hindu.com/2008/12/01/stories/2008120155681000.h tm [10] M. J. H. Lim, \u201cComputational intelligence in Email t raffic analysis\u201d, PhD Dissertation, University of Tasmania, 2008. [11] R. Clayton, \u201cEmail Traffic: A Quantitative Snapshot\u201d, In proc. of Fourth Conference on Email and Anti-Spam , California USA. 2007.\n[12] P. S. Keila, D. B. Skillicorn, \u201cDetecting unusual email communication\u201d, In proc. of the 2005 conference of the centre for advanced studies on collaborative research, 2005, pp. 117-125. [13] B. Klimt, Y. Yang, \"The Enron Corpus: A new dataset for Email classification research,\" European Conference on Machine Learning, Pisa, Italy, 2004. [14] .K.Selvakuberan, M. Indradevi, R.. Rajaram, (2008). Combined feature selection and classification \u2013 A novel approach for categorization of web pages. Journal of Information and Computing Science. 3 (2). pp. 83-89. [15] A. Arauzo-Azofra, J. M. Benitez, \u201cEmpirical Study of Feature Selection Methods in Classification\u201d, In proc. of Eighth Internation Conference on Hybrid Intelligent Systems, 2008, pp. 584-589 [16] W. Duch,\u201dFilter Methods\u201d, \u201cFeature Extraction and Foundations\u201d, In proc. NIPS 2003 Workshop ,Springer. 2003, pp. 89-118 [17] B. B. Pineda-Bautista, J. A. Carrasco-Ochao, J. F. . Martinez-Trindad,\u201d Taking Advantage of Class-Specific Feature Selection\u201d, IDEAL Springer-Verlag Berlin. 2009, pp. 1-8. [18] P. Chuningham, S. J. Delany: k-Nearest Neighbours Classifiers, Technical Report, UCD-CSI-2007. [19] J. R.Quinlan, \"C4.5: Programs for machine learning\". San Francisco, CA, USA: Morgan Kaufmann. 1993. [20] K. T . Durant, M. D. Smith \u201cPredicting the political sentiment of web log posts using supervised machine learning techniques coupled with feature selection\".LNCS, 2007, pp. 187-206. [21] S. Verma, N. Jain, \u201cImplementation of ID3 - Decision Tree Algorithm.\u201d available online at http://www.scribd.com/doc/22639832/ID3-Algorithm (Accessed on June 08, 2010). [22] D.K. Renuka, T . Hamsapriya,(2010). Email classification for spam detection using word stemming. International Journal of Computer Applications.1(5). pp. 45-47.\n[23] C. Cortes, V. Vapnik, (1995) Support -Vector Networks. Machine Learning..20(3). p p. 273-297. [24] G. P. Zhang \u201cNeural networks for classification: A survey\u201d, IEEE Transactions On Systems Man and Cybernetics- Part C: Applications and Reviews, 30(.4) , 2000, pp. 451-462 [25] A. Sun, M. M. Naing, E. P. Lim, W. Lam, \u201cUsing Support Vector Machines for terrorist information extraction\u201d, In proc. of 1st NSF/NIJ conference on Intelligence and Security Informatics, 2003, pp. 1-12. [26] A. Das , D. Kempe, \u201cAlgorithms for subset selection in linear regression\u201d, In proc. of the 40th annual ACM symposium on Theory of computing, Victoria, British Columbia, Canada, 2008. [27] A. L. Blum, P. Langley, (1997). Selection of relevant features and examples in machine learning., Artificial Intelligence. 97(2), pp.245-271. [28] T. Joachims ,\u201cA Statistical Learning Model of Text Classification for Support Vector Machines\u201d, SIGIR '01, New Orleans, Louisiana, USA 2001, pp. 128-136. [29] H.Witten and E. Frank. DataMining..PracticalMachine Learning Tools and Techniques, 2nd edition, Morgan Kaufmann,2005.\nSarwat Nizamani received her B.Sc. (Hons.) and M.Sc. (Hons.) in Computer Science in the years 1998 and 1999 respectively from University of Sindh, Pakistan. The author received her Master's of Science in Robotic Engineering, from University of Southern, Denmark in 2011.\nShe worked as Research Associate from 2000-2003, then as lecturer from 2003-2007 and as Assistant Professor from 2007 to-date in University of Sindh, Pakistan. Since April 2010, she is PhD student, at University of Southern, Denmark. She has total six publications, out of which two are published in Springer Lecture Notes series and four in Conferences. Her field of research interest is machine learning, natural language processing, data mining, data structures and algorithm analysis.\nNasrullah Memon holds a PhD in Intelligence and Security Informatics (Computer Science and Engineering) from the Aalborg University Denmark. He has received Master in Software Development from University of Huddersfield, U.K. His research interests include machine learning, natural language processing, information retrieval, information extraction, open source intelligence and investigative data mining. He has published more than 90 research articles in journals/ conferences/ book chapters of national and international repute. He served as Editor-in-Chief of Social Network Analysis and Mining and affiliated with a number of international journals and conferences.\nUffe Kock Wiil is a Professor of Software Engineering and Technology at the Maersk Mc- Kinney Moller Institute, University of Southern Denmark. He holds a M.Sc. degree in Computer Engineering (1990) and a Ph.D. degree in Computer Science (1993) both from Aalborg University, Denmark. His research interest includes knowledge management, hypertext, computer supported cooperative work, software technology, and distributed systems. These research interests are currently being applied in three overall areas: counterterrorism, healthcare, and planning. He has published more than 150 research papers. His research papers have been cited more than 1300 times. He is currently serving on the editorial boards of the Elsevier Journal on Network and Computer Applications, the Springer Journal on Social Network Analysis and Mining, and the Hindawi ISRN Software Engineering Journal, and on the advisory board of the Springer Journal on Security Informatics.\nPanagiotis Karampelas holds a PhD in Electronic Engineering from the University of Kent at Canterbury, UK. He also holds a Master of Science from the Department of Informatics, Kapodistrian University of Athens and a Bachelor degree in Mathematics from the same University. He has worked for 3\u00bd years as an associate researcher in the Foundation for Research & Technology-Hellas (FORTH), Institute of Computer Science, and several years as a user interface designer and usability expert in several IT companies designing and implementing\nlarge-scale informat ion systems. He has also participated in many European research projects and published a number of articles in his major areas of interests. He serves as an associate editor in the Social Network Analysis and Mining journal and reviewer in various scientif ic journals and conferences in his fields of interests. He teaches human computer interaction, programming and databases. Panagiotis Karampelas is also the Director of the Business and Information Technology Division of the Hellenic American University."}], "references": [{"title": "Suspicious email detection via decision tree: a data mining approach", "author": ["S. Appavu", "R. Rajaram"], "venue": "Journal of Computing and Information Technology -CIT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Association rule mining for suspicious email detection: a data mining approach", "author": ["S. Appavu", "R. Rajaram"], "venue": "In Proc. of the IEEE International Conference on Intelligence and Security Informatics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Learning to classify threatening e-mail", "author": ["S. Appavu", "R. Rajaram"], "venue": "Int. Journal of Artificial Intelligence and Soft Computing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Detecting, tracking and counteracting t errorist networks via Hidden Markov Model", "author": ["J. Allanach", "H. Tu", "S. Singh", "P. Willet", "K. Pattipati"], "venue": "IEEE Aerospace Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A comparison of event models for Na\u00efve Bayes text classification", "author": ["K.A. McCallum"], "venue": "Journal of Machine Learning Research", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Computational intelligence in Email t raffic analysis", "author": ["M.J.H. Lim"], "venue": "PhD Dissertation, University of Tasmania,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Email Traffic: A Quantitative Snapshot", "author": ["R. Clayton"], "venue": "In proc. of Fourth Conference on Email and Anti-Spam , California USA", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Detecting unusual email communication", "author": ["P.S. Keila", "D.B. Skillicorn"], "venue": "In proc. of the 2005 conference of the centre for advanced studies on collaborative research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "The Enron Corpus: A new dataset for Email classification research", "author": ["B. Klimt", "Y. Yang"], "venue": "European Conference on Machine Learning, Pisa, Italy, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Combined feature selection and classification \u2013 A novel approach for categorization of web pages", "author": ["K.Selvakuberan", "M. Indradevi", "R.. Rajaram"], "venue": "Journal of Information and Computing Science", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Empirical Study of Feature Selection Methods in Classification", "author": ["A. Arauzo-Azofra", "J.M. Benitez"], "venue": "In proc. of Eighth Internation Conference on Hybrid Intelligent Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Methods\u201d, \u201cFeature Extraction and Foundations", "author": ["W. Duch", "\u201dFilter"], "venue": "In proc. NIPS 2003 Workshop ,Springer", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Taking Advantage of Class-Specific Feature Selection", "author": ["B.B. Pineda-Bautista", "J.A. Carrasco-Ochao", "J.F. . Martinez-Trindad"], "venue": "IDEAL Springer-Verlag Berlin", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "k-Nearest Neighbours Classifiers", "author": ["P. Chuningham", "S.J. Delany"], "venue": "Technical Report,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "C4.5: Programs for machine learning", "author": ["J.R.Quinlan"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Predicting the political sentiment of web log posts using supervised machine learning techniques coupled with feature selection\".LNCS", "author": ["K. T . Durant", "M.D. Smith"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Implementation of ID3 - Decision Tree Algorithm.", "author": ["S. Verma", "N. Jain"], "venue": "(Accessed on June", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Email classification for spam detection using word stemming", "author": ["T D.K. Renuka"], "venue": "International Journal of Computer", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Support -Vector Networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning..20(3)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Neural networks for classification: A survey", "author": ["G.P. Zhang"], "venue": "IEEE Transactions On Systems Man and Cybernetics- Part C: Applications and Reviews,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "Using Support Vector Machines for terrorist information extraction", "author": ["A. Sun", "M.M. Naing", "E.P. Lim", "W. Lam"], "venue": "In proc. of 1st NSF/NIJ conference on Intelligence and Security Informatics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Algorithms for subset selection in linear regression", "author": ["A. Das", "D. Kempe"], "venue": "In proc. of the 40th annual ACM symposium on Theory of computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Selection of relevant features and examples in machine learning", "author": ["A.L. Blum", "P. Langley"], "venue": "Artificial Intelligence", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "\u201cA Statistical Learning Model of Text Classification for Support Vector Machines", "author": ["T. Joachims"], "venue": "SIGIR '01,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "A great number of researchers [1],[2],[3],[4]focused in the area of counterterrorism after the disastrous events of 9/11 trying to predict terrorist plans from suspicious communication.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "A great number of researchers [1],[2],[3],[4]focused in the area of counterterrorism after the disastrous events of 9/11 trying to predict terrorist plans from suspicious communication.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "A great number of researchers [1],[2],[3],[4]focused in the area of counterterrorism after the disastrous events of 9/11 trying to predict terrorist plans from suspicious communication.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "A great number of researchers [1],[2],[3],[4]focused in the area of counterterrorism after the disastrous events of 9/11 trying to predict terrorist plans from suspicious communication.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "We have applied decision tree (ID3) [5], Na\u00efve Bayes [6], logistic regression [7], and SVM [7] algorithms, emphasizing initially on feature space creation, then applying various feature selection techniques by selecting a subset of the original feature space.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Few of the emails are real emails of 9/11 incident which are also used by authors [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "A lot of research has been conducted for Email traffic analysis [10],[11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "A lot of research has been conducted for Email traffic analysis [10],[11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "[11] of suspicious individual.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Email content analysis [11], [1], [22] on the other hand is the study of the unstructured part of the email such as the subject and body.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Email content analysis [11], [1], [22] on the other hand is the study of the unstructured part of the email such as the subject and body.", "startOffset": 29, "endOffset": 32}, {"referenceID": 17, "context": "Email content analysis [11], [1], [22] on the other hand is the study of the unstructured part of the email such as the subject and body.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "Keila and Skillicorn [11] have investigated on the Enron [13]data set which contains email communications among employees of an organization who were involved in the collapse of the organization.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "Keila and Skillicorn [11] have investigated on the Enron [13]data set which contains email communications among employees of an organization who were involved in the collapse of the organization.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "The authors [1] have applied ID3 algorithm to detect suspicious emails by using keyword base approach and by applying rules.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "Rajaram [2] have applied association rule mining to detect suspicious emails with the additional benefits of classifying the (suspicious in terms of terror plots) emails further into specialized classes such as suspicious alert or suspicious info.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "The authors [14],[15] incorporated feature selection strategies along with classification systems.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "The authors [14],[15] incorporated feature selection strategies along with classification systems.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "According to [15], by using feature selection methods one can improve the accuracy, applicability, and understandability of the learning process.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "[14]have applied filtered feature selection methods [16] on web page classification; according to their results the evaluator CfsSubsetEval yields better performance with search methods BestFirst, Ranker search, and Forward selection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14]have applied filtered feature selection methods [16] on web page classification; according to their results the evaluator CfsSubsetEval yields better performance with search methods BestFirst, Ranker search, and Forward selection.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "[17] proposed a method for selecting the subset of features for each class in multi-class classification task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The classifiers that were used by the authors were Naive Baye's (NB) [6], k-Nearest Neighbors (k-NN) [17], C4.", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "The classifiers that were used by the authors were Naive Baye's (NB) [6], k-Nearest Neighbors (k-NN) [17], C4.", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "5 [19], and MultiLayer Perceptron (MLP).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "Durant and Smith [20]have emphasized the use of a feature selection method for achieving accuracy of sentiment classification.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "The ID3 (a type of decision tree) [21], algorithm is mostly used for email classification and content analysis systems [1],[2].", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "The ID3 (a type of decision tree) [21], algorithm is mostly used for email classification and content analysis systems [1],[2].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "The ID3 (a type of decision tree) [21], algorithm is mostly used for email classification and content analysis systems [1],[2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 16, "context": "In the classification experiments for email spam filtering [21], decision tree classifiers outperform the other classifiers like SVM [23], neural networks [24], and others.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "In the classification experiments for email spam filtering [21], decision tree classifiers outperform the other classifiers like SVM [23], neural networks [24], and others.", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "In the classification experiments for email spam filtering [21], decision tree classifiers outperform the other classifiers like SVM [23], neural networks [24], and others.", "startOffset": 155, "endOffset": 159}, {"referenceID": 20, "context": "SVM have also been applied for content extraction in the terrorism domain [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Indeed, selecting an optimal set of features is generally difficult, both theoretically and empirically [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "For the classification tasks, the proper strategy for feature selection is of utmost importance as emphasized in [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 4, "context": "In this paper, we compare the evaluation of state of art decision tree, logistic regression, SVM, and Na\u00efve Bayes [6] algorithms first without explicitly selecting features and then with various feature selection approaches.", "startOffset": 114, "endOffset": 117}, {"referenceID": 14, "context": "A decision tree [5],[19] consists of two types of nodes, namely; internal and external.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "5 [19] algorithm which can address the restrictions of ID3.", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": "Na\u00efve Bayes [6] is a generative classification method that is based on Bayes theorem.", "startOffset": 12, "endOffset": 15}, {"referenceID": 23, "context": "Author [28] has described a number of features that explain why SVM is ideal for text classification tasks.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "The key idea behind feature selection is to search a feasible subset of features by evaluating them, through some evaluators [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "For our task, we have applied a supervised filtered feature selection method [16] because our task is a typical supervised machine learning classification task.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "We also have used some emails that are used by authors [1],[2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "We also have used some emails that are used by authors [1],[2].", "startOffset": 59, "endOffset": 62}], "year": 2013, "abstractText": "The paper presents a suspicious email detection model which incorporates enhanced feature selection. In the paper we proposed the use of feature selection strategies along with classification technique for terrorists email detection. The presented model focuses on the evaluation of machine learning algorithms such as decision tree (ID3), logistic regression, Na\u00efve Bayes (NB), and Support Vector Machine (SVM) for detecting emails containing suspicious content. In the literature, various algorithms achieved good accuracy for the desired task. However, the results achieved by those algorithms can be further improved by using appropriate feature selection mechanisms. We have identified the use of a specific feature selection scheme that improves the performance of the existing", "creator": "Microsoft\u00ae Office Word 2007"}}}