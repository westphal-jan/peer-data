{"id": "1312.4564", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Adaptive Stochastic Alternating Direction Method of Multipliers", "abstract": "The Alternating Direction Method of Multipliers (ADMM) has been studied for years, since it can be applied to many large-scale and data-distributed machine learning tasks. The traditional ADMM algorithm needs to compute an (empirical) expected loss function on all the training examples for each iteration, which results in a computational complexity propositional to the number of training examples. To reduce the time complexity, stochastic ADMM algorithm is proposed to replace the expected loss function by a random loss function associated with one single uniformly drawn example and Bregman divergence for a second order proximal function. This optimization reduces the number of variance, and thus reduces the probability of learning any possible problems that arise in the comparison.\n\n\nThe ADMM algorithm was invented in 1883 by Albert Einstein and first reported in 1936 by John L. Schulman (and later by George F. Kennedy) using linear algebra. The algorithm's output should be a single \"mullet\", as this means that the loss function that follows any two constant values can be considered to be the same.\nWith this approach, a linear model with the expected loss function is made possible by a series of two discrete probabilities that are in step with the mean number of random deviations, while the mean total value of the loss function is reduced. These two probabilities are assumed to be equal to the number of expected loss functions that have been generated. To ensure accuracy, the linear models have to be applied to the sum of the initial steps to the sum of each individual steps, which can be determined by the expected loss function. The results are presented in two ways:\nTo ensure accuracy, the linear models have to be applied to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial steps to the sum of the initial", "histories": [["v1", "Mon, 16 Dec 2013 21:22:46 GMT  (42kb)", "https://arxiv.org/abs/1312.4564v1", "12 pages"], ["v2", "Sun, 22 Dec 2013 01:59:05 GMT  (0kb,I)", "http://arxiv.org/abs/1312.4564v2", "This paper has been withdrawn by the author, because this paper is not well-prepared"], ["v3", "Thu, 5 Jun 2014 07:03:48 GMT  (43kb)", "http://arxiv.org/abs/1312.4564v3", "12 pages"], ["v4", "Mon, 9 Jun 2014 09:31:13 GMT  (43kb)", "http://arxiv.org/abs/1312.4564v4", "13 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["peilin zhao", "jinwei yang", "tong zhang", "ping li 0001"], "accepted": true, "id": "1312.4564"}, "pdf": {"name": "1312.4564.pdf", "metadata": {"source": "CRF", "title": "Adaptive Stochastic Alternating Direction Method of Multipliers", "authors": ["Peilin Zhao", "Jinwei Yang", "Tong Zhang", "Ping Li"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n45 64\nv4 [\nst at\n.M L\n] 9\nJ un\n2 01\n4\nIn this paper, we present a new family of stochastic ADMM algorithms with optimal second order proximal functions, which produce a new family of adaptive subgradient methods. We theoretically prove that their regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms."}, {"heading": "1 Introduction", "text": "Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4]. Recent studies have shown that ADMM achieves a convergence rate of O(1/T ) [14, 12] (where T is number of iterations of ADMM), when the objective function is generally convex. Furthermore, ADMM enjoys a convergence rate of O(\u03b1T ), for some \u03b1 \u2208 (0, 1), when the objective function is strongly convex and smooth [13, 2]. ADMM has shown attractive performance in a wide range of real-world problems such as compressed sensing [18], image restoration [11], video processing, and matrix completion [10], etc.\nFrom the computational perspective, one drawback of ADMM is that, at every iteration, the method needs to compute an (empirical) expected loss function on all the training examples. The computational complexity is propositional to the number of training examples, which makes the original ADMM unsuitable for solving large-scale learning and big data mining problems. The online ADMM (OADMM) algorithm [17] was proposed to tackle the computational challenge. For OADMM, the objective function is replaced with an online function at every step, which only depends on a single training example. OADMM can achieve an average regret bound of O(1/ \u221a T ) for convex objective functions and O(log(T )/T ) for strongly convex objective functions. Interestingly, although the optimization of the loss function is assumed to be easy in the analysis of [17], it is actually not necessarily easy in practice. To address this issue, the stochastic ADMM algorithm was proposed, by linearizing the the online loss function [15, 16]. In stochastic ADMM algorithms, the online loss function is firstly uniformly drawn from all the loss functions associated with all the training examples. Then the loss function is replaced with its first order expansion at the current solution plus Bregman divergence from the current solution. The Bregman divergence is based on a simple proximal\n\u2217Department of Statistics and Biostatistics, Rutgers University, New Jersey 08854, USA. peilin.zhao@rutgers.edu \u2020Department of Mathematics, Rutgers University, New Jersey 08854, USA. yookinwi@math.rutgers.edu \u2021Department of Statistics and Biostatistics, Rutgers University, New Jersey 08854, USA. tzhang@stat.rutgers.edu \u00a7Department of Statistics and Biostatistics, Department of Computer Science, Rutgers University, New Jersey 08854, USA.\npingli@stat.rutgers.edu\nfunction, the half squared norm, so that the Bregman divergence is the half squared distance. In this way, the optimization of the loss function enjoys a closed-form solution. The stochastic ADMM achieves similar convergence rates as OADMM. Using half square norm as proximal function, however, may be a suboptimal choice. Our paper will address this issue.\nOur contribution. In the previous work [15, 16] the Bregman divergence is derived from a simple second order function, i.e., the half squared norm, which could be a suboptimal choice [3]. In this paper, we present a new family of stochastic ADMM algorithms with adaptive proximal functions, which can accelerate stochastic ADMM by using adaptive subgradient. We theoretically prove that the regret bounds of our methods are as good as those achieved by stochastic ADMM with the best proximal function that can be chosen in hindsight. The effectiveness and efficiency of the proposed algorithms are confirmed by encouraging empirical evaluations on several real-world datasets.\nOrganization. Section 2 presents the proposed algorithms. Section 3 gives our experimental results. Section 4 concludes our paper. Additional proofs can be found in the supplementary material."}, {"heading": "2 Adaptive Stochastic Alternating Direction Method of Multipli-", "text": "ers"}, {"heading": "2.1 Problem Formulation", "text": "In this paper, we will study a family of convex optimization problems, where our objective functions are composite. Specially, we are interested in the following equality-constrained optimization task:\nmin w\u2208W,v\u2208V\nf((w\u22a4,v\u22a4)\u22a4) := E\u03be\u2113(w, \u03be) + \u03d5(v), s.t. Aw +Bv = b, (1)\nwhere w \u2208 Rd1 , v \u2208 Rd2 , A \u2208 Rm\u00d7d1 , B \u2208 Rm\u00d7d2 , b \u2208 Rm, W and V are convex sets. For simplicity, the notation \u2113 is used for both the instance function value \u2113(w, \u03be) and its expectation \u2113(w) = E\u03be\u2113(w, \u03be). It is assumed that a sequence of identical and independent (i.i.d.) observations can be drawn from the random vector \u03be, which satisfies a fixed but unknown distribution. When \u03be is deterministic, the above optimization becomes the traditional problem formulation of ADMM [1]. In this paper, we will assume the functions \u2113 and \u03d5 are convex but not necessarily continuously differentiable. In addition, we denote the optimal solution of (1) as (w\u22a4\u2217 ,v \u22a4 \u2217 )\n\u22a4. Before presenting the proposed algorithm, we first introduce some notations. For a positive definite\nmatrix G \u2208 Rd1\u00d7d1 , we define the G-norm of a vector w as \u2016w\u2016G := \u221a w\u22a4Gw. When there is no ambiguity, we often use \u2016 \u00b7 \u2016 to denote the Euclidean norm \u2016 \u00b7 \u20162. We use \u3008\u00b7, \u00b7\u3009 to denote the inner product in a finite dimensional Euclidean space. Let Ht be a positive definite matrix for t \u2208 N. Set the proximal function \u03c6t(\u00b7), as \u03c6t(w) =\n1 2\u2016w\u20162Ht = 1 2 \u3008w, Htw\u3009. Then the corresponding Bregman divergence for \u03c6t(w) is defined as\nB\u03c6t(w,u) = \u03c6t(w)\u2212 \u03c6t(u)\u2212 \u3008\u2207\u03c6t(u),w \u2212 u\u3009 = 1\n2 \u2016w\u2212 u\u20162Ht ."}, {"heading": "2.2 Algorithm", "text": "To solve the problem (1), a popular method is Alternating Direction Multipliers Method (ADMM). ADMM splits the optimizations with respect to w and v by minimizing the augmented Lagrangian:\nmin w,v\nL\u03b2(w,v, \u03b8) := \u2113(w) + \u03d5(v) \u2212 \u3008\u03b8, Aw +Bv \u2212 b\u3009+ \u03b2\n2 \u2016Aw +Bv \u2212 b\u20162,\nwhere \u03b2 > 0 is a pre-defined penalty. Specifically, the ADMM algorithm minimizes L\u03b2 as follows\nwt+1 = argmin w L\u03b2(w,vt, \u03b8t), vt+1 = argmin v L\u03b2(wt+1,v, \u03b8t), \u03b8t+1 = \u03b8t \u2212 \u03b2(Awt+1 +Bvt+1 \u2212 b).\nAt each step, however, ADMM requires calculation of the expectation E\u03be\u2113(w, \u03be), which may be unrealistic or computationally too expensive, since we may only have an unbiased estimate of \u2113(w) or the expectation E\u03be\u2113(w, \u03be) is an empirical one for big data problem. To solve this issue, we propose to minimize the its following stochastic approximation:\nL\u03b2,t(w,v, \u03b8) = \u3008gt,w\u3009+ \u03d5(v) \u2212 \u3008\u03b8, Aw +Bv \u2212 b\u3009+ \u03b2 2 \u2016Aw+ Bv \u2212 b\u20162 + 1 \u03b7 B\u03c6t(w,wt),\nwhere gt = \u2113 \u2032(wt, \u03bet) and Ht for \u03c6t = 1 2\u2016w\u20162Ht will be specified later. This objective linearizes the \u2113(w, \u03bet) and adopts a dynamic Bregman divergence function to keep the new model near to the previous one. It is easy to see that this proposed approximation includes the one proposed by [15] as a special case when Ht = I. To minimize the above function, we followed the ADMM algorithm to optimize over w, v, \u03b8 sequentially, by fixing the others. In addition, we also need to update the Ht for B\u03c6t at every step, which will be specified later. Finally the proposed Adaptive Stochastic Alternating Direction Multipliers Method (Ada-SADMM) is summarized in Algorithm 1.\nAlgorithm 1 Adaptive Stochastic Alternating Direction Method of Multipliers (Ada-SADMM).\nInitialize: w1 = 0, u1 = 0, \u03b81 = 0, H1 = aI, and a > 0. for t = 1, 2, . . . , T do Compute gt = \u2113\n\u2032(wt, \u03bet); Update Ht and compute B\u03c6t ; wt+1 = argminw\u2208W L\u03b2,t(w,vt, \u03b8t); vt+1 = argminv\u2208V L\u03b2,t(wt+1,v, \u03b8t); \u03b8t+1 = \u03b8t \u2212 \u03b2(Awt+1 +Bvt+1 \u2212 b);\nend for"}, {"heading": "2.3 Analysis", "text": "In this subsection we will analyze the performance of the proposed algorithm for general Ht, t = 1, . . . , T . Specifically, we will provide an expected convergence rate of the iterative solutions. To achieve this goal, we firstly begin with a technical lemma, which will facilitate the later analysis.\nLemma 1. Let \u2113(w, \u03bet) and \u03d5(w) be convex functions, and Ht be positive definite, for t \u2265 1. Then for Algorithm 1, we have the following inequality\n\u2113(wt) + \u03d5(vt+1)\u2212 \u2113(w)\u2212 \u03d5(v) + (zt+1 \u2212 z)\u22a4F (zt+1) \u2264 \u03b7\u2016gt\u20162H\u2217t\n2 +\n1 \u03b7 [B\u03c6t(wt,w)\u2212 B\u03c6t(wt+1,w)] + \u03b2 2 (\u2016Aw+ Bvt \u2212 b\u20162 \u2212 \u2016Aw +Bvt+1 \u2212 b\u20162) + \u3008\u03b4t,w\u2212wt\u3009\n+ 1\n2\u03b2 (\u2016\u03b8 \u2212 \u03b8t\u20162 \u2212 \u2016\u03b8 \u2212 \u03b8t+1\u20162),\nwhere zt = (w \u22a4 t ,v \u22a4 t , \u03b8 \u22a4 t ) \u22a4, z = (w\u22a4,v\u22a4, \u03b8\u22a4)\u22a4, \u03b4t = gt \u2212 \u2113\u2032(wt), and F (z) = ((\u2212A\u22a4\u03b8)\u22a4, (\u2212B\u22a4\u03b8)\u22a4, (Aw + Bv \u2212 b)\u22a4)\u22a4.\nProof. Firstly, using the convexity of \u2113 and the definition of \u03b4t, we can obtain\n\u2113(wt)\u2212 \u2113(w) \u2264 \u3008\u2113\u2032(wt),wt \u2212w\u3009 = \u3008gt,wt+1 \u2212w\u3009+ \u3008\u03b4t,w \u2212wt\u3009+ \u3008gt,wt \u2212wt+1\u3009.\nCombining the above inequality with the relation between \u03b8t and \u03b8t+1 will derive\n\u2113(wt)\u2212 \u2113(w) + \u3008wt+1 \u2212w,\u2212A\u22a4\u03b8t+1\u3009 \u2264 \u3008gt,wt+1 \u2212w\u3009+ \u3008\u03b4t,w \u2212wt\u3009+ \u3008gt,wt \u2212wt+1\u3009+ \u3008wt+1 \u2212w, A\u22a4[\u03b2(Awt+1 +Bvt+1 \u2212 b)\u2212 \u03b8t]\u3009 = \u3008gt +A\u22a4[\u03b2(Awt+1 +Bvt \u2212 b)\u2212 \u03b8t],wt+1 \u2212w\u3009\n\ufe38 \ufe37\ufe37 \ufe38\nLt\n+ \u3008w \u2212wt+1, \u03b2A\u22a4B(vt \u2212 vt+1)\u3009 \ufe38 \ufe37\ufe37 \ufe38\nMt\n+\u3008\u03b4t,w \u2212wt\u3009\n+ \u3008gt,wt \u2212wt+1\u3009 \ufe38 \ufe37\ufe37 \ufe38\nNt\n.\nTo provide an upper bound for the first term Lt, taking D(u,v) = B\u03c6t(u,v) = 12\u2016u \u2212 v\u20162Ht and applying Lemma 1 in [15] to the step of getting wt+1 in the Algorithm 1, we will have\n\u3008\u2113(wt, \u03bet) +A\u22a4[\u03b2(Awt+1 +Bvt \u2212 b)\u2212 \u03b8t],wt+1 \u2212w\u3009 \u2264 1\n\u03b7 [B\u03c6t(wt,w)\u2212 B\u03c6t(wt+1,w)\u2212 B\u03c6t(wt+1,wt)].\nTo provide an upper bound for the second term Mt, we can derive as follows\n\u3008w \u2212wt+1, \u03b2A\u22a4B(vt \u2212 vt+1)\u3009 = \u03b2\u3008Aw \u2212Awt+1, Bvt \u2212Bvt+1\u3009 = \u03b2\n2 [(\u2016Aw +Bvt \u2212 b\u20162 \u2212 \u2016Aw+Bvt+1 \u2212 b\u20162) + (\u2016Awt+1 +Bvt+1 \u2212 b\u20162 \u2212 \u2016Awt+1 +Bvt \u2212 b\u20162)]\n\u2264 \u03b2 2 (\u2016Aw +Bvt \u2212 b\u20162 \u2212 \u2016Aw+Bvt+1 \u2212 b\u20162) + 1 2\u03b2 \u2016\u03b8t+1 \u2212 \u03b8t\u20162.\nTo drive an upper bound for the final term Nt, we can use Young\u2019s inequality to get\n\u3008gt,wt \u2212wt+1\u3009 \u2264 \u03b7\u2016gt\u20162H\u2217t\n2 + \u2016wt \u2212wt+1\u20162Ht 2\u03b7 = \u03b7\u2016gt\u20162H\u2217t 2 + B\u03c6t(wt,wt+1) \u03b7 .\nReplacing the terms Lt, Mt and Nt with their upper bounds, we will get\n\u2113(wt)\u2212 \u2113(w) + \u3008wt+1 \u2212w,\u2212A\u22a4\u03b8t+1\u3009 \u2264 1\n\u03b7 [B\u03c6t(wt,w)\u2212 B\u03c6twt+1,w)] + \u03b7\u2016gt\u20162H\u2217t 2 + \u3008\u03b4t,w\u2212wt\u3009\n+ \u03b2\n2 (\u2016Aw+Bvt \u2212 b\u20162 \u2212 \u2016Aw+Bvt+1 \u2212 b\u20162) +\n1\n2\u03b2 \u2016\u03b8t+1 \u2212 \u03b8t\u20162.\nDue to the optimality condition of the step of updating v in Algorithm 1, i.e., \u2202vL\u03b2,t(wt+1,vt+1, \u03b8t) and the convexity of \u03d5, we have\n\u03d5(vt+1)\u2212 \u03d5(v) + \u3008vt+1 \u2212 v,\u2212B\u22a4\u03b8t+1\u3009 \u2264 0.\nUsing the fact Awt+1 +Bvt+1 \u2212 b = (\u03b8t \u2212 \u03b8t+1)/\u03b2, we have\n\u3008\u03b8t+1 \u2212 \u03b8, Awt+1 +Bvt+1 \u2212 b\u3009 = 1\n2\u03b2 (\u2016\u03b8 \u2212 \u03b8t\u20162\u2212 \u2016\u03b8 \u2212 \u03b8t+1\u20162\u2212 \u2016\u03b8t+1 \u2212 \u03b8t\u20162).\nCombining the above three inequalities and re-arranging the terms will conclude the proof.\nGiven the above lemma, now we can analyze the convergence behavior of Algorithm 1. Specifically, we provide an upper bound on the the objective value and the feasibility violation.\nTheorem 1. Let \u2113(w, \u03bet) and \u03d5(w) be convex functions, and Ht be positive definite, for t \u2265 1. Then for Algorithm 1, we have the following inequality for any T \u2265 1 and \u03c1 > 0:\nE[f(u\u0304T )\u2212 f(u\u2217) + \u03c1\u2016Aw\u0304T +Bv\u0304T \u2212 b\u2016]\n\u2264 1 2T\n(\nE\nT\u2211\nt=1\n[2\n\u03b7 (B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)) + \u03b7\u2016gt\u20162H\u2217t\n] + \u03b2D2v\u2217,B +\n\u03c12\n\u03b2\n)\n. (2)\nwhere u\u0304T = ( 1 T \u2211T t=1 w \u22a4 t , 1 T \u2211T+1 t=2 v \u22a4 t )\u22a4 , u\u2217 = (w\u22a4\u2217 ,v \u22a4 \u2217 ) \u22a4, and (w\u0304T , v\u0304T ) = ( 1 T \u2211T+1 t=2 wt, 1 T \u2211T+1 t=2 vt), and Dv\u2217,B = \u2016Bv\u2217\u2016.\nProof. For convenience, we denote u = (w\u22a4,v\u22a4)\u22a4, \u03b8\u0304T = 1 T \u2211T+1 t=2 \u03b8t, and z\u0304T = (w\u0304 \u22a4 T , v\u0304 \u22a4 T , \u03b8\u0304 \u22a4 T ) \u22a4. With these notations, using convexity of \u2113(w) and \u03d5(v) and the monotonicity of operator F (\u00b7), we have for any z:\nf(u\u0304T )\u2212 f(u) + (z\u0304T \u2212 z)\u22a4F (z\u0304T ) \u2264 1\nT\nT\u2211\nt=1\n[f((w\u22a4t ,v \u22a4 t+1) \u22a4)\u2212 f(u) + (zt+1 \u2212 z)\u22a4F (zt+1)]\n= 1\nT\nT\u2211\nt=1\n[\u2113(wt) + \u03d5(vt+1)\u2212 \u2113(w)\u2212 \u03d5(v) + (zt+1 \u2212 z)\u22a4F (zt+1)].\nCombining this inequality with Lemma 1 at the optimal solution (w,v) = (w\u2217,v\u2217), we can derive\nf(u\u0304T )\u2212 f(u\u2217) + (z\u0304T \u2212 z\u2217) \u22a4 F (z\u0304T )\n\u2264 1\nT\nT \u2211\nt=1\n{1\n\u03b7 [B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)] +\n\u03b7\u2016gt\u2016 2 H\u2217t\n2 + \u3008\u03b4t,w\u2217 \u2212wt\u3009+\n\u03b2 2 (\u2016Aw\u2217 +Bvt \u2212 b\u2016 2\n\u2212 \u2016Aw\u2217 +Bvt+1 \u2212 b\u2016 2) +\n1\n2\u03b2 (\u2016\u03b8 \u2212 \u03b8t\u2016\n2 \u2212 \u2016\u03b8 \u2212 \u03b8t+1\u2016 2) }\n\u2264 1\nT\n{ T \u2211\nt=1\n[1\n\u03b7 [B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)] +\n\u03b7\u2016gt\u2016 2 H\u2217t\n2 + \u3008\u03b4t,w\u2217\u2212wt\u3009\n] + \u03b2\n2 \u2016Aw\u2217 +Bv1\u2212 b\u2016\n2 + 1\n2\u03b2 \u2016\u03b8\u2212 \u03b81\u2016\n2 }\n\u2264 1\nT\n{ T\u22121 \u2211\nt=0\n[1\n\u03b7 (B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)) +\n\u03b7\u2016gt\u2016 2 H\u2217t\n2 + \u3008\u03b4t,w\u2217 \u2212wt\u3009\n] + \u03b2\n2 D\n2 v\u2217,B +\n1\n2\u03b2 \u2016\u03b8 \u2212 \u03b81\u2016\n2 }\n.\nBecause the above inequality is valid for any \u03b8, it also holds in the ball B\u03c1 = {\u03b8 : \u2016\u03b8\u2016 \u2264 \u03c1}. Combining with the fact that the optimal solution must also be feasible, it follows that\nmax \u03b8\u2208B\u03c1\n{f(u\u0304T )\u2212 f(u\u2217) + (z\u0304T \u2212 z\u2217)\u22a4F (z\u0304T )}\n=max \u03b8\u2208B\u03c1\n{f(u\u0304T )\u2212 f(u\u2217) + \u03b8\u0304\u22a4T (Aw\u2217 +Bv\u2217 \u2212 b)\u2212 \u03b8\u22a4(Aw\u0304T +Bv\u0304T \u2212 b)}\n=max \u03b8\u2208B\u03c1\n{f(u\u0304T )\u2212 f(u\u2217)\u2212 \u03b8\u22a4(Aw\u0304T +Bv\u0304T \u2212 b)} = f(u\u0304T )\u2212 f(u\u2217) + \u03c1\u2016Aw\u0304T +Bv\u0304T \u2212 b\u2016.\nCombining the above two inequalities and taking expectation, we have\nE[f(u\u0304T )\u2212 f(u\u2217) + \u03c1\u2016Aw\u0304T +Bv\u0304T \u2212 b\u2016]\n\u2264 1 T E\n{ T\u2211\nt=1\n(1\n\u03b7 [B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)] + \u03b7\u2016gt\u20162H\u2217t 2 ) + \u3008\u03b4t,w\u2217 \u2212wt\u3009 ) + \u03b2 2 D2v\u2217,B + 1 2\u03b2 \u2016\u03b8 \u2212 \u03b81\u20162 }\n\u2264 1 2T\n{\nE\nT\u2211\nt=1\n[ 2\n\u03b7 [B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)] + \u03b7\u2016gt\u20162H\u2217t ] + \u03b2D 2 v\u2217,B +\n\u03c12\n\u03b2\n}\n,\nwhere we used the fact E\u03b4t = 0 in the last step. This completes the proof.\nThe above theorem allows us to derive regret bounds for a family of algorithms that iteratively modify the proximal functions \u03c6t in attempt to lower the regret bounds. Since the rate of convergence is still dependent on Ht and \u03b7, next we are going to choose appropriate positive definite matrix Ht and the constant \u03b7 to optimize the rate of convergence."}, {"heading": "2.4 Diagonal Matrix Proximal Functions", "text": "In this subsection, we restrict Ht as a diagonal matrix, for two reasons: (i) the diagonal matrix will provide results easier to understand than that for the general matrix; (ii) for high dimension problem the general matrix may result in prohibitively expensive computational cost, which is not desirable.\nFirstly, we notice that the upper bound in the Theorem 1 relies on \u2211T\nt=1 \u2016gt\u20162H\u2217t . If we assume all the gt\u2019s are known in advance, we could minimize this term by setting Ht = diag(s), \u2200t. We shall use the following proposition.\nProposition 1. For any g1,g2, . . . ,gT \u2208 Rd1 , we have\nmin diag(s) 0, 1\u22a4s\u2264c\nT\u2211\nt=1\n\u2016gt\u20162diag(s) = 1\nc\n( d1\u2211\ni=1\n\u2016g1:T,i\u2016 )2 ,\nwhere g1:T,i = (g1,i, . . . , gT,i) \u22a4 and the minimum is attained at si = c\u2016g1:T,i\u2016/ \u2211d1 j=1 \u2016g1:T,j\u2016.\nWe omit proof of this proposition, since it is easy to derive. Since we do not have all the gt\u2019s in advance, we receives the stochastic (sub)gradients gt sequentially instead. As a result, we propose to update the Ht incrementally as:\nHt = aI + diag(st),\nwhere st,i = \u2016g1:t,i\u2016 and a \u2265 0. For these Hts, we have the following inequality T\u2211\nt=1\n\u2016gt\u20162H\u2217t = T\u2211\nt=1\n\u3008gt, (aI + diag(st))\u22121gt\u3009 \u2264 T\u2211\nt=1\n\u3008gt, diag(st)\u22121gt\u3009 \u2264 2 d1\u2211\ni=1\n\u2016g1:T,i\u2016, (3)\nwhere the last inequality used the Lemma 4 in [3], which implies this update is a nearly optimal update method for the diagonal matrix case. Finally, the adaptive stochastic ADMM with diagonal matrix update (Ada-SADMMdiag) is summarized into the Algorithm 2.\nAlgorithm 2 Adaptive Stochastic ADMM with Diagonal Matrix Update (Ada-SADMMdiag).\nInitialize: w1 = 0, u1 = 0 , \u03b81 = 0, and a > 0. for t = 1, 2, . . . , T do Compute gt = \u2113\n\u2032(wt, \u03bet); Update Ht = aI + diag(st), where st,i = \u2016g1:t,i\u2016; wt+1 = argminw L\u03b2,t(w,vt, \u03b8t); vt+1 = argminv\u2208V L\u03b2,t(wt+1,v, \u03b8t); \u03b8t+1 = \u03b8t \u2212 \u03b2(Awt+1 +Bvt+1 \u2212 b);\nend for\nFor the convergence rate of the proposed Algorithm 2, we have the following specific theorem.\nTheorem 2. Let \u2113(w, \u03bet) and \u03d5(w) be convex functions for any t > 0. Then for Algorithm 2, we have the following inequality for any T \u2265 1 and \u03c1 > 0\nE[f(u\u0304T )\u2212 f(u\u2217) + \u03c1\u2016Aw\u0304T +Bv\u0304T \u2212 b\u2016]\n\u2264 1 2T\n(\nE[2\u03b7\nd1\u2211\ni=1\n\u2016g1:T,i\u2016+ 2\n\u03b7 max t\u2264T\n\u2016wt \u2212w\u2217\u20162\u221e d1\u2211\ni=1\n\u2016g1:T,i\u2016] + \u03b2D2v\u2217,B + \u03c12\n\u03b2\n)\n.\nIf we further set \u03b7 = Dw,\u221e/ \u221a 2 where Dw,\u221e = maxw,w\u2032 \u2016w \u2212w\u2032\u2016\u221e, then we have\nE[f(u\u0304T )\u2212 f(u\u2217) + \u03c1\u2016Aw\u0304T +Bv\u0304T \u2212 b\u2016] \u2264 1\nT\n(\u221a 2E[Dw,\u221e d1\u2211\ni=1\n\u2016g1:T,i\u2016] + \u03b2\n2 D2v\u2217,B +\n\u03c12\n2\u03b2\n) .\nProof. We have the following inequality\n2 T\u2211\nt=1\n[B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)] = T\u2211\nt=1\n(\u2016wt \u2212w\u2217\u20162Ht \u2212 \u2016wt+1 \u2212w\u2217\u2016 2 Ht)\n\u2264 \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\n(\u2016wt+1 \u2212w\u2217\u20162Ht+1 \u2212 \u2016wt+1 \u2212w\u2217\u2016 2 Ht)\n= \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\n\u3008wt+1 \u2212w\u2217, diag(st+1 \u2212 st)wt+1 \u2212w\u2217\u3009\n\u2264 \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\nmax i\n(wt+1,i \u2212w\u2217,i)2\u2016st+1 \u2212 st\u20161\n= \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\n\u2016wt+1 \u2212w\u2217\u20162\u221e(st+1 \u2212 st)\u22a41\n\u2264 \u2016w1 \u2212w\u2217\u20162H1 +maxt\u2264T \u2016wt \u2212w\u2217\u2016 2 \u221es \u22a4 T 1\u2212 \u2016w1 \u2212w\u2217\u20162\u221es\u22a41 1 \u2264 max t\u2264T \u2016wt \u2212w\u2217\u20162\u221e\nd1\u2211\ni=1\n\u2016g1:T,i\u2016,\nwhere the last inequality used \u3008sT ,1\u3009 = \u2211d1 i=1 \u2016g1:T,i\u2016 and \u2016w1 \u2212w\u2217\u20162H1 \u2264 \u2016w1 \u2212w\u2217\u20162\u221es\u22a41 1. Plugging the above inequality and the inequality (4) into the inequality (2), will conclude the first part of the theorem. Then the second part is trivial to be derived.\nRemark 3. For the example of sparse random data, assume that at each round t, feature i appears with probability pi = min {1, ci\u2212\u03b1} for some \u03b1 \u2265 2 and a constant c. Then\nE[\nd\u2211\ni=1\n\u2016g1:T,i\u2016] = d\u2211\ni=1\nE[ \u221a |{t : |gt,i| = 1}|] \u2264 d\u2211\ni=1\n\u221a E|{t : |gt,i| = 1}| = d\u2211\ni=1\n\u221a\nTpi.\nIn this case, the convergence rate equals O( log d\u221a T )."}, {"heading": "2.5 Full Matrix Proximal Functions", "text": "In this subsection, we derive and analyze new updates when we estimate a full matrix Ht for the proximal function instead of a diagonal one. Although full matrix computation may not be attractive for high dimension problems, it may be helpful for tasks with low dimension. Furthermore, it will provide us with a more complete insight. Similar with the analysis for the diagonal case, we first introduce the following proposition (Lemma 15 in [3]).\nProposition 2. For any g1,g2, . . . ,gT \u2208 Rd1 , we have the following inequality\nmin S 0, tr(S)\u2264c\nT\u2211\nt=1\n\u2016gt\u20162S\u22121 = 1\nc tr(GT )\nwhere, GT = \u2211T t=1 gtg \u22a4 t . and the minimizer is attained at S = cG 1/2 T /tr(G 1/2 T ). If GT is not of full rank, then we use its pseudo-inverse to replace its inverse in the minimization problem.\nBecause the (sub)gradients are received sequentially, we propose to update the Ht incrementally as\nHt = aI +G 1 2 t ,\nwhere Gt = \u2211t i=1 gig \u22a4 i , t = 1, . . . , T . For these Hts, we have the following inequalities\nT\u2211\nt=1\n\u2016gt\u20162H\u2217t \u2264 T\u2211\nt=1\n\u2016 gt \u20162S\u22121t \u2264 2 T\u2211\nt=1\n\u2016 gt \u20162S\u22121 T = 2tr(G 1/2 T ), (4)\nwhere the last inequality used the Lemma 10 in [3], which implies this update is a nearly optimal update method for the full matrix case. Finally, the adaptive stochastic ADMM with full matrix update can be summarized into the Algorithm 3.\nAlgorithm 3 Adaptive Stochastic ADMM with Full Matrix Update (Ada-SADMMfull).\nInitialize: w1 = 0, u1 = 0, \u03b81 = 0, G0 = 0, and a > 0 for t = 1, 2, . . . , T do Compute gt = \u2113 \u2032(wt, \u03bet) and update Gt = Gt\u22121 + gtg\u22a4t ;\nUpdate Ht = aI + St, where St = G 1 2\nt ; wt+1 = argminw L\u03b2,t(w,vt, \u03b8t); vt+1 = argminv\u2208V L\u03b2,t(wt+1,v, \u03b8t); \u03b8t+1 = \u03b8t \u2212 \u03b2(Awt+1 +Bvt+1 \u2212 b);\nend for\nFor the convergence rate of the above proposed Algorithm 3, we have the following specific theorem.\nTheorem 4. Let l(w, \u03bet) and \u03d5(w) are convex functions for any t > 0. Then for Algorithm 3, we have the following inequality for any T \u2265 1, \u03c1 > 0,\nE[f(u\u0304T )\u2212 f(u\u2217) + \u03c1 \u2016 Aw\u0304T +Bv\u0304T \u2212 b \u2016]\n\u2264 1 2T ( E[2\u03b7tr (G 1/2 T ) + 1 \u03b7 maxt\u2264T \u2016w\u2217 \u2212wt\u20162tr (G 1 2 T )] + \u03b2D 2 v\u2217,B +\n\u03c12\n\u03b2\n) .\nFurthermore, if we set \u03b7 = Dw,2/2, where Dw,2 = maxw1,w2 \u2016w1 \u2212w2\u2016, then we have\nE[f(u\u0304T )\u2212 f(u\u2217) + \u03c1\u2016Aw\u0304T +By\u0304T \u2212 b\u2016] \u2264 1\nT\n(\u221a 2E[Dw,2tr (G 1/2 T )] + \u03b2\n2 D2v\u2217,B +\n\u03c12\n2\u03b2\n) .\nProof. We consider the sum of the difference\n2\nT\u2211\nt=1\n[B\u03c6t(wt,w\u2217)\u2212 B\u03c6t(wt+1,w\u2217)] = T\u2211\nt=1\n(\u2016wt \u2212w\u2217\u20162Ht \u2212 \u2016wt+1 \u2212w\u2217\u20162Ht)\n\u2264 \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\n(\u2016wt+1 \u2212w\u2217\u20162Ht+1 \u2212 \u2016wt+1 \u2212w\u2217\u2016 2 Ht)\n= \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\n\u3008wt+1 \u2212w\u2217, (G 1 2 t+1 \u2212G 1 2 t )(wt+1 \u2212w\u2217)\u3009\n\u2264 \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\n\u2016wt+1 \u2212w\u2217\u20162\u03bbmax(G 1 2 t+1 \u2212G 1 2 t )\n= \u2016w1 \u2212w\u2217\u20162H1 + T\u22121\u2211\nt=1\n\u2016wt+1 \u2212w\u2217\u20162tr(G 1 2 t+1 \u2212G 1 2 t )\n\u2264 \u2016w1 \u2212w\u2217\u20162H1 + maxt\u2264T\u22121 \u2016wt \u2212w\u2217\u2016 2tr(G\n1 2 T )\u2212 \u2016w1 \u2212w\u2217\u20162tr(G 1 2\n1 ) \u2264 max t\u2264T\n\u2016wt \u2212w\u2217\u20162tr(G 1 2 T ).\nPlugging the above inequality and the inequality (4) into the inequality (2), will conclude the first part of the theorem. Then the second part is trivial to be derived."}, {"heading": "3 Experiment", "text": "In this section, we will evaluate the empirical performance of the proposed adaptive stochastic ADMM algorithms for solving GGSVM tasks, which is formulated as the following problem [15]:\nmin w,v\n1\nn\nn\u2211\ni=1\n[1\u2212 yix\u22a4i w]+ + \u03b3\n2 \u2016w\u20162 + \u03bd\u2016v\u20161, s.t. Fw\u2212 v = 0,\nwhere [z]+ = max(0, z) and the matrix F is constructed based on a graph G = {V , E}. For this graph, V = {w1, . . . , wd1} is a set of variables and E = {e1, . . . , e|E|}, where ek = {i, j} is assigned with a weight \u03b1ij . And the corresponding F is in the form: Fki = \u03b1ij and Fkj = \u2212\u03b1ij . To construct a graph for a given dataset, we adopt the sparse inverse covariance estimation [5] and determine the sparsity pattern of the inverse covariance matrix \u03a3\u22121. Based on the inverse covariance matrix, we connect all index pairs (i, j) with \u03a3\u22121ij 6= 0 and assign \u03b1ij = 1."}, {"heading": "3.1 Experimental Testbed and Setup", "text": "To examine the performance, we test all the algorithms on 6 real-world datasets from web machine learning repositories, which are listed in the Table 1. \u201cnews20\u201d is the \u201c20 Newsgroups\u201d downloaded from 1, while the other datasets can be downloaded from LIBSVM website2. For each dataset, we randomly divide it into two folds: training set with 80% of examples and test set with the rest.\nTo make a fair comparison, all algorithms adopt the same experimental setup. In particular, we set the penalty parameter \u03b3 = \u03bd = 1/n, where n is the number of training examples, and the trade-off parameter \u03b2 = 1. In addition, we set the step size parameter \u03b7t = 1/(\u03b3t) for SADMM according to the theorem 2 in [15]. Finally, the smooth parameter a is set as 1, and the step size for adaptive stochastic ADMM algorithms are searched from 2[\u22125:5] using cross validation.\nAll the experiments were conducted with 5 different random seeds and 2 epochs (2n iterations) for each dataset. All the result were reported by averaging over these 5 runs. We evaluated the learning performance by measuring objective values, i.e., f(u), and test error rates on the test datasets. In addition, we also evaluate computational efficiency of all the algorithms by their running time. All experiments were run in Matlab over a machine of 3.4GHz CPU."}, {"heading": "3.2 Performance Evaluation", "text": "The figure 1 shows the performance of all the algorithms in comparison over trials, from which we can draw several observations. Firstly, the left column shows the objective values of the three algorithms. We can observe that the two adaptive stochastic ADMM algorithms converge much faster than SADMM, which shows the effectiveness of exploration of adaptive (sub)gradient to accelerate stochastic ADMM. Secondly, compared with Ada-SADMMdiag, Ada-SADMMfull achieves slightly smaller objective values on most of the datasets, which indicates full matrix is slightly more informative than the diagonal one. Thirdly, the central column provides test error rates of three algorithms, where we observe that the two adaptive algorithms achieve significantly smaller or comparable test error rates at 0.25-th epoch than SADMM at 2-th epoch.\n1http://www.cs.nyu.edu/~roweis/data.html 2http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets\nThis observation indicates that we can terminate the two adaptive algorithms earlier to save time and at the same time achieve similar performance compared with SADMM. Finally, the right column shows the running time of three algorithms, which shows that during the learning process, the Ada-SADMMfull is significantly slower while the Ada-SADMMdiag is overall efficient compared with SADMM. In summary, the Ada-SADMMdiag algorithm achieves a good trade-off between the efficiency and effectiveness.\nTable 2 summarizes the performance of all the compared algorithms over the 6 datasets, from which we can make similar observations. This again verifies the effectiveness of the proposed algorithms."}, {"heading": "4 Conclusion", "text": "ADMM is a popular technique in machine learning. This paper studied to accelerate stochastic ADMM with adaptive subgradient, by replacing the fixed proximal function with adaptive proximal function. Compared with traditional stochastic ADMM, we show that the proposed adaptive algorithms converge significantly faster through the proposed adaptive strategies. Promising experimental results on a variety of real-world datasets further validate the effectiveness of our techniques."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["Wei Deng", "Wotao Yin"], "venue": "Technical report, DTIC Document,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "On the douglasrachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["Jonathan Eckstein", "Dimitri P Bertsekas"], "venue": "Mathematical Programming,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Chapter ix applications of the method of multipliers to variational inequalities", "author": ["Daniel Gabay"], "venue": "Studies in mathematics and its applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1983}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["Daniel Gabay", "Bertrand Mercier"], "venue": "Computers & Mathematics with Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1976}, {"title": "Sur lapproximation, par elements nis dordre un, et la resolution, par penalisationdualite, dune classe de problems de dirichlet non lineares", "author": ["R. Glowinski", "A. Marroco"], "venue": "Revue Francaise dAutomatique, Informatique,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1975}, {"title": "Augmented Lagrangian and operator-splitting methods in nonlinear mechanics, volume", "author": ["Roland Glowinski", "Patrick Le Tallec"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Fast alternating linearization methods for minimizing the sum of two convex functions", "author": ["Donald Goldfarb", "Shiqian Ma", "Katya Scheinberg"], "venue": "Math. Program.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "The split bregman method for l1-regularized problems", "author": ["Tom Goldstein", "Stanley Osher"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "On the o(1/n) convergence rate of the douglas-rachford alternating direction method", "author": ["Bingsheng He", "Xiaoming Yuan"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["Zhi-Quan Luo"], "venue": "arXiv preprint arXiv:1208.3922,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Iteration-complexity of block-decomposition algorithms and the alternating direction method of multipliers", "author": ["Renato D.C. Monteiro", "Benar Fux Svaiter"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Stochastic alternating direction method of multipliers", "author": ["Hua Ouyang", "Niao He", "Long Tran", "Alexander G Gray"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Dual averaging and proximal gradient descent for online alternating direction multiplier method", "author": ["Taiji Suzuki"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Online alternating direction method", "author": ["Huahua Wang", "Arindam Banerjee"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Alternating direction algorithms for l1-problems in compressive sensing", "author": ["Junfeng Yang", "Yin Zhang"], "venue": "SIAM journal on scientific computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 40, "endOffset": 46}, {"referenceID": 6, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 40, "endOffset": 46}, {"referenceID": 5, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 215, "endOffset": 224}, {"referenceID": 8, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 215, "endOffset": 224}, {"referenceID": 3, "context": "1 Introduction Originally introduced in [8, 7], the offline/batch Alternating Direction Method of Multipliers (ADMM) stemmed from the augmented Lagrangian method, with its global convergence property established in [6, 9, 4].", "startOffset": 215, "endOffset": 224}, {"referenceID": 13, "context": "Recent studies have shown that ADMM achieves a convergence rate of O(1/T ) [14, 12] (where T is number of iterations of ADMM), when the objective function is generally convex.", "startOffset": 75, "endOffset": 83}, {"referenceID": 11, "context": "Recent studies have shown that ADMM achieves a convergence rate of O(1/T ) [14, 12] (where T is number of iterations of ADMM), when the objective function is generally convex.", "startOffset": 75, "endOffset": 83}, {"referenceID": 12, "context": "Furthermore, ADMM enjoys a convergence rate of O(\u03b1 ), for some \u03b1 \u2208 (0, 1), when the objective function is strongly convex and smooth [13, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 1, "context": "Furthermore, ADMM enjoys a convergence rate of O(\u03b1 ), for some \u03b1 \u2208 (0, 1), when the objective function is strongly convex and smooth [13, 2].", "startOffset": 133, "endOffset": 140}, {"referenceID": 17, "context": "ADMM has shown attractive performance in a wide range of real-world problems such as compressed sensing [18], image restoration [11], video processing, and matrix completion [10], etc.", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "ADMM has shown attractive performance in a wide range of real-world problems such as compressed sensing [18], image restoration [11], video processing, and matrix completion [10], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "ADMM has shown attractive performance in a wide range of real-world problems such as compressed sensing [18], image restoration [11], video processing, and matrix completion [10], etc.", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "The online ADMM (OADMM) algorithm [17] was proposed to tackle the computational challenge.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Interestingly, although the optimization of the loss function is assumed to be easy in the analysis of [17], it is actually not necessarily easy in practice.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "To address this issue, the stochastic ADMM algorithm was proposed, by linearizing the the online loss function [15, 16].", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "To address this issue, the stochastic ADMM algorithm was proposed, by linearizing the the online loss function [15, 16].", "startOffset": 111, "endOffset": 119}, {"referenceID": 14, "context": "In the previous work [15, 16] the Bregman divergence is derived from a simple second order function, i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 15, "context": "In the previous work [15, 16] the Bregman divergence is derived from a simple second order function, i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 2, "context": ", the half squared norm, which could be a suboptimal choice [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "When \u03be is deterministic, the above optimization becomes the traditional problem formulation of ADMM [1].", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "It is easy to see that this proposed approximation includes the one proposed by [15] as a special case when Ht = I.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "To provide an upper bound for the first term Lt, taking D(u,v) = B\u03c6t(u,v) = 1 2\u2016u \u2212 v\u2016Ht and applying Lemma 1 in [15] to the step of getting wt+1 in the Algorithm 1, we will have \u3008l(wt, \u03bet) +A[\u03b2(Awt+1 +Bvt \u2212 b)\u2212 \u03b8t],wt+1 \u2212w\u3009 \u2264 1 \u03b7 [B\u03c6t(wt,w)\u2212 B\u03c6t(wt+1,w)\u2212 B\u03c6t(wt+1,wt)].", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "where the last inequality used the Lemma 4 in [3], which implies this update is a nearly optimal update method for the diagonal matrix case.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "Similar with the analysis for the diagonal case, we first introduce the following proposition (Lemma 15 in [3]).", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "where the last inequality used the Lemma 10 in [3], which implies this update is a nearly optimal update method for the full matrix case.", "startOffset": 47, "endOffset": 50}, {"referenceID": 14, "context": "3 Experiment In this section, we will evaluate the empirical performance of the proposed adaptive stochastic ADMM algorithms for solving GGSVM tasks, which is formulated as the following problem [15]:", "startOffset": 195, "endOffset": 199}, {"referenceID": 4, "context": "To construct a graph for a given dataset, we adopt the sparse inverse covariance estimation [5] and determine the sparsity pattern of the inverse covariance matrix \u03a3\u22121.", "startOffset": 92, "endOffset": 95}, {"referenceID": 14, "context": "In addition, we set the step size parameter \u03b7t = 1/(\u03b3t) for SADMM according to the theorem 2 in [15].", "startOffset": 96, "endOffset": 100}], "year": 2014, "abstractText": "The Alternating Direction Method of Multipliers (ADMM) has been studied for years. The traditional ADMM algorithm needs to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the time complexity, stochastic ADMM algorithms were proposed to replace the expected function with a random loss function associated with one uniformly drawn example plus a Bregman divergence. The Bregman divergence, however, is derived from a simple second order proximal function, the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal second order proximal functions, which produce a new family of adaptive subgradient methods. We theoretically prove that their regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}