{"id": "1312.5023", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2013", "title": "Contextually Supervised Source Separation with Application to Energy Disaggregation", "abstract": "We propose a new framework for single-channel source separation that lies between the fully supervised and unsupervised setting. Instead of supervision, we provide input features for each source signal and use convex methods to estimate the correlations between these features and the unobserved signal decomposition. We analyze the case of $\\ell_2$ loss theoretically and show that recovery of the signal components depends only on cross-correlation between features for different signals, not on correlations between features for the same signal. Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision; our motivating application is energy disaggregation of hourly smart meter data (the separation of whole-home power signals into different energy uses). Here we apply contextual supervision to disaggregate the energy usage of thousands homes over four years, a significantly larger scale than previously published efforts, and demonstrate on synthetic data that our method outperforms the unsupervised approach. We use a similar approach to the control of the intensity of the distributed time signal (N2) on two different signals. As a result of this, we investigate the relationship between the intensity of the distributed time signal and the N2 data.\n\n\n\nThe current design of a supervised source separation was conceived as a system for isolating signal-based energy consumption of large energy resources (eg, the distributed time signal). The design focused solely on energy consumption, which is a consequence of the very efficient design and a fundamental role of the power source. The new supervised source separation is a relatively small scale compared to the earlier versions, but uses a variety of methods and methods to generate the energy generated from each signal. However, a common approach is to build a distributed time signal on multiple sources, or separate data sources, but rather focus on energy consumption, as these sources are a common feature in the power source. The following approach is applied for all four main sources.\n\nA distributed time signal can be summed up as\n\n\n\n\n\n\nThe distribution is a direct result of\n\n\n\n\n\n\nThe signal decomposition\n\n\n\n\nThe distribution is a direct result of\n\n\n\nWe propose a fully supervised source separation as a rule for the continuous and distributed time signal. The network is a simple system that is based on both the supervised and unsupervised power source, and a subset of the energy consumption from the distributed time signal.\nAn important role of the supervised source separation is to capture the energy spent in a distributed time signal, and to capture", "histories": [["v1", "Wed, 18 Dec 2013 02:21:01 GMT  (69kb,D)", "http://arxiv.org/abs/1312.5023v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["matt wytock", "j zico kolter"], "accepted": true, "id": "1312.5023"}, "pdf": {"name": "1312.5023.pdf", "metadata": {"source": "CRF", "title": "Contextually Supervised Source Separation with Application to Energy Disaggregation", "authors": ["Matt Wytock"], "emails": ["mwytock@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "We consider the single-channel source separation problem, in which we wish to separate a single aggregate signal into mixture of unobserved component signals. Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal. However, both settings have potential drawbacks: for many problems, including energy disaggregation\u2014which looks to separate individual energy uses from a whole-home power signal [9]\u2014it can be difficult to obtain training data with the true separated signals needed for the supervised setting; in contrast, the unsupervised setting is an ill-defined problem with arbitrarily many solutions, and thus algorithms are highly task-dependent.\nIn this work, we propose an alternative approach that lies between these two extremes. We propose a framework of contextual supervision, whereby along with the input signal to be separated, we provide contextual features correlated with the unobserved component signals. In practice, we find that this is often much easier than providing a fully supervised training set; yet it also allows for a well-defined problem, unlike the unsupervised setting. The approach is a natural fit for energy disaggregation, since we have strong correlations between energy usage and easily observed context\u2014air conditioning spikes in hot summer months, lighting increases when there is a lack of sunlight, etc. We formulate our model directly as an optimization problem in which we jointly estimate these correlations along with the most likely source separation. Theoretically, we show that when the contextual features are relatively uncorrelated between different groups, we can recover the correct separation with high probability. We demonstrate that our model recovers the correct separation on synthetic examples resembling the energy disaggregation task and apply the method\nar X\niv :1\n31 2.\n50 23\nv1 [\nst at\n.M L\n] 1\n8 D\nto the task of separating sources of energy usage for thousands of homes over 4 years, a scale much larger than previously published efforts. The main contributions of this paper are 1) the proposed contextually supervised setting and the optimization formulation; 2) the theoretical analysis showing that accurate separation only requires linear independence between features for different signals; and 3) the application of this approach to the problem of energy disaggregation, a task with significant potential to help inform consumers about their energy behavior, thereby increasing efficiency."}, {"heading": "2 Related work", "text": "As mentioned above, work in single-channel source separation has been separated along the lines of supervised and unsupervised algorithms, although several algorithms can be applied to either setting. A common strategy is to separate the observed aggregate signal into a linear combination of several bases, where different bases correspond to different components of the signal; algorithms such as Probabilistic Latent Component Analysis (PLCA) [20], sparse coding [15], and factorial hidden Markov models (FHMMs) [8] all fall within this category, with the differences concerning 1) how bases are represented and assigned to different signal components and 2) how the algorithm infers the activation of the different bases given the aggregate signal. For example, PLCA typically uses pre-defined basis functions (commonly Fourier or Wavelet bases), with a probabilistic model for how sources generate different bases; sparse coding learns bases tuned to data while encouraging sparse activations; and FHMMs use hidden Markov models to represent each source. In the supervised setting, one typically uses the individual signals to learn parameters for each set of bases (e.g., PLCA will learn which bases are typical for each signal), whereas unsupervised methods learn through an EM-like procedure or by maximizing some separation criteria for the learned bases. The method we propose here is conceptually similar, but the nature of these bases is rather different: instead of fixed bases with changing activations, we require features that effectively generate time-varying bases and learn activations that are constant over time.\nOrthogonal to this research, there has also been a great deal of work in multi-channel blind source separation problems, where we observe multiple mixings of the same sources (typically, as many mixings as there are signals) rather than in isolation. These methods can exploit significantly more structure and algorithms like Independent Component Analysis [4, 2] can separate signals with virtually no supervised information. However, when applied to the single-channel problem (when this is even possible), they typically perform substantially worse than methods which exploit structure in the problem, such as those described above.\nFrom the applied point of view, algorithms for energy disaggregation have received growing interest in recently years [11, 22, 12, 16]. This is an important task since many studies have shown that consumers naturally adopt energy conserving behaviors when presented with a breakdown of their energy usage [5, 14, 7]. Algorithmic approaches to disaggregation are appealing as they allow for these types of breakdowns, but existing disaggregation approaches virtually all use high-frequency sampling of the whole-building power signal (e.g. per second) requiring the installation of custom monitoring hardware for data collection. In contrast, this work focuses on disaggregation using data from \u201csmart meters\u201d, communication-enabled power meters that are currently installed in more than 32 million homes [21], but are limited to recording usage at low frequencies (every 15 minutes or hour), leading to a substantially different set of challenges. Smart meters are relatively new and deployment is ongoing, but due to the large amount of data available now and in the near future, successful disaggregation has the potential to have a profound impact on energy efficiency."}, {"heading": "3 Optimization Formulation", "text": "We begin by formulating the optimization problem for contextual source separation. Formally, we assume there is some unknown matrix of k component signals\nY \u2208 RT\u00d7k = [ | | | y1 y2 \u00b7 \u00b7 \u00b7 yk | | | ] (1)\nfrom which we observe the sum y\u0304 = \u2211k i=1 yi. For example, in our disaggregation setting, yi \u2208 RT could denote a power trace (with T total readings) for a single type of appliance, such as the air\nconditioning, lighting, or electronics, and y\u0304 denotes the sum of all these power signals, which we observe from a home\u2019s power meter.\nIn our proposed model, we represent each individual component signal yi as a linear function of some component-specific bases Xi \u2208 RT\u00d7ni\nyi \u2248 Xi\u03b8i (2)\nwhere \u03b8i \u2208 Rni are the signal\u2019s coefficients. The formal objective of our algorithm is: given the aggregate signal y\u0304 and the component featuresXi, i = 1, . . . , k, estimate both the parameters \u03b8i and the unknown source components yi. We cast this as an optimization problem\nminimize Y,\u03b8 k\u2211 i=1 {`i(yi, Xi\u03b8i) + gi(yi) + hi(\u03b8i)}\nsubject to k\u2211 i=1 yi = y\u0304\n(3)\nwhere `i : RT \u00d7 RT \u2192 R is a loss function penalizing differences between the ith reconstructed signal and its linear representation; gi is a regularization term encoding the \u201clikely\u201d form of the signal yi, independent of the features; and hi is a regularization penalty on \u03b8i. Choosing `i, gi and hi to be convex functions results in a convex optimization problem.\nA natural choice of loss function `i is a norm penalizing the difference between the reconstructed signal and its features \u2016yi \u2212 Xi\u03b8i\u2016, but since our formulation enables loss functions that depend simultaneously on all T values of the signal, we allow for more complex choices as well. For example in the energy disaggregation problem, air conditioning is correlated with high temperature but does not respond to outside temperature changes instantaneously; thermal mass and the varying occupancy in buildings often results in air conditioning usage that correlates with high temperature over some window (for instance, if no one is in a room during a period of high temperature, we may not use electricity then, but need to \u201cmake up\u201d for this later when someone does enter the room). Thus, the loss function `i(yi, Xi\u03b8i) = \u2016(yi \u2212Xi\u03b8i)(I \u2297 1T )\u201622 (4) which penalizes the aggregate difference of yi and Xi\u03b8i over a sliding window, can be used to capture such dynamics. In many settings, it may also make sense to use `1 or `\u221e rather than `2 loss, depending on the nature of the source signal.\nLikewise, since the objective term gi depends on all T values of yi, we can use it to encode the likely dynamics of the source signal independent of Xi\u03b8i. For air conditioning and other single appliance types, we expect transitions between on/off states which we can encode by penalizing the `1 norm of Dyi where D is the linear difference operator subtracting (yi)t\u22121 \u2212 (yi)t. For other types of energy consumption, for example groups of many electronic appliances, we expect the signal to have smoother dynamics and thus `2 loss is more appropriate. Finally, we also include hi for statistical regularization purposes\u2014but for problems where T ni, such as the ones we consider in our experiments, the choice of hi is less important."}, {"heading": "4 Theoretical analysis", "text": "Next, we consider the ability of our model to recover the true source signals as T grows while k and ni remain fixed. For the purposes of this section only, we restrict our attention to the choice of `2 loss, no gi or hi terms, and Gaussian noise (the extension to the sub-Gaussian case is straightforward). We show that under this specialization of the model, the optimization problem recovers the underlying signals at a rate dependent on the linear independence between blocks of input features Xi. In practice, the choice of `i, gi and hi is problem-specific, but `2 loss is a reasonable default and while simplifying the theoretical analysis dramatically, captures the essential behavior of the model in the large T regime.\nFormally, for this section we assume the source signals have Gaussian noise\nyi = Xi\u03b8 ? i + wi (5)\nfor some \u03b8?i \u2208 Rni and wi \u223c N (0, \u03c32i I). Under the choice of `2 loss, our optimization problem simplifies to\nminimize Y,\u03b8\n\u2016Y 1\u2212X\u03b8\u201622\nsubject to Y 1 = y\u0304 (6)\nwhere 1 \u2208 Rk is the all-ones vector, \u03b8 \u2208 Rn is a concatenation of all the \u03b8i\u2019s, X \u2208 RT\u00d7n is a concatenation of all the Xi\u2019s and n = \u2211k i=1 ni is the total number of features. In particular, estimation of \u03b8 is equivalent to least-squares\n\u03b8\u0302 \u2208 Rn = arg min \u03b8 \u2016y\u0304 \u2212X\u03b8\u201622 = (XTX)\u22121XT y\u0304. (7)\nSince each yi has it\u2019s own noise term, we can never expect to recover yi exactly, but we can recover the true \u03b8? with analysis that is the same as for standard linear regression. However, in the context of source separation, we are interested in the recovery of the \u201cnoiseless\u201d yi, Xi\u03b8?i , and thus in our analysis we consider how the root mean squared error\nRMSE(Xi\u03b8\u0302i) =\n\u221a 1\nT \u2225\u2225\u2225Xi\u03b8\u0302i \u2212Xi\u03b8?i \u2225\u2225\u22252 2\n(8)\nvanishes for large T ; indeed, a key feature of our analysis is to show that we may recover the underlying signals faster than \u03b8?i .\nTheorem 1. Given data generated by the model (5), and estimating \u03b8\u0302 via (7), we have that E [ \u2016Xi\u03b8\u0302i \u2212Xi\u03b8?\u201622 ] = \u03c32 trXTi Xi(X TX)\u22121ii \u2264 \u03c3 2ni\u03c1i (9)\nwhere \u03c32 = \u2211k i=1 \u03c3 2 i and \u03c1i = \u03bbmax(X T i Xi(X\nTX)\u22121ii ). Furthermore, for \u03b4 \u2264 0.1, with probability greater than 1\u2212 \u03b4\nRMSE(Xi\u03b8\u0302i) \u2264 \u221a 4\u03c32ni\u03c1i log(1/\u03b4)\nT . (10)\nA key quantity in this theorem is the matrix XTi Xi(X TX)\u22121ii \u2208 Rni\u00d7ni ; (XTX) \u22121 ii denotes the i, i block of the full inverse (XTX)\u22121 (i.e., first inverting the joint covariance matrix of all the features, and then taking the i, i block), and this term provides a measure of the linear independence between features corresponding to different signals. To see this, note that if features across different signals are orthogonal, XTX is block diagonal, and thus XTi Xi(X TX)\u22121ii = X T i Xi(X T i Xi)\n\u22121 = I , so \u03c1i = 1. Alternatively, if two features provided for different signals are highly correlated, entries of (XTX)\u22121ii will have large magnitude that is not canceled by X T i Xi and \u03c1i will be large. This formalizes an intuitive notion for contextually supervised source separation: for recovering the underlying signals, it does not matter if two features for the same signal are highly correlated (this contrasts to the case of recovering \u03b8? itself which depends on all correlations), but two correlated signals for different features make estimation difficult; intuitively, if two very similar features are provided for two different source signals, attribution becomes difficult. A particularly useful property of these bounds is that all terms can be computed using just Xi, so we can estimate recovery rates when choosing our design matrix."}, {"heading": "4.1 Proofs", "text": "The proof of Theorem 1 proceeds in two steps. First, using rules for linear transformations of Gaussian random variables, we show that the quantity Xi(\u03b8\u0302i \u2212 \u03b8?i ) is also (zero-mean) Gaussian, which immediately leads to (9). Second, we derive a tail bound on the probability that Xi(\u03b8\u0302i \u2212 \u03b8?i ) exceeds some threshold, which leads to the sample complexity bound (10); because this quantity has a singular covariance matrix, this requires a slightly specialized probability bound, given by the following lemma. Lemma 1. Suppose x \u2208 Rp \u223c N (0,\u03a3) with rank(\u03a3) = n. Then\nP ( \u2016x\u201622 \u2265 t ) \u2264 ( t\nn\u03bb\n)n/2 exp { \u22121\n2 (t/\u03bb\u2212 n)\n} (11)\nwhere \u03bb is the largest eigenvalue of \u03a3.\nProof. By Chernoff\u2019s bound\nP ( \u2016x\u201622 \u2265 t ) \u2264\nE [ e\u03b1\u2016x\u2016 2 2 ] e\u03b1t . (12)\nfor any \u03b1 \u2265 0. For any > 0, z \u223c N (0,\u03a3 + I),\nE [ e\u03b1\u2016z\u2016 2 2 ] = (2\u03c0)\u2212p/2|\u03a3 + I|\u22121/2 \u222b exp { \u22121\n2 zT (\u03a3 + I)\u22121z + \u03b1zT z\n} dz\n= (2\u03c0)\u2212p/2|\u03a3 + I|\u22121/2 \u222b exp { \u22121\n2 zT (\u03a3 + I)\u22121(I \u2212 2\u03b1(\u03a3 + I)\u22121)z\n} dz\n= (2\u03c0)\u2212p/2|\u03a3 + I|\u22121/2(2\u03c0)p/2|\u03a3 + I|1/2|I \u2212 2\u03b1(\u03a3 + I)|\u22121/2\n(13)\nso taking the limit \u2192 0, we have that E[e\u03b1\u2016x\u201622 ] = |I \u2212 2\u03b1\u03a3|\u22121/2. Since \u03a3 has only n nonzero eigenvalues,\n|I \u2212 2\u03b1\u03a3| = n\u220f i=1 (1\u2212 2\u03b1\u03bbi) \u2265 (1\u2212 2\u03b1\u03bb)n (14)\nand so P ( \u2016x\u201622 \u2265 t ) \u2264 1\n(1\u2212 2\u03b1\u03bb)n/2e\u03b1t . (15)\nMinimizing this expression over \u03b1 gives \u03b1 = t\u2212n\u03bb2t\u03bb and substituting this into the equation above gives the desired bound.\nTo write the problem more compactly, we define the block matrix W \u2208 RT\u00d7k with columns wi, and define the \u201cblock-diagonalization\u201d operator B : Rn \u2192 Rn\u00d7k as\nB(\u03b8) =  \u03b81 0 \u00b7 \u00b7 \u00b7 0 0 \u03b82 \u00b7 \u00b7 \u00b7 0 ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 \u03b8k  . (16) Proof. (of Theorem 1) Since Y = XB(\u03b8?) +W ,\nXB(\u03b8\u0302) = XB ( (XTX)\u22121XT (XB(\u03b8?) +W )1 ) = XB(B(\u03b8?))1 +XB ( (XTX)\u22121XTW1\n) = XB(\u03b8?) +XB ( (XTX)\u22121XTW1\n) (17) For simplicity of notation we also denote\nu \u2208 Rn \u2261 (XTX)\u22121XTW1. (18)\nThus XB(\u03b8?)\u2212XB(\u03b8\u0302) = XB(u). (19)\nNow, using rules for Gaussian random variables under linear transformations W1 \u223c N (0, \u03c32IT ) and so u \u223c N (0, \u03c32(XTX)\u22121). Finally, partioning u1, u2, . . . , uk conformally with \u03b8,\nXi\u03b8 ? \u2212Xi\u03b8\u0302i = Xiui \u223c N (0, \u03c32Xi(XTX)\u22121ii X T i ) (20)\nso\nE [\u2225\u2225\u2225Xi\u03b8? \u2212Xi\u03b8\u0302i\u2225\u2225\u22252 2 ] = \u03c32 trXTi Xi(X TX)\u22121ii . (21)\nSince \u03c32Xi(XTX)\u22121ii X T i is a rank ni matrix with maximum eigenvalue equal to \u03c3 2\u03c1i, applying Lemma 1 above gives\nP ( RMSE(Xi\u03b8\u0302i) \u2265 ) = P ( \u2016Xiui\u201622 \u2265 T 2 ) \u2264 ( T 2\nni\u03c32\u03c1i\n)ni/2 exp { \u22121\n2\n( T 2\n\u03c32\u03c1i \u2212 ni\n)} .\n(22)\nSetting the right hand side equal to \u03b4 and solving for gives\n=\n\u221a \u2212W (\u2212\u03b42/n/e)ni\u03c1i\u03c32\nT (23)\nwhere W denotes the Lambert W function (the inverse of f(x) = xex). The theorem follows by noting that \u2212W (\u2212\u03b42/n/e) \u2264 4 log 1\u03b4 for all n \u2265 1 when \u03b4 \u2264 0.1, with both quantities always positive in this range (note that leaving the W term in the bound can be substantially tighter in some cases)."}, {"heading": "5 Experimental results", "text": "In this section we evaluate contextual supervision on synthetic data and apply in to disaggregate smart meter data collected from thousands of homes. Since labeled data is unavailable, we design a synthetic disaggregation task similar to energy disaggregation from smart meters in order to evaluate our performance on this task quantitatively. Here we explore the choice of loss functions and demonstrate that contextual supervision dramatically outperforms the unsupervised approach.\nRates for signal recovery. We begin with a set of experiments examining the ability of the model to recover the underlying source signals as predicted by our theoretical analysis. In these experiments, we consider the problem of separating two source signals with Xi \u2208 RT\u00d716 sampled independently from a zero-mean Normal with covariance I+(1\u2212\u00b5)11T ; we sample \u03b8?i uniformly from [\u22121, 1] and Y \u223c N (X\u03b8?, I). Setting \u00b5 = 0.01 causes the features for each signal to be highly correlated with each other but sinceXi are sampled independently, not highly correlated across signals. In Figure 1, we see that MSE vanishes as T grows; when comparing these experimental results to the theoretical mean and 90% upper bound, we see that at least in these experiments, the bound is somewhat loose for large values of \u03c1i. We are also able to recover \u03b8? (which is expected since the least-squares estimator \u03b8\u0302 is consistent), but the rate is much slower due to high correlations in Xi.\nDisaggregation of synthetic data. The next set of experiments considers a synthetic generation process that more closely mimics signals that we encounter in energy disaggregation. The process described visually in Figure 2 (top) begins with two signals, the first is smoothly varying over time while the other is a repeating step function\nX1(t) = sin(2\u03c0t/\u03c41) + 1, X2(t) = I(t mod \u03c42 < \u03c42/2) (24)\nwhere I(\u00b7) is the indicator function and \u03c41, \u03c42 are the period of each signal. We also use two different noise models: for the smooth signal we sample Gaussian noise from N (0, \u03c32) while for the step function, we sample a distribution with a point mass at zero, uniform probability over [1, 0) \u222a (0, 1] and correlate it across time by summing over a window of size \u03b2. Finally, we constrain both noisy signals to be nonnegative and sum them to generate our input.\nWe generate data under this model for T = 50000 time points and consider increasingly specialized optimization objectives while measuring the error in recovering Y ? = XD(\u03b8?)+W , the underlying source signals corrupted by noise. As can be seen in Table 5, by using `1 loss for y2 and adding\ngi(yi) terms penalizing \u2016Dy1\u201622 and \u2016Dy2\u20161, error decreases by 25% over just `2 loss alone; in Figure 2, we observe that our estimations recovers the true source signals closely with the gi terms helping to capture the dynamics of the noise model for w2.\nAs a baseline for this result, we compare to an unsupervised method, nonnegative sparse coding [10]. We apply sparse coding by segmenting the input signal into 1000 examples of 50 time points (1/4 the period of the sine wave, X1(t)) and fit a sparse model of 200 basis functions. We report the best possible source separation by assigning each basis function according to an oracle measuring correlation with the true source signal and using the best value over a grid of hyperparameters; however, performance is still significantly worse than the contextually supervised method which makes explicit use of additional information.\nEnergy disaggregation on smart meter data. Next, we turn to the motivating problem for our model: disaggregating large-scale low resolution smart meter data into its component sources of consumption. Our dataset comes from PG&E and was collected by the utility from customers in Northern California who had smart meters between 1/2/2008 and 12/31/2011. According to estimations based on survey data, heating and cooling (air conditioning and refrigerators) comprise over 39% of total consumer electricity usage [1] and thus are dominant uses for consumers. Clearly, we expect temperature to have a strong correlation with these uses and thus we provide contextual supervision in the form of temperature information. The PG&E data is anonymized, but the location of individual customers is identified at the census block level; we use this information to construct a parallel temperature dataset using data from Weather Underground (http://www.wunderground.com/).\nThe exact specification of our energy disaggregation model is given in Table 5\u2014we capture the nonlinear dependence on temperature with radial-basis functions (RBFs), include a \u201cBase\u201d category which models energy used as a function of time of day, and featureless \u201cOther\u201d category representing end-uses not explicitly modeled. For simplicity, we penalize each category\u2019s deviations from the model using `1 loss; but for heating and cooling we first multiply by a smoothing matrix Sn (1\u2019s on the diagonal and n super diagonals) capturing the thermal mass inherent in heating and cooling: we expect energy usage to correlate with temperature over a window of time, not immediately. Finally,\nwe use gi(yi) and the difference operator to encode our intuition of how energy consumption in each category evolves over time. The \u201cBase\u201d category represents an aggregation of many sources of consumption and which we expect to evolve smoothly over time, while the on/off behavior in other categories is best represented by the `1 penalty.\nWe present the result of our model at two time scales, starting with Figure 3 (top), where we show aggregate energy consumption across all homes at the week level to demonstrate basic trends in usage. Quantitatively, our model assigns 15.6% of energy consumption to \u201cCooling\u201d and 7.7% to \u201cHeating\u201d, which is reasonably close to estimations based on survey data [1] (10.4% for air conditioning and 5.4% for space heating). We have deliberately kept the model simple and thus our higher estimations are likely due to conflating other temperature-related energy usages, such as refrigerators and water heating. In Figure 3 (bottom), we present the results of the model in disaggregating energy usage for a single hot summer week where the majority of energy usage is estimated to be cooling due to the context provided by high temperature."}, {"heading": "6 Conclusion and discussion", "text": "We believe the advances in this work, formalizing contextually supervised source separation and theoretically analyzing the requirements for accurate source signal recovery, will enable new applications of single-channel source separation in domains with large amounts of data but no access to explicit supervision. In energy disaggregation, this approach has allowed us to reasonably sep-\narate sources of consumption from extremely low-frequency smart meter data. This a significant advancement with the potential to drive increases in energy efficiency through programs that expose this information to consumers and automated systems for demand response. Developing algorithms that use this information to achieve these goals is an interesting direction for future work.\nAnother interesting direction is the explicit connection of our large-scale low-resolution methods with the more sophisticated appliance models developed on smaller supervised datasets with highfrequency measurements. As a few examples, a small amount of supervised information would enable us to calibrate our models automatically, while a semi-supervised approach would enable spreading some of the benefit of high-resolution load monitoring to the vast number of homes where only smart meter data is available."}], "references": [{"title": "An information-maximization approach to blind separation and blind deconvolution", "author": ["A.J. Bell", "T.J. Sejnowski"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Shift-invariant sparse coding for single channel blind source separation", "author": ["T. Blumensath", "M. Davies"], "venue": "SPARS, 5:75\u201378,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Independent component analysis, a new concept", "author": ["P. Comon"], "venue": "Signal processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "The effectiveness of feedback on energy consumption", "author": ["S. Darby"], "venue": "Technical report, Environmental Change Institute, University of Oxford,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Source separation using single channel ica", "author": ["M. Davies", "C. James"], "venue": "Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Advanced metering initiatives and residential feedback programs: a meta-review for household electricity-saving opportunities", "author": ["K. Ehrhardt-Martinez", "K.A. Donnelly", "S. Laitner"], "venue": "In American Council for an Energy-Efficient Economy,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Factorial hidden markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Nonintrusive appliance load monitoring", "author": ["G.W. Hart"], "venue": "Proceedings of the IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "In Neural Networks for Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Energy disaggregation via discriminative sparse coding", "author": ["J.Z. Kolter", "S. Batra", "A.Y. Ng"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Approximate inference in additive factorial hmms with application to energy disaggregation", "author": ["J.Z. Kolter", "T. Jaakkola"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning overcomplete representations", "author": ["M.S. Lewicki", "T.J. Sejnowski"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Residential electricity use feedback: A research synthesis and economic framework", "author": ["B. Neenan", "J. Robinson"], "venue": "Technical report, Electric Power Research Institute,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by vi", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Non-intrusive load monitoring using prior models of general appliance types", "author": ["O. Parson", "S. Ghosh", "M. Weal", "A. Rogers"], "venue": "In 26th AAAI Conference on Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "One microphone source separation", "author": ["S.T. Roweis"], "venue": "Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Single-channel speech separation using sparse non-negative matrix factorization", "author": ["M. Schmidt", "R. Olsson"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Nonnegative matrix factor 2-d deconvolution for blind single channel source separation", "author": ["M.N. Schmidt", "M. M\u00f8rup"], "venue": "In Independent Component Analysis and Blind Signal Separation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "A probabilistic latent variable model for acoustic modeling. Advances in models for acoustic processing", "author": ["P. Smaragdis", "B. Raj", "M. Shashanka"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Nonintrusive appliance load monitoring: Review and outlook", "author": ["M. Ziefman", "K. Roth"], "venue": "IEEE Transactions on Consumer Electronics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal.", "startOffset": 84, "endOffset": 96}, {"referenceID": 15, "context": "Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal.", "startOffset": 84, "endOffset": 96}, {"referenceID": 16, "context": "Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal.", "startOffset": 84, "endOffset": 96}, {"referenceID": 1, "context": "Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal.", "startOffset": 211, "endOffset": 225}, {"referenceID": 4, "context": "Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal.", "startOffset": 211, "endOffset": 225}, {"referenceID": 11, "context": "Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal.", "startOffset": 211, "endOffset": 225}, {"referenceID": 17, "context": "Traditionally, this problem has been approached in two ways: the supervised setting [11, 17, 18], where we have access to training data with the true signal separations and the unsupervised (or \u201cblind\u201d) setting [3, 6, 13, 19], where we have only the aggregate signal.", "startOffset": 211, "endOffset": 225}, {"referenceID": 7, "context": "However, both settings have potential drawbacks: for many problems, including energy disaggregation\u2014which looks to separate individual energy uses from a whole-home power signal [9]\u2014it can be difficult to obtain training data with the true separated signals needed for the supervised setting; in contrast, the unsupervised setting is an ill-defined problem with arbitrarily many solutions, and thus algorithms are highly task-dependent.", "startOffset": 178, "endOffset": 181}, {"referenceID": 18, "context": "A common strategy is to separate the observed aggregate signal into a linear combination of several bases, where different bases correspond to different components of the signal; algorithms such as Probabilistic Latent Component Analysis (PLCA) [20], sparse coding [15], and factorial hidden Markov models (FHMMs) [8] all fall within this category, with the differences concerning 1) how bases are represented and assigned to different signal components and 2) how the algorithm infers the activation of the different bases given the aggregate signal.", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "A common strategy is to separate the observed aggregate signal into a linear combination of several bases, where different bases correspond to different components of the signal; algorithms such as Probabilistic Latent Component Analysis (PLCA) [20], sparse coding [15], and factorial hidden Markov models (FHMMs) [8] all fall within this category, with the differences concerning 1) how bases are represented and assigned to different signal components and 2) how the algorithm infers the activation of the different bases given the aggregate signal.", "startOffset": 265, "endOffset": 269}, {"referenceID": 6, "context": "A common strategy is to separate the observed aggregate signal into a linear combination of several bases, where different bases correspond to different components of the signal; algorithms such as Probabilistic Latent Component Analysis (PLCA) [20], sparse coding [15], and factorial hidden Markov models (FHMMs) [8] all fall within this category, with the differences concerning 1) how bases are represented and assigned to different signal components and 2) how the algorithm infers the activation of the different bases given the aggregate signal.", "startOffset": 314, "endOffset": 317}, {"referenceID": 2, "context": "These methods can exploit significantly more structure and algorithms like Independent Component Analysis [4, 2] can separate signals with virtually no supervised information.", "startOffset": 106, "endOffset": 112}, {"referenceID": 0, "context": "These methods can exploit significantly more structure and algorithms like Independent Component Analysis [4, 2] can separate signals with virtually no supervised information.", "startOffset": 106, "endOffset": 112}, {"referenceID": 9, "context": "From the applied point of view, algorithms for energy disaggregation have received growing interest in recently years [11, 22, 12, 16].", "startOffset": 118, "endOffset": 134}, {"referenceID": 19, "context": "From the applied point of view, algorithms for energy disaggregation have received growing interest in recently years [11, 22, 12, 16].", "startOffset": 118, "endOffset": 134}, {"referenceID": 10, "context": "From the applied point of view, algorithms for energy disaggregation have received growing interest in recently years [11, 22, 12, 16].", "startOffset": 118, "endOffset": 134}, {"referenceID": 14, "context": "From the applied point of view, algorithms for energy disaggregation have received growing interest in recently years [11, 22, 12, 16].", "startOffset": 118, "endOffset": 134}, {"referenceID": 3, "context": "This is an important task since many studies have shown that consumers naturally adopt energy conserving behaviors when presented with a breakdown of their energy usage [5, 14, 7].", "startOffset": 169, "endOffset": 179}, {"referenceID": 12, "context": "This is an important task since many studies have shown that consumers naturally adopt energy conserving behaviors when presented with a breakdown of their energy usage [5, 14, 7].", "startOffset": 169, "endOffset": 179}, {"referenceID": 5, "context": "This is an important task since many studies have shown that consumers naturally adopt energy conserving behaviors when presented with a breakdown of their energy usage [5, 14, 7].", "startOffset": 169, "endOffset": 179}, {"referenceID": 8, "context": "As a baseline for this result, we compare to an unsupervised method, nonnegative sparse coding [10].", "startOffset": 95, "endOffset": 99}], "year": 2013, "abstractText": "We propose a new framework for single-channel source separation that lies between the fully supervised and unsupervised setting. Instead of supervision, we provide input features for each source signal and use convex methods to estimate the correlations between these features and the unobserved signal decomposition. We analyze the case of `2 loss theoretically and show that recovery of the signal components depends only on cross-correlation between features for different signals, not on correlations between features for the same signal. Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision; our motivating application is energy disaggregation of hourly smart meter data (the separation of whole-home power signals into different energy uses). Here we apply contextual supervision to disaggregate the energy usage of thousands homes over four years, a significantly larger scale than previously published efforts, and demonstrate on synthetic data that our method outperforms the unsupervised approach.", "creator": "LaTeX with hyperref package"}}}