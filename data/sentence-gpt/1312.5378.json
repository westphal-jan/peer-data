{"id": "1312.5378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Skolemization for Weighted First-Order Model Counting", "abstract": "First-order model counting emerged recently as a novel reasoning task, at the core of efficient algorithms for probabilistic logics. We present a Skolemization algorithm for model counting problems that eliminates existential quantifiers from a first-order logic theory without changing its weighted model count. For certain subsets of first-order logic, lifted model counters were shown to run in time polynomial in the number of objects in the domain of discourse, where propositional model counters require exponential time polynomials to be added at a time polynomial. In this sense, a model's expected log value will be less than the prior model, and for all classes of the second-order logic, the expected log value will be less than the prior model. To obtain the optimal model-by-model log value, we tested a subset of first-order log operations that would be a partial model for the second-order logic. The goal of this study was to assess whether a first-order log function can work in general on the first-order log problem, as described below. The first-order log function has not yet been constructed and is not explicitly tested for the first-order log problem in the second-order log problem.\n\n\n\nThe goal of this study was to identify what is called a \"first-order log function.\" The first-order log function is the first-order log function. The first-order log function is the first-order log function. The first-order log function is the first-order log function.\n\nThe first-order log function is the first-order log function. The first-order log function is the first-order log function. The first-order log function is the first-order log function. The first-order log function is the first-order log function. The first-order log function is the first-order log function. The first-order log function is the first-order log function.\nAfter testing, we found that the first-order log function was actually an approximation to the second-order log function. As expected, the first-order log function proved that it is a model for the second-order log function. The first-order log function was shown to run in time polynomial in the number of objects in the domain of discourse, where propositional model counters require exponential time polynomial in the number of objects in the domain of discourse, where propositional model counters require exponential time polynomials to be added", "histories": [["v1", "Thu, 19 Dec 2013 00:40:56 GMT  (35kb)", "https://arxiv.org/abs/1312.5378v1", null], ["v2", "Wed, 5 Mar 2014 13:50:15 GMT  (33kb)", "http://arxiv.org/abs/1312.5378v2", "To appear in Proceedings of the 14th International Conference on Principles of Knowledge Representation and Reasoning (KR), Vienna, Austria, July 2014"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["guy van den broeck", "wannes meert", "adnan darwiche"], "accepted": false, "id": "1312.5378"}, "pdf": {"name": "1312.5378.pdf", "metadata": {"source": "CRF", "title": "Skolemization for Weighted First-Order Model Counting", "authors": ["Guy Van den Broeck", "Wannes Meert", "Adnan Darwiche"], "emails": ["guyvdb@cs.ucla.edu", "wannes.meert@cs.kuleuven.be", "darwiche@cs.ucla.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n53 78\nv2 [\ncs .A\nI] 5\nM ar\n2 01"}, {"heading": "1 Introduction", "text": "Weighted model counting (WMC) is a generalization of model counting (Gomes, Sabharwal, and Selman 2009). In model counting, also known as #SAT, one counts the number of satisfying assignments of a propositional sentence. In WMC, each assignment has an associated weight and the task is to compute the sum of the weights of all satisfying assignments. One application of WMC is to probabilistic graphical models. For example, exact inference algorithms for Bayesian networks encode probabilistic inference as a WMC task, which can then be solved by knowledge compilation (Darwiche 2002) or exhaustive DPLL search (Sang, Beame, and Kautz 2005).\nWMC also plays an important role in first-order probabilistic representations. These became popular in recent years, in statistical relational learning (Getoor and Taskar 2007) and probabilistic logic learning (De Raedt et al. 2008), which are concerned with modeling and learning complex logical and probabilistic interactions between large numbers of objects. Efficient algorithms again reduce exact probabilistic inference to a WMC problem on a propositional knowledge base (Chavira, Darwiche, and Jaeger 2006;\nFierens et al. 2011; 2013). Encoding first-order probabilistic models into propositional logic retains a key advantage of the Bayesian network algorithms: WMC naturally exploits determinism and local structure in the probabilistic model (Boutilier et al. 1996; Chavira and Darwiche 2005). A disadvantage is that the high-level first-order structure is lost. Poole (2003) observed that knowing the symmetries that are abundant in first-order structure can speed up probabilistic inference. Lifted inference algorithms reason about groups of objects as a whole, similar to the high-level reasoning of first-order resolution. This has lead Van den Broeck et al. (2011) and Gogate and Domingos (2011) to propose weighted first-order model counting (WFOMC) as the core reasoning task underlying lifted inference algorithms. WFOMC assigns a weight to interpretations in finitedomain, function-free first-order logic, and computes the sum of the weights of all models.\nCounting models at the first-order level has computational advantages. For certain classes of theories, knowing the firstorder structure gives exponential speedups (Van den Broeck 2011). For example, counting the models of a first-order universally quantified CNF with up to two logical variables per clause can always be done in time polynomial in the size of the domain of discourse. In contrast, a propositionalization of these CNFs will often have a treewidth polynomial in the domains size, and propositional model counting runs in exponential time.\nOne major limitation of first-order model counters, however, is that they require input in Skolem normal form (i.e., without existential quantifiers). This is a common requirement for first-order automated reasoning algorithms, such as theorem provers. It is usually dealt with by Skolemization, which introduces Skolem constants and functions. However, the introduction of functions is problematic for first-order model counters as they expect a function-free input.\nThe main contribution of this paper is a Skolemization procedure that is specific for weighted first-order model counting. The procedure maps a logical input theory to an output theory that is devoid of existential quantifiers and functions, yet has an identical weighted first-order model count. The procedure is modular, in that it remains sound when extending the input and output theories with a new sentence. Furthermore, it is purely first-order as it is independent of the domain of discourse.\nThe proposed Skolemization algorithm has a range of implications. First, it opens up new possibilities for lifted inference algorithms. For example, on Markov Logic Networks with quantifiers (Richardson and Domingos 2006), and various forms of Probabilistic Logic Programs (e.g., De Raedt, Kimmig, and Toivonen (2007)), lifted algorithms currently provide little or no benefit over propositional ones. The main reason is that the WFOMC form of these representations generally contain existential quantifiers. The proposed Skolemization algorithm allows us, for the first time, to perform lifted inference on these representations. Second, there are liftability theorems that define classes of theories for which WFOMC is domain-lifted, meaning that it runs in time polynomial in the domain size (Jaeger and Van den Broeck 2012). These theorems had to assume Skolem normal form for the mentioned reason, but now apply more generally. Finally, the Skolemization algorithm averts the need for special inference rules that deal with existential quantifiers, simplifying the design of future WFOMC algorithms."}, {"heading": "2 Weighted First-Order Model Counting", "text": "We start by formally defining the weighted first-order model counting task. We also compare it to propositional weighted model counting and discuss existing algorithms."}, {"heading": "Background", "text": "Throughout this paper, we will work with the function-free finite-domain fragment of first-order logic (FOL), which we now briefly review. An atom P(t1, . . . , tn) consists of predicate P/n of arity n followed by n arguments, which are either constants from a finite domain D = {A,B, . . . } or logical variables {x, y, . . . }. We use y to denote a sequence of logical variables. A literal is an atom or its negation. A formula combines atoms with logical connectives and quantifiers \u2203 and \u2200. A logical variable x is quantified if it is enclosed by a \u2200x or \u2203x. A free variable is one that is not quantified. A sentence is a formula without free variables. A formula is ground if it contains no logical variables. A clause is a disjunction of literals and a CNF is a conjunction of clauses. The groundings of a quantifier-free formula is the set of formulas obtained by instantiating the free variables with any possible combination of constants from D. The grounding of \u2200x, \u03c6 and \u2203x, \u03c6 is the conjunction resp. disjunction of all groundings of \u03c6.\nWe will make use of Herbrand semantics (Hinrichs and Genesereth 2006), as is customary in statistical relational learning and probabilistic logic learning. The Herbrand base of sentence \u2206 for domain D is the set of all ground atoms that can be constructed from predicates and constants in D. A Herbrand interpretation is a truth-value assignment to all atoms in the Herbrand base. We will find it convenient to represent interpretations as sets of literals. A Herbrand model of \u2206 is a Herbrand interpretation \u03c9 that satisfies \u2206, denoted by \u03c9 |=D \u2206."}, {"heading": "Definitions", "text": "We first review propositional weighted model counting.\nDefinition 1 (WMC). Given \u2013 a sentence \u2206 in propositional logic over literals L, and \u2013 a weight function w : L \u2192 R\u22650,\nthe weighted model count (WMC) is\nWMC(\u2206,w) = \u2211\n\u03c9|=\u2206\n\u220f\nl\u2208\u03c9\nw(l).\nWFOMC lifts WMC to the first-order level as follows.\nDefinition 2 (WFOMC1). Given\n\u2013 a sentence \u2206 in FOL containing predicates P , \u2013 a set of constants D, including the constants in \u2206, and \u2013 a pair of weight functions w, w\u0304 : P \u2192 R,\nthe weighted first-order model count (WFOMC) is\nWFOMC(\u2206,D,w, w\u0304)\n= \u2211\n\u03c9|=D\u2206\n\u220f\nl\u2208\u03c90\nw\u0304(pred(l)) \u220f\nl\u2208\u03c91\nw(pred(l)),\nwhere \u03c90 and \u03c91 consists of the true, respectively false, literals in \u03c9, and pred maps literals to their predicate.\nThe weight functions assign a weight to each predicate. The weight of a positive (negative) literal is the weight of its predicate in w (w\u0304). The weight of a model is the product of its literal weights. Finally, the total count is the sum of the weights of all the Herbrand models of \u2206.\nOur WFOMC definition deviates from WMC in two ways. First, WMC directly assigns weights to literals. WFOMC instead assigns weights to predicates, and defines literal weights in terms of predicate weights. This distinguishes WFOMC from probabilistic databases (see Section 6). If for modeling reasons, certain literals need to be assigned unique weights, this can always be achieved by introducing new predicates.\nSecond, our definition permits predicate weights to be negative numbers. Negative weights will turn out to be crucial for our Skolemization algorithm. Historically, the WMC weight function has mostly been used to represent probabilities. This led to the (sometimes implicit) assumption that weights are between zero and one, or at least nonnegative. Nevertheless, all exact weighted model counters we are aware of can handle negative weights.2 It appears that the positive weight assumption is more intrinsic to approximate weighted model counters (Wei and Selman 2005; Gogate and Dechter 2011). Section 6 discusses negative weights in more detail."}, {"heading": "Motivation", "text": "A WFOMC problem can always be propositionalized into a WMC problem. We can ground \u2206 for D, turn every atom\n1This definition is based on Van den Broeck et al. (2011). WFOMC is called Lifted WMC in Gogate and Domingos (2011).\n2In fact, the only underlying requirement of exact model counting approaches is that literal weights are elements from a commutative semiring (Kimmig, Van den Broeck, and De Raedt 2012).\nin the Herbrand base into a propositional atom, and associate with every propositional literal the weight of its original predicate. One may wonder why we define this task at the first-order level.\nOur motivation is computational. Similar to how a single step of first-order resolution can perform a large number of propositional resolution steps, a WFOMC solver can often provide exponential speedups over WMC solvers. Firstorder quantifiers make statements about groups of symmetric objects, which we can reason about jointly.\nWithout going into algorithmic details, we will now illustrate this principle on concrete examples. For the sake of simplicity, the examples are non-weighted model counting problems, corresponding to WFOMC problems where w(P) = w\u0304(P) = 1 for all predicates P. Consider \u2206 to be\nStress(A) \u21d2 Smokes(A). (1)\nAssuming that D = {A}, every interpretation of Stress(A) and Smokes(A) satisfies \u2206, except when Stress(A) is true and Smokes(A) is false. Therefore, the model count is 3. Now let \u2206 be\n\u2200x, Stress(x) \u21d2 Smokes(x). (2)\nWithout changing D, the model count is still 3. When we expand D to contain n constants, we get n independent copies of Formula 1. For each person x, atoms Stress(x) and Smokes(x) can jointly take 3 values, and the total model count becomes 3n.\nThis example already demonstrates the benefits of firstorder counting. A propositional model counter on the groundings of Formula 2 would detect that all n clauses are independent, recompute for every clause that it has 3 models, and multiply these counts n times. Propositional model counters have no elementary operation for exponentiation. A first-order model counter reads from the first-order structure that it suffices to compute the model count of a single ground clause, and then knows to exponentiate. It never actually grounds the formula, and given the size of D, it runs in logarithmic time. This gives an exponential speedup over propositional counting, which runs in linear time.\nThese first-order counting techniques can interplay with propositional ones. Take for example \u2206 to be\n\u2200y, ParentOf(y) \u2227 Female\u21d2 MotherOf(y). (3)\nThis sentence is about a specific individual who may be a female, depending on whether the proposition Female is true. We can separately count the models in which Female is true, and those in which it is false (i.e., a Shannon decomposition). When Female is false, \u2206 is satisfied, and the ParentOf and MotherOf atoms can take on any value. This gives 4n models. When Female is true, \u2206 is structurally identical to Formula 2, and has 3n models. The total model count is then 3n + 4n.\nThese concepts can be applied recursively to count more complicated formulas. Take for example\n\u2200x, \u2200y, ParentOf(x, y) \u2227 Female(x) \u21d2 MotherOf(x, y).\nThere is now a partition of the ground clauses into n independent sets of n clauses. The sets correspond to values of\nx, and the individual clauses to values of y. The formula for each specific x, that is, each set of clauses, is structurally identical to Formula 3 and has count of 3n + 4n. The total model count is then (3n + 4n)n.\nThe most impressive improvements are attained when propositional model counters run in time exponential in n, yet first-order model counters run in polynomial time. To consider an example where this comes up, let \u2206 be\n\u2200x, \u2200y, Smokes(x) \u2227 Friends(x, y) \u21d2 Smokes(y). (4)\nThis time, the clauses in the grounding of \u2206 are no longer independent, and it would be wrong to simply exponentiate their counts. Let us first assume that we know a partial interpretation of the Smokes atoms with k positive literals (i.e., k people smoke). The question is now: how many models extend this partial interpretation? Formula 4 encodes that a smoker cannot be friends with a non smoker. Hence, out of n2 Friends atoms, k(n \u2212 k) have to be false, and the others can take either truth value. Thus, the number of models is 2n 2\u2212k(n\u2212k). Second, we know that there are ( n\nk\n)\npartial interpretations with k smokers, and k can range from 0 to n. This results in the total model count of\nn \u2211\nk=0\n(\nn\nk\n)\n2n 2\u2212k(n\u2212k).\nIn fact, the systems discussed in the next section can automatically construct this formula and compute the model count of Formula 4 in time polynomial in n. On the other hand, existing propositional WMC algorithms require time that is exponential in n on this problem. We note here that the treewidth of the grounding of \u2206 is linear in n.\nThere are space considerations that motivate first-order model counting as well. When converting a WFOMC problem to WMC, the grounding of \u2206 has size polynomial in the size of D, but the degree of this polynomial can be high. When the grounding does not fit into memory, even approximate WMC becomes a problem."}, {"heading": "Algorithms", "text": "Several algorithms exist for solving propositional WMC. Exact solvers are based on either exhaustive DPLL search (Sang, Beame, and Kautz 2005), or knowledge compilation to a circuit language that supports efficient model counting, such as d-DNNF (Darwiche 2002; Chavira and Darwiche 2008) or SDD (Choi, Kisa, and Darwiche 2013). Approximate WMC algorithms use local search (Wei and Selman 2005) or sampling (Gogate and Dechter 2011).\nMore recently, algorithms were introduced that directly solve the WFOMC task. They take a WFOMC problem and automatically generate and evaluate the types of expressions shown in the previous section. Their elementary operations include exponentiation, summation and binomial coefficients. They are called lifted inference algorithms. In particular, two lifted algorithms were proposed for exact WFOMC, one based on first-order knowledge compilation (Van den Broeck et al. 2011; Van den Broeck 2011; Van den Broeck 2013), and the other based on first-order DPLL search (Gogate and Domingos 2011). Approximate\nalgorithms were also proposed, including lifted importance sampling (Gogate and Domingos 2011; Gogate, Jha, and Venugopal 2012). More generally, there is a large literature on exact and approximate lifted probabilistic inference in statistical relational models, which can be adapted to solve certain WFOMC tasks. See Kersting (2012) for an overview."}, {"heading": "Normal Forms", "text": "It is common for logical reasoning algorithms to operate on normal form representations instead of arbitrary sentences. For example, propositional SAT solvers and weighted model counters often expect CNF inputs. We distinguish the following first-order normal forms.\n\u2013 A theory in prenex normal form consists of formulas Q1x1, . . . , Qnxn, \u03c6, where each Qi is either a universal or existential quantifier, and \u03c6 is quantifier-free.\n\u2013 A theory in prenex clausal form is a theory in prenex normal form where \u03c6 is a clause.\n\u2013 A theory in Skolem normal form is a theory in prenex normal form where all Qi are universal quantifiers.\n\u2013 A first-order CNF is a theory in Skolem and prenex clausal form. Thus, all sentences take the form \u2200x1, . . . , \u2200xn, l1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 lm.\nExisting WFOMC algorithms require a theory to be in first-order CNF. The same requirement is often posed by automated theorem provers, such as first-order resolution."}, {"heading": "3 Skolemization for WFOMC", "text": "It is well known that one can take any arbitrary formula and convert it to prenex clausal form. This involves pushing negations inside, pushing quantifiers to the front, and distributing disjunctions over conjunctions. The situation for Skolem normal form is different."}, {"heading": "Motivation", "text": "Not every formula can be transformed into an equivalent Skolem normal form. This problem is typically dealt with by Skolemization, which eliminates existential quantifiers from a prenex normal form. This is done by replacing existentially quantified variables by Skolem constants and functions. The result is not logically equivalent to the original formula, but only equisatisfiable (i.e., satisfiable precisely when the original formula is satisfiable).\nThe standard Skolemization algorithm is specific to the satisfiability task and may be unsuitable for other tasks. It is particularly unsuitable for WFOMC as it may produce a result with functions, which are not permitted in the WFOMC task. For example, standard Skolemization would transform the formula\n\u2200x, \u2203y, WorksFor(x, y) \u2228 Boss(x) (5)\ninto the following formula with the Skolem function Sk().\n\u2200x, WorksFor(x, Sk(x)) \u2228 Boss(x).\nAs soon as we allow functions, the Herbrand base becomes infinite, which makes the model counting task ill-defined, therefore, ruling out standard Skolemization for WFOMC.3"}, {"heading": "Algorithm", "text": "This section introduces a Skolemization technique for WFOMC. It takes as input a triple (\u2206,w, w\u0304) whose \u2206 is an arbitrary sentence and returns a triple (\u2206\u2032,w\u2032, w\u0304\u2032) whose \u2206\u2032 is in Skolem normal form (i.e., no existential quantifiers). Such a \u2206\u2032 can then be turned into first-order CNF using standard transformations. The proposed technique does not introduce functions. It satisfies two properties, one is essential and the other expands the applications of the technique.\nThe essential property is soundness. Property 1 (Soundness). Skolemization of (\u2206,w, w\u0304) to (\u2206\u2032,w\u2032, w\u0304\u2032) is sound iff for any D, we have that\nWFOMC(\u2206,D,w, w\u0304) = WFOMC(\u2206\u2032,D,w\u2032, w\u0304\u2032). To motivate the second property, we note that one may be\ninterested in queries of the form WFOMC(\u2206\u2227\u03c6,D,w, w\u0304), where \u2206, w and w\u0304 are fixed, but where \u03c6 is changing. For example, we will see in Section 4 that probabilistic inference can be reduced to these types of queries. Therefore, we want to achieve a stronger form of soundness. Property 2 (Modularity). Skolemization of (\u2206,w, w\u0304) to (\u2206\u2032,w\u2032, w\u0304\u2032) is modular iff for any D and any sentence \u03c6, WFOMC(\u2206 \u2227 \u03c6,D,w, w\u0304) = WFOMC(\u2206\u2032 \u2227 \u03c6,D,w\u2032, w\u0304\u2032).\nThat is, by replacing \u03c6, one does not invalidate the Skolemization obtained under a different \u03c6.\nThe proposed Skolemization algorithm eliminates existential quantifiers one by one. Its basic building block is the following transformation. Definition 3. Suppose that \u2206 contains a subexpression of the form \u2203x, \u03c6(x,y), where \u03c6(x,y) is an arbitrary sentence containing the free logical variables x and y. Let n be the number of variables in y. First, we introduce two new predicates: the Tseitin predicate Z/n and the Skolem predicate S/n. Second, we replace the expression \u2203x, \u03c6(x,y) in \u2206 by the atom Z(y), and append the formulas\n\u2200y, \u2200x, Z(y) \u2228 \u00ac\u03c6(x,y)\n\u2200y, S(y) \u2228 Z(y)\n\u2200y, \u2200x, S(y) \u2228 \u00ac\u03c6(x,y).\nThe functions w\u2032 and w\u0304\u2032 are equal to w and w\u0304, except that w\u2032(Z) = w\u0304\u2032(Z) = w\u2032(S) = 1 and w\u0304\u2032(S) = \u22121.\nIn the resulting theory \u2206\u2032, a single existential quantifier is now eliminated. This building block can eliminate single universal quantifiers as well. When \u2206 contains a subexpression \u2200x, \u03c6(x,y), we replace it by \u00ac\u2203x,\u00ac\u03c6(x,y), whose existential quantifier can be eliminated with Definition 3.\nWe can now show the following. 3One could obtain a Skolem normal form by grounding existential quantifiers, replacing them by large, but finite disjunctions. While this may still permit limited runtime improvements on vacuous formulas, it is for all practical purposes equivalent to reducing the WFOMC problem to a WMC problem. Moreover, that transformation is dependent on the domain and leads to large formulas whose conversion to CNF blows up (e.g., when grounding \u2203x\u2200y).\nTheorem 3 (Modularity). Repeated application of Definition 3 comprises a modular Skolemization algorithm.\nThe detailed proof can be found in the appendix."}, {"heading": "Intuition", "text": "Our Skolemization algorithm implicitly tries to enforce an equivalence between the eliminated subexpression and the Tseitin predicate4, which is explicitly written as\n\u2200y, Z(y) \u21d4 [\u2203x, \u03c6(x,y)] .\nThis equivalence contains an existential quantifier so it cannot be represented explicitly. Instead, the algorithms enforces a relaxed equivalence, represented by the three formulas in Definition 3. The intuition is that by relaxing the equivalence we introduce additional models to the theory, but for every additional model with weight W , there is exactly one additional model with weight \u2212W .5 The WFOMC therefore stays the same.\nThe interaction between the three relaxed formulas, the intended equivalence, and the model weights becomes more apparent after a case analysis on Z(y):\n1. When Z(y) is false, it implies that \u2203x, \u03c6(x,y) is false, which is intended. It also implies that S(y) is true, which does not change the model count, since we multiply by 1.\n2. When Z(y) is true, it implies that only three states of S(y) and \u2203x, \u03c6(x,y) are allowed:\n(a) \u2203x, \u03c6(x,y) is true and S(y) is true. This is again intended, because Z(y) and \u2203x, \u03c6(x,y) are equivalent.\n(b) \u2203x, \u03c6(x,y) is false and S(y) is true. This is an unintended state with a positive weight W .\n(c) \u2203x, \u03c6(x,y) is true and S(y) is false. This is an unintended state with a weight \u2212W . The negative weight comes from the fact that w\u0304(S) = \u22121.\nThe weights of the unintended models cancel each other out."}, {"heading": "Examples", "text": "We will now illustrate our Skolemization algorithm on concrete examples. Suppose that \u2206 is Formula 5, that is,\n\u2200x, \u2203y, WorksFor(x, y) \u2228 Boss(x).\nWe can apply Definition 3 to the subexpression \u2203y, WorksFor(x, y) \u2228 Boss(x), resulting in a \u2206\u2032 equal to\n\u2200x, Z(x)\n\u2200x, \u2200y, Z(x) \u2228 \u00ac [WorksFor(x, y) \u2228 Boss(x)]\n\u2200x, Z(x) \u2228 S(x)\n\u2200x, \u2200y, S(x) \u2228 \u00ac [WorksFor(x, y) \u2228 Boss(x)] .\nThe first formulas is the original formula with the subexpression substituted by Z(x).\nTo get a better insight into the result, we will simplify it using first-order unit propagation (Van den Broeck et al.\n4This equivalence represents a set of propositional Tseitin encodings, in which each Z atom is a Tseitin variable (Tseitin 1983).\n5This is not dissimilar to the inclusion-exclusion principle.\n2011) while noting that the first formula is a unit clause. The simplified theory is\n\u2200x, \u2200y, S(x) \u2228 \u00acWorksFor(x, y)\n\u2200x, S(x) \u2228 \u00acBoss(x).\nWe verify the correctness of this Skolemization as follows.\n\u2013 When Boss(x) is true, the formula is satisfied for x, and the models of \u2206\u2032 are intended, that is, they correspond to models of \u2206. Indeed, S(x) is entailed to be true and the model weights are multiplied by one.\n\u2013 When Boss(x) is false and WorksFor(x, y) is true for at least one y, then S(x) is entailed to be true. Again these models are intended, because \u2203y, WorksFor(x, y)\u2228 Boss(x) is now satisfied for x. The model weights are multiplied by one.\n\u2013 When Boss(x) is false and WorksFor(x, y) is false for all y then S(x) can be either true or false. This is where unintended models appear, once with S(x) true and once with S(x) false. Because they have opposing weights, the contributions of these unintended models cancel out.\nAs a second example, consider \u2206 to be\n\u2200x, \u2203y, \u2203z, Parents(x, y, z) \u2228 Adam(x).\nSkolemization of the inner existential quantifier results in\n\u2200x, \u2203y, Z1(x, y)\n\u2200x, \u2200y, \u2200z, Z1(x, y) \u2228 \u00ac [Parents(x, y, z) \u2228 Adam(x)]\n\u2200x, \u2200y, Z1(x, y) \u2228 S1(x, y)\n\u2200x, \u2200y, \u2200z, S1(x, y) \u2228 \u00ac [Parents(x, y, z) \u2228 Adam(x)]\nThis example shows the need for a Tseitin predicate Z1. The first sentence still contains an existential quantifier. One more elimination and unit propagation step replaces that sentence by \u2200y, \u2200x, S2(y) \u2228 \u00acZ1(x, y) and the result is in Skolem normal form."}, {"heading": "Properties", "text": "Theorem 3 suggests the repeated application of Definition 3, as long as the sentence contains an existential quantifier, or a universal quantifier not in prenex form. This approach has one caveat: eliminating \u2203x, \u03c6(x,y) adds the expression \u00ac\u03c6(x,y) to \u2206\u2032. When we eliminate quantifiers from left to right, \u03c6(x,y) itself can contain quantifiers. This operation will introduce new quantifiers in \u00ac\u03c6(x,y) and cause a blow up due to the duplication in the newly added formulas. This can be avoided by eliminating from right to left, that is, from innermost to outermost. We can show the following theorem, whose proof is in the appendix.\nTheorem 4 (Termination and Complexity). Repeated application of Definition 3 will terminate with a sentence in Skolem normal form. Moreover, this can be achieved in time polynomial in the size of \u2206.\nThe resulting Skolem normal form sentence can subsequently be transformed into first-order CNF. When using Tseitin\u2019s transformation (Tseitin 1983), this can even be done in polynomial time.\nIn our first example, the Tseitin predicate Z could be removed from \u2206\u2032 by unit propagation. The following proposition generalizes that observation.\nProposition 5. Suppose that we are eliminating a subexpression \u2203x, \u03c6(x,y) from a sentence \u2200y, \u2203x, \u03c6(x,y) using the procedure of Definition 3. That is, the existential quantifier in this subexpression is only preceded by universal quantifiers. Then, we can avoid adding Tseitin predicate Z and instead define \u2206\u2032 to be\n\u2200y, \u2200x, S(y) \u2228 \u00ac\u03c6(x,y).\nThis simplifies the transformation when applicable, in particular when \u2206 is already in prenex normal form."}, {"heading": "4 WFOMC Encodings", "text": "We will show in this section how the proposed Skolemization technique can extend the scope of first-order model counters to new situations. We will consider in particular one undirected first-order probabilistic language (Markov Logic) and one directed language (Probabilistic Logic Programs). First-order model counters currently apply to a subset of the first representation, and not to the second representation. With Skolemization, these model counters can now be applied to both. Our treatment is based on providing WFOMC encodings of these representations, to which our Skolemization technique is then applied.6\nConsider a first-order probabilistic model that induces the distribution PrD(.) for domain D. A WFOMC encoding of this model is a triple (\u2206,w, w\u0304) which guarantees that for any sentence \u03c6 (usually a conjunction of literals) and domain D, we have that\nPrD(\u03c6) = WFOMC(\u2206 \u2227 \u03c6,D,w, w\u0304) WFOMC(\u2206,D,w, w\u0304) ."}, {"heading": "Markov Logic Networks", "text": "We will now introduce a WFOMC encoding for Markov logic networks (MLN) (Richardson and Domingos 2006).\nRepresentation An MLN is a set of tuples (w,\u03c8), where w is a real number representing a weight and \u03c8 is a formula in first-order logic. When w is infinite, \u03c8 represents a firstorder logic constraint, also called a hard formula.\nBuilding further on the example given before, consider the following MLN\n1.3 \u2203y, WorksFor(x, y) \u2228 Boss(x). (6)\nThis statement softens the logical sentence we saw earlier. Instead of saying that every person either has a boss, or is a boss, it states that worlds with many employed people are more likely. That is, it is now possible to have a world with unemployed people, but the more unemployed people there are, the lower the probability of that world.\nThe semantics of a first-order MLN \u03a6 is defined in terms of its grounding for a given domain of constants D. The grounding of \u03a6 is the MLN obtained by first grounding all\n6These encodings are implemented in the WFOMC system: http://dtai.cs.kuleuven.be/wfomc\nits quantifiers and then replacing each formula in \u03a6 with all its groundings (using the same weight). With the domain D = {A,B} (e.g., two people, Alice and Bob), the above first-order MLN represents the following grounding.\n1.3 WorksFor(A,A) \u2228 WorksFor(A,B) \u2228 Boss(A)\n1.3 WorksFor(B,A) \u2228 WorksFor(B,B) \u2228 Boss(B)\nThis ground MLN contains six different random variables, which correspond to all groundings of atoms WorksFor(x, y) and Boss(x). This leads to a distribution over 26 possible worlds (i.e., interpretation). The weight of each world is simply the product of all weights ew, where (w, \u03b3) is a ground MLN formula and \u03b3 is satisfied by the world. The weights of worlds that do not satisfy a hard formula are set to zero. The probabilities of worlds are obtained by normalizing their weights."}, {"heading": "Encoding a Markov Logic Network", "text": "Definition 4. The WFOMC encoding (\u2206,w, w\u0304) of an MLN is constructed as follows. For each MLN formula (wi, \u03c6i(xi)), where xi denotes the free logical variables in \u03c6i, we introduce a parameter predicate Pi/|xi|. For each MLN formula, \u2206 contains the sentence \u2200xi, Pi(xi) \u21d4 \u03c6i(xi). The weight function sets w(Pi) = ewi , w\u0304(Pi) = 1, and w(Q) = w\u0304(Q) = 1 for all other predicates Q.\nEach Pi captures the truth value of \u03c6i and carries its weight. Hard formulas can directly be encoded as constraints.\nThe encoding of Formula 6 has \u2206 equal to\n\u2200x, P(x) \u21d4 \u2203y, WorksFor(x, y) \u2228 Boss(x).\nIts w maps P to e1.3 and all other predicates to 1. Its w\u0304 maps all predicates to 1.\nAs discussed in Section 2, WFOMC algorithms require first-order CNF input. Definition 4 will only yield a \u2206 in Skolem normal form (and thus rewritable into CNF) if the MLN formulas are quantifier-free. Then, the only quantifiers in \u2206 are the universal ones introduced by the encoding itself. Therefore, Van den Broeck et al. (2011) and Gogate and Domingos (2011) resort to grounding all quantifiers in the MLN formulas so as to obtain a CNF. This makes the WFOMC encoding specific to the domain D, and partly removes first-order structure from the problem.\nOur discussion is based on Van den Broeck et al. (2011). It is similar to the encoding of Gogate and Domingos (2011), whose parameter predicates have more arguments. While these encodings are specific to MLNs, it is straightforward to generalize them to other undirected languages, such as parfactor graphs (Poole 2003).\nApplying Skolemization We can now perform WFOMC inference in MLNs with quantifiers. Skolemization and CNF\nconversion for the example above results in a \u2206\u2032 equal to\n\u2200x, P(x) \u2228 \u00acZ(x)\n\u2200x, \u00acP(x) \u2228 Z(x)\n\u2200x, \u2200y, S(x) \u2228 \u00acWorksFor(y, x)\n\u2200x, S(x) \u2228 \u00acBoss(x)\n\u2200x, S(x) \u2228 Z(x)\n\u2200x, \u2200y, Z(x) \u2228 \u00acWorksFor(x, y)\n\u2200x, \u2200y, Z(x) \u2228 \u00acBoss(x)\nThis theory can be used for WFOMC inference."}, {"heading": "Probabilistic Logic Programs", "text": "We now show a WFOMC encoding for a directed first-order probabilistic language. The encoding is explained for the ProbLog language (De Raedt, Kimmig, and Toivonen 2007; Fierens et al. 2013).\nProbLog Representation ProbLog extends logic programs with facts that are annotated with probabilities. A ProbLog program \u03a6 is a set of probabilistic facts F and a regular logic program L. A probabilistic fact p :: a consists of a probability p and an atom a. A logic program is a set of rules, with the form Head : - Body, where the head is an atom and the body is a conjunction of literals. For example,\n0.1 :: Attends(x).\n0.3 :: ToSeries(x).\nSeries : - Attends(x), ToSeries(x).\nThis program expresses that if more people attend a workshop, it more likely turns into a series of workshops.\nThe semantics of a ProbLog program \u03a6 are defined by a distribution over the groundings of the probabilistic facts for a given domain of constants D (Sato 1995).7 The probabilistic facts pi ::ai induce a set of possible worlds, one for each possible partition of ai in positive and negative literals. The set of true ai literals with the logic program L define a wellfounded model (Van Gelder, Ross, and Schlipf 1991). The probability of such a model is the product of pi for all true ai literals and 1\u2212 pi for all false ai literals.\nFor the domain D = {A,B} (two people), the above firstorder ProbLog program represents the following grounding:\n0.1 :: Attends(A).\n0.1 :: Attends(B).\n0.3 :: ToSeries(A).\n0.3 :: ToSeries(B).\nSeries : - Attends(A), ToSeries(A).\nSeries : - Attends(B), ToSeries(B).\nThis ground ProbLog program contains 4 probabilistic facts which corresponds to 24 possible worlds. The\n7Our treatment assumes a function-free and finite-domain fragment of ProbLog. Starting from classical ProbLog semantics, one can obtain the a finite function-free domain for a given query by exhaustively executing the Prolog program and keeping track of the goals that are called during resolution.\nweight of, for example, the world in which Attends(A) and ToSeries(A) are true would be 0.1 \u00b7 (1 \u2212 0.1) \u00b7 0.3 \u00b7 (1 \u2212 0.3) = 0.0189 and the model would be {Attends(A), ToSeries(A), Series}.\nEncoding a ProbLog Program The transformation from a ProbLog program to a first-order logic theory is based on Clark\u2019s completion (Clark 1978). This is a transformation from logic programs to first-order logic. For certain classes of programs, called tight logic programs (Fages 1994), it is correct, in the sense that every model of the logic program is a model of the completion, and vice versa. Intuitively, for each predicate P, the completion contains a single sentence encoding all its rules. These rules have the form P(x) : - bi(x,yi), where bi is a body and yi are the variables that appear in the body bi but not in the head. The sentence encoding these rules in the completion is \u2200x, P(x) \u21d4 \u2228\ni \u2203yi, bi(x,yi). If the program contains cyclic rules, the completion is not sound, and, it is necessary to first apply a conversion to remove positive loops (Janhunen 2004).\nDefinition 5. The WFOMC encoding (\u2206,w, w\u0304) of a tight ProbLog program has \u2206 equal to Clark\u2019s completion of L. For each probabilistic fact8 p ::a we set the weight function to w(pred(a)) = p and w\u0304(pred(a)) = 1\u2212 p.\nAgain, a Skolem normal form is required to use WFOMC. However, we get this form only when the variables that appear in the body of a rule also appear in the head of a rule. This is not the case for most Prolog programs though. For example, if we apply Definition 5 to the example above, an existential quantifier appears in the sentence:\nSeries \u21d4 \u2203x, Attends(x) \u2227 ToSeries(x).\nFurthermore, w maps Attends to 0.1 and ToSeries to 0.3, and w\u0304 maps Attends to 0.9 and ToSeries to 0.7. Both w and w\u0304 are 1 for all other predicates. This example is not in Skolem normal form and requires Skolemization before it can be processed by WFOMC algorithms.\nApplying Skolemization Skolemization followed by CNF conversion gives a \u2206\u2032 equal to\nSeries\u2228 \u00acZ\n\u00acSeries\u2228 Z\n\u2200x, Z \u2228 \u00acAttends(x) \u2228 \u00acToSeries(x)\nZ \u2228 S\n\u2200x, S \u2228 \u00acAttends(x) \u2228 \u00acToSeries(x)\nSentence \u2206\u2032 is in Skolem normal form and is now processable by WFOMC algorithms.\nA simple ProbLog program as the one above is identical to a noisy-or structure (Cozman 2004), popular in Bayesian network modeling. Skolemization thus offers a fundamental method to lift first-order, directed structures, such as the noisy-or, in a generic manner (see also Section 6).\n8If multiple probabilistic facts are defined for the same predicate, auxiliary predicates need to be introduced."}, {"heading": "5 Liftability Implications", "text": "In our motivation for introducing first-order model counting, we touched upon the runtime and complexity improvements that can be attained by first-order counting. These complexity improvements have inspired a particular notion of lifted inference, called domain-lifted inference, which says that a WFOMC algorithm is lifted when it runs in time polynomial in the size of D (Van den Broeck 2011).\nWhile this notion of lifted inference may not capture everyone\u2019s perception of lifting, it does provide a clear formal framework. In particular, we can now talk about classes of sentences \u2206 for which an algorithm is domain-lifted. We say that the algorithm is complete for those classes. We can also talk about classes of sentences \u2206 for which there exists, or cannot exist a domain-lifted algorithm. We call the former classes liftable (Jaeger and Van den Broeck 2012).\nAll existing completeness and liftability theorems require that \u2206 is in first-order CNF. This requirement carries over from the existing WFOMC algorithms. Given our Skolemization algorithm, we can now restate these theorems to apply more generally. For example, the positive liftability result of Van den Broeck (2011) becomes the following\nCorollary 6. Suppose that \u2206 is a theory of sentences with up to two logical variables, and otherwise arbitrary structure. The complexity of computing the WFOMC of \u2206 is polynomial in the size of D. That is, this class is domain-liftable.\nOther notions of liftability also include queries \u03c6 in the complexity analysis, since they are important for lifted probabilistic inference. Based on Van den Broeck and Davis (2012), and Van den Broeck and Darwiche (2013), we can now claim the following.\nCorollary 7. Suppose that \u2206 is a theory of sentences with up to two logical variables, and otherwise arbitrary structure. The complexity of computing the WFOMC of \u2206 \u2227 \u03c6 is polynomial in the size of D and \u03c6, provided that \u03c6 is a conjunction of only unary literals, and binary literals of bounded Boolean rank.\nThese WFOMC liftability theorems have direct implications for all languages with a WFOMC encoding. For example, we can now say that MLNs with up to two logical variables per formula are domain-liftable, regardless of the quantifiers used. Previously, this was only true for quantifierfree MLNs. We can now also show that ProbLog programs with up to two logical variables per clause are guaranteed to be liftable. This is the first such liftability result for probabilistic logic programs."}, {"heading": "6 Related Work", "text": "In the encodings for MLNs and probabilistic logics, the weight functions (indirectly) represent probabilities and are therefore always positive. Our Skolemization algorithm introduces negative weights. This might appear odd when interpreting the weights as negative probabilities. This issue has been discussed before. For example, Feynman (1987) writes \u201cNegative probabilities allow an abstract calculation which permits freedom to do mathematical calculations in any order simplifying the analysis enormously\u201d.\nThe potential of negative probabilities was already observed by Jha and Suciu (2012) for answering queries in probabilistic databases and served as inspiration for our approach. Probabilistic databases (Suciu et al. 2011) are fundamentally a type of first-order probabilistic model. It can be viewed as a special type of weighted model counting problem (\u2206, w), where the weight function encodes the probability w(t) with which a tuple t can be found in the database. A query on such a database is typically a union of conjunctive queries (UCQ), which corresponds to a monotone DNF sentence \u2206. A noticeable difference with most WMC solvers (and WFOMC) is that the solvers for probabilistic databases expect the theory \u2206 to be in DNF instead of CNF. Different from WFOMC is that although the query (i.e., \u2206) is firstorder, the weight function is defined on the propositional level like in WMC. Weights are thus assigned to ground literals (the tuples) whereas for WFOMC weights are assigned to predicates (the tables). This allows WFOMC to exploit more types of symmetries.\nJha and Suciu (2012) propose to extend probabilistic databases with MarkoViews, a representation similar to MLNs, in which each weighted formula is again a UCQ query, that is, a monotone DNF. To compute the probability of a query, they introduce negative tuple probabilities.\nThe use of negative probabilities has also come up for optimizing calculations for specific structures in probabilistic graphical models like noisy-or (D\u0131\u0301ez and Gala\u0301n 2003). This particular case has been translated to the first-order case by Kisynski and Poole (2011) and resulted in an approach to lift noisy-or structures. In Section 4 we showed how the application of Skolemization leads to lifting noisy-or and both methods turn out to output a similar encoding for this particular case. Therefore, the approach followed by Kisynski and Poole (2011) can be considered a special case of the Skolemization algorithm applied to a noisy-or model.\nJaeger (2012) shows a negative liftability proof that uses relational Skolemization. Similar to our approach, subexpressions containing an existential quantifier are transformed and relaxed to eliminate the quantifier. Relational Skolemization, however, does not guarantee a correct model count. It rather guarantees that if the weight of a model is non-zero it will also be non-zero in the Skolemized version."}, {"heading": "7 Conclusions", "text": "In this paper, we introduced a Skolemization procedure that is sound for weighted first-order model counting. It extends the applicability of first-order model counters to encodings which require an existential quantifier such as Markov logic models with quantifiers and probabilistic logic programs. It also extends the class of first-order sentences whose models we can count efficiently."}, {"heading": "Acknowledgments", "text": "This work was supported by ONR grant #N00014-12-10423, NSF grant #IIS-1118122, NSF grant #IIS-0916161, and the Research Foundation-Flanders (FWO-Vlaanderen). GVdB is also at KU Leuven, Belgium."}, {"heading": "Appendix", "text": ""}, {"heading": "A Proof of Theorem 3", "text": "We will now prove the sequence of steps that leads to the removal of an existential quantifier in \u2206 to obtain \u2206\u2032, w\u2032 and w\u0304\u2032 while maintaining modularity. To replace the expression \u2203x, \u03c6(x,y) we perform the following steps. Isolate the Quantifier Introduce a new Tseitin predicate Z/n, whose arity n is the number of y variables. Set w\u2032(Z) = w\u0304\u2032(Z) = 1 and for all other predicates P, set w\u2032(P) = w(P) and w\u0304\u2032(P) = w\u0304(P). Construct \u2206\u2032 by replacing the expression \u2203x, \u03c6(x,y) in \u2206 by the atom Z(y), and appending the equivalence . In any grounding of \u2206, this step performs a Tseitin encoding of all groundings of \u2203x, \u03c6(x,y). The groundings of Z(y) play the role of Tseitin variables. This step therefore satisfies Property 2.\nSplit the Equivalence Rewrite equivalence \u2200y, Z(y) \u21d4 \u2203x, \u03c6(x,y) as two implications, \u2200y, Z(y) \u21d2 \u2203x, \u03c6(x,y) and \u2200y, Z(y) \u21d0 \u2203x, \u03c6(x,y). In clausal form, these become\n\u2200y, \u2203x, \u00acZ(y) \u2228 \u03c6(x,y)\n\u2200y, \u2200x, Z(y) \u2228 \u00ac\u03c6(x,y).\nThis step satisfies Property 2 because it is a logical equivalence.\nConvert to a Feature Introduce a new Skolem predicate predicate S/n. Set w(S) = 1 and w\u0304(S) = 0 and replace the sentence \u2200y, \u2203x, \u00acZ(y) \u2228 \u03c6(x,y) by\n\u2200y, S(y) \u21d4 \u2203x,\u00acZ(y) \u2228 \u03c6(x,y).\nIn all models of the resulting theory where \u2200y, \u2203x, \u00acZ(y) \u2228 \u03c6(x,y) is not satisfied, there will exist a y for which \u2203x,\u00acZ(y) \u2228 \u03c6(x,y) is not satisfied. This will cause at least one S(y) atom to be false in those models, which means that the weight of those models is multiplied by 0. The weight of all other models remains the same. This step therefore satisfies Property 2.\nConvert to an Implication Set w\u0304(S) = \u22121 and turn the equivalence \u2200y, S(y) \u21d4 \u2203x,\u00acZ(y) \u2228 \u03c6(x,y) into an implication \u2200y, S(y) \u21d0 \u2203x,\u00acZ(y) \u2228 \u03c6(x,y), which in clausal form becomes\n\u2200y, S(y) \u2228 Z(y)\n\u2200y, \u2200x, S(y) \u2228 \u00ac\u03c6(x,y).\nReplacing the equivalence by an implication and changing w\u0304(S) to \u22121 is correct for the following reason. Let S(y) \u21d4 \u03a3(y) be the above equivalence which is in \u2206, and let \u0393 represent all other sentences in \u2206 (i.e., \u2206 \u2261 (\u03a3(y) \u21d4 S(y)) \u2227 \u0393). Our goal is now to construct a triple (\u2206\u2032,w\u2032, w\u0304\u2032), where \u2206\u2032 \u2261 (\u03a3(y) \u21d2 S(y)) \u2227 \u0393, such that WFOMC(\u2206 \u2227 \u03c6,D,w, w\u0304) = WFOMC(\u2206\u2032 \u2227 \u03c6,D,w\u2032, w\u0304\u2032) for all domains D and all sentences \u03c6. Let \u03a3(A) and S(A) be any arbitrary grounding of \u03a3(y) and S(y). A case analysis on the values of \u03a3(A) and S(A) shows thatWFOMC(\u2206\u2227\u03c6,D,w, w\u0304) andWFOMC(\u2206\u2032\u2227 \u03c6,D,w\u2032, w\u0304\u2032) consist of the following terms (for compactness we drop D from the notation since it doesn\u2019t change).\n\u03a3(A) S(A) WFOMC(\u2206 \u2227 \u03c6,w, w\u0304)\n1 1 w(S) \u00b7WFOMC(\u0393 \u2227 \u03a3(A) \u2227 \u03c6,w, w\u0304)\n1 0 0 0 1 0 0 0 w\u0304(S) \u00b7WFOMC(\u0393 \u2227 \u00ac\u03a3(A) \u2227 \u03c6,w, w\u0304)\n\u03a3(A) S(A) WFOMC(\u2206\u2032 \u2227 \u03c6,w\u2032, w\u0304\u2032)\n1 1 w\u2032(S) \u00b7WFOMC(\u0393 \u2227 \u03a3(A) \u2227 \u03c6,w\u2032, w\u0304\u2032)\n1 0 0 0 1 w\u2032(S) \u00b7WFOMC(\u0393 \u2227 \u00ac\u03a3(A) \u2227 \u03c6,w\u2032, w\u0304\u2032)\n0 0 w\u0304\u2032(S) \u00b7WFOMC(\u0393 \u2227 \u00ac\u03a3(A) \u2227 \u03c6,w\u2032, w\u0304\u2032)\nNote that w\u0304(S) = 0 in the encoding of \u2206, and that thus\nWFOMC(\u2206 \u2227 \u03c6,w, w\u0304) =\nw(S) \u00b7WFOMC(\u0393 \u2227 \u03a3(A) \u2227 \u03c6,w, w\u0304)\n+ 0 \u00b7WFOMC(\u0393 \u2227 \u00ac\u03a3(A) \u2227 \u03c6,w, w\u0304)\nWFOMC(\u2206\u2032 \u2227 \u03c6,w\u2032, w\u0304\u2032) =\nw\u2032(S) \u00b7WFOMC(\u0393 \u2227 \u03a3(A) \u2227 \u03c6,w\u2032, w\u0304\u2032)\n+ [w\u2032(S) + w\u0304\u2032(S)]\n\u00b7WFOMC(\u0393 \u2227 \u00ac\u03a3(A) \u2227 \u03c6,w\u2032, w\u0304\u2032)\nSetting w\u2032(P) = w(P) for all predicates P except for S ensures that WFOMC(\u0393 \u2227 \u03a3(A) \u2227 \u03c6,w, w\u0304) = WFOMC(\u0393 \u2227 \u03a3(A) \u2227 \u03c6,w\u2032, w\u0304\u2032), that WFOMC(\u0393 \u2227 \u00ac\u03a3(A) \u2227 \u03c6,w, w\u0304) = WFOMC(\u0393 \u2227 \u00ac\u03a3(A) \u2227 \u03c6,w\u2032, w\u0304\u2032). Furthermore, set w(S) = w\u2032(S). What remains for WFOMC(\u2206 \u2227 \u03c6,w, w\u0304) to equal WFOMC(\u2206\u2032 \u2227 \u03c6,w\u2032, w\u0304\u2032) is that w\u2032(S) + w\u0304\u2032(S) = w(S) + w\u0304\u2032(S) = 0, which is achieved by setting w\u0304\u2032(S) = \u2212w(S) = \u22121."}, {"heading": "B Proof of Theorem 4", "text": "Proof. We begin by proving termination. Let the internal quantifier count of a sentence be the number of quantifiers it contains, excluding the leading universal quantifiers. Suppose that a sentence has an internal quantifier count of m. We can select any subexpression that starts with a quantifier and apply Skolemization to it (potentially converting \u2200 into \u2203 first). This reduces the internal quantifier count of \u2206 to be at most m \u2212 1 because at least one quantifier is removed. New sentences are added, however, containing the Tseitin and Skolem predicates, and expressions \u00ac\u03c6(x,y). These sentences also have an internal quantifier count of at mostm\u22121. Suppose that the sentences in \u2206 have an internal quantifier count of at most mmax . Applying Skolemization to one quantifier in each sentence reduces the maximal internal quantifier count to at most mmax \u2212 1. Therefore, by repeating this procedure for a finite number of steps, we obtain a theory with an internal quantifier count of zero, which is in Skolem normal form.\nNext, we prove polynomial complexity. We can remove the quantifiers in a sentence \u2206 one by one, starting from the innermost quantifier. The removed subexpression \u03c6(x,y) does not contain any quantifiers, so the internal quantifier count of the added formulas is zero. They are in Skolem normal form. The innermost subexpression is replaced by a Tseitin predicate, reducing the internal quantifier count by\none. The number of required elimination steps before the entire sentence is in Skolem normal form is thus equal to the number of quantifiers in \u2206. Moreover, the number of added formulas, and their size, is polynomial in the size of \u2206."}], "references": [{"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "Proceedings of UAI, 115\u2013123. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Boutilier et al\\.,? 1996", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Compiling Bayesian networks with local structure", "author": ["M. Chavira", "A. Darwiche"], "venue": "Proceedings of IJCAI, volume 19, 1306.", "citeRegEx": "Chavira and Darwiche,? 2005", "shortCiteRegEx": "Chavira and Darwiche", "year": 2005}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artificial Intelligence 172(6-7):772\u2013 799.", "citeRegEx": "Chavira and Darwiche,? 2008", "shortCiteRegEx": "Chavira and Darwiche", "year": 2008}, {"title": "Compiling relational Bayesian networks for exact inference", "author": ["M. Chavira", "A. Darwiche", "M. Jaeger"], "venue": "International Journal of Approximate Reasoning 42(1-2):4\u201320.", "citeRegEx": "Chavira et al\\.,? 2006", "shortCiteRegEx": "Chavira et al\\.", "year": 2006}, {"title": "Compiling probabilistic graphical models using sentential decision diagrams", "author": ["A. Choi", "D. Kisa", "A. Darwiche"], "venue": "Proceedings of the 12th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU).", "citeRegEx": "Choi et al\\.,? 2013", "shortCiteRegEx": "Choi et al\\.", "year": 2013}, {"title": "Negation as failure", "author": ["K. Clark"], "venue": "Readings in nonmonotonic reasoning, 311\u2013325. Morgan Kaufmann Publishers.", "citeRegEx": "Clark,? 1978", "shortCiteRegEx": "Clark", "year": 1978}, {"title": "Axiomatizing noisy-or", "author": ["F.G. Cozman"], "venue": "Proceedings of European Conference on Artificial Intelligence (ECAI), 979\u2013980.", "citeRegEx": "Cozman,? 2004", "shortCiteRegEx": "Cozman", "year": 2004}, {"title": "A logical approach to factoring belief networks", "author": ["A. Darwiche"], "venue": "Proceedings of KR 409\u2013420.", "citeRegEx": "Darwiche,? 2002", "shortCiteRegEx": "Darwiche", "year": 2002}, {"title": "Probabilistic inductive logic programming: theory and applications", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "Muggleton", "eds"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Problog: A probabilistic prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "Proceedings of IJCAI, volume 7, 2462\u20132467.", "citeRegEx": "Raedt et al\\.,? 2007", "shortCiteRegEx": "Raedt et al\\.", "year": 2007}, {"title": "Efficient computation for the noisy max", "author": ["F.J. D\u0131\u0301ez", "S.F. Gal\u00e1n"], "venue": "International Journal of Intelligent Systems", "citeRegEx": "D\u0131\u0301ez and Gal\u00e1n,? \\Q2003\\E", "shortCiteRegEx": "D\u0131\u0301ez and Gal\u00e1n", "year": 2003}, {"title": "Consistency of Clark\u2019s completion and existence of stable models", "author": ["F. Fages"], "venue": "Journal of Methods of Logic in Computer Science 1:51\u201360.", "citeRegEx": "Fages,? 1994", "shortCiteRegEx": "Fages", "year": 1994}, {"title": "Negative probability", "author": ["R.P. Feynman"], "venue": "Quantum implications: essays in honour of David Bohm 235\u2013248.", "citeRegEx": "Feynman,? 1987", "shortCiteRegEx": "Feynman", "year": 1987}, {"title": "Inference in probabilistic logic programs using weighted CNF\u2019s", "author": ["D. Fierens", "G. Van den Broeck", "I. Thon", "B. Gutmann", "L. De Raedt"], "venue": "Proceedings of UAI, 211\u2013220.", "citeRegEx": "Fierens et al\\.,? 2011", "shortCiteRegEx": "Fierens et al\\.", "year": 2011}, {"title": "Inference and learning in probabilistic logic programs using weighted Boolean formulas", "author": ["D. Fierens", "G. Van den Broeck", "J. Renkens", "D. Shterionov", "B. Gutmann", "I. Thon", "G. Janssens", "L. De Raedt"], "venue": "Theory and Practice of Logic Programming.", "citeRegEx": "Fierens et al\\.,? 2013", "shortCiteRegEx": "Fierens et al\\.", "year": 2013}, {"title": "An Introduction to Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar", "eds"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "Samplesearch: Importance sampling in presence of determinism", "author": ["V. Gogate", "R. Dechter"], "venue": "Artificial Intelligence 175(2):694\u2013729.", "citeRegEx": "Gogate and Dechter,? 2011", "shortCiteRegEx": "Gogate and Dechter", "year": 2011}, {"title": "Probabilistic theorem proving", "author": ["V. Gogate", "P. Domingos"], "venue": "Proceedings of UAI, 256\u2013265.", "citeRegEx": "Gogate and Domingos,? 2011", "shortCiteRegEx": "Gogate and Domingos", "year": 2011}, {"title": "Advances in lifted importance sampling", "author": ["V. Gogate", "A.K. Jha", "D. Venugopal"], "venue": "Proceedings of AAAI, 1910\u20131916.", "citeRegEx": "Gogate et al\\.,? 2012", "shortCiteRegEx": "Gogate et al\\.", "year": 2012}, {"title": "Model counting", "author": ["C.P. Gomes", "A. Sabharwal", "B. Selman"], "venue": "Handbook of Satisfiability 185:633\u2013654.", "citeRegEx": "Gomes et al\\.,? 2009", "shortCiteRegEx": "Gomes et al\\.", "year": 2009}, {"title": "Herbrand logic", "author": ["T. Hinrichs", "M. Genesereth"], "venue": "Technical Report LG-2006-02, Stanford University, Stanford, CA. http://logic.stanford.edu/reports/LG-2006-02.pdf.", "citeRegEx": "Hinrichs and Genesereth,? 2006", "shortCiteRegEx": "Hinrichs and Genesereth", "year": 2006}, {"title": "Liftability of probabilistic inference: Upper and lower bounds", "author": ["M. Jaeger", "G. Van den Broeck"], "venue": "Proceedings of StarAI.", "citeRegEx": "Jaeger and Broeck,? 2012", "shortCiteRegEx": "Jaeger and Broeck", "year": 2012}, {"title": "Lower complexity bounds for lifted inference", "author": ["M. Jaeger"], "venue": "arXiv preprint arXiv:1204.3255.", "citeRegEx": "Jaeger,? 2012", "shortCiteRegEx": "Jaeger", "year": 2012}, {"title": "Representing normal programs with clauses", "author": ["T. Janhunen"], "venue": "Proceedings of European Conference on Artificial Intelligence (ECAI), volume 16, 358.", "citeRegEx": "Janhunen,? 2004", "shortCiteRegEx": "Janhunen", "year": 2004}, {"title": "Probabilistic databases with MarkoViews", "author": ["A. Jha", "D. Suciu"], "venue": "Proceedings of the VLDB Endowment 5(11):1160\u2013 1171.", "citeRegEx": "Jha and Suciu,? 2012", "shortCiteRegEx": "Jha and Suciu", "year": 2012}, {"title": "Lifted probabilistic inference", "author": ["K. Kersting"], "venue": "Proceedings of European Conference on Artificial Intelligence (ECAI).", "citeRegEx": "Kersting,? 2012", "shortCiteRegEx": "Kersting", "year": 2012}, {"title": "Algebraic model counting", "author": ["A. Kimmig", "G. Van den Broeck", "L. De Raedt"], "venue": "arXiv preprint arXiv:1211.4475.", "citeRegEx": "Kimmig et al\\.,? 2012", "shortCiteRegEx": "Kimmig et al\\.", "year": 2012}, {"title": "Lifted aggregation in directed first-order probabilistic models", "author": ["J. Kisynski", "D. Poole"], "venue": "Proceedings of IJCAI, 1922\u2013 1929.", "citeRegEx": "Kisynski and Poole,? 2011", "shortCiteRegEx": "Kisynski and Poole", "year": 2011}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "Proceedings of IJCAI, 985\u2013991.", "citeRegEx": "Poole,? 2003", "shortCiteRegEx": "Poole", "year": 2003}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning 62(1-2):107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Solving Bayesian networks by weighted model counting", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "Proceedings of AAAI, volume 1, 475\u2013482.", "citeRegEx": "Sang et al\\.,? 2005", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "A statistical learning method for logic programs with distribution semantics", "author": ["T. Sato"], "venue": "Proceedings of the 12th International Conference on Logic Programming (ICLP), 715\u2013729.", "citeRegEx": "Sato,? 1995", "shortCiteRegEx": "Sato", "year": 1995}, {"title": "Probabilistic databases", "author": ["D. Suciu", "D. Olteanu", "C. R\u00e9", "C. Koch"], "venue": "Synthesis Lectures on Data Management 3(2):1\u2013180.", "citeRegEx": "Suciu et al\\.,? 2011", "shortCiteRegEx": "Suciu et al\\.", "year": 2011}, {"title": "On the complexity of derivation in propositional calculus", "author": ["G.S. Tseitin"], "venue": "Automation of Reasoning. Springer. 466\u2013483.", "citeRegEx": "Tseitin,? 1983", "shortCiteRegEx": "Tseitin", "year": 1983}, {"title": "On the complexity and approximation of binary evidence in lifted inference", "author": ["G. Van den Broeck", "A. Darwiche"], "venue": "Advances in Neural Information Processing Systems 26 (NIPS).", "citeRegEx": "Broeck and Darwiche,? 2013", "shortCiteRegEx": "Broeck and Darwiche", "year": 2013}, {"title": "Conditioning in firstorder knowledge compilation and lifted probabilistic inference", "author": ["G. Van den Broeck", "J. Davis"], "venue": "Proceedings of AAAI.", "citeRegEx": "Broeck and Davis,? 2012", "shortCiteRegEx": "Broeck and Davis", "year": 2012}, {"title": "Lifted Probabilistic Inference by First-Order Knowledge Compilation", "author": ["G. Van den Broeck", "N. Taghipour", "W. Meert", "J. Davis", "L. De Raedt"], "venue": "Proceedings of IJCAI, 2178\u20132185.", "citeRegEx": "Broeck et al\\.,? 2011", "shortCiteRegEx": "Broeck et al\\.", "year": 2011}, {"title": "On the completeness of firstorder knowledge compilation for lifted probabilistic inference", "author": ["G. Van den Broeck"], "venue": "Advances in Neural Information Processing Systems 24 (NIPS), 1386\u20131394.", "citeRegEx": "Broeck,? 2011", "shortCiteRegEx": "Broeck", "year": 2011}, {"title": "Lifted Inference and Learning in Statistical Relational Models", "author": ["G. Van den Broeck"], "venue": "Ph.D. Dissertation, KU Leuven.", "citeRegEx": "Broeck,? 2013", "shortCiteRegEx": "Broeck", "year": 2013}, {"title": "The wellfounded semantics for general logic programs", "author": ["A. Van Gelder", "K.A. Ross", "J.S. Schlipf"], "venue": "Journal of the ACM (JACM) 38(3):619\u2013649.", "citeRegEx": "Gelder et al\\.,? 1991", "shortCiteRegEx": "Gelder et al\\.", "year": 1991}, {"title": "A new approach to model counting", "author": ["W. Wei", "B. Selman"], "venue": "Theory and Applications of Satisfiability Testing, 96\u201397. Springer.", "citeRegEx": "Wei and Selman,? 2005", "shortCiteRegEx": "Wei and Selman", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "For example, exact inference algorithms for Bayesian networks encode probabilistic inference as a WMC task, which can then be solved by knowledge compilation (Darwiche 2002) or exhaustive DPLL search (Sang, Beame, and Kautz 2005).", "startOffset": 158, "endOffset": 173}, {"referenceID": 13, "context": "Efficient algorithms again reduce exact probabilistic inference to a WMC problem on a propositional knowledge base (Chavira, Darwiche, and Jaeger 2006; Fierens et al. 2011; 2013).", "startOffset": 115, "endOffset": 178}, {"referenceID": 0, "context": "Encoding first-order probabilistic models into propositional logic retains a key advantage of the Bayesian network algorithms: WMC naturally exploits determinism and local structure in the probabilistic model (Boutilier et al. 1996; Chavira and Darwiche 2005).", "startOffset": 209, "endOffset": 259}, {"referenceID": 1, "context": "Encoding first-order probabilistic models into propositional logic retains a key advantage of the Bayesian network algorithms: WMC naturally exploits determinism and local structure in the probabilistic model (Boutilier et al. 1996; Chavira and Darwiche 2005).", "startOffset": 209, "endOffset": 259}, {"referenceID": 0, "context": "Encoding first-order probabilistic models into propositional logic retains a key advantage of the Bayesian network algorithms: WMC naturally exploits determinism and local structure in the probabilistic model (Boutilier et al. 1996; Chavira and Darwiche 2005). A disadvantage is that the high-level first-order structure is lost. Poole (2003) observed that knowing the symmetries that are abundant in first-order structure can speed up probabilistic inference.", "startOffset": 210, "endOffset": 343}, {"referenceID": 0, "context": "Encoding first-order probabilistic models into propositional logic retains a key advantage of the Bayesian network algorithms: WMC naturally exploits determinism and local structure in the probabilistic model (Boutilier et al. 1996; Chavira and Darwiche 2005). A disadvantage is that the high-level first-order structure is lost. Poole (2003) observed that knowing the symmetries that are abundant in first-order structure can speed up probabilistic inference. Lifted inference algorithms reason about groups of objects as a whole, similar to the high-level reasoning of first-order resolution. This has lead Van den Broeck et al. (2011) and Gogate and Domingos (2011) to propose weighted first-order model counting (WFOMC) as the core reasoning task underlying lifted inference algorithms.", "startOffset": 210, "endOffset": 638}, {"referenceID": 0, "context": "Encoding first-order probabilistic models into propositional logic retains a key advantage of the Bayesian network algorithms: WMC naturally exploits determinism and local structure in the probabilistic model (Boutilier et al. 1996; Chavira and Darwiche 2005). A disadvantage is that the high-level first-order structure is lost. Poole (2003) observed that knowing the symmetries that are abundant in first-order structure can speed up probabilistic inference. Lifted inference algorithms reason about groups of objects as a whole, similar to the high-level reasoning of first-order resolution. This has lead Van den Broeck et al. (2011) and Gogate and Domingos (2011) to propose weighted first-order model counting (WFOMC) as the core reasoning task underlying lifted inference algorithms.", "startOffset": 210, "endOffset": 669}, {"referenceID": 29, "context": "For example, on Markov Logic Networks with quantifiers (Richardson and Domingos 2006), and various forms of Probabilistic Logic Programs (e.", "startOffset": 55, "endOffset": 85}, {"referenceID": 28, "context": "For example, on Markov Logic Networks with quantifiers (Richardson and Domingos 2006), and various forms of Probabilistic Logic Programs (e.g., De Raedt, Kimmig, and Toivonen (2007)), lifted algorithms currently provide little or no benefit over propositional ones.", "startOffset": 56, "endOffset": 182}, {"referenceID": 20, "context": "We will make use of Herbrand semantics (Hinrichs and Genesereth 2006), as is customary in statistical relational learning and probabilistic logic learning.", "startOffset": 39, "endOffset": 69}, {"referenceID": 40, "context": "2 It appears that the positive weight assumption is more intrinsic to approximate weighted model counters (Wei and Selman 2005; Gogate and Dechter 2011).", "startOffset": 106, "endOffset": 152}, {"referenceID": 16, "context": "2 It appears that the positive weight assumption is more intrinsic to approximate weighted model counters (Wei and Selman 2005; Gogate and Dechter 2011).", "startOffset": 106, "endOffset": 152}, {"referenceID": 35, "context": "This definition is based on Van den Broeck et al. (2011). WFOMC is called Lifted WMC in Gogate and Domingos (2011).", "startOffset": 36, "endOffset": 57}, {"referenceID": 17, "context": "WFOMC is called Lifted WMC in Gogate and Domingos (2011). In fact, the only underlying requirement of exact model counting approaches is that literal weights are elements from a commutative semiring (Kimmig, Van den Broeck, and De Raedt 2012).", "startOffset": 30, "endOffset": 57}, {"referenceID": 7, "context": "Exact solvers are based on either exhaustive DPLL search (Sang, Beame, and Kautz 2005), or knowledge compilation to a circuit language that supports efficient model counting, such as d-DNNF (Darwiche 2002; Chavira and Darwiche 2008) or SDD (Choi, Kisa, and Darwiche 2013).", "startOffset": 190, "endOffset": 232}, {"referenceID": 2, "context": "Exact solvers are based on either exhaustive DPLL search (Sang, Beame, and Kautz 2005), or knowledge compilation to a circuit language that supports efficient model counting, such as d-DNNF (Darwiche 2002; Chavira and Darwiche 2008) or SDD (Choi, Kisa, and Darwiche 2013).", "startOffset": 190, "endOffset": 232}, {"referenceID": 40, "context": "Approximate WMC algorithms use local search (Wei and Selman 2005) or sampling (Gogate and Dechter 2011).", "startOffset": 44, "endOffset": 65}, {"referenceID": 16, "context": "Approximate WMC algorithms use local search (Wei and Selman 2005) or sampling (Gogate and Dechter 2011).", "startOffset": 78, "endOffset": 103}, {"referenceID": 17, "context": "2011; Van den Broeck 2011; Van den Broeck 2013), and the other based on first-order DPLL search (Gogate and Domingos 2011).", "startOffset": 96, "endOffset": 122}, {"referenceID": 17, "context": "algorithms were also proposed, including lifted importance sampling (Gogate and Domingos 2011; Gogate, Jha, and Venugopal 2012).", "startOffset": 68, "endOffset": 127}, {"referenceID": 17, "context": "algorithms were also proposed, including lifted importance sampling (Gogate and Domingos 2011; Gogate, Jha, and Venugopal 2012). More generally, there is a large literature on exact and approximate lifted probabilistic inference in statistical relational models, which can be adapted to solve certain WFOMC tasks. See Kersting (2012) for an overview.", "startOffset": 69, "endOffset": 334}, {"referenceID": 33, "context": "This equivalence represents a set of propositional Tseitin encodings, in which each Z atom is a Tseitin variable (Tseitin 1983).", "startOffset": 113, "endOffset": 127}, {"referenceID": 33, "context": "When using Tseitin\u2019s transformation (Tseitin 1983), this can even be done in polynomial time.", "startOffset": 36, "endOffset": 50}, {"referenceID": 29, "context": "Markov Logic Networks We will now introduce a WFOMC encoding for Markov logic networks (MLN) (Richardson and Domingos 2006).", "startOffset": 93, "endOffset": 123}, {"referenceID": 35, "context": "Therefore, Van den Broeck et al. (2011) and Gogate and Domingos (2011) resort to grounding all quantifiers in the MLN formulas so as to obtain a CNF.", "startOffset": 19, "endOffset": 40}, {"referenceID": 17, "context": "(2011) and Gogate and Domingos (2011) resort to grounding all quantifiers in the MLN formulas so as to obtain a CNF.", "startOffset": 11, "endOffset": 38}, {"referenceID": 28, "context": "While these encodings are specific to MLNs, it is straightforward to generalize them to other undirected languages, such as parfactor graphs (Poole 2003).", "startOffset": 141, "endOffset": 153}, {"referenceID": 34, "context": "Our discussion is based on Van den Broeck et al. (2011). It is similar to the encoding of Gogate and Domingos (2011), whose parameter predicates have more arguments.", "startOffset": 35, "endOffset": 56}, {"referenceID": 17, "context": "It is similar to the encoding of Gogate and Domingos (2011), whose parameter predicates have more arguments.", "startOffset": 33, "endOffset": 60}, {"referenceID": 14, "context": "The encoding is explained for the ProbLog language (De Raedt, Kimmig, and Toivonen 2007; Fierens et al. 2013).", "startOffset": 51, "endOffset": 109}, {"referenceID": 31, "context": "The semantics of a ProbLog program \u03a6 are defined by a distribution over the groundings of the probabilistic facts for a given domain of constants D (Sato 1995).", "startOffset": 148, "endOffset": 159}, {"referenceID": 5, "context": "Encoding a ProbLog Program The transformation from a ProbLog program to a first-order logic theory is based on Clark\u2019s completion (Clark 1978).", "startOffset": 130, "endOffset": 142}, {"referenceID": 11, "context": "For certain classes of programs, called tight logic programs (Fages 1994), it is correct, in the sense that every model of the logic program is a model of the completion, and vice versa.", "startOffset": 61, "endOffset": 73}, {"referenceID": 23, "context": "If the program contains cyclic rules, the completion is not sound, and, it is necessary to first apply a conversion to remove positive loops (Janhunen 2004).", "startOffset": 141, "endOffset": 156}, {"referenceID": 6, "context": "A simple ProbLog program as the one above is identical to a noisy-or structure (Cozman 2004), popular in Bayesian network modeling.", "startOffset": 79, "endOffset": 92}, {"referenceID": 22, "context": "We call the former classes liftable (Jaeger and Van den Broeck 2012). All existing completeness and liftability theorems require that \u2206 is in first-order CNF. This requirement carries over from the existing WFOMC algorithms. Given our Skolemization algorithm, we can now restate these theorems to apply more generally. For example, the positive liftability result of Van den Broeck (2011) becomes the following", "startOffset": 37, "endOffset": 389}, {"referenceID": 33, "context": "Based on Van den Broeck and Davis (2012), and Van den Broeck and Darwiche (2013), we can now claim the following.", "startOffset": 17, "endOffset": 41}, {"referenceID": 7, "context": "Based on Van den Broeck and Davis (2012), and Van den Broeck and Darwiche (2013), we can now claim the following.", "startOffset": 65, "endOffset": 81}, {"referenceID": 32, "context": "Probabilistic databases (Suciu et al. 2011) are fundamentally a type of first-order probabilistic model.", "startOffset": 24, "endOffset": 43}, {"referenceID": 10, "context": "The use of negative probabilities has also come up for optimizing calculations for specific structures in probabilistic graphical models like noisy-or (D\u0131\u0301ez and Gal\u00e1n 2003).", "startOffset": 151, "endOffset": 173}, {"referenceID": 11, "context": "For example, Feynman (1987) writes \u201cNegative probabilities allow an abstract calculation which permits freedom to do mathematical calculations in any order simplifying the analysis enormously\u201d.", "startOffset": 13, "endOffset": 28}, {"referenceID": 11, "context": "For example, Feynman (1987) writes \u201cNegative probabilities allow an abstract calculation which permits freedom to do mathematical calculations in any order simplifying the analysis enormously\u201d. The potential of negative probabilities was already observed by Jha and Suciu (2012) for answering queries in probabilistic databases and served as inspiration for our approach.", "startOffset": 13, "endOffset": 279}, {"referenceID": 11, "context": "For example, Feynman (1987) writes \u201cNegative probabilities allow an abstract calculation which permits freedom to do mathematical calculations in any order simplifying the analysis enormously\u201d. The potential of negative probabilities was already observed by Jha and Suciu (2012) for answering queries in probabilistic databases and served as inspiration for our approach. Probabilistic databases (Suciu et al. 2011) are fundamentally a type of first-order probabilistic model. It can be viewed as a special type of weighted model counting problem (\u2206, w), where the weight function encodes the probability w(t) with which a tuple t can be found in the database. A query on such a database is typically a union of conjunctive queries (UCQ), which corresponds to a monotone DNF sentence \u2206. A noticeable difference with most WMC solvers (and WFOMC) is that the solvers for probabilistic databases expect the theory \u2206 to be in DNF instead of CNF. Different from WFOMC is that although the query (i.e., \u2206) is firstorder, the weight function is defined on the propositional level like in WMC. Weights are thus assigned to ground literals (the tuples) whereas for WFOMC weights are assigned to predicates (the tables). This allows WFOMC to exploit more types of symmetries. Jha and Suciu (2012) propose to extend probabilistic databases with MarkoViews, a representation similar to MLNs, in which each weighted formula is again a UCQ query, that is, a monotone DNF.", "startOffset": 13, "endOffset": 1287}, {"referenceID": 10, "context": "The use of negative probabilities has also come up for optimizing calculations for specific structures in probabilistic graphical models like noisy-or (D\u0131\u0301ez and Gal\u00e1n 2003). This particular case has been translated to the first-order case by Kisynski and Poole (2011) and resulted in an approach to lift noisy-or structures.", "startOffset": 152, "endOffset": 269}, {"referenceID": 10, "context": "The use of negative probabilities has also come up for optimizing calculations for specific structures in probabilistic graphical models like noisy-or (D\u0131\u0301ez and Gal\u00e1n 2003). This particular case has been translated to the first-order case by Kisynski and Poole (2011) and resulted in an approach to lift noisy-or structures. In Section 4 we showed how the application of Skolemization leads to lifting noisy-or and both methods turn out to output a similar encoding for this particular case. Therefore, the approach followed by Kisynski and Poole (2011) can be considered a special case of the Skolemization algorithm applied to a noisy-or model.", "startOffset": 152, "endOffset": 555}, {"referenceID": 10, "context": "The use of negative probabilities has also come up for optimizing calculations for specific structures in probabilistic graphical models like noisy-or (D\u0131\u0301ez and Gal\u00e1n 2003). This particular case has been translated to the first-order case by Kisynski and Poole (2011) and resulted in an approach to lift noisy-or structures. In Section 4 we showed how the application of Skolemization leads to lifting noisy-or and both methods turn out to output a similar encoding for this particular case. Therefore, the approach followed by Kisynski and Poole (2011) can be considered a special case of the Skolemization algorithm applied to a noisy-or model. Jaeger (2012) shows a negative liftability proof that uses relational Skolemization.", "startOffset": 152, "endOffset": 662}], "year": 2014, "abstractText": "First-order model counting emerged recently as a novel reasoning task, at the core of efficient algorithms for probabilistic logics. We present a Skolemization algorithm for model counting problems that eliminates existential quantifiers from a first-order logic theory without changing its weighted model count. For certain subsets of first-order logic, lifted model counters were shown to run in time polynomial in the number of objects in the domain of discourse, where propositional model counters require exponential time. However, these guarantees apply only to Skolem normal form theories (i.e., no existential quantifiers) as the presence of existential quantifiers reduces lifted model counters to propositional ones. Since textbook Skolemization is not sound for model counting, these restrictions precluded efficient model counting for directed models, such as probabilistic logic programs, which rely on existential quantification. Our Skolemization procedure extends the applicability of first-order model counters to these representations. Moreover, it simplifies the design of lifted model counting algorithms.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}