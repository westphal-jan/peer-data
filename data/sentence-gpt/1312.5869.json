{"id": "1312.5869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Principled Non-Linear Feature Selection", "abstract": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides strong probabilistic guarantees for correct identification of relevant features. RandSel's characteristics make it an ideal candidate for identifying informative learned representations. We've conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 20 Dec 2013 10:16:13 GMT  (232kb,D)", "http://arxiv.org/abs/1312.5869v1", null], ["v2", "Tue, 18 Feb 2014 17:25:43 GMT  (277kb,D)", "http://arxiv.org/abs/1312.5869v2", "arXiv admin note: substantial text overlap witharXiv:1311.5636"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dimitrios athanasakis", "john shawe-taylor", "delmiro fernandez-reyes"], "accepted": false, "id": "1312.5869"}, "pdf": {"name": "1312.5869.pdf", "metadata": {"source": "CRF", "title": "Learning Non-Linear Feature Maps With An Application To Representation Learning", "authors": ["Dimitrios Athanasakis", "John Shawe-Taylor", "Delmiro Fernandez-Reyes"], "emails": ["dathanasakis@cs.ucl.ac.uk", "jst@cs.ucl.ac.uk", "dfernan@nimr.mrc.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Feature selection is an important aspect in the implementation of machine learning methods. The selection of informative features can reduce generalisation error as well as storage and processing requirements for large datasets. In addition, parsimonious models provide valuable insight into the relations underlying elements of the process under examination. There is a wealth of literature on the subject of feature selection when the relationship between variables is linear. Unfortunately when the relation is non-linear feature selection becomes substantially more nuanced.\nKernel methods excel in modelling non-linear relations. Unsurprisingly, a number of kernel-based feature selection algorithms have been proposed. Early propositions, such as Recursive Feature Elimination(RFE) [1] can be computationally prohibitive, while attempts to learn a convex combination of low-rank kernels may fail to encapsulate nonlinearities in the underlying relation. Recent approaches using explicit kernel approximations can capture non-linear relations, but increase the storage and computational requirements."}, {"heading": "1.1 Related Work", "text": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3], as the empirical estimator for the Hilbert-Schmidt Independence Criterion(HSIC). Work on HSIC [4] provides the foundation of using the alignment of centred kernel matrices as the basis for measuring statistical dependence. The Hilbert-Schmidt Independence criterion is the basis for further work in [5], where\n\u2217Department of Computer Science, University College London, London, UK \u2020National Institute For Medical Research, London, UK\nar X\niv :1\n31 2.\n58 69\nv1 [\ncs .L\nG ]\n2 0\nD ec\ngreedy optimisation of centred alignment is employed for feature selection. Additionally, [5] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework.\nStability selection [6] is a general framework for variable selection and structure estimation of high dimensional data. The core principle of stability selection is to combine subsampling with a sparse variable selection algorithm. By repeated estimation over a number of different subsamples, the framework keeps track of the number of times each variable was used, thus maintaining an estimate for the importance of each feature. In this work, we propose a synthesis of the two aforementioned approaches through a randomised feature selection algorithm based on estimating the statistical dependence between bootstrapped random subspaces of the dataset in RKHS. The dependence estimation of random subsets of variables is similar to the approach of [11], which is extended through bootstrapping and carefully controlled feature set sizes.\nOur proposal is simple to implement and compares favourably with other methods in terms of scalability. The rest of the paper is structured as follows: Section 2 presents the necessary background on feature selection for kernel-based learning. Section 3 introduces a basic randomised algorithm for nonlinear feature selection, along with some simple examples, while Section 4 provides some analysis.Section 5 provides examples of how randSel can be effectively utilised as an important constituent of representation learning. Section 6 provides experimental results, and the brief discussion of Section 7 concludes this paper."}, {"heading": "2 Preliminaries", "text": "We consider the supervised learning problem of modelling the relationship between a m \u00d7 n input matrix X and a corresponding m \u00d7 n\u2032 output matrix Y . The simplest instance of such a problem is binary classification where the objective is the learning problem is to learn a function f : x \u2192 y mapping input vectors x to the desired outputs y. In the binary case we are presented with a m\u00d7 n matrix X and a vector of outputs y, yi \u2208 {+1,\u22121} Limiting the class of discrimination functions to linear classifiers we wish to find a classifier\nf(x) = \u2211 i wixi = \u3008w, x\u3009\nThe linear learning formulation can be generalised to the nonlinear setting through the use of a nonlinear feature map \u03c6(x), leading to the kernelized formulation:\nf(x) = \u3008w, \u03c6(x)\u3009 = \u3008 \u2211 i aiyi\u03c6(xi), \u03c6(x)\u3009 = \u2211 i aiyik(xi, x)\nThe key quantit? of interest in our approach is the centred kernel target alignment which is the empirical estimator of the HSIC[4], which measures statistical dependence in RKHS:\na(Cx, Cy) = \u3008Cx, Cy\u3009F \u2016Cx\u2016F \u2016Cy\u2016F =\n\u2211 i,j cxijcyij\u2211\ni,j \u2016cxij\u2016 \u2211 i,j \u2016kyij\u2016\nThe matrices Cx and Cy correspond to centred kernels on the features X and outputs Y and are computed as:\nC = [ I \u2212 11 T\nm\n] K [ I \u2212 11 T\nm ] where 1, in the above equation denotes the m-dimensional vector with all entries set equal to one."}, {"heading": "3 Development of key ideas", "text": "The approach we will take will be based on the following well-known observation that links kernel target alignment with the degree to which an input space contains a linear projection that correlates with the target.\nProposition 3.1 Let P be a probability distribution on the product space X \u00d7 R, where X has a projection \u03c6 into a Hilbert space F defined by a kernel \u03ba. We have that\u221a\nE(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03ba(x,x\u2032)] =\n= sup w:\u2016w\u2016\u22641\nE(x,y)\u223cP [y\u3008w, \u03c6(x)\u3009]\nProof: sup\nw:\u2016w\u2016\u22641 E(x,y)\u223cP [y\u3008w, \u03c6(x)\u3009] =\n= sup w:\u2016w\u2016\u22641\n\u2329 w,E(x,y)\u223cP [\u03c6(x)y] \u232a = \u2225\u2225E(x,y)\u223cP [\u03c6(x)y]\u2225\u2225\n= \u221a\u222b \u222b dP (x, y)dP (x\u2032, y\u2032)\u3008\u03c6(x), \u03c6(x\u2032)\u3009yy\u2032 = \u221a E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03ba(x,x\u2032)]\nThe proposition suggests that we can detect useful representations by measuring kernel target alignment. For non-linear functions the difficulty is to identify which combination of features creates a useful representation. We tackle this problem by sampling subsets S of features and assessing whether on average the presence of a particular feature i contributes to an increase ci in the average kernel target alignment. In this way we derive an empirical estimate of a quantity we will term the contribution.\nDefinition 3.2 The contribution ci of feature i is defined as ci = ES\u223cSi [ E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)] ] \u2212 ES\u2032\u223cS\\i [ E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS\u2032(x,x\u2032)] ] ,\nwhere \u03baS denotes the (non-linear) kernel using features in the set S (in our case this will be a Gaussian kernel with equal width), Si the uniform distribution over sets of features of size bn/2c+1 that include the feature i, S\\i the uniform distribution over sets of features of size bn/2c that do not contain the feature i, and n is the number of features.\nNote that the two distributions over features Si and S\\i are matched in the sense that for each S with non-zero probability in S\\i, S \u222a {i} has equal probability in Si. This approach is a straightforward extension of the idea of BaHsic [5].\nWe will show that for variables that are independent of the target this contribution will be negative. On the other hand, provided there are combinations of variables including the given variable that can generate significant correlations then the contribution of the variable will be positive.\nDefinition 3.3 We will define an irrelevant feature to be one whose value is statistically independent of the label and of the other features.\nWe would like an assurance that irrelevant features do not increase alignment. This is guaranteed for the Gaussian kernel by the following result.\nProposition 3.4 Let P be a probability distribution on the product space X \u00d7 R, where X has a projection \u03c6Si into a Hilbert space F defined by the Gaussian kernel \u03baS on a set of features S. Suppose a feature i 6\u2208 S is irrelevant. We have that\nE(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS\u222a{i}(x,x\u2032)] \u2264 E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)]\nProof (sketch): Since the feature is independent of the target and the other features, functions of these features are also independent. Hence,\nE(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS\u222a{i}(x,x\u2032)] = E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032) exp(\u2212\u03b3(xi \u2212 x\u2032i)2)] = E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)]E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [exp(\u2212\u03b3(xi \u2212 x\u2032i)2)] = E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [yy\u2032\u03baS(x,x\u2032)]\u03b1\nfor \u03b1 = E(x,y)\u223cP,(x\u2032,y\u2032)\u223cP [exp(\u2212\u03b3(xi \u2212 x\u2032i)2)] \u2264 1. In fact the quantity \u03b1 is typically less than 1 so that adding irrelevant features decreases the alignment. Our approach will be to progressively remove sets of features that are deemed to be irrelevant, hence increasing the alignment together with the signal to noise ratio for the relevant features. Figure 1 shows how progressively removing features from a learning problem whose output is the XOR function of the first two features both increases the alignment contributions and helps to highlight the two relevant features.\nWe now introduce our definition of a relevant feature.\nDefinition 3.5 A feature i will be termed \u03b7-influential when its contribution ci \u2265 \u03b7 > 0.\nSo far have only considered expected alignment. In practice we must estimate this expectation from a finite sample. We will omit this part of the analysis as it is a straigthforward application of Ustatistics that ensures that with high probability for a sufficiently large sample from Si and S\\i and of samples from P (whose sizes depend on \u03b7, \u03b4, the number k of \u03b7-influential variables and the number T of iterations) an empirical estimate of the contribution of an \u03b7-influential variable will with probability at least 1 \u2212 \u03b4 be greater than 0 for all of the fixed number T of iterations of the algorithm.\nOur final ingredient is a method of removing irrelevant features that we will term culling. At each iteration of the algorithm the contributions of all of the features are estimated using the required sample size and the contributions are sorted. We then remove the bottom 12.5% of the features in this ordering. Our main result assures us that culling will work under the assumption that the irrelevant variables are independent.\nTheorem 3.6 Fix \u03b7 > 0. Suppose that there are k \u03b7-influential variables and all other variables are irrelevant. Fix \u03b4 > 0 and number T of iterations. Given sufficiently many samples as described above the culling algorithm will with probability at least 1\u2212 \u03b4 remove only irrelevant variables.\nProof (sketch): Each irrelevant variable has expected contribution less than the contributions of all the influential features. Hence, with high probability at least 30% of these features will have lower contributions than all the influential features. Hence, the bottom 12.5% will all be irrelevant."}, {"heading": "4 Properties of the algorithm", "text": "We now define our algorithm for randomised selection (randSel). Given a m \u00d7 n input matrix X and corresponding output matrix Y , randSel proceeds by estimating the individual contribution of features by estimating the alignment of a number of random subsamples that include n2 and n 2 + 1 randomly selected features. This leads to an estimate for the expected alignment contribution of including a feature. The algorithm is parametrized by the number of bootstraps N , a bootstrap sizenb and a percentage z% of features that are dropped after N bootstraps. The algorithm proceeds iteratively until only two features remain.\nThere are a number of benefits to this approach, aside from the tangible probabilistic guarantees. RandSel scales gracefully. Considering the computation of a kernel k(x, x\u2032) for samples x, x\u2032 atomic, the number of kernel computations for a single iteration are n2bN , which for a sensible choice of N can be substantially smaller than the m2n complexity of HSIC variants. For example setting nb = \u221a m and N = n an iteration would require mn kernel element computations, and in addition this process is trivial to parallelize.\nAlgorithm 1 randSel Input: input data X , labels Y , number of iterations r subsample size s, number of features n, drop percentile proportion z, top percentile proportion a, number of occasions t repeat\nfor i = 1 to r do (Xi, Yi) = Random subsample of size s over n2 randomly selected variables ai= alignment(Xi, Yi) (X\n(+) i , Y (+) i ) = Random subsample of size s over n 2 + 1 randomly selected variables\na (+) i = alignment(X (+) i , Y (+) i )\nend for for j = 1 to n do\nmean contribution cj = meani:j\u2208X(+)i (a\n(+) i )\u2212meani:j /\u2208Xi(ai),\nend for drop the z% bottom-contributing features if fixing features then\nif j top-contributor for t consecutive times then fix feature j\nend if end if\nuntil no features left to fix, or only 2 features remain Return Sequence of estimated contributions and Fixed Variables"}, {"heading": "5 Feature Selection for learned representations", "text": "Unsupervised feature learning algorithms such as sparse filtering [9] are often used to learn overcomplete representations of data. The depth of a learning architecture refers to the composition of different levels of non-linear operations in the learned function. This suggests that employing feature selection to refine a set of learned representations, would substantially benefit from capturing nonlinear interactions between the learned features. Utilising randSel for feature selection in this setting is predicated on a number of properties. RandSel is readily applicable to a large sample size, a key\nproperty for the large sample sizes typically involved in representation learning. In addition, the algorithm is readily applicable to domains that have some structure. The multi-class structure of the black box challenge is an example where this property is important. This is a shared property of all the HSIC-variants. Finally, randSel is granular. Here, granularity refers to the fact that at the end of each iteration the algorithm returns a list of the remaining features and their expected contributions, which leads to a series of kernels of increased granularity. This can be a highly attractive property when using MKL for the final prediction."}, {"heading": "5.1 Prediction", "text": "RandSel produces progressively fine-grained combinations of features. A prediction mechanism effectively utilising the increasingly granular combinations of features comprises the last step of our approach, where we take a boosting approach based on LPBOOST-MKL [10]. The architecture proceeds by building a number of Gaussian kernels, parametrised by the different sets of features they are defined on, and a kernel bandwidth parameter \u03c3 as\n\u03ba(si,\u03c3)(x, x \u2032) = exp(\u2212\u03c3(x(si) \u2212 x\u2032(si))2\n, Where, x(si), x\u2032(si) are vectors containing only variables included in the set si. For simplicity assume there are nK distinct combinations of feature sets si and corresponding bandwidths \u03c3. We define a kernel on each such combination of features and bandwidth. Kernel ridge regression was used to generate the individual weak predictors in our architecture. Thus, each weak predictor has the form\nh(x, x\u2032) = \u2211 i ai\u03basj ,\u03c3(xi, x \u2032)\nThe algorithm then computes the classification rule, which is a convex combination of the weak learners, through the following linear program:\nminimize \u03b2\ns.t. m\u2211 i=1 uiyiHij \u2264 \u03b2\nm\u2211 i=1 ui = 1\n0 \u2264 ui,\u2264 D\nWhere D is a regularization parameter. Provided a sensible range of kernel bandwidths \u03c3 is specified, the final LPBoost classifier only requires tuning the regularization parameter D. In our search for simplicity, this is a tangible benefit, substantially reducing the search space of parameter combinations, to tuning this single regularization parameter."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Results on the ICML Black Box Learning Challenge", "text": "We used our proposal in the recent ICML 2013 Challenges in Representation Learning Black Box Learning challenge [8][12]. The dataset used in the challenge was an obfuscated subset of the Street View House Numbers dataset [15]. The original data where projected down to 1875 dimensions by multiplication with a random matrix, and the organizers did not reveal the source of the dataset until the competition was over. The training set comprises only 1,000 labelled samples, while an additional 130,000 samples where provided for the purposes of unsupervised pre-training.\nFor our submissions, cross validation was used to select the number of features to learn with sparse filtering, with our best solution using a set of 625 learned features. Randsel was then used to select combinations of the 625 learned features dropping 12.5% of the least contributing features at the end of each iteration. The resulting set of 34 different sets of features was combined with 75 different \u03c3 parameters to result in 2550 weak learners. The regularisation parameter D was also set through\ncross validation. This approach led to a generalisation accuracy of 68.44% on the public and 68.48% on the private leaderboards, ranking third in both cases out of a total of 218 teams."}, {"heading": "6.2 Application to cleavage site prediction", "text": "Signal peptides are amino-acid sequences found in transported proteins that selectively guide the distribution of the proteins to specific cellular compartments. Often referred to as the zip-code sequences, owing to their role in sub-cellular localization, a substantial body of work is devoted to predicting the cleavage site of signal sequences. Current literature establishes the importance of a number of physicochemical properties of the signal sequence in determining the cleavage site location. The experimental pipeline presented in this section further supplements this approach, by learning a feature representation of multiple physicochemical property encodings.\nThe Predisi dataset [13] of eukaryotic signal sequences was used for experimentation. Initial filtering produced a dataset of 2,705 unique signal peptide sequences, with a sequence length of 50 aminoacids. The approach used for cleavage site prediction breaks each individual sequence into smaller windows. Cross validation was used to estimate the parameters relating to the window size. The resulting convention was to use windows that contain 9 aminoacids prior to what we deem the target of the window and 2 aminoacids following that position. For an individual prediction to be considered accurate, the window predicted as most likely to contain the cleavage site in its target position, must coincide with the actual window containing the cleavage target site for the sequence.\nWith the window parameters chosen through cross validation, this results in each individual sequence of 50 aminoacids producing 39 windows with a length of 12 aminoacids each. The resulting process generates a dataset comprising of 105,495 windows. The entirety of 54 distinct physicochemical encodings offered by the Matlab bioinformatics toolbox was used for numerical representation of each sequence window, which is then represented by a 648-dimensional vector of physicochemical properties.At this point, sparse filtering learns an overcomplete representation comprising of 1500 learned features. This process generates a dataset comprising of 105,495 1500-dimensional samples.\nThere are three interesting questions which the experiments where designed to address. Concretely, the experimental comparisons are designed to establish the performance gains from using a learned representation using sparse filtering, over learning in the original feature space, using randSel for feature selection as opposed to other possible feature selection methods, and finally establishing the importance of multiple kernel learning, used for prediction.\nTo this end, a number of competing solutions were implemented. The shallow approach uses the raw physichochemical properties for prediction. For comparing the performance of different feature selection algorithms on learned representations, `1-logistic regression stability selection and nonlinear SVM-RFE are used in addition to randSel. Finally we compare the performance of a prediction rule relying on a single gaussian kernel SVM, to the performance of MKL.\nThe large size of the dataset, as well as the fact that it is highly imbalanced make for some challenges. Stability selection can readily be applied to a problem of this dimensionality. While deterministic HSIC-variants are ill-equipped to deal with the size of the resulting kernel in most current commodity hardware, the use of sampling in randSel largely alleviates the problems related with size. In order to address the issue of the imbalance, subsamples where both classes are equally represented were used.\nIn terms of producing the actual prediction the experiments examine two options. The first is using a chunking, non-linear SVM, which is the same approach that enables the use of RFE. Using vanilla LPBoost-MKL for prediction is prohibitive, owing to the memory requirements of storing the kernel matrices. The approach to rectify this problem is to use subsampling from the negative class, combined with kernel ridge regression as a weak predictor in our boosting framework. This results in a mixed-norm MKL formulation which effectively addresses the limitation of not being able to store the kernel matrices. For the l2 regularization parameter \u03bb of individual weak predictors, a very small range of parameters was used. The l1 regularization parameter D for the LPBoost prediction rule was set through cross-validation."}, {"heading": "6.3 Experimental Results", "text": "Table 1 summarizes the results for the different attempted approaches. Using the original feature representation with a non-linear SVM leads to a generalization accuracy of 67.2%. This is substantially smaller than all the approaches that rely on the learned feature representation. This suggests that there are performance gains to be had in using a learned representation.\nIn terms of using feature selection on the learned representation, the results indicate that randSel has an edge over the two other methods, with RFE also performing slightly better than l1-regularized logistic regression-based stability selection. The learned representation offers a case where it is reasonable to suspect benign non-linear collusion between features, something that both RFE and randSel are designed to take advantage of, and the large sample size allows for increased confidence when inferring such relationships. The fact that randSel outperforms RFE suggests that RFE\u2019s reliance on the support vectors for feature selection can negatively bias the feature selection procedure. Finally, the use of MKL for prediction further improves the results. Direct comparisons to state-ofthe art methods for cleavage prediction is difficult as the reported accuracy highly depends on the dataset and modelling assumptions, such as the original sequence length. However, our proposed approach appears to outperform SignalP\u2019s [14] reported accuracy of 72.9% for eukaryotic sequences."}, {"heading": "7 Conclusions", "text": "In this paper we propose randSel, a new algorithm for non-linear feature selection based on randomised estimates of HSIC. RandSel, stochastically estimates the expected importance of features at each iteration, proceeding to cull uninformative features at the end of each iteration. Our theoretical analysis gives strong guarantees for the expected performance of this procedure which is further demonstrated by testing on a number of real and artificial datasets.\nAdditionally, we presented a simple system that produces a classification rule based on non-linear learned feature combinations of increasing granularity. The architecture of the system comprises a fast, unsupervised feature learning mechanism, randomised non-linear feature selection and a multiple kernel learning based classifier. The guiding principle of this approach is to use simple components that require minimal parameter tuning, with components further down the pipeline making up for the potential shortcomings upstream. Indeed, the three different constituents of this architecture, require minimal parameter tuning and scale gracefully, and the experimental results on both datasets we employed appear to validate the approach."}, {"heading": "PREDICTION", "text": ""}, {"heading": "OUTPUT LAYER MACHINERY", "text": ""}, {"heading": "KERNEL ON FEATURESET ...", "text": ""}, {"heading": "RANDSEL", "text": ""}, {"heading": "INCOMING DATA", "text": ""}, {"heading": "OVER-COMPLETE LEARNED", "text": ""}, {"heading": "SPARSE FILTERING", "text": ""}], "references": [{"title": "Gene selection for cancer classification using support vector machines", "author": ["Guyon", "Isabelle", "Jason Weston", "Stephen Barnhill", "Vladimir Vapnik"], "venue": "Machine learning 46,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "On kernel target alignment. Advances in neural information processing systems", "author": ["N. Shawe-Taylor", "A. Kandola"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Algorithms for learning kernels based on centered alignment", "author": ["Cortes", "Corinna", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Feature selection via dependence maximization", "author": ["Song", "Le", "Alex Smola", "Arthur Gretton", "Justin Bedo", "Karsten Borgwardt"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Feature selection for SVMs. Advances in neural information processing systems: 668-674", "author": ["Weston", "Jason", "Sayan Mukherjee", "Olivier Chapelle", "Massimiliano Pontil", "Tomaso Poggio", "Vladimir Vapnik"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Multiple Kernel Learning on the Limit Order Book", "author": ["Tristan Fletcher", "Zakria Hussain", "John Shawe-Taylor"], "venue": "Proceedings of the First Workshop on Applications of Pattern Analysis", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Fast dependency-aware feature selection in very-highdimensional pattern recognition", "author": ["Somol", "Petr", "Jiri Grim", "Pavel Pudil"], "venue": "In Systems, Man, and Cybernetics (SMC) 2011 IEEE International Conference on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Challenges in Representation Learning: A report on three machine learning contests", "author": ["I.J. Goodfellow", "D. Erhan", "P.L. Carrier", "A. Courville", "M. Mirza", "B. Hamner", "Y. Bengio"], "venue": "Proceedings of the 20th International Conference on Neural Information Processing", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "PrediSi: prediction of signal peptides and their cleavage positions.", "author": ["Hiller", "Karsten", "Andreas Grote", "Maurice Scheer", "Richard Mnch", "Dieter Jahn"], "venue": "Nucleic acids research 32, no. suppl", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "SignalP 4.0: discriminating signal peptides from transmembrane regions.", "author": ["Petersen", "Thomas Nordahl", "Sren Brunak", "Gunnar von Heijne", "Henrik Nielsen"], "venue": "Nature methods 8, no", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Early propositions, such as Recursive Feature Elimination(RFE) [1] can be computationally prohibitive, while attempts to learn a convex combination of low-rank kernels may fail to encapsulate nonlinearities in the underlying relation.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3], as the empirical estimator for the Hilbert-Schmidt Independence Criterion(HSIC).", "startOffset": 66, "endOffset": 71}, {"referenceID": 2, "context": "Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3], as the empirical estimator for the Hilbert-Schmidt Independence Criterion(HSIC).", "startOffset": 66, "endOffset": 71}, {"referenceID": 3, "context": "The Hilbert-Schmidt Independence criterion is the basis for further work in [5], where \u2217Department of Computer Science, University College London, London, UK \u2020National Institute For Medical Research, London, UK", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Additionally, [5] identifies numerous connections with other existing feature selection algorithms which can be considered as instances of the framework.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "The dependence estimation of random subsets of variables is similar to the approach of [11], which is extended through bootstrapping and carefully controlled feature set sizes.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "This approach is a straightforward extension of the idea of BaHsic [5].", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "A prediction mechanism effectively utilising the increasingly granular combinations of features comprises the last step of our approach, where we take a boosting approach based on LPBOOST-MKL [10].", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "We used our proposal in the recent ICML 2013 Challenges in Representation Learning Black Box Learning challenge [8][12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "The Predisi dataset [13] of eukaryotic signal sequences was used for experimentation.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "However, our proposed approach appears to outperform SignalP\u2019s [14] reported accuracy of 72.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides probabilistic guarantees for correct identification of relevant features under reasonable assumptions. RandSel\u2019s characteristics make it an ideal candidate for identifying informative learned representations. We\u2019ve conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "creator": "LaTeX with hyperref package"}}}