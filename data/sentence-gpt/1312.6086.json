{"id": "1312.6086", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "The return of AdaBoost.MH: multi-class Hamming trees", "abstract": "Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH.", "histories": [["v1", "Fri, 20 Dec 2013 19:33:26 GMT  (97kb,D)", "http://arxiv.org/abs/1312.6086v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bal\\'azs k\\'egl"], "accepted": true, "id": "1312.6086"}, "pdf": {"name": "1312.6086.pdf", "metadata": {"source": "CRF", "title": "The return of ADABOOST.MH: multi-class Hamming trees", "authors": ["Bal\u00e1zs K\u00e9gl"], "emails": ["BALAZS.KEGL@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "ADABOOST (Freund & Schapire, 1997) is one of the most influential supervised learning algorithms of the last twenty years. It has inspired learning theoretical developments and also provided a simple and easily interpretable modeling tool that proved to be successful in many applications (Caruana & Niculescu-Mizil, 2006). It is especially the method of choice when any-time solutions are required on large data sets, so it has been one of the most successful techniques in recent large-scale classification and ranking challenges (Dror et al., 2009; Chapelle et al., 2011).\nThe original ADABOOST paper of Freund and Schapire (Freund & Schapire, 1997), besides defining binary ADABOOST, also described two multi-class extensions, ADABOOST.M1 and ADABOOST.M2. Both required a quite strong performance from the base learners, partly defeating the purpose of boosting, and saw limited practical success. The breakthrough came with Schapire\nand Singer\u2019s seminal paper (Schapire & Singer, 1999), which proposed, among other interesting extensions, ADABOOST.MH. The main idea of the this approach is to use vector-valued base classifiers to build a multi-class discriminant function of K outputs (for K-class classification). The weight vector, which plays a crucial role in binary ADABOOST, is replaced by a weight matrix over instances and labels. The simplest implementation of the concept is to use K independent one-against-all classifiers in which base classifiers are only loosely connected through the common normalization of the weight matrix. This setup works well with single decision stumps, but in most of the practical problems, boosting stumps is suboptimal compared to boosting more complex base classifiers such as trees. Technically, it is possible to build K one-against-all binary decision trees in each iteration, but this approach, for one reason or another, has not produced state-of-the-art results. As a consequence, several recent papers concentrate on replacing the boosting objective and the engine that optimizes this objective (Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013).\nThe main misconception that comes back in several papers is that ADABOOST.MH has to train K parallel oneagainst-all classifiers in each iteration. It turns out that the original setup is more general. For example, staying within the classical ADABOOST.MH framework, Ke\u0301gl & Busa-Fekete (2009) trained products of simple classifiers and obtained state-of-the-art results on several data sets. In this paper, we describe multi-class Hamming trees, another base learner that optimizes the multi-class edge without reducing the problem to K binary classifications. The key idea is to factorize general vector-valued classifiers into an input-independent vector of length K, and labelindependent scalar classifier. It turns out that optimizing such base classifiers using decision stumps as the scalar component is almost as simple as optimizing simple binary stumps on binary data. The technique can be intuitively understood as optimizing a binary cut and an output code at the same time. The main consequence of the setup is that now it is easy to build trees of these classifiers by simply discarding the label-dependent vector and using the binary classifier for partitioning the input space into two regions. ar X iv :1\n31 2.\n60 86\nv1 [\ncs .L\nG ]\n2 0\nD ec\n2 01\n3\nThe algorithm retains the conceptual elegance, power, and computational efficiency of binary ADABOOST. Algorithmically it cannot fail (the edge is always positive) and in practice it almost never overfits. Inheriting the flexibility of ADABOOST.MH, it can be applied directly (without any modification) to multi-label and multi-task classification. In experiments (carried out using an open source package of Benbouzid et al. (2012) for reproducibility) we found that ADABOOST.MH with Hamming trees performs on par with the best existing multiclass boosting algorithm AOSOLOGITBOOST (Sun et al., 2012) and with support vector machines (SVMs; Boser et al. 1992). It is also significantly better than other known implementations of ADABOOST.MH (Zhu et al., 2009; Mukherjee & Schapire, 2013).\nThe paper is organized as follows. In Section 2 we give the formal multi-class setup used in the paper and ADABOOST.MH, and show how to train factorized base learners in general. The algorithm to build Hamming trees is described in Section 3. Experiments are described in Section 4 before a brief conclusion in Section 5."}, {"heading": "2. ADABOOST.MH", "text": "In this section we first introduce the general multiclass learning setup (Section 2.1), then we describe ADABOOST.MH in detail (Section 2.2). We proceed by explaining the general requirements for base learning in ADABOOST.MH, and introduce the notion of the factorized vector-valued base learner (Section 2.3). Finally, we explain the general objective for factorized base learners and the algorithmic setup to optimize that objective. (Section 2.4)."}, {"heading": "2.1. The multi-class setup: single-label and multi-label/multi-task", "text": "For the formal description of ADABOOST.MH, let the training data be D = { (x1,y1), . . . , (xn,yn) } , where xi \u2208 Rd are observation vectors, and yi \u2208 {\u00b11}K are label vectors. Sometimes we will use the notion of an n\u00d7 d observation matrix of X = (x1, . . . ,xn) and an n \u00d7 K label matrix Y = (y1, . . . ,yn) instead of the set of pairs D.1 In multi-class classification, the single label `(x) of the observation x comes from a finite set. Without loss of generality, we will suppose that ` \u2208 L = {1, . . . ,K}. The label vector y is a one-hot representation of the correct class: the `(x)th element of y will be 1 and all the other elements will be \u22121. Besides expressing faithfully the architecture of a multi-class neural network or multi-class ADABOOST,\n1We will use bold capitals X for matrices, bold small letters xi and x,.j for its row and column vectors, respectively, and italic for its elements xi,j .\nthis representation has the advantage to be generalizable to multi-label or multi-task learning when an observation x can belong to several classes. To avoid confusion, from now on we will call y and ` the label and the label index of x, respectively. For emphasizing the distinction between multi-class and multi-label classification, we will use the term single-label for the classical multi-class setup, and reserve multi-class to situations when we talk about the three setups in general.\nThe goal of learning is to infer a vector-valued multi-class discriminant function f : X \u2192 RK .2 The single-label output of the algorithm is then `f (x) = arg max` f`(x). The classical measure of the performance of the multiclass discriminant function f is the single-label one-loss LI ( f , (x, `)) = I {` 6= `f (xi)}, which defines the singlelabel training error\nR\u0302I(f) = 1\nn n\u2211 i=1 I {`(xi) 6= `f (xi)}.3 (1)\nAnother, perhaps more comprehensive, way to measure the performance of f is by computing the weighted Hamming loss LH ( f , (x,y),w ) = \u2211K `=1 w`I { sign ( f`(x) ) 6= y` } where w = [ w` ]\nis an RK-valued \u201cuser-defined\u201d weight vector over labels. The corresponding empirical risk (training error) is\nR\u0302H(f ,W) = 1\nn n\u2211 i=1 K\u2211 `=1 wi,`I { sign ( f`(xi) ) 6= yi,` } , (2)\nwhere W = [ wi,` ] is an n \u00d7 k weight matrix over data points and labels.\nIn the multi-label/multi-task setup, when, for example, it is equally important to predict that a song is \u201cfolk\u201d as predicting that it is sung by a woman, the Hamming loss with uniform weights w` = 1/K, ` = 1, . . . ,K is a natural measure of performance: it represents the uniform error rate of missing any class sign y` of a given observation x. In single-label classification, w is usually set asymmetrically to\nw` =\n{ 1 2 if ` = `(x) (i.e., if y` = 1),\n1 2(K\u22121) otherwise (i.e., if y` = \u22121).\n(3)\nThe idea behind this scheme is that it will create K wellbalanced one-against-all binary classification problems: if\n2Instead of the original notation of (Schapire & Singer, 1999) where both x and ` are inputs of a function f(x, `) outputting a single real-valued score, we use the notation f(x) = (f1(x), . . . , fK(x) ) since we feel it expresses better that x is (in general) continuous and ` is a discrete index. 3The indicator function I {A} is 1 if its argumentA is true and 0 otherwise.\nwe start with a balanced single-label multi-class problem, that is, if each of the K classes have n/K examples in D, then for each class `, the sum of the weights of the positive examples in the column w\u00b7,` of the weight matrix W will be equal to the sum of the weights of the negative examples. Note that both schemes boil down to the classical uniform weighting in binary classification."}, {"heading": "2.2. ADABOOST.MH", "text": "The goal of the ADABOOST.MH algorithm (Schapire & Singer 1999; Figure 1) is to return a vector-valued discriminant function f (T ) : Rd \u2192 RK with a small Hamming loss R\u0302H(f ,W) (2) by minimizing the weighted multi-class exponential margin-based error\nR\u0302EXP ( f (T ),W ) = 1\nn n\u2211 i=1 K\u2211 `=1 wi,` exp ( \u2212f (T )` (xi)yi,` ) .\n(4) Since exp(\u2212\u03c1) \u2265 I {\u03c1 < 0}, (4) upper bounds the Hamming loss R\u0302H ( f (T ),W ) (2). ADABOOST.MH builds the\nfinal discriminant function f (T )(x) = \u2211T t=1 h\n(t)(x) as a sum of T base classifiers h(t) : X \u2192 RK returned by a base learner algorithm BASE ( X,Y,W(t) ) in each iteration t."}, {"heading": "2.3. Base learning for ADABOOST.MH", "text": "The goal of multi-class base learning is to minimize the base objective\nZ(t) = min h Z ( h,W(t) ) = n\u2211 i=1 K\u2211 `=1 w (t) i,` e \u2212h`(xi)yi,` . (5)\nIt is easy to show (Schapire & Singer, 1999) that i) the oneerror R\u0302I(f (T )) (1) is upper bounded by \u220fT t=1 Z\n(t), and so ii) if the standard weak-learning condition Z(t) \u2264 1 \u2212 \u03b4 holds, R\u0302I(f) becomes zero in T \u223c O(log n) iterations.\nIn general, any vector-valued multi-class learning algorithm can be used to minimize (5). Although this goal is clearly defined in (Schapire & Singer, 1999), efficient base learning algorithms have never been described in detail. In most recent papers (Zhu et al., 2009; Mukherjee & Schapire, 2013) where ADABOOST.MH is used as baseline, the base learner is a classical single-label decision tree which has to be grown rather large to satisfy the weaklearning condition, and, when boosted, yields suboptimal results (Section 4). The reason why methods for learning multi-class {\u00b11}K-valued base classifiers had not been developed before is because they have to be boosted: since they do not select a single label, they cannot be used as stand-alone multi-class classifiers.\nAlthough it is not described in detail, it seems that the base classifier used in the original paper of Schapire & Singer (1999) is a vector ofK independent decision stumps h(x) = ( h1(x), . . . , hK(x) ) . These stumps cannot be used as node classifiers to grow decision trees since they do not define a single cut that depends only on the input (see Section 3 for a more detailed discussion). To overcome this problem, we propose base learning algorithms that factorize h(x) into\nh(x) = \u03b1v\u03d5(x), (6)\nwhere \u03b1 \u2208 R+ is a positive real valued base coefficient, v is an input-independent vote vector of length K, and \u03d5(x) is a label-independent scalar classifier. In discrete ADABOOST.MH, both components are binary, that is, v \u2208 {\u00b11}K and \u03d5(x) : Rd \u2192 {\u00b11}. The setup can be extended to real-valued classifiers \u03d5(x) : Rd \u2192 R, also known as confidence-rated classifiers, and it is also easy to make the vote vector v real-valued (in which case, without the loss of generality, \u03b1 would be set to 1). Both variants are known under the name of real ADABOOST.MH. Although there might be slight differences in the practical performance of real and discrete ADABOOST.MH, here we decided to stick to the discrete case for the sake of simplicity."}, {"heading": "2.4. Casting the votes", "text": "To start, we show how to set \u03b1 and v in general if the scalar base classifier \u03d5 is given. The intuitive semantics of (6) is the following. The binary classifier \u03d5(x) cuts the input space into a positive and a negative region. In binary classification this is the end of the story: we need \u03d5(x) to be well-correlated with the binary class labels y. In multiclass classification it is possible that \u03d5(x) correlates with some of the class labels y` and anti-correlates with others. This free choice is expressed by the binary \u201cvotes\u201d v` \u2208 {\u00b11}. We say that \u03d5(x) votes for class ` if v` = +1 and it votes against class ` if v` = \u22121. As in binary classification, \u03b1 expresses the overall quality of the classifier v\u03d5(x): \u03b1 is monotonically decreasing with respect to the weighted error of v\u03d5(x).\nThe advantage of the setup is that, given the binary classifier \u03d5(x), the optimal vote vector v and the coefficient \u03b1 can be set in an efficient way. To see this, first let us define the weighted per-class error rate\n\u00b5`\u2212 = n\u2211 i=1 wi,`I {\u03d5(xi) 6= yi,`}, (7)\nand the weighted per-class correct classification rate\n\u00b5`+ = n\u2211 i=1 wi,`I {\u03d5(xi) = yi,`} (8)\nfor each class ` = 1, . . . ,K. With this notation, Z ( h,W ) simplifies to (see Appendix A)\nZ(h,W) = e\u03b1 + e\u2212\u03b1\n2 \u2212 e\n\u03b1 \u2212 e\u2212\u03b1\n2\nK\u2211 `=1 v` ( \u00b5`+ \u2212 \u00b5`\u2212 ) .\n(9) The quantity\n\u03b3` = v` ( \u00b5`+ \u2212 \u00b5`\u2212 ) = n\u2211 i=1 wi,`v`\u03d5(xi)yi,` (10)\nis called the classwise edge of h(x). The full multi-class edge of the classifier is then\n\u03b3 = \u03b3(v, \u03d5,W) = K\u2211 `=1 \u03b3` = K\u2211 `=1 v` ( \u00b5`+ \u2212 \u00b5`\u2212 ) =\nn\u2211 i=1 K\u2211 `=1 wi,`v`\u03d5(xi)yi,`.\n(11)\nWith this notation, the classical (Freund & Schapire, 1997) binary coefficient \u03b1 is recovered: it is easy to see that (9) is minimized when\n\u03b1 = 1\n2 log\n1 + \u03b3 1\u2212 \u03b3 . (12)\nWith this optimal coefficient, (9) becomes Z(h,W) =\u221a 1\u2212 \u03b32, so Z(h,W) is minimized when \u03b3 is maximized. From (11) it then follows that Z(h,W) is minimized if v` agrees with the sign of ( \u00b5`+ \u2212 \u00b5`\u2212 ) , that is,\nv` = { 1 if \u00b5`+ > \u00b5`\u2212 \u22121 otherwise\n(13)\nfor all classes ` = 1, . . . ,K.\nThe setup of factorized base classification (6) has another important consequence: the preservation of the weaklearning condition. Indeed, if \u03d5(x) is slightly better then a coin toss, \u03b3 will be positive. Another way to look at it is to say that if a (\u03d5,v) combination has a negative edge \u03b3 < 0, then the edge of its complement (either (\u2212\u03d5,v) or (\u03d5,\u2212v)) will be \u2212\u03b3 > 0. To understand the significance of this, consider a classical single-label base classifier h : X \u2192 L = {1, . . . ,K}, required by ADABOOST.M1. Now if h(x) is slightly better than a coin toss, all one can hope for is an error rate slightly lower than K\u22121 K (which is equivalent to an edge slightly higher than 2\u2212K K ). To achieve the error of 1 2 (zero edge), required for continuing boosting, one has to come up with a base learner which is significantly better than a coin toss.\nThere is a long line of research on output codes similar in spirit to our setup. The boosting engine in these works is usually slightly different from ADABOOST.MH since it attempts to optimize the multi-class hinge loss, but the factorization of the multi-class base classifier is similar to (6). Formally, the vote vector v in this framework is one column in an output code matrix. In the simplest setup this matrix is fixed beforehand by maximizing the error correcting capacity of the matrix (Dietterich & Bakiri, 1995; Allwein et al., 2001). A slightly better solution (Schapire, 1997; Guruswami & Sahai, 1999; Sun et al., 2005) is to wait until the given iteration to pick v by maximizing\nv\u2217 = arg max v n\u2211 i=1 K\u2211 `=1 wi,`I { v` 6= v`(xi) } ,\nand then to choose the optimal binary classifier \u03d5 with this fixed vote (or code) vector v\u2217 (although in practice it seems to be better to fix v to a random binary vector; Sun et al. 2005). The state of the art in this line of research is to iterate between optimizing \u03d5 with a fixed v and then picking the best v with a fixed \u03d5 (Li, 2006; Ke\u0301gl & Busa-Fekete, 2009; Gao & Koller, 2011).\nIt turns out that if \u03d5 is a decision stump, exhaustive search for both the best binary cut (threshold) and the best vote vector can be carried out using one single sweep in \u0398(nK) time. The algorithm is a simple extension of the classical binary decision stump learner; for the sake of completeness, we provide the pseudocode in Appendix B. The\ncomputational efficiency of this learning algorithm combined with the factorized form (6) of the classifier allows us to build multiclass Hamming trees in an efficient manner, circumventing the problem of global maximization of the edge with respect to \u03d5 and v."}, {"heading": "3. Hamming trees", "text": "Classification trees (Quinlan, 1986) have been widely used for multivariate classification since the 80s. They are especially efficient when used as base learners in ADABOOST (Caruana & Niculescu-Mizil, 2006; Quinlan, 1996). Their main disadvantage is their variance with respect to the training data, but when averaged over T different runs, this problem largely disappears. The most commonly used tree learner is C4.5 of Quinlan (1993). Whereas this tree implementation is a perfect choice for binary ADABOOST, it is suboptimal for ADABOOST.MH since it outputs a single-label classifier with no guarantee of a positive multi-class edge (11). Although this problem can be solved in practice by building large trees, it seems that using these large single-class trees is suboptimal (Section 4).\nThe main technical difficulty of building trees out of generic {\u00b11}K-valued multi-class classifiers h(x) is that they do not necessarily implement a binary cut x 7\u2192 {\u00b11}, and partitioning the data into all the possibly 2K children at a tree node leads to rapid overfitting. Factorizing the multiclass classifier h(x) into an input-independent vote vector v and a label-independent binary classifier \u03d5(x) as in (6) solves this problem. Base classifiers are trained as usual at each new tree leaf. In case this leaf remains a leaf, the full classifier h(x) is used for instances x that arrive to this leaf. If it becomes an inner node, the vote vector v is discarded, and the partitioning of the data set is based on solely the binary classifier \u03d5(x). An advantage of this formalization is that we can use any multi-class base classifier of the form (6) for the tree cuts, so the Hamming tree algorithm can be considered as a \u201cmeta learner\u201d which can be used on the top of any factorized base learner.\nFormally, a binary classification tree with N inner nodes (N + 1 leaves) consists of a list of N base classifiers H = (h1, . . . ,hN ) of the form hj(x) = \u03b1jvj\u03d5j(x) and two index lists l = (l1, . . . , lN ) and r = (r1, . . . , rN ) with l, r \u2208 (N \u222a {NULL})N . lj and rj represent the indices of the left and right children of the jth node of the tree, respectively. The node classifier in the jth node is defined\nrecursively as\nhj(x) =  \u2212vj if \u03d5j(x) = \u22121 \u2227 lj = NULL (left leaf), vj if \u03d5j(x) = +1 \u2227 rj = NULL (right leaf), hlj (x) if \u03d5j(x) = \u22121 \u2227 lj 6= NULL (left inner node),\nhrj (x) if \u03d5j(x) = +1 \u2227 rj 6= NULL (right inner node).\n(14)\nThe final tree classifier hH,l,r(x) = \u03b1h1(x) itself is not a factorized classifier (6).4 In particular, hH,l,r(x) uses the local vote vectors vj determined by each leaf instead of a global vote vector. On the other hand, the coefficient \u03b1 is unique, and it is determined in the standard way\n\u03b1 = 1\n2 log\n1 + \u03b3(h1,W) 1\u2212 \u03b3(h1,W)\nbased on the edge of the tree classifier h1. The local coefficients \u03b1j returned by the base learners are discarded (along with the vote vectors in the inner nodes).\nFinding the optimal N -inner-node tree is a difficult combinatorial problem. Most tree-building algorithms are therefore sub-optimal by construction. For ADABOOST this is not a problem: we can continue boosting as long as the edge is positive. Classification trees are usually built in a greedy manner: at each stage we try to cut all the current leaves j by calling the base learner of the data points reaching the jth leaf, then select the best node to cut, convert the old leaf into an inner node, and add two new leaves. The difference between the different algorithms is in the way the best node is selected. Usually, we select the node that improves a gain function the most. In ADABOOST.MH the natural gain is the edge (11) of the base classifier. Since the data set (X,Y) is different at each node, we include it explicitly in the argument of the full multi-class edge\n\u03b3(v, \u03d5,X,Y,W) = n\u2211 i=1 K\u2211 `=1 I {xi \u2208 X}wi,`v`\u03d5(xi)yi,`.\nNote that in this definition we do not require that the weights of the selected points add up to 1. Also note that this gain function is additive on subsets of the original data set, so the local edges in the leaves add up to the edge of the full tree. This means that any improvement in the local edge directly translates to an improvement of the tree edge. This is a crucial property: it assures that the edge of the tree is always positive as long as the local edges in the\n4Which is not a problem: we will not want to build trees of trees.\ninner nodes are positive, so any weak binary classifier \u03c6(x) can be used to define the inner cuts and the leaves.\nThe basic operation when adding a tree node with a scalar binary classifier (cut) \u03d5 is to separate the data matrices X, Y, and W according to the sign of classification \u03d5(xi) for all xi \u2208 X. The pseudocode is straightforward, but for the sake of completeness, we include it in the supplementary (Appendix C, Figure 5).\nBuilding a tree is usually described in a recursive way but we find the iterative procedure easier to explain, so our pseudocode in Figure 2 contains this version. The main idea is to maintain a priority queue, a data structure that allows inserting objects with numerical keys into a set, and extracting the object with the maximum key (Cormen et al., 2009). The key will represent the improvement of the edge when cutting a leaf. We first call the base learner on the full data set (line 1) and insert it into the priority queue with its edge \u03b3(v, \u03d5,X,Y,W) (line 3) as the key. Then in each iteration, we extract the leaf that would provide the best edge improvement among all the leaves in the priority queue (line 7), we partition the data set (line 11), call the base learners on the two new leaves (line 12), and insert them into the priority queue using the difference between the old edge on the partitioned data sets and the new edges of the base classifiers in the two new leaves (line 13). When inserting a leaf into the queue, we also save the sign of the cut (left or right child) and the index of the parent, so the index vectors l and r can be set properly in line 8.\nWhen the priority queue is implemented as a heap, both the insertion and the extraction of the maximum takes O(logN) time (Cormen et al., 2009), so the total running time of the procedure is O ( N(TBASE +n+ logN) ) , where TBASE is the running time of the base learner. Since N cannot be more than n, the running time is O ( N(TBASE + n) ) . If the base learners cutting the leaves are decision stumps, the total running time is O(nKdN). In the procedure we have no explicit control over the shape of the tree, but if it happens to be balanced, the running time can further be improved to O(nKd logN)."}, {"heading": "4. Experiments", "text": "Full reproducibility was one of the key motivations when we designed our experimental setup. All experiments were done using the open source multiboost software of Benbouzid et al. (2012), version 1.2. In addition, we will make public all the configuration files, train/test/validation cuts, and the scripts that we used to set up the hyperparameter validation.\nWe carried out experiments on five mid-sized (isolet, letter, optdigits, pendigits, and USPS) and nine small (balance, blood, wdbc, breast, ecoli, iris, pima, sonar,\nand wine) data sets from the UCI repository. The five sets were chosen to overlap with the selections of most of the recent multi-class boosting papers (Ke\u0301gl & Busa-Fekete, 2009; Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013), The small data sets were selected for comparing ADABOOST.MH with SVMs using Gaussian kernels, taking the results of a recent paper (Duch et al., 2012) whose experimental setup we adopted. All numerical results (multi-class test errors R\u0302I(f) (1) and test learning curves) are available at https://www.lri.fr/\u02dckegl/research/ multiboostResults.pdf, one experiment per page for clarity. Tables 1 and 2 contain summaries of the results.\nHyperparameter optimization is largely swept under the rug in papers describing alternative multi-class boosting methods. Some report results with fixed hyperparameters (Zhu et al., 2009; Sun et al., 2012) and others give the full table of test errors for a grid of hyperparameters (Ke\u0301gl & BusaFekete, 2009; Li, 2009a;b; Mukherjee & Schapire, 2013). Although the following procedure is rather old, we feel the need to detail it for promoting a more scrupulous comparison across papers.\nFor the small data sets we ran 10\u00d710 cross-validation (CV) to optimize the hyperparameters and the estimate the generalization error. For the number of inner nodes we do a grid search (we also considered using the \u201cone sigma\u201d rule for biasing the selection towards smaller trees, but the simple minimization proved to be better). For robustly estimating the optimal stopping time we use a smoothed test error. For the formal description, let R\u0302(t) be the average test error (1) of the ten validation runs after t iterations. We run ADABOOST.MH for Tmax iterations, and compute the optimal stopping time using the minimum of the smoothed test error using a linearly growing sliding window, that is,\nT \u2217 = arg min T :Tmin<T\u2264Tmax\n1\nT \u2212 b0.8T c T\u2211 t=b0.8Tc R\u0302(t), (15)\nwhere Tmin was set to a constant 50 to avoid stopping too early due to fluctuations. For selecting the best number of inner nodes N , we simply minimized the smoothed test\nerror over a predefined grid\nN\u2217 = min N\u2208N R\u0302(T \u2217 N )(N)\nwhere T \u2217N and R\u0302 (t)(N) are the optimal stopping time (15) and the test error, respectively, in the run with N inner nodes, and N is the set of inner nodes participating in the grid search. Then we re-run ADABOOST.MH on the joined training/validation set using the selected hyperparameters N\u2217 and T \u2217N\u2217 . The error R\u0302i in the ith training/test fold is then computed on the held-out test set. In the tables we report the mean error and the standard deviation. On the medium-size data sets we ran 1 \u00d7 5 CV (using the designated test sets where available) following the same procedure. In this case the report the binomial standard devia-\ntion \u221a R\u0302(1\u2212 R\u0302)/n. Further details and the description and\nexplanation of some slight variations of this experimental setup are available at https://www.lri.fr/\u02dckegl/ research/multiboostResults.pdf.\nOn the small data sets, Duch et al. (2012) used the exact same protocol, so, although the folds are not the same, the results are directly comparable. The error bars represent the standard deviation of the test errors over the ten test folds not divided by \u221a 10, contrary to common practice, since the training set of the folds are highly correlated. The large error bars are the consequence of the small size and the noisiness of these sets. They make it difficult to establish any significant trends. We can safely state that ADABOOST.MH is on par with SVM (it is certainly not worse, \u201cwinning\u201d on six of the nine sets), widely considered one of the the best classification methods for small data sets.\nEven though on the mid-sized data sets there are dedicated test sets used by most of the experimenters, comparing ADABOOST.MH to alternative multi-class boosting techniques is somewhat more difficult since none of the papers do proper hyperparameter tuning. Most of the papers report results with a table of errors given for a set of hyperparameter choices, without specifying which hyperparameter choice would be picked by proper validation. For methods that are non-competitive with ADABOOST.MH (SAMME of Zhu et al. (2009) and ADABOOST.MM of Mukherjee & Schapire (2013)) we report the post-validated best error which may be significantly lower than the error corresponding to the hyperparameter choice selected by proper validation. For methods where this choice would unfairly bias the comparison (AOSOLOGITBOOST (Sun et al., 2012), ABCLOGITBOOST, LOGITBOOST, and ABCMART (Li, 2009a;b)), we chose the best overall hyperparameter J = 20 and \u03bd = 0.1, suggested by the Li (2009a;b). At https://www.lri.fr/\u02dckegl/ research/multiboostResults.pdf (but not in Table 1) we give both errors for some of the methods. Proper hyperparameter-validation should put the correct test error estimates between those two limits. Since ADABOOST.MH with decision products (Ke\u0301gl & BusaFekete, 2009) is also implemented in multiboost (Benbouzid et al., 2012), for this method we re-ran experiments with the protocol described above.\nThe overall conclusion is that AOSOLOGITBOOST (Sun et al., 2012) and ADABOOST.MH with Hamming trees are the best algorithms (ADABOOST.MH winning on all the five data sets but within one standard deviation). ADABOOST.MH with decision products (Ke\u0301gl & BusaFekete, 2009) and ABCLOGITBOOST are slightly weaker, as also noted by (Sun et al., 2012). SAMME (Zhu et al., 2009) and ADABOOST.MM (Mukherjee & Schapire, 2013) perform below the rest of the methods on the two data sets shared among all the papers (even though we give post-validated results). Another important conclusion is that ADABOOST.MH with Hamming trees is significantly better then other implementations of ADABOOST.MH in (Zhu et al., 2009; Mukherjee & Schapire, 2013), assumably implemented using single-label trees (the errors reported by Mukherjee & Schapire (2013) are especially conspicuous).\nADABOOST.MH with Hamming trees also achieves good results on image recognition problems. On MNIST, boosting trees of stumps over pixels with eight inner nodes and about 50000 iterations has a test error of 1.25%, making it one of the best no-domain-knowledge \u201cshallow\u201d classifiers. Using stumps over Haar filters (Viola & Jones, 2004), boosted trees with four inner nodes and 10000 iterations achieves a test error of 0.85%, comparable to classical convolutional nets (LeCun et al., 1998).\nADABOOST.MH with Hamming trees, usually combined with calibration (Platt, 2000; Niculescu-Mizil & Caruana, 2005) and model averaging, has been also successful in recent data challenges. On the Kaggle emotions data challenge, although not competitive with deep learning techniques, out-of-the-box ADABOOST.MH with Hamming trees over Haar filters finished 17th place with a test error of 57%. In the Yahoo! Learning-to-Rank Challenge (Chapelle et al., 2011) it achieved top ten performances with results not significantly different from the winning scores. Finally, in the recent INTERSPEECH Challenge it won the Emotion sub-challenge and it was runner up in the Social Signals sub-challenge."}, {"heading": "5. Conclusion", "text": "In this paper we introduced Hamming trees that optimize the multi-class edge prescribed by ADABOOST.MH without reducing the multi-class problem to K binary oneagainst-all classifications. We showed that without this restriction, often considered mandatory, ADABOOST.MH is one of the best off-the-shelf multi-class classification algorithms. The algorithm retains the conceptual elegance, power, and computational efficiency of binary ADABOOST.\nUsing decision stumps at the inner nodes and at the leaves of the tree is a natural choice due to the efficiency of the learning algorithm, nevertheless, the general setup described in this paper allows for using any binary classifier. One of the avenues investigated for future work is to try stronger classifiers, such as SVMs, as binary cuts. The formal setup described in Section 2.1 does not restrict the algorithm to single-label problems; another direction for future work is to benchmark it on standard multi-label and sequence-to-sequence classification problems (Dietterich et al., 2008)."}, {"heading": "A. Showing (9)", "text": "Z(h,W) = n\u2211 i=1 K\u2211 `=1 wi,` exp ( \u2212h`(xi)yi,` ) = n\u2211 i=1 K\u2211 `=1 wi,` exp ( \u2212\u03b1v`\u03d5(xi)yi,` ) (16)\n= n\u2211 i=1 K\u2211 `=1 ( wi,`I {v`\u03d5(xi)yi,` = 1}e\u2212\u03b1 + wi,`I {v`\u03d5(xi)yi,` = \u22121}e\u03b1 ) =\nK\u2211 `=1 ( \u00b5`+I {v` = +1}+ \u00b5`\u2212I {v` = \u22121} ) e\u2212\u03b1\n+ K\u2211 `=1 ( \u00b5`\u2212I {v` = +1}+ \u00b5`+I {v` = \u22121} ) e\u03b1 (17)\n= K\u2211 `=1 ( I {v` = +1} ( e\u2212\u03b1\u00b5`+ + e \u03b1\u00b5`\u2212 ) + I {v` = \u22121} ( e\u2212\u03b1\u00b5`\u2212 + e \u03b1\u00b5`+ ))\n= K\u2211 `=1 ( 1 + v` 2 ( e\u2212\u03b1\u00b5`+ + e \u03b1\u00b5`\u2212 ) + 1\u2212 v` 2 ( e\u2212\u03b1\u00b5`\u2212 + e \u03b1\u00b5`+ ))\n= 1\n2 K\u2211 `=1 (( e\u03b1 + e\u2212\u03b1 )( \u00b5`+ + \u00b5`\u2212 ) \u2212 v` ( e\u03b1 \u2212 e\u2212\u03b1 )( \u00b5`+ \u2212 \u00b5`\u2212 )) = e\u03b1 + e\u2212\u03b1\n2 \u2212 e\n\u03b1 \u2212 e\u2212\u03b1\n2\nK\u2211 `=1 v` ( \u00b5`+ \u2212 \u00b5`\u2212 ) . (18)\n(16) comes from the definition (6) of h and (17) follows from the definitions (7) and (8) of \u00b5`\u2212 and \u00b5`+. In the final step (18) we used the fact that\nK\u2211 `=1 ( \u00b5`+ + \u00b5`\u2212 ) = n\u2211 i=1 K\u2211 `=1 wi,` = 1."}, {"heading": "B. Multi-class decision stumps", "text": "The simplest scalar base learner used in practice on numerical features is the decision stump, a one-decision two-leaf decision tree of the form\n\u03d5j,b(x) = { 1 if x(j) \u2265 b, \u22121 otherwise,\nwhere j is the index of the selected feature and b is the decision threshold. If the feature values ( x (j) 1 , . . . , x (j) n ) are preordered before the first boosting iteration, a decision stump maximizing the edge (11) (or minimizing the energy (16)5) can be found very efficiently in \u0398(ndK) time.\nThe pseudocode of the algorithm is given in Figure 3. STUMPBASE first calculates the edge vector \u03b3(0) of the constant classifier h(0)(x) \u2261 1 which will serve as the initial edge vector for each featurewise edge-maximizer. Then it loops over the features, calls BESTSTUMP to return the best featurewise stump, and then selects the best of the best by minimizing the energy (16). BESTSTUMP loops over all (sorted) feature values s1, . . . , sn\u22121. It considers all thresholds b halfway between two non-identical feature values si 6= si+1. The main trick (and, at the same time, the bottleneck of the algorithm) is the update of the classwise edges in lines 4-5: when the threshold moves from b = si\u22121+si2 to b = si+si+1 2 , the classwise edge \u03b3` of 1\u03d5(x) (that is, v\u03d5(x) with v = 1) can only change by \u00b1wi,`, depending on the sign yi,` (Figure 4). The total edge of v\u03d5(x) with optimal votes (13) is then the sum of the absolute values of the classwise edges of 1\u03d5(x) (line 7).\n5Note the distinction: for full binary v the two are equivalent, but for ternary or real valued v and/or real valued \u03c6(x) they are not. In Figure 3 we are maximizing the edge within each feature (line 7 in BESTSTUMP) but across features we are minimizing the energy (line 7 in STUMPBASE). Updating the energy inside the inner loop (line 4) could not be done in \u0398(K) time."}, {"heading": "C. Cutting the data set", "text": "The basic operation when adding a tree node with a scalar binary classifier (cut) \u03d5 is to separate the data matrices X, Y, and W according to the sign of the classification \u03d5(xi) for all xi \u2208 X. Figure 5 contains the pseudocode of this simple operation."}], "references": [{"title": "Reducing multiclass to binary: a unifying approach for margin classifiers", "author": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "MultiBoost: a multi-purpose boosting package", "author": ["D. Benbouzid", "R. Busa-Fekete", "N. Casagrande", "Collin", "F.-D", "B. K\u00e9gl"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Benbouzid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Benbouzid et al\\.", "year": 2012}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B. Boser", "I. Guyon", "V. Vapnik"], "venue": "In Fifth Annual Workshop on Computational Learning Theory, pp", "citeRegEx": "Boser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Boser et al\\.", "year": 1992}, {"title": "eds.). Yahoo! Learning-to-Rank Challenge, volume 14 of JMLR W&CP", "author": ["O. Chapelle", "Y. Chang", "T.Y. Liu"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Introduction to Algorithms", "author": ["T. Cormen", "C. Leiserson", "R. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2009}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri", "year": 1995}, {"title": "Gradient tree boosting for training conditional random fields", "author": ["T.G. Dietterich", "Hao", "Guohua", "A. Ashenfelter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dietterich et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 2008}, {"title": "Make it cheap: Learning with O(nd) complexity", "author": ["W. Duch", "N. Jankowski", "T. Maszczyk"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Duch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duch et al\\.", "year": 2012}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "Multiclass boosting with hinge loss based on output coding", "author": ["T. Gao", "D. Koller"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Gao and Koller,? \\Q2011\\E", "shortCiteRegEx": "Gao and Koller", "year": 2011}, {"title": "Multiclass learning, boosting, and error-correcting codes", "author": ["V. Guruswami", "A. Sahai"], "venue": "In Conference on Computational Learning Theory,", "citeRegEx": "Guruswami and Sahai,? \\Q1999\\E", "shortCiteRegEx": "Guruswami and Sahai", "year": 1999}, {"title": "Boosting products of base classifiers", "author": ["B. K\u00e9gl", "R. Busa-Fekete"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "K\u00e9gl and Busa.Fekete,? \\Q2009\\E", "shortCiteRegEx": "K\u00e9gl and Busa.Fekete", "year": 2009}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Multiclass boosting with repartitioning", "author": ["Li", "Ling"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li and Ling.,? \\Q2006\\E", "shortCiteRegEx": "Li and Ling.", "year": 2006}, {"title": "ABC-Boost: Adaptive base class boost for multiclass classification", "author": ["P. Li"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li,? \\Q2009\\E", "shortCiteRegEx": "Li", "year": 2009}, {"title": "ABC-LogitBoost for multi-class classification", "author": ["P. Li"], "venue": "Technical Report arXiv:0908.4144, Arxiv preprint,", "citeRegEx": "Li,? \\Q2009\\E", "shortCiteRegEx": "Li", "year": 2009}, {"title": "A theory of multiclass boosting", "author": ["I. Mukherjee", "R.E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mukherjee and Schapire,? \\Q2013\\E", "shortCiteRegEx": "Mukherjee and Schapire", "year": 2013}, {"title": "Obtaining calibrated probabilities from boosting", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "In Proceedings of the 21st International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Niculescu.Mizil and Caruana,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil and Caruana", "year": 2005}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt,? \\Q2000\\E", "shortCiteRegEx": "Platt", "year": 2000}, {"title": "Induction of decision trees", "author": ["J. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Programs for Machine Learning", "author": ["Quinlan", "J. C"], "venue": null, "citeRegEx": "Quinlan and C4.5,? \\Q1993\\E", "shortCiteRegEx": "Quinlan and C4.5", "year": 1993}, {"title": "Bagging, boosting and C4.5", "author": ["J. Quinlan"], "venue": "In Proceedings of the 13th National Conference on Artificial Intelligence, pp", "citeRegEx": "Quinlan,? \\Q1996\\E", "shortCiteRegEx": "Quinlan", "year": 1996}, {"title": "Using output codes to boost multiclass learing problems", "author": ["R.E. Schapire"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Schapire,? \\Q1997\\E", "shortCiteRegEx": "Schapire", "year": 1997}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "AOSO-LogitBoost: Adaptive one-vs-one LogitBoost for multi-class problem", "author": ["P. Sun", "M.D. Reid", "J. Zhou"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Sun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Unifying the error-correcting and output-code AdaBoost within the margin framework", "author": ["Y. Sun", "S. Todorovic", "J. Li", "D. Wu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Sun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2005}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola and Jones,? \\Q2004\\E", "shortCiteRegEx": "Viola and Jones", "year": 2004}, {"title": "Multi-class AdaBoost", "author": ["J. Zhu", "H. Zou", "S. Rosset", "T. Hastie"], "venue": "Statistics and its Interface,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "It is especially the method of choice when any-time solutions are required on large data sets, so it has been one of the most successful techniques in recent large-scale classification and ranking challenges (Dror et al., 2009; Chapelle et al., 2011).", "startOffset": 208, "endOffset": 250}, {"referenceID": 27, "context": "As a consequence, several recent papers concentrate on replacing the boosting objective and the engine that optimizes this objective (Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013).", "startOffset": 133, "endOffset": 210}, {"referenceID": 24, "context": "As a consequence, several recent papers concentrate on replacing the boosting objective and the engine that optimizes this objective (Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013).", "startOffset": 133, "endOffset": 210}, {"referenceID": 24, "context": "MH with Hamming trees performs on par with the best existing multiclass boosting algorithm AOSOLOGITBOOST (Sun et al., 2012) and with support vector machines (SVMs; Boser et al.", "startOffset": 106, "endOffset": 124}, {"referenceID": 2, "context": ", 2012) and with support vector machines (SVMs; Boser et al. 1992).", "startOffset": 41, "endOffset": 66}, {"referenceID": 27, "context": "MH (Zhu et al., 2009; Mukherjee & Schapire, 2013).", "startOffset": 3, "endOffset": 49}, {"referenceID": 1, "context": "In experiments (carried out using an open source package of Benbouzid et al. (2012) for reproducibility) we found that ADABOOST.", "startOffset": 60, "endOffset": 84}, {"referenceID": 27, "context": "In most recent papers (Zhu et al., 2009; Mukherjee & Schapire, 2013) where ADABOOST.", "startOffset": 22, "endOffset": 68}, {"referenceID": 22, "context": "Although it is not described in detail, it seems that the base classifier used in the original paper of Schapire & Singer (1999) is a vector ofK independent decision stumps h(x) = ( h1(x), .", "startOffset": 104, "endOffset": 129}, {"referenceID": 0, "context": "In the simplest setup this matrix is fixed beforehand by maximizing the error correcting capacity of the matrix (Dietterich & Bakiri, 1995; Allwein et al., 2001).", "startOffset": 112, "endOffset": 161}, {"referenceID": 22, "context": "A slightly better solution (Schapire, 1997; Guruswami & Sahai, 1999; Sun et al., 2005) is to wait until the given iteration to pick v by maximizing", "startOffset": 27, "endOffset": 86}, {"referenceID": 25, "context": "A slightly better solution (Schapire, 1997; Guruswami & Sahai, 1999; Sun et al., 2005) is to wait until the given iteration to pick v by maximizing", "startOffset": 27, "endOffset": 86}, {"referenceID": 25, "context": "and then to choose the optimal binary classifier \u03c6 with this fixed vote (or code) vector v\u2217 (although in practice it seems to be better to fix v to a random binary vector; Sun et al. 2005).", "startOffset": 92, "endOffset": 188}, {"referenceID": 19, "context": "Classification trees (Quinlan, 1986) have been widely used for multivariate classification since the 80s.", "startOffset": 21, "endOffset": 36}, {"referenceID": 21, "context": "They are especially efficient when used as base learners in ADABOOST (Caruana & Niculescu-Mizil, 2006; Quinlan, 1996).", "startOffset": 69, "endOffset": 117}, {"referenceID": 19, "context": "Classification trees (Quinlan, 1986) have been widely used for multivariate classification since the 80s. They are especially efficient when used as base learners in ADABOOST (Caruana & Niculescu-Mizil, 2006; Quinlan, 1996). Their main disadvantage is their variance with respect to the training data, but when averaged over T different runs, this problem largely disappears. The most commonly used tree learner is C4.5 of Quinlan (1993). Whereas this tree implementation is a perfect choice for binary ADABOOST, it is suboptimal for ADABOOST.", "startOffset": 22, "endOffset": 438}, {"referenceID": 4, "context": "The main idea is to maintain a priority queue, a data structure that allows inserting objects with numerical keys into a set, and extracting the object with the maximum key (Cormen et al., 2009).", "startOffset": 173, "endOffset": 194}, {"referenceID": 4, "context": "When the priority queue is implemented as a heap, both the insertion and the extraction of the maximum takes O(logN) time (Cormen et al., 2009), so the total running time of the procedure is O ( N(TBASE +n+ logN) ) , where TBASE is the running time of the base learner.", "startOffset": 122, "endOffset": 143}, {"referenceID": 1, "context": "All experiments were done using the open source multiboost software of Benbouzid et al. (2012), version 1.", "startOffset": 71, "endOffset": 95}, {"referenceID": 27, "context": "The five sets were chosen to overlap with the selections of most of the recent multi-class boosting papers (K\u00e9gl & Busa-Fekete, 2009; Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013), The small data sets were selected for comparing ADABOOST.", "startOffset": 107, "endOffset": 210}, {"referenceID": 24, "context": "The five sets were chosen to overlap with the selections of most of the recent multi-class boosting papers (K\u00e9gl & Busa-Fekete, 2009; Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013), The small data sets were selected for comparing ADABOOST.", "startOffset": 107, "endOffset": 210}, {"referenceID": 7, "context": "MH with SVMs using Gaussian kernels, taking the results of a recent paper (Duch et al., 2012) whose experimental setup we adopted.", "startOffset": 74, "endOffset": 93}, {"referenceID": 27, "context": "Some report results with fixed hyperparameters (Zhu et al., 2009; Sun et al., 2012) and others give the full table of test errors for a grid of hyperparameters (K\u00e9gl & BusaFekete, 2009; Li, 2009a;b; Mukherjee & Schapire, 2013).", "startOffset": 47, "endOffset": 83}, {"referenceID": 24, "context": "Some report results with fixed hyperparameters (Zhu et al., 2009; Sun et al., 2012) and others give the full table of test errors for a grid of hyperparameters (K\u00e9gl & BusaFekete, 2009; Li, 2009a;b; Mukherjee & Schapire, 2013).", "startOffset": 47, "endOffset": 83}, {"referenceID": 24, "context": "1 (Sun et al., 2012) 3.", "startOffset": 2, "endOffset": 20}, {"referenceID": 27, "context": "5 SAMME w single-label trees (Zhu et al., 2009) 2.", "startOffset": 29, "endOffset": 47}, {"referenceID": 27, "context": "MH w single-label trees (Zhu et al., 2009) 2.", "startOffset": 24, "endOffset": 42}, {"referenceID": 7, "context": "On the small data sets, Duch et al. (2012) used the exact same protocol, so, although the folds are not the same, the results are directly comparable.", "startOffset": 24, "endOffset": 43}, {"referenceID": 24, "context": "For methods where this choice would unfairly bias the comparison (AOSOLOGITBOOST (Sun et al., 2012), ABCLOGITBOOST, LOGITBOOST, and ABCMART (Li, 2009a;b)), we chose the best overall hyperparameter J = 20 and \u03bd = 0.", "startOffset": 81, "endOffset": 99}, {"referenceID": 1, "context": "MH with decision products (K\u00e9gl & BusaFekete, 2009) is also implemented in multiboost (Benbouzid et al., 2012), for this method we re-ran experiments with the protocol described above.", "startOffset": 86, "endOffset": 110}, {"referenceID": 21, "context": "MH (SAMME of Zhu et al. (2009) and ADABOOST.", "startOffset": 13, "endOffset": 31}, {"referenceID": 19, "context": "MM of Mukherjee & Schapire (2013)) we report the post-validated best error which may be significantly lower than the error corresponding to the hyperparameter choice selected by proper validation.", "startOffset": 18, "endOffset": 34}, {"referenceID": 24, "context": "The overall conclusion is that AOSOLOGITBOOST (Sun et al., 2012) and ADABOOST.", "startOffset": 46, "endOffset": 64}, {"referenceID": 24, "context": "MH with decision products (K\u00e9gl & BusaFekete, 2009) and ABCLOGITBOOST are slightly weaker, as also noted by (Sun et al., 2012).", "startOffset": 108, "endOffset": 126}, {"referenceID": 27, "context": "SAMME (Zhu et al., 2009) and ADABOOST.", "startOffset": 6, "endOffset": 24}, {"referenceID": 27, "context": "MH in (Zhu et al., 2009; Mukherjee & Schapire, 2013), assumably implemented using single-label trees (the errors reported by Mukherjee & Schapire (2013) are especially conspicuous).", "startOffset": 6, "endOffset": 52}, {"referenceID": 22, "context": "MM (Mukherjee & Schapire, 2013) perform below the rest of the methods on the two data sets shared among all the papers (even though we give post-validated results). Another important conclusion is that ADABOOST.MH with Hamming trees is significantly better then other implementations of ADABOOST.MH in (Zhu et al., 2009; Mukherjee & Schapire, 2013), assumably implemented using single-label trees (the errors reported by Mukherjee & Schapire (2013) are especially conspicuous).", "startOffset": 16, "endOffset": 449}, {"referenceID": 12, "context": "85%, comparable to classical convolutional nets (LeCun et al., 1998).", "startOffset": 48, "endOffset": 68}, {"referenceID": 18, "context": "MH with Hamming trees, usually combined with calibration (Platt, 2000; Niculescu-Mizil & Caruana, 2005) and model averaging, has been also successful in recent data challenges.", "startOffset": 57, "endOffset": 103}, {"referenceID": 3, "context": "In the Yahoo! Learning-to-Rank Challenge (Chapelle et al., 2011) it achieved top ten performances with results not significantly different from the winning scores.", "startOffset": 41, "endOffset": 64}, {"referenceID": 6, "context": "1 does not restrict the algorithm to single-label problems; another direction for future work is to benchmark it on standard multi-label and sequence-to-sequence classification problems (Dietterich et al., 2008).", "startOffset": 186, "endOffset": 211}], "year": 2013, "abstractText": "Within the framework of ADABOOST.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem toK binary one-againstall classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length K and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary ADABOOST. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLOGITBOOST, and it is significantly better than other known implementations of ADABOOST.MH.", "creator": "LaTeX with hyperref package"}}}