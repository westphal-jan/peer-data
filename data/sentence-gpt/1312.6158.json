{"id": "1312.6158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Deep Belief Networks for Image Denoising", "abstract": "Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs' ability in feature representation and classification for DNF data. The DBN can be used in multiple ways, such as classification and hierarchical classification. We hope that in this paper we propose a tool for a tool for a model for DNF data, as well as for another alternative tool.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 20 Dec 2013 21:56:38 GMT  (604kb,D)", "https://arxiv.org/abs/1312.6158v1", "ICLR 2014 Conference track"], ["v2", "Thu, 2 Jan 2014 17:04:35 GMT  (604kb,D)", "http://arxiv.org/abs/1312.6158v2", "ICLR 2014 Conference track"]], "COMMENTS": "ICLR 2014 Conference track", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["mohammad ali keyvanrad", "mohammad pezeshki", "mohammad ali homayounpour"], "accepted": false, "id": "1312.6158"}, "pdf": {"name": "1312.6158.pdf", "metadata": {"source": "CRF", "title": "Deep Belief Networks for Image Denoising", "authors": ["Mohammad Ali Keyvanrad", "Mohammad Pezeshki", "Mohammad Mehdi Homayounpour"], "emails": ["keyvanrad@aut.ac.ir", "m.pezeshki@aut.ac.ir", "homayoun@aut.ac.ir"], "sections": [{"heading": "1 Introduction", "text": "Image signals are often corrupted due to noise. Removing noise from image is an important issue in computer vision, because this step could be the preprocessing step of many other applications. Up to now various methods have been proposed to remove noise (denoise) from visual data. Focus of many of these methods is on Fourier Analysis [1], Spatial Filtering [2], and Wavelet Transform [3]. Also there are some other methods based on Spare Coding and Dictionary Learning [4]. On the other hand, Machine Learning tools such as Convolutional Neural Networks (CNN) or Deep Neural Networks (DNN) have been used in several papers to tackle this issue[5][6]. One of the biggest difficulties in training such deep networks is that the cost function of such deep architectures gets stuck in poor local optima due to random initialization of weights. Hinton et al. [7] proposed a new greedy layer-wise algorithm to tackle this issue and introduced Deep Belief Networks (DBNs). DBNs are able to present a good \u201dfeature\u201d representation of data. These features which are defined as the properties of input data are presented as nodes of the last layer of DBN. As a result, in this paper we train a DBN such that it learns to extract image features. The trained DBN distinguishes between \u201dnoise features\u201d and \u201dclean image features\u201d in the last layer and presents them into two distinct groups of nodes. Furthermore, DBNs are capable of reconstructing the input data based\nar X\niv :1\n31 2.\n61 58\nv2 [\ncs .L\nG ]\non the values of last layer nodes. Subsequently, if we eliminate the effects of nodes presenting the noise, the reconstructed image will be noiseless. From now on,we called the nodes presenting noise and the nodes presenting image content \u201cnoise nodes\u201d and \u201cimage nodes\u201d, respectively. The rest of the paper is organized as follows: In Section 2 we briefly describe Deep Learning. In Section 3 we describe the learning process. Section 4 is our experimental results and finally, we conclude the paper in Section 5."}, {"heading": "2 Deep Learning", "text": "Restricted Boltzmann Machines (RBMs) are the building blocks of DBNs. Hence, in this section first we briefly describe RBMs and then will explore DBNs."}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "Boltzmann Machines (BMs) and Restricted Boltzmann Machines (RBMs) were introduced in 1980s. But they attracted more attention since 2006 after Hinton et al. paper [8]. He showed that a very powerful neural network can be made by stacking RBMs. RBMs are a kind of Markov Random Fields (MRF) which have a two-layer structure. One layer is called visible and another is called hidden layer. They are restricted to have no visible-visible or hidden-hidden connection and connections are inter layer. A graphical depiction of RBM is shown in Figure 1.\nA joint configuration, (v, h) of visible and hidden units has an energy given by [9]: E(v, h) = \u2212 \u2211\ni \u2208visible\naivi \u2212 \u2211\nj \u2208hidden bjhj \u2212 \u2211 i,j vihjwi,j (1)\nwhere vi, hj are the binary states of visible unit i and hidden unit j and ai, bj are their biases and wij is the weight between them. The network assigns a probability to every possible pair of a visible and a hidden vector via this energy function [10]:\np(v, h) = e\u2212E(v,h)\u2211 v,h e \u2212E(v,h) (2)\nThe probability that the network assigns to a visible vector, v , is given by summing over all possible hidden vectors:\np(v) = \u2211 h e \u2212E(v,h)\u2211\nv,h e \u2212E(v,h) (3)\n\u2202 log p(v))\n\u2202\u03c9i,j =< vihj >data \u2212 < vihj >model (4)\nwhere the angle brackets are used to denote expectations under the distribution species by the subscript that follows. This leads to a very simple learning rule for performing stochastic steepest ascent in the log probability of the training data:\n\u2206\u03c9ij = (< vihj >data \u2212 < vihj >model) (5)\nwhere is a learning rate. But, exact maximum likelihood learning in this model is intractable because exact computation of the expectation model is very expensive. Hence, in practice, learning is done by following an approximation to the gradient of a different objective function, called the Contrastive Divergence (CD) [11]."}, {"heading": "2.2 Deep Belief Networks", "text": "One of the main problems in training deep networks is how to initialize weights. It is difficult to optimize the weights in nonlinear Deep Networks with multiple hidden layers. With good initial weights, gradient descent works well, but finding such initial weights requires a very different type of algorithm that learns one layer of features at a time. Hinton et al. [1] introduced a new algorithm to solve the above problem based on the training of a sequence of RBMs. To construct a DBN we train sequentially as many RBMs as the number of hidden layers in the DBN, i.e. for a DBN with h hidden layers we have to train h RBMs. These RBMs are placed one on top of the other. Figure 2 gives an overview of the basic concept. For the first RBM, which consists of the DBNs input layer and the first hidden layer, the input of RBM is the training set. For the second RBM, which consists of the DBNs first and second hidden layers, the input is the output of the previous RBM and so for other RBMs.\nAfter performing this layer-wise algorithm we have obtained a good initialization for the hidden weights and biases of the DBN and then the DBN is fine-tuned with respect to a typical supervised criterion (such as mean square error or cross-entropy) ."}, {"heading": "3 Learning", "text": "To train a DBN for image denoising, the normalized values of an image pixels are used. Using min-max normalization, grayscale value of a pixel (an integer between 0 and 255) is transformed to a floating point number between 0 and 1. Unlike first and last layer of DBN, other layers have binary nodes. The main idea is to train a DBN such that it learns to map noisy images to images with lower noise or even without noise. The idea can be implemented by learning the behavior of noise and image contents and presenting these behaviors in some nodes at the last layer of the network. The network is trained with a collection consisted of both noisy and noiseless images. We used a criterion called relative activity to detect noise nodes. Relative activity of each node is defined as the difference between two values of a particular node resulted from feeding the network using a noiseless image and its corresponding noisy image (Two images with same contents but one of them\nis corrupted by noise). As a result, if a particular node is a noise node, it should have higher relative activity. On the other hand, if it is an image node, it should have lower relative activity. This theory is justified by the fact that the activation of image nodes should be same for both noiseless and its corresponding noisy images. This process is illustrated in Figure 3.\nBy performing above operation for all images and averaging on the values of each node in the last layer, average relative activity of the last layer nodes is computed. The nodes that still have high average relative activity are considered noise nodes. Now that the noise nodes are discovered, the next step is to lower their activity. Since noise nodes do not change much when clean images are fed to the network, we choose their average value for all clean images as their neutral values (the values which make nodes inactive). Finally, the noise nodes are inactive and consequently, a noiseless image can be reconstructed as it is shown in Figure 4."}, {"heading": "4 Experimental Results", "text": "MNIST dataset is a dataset of handwritten digits consisted of 60,000 images for training and 10,000 images for test. To model natural noise, we added additive white Gaussian noise (AWGN) to images with a variance of 0.20. Therefore, our new dataset was consisted of 120,000 noisy and clean images\nalong with 10,000 noisy images for test (the whole test set is noisy). We used a subset of training set with 20,000 elements for the training phase and the whole test set for the test phase. According to empirical results we created a DBN with 4 hidden layers: 784-1000-500-250-100. We trained this network by 200 batches of data each including 100 images.\nAccording to previous discussions we used relative activity to find noise nodes in the last layer of the DBN: For all images in the dataset, we put a clean image and then its corresponding noisy image as the input of the network. Afterward, we computed the difference between the last nodes\u2019 values. The average of difference in each node considering all images showed average relative activity of nodes (noisy vs. clean). Based on experimental results, nodes with an average relative activity higher than 0.9 were considered noise nodes.\nNow for all 10000 clean images in our training set, we compute the values of nodes in the last layer of our trained DBN. The average of these values for each node considered neutral value of node. Finally, to reconstruct a noiseless image from a noisy image, we change the values noise nodes to their neutral values. As a result, noise nodes are inactive and reconstruction would be noiseless. Figure 4 shows how the reconstructed results have lower noise. Also Table 5 shows that a reduction of 65.9% in average Mean Square Error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images."}, {"heading": "5 Conclusion and Future Works", "text": "In this paper, a novel method for image denoising was proposed. The proposed method makes a model to learn noise behavior using a Deep Belief Network (DBN) and tries to present noise and image contents behavior into two distinct groups of nodes. Then by omitting noisy nodes, the network will be able to produce a noiseless (clean) image by reconstruction of the input image. In our work, thresholds for detection of noise nodes were determined manually. Future works will include: 1. Using an automatic technique to determine thresholds for detecting the noisy nodes in denoising technique presented in this paper. 2. Employing our denoising approach to tackle some other issues in Computer Vision, Speech Recognition, etc. These areas will be addressed in future phases of this project."}, {"heading": "6 References", "text": "[1] Brigham, E. O., & Morrow, R. E. (1967). The fast Fourier transform. Spectrum, IEEE, 4(12), 63-70.\n[2] Kervrann, C., & Boulanger, J. (2006). Optimal spatial adaptation for patch-based image denoising. Image Processing, IEEE Transactions on, 15(10), 2866-2878.\n[3] Pan, Q., Zhang, L., Dai, G., & Zhang, H. (1999). Two denoising methods by wavelet transform. Signal Processing, IEEE Transactions on, 47(12), 3401-3406.\n[4] Elad, M., & Aharon, M. (2006). Image denoising via sparse and redundant representations over learned dictionaries. Image Processing, IEEE Transactions on, 15(12), 3736-3745.\n[5] LeCun, Y., Kavukcuoglu, K., & Farabet, C. (2010, May). Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on (pp. 253- 256). IEEE.\n[6] Xie, Junyuan, Linli Xu, and Enhong Chen. \u201dImage denoising and inpainting with deep neural networks.\u201d Advances in Neural Information Processing Systems. 2012.\n[7] Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786), 504-507.\n[8] Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural computation, 18(7), 1527-1554.\n[9] Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural computation, 14(8), 1771-1800.\n[9] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P. A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 9999, 3371-3408.\n[10] Hinton, G. (2010). A practical guide to training restricted Boltzmann machines. Momentum, 9(1).\n[11] Salakhutdinov, R. (2009). Learning deep generative models (Doctoral dissertation, University of Toronto)."}], "references": [{"title": "Optimal spatial adaptation for patch-based image denoising", "author": ["C. Kervrann", "J. Boulanger"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Two denoising methods by wavelet transform", "author": ["Q. Pan", "L. Zhang", "G. Dai", "H. Zhang"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Convolutional networks and applications in vision", "author": ["Y. LeCun", "K. Kavukcuoglu", "Farabet", "May"], "venue": "In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on (pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Image denoising and inpainting with deep neural networks.", "author": ["Xie", "Junyuan", "Linli Xu", "Enhong Chen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "A practical guide to training restricted Boltzmann machines. Momentum", "author": ["G. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Learning deep generative models (Doctoral dissertation, University of Toronto)", "author": ["R. Salakhutdinov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Focus of many of these methods is on Fourier Analysis [1], Spatial Filtering [2], and Wavelet Transform [3].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Focus of many of these methods is on Fourier Analysis [1], Spatial Filtering [2], and Wavelet Transform [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Also there are some other methods based on Spare Coding and Dictionary Learning [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "On the other hand, Machine Learning tools such as Convolutional Neural Networks (CNN) or Deep Neural Networks (DNN) have been used in several papers to tackle this issue[5][6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "On the other hand, Machine Learning tools such as Convolutional Neural Networks (CNN) or Deep Neural Networks (DNN) have been used in several papers to tackle this issue[5][6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "[7] proposed a new greedy layer-wise algorithm to tackle this issue and introduced Deep Belief Networks (DBNs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "paper [8].", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "A joint configuration, (v, h) of visible and hidden units has an energy given by [9]:", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "The network assigns a probability to every possible pair of a visible and a hidden vector via this energy function [10]:", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "Hence, in practice, learning is done by following an approximation to the gradient of a different objective function, called the Contrastive Divergence (CD) [11].", "startOffset": 157, "endOffset": 161}], "year": 2014, "abstractText": "Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs\u2019 ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distinguishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noiseless image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.", "creator": "LaTeX with hyperref package"}}}