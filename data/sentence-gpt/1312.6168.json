{"id": "1312.6168", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Factorial Hidden Markov Models for Learning Representations of Natural Language", "abstract": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods. As a result, the classification system can easily be used for classification by a specific target or group. We present a technique to measure all of the features that are in the input sequence of a word. Here we show a model in action for a text classification algorithm called Coding.\n\n\n\nWe identify the three possible features for the word. In this instance, we assign a variable to the word to represent a word that is the word that the word in question is. This variable (the variable itself) has the name 'X' in the left corner and is associated with the word that it represents in the right corner of the word. The variable (the variable itself) has the name 'X', as in the left corner of the word. It is associated with the word that it represents in the right corner of the word. This variable (the variable itself) has the name 'X', as in the left corner of the word. It is associated with the word that it represents in the right corner of the word. The variable (the variable itself) has the name 'X', as in the left corner of the word. It is associated with the word that it represents in the right corner of the word. The variable (the variable itself) has the name 'X', as in the left corner of the word. It is associated with the word that it represents in the right corner of the word.\nThe feature is not just a new feature of Coding, but in the new features. Coding can be used to categorize and identify certain features for a word. This is one of the many ways in which we can distinguish feature information from input data. For example, in Coding we can identify features for a word, such as text,", "histories": [["v1", "Fri, 20 Dec 2013 22:44:26 GMT  (43kb)", "https://arxiv.org/abs/1312.6168v1", "11 pages, 2 tabels, ICLR-2014"], ["v2", "Wed, 15 Jan 2014 04:49:47 GMT  (43kb)", "http://arxiv.org/abs/1312.6168v2", "11 pages, 2 tables, ICLR-2014"], ["v3", "Tue, 18 Feb 2014 11:22:30 GMT  (45kb)", "http://arxiv.org/abs/1312.6168v3", "12 pages, 2 tables, ICLR-2014"]], "COMMENTS": "11 pages, 2 tabels, ICLR-2014", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["anjan nepal", "alexander yates"], "accepted": false, "id": "1312.6168"}, "pdf": {"name": "1312.6168.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anjan.nepal@temple.edu", "yates@temple.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n61 68\nv3 [\ncs .L\nG ]\n1 8"}, {"heading": "1 Introduction", "text": "Most existing representation learning algorithms project and transform the local context of data to produce their representations. Yet in many applications, particularly in natural language processing (NLP), joint prediction is crucial to the success of the application, and this requires reasoning over global structure.\nWe investigate a representation learning approach based on Factorial Hidden Markov Models [23]. Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued. The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1]. More importantly, FHMMs provide the ability to assign a representation to a word that is sensitive to the full observation sequence. In contrast, both spectral methods and neural network models rely on fixed-length context windows to produce word representations.\nOur contributions in this paper are mainly two fold: 1) We develop novel variational approximation algorithms for FHMMs with discrete-valued variables and 2) we provide empirical evaluation of the distributed representations from FHMMs which can provide different representations per word depending on the whole sentence context. We compare the performance against the systems that produce either only global context dependent but not distributed representations, or fixed representations per word which are not distributed, or fixed distributed representations per word which are dependent on the local context of the sequence.\nIn experiments, we investigate the ability of FHMMs to provide useful representations for part-ofspeech (PoS) tagging and noun-phrase chunking. In comparison with existing HMM representations, we find that our FHMM-based representations are more scalable to large latent state spaces and provide better application performance, reducing error by 4% on words that never appear in labeled PoS-tagging training data, and by 5% on chunks whose initial word never appears in labeled chunking training data.\nThe next section discusses previous work in representation learning for NLP. Section 3 introduces our FHMM model for representation learning, and Section 4 presents the inference and learning algorithms. Section 5 describes our empirical evaluation of the model as a technique for learning latent features of words for subsequent classification tasks. Section 6 concludes."}, {"heading": "2 Previous Work", "text": "There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19]. Our work combines the power of distributed representations from the neural network models with the joint inference from the HMMbased clustering approaches.\nTo the best of our knowledge, FHMMs have not been used for learning representations in NLP or elsewhere, and they are rarely used with discrete-valued observations. This may be in part due to the perception that training with discrete-valued observations is too computationally challenging, at least for real NLP datasets. Jacobs et al. [31] use a generalized backfitting approach for training a discrete-observation FHMM, but they have not run experiments on a naturally-occurring dataset, and they focus on language modeling rather than representation learning. We use a different training procedure based on variational EM [33], and provide empirical results for a representation learning task on a standard dataset.\nRecent work by Socher et al. [47] has annotated parse trees with latent vectors that compose hierarchically. Like our model, these techniques operate on distributed representations for natural language, but as with neural network models they do not perform any kind of joint prediction over the whole structure to optimize the representations for individual words. Titov and Henderson [48] and Henderson and Titov [25] also learn representations that are affected by global context and also use variational approximation for inference but apply the technique to parsing. Grave et al. [24] use mainly two types of latent variable models: one essentially an HMM and the other has the latent variables associated in a syntactic dependency tree. While they also get the latent states for the words that are both context dependent and globally affected, our method can handle exponentially large state space and the representations are distributed.\nMost previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20]. Learning bounds are known [6, 39]. Daume\u0301 III et al. [14] use semi-supervised learning to incorporate labeled and unlabeled data from the target domain. In contrast, we investigate a domain adaptation setting where no labeled data is available for the target domain."}, {"heading": "3 The Factorial Hidden Markov Model for Learning Representations", "text": "The Factorial Hidden Markov Model (FHMM) [23] is a bayesian graphical model in which a sequence of observed variables is generated from a latent sequence of discrete-valued vectors, as shown in Figure 1a. At a given time step t \u2208 {1, . . . , T }, the latent state St is factored into M dimensions, where each latent factor Smt (m \u2208 {1, . . . ,M}) is a vector of Km boolean variables with the constraint that exactly one of the boolean variables has value 1 at any time. We refer to the variable Smt,k that has value 1 as the state of layer m at time step t. In general the model could allow a different number of states Km for each layer m, but in our experiments we use a single value K\nfor all layers. The observed variables Yt are also discrete variables, taking on V possible values, where V is the size of the vocabulary of words in the data.\nThe model provides a joint distribution over these variables, which factors as follows:\nP ({St, Yt}) = P (S1)P (Y1|S1)\nT \u220f\nt=2\nP (St|St\u22121)P (Yt|St) (1)\nwhere we refer to P (S1) as the initial distribution, P (Yt|St) as the observation distribution, and P (St|St\u22121) as the transition distribution.\nThe model assumes that states in one layer evolve independently of the other layers, and it uses log-linear models for the transition distribution of each layer. This allows the transition distribution to factor as follows:\nP (St|St\u22121) =\nM \u220f\nm=1\nP (Smt |S m t\u22121) (2)\n= M \u220f\nm=1\nK \u220f\nj=1\n\u220fK k=1 (exp \u03b8mjk) Smt\u22121,j \u00b7S m t,k\n\u2211K k\u2032=1(exp \u03b8mjk\u2032 ) Sm t\u22121,j\n(3)\nwhere \u03b8mjk is the transition parameter of the model indexed by the layer, previous state and current state respectively. The initial distribution is defined the same way, except that we drop indicators for the previous time step and use parameters \u03b8mk rather than \u03b8mjk .\nThere are several possible functions to choose from for the observation distribution P (Yt|St). A table approach would need a matrix with V \u00d7 KM parameters, with exponential blow-up as we increase the number of layers. Two standard ways of approximating this matrix of parameters are the noisy-or model and the log-linear model, and we choose log-linear distribution in our model.\nThe observations Yt are dependent on the states in all layers of St:\nP (Yt|St) =\n\u220f m,k (exp \u03b8Ytmk) Smt,k\n\u2211\nY\n\u220f m,k (exp \u03b8Ymk) Sm t,k\n(4)\nwhere \u03b8Ymk is the observation parameter of the model indexed by the observed value, the latent layer, and the state of the latent layer respectively.\nThe number of parameters for the initial distribution is M \u00d7 K , for the transition distribution it is M \u00d7K2, and for the observation distribution it is M \u00d7K \u00d7 V .\nFor finding representations using this model, we first estimate the parameters of the model by training it on unlabeled data in unsupervised fashion. Later, we use these parameters for finding the latent states for the data and use them as the representations.\nWhile we borrow the model structure from Ghahramani and Jordan [23], the main between their model and our model is in the observation distribution. Crucially, their model works with real-valued observations, while our model works with discrete-valued observations like words. As a result of the change to the observation distribution, we need to change the inference and learning procedures, although we continue to follow [23] in using variational approximations, which we explain below."}, {"heading": "4 Variational Methods for Learning and Inference", "text": "We aim to learn our representations from unlabeled text. The objective for our unsupervised parameter estimation procedure is to maximize marginal log likelihood:\nL(\u03b8) = logP ({Yt}) = log \u2211\n{St}\nP ({St, Yt}) (5)\nWe can re-write the objective using an arbitrary new distribution Q({St}), which we call the variational distribution, and apply Jensen\u2019s inequality, as follows:\nlog \u2211\n{St}\nP ({St, Yt}) = log \u2211\n{St}\nQ({St}) P ({St, Yt})\nQ({St}) (6)\n\u2265 \u2211\n{St}\nQ({St}) log P ({St, Yt})\nQ({St}) = F (Q, \u03b8) (7)\nF (Q, \u03b8) is a lower bound on the objective function that becomes exact when Q is equal to the posterior distribution P ({St}|{Yt}). We can re-write F (Q, \u03b8) in two different ways:\nF (Q, \u03b8) = \u2211\n{St}\nQ({St}) logP ({Yt}) + \u2211\n{St}\nQ({St}) log P ({St}|{Yt})\nQ({St})\n= L(\u03b8)\u2212KL(Q({St})||P ({St}|{Yt})) (8)\nand\nF (Q, \u03b8) = E{St}\u223cQ[logP ({St, Yt})]\u2212 E{St}\u223cQ[logQ({St})] (9)\nWe use the Expectation Maximization (EM) algorithm [17], a block coordinate-ascent algorithm, to learn the parameters \u03b8. The E-step consists of maximizingF with respect to Q while keeping \u03b8 fixed to the current set of parameters, which is equivalent to minimizing the KL-divergence in Equation 8. The M-step consists of maximizing F with respect to \u03b8 while keeping Q fixed, which is equivalent to maximizing the first term in Equation 9. The algorithm guarantees that these two steps converge to a (local) maximum.\nE-step:\nQt+1 = argmax Q F (Q, \u03b8t) = argmin Q KL(Q({St})||P ({St}|{Yt}, \u03b8 t) (10)\nM-step:\n\u03b8t+1 = argmax \u03b8 F (Qt+1, \u03b8) = argmax \u03b8 E{St}\u223cQt+1 [logP ({St, Yt}|\u03b8)] (11)"}, {"heading": "4.1 E-step", "text": "In the E-step, we can minimize the KL-divergence to zero by setting the Q distribution equal to the posterior distribution P ({St}|{Yt}) if we can compute the posterior exactly. The posterior for the model is given by\nP ({St}|{Yt}) = 1\nZ\n\u220f\nt\n\u220f m,k (exp \u03b8Ytmk) Smt,k\n\u2211\nY\n\u220f m,k (exp \u03b8Ymk) Sm t,k\n\u220f\nt,m,j\n\u220f k (exp \u03b8mjk) Smt\u22121,j \u00b7S m t,k\n\u2211 k\u2032(exp \u03b8mjk\u2032 ) Sm t\u22121,j\n(12)\nwhere, Z = \u2211\n{St} P ({St, Yt}).\nHowever, in an FHMM, the exact computation of this posterior is computationally intractable, even using a forward-backward algorithm. The hidden state variables at each time-step become dependent conditioned on the observed variable requiring the computation of KM expectations, which is infeasible to compute for large M .\nFor this reason, we resort to a variational approximation of the posterior P by distribution Q with its own variational parameters. The graphical model for the variational distribution Q is chosen such that most of the structure of the original model is preserved, but such that the inference becomes tractable. We borrow the structured variational approximation model from Ghahramani and Jordan [23] which is shown in 1b. The variational model is essentially M independent Markov chains, one for each layer of the FHMM. Markov chains permit efficient maximum a posteriori inference using the standard Viterbi algorithm and efficient computation of marginal distributions using the standard forward-backward algorithm.\nThe full variational distribution is given by\nQ({St}|\u03d5) = 1\nZQ\n\u220f\nt,m,k\nexp(\u03d5tmk) Smt,k\n\u220f\nt,m,j\n\u220f k (exp\u03d5mjk) Smt\u22121,j \u00b7S m t,k\n\u2211 k\u2032 (exp\u03d5mjk\u2032 ) Sm t\u22121,j\n(13)\nwhere \u03d5 are the variational parameters of the model. \u03d5tmk indicate observation variational parameters, and \u03d5mjk indicate transition variational parameters. The transition variational distribution matches the FHMM transition distribution. Notice that the observation parameters are indexed by time step, rather than by the actual observation at that time step. Thus the variational distribution can be thought of as having each layer a fictitious observation, chosen so as to mimic the true posterior as closely as possible.\nThe variational parameters are optimized by minimizing the KL-divergence, which obtains its minimum when the initial and transition variational parameters are equal to the initial and transition parameters of the original model, i.e. \u2200m, j, k . \u03d5mjk = \u03b8mjk and \u03d5mk = \u03b8mk. In this subspace of the parameter space, the KL-divergence simplifies to\nKL(Q||P ) =E\n\n\u2212 logZQ + logZ + \u2211\nt,m,k\nSmt,k(\u03d5tmk \u2212 \u03b8Ytmk)\n\n\n+ E\n[\n\u2211\nt\nlog \u2211\nY\n\u220f\nm\nexp\n(\n\u2211\nk\n\u03b8YmkS m t,k\n)]\n(14)\nwhere, the expectations are taken using the Q({St}) distribution.\nUnfortunately, Equation 14 still does not permit efficient optimization of the \u03d5 parameters because of the log-sum-exp expression in the final term. While we followed the variational approximation from the original FHMM model so far, we provide a new algorithm to handle the log-sum-exp term introduced due to the observation distribution in our model. To handle this term, we make use of a second variational bound, log x \u2264 \u03c6x \u2212 log\u03c6 \u2212 1 [33, 4], where \u03c6 is a new variational parameter which can be varied to make the bound tight. This allows us to rewrite the final term in 14 as follows:\nE\n[\n\u2211\nt\nlog \u2211\nY\n\u220f\nm\nexp\n(\n\u2211\nk\n\u03b8YmkS m t,k\n)]\n\u2264 \u2211\nt\n\u03c6t\n{\n\u2211\nY\nE\n[\n\u220f\nm\nexp\n(\n\u2211\nk\n\u03b8YmkS m t,k\n)]\n\u2212 log\u03c6t \u2212 1\n}\n(15)\n= \u2211\nt\n\u03c6t\n{\n\u2211\nY\n\u220f\nm\nE\n[\nexp\n(\n\u2211\nk\n\u03b8YmkS m t,k\n)]\n\u2212 log\u03c6t \u2212 1\n}\n(16)\n= \u2211\nt\n\u03c6t\n{\n\u2211\nY\n\u220f\nm\n[\n\u2211\nk\nexp(\u03b8Y mk)E [ Smt,k ]\n]\n\u2212 log\u03c6t \u2212 1\n}\n(17)\nIn Eqn. 16, we move the expectation inside the product over M independent layers. Eqn. 17 follows from the moment generating function for the multinomial random variable Smt .\nMaking use of this bound in Eqn. 14, we get a new upper bound on the KL-divergence, which we denote by KL. The variational parameters\u03c6 have a closed-form solution when we set the gradient of KL with respect to the parameters \u03c6 to zero. We get \u03c6t = ( \u2211\nY\n\u220f\nm\n\u2211 k\u2032 E [ Smt,k\u2032 ] exp \u03b8Ymk\u2032 )\u22121 .\nTo optimize the variational parameters \u03d5, we find the gradient to minimize KL.\n\u2202KL\n\u2202\u03d5tmk =\n\u2202KL\n\u2202E [\nSmt,k\n]\n\u2202E [\nSmt,k\n]\n\u2202\u03d5tmk (18)\nSetting the derivative with respect to E [\nSmt,k\n]\nto zero, we get\n\u03d5tmk = \u03b8Ytmk \u2212 \u03c6t \u2211\nY\n\n\n\n\n\n\u220f\nn6=m\n\u2211\nk\u2032\nE [ Smt,k\u2032 ] exp \u03b8Y mk\u2032\n\n exp \u03b8Ymk\n\n\n\n(19)\nTo find the optimized value for \u03d5tmk , we initially set the values to random values. We then run the forward-backward algorithm algorithm in each markov chain of the variational model and find the expectations E [\nSmt,k\n]\nusing these variational parameters. Then these expectations are used to find\nthe new set of variational parameters using the above equation. These two steps are iterated until convergence.\nOnce the variational parameters have been optimized, the variational distribution is used in the forward-backward algorithm in each layer of Q for computing the expectations E [\nSmt,k\n]\nand\nE [\nSmt\u22121,j , S m t,k\n]\nrequired to compute the expected sufficient statistics for the M-step."}, {"heading": "4.2 M-step", "text": "In the M-step, the best set of parameters of the model are found by maximizing the objective function\nF = E[logP ({St, Yt})] (20)\nInitial and transition parameters are normalized to proper distribution like in a standard HMM using the expected sufficient statistics gathered in the E-step. For the observation parameters, no closed form solution exists and we resort to gradient descent. We again use the bound on the objective function to move the expectation inside the parameter of the log. The gradient of the lower bound F of the objective is\n\u2202F\n\u2202\u03b8Ymk =\n\u2211\nt\n\nE [ Smt,k ] 1[Y = Yt]\u2212 \u03c6t\n\n\n\n\u220f\nn6=m\n(\n\u2211\nk\u2032\nE [ Snt,k\u2032 ] exp \u03b8Y nk\u2032\n)\n\n\n\nE [ Smt,k ] exp \u03b8Ymk\n\n\n(21)\nwhere, \u03c6t = ( \u2211\nY\n\u220f\nm\n\u2211 k\u2032 E [ Smt,k\u2032 ] exp \u03b8Ymk\u2032 )\u22121 ."}, {"heading": "4.3 Inference", "text": "To compute the posterior P ({St}|{Yt}), we find the variational distribution Q that best approximates the posterior using the same procedure as in the E-step. Q is also used to find the maximum a posteriori state sequence using Viterbi algorithm."}, {"heading": "4.4 Implementation Details", "text": "Rather than using Equations 19 and 21 naively, we precomputed few values to serve as a lookup for reducing the runtime complexity. First, we precomputed \u03c6t and also stored for each Y the product over all m (requiring memory O(V )). To compute the sum over all Y s, we reused these values, which only vary by a factor involving the dot product involving the layer m and Y . We then used\nthese computations to finally update the parameter in time O(MKV T ). We used similar precomputation technique in the M-step making the expensive gradient computation runtime O(MKV T ). Note that we can clear the memory after processing the result for a token making the memory use independent of the number of tokens. However, the runtime is dependent on the number of tokens. We used online learning technique [37, 8] for learning our model on large corpus. We took a mini-batch of 1000 sentences per iteration and ran for 5 epochs. We found that using observation parameters from previous iterations of EM to initialize the variational parameters allowed the variational parameters to converge much more quickly than random initialization. The variational parameter usually converged under 10 iterations. We used an existing package of L-BFGS for optimizing the parameters in the M-step, running for a maximum of 100 iterations."}, {"heading": "5 Experiments", "text": "We ran the experiments on two sequence labeling tasks in domain adaptation setting: part-of-speech (PoS) tagging and noun-phrase (NP) chunking. In both experiments, the source domain is from the newswire text and the target domain is from the biomedical text. However, the setting for the domain adaptation for the two experiments are different. In PoS tagging experiment, we get access to the text from the test domain and can use it to train the representation learning model. In the chunking experiment, however, we get access to a domain related to the test domain but not to the text from the test domain itself. In both experiments, latent vectors from the FHMMs are used as representations in the supervised experiments."}, {"heading": "5.1 Experimental Setup", "text": "In the PoS tagging experiment, the labeled training data is from sections 02-21 of the Penn Treebank containing 39,832 sentences which have manually labeled PoS tags. We used a standard test set previously used by Huang et al. [28] and Blitzer et al. [7] which contains 561 MEDLINE sentences with 14554 tokens from the Penn BioIE project. In the NP chunking experiment, we used the CoNLL-2000 shared task training data by replacing all non-NP chunk tags with outside chunk tag, O. The dataset consist of sections 15-18 of the Penn Treebank containing 8932 sentences which are annotated with PoS tags and the chunk tags[46]. The test set consist of 198 manually annotated sentences [28] from the biomedical domain of Open American National Corpus (OANC).\nFor representation learning, we used the unlabeled data from both domains: WSJ corpus and from the MEDLINE corpus. We did not include any data from the biomedical domain of the OANC. Then we used a preprocessing step defined in [36] to select a subset of good sentences for training the FHMM, resulting in 112,824 sentences. This was done to train our system in reasonable amount of time but still learning a good representation from the data from both the domains. This data size is smaller than what previous researchers have used and covers fewer number of words in the supervised text. However, we think that a good representation for such words will be found by making use of our model which predicts it depending on the whole context of the sentence in the inference time. The words appearing only once in the corpus were collapsed into a special *unk* symbol and numbers were collapsed into a special *num* symbol, resulting in a vocabulary size of 40,596. This vocabulary did not cover 2% and 0.3% of the tokens in the supervised PoS training data and testing data respectively and 3% of the tokens in both the training and test data for the chunking task.\nWe ran FHMMs with different state spaces and compared their representation capacity in the supervised experiments. We also compared the global context dependent distributed representations from FHMMs against other systems using the same unlabeled data for learning the representations. HMM provides globally context-dependent representations for the words which are not distributed representations. Brown Clusters provided a fixed cluster independent of the context and are not distributed representations. Word embeddings from neural network models also provided a fixed representation per word but are distributed representations. We trained 80 states HMM and used the Viterbi decoded states as representations [28] and a Brown Clustering algorithm [9] with 1000 clusters as representations. Following Turian et al. [49], we used the prefixes of length 4, 6, 10 and 20 in addition to the whole path for the Brown representation as the features. We also trained a neural network model described in Collobert and Weston [11] and used previous 10 words to predict the next word in the sequence. We learned 50-dimensional word embeddings using 100 units in the hid-\nden layer. We make the comparison of the representation systems by using them in the supervised experiments as features. We used Conditional Random Fields (CRF) with the same features as defined in Huang et al. [28], with addition of the context words and context representation dimension as features in a window of size 2. In the chunking dataset, the representation learning text we used is different but the test set is the same as the previous work [28] we compare against, who also used the test sentences in the representation learning system.\nWe use the standard evaluation metric of word error rate for PoS tagging and F1 score for NPchunking, focusing on the words that never appeared (OOV) and that appeared at most two times (rare) in the supervised training corpus. To calculate the precision for the OOV and rare words, we define false positive as the phrases predicted by the model beginning with the OOV or rare word but actually are not phrases in the gold dataset."}, {"heading": "5.2 Results and Discussion", "text": "We represent the baseline system trained using traditional features with no features from a representation learning system as BASELINE. FHMM-M-K is our system using Viterbi decoded states as representations from a M layer FHMM with K states in each layer. FHMM-POST-M-K is our system using posterior probability distribution over states as representations from a M layer FHMM with K states in each layer. HMM-80 is a system using Viterbi decoded states as representation from an HMM model with 80 states and BROWN-1000 is a system using Brown clustering algorithm with 1000 clusters as representations. The 50-dimensional word embeddings learned using neural network model from Collobert and Weston [11] is represented as EMBEDDINGS-50. All these systems were trained on the same unlabeled representation text. In PoS tagging experiment, we report the error on all tokens (14,554). HUANGHMM-80 is a system defined in [28] and SCL is a system defined in [7] and report the error rate on the same test data as ours but with fewer number of tokens (9576 and approx 13,000 respectively). Also, they used unlabeled corpus of larger size to train their representation learning system. This makes the direct comparison with their system difficult. Table 1 shows the error rate of the supervised classifier using different representation learning systems in PoS tagging experiment. Table 2 shows the Recall(R), Precision(P) and F1 score of the supervised classifier using different representation learning system. Previous system HUANGHMM-80 [28] included the text from the test domain itself for representation learning.\nWe found that increasing the state space by increasing the number of layers in the representation learning system generally improved the performance of the supervised classifier. We also found model FHMM-POST-5-10 performs better than the same state size model FHMM-5-10. We think this is because the soft probabilities over the hidden states give more information to the supervised classifier compared to the one-hot representation as in Viterbi decoded states. Compared to the baseline, our best performing system reduced the error by 3.8% on the OOV words and by 3.2% on the rare words in the PoS tagging experiment and increased the F1 score by 5% on both the OOV and the rare words in the chunking experiment.\nWe present analysis on why the performance of the different representation learning systems differ. BROWN-1000 suffers mainly for two reasons. First, it assigns a fixed cluster to a word irrespective of its different usage as meaning in different domains. For example, the word share which appears most frequently as NN in the train domain appears only as a VBP in the test domain and the word signaling which only appears as VBG in the training domain appears only as NN in the test domain. Second, it provides no representation for the words that did not appear in the training of representation learning system. Models like HMM and FHMM do not suffer from these limitations. They both can provide different representations for the same word with different meaning in two domains. They also give a representation to a word by looking at the context even if it is not seen by the representation learning system. We also experimented by giving the context clusters when using the Brown clusters as representations but it did not improve the performance. EMBEDDINGS-50 also suffers in the PoS tagging experiment because of the fixed representations provided to the words irrespective of its different usage as meaning in different domains. In the chunking experiment, the performance of the HMM-80 model is worse than the baseline. We found that most of the errors were on the phrases surrounding the words like and and punctuations where the HMM-80 system got confused on whether to continue the phrase or not whereas FHMM-5-10 is performing much better in such situations. The word and in the training text of the chunking data is clustered into fairly small (5-10) number of cluster in an 80 state HMM however it is clustered into 400 different\nclusters in FHMM model, usually varying in only few dimensions. Although we have not done further analysis, we think the representation using FHMM is able to capture larger context information into the representation which helps the supervised system to make better prediction. The reason HUANGHMM-80 performed better than the BASELINE and HMM-80 might be because it is able to see the test sentences during the representation learning phase. The best results for the chunking task are provided by the distributed representations from the FHMMs or the word embeddings. The results show that the distributed representations from the FHMMs which can provide different representations per word depending on the whole sentence context can give either superior or comparable performance to the other representations."}, {"heading": "6 Conclusion and Future Work", "text": "We have developed a learning mechanism for discrete-valued FHMMs involving a novel two-step variational approximation that can train on datasets of nearly three million tokens in reasonable time. Using the latent vectors predicted by our best FHMM model as features, we have shown that a standard classifier for two NLP tasks, chunking and tagging, and improve significantly on new domains, especially on words that were never or rarely observed in labeled training data.\nSignificant work remains to make the FHMM more useful as a representation learning technique. Most importantly, it needs to scale to larger datasets using a distributed implementation. More work is also needed to fully compare the relative merits of using global context against using local context in a representation learning framework. The same idea of using global context to predict features can also be extended to other tasks, including hierarchical segmentation tasks like parsing, or perhaps even to image processing tasks using different classes of models. Finally, it remains an open question whether there are efficient and useful ways to combine the distinct benefits of joint prediction and deep architectures for representation learning."}, {"heading": "Acknowledgments", "text": "This research was supported in part by NSF grant IIS-1065397."}], "references": [{"title": "Improved extraction assessment through better language models", "author": ["Arun Ahuja", "Doug Downey"], "venue": "In Proceedings of the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (NAACL-HLT),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Neural net language models", "author": ["Yoshua Bengio"], "venue": "Scholarpedia, 3(1):3881,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Correlated topic models", "author": ["David Blei", "John Lafferty"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Learning bounds for domain adaptation", "author": ["John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jenn Wortman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "The tradeoffs of large scale learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "In IN: ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Improving generative statistical parsing with semi-supervised word clustering", "author": ["M. Candito", "B. Crabbe"], "venue": "In IWPT, pages 138\u2013141,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III"], "venue": "In ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Frustratingly easy semi-supervised domain adaptation", "author": ["Hal Daum\u00e9 III", "Abhishek Kumar", "Avishek Saha"], "venue": "In Proceedings of the ACL Workshop on Domain Adaptation (DANLP),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Domain adaptation for statistical classifiers", "author": ["Hal Daum\u00e9 III", "Daniel Marcu"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "Journal of the American Society of Information Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Maximum likelihood from incomplete data via the em algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "SERIES B,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1977}, {"title": "Two step cca: A new spectral method for estimating vector models of words", "author": ["Paramveer S. Dhillon", "Jordan Rodu", "Dean P. Foster", "Lyle H. Ungar"], "venue": "In Proceedings of the 29th International Conference on Machine learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Sparse information extraction: Unsupervised language models to the rescue", "author": ["Doug Downey", "Stefan Schoenmackers", "Oren Etzioni"], "venue": "In ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Online methods for multi-domain learning and adaptation", "author": ["Mark Dredze", "Koby Crammer"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Multi-domain learning by confidence weighted parameter combination", "author": ["Mark Dredze", "Alex Kulesza", "Koby Crammer"], "venue": "Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Hierarchical bayesian domain adaptation", "author": ["Jenny Rose Finkel", "Christopher D. Manning"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Factorial hidden markov models", "author": ["Zoubin Ghahramani", "Michael I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Hidden markov tree models for semantic class induction", "author": ["Edouard Grave", "Guillaume Obozinski", "Francis Bach"], "venue": "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Incremental sigmoid belief networks for grammar learning", "author": ["James Henderson", "Ivan Titov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Self-organizing maps of words for natural language processing applications", "author": ["T. Honkela"], "venue": "In Proceedings of the International ICSC Symposium on Soft Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1997}, {"title": "Learning Representations for Weakly Supervised Natural Language Processing Tasks", "author": ["Fei Huang", "Arun Ahuja", "Doug Downey", "Yi Yang", "Yuhong Guo", "Alexander Yates"], "venue": "Computational Linguistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Distributional representations for handling sparsity in supervised sequence labeling", "author": ["Fei Huang", "Alexander Yates"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Exploring representation-learning approaches to domain adaptation", "author": ["Fei Huang", "Alexander Yates"], "venue": "In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Open-domain semantic role labeling by modeling word spans", "author": ["Fei Huang", "Alexander Yates"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Factorial Hidden Markov Models and the Generalized Backfitting Algorithm", "author": ["Robert A. Jacobs", "Wenxin Jiang", "Martin A. Tanner"], "venue": "Neural Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Instance weighting for domain adaptation in NLP", "author": ["Jing Jiang", "ChengXiang Zhai"], "venue": "In ACL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "An introduction to variational methods for graphical models", "author": ["Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul"], "venue": "Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "Dimensionality reduction by random mapping: Fast similarity computation for clustering", "author": ["S. Kaski"], "venue": "In IJCNN,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Simple semi-supervised dependency parsing", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": "In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Semi-supervised learning for natural language", "author": ["Percy Liang"], "venue": "In MASTERS THESIS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Online em for unsupervised models", "author": ["Percy Liang", "Dan Klein"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Phrase clustering for discriminative learning", "author": ["D. Lin", "X Wu"], "venue": "In ACL-IJCNLP,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Domain adaptation with multiple sources", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["S. Martin", "J. Liermann", "H. Ney"], "venue": "Speech Communication,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1998}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1993}, {"title": "An introduction to random indexing", "author": ["M. Sahlgren"], "venue": "In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2005}, {"title": "The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces", "author": ["M. Sahlgren"], "venue": "PhD thesis, Stockholm University,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2006}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M.J. McGill"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1983}, {"title": "Introduction to the conll-2000 shared task: Chunking", "author": ["Erik F. Tjong Kim Sang", "Sabine Buchholz", "Kim Sang"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2000}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Association for Computational Linguistics ACL),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Constituent parsing with incremental sigmoid belief networks", "author": ["Ivan Titov", "James Henderson"], "venue": "In Association for Computational Linguistics ACL,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2007}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Towards explicit semantic features using independent component analysis", "author": ["J.J. V\u00e4yrynen", "T. Honkela", "L. Lindqvist"], "venue": "In Proceedings of the Workshop Semantic Content Acquisition and Representation (SCAR),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "Deep learning via semi-supervised embedding", "author": ["Jason Weston", "Frederic Ratle", "Ronan Collobert"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2008}, {"title": "Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing", "author": ["Hai Zhao", "Wenliang Chen", "Chunyu Kit", "Guodong Zhou"], "venue": "In CoNLL 2009 Shared Task,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "We investigate a representation learning approach based on Factorial Hidden Markov Models [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued.", "startOffset": 22, "endOffset": 26}, {"referenceID": 51, "context": "Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued.", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": "Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued.", "startOffset": 54, "endOffset": 62}, {"referenceID": 26, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 180, "endOffset": 188}, {"referenceID": 27, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 180, "endOffset": 188}, {"referenceID": 8, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 207, "endOffset": 214}, {"referenceID": 48, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 207, "endOffset": 214}, {"referenceID": 0, "context": "The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1].", "startOffset": 370, "endOffset": 373}, {"referenceID": 44, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 212, "endOffset": 224}, {"referenceID": 49, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 212, "endOffset": 224}, {"referenceID": 43, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 212, "endOffset": 224}, {"referenceID": 15, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 25, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 33, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 42, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 4, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 50, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 289, "endOffset": 312}, {"referenceID": 8, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 41, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 39, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 380, "endOffset": 391}, {"referenceID": 37, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 9, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 34, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 52, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 415, "endOffset": 431}, {"referenceID": 2, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 466, "endOffset": 473}, {"referenceID": 40, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 466, "endOffset": 473}, {"referenceID": 11, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 51, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 10, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 1, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 493, "endOffset": 508}, {"referenceID": 26, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 28, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 29, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 48, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 27, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 601, "endOffset": 621}, {"referenceID": 0, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 655, "endOffset": 662}, {"referenceID": 18, "context": "2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19].", "startOffset": 655, "endOffset": 662}, {"referenceID": 30, "context": "[31] use a generalized backfitting approach for training a discrete-observation FHMM, but they have not run experiments on a naturally-occurring dataset, and they focus on language modeling rather than representation learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "We use a different training procedure based on variational EM [33], and provide empirical results for a representation learning task on a standard dataset.", "startOffset": 62, "endOffset": 66}, {"referenceID": 46, "context": "[47] has annotated parse trees with latent vectors that compose hierarchically.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Titov and Henderson [48] and Henderson and Titov [25] also learn representations that are affected by global context and also use variational approximation for inference but apply the technique to parsing.", "startOffset": 20, "endOffset": 24}, {"referenceID": 24, "context": "Titov and Henderson [48] and Henderson and Titov [25] also learn representations that are affected by global context and also use variational approximation for inference but apply the technique to parsing.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "[24] use mainly two types of latent variable models: one essentially an HMM and the other has the latent variables associated in a syntactic dependency tree.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 31, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 14, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 21, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 20, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 19, "context": "Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20].", "startOffset": 139, "endOffset": 163}, {"referenceID": 5, "context": "Learning bounds are known [6, 39].", "startOffset": 26, "endOffset": 33}, {"referenceID": 38, "context": "Learning bounds are known [6, 39].", "startOffset": 26, "endOffset": 33}, {"referenceID": 13, "context": "[14] use semi-supervised learning to incorporate labeled and unlabeled data from the target domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "3 The Factorial Hidden Markov Model for Learning Representations The Factorial Hidden Markov Model (FHMM) [23] is a bayesian graphical model in which a sequence of observed variables is generated from a latent sequence of discrete-valued vectors, as shown in Figure 1a.", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "While we borrow the model structure from Ghahramani and Jordan [23], the main between their model and our model is in the observation distribution.", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "As a result of the change to the observation distribution, we need to change the inference and learning procedures, although we continue to follow [23] in using variational approximations, which we explain below.", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "{St} Q({St}) log P ({St}|{Yt}) Q({St}) = L(\u03b8)\u2212KL(Q({St})||P ({St}|{Yt})) (8) and F (Q, \u03b8) = E{St}\u223cQ[logP ({St, Yt})]\u2212 E{St}\u223cQ[logQ({St})] (9) We use the Expectation Maximization (EM) algorithm [17], a block coordinate-ascent algorithm, to learn the parameters \u03b8.", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "We borrow the structured variational approximation model from Ghahramani and Jordan [23] which is shown in 1b.", "startOffset": 84, "endOffset": 88}, {"referenceID": 32, "context": "To handle this term, we make use of a second variational bound, log x \u2264 \u03c6x \u2212 log\u03c6 \u2212 1 [33, 4], where \u03c6 is a new variational parameter which can be varied to make the bound tight.", "startOffset": 86, "endOffset": 93}, {"referenceID": 3, "context": "To handle this term, we make use of a second variational bound, log x \u2264 \u03c6x \u2212 log\u03c6 \u2212 1 [33, 4], where \u03c6 is a new variational parameter which can be varied to make the bound tight.", "startOffset": 86, "endOffset": 93}, {"referenceID": 36, "context": "We used online learning technique [37, 8] for learning our model on large corpus.", "startOffset": 34, "endOffset": 41}, {"referenceID": 7, "context": "We used online learning technique [37, 8] for learning our model on large corpus.", "startOffset": 34, "endOffset": 41}, {"referenceID": 27, "context": "[28] and Blitzer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] which contains 561 MEDLINE sentences with 14554 tokens from the Penn BioIE project.", "startOffset": 0, "endOffset": 3}, {"referenceID": 45, "context": "The dataset consist of sections 15-18 of the Penn Treebank containing 8932 sentences which are annotated with PoS tags and the chunk tags[46].", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "The test set consist of 198 manually annotated sentences [28] from the biomedical domain of Open American National Corpus (OANC).", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "Then we used a preprocessing step defined in [36] to select a subset of good sentences for training the FHMM, resulting in 112,824 sentences.", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "We trained 80 states HMM and used the Viterbi decoded states as representations [28] and a Brown Clustering algorithm [9] with 1000 clusters as representations.", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "We trained 80 states HMM and used the Viterbi decoded states as representations [28] and a Brown Clustering algorithm [9] with 1000 clusters as representations.", "startOffset": 118, "endOffset": 121}, {"referenceID": 48, "context": "[49], we used the prefixes of length 4, 6, 10 and 20 in addition to the whole path for the Brown representation as the features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We also trained a neural network model described in Collobert and Weston [11] and used previous 10 words to predict the next word in the sequence.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "[28], with addition of the context words and context representation dimension as features in a window of size 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In the chunking dataset, the representation learning text we used is different but the test set is the same as the previous work [28] we compare against, who also used the test sentences in the representation learning system.", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "The 50-dimensional word embeddings learned using neural network model from Collobert and Weston [11] is represented as EMBEDDINGS-50.", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "HUANGHMM-80 is a system defined in [28] and SCL is a system defined in [7] and report the error rate on the same test data as ours but with fewer number of tokens (9576 and approx 13,000 respectively).", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "HUANGHMM-80 is a system defined in [28] and SCL is a system defined in [7] and report the error rate on the same test data as ours but with fewer number of tokens (9576 and approx 13,000 respectively).", "startOffset": 71, "endOffset": 74}, {"referenceID": 27, "context": "Previous system HUANGHMM-80 [28] included the text from the test domain itself for representation learning.", "startOffset": 28, "endOffset": 32}], "year": 2014, "abstractText": "Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.", "creator": "LaTeX with hyperref package"}}}