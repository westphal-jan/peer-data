{"id": "1312.6186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "abstract": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.\n\n\n\n\nThe development of GPU A-SGD for computational purposes may well allow the computational power of GPUs to be distributed to large networks with minimal cost.\nThe following studies have been carried out by other researchers;\n1.\nThe paper summarizes the results of three experiments. First, the first was performed in a large-scale network, with high performance and fast and fast, with a very low cost. The second was conducted in a large-scale computer vision, which produced a very high performance and fast performance performance in a very low cost (Fig. 1). The second was conducted in a larger-scale network with high performance and fast, with a very low cost. We identified two main differences between the two studies. The main difference between the two results was that the results showed the only significant difference between the two models between the different studies, for example, in the study of the use of a GPU parallelism. The difference between the two results was that the study was not only about GPU A-SGD but also about GPU B. The results show a similar pattern of performance (Fig. 2c).\n\nThis study was performed in the large-scale, large-scale and high-performance computer vision system of the University of Chicago. The main differences between the two studies are that the study was based on a large-scale system and its large-scale performance. The main difference between the two studies is that the results show the only significant difference between the two studies, for example, in the study of the use of a GPU parallelism. The second study was conducted in a large-scale computer vision, which produced a very high performance and fast performance performance in a", "histories": [["v1", "Sat, 21 Dec 2013 00:56:56 GMT  (5891kb,D)", "http://arxiv.org/abs/1312.6186v1", "6 pages, 4 figures"]], "COMMENTS": "6 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CV cs.DC cs.LG cs.NE", "authors": ["thomas paine", "hailin jin", "jianchao yang", "zhe lin", "thomas huang"], "accepted": false, "id": "1312.6186"}, "pdf": {"name": "1312.6186.pdf", "metadata": {"source": "CRF", "title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training", "authors": ["Tom Paine", "Hailin Jin"], "emails": ["paine1@illinois.edu"], "sections": [{"heading": null, "text": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time."}, {"heading": "1 Introduction", "text": "Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10]. This is partly the result of larger datasets, e.g. the Imagenet Large Scale Visual Recognition Challenge (ILSVRC) [7] and accelerated training algorithms that can make use of the data. These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4]. We believe accelerating training further will result in more break throughs in computer vision.\nWe present experiments using a new system for accelerating neural network training, using asynchronous stochastic gradient descent (A-SGD) with many GPUs, which we call GPU A-SGD. We show that this system can be used to speed up training by several times, and explore how to best use GPU A-SGD to further speed up training. To benchmark our speed up, we use the pipeline found in [12]. We train a convolutional neural network on the ILSVRC 2012 dataset, which has 1000 classes, and 1.2 million images. Like that work, our network uses dropout [11], relu neurons [14], and is trained use data augmentation.\nWe will first review neural network training algorithms. And then highlight how our training algorithm differs from existing methods.\nar X\niv :1\n31 2.\n61 86\nv1 [\ncs .C\nV ]\n2 1"}, {"heading": "2 Training neural networks", "text": "A neural network can be seen as a large parameterized function. The parameters in this function can be learned through gradient descent style algorithms. In traditional gradient descent, the gradient of the objective function needs to be calculated over the entire dataset. The parameters are then updated with this gradient. This is repeated until convergence. There are two main issues with this approach: The dataset may be too large to fit into memory, and the gradient may take too long to compute.\nWhen the dataset is too large, stochastic gradient descent (SGD) may be used. Here the gradient of the objective function is calculated over a small random partition of the dataset called a minibatch. The parameters are updated with this minibatch gradient, and a new minibatch is chosen. This process is repeated until convergence. This algorithm can be accelerated in two ways: speeding up the calculation of the minibatch gradient (model parallelism), and parallelization of the stochastic gradient descent steps (data parallelism)."}, {"heading": "2.1 Model parallelism", "text": "In many approaches, the structure of neural network computations is exploited to speed up the calculation of the minibatch gradient. This can be called model parallelism. This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4]. The distributed approaches have the added benefit that they can train models that are too big to fit in memory on a single device. In many cases, these models ignore parallelization of SGD, with [6] being the notable exception. It\u2019s DistBelief technique makes use of both model parallelism, and data parallelism, which we will talk about more below.\nOne work [4] is similar to ours in that they experiment with many GPUs in a distributed framework to accelerate computation of very large models. Their work differs from ours because they primarily focus on model parallelism to train models too big to fit on a single device, especially for unsupervised pre-training of locally-connected neural networks. They are able to train the billion parameter model of [13], using a significantly smaller number of nodes by leveraging consumer off-the-shelf GPUs and high-speed interconnect. While this line of research is very promising, these locally-connected, unsupervised models are not currently the top performing models on common computer vision benchmarks like ILSVRC. We believe our approach is complementary to theirs."}, {"heading": "2.2 Data parallelism", "text": "Another method for speeding up training of neural networks is using distributed versions of stochastic gradient decent [1, 6, 15, 19]. These methods can be called data parallel because they speed up the rate as which the entire dataset contributes to the optimization.\nThe data parallel part of the DistBelief [6] model, (A-SGD) is especially interesting, because it is essentially many neural network models training independently, and occasionally communicating with a central parameter server to synchronize the overall effect for many distributed gradient updates. This makes it straight-forward to apply with various model parallel approaches. This model has also proved useful for computer vision problems, achieving state-of-the-art performance on a computer vision benchmark with 14 million images1 [13]. While these methods may outperform single GPU based methods, by leveraging many more parameters, they operate at a very large scale (thousands of CPU cores)."}, {"heading": "2.3 Our contribution: GPU A-SGD", "text": "Our work also exploits both model parallelism, and data parallelism. We use GPUs for model parallelism, and A-SGD for data parallelism. A-SGD is a subset of the DistBelief system described in [6]. Our technique ignores their distributed CPU approach for model parallelism, and instead used GPUs to accelerate gradient computation. Multiple replicas of a model are used to optimize a single objective. Each model replica is trained using a GPU. This is achieved by extending the\n1Imagenet Fall 2011 release, not to be confused with the ILSVRC 2012, which is a subset of the Fall release.\npublicly available cuda-convnet code2 used in [12] to allow several GPU clients to communicate with a server. We use MPI for communication.\nEach model requests updated parameters every nfetch steps, and sends updated gradient values every npush steps. In the DistBelief paper [6] nfetch = npush = 1. This regime would not work well for GPUs, where the gradients are not usually communicated to the CPU after every minibatch. Typically the parameters are updated on the GPU for nsync steps before being copied to the CPU, where nsync can be large, e.g. 600. This is because there is additional overhead cost for transferring the parameters from the GPU to the CPU. This overhead can reduce the benefit for GPU accelerate gradient calculations. In our experiments we set nfetch = npush = nsync. We experiment with different values of nsync."}, {"heading": "3 Experiments", "text": "To test GPU A-SGD, we train a convolutional neural network with the same architecture as described in [12] on the ILSVRC 2012 dataset. On a single NVIDIA Tesla K20X GPU this takes about 10.7 days.\nWe performed all experiments on the Blue Water supercomputer. It has over 4000 Nvidia Tesla K20X nodes, and has a Gemini high-speed interconnect. While we make use of a very highperformance machine, [4] notes that GPUs and high speed interconnect are now available off-theshelf. All of our experiments are performed with 32 or less GPU nodes."}, {"heading": "3.1 Experiment 1", "text": "Our first experiment is to test whether we can achieve similar performance to [12] with GPU A-SGD. We used the same settings we used in the single GPU cases, with nsync = 600. For this experiment we use 8 GPU clients. The resulting learning curves are shown in (fig .1). We get near state of the art performance by epoch 22 which takes 3.3 days, before overfitting. This is about a 3.2x speed up. In our experience, the minibatch test set performance is usually 2-3% higher than the overall test set performance after averaging 10 crops as in [12]. That is true here, the checkpoint before over-fitting gets a test error of 42.2%.\nFor the next experiments we want to compare the speed up using varying numbers of GPU clients and varying values of nsync. Since, it is hard to interpret many raw learning curves on a single plot, we smooth each plot using a sliding window of 400 mini batches. Also, we plot only the training error, so that the sliding window doesn\u2019t need to be adjusted for different values of nsync. Since the\n2The original cuda-convnet code is available at: https://code.google.com/p/cuda-convnet/\ntraining and testing error are very similar for the early training period we observe, we feel this is indicative of performance."}, {"heading": "3.2 Experiment 2", "text": "In our second experiment, we examined the effect of a cold start on the learning, as the number of GPU clients increases from 2 to 32 (fig. 2). Each GPU A-SGD instance is run for 24 hours. We observe that as the number of GPUs increase, initial learning becomes much slower. We also observe that later in training, GPU A-SGD instances with more GPU clients learn more rapidly. We hypothesize that early in training, there are many gradient directions that may decrease error. Since each GPU client calculates different gradients, averaging them may slow progress. Later in training gradients become more consistent and averaging them increases the speed of learning. This result suggests that a warm start may be beneficial as suggested in [6]. This may also be improved by methods that explicitly deal with variance in gradients such as adagrad [9] and adadelta [17]."}, {"heading": "3.3 Experiment 3", "text": "In our third experiment, we explore how nsync effects learning with many GPU clients. We try nsync values from 100 to 900 and 1-8 GPU clients (fig. 3). We begin all experiments from a warm start, which we obtained by training the network on a single GPU for 12 hours. With a warm start, the effect of many GPU clients is clearer. When nsync = 100, our error decreases from 70% with a single GPU to 58% with 8 GPUs. Note that as nsync increases, the error curve has jagged artifacts. We believe these are from stale updates.\nAlso note that when nsync = 100, significantly fewer minibatches are processed in 24 hours, but the error rate is still lower. This suggests that while there is a cost associated with increased update frequency, it may still be a net win.\nTo emphasize these observations, we plot the learning curves for 8 GPU clients with nsync values from 100 to 900 (fig. 4)."}, {"heading": "4 Future directions", "text": "We plan to explore Adagrad [9] and Adadelta [17] to see if they can further boost performance. We believe GPU A-SGD is a promising direction. Recently [18] showed that larger models can further improve performance on computer vision tasks, and that these larger models begin to over\nfit, suggesting they would benefit from more training data. Both larger models, and larger training sets would benefit from faster training times."}, {"heading": "Acknowledgments", "text": "This material is based upon work supported by the National Science Foundation under Grant No. 392 NSF IIS 13-18971.\nThis research is part of the Blue Waters sustained-petascale computing project, which is supported by the National Science Foundation (award number OCI 07-25070) and the state of Illinois. Blue Waters is a joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "In Decision and Control (CDC),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Deep learning with cots hpc systems", "author": ["A. Coates", "B. Huval", "T. Wang", "D. Wu", "B. Catanzaro", "N. Andrew"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "arXiv preprint arXiv:1311.2524,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1112.6209,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. R\u00e9", "S.J. Wright"], "venue": "arXiv preprint arXiv:1106.5730,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["R. Raina", "A. Madhavan", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ArXiv e-prints,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10].", "startOffset": 155, "endOffset": 158}, {"referenceID": 7, "context": "Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10].", "startOffset": 179, "endOffset": 194}, {"referenceID": 11, "context": "Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10].", "startOffset": 179, "endOffset": 194}, {"referenceID": 12, "context": "Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10].", "startOffset": 179, "endOffset": 194}, {"referenceID": 17, "context": "Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10].", "startOffset": 179, "endOffset": 194}, {"referenceID": 9, "context": "Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10].", "startOffset": 217, "endOffset": 221}, {"referenceID": 6, "context": "the Imagenet Large Scale Visual Recognition Challenge (ILSVRC) [7] and accelerated training algorithms that can make use of the data.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 55, "endOffset": 69}, {"referenceID": 5, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 55, "endOffset": 69}, {"referenceID": 14, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 55, "endOffset": 69}, {"referenceID": 18, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 55, "endOffset": 69}, {"referenceID": 1, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 79, "endOffset": 92}, {"referenceID": 2, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 79, "endOffset": 92}, {"referenceID": 4, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 79, "endOffset": 92}, {"referenceID": 11, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 79, "endOffset": 92}, {"referenceID": 15, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 79, "endOffset": 92}, {"referenceID": 3, "context": "These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "To benchmark our speed up, we use the pipeline found in [12].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "Like that work, our network uses dropout [11], relu neurons [14], and is trained use data augmentation.", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "Like that work, our network uses dropout [11], relu neurons [14], and is trained use data augmentation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4].", "startOffset": 32, "endOffset": 49}, {"referenceID": 2, "context": "This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4].", "startOffset": 32, "endOffset": 49}, {"referenceID": 4, "context": "This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4].", "startOffset": 32, "endOffset": 49}, {"referenceID": 11, "context": "This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4].", "startOffset": 32, "endOffset": 49}, {"referenceID": 15, "context": "This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4].", "startOffset": 32, "endOffset": 49}, {"referenceID": 5, "context": "This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "In many cases, these models ignore parallelization of SGD, with [6] being the notable exception.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "One work [4] is similar to ours in that they experiment with many GPUs in a distributed framework to accelerate computation of very large models.", "startOffset": 9, "endOffset": 12}, {"referenceID": 12, "context": "They are able to train the billion parameter model of [13], using a significantly smaller number of nodes by leveraging consumer off-the-shelf GPUs and high-speed interconnect.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Another method for speeding up training of neural networks is using distributed versions of stochastic gradient decent [1, 6, 15, 19].", "startOffset": 119, "endOffset": 133}, {"referenceID": 5, "context": "Another method for speeding up training of neural networks is using distributed versions of stochastic gradient decent [1, 6, 15, 19].", "startOffset": 119, "endOffset": 133}, {"referenceID": 14, "context": "Another method for speeding up training of neural networks is using distributed versions of stochastic gradient decent [1, 6, 15, 19].", "startOffset": 119, "endOffset": 133}, {"referenceID": 18, "context": "Another method for speeding up training of neural networks is using distributed versions of stochastic gradient decent [1, 6, 15, 19].", "startOffset": 119, "endOffset": 133}, {"referenceID": 5, "context": "The data parallel part of the DistBelief [6] model, (A-SGD) is especially interesting, because it is essentially many neural network models training independently, and occasionally communicating with a central parameter server to synchronize the overall effect for many distributed gradient updates.", "startOffset": 41, "endOffset": 44}, {"referenceID": 12, "context": "This model has also proved useful for computer vision problems, achieving state-of-the-art performance on a computer vision benchmark with 14 million images1 [13].", "startOffset": 158, "endOffset": 162}, {"referenceID": 5, "context": "A-SGD is a subset of the DistBelief system described in [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 11, "context": "publicly available cuda-convnet code2 used in [12] to allow several GPU clients to communicate with a server.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "In the DistBelief paper [6] nfetch = npush = 1.", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "To test GPU A-SGD, we train a convolutional neural network with the same architecture as described in [12] on the ILSVRC 2012 dataset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "While we make use of a very highperformance machine, [4] notes that GPUs and high speed interconnect are now available off-theshelf.", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "Our first experiment is to test whether we can achieve similar performance to [12] with GPU A-SGD.", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "In our experience, the minibatch test set performance is usually 2-3% higher than the overall test set performance after averaging 10 crops as in [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "This result suggests that a warm start may be beneficial as suggested in [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "This may also be improved by methods that explicitly deal with variance in gradients such as adagrad [9] and adadelta [17].", "startOffset": 101, "endOffset": 104}, {"referenceID": 16, "context": "This may also be improved by methods that explicitly deal with variance in gradients such as adagrad [9] and adadelta [17].", "startOffset": 118, "endOffset": 122}, {"referenceID": 8, "context": "We plan to explore Adagrad [9] and Adadelta [17] to see if they can further boost performance.", "startOffset": 27, "endOffset": 30}, {"referenceID": 16, "context": "We plan to explore Adagrad [9] and Adadelta [17] to see if they can further boost performance.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Recently [18] showed that larger models can further improve performance on computer vision tasks, and that these larger models begin to over", "startOffset": 9, "endOffset": 13}], "year": 2013, "abstractText": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "creator": "LaTeX with hyperref package"}}}