{"id": "1312.6190", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Adaptive Feature Ranking for Unsupervised Transfer Learning", "abstract": "Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. In this paper, we propose a method and efficient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs. Our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain, and it works with unsupervised learning and knowledge-based transfer. This paper is the first to introduce such a technique, demonstrating that, on the basis of the concept of learning, it works as a way of learning to fit the given target domain, regardless of whether a target domain or the target domain is an object of choice.\n\n\n\n\n\nThe proposed method for learning is described in a table of learning concepts in the next section. The following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational problem domains in question.\nThe following concepts are used to identify the underlying computational", "histories": [["v1", "Sat, 21 Dec 2013 01:50:08 GMT  (413kb,D)", "https://arxiv.org/abs/1312.6190v1", "9 pages 7 figures, summitting to 2nd International Conference on Learning Representations (ICLR2014)"], ["v2", "Wed, 28 May 2014 16:35:17 GMT  (529kb,D)", "http://arxiv.org/abs/1312.6190v2", "9 pages 7 figures, new experimental results on ranking and transfer have been added, typo fixed"]], "COMMENTS": "9 pages 7 figures, summitting to 2nd International Conference on Learning Representations (ICLR2014)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["son n tran", "artur d'avila garcez"], "accepted": false, "id": "1312.6190"}, "pdf": {"name": "1312.6190.pdf", "metadata": {"source": "CRF", "title": "Adaptive Feature Ranking for Unsupervised Transfer Learning", "authors": ["Son N. Tran", "Artur d\u2019Avila Garcez"], "emails": ["Son.Tran.1@city.ac.uk", "aag@soi.city.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3]. In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common. In connectionist transfer learning, many approaches transfer the knowledge selected specifically based on the target, and (in some cases) with the provision of labels in the source domain [8, 16]. In constrast, we are interested in selecting the representations that can be transferred in general to the target without the provision of labels in the source domain, which is similar to self-taught learning [11, 6]. In addition, we propose to study how much knowledge should be transferred to the target domain, which has not been studied yet in self-taught mode.\nThis paper introduces a method and efficient algorithm for ranking and selecting representation knowledge from a Restricted Boltzmann Machine (RBM) [13, 4] trained on a source domain to be transferred onto a target domain. A ranking function is defined that is shown to minimize information loss in the source RBM. High-ranking features are then transferred onto a target RBM by setting some of the network\u2019s parameters. The target RBM is then trained to adapt a set of additional parameters on a target dataset, while keeping the parameters set by transfer learning fixed.\nExperiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs in three out of four cases tested, where the MNIST dataset is used as source and either the ICDAR or TiCC dataset is used as target. As expected, our method improves on the predictive accuracy of the standard self-taught learning method for unsupervised transfer learning [11].\nar X\niv :1\n31 2.\n61 90\nv2 [\ncs .L\nG ]\n2 8\nM ay\nOur method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain. In transfer learning, it is normal for the choice of the knowledge to be transferred from the source domain to rely on the nature of the target domain. For example, in [7], knowledge such as data samples in the source domain are transformed to a target domain to enrich supervised learning. In our approach, the transfer learning is unsupervised, as done in [11]. We are concerned with inductive transfer learning in that representation learned from a domain can be useful in analogous domains. For example, in [1] a common orthonormal matrix is learned to construct linear features among multiple tasks. Self-taught learning [11], on the other hand, applies sparse coding to transfer the representations learned from source data onto the target domain. Like self-taught, we are interested in unsupervised transfer learning using cross-domain features. The system has been implemented in MATLAB; all learning parameters and data folds are available upon request.\nThe outline of the paper is as follows. In Section 2, we define feature selection by ranking in which high scoring features are associated with significant part of the network. In Section 3, we introduce the adaptive feature learning method and algorithm to combine selective features in source domain with features in target domain. In Section 4, we present and discuss the experimental results. Section 5 concludes and discusses directions for future work."}, {"heading": "2 Feature Selection by Ranking", "text": "In this section, we present a method for selecting representations from an RBM by ranking their features. We define a ranking function and show that it can capture the most significant information in the RBM, according to a measure of information loss.\nIn a trained RBM, we define cj to be a score for each unit j in the hidden layer. The score represents the weight-strength of a sub-network Nj (hj \u223c V ) consisting of all the connections from unit j to the visible layer. A score cj is a positive real number that can be seen as capturing the uncertainty in the sub-network. We expect to be able to replace all the weights of a network by appropriate cj or \u2212cj with minimum information loss. We define the information loss as:\nIloss = \u2211 j \u2016wj \u2212 cjsj\u20162 (1)\nwhere wj is a vector of connection weights from unit j in the hidden layer to all the units in the visible layer, sj is a vector of the same size visN as wj , and sj \u2208 [\u22121 1]visN . Since equation (1) is a quadratic function, the value of cj that minimizes information loss can be found by setting the derivatives to zero, as follows:\n\u2211 i\n2(wij \u2212 cjsij)sij = 0 with all j\u2211 i wijsij \u2212 cj \u2211 i s2ij = 0 with all j\ncj =\n\u2211 i wijsij\u2211\ni s 2 ij\n(2)\nSince sij = [\u22121 1] and from equation (1), we can see that:\n\u2016wij \u2212 cjsij\u20162 = \u2016abs(wij)\u2212 cj sij\nsign(wij) \u20162 \u2265 \u2016abs(wij)\u2212 cj\u20162 (3)\nholds if and only if sij = sign(wij), which will also minimize Iloss. Applying equation (3) to (2), we obtain:\ncj =\n\u2211 i abs(wij)\nvisN (4)\nWe may notice that using cj instead of weights would result in a compression of the network; however, in this paper we focus on the scores only for selecting features to use for transfer learning. In what follows, we give a practical example which shows that low-score features are semantically less meaningful than high-scoring ones.\nExample 2.1 We have trained an RBM with 10 hidden nodes to model the XOR function from its truth-table such that z = x XOR y. The true, false logical values are represented by integers 1, 0, respectively. After training the network, a score for each sub-network can be obtained. The score allows us to replace each real-value weight by its sign, and interpret those signs logically where a negative sign (\u2212) represents logical negation (\u00ac), as exemplified in Table 1.\nTable 2 shows all the sub-networks with associated scores. As one may recognize, each sub-network represents a logical rule learned from the RBM. However, not all of the rules are correct w.r.t. the XOR function. In particular, the sub-network scoring 0.158 encodes a rule \u00acz = \u00acx \u2227 y, which is inconsistent with z = x XOR y. By ranking the sub-networks according to their scores, this can be identified: high-scoring sub-networks are consistent with the data, and low-scoring ones are not. We have repeated the training several times with different numbers of hidden units, obtaining similar intuitive results.\nIn what follows, we will study the effect of pruning low-scoring sub-networks from an RBM trained on complex image domain data. So far, we have seen that our scoring can be useful at identifying relevant representations. In particular, if the score of a sub-network is small in relation to the average score of the network, they seem more likely to represent noise."}, {"heading": "3 Adaptive Feature Learning", "text": "In this section, we introduce adaptive feature learning. Given an RBM trained on a dataset, we are interested in investigating whether the score ranking introduced in Section 2 can be useful at\nimproving the predictive accuracy of another RBM trained on a target domain. From a transfer learning perspective, we intend to produce a general transfer learning algorithm, ranking knowledge from a source domain to guide the learning on a target domain, using RBMs as transfer medium. In particular, we propose a transfer learning model as shown in Figure 1. Knowledge learned from a source domain is selected by the ranking function for transferring onto a target domain, as explained in what follows. The selection of features in the source domain is general in that it is independent from the data from the target domain.\nIn the target domain, an RBM is trained given a number of transferred parameters (W (t)): a fixed set of weights associated with high-ranking sub-networks from the source domain. The output of transferred hidden nodes in target domain is considered as self-taught features [11]. The connections between the visible layer and the hidden units transferred onto the target RBM can be seen as a set of up-weights and down-weights. How much the down-weights affect the learning in the target RBM depends on an influence factor \u03b8; in this paper \u03b8 \u2208 [0 1]. If \u03b8 = 0 then the features are combined but the transferred knowledge will not influence learning in the target domain. In this case, the result is a combination of self-taught features and target RBM features. Otherwise, if \u03b8 = 1 then the transferred knowledge will influence learning in the target domain. We refer to this case as adaptive feature learning, because the knowledge transferred from the source domain is used to guide the learning of new features in the target domain. The outputs of additional hidden nodes (associated with parameter U ) in target RBM are called adaptive features.\nAs usual, we train the target RBM to maximize the log-likelihood:\nL(U |D;W (t)) (5)\nusing Contrastive Divergence [4].\nAlgorithm 1 Adaptive feature learning Require: A trained RBM N\n1: Select a number of sub-networks N (t) \u2208 N with the highest scores 2: Encode parameters W (t) from N (t) into a new RBM 3: Add hidden units (U ) to the RBM 4: loop 5: % until convergence 6: V+ \u21d0 X 7: H+ \u2190 p(H|V+); H\u0302+\nsmp\u2190 p(H|V+); 8: V\u2212 \u2190 p(V |H\u0302+); V\u0302\u2212\nsmp\u2190 p(V |H\u0302+) 9: H\u2212 \u2190 p(H|V\u0302\u2212)\n10: Y = U + \u03b7(\u3008V T+ H (t) + \u2212 V\u0302 T\u2212H (t) \u2212 \u3009) 11: end loop"}, {"heading": "4 Experimental Results", "text": "We start by evaluating the approach on the MNIST handwritten dataset. First, we want to check if hidden units with low scores are indeed less significant in the case of image domains. Subsequently,\nwe show that knowledge from one image domain can be used to improve predictive accuracy in another image domain."}, {"heading": "4.1 Feature Selection", "text": "We have trained an RBM with 500 hidden nodes on 20,000 samples from the MNIST dataset in order to visualized the filter bases of the 50 highest scoring sub-networks and the 50 lowest scoring subnetworks (each takes 10% of the network\u2019s capacity). Figure 2 shows the result of using a standard RBM, and Figure 3 shows the result of using a sparse RBM [6]. As can be seen, in Figure 2, high scores are mostly associated with more concrete visualizations of the expected MNIST patterns, while low scores are mostly associated with fading or noisy patterns. In Figure 3, high-scores are associated with sparse representations, while low-scores produce less meaningful representations according to the way in which the RBM was trained. In sparse RBM we use PCA to reduce the dimensionality of the images to 69 and train the network with Gaussian visible units.\nWe also examined visually the impact of the scores on an RBM with 500 hidden units trained on the MNIST dataset\u2019s 10,000 examples. Sub-networks with the highest scores were gradually removed, and the pruned RBM was compared on the reconstruction of images with that of the original RBM, as illustrated in Figure 4. In Figure 5 we shows the reconstruction of test images from RBMs in which low-scored features were gradually removed.\nFinally, in order to obtain accuracy measures, we have provided the features obtained from the pruned RBMs as an input to an SVM classifier. Figure 6 shows the drop in accuracy with the gradual pruning of the RBM. In case of pruning low-scored features, at first, some removal of\nunits have produced a slight increase in accuracy. Then, the results indicate that it is possible to remove nodes and maintain performance almost unchanged (until relevant units start to be removed, at which point accuracy deteriorates, when more than half of the number of units is removed). In case of pruning high-scored features, the accuracy decreases significantly when more than 10% of hidden units are removed."}, {"heading": "4.2 Unsupervised Transfer Learning", "text": "In order to evaluate transfer learning in the context of images, we have transferred sub-networks from an RBM trained on the MNIST1 dataset to RBMs trained on the ICDAR2 and TiCC datasets3.\nBelow, we use MNIST30k and MNIST05K to denote datasets with 30,000 and 5,000 samples from the MNIST data collection. For the TiCC collection, we denote TiCCd and TiCCa as the datasets of digits and letters, respectively. TiCCc and TiCCw are character samples from two different groups of writers. TiCCw A and TiCCw B are from the same group but the latter has a much smaller training set. Each column in Table 3 indicates a transfer experiment, e.g. MNIST30k : ICDARd uses the\n1 http://yann.lecun.com/exdb/mnist/ 2 http://algoval.essex.ac.uk:8080/icdar2005/index.jsp?page=ocr.html 3 http://homepage.tudelft.nl/19j49/Datasets.html\nMNIST dataset as source domain and ICDAR as target domain. The percentages show the predictive accuracy on the target domain, as detailed in the sequel.\nIn Table 3, we have measured classification performance when using the knowledge from the source domain to guide the learning in the target domain. For each domain, the data is divided into training, validation and test sets. Each experiment is repeated 50 times and we use the validation set to select the best model (number of transferred sub-networks, number of added units, learning rates and SVM hyper-parameters). The table reports the average results on the test sets. The results show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs in three out of four cases tested, where the MNIST dataset is used as source and either the ICDAR or TiCC dataset is used as target. As expected, our method improves on the predictive accuracy of the standard self-taught learning method for unsupervised transfer learning. In order to compare with transferring low-scored features, we performed experiments follows what we have done with transferring high-scored features, except that in this case the features are ranked from low-scores to high-scores. We observed that the highest accuracies were achieved only when a large number of high-scored features are among those which have been transferred.\nWe also carried out experiments on the TiCC collection transferring from characters to digits, digits to characters, and from group of writers to group of other writers. The results in Table 4 show that in two out of three experiments adaptive learning and combining features did not gain advantages over combination of selective features from source domain and features from target domain. It may suggest that using selective combination of features would be better and more efficient in a case that the source and target domains are considerably similar(i.e TiCCc:TICCw B)\nTo compare our transfer learning approach with self-taught learning [11], we train an RBM in the source domain and use it to extract common features in the target domain for classification. In the experiments where the domains have a close relation such as the same type of data (digits in MNIST05k:TiCCd) or in the same collections (TiCCd:TiCCa,TiCCd:TiCCd,TiCCc:TiCCwB), sefltaught learning works very well especially when the training dataset in the target domain is small (TiCCc:TiCCwB). We also use sparse-coder [5] provided by Lee4 for self-taught learning as in [11], except that instead of using PCA for preprocessing data we apply the model directly to the raw\n4http://ai.stanford.edu/\u02dchllee/softwares/nips06-sparsecoding.htm\npixels since that is what has been done with the RBM. Figure 7 shows the performance of self-taught learning on the datasets using RBMs and sparse-coder as feature learners.\nWith transfer, it is generally accepted that the performance of the model in a target domain will depend on the quality of the knowledge it received and the structure of the model. We then evaluated performance of the model using different sizes of transferred knowledge and number of units added to the hidden layer. Figure 8 shows that if the size of transferred knowledge is too small, it will be dominated by the data from the target domain. However, if the size of transferred knowledge is too large it can cause a drop in performance since the model will try to learn new knowledge mainly based on the transferred knowledge with little knowledge from the target domain."}, {"heading": "5 Conclusion and Future Work", "text": "We have presented a method and efficient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. The method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain, and it works with unsupervised learning and knowledgebased transfer. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs.\nIn this paper we focus on selecting features from shallow network (RBM) for general transfer learning. In future work we are interested in learning and transferring high-level features from deep networks selectively for a specific domain."}], "references": [{"title": "Multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "The connectionist inductive learning and logic programming system", "author": ["Artur S. Avila Garcez", "Gerson Zaverucha"], "venue": "Applied Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Deep transfer via second-order markov logic", "author": ["Jesse Davis", "Pedro Domingos"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Sparse deep belief net model for visual area v2", "author": ["Honglak Lee", "Chaitanya Ekanadham", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Transfer learning by borrowing examples for multiclass object detection", "author": ["Joseph J. Lim", "Ruslan Salakhutdinov", "Antonio Torralba"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Unsupervised and transfer learning challenge: a deep learning approach", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Xavier Glorot", "Salah Rifai", "Yoshua Bengio", "Ian J. Goodfellow", "Erick Lavoie", "Xavier Muller", "Guillaume Desjardins", "David Warde-Farley", "Pascal Vincent", "Aaron C. Courville", "James Bergstra"], "venue": "In ICML Unsupervised and Transfer Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Mapping and revising markov logic networks for transfer learning", "author": ["Lilyana Mihalkova", "Tuyen Huynh", "Raymond J. Mooney"], "venue": "In Proceedings of the 22nd national conference on Artificial intelligence - Volume 1,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["Rajat Raina", "Alexis Battle", "Honglak Lee", "Benjamin Packer", "Andrew Y. Ng"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1986}, {"title": "Transfer learning via advice taking", "author": ["Lisa Torrey", "Jude W. Shavlik", "Trevor Walker", "Richard Maclin"], "venue": "In Advances in Machine Learning I,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Knowledge-based artificial neural networks", "author": ["Geoffrey G. Towell", "Jude W. Shavlik"], "venue": "Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Heterogeneous transfer learning with rbms", "author": ["Bin Wei", "Christopher Pal"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 1, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 0, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 10, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 12, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 8, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 2, "context": "A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3].", "startOffset": 143, "endOffset": 171}, {"referenceID": 9, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 0, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 10, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 12, "context": "In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common.", "startOffset": 21, "endOffset": 36}, {"referenceID": 7, "context": "In connectionist transfer learning, many approaches transfer the knowledge selected specifically based on the target, and (in some cases) with the provision of labels in the source domain [8, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 14, "context": "In connectionist transfer learning, many approaches transfer the knowledge selected specifically based on the target, and (in some cases) with the provision of labels in the source domain [8, 16].", "startOffset": 188, "endOffset": 195}, {"referenceID": 10, "context": "In constrast, we are interested in selecting the representations that can be transferred in general to the target without the provision of labels in the source domain, which is similar to self-taught learning [11, 6].", "startOffset": 209, "endOffset": 216}, {"referenceID": 5, "context": "In constrast, we are interested in selecting the representations that can be transferred in general to the target without the provision of labels in the source domain, which is similar to self-taught learning [11, 6].", "startOffset": 209, "endOffset": 216}, {"referenceID": 11, "context": "This paper introduces a method and efficient algorithm for ranking and selecting representation knowledge from a Restricted Boltzmann Machine (RBM) [13, 4] trained on a source domain to be transferred onto a target domain.", "startOffset": 148, "endOffset": 155}, {"referenceID": 3, "context": "This paper introduces a method and efficient algorithm for ranking and selecting representation knowledge from a Restricted Boltzmann Machine (RBM) [13, 4] trained on a source domain to be transferred onto a target domain.", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "As expected, our method improves on the predictive accuracy of the standard self-taught learning method for unsupervised transfer learning [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "For example, in [7], knowledge such as data samples in the source domain are transformed to a target domain to enrich supervised learning.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "In our approach, the transfer learning is unsupervised, as done in [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "For example, in [1] a common orthonormal matrix is learned to construct linear features among multiple tasks.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "Self-taught learning [11], on the other hand, applies sparse coding to transfer the representations learned from source data onto the target domain.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "The output of transferred hidden nodes in target domain is considered as self-taught features [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "How much the down-weights affect the learning in the target RBM depends on an influence factor \u03b8; in this paper \u03b8 \u2208 [0 1].", "startOffset": 116, "endOffset": 121}, {"referenceID": 3, "context": "using Contrastive Divergence [4].", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "Figure 2 shows the result of using a standard RBM, and Figure 3 shows the result of using a sparse RBM [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "e TiCCc:TICCw B) To compare our transfer learning approach with self-taught learning [11], we train an RBM in the source domain and use it to extract common features in the target domain for classification.", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "We also use sparse-coder [5] provided by Lee4 for self-taught learning as in [11], except that instead of using PCA for preprocessing data we apply the model directly to the raw", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "We also use sparse-coder [5] provided by Lee4 for self-taught learning as in [11], except that instead of using PCA for preprocessing data we apply the model directly to the raw", "startOffset": 77, "endOffset": 81}], "year": 2014, "abstractText": "Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. In this paper, we propose a method and efficient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically significant improvements on the training of RBMs. Our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any specific target domain, and it works with unsupervised learning and knowledge-based transfer.", "creator": "LaTeX with hyperref package"}}}