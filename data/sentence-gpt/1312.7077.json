{"id": "1312.7077", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2013", "title": "Language Modeling with Power Low Rank Ensembles", "abstract": "We present power low rank ensembles, a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method is a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. On English and Russian evaluation sets, we obtain noticeably lower perplexities relative to state-of-the-art modified Kneser-Ney and class-based n-gram models to reduce confusion with the normal distribution of uncorrected probabilities, and to ensure that the generalization of a n-gram model with normalization is not required. Thus, a standardized approach for non-integer n is not required. For example, we use N-gram modeling for class-based n-gram models, rather than N-gram modeling for class-based n-gram models. We are not proposing a generalization approach to numerical analysis for class-based n-gram models because n-gram models are not standardized. We propose a generalization approach to numerical analysis for class-based n-gram models because n-gram models are not standardized.", "histories": [["v1", "Thu, 26 Dec 2013 09:45:02 GMT  (38kb)", "https://arxiv.org/abs/1312.7077v1", null], ["v2", "Fri, 3 Oct 2014 08:28:03 GMT  (157kb,D)", "http://arxiv.org/abs/1312.7077v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["ankur p parikh", "avneesh saluja", "chris dyer", "eric p xing"], "accepted": true, "id": "1312.7077"}, "pdf": {"name": "1312.7077.pdf", "metadata": {"source": "CRF", "title": "Language Modeling with Power Low Rank Ensembles", "authors": ["Ankur P. Parikh", "Avneesh Saluja", "Eric P. Xing"], "emails": ["apparikh@cs.cmu.edu", "avneesh@cs.cmu.edu", "cdyer@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010). The predominant approach to language modeling is the n-gram model, wherein the probability of a word sequence P (w1, . . . , w`) is decomposed using the chain rule, and then a Markov assumption is made: P (w1, . . . , w`) \u2248\u220f` i=1 P (wi|w i\u22121 i\u2212n+1). While this assumption substantially reduces the modeling complexity, parameter estimation remains a major challenge. Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed in the training data (Manning and Schu\u0308tze, 1999).\nMany smoothing techniques have been proposed to address the estimation challenge. These reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999).\nSomewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cande\u0300s and Recht, 2009; Cai et al., 2010). In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011). For example, in recommender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items. By projecting the low rank representation of a user\u2019s (sparse) preferences into the original space, an estimate of ratings for new items is obtained. These methods are attractive due to their computational efficiency and mathematical well-foundedness.\nIn this paper, we introduce power low rank ensembles (PLRE), in which low rank tensors are used to produce smoothed estimates for n-gram probabilities. Ideally, we would like the low rank structures to discover semantic and syntactic relatedness among words and n-grams, which are used to produce smoothed estimates for word sequence probabilities. In contrast to the few previous low rank language modeling approaches, PLRE is not orthogonal to n-gram models, but rather a general framework where existing n-gram smoothing methods such as Kneser-Ney smoothing are special cases. A key insight is that PLRE does not compute low rank approximations of the original\nar X\niv :1\n31 2.\n70 77\nv2 [\ncs .C\nL ]\n3 O\nct 2\n01 4\njoint count matrices (in the case of bigrams) or tensors i.e. multi-way arrays (in the case of 3-grams and above), but instead altered quantities of these counts based on an element-wise power operation, similar to how some smoothing methods modify their lower order distributions.\nMoreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabularies. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V \u2248 1 \u00d7 106) leading to fast training times. This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a sequence can be queried in time O(\u03bamax) where \u03bamax is the maximum rank of the low rank matrices/tensors used. While this is larger than Kneser Ney\u2019s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant. See Section 7 for a more detailed discussion of related work.\nOutline: We first review existing n-gram smoothing methods (\u00a72) and then present the intuition behind the key components of our technique: rank (\u00a73.1) and power (\u00a73.2). We then show how these can be interpolated into an ensemble (\u00a74). In the experimental evaluation on English and Russian corpora (\u00a75), we find that PLRE outperforms Kneser-Ney smoothing and all its variants, as well as class-based language models. We also include a comparison to the log-bilinear neural language model (Mnih and Hinton, 2007) and evaluate performance on a downstream machine translation task (\u00a76) where our method achieves consistent improvements in BLEU."}, {"heading": "2 Discount-based Smoothing", "text": "We first provide background on absolute discounting (Ney et al., 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods. Both methods can be formulated as back-off or interpolated models; we describe the latter here since that is the basis of our\nlow rank approach."}, {"heading": "2.1 Notation", "text": "Let c(w) be the count of word w, and similarly c(w,wi\u22121) for the joint count of words w and wi\u22121. For shorthand we will define w j i to denote the word sequence {wi, wi+1, ..., wj\u22121, wj}. Let P\u0302 (wi) refer to the maximum likelihood estimate (MLE) of the probability of word wi, and similarly P\u0302 (wi|wi\u22121) for the probability conditioned on a history, or more generally, P\u0302 (wi|wi\u22121i\u2212n+1).\nLet N\u2212(wi) := |{w : c(wi, w) > 0}| be the number of distinct words that appear before wi. More generally, let N\u2212(wii\u2212n+1) = |{w : c(wii\u2212n+1, w) > 0}|. Similarly, let N+(w i\u22121 i\u2212n+1) = |{w : c(w,w i\u22121 i\u2212n+1) > 0}|. V denotes the vocabulary size."}, {"heading": "2.2 Absolute Discounting", "text": "Absolute discounting works on the idea of interpolating higher order n-gram models with lowerorder n-gram models. However, first some probability mass must be \u201csubtracted\u201d from the higher order n-grams so that the leftover probability can be allocated to the lower order n-grams. More specifically, define the following discounted conditional probability:\nP\u0302D(wi|wi\u22121i\u2212n+1) = max{c(wi, wi\u22121i\u2212n+1)\u2212D, 0}\nc(wi\u22121i\u2212n+1)\nThen absolute discounting Pabs(\u00b7) uses the following (recursive) equation:\nPabs(wi|wi\u22121i\u2212n+1) = P\u0302D(wi|w i\u22121 i\u2212n+1)\n+ \u03b3(wi\u22121i\u2212n+1)Pabs(wi|w i\u22121 i\u2212n+2)\nwhere \u03b3(wi\u22121i\u2212n+1) is the leftover weight (due to the discounting) that is chosen so that the conditional distribution sums to one: \u03b3(wi\u22121i\u2212n+1) =\nD c(wi\u22121i\u2212n+1) N+(w i\u22121 i\u2212n+1). For the base case, we set Pabs(wi) = P\u0302 (wi). Discontinuity: Note that if c(wi\u22121i\u2212n+1) = 0, then \u03b3(wi\u22121i\u2212n+1) = 0 0 , in which case \u03b3(w i\u22121 i\u2212n+1) is set to 1. We will see that this discontinuity appears in PLRE as well."}, {"heading": "2.3 Kneser Ney Smoothing", "text": "Ideally, the smoothed probability should preserve the observed unigram distribution:\nP\u0302 (wi) = \u2211\nwi\u22121i\u2212n+1\nPsm(wi|wi\u22121i\u2212n+1)P\u0302 (w i\u22121 i\u2212n+1) (1)\nwhere Psm(wi|wi\u22121i\u2212n+1) is the smoothed conditional probability that a model outputs. Unfortunately, absolute discounting does not satisfy this property, since it exclusively uses the unaltered MLE unigram model as its lower order model. In practice, the lower order distribution is only utilized when we are unsure about the higher order distribution (i.e., when \u03b3(\u00b7) is large). Therefore, the unigram model should be altered to condition on this fact.\nThis is the inspiration behind Kneser-Ney (KN) smoothing, an elegant algorithm with robust performance in n-gram language modeling. KN smoothing defines alternate probabilities P alt(\u00b7):\nP altD (wi|wi\u22121i\u2212n\u2032+1) =  P\u0302D(wi|wi\u22121i\u2212n\u2032+1), if n \u2032 = n\nmax{N\u2212(wii\u2212n\u2032+1)\u2212D,0}\u2211 wi N\u2212(wii\u2212n\u2032+1) , if n\u2032 < n\nThe base case for unigrams reduces to P alt(wi) = N\u2212(wi)\u2211 wi N\u2212(wi) . Intuitively P alt(wi) is proportional to the number of unique words that precede wi. Thus, words that appear in many different contexts will be given higher weight than words that consistently appear after only a few contexts. These alternate distributions are then used with absolute discounting:\nPkn(wi|wi\u22121i\u2212n+1) = P alt D (wi|wi\u22121i\u2212n+1)\n+ \u03b3(wi\u22121i\u2212n+1)Pkn(wi|w i\u22121 i\u2212n+2) (2)\nwhere we set Pkn(wi) = P alt(wi). By definition, KN smoothing satisfies the marginal constraint in Eq. 1 (Kneser and Ney, 1995)."}, {"heading": "3 Power Low Rank Ensembles", "text": "In n-gram smoothing methods, if a bigram count c(wi, wi\u22121) is zero, the unigram probabilities are used, which is equivalent to assuming that wi and wi\u22121 are independent ( and similarly for general n). However, in this situation, instead of backing off to a 1-gram, we may like to back off to a \u201c1.5-gram\u201d or more generally an order between 1 and 2 that captures a coarser level of dependence\nbetween wi and wi\u22121 and does not assume full independence.\nInspired by this intuition, our strategy is to construct an ensemble of matrices and tensors that not only consists of MLE-based count information, but also contains quantities that represent levels of dependence in-between the various orders in the model. We call these combinations power low rank ensembles (PLRE), and they can be thought of as n-gram models with non-integer n. Our approach can be recursively formulated as:\nPplre(wi|wi\u22121i\u2212n+1) = P alt D0(wi|w i\u22121 i\u2212n+1)\n+ \u03b30(w i\u22121 i\u2212n+1) ( ZD1(wi|wi\u22121i\u2212n+1) + .....\n+ \u03b3\u03b7\u22121(w i\u22121 i\u2212n+1) ( ZD\u03b7(wi|wi\u22121i\u2212n+1)\n+ \u03b3\u03b7(w i\u22121 i\u2212n+1) ( Pplre(wi|wi\u22121i\u2212n+2) )) ... ) (3)\nwhere Z1, ...,Z\u03b7 are conditional probability matrices that represent the intermediate n-gram orders1 and D is a discount function (specified in \u00a74).\nThis formulation begs answers to a few critical questions. How to construct matrices that represent conditional probabilities for intermediate n? How to transform them in a way that generalizes the altered lower order distributions in KN smoothing? How to combine these matrices such that the marginal constraint in Eq. 1 still holds? The following propose solutions to these three queries:\n1. Rank (Section 3.1): Rank gives us a concrete measurement of the dependence between wi and wi\u22121. By constructing low rank approximations of the bigram count matrix and higher-order count tensors, we obtain matrices that represent coarser dependencies, with a rank one approximation implying that the variables are independent.\n2. Power (Section 3.2): In KN smoothing, the lower order distributions are not the original counts but rather altered estimates. We propose a continuous generalization of this alteration by taking the element-wise power of the counts.\n1with a slight abuse of notation, let ZDj be shorthand for Zj,Dj\n3. Creating the Ensemble (Section 4): Lastly, PLRE also defines a way to interpolate the specifically constructed intermediate n-gram matrices. Unfortunately a constant discount, as presented in Section 2, will not in general preserve the lower order marginal constraint (Eq. 1). We propose a generalized discounting scheme to ensure the constraint holds."}, {"heading": "3.1 Rank", "text": "We first show how rank can be utilized to construct quantities between an n-gram and an n\u2212 1-gram. In general, we think of an n-gram as an nth order tensor i.e. a multi-way array with n indices {i1, ..., in}. (A vector is a tensor of order 1, a matrix is a tensor of order 2 etc.) Computing a special rank one approximation of slices of this tensor produces the n\u2212 1-gram. Thus, taking rank \u03ba approximations in this fashion allows us to represent dependencies between an n-gram and n\u22121-gram.\nConsider the bigram count matrix B with N counts which has rank V . Note that P\u0302 (wi|wi\u22121) = B(wi,wi\u22121)\u2211\nwB(w,wi\u22121) . Additionally, B\ncan be considered a random variable that is the result of sampling N tuples of (wi, wi\u22121) and agglomerating them into a count matrix. Assuming wi and wi\u22121 are independent, the expected value (with respect to the empirical distribution) E[B] = NP (wi)P (wi\u22121), which can be rewritten as being proportional to the outer product of the unigram probability vector with itself, and is thus rank one.\nThis observation extends to higher order n-grams as well. Let Cn be the nth order tensor where Cn(wi, ...., wi\u2212n+1) = c(wi, ..., wi\u2212n+1). Furthermore denote Cn(:, w\u0303i\u22121i\u2212n+2, :) to be the V \u00d7 V matrix slice of Cn where wi\u2212n+2, ..., wi\u22121 are held fixed to a particular sequence w\u0303i\u2212n+2, ..., w\u0303i\u22121. Then if wi is conditionally independent of wi\u2212n+1 given wi\u22121i\u2212n+2, then E[Cn(:, w\u0303i\u22121i\u2212n+2, :)] is rank one \u2200w\u0303 i\u22121 i\u2212n+2.\nHowever, it is rare that these matrices are actually rank one, either due to sampling variance or the fact that wi and wi\u22121 are not independent. What we would really like to say is that the best rank one approximation B(1) (under some norm) of B is \u221d P\u0302 (wi)P\u0302 (wi\u22121). While this statement is not true under the `2 norm, it is true under generalized KL divergence (Lee and Seung, 2001): gKL(A||B) =\u2211\nij\n( Aij log(\nAij Bij\n)\u2212Aij +Bij) ) .\nIn particular, generalized KL divergence preserves row and column sums: if M (\u03ba) is the best rank \u03ba approximation of M under gKL then the row sums and column sums of M (\u03ba) and M are equal (Ho and Van Dooren, 2008). Leveraging this property, it is straightforward to prove the following lemma:\nLemma 1. Let B(\u03ba) be the best rank \u03ba approximation of B under gKL. Then B(1) \u221d P\u0302 (wi)P\u0302 (wi\u22121) and \u2200wi\u22121 s.t. c(wi\u22121) 6= 0:\nP\u0302 (wi) = B(1)(wi, wi\u22121)\u2211 wB (1)(w,wi\u22121)\nFor more general n, let Cn,(\u03ba)i\u22121,...,i\u2212n+2 be the best rank \u03ba approximation of Cn(:, w\u0303i\u22121i\u2212n+2, : ) under gKL. Then similarly, \u2200wi\u22121i\u2212n+1 s.t. c(wi\u22121i\u2212n+1) > 0:\nP\u0302 (wi|wi\u22121, ..., wi\u2212n+2)\n= C n,(1) i\u22121,...,i\u2212n+2(wi, w i\u22121 i\u2212n+1)\u2211\nwC n,(1) i\u22121,...,i\u2212n+2(w,w i\u22121 i\u2212n+1)\n(4)\nThus, by selecting 1 < \u03ba < V , we obtain count matrices and tensors between n and n \u2212 1-grams. The condition that c(wi\u22121i\u2212n+1) > 0 corresponds to the discontinuity discussed in \u00a72.2."}, {"heading": "3.2 Power", "text": "Since KN smoothing alters the lower order distributions instead of simply using the MLE, varying the rank is not sufficient in order to generalize this suite of techniques. Thus, PLRE computes low rank approximations of altered count matrices. Consider taking the elementwise power \u03c1 of the bigram count matrix, which is denoted by B\u00b7\u03c1. For example, the observed bigram count matrix and associated row sum:\nB\u00b71 = ( 1.0 2.0 1.0 0 5.0 0 2.0 0 0 ) row sum\u2192 ( 4.0 5.0 2.0 ) As expected the row sum is equal to the unigram counts (which we denote as u). Now consider B\u00b70.5:\nB\u00b70.5 = ( 1.0 1.4 1.0 0 2.2 0 1.4 0 0 ) row sum\u2192 ( 3.4 2.2 1.4 ) Note how the row sum vector has been altered. In particular since w1 (corresponding to the first\nrow) has a more diverse history than w2, it has a higher row sum (compared to in u where w2 has the higher row sum). Lastly, consider the case when p = 0:\nB\u00b70 = ( 1.0 1.0 1.0 0 1.0 0 1.0 0 0 ) row sum\u2192 ( 3.0 1.0 1.0 )\nThe row sum is now the number of unique words that precede wi (since B0 is binary) and is thus equal to the (unnormalized) Kneser Ney unigram. This idea also generalizes to higher order n-grams and leads us to the following lemma:\nLemma 2. Let B(\u03c1,\u03ba) be the best rank \u03ba approximation of B\u00b7\u03c1 under gKL. Then \u2200wi\u22121 s.t. c(wi\u22121) 6= 0:\nP alt(wi) = B(0,1)(wi, wi\u22121)\u2211 wB (0,1)(w,wi\u22121)\nFor more general n, let Cn,(\u03c1,\u03ba)i\u22121,...,i\u2212n+2 be the best rank \u03ba approximation of Cn,(\u03c1)(:, w\u0303i\u22121i\u2212n+2, :) under gKL. Similarly, \u2200wi\u22121i\u2212n+1 s.t. c(w i\u22121 i\u2212n+1) > 0:\nP alt(wi|wi\u22121, ..., wi\u2212n+2)\n= C n,(0,1) i\u22121,...,i\u2212n+2(wi, w i\u22121 i\u2212n+1)\u2211\nwC n,(0,1) i\u22121,...,i\u2212n+2(w,w i\u22121 i\u2212n+1)\n(5)"}, {"heading": "4 Creating the Ensemble", "text": "Recall our overall formulation in Eq. 3; a naive solution would be to set Z1, ...,Z\u03b7 to low rank approximations of the count matrices/tensors under varying powers, and then interpolate through constant absolute discounting. Unfortunately, the marginal constraint in Eq. 1 will generally not hold if this strategy is used. Therefore, we propose a generalized discounting scheme where each nonzero n-gram count is associated with a different discount Dj(wi, wi\u22121i\u2212n\u2032+1). The low rank approximations are then computed on the discounted matrices, leaving the marginal constraint intact.\nFor clarity of exposition, we focus on the special case where n = 2 with only one low rank matrix before stating our general algorithm:\nPplre(wi|wi\u22121) = P\u0302D0(wi|wi\u22121)\n+ \u03b30(wi\u22121) ( ZD1(wi|wi\u22121) + \u03b31(wi\u22121)P alt(wi) ) (6)\nOur goal is to compute D0,D1 and Z1 so that the following lower order marginal constraint holds:\nP\u0302 (wi) = \u2211 wi\u22121 Pplre(wi|wi\u22121)P\u0302 (wi\u22121) (7)\nOur solution can be thought of as a twostep procedure where we compute the discounts D0,D1 (and the \u03b3(wi\u22121) weights as a byproduct), followed by the low rank quantity Z1. First, we construct the following intermediate ensemble of powered, but full rank terms. Let Y \u03c1j be the matrix such that Y \u03c1j (wi, wi\u22121) := c(wi, wi\u22121) \u03c1j . Then define\nPpwr(wi|wi\u22121) := Y (\u03c10=1)D0 (wi|wi\u22121)\n+ \u03b30(wi\u22121)\n( Y\n(\u03c11) D1 (wi|wi\u22121)\n+ \u03b31(wi\u22121)Y (\u03c12=0)(wi|wi\u22121) ) (8)\nwhere with a little abuse of notation:\nY \u03c1j Dj (wi|wi\u22121) = c(wi, wi\u22121) \u03c1j \u2212Dj(wi, wi\u22121)\u2211 wi c(wi, wi\u22121)\u03c1j\nNote that P alt(wi) has been replaced with Y (\u03c12=0)(wi|wi\u22121), based on Lemma 2, and will equal P alt(wi) once the low rank approximation is taken as discussed in \u00a7 4.2).\nSince we have only combined terms of different power (but all full rank), it is natural choose the discounts so that the result remains unchanged i.e., Ppwr(wi|wi\u22121) = P\u0302 (wi|wi\u22121), since the low rank approximation (not the power) will implement smoothing. Enforcing this constraint gives rise to a set of linear equations that can be solved (in closed form) to obtain the discounts as we now show below."}, {"heading": "4.1 Step 1: Computing the Discounts", "text": "To ensure the constraint that Ppwr(wi|wi\u22121) = P\u0302 (wi|wi\u22121), it is sufficient to enforce the following two local constraints:\nY (\u03c1j)(wi|wi\u22121) = Y (\u03c1j) Dj (wi|wi\u22121)\n+ \u03b3j(wi\u22121)Y (\u03c1j+1)(wi|wi\u22121) for j = 0, 1\n(9)\nThis allows each Dj to be solved for independently of the other {Dj\u2032}j\u2032 6=j . Let ci,i\u22121 = c(wi, wi\u22121), c j i,i\u22121 = c(wi, wi\u22121) \u03c1j , and dji,i\u22121 =\nDj(wi, wi\u22121). Expanding Eq. 9 yields that \u2200wi, wi\u22121:\ncji,i\u22121\u2211 i c j i,i\u22121 = cji,i\u22121 \u2212 d j i,i\u22121\u2211\ni c j i,i\u22121\n+ (\u2211 i d j i,i\u22121\u2211\ni c j i,i\u22121 ) cj+1i,i\u22121\u2211 i c j+1 i,i\u22121 (10)\nwhich can be rewritten as:\n\u2212dji,i\u22121 + (\u2211 i dji,i\u22121 ) cj+1i,i\u22121\u2211 i c j+1 i,i\u22121 = 0 (11)\nNote that Eq. 11 decouples across wi\u22121 since the only dji,i\u22121 terms that are dependent are the ones that share the preceding context wi\u22121.\nIt is straightforward to see that setting dji,i\u22121 proportional to cj+1i,i\u22121 satisfies Eq. 11. Furthermore it can be shown that all solutions are of this form (i.e., the linear system has a null space of exactly one). Moreover, we are interested in a particular subset of solutions where a single parameter d\u2217 (independent of wi\u22121) controls the scaling as indicated by the following lemma:\nLemma 3. Assume that \u03c1j \u2265 \u03c1j+1. Choose any 0 \u2264 d\u2217 \u2264 1. Set dji,i\u22121 = d\u2217c j+1 i,i\u22121 \u2200i, j. The resulting discounts satisfy Eq. 11 as well as the inequality constraints 0 \u2264 dji,i\u22121 \u2264 c j i,i\u22121. Furthermore, the leftover weight \u03b3j takes the form:\n\u03b3j(wi\u22121) = \u2211 i d j i,i\u22121\u2211\ni c j i,i\u22121\n= d\u2217 \u2211 i c j+1 i,i\u22121\u2211\ni c j i,i\u22121\nProof. Clearly this choice of dji,i\u22121 satisfies Eq. 11. The largest possible value of dji,i\u22121 is cj+1i,i\u22121. \u03c1j \u2265 \u03c1j+1, implies c j i,i\u22121 \u2265 c j+1 i,i\u22121. Thus the inequality constraints are met. It is then easy to verify that \u03b3 takes the above form.\nThe above lemma generalizes to longer contexts (i.e. n > 2) as shown in Algorithm 1. Note that if \u03c1j = \u03c1j+1 then Algorithm 1 is equivalent to scaling the counts e.g. deleted-interpolation/Jelinek Mercer smoothing (Jelinek and Mercer, 1980). On the other hand, when \u03c1j+1 = 0, Algorithm 1 is equal to the absolute discounting that is used in Kneser-Ney. Thus, depending on \u03c1j+1, our method generalizes different types of interpolation schemes to construct an ensemble so that the marginal constraint is satisfied.\nAlgorithm 1 Compute D In: Count tensor Cn, powers \u03c1j , \u03c1j+1 such that \u03c1j \u2265 \u03c1j+1, and parameter d\u2217. Out: Discount Dj for powered counts Cn,(\u03c1j) and associated leftover weight \u03b3j\n1: Set Dj(wi, wi\u22121i\u2212n+1) = d\u2217c(wi, w i\u22121 i\u2212n+1) \u03c1j+1 . 2:\n\u03b3j(wi, w i\u22121 i\u2212n+1) =\nd\u2217 \u2211\nwi c(wi, w i\u22121 i\u2212n+1) \u03c1j+1\u2211 wi c(wi, w i\u22121 i\u2212n+1) \u03c1j\nAlgorithm 2 Compute Z In: Count tensor Cn, power \u03c1, discounts D, rank \u03ba Out: Discounted low rank conditional probability table Z(\u03c1,\u03ba)D (wi|w i\u22121 i\u2212n+1) (represented implicitly)\n1: Compute powered counts Cn,(\u00b7\u03c1). 2: Compute denominators \u2211 wi c(wi, w i\u22121 i\u2212n+1) \u03c1\n\u2200wi\u22121i\u2212n+1 s.t. c(w i\u22121 i\u2212n+1) > 0.\n3: Compute discounted powered counts C n,(\u00b7\u03c1) D = C\nn,(\u00b7\u03c1) \u2212D. 4: For each slice Mw\u0303i\u22121i\u2212n+2 := C n,(\u00b7\u03c1) D (:\n, w\u0303i\u22121i\u2212n+2, :) compute M (\u03ba) := min A\u22650:rank(A)=\u03ba \u2016Mw\u0303i\u22121i\u2212n+2 \u2212A\u2016KL\n(stored implicitly as M (\u03ba) = LR)\nSet Z(\u03c1,\u03ba)D (:, w\u0303 i\u22121 i\u2212n+2, :) = M (\u03ba)\n5: Note that\nZ (\u03c1,\u03ba) D (wi|w i\u22121 i\u2212n+1) =\nZ (\u03c1,\u03ba) D (wi, w i\u22121 i\u2212n+1)\u2211\nwi c(wi, w i\u22121 i\u2212n+1) \u03c1"}, {"heading": "4.2 Step 2: Computing Low Rank Quantities", "text": "The next step is to compute low rank approximations of Y (\u03c1j)Dj to obtain ZDj such that the intermediate marginal constraint in Eq. 7 is preserved. This constraint trivially holds for the intermediate ensemble Ppwr(wi|wi\u22121) due to how the discounts were derived in \u00a7 4.1. For our running bigram example, define Z(\u03c1j ,\u03baj)Dj to be the best rank \u03baj approximation to Y (\u03c1j ,\u03baj)Dj according to gKL and let\nZ \u03c1j ,\u03baj Dj\n(wi|wi\u22121) = Z \u03c1j ,\u03baj Dj (wi, wi\u22121)\u2211 wi c(wi, wi\u22121)\u03c1j\nNote that Z\u03c1j ,\u03bajDj (wi|wi\u22121) is a valid (discounted) conditional probability since gKL preserves row/column sums so the denominator remains unchanged under the low rank approximation. Then\nusing the fact that Z(0,1)(wi|wi\u22121) = P alt(wi) (Lemma 2) we can embellish Eq. 6 as\nPplre(wi|wi\u22121) = PD0(wi|wi\u22121)+\n\u03b30(wi\u22121)\n( Z\n(\u03c11,\u03ba1) D1\n(wi|wi\u22121) + \u03b31(wi\u22121)Palt(wi) )\nLeveraging the form of the discounts and row/column sum preserving property of gKL, we then have the following lemma (the proof is in the supplementary material):\nLemma 4. Let Pplre(wi|wi\u22121) indicate the PLRE smoothed conditional probability as computed by Eq. 6 and Algorithms 1 and 2. Then, the marginal constraint in Eq. 7 holds."}, {"heading": "4.3 More general algorithm", "text": "In general, the principles outlined in the previous sections hold for higher order n-grams. Assume that the discounts are computed according to Algorithm 1 with parameter d\u2217 and Z\n(\u03c1j ,\u03baj) Dj\nis computed according to Algorithm 2. Note that, as shown in Algorithm 2, for higher order n-grams, the Z(\u03c1j ,\u03baj)Dj are created by taking low rank approximations of slices of the (powered) count tensors (see Lemma 2 for intuition). Eq. 3 can now be embellished:\nPplre(wi|wi\u22121i\u2212n+1) = P alt D0(wi|w i\u22121 i\u2212n+1)\n+ \u03b30(w i\u22121 i\u2212n+1)\n( Z\n(\u03c11,\u03ba1) D1 (wi|wi\u22121i\u2212n+1) + .....\n+ \u03b3\u03b7\u22121(w i\u22121 i\u2212n+1)\n( Z\n(\u03c1\u03b7 ,\u03ba\u03b7) D\u03b7 (wi|wi\u22121i\u2212n+1)\n+ \u03b3\u03b7(w i\u22121 i\u2212n+1) ( Pplre(wi|wi\u22121i\u2212n+2) )) ... ) (12)\nLemma 4 also applies in this case and is given in Theorem 1 in the supplementary material."}, {"heading": "4.4 Links with KN Smoothing", "text": "In this section, we explicitly show the relationship between PLRE and KN smoothing. Rewriting Eq. 12 in the following form:\nPplre(wi|wi\u22121i\u2212n+1) = P terms plre (wi|wi\u22121i\u2212n+1)\n+\u03b30:\u03b7(w i\u22121 i\u2212n+1)Pplre(wi|w i\u22121 i\u2212n+2) (13)\nwhere P termsplre (wi|w i\u22121 i\u2212n+1) contains the terms in Eq. 12 except the last, and \u03b30:\u03b7(wi\u22121i\u2212n+1) =\u220f\u03b7 h=0 \u03b3h(w i\u22121 i\u2212n+1), we can leverage the form of\nthe discount, and using the fact that \u03c1\u03b7+1 = 02:\n\u03b30:\u03b7(w i\u22121 i\u2212n\u22121) =\nd\u2217 \u03b7+1N+(w i\u22121 i\u2212n+1)\nc(wi\u22121i\u2212n+1)\nWith this form of \u03b3(\u00b7), Eq. 13 is remarkably similar to KN smoothing (Eq. 2) if KN\u2019s discount parameter D is chosen to equal (d\u2217)\n\u03b7+1. The difference is that P alt(\u00b7) has been replaced with the alternate estimate P termsplre (wi|w i\u22121 i\u2212n+1), which have been enriched via the low rank structure. Since these alternate estimates were constructed via our ensemble strategy they contain both very fine-grained dependencies (the original n-grams) as well as coarser dependencies (the lower rank n-grams) and is thus fundamentally different than simply taking a single matrix/tensor decomposition of the trigram/bigram matrices.\nMoreover, it provides a natural way of setting d\u2217 based on the Good-Turing (GT) estimates employed by KN smoothing. In particular, we can set d\u2217 to be the (\u03b7 + 1)\nth root of the KN discount D that can be estimated via the GT estimates."}, {"heading": "4.5 Computational Considerations", "text": "PLRE scales well even as the order n increases. To compute a low rank bigram, one low rank approximation of a V \u00d7 V matrix is required. For the low rank trigram, we need to compute a low rank approximation of each slice Cn,(\u00b7p)D (:, w\u0303i\u22121, : ) \u2200w\u0303i\u22121. While this may seem daunting at first, in practice the size of each slice (number of non-zero rows/columns) is usually much, much smaller than V , keeping the computation tractable.\nSimilarly, PLRE also evaluates conditional probabilities at evaluation time efficiently. As shown in Algorithm 2, the normalizer can be precomputed on the sparse powered matrix/tensor. As a result our test complexity is O( \u2211\u03b7total i=1 \u03bai) where \u03b7total is the total number of matrices/tensors in the ensemble. While this is larger than Kneser Ney\u2019s practically constant complexity of O(n), it is much faster than other recent methods for language modeling such as neural networks and conditional exponential family models where exact computation of the normalizing constant costs O(V )."}, {"heading": "5 Experiments", "text": "To evaluate PLRE, we compared its performance on English and Russian corpora with several vari-\n2for derivation see proof of Lemma 4 in the supplementary material\nants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007). We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task. We have made the code for our approach publicly available 3.\nTo build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing. We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities. SRILM was also used for the baseline KN-smoothed models.\nFor our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010). The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011)."}, {"heading": "5.1 Datasets", "text": "For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were replaced with \u201c<unk>\u201d tokens in the training corpus, and any word not in the vocabulary was replaced with this token during evaluation. There is a general dearth of evaluation on large-scale corpora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3.\n\u2022 Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test - 963,000. Vocabulary- 18,000 types. \u2022 Small-Russian: Subset of Russian news com-\nmentary data from 2013 WMT translation task5: Train- 3.5 million words, Dev - 400,000 Test - 400,000. Vocabulary - 77,000 types. \u2022 Large-English: English Gigaword, Training -\n837 million words, Dev - 8.7 million, Test - 8.7 million. Vocabulary- 836,980 types. \u2022 Large-Russian: Monolingual data from WMT\n2013 task. Training - 521 million words, Validation - 50,000, Test - 50,000. Vocabulary- 1.3 million types.\n3http://www.cs.cmu.edu/\u223capparikh/plre.html 4http://code.google.com/p/giza-pp/ 5http://www.statmt.org/wmt13/training-monolingual-\nnc-v8.tgz\nFor the MT evaluation, we used the parallel data from the WMT 2013 shared task, excluding the Common Crawl corpus data. The newstest2012 and newstest2013 evaluation sets were used as the development and test sets respectively."}, {"heading": "5.2 Small Corpora", "text": "For the class-based baseline LMs, the number of classes was selected from {32, 64, 128, 256, 512, 1024} (Small-English) and {512, 1024} (Small-Russian). We could not go higher due to the computationally laborious process of hard clustering. For Kneser-Ney, we explore four different variants: back-off (BO-KN) interpolated (int-KN), modified back-off (BOMKN), and modified interpolated (int-MKN). Good-Turing estimates were used for discounts. All models trained on the small corpora are of order 3 (trigrams).\nFor PLRE, we used one low rank bigram and one low rank trigram in addition to the MLE ngram estimates. The powers of the intermediate matrices/tensors were fixed to be 0.5 and the discounts were set to be square roots of the Good Turing estimates (as explained in \u00a7 4.4). The ranks were tuned on the development set. For SmallEnglish, the ranges were {1e \u2212 3, 5e \u2212 3} (as a fraction of the vocabulary size) for both the low rank bigram and low rank trigram models. For Small-Russian the ranges were {5e \u2212 4, 1e \u2212 3} for both the low rank bigram and the low rank trigram models.\nThe results are shown in Table 1. The best classbased LM is reported, but is not competitive with the KN baselines. PLRE outperforms all of the baselines comfortably. Moreover, PLRE\u2019s performance over the baselines is highlighted in Russian. With larger vocabulary sizes, the low rank approach is more effective as it can capture linguistic similarities between rare and common words.\nNext we discuss how the maximum n-gram order affects performance. Figure 1 shows the relative percentage improvement of our approach over int-MKN as the order is increased from 2 to 4 for both methods. The Small-English dataset has a rather small vocabulary compared to the number of tokens, leading to lower data sparsity in the bigram. Thus the PLRE improvement is small for order = 2, but more substantial for order = 3. On the other hand, for the Small-Russian dataset, the vocabulary size is much larger and consequently the bigram counts are sparser. This leads to sim-\nilar improvements for all orders (which are larger than that for Small-English).\nOn both these datasets, we also experimented with tuning the discounts for int-MKN to see if the baseline could be improved with more careful choices of discounts. However, this achieved only marginal gains (reducing the perplexity to 98.94 on the Small-English test set and 259.0 on the Small-Russian test set).\nComparison to LBL (Mnih and Hinton, 2007): Mnih and Hinton (2007) evaluate on the Small-English dataset (but remove end markers and concatenate the sentences). They obtain perplexities 117.0 and 107.8 using contexts of size 5 and 10 respectively. With this preprocessing, a 4- gram (context 3) PLRE achieves 108.4 perplexity."}, {"heading": "5.3 Large Corpora", "text": "Results on the larger corpora for the top 2 performing methods \u201cPLRE\u201d and \u201cint-MKN\u201d are presented in Table 2. Due to the larger training size, we use 4-gram models in these experiments. However, including the low rank 4-gram tensor provided little gain and therefore, the 4-gram PLRE only has additional low rank bigram and low rank trigram matrices/tensors. As above, ranks were tuned on the development set. For Large-English, the ranges were {1e\u22124, 5e\u22124, 1e\u22123} (as a fraction of the vocabulary size) for both the low rank\nbigram and low rank trigram models. For SmallRussian the ranges were {1e\u22125, 5e\u22125, 1e\u22124} for both the low rank bigram and the low rank trigram models. For statistical validity, 10 test sets of size equal to the original test set were generated by randomly sampling sentences with replacement from the original test set. Our method outperforms \u201cintMKN\u201d with gains similar to that on the smaller datasets. As shown in Table 3, our method obtains fast training times even for large datasets."}, {"heading": "6 Machine Translation Task", "text": "Table 4 presents results for the MT task, translating from English to Russian7. We used MIRA (Chiang et al., 2008) to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs - the set of feature weights obtained using int-MKN is the same, only the language model changes. The\n6As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting).\n7the best score at WMT 2013 was 19.9 (Bojar et al., 2013)\nprocedure is repeated 10 times to control for optimizer instability (Clark et al., 2011). Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN. On average, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score."}, {"heading": "7 Related Work", "text": "Recent attempts to revisit the language modeling problem have largely come from two directions: Bayesian nonparametrics and neural networks. Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process. These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009).\nThe idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance. These methods can be quite expensive to train and query (especially as the vocabulary size increases). Techniques such as noise contrastive estimation (Gutmann and Hyva\u0308rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost of exact normalization to O( \u221a V ). In contrast, our approach is much more scalable, since it is trivially parallelized in training and does not require explicit normalization during evaluation.\nThere are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted set-\ntings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts."}, {"heading": "8 Conclusion", "text": "We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as well as low rank matrices and tensors, our method captures both the fine-grained and coarse structures in word sequences. Our discounting strategy preserves the marginal constraint and thus generalizes Kneser Ney, and under slight changes can also extend other smoothing methods such as deleted-interpolation/JelinekMercer smoothing. Experimentally, PLRE convincingly outperforms Kneser-Ney smoothing as well as class-based baselines."}, {"heading": "Acknowledgements", "text": "This work was supported by NSF IIS1218282, NSF IIS1218749, NSF IIS1111142, NIH R01GM093156, the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533, the NSF Graduate Research Fellowship Program under Grant No. 0946825 (NSF Fellowship to APP), and a grant from Ebay Inc. (to AS)."}], "references": [{"title": "Large vocabulary speech recognition with multispan statistical language models", "author": ["Jerome R. Bellegarda."], "venue": "IEEE Transactions on Speech and Audio Processing, 8(1):76\u201384.", "citeRegEx": "Bellegarda.,? 2000", "shortCiteRegEx": "Bellegarda.", "year": 2000}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res., 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Findings of the 2013 Workshop on Statistical Machine Translation", "author": ["Ond\u0159ej Bojar", "Christian Buck", "Chris Callison-Burch", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia."], "venue": "Pro-", "citeRegEx": "Bojar et al\\.,? 2013", "shortCiteRegEx": "Bojar et al\\.", "year": 2013}, {"title": "A singular value thresholding algorithm", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht."], "venue": "Foundations of Computational mathematics, 9(6):717\u2013 772.", "citeRegEx": "Cand\u00e8s and Recht.,? 2009", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman."], "venue": "Computer Speech & Language, 13(4):359\u2013393.", "citeRegEx": "Chen and Goodman.,? 1999", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "A survey of smoothing techniques for me models", "author": ["Stanley F Chen", "Ronald Rosenfeld."], "venue": "Speech and Audio Processing, IEEE Transactions on, 8(1):37\u2013", "citeRegEx": "Chen and Rosenfeld.,? 2000", "shortCiteRegEx": "Chen and Rosenfeld.", "year": 2000}, {"title": "Shrinking exponential language models", "author": ["Stanley F. Chen."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL \u201909, pages", "citeRegEx": "Chen.,? 2009", "shortCiteRegEx": "Chen.", "year": 2009}, {"title": "Online large-margin training of syntactic and structural translation features", "author": ["David Chiang", "Yuval Marton", "Philip Resnik."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 224\u2013233. Association for Com-", "citeRegEx": "Chiang et al\\.,? 2008", "shortCiteRegEx": "Chiang et al\\.", "year": 2008}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Comput. Linguist., 33(2):201\u2013228, June.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Jonathan H. Clark", "Chris Dyer", "Alon Lavie", "Noah A. Smith."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Chris Dyer", "Jonathan Weese", "Hendra Setiawan", "Adam Lopez", "Ferhan Ture", "Vladimir Eidelman", "Juri Ganitkevitch", "Phil Blunsom", "Philip Resnik"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Interpolating between types and tokens by estimating power-law generators", "author": ["Sharon Goldwater", "Thomas Griffiths", "Mark Johnson."], "venue": "Advances in Neural Information Processing Systems, volume 18.", "citeRegEx": "Goldwater et al\\.,? 2006", "shortCiteRegEx": "Goldwater et al\\.", "year": 2006}, {"title": "Classes for fast maximum entropy training", "author": ["Joshua Goodman."], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on, volume 1, pages 561\u2013564. IEEE.", "citeRegEx": "Goodman.,? 2001", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "Journal of Machine Learning Research, 13:307\u2013 361.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2012", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2012}, {"title": "KenLM: faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland, United Kingdom, July.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Nonnegative matrix factorization with fixed row and column sums", "author": ["Ngoc-Diep Ho", "Paul Van Dooren."], "venue": "Linear Algebra and its Applications, 429(5):1020\u20131025.", "citeRegEx": "Ho and Dooren.,? 2008", "shortCiteRegEx": "Ho and Dooren.", "year": 2008}, {"title": "Low rank language models for small training sets", "author": ["Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel."], "venue": "Signal Processing Letters, IEEE, 18(9):489\u2013 492.", "citeRegEx": "Hutchinson et al\\.,? 2011", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2011}, {"title": "Interpolated estimation of markov source parameters from sparse data", "author": ["Frederick Jelinek", "Robert Mercer."], "venue": "Pattern recognition in practice.", "citeRegEx": "Jelinek and Mercer.,? 1980", "shortCiteRegEx": "Jelinek and Mercer.", "year": 1980}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Reinhard Kneser", "Hermann Ney."], "venue": "Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181\u2013184. IEEE.", "citeRegEx": "Kneser and Ney.,? 1995", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky."], "venue": "Computer, 42(8):30\u201337.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung."], "venue": "Advances in Neural Information Processing Systems, 13:556\u2013562.", "citeRegEx": "Lee and Seung.,? 2001", "shortCiteRegEx": "Lee and Seung.", "year": 2001}, {"title": "Divide-and-conquer matrix factorization", "author": ["Lester Mackey", "Ameet Talwalkar", "Michael I Jordan."], "venue": "arXiv preprint arXiv:1107.0789.", "citeRegEx": "Mackey et al\\.,? 2011", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "Foundations of statistical natural language processing, volume 999", "author": ["Christopher D Manning", "Hinrich Sch\u00fctze."], "venue": "MIT Press.", "citeRegEx": "Manning and Sch\u00fctze.,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "Natural language processing with modular pdp networks and distributed lexicon", "author": ["Risto Miikkulainen", "Michael G. Dyer."], "venue": "Cognitive Science, 15:343\u2013 399.", "citeRegEx": "Miikkulainen and Dyer.,? 1991", "shortCiteRegEx": "Miikkulainen and Dyer.", "year": 1991}, {"title": "Recurrent neural network based language model", "author": ["Tom Mikolov", "Martin Karafit", "Luk Burget", "Jan ernock", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTER-", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "JH Cernocky", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Three new graphical models for statistical language modelling", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Proceedings of the 24th international conference on Machine learning, pages 641\u2013648. ACM.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Structured penalties for log-linear language models", "author": ["Anil Kumar Nelakanti", "Cedric Archambeau", "Julien Mairal", "Francis Bach", "Guillaume Bouchard."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Nelakanti et al\\.,? 2013", "shortCiteRegEx": "Nelakanti et al\\.", "year": 2013}, {"title": "On Structuring Probabilistic Dependencies in Stochastic Language Modelling", "author": ["Hermann Ney", "Ute Essen", "Reinhard Kneser."], "venue": "Computer Speech and Language, 8:1\u201338.", "citeRegEx": "Ney et al\\.,? 1994", "shortCiteRegEx": "Ney et al\\.", "year": 1994}, {"title": "Maximum-likelihoodsch\u00e4tzung von wortkategorien mit verfahren der kombinatorischen optimierung", "author": ["Franz Josef Och."], "venue": "Bachelor\u2019s thesis (Studienarbeit), University of Erlangen.", "citeRegEx": "Och.,? 1995", "shortCiteRegEx": "Och.", "year": 1995}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei jing Zhu."], "venue": "pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Fundamentals of speech recognition", "author": ["Lawrence Rabiner", "Biing-Hwang Juang"], "venue": null, "citeRegEx": "Rabiner and Juang.,? \\Q1993\\E", "shortCiteRegEx": "Rabiner and Juang.", "year": 1993}, {"title": "Smoothed marginal distribution constraints for language modeling", "author": ["Brian Roark", "Cyril Allauzen", "Michael Riley."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 43\u201352.", "citeRegEx": "Roark et al\\.,? 2013", "shortCiteRegEx": "Roark et al\\.", "year": 2013}, {"title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "author": ["Ruslan Salakhutdinov", "Andriy Mnih."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 880\u2013887. ACM.", "citeRegEx": "Salakhutdinov and Mnih.,? 2008", "shortCiteRegEx": "Salakhutdinov and Mnih.", "year": 2008}, {"title": "Aggregate and mixed-order markov models for statistical language processing", "author": ["Lawrence Saul", "Fernando Pereira."], "venue": "Proceedings of the second conference on empirical methods in natural language processing, pages 81\u201389. Somerset, New Jer-", "citeRegEx": "Saul and Pereira.,? 1997", "shortCiteRegEx": "Saul and Pereira.", "year": 1997}, {"title": "SRILM - An Extensible Language Modeling Toolkit", "author": ["Andreas Stolcke."], "venue": "Proceedings of the International Conference in Spoken Language Processing.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "A survey of collaborative filtering techniques", "author": ["Xiaoyuan Su", "Taghi M Khoshgoftaar."], "venue": "Advances in artificial intelligence, 2009:4.", "citeRegEx": "Su and Khoshgoftaar.,? 2009", "shortCiteRegEx": "Su and Khoshgoftaar.", "year": 2009}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["Yee Whye Teh."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Lin-", "citeRegEx": "Teh.,? 2006", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "Decoding with largescale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation", "author": ["F. Wood", "Y.W. Teh."], "venue": "Artificial Intelligence and Statistics, pages 607\u2013614.", "citeRegEx": "Wood and Teh.,? 2009", "shortCiteRegEx": "Wood and Teh.", "year": 2009}, {"title": "A stochastic memoizer for sequence data", "author": ["Frank Wood", "C\u00e9dric Archambeau", "Jan Gasthaus", "Lancelot James", "Yee Whye Teh."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 1129\u20131136. ACM.", "citeRegEx": "Wood et al\\.,? 2009", "shortCiteRegEx": "Wood et al\\.", "year": 2009}, {"title": "Efficient training methods for maximum entropy language modeling", "author": ["Jun Wu", "Sanjeev Khudanpur."], "venue": "Interspeech, pages 114\u2013118.", "citeRegEx": "Wu and Khudanpur.,? 2000", "shortCiteRegEx": "Wu and Khudanpur.", "year": 2000}, {"title": "Efficient subsampling for training complex language models", "author": ["Puyang Xu", "Asela Gunawardana", "Sanjeev Khudanpur."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 1128\u20131136,", "citeRegEx": "Xu et al\\.,? 2011", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Human behaviour and the principle of least-effort", "author": ["George Zipf."], "venue": "Addison-Wesley, Cambridge, MA.", "citeRegEx": "Zipf.,? 1949", "shortCiteRegEx": "Zipf.", "year": 1949}], "referenceMentions": [{"referenceID": 34, "context": "Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010).", "startOffset": 185, "endOffset": 210}, {"referenceID": 20, "context": "Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition (Rabiner and Juang, 1993) and machine translation (Koehn, 2010).", "startOffset": 235, "endOffset": 248}, {"referenceID": 46, "context": "Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed in the training data (Manning and Sch\u00fctze, 1999).", "startOffset": 40, "endOffset": 52}, {"referenceID": 24, "context": "Due to the power-law nature of language (Zipf, 1949), the maximum likelihood estimator massively overestimates the probability of rare events and assigns zero probability to legitimate word sequences that happen not to have been observed in the training data (Manning and Sch\u00fctze, 1999).", "startOffset": 259, "endOffset": 286}, {"referenceID": 5, "context": "These reassign probability mass (generally from overestimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models (Chen and Goodman, 1999).", "startOffset": 196, "endOffset": 220}, {"referenceID": 21, "context": "Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al.", "startOffset": 179, "endOffset": 226}, {"referenceID": 39, "context": "Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering (Koren et al., 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al.", "startOffset": 179, "endOffset": 226}, {"referenceID": 4, "context": ", 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al., 2010).", "startOffset": 56, "endOffset": 98}, {"referenceID": 3, "context": ", 2009; Su and Khoshgoftaar, 2009) or matrix completion (Cand\u00e8s and Recht, 2009; Cai et al., 2010).", "startOffset": 56, "endOffset": 98}, {"referenceID": 22, "context": "In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 36, "context": "In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 23, "context": "In these areas, low rank approaches based on matrix factorization play a central role (Lee and Seung, 2001; Salakhutdinov and Mnih, 2008; Mackey et al., 2011).", "startOffset": 86, "endOffset": 158}, {"referenceID": 1, "context": "This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 117, "endOffset": 183}, {"referenceID": 28, "context": "This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 117, "endOffset": 183}, {"referenceID": 26, "context": "This differentiates our approach over other methods that leverage an underlying latent space such as neural networks (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 117, "endOffset": 183}, {"referenceID": 37, "context": ", 2010) or soft-class models (Saul and Pereira, 1997) where the underlying dimension is required to be quite large to obtain good performance.", "startOffset": 29, "endOffset": 53}, {"referenceID": 6, "context": "While this is larger than Kneser Ney\u2019s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant.", "startOffset": 140, "endOffset": 202}, {"referenceID": 7, "context": "While this is larger than Kneser Ney\u2019s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant.", "startOffset": 140, "endOffset": 202}, {"referenceID": 30, "context": "While this is larger than Kneser Ney\u2019s virtually constant query time, it is substantially faster than conditional exponential family models (Chen and Rosenfeld, 2000; Chen, 2009; Nelakanti et al., 2013) and neural networks which require O(V ) for exact computation of the normalization constant.", "startOffset": 140, "endOffset": 202}, {"referenceID": 28, "context": "We also include a comparison to the log-bilinear neural language model (Mnih and Hinton, 2007) and evaluate performance on a downstream machine translation task (\u00a76) where our method achieves consistent improvements in BLEU.", "startOffset": 71, "endOffset": 94}, {"referenceID": 31, "context": "We first provide background on absolute discounting (Ney et al., 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods.", "startOffset": 52, "endOffset": 70}, {"referenceID": 19, "context": ", 1994) and Kneser-Ney smoothing (Kneser and Ney, 1995), two common n-gram smoothing methods.", "startOffset": 33, "endOffset": 55}, {"referenceID": 19, "context": "1 (Kneser and Ney, 1995).", "startOffset": 2, "endOffset": 24}, {"referenceID": 22, "context": "While this statement is not true under the `2 norm, it is true under generalized KL divergence (Lee and Seung, 2001): gKL(A||B) = \u2211", "startOffset": 95, "endOffset": 116}, {"referenceID": 18, "context": "deleted-interpolation/Jelinek Mercer smoothing (Jelinek and Mercer, 1980).", "startOffset": 47, "endOffset": 73}, {"referenceID": 28, "context": "ants of KN smoothing, class-based models, and the log-bilinear neural language model (Mnih and Hinton, 2007).", "startOffset": 85, "endOffset": 108}, {"referenceID": 33, "context": "We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU (Papineni et al., 2002) on a downstream machine translation (MT) task.", "startOffset": 102, "endOffset": 125}, {"referenceID": 32, "context": "To build the hard class-based LMs, we utilized mkcls4, a tool to train word classes that uses the maximum likelihood criterion (Och, 1995) for classing.", "startOffset": 127, "endOffset": 138}, {"referenceID": 38, "context": "We subsequently trained trigram class language models on these classes (corresponding to 2nd-order HMMs) using SRILM (Stolcke, 2002), with KN-smoothing for the class transition probabilities.", "startOffset": 117, "endOffset": 132}, {"referenceID": 9, "context": "For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al.", "startOffset": 66, "endOffset": 80}, {"referenceID": 11, "context": "For our MT evaluation, we built a hierarchical phrase translation (Chiang, 2007) system using cdec (Dyer et al., 2010).", "startOffset": 99, "endOffset": 118}, {"referenceID": 15, "context": "The KN-smoothed models in the MT experiments were compiled using KenLM (Heafield, 2011).", "startOffset": 71, "endOffset": 87}, {"referenceID": 1, "context": "\u2022 Small-English: APNews corpus (Bengio et al., 2003): Train - 14 million words, Dev - 963,000, Test - 963,000.", "startOffset": 31, "endOffset": 52}, {"referenceID": 28, "context": "Comparison to LBL (Mnih and Hinton, 2007): Mnih and Hinton (2007) evaluate on the Small-English dataset (but remove end markers and concatenate the sentences).", "startOffset": 18, "endOffset": 41}, {"referenceID": 28, "context": "Comparison to LBL (Mnih and Hinton, 2007): Mnih and Hinton (2007) evaluate on the Small-English dataset (but remove end markers and concatenate the sentences).", "startOffset": 19, "endOffset": 66}, {"referenceID": 8, "context": "We used MIRA (Chiang et al., 2008) to learn the feature weights.", "startOffset": 13, "endOffset": 34}, {"referenceID": 2, "context": "9 (Bojar et al., 2013)", "startOffset": 2, "endOffset": 22}, {"referenceID": 10, "context": "procedure is repeated 10 times to control for optimizer instability (Clark et al., 2011).", "startOffset": 68, "endOffset": 88}, {"referenceID": 41, "context": "Unlike other recent approaches where an additional feature weight is tuned for the proposed model and used in conjunction with KN smoothing (Vaswani et al., 2013), our aim is to show the improvements that PLRE provides as a substitute for KN.", "startOffset": 140, "endOffset": 162}, {"referenceID": 42, "context": "These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 43, "context": "These have led to generalizations that account for domain effects (Wood and Teh, 2009) and unbounded contexts (Wood et al., 2009).", "startOffset": 110, "endOffset": 129}, {"referenceID": 39, "context": "Teh (2006) and Goldwater et al.", "startOffset": 0, "endOffset": 11}, {"referenceID": 12, "context": "Teh (2006) and Goldwater et al. (2006) discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor process.", "startOffset": 15, "endOffset": 39}, {"referenceID": 25, "context": "The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al.", "startOffset": 67, "endOffset": 96}, {"referenceID": 28, "context": "The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance.", "startOffset": 117, "endOffset": 162}, {"referenceID": 26, "context": "The idea of using neural networks for language modeling is not new (Miikkulainen and Dyer, 1991), but recent efforts (Mnih and Hinton, 2007; Mikolov et al., 2010) have achieved impressive performance.", "startOffset": 117, "endOffset": 162}, {"referenceID": 14, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al.", "startOffset": 48, "endOffset": 119}, {"referenceID": 29, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al.", "startOffset": 48, "endOffset": 119}, {"referenceID": 41, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al.", "startOffset": 48, "endOffset": 119}, {"referenceID": 45, "context": ", 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types.", "startOffset": 21, "endOffset": 38}, {"referenceID": 44, "context": ", 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types.", "startOffset": 114, "endOffset": 138}, {"referenceID": 13, "context": "An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost of exact normalization to O( \u221a V ).", "startOffset": 47, "endOffset": 84}, {"referenceID": 27, "context": "An alternate technique is to use word-classing (Goodman, 2001; Mikolov et al., 2011), which can reduce the cost of exact normalization to O( \u221a V ).", "startOffset": 47, "endOffset": 84}, {"referenceID": 13, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al.", "startOffset": 49, "endOffset": 539}, {"referenceID": 13, "context": "Techniques such as noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2012; Mnih and Teh, 2012; Vaswani et al., 2013), subsampling (Xu et al., 2011), or careful engineering approaches for maximum entropy LMs (which can also be applied to neural networks) (Wu and Khudanpur, 2000) have improved training of these models, but querying the probability of the next word given still requires explicitly normalizing over the vocabulary, which is expensive for big corpora or in languages with a large number of word types. Mnih and Teh (2012) and Vaswani et al. (2013) propose setting the normalization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation.", "startOffset": 49, "endOffset": 565}, {"referenceID": 37, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.", "startOffset": 36, "endOffset": 103}, {"referenceID": 0, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.", "startOffset": 36, "endOffset": 103}, {"referenceID": 17, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.", "startOffset": 36, "endOffset": 103}, {"referenceID": 0, "context": "There are a few low rank approaches (Saul and Pereira, 1997; Bellegarda, 2000; Hutchinson et al., 2011), but they are only effective in restricted settings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. Roark et al. (2013) also use the idea of marginal constraints for re-estimating back-off parameters for heavilypruned language models, whereas we use this concept to estimate n-gram specific discounts.", "startOffset": 61, "endOffset": 306}], "year": 2014, "abstractText": "We present power low rank ensembles (PLRE), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method can be understood as a generalization of ngram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task.", "creator": "TeX"}}}