{"id": "1312.7302", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Dec-2013", "title": "Learning Human Pose Estimation Features with Convolutional Networks", "abstract": "This paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modified learning technique that learns low-level features and higher-level weak spatial models. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows significant improvement over the current state-of-the-art results. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to outperform all existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on features that might even just cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent then expected. Many researchers previously argued that the kinematic structure and top-down information is crucial for this domain, but with our purely bottom up, and weak spatial model, we could improve other more complicated architectures that currently produce the best results. This mirrors what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced. Moreover, there are new ways to do these kinds of work, but they can be challenging.\n\n\nThe paper is a continuation of a previous paper showing that deep learning can learn large-scale low-level features in the top-down, but not even deep-learning in the top-down, and low-level high-level low-level high-level high-level high-level high-level high-level high-level high-level low-level low-level low-level high-level low-level high-level high-level high-level high-level high-level high-level high-level high-level low-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level low-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level high-level", "histories": [["v1", "Fri, 27 Dec 2013 17:41:13 GMT  (3122kb,D)", "https://arxiv.org/abs/1312.7302v1", null], ["v2", "Mon, 30 Dec 2013 04:29:34 GMT  (3097kb,D)", "http://arxiv.org/abs/1312.7302v2", null], ["v3", "Fri, 3 Jan 2014 20:56:34 GMT  (3206kb,D)", "http://arxiv.org/abs/1312.7302v3", null], ["v4", "Tue, 18 Feb 2014 16:22:38 GMT  (3212kb,D)", "http://arxiv.org/abs/1312.7302v4", null], ["v5", "Tue, 25 Feb 2014 05:32:32 GMT  (3212kb,D)", "http://arxiv.org/abs/1312.7302v5", null], ["v6", "Wed, 23 Apr 2014 19:23:46 GMT  (3213kb,D)", "http://arxiv.org/abs/1312.7302v6", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["arjun jain", "jonathan tompson", "mykhaylo", "riluka", "graham w taylor", "christoph bregler"], "accepted": true, "id": "1312.7302"}, "pdf": {"name": "1312.7302.pdf", "metadata": {"source": "CRF", "title": "Learning Human Pose Estimation Features with Convolutional Networks", "authors": ["Arjun Jain", "Jonathan Tompson", "Mykhaylo Andriluka"], "emails": ["ajain@nyu.edu", "tompson@cims.nyu.edu", "andriluk@mpi-inf.mpg.de", "gwtaylor@uoguelph.ca", "chris.bregler@nyu.edu"], "sections": [{"heading": null, "text": "Figure 1: The green cross is our new technique\u2019s wrist locator, the red cross is the state-of-the-art CVPR13 MODEC detector [38] on the FLIC database."}, {"heading": "1 Introduction", "text": "One of the hardest tasks in computer vision is determining the high degree-of-freedom configuration of a human body with all its limbs, complex self-occlusion, self-similar parts, and large variations due to clothing, body-type, lighting, and many other factors. The most challenging scenario for this problem is from a monocular RGB image and with no prior assumptions made using motion models, pose models, background models, or any other common heuristics that current state-of-theart systems utilize. Finding a face in frontal or side view is relatively simple, but determining the\nar X\niv :1\n31 2.\n73 02\nv6 [\ncs .C\nV ]\n2 3\nA pr\nexact location of body parts such as hands, elbows, shoulders, hips, knees and feet, each of which sometimes only occupy a few pixels in the image in front of an arbitrary cluttered background, is significantly harder.\nThe best performing pose estimation methods, including those based on deformable part models, typically are based on body part detectors. Such body part detectors commonly consist of multiple stages of processing. The first stage of processing in a typical pipeline consists of extracting sets of low-level features such as SIFT [25], HoG [11], or other filters that describe orientation statistics in local image patches. Next, these features are pooled over local spatial regions and sometimes across multiple scales to reduce the size of the representation and also develop local shift/scale invariance. Finally, the aggregate features are mapped to a vector, which is then either input to 1) a standard classifier such as a support vector machine (SVM) or 2) the next stage of processing (e.g. assembling the parts into a whole). Much work is devoted to engineering the system to produce a vector representation that is sensitive to class (e.g. head, hands, torso) while remaining invariant to the various nuisance factors (lighting, viewpoint, scale, etc.)\nAn alternative approach is representation learning: relying on the data instead of feature engineering, to learn a good representation that is invariant to nuisance factors. For a recent review, see [6]. It is common to learn multiple layers of representation, which is referred to as deep learning. Several such techniques have used unsupervised or semi-supervised learning to extract multi-layer domain-specific invariant representations, however, it is purely supervised techniques that have won several recent challenges by large margins, including ImageNet LSVRC 2012 and 2013 [23, 51]. These end-to-end learning systems have capitalized on advances in computing hardware (notably GPUs), larger datasets like ImageNet, and algorithmic advances (specifically gradient-based training methods and regularization).\nWhile these methods are now proven in generic object recognition, their use in pose estimation has been limited. Part of the challenge in making end-to-end learning work for human pose estimation is related to the nonrigid structure of the body, the necessity for precision (deep recognition systems often throw away precise location information through pooling), and the complex, multi-modal nature of pose.\nIn this paper, we present the first end-to-end learning approach for full-body human pose estimation. While our approach is based on convolutional networks (convnets) [24], we want to stress that the na\u0131\u0308ve implementation of applying this model \u201coff-the-shelf\u201d will not work. Therefore, the contribution of this work is in both a model that outperforms state of the art deformable part models (DPMs) on a modern, challenging dataset, and also an analysis of what is needed to make convnets work in human pose estimation. In particular, we present a two-stage filtering approach whereby the response maps of convnet part detectors are denoised by a second process informed by the part hierarchy."}, {"heading": "2 Related Work", "text": "Detecting people and their pose has been investigated for decades. Many early techniques rely on sliding-window part detectors based on hand-crafted or learned features or silhouette extraction techniques applied to controlled recording conditions. Examples include [14, 49, 5, 30]. We refer to [35] for a complete survey of this era. More recently, several new approaches have been proposed that are applied to unconstrained domains. In such domains, good performance has been achieved with so-called \u201cbag of features\u201d followed by regression-based, nearest neighbor or SVM-based architectures. Examples include \u201cshape-context\u201d edge-based histograms from the human body [28, 1] or just silhouette features [19]. Shakhnarovich et al. [39] learn a parameter sensitive hash function to perform example-based pose estimation. Many relevant techniques have also been applied to hand tracking such as [48]. A more general survey of the large field of hand tracking can be found in [12].\nMany techniques have been proposed that extract, learn, or reason over entire body features. Some use a combination of local detectors and structural reasoning (see [36] for coarse tracking and [10] for person-dependent tracking). In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention. We will focus on these techniques and their latest incarnations in the following sections.\nFurther examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8]. These differ from our domain in that the images considered are of higher quality and less cluttered. Also many of these techniques work on images from a single camera, but need video sequence input (not single images) to achieve impressive results [42, 52].\nAs an example of a technique that works for single images against cluttered backgrounds, Shotton et al.\u2019s Kinect based body part detector [40] uses a random forest of decision trees trained on synthetic depth data to create simple body part detectors. In the proposed work, we also adopt simple partbased detectors, however, we focus on a different learning strategy.\nThere are a number of successful end-to-end representation learning techniques which perform pose estimation on a limited subset of body parts or body poses. One of the earliest examples of this type was Nowlan and Platt\u2019s convolutional neural network hand tracker [30], which tracked a single hand. Osadchy et al. applied a convolutional network to simultaneously detect and estimate the pitch, yaw and roll of a face [31]. Taylor et al. [44] trained a convolutional neural network to learn an embedding in which images of people in similar pose lie nearby. They used a subset of body parts, namely, the head and hand locations to learn the \u201cgist\u201d of a pose, and resorted to nearest-neighbour matching rather than explicitly modeling pose. Perhaps most relevant to our work is Taylor et al.\u2019s work on tracking people in video [45], augmenting a particle filter with a structured prior over human pose and dynamics based on learning representations. While they estimated a posterior over the whole body (60 joint angles), their experiments were limited to the HumanEva dataset [41], which was collected in a controlled laboratory setting. The datasets we consider in our experiments are truly poses \u201cin the wild\u201d, though we do not consider dynamics.\nA factor limiting earlier methods from tacking full pose-estimation with end-to-end learning methods, in particular deep networks, was the limited amount of labeled data. Such techniques, with millions or more parameters, require more data than structured techniques that have more a priori knowledge, such as DPMs. We attack this issue on two fronts. First, directly, by using larger labeled training sets which have become available in the past year or two, such as FLIC [38]. Second, indirectly, by better exploiting the data we have. The annotations provided by typical pose estimation datasets contain much richer information compared to the class labels in object recognition datasets In particular, we show that the relationships among parts contained in these annotations can be used to build better detectors."}, {"heading": "3 Model", "text": "To perform pose estimation with a convolutional network architecture [24] (convnet), the most obvious approach would be to map the image input directly to a vector coding the articulated pose: i.e. the type of labels found in pose datasets. The convnet output would represent the unbounded 2-D or 3-D positions of joints, or alternatively a hierarchy of joint angles. However, we found that this worked very poorly. One issue is that pooling, while useful for improving translation invariance during object recognition, destroys precise spatial information which is necessary to accurately predict pose. Convnets that produce segmentation maps, for example, avoid pooling completely [47, 13]. Another issue is that the direct mapping from input space to kinematic body pose coefficients is highly non-linear and not one-to-one. However, even if we took this route, there is a deeper issue with attempting to map directly to a representation of full body pose. Valid poses represent a much lower-dimensional manifold in the high-dimensional space in which they are captured. It seems troublesome to make a discriminative network map to a space in which the majority of configurations do not represent valid poses. In other words, it makes sense to restrict the net\u2019s output to a much smaller class of valid configurations.\nRather than perform multiple-output regression using a single convnet to learn pose coefficients directly, we found that training multiple convnets to perform independent binary body-part classification, with one network per feature, resulted in improved performance on our dataset. These convnets are applied as sliding windows to overlapping regions of the input, and map a window of pixels to a single binary output: the presence or absence of that body part. The result of applying the convnet is a response-map indicating the confidence of the body part at that location. This lets us use much smaller convnets, and retain the advantages of pooling, at the expense of having to maintain a separate set of parameters for each body part. Of course, a series of independent part\ndetectors cannot enforce consistency in pose in the same way as a structured output model, which produces valid full-body configurations. In the following sections, we first describe in detail the convolutional network architecture and then a method of enforcing pose consistency using parent-child relationships."}, {"heading": "3.1 Convolutional Network Architecture", "text": "The lowest level of our two-stage feature detection pipeline is based on a standard convnet architecture, an overview of which is shown in Figure 2. Convnets, like their fully-connected, deep neural network counterparts, perform end-to-end feature learning and are trained with the back-propagation algorithm. However, they differ in a number of respects, most notably local connectivity, weight sharing, and local pooling. The first two properties significantly reduce the number of free parameters, and reduce the need to learn repeated feature detectors at different locations of the input. The third property makes the learned representation invariant to small translations of the input.\nThe convnet pipeline shown in Figure 2 starts with a 64\u00d764 pixel RGB input patch which has been local contrast normalized (LCN) [22] to emphasize geometric discontinuities and improve generalization performance [32]. The LCN layer is comprised of a 9\u00d79 pixel local subtractive normalization, followed by a 9\u00d79 local divisive normalization. The input is then processed by three convolution and subsampling layers, which use rectified linear units (ReLUs) [18] and max-pooling.\nAs expected, we found that internal pooling layers help to a) reduce computational complexity1 and b) improve classification tolerance to small input image translations. Unfortunately, pooling also results in a loss of spatial precision. Since the target application for this convnet was offline (rather than real-time) body-pose detection, and since we found that with sufficient training exemplars, invariance to input translations can be learned, we choose to use only 2 stages of 2 \u00d7 2 pooling (where the total image downsampling rate is 4\u00d7 4). Following the three stages of convolution and subsampling, the top-level pooled map is flattened to a vector and processed by three fully connected layers, analogous to those used in deep neural networks. Each of these output stages is composed of a linear matrix-vector multiplication with learned bias, followed by a point-wise non-linearity (ReLU). The output layer has a single logistic unit, representing the probability of the body part being present in that patch.\nTo train the convnet, we performed standard batch stochastic gradient descent. From the training set images, we set aside a validation set to tune the network hyper-parameters, such as number and size of features, learning rate, momentum coefficient, etc. We used Nesterov momentum [43] as well as RMSPROP [46] to accelerate learning and we used L2 regularization and dropout [21] on the input to each of the fully-connected linear stages to reduce over-fitting the restricted-size training set.\n1The number of operations required to calculate the output of the the three fully-connected layers is O ( n2 ) in the size of the Rn input vectors. Therefore, even small amounts of pooling in earlier stages can drastically reduce training time."}, {"heading": "3.2 Enforcing Global Pose Consistency with a Spatial Model", "text": "When applied to the validation set, the raw output of the network presented in Section 3.1 produces many false-positives. We believe this is due to two factors: 1) the small image context as input to the convnet (64\u00d764 pixels or approximately 5% of the input image area) does not give the model enough contextual information to perform anatomically consistent joint position inference and 2) the training set size is limited. We therefore use a higher-level spatial model with simple body-pose priors to remove strong outliers from the convnet output. We do not expect this model to improve the performance of poses that are close to the ground truth labels (within 10 pixels for instance), but rather it functions as a post processing step to de-emphasize anatomically impossible poses due to strong outliers.\nThe inter-node connectivity of our simple spatial model is displayed in Figure 3. It consists of a linear chain of kinematic 2D nodes for a single side of the human body. Throughout our experiments we used the left shoulder, elbow and wrist; however we could have used the right side joints without loss of generality (since detection of the right body parts simply requires a horizontal mirror of the input image). For each node in the chain, our convnet detector generates response-map unary distributions pfac (x), psho (x), pelb (x), pwri (x) over the dense pixel positions x, for the face, shoulder, elbow and wrist joints respectively. For the remainder of this section, all distributions are assumed to be a function over the pixel position, and so the x notation will be dropped. The output of our spatial model will produce filtered response maps: p\u0302fac, p\u0302sho, p\u0302elb, and p\u0302wri.\nThe body part priors for a pair of joints (a, b), pa|b=~0, are calculated by creating a histogram of joint a locations over the training set, given that the adjacent joint b is located at the image center (x = ~0). The histograms are then smoothed (using a gaussian filter) and normalized. The learned priors for psho|fac=~0, pelb|sho=~0, and pwri|elb=~0 are shown in Figure 4. Note that due to symmetry, the prior for pelb|wri=~0 is a 180\u00b0 rotation of pwri|elb=~0 (as is the case of other adjacent pairs). Rather than assume a simple Gaussian distribution for modeling pairwise interactions of adjacent nodes, as is standard in many parts-based detector implementations, we have found that the these non-parametric spatial priors lead to improved detection performance.\n50 100 150 200 250 300\n50\n100\n150\n200\n250\n300 50 100 150 200 250 300\n50\n100\n150\n200\n250\n300 50 100 150 200 250 300\n50\n100\n150\n200\n250\n300 50 100 150 200 250 300\n50\n100\n150\n200\n250\n300 50 100 150 200 250 300\n50\n100\n150\n200\n250\n300 50 100 150 200 250 300\n50\n100\n150\n200\n250\n300 a) psho|fac=~0 b) pelb|sho=~0 c) pwri|elb=~0\nFigure 4: Part priors for left body parts\nGiven the full set of prior conditional distributions and the convnet unary distributions, we can now construct the filtered distribution for each part by using an approach that is analogous to the sumproduct belief propagation algorithm. For body part i, with a set of neighbouring nodes U , the final distribution is defined as:\np\u0302i \u221d pi\u03bb \u220f u\u2208U ( pi|u=~0\u2217 pu ) (1)\nwhere \u03bb is a mixing parameter and controls the confidence of each joint\u2019s unary distribution towards its final filtered distribution (we used \u03bb = 1 for our experiments). The final joint distribution is therefore a product of the unary distribution for that joint, as well as the beliefs from neighbouring nodes (as with standard sum-product belief propagation). In log space, the above product for the shoulder joint becomes:\nlog (p\u0302sho) \u221d \u03bb log (psho) + log ( psho|fac=~0\u2217 pfac ) + log ( psho|elb=~0\u2217 pelb ) (2)\nWe also perform an equivalent computation for the elbow and wrist joints. The face joint is treated as a special case. Empirically, we found that incorporating image evidence from the shoulder joint to the filtered face distribution resulted in poor performance. This is likely due to the fact that the convnet does a very good job of localizing the face position, and so incorporating noisy evidence from the shoulder detector actually increases uncertainty. Instead, we use a global position prior for the face, hfac, which is obtained by learning a location histogram over the face positions in the training set images, as shown in Figure 5. In log space, the output distribution for the face is then given by:\nlog (p\u0302fac) \u221d \u03bb log (pfac) + log (hfac) (3)\nLastly, since the learned neural network convolution features and the spatial priors are not explicitly invariant to scale, we must run the convnet and spatial model on images at multiple scales at test time, and then use the most likely joint location across those scales as the final joint location. For datasets containing examples with multiple persons (known a priori), we use non-maximal suppression [29] to find multiple local maxima across the filtered response-maps from each scale, and we then take the top n most likely joint candidates from each person in the scene."}, {"heading": "4 Results", "text": "We evaluated our architecture on the FLIC [38] dataset, which is comprised of 5003 still RGB images taken from an assortment of Hollywood movies. Each frame in the dataset contains at least one person in a frontal pose (facing the camera), and each frame was processed by Amazon Mechanical Turk to obtain ground truth labels for the joint positions of the upper body of a single person. The FLIC dataset is very challenging for state-of-the-art pose estimation methodologies because the poses are unconstrained, body parts are often occluded, and clothing and background are not consistent.\nWe use 3987 training images from the dataset, which we also mirror horizontally to obtain a total of 3987 \u00d7 2 = 7974 examples. Since the training images are not at the same scale, we also manually annotate the bounding box for the head in these training set images, and bring them to canonical scale. Further, we crop them to 320\u00d7240 such that the center of the shoulder annotations lies at (160 px, 80 px). We do not perform this image normalization at test time. Following the methodology of Felzenszwalb et al. [15], at test time we run our model on images with only one person (351 images\nof the 1016 test examples). As stated in Section 3, the model is run on 6 different input image scales and we then use the joint location with highest confidence across those scales as the final location.\nFor training the convnet we use Theano [7], which provides a Python-based framework for efficient GPU processing and symbolic differentiation of complex compound functions. To reduce GPU memory usage while training, we cache only 100 mini-batches on the GPU; this allows us to use larger convnet models and keep all training data on a single GPU. As part of this framework, our system has two main threads of execution: 1) a training function which runs on the GPU evaluating the batched-SGD updates, and 2) a data dispatch function which preprocesses the data on the CPU and transfers it on the GPU when thread 1) is finished processing the 100 mini batches. Training each convnet on an NVIDIA TITAN GPU takes 1.9ms per patch (fprop + bprop) = 41min total. We test on a cpu cluster with 5000 nodes. Testing takes: 0.49sec per image (0.94x scale) = 2.8min total. NMS and spatial model take negligible time.\nFor testing, because of the shared nature of weights for all windows in each image, we convolve the learned filters with the full image instead of individual windows. This dramatically reduces the time to perform forward propagation on the full test set."}, {"heading": "4.1 Evaluation", "text": "To evaluate our model on the FLIC dataset we use a measure of accuracy suggested by Sapp et al. [38]: for a given joint precision radius we report the percentage of joints in the test set correct within the radius threshold (where distance is defined as 2D Euclidean distance in pixels). In Figure 4.1 we evaluate this performance measure on the the wrist, elbow and shoulder joints. We also compare our detector to the DPM [15] and MODEC [38] architectures. Note that we use the same subset of 351 images when testing all detectors.\nFigure 4.1 shows that our architecture out-performs or is equal to the MODEC and DPM detectors for all three body parts. For the wrist and elbow joints our simple spatial model improves joint localization for approximately 5% of the test set cases (at a 5 pixel threshold), which enables us to outperform all other detectors. However, for the shoulder joint our spatial model actual decreases the joint location accuracy for large thresholds. This is likely due to the poor performance of the convnet on the elbow.\nAs expected, the spatial model cannot improve the joint accuracy of points that are already close to the correct value, however it is never-the-less successful in removing outliers for the wrist and elbow joints. Figure 4.1 is an example where a strong false positive results in an incorrect part location before the spatial model is applied, which is subsequently removed after applying our spatial model."}, {"heading": "5 Conclusion", "text": "We have shown successfully how to improve the state-of-the-art on one of the most complex computer vision tasks: unconstrained human pose estimation. Convnets are impressive low-level feature detectors, which when combined with a global position prior is able to outperform much more complex and popular models. We explored many different higher level structural models with the aim to\nfurther improve the results, but the most generic higher level spatial model achieved the best results. As mentioned in the introduction, this is counter-intuitive to common belief for human kinematic structures, but it mirrors results in other domains. For instance in speech recognition, researchers observed, if the learned transition probabilities (higher level structure) are reset to equal probabilities, the recognition performance, now mainly driven by the emission probabilities does not reduce significantly [27]. Other domains are discussed in more detail by [26].\nWe expect to obtain further improvement by enlarging the training set with a new pose-based warping technique that we are currently investigating. Furthermore, we are also currently experimenting with multi-resolution input representations, that take a larger spatial context into account."}, {"heading": "6 Acknowledgements", "text": "This research was funded in part by the Office of Naval Research ONR Award N000141210327 and by a Google award."}], "references": [{"title": "Recovering 3D human pose from monocular images", "author": ["A. Agarwal", "B. Triggs", "I. Rhone-Alpes", "F. Montbonnot"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(1):44\u201358,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Pictorial structures revisited: People detection and articulated pose estimation", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Monocular 3d pose estimation and tracking by detection", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 623\u2013630. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Scape: shape completion and animation of people", "author": ["D. Anguelov", "P. Srinivasan", "D. Koller", "S. Thrun", "J. Rodgers", "J. Davis"], "venue": "ACM Transactions on Graphics (TOG), volume 24, pages 408\u2013416. ACM,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Boostmap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": "Technical report, University of Montreal,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), June", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Co-registration \u2013 simultaneous alignment and modeling of articulated 3D shapes", "author": ["M. Black", "D. Hirshberg", "M. Loper", "E. Rachlin", "A. Weiss"], "venue": "European patent application EP12187467.1 and US Provisional Application, Oct.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Poselets: Body part detectors trained using 3d human pose annotations", "author": ["L. Bourdev", "J. Malik"], "venue": "ICCV, sep", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning sign language by watching TV (using weakly aligned subtitles)", "author": ["P. Buehler", "A. Zisserman", "M. Everingham"], "venue": "CVPR,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886\u2013893. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Vision-based hand pose estimation: A review", "author": ["A. Erol", "G. Bebis", "M. Nicolescu", "R.D. Boyle", "X. Twombly"], "venue": "Computer Vision and Image Understanding, 108(1):52\u201373,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer Learning in Sign language", "author": ["A. Farhadi", "D. Forsyth", "R. White"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P. Felzenszwalb", "D. McAllester", "D. Ramanan"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Pose search: Retrieving people using their pose", "author": ["V. Ferrari", "M. Marin-Jimenez", "A. Zisserman"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pages 315\u2013323,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring 3d structure with a statistical image-based shape model", "author": ["K. Grauman", "G. Shakhnarovich", "T. Darrell"], "venue": "ICCV, pages 641\u2013648,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "A statistical model of human pose and body shape", "author": ["N. Hasler", "C. Stoll", "M. Sunkel", "B. Rosenhahn", "H.-P. Seidel"], "venue": "P. Dutr\u2019e and M. Stamminger, editors, Computer Graphics Forum (Proc. Eurographics 2008), volume 2, Munich, Germany, Mar.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "In Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1106\u20131114,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 86(11):2278\u20132324,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150\u20131157. Ieee,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Are spatial and global constraints really necessary for segmentation? In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 9\u201316", "author": ["A. Lucchi", "Y. Li", "X. Boix", "K. Smith", "P. Fua"], "venue": "IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating human body configurations using shape context matching", "author": ["G. Mori", "J. Malik"], "venue": "ECCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient non-maximum suppression", "author": ["A. Neubeck", "L. Van Gool"], "venue": "Proceedings of the 18th International Conference on Pattern Recognition - Volume 03, ICPR \u201906, pages 850\u2013855, Washington, DC, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "A convolutional neural network hand tracker", "author": ["S.J. Nowlan", "J.C. Platt"], "venue": "Advances in Neural Information Processing Systems, pages 901\u2013908,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1995}, {"title": "Synergistic face detection and pose estimation with energybased models", "author": ["M. Osadchy", "Y.L. Cun", "M.L. Miller"], "venue": "The Journal of Machine Learning Research, 8:1197\u20131215,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D.D. Cox", "J.J. DiCarlo"], "venue": "PLoS computational biology,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Vision-based human motion analysis: An overview", "author": ["R. Poppe"], "venue": "Computer Vision and Image Understanding, 108(1-2):4\u201318,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Strike a pose: Tracking people by finding stylized poses", "author": ["D. Ramanan", "D. Forsyth", "A. Zisserman"], "venue": "CVPR,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive pose priors for pictorial structures", "author": ["B. Sapp", "C. Jordan", "B.Taskar"], "venue": "In CVPR,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "ICCV, pages 750\u2013759,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "Realtime human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": "Communications of the ACM, 56(1):116\u2013 124,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human", "author": ["L. Sigal", "A. Balan", "B.M. J"], "venue": "motion. IJCV,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Fast articulated motion tracking using a sums of gaussians body model", "author": ["C. Stoll", "N. Hasler", "J. Gall", "H. Seidel", "C. Theobalt"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 951\u2013958. IEEE,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Pose-sensitive embedding by nonlinear NCA regression", "author": ["G. Taylor", "R. Fergus", "I. Spiro", "G. Williams", "C. Bregler"], "venue": "Advances in Neural Information Processing Systems 23 (NIPS), pages 2280\u20132288,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamical binary latent variable models for 3d human pose tracking", "author": ["G. Taylor", "L. Sigal", "D. Fleet", "G. Hinton"], "venue": "Proc. of the 23rd IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "Convolutional networks can learn to generate affinity graphs for image segmentation", "author": ["S.C. Turaga", "J.F. Murray", "V. Jain", "F. Roth", "M. Helmstaedter", "K. Briggman", "W. Denk", "H.S. Seung"], "venue": "Neural Computation, 22:511\u2013538,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Real-time hand-tracking with a color glove", "author": ["R.Y. Wang", "J. Popovi\u0107"], "venue": "ACM Transactions on Graphics (TOG), volume 28, page 63. ACM,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Pfinder: Real-time tracking of the human body", "author": ["C. Wren", "A. Azarbayejani", "T. Darrell", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):780\u2013785,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1997}, {"title": "Articulated pose estimation with flexible mixtures-of-parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1385\u20131392. IEEE,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 24, "context": "This echos what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "The first stage of processing in a typical pipeline consists of extracting sets of low-level features such as SIFT [25], HoG [11], or other filters that describe orientation statistics in local image patches.", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "The first stage of processing in a typical pipeline consists of extracting sets of low-level features such as SIFT [25], HoG [11], or other filters that describe orientation statistics in local image patches.", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "For a recent review, see [6].", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "Several such techniques have used unsupervised or semi-supervised learning to extract multi-layer domain-specific invariant representations, however, it is purely supervised techniques that have won several recent challenges by large margins, including ImageNet LSVRC 2012 and 2013 [23, 51].", "startOffset": 282, "endOffset": 290}, {"referenceID": 44, "context": "Several such techniques have used unsupervised or semi-supervised learning to extract multi-layer domain-specific invariant representations, however, it is purely supervised techniques that have won several recent challenges by large margins, including ImageNet LSVRC 2012 and 2013 [23, 51].", "startOffset": 282, "endOffset": 290}, {"referenceID": 22, "context": "While our approach is based on convolutional networks (convnets) [24], we want to stress that the na\u0131\u0308ve implementation of applying this model \u201coff-the-shelf\u201d will not work.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 42, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 4, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 27, "context": "Examples include [14, 49, 5, 30].", "startOffset": 17, "endOffset": 32}, {"referenceID": 30, "context": "We refer to [35] for a complete survey of this era.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Examples include \u201cshape-context\u201d edge-based histograms from the human body [28, 1] or just silhouette features [19].", "startOffset": 75, "endOffset": 82}, {"referenceID": 0, "context": "Examples include \u201cshape-context\u201d edge-based histograms from the human body [28, 1] or just silhouette features [19].", "startOffset": 75, "endOffset": 82}, {"referenceID": 17, "context": "Examples include \u201cshape-context\u201d edge-based histograms from the human body [28, 1] or just silhouette features [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 33, "context": "[39] learn a parameter sensitive hash function to perform example-based pose estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "Many relevant techniques have also been applied to hand tracking such as [48].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "A more general survey of the large field of hand tracking can be found in [12].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Some use a combination of local detectors and structural reasoning (see [36] for coarse tracking and [10] for person-dependent tracking).", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "Some use a combination of local detectors and structural reasoning (see [36] for coarse tracking and [10] for person-dependent tracking).", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 2, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 15, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 32, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 72, "endOffset": 94}, {"referenceID": 8, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 107, "endOffset": 110}, {"referenceID": 43, "context": "In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], \u201cposelets\u201d [9], and other part-models [16, 50] have received increased attention.", "startOffset": 134, "endOffset": 142}, {"referenceID": 35, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 18, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 162, "endOffset": 169}, {"referenceID": 7, "context": "Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8].", "startOffset": 162, "endOffset": 169}, {"referenceID": 36, "context": "Also many of these techniques work on images from a single camera, but need video sequence input (not single images) to achieve impressive results [42, 52].", "startOffset": 147, "endOffset": 155}, {"referenceID": 34, "context": "\u2019s Kinect based body part detector [40] uses a random forest of decision trees trained on synthetic depth data to create simple body part detectors.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "One of the earliest examples of this type was Nowlan and Platt\u2019s convolutional neural network hand tracker [30], which tracked a single hand.", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "applied a convolutional network to simultaneously detect and estimate the pitch, yaw and roll of a face [31].", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "[44] trained a convolutional neural network to learn an embedding in which images of people in similar pose lie nearby.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "\u2019s work on tracking people in video [45], augmenting a particle filter with a structured prior over human pose and dynamics based on learning representations.", "startOffset": 36, "endOffset": 40}, {"referenceID": 35, "context": "While they estimated a posterior over the whole body (60 joint angles), their experiments were limited to the HumanEva dataset [41], which was collected in a controlled laboratory setting.", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "To perform pose estimation with a convolutional network architecture [24] (convnet), the most obvious approach would be to map the image input directly to a vector coding the articulated pose: i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 40, "context": "Convnets that produce segmentation maps, for example, avoid pooling completely [47, 13].", "startOffset": 79, "endOffset": 87}, {"referenceID": 12, "context": "Convnets that produce segmentation maps, for example, avoid pooling completely [47, 13].", "startOffset": 79, "endOffset": 87}, {"referenceID": 20, "context": "The convnet pipeline shown in Figure 2 starts with a 64\u00d764 pixel RGB input patch which has been local contrast normalized (LCN) [22] to emphasize geometric discontinuities and improve generalization performance [32].", "startOffset": 128, "endOffset": 132}, {"referenceID": 29, "context": "The convnet pipeline shown in Figure 2 starts with a 64\u00d764 pixel RGB input patch which has been local contrast normalized (LCN) [22] to emphasize geometric discontinuities and improve generalization performance [32].", "startOffset": 211, "endOffset": 215}, {"referenceID": 16, "context": "The input is then processed by three convolution and subsampling layers, which use rectified linear units (ReLUs) [18] and max-pooling.", "startOffset": 114, "endOffset": 118}, {"referenceID": 39, "context": "We used Nesterov momentum [43] as well as RMSPROP [46] to accelerate learning and we used L2 regularization and dropout [21] on the input to each of the fully-connected linear stages to reduce over-fitting the restricted-size training set.", "startOffset": 50, "endOffset": 54}, {"referenceID": 19, "context": "We used Nesterov momentum [43] as well as RMSPROP [46] to accelerate learning and we used L2 regularization and dropout [21] on the input to each of the fully-connected linear stages to reduce over-fitting the restricted-size training set.", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "For datasets containing examples with multiple persons (known a priori), we use non-maximal suppression [29] to find multiple local maxima across the filtered response-maps from each scale, and we then take the top n most likely joint candidates from each person in the scene.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "[15], at test time we run our model on images with only one person (351 images", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "For training the convnet we use Theano [7], which provides a Python-based framework for efficient GPU processing and symbolic differentiation of complex compound functions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 14, "context": "We also compare our detector to the DPM [15] and MODEC [38] architectures.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Other domains are discussed in more detail by [26].", "startOffset": 46, "endOffset": 50}], "year": 2014, "abstractText": "This paper introduces a new architecture for human pose estimation using a multilayer convolutional network architecture and a modified learning technique that learns low-level features and a higher-level weak spatial model. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows improvement over the current stateof-the-art. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to meet the performance, and in many cases outperform, existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on regions that might only cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent than expected. Many researchers previously argued that the kinematic structure and top-down information are crucial for this domain, but with our purely bottom-up, and weak spatial model, we improve on other more complicated architectures that currently produce the best results. This echos what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced [26]. Figure 1: The green cross is our new technique\u2019s wrist locator, the red cross is the state-of-the-art CVPR13 MODEC detector [38] on the FLIC database.", "creator": "LaTeX with hyperref package"}}}