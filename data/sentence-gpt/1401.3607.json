{"id": "1401.3607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "A Brief History of Learning Classifier Systems: From CS-1 to XCS", "abstract": "Modern Learning Classifier Systems can be characterized by their use of rule accuracy as the utility metric for the search algorithm(s) discovering useful rules. Such searching typically takes place within the restricted space of co-active rules for efficiency. This paper gives an historical overview of the evolution of such systems. It outlines how to classify and classify rules by the general classification criteria and gives a general overview of the techniques in which to classify and classify rules. It then describes how to classify rules by its general classification criteria and discusses the principles that are applied to the following domains: rules\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 15 Jan 2014 14:37:48 GMT  (829kb)", "http://arxiv.org/abs/1401.3607v1", "17 pages, 5 figures"], ["v2", "Fri, 7 Feb 2014 11:55:12 GMT  (1192kb)", "http://arxiv.org/abs/1401.3607v2", "37 pages, 9 figures"]], "COMMENTS": "17 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["larry bull"], "accepted": false, "id": "1401.3607"}, "pdf": {"name": "1401.3607.pdf", "metadata": {"source": "CRF", "title": "A Brief History of Learning Classifier Systems: From CS-1 to XCS", "authors": ["Larry Bull"], "emails": ["larry.bull@uwe.ac.uk"], "sections": [{"heading": null, "text": "accuracy as the utility metric for the search algorithm(s) discovering useful rules. Such searching typically takes place within the restricted space of co-active rules for efficiency. This paper gives an historical overview of the evolution of such systems."}, {"heading": "1. Introduction", "text": "Learning Classifier Systems (LCS) are rule-based systems, where the rules are usually in the traditional production system form of \u201cIF condition THEN assertion\u201d. An evolutionary algorithm and/or other heuristics are used to search the space of possible rules, whilst another learning process is used to assign utility to existing rules, thereby guiding the search for better rules. The LCS formalism was introduced by John Holland [1976] and based around his more well-known invention \u2013 the Genetic Algorithm (GA)[Holland, 1975]. A few years later, in collaboration with Judith Reitman, he presented the first implementation of an LCS in \u201cCognitive System Level 1\u201d (CS-1) [Holland & Reitman, 1978]. Holland then revised the framework to define what would become the standard system for many years [Holland, 1980]. However, Holland\u2019s full system was somewhat complex and practical experience found it difficult to realize the envisaged behaviour/performance, despite numerous modifications (e.g., see [Wilson & Goldberg, 1989]), and interest waned. Some years later, Stewart Wilson introduced a form of LCS in which rule fitness is calculated solely by the accuracy of the predicted consequences of rule assertions \u2013 the \u201ceXtended Classifier System\u201d (XCS) [Wilson, 1995]. The following two decades have seen a resurgence in the use of LCS as XCS in particular has been found able to solve a number of well-known problems optimally (e.g., see [Butz, 2006]). Perhaps more importantly, LCS have been applied to a number of real-world problems (e.g., see [Bull, 2004]), particularly data mining (e.g., see [Bull et al., 2008]), to great effect. Formal understanding of LCS has also increased (e.g., see [Bull & Kovacs, 2005]). The purpose of this paper is to provide some historical context to the area of modern accuracy-based Learning Classifier Systems."}, {"heading": "2. The Evolution of Accuracy-based LCS", "text": "Holland developed the LCS formalism as an approach to reinforcement learning, that is, learning through trial-and-error. Reinforcement learning methods seek to ascertain the value of executing each possible action (assertion) available within each state (condition) of a given problem. External reward signals are periodically applied to the system as it learns the problem, with the system aiming to maximise the accumulated reward received (see [Sutton & Barto, 1998]). Within psychology, the study of trialand-error learning can be traced back to Edward Thorndike and his \u201cLaw of Effect\u201d [Thorndike, 1911], and within computer science to Alan Turing and his \u201cP-type unorganised machine\u201d [Turing, 1948]. Whilst Farley and Clark [1954] were perhaps the first to implement reinforcement learning within a computer, Holland was influenced by Arthur Samuel\u2019s seminal early work on draughts/checkers [Samuel, 1959], which itself drew upon Claude Shannon\u2019s work on chess, seemingly the first consideration of learning a value function through experience [Shannon, 1950]. Samuel [1959] described a scheme for adjusting temporally successive estimates of the end reward value from a sequence of moves (improved in [Samuel, 1967]).\nHolland\u2019s [1976] interest was in how an artificial system may continuously adapt to novelty, significantly, extending previous studies by also considering how to build suitable knowledge representations thereby enabling flexible, continual learning through trial-and-error. A variant of his Genetic Algorithm was incorporated as an effective approach to this ability. The suggestion that a simulated evolutionary process may prove useful in artificial systems was first made by Turing [1948], with early implementations within a computer by Fraser [1957] and Box [1957]. Evolutionary computing techniques maintain a population of solutions and iteratively apply a stochastic selection process to identify good existing solutions which are then altered by stochastic variation procedures to produce new, hopefully better/fitter, solutions (see [Eiben & Smith, 2003]). The combined reinforcement learning and evolutionary computing architecture was termed CS-1 [Holland & Reitman, 1978]. Figure 1 shows a family tree of the LCS considered as the significant steps in the evolution of XCS from CS-1 to be discussed in this chapter, beginning with CS-1.\n2.1 Cognitive System Level 1\nOn each discrete time cycle, CS-1 receives a binary encoded description of the current state of its environment. The system determines an appropriate response based on this input, its last action, and the current contents of an internal memory space, termed a message list (Figure 2). The rule-base consists of a population of condition-assertion rules or \"classifiers\". The rule conditions are strings of characters from the ternary alphabet {0,1,#}. The # acts as a wildcard allowing generalisation such that the rule condition 1#1 matches both the input 111 and the input 101, for example. Rule assertions contain both an action and an internal message, both formed from binary strings. All rule components are initialised randomly. Also associated with each rule are a number of parameters, including age, frequency of use, and a prediction of the typical reward received from its use, which is also the fitness metric (explained later).\nOn receipt of an input message, the rule-base is scanned and any rule whose condition matches the external message, the content of the message list, and the previous action becomes a member of the current \"match set\" [M]. A heuristic considering aspects such as the specificity of matching and the predicted future reward is then used to determine the top ten eligible rules. The system response is then chosen probabilistically from that set. The chosen rule\u2019s action is executed in the environment, the message list updated, its age halved, and frequency increased.\nCS-1 uses an epochal reinforcement learning scheme such that the identification of all rules that have provided an action is recorded, in order, between rewards. If an external reward is received, the predicted reward of each rule in this set [E] is adjusted at a rate inversely proportional to their frequency parameter \u201cto reflect their accuracy in anticipating this reward. Those predicted payoffs that were consistent with (not greater than) this reward are maintained or increased; those that overpredicted are significantly reduced\u201d [Holland & Reitman, 1978]. A further heuristic is applied to the predictions such that the actual value of reward used to update each member of [E] is \u201cattenuated\u201d, an adjustment based on the relative size of reward predicted by rules and by their successors in [E], it being incremented each time the latter is higher than the former. \u201cThis parameter is highly correlated with the\ndelay between a response and the reward\u201d [Holland & Reitman, 1978] and may be seen as an early form of temporal difference learning.\nAfter every ten rewards received, the contents of the rule-base are altered by the simulated evolutionary process of the GA. The implemented CS-1 could take one of two actions and so ran the evolutionary search process within the two subpopulations, that is, action niches. Fitness proportionate selection using the predicted reward of each rule as the fitness value picks two parents from the rule-base population. These are then combined using one-point crossover (mutation is not mentioned). One of the two offspring produced is selected at random to be inserted into the rule-base. Replacement uses the age of rules. \u201cRecall that a classifier with a poor predicted payoff rarely wins competitions; without a win, its age increases steadily. Age therefore, reflects the classifier\u2019s quality as well as its frequency of use. To make room for the new classifier therefore, one with an old age is deleted.\u201d [Holland & Reitman, 1978]. Moreover, from the set of oldest rules within the niche, the one closest in Hamming(-like) distance is chosen; a form of crowding is used. The use of overlapping generations and crowding were later adopted in the application of GAs to function optimization [DeJong, 1975].\nCS-1 was shown able to solve a simple maze task with seven locations, two actions, and two types of reward, before being applied to an extended maze. Holland and Reitman report faster learning of the second maze using a system previously trained on the smaller maze, in comparison to a na\u00efve system. Analysis of the external input patterns indicates minor changes in effective general rules in the smaller maze\nare close in rule-space to those in the larger, as might be expected (see [Iqbal et al., 2013] for related recent work). Wilson [1981] was also able to use a CS-1 based system for a camera centring control problem, which included some complex image processing to create the binary inputs to the LCS.\nLCS aim to build an efficient representation of any underlying regularities within the given problem domain during learning. The inherent pressure within CS-1 to discover maximally general rules - rules which aggregate the most problem states together from which the same action results in the same reward - over more specific (less # symbols in their condition) but equally accurate predictors comes from the evolutionary deletion scheme. More specific rules tend not to match so often and so their age increases more rapidly than more general, but also accurate rules; the probability of removal of specific rules from the rule-base increases with specificity. Similarly, the pressure to remove over general rules \u2013 rules that aggregate too many states together such that the level of reward received from their use varies \u2013 comes from the reinforcement scheme and evolutionary selection. Over or under prediction of a reward value results in a significant reduction in the predicted reward of a rule, the parameter used as the fitness measure for reproduction by the GA; inaccurate rules have lower fitness and hence are unlikely to be selected.\nHowever, the described system struggles to maintain more than one or a very few rules within a population. That is, the GA tends to converge upon a single (maximally general) solution. This explains why CS-1 runs the GA in the two explicit action subpopulation niches. In the two mazes, CS-1 always started in the middle and had to maintain the same action across a number of states to an end goal state where reward was given. Whether the system should go left or right depended upon an internal value. Hence a rule which generalised over all the states to the left and one which generalised over all the states to the right was the optimal solution. By running the GA in two niches based on actions, both were sustainable indefinitely.\nWith this apparent limitation, Holland subsequently altered a number of aspects of CS-1. Importantly, Wilson would return to the use of reward prediction accuracy and frequency of use in XCS.\n2.2 Holland\u2019s Standard Architecture\nA few years later Holland [1980] revised CS-1 and described what would become the standard architecture, here termed \u201cLearning Classifier System\u201d (LCS). It should be noted that Holland seems not to have used the prefix \u201clearning\u201d at the time; Goldberg [1985] may have been first to add the emphasis. The main change from CS-1 was to introduce a reinforcement learning scheme based upon an economic metaphor, known as the \u201cbucket brigade\u201d (after the water passing chains of fire fighters), in which rule utility is judged by the accruement of credit. In this way, rules acting in temporal chains leading to external reward are viewed as the middlemen of supply and demand chains. Rules maintain a single parameter of credit (termed strength) received. This is used both for action selection and in rule discovery by the GA. The message list is extended to enable multiple rules to post their assertions. Rule conditions no longer have a fixed structure to consider the current environmental state, the contents of the message list and the last action. Instead, all conditions and assertions are of the same length, with conditions also able to include a logical NOT. Assertions are now built\nfrom the same alphabet as conditions {0,1,#} such that information may \u201cpass through\u201d from either the condition or the string (external input or internal message) which the rule matches where a # exists.\nOn each cycle, a binary external state description is placed onto the message list, the rule-base is scanned, and any rule whose condition matches the external message and/or the other contents of the message list becomes a member of [M] (Figure 3). Rules are selected from those comprising [M], through a bidding mechanism, firstly to become the system's external action and then to post their assertion onto the (fixed size) message list for the next cycle. This selection is performed by the roulette wheel scheme based on rule bids. Rules' bids consist of two components, their strength and the specificity (fraction of non-# bits) of their condition. Further, a constant (here\ntermed , where 0<<1) is factored in, i.e., for a rule C in [M] at time t:\nBid(C,t) = \u03b2.specificity(C).strength(C,t)\nReinforcement consists of redistributing bids made between subsequently chosen rules. The bid of each winner at each time-step is placed in a \"bucket\". A record is kept of the winners on the previous time step and they each receive an equal share of the contents of the current bucket; fitness is shared amongst concurrently activated rules. If a reward is received from the environment then this is paid to the winning rule which produced the last system output. Although, whether all rules that have\nposted a message share the external reward appears to vary in the literature, being both included [Holland, 1985] and excluded [Holland, 1986]. \u201cThus, the bucket brigade assures that early-acting, stage-setting classifiers receive credit if they (on average) make possible later, overtly rewarding acts\u201d [Holland, 1986]. The emphasis upon average ability relaxes the previous explicit focus on accurately predicting reward; there is an apparent reduction in the selective pressure for consistent behaviour which formed the basis of CS-1. With hindsight, this change was perhaps the most significant between the two LCS.\nAs noted above, the periodically applied GA uses rule strength to select two parents, these are then combined using one-point crossover and mutated. Both offspring are inserted into the rule-base, replacing rules chosen inversely proportional to their strength. Since reward is shared amongst rules, the GA is in principal able to maintain multiple useful rules within the rule-base (discussed later).\nA number of other mechanisms were proposed by Holland but for the sake of clarity they are not described in detail here. In particular, the idea that hierarchical rule associations could emerge via specific rules out-bidding more general rules in certain important situations, and extra \u201ctag\u201d regions of conditions and assertions being added would aid the formation of sequential induction (see [Holland et al., 1986] for a full treatment). These ideas do not feature in modern LCS (see [Smith et al., 2010] for an exception)."}, {"heading": "2.3 GOFER", "text": "Lashon Booker [1982] presented a form of Holland\u2019s standard LCS which extends the principle of using a GA to discover any underlying regularities in the problem space, dividing the task of learning such structure from that of supplying appropriate actions to receive external reward (see [Booker, 1988] for an overview]). Here a separate LCS exists for each of these two aspects. A first LCS receives binary encoded descriptions of the external environment, with the objective to learn appropriate regularities through generalizations over the \u201cperception\u201d space. This is seen as analogous to learning to represent categories of objects. The matching rules not only post their messages onto their own message list but some are passed as inputs to a second LCS. The second LCS therefore only receives reward when it correctly exploits such categorizations with respect to the current task. GOFER contains a number of innovations including partial matching and rule excitation levels, however it is the use of restricting the actions of rule-discovery to concurrently active rules which has proven most influential (see [Booker, 1985]). Here parents are chosen from within a given [M] thereby avoiding the mixing of rules with generalizations which (potentially) consider markedly different aspects of the problem. Booker [1989] later extended the idea to trigger the GA during learning whilst also leaving it running at a constant rate under the reinforcement process as Holland did. In particular, rules maintain an approximation of their \u201cconsistency\u201d, a measure of the variance in the reward they receive. If a given percentage of rules in [M] have a level of inconsistency above a threshold, the fitness of all consistent rules is increased and the GA run: \u201c \u2026 consistent classifiers are thereby made more attractive to the genetic algorithm\u201d [ibid.]. XCS uses both a form of triggered niche GA and rule consistency.\n2.4 ANIMAT and ZCS\nHaving explored CS-1 for a real-world task, Stewart Wilson began to develop versions of Holland\u2019s LCS as an approach to understanding animal/human intelligence through the computer simulation of simple agents in progressively more complex domains - termed the animat approach. The first of these, ANIMAT [Wilson, 1985], makes a number of simplifications to Holland\u2019s architecture. In particular, the message list is removed and matching rules are grouped by their action in the bucket brigade process, forming actions sets [A]. The GA is also sensitive to rule actions, somewhat akin to CS-1: a first parent is chosen based upon its strength from the rulebase, the second is then chosen from the subset of the population with the same action. ANIMAT controlled a simple agent in a 2D gridworld, able to sense the contents of the eight locations surrounding it and able to move in each such direction if clear. Wilson showed learning was possible such that effective paths to food reward signals were discovered. However, he noted that the system had \u201cnothing which preferentially reinforces the most expeditious classifiers\u201d [ibid.]. To encourage the shortest path to reward from a given start location, rules were extended to maintain an estimate of the number of subsequent steps to reward from their use, updated locally based upon the estimates of successor rules. This was factored into action selection via dividing strength by distance. ANIMAT also includes a guided recombination operator, replacing dissimilar bits in parent conditions with a # to aid the formation of useful generalizations.\nWilson later returned to ANIMAT, further simplifying it in his \u201czeroth-level\u201d classifier system (ZCS) [Wilson, 1994] (Figure 4). Importantly, the bucket brigade was again modified to incorporate a mechanism from temporal difference learning [Sutton & Barto, 1981] (see also [Dorigo & Bersini, 1994] for an early connection). Here the fraction of the total strength of a given [A] in the bucket is further reduced\nby a discount rate  (0<<1) before being shared equally amongst the rules of the previous action set [A]-1. Discounting allows systematic control over the influence of future rewards, replacing Wilson\u2019s previous distance approximation mechanism. The\neffective update of action sets is thus (0<<1):\nstrength([A],t+1) = strength([A],t) + [ Reward + .strength([A]+1) \u2013 strength([A],t) ]\nTo give increased focus to the search, rules in a given [M] but not [A] have their strengths reduced by a tax; rules can only persist if they regularly receive (high) reward. A \u201ccreate\u201d mechanism in ANIMAT is also retained in ZCS, but slightly modified. Here, if an [M] is empty or if the total strength of [M] is below a given threshold, a new rule is created to cover the current environmental input, randomly augmented with some #, and given a random action. The action niche restriction and generalization mechanisms of the GA are removed. Parental rules give half of their\nstrength to their offspring under the GA which fires at a fixed rate .\nResults with ZCS indicated it was capable of good, but not optimal, performance [Wilson, 1994][Cliff & Ross, 1995]. Wilson [1994] also included a version of the offpolicy temporal difference learning algorithm Q-learning [Watkins, 1989] to some benefit. He also proposed to use the triggered niche GA of GOFER on top of the panmictic/global scheme described above. Bull [2005] showed the potential for disruption of the reward sharing scheme using just a niche GA in a similar LCS but no combination is known. It has been shown that ZCS is capable of optimal performance in a number of well-known test problems but that it appears to be particularly sensitive to some of its parameters [Bull & Hurst, 2002], and it has been shown to outperform XCS in classes of noisy domain [Stone & Bull, 2005]. XCS maintains a number of ZCS\u2019s basic features but makes significant alterations.\n2.5 BOOLE, NEWBOOLE and AU-BOOLE\nAfter introducing a number of modifications to Holland\u2019s architecture in ANIMAT, Wilson presented a specialised form designed for reinforcement learning tasks where immediate reward is given. In particular, his BOOLE system was designed for binary decision tasks [Wilson, 1987]. BOOLE maintains the [M]\u2192[A] mechanism of ANIMAT, also removing the message list. The GA no longer restricts selection of the second parent to having the same action when using crossover, and reproduction causes the strength of parents to be reduced and donated to offspring akin to the mechanism later used in ZCS. It has been shown that reducing strength can create a pressure for more general rules as they update more frequently and therefore regain reward faster [Bull, 2005]. Again, as in the later ZCS, rules in [M] but not [A] have their strengths reduced by a tax. BOOLE was shown able to learn two- and threeaddress bit multiplexer problems (6MUX and 11MUX, respectively), with the effects of varying the tax rates, genetic operators and including a reward bias based upon the\ndegree of generalisation in rules explored.\nBonelli et al. [1990] made the significant step of presenting a form of LCS for supervised learning tasks, that is, tasks where the correct response is known at the point of internal updating. Extending BOOLE, they noted that the set of rules in [M] providing the correct response, regardless of whether they formed [A], should receive reward. Hence they split [M] into the correct set [C] and incorrect set Not[C] for their NEWBOOLE system. BOOLE\u2019s uses of taxes and a bias in the distribution of reward based upon generality were kept. They showed significant improvement in learning speed compared to BOOLE and to an artificial neural network using backpropagation on the 6MUX and 11MUX tasks. Hartley [1999] showed NEWBOOLE to be competitive with XCS on a well-known set of binary classification tasks, although XCS\u2019s maintenance of a full state-action-reward map gave it an advantage in some forms of non-stationary task (see [Bull & Hurst, 2002] for discussion).\nSeemingly independently, Frey and Slate [1991] also presented a variant of BOOLE for supervised learning tasks in which they also update the correct set within [M] regardless of the output. Having struggled to find the correct balance of taxing and bid biasing for a letter recognition task, with reference to Holland\u2019s [1976] original ideas, they introduced the accuracy-utility system (here termed AU-BOOLE). Here each rule maintains two parameters: accuracy, the ratio of correct bids to total bids made; and, utility, the ratio of correct bids when chosen to total number of times chosen as the output. Accuracy is used in bidding in [M] and for reproduction, and utility is used for deletion. Whilst performance with AU-BOOLE was found to be similar to their version of NEWBOOLE, they report greater ease in finding useful parameters. These ideas have been incorporated into XCS, resulting in the \u201csUpervised Classifier System\u201d (UCS) [Bernado Mansilla & Garrell, 2003].\n2.6 CFSC2 and ACS\nHolland and Reitman [1978] suggested a number of extensions to CS-1 at the end of their paper, particularly ways by which to learn more sophisticated cognitive maps than the stimulus-response relations they had achieved. \u201cCognitive maps allow the system to use lookahead to explore, without overt acts, the consequences of various courses of action.\u201d [ibid.]. Again, following Samuel [1959], they describe a scenario of rules being linked over system cycles through the message list which do not cause external actions on each step. Holland [1990] later returned to this aspect, proposing that the aforementioned extra \u201ctag\u201d regions of conditions and assertions that can be added as arbitrary patterns would aid the formation of sequential induction of the necessary form: IF condition AND assertion THEN next-condition. That is, Holland did not seek to change the rule structure from his standard LCS to this direct form. Riolo [1991] was first to implement lookahead capabilities within LCS with his CFSC2. He allowed the system to execute more than one cycle before providing an action, added tags along the lines Holland [1990] had suggested, and introduced an extra strength parameter to represent the predictive accuracy of a rule. Through tags, rules are either connected to external or internal events, or both. Bidding is adjusted to also factor the accuracy of predicted next states of a rule (if any). Rule chains which accurately map features in simple mazes with or without overt reward (latent learning) are reported to emerge under a rule discovery process which is driven by\ninternal and external messages rather than a GA. That is, when no rules match or none are chained across system cycles, various heuristics are used to form appropriate rules via tags. Roberts [1993] presented a related approach within ANIMAT which maintained \u201cfollowsets\u201d, time-stamped information regarding rewards received or next states obtained after a rule had fired. The value of such rewards is factored into rule strengths.\nWilson [1995] proposed altering the rule structure to contain the anticipated next state, with an \u201cexpecton\u201d in XCS. Stolzmann [1998] presented a system in which such a rule structure is used (the expecton component termed the \u201ceffect\u201d) \u2013 the anticipatory classifier system (ACS). Drawing upon a learning theory from cognitive psychology, sub-populations of rules are learned per [A] via the specialisation of rules based upon the environmental input, both in the condition and effect components. Rule utility is represented by the accuracy of anticipations whilst external reward is used in bidding. A famous experiment with rats in a T-maze [Seward, 1949] is simulated and the results indicate similar behaviour from the ACS. Variants of XCS have been presented to achieve such model learning (see [Butz & Goldberg, 2003])."}, {"heading": "2.7 Wilson\u2019s XCS", "text": "The most significant difference between XCS (Figure 5) and all other LCS prior to its presentation is that rule fitness for the GA is not based on the amount of received by rules in anyway but purely upon the accuracy of predictions (p) of reward. The intention in XCS is to form a complete and accurate mapping of the problem space (rather than simply focusing on the higher payoff niches in the environment) through efficient generalizations: XCS learns a value function over the complete state-action space. On each time step a match set is created. A system prediction is then formed for each action in [M] according to a fitness-weighted average of the predictions of rules in each [A]. The system action is then selected either deterministically or randomly (usually 0.5 probability per trial). If [M] is empty covering is used.\nFitness reinforcement in XCS consists of updating three parameters, , p and F for each appropriate rule; the fitness is updated according to the relative accuracy of the rule within the set in five steps:\ni) Each rule\u2019s error is updated: j = j + ( | Reward - pj | - j) ii) Rule predictions are then updated: pj = pj + (Reward-pj) iii) Each rule\u2019s accuracy j is determined: j = (0/) \norwhere  where  ,andare constants controlling the shape of the accuracy function.\niv) A relative accuracy j\u2019 is determined for each rule by dividing its accuracy by the total of the accuracies in the action set. v) The relative accuracy is then used to adjust the classifier\u2019s fitness Fj using the moyenne adaptive modifee (MAM) procedure: If the fitness has been\nadjusted 1/ times, Fj = Fj + (j\u2019 - Fj). Otherwise Fj is set to the average of the values of  \u2019 seen so far.\nIn short, in XCS fitness is an inverse function of the error in reward prediction, with\nerrors belownot reducing fitness. The maximum P(ai) of the system\u2019s prediction array is discounted by a factor  and used to update rules from the previous time step. Thus XCS exploits a form of Q-learning [Watkins, 1989] in its reinforcement procedure. The GA originally occurred in [M] but Wilson [1998] later move it to [A] to further reduce the potential for recombining rules inappropriately, i.e., when there is significant asymmetry in the generalisation space for each action in a given match set (see [Bull, 2014] for discussion). Two rules are selected based on fitness from within the chosen [A]. Rule replacement is global and based on the estimated size of each action set a rule participates in with the aim of balancing resources across niches. The GA is triggered within a given action set based on the average time since the members of the niche last participated in a GA (after [Booker, 1989]). See [Butz & Wilson, 2002] for a full algorithmic description of XCS.\nWilson originally demonstrated results on multiplexer functions and a maze problem. Importantly, he shows how maximally general solutions are evolved by XCS. This is explained by his \u201cgeneralization hypothesis\u201d:\n\u201cConsider two classifiers C1 and C2 having the same action, where C2\u2019s condition is a generalization of C1\u2019s. \u2026. Suppose C1 and C2 are equally accurate in that their\nvalues of are the same. Whenever C1 and C2 occur in the same action set, their fitness values will be updated by the same amounts. However, since C2 is a generalization of C1, it will tend to occur in more [niches] than C1. Since the GA\noccurs in [niches], C2 would have more reproductive opportunities and thus its number of exemplars would tend to grow with respect to C1\u2019s. \u2026. C2 would displace C1 from the population\u201d [Wilson, 1995].\nThis has been studied formally and indicates the scalability of XCS\u2019s learning mechanisms [Butz et al., 2004; 2007]."}, {"heading": "3. Conclusion", "text": "Architecturally, XCS can be traced from ANIMAT via ZCS, with GOFER\u2019s triggered niche GA being included. The use of accuracy began with CS-1, although it was focused on the highest reward per niche. Moreover, XCS\u2019s generalization pressure shares features with that in CS-1 since it is also based on accuracy and rate of use. In CS-1, predicted rewards are only updated if they are accurate or below the current estimate, with action and GA selection based upon this parameter: more accurate rules are more likely to reproduce. Rule ages are reset after use and deletion is based upon age: more frequently used rules are less likely to be replaced. Thus accurate, more general (frequently used) rules are propagated in CS-1. XCS combines both accuracy, in its pure form, and frequency of use into the selection process of the GA. This creates a generalization pressure but, importantly, also frees the deletion process of the GA to be used to maintain multiple niches in an emergent way thereby addressing one of the main issues in CS-1 that Holland sought to tackle by switching to strength sharing in his subsequent LCS.\nAs Wilson prophetically suggests at the end of the paper introducing XCS: \u201cThe results point to the conclusion that accuracy-based fitness and a niche GA form a promising foundation for future classifier system research\u201d [Wilson, 1995]. As mentioned above and shown in Figure 1, XCS and these key features have been extended beyond reinforcement learning to supervised [Bernado Mansiall & Garrell, 2003], unsupervised (clustering) [Tammee et al., 2007], function [Wilson, 2002] and model [Butz & Goldberg, 2003] learning."}], "references": [{"title": "Accuracy-Based Learning Classifier Systems: Models, Analysis and Applications to Classification Tasks. Evolutionary Computation", "author": ["E. Bernado Mansilla", "J. Garrell"], "venue": null, "citeRegEx": "Mansilla and Garrell,? \\Q2003\\E", "shortCiteRegEx": "Mansilla and Garrell", "year": 2003}, {"title": "Evolutionary Operation: A Method for Increasing Industrial Productivity", "author": ["G. Kaufmann. Box"], "venue": null, "citeRegEx": "Box,? \\Q1957\\E", "shortCiteRegEx": "Box", "year": 1957}, {"title": "systems: An initial study", "author": ["L. Bull", "J. Hurst"], "venue": null, "citeRegEx": "Bull and Hurst,? \\Q2002\\E", "shortCiteRegEx": "Bull and Hurst", "year": 2002}, {"title": "Rule-based Evolutionary Online Learning Systems", "author": ["M.V. Mining. Springer. Butz"], "venue": null, "citeRegEx": "Butz,? \\Q2006\\E", "shortCiteRegEx": "Butz", "year": 2006}, {"title": "Toward a Theory of Generalization and Learning in XCS", "author": ["M.V. Butz", "T. Kovacs", "Lanzi", "P-L", "S.W. Wilson"], "venue": "IEEE Transactions on Evolutionary Computation", "citeRegEx": "Butz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Butz et al\\.", "year": 2004}, {"title": "Problem solution sustenance in XCS: Markov chain analysis of niche support distributions and the impact on computational complexity", "author": ["M.V. Butz", "D. Goldberg", "Lanzi", "P-L", "K. Sastry"], "venue": "Genetic Programming and Evolvable Machines", "citeRegEx": "Butz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Butz et al\\.", "year": 2007}, {"title": "Adding Temporary Memory to ZCS", "author": ["D. Cliff", "S. Ross"], "venue": "Adaptive Behavior", "citeRegEx": "Cliff and Ross,? \\Q1995\\E", "shortCiteRegEx": "Cliff and Ross", "year": 1995}, {"title": "An Analysis of the Behaviour of a Class of Genetic Adaptive Systems", "author": ["K. DeJong"], "venue": "Ph.D. Thesis, the University of Michigan", "citeRegEx": "DeJong,? \\Q1975\\E", "shortCiteRegEx": "DeJong", "year": 1975}, {"title": "A Comparison of Q-learning and Classifier Systems", "author": ["M. Dorigo", "H. Bersini"], "venue": "From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behaviour. MIT Press,", "citeRegEx": "Dorigo and Bersini,? \\Q1994\\E", "shortCiteRegEx": "Dorigo and Bersini", "year": 1994}, {"title": "Introduction to Evolutionary Computing", "author": ["A. Eiben", "J. Smith"], "venue": null, "citeRegEx": "Eiben and Smith,? \\Q2003\\E", "shortCiteRegEx": "Eiben and Smith", "year": 2003}, {"title": "Simulation of Self-organizing Systems by Digital Computer", "author": ["B. Farley", "W. Clark"], "venue": "IRE Transactions on Information Theory", "citeRegEx": "Farley and Clark,? \\Q1954\\E", "shortCiteRegEx": "Farley and Clark", "year": 1954}, {"title": "Simulation of Genetic Systems by Automatic Digital Computers", "author": ["A. Fraser"], "venue": "I. Introduction. Australian Journal of Biological Sciences", "citeRegEx": "Fraser,? \\Q1957\\E", "shortCiteRegEx": "Fraser", "year": 1957}, {"title": "Letter Recognition using Holland-style Adaptive Classifiers", "author": ["P. Frey", "D. Slate"], "venue": "Machine Learning", "citeRegEx": "Frey and Slate,? \\Q1991\\E", "shortCiteRegEx": "Frey and Slate", "year": 1991}, {"title": "Genetic Algorithms and Rule Learning in Dynamic System Control", "author": ["D. Goldberg"], "venue": "In J.J. Grefenstette (ed) Proceedings of the First International Conference on Genetic Algorithms and their Applications. Lawrence Erlbaum Associates,", "citeRegEx": "Goldberg,? \\Q1985\\E", "shortCiteRegEx": "Goldberg", "year": 1985}, {"title": "Accuracy-based Fitness Allows Similar Performance to Humans in Static and Dynamic Classification Environments", "author": ["A. Hartley"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference", "citeRegEx": "Hartley,? \\Q1999\\E", "shortCiteRegEx": "Hartley", "year": 1999}, {"title": "Adaptation in Natural and Artificial Systems", "author": ["J.H. Holland"], "venue": null, "citeRegEx": "Holland,? \\Q1975\\E", "shortCiteRegEx": "Holland", "year": 1975}, {"title": "Adaptive Algorithms for Discovering and using General Patterns in Growing Knowledge Bases", "author": ["J.H. Holland"], "venue": "International Journal of Policy Analysis and Information Systems", "citeRegEx": "Holland,? \\Q1980\\E", "shortCiteRegEx": "Holland", "year": 1980}, {"title": "Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems", "author": ["J.H. Holland"], "venue": null, "citeRegEx": "Holland,? \\Q1986\\E", "shortCiteRegEx": "Holland", "year": 1986}, {"title": "Concerning the Emergence of Tag-Mediated Lookahead in Classifier Systems", "author": ["J.H. Holland"], "venue": "Physica D", "citeRegEx": "Holland,? \\Q1990\\E", "shortCiteRegEx": "Holland", "year": 1990}, {"title": "Cognitive Systems Based in Adaptive Algorithms. In Waterman & Hayes-Roth (eds) Pattern-directed Inference Systems", "author": ["J.H. Holland", "J.H. Reitman"], "venue": null, "citeRegEx": "Holland and Reitman,? \\Q1978\\E", "shortCiteRegEx": "Holland and Reitman", "year": 1978}, {"title": "Induction: Processes of Inference, Learning and Discovery", "author": ["J.H. Holland", "K.J. Holyoak", "R.E. Nisbett", "P.R. Thagard"], "venue": null, "citeRegEx": "Holland et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Holland et al\\.", "year": 1986}, {"title": "Reusing Building Blocks of Extracted Knowledge to Solve Complex, Large-Scale Boolean Problems", "author": ["M. Iqbal", "W. Browne", "M. Zhang"], "venue": "IEEE Transactions on Evolutionary Computation", "citeRegEx": "Iqbal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Iqbal et al\\.", "year": 2013}, {"title": "Lookahead Planning and Latent Learning in a Classifier System", "author": ["R. Riolo"], "venue": "From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behaviour. MIT Press,", "citeRegEx": "Riolo,? \\Q1991\\E", "shortCiteRegEx": "Riolo", "year": 1991}, {"title": "Dynamic Planning for Classifier Systems", "author": ["G. Roberts"], "venue": "Proceedings of the 5 International Conference on Genetic Algorithms", "citeRegEx": "Roberts,? \\Q1993\\E", "shortCiteRegEx": "Roberts", "year": 1993}, {"title": "Some Studies in Machine Learning using the Game of Checkers", "author": ["A.L. Samuel"], "venue": "IBM Journal of Research and Development", "citeRegEx": "Samuel,? \\Q1959\\E", "shortCiteRegEx": "Samuel", "year": 1959}, {"title": "Some Studies in Machine Learning using the Game of Checkers. II. Recent Progress", "author": ["A.L. Samuel"], "venue": "IBM Journal of Research and Development", "citeRegEx": "Samuel,? \\Q1967\\E", "shortCiteRegEx": "Samuel", "year": 1967}, {"title": "An Experimental Analysis of Latent Learning", "author": ["J. Seward"], "venue": "Journal of Experimental Psychology", "citeRegEx": "Seward,? \\Q1949\\E", "shortCiteRegEx": "Seward", "year": 1949}, {"title": "Programming a Computer for Playing Chess", "author": ["C. Shannon"], "venue": "Philosophical Magazine", "citeRegEx": "Shannon,? \\Q1950\\E", "shortCiteRegEx": "Shannon", "year": 1950}, {"title": "A Learning Classifier System with Mutual-Information-based Fitness", "author": ["R. Smith", "M. Jiang", "J. Bacardit", "M. Stout", "N. Krasnogor", "J. Hirst"], "venue": "Evolutionary Intelligence", "citeRegEx": "Smith et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2010}, {"title": "Anticipatory Classifier Systems", "author": ["W. Stolzmann"], "venue": "In Koza et al. (eds.) Genetic Programming", "citeRegEx": "Stolzmann,? \\Q1998\\E", "shortCiteRegEx": "Stolzmann", "year": 1998}, {"title": "Comparing XCS and ZCS on Noisy Continuous-Valued Environments", "author": ["C. Stone", "L. Bull"], "venue": null, "citeRegEx": "Stone and Bull,? \\Q2005\\E", "shortCiteRegEx": "Stone and Bull", "year": 2005}, {"title": "Reinforcement Learning", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Towards Clustering with XCS", "author": ["K. Tammee", "L. Bull", "P. Ouen"], "venue": "In D. Thierens et al. (eds) GECOO-2007: Proceedings of the Genetic and Evolutionary Computation Conference. ACM Press,", "citeRegEx": "Tammee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tammee et al\\.", "year": 2007}, {"title": "Learning from Delayed Rewards", "author": ["Turing. Oxford"], "venue": "Ph.D. Thesis,", "citeRegEx": "Oxford,? \\Q1989\\E", "shortCiteRegEx": "Oxford", "year": 1989}, {"title": "Knowledge Growth in an Artificial Animal", "author": ["S.W. Corporation. Wilson"], "venue": null, "citeRegEx": "Wilson,? \\Q1985\\E", "shortCiteRegEx": "Wilson", "year": 1985}, {"title": "ZCS: A Zeroth-level Classifier System. Evolutionary Computation", "author": ["S.W. Wilson"], "venue": null, "citeRegEx": "228", "shortCiteRegEx": "228", "year": 1994}, {"title": "Classifier Fitness Based on Accuracy. Evolutionary Computation", "author": ["S.W. Wilson"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Generalization in the XCS Classifier System", "author": ["S.W. 149-76. Wilson"], "venue": "In Koza et al. (eds.) Genetic", "citeRegEx": "Wilson,? \\Q1998\\E", "shortCiteRegEx": "Wilson", "year": 1998}, {"title": "A critical review of classifier systems", "author": ["S.W. Kaufmann. Wilson", "D.E. Goldberg"], "venue": null, "citeRegEx": "Wilson and Goldberg,? \\Q1989\\E", "shortCiteRegEx": "Wilson and Goldberg", "year": 1989}], "referenceMentions": [{"referenceID": 15, "context": "The LCS formalism was introduced by John Holland [1976] and based around his more well-known invention \u2013 the Genetic Algorithm (GA)[Holland, 1975].", "startOffset": 131, "endOffset": 146}, {"referenceID": 16, "context": "Holland then revised the framework to define what would become the standard system for many years [Holland, 1980].", "startOffset": 98, "endOffset": 113}, {"referenceID": 14, "context": "The LCS formalism was introduced by John Holland [1976] and based around his more well-known invention \u2013 the Genetic Algorithm (GA)[Holland, 1975].", "startOffset": 41, "endOffset": 56}, {"referenceID": 3, "context": ", see [Butz, 2006]).", "startOffset": 6, "endOffset": 18}, {"referenceID": 24, "context": "Whilst Farley and Clark [1954] were perhaps the first to implement reinforcement learning within a computer, Holland was influenced by Arthur Samuel\u2019s seminal early work on draughts/checkers [Samuel, 1959], which itself drew upon Claude Shannon\u2019s work on chess, seemingly the first consideration of learning a value function through experience [Shannon, 1950].", "startOffset": 191, "endOffset": 205}, {"referenceID": 27, "context": "Whilst Farley and Clark [1954] were perhaps the first to implement reinforcement learning within a computer, Holland was influenced by Arthur Samuel\u2019s seminal early work on draughts/checkers [Samuel, 1959], which itself drew upon Claude Shannon\u2019s work on chess, seemingly the first consideration of learning a value function through experience [Shannon, 1950].", "startOffset": 344, "endOffset": 359}, {"referenceID": 25, "context": "Samuel [1959] described a scheme for adjusting temporally successive estimates of the end reward value from a sequence of moves (improved in [Samuel, 1967]).", "startOffset": 141, "endOffset": 155}, {"referenceID": 10, "context": "Whilst Farley and Clark [1954] were perhaps the first to implement reinforcement learning within a computer, Holland was influenced by Arthur Samuel\u2019s seminal early work on draughts/checkers [Samuel, 1959], which itself drew upon Claude Shannon\u2019s work on chess, seemingly the first consideration of learning a value function through experience [Shannon, 1950].", "startOffset": 7, "endOffset": 31}, {"referenceID": 10, "context": "Whilst Farley and Clark [1954] were perhaps the first to implement reinforcement learning within a computer, Holland was influenced by Arthur Samuel\u2019s seminal early work on draughts/checkers [Samuel, 1959], which itself drew upon Claude Shannon\u2019s work on chess, seemingly the first consideration of learning a value function through experience [Shannon, 1950]. Samuel [1959] described a scheme for adjusting temporally successive estimates of the end reward value from a sequence of moves (improved in [Samuel, 1967]).", "startOffset": 7, "endOffset": 375}, {"referenceID": 10, "context": "The suggestion that a simulated evolutionary process may prove useful in artificial systems was first made by Turing [1948], with early implementations within a computer by Fraser [1957] and Box [1957].", "startOffset": 173, "endOffset": 187}, {"referenceID": 1, "context": "The suggestion that a simulated evolutionary process may prove useful in artificial systems was first made by Turing [1948], with early implementations within a computer by Fraser [1957] and Box [1957].", "startOffset": 191, "endOffset": 202}, {"referenceID": 7, "context": "The use of overlapping generations and crowding were later adopted in the application of GAs to function optimization [DeJong, 1975].", "startOffset": 118, "endOffset": 132}, {"referenceID": 21, "context": "are close in rule-space to those in the larger, as might be expected (see [Iqbal et al., 2013] for related recent work).", "startOffset": 74, "endOffset": 94}, {"referenceID": 21, "context": "are close in rule-space to those in the larger, as might be expected (see [Iqbal et al., 2013] for related recent work). Wilson [1981] was also able to use a CS-1 based system for a camera centring control problem, which included some complex image processing to create the binary inputs to the LCS.", "startOffset": 75, "endOffset": 135}, {"referenceID": 15, "context": "A few years later Holland [1980] revised CS-1 and described what would become the standard architecture, here termed \u201cLearning Classifier System\u201d (LCS).", "startOffset": 18, "endOffset": 33}, {"referenceID": 13, "context": "noted that Holland seems not to have used the prefix \u201clearning\u201d at the time; Goldberg [1985] may have been first to add the emphasis.", "startOffset": 77, "endOffset": 93}, {"referenceID": 17, "context": "posted a message share the external reward appears to vary in the literature, being both included [Holland, 1985] and excluded [Holland, 1986].", "startOffset": 127, "endOffset": 142}, {"referenceID": 17, "context": "\u201cThus, the bucket brigade assures that early-acting, stage-setting classifiers receive credit if they (on average) make possible later, overtly rewarding acts\u201d [Holland, 1986].", "startOffset": 160, "endOffset": 175}, {"referenceID": 20, "context": "In particular, the idea that hierarchical rule associations could emerge via specific rules out-bidding more general rules in certain important situations, and extra \u201ctag\u201d regions of conditions and assertions being added would aid the formation of sequential induction (see [Holland et al., 1986] for a full treatment).", "startOffset": 274, "endOffset": 296}, {"referenceID": 28, "context": "These ideas do not feature in modern LCS (see [Smith et al., 2010] for an exception).", "startOffset": 46, "endOffset": 66}, {"referenceID": 34, "context": "The first of these, ANIMAT [Wilson, 1985], makes a number of simplifications to Holland\u2019s architecture.", "startOffset": 27, "endOffset": 41}, {"referenceID": 34, "context": "Results with ZCS indicated it was capable of good, but not optimal, performance [Wilson, 1994][Cliff & Ross, 1995]. Wilson [1994] also included a version of the offpolicy temporal difference learning algorithm Q-learning [Watkins, 1989] to some benefit.", "startOffset": 81, "endOffset": 130}, {"referenceID": 34, "context": "Results with ZCS indicated it was capable of good, but not optimal, performance [Wilson, 1994][Cliff & Ross, 1995]. Wilson [1994] also included a version of the offpolicy temporal difference learning algorithm Q-learning [Watkins, 1989] to some benefit. He also proposed to use the triggered niche GA of GOFER on top of the panmictic/global scheme described above. Bull [2005] showed the potential for disruption of the reward sharing scheme using just a niche GA in a similar LCS but no combination is known.", "startOffset": 81, "endOffset": 377}, {"referenceID": 13, "context": "Hartley [1999] showed NEWBOOLE to be competitive with XCS on a well-known set of binary classification tasks, although XCS\u2019s maintenance of a full state-action-reward map gave it an advantage in some forms of non-stationary task (see [Bull & Hurst, 2002] for discussion).", "startOffset": 0, "endOffset": 15}, {"referenceID": 12, "context": "Seemingly independently, Frey and Slate [1991] also presented a variant of BOOLE for supervised learning tasks in which they also update the correct set within [M] regardless of the output.", "startOffset": 25, "endOffset": 47}, {"referenceID": 12, "context": "Seemingly independently, Frey and Slate [1991] also presented a variant of BOOLE for supervised learning tasks in which they also update the correct set within [M] regardless of the output. Having struggled to find the correct balance of taxing and bid biasing for a letter recognition task, with reference to Holland\u2019s [1976] original ideas, they introduced the accuracy-utility system (here termed AU-BOOLE).", "startOffset": 25, "endOffset": 327}, {"referenceID": 15, "context": "That is, Holland did not seek to change the rule structure from his standard LCS to this direct form. Riolo [1991] was first to implement lookahead capabilities within LCS with his CFSC2.", "startOffset": 9, "endOffset": 115}, {"referenceID": 15, "context": "That is, Holland did not seek to change the rule structure from his standard LCS to this direct form. Riolo [1991] was first to implement lookahead capabilities within LCS with his CFSC2. He allowed the system to execute more than one cycle before providing an action, added tags along the lines Holland [1990] had suggested, and introduced an extra strength parameter to represent the predictive accuracy of a rule.", "startOffset": 9, "endOffset": 311}, {"referenceID": 23, "context": "Roberts [1993] presented a related approach within ANIMAT which maintained \u201cfollowsets\u201d, time-stamped information regarding rewards received or next states obtained after a rule had fired.", "startOffset": 0, "endOffset": 15}, {"referenceID": 23, "context": "Roberts [1993] presented a related approach within ANIMAT which maintained \u201cfollowsets\u201d, time-stamped information regarding rewards received or next states obtained after a rule had fired. The value of such rewards is factored into rule strengths. Wilson [1995] proposed altering the rule structure to contain the anticipated next", "startOffset": 0, "endOffset": 262}, {"referenceID": 26, "context": "A famous experiment with rats in a T-maze [Seward, 1949] is simulated and the results indicate similar behaviour from the ACS.", "startOffset": 42, "endOffset": 56}, {"referenceID": 26, "context": "Stolzmann [1998] presented a system in which such a rule structure is used (the expecton component termed the \u201ceffect\u201d) \u2013 the anticipatory classifier system (ACS).", "startOffset": 0, "endOffset": 17}, {"referenceID": 34, "context": "The GA originally occurred in [M] but Wilson [1998] later move it to [A] to further reduce the potential for recombining rules inappropriately, i.", "startOffset": 38, "endOffset": 52}, {"referenceID": 4, "context": "This has been studied formally and indicates the scalability of XCS\u2019s learning mechanisms [Butz et al., 2004; 2007].", "startOffset": 90, "endOffset": 115}, {"referenceID": 32, "context": "As mentioned above and shown in Figure 1, XCS and these key features have been extended beyond reinforcement learning to supervised [Bernado Mansiall & Garrell, 2003], unsupervised (clustering) [Tammee et al., 2007], function [Wilson, 2002] and model [Butz & Goldberg, 2003] learning.", "startOffset": 194, "endOffset": 215}], "year": 2014, "abstractText": "Modern Learning Classifier Systems can be characterized by their use of rule accuracy as the utility metric for the search algorithm(s) discovering useful rules. Such searching typically takes place within the restricted space of co-active rules for efficiency. This paper gives an historical overview of the evolution of such systems.", "creator": "Microsoft\u00ae Office Word 2007"}}}