{"id": "1401.3848", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Approximate Model-Based Diagnosis Using Greedy Stochastic Search", "abstract": "We propose a StochAstic Fault diagnosis AlgoRIthm, called SAFARI, which trades off guarantees of computing minimal diagnoses for computational efficiency. We empirically demonstrate, using the 74XXX and ISCAS-85 suites of benchmark combinatorial circuits, that SAFARI achieves several orders-of-magnitude speedup over two well-known deterministic algorithms, CDA* and HA*, for multiple-fault diagnoses; further, SAFARI can compute a range of multiple-fault diagnoses that CDA* and HA* cannot. We also prove that SAFARI is optimal for a range of propositional fault models, such as the widely-used weak-fault models (models with ignorance of abnormal behavior). We discuss the optimality of SAFARI in a class of strong-fault circuit models with stuck-at failure modes. By modeling the algorithm itself as a Markov chain, we provide exact bounds on the minimality of the diagnosis computed. SAFARI also displays strong anytime behavior, and will return a diagnosis after any non-trivial inference time. Finally, we show that the optimality of SAFARI is superior to those of a general-purpose model. The best approach to generating prediction algorithms is to develop two algorithms: first, we consider the algorithm as a model for all other possible problems; second, we examine the implementation of the algorithm to quantify its accuracy. These two algorithms converge on the assumption of a single Bayesian algorithm, that the algorithm will have the least posterior error rate. Finally, we explore how we model a problem by estimating how the algorithm can compute the probability of errors. We also consider the use of a combination of four different Bayesian algorithms (two for each model and one for each model): a classical general-purpose model and a general-purpose model. Each algorithm can produce only one prediction model per problem; all algorithms can calculate multiple models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 16 Jan 2014 04:57:50 GMT  (468kb)", "http://arxiv.org/abs/1401.3848v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["alexander feldman", "gregory provan", "arjan van gemund"], "accepted": false, "id": "1401.3848"}, "pdf": {"name": "1401.3848.pdf", "metadata": {"source": "CRF", "title": "Approximate Model-Based Diagnosis Using Greedy Stochastic Search", "authors": ["Alexander Feldman", "Gregory Provan", "Arjan van Gemund"], "emails": ["a.b.feldman@tudelft.nl", "g.provan@cs.ucc.ie", "a.j.c.vangemund@tudelft.nl"], "sections": [{"heading": "1. Introduction", "text": "Model-Based Diagnosis (MBD) is an area of artificial intelligence that uses a system model, together with observations about system behavior, to isolate sets of faulty components (diagnoses) that explain the observed behavior according to some minimality criterion. The standard MBD formalization (Reiter, 1987) frames a diagnostic problem in terms of a set of logical clauses that include mode-variables describing the nominal and fault status of system components; from this the diagnostic status of the system can be computed given an observation of the system\u2019s sensors. MBD provides a sound and complete approach to enumerating multiple-fault diagnoses, and exact algorithms can guarantee finding a diagnosis optimal with respect to the number of faulty components, probabilistic likelihood, etc.\nThe biggest challenge (and impediment to industrial deployment) is the computational complexity of the MBD problem. The MBD problem of determining if there exists a diagnosis with at most k faults is NP-hard for the arbitrary propositional models we consider in this article (Bylander, Allemang, Tanner, & Josephson, 1991; Friedrich, Gottlob, & Nejdl, 1990). Computing the set of all diagnoses is harder still, since there are possibly exponen-\nc\u00a92010 AI Access Foundation. All rights reserved.\ntially many such diagnoses. Since almost all proposed MBD algorithms have been complete and exact, with some authors proposing possible trade-offs between completeness and faster consistency checking by employing methods such as BCP (Williams & Ragno, 2007), the complexity problem still remains a major challenge to MBD.\nTo overcome this complexity problem, we propose a novel approximation approach for multiple-fault diagnosis, based on a stochastic algorithm. Safari (StochAstic Fault diagnosis AlgoRIthm) sacrifices guarantees of optimality, but for diagnostic systems in which faults are described in terms of an arbitrary deviation from nominal behavior, Safari can compute diagnoses several orders of magnitude faster than competing algorithms.\nOur contributions are as follows. (1) This paper introduces an approximation algorithm for computing diagnoses within an MBD framework, based on a greedy stochastic algorithm. (2) We show that we can compute minimal-cardinality diagnoses for weak fault models in polynomial time (calling an incomplete SAT-solver that implements Boolean Constraint Propagation1 (BCP) only), and that more general frameworks (such as a sub-class of strong fault models) are also amenable to this class of algorithm. (3) We model Safari search as a Markov chain to show the performance and optimality trade-offs that the algorithm makes. (4) We apply this algorithm to a suite of benchmark combinatorial circuits, demonstrating order-of-magnitude speedup over two state-of-the-art deterministic algorithms, CDA\u2217 and HA\u2217, for multiple-fault diagnoses. (5) We compare the performance of Safari against a range of Max-SAT algorithms for our benchmark problems. Our results indicate that, whereas the search complexity for the deterministic algorithms tested increases exponentially with fault cardinality, the search complexity for this stochastic algorithm appears to be independent of fault cardinality. Safari is of great practical significance, as it can compute a large fraction of minimal-cardinality diagnoses for discrete systems too large or complex to be diagnosed by existing deterministic algorithms."}, {"heading": "2. Technical Background", "text": "Our discussion continues by formalizing some MBD notions. This paper uses the traditional diagnostic definitions (de Kleer & Williams, 1987), except that we use propositional logic terms (conjunctions of literals) instead of sets of failing components.\nCentral to MBD, a model of an artifact is represented as a propositional formula over some set of variables. Discerning two subsets of these variables as assumable and observable2 variables gives us a diagnostic system.\nDefinition 1 (Diagnostic System). A diagnostic system DS is defined as the triple DS = \u3008SD,COMPS,OBS\u3009, where SD is a propositional theory over a set of variables V , COMPS \u2286 V , OBS \u2286 V , COMPS is the set of assumables, and OBS is the set of observables.\nThroughout this paper we will assume that OBS \u2229 COMPS = \u2205 and SD 6|=\u22a5. Not all propositional theories used as system descriptions are of interest to MBD. Diagnostic systems can be characterized by a restricted set of models, the restriction making the problem\n1. With formulae in Conjunctive Normal Form (CNF), BCP is implemented through the unit resolution rule. 2. In the MBD literature the assumable variables are also referred to as \u201ccomponent\u201d, \u201cfailure-mode\u201d, or \u201chealth\u201d variables. Observable variables are also called \u201cmeasurable\u201d, or \u201ccontrol\u201d variables.\nof computing diagnosis amenable to algorithms like the one presented in this paper. We consider two main classes of models.\nDefinition 2 (Weak-Fault Model). A diagnostic system DS = \u3008SD,COMPS,OBS\u3009 belongs to the class WFM iff for COMPS = {h1, h2, . . . , hn}, SD is equivalent to (h1 \u21d2 F1)\u2227(h2 \u21d2 F2) \u2227 . . . \u2227 (hn \u21d2 Fn) and COMPS \u2229 V\n\u2032 = \u2205, where V \u2032 is the set of all variables appearing in the propositional formulae F1, F2, . . . , Fn.\nNote the conventional selection of the sign of the \u201chealth\u201d variables h1, h2, . . . hn. Alternatively, negative literals, e.g., f1, f2, . . . , fn can be used to express faults, in which case a weak-fault model is in the form (\u00acf1 \u21d2 F1)\u2227 . . .\u2227 (\u00acfn \u21d2 Fn). Other authors use \u201cab\u201d for abnormal or \u201cok\u201d for healthy.\nWeak-fault models are sometimes referred to as models with ignorance of abnormal behavior (de Kleer, Mackworth, & Reiter, 1992), or implicit fault systems. Alternatively, a model may specify faulty behavior for its components. In the following definition, with the aim of simplifying the formalism throughout this paper, we adopt a slightly restrictive representation of faults, allowing only a single fault-mode per assumable variable. This can be easily generalized by introducing multi-valued logic or suitable encodings (Hoos, 1999).\nDefinition 3 (Strong-Fault Model). A diagnostic system DS = \u3008SD,COMPS,OBS\u3009 belongs to the class SFM iff SD is equivalent to (h1 \u21d2 F1,1) \u2227 (\u00ach1 \u21d2 F1,2) \u2227 . . . \u2227 (hn \u21d2 Fn,1) \u2227 (\u00achn \u21d2 Fn,2) such that 1 \u2264 i, j \u2264 n, k \u2208 {1, 2}, {hi} \u2286 COMPS, F{j,k} is a propositional formula, and none of hi appears in Fj,k.\nMembership testing for the WFM and SFM classes can be performed efficiently in many cases, for example, when a model is represented explicitly as in Def. 2 or Def. 3."}, {"heading": "2.1 A Running Example", "text": "We will use the Boolean circuit shown in Fig. 1 as a running example for illustrating all the notions and algorithms in this paper. The subtractor, shown there, consists of seven components: an inverter, two or-gates, two xor-gates, and two and-gates. The expression h \u21d2 (o \u21d4 \u00aci) models the normative (healthy) behavior of an inverter, where the variables i, o, and h represent input, output and health respectively. Similarly, an and-gate is modeled as h \u21d2 [o \u21d4 (i1 \u2227 i2)] and an or-gate by h \u21d2 [o \u21d4 (i1 \u2228 i2)]. Finally, an xor-gate is specified as h \u21d2 [o \u21d4 \u00ac (i1 \u21d4 i2)].\nThe above propositional formulae are copied for each gate in Fig. 1 and their variables renamed in such a way as to properly connect the circuit and disambiguate the assumables, thus obtaining a propositional formula for the Boolean subtractor, given by:\nSDw = {h1 \u21d2 [i \u21d4 \u00ac (y \u21d4 p)]} \u2227 {h2 \u21d2 [d \u21d4 \u00ac (x \u21d4 i)]} \u2227 [h3 \u21d2 (j \u21d4 y \u2228 p)]\u2227 \u2227 [h4 \u21d2 (m \u21d4 l \u2227 j)] \u2227 [h5 \u21d2 (b \u21d4 m \u2228 k)] \u2227 [h6 \u21d2 (x \u21d4 \u00acl)]\u2227 \u2227 [h7 \u21d2 (k \u21d4 y \u2227 p)]\n(1)\nA strong-fault model for the Boolean circuit shown in Fig. 1 is constructed by assigning fault-modes to the different gate types. We will assume that, when malfunctioning, the output of an xor-gate has the value of one of its inputs, an or-gate can be stuck-at-one,\nan and-gate can be stuck-at-zero, and an inverter behaves like a buffer. This gives us the following strong-fault model formula for the Boolean subtractor circuit:\nSDs = SDw \u2227 [\u00ach1 \u21d2 (i \u21d4 y)] \u2227 [\u00ach2 \u21d2 (d \u21d4 x)] \u2227 (\u00ach3 \u21d2 j)\u2227 \u2227 (\u00ach4 \u21d2 \u00acm) \u2227 (\u00ach5 \u21d2 b) \u2227 [\u00ach6 \u21d2 (x \u21d4 l)] \u2227 (\u00ach7 \u21d2 \u00ack)\n(2)\nFor both models (SDs and SDw), the set of assumable variables is COMPS = {h1, h2, . . . , h7} and the set of observable variables is OBS = {x, y, p, d, b}."}, {"heading": "2.2 Diagnosis and Minimal Diagnosis", "text": "The traditional query in MBD computes terms of assumable variables which are explanations for the system description and an observation.\nDefinition 4 (Health Assignment). Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009, an assignment \u03c9 to all variables in COMPS is defined as a health assignment.\nA health assignment \u03c9 is a conjunction of propositional literals. In some cases it is convenient to use the set of negative or positive literals in \u03c9. These two sets are denoted as Lit\u2212(\u03c9) and Lit+(\u03c9), respectively.\nIn our example, the \u201call nominal\u201d assignment is \u03c91 = h1 \u2227 h2 \u2227 . . . \u2227 h7. The health assignment \u03c92 = h1\u2227h2\u2227h3\u2227\u00ach4\u2227h5\u2227h6\u2227\u00ach7 means that the two and-gates from Fig. 1 are malfunctioning. What follows is a formal definition of consistency-based diagnosis.\nDefinition 5 (Diagnosis). Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009, an observation \u03b1, which is an instantiation of some variables in OBS, and a health assignment \u03c9, \u03c9 is a diagnosis iff SD \u2227 \u03b1 \u2227 \u03c9 6|=\u22a5.\nTraditionally, other authors (de Kleer &Williams, 1987) arrive at minimal diagnosis by computing a minimal hitting set of the minimal conflicts (broadly, minimal health assignments incompatible with the system description and the observation), while this paper makes no use of conflicts, hence the equivalent, direct definition above.\nThere is a total of 96 possible diagnoses given SDw and an observation \u03b11 = x\u2227 y \u2227 p\u2227 b \u2227 \u00acd. Example diagnoses are \u03c93 = \u00ach1 \u2227 h2 \u2227 . . . \u2227 h7 and \u03c94 = h1 \u2227 \u00ach2 \u2227 h3 \u2227 . . . \u2227 h7. Trivially, given a weak-fault model, the \u201call faulty\u201d health assignment (in our example\n\u03c9a = \u00ach1\u2227 . . .\u2227\u00ach7) is a diagnosis for any instantiation of the observable variables in OBS (cf. Def. 2).\nIn the analysis of our algorithm we need the opposite notion of diagnosis, i.e., health assignments inconsistent with a model and an observation. In the MBD literature these assignments are usually called conflicts. Conflicts, however, do not necessarily instantiate all variables in COMPS. As in this paper we always use full health instantiations, the use of the term conflict is avoided to prevent confusion.\nIn the MBD literature, a range of types of \u201cpreferred\u201d diagnosis has been proposed. This turns the MBD problem into an optimization problem. In the following definition we consider the common subset-ordering.\nDefinition 6 (Minimal Diagnosis). A diagnosis \u03c9\u2286 is defined as minimal, if no diagnosis \u03c9\u0303\u2286 exists such that Lit\u2212(\u03c9\u0303\u2286) \u2282 Lit\u2212(\u03c9\u2286).\nConsider the weak-fault model SDw of the circuit shown in Fig. 1 and an observation \u03b12 = \u00acx \u2227 y \u2227 p \u2227 \u00acb \u2227 d. In this example, two of the minimal diagnoses are \u03c9 \u2286 5 = \u00ach1 \u2227 h2 \u2227 h3 \u2227 h4 \u2227\u00ach5 \u2227 h6 \u2227 h7 and \u03c9 \u2286 6 = \u00ach1 \u2227 h2 \u2227 . . .\u2227 h5 \u2227\u00ach6 \u2227\u00ach7. The diagnosis \u03c97 = \u00ach1 \u2227\u00ach2 \u2227 h3 \u2227 h4 \u2227\u00ach5 \u2227 h6 \u2227 h7 is non-minimal as the negative literals in \u03c9 \u2286 5 form a subset of the negative literals in \u03c97. Note that the set of all minimal diagnoses characterizes all diagnoses for a weak-fault model, but that does not hold in general for strong-fault models (de Kleer et al., 1992). In the latter case, faulty components may \u201cexonerate\u201d each other, resulting in a health assignment containing a proper superset of the negative literals of another diagnosis not to be a diagnosis. In our example, given SDs and \u03b13 = \u00acx \u2227 \u00acy \u2227 \u00acp \u2227 b \u2227 \u00acd, it follows that \u03c9\u22868 = h1 \u2227 h2 \u2227 \u00ach3 \u2227 h4 \u2227 . . . \u2227 h7 is a diagnosis, but \u03c99 = h1 \u2227 h2 \u2227 \u00ach3 \u2227 \u00ach4 \u2227 . . . \u2227 h7 is not a diagnosis, despite the fact that the negative literals in \u03c99 form a superset of the negative literals in \u03c9\u22868 .\nDefinition 7 (Number of Minimal Diagnoses). Let the set \u2126\u2286(SD \u2227 \u03b1) contain all minimal diagnoses of a system description SD and an observation \u03b1. The number of minimal diagnoses, denoted as |\u2126\u2286(SD \u2227 \u03b1)|, is defined as the cardinality of \u2126\u2286(SD \u2227 \u03b1).\nContinuing our running example, |\u2126\u2286(SDw\u2227\u03b12)| = 8 and |\u2126 \u2286(SDs\u2227\u03b13)| = 2. The number of non-minimal diagnoses of SDw \u2227 \u03b12 is 61.\nDefinition 8 (Cardinality of a Diagnosis). The cardinality of a diagnosis, denoted as |\u03c9|, is defined as the number of negative literals in \u03c9.\nDiagnosis cardinality gives us another partial ordering: a diagnosis is defined as minimal cardinality iff it minimizes the number of negative literals.\nDefinition 9 (Minimal-Cardinality Diagnosis). A diagnosis \u03c9\u2264 is defined as minimalcardinality if no diagnosis \u03c9\u0303\u2264 exists such that |\u03c9\u0303\u2264| < |\u03c9\u2264|.\nThe cardinality of a minimal-cardinality diagnosis computed from a system description SD and an observation \u03b1 is denoted as MinCard(SD\u2227\u03b1). For our example model SDw and an observation \u03b14 = x \u2227 y \u2227 p \u2227 \u00acb \u2227 \u00acd , it follows that MinCard(SDw \u2227 \u03b14) = 2. Note that in this case all minimal diagnoses are also minimal-cardinality diagnoses.\nA minimal cardinality diagnosis is a minimal diagnosis, but the opposite need not hold. In the general case, there are minimal diagnoses which are not minimal-cardinality diagnoses. Consider the example SDw and \u03b12 given earlier in this section, and the two resulting minimal diagnoses \u03c9\u22865 and \u03c9 \u2286 6 . From these two, only \u03c9 \u2286 5 is a minimal-cardinality diagnosis.\nDefinition 10 (Number of Minimal-Cardinality Diagnoses). Let the set \u2126\u2264(SD \u2227 \u03b1) contain all minimal-cardinality diagnoses of a system description SD and an observation \u03b1. The number of minimal-cardinality diagnoses, denoted as |\u2126\u2264(SD \u2227 \u03b1)|, is defined as the cardinality of \u2126\u2264(SD \u2227 \u03b1).\nComputing the number of minimal-cardinality diagnoses for the running example results in |\u2126\u2264(SDw \u2227 \u03b12)| = 2, |\u2126 \u2264(SDs \u2227 \u03b13)| = 2, and |\u2126 \u2264(SDw \u2227 \u03b14)| = 4."}, {"heading": "2.3 Converting Propositional Formulae to Clausal Form", "text": "Our approach is related to satisfiability, and Safari uses a SAT solver. SAT solvers commonly accept their input in Conjunctive Normal Form (CNF), although there exist SAT solvers that work directly on propositional formulae (Thiffault, Bacchus, & Walsh, 2004). Converting a propositional formula to CNF can be done with (Tseitin, 1983) or without (Forbus & de Kleer, 1993) the introduction of intermediate variables. In both cases important structural information is lost, which may lead to performance degradation when checking if a formula is consistent or when computing a solution.\nLemma 1. A fault-model SD = F1 \u2227 F2 \u2227 . . . \u2227 Fn (SD \u2208 WFM or SD \u2208 SFM) with n = |COMPS| component variables can be converted to CNF in time O(|COMPS|\u03b6) where \u03b6 is the time for converting the largest subformula Fi (1 \u2264 i \u2264 n) to CNF.\nProof (Sketch). The conversion of SD to CNF can be done by (1) converting each subformula Fi to CNF and (2) concatenating the resulting CNFs in the final CNF equivalent of SD. The complexity of (1) is O(n) while the complexity of (2) is, in the worst-case, O(2m) < \u03b6, where m is the largest number of variables in a subformula Fi. As a result, the total time for converting SD is dominated by \u03b6 and it is linear in |COMPS|.\nLemma 1 is useful in the cases in which each subformula Fi is small. This is the case in many practical situations where SD is composed of small component models. This is also the case with our experimental benchmark (cf. Sec. 6) where the model of a combinational circuit is the conjunction of fault models of simple logic gates (x-bit and-gates, typically x < 10, xor-gates, etc.). Ideally, Safari would use a non-CNF SAT solver, but for practical reasons we have constrained our reasoning to diagnostic models with concise CNF encodings.\nConsider, for example, the formula (x1 \u2227 y1) \u2228 (x2 \u2227 y2) \u2228 \u00b7 \u00b7 \u00b7 \u2228 (xn \u2227 yn), which is in Disjunctive Normal Form3 (DNF) and, converted to CNF, has 2n clauses. Although similar examples of propositional formulae having exponentially many clauses in their CNF representations are easy to find, they are artificial and are rarely encountered in MBD. Furthermore, the Boolean circuits with which we have tested the performance of Safari do not show exponential blow-up when converted to CNF.\n3. Note that all DNF formulae are also propositional formulae."}, {"heading": "2.4 Complexity of Diagnostic Inference", "text": "This section discusses the complexity of the problems in which we are interested, namely the problem of computing a single or the set of all minimal diagnoses, using two minimality criteria, subset-minimality (\u2286) and cardinality-minimality (\u2264). We assume as input a CNF formula defined over a variable set V , of which \u03b3 = |COMPS| are assumable (or fault) variables. Table 1 introduces the notation that we use to define these 4 types of diagnosis.\nThe complexity of computing the set of all diagnoses is harder than computing a single diagnosis, since the number of diagnoses is, in the worst case, exponential in the input size (number of components). This problem is bounded from below by the problem of counting the number of diagnoses. This problem has been shown to be #co-NP -Complete (Hermann & Pichler, 2007).\nIf we restrict our clauses to be Horn or definite Horn, then we can reduce the complexity of the problems that we are solving, at the expense of decreased model expressiveness. Under a Horn-clause restriction, for SD \u2208 WFM, determining if a first minimal diagnosis exists is in P . Under the same restriction, for SD \u2208 SFM, deciding if a first minimal diagnosis exists is NP-hard (Friedrich et al., 1990). In both cases (SD \u2208 WFM,SFM) deciding if a next diagnosis exists is NP-hard.\nThe diagnosis problems of interest in this article are intractable in the worst-case. The complexity of a closely-related problem, Propositional Abduction Problems (PAPs), has been studied by Eiter and Gottlob (1995). They show that for a propositional PAP, the problem of determining if a solution exists is \u03a3P2 -complete. Computing a minimal diagnosis is a search problem, and hence it is more difficult to pose a decision question for proving complexity results. Consequently, one can just note that computing a diagnosis minimal with respect to \u2286 / \u2264 requires O(log |COMPS|) calls to an NP oracle (Eiter & Gottlob, 1995), asking the oracle at each step if a diagnosis containing at most k faulty components exists.\nResults on abduction problems indicate that the task of approximate diagnosis is intractable. Roth (1996) has addressed the problems of abductive inference, and of approximating such inference. Roth focuses on counting the number of satisfying assignments for a range of AI problems, including some instances of PAPs. In addition, Roth shows that approximating the number of satisfying assignments for these problems is intractable.\nAbdelbar (2004) has studied the complexity of approximating Horn abduction problems, showing that even for a particular Horn restriction of the propositional problem of interest, the approximation problem is intractable. In particular, for an abduction problem with costs assigned to the assumables (which can be used to model the preference-ordering \u2264),\nhe has examined the complexity of finding the Least Cost Proof (LCP) for the evidence (OBS), where the cost of a proof is taken to be the sum of the costs of all hypotheses that must be assumed in order to complete the proof. For this problem he has shown that it is NP -hard to approximate an LCP within a fixed ratio r of the cost of an optimal solution, for any r < 0.\nSafari approximates the intractable problems denoted in Table 1. We show that for WFM, Safari can efficiently compute a single diagnosis that is minimal under \u2286 by using a satisfiability oracle. For SD \u2208 SFM, Safari generates a sound but possibly sub-optimal diagnosis (or set of diagnoses). We have referred to papers indicating that it is intractable to approximate, within a fixed ratio, a minimal diagnosis. In the following, we adopt a stochastic approach that cannot provide fixed-ratio guarantees. However, Safari trades off optimality for efficiency and can compute most diagnoses with high likelihood."}, {"heading": "3. Stochastic MBD Algorithm", "text": "In this section we discuss an algorithm for computing multiple-fault diagnoses using stochastic search."}, {"heading": "3.1 A Simple Example (Continued)", "text": "Consider the Boolean subtractor shown in Fig. 1, its weak-fault model SDw given by (1), and the observation \u03b14 from the preceding section. The four minimal diagnoses associated to SDw\u2227\u03b14 are: \u03c91 = \u00ach1\u2227h2\u2227h3\u2227h4\u2227\u00ach5\u2227h6\u2227h7, \u03c92 = h1\u2227\u00ach2\u2227h3\u2227h4\u2227\u00ach5\u2227h6\u2227h7, \u03c93 = \u00ach1 \u2227 h2 \u2227 . . . \u2227 h6 \u2227 \u00ach7, and \u03c94 = h1 \u2227 \u00ach2 \u2227 h3 \u2227 . . . \u2227 h6 \u2227 \u00ach7.\nA na\u0308\u0131ve deterministic algorithm would check the consistency of all the 2|COMPS| possible health assignments for a diagnostic problem, 128 in the case of our running example. Furthermore, most deterministic algorithms first enumerate health assignments of small cardinality but with high a priori probability, which renders these algorithms impractical in situations when the minimal diagnosis is of a higher cardinality. Such performance is not surprising even when using state-of-the art MBD algorithms which utilize, for example conflict learning, or partial compilation, considering the bad worst-case complexity of finding all minimal diagnoses (cf. Sec. 2.4).\nIn what follows, we will show a two-step diagnostic process that requires fewer consistency checks. The first step involves finding a random non-minimal diagnosis as a starting point (cf. Sec. 3.2 for details on computing random SAT solutions with equal likelihood). The second step attempts to minimize the fault cardinality of this diagnosis by repeated modification of the diagnosis.\nThe first step is to find one random, possibly non-minimal diagnosis of SDw \u2227 \u03b14. Such a diagnosis we can obtain from a classical DPLL solver after modifying it in two ways: (1) not only determine if the instance is satisfiable but also extract the satisfying solution and (2) find a random satisfiable solution every time the solver is invoked. Both modifications are trivial, as DPLL solvers typically store their current variable assignments and it is easy to choose a variable and value randomly (according to a uniform distribution) instead of deterministically when branching. The latter modification may possibly harm a DPLL variable or value selection heuristics, but later in this paper we will see that this is of no\nconcern for the type of problems we are considering as diagnostic systems are typically underconstrained.\nIn the subtractor example we call the DPLL solver with SDw \u2227 \u03b14 as an input and we consider the random solution (and obviously a diagnosis) \u03c95 = \u00ach1 \u2227 h2 \u2227 \u00ach3 \u2227 h4 \u2227 h5 \u2227 \u00ach6\u2227\u00ach7 (|\u03c95| = 4). In the second step of our stochastic algorithm, we will try to minimize \u03c95 by repetitively choosing a random negative literal, \u201cflipping\u201d its value to positive (thus obtaining a candidate with a smaller number of faults), and calling the DPLL solver. If the new candidate is a diagnosis, we will try to improve further this newly discovered diagnosis, otherwise we will mark the attempt a \u201cfailure\u201d and choose another negative literal. After some constant number of \u201cfailures\u201d (two in this example), we will terminate the search and will store the best diagnosis discovered so far in the process.\nAfter changing the sign of \u00ach7 in \u03c95 we discover that the new health assignment is not consistent with SDw \u2227 \u03b14, hence it is not a diagnosis and we discard it. Instead, the algorithm attempts changing \u00ach6 to h6 in \u03c95, this time successfully obtaining a new diagnosis \u03c96 = \u00ach1 \u2227 h2 \u2227 \u00ach3 \u2227 h4 \u2227 h5 \u2227 h6 \u2227 \u00ach7 of cardinality 3. Next the algorithm tries to find a diagnosis of even smaller cardinality by randomly choosing \u00ach1 and \u00ach7 in \u03c96, respectively, and trying to change their sign, but both attempts return an inconsistency. Hence the \u201cclimb\u201d is aborted and \u03c96 is stored as the current best diagnosis.\nRepeating the process from another random initial DPLL solution, gives us a new diagnosis \u03c97 = \u00ach1 \u2227 \u00ach2 \u2227 h3 \u2227 \u00ach4 \u2227 h5 \u2227 h6 \u2227 \u00ach7. Changing the sign of \u00ach7, again, leads to inconsistency, but the next two \u201cflips\u201d (of \u00ach4 and \u00ach2) lead to a double-fault diagnosis \u03c98 = \u00ach1 \u2227 h2 \u2227 . . . \u2227 h6 \u2227 \u00ach7. The diagnosis \u03c98 can not be improved any further as it is minimal. Hence the next two attempts to improve \u03c98 fail and \u03c98 is stored in the result.\nThis process is illustrated in Fig. 2, the search for \u03c96 is on the left and for \u03c98 on the right. Gates which are shown in solid black are \u201csuspected\u201d as faulty when the health assignment they participate in is tested for consistency, and inconsistent candidates are crossed-out.\nLet us consider the result. We have found two diagnoses: \u03c96 and \u03c98, where \u03c96 is not a minimal diagnosis. This we have done at the price of 11 calls to a DPLL subroutine. The suboptimal diagnosis \u03c96 is of value as its cardinality is near the one of a minimal diagnosis. Hence we have demonstrated a way to find an approximation of all minimal diagnoses, while drastically reducing the number of consistency checks in comparison to a deterministic algorithm, sacrificing optimality. Next we will formalize our experience into an algorithm, the behavior of which we will analyze extensively in the section that follows.\nDiagnosing a strong-fault model is known to be strictly more difficult than a weak-fault model (Friedrich et al., 1990). In many diagnostic instances this problem is alleviated by the fact that there exist, although without a guarantee, continuities in the diagnostic search space similar to the one in the weak-fault models. Let us discuss the process of finding a minimal diagnosis of the subtractor\u2019s strong-fault model SDs and the observation \u03b12 (both from Sec. 2.1).\nThe six distinct diagnoses \u03c99, . . . , \u03c914 of SDs and \u03b12 are shown in Fig. 3. Of these only \u03c99 and \u03c910 are minimal such that |\u03c99| = |\u03c910| = 3. It is visible in Fig. 3 that in all diagnoses component variables h2 and h5 are false, while h1 and h7 are true (healthy). Hence, any satisfying assignment of SDs \u2227 \u03b12 would contain h1 \u2227 \u00ach2 \u2227 \u00ach5 \u2227 h7. Starting from the maximal-cardinality diagnosis \u03c914, we must \u201cflip\u201d the variables h3, h4, and h6 in order to reach the two minimal diagnoses. The key insight is that, as shown in Fig. 3, this is always\npossible by \u201cflipping\u201d a single literal at a time from health to faulty and receiving another consistent assignment (diagnosis).\nIn what follows we will formalize our experience so far in a stochastic algorithm for finding minimal diagnoses."}, {"heading": "3.2 A Greedy Stochastic Algorithm", "text": "Algorithm 1 shows the pseudocode of Safari.\nAlgorithm 1 Safari: A greedy stochastic hill climbing algorithm for approximating the set of minimal diagnoses\n1: function Safari(DS, \u03b1,M,N) returns a trie\ninputs: DS = \u3008SD,COMPS,OBS\u3009, diagnostic system \u03b1, term, observation M , integer, climb restart limit N , integer, number of tries local variables: SDcnf , CNF m,n, integers \u03c9, \u03c9\u2032, terms R, set of terms, result\n2: SDcnf \u2190 WffToCNF(SD) 3: for n = 1, 2, . . . , N do 4: \u03c9 \u2190 RandomDiagnosis(SDcnf , \u03b1) \u22b2 Get a random SAT solution. 5: m \u2190 0 6: while m < M do 7: \u03c9\u2032 \u2190 ImproveDiagnosis(\u03c9) \u22b2 Flip an \u201cunflipped\u201d health variable. 8: if SDcnf \u2227 \u03b1 \u2227 \u03c9\n\u2032 6|=\u22a5 then \u22b2 Consistency check. 9: \u03c9 \u2190 \u03c9\u2032\n10: m \u2190 0 11: else 12: m \u2190 m+ 1 13: end if 14: end while 15: unless IsSubsumed(R,\u03c9) then 16: AddToTrie(R,\u03c9) 17: RemoveSubsumed(R,\u03c9) 18: end unless 19: end for 20: return R 21: end function\nSafari accepts two input parameters: M and N . There are N independent searches that start from randomly generated starting points. The algorithm tries to improve the cardinality of the initial diagnoses (while preserving their consistency) by randomly \u201cflipping\u201d fault literals. The change of a sign of literal is done in one direction only: from faulty to healthy. Each attempt to find a minimal diagnosis terminates after M unsuccessful attempts to \u201cimprove\u201d the current diagnosis stored in \u03c9. Thus, increasing M will lead to a better exploration of the search space and, possibly, to diagnoses of lower cardinality, while decreasing it will improve the overall speed of the algorithm.\nSafari uses a number of utility functions. WffToCNF converts the propositional formula in SD to CNF (cf. Sec 2.3). The ImproveDiagnosis subroutine takes a term \u03c9 as an argument and changes the sign of a random negative literal in \u03c9. If there are no negative literals, the function returns its original argument.\nThe implementation of RandomDiagnosis uses a modified DPLL solver returning a random SAT solution of SD\u2227\u03b1. Consider the original DPLL algorithm (Davis, Logemann, & Loveland, 1962) without the unit resolution rule. One can show that if, in the event of branching, the algorithm chooses unassigned variables and their polarity with equal probability, the DPLL algorithm is equally likely to compute any satisfiable solution (if such exists). Note that the order in which variables are assigned does not matter. Of course, the DPLL algorithm may end-up with a partial assignment, i.e., some of the variables are \u201cdon\u2019t care\u201d. This is not a problem because the partial assignment can be extended to a full satisfiable assignment by randomly choosing the signs of the unassigned variables from a uniform distribution. Taking into consideration the unit resolution rule, does not change the likelihood of the modified DPLL solver finding a particular solution because it only changes the order in which variables are assigned. A formal proof that this modified DPLL solver computes a SAT assignment with equal probability is beyond the scope of this paper, but the idea is to build a probabilistic model of the progress of the DPLL solver. This probabilistic model is a balanced tree where nodes iterate between branching and performing unit resolution (assigning values to zero or more unit clauses). As the branching probability is set to be equal and all leaf nodes (SAT solutions) are at equal depth, one can show the equal likelihood of arriving to any SAT solution. As most up-to-date SAT solvers are based on DPLL, creating a randomized DPLL solver that computes any satisfiable solution with equal probability is not difficult. Of course, random polarity decisions may effect negatively branching heuristics (Marques-Silva, 1999) but such analysis is also beyond the scope of this paper.\nSimilar to deterministic methods for MBD, Safari uses a SAT-based procedure for checking the consistency of SD\u2227\u03b1\u2227\u03c9. To increase the implementation efficiency of Safari, we combine a BCP-based LTMS engine (McAllester, 1990) and a full-fledged DPLL solver in two-stage consistency checking. Experimentation shows that combining LTMS and DPLL in such a way allows an order-of-magnitude Safari speed-up compared to pure DPLL, while the soundness and completeness properties of consistency checking are preserved.\nWe have implemented the two-stage consistency checking as follows. First, Safari calls a BCP-based LTMS (Forbus & de Kleer, 1993) to check if SD \u2227 \u03b1 \u2227 \u03c9 |=\u22a5. If the result is UNSAT then the candidate \u03c9 is not a diagnosis.4 If the LTMS result is not UNSAT, it means that the consistency of the candidate is unknown and a call to a complete DPLL engine is needed. For the full DPLL checking we use POSIT (Freeman, 1995) or MiniSat (Ee\u0301n & So\u0308rensson, 2003).\nSafari benefits from the two-stage SAT procedure because a typical MBD instance involves many consistency checks (O(|COMPS|2) for N = 1,M = |COMPS|). As SD \u2227 \u03b1 does not change during the search and each time only a small number of assumption clauses have to be updated, the incremental nature of LTMS greatly improves the search efficiency. Even though the DPLL running time per instance is the same as LTMS (DPLL performs BCP when doing unit propagation), DPLL construction is expensive and should be avoided when possible. DPLL initialization is typically slow as it involves building data structures for clauses and variables, counting literals, initializing conflict databases, etc. On the other hand, our implementation of LTMS is both incremental (does not have to be reinitialized\n4. It can be shown that if a BCP consistency check of SD \u2227 \u03b1 \u2227 \u03c9 returns UNSAT, then the formula is UNSAT (the opposite is not necessarily true).\nbefore each consistency check) and efficient as it maintains only counters for each clause. Each counter keeps the number of unassigned literals. Assigning a value to a variable requires decrementing some or all of the clause counters. If a counter becomes zero, a contradiction handler is signaled.\nThere is no guarantee that two diagnostic searches, starting from random diagnoses, would not lead to the same minimal diagnosis. To prevent this, we store the generated diagnoses in a trie R (Forbus & de Kleer, 1993), from which it is straightforward to extract the resulting diagnoses by recursively visiting its nodes. A diagnosis \u03c9 is added to the trie R by the function AddToTrie, iff no subsuming diagnosis is contained in R (the IsSubsumed subroutine checks on that condition). After adding a diagnosis \u03c9 to the resulting trie R, all diagnoses contained in R and subsumed by \u03c9 are removed by a call to RemoveSubsumed."}, {"heading": "3.3 Basic Properties of the Greedy Stochastic Search", "text": "Before we continue with the topics of completeness and optimality, we show that Safari is sound, i.e., it returns diagnoses only.\nLemma 2 (Soundness). Safari is sound.\nProof (Sketch). The consistency check in line 8 of Alg. 1 guarantees that only terms \u03c9 for which it holds that SD \u2227 \u03b1 \u2227 \u03c9 6|=\u22a5 will be added to the result set R. According to Def. 5 these terms \u03c9 are diagnoses.\nOne of the key factors in the success of the proposed algorithm is the exploitation of the continuity of the search-space of diagnosis models, where by continuity we mean that we can monotonically reduce the cardinality of a non-minimal diagnosis. Through the exploitation of this continuity property, Safari can be configured to guarantee finding a minimal diagnosis in weak fault models in a polynomial number of calls to a satisfiability oracle.\nThe hypothesis which comes next is well studied in prior work (de Kleer et al., 1992), as it determines the conditions under which minimal diagnoses represent all diagnoses of a model and an observation. This paper is interested in the hypothesis from the computational viewpoint: it defines a class of models for which it is possible to establish a theoretical bound on the optimality and performance of Safari.\nHypothesis 1 (Minimal Diagnosis Hypothesis). Let DS = \u3008SD,COMPS,OBS\u3009 be a diagnostic system and \u03c9\u2032 a diagnosis for an arbitrary observation \u03b1. The Minimal Diagnosis Hypothesis (MDH) holds in DS iff for any health assignment \u03c9 such that Lit\u2212(\u03c9) \u2283 Lit\u2212(\u03c9\u2032), \u03c9 is also a diagnosis.\nIt is easy to show that MDH holds for all weak-fault models. There are other theories SD 6\u2208 WFM for which MDH holds (e.g., one can directly construct a theory as a conjunction of terms for which MDH holds). Unfortunately, no necessary condition is known for MDH to hold in an arbitrary SD. The lemma which comes next is a direct consequence of MDH and weak-fault models.\nLemma 3. Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009, SD \u2208 WFM, and a diagnosis \u03c9 for some observation \u03b1, it follows that \u03c9 is non-minimal iff another diagnosis \u03c9\u2032 can be obtained by changing the sign of exactly one negative literal in \u03c9.\nProof (Sketch). From Def. 2 and SD \u2208 WFM, it follows that if \u03c9 is a minimal diagnosis, any diagnosis \u03c9\u2032 obtained by flipping one positive literal in \u03c9 is also a diagnosis. Applying the argument in the other direction gives us the above statement.\nSafari operates by performing subset flips on non-minimal diagnoses, attempting to compute minimal diagnoses. We next formalize this notion of flips, in order to characterize when Safari will be able to compute a minimal diagnosis.\nDefinition 11 (Subset Flip \u03a6\u21d3). Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009 and a health assignment \u03c9 with a non-empty set of negative literals (Lit\u2212(\u03c9) 6= \u2205), a subset flip \u03a6\u21d3 turns one of the negative literals in \u03c9 to a positive literal, i.e., it creates a health assignment \u03c9\u2032 with one more positive literal.\nWe next characterize flips based on whether they produce consistent models after the flip.\nDefinition 12 (Valid Subset Flip). Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009, an observation \u03b1, and a non-minimal diagnosis \u03c9, a valid flip exists if we can perform a subset flip in \u03c9 to create \u03c9\u2032 such that SD \u2227 \u03b1 \u2227 \u03c9\u2032 6|=\u22a5.\nGiven these notions, we can define continuity of the diagnosis search space in terms of literal flipping.\nDefinition 13 (Continuity). A system model SD and an observation \u03b1 satisfy the continuity property with respect to the set of diagnoses \u2126\u2286(SD\u2227\u03b1), iff for any diagnosis \u03c9k \u2208 \u2126(SD\u2227\u03b1) there exists a sequence \u03a6 = \u3008\u03c91, \u03c92, \u00b7 \u00b7 \u00b7 , \u03c9k\u22121, \u03c9k, \u03c9k+1, \u00b7 \u00b7 \u00b7 , \u03c9n\u3009, such that for i = 1, 2, \u00b7 \u00b7 \u00b7 , n \u2212 1, it is possible to go from \u03c9i to \u03c9i+1 via a valid subset flip, \u03c9i \u2208 \u2126(SD \u2227 \u03b1), and \u03c9n \u2208 \u2126 \u2286(SD \u2227 \u03b1).\nThe above definition allows for trivial continuity in the cases when a model and an observation lead to minimal diagnoses only (no non-minimal diagnoses). As we will see in Sec. 6, models and observations such that all diagnoses are minimal are rare in practice (of course, such problems can be created artificially). Note that the Safari algorithm still works and its theoretical properties are preserved even in the case of trivial continuity.\nGiven Def. 13, we can easily show the following two lemmata:\nLemma 4. If SD satisfies MDH, then it satisfies the continuity property.\nProof. Follows directly from Hypothesis 1 and Def 13.\nLemma 5. SD \u2208 WFM satisfies the continuity property.\nProof (Sketch). It is straightforward to show that if SD \u2208 WFM then SD satisfies MDH. Then from Lemma 4 it follows that SD satisfies the continuous property.\nOur greedy algorithm starts with an initial diagnosis and then randomly flips faulty assumable variables. We now use the MDH property to show that, starting with a non-minimal diagnosis \u03c9, the greedy stochastic diagnosis algorithm can monotonically reduce the size of the \u201cseed\u201d diagnosis to obtain a minimal diagnosis through appropriately flipping a fault variable from faulty to healthy; if we view this flipping as search, then this search is continuous in the diagnosis space.\nProposition 1. Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009, an observation \u03b1, and SD \u2208 WFM, Safari configured with M = |COMPS| and N = 1 returns one minimal diagnosis.\nProof. The diagnosis improvement loop starts, in the worst case, from a health assignment \u03c9 which is a conjunction of negative literals only. Necessarily, in this case, \u03c9 is a diagnosis as SD \u2208 WFM. A diagnosis \u03c9\u2032 that is subsumed by \u03c9 would be found with at most M consistency checks (provided that \u03c9\u2032 exists) as M is set to be equal to the number of literals in \u03c9 and there are no repetitions in randomly choosing of which literal to flip next. If, after trying all the negative literals in \u03c9, there is no diagnosis, then from Lemma 3 it follows that \u03c9 is a minimal diagnosis.\nThrough a simple inductive argument, we can continue this process until we obtain a minimal diagnosis.\nFrom Proposition 1 it follows that there is an upper bound of O(|COMPS|) consistency checks for finding a single minimal diagnosis. In most of the practical cases, however, we are interested in finding an approximation to all minimal-cardinality diagnoses. As a result the complexity of the optimally configured Safari algorithm becomes O(|COMPS|2S), where S is the number of minimal-cardinality diagnoses for the given observation. Section 5 discusses in more detail the computation of multiple minimal-cardinality diagnoses.\nThe number of assumable variables in a system of practical significance may exceed thousands, rendering an optimally configured Safari computationally too expensive. In Sec 4 we will see that while it is more computationally efficient to configure M < |COMPS|, it is still possible to find a minimal diagnosis with high probability.\nIt is simple to show that flip-based search algorithms are complete for continuous diagnosis search spaces given weak fault models, i.e., SD \u2208 WFM, and models that follow MDH, i.e., Lemma 3. We can formally characterize the guarantee of finding a minimal diagnosis with Safari in terms of a continuous diagnosis space. Note that this is a sufficient, but not necessary, condition; for example, we may configure Safari to flip multiple literals at a time to circumvent problems of getting trapped in discontinuous diagnosis spaces.\nTheorem 1. Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009, and a starting diagnosis \u03c9, Safari configured with M = |COMPS| and N = 1 is guaranteed to compute a minimal diagnosis if the diagnosis space is continuous.\nProof. Given an initial diagnosis \u03c9, Safari attempts to compute a minimal diagnosis by performing subset flips. If the diagnosis space is continuous, then we know that there exists a sequence of valid flips leading to a minimal diagnosis. Hence Safari is guaranteed to find a minimal diagnosis from \u03c9.\nFinally, we show that Safari provides a strong probabilistic guarantee of computing all minimal diagnoses.\nTheorem 2. The probability of Safari, configured with M = |COMPS|, of computing all minimal diagnoses of a diagnostic system DS = \u3008SD,COMPS,OBS\u3009 and an observation \u03b1 is denoted as Pr\u22c6. Given a continuous diagnosis space \u2126(SD, \u03b1), it holds that Pr\u22c6 \u2192 1 for N \u2192 \u221e.\nProof (Sketch). Since (1) the search space is continuous, (2) at each step there is a non-zero probability of flipping any unflipped literal, and (3) there is a polynomial upper bound of steps (|COMPS|) for computing a diagnosis, Safari can compute any non-minimal diagnosis with non-zero probability. Hence as N \u2192 \u221e, Safari will compute all minimal diagnoses."}, {"heading": "3.4 Complexity of Inference Using Greedy Stochastic Search", "text": "We next look at the complexity of Safari, and its stochastic approach to computing sound but incomplete diagnoses. We show that the primary determinant of the inference complexity is the consistency checking. Safari randomly computes a partial assignment \u03c0, and then checks if \u03c0 can be extended to create a satisfying assignment during each consistency check, i.e., it checks the consistency of \u03c0 with SD. This is solving the satisfiability problem (SAT), which is NP-complete (Cook, 1971). We will show how we can use incomplete satisfiability checking to reduce this complexity, at the cost of completeness guarantees.\nIn the following, we call \u0398 the complexity of a consistency check, and assume that there are \u03b3 components that can fail, i.e., \u03b3 = |COMPS|.\nLemma 6. Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009 with SD \u2208 WFM, the worst-case complexity of finding any minimal diagnosis is O(\u03b32\u0398), where \u0398 is the cost of a consistency check.\nProof. There is an upper bound of \u03b3 succeeding consistency checks for finding a single minimal diagnosis since there is a maximum of \u03b3 steps for computing the \u201call healthy\u201d diagnosis. As Safari performs a consistency check after each flip and at each step the algorithm must flip at most \u03b3 literals, the total complexity is O(\u03b32\u0398).\nIn most practical cases, however, we are interested in finding an approximation to all minimal-cardinality diagnoses. As a result the complexity of the optimally configured Safari algorithm becomes O ( \u03b3 (\n|\u03c9| \u03b3\n) \u0398 ) , where |\u03c9| is the cardinality of the minimal-cardinality\ndiagnoses for the given observation (cf. Sec. 6.6).\nThe complexity of BCP is well-known, allowing us to get more precise bounds on the worst-case complexity of computing one minimal-diagnosis with Safari. In what follows we will assume that SD is represented in CNF (cf. Sec. 2.3).\nLemma 7. Given a diagnostic system DS = \u3008SD,COMPS,OBS\u3009, SD \u2208 WFM, and SD having c clauses and n variables, the worst-case complexity under WFM of finding any minimal diagnosis is O(\u03b32cn) when using BCP for consistency checks.5\nProof (Sketch). An implementation of BCP (Forbus & de Kleer, 1993) maintains a total of c counters for the number of unsatisfied literals in each clause. A consistency check requires decrementing some or all counters for each of the n variables in SD. This gives us an upper bound of O(cn) on the execution time of BCP. Combining the complexity of BCP with Lemma 6 gives us the desired result.\n5. More efficient implementations of BCP exist (Zhang & Stickel, 1996)."}, {"heading": "4. Optimality Analysis (Single Diagnosis)", "text": "In contrast to deterministic algorithms, in the Safari algorithm there is no absolute guarantee that the optimum solution (minimal diagnosis) is found. Below we will provide an intuition behind the performance of the Safari algorithm by means of an approximate, analytical model that estimates the probability of reaching a diagnostic solution of specific minimality."}, {"heading": "4.1 Optimality of Safari in Weak-Fault Models", "text": "We will start by considering a single run of the algorithm without retries where we will assume the existence of only one minimal diagnosis. Next, we will extend the model by considering retries."}, {"heading": "4.1.1 Basic Model", "text": "Consider a diagnostic system DS = \u3008SD,COMPS,OBS\u3009 such that SD \u2208 WFM, and an observation \u03b1 such that \u03b1 manifests only one minimal diagnosis \u03c9. For the argument that follows we will configure Safari with M = 1, N = 1, and we will assume that the starting solution is the trivial \u201call faulty\u201d diagnosis.\nWhen Safari randomly chooses a faulty variable and flips it, we will be saying that it is a \u201csuccess\u201d if the new candidate is a diagnosis, and a \u201cfailure\u201d otherwise. Let k denote the number of steps that the algorithm successfully traverses in the direction of the minimal diagnosis of cardinality |\u03c9|. Thus k also measures the number of variables whose values are flipped from faulty to healthy in the process of climbing.\nLet f(k) denote the probability distribution function (pdf) of k. In the following we derive the probability p(k) of successfully making a transition from k to k+ 1. A diagnosis at step k has k positive literals and |COMPS| \u2212 k negative literals. The probability of the next variable flip being successful equals the probability that the next negative to positive flip out of the H \u2212 k negative literals does not conflict with a negative literal belonging to a diagnosis solution \u03c9. Consequently, of the |\u03c9| \u2212 k literals only COMPS| \u2212 |\u03c9| \u2212 k literals are allowed to flip, and therefore the success probability equals:\np (k) = |COMPS| \u2212 |\u03c9| \u2212 k\n|COMPS| \u2212 k = 1\u2212\n|\u03c9|\n|COMPS| \u2212 k (3)\nThe search process can be modeled in terms of the Markov chain depicted in Fig. 4, where k equals the state of the algorithm. Running into an inconsistency is modeled by the transitions to the state denoted \u201cfail\u201d.\nThe probability of exactly attaining step k (and subsequently failing) is given by:\nf(k) = (1\u2212 p(k + 1)) k \u220f\ni=0\np(i) (4)\nSubstituting (3) in (4) gives us the pdf of k:\nf(k) = |\u03c9|\n|COMPS| \u2212 k + 1\nk \u220f\ni=0\n[\n1\u2212 |\u03c9|\n|COMPS| \u2212 i\n]\n(5)\nAt the optimum goal state k = |COMPS| \u2212 |\u03c9| the failure probability term in (5) is correct as it equals unity.\nIf p were independent of k, f would be geometrically distributed, which implies that the chance of reaching a goal state k = |COMPS|\u2212|\u03c9| is slim. However, the fact that p decreases with k moves the probability mass to the tail of the distribution, which works in favor of reaching higher-k solutions. For instance, for single-fault solutions (|\u03c9| = 1) the distribution becomes uniform. Figure 5 shows the pdf for problem instances with |COMPS| = 100 for an increasing fault cardinality |\u03c9|. In order to decrease sampling noise, the empirical f(k) values in Fig. 5 are computed by taking the average over 10 samples of k.\nIn the next section we show that retries will further move probability mass towards the optimum, increasing the tail of the distribution, which is needed for (almost always) reaching optimality."}, {"heading": "4.1.2 Modeling Retries", "text": "In this section we extend the model to account for retries, which has a profound effect on the resulting pdf of f . Again, consider the transition between step k and k + 1, where the algorithm can spend up to m = 1, . . . ,M retries before exiting with failure. As can be\nseen by the algorithm (cf. Alg. 1), when a variable flip produces an inconsistency a retry is executed while m is incremented.\nFrom elementary combinatorics we can compute the probability of having a diagnosis after flipping any of M different negative literals at step k. Similar to (3), at stage k there are |COMPS| \u2212 k faulty literals from which M are chosen (as variable \u201cflips\u201d leading to inconsistency are recorded and not attempted again, there is no difference between choosing the M variables in advance or one after another). The probability of advancing from stage k to stage k + 1 becomes:\np\u2032(k) = 1\u2212\n(|\u03c9| M )\n(|COMPS|\u2212k M\n) (6)\nThe progress of Safari can be modeled for values of M > 1 as a Markov chain, similar to the one shown in Fig. 4 with the transition probability of p replaced by p\u2032. The resulting pdf of the number of successful steps becomes:\nf \u2032(k) =\n(|\u03c9| M )\n(|COMPS|\u2212k+1 M )\nk \u220f\ni=0\n[\n1\u2212\n(|\u03c9| M )\n(|COMPS|\u2212i M )\n]\n(7)\nIt can be seen that (5) is a restricted case of (7) for M = 1. The retry effect on the shape of the pdf is profound. Whereas for single-fault solutions the shape for M = 0 is uniform, for M = 1 most of the probability mass is already located at the optimum k = |COMPS| \u2212 |\u03c9|. Fig. 6 plots f for a number of problem instances with increasing M . As expected, the effect of M is extremely significant. Note that in case of the real system, for M = |COMPS| the pdf would consist of a single, unit probability spike at |COMPS| \u2212 |\u03c9|.\nAlthough we were unable to find an analytic treatment of the transition model above, the graphs immediately show that for large M the probability of moving to k = |COMPS|\u2212 |\u03c9| is very large. Hence, we expect the pdf to have a considerable probability mass located at k = |COMPS| \u2212 |\u03c9|, depending on M relative to |COMPS|."}, {"heading": "4.2 Optimality of Safari in Strong-Fault Models", "text": "From the above analysis we have seen that in WFM it is easy, starting from a non-minimal diagnosis, to reach a subset minimal diagnosis. As will be discussed in more detail below, this is not necessarily the case for strong-fault models. In many practical cases, however, strong-fault models exhibit, at least partially, behavior similar to MDH, thus allowing greedy algorithms like Safari to achieve results that are close to the optimal values."}, {"heading": "4.2.1 Partial Continuity in Strong-Fault Stuck-At Models", "text": "In what follows we will restrict our attention to a large subclass of SFM, called SFSM (Struss & Dressler, 1992).\nDefinition 14 (Strong-Fault Stuck-At Model). A system DS = \u3008SD,COMPS,OBS\u3009 belongs to the class SFSM iff SD is equivalent to (h1 \u21d2 F1) \u2227 (\u00ach1 \u21d2 l1) \u2227 \u00b7 \u00b7 \u00b7 \u2227 (hn \u21d2 Fn) \u2227 (\u00achn \u21d2 ln) such that 1 \u2264 i, j \u2264 n, {hi} \u2286 COMPS, Fj is a propositional formula, none of hi appears in Fj , and lj is a positive or negative literal in Fj .\nMDH (cf. Hypothesis 1) does not hold for SFSM models. Consider an adder whose inputs and outputs are all zeroes, and whose gate models are all stuck-at-1 when faulty. In this case, the \u201call nominal\u201d assignment is a diagnosis, but, for example, a stuck-at-1 output gate is not a diagnosis (there is a contradiction with the zero output).\nMany practical observations involving SFSM models, however, lead to partial continuity. This means that there are groups of diagnoses that differ in at most one literal, i.e., a flip based search can improve the cardinality of a diagnosis. We next formalize this notion.\nDefinition 15 (Partial Continuity). A system model SD and an observation \u03b1 satisfy the partial continuity property with respect to a set S \u2282 \u2126(SD \u2227 \u03b1), iff for every diagnosis \u03c9 such that \u2203\u03c9i \u2208 S satisfying Lit\n\u2212(\u03c9) \\Lit\u2212(\u03c9i) there exists a finite sequence of valid subset flips from \u03c9i to \u03c9.\nAt one extreme of the spectrum, SD and \u03b1 satisfy the partial continuity property with respect to the set of all of its diagnoses while at the other extreme, the partial continuity\nproperty is satisfied with respect to a singleton S (consider, for example, SD \u2208 WFM where S consists of the single \u201call faulty\u201d diagnosis).\nNote that the continuous property is trivally satisfied with respect to any diagnosis \u03c9k \u2208 \u2126(SD\u2227\u03b1), i.e., there always exists a sequence containing \u03c9k only (\u03a6 = \u3008\u03c9k\u3009). We are only interested in the non-trivial cases, for which |\u03a6| > 1.\nConsider a system SD and an observation \u03b1 that satisfy the partial continuity property with respect to some diagnosis \u03c9k. We say that the diagnoses in the flip sequence \u03a6 that contains \u03c9k form a continuous subspace. Alternatively, given a diagnostic system SD and an observation \u03b1, a continuous diagnostic subspace of SD\u2227\u03b1 is a set of diagnoses \u2126\u0304 \u2286 \u2126(SD\u2227\u03b1) with the property that, for any diagnosis \u03c9 \u2208 \u2126\u0304, there is another diagnosis \u03c9\u0304 \u2208 \u2126\u0304 such that |Lit\u2212(\u03c9)| \u2212 |Lit\u2212(\u03c9\u0304)| = \u00b11.\nUnfortunately, in the general SFSM case, we cannot derive bounds for the sizes of the continuous subspaces, and hence, for the optimality of Safari. In what follows, and with the help of a few examples, we illustrate the fact that partial continuity depends on the model and the observation and then we express the optimality of Safari as a function of this topologically-dependent property. Later, in Sec. 6, we collect empirical data that continuous subspaces leading to near-optimal diagnoses exist for a class of benchmark SFSM circuits.\nOur first example illustrates the notion of discontinuity (lack of partial continuity with respect to any diagnoses). We show a rare example of a model and and an observation leading to a set of diagnoses that contains diagnoses of cardinality m and m + q (q > 1), but has no diagnoses of cardinality m+ 1,m+ 2, \u00b7 \u00b7 \u00b7 ,m+ q \u2212 1.\nA Discontinuity Example Consider, for example, the Boolean circuit shown in Fig. 7 and modeled by the propositional formula:\nSDd =\n{\n[h1 \u21d2 (y \u21d4 \u00acx)] \u2227 [\u00ach1 \u21d2 (y \u21d4 x)] [h2 \u21d2 (y \u21d4 \u00acx)] \u2227 [\u00ach2 \u21d2 (y \u21d4 x)]\n(8)\nand an observation \u03b1d = x\u2227\u00acy. Note, that SDd 6\u2208 SFSM. There are exactly two diagnoses of SDd \u2227 \u03b1d: \u03c915 = h1 \u2227 h2 and \u03c916 = \u00ach1 \u2227 \u00ach2. Note that this model cannot have single faults. As only \u03c915 is minimal, |\u03c915| = 0, and |\u03c916| = 2, if the algorithm starts from \u03c916 it is not possible to reach the minimal diagnosis \u03c915 by performing single flips. Similarly we can construct models which impose an arbitrarily bad bound on the optimality of Safari. Such models, however, are not common and we will see that the greedy algorithm performs well on a wide class of strong-fault models.\nObviously, continuity in the distribution of the cardinalities in a set of diagnoses is a necessary (but not sufficient) condition for Safari to progress. Such models impose arbitrary difficulty to Safari, leading to suboptimal diagnoses of any cardinality.\nAn Example of Partial Continuity We continue the running example started in Sec. 2. First, we create a system description SDsa for a SFSM model. Let SDsa = SDw \u2227 SDf , where SDw is given by (1). The second part of SDsa, the strong fault description SDf , specifies that the output of a faulty gate must be stuck-at-1:\nSDf =(\u00ach1 \u21d2 i) \u2227 (\u00ach2 \u21d2 d) \u2227 (\u00ach3 \u21d2 j) \u2227 (\u00ach4 \u21d2 m)\u2227 \u2227 (\u00ach5 \u21d2 b) \u2227 (\u00ach6 \u21d2 l) \u2227 (\u00ach7 \u21d2 k)\n(9)\nIt is clear that SDsa \u2208 SFSM. We next compute the diagnoses of SDsa \u2227 \u03b11 (\u03b11 = x \u2227 y \u2227 p\u2227b\u2227\u00acd). There is one minimal diagnosis of SDsa\u2227\u03b11 and it is \u03c9 \u2286 5 = \u00ach1\u2227h2\u2227h3\u2227\u00b7 \u00b7 \u00b7\u2227h7 (cf. Fig. 8). If we choose the two literals h3 and h4 from \u03c9 \u2286 5 and change the signs of h3 and h4, we create two new health assignments: \u03c915 = \u00ach1 \u2227 h2 \u2227 \u00ach3 \u2227 h4 \u2227 h5 \u2227 h6 \u2227 h7 and \u03c916 = \u00ach1 \u2227 h2 \u2227 h3 \u2227 \u00ach4 \u2227 h5 \u2227 h6 \u2227 h7. It can be checked that both \u03c915 and \u03c916 are diagnoses, i.e., SDsa \u2227 \u03b11 \u2227 \u03c915 6|=\u22a5 and SDsa \u2227 \u03b11 \u2227 \u03c916 6|=\u22a5. Note that \u03c915 and \u03c916 are diagnoses of the weak-part of the model, i.e., {\u03c915, \u03c916} \u2282 \u2126(SDw \u2227 \u03b11). This follows from MDH and the fact that \u03c9\u22865 is a minimal diagnosis of SDw \u2227 \u03b11. Furthermore, \u03c915 is also a diagnosis in the strong-fault stuck-at model (\u03c915 \u2208 \u2126(SDsa \u2227 \u03b11)) because SDw \u2227 \u03b11 \u2227 \u00ach3 does not lead to a contradictory value for j in the strong-fault part SDf . A similar argument applies to \u03c916: SDw \u2227 \u03b11 \u2227 \u00ach4 does not contradict m in SDf . Equivalently, if negating h3 in \u03c9\u22865 , which makes j stuck-at-1, results in a diagnosis, and negating h4 in \u03c9 \u2286 5 , which makes m stuck-at-1, also results in a diagnosis, negating both h3 and h4 in \u03c9 \u2286 5 will also result in a diagnosis (consider the fact that the fault mode of h4 sets m only, but does not impose constraints on j). The above argument can be extended similarly to h5, h6, and h7. Hence, any assignment of COMPS containing \u00ach1 \u2227h2 is a diagnosis of SDsa \u2227\u03b11, no matter what combination of signs we take for h3, h4, h5, h6, and h7. Note that a health assignment containing \u00ach4 is a diagnosis conditioned on k = 1.\nConsider an alternative way of computing a set of ambiguous diagnoses of SDsa\u2227\u03b11. Given SDsa\u2227\u03b11\u2227\u03c9 \u2286 5 , we can compute a consistent assignment to all internal variables (for example by propagation). There is exactly one such assignment \u03c6 and it is \u03c6 = i \u2227 j \u2227 k \u2227 \u00acl \u2227 \u00acm, SDsa \u2227 \u03b11 \u2227 \u03c9 \u2286 5 \u2227 \u03c6 6|=\u22a5 (cf. Fig. 8). Note that for components h1, h3, h5, and h7, a change in the state of a component (healthy or faulty) does not lead to a different output value. For example the output j of the h3 or-gate is 1 because the gate is healthy and its\ninputs are 1 but j would also be 1 for a stuck-at-1 or-gate (\u00ach3). As a result, no diagnostic reasoner can determine if the components in the dashed region of Fig. 8 are healthy or faulty (stuck-at-1). Equivalently, one can change the signs of h3, h5, and h7 in the diagnosis \u03c9 \u2286 5 and the resulting assignments are still diagnoses. We call the set of components modeled by h1, h3, h5, and h7 an ambiguity group. Clearly, Safari can start from a diagnosis \u03c917 = \u00ach1 \u2227 h2 \u2227 \u00ach3 \u2227 h4 \u2227 \u00ach5 \u2227 h6 \u2227 \u00ach7 (|\u03c917| = 4) and reach \u03c9 \u2286 5 (|\u03c9 \u2286 5 | = 1) by performing valid subset flips.\nTo make our reasoning precise, we restrict the class of SFSM models to exclude malformed circuits such as ones having disconnected inputs or outputs, etc. Furthermore, we assume that each component has exactly one output (the set of all component output variables is denoted as COUT). The latter is not a big restriction as multi-output component models can be replaced by multiple components, each having a single output.6\nDefinition 16 (Well-Formed Diagnostic System (Wfds)). The diagnostic system DS = \u3008SD, COMPS, OBS\u3009 is well-formed (DS \u2208 Wfds) iff for any observation \u03b1 and for any diagnosis \u03c9 \u2208 \u2126(SD \u2227 \u03b1), there is exactly one assignment \u03c6 to all component outputs COUT such that SD \u2227 \u03b1 \u2227 \u03c9 \u2227 \u03c6 6|=\u22a5.\nConsider an SFSM model SD = (h1 \u21d2 F1) \u2227 (\u00ach1 \u21d2 l1) \u2227 \u00b7 \u00b7 \u00b7 \u2227 (hn \u21d2 Fn) \u2227 (\u00achn \u21d2 ln). We denote as COMPS\u2212 the set of those hi (1 \u2264 i \u2264 n) for which the respective li literals are negative (cf. Def. 14), i.e., COMPS\u2212 is the set of components whose failure modes are stuckat-0. Similarly, we use COMPS+ for the set of component variables whose stuck-at li literals are positive (COMPS\u2212 \u222a COMPS+ = COMPS, COMPS\u2212 \u2229 COMPS+ = \u2205). In a Wfds, an observation \u03b1 and a diagnosis \u03c9 force the output of each component either to a negative or to a positive value. We denote the set of health variables whose respective component outputs are forced to negative values as G\u2212(DS, \u03b1, \u03c9). Similarly, we have G+(DS, \u03b1, \u03c9) for the components whose outputs have positive values. With all this we can define the notion of a component ambiguity group.\nDefinition 17 (Component Ambiguity Group). Given a system DS = \u3008SD, COMPS, OBS\u3009, SD \u2208 SFSM, SD \u2208 Wfds, an observation \u03b1, and a diagnosis \u03c9 \u2208 \u2126(SD\u2227\u03b1), the component ambiguity group U(DS, \u03b1, \u03c9), U \u2286 COMPS, is defined as U(DS, \u03b1, \u03c9) = {G\u2212(DS, \u03b1, \u03c9) \u2229 COMPS\u2212} \u222a {G+(DS, \u03b1, \u03c9) \u2229COMPS+}.\nFinally, we show that a component ambiguity group leads to a continuous subspace. In the general case we cannot say much about the size of the component ambiguity groups. From experimentation, we have noticed that it is difficult to assign the inputs of an SFSM to values that generate small continuous subspaces (either SD \u2227 \u03b1 |=\u22a5, or SD \u2227 \u03b1 leads to large component ambiguity groups). Of course, it is possible to consider an adder, or a multiplier, for example, whose inputs are all zeroes and whose gate models are all stuck-at-1 when faulty, but the number of such inputs/circuit combinations is small.\nProposition 2. A diagnostic system SD, SD \u2208 SFSM, SD \u2208 Wfds, and an observation \u03b1 entail continuous diagnostic subspaces.\n6. Any multi-output Boolean function can be replaced by a composition of single-output Boolean functions.\nProof. From Def. 16 and the fact that SD \u2208 Wfds it follows that the output values of a subset of the components have the same sign as the model\u2019s stuck-at value. We denote this set as COMPS\u2032, COMPS\u2032 \u2286 COMPS. Any health assignment \u03c9\u0304 that differs only in signs of components belonging to COMPS\u2032 is also a diagnosis. If the set of diagnoses of SD \u2227 \u03b1 contains all possible assignments to the assumables in COMPS\u2032 then those diagnoses form a continuous space (cf. Def. 17).\nTo best illustrate Proposition 2, consider the or-gate modeled by h3 in Fig. 8. Its output is 1 either because the gate is healthy and one of the gate\u2019s inputs is 1, or because the gate is stuck-at-1. In this situation, it is not possible to determine if the component is healthy or faulty.\nClearly, |U(DS, \u03b1, \u03c9)| is a lower bound for the progress of Safari in stuck-at models. It can be shown that if Safari starts from a diagnosis \u03c9 of maximum cardinality for the given subspace, Safari is guaranteed (for M = |COMPS|) to improve the cardinality of \u03c9 by at least |U(DS, \u03b1, \u03c9)|. In practice, Safari can proceed even further as the stuckat ambiguity groups are only one factor of diagnostic uncertainty. A stuck-at component effectively \u201cdisconnects\u201d inputs from outputs, hence gates from the fan-in region are not constrained. For instance, continuing our example, for \u00ach5, all predecessors in the cone of \u00ach5 (components \u00ach3, \u00ach4, \u00ach5, \u00ach6, and \u00ach7) constitute a continuous health subspace. Contrary to a component ambiguity group, this set is conditional on the health state of another component. A thorough study of stuck-at continuity is outside the scope of this paper but as we shall see in Sec. 6, continuous subspaces justify Safari experiments on stuck-at models."}, {"heading": "4.2.2 Performance Modeling with Stuck-At Models", "text": "To further study the optimality of Safari in strong-fault models, we first define a case in which the algorithm cannot improve a non-minimal diagnosis by changing the sign of a faulty literal. Note that the existence of such cases is not a sufficient condition for Safari to be suboptimal, as it is possible to reach a minimal diagnosis by first changing the sign of some other faulty literal, thus \u201ccircumventing\u201d the missing diagnosis.\nFrom the preceding section we know that the number of \u201cinvalid flips\u201d does not depend on k, i.e., it is determined by the observation vector and the fault modes. The probability of Safari to progress from any non-minimal diagnosis becomes\np(k) = 1\u2212\n(|\u03c9|+|X| M )\n(|COMPS|\u2212k M\n) (10)\nwhere |X| is the number of \u201cinvalid flips\u201d. The ratio of the number of \u201cinvalid flips\u201d |X| to |COMPS| we will call SFM density d. The density d gives the average probability of trying an \u201cinvalid flip\u201d throughout the diagnostic search. An approximation of the probability of success of Safari is:\np(k) = 1\u2212\n(|\u03c9| M )\n(|COMPS|\u2212k M\n) \u2212 d (11)\nPlugging p into (4) allows us to predict f(k) for the SFM models for which our assumptions hold. This pdf, both measured from an implementation of Safari and generated from (4) and (11) is shown in Fig. 9 for different values of the density d.\nFrom Fig. 9 it is visible that increasing the density d leads to a shift of the probability density of the length of the walk k to the left. The effect, however, is not that profound even for large values of d, and is easily compensated by increasing M , as discussed in the preceding sections.\nIt is interesting to note that bounds on d can be computed from SD (independent of \u03b1), and these bounds can be used to further improve the performance of Safari."}, {"heading": "4.3 Validation", "text": "In the preceding sections we have illustrated the progress of Safari with synthetic circuits exposing specific behavior (diagnoses). In the remainder of this section we will plot the pdf of the greedy search on one of the small benchmark circuits (for more information on the 74181 model cf. Sec. 6).\nThe progress of Safari with a weak-fault model of the 74181 circuit is shown in Fig. 10. We have chosen a difficult observation leading to a minimal diagnosis of cardinality 7 (left) and an easy observation leading to a single fault diagnosis (right). Both plots show that the probability mass shifts to the right when increasing M and the effect is more profound for the smaller cardinality.\nThe effect of the stuck-at-0 and stuck-at-1 fault modes (SFM) on the probability of success of Safari is shown in Fig. 11.\nObviously, in this case the effect of increasing M is smaller, although still depending on the difficulty of the observation vector. Last, even for small values of M , the absolute probability of Safari finding a minimal diagnosis is sizeable, allowing the use of Safari\nas a practical anytime algorithm which always returns a diagnosis, the optimality of which depends on the time allocated to its computation."}, {"heading": "5. Optimality Analysis (Multiple Diagnoses)", "text": "The preceding section described the process of computing one diagnosis with Safari (N = 1). In this section we discuss the use of Safari in computing (or counting) all minimalcardinality diagnoses (N > 1). For the rest of the section we will assume that Safari is configured with M = |COMPS|.\nConsider a system description SD (SD \u2208 WFM) and an observation \u03b1. The number of minimal diagnoses |\u2126\u2286(SD \u2227 \u03b1)| can be exponential in |COMPS|. Furthermore, in practice, diagnosticians are interested in sampling from the set of minimal-cardinality diagnoses \u2126\u2264(SD \u2227 \u03b1) (recall that \u2126\u2264(SD \u2227 \u03b1) \u2286 \u2126\u2286(SD \u2227 \u03b1)) as the minimal-cardinality diagnoses cover a significant part of the a posteriori diagnosis probability space (de Kleer, 1990). In what follows, we will see that Safari is very well suited for that task.\nTheorem 3. The probability of Safari configured with M = |COMPS| computing a minimal diagnosis of cardinality |\u03c9| in a system with |COMPS| component variables approaches |COMPS|\u2212|\u03c9| for |COMPS|/|\u03c9| \u2192 \u221e.\nProof (Sketch). Assume a minimal diagnosis of cardinality |\u03c9| exists. From Proposition 1 it follows that Safari configured with M = |COMPS| is guaranteed to compute minimal diagnoses. Starting from the \u201call faulty\u201d assignment, consider a step k in \u201cimproving\u201d the diagnosis cardinality. If state k contains more than one diagnosis, then at state k+1, Safari will either (1) flip a literal belonging to this diagnosis (note that a literal may belong to more than one diagnosis) and subsequently prevent Safari from reaching this diagnosis or (2) flip a literal belonging to a diagnosis which has already been invalidated (i.e., one or more of its literals have been flipped at an earlier step).\nThe probability that a solution of cardinality |\u03c9| \u201csurvives\u201d a flip at iteration k (i.e., is not invalidated) is:\np (k) = 1\u2212 |\u03c9|\n|COMPS| \u2212 k =\n|COMPS| \u2212 |\u03c9| \u2212 k\n|COMPS| \u2212 k (12)\nSimilarly to our basic model (Sec. 4.1.1), the probability that a diagnosis \u03c9 \u201csurvives\u201d until it is returned by the algorithm:\nf(|COMPS| \u2212 |\u03c9| \u2212 1) =\n|COMPS|\u2212|\u03c9|\u22121 \u220f\ni=0\np(i) =\n|COMPS|\u2212|\u03c9|\u22121 \u220f\ni=0\n|COMPS| \u2212 |\u03c9| \u2212 i\n|COMPS| \u2212 i (13)\nRewriting the right hand side of Eq. (13) gives us:\nf(|COMPS| \u2212 |\u03c9| \u2212 1) = (|COMPS| \u2212 |\u03c9|)!\n(|\u03c9|+ 1)(|\u03c9|+ 2) \u00b7 \u00b7 \u00b7 |COMPS| =\n|\u03c9|!(|COMPS| \u2212 |\u03c9|)!\n|COMPS|! (14)\nSince\n(|COMPS| \u2212 |\u03c9|)!\n|COMPS|! =\n1\n(|COMPS| \u2212 |\u03c9|+ 1)(|COMPS| \u2212 |\u03c9|+ 2) \u00b7 \u00b7 \u00b7 |COMPS| (15)\nit holds that\nlim |COMPS|/|\u03c9|\u2192\u221e\n(|COMPS| \u2212 |\u03c9|)!\n|COMPS|! = |COMPS|\u2212|\u03c9| (16)\nAs a result, for small |\u03c9| relative to |COMPS|,\nf(|COMPS| \u2212 |\u03c9| \u2212 1) = |\u03c9|!|COMPS|\u2212|\u03c9| (17)\nwhich gives us the above theorem.\nThe distribution hi(|\u03c9|) of the cardinalities of the minimal diagnoses in \u2126 \u2286(SD\u2227\u03b1) depends on the topology of SD and on \u03b1; i.e., we can create SD and \u03b1 having any hi(|\u03c9|). We denote the cardinality distribution of the minimal diagnoses computed by Safari as h(|\u03c9|).\nTheorem 3 gives us a termination criterion for Safari which can be used for enumerating and counting minimal-cardinality diagnoses. Instead of running Safari with a fixed N , it is sufficient to compute the area under the output distribution function \u2211\nh. This value will converge to a single value, hence we can terminate Safari after the change of \u2211\nh drops below a fixed threshold. Note that Safari is efficient in enumerating the minimalcardinality diagnoses, as they are computed with a probability that is exponentially higher than that of the probability of computing minimal diagnoses of higher-cardinality, as shown in Theorem 3.\nCorollary 1. Safari computes diagnoses of equal cardinality with equal probability.\nProof (Sketch). From Theorem 3 it follows that the probability of success f of Safari in computing a diagnosis \u03c9 depends only on |\u03c9| and not on the actual composition of \u03c9.\nThe above corollary gives us a simple termination criterion for Safari in the cases when all minimal diagnoses are also minimal-cardinality diagnoses; it can be proven that in this case all minimal-cardinality diagnoses are computed with the same probability.\nWe will see that, given an input cardinality distribution hi(|\u03c9|), Safari produces an output distribution h(|\u03c9|) that is highly skewed to the right, due to Theorem 3. To facilitate the study of how Safari transforms hi(|\u03c9|) into h(|\u03c9|) we will use a Monte Carlo simulation of Safari. The advantage is that the Monte Carlo simulation is much simpler for analysing the run-time behavior of Safari than studying the algorithm itself.\nAlgorithm 2 Monte Carlo simulation of Safari\n1: function SafariSimulate(\u2126\u2286, N) returns a cardinality distribution\ninputs: \u2126\u2286, a set of minimal diagnoses N , integer, number of tries local variables: hi, h, vectors, cardinality distributions b, vector, fault distribution, n, i, c, integers\n2: hi \u2190 CardinalityDistribution(\u2126 \u2286) 3: for n \u2190 1, 2, . . . , N do 4: for c \u2190 1, 2, . . . , |hi| do 5: b[c] \u2190 c \u00b7 hi[c] 6: end for 7: for i \u2190 1, 2, . . . , |\u2126\u2286| do 8: c \u2190 DiscreteInverseRandomValue ( b P\nb\n)\n9: b[c] \u2190 b[c]\u2212 c 10: end for 11: h[c] \u2190 h[c] + 1 12: end for 13: return h 14: end function\nAlgorithm 2 simulates which diagnoses from the input set of minimal diagnoses \u2126 are \u201creached\u201d by Safari in N tries. The auxiliary subroutine CardinalityDistribution computes the input distribution hi by iterating over all diagnoses in \u2126\n\u2286. We store the input cardinality distribution hi and the resulting cardinality distribution h in vectors (note the vector sums in lines 7 and 8 and the division of a vector by scalar in line 8).\nThe outermost loop of Alg. 2 (lines 3 \u2013 12) simulates the N runs of Safari. This is done by computing and updating an auxiliary vector b, which contains the distribution of the component variables in \u2126\u2286 according to the cardinalities of the diagnoses these variables belong to. Initially, b is initialized with the number of literals in single faults in position 1, the number of literals in double faults in position 2 (for example if there are three double faults in hi, b[2] = 6), etc. This is done in lines 4 \u2013 6 of Alg. 2. We assume that diagnoses do not share literals. This restriction can be easily dropped by counting all the assumables in the input \u2126\u2286 (the latter assumption does not change the results of this section).\nLines 7 \u2013 10 simulate the process of the actual bit flipping of Safari. At each step the simulation draws a random literal from the probability distribution function (pdf) bP b ; this is done by the DiscreteInverseRandomValue function in line 8. Each bit flip \u201cinvalidates\u201d a diagnosis from the set \u2126\u2286, i.e., a diagnosis of cardinality c cannot be reached by Safari. After a diagnosis has been \u201cinvalidated\u201d, the vector b is updated, for example, if the simulation \u201cinvalidates\u201d a quadruple fault, b[4] = b[4]\u22124 (line 9). Note that the number of iterations in the loop in lines 7 \u2013 10 equals the number of diagnoses in \u2126\u2286. As a result after terminating this loop, the value of the integer variable c is equal to the cardinality of the last \u201cinvalidated\u201d diagnosis. The latter is the diagnosis which Safari computes in this run. What remains is to update the resulting pdf with the right cardinality (line 11).\nThe simulation in Alg. 2 links the distribution of the actual diagnoses in \u2126\u2286 to the distribution of the cardinalities of the diagnoses returned by Safari. As \u2126\u2286 can be arbitrarily set, we will apply Alg. 2 to a range of typical input distributions. The results of the simulation as well as the results of running Safari on synthetic problems with the same input distributions are shown in Fig. 12.\nFig. 12 shows (1) that Alg. 2 predicts the actual behavior of Safari (compare the second and third column of plots), and (2) that Safari computes diagnoses of small cardinality in agreement with Theorem 3. The only case when the output distribution is not a steep exponential is when the cardinalities in the set of the input minimal diagnoses grow exponentially. Table 2 summarizes the parameters of exponential fits for the input cardinality distributions shown in Fig. 12 (a is the initial (zero) cardinality, \u03bb is the decay constant, and R2 is the coefficient of determination). We have seen that Safari is suited for computing multiple diagnoses of small probability of occurrence. In the next section we will provide an alternative argument leading to similar conclusions."}, {"heading": "6. Experimental Results", "text": "This section discusses empirical results measured from an implementation of Safari. In order to compare the optimality and performance of Safari to various diagnostic algorithms, we have performed more than a million diagnosis computations on 64 dual-CPU nodes belonging to a cluster. Each node contains two 2.4 GHz AMD Opteron DP 250 processors and 4 Gb of RAM.\nThe default configuration of Safari (when not stated otherwise) was M = 8 and N = 4; that is, Safari is configured for a maximum number of 8 retries before giving up the climb, and a total of 4 attempts. To provide more precise average run-time optimality and performance data, all stochastic algorithms (i.e., ones based on SLS Max-SAT and Safari) have been repeatedly run 10 times on each model and observation vector."}, {"heading": "6.1 Implementation Notes and Test Set Description", "text": "We have implemented Safari in approximately 1 000 lines of C code (excluding the LTMS, interface, and DPLL code) and it is a part of the Lydia package.7\nTraditionally, MBD algorithms have been tested on diagnostic models of digital circuits like the ones included in the ISCAS85 benchmark suite (Brglez & Fujiwara, 1985). As models derived from ISCAS85 are large (from a traditional diagnostic perspective), we have also considered four medium-sized circuits from the 74XXX family (Hansen, Yalcin, & Hayes, 1999). In order to provide both weak- and strong-fault cases, we have translated each circuit to a weak, stuck-at-0 (S-A-0), and stuck-at-1 (S-A-1) model. In the stuck-at models, the output of each faulty gate is assumed to be the same constant (cf. Def. 14).\nThe performance of diagnostic algorithms depends to various degrees on the observation vectors (algorithm designers strive to produce algorithms, the performance of which is not dependent on the observation vectors). Hence, we have performed our experimentation with a number of different observations for each model. We have implemented an algorithm (Alg. 3) that generates observations leading to diagnoses of different minimal-cardinality, varying from 1 to nearly the maximum for the respective circuits (for the 74XXX models it is the maximum). The experiments omit nominal scenarios as they are trivial from the viewpoint of MBD.\nAlgorithm 3 uses a number of auxiliary functions. RandomInputs (line 3) assigns uniformly distributed random values to each input in IN (note that for the generation of observation vectors we partition the observable variables OBS into inputs IN and outputs OUT and use the input/output information which comes with the original 74XXX/ISCAS85 circuits for simulation). Given the \u201call healthy\u201d health assignment and the diagnostic system, ComputeNominalOutputs (line 4) performs simulation by propagating the input assignment \u03b1. The result is an assignment \u03b2 which contains values for each output variable in OUT.\n7. Lydia, Safari, and the diagnostic benchmark can be downloaded from http://fdir.org/lydia/.\nAlgorithm 3 Algorithm for generation of observation vectors\n1: function MakeAlphas(DS, N,K) returns a set of observations\ninputs: DS = \u3008SD,COMPS,OBS\u3009, diagnostic system OBS = IN \u222aOUT, IN \u2229OUT = \u2205 N , integer, number of tries for Safari K, integer, maximal number of diagnoses per cardinality local variables: \u03b1, \u03b2, \u03b1n, \u03c9, terms c, integer, best cardinality so far A, set of terms (observation vectors), result\n2: for k \u2190 1, 2, . . . ,K do 3: \u03b1 \u2190 RandomInputs(IN) 4: \u03b2 \u2190 ComputeNominalOutputs(DS, \u03b1) 5: c \u2190 0 6: for all v \u2208 OUT do 7: \u03b1n \u2190 \u03b1 \u2227 Flip(\u03b2, v) 8: \u03c9 \u2190 SmallestCardinalityDiagnosis(Safari(SD, \u03b1n, |COMPS|, N)) 9: if |\u03c9| > c then\n10: c \u2190 |\u03c9| 11: A \u2190 A \u222a \u03b1n 12: end if 13: end for 14: end for 15: return A 16: end function\nThe loop in lines 6 \u2013 13 increases the cardinality by greedily flipping the values of the output variables. For each new candidate observation \u03b1n, Alg. 3 uses the diagnostic oracle Safari to compute a minimal diagnosis of cardinality c. As Safari returns more than one diagnosis (up to N), we use SmallestCardinalityDiagnosis to choose the one of smallest cardinality. If the cardinality c of this diagnosis increases in comparison to the previous iteration, the observation is added to the list.\nBy running Alg. 3 we get up toK observations leading to faults of cardinality 1, 2, . . . ,m, where m is the cardinality of the MFMC diagnosis (Feldman, Provan, & van Gemund, 2008b) for the respective circuit. Alg. 3 clearly shows a bootstrapping problem. In order to create potentially \u201cdifficult\u201d observations for Safari, we require Safari to solve those \u201cdifficult\u201d observations. Although we have seen in Sec. 5 that Safari is heavily biased towards generating diagnoses of small cardinality, there is no guarantee. To alleviate this problem, for the generation of observation vectors, we have configured Safari to compute subset-minimal diagnoses with M = |COMPS| and N increased to 20.\nTable 3 provides an overview of the fault diagnosis benchmark used for our experiments. The third and fourth columns show the number of observable and assumable variables, which characterize the size of the circuits. The next three columns show the number of observation vectors with which we have tested the weak, S-A-0, and S-A-1 models. For the stuck-at models, we have chosen those weak-fault model observations which are consistent with their\nrespective system descriptions (as in strong-fault models it is often the case that SD\u2227\u03b1 |=\u22a5, we have not considered such scenarios)."}, {"heading": "6.2 Comparison to Complete Algorithms", "text": "Table 4 shows the results from comparing Safari to implementations of two state-of-the-art complete and deterministic diagnostic algorithms: a modification for completeness of CDA\u2217 (Williams & Ragno, 2007) and HA\u2217 (Feldman & van Gemund, 2006). Table 4 shows, for each model and for each algorithm, the percentage of all tests for which a diagnosis could be computed within a cut-off time of 1 minute.\nAs it is visible from the three rightmost columns of Table 4, Safari could find diagnoses for all observation vectors, while the performance of the two deterministic algorithms (columns two to seven) degraded with the increase of the model size and the cardinality of the observation vector. Furthermore, we have observed a degradation of the performance of CDA\u2217 and HA\u2217 with increased cardinality of the minimal-cardinality diagnoses, while the performance of Safari remained unaffected."}, {"heading": "6.3 Comparison to Algorithms Based on ALLSAT and Model Counting", "text": "We have compared the performance of Safari to that of a pure SAT-based approach, which uses blocking clauses for avoiding duplicate diagnoses (Jin, Han, & Somenzi, 2005). Although SAT encodings have worked efficiently on a variety of other domains, such as planning, the weak health modeling makes the diagnostic problem so underconstrained that an uninformed ALLSAT strategy (i.e., a search not exploiting the continuity imposed by the weak-fault modeling) is quite inefficient, even for small models.\nTo substantiate our claim, we have experimented with the state-of-the-art satisfiability solver RelSat, version 2.02 (Bayardo & Pehoushek, 2000). Instead of enumerating all solutions and filtering the minimal diagnoses only, we have performed model-counting, whose relation to MBD has been extensively studied (Kumar, 2002). While it was possible to solve the two smallest circuits, the solver did not terminate for any of the larger models within the predetermined time of 1 hour. The results are shown in Table 5.\nThe second column of Table 5 shows the model count returned by RelSat, with sample single-fault observations from our benchmark. The third column reports the time for model counting. This slow performance on relatively small diagnostic instances leads us to the conclusion that specialized solvers like Safari are better suited for finding minimal diagnoses than off-the-shelf ALLSAT (model counting) implementations that do not encode inference properties similar to those encoded in Safari.\nWe have used the state-of-the-art, non-exact model counting method SampleCount (Gomes, Hoffmann, Sabharwal, & Selman, 2007) to compute lower bounds of the model counts. The results are shown in the third and fourth columns of Table 5. Configured with the default settings (\u03b1 = 3.5, t = 2, z = 20, cutoff 10 000 flips), SampleCount could not find lower bounds for circuits larger than c1355. Although the performance of SampleCount is significantly better than RelSAT, the fact that SampleCount computes lower bounds and does not scale to large circuits prevent us from building a diagnosis algorithm based on approximate model counting.\nA satisfiability-based method for diagnosing an optimized version of ISCAS85 has been used by Smith, Veneris, and Viglas (2004). In a more recent paper (Smith, Veneris, Ali, & Viglas, 2005), the SAT-based approach has been replaced by a Quantified Boolean Formula (QBF) solver for computing multiple-fault diagnoses. These methods report good absolute\nexecution time for single and double-faults (and we believe that they scale well for higher cardinalities), but require modifications of the initial circuits (i.e., introduce cardinality and test constraints) and suggest specialized heuristics for the SAT solvers in order to improve the search performance. Comparison of the performance of Safari to the timings reported by these papers would be difficult due to a number of reasons like the use of different and optimized benchmark sets, trading-off memory for speed, rewriting the original circuits, etc."}, {"heading": "6.4 Performance of the Greedy Stochastic Search", "text": "Table 6 shows the absolute performance of Safari (M = |COMPS|, N = 4). This varies from under a millisecond for the small models, to approx. 30 s for the largest strong-fault model. These fast absolute times show that Safari is suitable for on-line reasoning tasks, where autonomy depends on speedy computation of diagnoses.\nFor each model, the minimum and maximum time for computing a diagnosis has been computed. These values are shown under columns tmin and tmax , respectively. The small range of tmax \u2212 tmin confirms our theoretical results that Safari is insensitive to the fault cardinalities of the diagnoses it computes. The performance of CDA\u2217 and HA\u2217, on the other hand, is dependent on the fault cardinality and quickly degrades with increasing fault cardinality."}, {"heading": "6.5 Optimality of the Greedy Stochastic Search", "text": "From the results produced by the complete diagnostic methods (CDA\u2217 and HA\u2217) we know the exact cardinalities of the minimal-cardinality diagnoses for some of the observations. By considering these observations, which lead to single and double faults, we have evaluated\nthe average optimality of Safari. Table 7 shows these optimality results for the greedy search. The second column of Table 7 shows the number of observation vectors leading to single faults for each weak-fault model. The third column shows the average cardinality of Safari. The second and third column are repeated for the S-A-0 and S-A-1 models.\nTable 7 shows that, for SD \u2208 WFM, the average cardinality returned by Safari is near-optimal for both single and double faults. The c1355 model shows the worst-case results for the single-fault observations, while c499 is the most difficult weak-fault model for computing a double-fault diagnosis. These results can be improved by increasing M and N as discussed in Sec. 4.\nWith strong-fault models, results are close to optimal for the small models and the quality of diagnosis deteriorates for c3540 and bigger. This is not surprising, considering the modest number of retries and number of \u201cflips\u201d with which Safari was configured."}, {"heading": "6.6 Computing Multiple Minimal-Cardinality Diagnoses", "text": "We next show the results of experiments supporting the claims made in Sec. 5. For that, we have first chosen these observations \u03b1 for which we could compute |\u2126\u2264(SD \u2227 \u03b1)| with a deterministic algorithm like CDA\u2217 or HA\u2217 (mostly observations leading to single or double faults). We have then configured Safari with M = |COMPS| and N = 10|\u2126\u2264(SD \u2227 \u03b1)|. Finally, from the diagnoses computed by Safari we have filtered the minimal-cardinality ones. The results are summarized in Table 8.\nTable 8 repeats the same columns for weak, S-A-0, and S-A-1 models and the data in these columns are to be interpreted as follows. The columns marked with |\u2126\u2264| show the minimal and maximal number of minimal-cardinality diagnoses per model as computed by a deterministic algorithm. The columns Mc show the percentage of minimal-cardinality\ndiagnoses returned by Safari (from all minimal-cardinality diagnoses) for those \u03b1 for which |\u2126\u2264(SD \u2227 \u03b1)| > 1. The columns Mf show the percentage of observations for which Safari could not compute any minimal-cardinality diagnosis.\nThe results shown in Table 8 show that even for moderate values of N (N \u2264 27 770), Safari was capable of computing a significant portion of all minimal-cardinality diagnoses. This portion varies from 78.5% to 100% for weak-fault models and from 78% to 100% for strong-fault models. The percentage of cases in which Safari could not reach a minimalcardinality diagnosis is limited (at most 13.55%) and is mainly in the cases in which there exists only one single-fault diagnosis. Note that even in the cases in which Safari cannot compute any minimal-cardinality diagnoses, the result of Safari can still be useful. For example, a subset-minimal diagnosis of small cardinality differing in one or two literals only nevertheless brings useful diagnostic information (a discussion on diagnostic metrics is beyond the scope of this paper)."}, {"heading": "6.7 Experimentation Summary", "text": "We have applied Safari to a suite of benchmark combinatorial circuits encoded using weak-fault models and stuck-at strong fault models, and shown significant performance improvements for multiple-fault diagnoses, compared to two state-of-the-art deterministic algorithms, CDA\u2217 and HA\u2217. Our results indicate that Safari shows at least an order-ofmagnitude speedup over CDA\u2217 and HA\u2217 for multiple-fault diagnoses. Moreover, whereas the search complexity for the deterministic algorithms tested increases exponentially with fault cardinality, the search complexity for this stochastic algorithm appears to be independent of fault cardinality.\nWe have compared the performance of Safari to that of an algorithm based on MaxSAT, and Safari shows at least an order-of-magnitude speedup in computing diagnoses. We have compared the optimality of Safari to that of an algorithm based on SLS MaxSAT, and Safari consistently computes diagnoses of smaller cardinality whereas the SLS Max-SAT diagnostic algorithm often fails to compute any diagnosis."}, {"heading": "7. Related Work", "text": "This paper (1) generalizes Feldman, Provan, and van Gemund (2008a), (2) introduces important theoretical results for strong-fault models, (3) extends the experimental results there, and (4) provides a comprehensive optimality analysis of Safari.\nOn a gross level, one can classify the types of algorithms that have been applied to solve MBD as being based on search or compilation. The search algorithms take as input the diagnostic model and an observation, and then search for a diagnosis, which may be minimal with respect to some minimality criterion. Examples of search algorithms include A\u2217-based algorithms, such as CDA\u2217 (Williams & Ragno, 2007) and hitting set algorithms (Reiter, 1987). Compilation algorithms pre-process the diagnostic model into a form that is more efficient for on-line diagnostic inference. Examples of such algorithms include the ATMS (de Kleer, 1986) and other prime-implicant methods (Kean & Tsiknis, 1993), DNNF (Darwiche, 1998), and OBDD (Bryant, 1992). To our knowledge, all of these approaches adopt exact methods to compute diagnoses; in contrast, Safari adopts a stochastic approach to computing diagnoses.\nAt first glance, it seems like MBD could be efficiently solved using an encoding as a SAT (Jin et al., 2005), constraint satisfaction (Freuder, Dechter, Ginsberg, Selman, & Tsang, 1995) or Bayesian network (Kask & Dechter, 1999) problem. However, one needs to\ntake into account the increase in formula size (over a direct MBD encoding), in addition to the underconstrained nature of MBD problems.\nSafari has close resemblance to Max-SAT (Hoos & Stu\u0308tzle, 2004) and we have conducted extensive experimentation with both complete (partial and weighted) and SLS-based Max-SAT. As the results of these experiments are long, we have published them in a separate technical report (Feldman, Provan, & van Gemund, 2009a). The results show that although Max-SAT can compute diagnoses in many of the cases, the performance of MaxSAT degrades when increasing the circuit size or the cardinality of the injected faults. In particular, Safari outperforms Max-SAT by at least an order-of-magnitude for the class of diagnostic problems we have considered. In the case of SLS-based Max-SAT, the optimality of Max-SAT-based inference is significantly worse than that of Safari.\nWe show that Safari exploits a particular property of MBD problems, called diagnostic continuity, which improves the optimality of Safari compared to, for example, straightforward ALLSAT encodings (Jin et al., 2005). We experimentally confirm this favorable performance and optimality of Safari. Although Safari has close resemblance to MaxSAT, Safari exploits specific landscape properties of the diagnostic problems, which allow (1) simple termination criteria and (2) optimality bounds. Due to the hybrid nature of Safari (the use of LTMS and SAT), Safari avoids getting stuck in local optima and performs better than Max-SAT based methods. Incorporating approaches from Max-SAT, and in particular SAPS (Hutter, Tompkins, & Hoos, 2002), in future versions of Safari may help in solving more general abduction problems, which may not expose the continuous properties of the models we have considered.\nStochastic algorithms have been discussed in the framework of constraint satisfaction (Freuder et al., 1995) and Bayesian network inference (Kask & Dechter, 1999). The latter two approaches can be used for solving suitably translated MBD problems. It is often the case, though, that these encodings are more difficult for search than specialized ones.\nMBD is an instance of constraint optimization, with particular constraints over failure variables. MBD has developed algorithms to exploit these domain properties, and our proposed approach differs significantly from almost all MBD algorithms that appear in the literature. While most advanced MBD algorithms are deterministic, Safari borrows from SLS algorithms that, rather than backtracking, may randomly flip variable assignments to determine a satisfying assignment. Complete MBD algorithms typically make use of preferences, e.g., fault-mode probabilities, to improve search efficiency; Safari uses this technique on top of its stochastic search over the space of diagnoses.\nA closely-related diagnostic approach is that of Fijany, Vatan, Barrett, James, Williams, and Mackey (2003), who map the minimal-hitting set problem into the problem of finding an assignment with bounded weight satisfying a monotone SAT problem, and then propose to use efficient SAT algorithms for computing diagnoses. The approach of Fijany et al. has shown speedups in comparison with other diagnosis algorithms; the main drawback is the number of extra variables and clauses that must be added in the SAT encoding, which is even more significant for strong fault models and multi-valued variables. In contrast, our approach works directly on the given diagnosis model and requires no conversion to another representation.\nOur work bears the closest resemblance to preference-based or Cost-Based Abduction (CBA) (Charniak & Shimony, 1994; Santos Jr., 1994). Of the algorithmic work in this\narea, the primary paper that adopts stochastic local search is by Abdelbar, Gheita, and Amer (2006). In this paper, they present a hybrid two-stage method that is based on Iterated Local Search (ILS) and Repetitive Simulated Annealing (RSA). The ILS stage of the algorithm uses a simple hill-climbing method (randomly flipping assumables) for the local search phase, and tabu search for the perturbation phase. RSA repeatedly applies Simulated Annealing (SA), starting each time from a random initial state. The hybrid method initially starts from an arbitrary state, or a greedily-chosen state. It then applies the ILS algorithm; if this algorithm fails to find the optimal solution after a fixed number \u03c4 of hill-climbing steps8 or after a fixed number R of repetitions of the perturbation-local search cycle,9 ILS-based search is terminated and the RSA algorithm is run until the optimal solution is found.\nOur work differs from that of Abdelbar et al. (2006) in several ways. First, our initial state is generated using a random SAT solution. The hill-climbing phase that we use next is similar to that of Abdelbar et al.; however, we randomly restart should hill-climbing not identify a \u201cbetter\u201d diagnosis, rather than applying tabu search or simulated annealing. Our approach is simpler than that of Abdelbar et al., and for the case of weak fault models is guaranteed to be optimal; in future work we plan to compare our approach to that of Abdelbar et al. for strong fault models.\nIn 2009 Safari competed against the diagnostic algorithms NGDE (de Kleer, 2009) and RODON (Bunus, Isaksson, Frey, & Mu\u0308nker, 2009) in the synthetic track of the first diagnostic competition DXC\u201909 (Kurtoglu, Narasimhan, Poll, Garcia, Kuhn, de Kleer, van Gemund, & Feldman, 2009). The conditions under which the DXC\u201909 experiments were conducted were similar to the ones described in this paper. The CPU and memory performance of Safari were an order of magnitude better than the competing algorithms despite the fact that NGDE and RODON performed better than the complete algorithms discussed in this section. In this paper, in addition to computational metrics, we have informally used the minimality of a diagnosis as an optimality criterion. The DXC\u201909 organizers, however, have defined a utility metric which approximates the expected repair effort of a circuit (Feldman, Provan, & van Gemund, 2009b). With this utility metric, Safari scored slightly worse than the two competing algorithms, which is to be expected as Safari trades off diagnostic precision for computational efficiency. We refer the reader to the DXC papers mentioned above for a more thorough analysis of the competition results."}, {"heading": "8. Conclusion and Future Work", "text": "We have described a greedy stochastic algorithm for computing diagnoses within a modelbased diagnosis framework. We have shown that subset-minimal diagnoses can be computed optimally in weak fault models and in an important subset of strong fault models, and that almost all minimal-cardinality diagnoses can be computed for more general fault models.\n8. Hill-climbing proceeds as follows: given a current state s with a cost of f(s), a neighbouring state s\u2032\nis generated by flipping a randomly chosen assumable hypothesis. If f(s\u2032) is better than f(s), then s\u2032 becomes the current state; otherwise, it is discarded. If \u03c4 iterations elapse without a change in the current state, the local search exits. 9. Perturbation-local search, starting from a current state s with a cost of f(s), randomly chooses an assumable variable h, and applies tabu search to identify a better state by flipping h based on its tabu status.\nWe argue that Safari can be of broad practical significance, as it can compute a significant fraction of minimal-cardinality diagnoses for systems too large or complex to be diagnosed by existing deterministic algorithms.\nIn future work, we plan to experiment on models with a combination of weak and strong failure-mode descriptions. We also plan on experimenting with a wider variety of stochastic methods, such as simulated annealing and genetic search, using a larger set of benchmark models. Last, we plan to apply our algorithms to a wider class of abduction and constraint optimization problems."}], "references": [{"title": "Approximating cost-based abduction is NP-hard", "author": ["A.M. Abdelbar"], "venue": "Artificial Intelligence, 159 (1-2), 231\u2013239.", "citeRegEx": "Abdelbar,? 2004", "shortCiteRegEx": "Abdelbar", "year": 2004}, {"title": "Exploring the fitness landscape and the run-time behaviour of an iterated local search algorithm for cost-based abduction", "author": ["A.M. Abdelbar", "S.H. Gheita", "H.A. Amer"], "venue": "Experimental & Theoretical Artificial Intelligence,", "citeRegEx": "Abdelbar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Abdelbar et al\\.", "year": 2006}, {"title": "Counting models using connected components", "author": ["R.J. Bayardo", "J.D. Pehoushek"], "venue": "In Proc. AAAI\u201900,", "citeRegEx": "Bayardo and Pehoushek,? \\Q2000\\E", "shortCiteRegEx": "Bayardo and Pehoushek", "year": 2000}, {"title": "A neutral netlist of 10 combinational benchmark circuits and a target translator in fortran", "author": ["F. Brglez", "H. Fujiwara"], "venue": "In Proc. ISCAS\u201985,", "citeRegEx": "Brglez and Fujiwara,? \\Q1985\\E", "shortCiteRegEx": "Brglez and Fujiwara", "year": 1985}, {"title": "Symbolic Boolean manipulation with ordered binary-decision diagrams", "author": ["R.E. Bryant"], "venue": "ACM Computing Surveys, 24 (3), 293\u2013318.", "citeRegEx": "Bryant,? 1992", "shortCiteRegEx": "Bryant", "year": 1992}, {"title": "RODON - a model-based diagnosis approach for the DX diagnostic competition", "author": ["P. Bunus", "O. Isaksson", "B. Frey", "B. M\u00fcnker"], "venue": "In Proc", "citeRegEx": "Bunus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bunus et al\\.", "year": 2009}, {"title": "The computational complexity of abduction", "author": ["T. Bylander", "D. Allemang", "M. Tanner", "J. Josephson"], "venue": "Artificial Intelligence,", "citeRegEx": "Bylander et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Bylander et al\\.", "year": 1991}, {"title": "Cost-based abduction and MAP explanation", "author": ["E. Charniak", "S.E. Shimony"], "venue": "Artificial Intelligence,", "citeRegEx": "Charniak and Shimony,? \\Q1994\\E", "shortCiteRegEx": "Charniak and Shimony", "year": 1994}, {"title": "The complexity of theorem-proving procedures", "author": ["S.A. Cook"], "venue": "Proc. STOC\u201971, pp. 151\u2013158.", "citeRegEx": "Cook,? 1971", "shortCiteRegEx": "Cook", "year": 1971}, {"title": "Model-based diagnosis using structured system descriptions", "author": ["A. Darwiche"], "venue": "Journal of Artificial Intelligence Research, 8, 165\u2013222.", "citeRegEx": "Darwiche,? 1998", "shortCiteRegEx": "Darwiche", "year": 1998}, {"title": "A machine program for theorem-proving", "author": ["M. Davis", "G. Logemann", "D. Loveland"], "venue": "Communications of the ACM,", "citeRegEx": "Davis et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1962}, {"title": "An assumption-based TMS", "author": ["J. de Kleer"], "venue": "Artificial Intelligence,", "citeRegEx": "Kleer,? \\Q1986\\E", "shortCiteRegEx": "Kleer", "year": 1986}, {"title": "Using crude probability estimates to guide diagnosis", "author": ["J. de Kleer"], "venue": "Artificial Intelligence,", "citeRegEx": "Kleer,? \\Q1990\\E", "shortCiteRegEx": "Kleer", "year": 1990}, {"title": "Minimum cardinality candidate generation", "author": ["J. de Kleer"], "venue": "In Proc. DX\u201909,", "citeRegEx": "Kleer,? \\Q2009\\E", "shortCiteRegEx": "Kleer", "year": 2009}, {"title": "Characterizing diagnoses and systems", "author": ["J. de Kleer", "A. Mackworth", "R. Reiter"], "venue": "Artificial Intelligence,", "citeRegEx": "Kleer et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kleer et al\\.", "year": 1992}, {"title": "Diagnosing multiple faults", "author": ["J. de Kleer", "B. Williams"], "venue": "Artificial Intelligence,", "citeRegEx": "Kleer and Williams,? \\Q1987\\E", "shortCiteRegEx": "Kleer and Williams", "year": 1987}, {"title": "An extensible SAT-solver", "author": ["N. E\u00e9n", "N. S\u00f6rensson"], "venue": "In Proc. SAT\u201903,", "citeRegEx": "E\u00e9n and S\u00f6rensson,? \\Q2003\\E", "shortCiteRegEx": "E\u00e9n and S\u00f6rensson", "year": 2003}, {"title": "The complexity of logic-based abduction", "author": ["T. Eiter", "G. Gottlob"], "venue": "Journal of the ACM,", "citeRegEx": "Eiter and Gottlob,? \\Q1995\\E", "shortCiteRegEx": "Eiter and Gottlob", "year": 1995}, {"title": "Computing minimal diagnoses by greedy stochastic search", "author": ["A. Feldman", "G. Provan", "A. van Gemund"], "venue": "In Proc. AAAI\u201908,", "citeRegEx": "Feldman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2008}, {"title": "Computing observation vectors for max-fault min-cardinality diagnoses", "author": ["A. Feldman", "G. Provan", "A. van Gemund"], "venue": "In Proc. AAAI\u201908,", "citeRegEx": "Feldman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2008}, {"title": "A family of model-based diagnosis algorithms based on Max-SAT", "author": ["A. Feldman", "G. Provan", "A. van Gemund"], "venue": "Tech. rep. ES-2009-02,", "citeRegEx": "Feldman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2009}, {"title": "The Lydia approach to combinational model-based diagnosis", "author": ["A. Feldman", "G. Provan", "A. van Gemund"], "venue": "In Proc. DX\u201909,", "citeRegEx": "Feldman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2009}, {"title": "A two-step hierarchical algorithm for model-based diagnosis", "author": ["A. Feldman", "A. van Gemund"], "venue": "In Proc. AAAI\u201906,", "citeRegEx": "Feldman and Gemund,? \\Q2006\\E", "shortCiteRegEx": "Feldman and Gemund", "year": 2006}, {"title": "A novel model-based diagnosis engine: Theory and applications", "author": ["A. Fijany", "F. Vatan", "A. Barrett", "M. James", "C. Williams", "R. Mackey"], "venue": "In Proc. IEEE Aerospace\u201903,", "citeRegEx": "Fijany et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fijany et al\\.", "year": 2003}, {"title": "Building Problem Solvers", "author": ["K. Forbus", "J. de Kleer"], "venue": null, "citeRegEx": "Forbus and Kleer,? \\Q1993\\E", "shortCiteRegEx": "Forbus and Kleer", "year": 1993}, {"title": "Improvements to Propositional Satisfiability Search Algorithms", "author": ["J.W. Freeman"], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Freeman,? 1995", "shortCiteRegEx": "Freeman", "year": 1995}, {"title": "Systematic versus stochastic constraint satisfaction", "author": ["E.C. Freuder", "R. Dechter", "M.L. Ginsberg", "B. Selman", "E.P.K. Tsang"], "venue": "In Proc. IJCAI\u201995,", "citeRegEx": "Freuder et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Freuder et al\\.", "year": 1995}, {"title": "Physical impossibility instead of fault models", "author": ["G. Friedrich", "G. Gottlob", "W. Nejdl"], "venue": "In Proc. AAAI\u201990,", "citeRegEx": "Friedrich et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Friedrich et al\\.", "year": 1990}, {"title": "From sampling to model counting", "author": ["C.P. Gomes", "J. Hoffmann", "A. Sabharwal", "B. Selman"], "venue": "In Proc. IJCAI\u201907,", "citeRegEx": "Gomes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2007}, {"title": "Unveiling the ISCAS-85 benchmarks: A case study in reverse engineering", "author": ["M. Hansen", "H. Yalcin", "J. Hayes"], "venue": "IEEE Design & Test,", "citeRegEx": "Hansen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1999}, {"title": "Counting complexity of propositional abduction", "author": ["M. Hermann", "R. Pichler"], "venue": "In Proc. IJCAI\u201907,", "citeRegEx": "Hermann and Pichler,? \\Q2007\\E", "shortCiteRegEx": "Hermann and Pichler", "year": 2007}, {"title": "SAT-encodings, search space structure, and local search performance", "author": ["H. Hoos"], "venue": "Proc. IJCAI\u201999, pp. 296\u2013303.", "citeRegEx": "Hoos,? 1999", "shortCiteRegEx": "Hoos", "year": 1999}, {"title": "Stochastic Local Search: Foundations and Applications", "author": ["H. Hoos", "T. St\u00fctzle"], "venue": null, "citeRegEx": "Hoos and St\u00fctzle,? \\Q2004\\E", "shortCiteRegEx": "Hoos and St\u00fctzle", "year": 2004}, {"title": "Scaling and probabilistic smoothing: Efficient dynamic local search for SAT", "author": ["F. Hutter", "D.A.D. Tompkins", "H.H. Hoos"], "venue": "In Proc. CP\u201902,", "citeRegEx": "Hutter et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2002}, {"title": "Efficient conflict analysis for finding all satisfying assignments of a Boolean circuit", "author": ["H. Jin", "H. Han", "F. Somenzi"], "venue": "In Proc. TACAS\u201905,", "citeRegEx": "Jin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2005}, {"title": "Stochastic local search for Bayesian networks", "author": ["K. Kask", "R. Dechter"], "venue": "In Proc. AISTAT\u201999,", "citeRegEx": "Kask and Dechter,? \\Q1999\\E", "shortCiteRegEx": "Kask and Dechter", "year": 1999}, {"title": "A model counting characterization of diagnoses", "author": ["T.K.S. Kumar"], "venue": "Proc. DX\u201902, pp. 70\u201376.", "citeRegEx": "Kumar,? 2002", "shortCiteRegEx": "Kumar", "year": 2002}, {"title": "First international diagnosis competition - DXC\u201909", "author": ["T. Kurtoglu", "S. Narasimhan", "S. Poll", "D. Garcia", "L. Kuhn", "J. de Kleer", "A. van Gemund", "A. Feldman"], "venue": "In Proc", "citeRegEx": "Kurtoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kurtoglu et al\\.", "year": 2009}, {"title": "The impact of branching heuristics in propositional satisfiability algorithms", "author": ["J.P. Marques-Silva"], "venue": "Proc. EPIA\u201999, pp. 62\u201374.", "citeRegEx": "Marques.Silva,? 1999", "shortCiteRegEx": "Marques.Silva", "year": 1999}, {"title": "Truth maintenance", "author": ["D.A. McAllester"], "venue": "Proc. AAAI\u201990, Vol. 2, pp. 1109\u20131116.", "citeRegEx": "McAllester,? 1990", "shortCiteRegEx": "McAllester", "year": 1990}, {"title": "A theory of diagnosis from first principles", "author": ["R. Reiter"], "venue": "Artificial Intelligence, 32 (1), 57\u201395.", "citeRegEx": "Reiter,? 1987", "shortCiteRegEx": "Reiter", "year": 1987}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artificial Intelligence, 82 (1-2), 273\u2013302.", "citeRegEx": "Roth,? 1996", "shortCiteRegEx": "Roth", "year": 1996}, {"title": "A linear constraint satisfaction approach to cost-based abduction", "author": ["E. Santos Jr."], "venue": "Artificial Intelligence, 65 (1), 1\u201328.", "citeRegEx": "Jr.,? 1994", "shortCiteRegEx": "Jr.", "year": 1994}, {"title": "Fault diagnosis and logic debugging using Boolean satisfiability", "author": ["A. Smith", "A. Veneris", "M.F. Ali", "A. Viglas"], "venue": "IEEE Transactions on CAD of Integrated Circuits and Systems,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Design diagnosis using Boolean satisfiability", "author": ["A. Smith", "A. Veneris", "A. Viglas"], "venue": "In Proc. ASP-DAC\u201904,", "citeRegEx": "Smith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2004}, {"title": "Physical negation\u201d - integrating fault models into the General Diagnostic Engine", "author": ["P. Struss", "O. Dressler"], "venue": "In Readings in Model-Based Diagnosis,", "citeRegEx": "Struss and Dressler,? \\Q1992\\E", "shortCiteRegEx": "Struss and Dressler", "year": 1992}, {"title": "Solving non-clausal formulas with DPLL search", "author": ["C. Thiffault", "F. Bacchus", "T. Walsh"], "venue": "In Proc. CP\u201904,", "citeRegEx": "Thiffault et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Thiffault et al\\.", "year": 2004}, {"title": "On the complexity of proofs in propositional logics", "author": ["G. Tseitin"], "venue": "Siekmann, J., & Wrightson, G. (Eds.), Automation of Reasoning: Classical Papers in Computational Logic (1967\u20131970), Vol. 2. Springer-Verlag.", "citeRegEx": "Tseitin,? 1983", "shortCiteRegEx": "Tseitin", "year": 1983}, {"title": "Conflict-directed A* and its role in model-based embedded systems", "author": ["B. Williams", "R. Ragno"], "venue": "Journal of Discrete Applied Mathematics,", "citeRegEx": "Williams and Ragno,? \\Q2007\\E", "shortCiteRegEx": "Williams and Ragno", "year": 2007}, {"title": "An efficient algorithm for unit propagation", "author": ["H. Zhang", "M.E. Stickel"], "venue": "In Proc. AI-MATH\u201996,", "citeRegEx": "Zhang and Stickel,? \\Q1996\\E", "shortCiteRegEx": "Zhang and Stickel", "year": 1996}], "referenceMentions": [{"referenceID": 40, "context": "The standard MBD formalization (Reiter, 1987) frames a diagnostic problem in terms of a set of logical clauses that include mode-variables describing the nominal and fault status of system components; from this the diagnostic status of the system can be computed given an observation of the system\u2019s sensors.", "startOffset": 31, "endOffset": 45}, {"referenceID": 31, "context": "This can be easily generalized by introducing multi-valued logic or suitable encodings (Hoos, 1999).", "startOffset": 87, "endOffset": 99}, {"referenceID": 47, "context": "Converting a propositional formula to CNF can be done with (Tseitin, 1983) or without (Forbus & de Kleer, 1993) the introduction of intermediate variables.", "startOffset": 59, "endOffset": 74}, {"referenceID": 27, "context": "Under the same restriction, for SD \u2208 SFM, deciding if a first minimal diagnosis exists is NP-hard (Friedrich et al., 1990).", "startOffset": 98, "endOffset": 122}, {"referenceID": 16, "context": "The complexity of a closely-related problem, Propositional Abduction Problems (PAPs), has been studied by Eiter and Gottlob (1995). They show that for a propositional PAP, the problem of determining if a solution exists is \u03a32 -complete.", "startOffset": 106, "endOffset": 131}, {"referenceID": 16, "context": "The complexity of a closely-related problem, Propositional Abduction Problems (PAPs), has been studied by Eiter and Gottlob (1995). They show that for a propositional PAP, the problem of determining if a solution exists is \u03a32 -complete. Computing a minimal diagnosis is a search problem, and hence it is more difficult to pose a decision question for proving complexity results. Consequently, one can just note that computing a diagnosis minimal with respect to \u2286 / \u2264 requires O(log |COMPS|) calls to an NP oracle (Eiter & Gottlob, 1995), asking the oracle at each step if a diagnosis containing at most k faulty components exists. Results on abduction problems indicate that the task of approximate diagnosis is intractable. Roth (1996) has addressed the problems of abductive inference, and of approximating such inference.", "startOffset": 106, "endOffset": 738}, {"referenceID": 0, "context": "Abdelbar (2004) has studied the complexity of approximating Horn abduction problems, showing that even for a particular Horn restriction of the propositional problem of interest, the approximation problem is intractable.", "startOffset": 0, "endOffset": 16}, {"referenceID": 27, "context": "Diagnosing a strong-fault model is known to be strictly more difficult than a weak-fault model (Friedrich et al., 1990).", "startOffset": 95, "endOffset": 119}, {"referenceID": 38, "context": "Of course, random polarity decisions may effect negatively branching heuristics (Marques-Silva, 1999) but such analysis is also beyond the scope of this paper.", "startOffset": 80, "endOffset": 101}, {"referenceID": 39, "context": "To increase the implementation efficiency of Safari, we combine a BCP-based LTMS engine (McAllester, 1990) and a full-fledged DPLL solver in two-stage consistency checking.", "startOffset": 88, "endOffset": 106}, {"referenceID": 25, "context": "For the full DPLL checking we use POSIT (Freeman, 1995) or MiniSat (E\u00e9n & S\u00f6rensson, 2003).", "startOffset": 40, "endOffset": 55}, {"referenceID": 8, "context": "This is solving the satisfiability problem (SAT), which is NP-complete (Cook, 1971).", "startOffset": 71, "endOffset": 83}, {"referenceID": 36, "context": "Instead of enumerating all solutions and filtering the minimal diagnoses only, we have performed model-counting, whose relation to MBD has been extensively studied (Kumar, 2002).", "startOffset": 164, "endOffset": 177}, {"referenceID": 36, "context": "Instead of enumerating all solutions and filtering the minimal diagnoses only, we have performed model-counting, whose relation to MBD has been extensively studied (Kumar, 2002). While it was possible to solve the two smallest circuits, the solver did not terminate for any of the larger models within the predetermined time of 1 hour. The results are shown in Table 5. The second column of Table 5 shows the model count returned by RelSat, with sample single-fault observations from our benchmark. The third column reports the time for model counting. This slow performance on relatively small diagnostic instances leads us to the conclusion that specialized solvers like Safari are better suited for finding minimal diagnoses than off-the-shelf ALLSAT (model counting) implementations that do not encode inference properties similar to those encoded in Safari. We have used the state-of-the-art, non-exact model counting method SampleCount (Gomes, Hoffmann, Sabharwal, & Selman, 2007) to compute lower bounds of the model counts. The results are shown in the third and fourth columns of Table 5. Configured with the default settings (\u03b1 = 3.5, t = 2, z = 20, cutoff 10 000 flips), SampleCount could not find lower bounds for circuits larger than c1355. Although the performance of SampleCount is significantly better than RelSAT, the fact that SampleCount computes lower bounds and does not scale to large circuits prevent us from building a diagnosis algorithm based on approximate model counting. A satisfiability-based method for diagnosing an optimized version of ISCAS85 has been used by Smith, Veneris, and Viglas (2004). In a more recent paper (Smith, Veneris, Ali, & Viglas, 2005), the SAT-based approach has been replaced by a Quantified Boolean Formula (QBF) solver for computing multiple-fault diagnoses.", "startOffset": 165, "endOffset": 1628}, {"referenceID": 40, "context": "Examples of search algorithms include A\u2217-based algorithms, such as CDA\u2217 (Williams & Ragno, 2007) and hitting set algorithms (Reiter, 1987).", "startOffset": 124, "endOffset": 138}, {"referenceID": 9, "context": "Examples of such algorithms include the ATMS (de Kleer, 1986) and other prime-implicant methods (Kean & Tsiknis, 1993), DNNF (Darwiche, 1998), and OBDD (Bryant, 1992).", "startOffset": 125, "endOffset": 141}, {"referenceID": 4, "context": "Examples of such algorithms include the ATMS (de Kleer, 1986) and other prime-implicant methods (Kean & Tsiknis, 1993), DNNF (Darwiche, 1998), and OBDD (Bryant, 1992).", "startOffset": 152, "endOffset": 166}, {"referenceID": 34, "context": "At first glance, it seems like MBD could be efficiently solved using an encoding as a SAT (Jin et al., 2005), constraint satisfaction (Freuder, Dechter, Ginsberg, Selman, & Tsang, 1995) or Bayesian network (Kask & Dechter, 1999) problem.", "startOffset": 90, "endOffset": 108}, {"referenceID": 34, "context": "We show that Safari exploits a particular property of MBD problems, called diagnostic continuity, which improves the optimality of Safari compared to, for example, straightforward ALLSAT encodings (Jin et al., 2005).", "startOffset": 197, "endOffset": 215}, {"referenceID": 26, "context": "Stochastic algorithms have been discussed in the framework of constraint satisfaction (Freuder et al., 1995) and Bayesian network inference (Kask & Dechter, 1999).", "startOffset": 86, "endOffset": 108}, {"referenceID": 25, "context": "Stochastic algorithms have been discussed in the framework of constraint satisfaction (Freuder et al., 1995) and Bayesian network inference (Kask & Dechter, 1999). The latter two approaches can be used for solving suitably translated MBD problems. It is often the case, though, that these encodings are more difficult for search than specialized ones. MBD is an instance of constraint optimization, with particular constraints over failure variables. MBD has developed algorithms to exploit these domain properties, and our proposed approach differs significantly from almost all MBD algorithms that appear in the literature. While most advanced MBD algorithms are deterministic, Safari borrows from SLS algorithms that, rather than backtracking, may randomly flip variable assignments to determine a satisfying assignment. Complete MBD algorithms typically make use of preferences, e.g., fault-mode probabilities, to improve search efficiency; Safari uses this technique on top of its stochastic search over the space of diagnoses. A closely-related diagnostic approach is that of Fijany, Vatan, Barrett, James, Williams, and Mackey (2003), who map the minimal-hitting set problem into the problem of finding an assignment with bounded weight satisfying a monotone SAT problem, and then propose to use efficient SAT algorithms for computing diagnoses.", "startOffset": 87, "endOffset": 1141}, {"referenceID": 0, "context": "area, the primary paper that adopts stochastic local search is by Abdelbar, Gheita, and Amer (2006). In this paper, they present a hybrid two-stage method that is based on Iterated Local Search (ILS) and Repetitive Simulated Annealing (RSA).", "startOffset": 66, "endOffset": 100}, {"referenceID": 0, "context": "area, the primary paper that adopts stochastic local search is by Abdelbar, Gheita, and Amer (2006). In this paper, they present a hybrid two-stage method that is based on Iterated Local Search (ILS) and Repetitive Simulated Annealing (RSA). The ILS stage of the algorithm uses a simple hill-climbing method (randomly flipping assumables) for the local search phase, and tabu search for the perturbation phase. RSA repeatedly applies Simulated Annealing (SA), starting each time from a random initial state. The hybrid method initially starts from an arbitrary state, or a greedily-chosen state. It then applies the ILS algorithm; if this algorithm fails to find the optimal solution after a fixed number \u03c4 of hill-climbing steps8 or after a fixed number R of repetitions of the perturbation-local search cycle,9 ILS-based search is terminated and the RSA algorithm is run until the optimal solution is found. Our work differs from that of Abdelbar et al. (2006) in several ways.", "startOffset": 66, "endOffset": 963}], "year": 2010, "abstractText": "We propose a StochAstic Fault diagnosis AlgoRIthm, called Safari, which trades off guarantees of computing minimal diagnoses for computational efficiency. We empirically demonstrate, using the 74XXX and ISCAS85 suites of benchmark combinatorial circuits, that Safari achieves several orders-of-magnitude speedup over two well-known deterministic algorithms, CDA\u2217 and HA\u2217, for multiple-fault diagnoses; further, Safari can compute a range of multiple-fault diagnoses that CDA\u2217 and HA\u2217 cannot. We also prove that Safari is optimal for a range of propositional fault models, such as the widely-used weak-fault models (models with ignorance of abnormal behavior). We discuss the optimality of Safari in a class of strong-fault circuit models with stuck-at failure modes. By modeling the algorithm itself as a Markov chain, we provide exact bounds on the minimality of the diagnosis computed. Safari also displays strong anytime behavior, and will return a diagnosis after any non-trivial inference time.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}