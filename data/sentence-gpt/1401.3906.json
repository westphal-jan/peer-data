{"id": "1401.3906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Making Decisions Using Sets of Probabilities: Updating, Time Consistency, and Calibration", "abstract": "We consider how an agent should update her beliefs when her beliefs are represented by a set P of probability distributions, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from P. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities. Our results emphasize the key role of the rectangularity condition of Epstein and Schneider. We do not consider that the ideal choice is to maximize uncertainty by avoiding an uncertainty. We also note that a set of probabilities and P values can be applied to various levels of the set, and that one can do different things about the set. Furthermore, we argue that the optimal choice is to maximise uncertainty by avoiding an uncertainty.\n\n\n\n\n\n\nIn other words, we conclude that there is a very large number of questions about the choice of the random-choice choice. A number of variables are included in the following table in order to determine whether a set of probabilities and P values can be used in the set. The key differences in the set of probabilities and P values are presented in the following table in order to identify the variables that affect the selection of the random-choice choice.\n\nRandom Choice\n\nA set of random-choice choices has been proposed to help define what it means to be \"a random choice\". We find that the selection of different stimuli (i.e., objects in a series) is also the main determinant of the choice: the selection of the desired stimulus, or the selection of the chosen stimulus (the first or last, for instance, a given experiment). We find that we can find a selection of different stimuli (i.e., a novel, toy, or a random-choice choice) that has more information about the selected stimulus than is necessary for the selection of the stimulus. In this study we used the random-choice choice criterion for the choice of a set of random-choice choice: the Random Choice criterion. To demonstrate", "histories": [["v1", "Thu, 16 Jan 2014 05:22:33 GMT  (396kb)", "http://arxiv.org/abs/1401.3906v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.GT", "authors": ["peter d grunwald", "joseph y halpern"], "accepted": false, "id": "1401.3906"}, "pdf": {"name": "1401.3906.pdf", "metadata": {"source": "CRF", "title": "Making Decisions Using Sets of Probabilities: Updating, Time Consistency, and Calibration", "authors": ["Peter D. Gr\u00fcnwald", "Joseph Y. Halpern"], "emails": ["pdg@cwi.nl", "halpern@cs.cornell.edu"], "sections": [{"heading": "1. Introduction", "text": "Suppose that an agent models her uncertainty about a domain using a set P of probability distributions. How should the agent update P in light of observing that random variable X takes on value x? Perhaps the standard answer is to condition each distribution in P on X = x (more precisely, to condition those distributions in P that give X = x positive probability on X = x), and adopt the resulting set of conditional distributions P | X = x as her representation of uncertainty. In contrast to the case where P is a singleton, it is often not clear whether conditioning is the right way to update a set P. It turns out that in general, there is no single \u201cright\u201d way to update P. Different updating methods satisfy different desirata, and for some sets P, not all of these desiderata can be satisfied at the same time. In this paper, we determine to what extent conditioning and some related update methods satisfy common decision-theoretic optimality properties. The main three questions we pose are:\n1. Is conditioning the right thing to do under a minimax criterion, that is, does it lead to minimax-optimal decision rules?\n2. Is the minimax criterion itself reasonable in the sense that it satisfies consistency criteria such as time consistency (defined formally below)?\nc\u00a92011 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\n3. Is conditioning the right thing do under a calibration criterion?\nWe show that the answer to the first two questions is \u201cyes\u201d if P satisfies a condition that Epstein and Schneider (2003) call rectangularity, while the answer to the third question is \u201cyes\u201d if P is convex and satisfies the rectangularity condition.1 Thus, the main contribution of this paper is to show that, under the rectangularity condition, conditioning is the right thing to do under a wide variety of criteria. Apart from this main conclusion, our analysis provides new insights into the relation between minimax optimality, time consistency, and variants of conditioning (such as ignoring the information that X = x altogether). We now discuss our contributions in more detail.\n1.1 The Minimax Criterion, Dilation, and Time Inconsistency How should an agent make decisions based on a set P of distributions? Perhaps the best-studied and most commonly-used approach in the literature is to use the minimax criterion (Wald, 1950; Ga\u0308rdenfors & Sahlin, 1982; Gilboa & Schmeidler, 1989). According to the minimax criterion, action a1 is preferred to action a2 if the worst-case expected loss of a1 (with respect to all the probability distributions in the set P under consideration) is better than the worst-case expected loss of a2. Thus, the action chosen is the one with the best worstcase outcome.\nAs has been pointed out by several authors, conditioning a set P on observation X = x sometimes leads to a phenomenon called dilation (Augustin, 2003; Cozman & Walley, 2001; Herron, Seidenfeld, & Wasserman, 1997; Seidenfeld & Wasserman, 1993): the agent may have substantial knowledge about some other random variable Y before observing X = x, but know significantly less after conditioning. Walley (1991, p. 299) gives a simple example of dilation: suppose that a fair coin is tossed twice, where the second toss may depend in an arbitrary way on the first. (In particular, the tosses might be guaranteed to be identical, or guaranteed to be different.) If X represents the outcome of the first toss and Y represents the outcome of the second toss, then before observing X, the agent believes that the probability that Y is heads is 1/2, while after observing X, the agent believes that the probability that Y is heads can be an arbitrary element of [0, 1].\nWhile, as this example and others provided by Walley show, such dilation can be quite reasonable, it interacts rather badly with the minimax criterion, leading to anomalous behavior that has been called time inconsistency (Gru\u0308nwald & Halpern, 2004; Seidenfeld, 2004): the minimax-optimal conditional decision rule before the value of X is observed (which has the form \u201cIf X = 0 then do a1; if X = 1 then do a2; . . . \u201d) may be different from the minimax-optimal decision rule after conditioning. For example, the minimax-optimal conditional decision rule may say \u201cIf X = 0 then do a1\u201d, but the minimax-optimal decision rule conditional on observing X = 0 may be a2. (See Example 2.1.) If uncertainty is modeled using a single distribution, such time inconsistency cannot arise.\n1.2 The Two Games To understand this phenomenon better, we model the decision problem as a game between the agent and a bookie (for a recent approach that is similar in spirit but done independently, see Ozdenoren & Peck, 2008). It turns out that there is more than one possible game that can be considered, depending on what information the\n1. All these results are proved under the assumption that the domain of the probability measures in P is finite and the set of actions that the decision maker is choosing among is finite.\nbookie has. We focus on two (closely related) games here. In the first game, the bookie chooses a distribution from P before the agent moves. We show that the Nash equilibrium of this game leads to a minimax decision rule. (Indeed, this can be viewed as a justification of using the minimax criterion). However, in this game, conditioning on the information is not always optimal.2 In the second game, the bookie gets to choose the distribution after the value of X is observed. Again, in this game, the Nash equilibrium leads to the use of minimax, but now conditioning is the right thing to do.\nIf P is a singleton, the two games coincide (since there is only one choice the bookie can make, and the agent knows what it is). Not surprisingly, conditioning is the appropriate thing to do in this case. The moral of this analysis is that, when uncertainty is characterized by a set of distributions, if the agent is making decision using the minimax criterion, then the right decision depends on the game being played. The agent must consider if she is trying to protect herself against an adversary who knows the value of X = x when choosing the distribution or one that does not know the value of X = x.\n1.3 Rectangularity and Time Consistency In earlier work (Gru\u0308nwald & Halpern, 2004) (GH from now on), we essentially considered the first game, and showed that, in this game, conditioning was not always the right thing to do when using the minimax criterion. Indeed, we showed there are sets P and games for which the minimax-optimal decision rule is to simply ignore the information. Our analysis of the first game lets us go beyond GH here in two ways. First, we provide a simple sufficient condition for when conditioning on the information is minimax optimal (Theorem 4.4). Second, we provide a sufficient condition for when it is minimax optimal to ignore information (Theorem 5.1).\nOur sufficient condition guaranteeing that conditioning is minimax optimal can be viewed as providing a sufficient condition for time consistency. Our condition is essentially Epstein and Schneider\u2019s (2003) rectangularity condition, which they showed was sufficient to guarantee what has been called in the decision theory community dynamic consistency. Roughly speaking, dynamic consistency says that if, no matter what the agent learns, he will prefer decision rule \u03b4 to decision rule \u03b4\u2032, then he should prefer \u03b4 to \u03b4\u2032 before learning anything. Dynamic consistency is closely related to Savage\u2019s (1954) sure-thing principle. Epstein and Schneider show that, if an agent\u2019s uncertainty is represented using sets of probability distributions, all observations are possible (in our setting, this means that all probability distributions that the agent considers possible assign positive probability to all basic events of the form X = x), and the set of distributions satisfies the rectangularity condition then, no matter what the agent\u2019s loss function,3 if the agent prefers \u03b4 to \u03b4\u2032 after making an observation, then he will also prefer \u03b4 to \u03b4\u2032 before making the observation. Conversely, they show that if the agent\u2019s preferences are dynamically consistent, then the agent\u2019s uncertainty can be represented by a set of probability measures that satisfies the rectangularity condition, and the agent can be viewed as making decisions using the minimax criterion.\nOur results show that if all observations are possible and the rectangularity condition holds, then, no matter what the loss function, time consistency holds. Time consistency\n2. In some other senses of the words \u201cconditioning\u201d and \u201coptimal,\u201d conditioning on the information is always optimal. This is discussed further in Section 7. 3. We work with loss functions in this paper rather than utility functions, since losses seem to be somewhat more standard in this literature. However, we could trivially restate our results in terms of utility.\nholds if a decision is minimax optimal before making an observation iff it is optimal after making the observation. Note that time consistency just considers just the optimal decision, while dynamic consistency considers the whole preference order. However, time consistency is an \u201ciff\u201d requirement: a decision is optimal before making the observation if and only if that decision is optimal after making the observation. By way of contrast, dynamic consistency is uni-directional: if a is preferred to a\u2032 after making the observation, then it must still be preferred before making the observation.\nThese results show that if uncertainty is represented by a rectangular set of measures, all observations are possible, and the minimax criterion is used, then both dynamic consistency and time consistency hold. On the other hand, as we show in Proposition 4.7, in general dynamic consistency and time consistency are incomparable.\n1.4 C-conditioning and Calibration As stated, we provide a general condition on P under which conditioning is minimax optimal, as well as a general condition under which ignoring the information is minimax optimal. Note that ignoring the information can also be viewed as the result of conditioning; not conditioning on the information, but conditioning on the whole space. This leads us to consider a generalization of conditioning. Let C be a partition of the set of values of the random variable X, and let C(x) be the element of the partition that contains x. Suppose that when we observe x, we condition on the event X \u2208 C(x). We call this variant of conditioning C-conditioning; standard conditioning is just the special case where each element of C is a singleton. Is C-conditioning always minimax optimal in the first game? That is, is it always optimal to condition on something? As we show by considering a variation of the Monty Hall Problem (Example 5.4), this is not the case in general.\nNevertheless, it turns out that considering C-conditioning is useful; it underlies our analysis of calibration. As pointed out by Dawid (1982), an agent updating her beliefs and making decisions on the basis of these beliefs should also be concerned about being calibrated. Calibration is usually defined in terms of empirical data. To explain what it means and its connection to decision making, consider an agent that is a weather forecaster on your local television station. Every night the forecaster makes a prediction about whether or not it will rain the next day in the area where you live. She does this by asserting that the probability of rain is p, where p \u2208 {0, 0.1, . . . , 0.9, 1}. How should we interpret these probabilities? The usual interpretation is that, in the long run, on those days at which the weather forecaster predict probability p, it will rain approximately 100p% of the time. Thus, for example, among all days for which she predicted 0.1, the fraction of days with rain was close to 0.1. A weather forecaster with this property is said to be calibrated. If a weather forecaster is calibrated, and you make bets which, based on her probabilistic predictions, seem favorable, then in the long run you cannot lose money. On the other hand, if a weather forecaster is not calibrated, there exist bets that may seem favorable but result in a loss. So clearly there is a close connection between calibration and decision making.\nCalibration is usually defined relative to empirical data or singleton distributions. We first consider the obvious extension to sets of probabilities, but the obvious extension turns out to be only a very weak requirement. We therefore define a stronger and arguably more interesting variation that we call sharp calibration. We take an update rule \u03a0 to map a set\nP and a value x to a new set \u03a0(P, x) of probabilities. Intuitively, \u03a0(P, x) is the result of updating P given the observation X = x, according to update rule \u03a0. A calibrated update rule \u03a0 is sharply calibrated for P if there is no other rule \u03a0\u2032 that is also calibrated such that, for all x, \u03a0\u2032(P, x) \u2282 \u03a0(P, x), and for some x, the inclusion is strict. We first show that if P is convex, then C-conditioning is sharply calibrated for some C; different choices of P require different C. We then show that, if P also satisfies the rectangularity condition, then standard conditioning is sharply calibrated.\n1.5 Discussion Both the idea of representing uncertainty by a set P of distributions and that of handling decisions in a worst-case optimal manner may, of course, be criticized. While we do not claim that this is necessarily the \u201cright\u201d or the \u201cbest\u201d approach, it is worth pointing out that two of the most common criticisms are, to some extent, unjustified. First, since it may be hard for an agent to determine the precise boundaries of the set P, it has been argued that \u201csoft boundaries\u201d are more appropriate. These soft boundaries may be thought of as inducing a single distribution on \u2206(X \u00d7 Y), the set of probability distributions on X \u00d7 Y (with the density of Pr \u2208 \u2206(X \u00d7 Y) proportional to the extent to which \u201cPr is included in the set P\u201d). With this single distribution, the setting becomes equivalent to the setting of standard Bayesian decision theory. The problem with this criticism is that in some cases, hard boundaries are in fact natural. For example, some conditional probabilities may be known to be precisely 0, as is the case in the Monty Hall game (Example 5.4). Similarly, the use of the minimax criterion is not as pessimistic as is often thought. The minimax solution often coincides with the Bayes-optimal solution under some \u201cmaximum entropy\u201d prior (Gru\u0308nwald & Dawid, 2004), which is not commonly associated with being overly pessimistic. In fact, in the Monty Hall problem, the minimaxoptimal decision rule coincides with the solution usually advocated, which requires making further assumptions about P to reduce it to a singleton.\nThe rest of this paper is organized as follows. In Section 2, we define the basic framework. In Section 3, we formally define the two games described above and show that the minimaxoptimal decision rule gives a Nash equilibrium. In Section 4, we characterize the minimaxoptimal decision rule for the first game, in which the bookie chooses a distribution before X is observed. In Section 5 we discuss C-conditioning and show that, in general, it is not minimax optimal. In Section 6, we discuss calibration and C-conditioning. We conclude with some discussion in Section 7. All proofs can be found in the appendix."}, {"heading": "2. Notation and Definitions", "text": "In this paper, uncertainty is represented by a set P of probability distributions. For ease of exposition, we assume throughout this paper that we are interested in two random variables, X and Y , which can take values in spaces X and Y, respectively. P always denotes a set of distributions on X \u00d7 Y; that is, P \u2286 \u2206(X \u00d7 Y), where \u2206(S) denotes the set of probability distributions on S. For ease of exposition, we assume that P is a closed set; this is a standard assumption in the literature that seems quite natural in our applications, and makes the statement of our results simpler (otherwise we have to state our results using closures). If Pr \u2208 \u2206(X \u00d7Y), let PrX and PrY denote the marginals of Pr on X and Y, respectively. Let PY = {PrY : Pr \u2208 P}. If E \u2286 X \u00d7 Y, then let P | E = {Pr | E : Pr \u2208 P,Pr(E) > 0}. Here\nPr | E (often written Pr(\u00b7 | E)) is the distribution on X \u00d7 Y obtained by conditioning on E.\nThe represesentation of uncertainty using sets of probability distributions is closely related to Walley\u2019s (1991) use of (lower and upper) previsions. A prevision is an expectation function; that is, a lower prevision is a mapping random variables to the reals satisfying certain properties. It is well known (Huber, 1981) that what Walley calls a coherent lower prevision (a lower prevision satisfying some minimal properties) can be identified with the lower expectation of a set of probability measures (that is, the function E such that E(X) = infPr\u2208P EPr(X)). Indeed, there is a one-to-one map between lower previsions and closed convex sets of probability measures. The notion of conditioning we are using corresponds to what Walley calls the regular extension of a lower prevision (see Walley, 1991, Appendix J).\n2.1 Loss Functions As in GH, we are interested in an agent who must choose some action from a set A, where the loss of the action depends only on the value of random variable Y . We assume in this paper that X , Y, and A are finite, and that |A| \u2265 2, so that there are always at least two possible choices. (If we allowed A to be a singleton, then some of our results would not hold for trivial reasons.)\nWe assume that with each action a \u2208 A and value y \u2208 Y is associated some loss to the agent. (The losses can be negative, which amounts to a gain.) Let L : Y \u00d7 A \u2192 IR be the loss function.\nSuch loss functions arise quite naturally. For example, in a medical setting, we can take Y to consist of the possible diseases and X to consist of symptoms. The set A consists of possible courses of treatment that a doctor can choose. The doctor\u2019s loss function depends only on the patient\u2019s disease and the course of treatment, not on the symptoms. But, in general, the doctor\u2019s choice of treatment depends on the symptoms observed.\n2.3 Decision Problems and Decision Settings For our purposes, a decision setting is a tuple DS = (X ,Y,A,P), where X , Y, A, and P are as above. A decision problem is characterized by a tuple DP = (X ,Y,A,P, L), where L is a loss function. That is, a decision problem is a decision setting together with a loss function. We say that the decision problem (X ,Y,A,P, L) is based on the decision setting (X ,Y,A,P).\n2.4 Decision Rules Given a decision problem DP = (X ,Y,A,P, L), suppose that the agent observes the value of the variable X. After having observed X, she must perform an act, the quality of which is judged according to loss function L. The agent must choose a decision rule that determines what she does as a function of her observations. We allow decision rules to be randomized. Thus, a decision rule is a function \u03b4 : X \u2192 \u2206(A) that chooses a distribution over actions based on the agent\u2019s observations. Let D(X ,A) be the set of all decision rules. A special case is a deterministic decision rule, which assigns probability 1 to a particular action. If \u03b4 is deterministic, we sometimes abuse notation and write \u03b4(x) for the action that is assigned probability 1 by the distribution \u03b4(x). Given a decision rule \u03b4 and a loss function L, let L\u03b4 be the random variable on X \u00d7 Y such that L\u03b4(x, y) = \u2211 a\u2208A \u03b4(x)(a)L(y, a). Here \u03b4(x)(a) stands for the probability of performing action a according to the distribution \u03b4(x) over actions that is adopted when x is observed. Note that in the special case that \u03b4 is a deterministic decision rule, L\u03b4(x, y) = L(y, \u03b4(x)).\nWe also extend this notation to randomized actions: for \u03b1 \u2208 \u2206(A), we let L\u03b1 be the random variable on Y such that L\u03b1(y) = \u2211 a\u2208A \u03b1(a)L(y, a).\nA decision rule \u03b40 is a priori minimax optimal for the decision problem DP if\nmax Pr\u2208P EPr[L\u03b40 ] = min \u03b4\u2208D(X ,A) maxPr\u2208PEPr[L\u03b4].\nThat is, \u03b40 is a priori minimax optimal if \u03b40 gives the best worst-case expected loss with respect to all the distributions in Pr. We can write max here instead of sup because of our assumption that P is closed. This ensures that there is some Pr \u2208 P for which EPr[L\u03b40 ] takes on its maximum value.\nA decision rule \u03b41 is a posteriori minimax optimal for DP if, for all x \u2208 X such that Pr(X = x) > 0 for some Pr \u2208 P,\nmax Pr\u2208P|X=x EPr[L\u03b41 ] = min \u03b4\u2208D(X ,A) max Pr\u2208P|X=x EPr[L\u03b4]. (1)\nTo get the a posteriori minimax-optimal decision rule we do the obvious thing: if x is observed, we simply condition each probability distribution Pr \u2208 P on X = x, and choose the action that gives the least expected loss (in the worst case) with respect to P | X = x. Since all distributions Pr mentioned in (1) satisfy Pr(X = x) = 1, the minimum over \u03b4 \u2208 D(X ,A) does not depend on the values of \u03b4(x\u2032) for x\u2032 6= x; the minimum is effectively over randomized actions rather than decision rules.\nAs the following example, taken from GH, shows, a priori minimax-optimal decision rules are in general different from a posteriori minimax-optimal decision rules.\nExample 2.1: Suppose that X = Y = A = {0, 1} and P = {Pr \u2208 \u2206(X \u00d7 Y) : PrY(Y = 1) = 2/3}. Thus, P consists of all distributions whose marginal on Y gives Y = 1 probability 2/3. We can think of the actions in A as predictions of the value of Y . The loss function is 0 if the right value is predicted and 1 otherwise; that is, L(i, j) = |i\u2212 j|. This is the so-called 0/1 or classification loss. It is easy to see that the optimal a priori decision rule is to choose 1 no matter what is observed (which has expected loss 1/3). Intuitively, observing the value of X tells us nothing about the value of Y , so the best decision is to predict according to the prior probability of Y = 1. However, all probabilities on Y = 1 are compatible with observing either X = 0 or X = 1. That is, both (P | X = 0)Y and (P | X = 1)Y consist of all distributions on Y. Thus, the minimax optimal a posteriori decision rule randomizes (with equal probability) between Y = 0 and Y = 1.\nTo summarize, if you make decisions according to the minimax optimality criterion, then before making an observation, you will predict Y = 1. However, no matter what observation you make, after making the observation, you will randomize (with equal probability) between predicting Y = 0 and Y = 1. Moreover, you know even before making the observation that your opinion as to the best decision rule will change in this way. (Note that this is an example of both time inconsistency and dynamic inconsistency.)\n2.5 Time and Dynamic Consistency Formally, a decision problem DP is time consistent iff, for all decision rules \u03b4, \u03b4 is a priori minimax optimal for DP iff \u03b4 is a posteriori minimax optimal. We say that DP is weakly time consistent if every a posteriori minimax optimal rule for DP is also a priori minimax optimal for DP . A decision setting DS is (weakly) time consistent if every decision problem based on DS is.\nFollowing Epstein and Schneider (2003), we say that a decision problem DP is dynamically consistent if for every pair \u03b4, \u03b4\u2032 of decision rules, the following conditions both hold:\n1. If, for all x such that Pr(X = x) > 0 for some Pr \u2208 P,\nmax Pr\u2208(P|X=x) EPr[L\u03b4] \u2264 max Pr\u2208(P|X=x) EPr[L\u03b4\u2032 ], (2)\nthen max Pr\u2208P EPr[L\u03b4] \u2264 max Pr\u2208P EPr[L\u03b4\u2032 ]. (3)\n2. If, for all x such that Pr(X = x) > 0 for some Pr \u2208 P, we have strict inequality in (2), then (3) must hold with strict inequality as well.\nInformally, dynamic consistency means that whenever \u03b4 is preferred to \u03b4\u2032 according to the minimax criterion a posteriori, then \u03b4 is also preferred to \u03b4\u2032 according to the minimax criterion a priori, and that whenever the a posteriori preference is strict for all possible observations, then the a priori preference must be strict as well.\nA decision setting DS is dynamically consistent if every decision problem based on DS is."}, {"heading": "3. Two Game-Theoretic Interpretations of P", "text": "What does it mean that an agent\u2019s uncertainty is characterized by a set P of probability distributions? How should we understand P? We give P a game-theoretic interpretation here: namely, an adversary gets to choose a distribution from the set P.4 But this does not completely specify the game. We must also specify when the adversary makes the choice. We consider two times that the adversary can choose: the first is before the agents observes the value of X , and the second is after. We formalize this as two different games, where we take the \u201cadversary\u201d to be a bookie.\nWe call the first game the P-game. It is defined as follows:\n1. The bookie chooses a distribution Pr \u2208 P. 2. The value x of X is chosen (by nature) according to PrX and observed by both bookie\nand agent.\n3. The agent chooses an action a \u2208 A. 4. The value y of Y is chosen according to Pr | X = x. 5. The agent\u2019s loss is L(y, a); the bookie\u2019s loss is \u2212L(y, a).\nThis is a zero-sum game; the agent\u2019s loss is the bookie\u2019s gain. In this game, the agent\u2019s strategy is a decision rule, that is, a function that gives a distribution over actions for each observed value of X. The bookie\u2019s strategy is a distribution over distributions in P.\nWe now consider a second interpretation of P, characterized by a different game that gives the bookie more power. Rather than choosing the distribution before observing the value of X, the bookie gets to choose the distribution after observing the value. We call this the P-X-game. Formally, it is specified as follows:\n4. This interpretation remains meaningful in several practical situations where there is no explicit adversary; see the final paragraph of this section.\n1. The value x of X is chosen (by nature) according to some procedure that is guaranteed to end up with a value of x for which Pr(X = x) > 0 for some Pr \u2208 P, and observed by both the bookie and the agent.5\n2. The bookie chooses a distribution Pr \u2208 P such that Pr(X = x) > 0.6\n3. The agent chooses an action a \u2208 A. 4. The value y of Y is chosen according to Pr | X = x. 5. The agent\u2019s loss is L(y, a); the bookie\u2019s loss is \u2212L(y, a). Recall that a pair of strategies (S1, S2) is a Nash equilibrium if neither party can do better by unilaterally changing strategies. If, as in our case, (S1, S2) is a Nash equilibrium in a zero-sum game, it is also known as a \u201csaddle point\u201d; S1 must be a minimax strategy, and S2 must be a maximin strategy (Gru\u0308nwald & Dawid, 2004). As the following results show, an agent must be using an a priori minimax-optimal decision rule in a Nash equilibrium of the P-game, and an a posteriori minimax-optimal decision rule is a Nash equilibrium of the P-X-game. This can be viewed as a justification for using (a priori and a posteriori) minimax-optimal decision rules.\nTheorem 3.1: Fix X , Y, A, L, and P \u2286 \u2206(X \u00d7 Y).\n(a) The P-game has a Nash equilibrium (\u03c0\u2217, \u03b4\u2217), where \u03c0\u2217 is a distribution over P with finite support.\n(b) If (\u03c0\u2217, \u03b4\u2217) is a Nash equilibrium of the P-game such that \u03c0\u2217 has finite support, then\n(i) for every distribution Pr\u2032 \u2208 P in the support of \u03c0\u2217, we have EPr\u2032 [L\u03b4\u2217 ] = maxPr\u2208PEPr[L\u03b4\u2217 ];\n(ii) if Pr\u2217 = \u2211\nPr\u2208P,\u03c0\u2217(Pr)>0 \u03c0 \u2217(Pr) Pr (i.e., Pr\u2217 is the convex combination of the\ndistributions in the support of \u03c0\u2217, weighted by their probability according to \u03c0\u2217), then\nEPr\u2217 [L\u03b4\u2217 ] = min\u03b4\u2208D(X ,A)EPr\u2217 [L\u03b4]\n= maxPr\u2208P min\u03b4\u2208D(X ,A)EPr[L\u03b4] = min\u03b4\u2208D(X ,A)maxPr\u2208PEPr[L\u03b4] = maxPr\u2208PEPr[L\u03b4\u2217 ].\nOnce nature has chosen a value for X in the P-X-game, we can regard steps 2\u20135 of the P-X-game as a game between the bookie and the agent, where the bookie\u2019s strategy is characterized by a distribution in P | X = x and the agent\u2019s is characterized by a distribution over actions. We call this the P-x-game.\nTheorem 3.2: Fix X , Y, A, L, P \u2286 \u2206(X \u00d7 Y).\n5. Because x is observed by both parties, and y is chosen after x is chosen, the procedure by which nature chooses x is irrelevant. We could assume for definiteness that nature chooses uniformly at random among the values x such that Pr(x) > 0 for some Pr \u2208 P, but any other choice would work equally well. 6. If we were to consider conditional probability distributions (de Finetti, 1936; Popper, 1968), for which Pr(Y = y | X = x) is defined even if Pr(X = x) = 0, then we could drop the restriction that x is chosen such that Pr(X = x) > 0 for some Pr \u2208 P.\n(a) The P-x-game has a Nash equilibrium (\u03c0\u2217, \u03b4\u2217(x)), where \u03c0\u2217 is a distribution over P | X = x with finite support.\n(b) If (\u03c0\u2217, \u03b4\u2217(x)) is a Nash equilibrium of the P-x-game such that \u03c0\u2217 has finite support, then\n(i) for all Pr\u2032 in the support of \u03c0\u2217, we have EPr\u2032 [L\u03b4\u2217 ] = maxPr\u2208P|X=xEPr[L\u03b4\u2217 ]; (ii) if Pr\u2217 = \u2211\nPr\u2208P,\u03c0\u2217(Pr)>0 \u03c0 \u2217(Pr) Pr, then\nEPr\u2217 [L\u03b4\u2217 ] = min\u03b4\u2208D(X ,A)EPr\u2217 [L\u03b4]\n= maxPr\u2208P|X=x min\u03b4\u2208D(X ,A)EPr[L\u03b4] = min\u03b4\u2208D(X ,A)maxPr\u2208P|X=xEPr[L\u03b4] = maxPr\u2208P|X=xEPr[L\u03b4\u2217 ].\nSince all distributions Pr in the expression min\u03b4\u2208D(X ,A)maxPr\u2208P|X=xEPr[L\u03b4] in part (b)(ii) are in P | X = x, as in (1), the minimum is effectively over randomized actions rather than decision rules.\nTheorems 3.1 and 3.2 can be viewed as although, according to the definition, there is time inconsistency, when viewed properly, there is no real inconsistency here; rather, we must just be careful about what game is being played. If the P-game is being played, the right strategy is the a priori minimax-optimal strategy, both before and after the value of X is observed; similarly, if the P-X-game is being played, the right strategy is the a posteriori minimax-optimal strategy, both before and after the value of X is observed. Indeed, thinking in terms of the games explains the apparent time inconsistency. In both games, the agent gains information by observing X = x. But in the P-X game, so does the bookie. The information may be of more use to the bookie than the agent, so, in this game, the agent can be worse off by being given the opportunity to learn the value of X.\nOf course, in most practical situations, agents (robots, statisticians,. . . ) are not really confronted with a bookie who tries to make them suffer. Rather, the agents may have no idea at all what distribution holds, except that it is in some set P. Because all they know is P, they decide to prepare themselves for the worst-case and play the minimax strategy. The fact that such a minimax strategy can be interpreted in terms of a Nash equilibrium of a game helps to understand differences between different forms of minimax (such as a priori and a posteriori minimax). From this point of view, it seems strange to have a bookie choose between different distributions in P according to some distribution \u03c0\u2217. However, if P is convex, we can replace the distribution \u03c0\u2217 on P by a single distribution in P, which consists of the convex combination of the distributions in the support of \u03c0\u2217; this is just the distribution Pr\u2217 of Theorems 3.1 and 3.2. Thus, Theorems 3.1 and 3.2 hold with the bookie restricted to a deterministic strategy."}, {"heading": "4. Conditioning, Rectangularity, and Time Consistency", "text": "To get the a posteriori minimax-optimal decision rule we do the obvious thing: if x is observed, we simply condition each probability distribution Pr \u2208 P on X = x, and choose the action that gives the least expected loss (in the worst case) with respect to P | X = x.\nWe might expect that the a priori minimax-optimal decision rule should do the same thing. That is, it should be the decision rule that says, if x is observed, then we choose\nthe action that again gives the best result (in the worst case) with respect to P | X = x. But Example 2.1 shows that this cannot be true in general, since in some cases the a priori optimal decision rule is not to condition, but to ignore the observed value of X, and just choose the action that gives the least expected loss (in the worst case) with respect to P, no matter what value X has. We later show that there are cases in which the optimal a priori rule is neither to condition nor to ignore (see Example 5.4). Our goal in this section is to show that the rectangularity condition of Epstein and Schneider (2003) suffices to guarantee that conditioning is optimal.\nDefinition 4.1: Let \u3008P\u3009, the hull of P, be the set\n{Pr \u2208 \u2206(X \u00d7 Y) : PrX \u2208 PX and, if Pr(X = x) 6= 0, then (Pr | X = x) \u2208 (P | X = x)} .\nThus, \u3008P\u3009 consists of all distributions Pr whose marginal on X is the marginal on X of some distribution in P and whose conditional on observing X = x is the conditional of some distribution in P, for all x \u2208 X . Clearly P \u2286 \u3008P\u3009, but the converse is not necessarily true, as the following example shows.\nExample 4.2: Suppose that X = Y = {0, 1}, and Pr1,Pr2,Pr3 \u2208 \u2206(X \u00d7Y) are defined as follows:\n\u2022 Pr1(0, 0) = Pr1(1, 0) = 1/3; Pr1(0, 1) = Pr1(1, 1) = 1/6;\n\u2022 Pr2(0, 0) = Pr2(1, 0) = 1/6; Pr2(0, 1) = Pr2(1, 1) = 1/3;\n\u2022 Pr3(0, 0) = Pr3(1, 1) = 1/3; Pr3(0, 1) = Pr3(1, 0) = 1/6.\nSuppose that P = {Pr1,Pr2}. Then Pr3 6\u2208 P, but it is easy to see that Pr3 \u2208 \u3008P\u3009. For (Pr1)X = (Pr2)X = (Pr3)X is the uniform distribution on X , Pr3 | (X = 0) = Pr1 | (X = 0), and Pr3 | (X = 1) = Pr2 | (X = 1).\nNote also that for the P in Example 2.1, we have \u3008P\u3009 = \u2206(X \u00d7 Y) 6= P. The notion of the hull arises in a number of contexts. In the language of Walley (1991), the hull of P is the natural extension of the marginals PX and the collection of sets of conditional probabilities P | X = x for x \u2208 X . Thus, if P = \u3008P\u3009, then we can reconstruct the joint probability distributions P from PX and the collection of sets of conditional probabilities. The assumption that P = \u3008P\u3009 is closely related to a set of probabilities being separately specified, introduced by da Rocha and Cozman (2002). As da Rocha and Cozman point out, this assumption makes it possible to apply ideas from Bayesian networks to uncertainty represented by sets of probability distributions.\nThe condition P = \u3008P\u3009 is an instance of the rectangularity condition which goes back at least to the work of Sarin and Wakker (1998). It was introduced in its most general form by Epstein and Schneider (2003). Epstein and Schneider define this condition for a sequence of random variables X1, . . . , Xt, where the support of each Xj is not necessarily finite. In the special case that t = 2, and X := X1 and Y := X2 are restricted to have finite support, the rectangularity condition is exactly equivalent to our condition that P = \u3008P\u3009.\nConsidering \u3008P\u3009 also gives some insight into the two games that we considered in Section 3. In the P-X -game, the bookie has more power than in the P-game, since he gets to choose the distribution after the agent observes x in the P-X -game, and must choose it before the agent observes x in the P-game. That means that the agent can draw inferences about the distribution that the bookie chose in the P-game. Such inferences cannot be drawn if P = \u3008P\u3009. More generally, in a precise sense, the agent has the same information about Y in the P-X -game as in the \u3008P\u3009-game. Rather than making this formal (since it is somewhat tangential to our main concerns), we give an example to show the intuition.\nExample 4.3: Suppose that X = Y = {0, 1}, and P = {Pr1,Pr2}, where\n\u2022 Pr1(0, 0) = (1\u2212 ), Pr1(0, 1) = (1\u2212 )2, Pr1(1, 0) = (1\u2212 ), and Pr1(1, 1) = 2;\n\u2022 Pr2(0, 0) = (1\u2212 ), and Pr2(0, 1) = 2, Pr2(1, 0) = (1\u2212 ), Pr2(1, 1) = (1\u2212 )2.\nIn the P-game, if the agent observes that X = 0, then he is almost certain that the bookie chose Pr1, and thus is almost certain that Y = 1. On the other hand, in the PX-game, when the agent observes x, he has no idea whether the bookie will choose Pr1 or Pr2 (since the bookie makes this choice after observing x), and has no idea whether Y is 0 or 1. Note that P 6= \u3008P\u3009; in particular, there is a distribution Pr3 \u2208 \u3008P\u3009 such that (Pr3)X = (Pr1)X and (Pr3) | (X = 0) = (Pr2) | (X = 0). For example, we can take Pr3 such that Pr3(0, 0) = (1\u2212 )2 and Pr3(0, 1) = (1\u2212 ) (the values of Pr3(1, 0) and Pr3(1, 1) are irrelevant, as long as they sum to and are nonnegative). Thus, after observing that X = 0 in the \u3008P\u3009 game, the agent would have no more of an idea of the value of Y than he does in the P-X game.\nThe key point for us here is that when P = \u3008P\u3009, conditioning is optimal, as the following theorem shows. We first need a definition. We call P conservative if for all Pr \u2208 P and all x \u2208 X , Pr(X = x) > 0.7\nTheorem 4.4: Given a decision setting DS = (X ,Y,A,P) such that P = \u3008P\u3009, then for all decision problems DP based on DS, there exists an a priori minimax-optimal rule that is also a posteriori minimax optimal. Indeed, every a posteriori minimax-optimal rule is also a priori minimax optimal, so DS and DP are weakly time consistent. Moreover, if P is conservative, then for every decision problem DP based on DS, every a priori minimaxoptimal rule is also a posteriori minimax optimal, so DS and DP are time consistent.\nThis raises the question as to whether the qualification \u201cthere exists\u201d in Theorem 4.4 is necessary, and whether the converse of the theorem also holds. Example 4.5 shows that the answer to the first question is yes; Example 4.6 shows that the answer to the second question is no.\nExample 4.5: If for some x \u2208 X , there exist Pr,Pr\u2032 \u2208 P such that Pr(X = x) = 0 and Pr\u2032(X = x) > 0, then there may be an a priori minimax decision rule that is not a posteriori minimax. For example, consider the decision problem DP = (X ,Y,A,P, L) with\n7. Our notion of conservative corresponds to what Epstein and Schneider (2003) call the full support condition.\nX = {0, 1},A = Y = {0, 1, 2}, L the classification loss (Example 2.1) and P = {Pr1,Pr2}. We first define Pr1:\nPr1(X = 1) = 1/2, Pr1(Y = 0 | X = 0) = Pr1(Y = 1 | X = 0) = Pr1(Y = 2 | X = 0) = 1/3, and Pr1(Y = 0 | X = 1) = 1/2, Pr1(Y = 1 | X = 1) = 2/5, Pr1(Y = 2 | X = 1) = 1/10.\nPr2 is defined as follows: Pr2(X = 0) = 1, and for all j \u2208 Y, Pr2(Y = j,X = 0) = Pr2(Y = j | X = 0) := Pr1(Y = j | X = 0). It is easy to see that P = \u3008P\u3009, so the rectangularity condition holds.\nNote that \u03b4(0), the decision taken when observing X = 0, does not affect the expected loss; for both Pr1 | X = 0 and Pr2 | X = 0, Y is uniform, so the expected loss is 2/3, regardless of \u03b4(0). This implies that every decision rule \u03b4 with \u03b4(1) a randomized combination of {0, 1} is a priori optimal, and has worst-case expected loss 2/3, since EPr2 [L\u03b4] = 2/3 and EPr1 [L\u03b4] < 2/3. But the minimax optimal rules with \u03b4(1) = 1 are not a posteriori optimal, since if the player observes X = 1, he knows that the distribution is Pr1, and the minimax loss relative to Pr1 is 1/2 for action 0 and 3/5 for action 1.\nBoth in this example and in Example 4.3, observing a particular value of X gives information about which distribution in P the bookie has chosen. In Example 4.3, observing X = 0 implies that the bookie almost certainly chose Pr1 in the P-game; in the present example, observing X = 1 implies that the bookie certainly chose Pr1 in both the P-game and the P \u2212X game. We note, however, that observing X = x can give information about the distribution chosen by the bookie in the P \u2212X game only if there exist Pr and Pr\u2032 in the P-game such that Pr(X = x) = 0 and Pr\u2032(X = x) > 0. If no such Pr and Pr\u2032 exists, then the bookie is completely free to choose any Pr \u2208 P he likes after x has been observed, so observing x gives no information about which Pr \u2208 P has been chosen.\nThere exist decision settings such that P is conservative and P 6= \u3008P\u3009, although we still have weak time consistency. Hence, the converse of Theorem 4.4 does not hold in general. We now give an example of such a P.\nExample 4.6: Let X = A = Y = {0, 1} and P = {Pr0,Pr1} with Pr0(X = 1) = Pr1(X = 1) = 1/2 and for x \u2208 {0, 1}, Pr0(Y = 0 | X = x) = 1 and Pr1(Y = 1 | X = x) = 1. Clearly P is conservative and P 6= \u3008P\u3009; for example, the distribution Pr3 such that Pr3(X = 1) = 1/2, Pr3(Y = 0 | X = 0) = 1, and Pr3(Y = 0 | X = 1) = 0 is in \u3008P\u3009 \u2212 P. Note that X and Y are independent with respect to both Pr0 and Pr1. Now take an arbitrary loss function L. Since (Pr | X = x)Y contains two distributions, one with Pr(Y = 1) = 0 and one with Pr(Y = 1) = 1, the minimax a posteriori act is to play \u03b4(0) = \u03b4(1) = (1 \u2212 \u03b1\u2217) \u00b7 0 + \u03b1\u2217 \u00b7 1 (i.e., the act that plays 0 with probability 1 \u2212 \u03b1\u2217 and 1 with probability \u03b1\u2217), where \u03b1\u2217 is chosen so as to minimize f(\u03b1) = max{(1\u2212 \u03b1)L(0, 0) + \u03b1L(0, 1), (1\u2212 \u03b1)L(1, 0) + \u03b1L(1, 1)}. For simplicity, assume that there is a unique such \u03b1\u2217. (If not, then it must be the case that all \u03b1 \u2208 [0, 1] minimize this expression, and it is easy to check L(0, 0) = L(0, 1) = L(1, 0) = L(1, 1), so time consistency holds trivially.)\nWe want to show that \u03b4 is also a priori minimax. It is easy to check that\nmax Pr\u2208{Pr0,Pr1}\nL\u03b4 = f(\u03b1 \u2217),\nwhere f is as above. So it suffices to show that for any decision rule \u03b4\u2032, we must have\nmax Pr\u2208{Pr0,Pr1}\nL\u03b4\u2032 \u2265 f(\u03b1\u2217),\nSuppose that \u03b4(x) = (1\u2212 \u03b2x) \u00b7 0 + \u03b2x \u00b7 1, for x \u2208 {0, 1}. Then\nmaxPr\u2208{Pr0,Pr1}EPr[L\u03b4\u2032 ]\n= max{12((1\u2212 \u03b20)L(0, 0) + \u03b20L(0, 1) + (1\u2212 \u03b21)L(0, 0) + \u03b21L(0, 1)), 1 2((1\u2212 \u03b20)L(1, 0) + \u03b20L(1, 1) + (1\u2212 \u03b21)L(1, 0) + \u03b21L(1, 1)} = max{(1\u2212 \u03b3)L(0, 0) + \u03b3L(0, 1), (1\u2212 \u03b3)L(1, 0) + \u03b3L(1, 1)}, where \u03b3 = \u03b20+\u03b212 = f(\u03b3) \u2265 f(\u03b1\u2217).\nIt is interesting to compare Theorem 4.4 with the results of Epstein and Schneider (2003). For this, we first compare our notion of time consistency with their notion of dynamic consistency. Both notions were formally defined at the end of Section 2. Our results are summarized in Proposition 4.7. First we need two definitions: Let P be a set of distributions on X \u00d7Y. A decision problem is based on P if it is of the form (X ,Y,A,P, L) for some arbitrary A and L. A decision problem satisfies strong dynamic consistency if it satisfies condition (2) of the definition of dynamic consistency and satisfies the following strengthening of (3):\n\u2022 If, for all x such that Pr(X = x) > 0 for some Pr \u2208 P, (2) holds, and for some x such that Pr(X = x) > 0, we have\nmax Pr\u2208(P|X=x) EPr[L\u03b4] < max Pr\u2208(P|X=x) EPr[L\u03b4\u2032 ], (4)\nthen (3) must hold with strict inequality.\nProposition 4.7:\n(a) Every dynamically consistent decision problem is also weakly time consistent.\n(b) Not every dynamically consistent decision problem is time consistent.\n(c) Every strongly dynamically consistent decision problem is time consistent.\n(d) There exist weakly time consistent decision problems that are not dynamically consistent.\n(e) All decision problems based on P are dynamically consistent if and only if all decision problems based on P are weakly time consistent.\nProposition 4.7(c) shows that the comparison between time consistency and dynamic consistency is subtle: replacing \u2018for all x\u2019 by \u201cfor some x\u2019 in the second half of the definition of dynamic consistency, which leads to a perfectly reasonable requirement, suffices to force time consistency. Proposition 4.7(e) leads us to suspect that a decision setting is weakly time consistent if and only if it is dynamically consistent. We have, however, no proof of this claim. The proof of part (e) involves two decision problems based on the same set P, but with different sets of actions, so these decision problems are not based on the same decision setting. It does not seem straightforward to extend the result to decision settings.\nEpstein and Schneider show, among other things, that if P is closed, convex, conservative, and rectangular, then DS is is dynamically consistent, and hence weakly time consistent. We remark that the convexity assumption is not needed for this result. It easy to check that \u03b4 is prefered to \u03b4\u2032 with respect to P according to the minimax criterion iff \u03b4 is preferred to \u03b4\u2032 with respect to the convex closure of P according to the minimax criterion. Proposition 4.7 shows that dynamic and time consistency are closely related. Yet, while there is clear overlap in what we prove in Theorem 4.4 and the Epstein-Schneider (ES from now on) result, in general the results are incomparable. For example, we can already prove weak time consistency without assuming conservativeness; ES assume conservativeness throughout. On the other hand, ES also show that if dynamic consistency holds, then the agent\u2019s actions can be viewed as being the minimax optimal actions relative to a rectangular convex conservative set; we have no analogous result for time consistency. Moreover, in contrast to the ES result, our results hold only for the restricted setting with just two time steps, one before and one after making a single observation.\n5. Belief Updates and C-conditioning\nIn this section we define the notion of a belief update rule, when belief is represented by sets of probabilities, and introduce a natural family of belief update rules which we call C-conditioning.\nTo motivate these notions, recall that Example 2.1 shows that the minimax-optimal a priori decision rule is not always the same as the minimax-optimal a posteriori decision rule. In this example, the minimax-optimal a priori decision rule ignores the information observed. Formally, a rule \u03b4 ignores information if \u03b4(x) = \u03b4(x\u2032) for all x, x\u2032 \u2208 X . If \u03b4 ignores information, define L\u2032\u03b4 to be the random variable on Y such that L\u2032\u03b4(y) = L\u03b4(x, y) for some choice of x. This is well defined, since L\u03b4(x, y) = L\u03b4(x\n\u2032, y) for all x, x\u2032 \u2208 X . The following theorem provides a general sufficient condition for ignoring information\nto be optimal.\nTheorem 5.1: Fix X , Y, L, A, and P \u2286 \u2206(X \u00d7 Y). If, for all PrY \u2208 PY , P contains a distribution Pr\u2032 such that X and Y are independent under Pr\u2032, and Pr\u2032Y = PrY , then there is an a priori minimax-optimal decision rule that ignores information. Under these conditions, if \u03b4 is an a priori minimax-optimal decision rule that ignores information, then \u03b4 essentially optimizes with respect to the marginal on Y ; that is, maxPr\u2208P EPr[L\u03b4] = maxPrY\u2208PY EPrY [L \u2032 \u03b4].\nGH focused on the case that PY is a singleton (i.e., the marginal probability on Y is the same for all distributions in P) and for all x, PY \u2286 (P | X = x)Y . It is immediate from Theorem 5.1 that ignoring information is a priori minimax optimal in this case.\nStandard conditioning and ignoring information are both instances of C-conditioning, which in turn is an instance of an update rule. We now define these notions formally.\nDefinition 5.2: A belief update rule (or just an update rule) is a function \u03a0 : 2\u2206(X\u00d7Y)\u00d7X \u2192 2\u2206(X\u00d7Y) \u2212 {\u2205} mapping a set P of distributions and an observation x to a nonempty set \u03a0(P, x) of distributions; intuitively, \u03a0(P, x) is the result of updating P with the observation x.\nIn the case where P is a singleton {Pr}, then one update rule is conditioning; that is, \u03a0({Pr}, x) = {Pr(\u00b7 | X = x)}. But other update rules are possible, even for a single distribution; for example, Lewis (1976) considered an approach to updating that he called imaging. There is even more scope when considering sets of probabilities; for example, both Walley\u2019s (1991) natural extension and regular extension provide update rules (as we said, our notion of conditioning can be viewed as an instance of Walley\u2019s regular extension). Simply ignoring information provides another update rule: \u03a0(P, x) = P. As we said above, ignoring information and standard conditioning are both instances of C-conditioning.\nDefinition 5.3: Let C = {X1, . . . ,Xk} be a partition of X ; that is, Xi 6= \u2205 for i = 1, . . . , k; X1 \u222a . . . \u222a Xk = X ; and Xi \u2229 Xj = \u2205 for i 6= j. If x \u2208 X , let C(x) be the cell containing x; that is, the unique element Xi \u2208 C such that x \u2208 Xi. The C-conditioning belief update rule is the function \u03a0 defined by taking \u03a0(P, x) = P | C(x) (if for all Pr \u2208 P, Pr(C(x)) = 0, then \u03a0(P, x) is undefined). A decision rule \u03b4 is based on C-conditioning if it amounts to first updating the set P to P | C(x), and then taking the minimax-optimal distribution over actions relative to (P | C(x))Y . Formally, \u03b4 is based on C-conditioning if, for all x \u2208 X with Pr(X = x) > 0 for some Pr \u2208 P,\nmax Pr\u2208(P|X\u2208C(x))Y EPr[L\u03b4(x)] = min \u03b3\u2208\u2206(A) max Pr\u2208(P|X\u2208C(x))Y EPr[L\u03b3 ].\nStandard conditioning is a special case of C-conditioning, where we take C to consist of all singletons; ignoring information is also based on C-conditioning, where C = {X}. Our earlier results suggest that perhaps an a priori minimax-optimal decision rule must be based on C-conditioning for some C. The Monty Hall problem again shows that this conjecture is false.\nExample 5.4: [Monty Hall] (Mosteller, 1965; vos Savant, 1990): We start with the original Monty Hall problem, and then consider a variant of it. Suppose that you\u2019re on a game show and given a choice of three doors. Behind one is a car; behind the others are goats. You pick door 1. Before opening door 1, Monty Hall, the host (who knows what is behind each door) opens one of the other two doors, say, door 3, which has a goat. He then asks you if you still want to take what\u2019s behind door 1, or to take what\u2019s behind door 2 instead. Should you switch? You may assume that initially, the car was equally likely to be behind each of the doors.\nWe formalize this well-known problem as a P-game, as follows: Y = {1, 2, 3} represents the door which the car is behind. X = {G2, G3}, where, for j \u2208 {2, 3}, Gj corresponds to the quizmaster showing that there is a goat behind door j. A = {1, 2, 3}, where action a \u2208 A corresponds to the door you finally choose, after Monty has opened door 2 or 3. The loss function is once again the classification loss, L(i, j) = 1 if i 6= j, that is, if you choose a door with a goat behind it, and L(i, j) = 0 if i = j, that is, if you choose a door with a car. P is the set of all distributions Pr on X \u00d7 Y satisfying\nPrY(Y = 1) = PrY(Y = 2) = PrY(Y = 3) = 1 3\nPr(Y = 2 | X = G2) = Pr(Y = 3 | X = G3) = 0.\nNote that P does not satisfy the rectangularity condition. For example, let Pr\u2217 be the distribution such that Pr\u2217(G2, 1) = Pr \u2217(G2, 3) = 1/3 and Pr \u2217(G3, 1) = Pr\n\u2217(G3, 2) = 1/6. It is easy to see that Pr\u2217 \u2208 \u3008P\u3009 \u2212 P.\nIt is well known, and easy to show, that the a priori minimax-optimal strategy is always to switch doors, no matter whether Monty opens door 2 or door 3. Formally, let \u03b4S be the decision rule such that \u03b4S(G2) = 3 and \u03b4S(G3) = 2. Then \u03b4S is the unique a priori minimaxoptimal decision rule (and has expected loss 1/3). The rule \u03b4S is also a posteriori minimax optimal. But now we modify the problem so that there is a small cost, say > 0, associated with switching. The cost is associated both with switching to door 2 and with switching to door 3. As long as is sufficiently small, the action \u03b4S of always switching is still uniquely a priori minimax optimal. However, now \u03b4S is not based on C-conditioning. There exist only two partitions of X . The corresponding two update rules based on C-conditioning amount to, respectively, (1) ignoring X, and (2) conditioning on X in the standard way. The decision rule based on ignoring the information is to stick to door 1, because there is a cost associated with switching. The decision rule based on conditioning is to switch doors with probability 1/(2+ ). To see this, consider the observation X = G2, and let \u03b1 be the randomized action of switching to door 3 with probability q and sticking to door 1 with probability 1\u2212 q. Let m(q) = maxPr\u2208(P|X=G2)Y EPr[L\u03b1]. Thus, m(q) = maxp\u2208[0,1/2](qp(1 + ) + (1 \u2212 q)(1 \u2212 p)). Again, to compute m(q), we need to consider only what happens when is at the extremes of the interval; that is, when p = 0 or p = 1/2, so m(q) = max(1\u2212 q, (1 + q )/2). Clearly m(q) is minimized when 1\u2212q = (1+q )/2, that is, when q = 1/(2+ ). A similar analysis applies when the observation X = G3. Thus, neither of the decision rules based on conditioning is minimax optimal.\nAlthough C-conditioning does not guarantee minimax optimality, it turns out to be a useful notion. As we show in the next section, it is quite relevant when we consider calibration."}, {"heading": "6. Calibration", "text": "As we said in the introduction, Dawid (1982) pointed out that an agent who is updating his beliefs should want to be calibrated. In this section, we consider the effect of requiring calibration. Up to now, calibration has been considered only when uncertainty is characterized by a single distribution. Below we generalize the notion of calibration to our setting, where uncertainty is characterized by a set of distributions. We then investigate the connection\nbetween calibration and some of the other conditions that we considered earlier, specifically the conditions that P is convex and P = \u3008P\u3009.8\nCalibration is typically defined with respect to empirical data. We view the set P of distributions not as describing empirical data, but as defining an agent\u2019s uncertainty regarding the true distribution. We want to define calibration in such a setting. For the case that P is a singleton, this has already been done, for example, by Vovk, Gammerman, and Shafer (2005). 9 Below, we first define calibration for the case where P is a singleton, and then extend the notion to general P.\nLet \u03a0 be an update rule such that \u03a0({Pr}, x) contains just a single distribution for each x \u2208 X (for example, \u03a0 could be ordinary conditioning). Given x \u2208 X and \u03a0, define [x]\u03a0,P = {x\u2032 : (\u03a0(P, x\u2032))Y = \u03a0(P, x)Y}. Thus, [x]\u03a0,P consists of all values x\u2032 that, when observed, lead to the same updated marginal distributions as x.\nDefinition 6.1 : The update rule \u03a0 is calibrated relative to Pr if, for all x \u2208 X , if Pr([x]\u03a0,{Pr}) 6= 0, then Pr(\u00b7 | [x]\u03a0,{Pr})Y = \u03a0({Pr}, x)Y .10\nIn words, this definition says that if Pr\u2032 is the distribution on Y that results from updating Pr after observing x according to \u03a0 and then marginalizing to Y, then \u03a0 is calibrated if Pr\u2032 is also the marginal distribution that results when conditioning Pr on the set of values x\u2032 that, when observed, result in Pr\u2032 being the marginal distribution according to \u03a0. Intuitively, for each x that may be observed, an agent who uses \u03a0 produces a distribution \u03a0({Pr}, x). The agent may then make decisions or predictions about Y based on this distribution, marginalized to Y. We consider the set P \u2032 of all distributions on Y that the agent may use to predict Y after observing the value of X. That is, Pr\u2032 \u2208 P \u2032 iff with positive Pr-probability the agent, after observing the value of X, uses Pr\u2032 to predict Y . The set P \u2032 has at most |X | elements. Definition 6.1 then says that, for each Pr\u2032 \u2208 P \u2032, whenever the agent predicts with Pr\u2032, the agent is \u201ccorrect\u201d in the sense that the distribution of Y given that the agent uses Pr\u2032 is indeed to Pr\u2032. Note that in Definition 6.1, as in all subsequent definitions in this section, we marginalize on Y. We discuss this further at the end of this section. It is straightforward to generalize Definition 6.1 to sets P of probability distributions that are not singletons, and update rules \u03a0 that map to sets of probabilities.\nDefinition 6.2: The update rule \u03a0 is calibrated relative to P if, for all x \u2208 X , if Pr([x]\u03a0,P) 6= 0 for some Pr \u2208 P, then (P | [x]\u03a0,P)Y = \u03a0(P, x)Y .\nWe now want to relate calibration and C-conditioning. The following result is a first step in that direction. It gives conditions under which standard conditioning is calibrated, and also shows that, for convex P and arbitrary C, C-conditioning satisfies one of the two inclusions required by Definition 6.2.\n8. Recall that convexity is an innocuous assumption in the context of time and dynamic consistency. However, as we show in this section, it is far from innocuous in the context of calibration. 9. Vovk et al.\u2019s setting is somewhat different from ours, because they are interested only in upper bounds on, rather than precise values of, probabilities. As a result, their definition of \u201cvalidity\u201d (as they call their notion of calibration) is somewhat different from Definition 6.1, but the underlying idea is the same. We have found no definition in the literature that coincides with ours.\n10. As usual, if A \u2286 X , then we identify P | A with P | (A\u00d7 Y).\nTheorem 6.3:\n(a) If \u03a0 is C-conditioning for some partition C of X and P is convex then, for all x \u2208 X , we have that (P | [x]\u03a0,P)Y \u2286 \u03a0(P, x)Y .\n(b) If \u03a0 is standard conditioning, P = \u3008P\u3009, and x \u2208 X , then \u03a0(P, x)Y \u2286 (P | [x]\u03a0,P)Y .\nCorollary 6.4 : If P is convex and P = \u3008P\u3009, then standard conditioning is calibrated relative to P.\nThis corollary will be significantly strengthened in Theorem 6.12 below. In general, both convexity and the P = \u3008P\u3009 condition are necessary in Corollary 6.4, as the following two examples show.\nExample 6.5: Let X = Y = {0, 1}, let P = {Pr1,Pr2,Pr3,Pr4}, where Pr1, . . . ,Pr4 are defined below as a sequence of four numbers (a, b, c, d), with Pri(0, 0) = a, Pri(0, 1) = b, Pri(1, 0) = c, and Pri(1, 1) = d):\n\u2022 Pr1 = (1/4, 1/4, 1/4, 1/4),\n\u2022 Pr2 = (1/8, 3/8, 1/8, 3/8),\n\u2022 Pr3 = (1/4, 1/4, 1/8, 3/8),\n\u2022 Pr4 = (1/8, 3/8, 1/4, 1/4).\nClearly P is not convex. Note that Pr1(Y = 0 | X = 0) = Pr1(Y = 0 | X = 1) = 1/2,Pr2(Y = 0 | X = 0) = Pr2(Y = 0 | X = 1) = 1/4, and Pr3(Y = 0 | X = 0) = 1/2, Pr3(Y = 0 | X = 1) = 1/4. Since, for all Pr \u2208 P, Pr(X = 0) = 1/2, and (P | X = 0)Y = (P | X = 1)Y = {Pra,Prb} where Pra(Y = 0) = 1/2 and Prb(Y = 0) = 1/4, we have P = \u3008P\u3009. We now show that standard conditioning is not calibrated relative to P. Let \u03a0 stand for standard conditioning. For x \u2208 {0, 1}, we have\n\u03a0(P, x)Y = (P | X = x)Y = {Pr\u20321,Pr\u20322}, (5)\nwhere Pr\u20321(Y = 0) = 1/2 and Pr \u2032 2(Y = 0) = 1/4. It also follows that, for x \u2208 {0, 1}, [x]\u03a0,P = {0, 1} = X , so that (P | [x]\u03a0,P)Y = PY . (6)\nSince PY contains a distribution Pr\u20323 such that Pr\u20323(Y = 0) = 3/8, (5) and (6) together show that \u03a0 is not calibrated.\nExample 6.6: Let X = Y = {0, 1}, and let P consist of all distributions on X \u00d7 Y with Pr(Y = 1) = 0.5. Clearly P is convex. However, P 6= \u3008P\u3009. To see this, note that P contains a distribution Pr with Pr(Y = 0 | X = 0) = 1 and a distribution Pr\u2032 with Pr\u2032(X = 0) = 1, but no distribution Pr\u2032\u2032 with Pr\u2032\u2032(X = 0) = 1 and Pr\u2032\u2032(Y = 0 | X = 0) = 1. Let \u03a0 stand for standard conditioning. We now show that \u03a0 is not calibrated. For x \u2208 {0, 1}, we have\n\u03a0(P, x)Y = (P | X = x)Y = \u2206(Y), (7)\nthat is, conditioning both on X = 0 and on X = 1 leads to the set of all distributions on Y. It follows that, for x \u2208 {0, 1}, [x]\u03a0,P = {0, 1} = X , so that\n(P | [x]\u03a0,P)Y = PY = {Pr \u2208 \u2206(Y) | Pr(Y = 1) = 0.5}. (8)\nTogether, (7) and (8) show that \u03a0 is not calibrated.\nCorollary 6.4 gives conditions under which standard conditioning is calibrated. Theorem 6.3(a) gives general conditions under which C-conditioning satisfies one inclusion required for calibration; specifically, (P | [x]\u03a0,P)Y \u2286 \u03a0(P, x)Y . Rather than trying to find conditions under which the other inclusion holds, we consider a strengthening of calibration, which is arguably a more interesting notion. For, as the following example shows, calibration it is arguably too weak a requirement.\nExample 6.7: Let X = Y = {0, 1}, and let P = {Pr} consist of all distributions on X \u00d7Y satisfying Pr(Y = X) = 1. Then the rule \u03a0 that ignores X, that is, with \u03a0(P, x) = P for x \u2208 {0, 1}, is calibrated, even though (a) it outputs all distributions on Y, and (b) there exists another calibrated rule (standard conditioning) that, upon observing X = x, outputs only one distribution on Y.\nIntuitively, the fewer distributions that there are in P, the more information P contains. Thus, we want to restrict ourselves to sets P that are as small as possible, while still being calibrated.\nDefinition 6.8: Update rule \u03a0\u2032 is narrower than update rule \u03a0 relative to P if, for all x \u2208 X , \u03a0\u2032(P, x)Y \u2286 \u03a0(P, x)Y . \u03a0\u2032 is strictly narrower relative to P if the inclusion is strict for some x. \u03a0 is sharply calibrated if there exists no update rule \u03a0\u2032 that is strictly narrower than \u03a0 and that is also calibrated.\nWe now show that if P is convex, then every sharply calibrated update rule must involve C-conditioning. To make this precise, we need the following definition.\nDefinition 6.9: \u03a0 is a generalized conditioning update rule if, for all convex P, there exists a partition C (that may depend on P) such that for all x \u2208 X , \u03a0(P, x) = P | C(x).\nNote that, as long as P is convex, in a generalized conditioning rule, we condition on a partition of X , but the partition may depend on the set P. For example, for some convex P, the rule may ignore the value of x, whereas for other convex P, it may amount to ordinary conditioning. Since we are only interested in generalized conditioning rules when P is convex, their behavior on nonconvex P is irrelevant. Indeed, the next result shows that, if we require only that P be convex (and do not require that P = \u3008P\u3009), then C-conditioning is calibrated, indeed, sharply calibrated, for some C; moreover, every sharply calibrated update rule must be a generalized conditioning rule.\nTheorem 6.10: Suppose that P is convex.\n(a) C-conditioning is sharply calibrated relative to P for some partition C.\n(b) If \u03a0 is sharply calibrated relative to P, then there exists some C such that \u03a0 is equivalent to C-conditioning on P (i.e., \u03a0(P, x) = \u03a0 | C(x) for all x \u2208 X ).\nCorollary 6.11: There exists a generalized conditioning update rule that is sharply calibrated relative to all convex P. Moreover, every update rule that is sharply calibrated relative to all convex P is a generalized conditioning update rule relative to the set of all convex P.\nTheorem 6.10 establishes a connection between sharp calibration and C-conditioning. We now show that the same conditions that make standard conditioning calibrated also make it sharply calibrated.\nTheorem 6.12: If P is convex and P = \u3008P\u3009, then standard conditioning is sharply calibrated relative to P.\nThis result shows that the P = \u3008P\u3009 condition in Theorem 6.12 is not just relevant for ensuring time consistency, but also for ensuring the well-behavedness of conditioning in terms of calibration. Note, however, that the result says nothing about C-conditioning for arbitrary partitions C. In general, C-conditioning may be sharply calibrated relative to some convex P with P = \u3008P\u3009, but not relative to others. For example, if P is a singleton, then it is convex, P = \u3008P\u3009, and the update rule that ignores x is sharply calibrated. In Example 6.7, P is also convex and P = \u3008P\u3009, yet ignoring x is not sharply calibrated.\nRemark All the results in this section were based on a definition of calibration in which the updated set of distributions \u03a0(P, x) is marginalized to Y. It is also possible to define calibration without this marginalization. However, we found that this makes for a less interesting notion. For example, without marginalizing on Y there no longer seems to be a straightforward way of defining \u201csharp\u201d calibration, and without sharpness, the notion is of quite limited interest. Moreover, it does not seem possible to state and prove an analogue of Theorem 6.3 (at least, we do not know how to do it)."}, {"heading": "7. Discussion and Related Work", "text": "We have examined how to update uncertainty represented by a set of probability distributions, where we motivate updating rules in terms of the minimax criterion. Our key innovation has been to show how different approaches can be understood in terms of a game between a bookie and an agent, where the bookie picks a distribution from the set and the agent chooses an action after making an observation. Different approaches to updating arise depending on whether the bookie\u2019s choice is made before or after the observation. We believe that this game-theoretic approach should prove useful more generally in understanding different approaches to updating. In fact, after the publication of the conference version of this paper, we learned that Ozdenoren and Peck (2008) use the same type of approach for analyzing dynamic situations related to the Ellsberg (1961) paradox. Like us, Ozdenoren and Peck resolve apparent time inconsistency by describing the decision problem as a game between an agent and a bookie (called \u201cmalevolent nature\u201d by them). Just as we do, they point out that different games lead to different Nash equilibria, and hence different minimax optimal strategies for the agent. In particular, although the precise definitions\ndiffer, their game \u03931 is similar in spirit to our P-game, and their game \u03933 is in the spirit of our P-X-game.\nWe (as well as Ozdenoren and Peck, 2008) prove our results under the assumptions that the set of possible values of X and Y is finite, as is the set of actions. It would be of interest to extend this results to the case where these sets are infinite. The extension seems completely straightforward in the case that the set of values and the set of actions is countable, and we only consider bounded loss functions (i.e. supy\u2208Y,a\u2208A |L(y, a)| < \u221e). Indeed, we believe that our results should go through without change in this case, although we have not checked the details. However, once we allow an uncountable set of values, then some subtleties arise. For example, in the P-X game, we required nature to choose a value x that was given positive probability by some Pr \u2208 P. But there may not be such an x if the set of possible values of X is the interval [0, 1]; all the measures in P may then assign individual points probability 0.\nWe conclude this paper by giving an overview of the senses in which conditioning is optimal and the senses in which it is not, when uncertainty is represented by a set of distributions. We have established that conditioning the full set P on X = x is minimax optimal in the P-x-game, but not in the P-game. The minimax-optimal decision rule in the P-game is often an instance of C-conditioning, a generalization of conditioning. The Monty Hall problem showed, however, that this is not always the case. On the other hand, if instead of the minimax criterion, we insist that update rules are sharply calibrated, then if P is convex, C-conditioning is always the right thing to do after all. While, in general, C may depend on P (Theorem 6.10), if P = \u3008P\u3009, we can take C(x) = {x}, so standard conditioning is the \u201cright\u201d thing to do (Theorem 6.12).\nThere are two more senses in which conditioning is the right thing to do. First, Walley (1991) shows that, in a sense, conditioning is the only updating rule that is coherent, according to his notion of coherence. He justifies coherence decision theoretically, but not by using the minimax criterion. Note that the minimax criterion puts a total order on decision rules. That is, we can say that \u03b4 is at least as good as \u03b4\u2032 if\nmax Pr\u2208P EPr[L\u03b4] \u2264 max Pr\u2208P EPr[L\u03b4\u2032 ].\nBy way of contrast, Walley (1991) puts a partial preorder11 on decision rules by taking \u03b4 to be at least as good as \u03b4\u2032 if\nmax Pr\u2208P\nEPr[L\u03b4 \u2212 L\u03b4\u2032 ] \u2264 0.\nSince both maxPr\u2208PEPr[L\u03b4\u2212L\u03b4\u2032 ] and maxPr\u2208PEPr[L\u03b4\u2032 \u2212L\u03b4] may be positive, this is indeed a partial order. If we use this ordering to determine the optimal decision rule then, as Walley shows, conditioning is the only right thing to do.\nSecond, in this paper, we interpreted \u201cconditioning\u201d as conditioning the full given set of distributions P. Then conditioning is not always an a priori minimax optimal strategy on the observation X = x. Alternatively, we could first somehow select a single Pr \u2208 P, condition Pr on the observedX = x, and then take the optimal action relative to Pr | X = x. It follows from Theorem 3.1 that the minimax-optimal decision rule \u03b4\u2217 in a P-game can be\n11. For a partial order is reflexive, transitive, and anti-symmetric, so that if x y and x y, we must have x = y. A partial preorder is just reflexive and transitive.\nunderstood this way. It defines the optimal response to the distribution Pr\u2217 \u2208 \u2206(X \u00d7 Y) defined in Theorem 3.1(b)(ii). If P is convex, then Pr\u2217 \u2208 P. In this sense, the minimaxoptimal decision rule can always be viewed as an instance of \u201cconditioning,\u201d but on a single special Pr\u2217 that depends on the loss function L rather than on the full set P.\nIt is worth noting that Grove and Halpern (1998) give an axiomatic characterization of conditioning sets of probabilities, based on axioms given by van Fraassen (1987, 1985) that characterize conditioning in the case that uncertainty is described by a single probability distribution. As Grove and Halpern point out, their axioms are not as compelling as those of van Fraassen. It would be interesting to know whether a similar axiomatization can be used to characterize the update notions that we have considered here."}, {"heading": "Acknowledgments", "text": "A preliminary version of this paper appears in Uncertainty in Artificial Intelligence, Proceedings of the Eighteenth Conference, 2008, with the title \u201cA Game-Theoretic Analysis of Updating Sets of Probabilities\u201d. The present paper expands on the conference version in several ways. Most importantly, the section on calibration has been entirely rewritten, with a significant error corrected. We would like to thank Wouter Koolen, who pointed out an error in a previous version of Definition 5.3, and the anonymous referees for their thoughtful remarks. Peter Gru\u0308nwald is also affiliated with Leiden University, Leiden, the Netherlands. He was supported by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. Joseph Halpern was supported in part by NSF under grants ITR-0325453, IIS-0534064, IIS-0812045, and IIS-0911036, by AFOSR under grant FA9550-05-1-0055 and FA9550-08-1-0438, and by ARO under grant W911NF-09-1-0281."}, {"heading": "Appendix A. Proofs", "text": "To prove Theorems 3.1 and Theorem 3.2, we need two preliminary observations. The first is a characterization of Nash equilibria. In the P-game, a Nash equilibrium or saddle point amounts to a pair (\u03c0\u2217, \u03b4\u2217) where \u03c0\u2217 is a distribution in P and \u03b4\u2217 is a randomized decision rule such that\nE\u03c0\u2217EPr[L\u03b4\u2217 ] = min\u03b4\u2208D(X ,A)E\u03c0\u2217 [EPr[L\u03b4]]\n= maxPr\u2208PEPr[L\u03b4\u2217 ], (9)\nwhere E\u03c0\u2217 [EPr[L\u03b4]] is just \u2211 Pr\u2208P,\u03c0\u2217(Pr)>0 \u03c0 \u2217(Pr)EPr[L\u03b4]. In the P-x-game, a Nash equilibrium is a pair (\u03c0\u2217, \u03b4\u2217) where \u03c0\u2217 is a distribution in P | X = x and \u03b4\u2217 is a randomized decision rule, such that (9) holds with P replaced by P | X = x.\nThe second observation we need is the following special case of Theorem 3.2 from the work of Gru\u0308nwald and Dawid (2004), itself an extension of Von Neumann\u2019s original minimax theorem.\nTheorem A.1: If Y \u2032 is a finite set, P \u2032 is a closed and convex subset of \u2206(Y \u2032), A\u2032 a closed and convex subset of IRk for some k \u2208 IN , and L\u2032 : Y \u2032\u00d7A\u2032 \u2192 IR is a bounded function such that, for each y \u2208 Y \u2032, L(y, a) is a continuous function of a, then there exists some Pr\u2217 \u2208 P \u2032\nand some \u03c1\u2217 \u2208 A\u2032 such that,\nEPr\u2217 [L \u2032(Y \u2032, \u03c1\u2217)] = min\u03c1\u2208A\u2032EPr\u2217 [L \u2032(Y \u2032, \u03c1)] = maxPr\u2208P \u2032EPr[L \u2032(Y \u2032, \u03c1\u2217)]. (10)\nWith these observations, we are ready to prove Theorem 3.1: Theorem 3.1: Fix X , Y, A, L, and P \u2286 \u2206(X \u00d7 Y).\n(a) The P-game has a Nash equilibrium (\u03c0\u2217, \u03b4\u2217), where \u03c0\u2217 is a distribution over P with finite support.\n(b) If (\u03c0\u2217, \u03b4\u2217) is a Nash equilibrium of the P-game such that \u03c0\u2217 has finite support, then\n(i) for every distribution Pr\u2032 \u2208 P in the support of \u03c0\u2217, we have\nEPr\u2032 [L\u03b4\u2217 ] = maxPr\u2208PEPr[L\u03b4\u2217 ];\n(ii) if Pr\u2217 = \u2211\nPr\u2208P,\u03c0\u2217(Pr)>0 \u03c0 \u2217(Pr) Pr (i.e., Pr\u2217 is the convex combination of the\ndistributions in the support of \u03c0\u2217, weighted by their probability according to \u03c0\u2217), then\nEPr\u2217 [L\u03b4\u2217 ] = min\u03b4\u2208D(X ,A)EPr\u2217 [L\u03b4]\n= maxPr\u2208P min\u03b4\u2208D(X ,A)EPr[L\u03b4] = min\u03b4\u2208D(X ,A)maxPr\u2208PEPr[L\u03b4] = maxPr\u2208PEPr[L\u03b4\u2217 ].\nProof: To prove part (a), we introduce a new loss function L\u2032 that is essentially equivalent to L, but is designed so that Theorem A.1 can be applied. Let Y \u2032 = X\u00d7Y, letA\u2032 = D(X ,A), and define the function L\u2032 : Y \u2032 \u00d7A\u2032 \u2192 IR as\nL\u2032((x, y), \u03b4) := L\u03b4(x, y) = \u2211 a\u2208A \u03b4(x)(a)L(y, a).\nObviously L\u2032 is equivalent to L in the sense that for all Pr \u2208 \u2206(X \u00d7Y), for all \u03b4 \u2208 D(X ,A),\nEPr[L\u03b4] = EPr[L \u2032((X,Y ), \u03b4)].\nIf we view A\u2032 = D(X ,A) as a convex subset of IR|X |\u00b7(|A|\u22121), then L\u2032((x, y), a) becomes a continuous function of a \u2208 A\u2032. Let P \u2032 be the convex closure of P. Since X \u00d7 Y is finite, P \u2032 consists of all distributions Pr\u2217 on (X ,Y) of the form c1 Pr1 + \u00b7 \u00b7 \u00b7 + ck Prk for k = |X \u00d7 Y|, where Pr1, . . . ,Prk \u2208 P and c1, . . . , ck are nonnegative real coefficients such that c1 + \u00b7 \u00b7 \u00b7 + ck = 1. Applying Theorem A.1 to L\u2032 and P \u2032, it follows that (10) holds for some Pr\u2217 \u2208 P \u2032 and some \u03b4\u2217 \u2208 A\u2032 = D(X ,A) (that is, the \u03c1\u2217 in (10) is \u03b4\u2217). Thus, there must be some distribution \u03c0\u2217 on P with finite support such that Pr\u2217 = \u2211 Pr\u2208P,\u03c0\u2217(Pr)>0 \u03c0\n\u2217(Pr) Pr. It is easy to see that the two equalities in (10) are literally the two equalities in (9). Thus, (\u03c0\u2217, \u03b4\u2217) is a Nash equilibrium. This proves part (a).\nTo prove part (b)(i), suppose first that (\u03c0\u2217, \u03b4\u2217) is a Nash equilibrium of the P-game such that \u03c0\u2217 has finite support. Let V = maxPr\u2208P EPr[L\u03b4\u2217 ]. By (9), we have that\u2211\nPr\u2208P,\u03c0\u2217(Pr)>0 \u03c0\u2217(Pr)EPr[L\u03b4\u2217 ] = V. (11)\nTrivially, for each Pr\u2032 \u2208 P, we must have EPr\u2032 [L\u03b4\u2217 ] \u2264 maxPr\u2208P EPr[L\u03b4\u2217 ]. If this inequality were strict for some Pr\u2032 \u2208 P in the support of \u03c0\u2217, then\u2211\nPr\u2208P,\u03c0\u2217(Pr)>0 \u03c0\u2217(Pr)EPr[L\u03b4\u2217 ] < V,\ncontradicting (11). This proves part (b)(i). To prove part (b)(ii), note that straightforward arguments show that\nmaxPr\u2208P EPr[L\u03b4\u2217 ] \u2265 min\u03b4\u2208D(X ,A) maxPr\u2208P EPr[L\u03b4] \u2265 maxPr\u2208P min\u03b4\u2208D(X ,A)EPr[L\u03b4] \u2265 min\u03b4\u2208D(X ,A)EPr\u2217 [L\u03b4].\n(The second inequality follows because, for all Pr\u2032 \u2208 P, min\u03b4\u2208D(X ,A) maxPr\u2208P EPr[L\u03b4] \u2265 min\u03b4\u2208D(X ,A)EPr\u2032 [L\u03b4].) Since (\u03c0\n\u2217, \u03b4\u2217) is a Nash equilibrium, part (b)(ii) is immediate, using the equalities in (9).\nTheorem 3.2: Fix X , Y, A, L, P \u2286 \u2206(X \u00d7 Y).\n(a) The P-x-game has a Nash equilibrium (\u03c0\u2217, \u03b4\u2217(x)), where \u03c0\u2217 is a distribution over P | X = x with finite support.\n(b) If (\u03c0\u2217, \u03b4\u2217(x)) is a Nash equilibrium of the P-x-game such that \u03c0\u2217 has finite support, then\n(i) for all Pr\u2032 in the support of \u03c0\u2217, we have\nEPr\u2032 [L\u03b4\u2217 ] = maxPr\u2208P|X=xEPr[L\u03b4\u2217 ];\n(ii) if Pr\u2217 = \u2211\nPr\u2208P,\u03c0\u2217(Pr)>0 \u03c0 \u2217(Pr) Pr, then\nEPr\u2217 [L\u03b4\u2217 ] = min\u03b4\u2208D(X ,A)EPr\u2217 [L\u03b4] = maxPr\u2208P|X=x min\u03b4\u2208D(X ,A)EPr[L\u03b4] = min\u03b4\u2208D(X ,A)maxPr\u2208P|X=xEPr[L\u03b4] = maxPr\u2208P|X=xEPr[L\u03b4\u2217 ].\nProof: To prove part (a), we apply Theorem A.1, setting L\u2032 = L, Y \u2032 = Y, A\u2032 = \u2206(A), and P \u2032 to the convex closure of P | X = x. Thus, (10) holds for some \u03c1\u2217 \u2208 A\u2032, which we denote \u03b4\u2217(x). As in the proof of Theorem 3.1, there must be some distribution \u03c0\u2217 on P | X = x with finite support such that Pr\u2217 = \u2211 Pr\u2208P|X=x,\u03c0\u2217(Pr)>0 \u03c0\n\u2217(Pr) Pr. The remainder of the argument is identical to that in Theorem 3.1.\nThe proof of part (b) is completely analogous to the proof of part (b) of Theorem 3.1, and is thus omitted.\nTheorem 4.4: Given a decision setting DS = (X ,Y,A,P) such that P = \u3008P\u3009, then for all decision probems DP based on DS, there exists an a priori minimax-optimal rule that\nis also a posteriori minimax optimal. Indeed, every a posteriori minimax-optimal rule is also an a priori minimax-optimal rule. If, for all Pr \u2208 P and all x \u2208 X , Pr(X = x) > 0, then for every decision problem based on DS, every a priori minimax-optimal rule is also a posteriori minimax optimal.\nProof: Let X+ = {x \u2208 X : maxPr\u2208P Pr(X = x) > 0}. Let m\u03b4 be a random variable on X defined by taking m\u03b4(x) = 0 if x /\u2208 X+, and m\u03b4(x) = maxPr\u2032\u2208P|X=xEPr\u2032 [L\u03b4] if x \u2208 X+. We first show that for every \u03b4 \u2208 D(X ,A),\nmax Pr\u2208P EPr[L\u03b4] = max Pr\u2208P \u2211 x\u2208X PrX (X = x)m\u03b4(x). (12)\nNote that EPr[L\u03b4] = \u2211 (x,y)\u2208X\u00d7Y Pr((X,Y ) = (x, y))L\u03b4(x, y)\n= \u2211 {x\u2208X :PrX (x)>0} PrX (X = x) \u2211 y\u2208Y Pr(Y = x | X = x)L\u03b4(x, y)\n= \u2211 {x\u2208X :PrX (x)>0} PrX (X = x)EPr|X=x[L\u03b4]\n\u2264 \u2211 {x\u2208X :PrX (x)>0} PrX (X = x) maxPr\u2032\u2208P|X=xEPr\u2032 [L\u03b4]\n= \u2211 {x\u2208X :PrX (x)>0} PrX (X = x)m\u03b4(x)\n= \u2211 x\u2208X PrX (X = x)m\u03b4(x).\nTaking the max over all Pr \u2208 P, we get that\nmax Pr\u2208P EPr[L\u03b4] \u2264 max Pr\u2208P \u2211 x\u2208X PrX (X = x)m\u03b4(x).\nIt remains to show the reverse inequality in (12). Since P is closed, there exists Pr\u2217 \u2208 P such that\nmax Pr\u2208P \u2211 x\u2208X PrX (X = x)m\u03b4(x) = \u2211 x\u2208X Pr\u2217X (X = x)m\u03b4(x).\nMoreover, since P | X = x is closed, if x \u2208 X+, there exists Prx \u2208 P | X = x such that m\u03b4(x) = EPrx [L\u03b4]. Define Pr \u2020 \u2208 \u2206(X \u00d7 Y) by taking\nPr\u2020((X,Y ) = (x, y)) = { 0 if x /\u2208 X+ Pr\u2217X (X = x) Pr x(Y = y) if x \u2208 X+.\nClearly Pr\u2020X = Pr \u2217 X and (Pr \u2020 | X = x) = (Prx | X = x) \u2208 P | X = x if x \u2208 X+. Thus, by definition, Pr\u2020 \u2208 \u3008P\u3009. Since, by assumption, \u3008P\u3009 = P, it follows that Pr\u2020 \u2208 P. In addition, it easily follows that\nmaxPr\u2208P \u2211 x\u2208X PrX (X = x)m\u03b4(x)\n= \u2211 x\u2208X Pr \u2020 X (X = x)m\u03b4(x)\n= \u2211 x\u2208X+ Pr \u2020 X (X = x) \u2211 y\u2208Y Pr\n\u2020(Y = y | X = x)L\u03b4(x, y) = EPr\u2020 [L\u03b4] \u2264 maxPr\u2208P EPr[L\u03b4].\nThis establishes (12). Now let \u03b4\u2217 be an a priori minimax decision rule. Since the P-game has a Nash equilibrium (Theorem 3.1), such a \u03b4\u2217 must exist. Let X \u2032 be the set of all x\u2032 \u2208 X for which \u03b4\u2217 is not\nminimax optimal in the P\u2013x\u2032-game, i.e., x\u2032 \u2208 X \u2032 iff x \u2208 X+ and maxPr\u2032\u2208P|X=x\u2032 EPr\u2032 [L\u03b4\u2217 ] > min\u03b4\u2208D(X ,A) maxPr\u2032\u2208P|X=x\u2032 EPr\u2032 [L\u03b4]. Define \u03b4\n\u2032 to be a decision rule that agrees with \u03b4\u2217 on X \\X \u2032 and is minimax optimal in the P | X = x\u2032 game for all x\u2032 \u2208 X \u2032; that is, \u03b4\u2032(x) = \u03b4(x) for x /\u2208 X \u2032 and, for x \u2208 X \u2032,\n\u03b4(x) \u2208 argmin\u03b4\u2208D(X ,A) max Pr\u2032\u2208P|X=x\u2032 EPr\u2032 [L\u03b4].\nBy construction, m\u03b4\u2032(x) \u2264 m\u03b4\u2217(x) for all x \u2208 X and m\u03b4\u2032(x) < m\u03b4\u2217(x) for all x \u2208 X \u2032. Thus, using (12), we have\nmaxPr\u2208P EPr[L\u03b4\u2032 ] = maxPr\u2208P \u2211 x\u2208X Pr(X = x)m\u03b4\u2032(x)\n\u2264 maxPr\u2208P \u2211 x\u2208X Pr(X = x)m\u03b4\u2217(x) = maxPr\u2208P EPr[L\u03b4\u2217 ].\n(13)\nThus, \u03b4\u2032 is also an a priori minimax-optimal decision rule. But, by construction, \u03b4\u2032 is also an a posteriori minimax-optimal decision rule, and it follows that there exists at least one decision rule (namely, \u03b4\u2032) that is both a priori and a posteriori minimax optimal. This proves the first part of the theorem. To prove the last part, note that if Pr(X = x) > 0 for all Pr \u2208 P and x \u2208 X , and X \u2032 6= \u2205, then the inequality in (13) is strict. It follows that X \u2032 is empty in this case, for otherwise \u03b4\u2217 would not be a priori minimax optimal, contradicting our assumptions. But, if X \u2032 is empty, then \u03b4\u2217 must also be a posteriori minimax optimal.\nIt remains to show that every a posteriori minimax-optimal rule is also a priori minimax optimal. For all x \u2208 X , define mm(x) = 0 if x 6\u2208 X+, and mm(x) = min\u03b4\u2208\u2206m\u03b4(x) if x \u2208 X+. Let \u2206\u2217 be the set of all a posteriori minimax-optimal rules. We have already shown that \u2206\u2217 has at least one element, say \u03b40, that is also a priori minimax optimal. For all \u03b4 \u2208 \u2206\u2217 and all x \u2208 X , we must have m\u03b4(x) = mm(x). By (12), it follows that for every \u03b4 \u2208 \u2206\u2217,\nmaxPr\u2208P EPr[L\u03b4] = maxPr\u2208P \u2211 x\u2208X PrX (X = x)m\u03b4(x)\n= maxPr\u2208P \u2211 x\u2208X PrX (X = x)mm(x).\nHence, max Pr\u2208P EPr[L\u03b4] = max Pr\u2208P EPr[L\u03b40 ]. Since \u03b40 is a priori minimax optimal, this implies that all \u03b4 \u2208 \u2206\u2217 are a priori minimax optimal.\nProposition 4.7:\n(a) Every dynamically consistent decision problem is also weakly time consistent.\n(b) Not every dynamically consistent decision problem is time consistent.\n(c) Every strongly dynamically consistent decision problem is time consistent.\n(d) There exist weakly time consistent decision problems that are not dynamically consistent.\n(e) All decision problems based on P are dynamically consistent if and only if all decision problems based on P are weakly time consistent.\nProof: Part (a) is immediate by part 1 of the definition of dynamic consistency. Part (b) follows because the decision problem of Example 4.5 is dynamically consistent but not time consistent. We already showed that it is not time consistent. To see that it is dynamically consistent, note that every decision rule that can be defined on the domain in the example is a priori minimax optimal, so part 1 of the definition of dynamic consistency holds automatically. Part 2 also holds automatically, since for every two decision rules \u03b4 and \u03b4\u2032, (2) does not hold with strict inequality for X = 0.\nFor part (c), consider an arbitrary decision problem DP that is strongly dynamically consistent. It is easy to construct an a posteriori minimax optimal decision rule; call it \u03b4. Since DP is strongly dynamically consistent, \u03b4 must be a priori minimax optimal. Suppose, by way of contradiction, that some decision rule \u03b4\u2032 is a priori minimax optimal but not a posteriori minimax. Since \u03b4 is a posteriori minimax optimal, it must be the case that (2) holds, and that the the inequality is strict for some x with Pr(X = x) > 0 for some Pr \u2208 P. Thus, by strong dynamic consistency, \u03b4 must be a priori preferred to \u03b4\u2032 according the minimax criterion, a contradiction to the assumption that \u03b4\u2032 is a priori minimax optimal.\nFor part (d), consider Example 2.1 again, in which there was both time and dynamic inconsistency. Randomizing with equal probabibility between 0 and 1, no matter what is observed, is a posteriori preferred over all other randomized actions, but it was not the a priori minimax optimal. Now we extend the example by adding an additional action 2 and defining L(0, 2) = L(1, 2) = \u22121; L(y, a) remains unchanged for y \u2208 Y and a \u2208 {0, 1}. Now both the a priori and the a posteriori minimax optimal act is to play 2, no matter what value of X is observed, so time consistency holds. Yet dynamic consistency still does not hold, because after observing both X = 0 and X = 1, randomizing with equal probabibility between 0 and 1 is preferred over playing action 1, but before observing X, the decision rule that plays action 1 no matter what is observed is strictly preferred over randomizing between 0 and 1.\nThe \u201conly if\u201d direction of part (e) already follows from part (a). For the \u201cif\u201d direction, suppose, by way of contradiction, that all decision problems based on P are weakly time consistent, but some decision problem based on P is not dynamically consistent. This decision problem has some loss function L, set A of actions, and two decision rules \u03b4 and \u03b4\u2032 such that \u03b4 is preferred a posteriori over \u03b4\u2032 but not a priori; thus, in the definition of dynamic consistency, (2) holds and (3) does not. Let Lmax be the a posteriori minimax expected loss of \u03b4. Extend A and L with an additional act a0 such that for all y, L(y, a0) = Lmax. Now we have a new decision problem with action set A\u222a{a0} in which \u03b4 has become a minimax optimal a posteriori rule (it is not the only one, but that does not matter). However, \u03b4 cannot be a priori minimax optimal, because (3) still does not hold for \u03b4 and \u03b4\u2032: \u03b4\u2032 is a priori strictly better than \u03b4. Hence, we do not have weak time consistency in this new decision problem. Since it is still a decision problems based on P, we do not have weak time consistency for all decision problems based on P, and we have arrived at the desired contradiction.\nTheorem 5.1: Fix X , Y, L, A, and P \u2286 \u2206(X \u00d7 Y). If, for all PrY \u2208 PY , P contains a distribution Pr\u2032 such that X and Y are independent under Pr\u2032, and Pr\u2032Y = PrY , then there is an a priori minimax-optimal decision rule that ignores information. Under these conditions, if \u03b4 is an a priori minimax-optimal decision rule that ignores information,\nthen \u03b4 essentially optimizes with respect to the marginal on Y ; that is, maxPr\u2208P EPr[L\u03b4] = maxPrY\u2208PY EPrY [L \u2032 \u03b4]. Proof: Let P \u2032 be the subset of P of distributions under which X and Y are independent. Let D(X ,A)\u2032 be the subset of D(X ,A) of rules that ignore information. Let \u03b4\u2217 \u2208 D(X ,A)\u2032 be defined as the optimal decision rule that ignores information relative to P \u2032, i.e.\nmax Pr\u2208P \u2032 EPr[L\u03b4\u2217 ] = min \u03b4\u2208D(X ,A)\u2032 max Pr\u2208P \u2032 EPr[L\u03b4].\nWe have\nmaxPr\u2208P EPr[L\u03b4\u2217 ] \u2265 min\u03b4\u2208D(X ,A) maxPr\u2208P EPr[L\u03b4] \u2265 min\u03b4\u2208D(X ,A) maxPr\u2208P \u2032 EPr[L\u03b4] = min\u03b4\u2208D(X ,A)\u2032 maxPr\u2208P \u2032 EPr[L\u03b4] [see below]\n= maxPr\u2208P \u2032 EPr[L\u03b4\u2217 ].\n(14)\nTo see that the equality between the third and fourth line in (14) holds, note that for Pr \u2208 P \u2032, we have\nEPr[L\u03b4] = \u2211 (x,y)\u2208X\u00d7Y Pr(x, y)L\u03b4(x, y)\n= \u2211 x\u2208X Pr(X = x) \u2211 y\u2208Y Pr(Y = y)( \u2211 a\u2208A \u03b4(x)(a)L(y, a))\nThe decision rule that minimizes this expression is independent of x; it is the distribution \u03b4\u2217 over actions that minimizes\u2211\ny\u2208Y Pr(Y = y)( \u2211 a\u2208A \u03b4\u2217(a)L(y, a)).\nThis calculation also shows that, since \u03b4\u2217 ignores information, for Pr \u2208 P \u2032, we have that\nmax Pr\u2208P EPr[L\u03b4\u2217 ] = max PrY\u2208PY\nEPrY [L \u2032 \u03b4\u2217 ] = max\nPr\u2208P \u2032 EPr[L\u03b4\u2217 ]. (15)\nThis implies that the first and last line of (14) are equal to each other, and therefore also equal to the second line of (14). It follows that \u03b4\u2217 is a priori minimax optimal. Since every a priori minimax optimal rule that ignores information must satisfy (15), the second result follows. We next prove Theorem 6.3. We first need three preliminary results.\nLemma A.2: If P is convex and X0 \u2286 X , then (P | X0)Y is convex.\nProof: Without loss of generality, assume that (P | X0)Y is nonempty. Given Pr\u20320,Pr\u20321 \u2208 (P | X0)Y , let Pr\u2032\u03b2 = \u03b2 Pr\u20321 +(1\u2212 \u03b2) Pr\u20320. We show that, for all \u03b2 \u2208 [0, 1], Pr\u2032\u03b2 \u2208 (P | X0)Y . Choose Pr0,Pr1 \u2208 P with Pr0(X0) > 0,Pr1(X0) > 0, (Pr0 | X0)Y = Pr\u20320, and (Pr1 | X0)Y = Pr\u20321. For c \u2208 [0, 1], let Prc = cPr1 +(1\u2212 c) Pr0. Then, for all y \u2208 Y,\nPrc(Y = y | X0) = Prc(X\u2208X0,Y=y)Prc(X\u2208X0) = cPr1(X\u2208X0) Pr1(Y=y|X\u2208X0)+(1\u2212c) Pr0(X\u2208X0) Pr0(Y=y|X\u2208X0)cPr1(X\u2208X0)+(1\u2212c) Pr0(X\u2208X0) = \u03b2c Pr \u2032 1(Y = y) + (1\u2212 \u03b2c) Pr\u20320(Y = y),\n(16)\nwhere \u03b2c = cPr1(X0)/(cPr1(X0) + (1 \u2212 c) Pr0(X0)). Clearly, \u03b2c is a continuous increasing function of c, with \u03b20 = 0 and \u03b21 = 1. Thus, there exists c\u03b2 such that \u03b2c\u03b2 = \u03b2. Since \u03b2c is independent of y, (16) holds for all y \u2208 Y (with the same choice of \u03b2c), That is, (Prc\u03b2 | X0)Y = \u03b2 Pr\u20320 +(1\u2212 \u03b2) Pr\u20321\u2212Pr\u2032\u03b2. Thus, Pr\u2032\u03b2 \u2208 (P | X0)Y , as desired.\nLemma A.3: If U = {X1, . . . ,Xk} is a collection of nonoverlapping subsets of X (i.e., for 1 \u2264 i < j \u2264 k, Xi \u2229Xj = \u2205), (P | X1)Y is convex, (P | X1)Y = (P | X2)Y = . . . = (P | Xk)Y , and V = \u22c3k i=1Xi, then for all j \u2208 {1, . . . , k}, (P | V)Y \u2286 (P | Xj)Y .\nProof: The result is immediate if (P | V) is empty. So suppose that Pr \u2208 P and Pr(V) > 0. Using Bayes\u2019 Rule, we have that\n(Pr | V)Y = \u2211\n{i:Pr(Xi|V)>0} Pr(Xi | V)(Pr | Xi)Y .\nNow (P | X1)Y = . . . = (P | Xk)Y by assumption. Thus, for all i such that Pr(Xi | V) > 0, there must exist some Pri \u2208 P such that (Pr | Xi)Y = (Pri | X1)Y . Thus, (Pr | V)Y = \u2211 {i:Pr(Xi|V)>0} Pr(Xi | V)(Pri | X1)Y . Since P is convex by assumption, by Lemma A.2, (P | X1)Y is convex as well. Thus, we can write (Pr | V)Y as a convex combination of elements of (P | X1)Y , It follows that (Pr | V)Y \u2208 (P | X1)Y . Since (P | X1)Y = . . . = (P | Xk)Y , it follows that (Pr | V)Y \u2208 (P | Xj)Y for all j = 1, . . . , k. Lemma A.4: If P = \u3008P\u3009 and U = {x1, . . . , xk}, then \u22c2k j=1(P | X = xj)Y \u2286 (P | U)Y .\nProof: Let Q \u2208 \u22c2k j=1(P | X = xj)Y . There must exist Pr1, . . . ,Prk \u2208 P such that, for j = 1, . . . , k, (Prj | X = xj)Y = Q. Clearly Pr1(x1) > 0. Since P = \u3008P\u3009, there also exists Pr \u2208 P such that PrX = (Pr1)X and for all j \u2208 {1, . . . , k} such that Pr1(xj) > 0, we have (Pr | X = xj)Y = (Prj | X = xj)Y = Q. It follows that (Pr | U)Y = Q, so Q \u2208 (P | U)Y .\nTheorem 6.3:\n(a) If \u03a0 is C-conditioning for some partition C of X and P is convex then, for all x \u2208 X , we have that (P | [x]\u03a0,P)Y \u2286 \u03a0(P, x)Y .\n(b) If \u03a0 is standard conditioning, P = \u3008P\u3009, and x \u2208 X , then \u03a0(P, x)Y \u2286 (P | [x]\u03a0,P)Y .\nProof: For part (a), since P is convex, by Lemma A.2, (P | X \u2032)Y is convex for all X \u2032 \u2286 X . Let U = {C(x\u2032) | x\u2032 \u2208 [x]\u03a0,P}. By the definition of [x]\u03a0,P , for all x\u2032 \u2208 [x]\u03a0,P , we have\n\u03a0(P, x\u2032) = P | C(x\u2032) = P | C(x) = \u03a0(P, x).\nThus, by Lemma A.3, (P | V)Y \u2286 \u03a0(P, x)Y , where V = \u22c3 U = [x]\u03a0,PC(x\u2032). This proves part (a). For part (b), since \u03a0 is standard conditioning, we have that (P | X = x)Y = (P | X = x\u2032)Y for all x \u2032 \u2208 U . By assumption, P = \u3008P\u3009. Thus, it follows immediately from Lemma A.4 (taking U = [x]\u03a0,P) that \u03a0(P, x)Y \u2286 (P | [x]\u03a0,P)Y , as desired.\nWe next want to prove Theorem 6.10. We first need a definition and a preliminary result.\nDefinition A.5: An update rule \u03a0 is semi-calibrated relative to P if (P | [x]\u03a0,P)Y \u2286 \u03a0(P, x)Y .\nNote that, by Theorem 6.3, if P is convex, then C-conditioning is semi-calibrated for all C.\nLemma A.6: If \u03a0 is semi-calibrated relative to P and C = {[x]\u03a0,P | x \u2208 X}, then C is a partition of X and\n(a) C-conditioning is narrower than \u03a0 relative to P.\n(b) If C-conditioning is not strictly narrower than \u03a0 relative to P, then \u03a0 is equivalent to C-conditioning on P, and is calibrated.\nProof: Clearly C is a partition of X . For part (a), if \u03a0\u2032 is C-conditioning then, by definition, \u03a0\u2032(P, x) = P | C(x) = P | [x]\u03a0,P . Since \u03a0 is semi-calibrated, (P | [x]\u03a0,P)Y \u2286 (\u03a0(P, x))Y . Thus, C-conditioning is narrower than \u03a0 relative to P.\nFor part (b), if C-conditioning (i.e., \u03a0\u2032) is not strictly narrower than \u03a0 relative to P, then we must have (P(\u03a0, x))Y = (P \u2032(\u03a0, x))Y for all x \u2208 X , so (P | [x]\u03a0,P)Y = \u03a0(P, x)Y , and \u03a0 is claibrated relative to P.\nTheorem 6.10:\n(a) C-conditioning is sharply calibrated relative to P for some partition C.\n(b) If \u03a0 is sharply calibrated relative to P, then there exists some C such that \u03a0 is equivalent to C-conditioning on P (i.e., \u03a0(P, x) = \u03a0 | C(x) for all x \u2208 X ).\nProof: We can place a partial order \u2264P on partitions C by taking C1 \u2264P C2 if C1conditioning is narrower than C2 conditioning relative to P. Since X is finite, there are only finitely many possible partitions of X . Thus, there must be some minimal elements of \u2264P . We claim that each minimal element of \u2264P is sharply calibrated relative to P. For suppose that C0 is minimal relative to \u2264P . Because P is convex, C0-conditioning is semi-calibrated (Theorem 6.3) and we can apply Lemma A.6 with \u03a0 as C0. Because C0 is minimal, the C defined in Lemma A.6 cannot be strictly narrower than C0. It follows by Lemma A.6(b) that C0-conditioning is calibrated. We now show that C0-conditioning is in fact sharply calibrated, by showing that there exists no calibrated update rule that is a strict narrowing of C0-conditioning. For suppose, by way of contradiction, that \u03a0 is an update rule that is calibrated and that is strictly narrower than C0 relative to P. Then by Lemma A.6(a) there exists a partition C such that C is narrower than \u03a0 relative to P. But then C <P C0, contradicting the minimality of C0. This proves part (a).\nFor part (b), suppose that \u03a0 is sharply calibrated relative to P. By Lemma A.6(a), there must be some partition C such that C-conditioning is narrower than \u03a0, relative to P. Let C0 be a minimal element of \u2264P such that C0 \u2264P C. Part (a) shows that C0-conditioning is sharply calibrated relative to P. Since C0-conditioning is narrower than \u03a0, and we \u03a0 is sharply calibrated relative to P, we must have that C0-conditioning is not strictly narrower than \u03a0 relative to P, and hence \u03a0 is equivalent to C0-conditioning on P.\nTheorem 6.12: If P is convex and P = \u3008P\u3009, then standard conditioning is sharply calibrated relative to P.\nProof: By Corollary 6.4, standard conditioning is calibrated relative to P under the stated assumptions on P. To show that it is sharply calibrated, suppose that there exists some update rule \u03a0\u2032 that is narrower than standard conditioning, and that is sharply calibrated relative to P. By Theorem 6.10, \u03a0\u2032 is equivalent to C-conditioning for some C relative to P. Thus, for all x \u2208 X and all x\u2032 \u2208 C(x), we have that\n(P | C(x))Y \u2286 (P | x\u2032)Y ,\nso\n(P | C(x))Y \u2286 \u22c2\nx\u2032\u2208C(x) (P | x\u2032)Y .\nBy Lemma A.4, it is immediate that\u22c2 x\u2032\u2208C(x) (P | x\u2032)Y \u2286 (P | C(x))Y .\nThus, we must have \u22c2 x\u2032\u2208C(x) (P | x\u2032)Y = (P | C(x))Y . (17)\nNow we want to show that, for all x\u2032 \u2208 C(x), we have that (P | C(x))Y = (P | x\u2032)Y . This will show that C is equivalent to conditioning, and that conditioning is sharply calibrated.\nSuppose not, and that Q \u2208 (P | x\u2032)Y \u2212 P | C(x)Y for some x\u2032 \u2208 C(x). Let Q\u2032 be the distribution in (P | C(x))Y that is closest to Q. The fact that there is such a distribution Q\u2032 follows from the fact that P is closed (recall that we assume that P is closed throughout the paper). (In fact, it follows from convexity that Q\u2032 is unique, but this is not necessary for our argument.) Since Q\u2032 \u2208 (P | C(x))Y , it follows from (17) that, for each x\u2032\u2032 \u2208 C(x), there must be some distribution Prx\u2032\u2032 \u2208 P such that Prx\u2032\u2032(x\u2032\u2032) > 0 and (Prx\u2032\u2032 | x\u2032\u2032)Y = Q. Since P is convex, there is some distribution Pr\u2217 \u2208 P such that Pr\u2217(x\u2032\u2032) > 0 for all x\u2032\u2032 \u2208 C(x) (indeed, Pr\u2217 can be any convex combination of the distributions Prx\u2032\u2032 for x\n\u2032\u2032 \u2208 C where all the coefficients are positive). Since P = \u3008P\u3009, there must exist a distribution Pr \u2208 P such that (Pr)X = (Pr\n\u2217)X (so that Pr is positive on all elements of C), (Pr | x\u2032\u2032)Y = Q\u2032 for all x\u2032\u2032 \u2208 C(x) other than x\u2032, and (Pr | x\u2032)Y = Q. Note that (Pr | (C(x)\u2212 {x\u2032}))Y = Q\u2032. Thus,\n(Pr | C(x)Y = c(Pr | C(x)\u2212 x\u2032)Y + (1\u2212 c)(Pr | x\u2032)Y = cQ\u2032 + (1\u2212 c)Q,\nfor some c such that 0 < c < 1. Clearly cQ\u2032 + (1\u2212 c)Q is closer to Q than Q\u2032 is. This gives the desired contradiction."}], "references": [{"title": "On the suboptimality of the generalized Bayes rule and robust Bayesian procedures from the decision theoretic point of view: A cautionary note on updating imprecise priors", "author": ["T. Augustin"], "venue": "In 3rd International Symposium on Imprecise Probabilities and Their Applications,", "citeRegEx": "Augustin,? \\Q2003\\E", "shortCiteRegEx": "Augustin", "year": 2003}, {"title": "Graphoid properties of epistemic irrelevance and independence", "author": ["F.G. Cozman", "P. Walley"], "venue": "In 2nd International Symposium on Imprecise Probabilities and Their Applications,", "citeRegEx": "Cozman and Walley,? \\Q2001\\E", "shortCiteRegEx": "Cozman and Walley", "year": 2001}, {"title": "Inference with separately specified sets of probabilities in credal networks", "author": ["J.C.F. da Rocha", "F.G. Cozman"], "venue": "In Proc. Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Rocha and Cozman,? \\Q2002\\E", "shortCiteRegEx": "Rocha and Cozman", "year": 2002}, {"title": "The well-calibrated Bayesian", "author": ["A.P. Dawid"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Dawid,? \\Q1982\\E", "shortCiteRegEx": "Dawid", "year": 1982}, {"title": "Les probabilit\u00e9s nulles", "author": ["B. de Finetti"], "venue": "Bulletins des Science Mathe\u0301matiques (premie\u0300re partie),", "citeRegEx": "Finetti,? \\Q1936\\E", "shortCiteRegEx": "Finetti", "year": 1936}, {"title": "Risk, ambiguity, and the Savage axioms", "author": ["D. Ellsberg"], "venue": "Quarterly Journal of Economics,", "citeRegEx": "Ellsberg,? \\Q1961\\E", "shortCiteRegEx": "Ellsberg", "year": 1961}, {"title": "Recursive multiple priors", "author": ["L.G. Epstein", "M. Schneider"], "venue": "Journal of Economic Theory,", "citeRegEx": "Epstein and Schneider,? \\Q2003\\E", "shortCiteRegEx": "Epstein and Schneider", "year": 2003}, {"title": "Unreliable probabilities, risk taking, and decision making", "author": ["P. G\u00e4rdenfors", "N. Sahlin"], "venue": null, "citeRegEx": "G\u00e4rdenfors and Sahlin,? \\Q1982\\E", "shortCiteRegEx": "G\u00e4rdenfors and Sahlin", "year": 1982}, {"title": "Maxmin expected utility with a non-unique prior", "author": ["I. Gilboa", "D. Schmeidler"], "venue": "Journal of Mathematical Economics,", "citeRegEx": "Gilboa and Schmeidler,? \\Q1989\\E", "shortCiteRegEx": "Gilboa and Schmeidler", "year": 1989}, {"title": "Updating sets of probabilities", "author": ["A.J. Grove", "J.Y. Halpern"], "venue": "In Proc. Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Grove and Halpern,? \\Q1998\\E", "shortCiteRegEx": "Grove and Halpern", "year": 1998}, {"title": "Game theory, maximum entropy, minimum discrepancy, and robust Bayesian decision theory", "author": ["P.D. Gr\u00fcnwald", "A.P. Dawid"], "venue": "The Annals of Statistics,", "citeRegEx": "Gr\u00fcnwald and Dawid,? \\Q2004\\E", "shortCiteRegEx": "Gr\u00fcnwald and Dawid", "year": 2004}, {"title": "When ignorance is bliss", "author": ["P.D. Gr\u00fcnwald", "J.Y. Halpern"], "venue": "In Proc. Twentieth Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Gr\u00fcnwald and Halpern,? \\Q2004\\E", "shortCiteRegEx": "Gr\u00fcnwald and Halpern", "year": 2004}, {"title": "Divisive conditioning: Further results on dilation", "author": ["T. Herron", "T. Seidenfeld", "L. Wasserman"], "venue": "Philosophy of Science,", "citeRegEx": "Herron et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Herron et al\\.", "year": 1997}, {"title": "Robust Statistics. Wiley, New York", "author": ["P.J. Huber"], "venue": null, "citeRegEx": "Huber,? \\Q1981\\E", "shortCiteRegEx": "Huber", "year": 1981}, {"title": "Symmetry arguments in probability kinematics", "author": ["R.I.G. Hughes", "B.C. van Fraassen"], "venue": "PSA 1984,", "citeRegEx": "Hughes and Fraassen,? \\Q1985\\E", "shortCiteRegEx": "Hughes and Fraassen", "year": 1985}, {"title": "Probability of conditionals and conditional probabilities", "author": ["D. Lewis"], "venue": "Philosophical Review,", "citeRegEx": "Lewis,? \\Q1976\\E", "shortCiteRegEx": "Lewis", "year": 1976}, {"title": "Fifty Challenging Problems in Probability with Solutions", "author": ["F. Mosteller"], "venue": "AddisonWesley, Reading, Mass", "citeRegEx": "Mosteller,? \\Q1965\\E", "shortCiteRegEx": "Mosteller", "year": 1965}, {"title": "Ambiguity aversion, games against nature, and dynamic", "author": ["Gr\u00fcnwald", "E. Halpern Ozdenoren", "J. Peck"], "venue": null, "citeRegEx": "Gr\u00fcnwald et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gr\u00fcnwald et al\\.", "year": 2008}, {"title": "Symmetries of personal probability kinematics", "author": ["B.C. van Fraassen"], "venue": null, "citeRegEx": "Fraassen,? \\Q1987\\E", "shortCiteRegEx": "Fraassen", "year": 1987}, {"title": "Algorithmic Learning in a Random World", "author": ["V. Vovk", "A. Gammerman", "G. Shafer"], "venue": "Parade Magazine on Dec", "citeRegEx": "Vovk et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Vovk et al\\.", "year": 1990}, {"title": "Statistical Decision Functions", "author": ["Springer", "A. New York. Wald"], "venue": null, "citeRegEx": "Springer and Wald,? \\Q1950\\E", "shortCiteRegEx": "Springer and Wald", "year": 1950}], "referenceMentions": [{"referenceID": 6, "context": "We show that the answer to the first two questions is \u201cyes\u201d if P satisfies a condition that Epstein and Schneider (2003) call rectangularity, while the answer to the third question is \u201cyes\u201d if P is convex and satisfies the rectangularity condition.", "startOffset": 92, "endOffset": 121}, {"referenceID": 0, "context": "As has been pointed out by several authors, conditioning a set P on observation X = x sometimes leads to a phenomenon called dilation (Augustin, 2003; Cozman & Walley, 2001; Herron, Seidenfeld, & Wasserman, 1997; Seidenfeld & Wasserman, 1993): the agent may have substantial knowledge about some other random variable Y before observing X = x, but know significantly less after conditioning.", "startOffset": 134, "endOffset": 242}, {"referenceID": 6, "context": "Our condition is essentially Epstein and Schneider\u2019s (2003) rectangularity condition, which they showed was sufficient to guarantee what has been called in the decision theory community dynamic consistency.", "startOffset": 29, "endOffset": 60}, {"referenceID": 6, "context": "Our condition is essentially Epstein and Schneider\u2019s (2003) rectangularity condition, which they showed was sufficient to guarantee what has been called in the decision theory community dynamic consistency. Roughly speaking, dynamic consistency says that if, no matter what the agent learns, he will prefer decision rule \u03b4 to decision rule \u03b4\u2032, then he should prefer \u03b4 to \u03b4\u2032 before learning anything. Dynamic consistency is closely related to Savage\u2019s (1954) sure-thing principle.", "startOffset": 29, "endOffset": 458}, {"referenceID": 3, "context": "As pointed out by Dawid (1982), an agent updating her beliefs and making decisions on the basis of these beliefs should also be concerned about being calibrated.", "startOffset": 18, "endOffset": 31}, {"referenceID": 13, "context": "It is well known (Huber, 1981) that what Walley calls a coherent lower prevision (a lower prevision satisfying some minimal properties) can be identified with the lower expectation of a set of probability measures (that is, the function E such that E(X) = infPr\u2208P EPr(X)).", "startOffset": 17, "endOffset": 30}, {"referenceID": 6, "context": "Following Epstein and Schneider (2003), we say that a decision problem DP is dynamically consistent if for every pair \u03b4, \u03b4\u2032 of decision rules, the following conditions both hold: 1.", "startOffset": 10, "endOffset": 39}, {"referenceID": 6, "context": "Our goal in this section is to show that the rectangularity condition of Epstein and Schneider (2003) suffices to guarantee that conditioning is optimal.", "startOffset": 73, "endOffset": 102}, {"referenceID": 2, "context": "The assumption that P = \u3008P\u3009 is closely related to a set of probabilities being separately specified, introduced by da Rocha and Cozman (2002). As da Rocha and Cozman point out, this assumption makes it possible to apply ideas from Bayesian networks to uncertainty represented by sets of probability distributions.", "startOffset": 118, "endOffset": 142}, {"referenceID": 2, "context": "The assumption that P = \u3008P\u3009 is closely related to a set of probabilities being separately specified, introduced by da Rocha and Cozman (2002). As da Rocha and Cozman point out, this assumption makes it possible to apply ideas from Bayesian networks to uncertainty represented by sets of probability distributions. The condition P = \u3008P\u3009 is an instance of the rectangularity condition which goes back at least to the work of Sarin and Wakker (1998). It was introduced in its most general form by Epstein and Schneider (2003).", "startOffset": 118, "endOffset": 447}, {"referenceID": 2, "context": "The assumption that P = \u3008P\u3009 is closely related to a set of probabilities being separately specified, introduced by da Rocha and Cozman (2002). As da Rocha and Cozman point out, this assumption makes it possible to apply ideas from Bayesian networks to uncertainty represented by sets of probability distributions. The condition P = \u3008P\u3009 is an instance of the rectangularity condition which goes back at least to the work of Sarin and Wakker (1998). It was introduced in its most general form by Epstein and Schneider (2003). Epstein and Schneider define this condition for a sequence of random variables X1, .", "startOffset": 118, "endOffset": 523}, {"referenceID": 6, "context": "Our notion of conservative corresponds to what Epstein and Schneider (2003) call the full support condition.", "startOffset": 47, "endOffset": 76}, {"referenceID": 6, "context": "4 with the results of Epstein and Schneider (2003). For this, we first compare our notion of time consistency with their notion of dynamic consistency.", "startOffset": 22, "endOffset": 51}, {"referenceID": 15, "context": "But other update rules are possible, even for a single distribution; for example, Lewis (1976) considered an approach to updating that he called imaging.", "startOffset": 82, "endOffset": 95}, {"referenceID": 15, "context": "But other update rules are possible, even for a single distribution; for example, Lewis (1976) considered an approach to updating that he called imaging. There is even more scope when considering sets of probabilities; for example, both Walley\u2019s (1991) natural extension and regular extension provide update rules (as we said, our notion of conditioning can be viewed as an instance of Walley\u2019s regular extension).", "startOffset": 82, "endOffset": 253}, {"referenceID": 16, "context": "4: [Monty Hall] (Mosteller, 1965; vos Savant, 1990): We start with the original Monty Hall problem, and then consider a variant of it.", "startOffset": 16, "endOffset": 51}, {"referenceID": 3, "context": "As we said in the introduction, Dawid (1982) pointed out that an agent who is updating his beliefs should want to be calibrated.", "startOffset": 32, "endOffset": 45}, {"referenceID": 5, "context": "In fact, after the publication of the conference version of this paper, we learned that Ozdenoren and Peck (2008) use the same type of approach for analyzing dynamic situations related to the Ellsberg (1961) paradox.", "startOffset": 192, "endOffset": 208}, {"referenceID": 9, "context": "It is worth noting that Grove and Halpern (1998) give an axiomatic characterization of conditioning sets of probabilities, based on axioms given by van Fraassen (1987, 1985) that characterize conditioning in the case that uncertainty is described by a single probability distribution.", "startOffset": 24, "endOffset": 49}, {"referenceID": 3, "context": "2 from the work of Gr\u00fcnwald and Dawid (2004), itself an extension of Von Neumann\u2019s original minimax theorem.", "startOffset": 32, "endOffset": 45}], "year": 2011, "abstractText": "We consider how an agent should update her beliefs when her beliefs are represented by a set P of probability distributions, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. We adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from P. We consider two reasonable games that differ in what the bookie knows when he makes his choice. Anomalies that have been observed before, like time inconsistency, can be understood as arising because different games are being played, against bookies with different information. We characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. Finally, we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities. Our results emphasize the key role of the rectangularity condition of Epstein and Schneider.", "creator": "TeX"}}}