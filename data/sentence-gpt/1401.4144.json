{"id": "1401.4144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Arguments using ontological and causal knowledge", "abstract": "We investigate an approach to reasoning about causes through argumentation. We consider a causal model for a physical system, and look for arguments about facts. Some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on. At the root of argumentation here, are causal links ({A_1, ... ,A_n} causes B) and ontological links (o_1 is_a o_2). We present a system that provides a candidate explanation ({A_1, ... ,A_n} explains {B_1, ... ,B_m}) by resorting to an underlying causal link substantiated with appropriate ontological links. Argumentation is then at work from these various explaining links. A case study is developed: a severe storm Xynthia that devastated part of France in 2010, with an unaccountably high number of casualties. This event, which triggered a major storm, caused over 900 deaths and a huge number of injuries, was blamed for the devastating events that claimed 100 lives. This case was cited as \"the strongest example of a causal link that has ever occurred\" by a large number of people. A major earthquake in 2008, which led to more than 12,000 deaths and more than 600 deaths, caused over 5,000 deaths. There are three ways to explain such a tragedy: 1) A causal link can be made to cause the event or cause the event. A causal link is a direct causal link between two events. The one which leads to the conclusion that it is the cause of the events is the cause of the event. This theory is used to describe two main arguments for the theory of causation (i.e. causality): one that leads to the conclusion that the cause of the events is the cause of the event (i.e. causation). Thus, the causal link can be made to cause the event. A causal link can be made to cause the event. A causal link is a direct causal link between two events. The one which leads to the conclusion that it is the cause of the events is the cause of the event. The one which leads to the conclusion that it is the cause of the events is the cause of the event. This theory is used to describe two main arguments for the theory of causation (i.e. causality): one that leads to the conclusion that the cause of the events is the cause of the event. This theory is used to describe two main arguments for the theory of causation (i.e. causation). Thus,", "histories": [["v1", "Thu, 16 Jan 2014 19:49:42 GMT  (64kb,D)", "http://arxiv.org/abs/1401.4144v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["philippe besnard", "marie-odile cordier", "yves moinard"], "accepted": false, "id": "1401.4144"}, "pdf": {"name": "1401.4144.pdf", "metadata": {"source": "CRF", "title": "Arguments using ontological and causal knowledge", "authors": ["Philippe Besnard", "Marie-Odile Cordier", "Yves Moinard"], "emails": ["besnard@irit.fr,", "cordier@irisa.fr,", "yves.moinard@inria.fr"], "sections": [{"heading": null, "text": "We investigate an approach to reasoning about causes through argumentation. We consider a causal model for a physical system, and look for arguments about facts. Some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on. At the root of argumentation here, are causal links ({A1, \u00b7 \u00b7 \u00b7 , An} causes B) and ontological links (o1 is_a o2). We present a system that provides a candidate explanation ({A1, \u00b7 \u00b7 \u00b7 , An} explains {B1, \u00b7 \u00b7 \u00b7 , Bm}) by resorting to an underlying causal link substantiated with appropriate ontological links. Argumentation is then at work from these various explaining links. A case study is developed : a severe storm Xynthia that devastated part of France in 2010, with an unaccountably high number of casualties."}, {"heading": "1 Introduction and Motivation", "text": "Looking for explanations is a frequent operation, in various domains, from judiciary to mechanical fields. We\nconsider the case where we have some precise (not necessarily exhaustive) description of some mechanism, or situation, and we are looking for explanations of some facts. The description contains logical formulas, plus some causal and ontological formulas (or links). Indeed, it is known that, while there are similarity between causation and implication, causation cannot be rendered by a simple logical implication. Also, confusing causation and co-occurrence could lead to undesirable relationships. This is why we use here a causal formalism where some causal links and ontological links are added to classical logical formulas. Then the causal formalism will produce various explanation links [1]. However, if the situation described is complex enough, this will result in a great number of possible explanations, and some argumentation is involved in order to get some reasons to choose between all these candidate explanations.\nIn this text, we will consider as an example a severe storm, called Xynthia, which made 26 deaths in a single group of houses in La Faute sur Mer, a village in Vend\u00e9e during a night in February 2010. This was a severe storm, with strong winds, low pressure, but it had been forecast. Since the casualties were excessive with respect to the strength of the meteorological phenomenon, various investigations have been ordered. This showed that various factors combined their effects. The weather had its role, however, other factors had been involved : recent houses and a fire station had been constructed in an area known as being susceptible of getting submerged. Also, the state authorities did not realize that asking people to stay at home was inappropriate in case of flooding given the traditionally low Vend\u00e9e houses.\nIn this paper, we define in section 2 an enriched causal\nar X\niv :1\n40 1.\n41 44\nmodel, built from a causal model and an ontological model. We then show in section 4 how explanations can be derived from this enriched causal model. We explain in section 5 the use of argumentation in order to deal with the great number of possible explanations obtained and conclude in section 6. The Xynthia example, introduced in section 3, is used as illustration throughout the article."}, {"heading": "2 Enriched causal model = Causal model + ontological model", "text": "The model that is used to build tentative explanations and support argumentation, called the enriched causal model, is built from a causal model relating literals in causal links, and from an ontological model where classes of objects are related through specialization/generalization links."}, {"heading": "2.1 The causal model", "text": "By a causal model [8], we mean a representation of a body of causal relationships to be used to generate arguments that display explanations for a given set of facts.\nThe basic case is that of a causal link \u201c\u03b1 causes \u03b2\u201d where \u03b1 and \u03b2 are literals. In this basic case, \u03b1 stands for the singleton {\u03b1} as the general case of a causal link is the form {\u03b11, \u03b12, \u00b7 \u00b7 \u00b7 , \u03b1n} causes \u03b2 where {\u03b11, \u03b12, \u00b7 \u00b7 \u00b7\u03b1n} is a set (interpreted conjunctively) of literals.\nPart of the causal model for our Xynthia example is given in Fig. 2 (each plain black arrow represents a causal link)."}, {"heading": "2.2 The ontological model", "text": "The literals P (o1, o2, \u00b7 \u00b7 \u00b7 , ok) occurring in the causal model use some predicates P applied to classes of objects oi. The ontological model consists of specialization/generalization links between classes of objects\no1 is-a\u2212\u2192 o2,\nwhere is-a\u2212\u2192 denotes the usual specialization link between classes. E.g., we have Hurri is-a\u2212\u2192 SWind, House1FPA is-a\u2212\u2192 HouseFPA and HouseFPA is-a\u2212\u2192 BFPA : a \u201churricane\u201d (Hurri) is a specialization of a \u201cstrong wing\u201d (SWind), and the class of \u201clow houses with one level only in the flood-prone area\u201d (House1FPA) is a specialization of the class of \u201chouses in the flood-prone area\u201d (HouseFPA), which itself is a specialization of the class of \u201cbuildings in this area\u201d (BFPA). A part of the ontological model for our Xynthia example is given in Fig. 1 (each white-headed arrow labelled with is-a represents an is-a\u2212\u2192 link)."}, {"heading": "2.3 The enriched causal model", "text": "The causal model is extended by resorting to the ontological model, and the result is called the enriched causal model. The enrichment lies in the so-called ontological deduction links (denoted ded ont\u2212\u2192 ) between literals. Such a link\n\u03b1 ded ont\u2212\u2192 \u03b2\nsimply means that \u03b2 can be deduced from \u03b1 due to specialization/generalization links, the is-a\u2212\u2192 links in the ontological model, that relate the classes of objects mentioned in \u03b1 and \u03b2. Note that the relation ded ont\u2212\u2192 is transitive and reflexive.\nHere is an easy illustration. A sedan is a kind of car, which is represented by sedan is-a\u2212\u2192 car in the ontological model. Then, Iown(sedan) ded ont\u2212\u2192 Iown(car) is an ontological deduction link in the enriched model.\nTechnically, the ded ont\u2212\u2192 links among literals are generated by means of a single principle as follows. The predicates used in the causal model are annotated so that each of their parameters is sorted either as universal or as existential.\nA universal parameter of a predicate \u201cinherits\u201d by specialization, meaning that if the predicate is true on this parameter then the predicate is also true for specializations of this parameter. The existential parameters of a predicate \u201cinherits\u201d by generalization, meaning that if a the predicate is true on this parameter, then the predicate is also true for generalizations of this parameter (cf above example of our owner of a sedan).\nAs another example, consider the unary predicate Flooded, where Flooded(o) means that class o (an area or a group of buildings) is submerged. Its unique parameter is taken to be \u201cuniversal\u201d so that if the literal Flooded(o) is true, then the literal Flooded(s) is also true whenever s is a specialization of o. The causal model is enriched by adding Flooded(o) ded ont\u2212\u2192 Flooded(s) for each class s satisfying s is-a\u2212\u2192 o.\nLet us now consider the unary predicate Occurs so that Occurs(Hurri) intuitively means : some hurricane occurs. Exactly as above \u201cI own\u201d predicate, this predicate is existential on its unique parameter. By means of the is-a\u2212\u2192 link Hurri is-a\u2212\u2192 SWind, we obtain the following ded ont\u2212\u2192 link : Occurs(Hurri) ded ont\u2212\u2192 Occurs(SWind).\nLet us provide the formal definition of ded ont\u2212\u2192 links in the case of unary predicates. Then, the general case is a natural, but intricate and thus ommitted here, generalization of the unary case.\nDefinition 1 Let us suppose that Prop1\u2203 and Prop2\u2200 are two unary predicates, of the \u201cexistential kind\u201d for the first one, and of the \u201cuniversal kind\u201d for the second one. If in the ontology is the link class1\nis-a\u2212\u2192 class2, then the following two links are added to the enriched model : Prop1\u2203(class1)\nded ont\u2212\u2192 Prop1\u2203(class2) and Prop2\u2200(class2) ded ont\u2212\u2192 Prop2\u2200(class1).\nIn our formalism, causal and ontological links ded ont\u2212\u2192 entail classical implication :\n{\u03b11, \u00b7 \u00b7 \u00b7\u03b1n} causes \u03b1 entails ( \u2227n\ni=1\u03b1i)\u2192 \u03b1. \u03b1 ded ont\u2212\u2192 \u03b2 entails \u03b1\u2192 \u03b2. (1)\nOrdinary logical formulas are also allowed (for example for describing exclusions), which are added to the formulas coming from (1).\nWhen resorting to explanations (see section 4 below), these ded ont\u2212\u2192 links are extended to sets of literals (links denoted by\u2192ded ontset ) as follows :\nDefinition 2 Let \u03a6 and \u03a8 be two sets of literals, we define \u03a6 \u2192ded ontset \u03a8, if for each \u03c8 \u2208 \u03a8, there exists \u03d5 \u2208 \u03a6 such that \u03d5 ded ont\u2212\u2192 \u03c8 (remind that \u03c8 ded ont\u2212\u2192 \u03c8).\nNotice that if a sedan is a kind of car (sedan is-a\u2212\u2192 car), and Own a predicate existential on its unique argument, then we get {Iown(sedan), IamHappy} \u2192ded ontset\n{Iown(car), IamHappy}, and {Iown(sedan), IamHappy} \u2192ded ontset {Iown(car)}."}, {"heading": "3 The Xynthia example", "text": "From various enquiries, including one from the French parliament 1 and one from the Cours des Comptes 2 and many articles on the subject, we have many information about the phenomenon and its dramatic consequences. We have extracted a small part from all the information as an illustration of our approach.\nThe classes we consider in the ontological model and/or in the causal model are the following ones : Hurri, SWind, BFPA, House1FPA, HouseFPA, and BFPA have already been introduced in \u00a72.2, together with a few is-a\u2212\u2192 links. Among the buildings in the flood-prone area, there is also a fire station FireSt (remind that we have also a group of houses HouseFPA, including a group of typical Vend\u00e9e low houses with one level only House1FPA). We consider also three kinds of natural disasters (NatDis) : Hurri, together with \u201ctsunami\u201d (Tsun) and \u201cflooding\u201d (Flooding). As far as meteorological phenomena are concerned, we restrict ourselves to \u201cVery low pressure\u201d (VLPress), together with already seen Hurri and SWind, and finally we add \u2018high spring tide\u201d (HSTide) to our list of classes.\nTwo kinds of alerts (Alert) may be given by the authorities, \u201cAlert-Evacuate\u201d (AlertEvac) and \u201cAlertStayAtHome\u201d (AlertStay). We consider also an anemoter (Anemo) able to measure the wind strength and a fact asserting that \u201cpeople stay at home\u201d (PeopleStay).\n1. http://www.assemblee-nationale.fr/13/rap-info/ i2697.asp\n2. www.ccomptes.fr/Publications/Publications/ Les-enseignements-des-inondations-de-2010-sur -le-littoral-atlantique-Xynthia-et-dans-le-Var\nThe following predicates are introduced : (Flooded) and (Victims_I) applied to a group of building respectively meaning that \u201cflooding\u201d occurs over this group, and that there were \u201cvictims\u201d in this group (I \u2208 {1, 2, 3} is a degree of gravity, e.g. Victims_1, Victims_2 and Victims_3 respectively mean, in % of the population of the group : \u201ca small number\u201d, \u201ca significant number\u201d and \u201ca large number\u201d of victims). OK means that its unique parameter is in a normal state. Occurs means that some fact has occurred (a strong wind, a disaster, . . .), Expected means that some fact is expected to occur.\nAll these predicates are \u201cuniversal\u201d on their unique parameter, except for the predicates Occurs and Expected which are \u201cexistential\u201d.\nNote that negation of a literal is expressed by Neg as in Neg OK(Anemo) meaning that the anemometer is not in its normal behaviour state (the formula \u00ac(OK(Anemo) \u2227 (NEG OK(anemo)) is thus added).\nThe classes and the ontological model are given in Figure 1 with the is-a\u2212\u2192 links represented as white-headed arrows labelled with (is-a).\nPart of the causal model is given in Figure 2 (remember, black-headed arrows represent causal links). It represents causal relations between [sets of] literals. It expresses that an alert occurs when a natural disaster is expected, or when a natural disaster occurs. Also, people stay at home if alerted to stay at home, and having one level home flooded results in many victims, and even more victims if the fire station itself is flooded,...\nFrom the ontological model and the causal model, the enriched causal model can be build, adding ded ont\u2212\u2192 links between literals when possible.\nFor instance, for our \u201cexistential\u201d predicates, from Hurri\nis-a\u2212\u2192 SWind, the links Occurs(Hurri)\nded ont\u2212\u2192 Occurs(SWind), and Expected(Hurri)\nded ont\u2212\u2192 Expected(SWind) are added.\nIn the same way, for a universal predicate, from House1FPA is-a\u2212\u2192 BFPA, is added the link Flooded(BFPA) ded ont\u2212\u2192 Flooded(House1FPA)\n(cf Fig. 6).\nFigure 3 represents a part of the enriched causal model, where the (unlabelled) white-headed arrows represent the ded ont\u2212\u2192 links and each black-headed arrow represents a causal link (from a literal or, in case of forked entry, form a set of literals)."}, {"heading": "4 Explanations", "text": ""}, {"heading": "4.1 Explaining a literal from a [set of] literal[s]", "text": "Causal and ontological links allow us to infer explanation links. We want to exhibit candidate reasons that can explain a fact by means of at least one causal link. We disregard explanations that involve only links of the implicational kind. Here is how causal and ontological links are used in our formalism :\nLet \u03a6 denote a set of literals and \u03b2 be a literal.\nThe basic case is that \u03a6 causes \u03b2 yields that \u03b2 can be explained by \u03a6. (2)\nThe general initial case involves two literals \u03b2 = Prop(cl2) and \u03b4 = Prop(cl1) built on the same predicate Prop (eventual other parameters equal in these two literals) :{\n\u03a6 causes \u03b2\n\u03b4 ded ont\u2212\u2192 \u03b2\n} yields that\n\u03b4 can be explained by \u03a6, provided \u03b4 is possible. (3)\n{\u03b4} is the set of justifications for this explanation of \u03b4 by \u03a6.\nWe require also that the origin of the explanation is possible, thus the full set of justifications is here the set \u03a6 \u222a {\u03b4}, while in case of (2), this full set is \u03a6, even if no set is given explicitly. Indeed, it is always understood that the starting point of the explanation link must be possible, thus we sometimes omit it. Remind that from (1) we get ( \u2227 \u03d5\u2208\u03a6 \u03d5) \u2192 \u03b2 and \u03b4 \u2192 \u03b2, thus adding \u03b2 to the justification set is useless.\nAn explaining link \u03a6 can explain \u03b1 provided \u03a8 is possible where \u03a8 is a set of literals means that \u03b1 can be explained by \u03a6 provided the set \u03a6 \u222a \u03a8 is possible : if adding \u03a6\u222a\u03a8 to the available data leads to inconsistency, then the explanation link cannot be used to explain \u03b4 by \u03a6.\nIn the figures, white-headed arrows represent \u201contological links\u201d : ded ont\u2212\u2192 links for bare arrows, and is-a\u2212\u2192 links for arrows with (is-a) mentioned, while dotted arrows represent explanation links (to be read can explain) produced by these rules, these arrows being sometimes labelled with the corresponding justification set.\nWe want also that our explanation links can follow ded ont\u2212\u2192 links as follows, in the case where \u03a6 = {\u03d5} is a singleton (thus we note sometimes \u03d5 instead of {\u03d5}) :{\n\u03d5 can explain \u03b4 provided \u03a8 is possible \u03d50 ded ont\u2212\u2192 \u03d5 \u03b4 ded ont\u2212\u2192 \u03b41 } yields that \u03d50 can explain \u03b41 provided \u03a8 is possible (4)\n{\u03d50} \u222a\u03a8 is the full justification set for this explanation of \u03b41 by \u03d50. Again we get \u03d50 \u2192 \u03d5 by (1), so we need not to mention \u03d5 here.\nGeneralizing the case of (4) when \u03a6 is a set of literals needs the use of\u2192ded ontset :{\n\u03a6 can explain \u03b4 provided \u03a8 is possible \u03a60 \u2192ded ontset \u03a6 \u03b4 ded ont\u2212\u2192 \u03b41 } yields that \u03a60 can explain \u03b41 provided \u03a8 is possible (5)\nNow, we also want that our explanation links are transitive, and this necessitates to be able to explain not only\nliterals, but sets of literals. Thus we introduce explanation links among sets of literals, which extend our explanation links from sets of literals towards literals (since it is an extension, we can keep the same name explanation link) :"}, {"heading": "4.2 Explaining a set of literals from a set of literals", "text": "Definition 3 Let n be some natural integer, and, for i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n} \u03a6,\u03a8i be sets of literals and \u03b4i be literals. If, for each i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}, we have \u03a6 can explain \u03b4i provided \u03a8i is possible,\nthen we define the following [set] explanation link\n\u03a6 can explain {\u03b4i/i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}} provided \u22c3n i=1 \u03a8i is possible.\nAgain, such an explanation link applies only when its (full) justification set, here \u03a6 \u222a \u22c3n i=1 \u03a8i, is possible (not contradicted by the data).\nNotice that we do not want to explain \u03a6 by \u03a6 itself, and we extend this restriction to\u2192ded ontset links :\nWe do not want to explain \u03a6 by \u03a60 is all we know is \u03a60 \u2192ded ontset \u03a6. Indeed, this seems to be cheating about what an explanation is (we want some causal information to play a role).\nHowever, in the line of (5), we want that explanations follow\u2192ded ontset links, thus we introduce the following definition :\nDefinition 4 If we have{ \u03a6 can explain \u2206 provided \u03a8 is possible\n\u03a60 \u2192ded ontset \u03a6 \u2206\u2192ded ontset \u22061 } then we have\n\u03a60 can explain \u22061 provided \u03a8 is possible\nAgain, the full justification set of the resulting explaining link is \u03a60 \u222a\u03a8.\nOur last definition of [set] explanation links concerns transitivity of explanations. This is a \u201cweak\u201d transitivity since the justifications are gathered, however, we will call this property \u201ctransitivity\u201d. We need to be able to omit in the resulting link the part of the intermediate set which is already explained in the first explanation giving rise to a transitive link : Definition 5 If{ \u03a6 can explain \u22061 \u222a\u22062 provided \u03a81 is possible and \u0393 \u222a\u22062 can explain \u0398 provided \u03a82 is possible,\n} then \u03a6 \u222a \u0393 can explain \u22061 \u222a\u22062 \u222a\u0398\nprovided \u03a81 \u222a\u03a82 is possible.\nAgain, the full justification set is \u03a6 \u222a \u0393 \u222a\u03a81 \u222a\u03a82."}, {"heading": "4.3 About explanation links and arguments", "text": "As a small example, let us suppose that in the causal model is the link\nreason causes Prop\u2203(class2) and that in the ontology are the links : class1\nis-a\u2212\u2192 class2 class1\nis-a\u2212\u2192 class3 Resulting are the next two links in the enriched model : Prop\u2203(class1)\nded ont\u2212\u2192 Prop\u2203(class2) Prop\u2203(class1)\nded ont\u2212\u2192 Prop\u2203(class3) By pattern matching over the diagrams in Fig. 4 and 5\n[cf patterns (3) and (5], we get\nreason can explain Prop\u2203(class3) throughProp\u2203(class1) (\u201cjustification\u201d).\nImportantly, it is assumed that the causal link is expressed on the appropriate level : in other words, should there be some doubts about the kind of objects (here class2) that enjoy Prop\u2203 due to reason, the causal link would be about another class.\nThe proviso accompanying the explanation takes place at the level of justifications : the candidate explanation is worth inasmuch as Prop\u2203(class1) is not contradicted. In particular it must be consistent with the reason causing Prop\u2203(class3) [remind (1)].\nHere is an example from Xynthia data. Consider the causal link Expected(V LPress) causes Expected(SWind) together with the following ontological links{\nHurri is-a\u2212\u2192 SWind\nHurri is-a\u2212\u2192 NatDis. } The links below can be obtained in the enriched model{\nExpected(Hurri) ded ont\u2212\u2192 Expected(SWind) Expected(Hurri) ded ont\u2212\u2192 Expected(NatDis)\n}\nWe get an argument to the effect that from the set of data \u0398 = Expected(V LPress) causes Expected(SWind) Expected(Hurri) ded ont\u2212\u2192 Expected(StrW )\nExpected(Hurri) ded ont\u2212\u2192 Expected(NatDis)  we obtain : \u0398 yields that\nExpected(V LPress) can explain Expected(NatDis) provided Expected(Hurri) is possible.\nThe intuition is that, from these data, it is reasonable to explainExpected(NatDis), byExpected(V LPress),\nprovided Expected(Hurri) is possible (not contradicted by other data)."}, {"heading": "4.4 An example of compound explanations", "text": "Figure 6 displays an example from Xynthia (cf also Fig. 3) of a few possible explanations, represented by dotted lines, with their label such as 1 or 1a. The sets of literals, from which the explaining links start, are framed and numbered from (1) to (5). This shows transitivity of explanations at work : e.g. set 1 can explain Victims_1(BFPA) (explanation link labelled 1+1a+1b) uses explanation links 1, 1a and 1b.\nAnother example is set 5 can explain Victims_3(House1FPA) (explanation link 1+1a+2+3) : explanations 1, 1a, 2 and 3 are at work here, and link 2 uses links 1 and 1a together with Flooded(BFPA)\nded ont\u2212\u2192 Flooded(House1FPA) while explanation 3 comes from links 1+1a together with Flooded(BFPA) ded ont\u2212\u2192 Flooded(FireSt)."}, {"heading": "5 Argumentation", "text": "As just seen, the enriched causal graph allows us to infer explanations for assertions and these explanations might be used in an argumentative context [2, 3]. Let us first provide some motivation from our example.\nA possible set of explanations for the flooded buildings is constituted by the bad weather conditions (\u201cvery low pressure\u201d and \u201cstrong wind\u201d) together with \u201chigh spring tide\u201d (see Fig. 2). Given this explanation (argument), it is possible to attack it by noticing : a strong wind is supposed to trigger the red alarm of my anemometer and I did not\nget any alarm. However, this counter-argument may itself be attacked by remarking that, in the case of a hurricane, that is a kind of strong wind, an anemometer is no longer operating, which can explain that a red alarm cannot be observed.\nLet us see how to consider formally argumentation when relying on an enriched causal model and explanations as described in sections 2 and 4. Of course, we begin with introducing arguments, as follows.\nArgument : That \u201c\u03a6 can explain \u03b3 in view of \u0398, provided \u03b4 is possible\u201d\nis formalized here as an argument whenever (3) holds, that is : \u0398 yields that \u03b3 can be explained by \u03a6, provided \u03b4 is possible.\nThe components of an argument consist of : \u2013 \u03a6, the explanation, a set of literals. \u2013 \u03b3, the statement being explained, a literal. \u2013 \u2206, the justification of the explanation (see Section 4),\na set of literals. \u2013 \u0398, the evidence, comprised of propositions (e.g.,(\u2227 \u03a6 ) \u2192 \u03b3), causal links\n(e.g., \u03a6 causes \u03b2), and ontological deduction links (e.g., \u03b4 ded ont\u2212\u2192 \u03b2).\nHere is an illustration from the Xynthia event. From Fig. 2, that the BFPA buildings are flooded can be explained via the set of two causal links\n\u0398 = { Occurs(Flooding) causes F looded(BFPA),Occurs(V LPress)Occurs(SWind) Occurs(HSTide)  causes Occurs(Flooding) } More precisely, using the basic case (2) twice, we obtain that Flooded(BFPA) can be explained by the set of literals\n\u03a6 =  Occurs(V LPress)Occurs(SWind) Occurs(HSTide)  The corresponding argument is\n\u0398 yields that \u03b3 can be explained by \u03a6, provided \u2206 is possible\nwhere \u2013 The explanation is \u03a6. \u2013 The statement being explained is \u03b3 = Flooded(BFPA). \u2013 The justification of the explanation is empty. \u2013 The evidence is \u0398."}, {"heading": "5.1 Counter-arguments", "text": "Generally speaking, an argument \u201c\u03a6 can explain \u03b3 in view of \u0398, provided \u2206 is possible\u201d is challenged by any statement which questions\n1. either \u03a6 (e.g., an argument exhibiting an explanation for the negation of \u2227 \u03a6)\n2. or \u03b3 (e.g., an argument exhibiting an explanation for the negation of \u03b3)\n3. or \u2206 (e.g., an argument exhibiting an explanation for the negation of \u2227 \u2206)\n4. or any item in \u0398 (e.g., an argument exhibiting an explanation for the negation of \u0398 for some \u03b8 occurring in \u0398)\n5. or does so by refutation : using any of \u03a6, \u0398, \u2206 and \u03b3 to explain some falsehood.\nSuch objections are counter-arguments (they have the form of an argument : they explain something \u2013but what they explain contradicts something in the challenged argument).\nDispute. Let us consider the illustration at the start of this section : The argument (that the buildings in the flood-prone area are flooded can be explained, partly, by a strong wind) is under attack on the grounds that my anemometer did not turn red \u2013indicating that no strong wind occurred. The latter is a counter-argument of type 5 in above list. Indeed, the statement to be explained by the counter-argument is the falsehood Red(anemo) (e.g., Green(Anemo) has been observed), using SWind, i.e., an item in the explanation in the attacked argument. Remember, the attacked argument is\n\u0398 yields that Flooded(BFPA) can be explained by Occurs(V LPress)Occurs(SWind) Occurs(HSTide)  Taking Red(Anemo) to be a falsehood, the counterargument at hand is\n\u0398\u2019 yields that Red(anemo) can be explained by{ Occurs(SWind) OK(Anemo) } where \u2013 The explanation is\n\u03a6\u2032 = {Occurs(SWind), OK(Anemo)}. \u2013 The statement being explained is \u03b3\u2032 = Red(Anemo). \u2013 The evidence is \u0398\u2032 ={{\nOccurs(SWind) OK(Anemo)\n} causes Red(anemo) } In our illustration, this counter-argument has in turn a counter-argument (of type 1.) explaining why the anemometer did not get red : i.e., explaining the negation of an\nitem (OK(Anemo) is the item in question) in the explanation in the counter-argument. So, the counter-counterargument here is : \u0398\u201d yields that\n\u00acOK(Anemo) can be explained by { Occurs(Hurri) } where\n\u2013 The explanation is \u03a6\u2032\u2032 = {Occurs(Hurri)}. \u2013 The statement being explained is \u03b3\u2032\u2032 = \u00acOK(Anemo). \u2013 The evidence is \u0398\u2032\u2032 = {Occurs(Hurri) causes \u00acOK(Anemo)}\nThe dispute can extend to a counter-counter-counterargument and so on as the process iterates."}, {"heading": "6 Conclusion", "text": "The aim of this work is to study the link between causes and explanations [5, 6], and to rely on explanations in an argumentative context [2].\nIn a first part, we define explanations as resulting from both causal and ontological links. An enriched causal model is built from a causal model and an ontology, from which the explaining links are derived (cf e.g. Fig4 and (2). Our work differs from other approaches in the literature in that it strictly separates causality, ontology and explanations, while considering that ontology is key in generating sensible explanations from causal statements. Note however that some authors have already introduced ontology to be used for problem solving tasks as planning [7, Chapter 2] and more recently for diagnosis and repair [9]. We then argue that these causal explanations are interesting building blocks to be used in an argumentative context.\nAlthough explanation and argumentation have long been identified as distinct processes [10], it is also recognized that the distinction is a matter of context, hence they both play a role [4] when it comes to eliciting an answer to a \u201cwhy\u201d question. This is exactly what is attempted in this paper, as we are providing \u201cpossible\u201d explanations, that thus can be turned into arguments. The argument format has some advantages inasmuch as its uniformity allows us to express objection in an iterated way : \u201cpossible\u201d explanations are challenged by counter-arguments that happen to represent rival, or incompatible, \u201cpossible\u201d explanations. However, a lot remains to be done. Among others, comparing competing explanations according to minimality, preferences, and generally a host of criteria.\nWe have designed a system in answer set programming that implements most of the explicative proposal introduced above. We plan to include it in an argumentative framework and think it will a good basis for a really practical system, able to manage with a as rich and tricky example as the full Xinthia example.\nR\u00e9f\u00e9rences\n[1] Philippe Besnard, Marie-Odile Cordier, and Yves Moinard. Ontology-based inference for causal explanation. Integrated Computer-Aided Engineering, 15 :351\u2013367, 2008.\n[2] Philippe Besnard and Anthony Hunter. Elements of Argumentation. MIT Press, Cambridge, 2008.\n[3] Phan Minh Dung. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial Intelligence, 77 :321\u2013357, 1995.\n[4] Justin Scott Giboney, Susan Brown, and Jay F. Nunamaker Jr. User acceptance of knowledge-based system recommendations : Explanations, arguments, and fit. In 45th Annual Hawaii International Conference on System Sciences (HICSS\u201945), pages 3719\u20133727. IEEE Computer Society, 2012.\n[5] Joseph Halpern and Judea Pearl. Causes and Explanations : A Structural-Model Approach. Part I : Causes. In Jack S. Breese and Daphne Koller, editors, 17th Conference in Uncertainty in Artificial Intelligence (UAI\u201901), pages 194\u2013202. Morgan Kaufmann, 2001.\n[6] Joseph Y. Halpern and Judea Pearl. Causes and Explanations : A Structural-Model Approach. Part II : Explanations. In Bernhard Nebel, editor, 17th International Joint Conference on Artificial Intelligence (IJCAI\u201901), pages 27\u201334. Morgan Kaufmann, 2001.\n[7] Henry Kautz. Reasoning about plans. In James F. Allen, Henry A. Kautz, Richard N. Pelavin, and Joshua D. Tennenberg, editors, Reasoning About Plans, chapter A Formal Theory of Plan Recognition and its Implementation, pages 69\u2013126. Morgan Kaufmann, 1991.\n[8] Dov Hugh Mellor. The Facts of Causation. Routledge, London, 1995.\n[9] Gianluca Torta, Daniele Theseider Dupr\u00e9, and Luca Anselma. Hypothesis discrimination with abstractions based on observation and action costs. In Alban Grastien and Markus Strumpter, editors, 19th Workshop on Principles of Diagnosis (DX\u201908), pages 189\u2013 196, Blue Mountains, NSW, Australia, 2008.\n[10] Douglas Walton. Explanations and arguments based on practical reasoning. In Thomas Roth-Berghofer, Nava Tintarev, and David B. Leake, editors, Workshop on Explanation-Aware Computing at IJCAI\u201909, pages 72\u201383, Pasadena, CA, U.S.A., July 2009."}], "references": [{"title": "Ontology-based inference for causal explanation", "author": ["Philippe Besnard", "Marie-Odile Cordier", "Yves Moinard"], "venue": "Integrated Computer-Aided Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Elements of Argumentation", "author": ["Philippe Besnard", "Anthony Hunter"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "author": ["Phan Minh Dung"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "User acceptance of knowledge-based system recommendations : Explanations, arguments, and fit", "author": ["Justin Scott Giboney", "Susan Brown", "Jay F. Nunamaker Jr."], "venue": "In 45th Annual Hawaii International Conference on System Sciences", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Causes and Explanations : A Structural-Model Approach", "author": ["Joseph Halpern", "Judea Pearl"], "venue": "Part I : Causes", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Causes and Explanations : A Structural-Model Approach", "author": ["Joseph Y. Halpern", "Judea Pearl"], "venue": "Part II : Explanations", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Reasoning about plans", "author": ["Henry Kautz"], "venue": "Reasoning About Plans, chapter A Formal Theory of Plan Recognition and its Implementation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "The Facts of Causation", "author": ["Dov Hugh Mellor"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Hypothesis discrimination with abstractions based on observation and action costs", "author": ["Gianluca Torta", "Daniele Theseider Dupr\u00e9", "Luca Anselma"], "venue": "In Alban Grastien and Markus Strumpter, editors, 19th Workshop on Principles of Diagnosis", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Explanations and arguments based on practical reasoning", "author": ["Douglas Walton"], "venue": "Workshop on Explanation-Aware Computing at IJCAI\u201909,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Then the causal formalism will produce various explanation links [1].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "By a causal model [8], we mean a representation of a body of causal relationships to be used to generate arguments that display explanations for a given set of facts.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "As just seen, the enriched causal graph allows us to infer explanations for assertions and these explanations might be used in an argumentative context [2, 3].", "startOffset": 152, "endOffset": 158}, {"referenceID": 2, "context": "As just seen, the enriched causal graph allows us to infer explanations for assertions and these explanations might be used in an argumentative context [2, 3].", "startOffset": 152, "endOffset": 158}, {"referenceID": 4, "context": "The aim of this work is to study the link between causes and explanations [5, 6], and to rely on explanations in an argumentative context [2].", "startOffset": 74, "endOffset": 80}, {"referenceID": 5, "context": "The aim of this work is to study the link between causes and explanations [5, 6], and to rely on explanations in an argumentative context [2].", "startOffset": 74, "endOffset": 80}, {"referenceID": 1, "context": "The aim of this work is to study the link between causes and explanations [5, 6], and to rely on explanations in an argumentative context [2].", "startOffset": 138, "endOffset": 141}, {"referenceID": 8, "context": "Note however that some authors have already introduced ontology to be used for problem solving tasks as planning [7, Chapter 2] and more recently for diagnosis and repair [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 9, "context": "Although explanation and argumentation have long been identified as distinct processes [10], it is also recognized that the distinction is a matter of context, hence they both play a role [4] when it comes to eliciting an answer to a \u201cwhy\u201d question.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "Although explanation and argumentation have long been identified as distinct processes [10], it is also recognized that the distinction is a matter of context, hence they both play a role [4] when it comes to eliciting an answer to a \u201cwhy\u201d question.", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "[1] Philippe Besnard, Marie-Odile Cordier, and Yves Moinard.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Philippe Besnard and Anthony Hunter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Phan Minh Dung.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Justin Scott Giboney, Susan Brown, and Jay F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Joseph Halpern and Judea Pearl.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Joseph Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Henry Kautz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Dov Hugh Mellor.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Gianluca Torta, Daniele Theseider Dupr\u00e9, and Luca Anselma.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Douglas Walton.", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "We investigate an approach to reasoning about causes through argumentation. We consider a causal model for a physical system, and look for arguments about facts. Some arguments are meant to provide explanations of facts whereas some challenge these explanations and so on. At the root of argumentation here, are causal links ({A1, \u00b7 \u00b7 \u00b7 , An} causes B) and ontological links (o1 is_a o2). We present a system that provides a candidate explanation ({A1, \u00b7 \u00b7 \u00b7 , An} explains {B1, \u00b7 \u00b7 \u00b7 , Bm}) by resorting to an underlying causal link substantiated with appropriate ontological links. Argumentation is then at work from these various explaining links. A case study is developed : a severe storm Xynthia that devastated part of France in 2010, with an unaccountably high number of casualties.", "creator": "LaTeX with hyperref package"}}}