{"id": "1401.4590", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks", "abstract": "Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen's F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted. This paper attempts to quantify this problem and presents a number of practical solutions that can be used to evaluate such differences.\n\n\nThe UIR results in an increase in the relative weights of two systems. This approach can be used to measure changes in the relative weights of two systems, as well as to determine whether a system is \"fairly fair\" to each other. A problem with the UIR is that the difference between systems can be measured only by comparing the relative weights of both systems. UIR works by assigning a system to the systems that allow the system to perform a given task. In fact, UIR is essentially an unanimous system when the two systems differ in its goals: it is, in fact, an unfavorably weighted system when it is, and the two systems will eventually agree to the UIR.\nWe believe that the UIR is the most reliable measure of the system's effectiveness. The United States Government's World Economic Forum, which began in 1970, estimates that over a million Americans use the UIR to measure their economic and economic quality, based on estimates by the UIR itself. It assumes that each system is fairly balanced and that all systems of each provide an overall system ranking.\nWe use UIR to quantify how well each system performs a given task. In fact, UIR's results in a total of 2,000 UIR-related errors. The UIR is not just one system, it is a whole other. In fact, it is, in fact, the total UIR is the highest in the world. The UIR is the most reliable measure of the system, and the system is the most reliable way of determining the overall UIR. For example, UIR's total of 4,500 UIR-related errors (more than 100 per year) is a total of 7,500 UIR-related errors. This is just", "histories": [["v1", "Sat, 18 Jan 2014 21:03:23 GMT  (1181kb)", "http://arxiv.org/abs/1401.4590v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["enrique amig\\'o", "julio gonzalo", "javier artiles", "felisa verdejo"], "accepted": false, "id": "1401.4590"}, "pdf": {"name": "1401.4590.pdf", "metadata": {"source": "CRF", "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks", "authors": ["Enrique Amig\u00f3", "Julio Gonzalo", "Javier Artiles", "Felisa Verdejo"], "emails": ["enrique@lsi.uned.es", "julio@lsi.uned.es", "javier.artiles@qc.cuny.edu", "felisa@lsi.uned.es"], "sections": [{"heading": null, "text": "Besides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed."}, {"heading": "1. Introduction", "text": "Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion, and some sort of weighted combination is needed to provide system rankings. Many problems, for instance, require considering both Precision (P) and Recall (R) to compare systems\u2019 performance. Perhaps the most common combining function is the F-measure (van Rijsbergen, 1974), which includes a parameter \u03b1 that sets the relative weight of metrics; when \u03b1 = 0.5, both metrics have the same relative weight and F computes their harmonic mean.\nA problem of weighted combination measures is that relative weights are established intuitively for a given task, but at the same time a slight change in the relative weights may produce substantial changes in the system rankings. The reason for this behavior is that an overall improvement in F often derives from an improvement in one of the individual\nc\u00a92011 AI Access Foundation. All rights reserved.\nmetrics at the expense of a decrement in the other. For instance, if a system A improves a system B in precision with a loss in recall, F may say that A is better than B or viceversa, depending on the relative weight of precision and recall (i.e. the \u03b1 value).\nThis situation is more common than one might expect. Table 1 shows evaluation results for different tasks extracted from the ACL 2009 (Su et al., 2009) conference proceedings, in which P and R are combined using the F-measure. For each paper we have considered three evaluation results: the one that maximizes F, which is presented as the best result in the paper, the baseline, and an alternative method that is also considered. Note that in all cases, the top ranked system improves the baseline according to the F-measure, but at the cost of decreasing one of the metrics. For instance, in the case of the paper on Word Alignment, the average R grows from 54.82 to 72.49, while P decreases from 72.76 to 69.19. In the paper on Sentiment Analysis, P increases in four points but R decreases in five points. How reasonable is to assume that the contrastive system is indeed improving the baseline?\nThe evaluation results for the alternative approach are also controversial: in all cases, the alternative approach improves the best system according to one metric, and it is improved according to the other. Therefore, depending on the relative metric weighting, the alternative approach could be considered better or worse than the best scored system.\nThe conclusion is that the \u03b1 parameter is crucial when comparing real systems. In practice, however, most authors set \u03b1 = 0.5 (equal weights for precision and recall) as a standard, agnostic choice that requires no further justification. Thus, without a notion of how much a perceived difference between systems depends on the relative weights between metrics, the interpretation of results with F \u2013 or any other combination scheme \u2013 can be misleading.\nOur goal is, therefore, to find a way of estimating to what extent a perceived difference using a metric combination scheme \u2013 F or any other \u2013 is robust to changes in the relative weights assigned to each individual metric.\nIn this paper we propose a novel measure, the Unanimity Improvement Ratio (UIR), which relies on a simple observation: when a system A improves other system B according to all individual metrics (the improvement is unanimous), A is better than B for any weighting scheme. Given a test collection with n test cases, the more test cases where improvements are unanimous, the more robust the perceived difference (average difference in F or any other combination scheme) will be.\nIn other words, as well as statistical significance tests provide information about the robustness of the evaluation across test cases (Is the perceived difference between two systems an artifact of the set of test cases used in the test collection? ), UIR is meant to provide information about the robustness of the evaluation across variations of the relative metric weightings (Is the perceived difference between two systems an artifact of the relative metric weighting chosen in the evaluation metric? ).\nOur experiments on clustering test collections show that UIR contributes to the analysis of evaluation results in two ways:\n\u2022 It allows to detect system improvements that are biased by the metric weighting scheme. In such cases, experimenters should carefully justify a particular choice of relative weights and check whether results are swapped in their vicinity.\n\u2022 It increases substantially the consistency of evaluation results across datasets: a result that is supported by a high Unanimous Improvement Ratio is much more likely to hold in a different test collection. This is, perhaps, the most relevant practical application of UIR: as a predictor of how much a result can be replicable across test collections.\nAlthough most of the work presented in this paper applies to other research areas, here we will focus on the clustering task as one of the most relevant examples because clustering tasks are specially sensitive to the metric relative weightings. Our research goals are:\n1. To investigate empirically whether clustering evaluation can be biased by precision and recall relative weights in F. We will use one of the most recent test collections focused on a text clustering problem (Artiles, Gonzalo, & Sekine, 2009).\n2. To introduce a measure that quantifies the robustness of evaluation results across metric combining criteria, which leads us to propose the UIR measure, which is derived from the Conjoint Measurement Theory (Luce & Tukey, 1964).\n3. To analyze empirically how UIR and F-measure complement each other.\n4. To illustrate the application of UIR when comparing systems in the context of a shared task, and measure how UIR serves as a predictor of the consistency of evaluation results across different test collections."}, {"heading": "2. Combining Functions for Evaluation Metrics", "text": "In this section we briefly review different metrics combination criteria. We present the rationale behind each metric weighting approach as well as its effects on the systems ranking."}, {"heading": "2.1 F-measure", "text": "The most frequent way of combining two evaluation metrics is the F-measure (van Rijsbergen, 1974). It was originally proposed for the evaluation of Information Retrieval systems (IR), but its use has expanded to many other tasks. Given two metrics P and R (e.g. precision and recall, Purity and Inverse Purity, etc.), van Rijsbergen\u2019s F-measure combines them into a single measure of efficiency as follows:\nF (R,P ) = 1\n\u03b1( 1P ) + (1\u2212 \u03b1)( 1 R)\nF assumes that the \u03b1 value is set for a particular evaluation scenario. This parameter represents the relative weight of metrics. In some cases the \u03b1 value is not crucial; in particular, when metrics are correlated. For instance, Figure 1 shows the precision and recall levels obtained at the CoNLL-2004 shared task for evaluating Semantic Role Labeling systems (Carreras & Ma\u0300rquez, 2004). Except for one system, every substantial improvement in precision involves also an increase in recall. In this case, the relative metric weighting does not substantially modify the system ranking.\nIn cases where the metrics are not completely correlated, the Decreasing Marginal Effectiveness property (van Rijsbergen, 1974) ensures a certain robustness across \u03b1 values. F satisfies this property, which states that a large decrease in one metric cannot be compensated by a large increase in the other metric. Therefore, systems with very low precision or recall will obtain low F-values for any \u03b1 value. This is discussed in more detail in Section 5.1. However, as we will show in Section 3.4, in other cases the Decreasing Marginal\nEffectiveness property does not prevent the F-measure from being overly sensitive to small changes in the \u03b1 value."}, {"heading": "2.2 Precision and Recall Break-even Point", "text": "Another way of combining metrics consists of evaluating the system at the point where one metric equals the other (Tao Li & Zhu, 2006). This method is applicable when each system is represented by a trade-off between both metrics, for instance, a precision/recall curve. This method relies on the idea that increasing both metrics implies necessarily a overall quality increase. For instance, it assumes that obtaining a 0.4 of precision at the recall point 0.4 is better than obtaining a 0.3 of precision at the recall point 0.3.\nActually, the break-even point assumes the same relevance for both metrics. It considers the precision/recall point where the system distributes its efforts equitably between both metrics. Indeed, we could change the relative relevance of metrics when computing the break-even point.\nFigure 2 illustrates this idea. The continuous curve represents the trade-off between precision and recall for system S1. The straight diagonal represents the points where both metrics return the same score. The quality of the system corresponds therefore with the intersection between this diagonal and the precision/recall curve. On the other hand, the discontinuous curve represents another system S2 which achieves an increase of precision at low recall levels at the cost of decreasing precision at high recall levels. According to the break-even points, the second system is superior than the first one.\nHowever, we could give more relevance to recall identifying the point where recall doubles precision. In that case, we would obtain the intersection points Q\u20321 and Q \u2032 2 shown in the figure, which reverses the quality order between systems. In conclusion, the break-even point also assumes an arbitrary relative relevance for the combined metrics."}, {"heading": "2.3 Area Under the Precision/Recall Curve", "text": "Some approaches average scores over every potential parameterization of the metric combining function. For instance, Mean Average Precision (MAP) is oriented to IR systems,\nand computes the average precision across a number of recall levels. Another example is the Receiver Operating Characteristic (ROC) function used to evaluate binary classifiers (Cormack & Lynam, 2005). ROC computes the probability that a positive sample receives a confidence score higher than a negative sample, independently from the threshold used to classify the samples. Both functions are related with the area AUC that exists under the precision/recall curve (Cormack & Lynam, 2005).\nIn both MAP and ROC the low and high recall regions have the same relative relevance when computing this area. Again, we could change the measures in order to assign different weights to high and low recall levels. Indeed in (Weng & Poon, 2008) a weighted Area Under the Curve is proposed. Something similar would happen if we average F across different \u03b1 values.\nNote that these measures can only be applied in certain kinds of problem, such as binary classification or document retrieval, where the system output can be seen as a ranking, and different cutoff points in the ranking give different precision/recall values. They are not directly applicable, in particular, to the clustering problem which is the focus of our work here."}, {"heading": "3. Combining Metrics in Clustering Tasks", "text": "In this section we present metric combination experiments on a specific clustering task. Our results corroborate the importance of quantifying the robustness of systems across different weighting schemes."}, {"heading": "3.1 The Clustering Task", "text": "Clustering (grouping similar items) has applications in a wide range of Artificial Intelligence problems. In particular, in the context of textual information access, clustering algorithms are employed for Information Retrieval (clustering text documents according to their content similarity), document summarization (grouping pieces of text in order to detect redundant information), topic tracking, opinion mining (e.g. grouping opinions about a specific topic), etc.\nIn such scenarios, clustering distributions produced by systems are usually evaluated according to their similarity to a manually produced gold standard (extrinsic evaluation). There is a wide set of metrics that measure this similarity (Amigo\u0301, Gonzalo, Artiles, & Verdejo, 2008), but all of them rely on two quality dimensions: (i) to what extent items in the same cluster also belong to the same group in the gold standard; and (ii) to what extent items in different clusters also belong to different groups in the gold standard. A wide set of extrinsic metrics has been proposed: Entropy and Class Entropy (Steinbach, Karypis, & Kumar, 2000; Ghosh, 2003), Purity and Inverse Purity (Zhao & Karypis, 2001), precision and recall Bcubed metrics (Bagga & Baldwin, 1998), metrics based on counting pairs (Halkidi, Batistakis, & Vazirgiannis, 2001; Meila, 2003), etc.1\n1. See the work of Amigo\u0301 et al. (2008) for a detailed overview."}, {"heading": "3.2 Dataset", "text": "WePS (Web People Search) campaigns are focused on the task of disambiguating person names in Web search results. The input for systems is a ranked list of web pages retrieved from a Web search engine using a person name as a query (e.g. \u201cJohn Smith\u201d). The challenge is to correctly estimate the number of different people sharing the name in the search results and group documents referring to the same individual. For every person name, WePS datasets provide around 100 web pages from the top search results, using the quoted person name as query. In order to provide different ambiguity scenarios, person names were sampled from the US Census, Wikipedia, and listings of Program Committee members of Computer Science Conferences.\nSystems are evaluated comparing their output with a gold standard: a manual grouping of documents produced by two human judges in two rounds (first they annotated the corpus independently and then they discussed the disagreements together). Note that a single document can be assigned to more than one cluster: an Amazon search results list, for instance, may refer to books written by different authors with the same name. The WePS task is, therefore, an overlapping clustering problem, a more general case of clustering where items are not restricted to belong to one single cluster. Both the WePS datasets and the official evaluation metrics reflect this fact.\nFor our experiments we have focused on the evaluation results obtained in the WePS-1 (Artiles, Gonzalo, & Sekine, 2007) and WePS-2 (Artiles et al., 2009) evaluation campaigns. The WePS-1 corpus also includes data from the Web03 test bed (Mann, 2006), which was used for trial purposes and follows similar annotation guidelines, although the number of document per ambiguous name is more variable. We will refer to these corpora as WePS-1a (trial), WePS-1b and WePS-2 2."}, {"heading": "3.3 Thresholds and Stopping Criteria", "text": "The clustering task involves three main aspects that determine the system\u2019s output quality. The first one is the method used for measuring similarity between documents; the second is the clustering algorithm (k-neighbors, Hierarchical Agglomerative Clustering, etc.); and the third aspect to be considered usually consists of a couple of related variables to be fixed: a similarity threshold \u2013 above which two pages will be considered as related \u2013 and a stopping criterion which determines when the clustering process stops and, consequently, the number of clusters produced by the system.\nFigure 3 shows how Purity and Inverse Purity values change for different clustering stopping points, for one of the systems evaluated on the WePS-1b corpus 3. Purity focuses on the frequency of the most common category into each cluster (Amigo\u0301 et al., 2008). Being C the set of clusters to be evaluated, L the set of categories (reference distribution) and\n2. The WePS datasets were selected for our experiments because (i) they address a relevant and well-defined clustering task; (ii) its use is widespread: WePS datasets have been used in hundreds of experiments since the first WePS evaluation in 2007; (iii) runs submitted by participants to WePS-1 and WePS-2 were available to us, which was essential to experiment with different evaluation measures. WePS datasets are freely available from http://nlp.uned.es/weps. 3. This system is based on bag of words, TF/IDF word weighting, stopword removal, cosine distance and a Hierarchical Agglomerative Clustering algorithm.\nN the number of clustered items, Purity is computed by taking the weighted average of maximal precision values:\nPurity = \u2211 i |Ci| N maxj Precision(Ci, Lj)\nwhere the precision of a cluster Ci for a given category Lj is defined as:\nPrecision(Ci, Lj) = |Ci\n\u22c2 Lj |\n|Ci|\nPurity penalizes the noise in a cluster, but it does not reward grouping items from the same category together; if we simply make one cluster per item, we reach trivially a maximum purity value. Inverse Purity focuses on the cluster with maximum recall for each category. Inverse Purity is defined as:\nInverse Purity = \u2211 i |Li| N maxj Precision(Li, Cj)\nInverse Purity rewards grouping items together, but it does not penalize mixing items from different categories; we can reach a maximum value for Inverse purity by making a single cluster with all items.\nAny change in the stopping point implies an increase in Purity at the cost of a decrease in Inverse Purity, or viceversa. Therefore, each possible \u03b1 value in F rewards different stopping points. This phenomenon produces a high dependency between clustering evaluation results and the metric combining function."}, {"heading": "3.4 Robustness Across \u03b1 Values", "text": "Determining the appropriate \u03b1 value for a given scenario is not trivial. For instance, from a user\u2019s point of view in the WePS task, it is easier to discard a few irrelevant documents from the good cluster (because its precision is not perfect but it has a high recall) than having to check for additional relevant documents in all clusters (because its precision is high but its recall is not). Therefore, it seems that Inverse Purity should have priority over Purity, i.e., the value of \u03b1 should be below 0.5. From the point of view of a company providing a web people search service, however, the situation is quite different: their priority is having a very high precision, because mixing the profiles of, say, a criminal and a doctor may result in the company being sued. From their perspective, \u03b1 should receive a high value. The WePS campaign decided to be agnostic and set a neutral \u03b1 = 0.5 value.\nTable 2 shows the resulting system ranking in WePS-1b according to F with \u03b1 set at 0.5 and 0.2. This ranking includes two baseline systems: B1 consists of grouping each document in a separate cluster, and B100 consists of grouping all documents into one single cluster. B1 maximizes Purity, and B100 maximizes Inverse Purity.\nB1 and B100 may obtain a high or low F-measure depending on the \u03b1 value. As the table shows, for \u03b1 = 0.5 B1 outperforms B100 and also a considerable number of systems. The reason for this result is that, in the WePS-1b test set, there are many singleton clusters (people which are referred to in only one web page). This means that a default strategy of making one cluster per document will not only achieve maximal Purity, but also an acceptable Inverse Purity (0.45). However, if \u03b1 is fixed at 0.2, B1 goes down to the bottom of the ranking and it is outperformed by all systems, including the other baseline B100.\nNote that outperforming a trivial baseline system such as B1 is crucial to optimize systems, given that the optimization cycle could otherwise lead to a baseline approach like B1. The drawback of B1 is that it is not informative (the output does not depend on the input) and, crucially, it is very sensitive to variations in \u03b1. In other words, its performance is not robust to changes in the metric combination criterion. Remarkably, the top scoring system, S1, is the best for both \u03b1 values. Our primary motivation in this article is to quantify the robustness across \u03b1 values in order to complement the information given by traditional system ranking."}, {"heading": "3.5 Robustness Across Test Beds", "text": "The average size of the clusters in the gold standard may change from one test bed to another. As this affects the Purity and Inverse Purity trade-off, the same clustering system may obtain a different balance between both metrics in different corpora; and this may produce contradictory evaluation results when comparing systems across different corpora, even for the same \u03b1 value.\nFor instance, in the WePS-1b test bed (Artiles et al., 2007), B1 substantially outperforms B100 (0.58 vs. 0.49 using F\u03b1=0.5). In the WePS-2 data set (Artiles et al., 2009), however, B100 outperforms B1 (0.53 versus 0.34). The reason is that singletons are less common in WePS-2. In other words, the comparison between B100 and B1 depends both on the \u03b1 value and of the particular distribution of reference cluster sizes in the test bed.\nOur point is that system improvements that are robust across \u03b1 values (which is not the case of B1 and B100) should not be affected by this phenomenon. Therefore, estimating the\nrobustness of system improvements to changes in \u03b1 should prevent reaching contradictory results for different test beds. Indeed, evidence for this is presented in Section 7."}, {"heading": "4. Proposal", "text": "Our primary motivation in this article is to quantify the robustness across \u03b1 values in order to complement the information given by traditional system rankings. To this end we introduce in this section the Unanimous Improvement Ratio."}, {"heading": "4.1 Unanimous Improvements", "text": "The problem of combining evaluation metrics is closely related with the theory of conjoint measurement (see Section 5.1 for a detailed discussion). Van Rijsbergen (1974) argued that it is not possible to determine empirically which metric combining function is the most adequate in the context of Information Retrieval evaluation. However, starting from the measurement theory principles, van Rijsbergen described the set of properties that a metric combining function should satisfy. This set includes the Independence axiom (also called Single Cancellation), from which the Monotonicity property derives. The Monotonicity property states that the quality of a system that surpasses or equals another one according to all metrics is necessarily equal or better than the other. In other words, one system is\nbetter than the other with no dependence whatsoever on how the relative importance of each metric is set.\nWe will define a combination procedure for metrics, Unanimous Improvement, which is based on this property:\nQX(a) \u2265\u2200 QX(b) if and only if \u2200x \u2208 X.Qx(a) \u2265 Qx(b)\nwhere QX(a) is the quality of a according to a set of metrics X.\nThis relationship has no dependence on how metrics are scaled or weighted, or on their degree of correlation in the metric set. Equality (=\u2200) can be derived directly from \u2265\u2200: The unanimous equality implies that both systems obtain the same score for all metrics:\nQX(a) =\u2200 QX(b) \u2261 (QX(a) \u2265\u2200 QX(b)) \u2227 (QX(b) \u2265\u2200 QX(a))\nThe strict unanimous improvement implies that one system improves the other strictly at least for one metric, and it is not improved according to any metric:\nQX(a) >\u2200 QX(b) \u2261 (QX(a) \u2265\u2200 QX(b)) \u2227 \u00ac(QX(a) =\u2200 QX(b)) \u2261\n(QX(a) \u2265\u2200 QX(b)) \u2227 \u00ac(QX(b) \u2265\u2200 QX(a))\nNon comparability \u2016 can also derived from here: it occurs when some metrics favor one system and some other metrics favor the other. We refer to this cases as metric-biased improvements.\nQX(a)\u2016\u2200QX(b) \u2261 \u00ac(QX(a) \u2265\u2200 QX(b)) \u2227 \u00ac(QX(b) \u2265\u2200 QX(a))\nThe theoretical properties of the Unanimous Improvement are described in depth in Section 5.2. The most important property is that the Unanimous Improvement is the only relational structure that does not depend on relative metric weightings, while satisfying the Independence (Monotonicity) axiom. In other words, we can claim that: A system improvement according to a metric combining function does not depend whatsoever on metric weightings if and only if there is no quality decrease according to any individual metric. The theoretical justification of this assertion is developed in Section 5.2.1."}, {"heading": "4.2 Unanimous Improvement Ratio", "text": "According to the Unanimous Improvement, our unique observable over each test case is a three-valued function (unanimous improvement, equality or biased improvement). We need, however, a way of quantitatively comparing systems.\nGiven two systems, a and b, and the Unanimous Improvement relationship over a set of test cases T , we have samples where a improves b (QX(a) \u2265\u2200 QX(b)), samples where b improves a (QX(b) \u2265\u2200 QX(a)) and also samples with biased improvements (QX(a)\u2016\u2200QX(b)). We will refer to these sets as Ta\u2265\u2200b, Tb\u2265\u2200a and Ta\u2016\u2200b, respectively. The Unanimous Improvement Ratio (UIR) is defined according to three formal restrictions:\n1. UIR(a, b) should decrease with the number of biased improvements (Ta\u2016\u2200b). In the boundary condition where all samples are biased improvements (Ta\u2016\u2200b = T ), then UIR(a, b) should be 0.\n2. If a improves b as much as b improves a (Ta\u2265\u2200b = Tb\u2265\u2200a) then UIR(a, b) = 0.\n3. Given a fixed number of biased improvements (Ta\u2016\u2200b), UIR(a, b) should be proportional to Ta\u2265\u2200b and inversely proportional to Tb\u2265\u2200a.\nGiven these restrictions, we propose the following UIR definition:\nUIRX,T (a, b) = |Ta\u2265\u2200b| \u2212 |Tb\u2265\u2200a|\n|T | =\n|t \u2208 T/QX(a) \u2265\u2200 QX(b)| \u2212 |t \u2208 T/QX(b) \u2265\u2200 QX(a)| |T |\nwhich can be alternatively formulated as:\nUIRX,T (a, b) = P(a \u2265\u2200 b)\u2212 P(b \u2265\u2200 a)\nwhere these probabilities are estimated in a frequentist manner. UIR range is [\u22121, 1] and is not symmetric: UIRX,T (a, b) = \u2212UIRX,T (b, a). As an illustration of how UIR is computed, consider the experiment outcome in Table 3. Systems A and B are compared in terms of precision and recall for 10 test cases. For test case 5, for instance, A has an unanimous improvement over B: it is better both in terms of precision (0.7 > 0.6) and recall (0.5 > 0.4). From the table, UIR value is:\nUIRX,T (A,B) = |TA\u2265\u2200B| \u2212 |TB\u2265\u2200A|\n|T | = 6\u2212 4 10 = 0.2 = \u2212UIRX,T (B,A)\nUIR has two formal limitations. First, it is not transitive (see Section 5.2). Therefore, it is not possible to define a linear system ranking based on UIR. This is, however, not\nnecessary: UIR is not meant to provide a ranking, but to complement the ranking provided by the F-measure (or other metric combining function), indicating how robust results are to changes in \u03b1. Section 6.4 illustrates how UIR can be integrated with the insights provided by a system ranking.\nThe second limitation is that UIR does not consider improvement ranges; therefore, it is less sensitive than the F-measure. Our empirical results, however, show that UIR is sensitive enough to discriminate robust improvements versus metric-biased improvements; and in Section 8 we make an empirical comparison of our non-parametric definition of UIR with a parametric version, with results that make the non-parametric definition preferable."}, {"heading": "5. Theoretical Foundations", "text": "In this section we discuss the theoretical foundations of the Unanimous Improvement Ratio in the framework of the Conjoint Measurement Theory. Then we proceed to describe the formal properties of UIR and their implications from the point of view of the evaluation methodology. Readers interested solely in the practical implications of using UIR may proceed directly to Section 6."}, {"heading": "5.1 Conjoint Measurement Theory", "text": "The problem of combining evaluation metrics is closely related with the Conjoint Measurement Theory, which was independently discovered by the economist Debreu (1959) and the mathematical psychologist R. Duncan Luce and statistician John Tukey (Luce & Tukey, 1964). The Theory of Measurement defines the necessary conditions to state an homomorphism between an empirical relational structure (e.g. \u201cJohn is bigger than Bill\u201d) and a numeric relational structure (\u201cJohn\u2019s height is 1.79 meters and Bill\u2019s height is 1.56 meters\u201d). In the case of the Conjoint Measurement Theory, the relational structure is factored into two (or more) ordered substructures (e.g. \u201cheight and weight\u201d).\nIn our context, the numerical structures are given by the evaluation metric scores (e.g. Purity and Inverse Purity). However, we do not have an empirical quality ordering for clustering systems. Different human assessors could assign more relevance to Purity than to Inverse Purity or viceversa. Nevertheless, the Conjoint Measurement Theory does provide mechanisms that state what kind of numerical structures can produce an homomorphism assuming that the empirical structure satisfies certain axioms. Van Rijsbergen (1974) used this idea to analyze the problem of combining evaluation metrics. These axioms shape an additive conjoint structure. Being (R,P ) the quality of a system according to two evaluation metrics R and P , these axioms are:\nConnectedness: All systems should be comparable to each other. Formally: (R,P ) \u2265 (R\u2032, P \u2032) or (R\u2032, P \u2032) \u2265 (R,P ).\nTransitivity: (R,P ) \u2265 (R\u2032, P \u2032) and (R\u2032, P \u2032) \u2265 (R\u2032\u2032, P \u2032\u2032) implies that (R,P ) \u2265 (R\u2032\u2032, P \u2032\u2032). The axioms Transitivity and Connectedness shape a weak order.\nThomsen condition: (R1, P3) \u223c (R3, P2) and (R3, P1) \u223c (R2, P3) imply that (R1, P1) \u223c (R2, P2) (where \u223c indicates equal effectiveness).\nIndependence: \u201cThe two components contribute their effects independently to the effectiveness\u201d. Formally, (R1, P ) \u2265 (R2, P ) implies that (R1, P \u2032) \u2265 (R2, P \u2032) for all P \u2032, and (R,P1) \u2265 (R,P2) implies that (R\u2032, P2) \u2265 (R\u2032, P2) for all R\u2032. This property implies Monotonicity (Narens & Luce, 1986) which states that an improvement in both metrics necessarily produces an improvement according to the metric combining function.\nRestricted Solvability: A property which is \u201c... concerned with the continuity of each component. It makes precise what intuitively we would expect when considering the existence of intermediate levels\u201d. Formally: whenever (R1, P\n\u2032) \u2265 (R,P ) \u2265 (R2, P \u2032) then exists R\u2019 such that (R\u2032, P \u2032) = (R,P ).\nEssential Components: \u201cVariation in one while leaving the other constant gives a variation in effectiveness\u201d. There exists R, R\u2032 and P such that it is not the case that (R,P ) = (R\u2032, P ); and there exists P , P \u2032 and R such that it is not the case that (R,P ) = (R,P \u2032).\nArchimedean Property: which \u201cmerely ensures that the intervals on a component are comparable\u201d.\nThe F-measure proposed by van Rijsbergen (1974) and the arithmetic mean of P,R satisfy all these axioms. According to these restrictions, indeed, an unlimited set of acceptable combining functions for evaluation metrics can be defined. The F relational structure, however, satisfies another property which is not satisfied by other functions such as the arithmetic mean. This property is the Decreasing Marginal Effectiveness. The basic idea is that increasing one unit in one metric and decreasing one unit in the other metric can improve the overall quality (i.e. if the first metric has more weight in the combining function), but this does not imply that a great loss in one metric can be compensated by a great increase in the other. It can be defined as:\n\u2200R,P > 0,\u2203n > 0 such that ((P + n,R\u2212 n) < (R,P ))\nAccording to this, high values in both metrics are required to obtain a high overall improvement. This makes measures observing this property - such as F - more robust to arbitrary metric weightings."}, {"heading": "5.2 Formal Properties of the Unanimous Improvement", "text": "The Unanimous Improvement \u2265\u2200x trivially satisfies most of the desirable properties proposed by van Rijsbergen (1974) for metric combining functions: transitivity, independence, Thomsen condition, Restricted Solvability, Essential Components and Decreasing Marginal Effectiveness; the exception being the connectedness property4. Given that the non comparability \u2016\u2200 (biased improvements, see Section 4.1) is derived from the Unanimous Improvement, it is possible to find system pairs where neither QX(a) \u2265\u2200 QX(b) nor QX(b) \u2265\u2200 QX(a) hold. Therefore, Connectedness is not satisfied.\nFormally, the limitation of the Unanimous Improvement is that it does not represent a weak order, because it cannot satisfy Transitivity and Connectedness simultaneously. Let us elaborate on this issue.\n4. For the sake of simplicity, we consider here the combination of two metrics (R,P ).\nWe could satisfy Connectedness by considering that biased improvements represent equivalent system pairs (=\u2200). But in this case, Transitivity would not be satisfied. See, for instance, Table 4. According to the table:\nQX(B)\u2016\u2200QX(A) and QX(C)\u2016\u2200QX(B)\nTherefore, considering that \u2016\u2200 represents equivalence, we have:\nQX(B) \u2265\u2200 QX(A) and QX(C) \u2265\u2200 QX(B)\nbut not\nQX(C) \u2265\u2200 QX(A)\nIn summary, we can choose to satisfy transitivity or connectedness, but not both: the Unanimous Improvement can not derive a weak order."}, {"heading": "5.2.1 Uniqueness of the Unanimous Improvement", "text": "The Unanimous Improvement has the interesting property that is does not contradict any evaluation result given by the F-measure, regardless of the \u03b1 value used in F:\nQX(a) \u2265\u2200 QX(b)\u2192 \u2200\u03b1F\u03b1(a) \u2265 F\u03b1(b)\nThis is due to the fact that the F-measure (for any \u03b1 value) satisfies the monotonicity axiom, in which the Unanimous Improvement is grounded. This property is essential for the purpose of checking the robustness of system improvements across \u03b1 values. And crucially, the Unanimous Improvement is the only function that satisfies this property. More precisely, the Unanimous Improvement is the only relational structure that, while satisfying monotonicity, does not contradict any Additive Conjoint Structure (see Section 5.1).\nIn order to prove this assertion, we need to define the concept of compatibility with any additive conjoint structure. Let \u2265add be any additive conjoint structure and let \u2265R be any relational structure. We will say that \u2265R is compatible with any conjoint structure if and only if:\n\u2200\u3008a, b,\u2265add\u3009.(QX(a) \u2265R QX(b))\u2192 (QX(a) \u2265add QX(b))\nIn other words: if \u2265R holds, then any other additive conjoint holds. We want to prove that the unanimous improvement is the only relation that satisfies this property; therefore, we have to prove that if \u2265R is a monotonic and compatible relational structure, then it necessarily matches the unanimous improvement definition:\n\u2265R is monotonic and compatible =\u21d2 (QX(a) \u2265R QX(b)\u2194 xi(a) \u2265 xi(b)\u2200xi \u2208 X)\nwhich can be split in:\n(1) \u2265R monotonic and compatible =\u21d2 (QX(a) \u2265R QX(b)\u2190 xi(a) \u2265 xi(b)\u2200xi \u2208 X)\n(2) \u2265R monotonic and compatible =\u21d2 (QX(a) \u2265R QX(b)\u2192 xi(a) \u2265 xi(b)\u2200xi \u2208 X)\nProving (1) is immediate, since the rightmost component corresponds with the monotonicity property definition. Let us prove (2) by reductio ad absurdum, assuming that there exists a relational structure \u2265o such that:\n(\u2265o monotonic and compatible) \u2227 (QX(a) \u2265o QX(b)) \u2227 (\u2203xi \u2208 X.xi(a) < xi(b))\nIn this case, we could define an additive conjoint structure over the combined measure Q\u2032X(a) = \u03b11x1(a)+ ..\u03b1ixi(a)..+\u03b1nxn(a) with \u03b1i big enough such that Q \u2032 X(a) < Q \u2032 X(b). The Q\u2032 additive conjoint structure would contradict \u2265o. Therefore, \u2265o would not be compatible (contradiction). In conclusion, predicate (2) is true and the Unanimous Improvement \u2265\u2200X is the only monotonic and compatible relational structure.\nAn interesting corollary can be derived from this analysis. If the Unanimous Improvement is the only compatible relational structure, then we can formally conclude that the measurement of system improvements without dependence on metric weighting schemes can not derive a weak order (i.e. one that satisfies both transitivity and connectedness). This corollary has practical implications: it is not possible to establish a system ranking which is independent on metric weighting schemes.\nA natural way to proceed is, therefore, to use the unanimous improvement as an addition to the standard F-measure (for a suitable \u03b1 value) which provides additional information about the robustness of system improvements across \u03b1 values."}, {"heading": "6. F versus UIR: Empirical Study", "text": "In this Section we perform a number of empirical studies on the WePS corpora in order to find out how UIR behaves in practice. First, we focus on a number of empirical results that show how UIR rewards robustness across \u03b1 values, and how this information is complementary to the information provided by F. Second, we examine to what extent \u2013 and why \u2013 F and UIR are correlated."}, {"heading": "6.1 UIR: Rewarding Robustness", "text": "Figure 4 shows three examples of system comparisons in WePS-1b corpus using the metrics Purity and Inverse Purity. Each curve represents the F\u03b1 value obtained for one system according to different \u03b1 values. System S6 (black curves) is compared with S10, S9 and S11 (grey curves) in each of the three graphs. In all cases there is a similar quality increase according to F\u03b1=0.5; UIR, however, ranges between 0.32 and 0.42, depending on how robust the difference is to changes in \u03b1. The highest difference in UIR is for the (S6,S11) system pair (rightmost graph), because these systems do not swap their F\u03b1 values for any \u03b1 value.\nThe smallest UIR value is for (S6,S10), where S6 is better than S10 for \u03b1 values below 0.8, and worse when \u03b1 is larger. This comparison illustrates how UIR captures, for similar increments in F, which ones are less dependent of the relative weighting scheme between precision and recall.\nLet us now consider all two-system combinations in the WePS-1b corpus, dividing them in two sets: (i) system pairs for which F\u03b1 increases for all \u03b1 values (i.e. both Purity and Inverse Purity increases), and (ii) pairs for which the relative system\u2019s performance swaps at some \u03b1 value; i.e. F\u03b1 increases for some \u03b1 values and decreases for the rest.\nOne would expect that the average increase in F\u03b1 should be larger for those system pairs where one beats the other for every \u03b1 value. Surprisingly, this is not true: Table 5 shows the average increments for UIR and F\u03b1=0.5 for both sets. UIR behaves as expected: its average value is substantially larger for the set where different \u03b1 do not lead to contradictory results (0.53 vs. 0.14). But the average relative increase of F\u03b1=0.5, however, is very similar in both sets (0.12 vs. 0.13).\nThe conclusion is that a certain F\u03b1=0.5 improvement range does not say anything about whether both Purity and Inverse Purity are being simultaneously improved or not. In other words: no matter how large is a measured improvement in F is, it can still be extremely dependent on how we are weighting the individual metrics in that measurement.\nThis conclusion can be corroborated by considering independently both metrics (Purity and Inverse Purity). According to the statistical significance of the improvements for independent metrics, we can distinguish three cases:\n1. Opposite significant improvements: One of the metrics (Purity or Inverse Purity) increases and the other decreases, and both changes are statistically significant.\n2. Concordant significant improvements: Both metrics improve significantly or at least one improves significantly and the other does not decrease significantly.\n3. Non-significant improvements: There is no statistically significant differences between both systems for any metric.\nWe use the Wilcoxon test with p < 0.05 to detect statistical significance. Table 6 shows the average UIR and | 4 F\u03b1=0.5| values in each of the three cases. Remarkably, the F\u03b1=0.5 average increase is even larger for the opposite improvements set (0.15) than for the concordant improvements set (0.11). According to these results, it would seem that F\u03b1=0.5 rewards individual metric improvements which are obtained at the cost of (smaller) decreases in the other metric. UIR, on the other hand, has a sharply different behavior, strongly rewarding the concordant improvements set (0.42 versus 0.08).\nAll these results confirm that UIR provides essential information about the experimental outcome of two-system comparisons, which is not provided by the main evaluation metric F\u03b1."}, {"heading": "6.2 Correlation Between F and UIR", "text": "The fact that UIR and F offer different information about the outcome of an experiment does not imply that UIR and F are orthogonal; in fact, there is some correlation between both values.\nFigure 5 represents F\u03b1=0.5 differences and UIR values for each possible system pair in the WePS-1 test bed. The general trends are (i) high UIR values imply a positive difference in F (ii) high |4F0,.5| values do not imply anything on UIR values; (iii) low UIR do not seem to imply anything on |4F0,.5| values. Overall, the figure suggest a triangle relationship, which gives a Pearson correlation of 0.58."}, {"heading": "6.2.1 Reflecting improvement ranges", "text": "When there is a consistent difference between two systems for most \u03b1 values, UIR rewards larger improvement ranges. Let us illustrate this behavior considering three sample system pairs taken from the WePS-1 test bed.\nFigure 6 represents the F\u03b1\u2208[0,1] values for three system pairs. In all cases, one system improves the other for all \u03b1 values. However, UIR assigns higher values to larger improvements in F (larger distance between the black and the grey curves). The reason is that a\nlarger average improvement over test cases makes less likely the cases where individual test cases (which are the ones that UIR considers) contradict the average result.\nAnother interesting finding is that, when both metrics are improved, the metric that has the weakest improvement determines the behavior of UIR. Figure 7 illustrates this relationship for the ten system pairs with a largest improvement; the Pearson correlation in this graph is 0.94. In other words, when both individual metrics improve, UIR is sensitive to the weakest improvement."}, {"heading": "6.2.2 Analysis of boundary cases", "text": "In order to have a better understanding of the relationship between UIR and F, we will now examine in detail two cases of system improvements in which UIR and F produce drastically different results. These two cases are marked as A and B in Figure 5.\nThe point marked as case A in the Figure corresponds with the comparison of systems S1 and S15. There exists a substantial (and statistically significant) difference between both systems according to F\u03b1=0.5. However, UIR has a low value, i.e., the improvement is not robust to changes in \u03b1 according to UIR.\nA visual explanation of these results can be seen in Figure 8. It shows the Purity and Inverse Purity results of systems S1, S15 for every test case. In most test cases, S1 has an important advantage in Purity at the cost of a slight \u2013 but consistent \u2013 loss in Inverse Purity. Given that F\u03b1=0.5 compares Purity and Inverse Purity ranges, it states that there exists an important and statistically significant improvement from S15 to S1. However, the slight but consistent decrease in Inverse Purity affects UIR, which decreases because in most test cases the improvements in F are metric biased (\u2016\u2200 in our notation).\nCase B (see Figure 9) is the opposite example: there is a small difference between systems S8 and S12 according to F\u03b1=0.5, because differences in both Purity and Inverse Purity are also small. S8, however, gives small but consistent improvements both for Purity and Inverse Purity (all test cases to the right of the vertical line in the figure); these are unanimous improvements. Therefore, UIR considers that there exists a robust overall improvement in this case.\nAgain, both cases show how UIR gives additional valuable information on the comparative behavior of systems."}, {"heading": "6.3 A Significance Threshold for UIR", "text": "We mentioned earlier that UIR has a parallelism with statistical significance tests, which are typically used in Information Retrieval to estimate the probability p that an observed difference between two systems is obtained by chance, i.e., the difference is an artifact of the test collection rather than a true difference between the systems. When computing statistical significance, it is useful to establish a threshold that allows for a binary decision; for instance, a result is often said to be statistically significant if p < 0.05, and not significant otherwise. Choosing the level of significance is arbitrary, but it nevertheless helps reporting and summarizing significance tests. Stricter thresholds increase confidence of the test, but run an increased risk of failing to detect a significant result.\nThe same situation applies to UIR: we would like to establish an UIR threshold that decides whether an observed difference is reasonably robust to changes in \u03b1. How can we set such a threshold? We could be very restrictive and decide, for instance, that an improvement is significantly robust when UIR \u2265 0.75. This condition, however, is so hard that it would never be satisfied in practice, and therefore the UIR test would not be informative. On the other hand, if we set a very permissive threshold it will be satisfied by most system pairs and, again, it will not be informative. The question now is whether there exists a threshold for UIR values such that obtaining a UIR above the threshold guarantees that an improvement is robust, and, at the same time, is not too strong to be satisfied in practice.\nGiven the set of two-system combinations for which UIR surpasses a certain candidate threshold, we can think of some desirable features:\n1. It must be able to differentiate between two types of improvements (robust vs. nonrobust); in other words, if one of the two types is usually empty or almost empty, the threshold is not informative.\n2. The robust set should contain a high ratio of two-system combinations such that the average F\u03b1 increases for all \u03b1 values (F\u03b1(a) > F\u03b1(b)\u2200\u03b1).\n3. The robust set should contain a high ratio of significant concordant improvements and a low ratio of significant opposite improvements (see Section 6.1).\n4. The robust set should contain a low ratio of cases where F contradicts UIR (the dots in Figure 5 in the region |4F0,.5| < 0).\nFigure 10 shows how these conditions are met for every threshold in the range [0, 0.8]. A UIR threshold of 0.25 accepts around 30% of all system pairs, with a low (4%) ratio of significant opposite improvements and a high (80%) ratio of significant concordant improvements. At this threshold, in half of the robust cases F\u03b1 increases for all \u03b1 values, and in most cases (94%) F\u03b1=0.5 increases. It seems, therefore, that UIR \u2265 0.25 can be a reasonable threshold, at least for this clustering task. Note, however, that this is a rough rule of thumb that should be revised/adjusted when dealing with clustering tasks other than WePS."}, {"heading": "6.4 UIR and System Rankings", "text": "All results presented so far are focused on pairwise system comparisons, according to the nature of UIR. We now turn to the question of how can we use UIR as a component in the analysis of the results of an evaluation campaign.\nIn order to answer this question we have applied UIR to the results of the WePS-2 evaluation campaign (Artiles et al., 2009). In this campaign, the best runs for each system were ranked according to Bcubed precision and recall metrics, combined with F\u03b1=0.5. In addition to all participant systems, three baseline approaches were included in the ranking:\nall documents in one cluster (B100), each document in one cluster (B1) and the union of both (BCOMB) 5.\nTable 7 shows the results of applying UIR to the WePS-2 participant systems. \u03b1-robust improvements are represented in the third column (\u201cimproved systems\u201d): for every system, it displays the set of systems that it improves with UIR \u2265 0.25. The fourth column is the reference system, which is defined as follows: given a system a, its reference system is the one that improves a with maximal UIR:\nSref (a) = ArgmaxS(UIR(S, a))\nIn other words, Sref (a) represents the system with which a should be replaced in order to robustly improve results across different \u03b1 values. Finally, the last column (\u201cUIR for the reference system\u201d) displays the UIR between the system and its reference (UIR(Sref , Si)).\nNote that UIR adds new insights into the evaluation process. Let us highlight two interesting facts:\n\u2022 Although the three top-scoring systems (S1, S2, S3) have a similar performance in terms of F (0.82, 0.81 and 0.81), S1 is consistently the best system according to UIR, because it is the reference for 10 other systems (S2, S4, S6, S8, S12, S13, S14, S15, S16 and the baseline B1). In contrast, S2 is reference for S7 only, and S3 is reference for S11 only. Therefore, F and UIR together strongly point towards S1 as the best system, while F alone was only able to discern a set of three top-scoring systems.\n\u2022 Although the non-informative baseline B100 (all documents in one cluster) is better than five systems according to F, this improvement is not robust according to UIR. Note that UIR will signal near-baseline behaviors in participant systems with a low value, while they can receive a large F depending on the nature of the test collection: when the average cluster is large or small, systems that tend to cluster everything or nothing can be artificially rewarded. This is, in our opinion, a substantial improvement over using F alone."}, {"heading": "7. UIR as Predictor of the Stability of Results across Test Collections", "text": "A common issue when evaluating systems that deal with Natural Language is that results on different test collections are often contradictory. In the particular case of Text Clustering, a factor that contributes to this problem is that the average size of clusters can vary across different test beds, and this variability modifies the optimal balance between precision and recall. A system which tends to favor precision, creating small clusters, may have good results in a dataset with a small average cluster size and worse results in a test collection with a larger average cluster size.\nTherefore, if we only apply F to combine single metrics, we can reach contradictory results over different test beds. As UIR does not depend on metric weighting criteria, our hypothesis is that a high UIR value ensures robustness of evaluation results across test beds.\n5. See the work of Artiles et al. (2009) for an extended explanation.\nIn other words: given a particular test bed, a high UIR value should be a good predictor that an observed difference between two systems will still hold in other test beds.\nThe following experiment is designed to verify our hypothesis. We have implemented four different systems for the WePS problem, all based on an agglomerative clustering algorithm (HAC) which was used by the best systems in WePS-2. Each system employs a certain cluster linkage technique (complete link or single link) and a certain feature extraction criterion (word bigrams or unigrams). For each system we have experimented with 20 stopping criteria. Therefore, we have used 20x4 system variants overall. We have evaluated these systems over WePS-1a, WePS-1b and WePS-2 corpora6.\nThe first observation is that, given all system pairs, F\u03b1=0.5 only gives consistent results for all three test beds in 18% of the cases. For all other system pairs, the best system is different depending of the test collection. A robust evaluation criterion should predict, given a single test collection, whether results will still hold in other collections.\nWe now consider two alternative ways of predicting that an observed difference (system A is better than system B) in one test-bed will still hold in all three test beds:\n\u2022 The first is using F (A)\u2212 F (B): the larger this value is on the reference test bed, the more likely that F (A)\u2212 F (B) will still be positive in a different test collection.\n6. WEPS-1a was originally used for training in the first WePS campaign, and WePS-1b was used for testing.\n\u2022 The second is using UIR(A,B) instead of F: the larger UIR is, the more likely that F (A)\u2212 F (B) is also positive in a different test bed.\nIn summary, we want to compare F and UIR as predictors of how robust is a result to a change of test collection. This is how we tested it:\n1. We select a reference corpus out of WePS-1a, WePS-1b and WePS-2 test beds.\nCref \u2208 {WePS-1a,WePS-1b,WePS-2}\n2. For each system pair in the reference corpus, we compute the improvement of one system with respect to the other according to F and UIR. We take those system pairs such that one improves the other over a certain threshold t. Being UIRC(s1, s2) the UIR results for systems s1 and s2 in the test-bed C, and being FC(s) the results of F for the system s in the test-bed C:\nSUIR,t(C) = {(s1, s2)|UIRC(s1, s2) > t}\nSF,t(C) = {s1, s2|(FC(s1)\u2212 FC(s2)) > t)}\nFor every threshold t, SUIR,t and SF,t represent the set of robust improvements as predicted by UIR and F, respectively.\n3. Then, we consider the system pairs such that one improves the other according to F for all the three test collections simultaneously.\nT = {s1, s2|FC(s1) > FC(s2)\u2200C}\nT is the gold standard to be compared with predictions SUIR,t and SF,t.\n4. For every threshold t, we can compute precision and recall of UIR and F predictions (SUIR,t(C) and SF,t(C)) versus the actual set of robust results across all collections (T ).\nPrecision(SUIR,t(C)) = |SUIR,t(C) \u2229 T | |SUIR,t|\nRecall(SUIR,t(C)) = |SUIR,t(C) \u2229 T |\n|T |\nPrecision(SF,t(C)) = |SF,t(C) \u2229 T | |SF,t(C)|\nRecall(SF,t(C)) = |SF,t(C) \u2229 T |\n|T |\nWe can now trace the precision/recall curve for each of the predictors F, UIR and compare their results. Figures 11, 12 and 13, show precision/recall values for F (triangles) and UIR (rhombi); each figure displays results for one of the reference test-beds: WEPS1a,WEPS-1b and WePS-27.\nAltogether, the figures show how UIR is much more effective than F as a predictor. Note that F suffers a sudden drop in performance for low recall levels, which suggests that\n7. The curve \u201cparametric UIR\u201d refers to an alternative definition of UIR which is explained in Section 8\nbig improvements in F tend to be due to the peculiarities of the test collection rather than to a real superiority of one system versus the other.\nThis is, in our opinion, a remarkable result: differences in UIR are better indicators of the reliability of a measured difference in F than the amount of the measured difference. Therefore, UIR is not only useful to know how stable are results to changes in \u03b1, but also to changes in the test collection, i.e., it is an indicator of how reliable a perceived difference is.\nNote that we have not explicitly tested the dependency (and reliability) of UIR results with the number of test cases in the reference collection. However, as working with a collection of less than 30 test cases is unlikely, in practical terms the usability of UIR is granted for most test collections, at least with respect of the number of test cases."}, {"heading": "8. Parametric versus Non-Parametric UIR", "text": "According to our analysis (see Section 5.2), given two measures P and R, the only relational structure over pairs \u3008Pi, Ri\u3009 that does not depend on weighting criteria is the unanimous improvement:\na \u2265\u2200 b \u2261 Pa \u2265 Pb \u2227Ra \u2265 Rb\nWhen comparing systems, our UIR measure counts the unanimous improvement results across test cases:\nUIRX,T (a, b) = |Ta\u2265\u2200b| \u2212 |Tb\u2265\u2200a|\n|T |\nAlternatively, this formulation can be expressed in terms of probabilities:\nUIRX,T (a, b) = Prob(a \u2265\u2200 b)\u2212 Prob(b \u2265\u2200 a)\nwhere these probabilities are estimated in a frequentist manner. As we said, the main drawback of the unanimous improvement is that it is a threevalued function which does not consider metric ranges; UIR inherits this drawback. As a consequence, UIR is less sensitive than other combining schemes such as the F measure. In order to solve this drawback, we could estimate UIR parametrically. However, the results in this section seem to indicate that this is not the best option.\nOne way of estimating Prob(a \u2265\u2200 b) and Prob(b \u2265\u2200 a) consists of assuming that the metric differences (\u2206P,\u2206R) between two systems across test cases follow a normal bivariate distribution. We can then estimate this distribution from the case samples provided in each test bed. After estimating the density function Prob(\u2206P,\u2206R), we can estimate Prob(a \u2265\u2200 b) as8:\nProb(a \u2265\u2200 b) = Prob(\u2206P \u2265 0 \u2227\u2206R \u2265 0) = \u222b \u2206P=1,\u2206R=1\n\u2206P=0,\u2206R=0 Prob(\u2206P,\u2206R) d\u2206P d\u2206R\nThis expression can be used to compute UIRX,T (a, b) = Prob(a \u2265\u2200 b) \u2212 Prob(b \u2265\u2200 a), and leads to a parametric version of UIR.\nIn order to compare the effectiveness of the parametric UIR versus the original UIR, we repeated the experiment described in Section 7, adding UIRparam to the precision/recall curves in Figures 11, 12 and 13. The squares in that figures represent the results for the parametric version of UIR. Note that its behavior lies somewhere between F and the nonparametric UIR: for low levels of recall, it behaves like the original UIR; for intermediate levels, it is in general worse than the original definition but better than F; and in the recall high-end, it overlaps with the results of F. This is probably due to the fact that the parametric UIR estimation considers ranges, and becomes sensitive to the unreliability of high improvements in F."}, {"heading": "9. Conclusions", "text": "Our work has addressed the practical problem of the strong dependency (and usually some degree of arbitrariness) on the relative weights assigned to metrics when applying metric combination criteria, such as F .\nBased on the theory of measurement, we have established some relevant theoretical results: the most fundamental is that there is only one monotonic relational structure that does not contradict any Additive Conjoint Structure, and that this unique relationship is not transitive. This implies that it is not possible to establish a ranking (a complete ordering) of systems without assuming some arbitrary relative metric weighting. A transitive relationship, however, is not necessary to ensure the robustness of specific pairwise system comparisons.\nBased on this theoretical analysis, we have introduced the Unanimous Improvement Ratio (UIR), which estimates the robustness of measured system improvements across potential metric combining schemes. UIR is a measure complementary to any metric combination\n8. For this computation we have employed the Matlab tool\nscheme and it works similarly to a statistical relevance test, indicating if a perceived difference between two systems is reliable or biased by the particular weighting scheme used to evaluate the overall performance of systems.\nOur empirical results on the text clustering task, which is particularly sensitive to this problem, confirm that UIR is indeed useful as an analysis tool for pairwise system comparisons: (i) For similar increments in F, UIR captures which ones are less dependent of the relative weighting scheme between precision and recall; (ii) unlike F, UIR rewards system improvements that are corroborated by statistical significance tests over each single measure; (iii) in practice, a high UIR tends to imply a large F increase, while a large increase in F does not imply a high UIR; in other words, a large increase in F can be completely biased by the weighting scheme, and therefore UIR is an essential information to add to F.\nWhen looking at results of an evaluation campaign, UIR has proved useful to (i) discern which is the best system among a set of systems with similar performance according to F ; (ii) penalize trivial baseline strategies and systems with a baseline-like behavior.\nPerhaps the most relevant result is a side effect on how our proposed measure is defined: UIR is a good estimator of how robust a result is to changes in the test collection. In other words, given a measured increase in F in a test collection, a high UIR value makes more likely that an increase will also be observed in other test collections. Remarkably, UIR estimates cross-collection robustness of F increases much better than the absolute value of the F increase.\nA limitation of our present study is that we have only tested UIR on the text clustering problem. While its usefulness for clustering problems already makes UIR a useful analysis tool, its potential goes well beyond this particular problem. Most Natural Language problems \u2013 and, in general, many problems in Artificial Intelligence \u2013 are evaluated in terms of many individual measures which are not trivial to combine. UIR should be a powerful tool in many of those scenarios.\nAn UIR evaluation package is available for download at http://nlp.uned.es."}, {"heading": "Acknowledgments", "text": "This research has been partially supported by the Spanish Government (grant Holopedia, TIN2010-21128-C02) and the Regional Government of Madrid under the Research Network MA2VICMR (S2009/TIC-1542)."}], "references": [{"title": "A comparison of extrinsic clustering evaluation metrics based on formal constraints", "author": ["E. Amig\u00f3", "J. Gonzalo", "J. Artiles", "F. Verdejo"], "venue": "Information Retrieval,", "citeRegEx": "Amig\u00f3 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Amig\u00f3 et al\\.", "year": 2008}, {"title": "WePS-2 Evaluation Campaign: Overview of the Web People Search Clustering Task", "author": ["J. Artiles", "J. Gonzalo", "S. Sekine"], "venue": "In Proceedings Of The 2nd Web People Search Evaluation Workshop (WePS", "citeRegEx": "Artiles et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2009}, {"title": "The SemEval-2007 WePS evaluation: Establishing a Benchmark for the Web People Search Task", "author": ["J. Artiles", "J. Gonzalo", "S. Sekine"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Artiles et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2007}, {"title": "Entity-Based Cross-Document Coreferencing Using the Vector Space Model", "author": ["A. Bagga", "B. Baldwin"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics", "citeRegEx": "Bagga and Baldwin,? \\Q1998\\E", "shortCiteRegEx": "Bagga and Baldwin", "year": 1998}, {"title": "Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling", "author": ["X. Carreras", "L. M\u00e0rquez"], "venue": "HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning", "citeRegEx": "Carreras and M\u00e0rquez,? \\Q2004\\E", "shortCiteRegEx": "Carreras and M\u00e0rquez", "year": 2004}, {"title": "Topological methods in cardinal utility theory", "author": ["G. Debreu"], "venue": "Mathematical Methods in the Social Sciences, Stanford University Press,", "citeRegEx": "Debreu,? \\Q1959\\E", "shortCiteRegEx": "Debreu", "year": 1959}, {"title": "Scalable clustering methods for data mining", "author": ["J. Ghosh"], "venue": "Handbook of Data Mining. Lawrence Erlbaum", "citeRegEx": "Ghosh,? \\Q2003\\E", "shortCiteRegEx": "Ghosh", "year": 2003}, {"title": "On Clustering Validation Techniques", "author": ["M. Halkidi", "Y. Batistakis", "M. Vazirgiannis"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "Halkidi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Halkidi et al\\.", "year": 2001}, {"title": "Simultaneous conjoint measurement: a new scale type of fundamental measurement", "author": ["R. Luce", "J. Tukey"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "Luce and Tukey,? \\Q1964\\E", "shortCiteRegEx": "Luce and Tukey", "year": 1964}, {"title": "Multi-Document Statistical Fact Extraction and Fusion", "author": ["G.S. Mann"], "venue": "Ph.D. thesis,", "citeRegEx": "Mann,? \\Q2006\\E", "shortCiteRegEx": "Mann", "year": 2006}, {"title": "Comparing clusterings", "author": ["M. Meila"], "venue": "In Proceedings of COLT", "citeRegEx": "Meila,? \\Q2003\\E", "shortCiteRegEx": "Meila", "year": 2003}, {"title": "Measurement: The theory of numerical assignments", "author": ["L. Narens", "R.D. Luce"], "venue": "Psychological Bulletin,", "citeRegEx": "Narens and Luce,? \\Q1986\\E", "shortCiteRegEx": "Narens and Luce", "year": 1986}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "In KDD Workshop on Text Mining,2000", "citeRegEx": "Steinbach et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Steinbach et al\\.", "year": 2000}, {"title": "Empirical Studies on Multilabel Classification", "author": ["C.Z. Tao Li", "S. Zhu"], "venue": "In Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI", "citeRegEx": "Li and Zhu,? \\Q2006\\E", "shortCiteRegEx": "Li and Zhu", "year": 2006}, {"title": "Foundation of evaluation", "author": ["C.J. van Rijsbergen"], "venue": "Journal of Documentation,", "citeRegEx": "Rijsbergen,? \\Q1974\\E", "shortCiteRegEx": "Rijsbergen", "year": 1974}, {"title": "A New Evaluation Measure for Imbalanced Datasets", "author": ["C.G. Weng", "J. Poon"], "venue": "Seventh Australasian Data Mining Conference (AusDM 2008), Vol. 87 of CRPIT,", "citeRegEx": "Weng and Poon,? \\Q2008\\E", "shortCiteRegEx": "Weng and Poon", "year": 2008}, {"title": "Criterion functions for document clustering: Experiments and analysis", "author": ["Y. Zhao", "G. Karypis"], "venue": "Technical Report TR 01\u201340,", "citeRegEx": "Zhao and Karypis,? \\Q2001\\E", "shortCiteRegEx": "Zhao and Karypis", "year": 2001}], "referenceMentions": [{"referenceID": 6, "context": "A wide set of extrinsic metrics has been proposed: Entropy and Class Entropy (Steinbach, Karypis, & Kumar, 2000; Ghosh, 2003), Purity and Inverse Purity (Zhao & Karypis, 2001), precision and recall Bcubed metrics (Bagga & Baldwin, 1998), metrics based on counting pairs (Halkidi, Batistakis, & Vazirgiannis, 2001; Meila, 2003), etc.", "startOffset": 77, "endOffset": 125}, {"referenceID": 10, "context": "A wide set of extrinsic metrics has been proposed: Entropy and Class Entropy (Steinbach, Karypis, & Kumar, 2000; Ghosh, 2003), Purity and Inverse Purity (Zhao & Karypis, 2001), precision and recall Bcubed metrics (Bagga & Baldwin, 1998), metrics based on counting pairs (Halkidi, Batistakis, & Vazirgiannis, 2001; Meila, 2003), etc.", "startOffset": 270, "endOffset": 326}, {"referenceID": 0, "context": "See the work of Amig\u00f3 et al. (2008) for a detailed overview.", "startOffset": 16, "endOffset": 36}, {"referenceID": 1, "context": "For our experiments we have focused on the evaluation results obtained in the WePS-1 (Artiles, Gonzalo, & Sekine, 2007) and WePS-2 (Artiles et al., 2009) evaluation campaigns.", "startOffset": 131, "endOffset": 153}, {"referenceID": 9, "context": "The WePS-1 corpus also includes data from the Web03 test bed (Mann, 2006), which was used for trial purposes and follows similar annotation guidelines, although the number of document per ambiguous name is more variable.", "startOffset": 61, "endOffset": 73}, {"referenceID": 0, "context": "Purity focuses on the frequency of the most common category into each cluster (Amig\u00f3 et al., 2008).", "startOffset": 78, "endOffset": 98}, {"referenceID": 2, "context": "For instance, in the WePS-1b test bed (Artiles et al., 2007), B1 substantially outperforms B100 (0.", "startOffset": 38, "endOffset": 60}, {"referenceID": 1, "context": "In the WePS-2 data set (Artiles et al., 2009), however, B100 outperforms B1 (0.", "startOffset": 23, "endOffset": 45}, {"referenceID": 14, "context": "Van Rijsbergen (1974) argued that it is not possible to determine empirically which metric combining function is the most adequate in the context of Information Retrieval evaluation.", "startOffset": 4, "endOffset": 22}, {"referenceID": 5, "context": "The problem of combining evaluation metrics is closely related with the Conjoint Measurement Theory, which was independently discovered by the economist Debreu (1959) and the mathematical psychologist R.", "startOffset": 153, "endOffset": 167}, {"referenceID": 5, "context": "The problem of combining evaluation metrics is closely related with the Conjoint Measurement Theory, which was independently discovered by the economist Debreu (1959) and the mathematical psychologist R. Duncan Luce and statistician John Tukey (Luce & Tukey, 1964). The Theory of Measurement defines the necessary conditions to state an homomorphism between an empirical relational structure (e.g. \u201cJohn is bigger than Bill\u201d) and a numeric relational structure (\u201cJohn\u2019s height is 1.79 meters and Bill\u2019s height is 1.56 meters\u201d). In the case of the Conjoint Measurement Theory, the relational structure is factored into two (or more) ordered substructures (e.g. \u201cheight and weight\u201d). In our context, the numerical structures are given by the evaluation metric scores (e.g. Purity and Inverse Purity). However, we do not have an empirical quality ordering for clustering systems. Different human assessors could assign more relevance to Purity than to Inverse Purity or viceversa. Nevertheless, the Conjoint Measurement Theory does provide mechanisms that state what kind of numerical structures can produce an homomorphism assuming that the empirical structure satisfies certain axioms. Van Rijsbergen (1974) used this idea to analyze the problem of combining evaluation metrics.", "startOffset": 153, "endOffset": 1207}, {"referenceID": 14, "context": "The F-measure proposed by van Rijsbergen (1974) and the arithmetic mean of P,R satisfy all these axioms.", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": "2 Formal Properties of the Unanimous Improvement The Unanimous Improvement \u2265\u2200x trivially satisfies most of the desirable properties proposed by van Rijsbergen (1974) for metric combining functions: transitivity, independence, Thomsen condition, Restricted Solvability, Essential Components and Decreasing Marginal Effectiveness; the exception being the connectedness property4.", "startOffset": 148, "endOffset": 166}, {"referenceID": 1, "context": "In order to answer this question we have applied UIR to the results of the WePS-2 evaluation campaign (Artiles et al., 2009).", "startOffset": 102, "endOffset": 124}, {"referenceID": 1, "context": "See the work of Artiles et al. (2009) for an extended explanation.", "startOffset": 16, "endOffset": 38}], "year": 2011, "abstractText": "Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen\u2019s F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted. Besides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed.", "creator": "TeX"}}}