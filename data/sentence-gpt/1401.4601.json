{"id": "1401.4601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2014", "title": "Counting-Based Search: Branching Heuristics for Constraint Satisfaction Problems", "abstract": "Designing a search heuristic for constraint programming that is reliable across problem domains has been an important research topic in recent years. This paper concentrates on one family of candidates: counting-based search. Such heuristics seek to make branching decisions that preserve most of the solutions by determining what proportion of solutions to each individual constraint agree with that decision. Whereas most generic search heuristics in constraint programming rely on local information at the level of the individual variable, our search heuristics are based on more global information at the constraint level. We design several algorithms that are used to count the number of solutions to specific families of constraints and propose some search heuristics exploiting such information. The experimental part of the paper considers eight problem domains ranging from well-established benchmark puzzles to rostering and sport scheduling. An initial empirical analysis identifies heuristic maxSD as a robust candidate among our proposals.eWe then evaluate the latter against the state of the art, including the latest generic search heuristics, restarts, and discrepancy-based tree traversals. Experimental results show that counting-based search generally outperforms other generic heuristics. In these studies, we suggest an improved search performance with the use of more local information. To examine the current state of the art, we design the algorithms with the right level of local information for each constraint. We analyze the average value of each constraint and their relative likelihood, using a number of possible weights (like the one for the constraint) to determine the weightedest weight. Using an optimized test-driven model of how the best solution can be tested by the most rigorous test-driven model of the algorithm, we demonstrate that the more local information the algorithms are based on, the better the algorithm is.", "histories": [["v1", "Sat, 18 Jan 2014 21:09:25 GMT  (2994kb)", "http://arxiv.org/abs/1401.4601v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gilles pesant", "claude-guy quimper", "alessandro zanarini"], "accepted": false, "id": "1401.4601"}, "pdf": {"name": "1401.4601.pdf", "metadata": {"source": "CRF", "title": "Counting-Based Search: Branching Heuristics for Constraint Satisfaction Problems", "authors": ["Gilles Pesant", "Claude-Guy Quimper", "Alessandro Zanarini"], "emails": ["gilles.pesant@polymtl.ca", "claude-guy.quimper@ift.ulaval.ca", "alessandro.zanarini@dynadec.com"], "sections": [{"heading": "1. Introduction", "text": "Constraint Programming (cp) is a powerful technique to solve combinatorial problems. It applies sophisticated inference to reduce the search space and a combination of variableand value-selection heuristics to guide the exploration of that search space. Because this inference is encapsulated in each constraint appearing in the model of a problem, users may consider it as a black box. In contrast, search in cp is programmable, which is a mixed blessing. It allows one to easily tailor search to a problem, adding expertise and domain knowledge, but it may also discourage the average user who would prefer a generic and fairly robust default search heuristic that works well most of the time. Some generic search heuristics are indeed available in cp but robustness remains an issue.\nWhereas most generic search heuristics in constraint programming rely on information at the level of the individual variable (e.g. its domain size and degree in the constraint network), we investigate search heuristics based on more global information. \u201cGlobal\u201d constraints in cp are successful because they encapsulate powerful dedicated inference algorithms but foremost because they bring out the underlying structure of combinatorial problems. That exposed structure can also be exploited during search. Search heuristics\nc\u00a92012 AI Access Foundation. All rights reserved.\nfollowing the fail-first principle (detect failure as early as possible) and centered on constraints can be guided by a count of the number of solutions left for each constraint. We might for example focus search on the constraint currently having the smallest number of solutions, recognizing that failure necessarily occurs through a constraint admitting no more solution. We can also count the number of solutions featuring a given variable-value assignment in an individual constraint, favoring assignments appearing in a high proportion of solutions with the hope that such a choice generally brings us closer to satisfying the whole csp.\nThe concept of counting-based search heuristics has already been introduced, most recently by Zanarini and Pesant (2009). The specific contributions of this paper are: additional counting algorithms, including for other families of constraints, thus broadening the applicability of these heuristics; experiments that include the effect of some common features of search heuristics such as search tree traversal order, restarts and learning; considerable empirical evidence that counting-based search outperforms other generic heuristics.\nIn the rest of the paper: Section 2 provides background and reviews related work; Sections 3 to 5 present counting algorithms for several of the most usual constraints; Section 6 introduces counting-based search heuristics which can exploit the algorithms of the previous sections; Section 7 reports on an extensive experimental study comparing our proposed heuristics to state-of-the-art generic heuristics on many problem domains; finally Section 8 concludes the paper."}, {"heading": "2. Background and Related Work", "text": "We start with the usual general representation formalism for cp.\nDefinition 1 (constraint satisfaction problem (csp)). Given a finite set of variables X = {x1, x2, . . .}, a finite domain of possible values for each of these variables, D = {D1, . . . , D|X|}, xi \u2208 Di (1 \u2264 i \u2264 |X|), and a finite set of constraints (relations) over subsets of X, C = {c1, c2, . . .}, the constraint satisfaction problem (X,D, C) asks for an assignment of a value from Di to each variable xi of X that satisfies (belongs to) each cj in C.\nAnd now recall some definitions and notation from Pesant (2005) and Zanarini and Pesant (2009).\nDefinition 2 (solution count). Given a constraint c(x1, . . . , xn) and respective finite domains Di 1\u2264i\u2264n, let #c(x1, . . . , xn) denote the number of n-tuples in the corresponding relation, called its solution count.\nDefinition 3 (solution density). Given a constraint c(x1, . . . , xn), respective finite domains Di 1\u2264i\u2264n, a variable xi in the scope of c, and a value d \u2208 Di, we will call\n\u03c3(xi, d, c) = #c(x1, . . . , xi\u22121, d, xi+1, . . . , xn)\n#c(x1, . . . , xn)\nthe solution density of pair (xi, d) in c. It measures how often a certain assignment is part of a solution to c.\nHeuristics are usually classified in two main categories: static variable ordering heuristics (SVOs) and dynamic variable ordering heuristics (DVOs). The former order the variables prior to search and do not revise the ordering during search. Common SVOs are lexicographic order, lexico, and decreasing degree (i.e. number of constraints in which a variable is involved), deg. DVOs are generally considered more effective as they exploit information gathered during search. They often follow the fail-first principle originally introduced by Haralick and Elliott (1980, p. 263) i.e. \u201cTo succeed, try first where you are most likely to fail.\u201d The same authors proposed the widely-used heuristic dom that branches on the variables with the smallest domain; the aim of such a heuristic is to minimize branch depth. A similar heuristic, proposed by Bre\u0301laz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree - ddeg 1 (that is, the one constraining the largest number of unbound variables). Bessie\u0300re and Re\u0301gin (1996) and Smith and Grant (1998) combined the domain and degree information by minimizing the ratio dom/deg or dom/ddeg."}, {"heading": "2.1 Impact-Based Heuristics", "text": "Refalo (2004) proposed Impact Based Search (IBS), a heuristic that chooses the variable whose instantiation triggers the largest search space reduction (highest impact) that is approximated as the reduction of the product of the variable domain cardinalities. More formally the impact of a variable-value pair is:\nI(xi = d) = 1\u2212 Pafter Pbefore\nwhere Pafter and Pbefore are the products of the domain cardinalities respectively after and before branching on xi = d (and propagating that decision). The impact is either computed exactly at a given node of the search (the exact computation provides better information but is more time consuming) or approximated as the average reduction observed during the search (hence automatically collected on-the-go at almost no additional cost), that is:\nI\u0304(xi = d) =\n\u2211 k\u2208K I k(xi = d)\n|K|\nwhere K is the index set of the impact observed so far for the assignment xi = d. The variable impact is defined by Refalo (2004) as\nI(xi) = \u2211 d\u2208D\u2032i 1\u2212 I\u0304(xi = d)\nwhere D\u2032i is the current domain of the variable xi. Impact initialization is fundamental to obtain a good performance even at the root of the search tree; therefore, Refalo proposed to initialize the impacts by probing each variable-value pair at the root node (note that this subsumes a reduced form of singleton consistency at the root node and can be quite computationally costly). IBS selects the variable having the largest impact (hence trying\n1. It is also referred to as future degree or forward degree in the literature.\nto maximize the propagation effects and the reduction of the search space) and then selects the value having the smallest impact (hence leaving more choices for the future variables).\nAs an interesting connection with impact-based heuristics, Szymanek and O\u2019Sullivan (2006) proposed to query the model constraints to approximate the number of filtered values by each constraint individually. This information is then exploited to design a variable and/or value selection heuristic. Nonetheless, it differs from impact-based search as they take into consideration each constraint separately, and from counting-based heuristics (Zanarini & Pesant, 2009) as the information provided is more coarse-grained than actual solution counts."}, {"heading": "2.2 Conflict-Driven Heuristics", "text": "Boussemart, Hemery, Lecoutre, and Sais (2004) proposed a conflict-driven variable ordering heuristic: they extended the concept of variable degree integrating a simple but effective learning technique that takes failures into account. Basically each constraint has an associated weight that is increased by one each time the constraint leads to a failure (i.e. a domain wipe-out). A variable has a weighted degree \u2013 wdeg \u2013 that is the sum of the weights of constraints in which it is involved. Formally, the weighted degree of a variable is:\n\u03b1wdeg(xi) = \u2211 c\u2208C weight[c] | V ars(c) 3 xi \u2227 |FutV ars(c)| > 1\nwhere FutV ars(c) denotes the uninstantiated variables of the constraint c, weight[c] is its weight and V ars(c) the variables involved in c. The heuristics proposed simply choose the variable that maximizes wdeg or minimizes dom/wdeg. These heuristics offer no general method to deal with global constraints: a natural extension is to increase the weight of every variable in a failed constraint but most of them may not have anything to do with the failure, which dilutes the conflict information. They are also particularly sensitive to revision orderings (i.e. the ordering of the propagation queue) hence leading to varying performance. Grimes and Wallace (2006, 2007) proposed some adaptations of dom/wdeg when combined with restarts or by updating weights on value deletions as well. Balafoutis and Stergiou (2008b) proposed, among other improvements over the original dom/wdeg, weight aging, that is the constraint weights are periodically reduced. This limits the inertia of constraints that got a significant weight early in the search but that are not critical anymore later on.\nNowadays heuristics dom/wdeg and IBS are considered to be the state of the art of generic heuristics with no clear dominance by one or the other (Balafoutis & Stergiou, 2008a). Finally we note that both rely on the hypothesis that what is learned early in the search will tend to remain true throughout the search tree: impacts should not change much from one search tree node to the other; the same constraints lead to domain wipe-outs in different parts of the search tree."}, {"heading": "2.3 Approximated Counting-Based Heuristics", "text": "The idea of using an approximation on the number of solutions of a problem as heuristic is not new. Kask, Dechter, and Gogate (2004) approximate the total number of solutions extending a partial solution to a csp and use it in a value selection heuristic, choosing\nthe value whose assignment to the current variable gives the largest approximate solution count. An implementation optimized for binary constraints performs well compared to other popular strategies. Hsu, Kitching, Bacchus, and McIlraith (2007) and later Bras, Zanarini, and Pesant (2009) apply a Belief Propagation algorithm within an Expectation Maximization framework (EMBP) in order to approximate variable biases (or marginals) i.e. the probability a variable takes a given value in a solution. The resulting heuristics tend to be effective but quite time-consuming. One way to differentiate our work from these is that we focus on fine-grained information from individual constraints whereas they work on coarser information over the whole problem."}, {"heading": "3. Counting for Alldifferent Constraints", "text": "The alldifferent constraint restricts a set of variables to be pairwise different (Re\u0301gin, 1994).\nDefinition 4 (Alldifferent Constraint). Given a set of variables X = {x1, . . . , xn} with respective domains D1, . . . , Dn, the set of tuples allowed by alldifferent(X) are:\n{(d1, d2, . . . , dn) | di \u2208 Di, di 6= dj\u2200i 6= j}\nWe define the associated (0-1) square matrix A = (aid) with | \u22c3 i=1,...,nDi| rows and columns such that aid = 1 iff d \u2208 Di 2. If there are more distinct values in the domains than there are variables, say p more, we add p rows filled with 1s to matrix A. An equivalent representation is given by the bipartite value graph with a vertex for each variable and value and edges corresponding to \u201c1\u201d entries in A.\nThen as discussed by Zanarini and Pesant (2009), counting the number of solutions to an alldifferent constraint is equivalent to computing the permanent of A (or the number of maximum matchings in the value graph), formally defined as\nperm(A) = n\u2211 d=1 a1,d perm(A1,d) (1)\nwhere A1,d denotes the submatrix obtained from A by removing row 1 and column d (the permanent of the empty matrix is equal to 1). If p extra rows were added, the result must be divided by p! as shown by Zanarini and Pesant (2010).\nBecause computing the permanent is well-known to be #P -complete (Valiant, 1979), Zanarini and Pesant (2009) developed an approach based on sampling which gave close approximations and led to very effective heuristics on hard instances. However it was not competitive on easy to medium difficulty instances because of the additional computational effort. The next section describes an approach based on upper bounds, trading approximation accuracy for a significant speedup in the counting procedure.3\n2. For notational convenience and without loss of generality, we identify domain values with consecutive natural numbers. 3. This was originally introduced by Zanarini and Pesant (2010)."}, {"heading": "3.1 Upper Bounds", "text": "In the following we assume for notational convenience that matrix A has n rows and columns and we denote by ri the sum of the elements in the i th row of A (i.e. ri = \u2211n d=1 aid).\nA first upper bound for the permanent was conjectured by Minc (1963) and later proved by Bre\u0301gman (1973):\nperm(A) \u2264 n\u220f i=1 (ri!) 1/ri . (2)\nRecently Liang and Bai (2004) proposed a second upper bound (with qi = min{d ri+12 e, d i 2e}):\nperm(A)2 \u2264 n\u220f i=1 qi(ri \u2212 qi + 1). (3)\nNeither of these two upper bounds strictly dominates the other. In the following we denote by UBBM (A) the Bre\u0301gman-Minc upper bound and by UBLB(A) the Liang-Bai upper bound. Jurkat and Ryser (1966) proposed another bound:\nperm(A) \u2264 n\u220f i=1 min(ri, i).\nHowever it is considered generally weaker than UBBM (A) (see Soules, 2005 for a comprehensive literature review)."}, {"heading": "3.1.1 Algorithm", "text": "We decided to adapt UBBM and UBLB in order to compute an approximation of solution densities for the alldifferent constraint. Assigning d to variable xi translates to replacing the ith row by the unit vector e(d) (i.e. setting the ith row of the matrix to 0 except for the element in column d). We write Axi=d to denote matrix A except that xi is fixed to d. We call local probe the assignment xi = d performed to compute Axi=d i.e. a temporary assignment that does not propagate to any other constraint except the one being processed.\nThe upper bound on the number of solutions of the alldifferent(x1, . . . , xn) constraint with a related adjacency matrix A is then simply\n#alldifferent(x1, . . . , xn) \u2264 min{UBBM (A), UBLB(A)}\nNote that in Formulas 2 and 3, the ri\u2019s are equal to |Di|; since |Di| ranges from 0 to n, the factors can be precomputed and stored: in a vector BMfactors[r] = (r!)1/r, r = 0, . . . , n for the first bound and similarly for the second one (with factors depending on both |Di| and i). Assuming that |Di| is returned in O(1), computing the formulas takes O(n) time. Solution densities are then approximated as\n\u03c3(xi, d, alldifferent) \u2248 min{UBBM (Axi=d), UBLB(Axi=d)}\n\u03b7 where \u03b7 is a normalizing constant so that \u2211\nd\u2208Di \u03c3(xi, d, alldifferent) = 1.\nThe local probe xi = d may trigger some local propagation according to the level of consistency we want to achieve; therefore Axi=d is subject to the filtering performed on the constraint being processed. Since the two bounds in Formulas 2 and 3 depend on |Di|, a stronger form of consistency would likely lead to more changes in the domains and on the bounds, and presumably to more accurate solution densities.\nIf we want to compute \u03c3(xi, d, alldifferent) for all i = 1, . . . , n and for all d \u2208 Di then a trivial implementation would compute Axi=d for each variable-value pair; the total time complexity would be O(mP +mn) (where m is the sum of the cardinalities of the variable domains and P the time complexity of the filtering).\nAlthough unable to improve over the worst case complexity, in the following we propose an algorithm that performs definitely better in practice. We introduce before some additional notation: we write as D\u2032k the variable domains after enforcing \u03b8-consistency 4 on that constraint alone and as I\u0303xi=d the set of indices of the variables that were subject to a domain change due to a local probe and the ensuing filtering, that is, k \u2208 I\u0303xi=d iff |D\u2032k| 6= |Dk|. We describe the algorithm for the Bre\u0301gman-Minc bound \u2014 it can be easily adapted for the Liang-Bai bound.\nThe basic idea is to compute the bound for matrix A and to reuse it to speed up the computation of the bounds for Axi=d for all i = 1, . . . , n and d \u2208 Di. Let\n\u03b3k =  BMfactors[|D\u2032k|] BMfactors[|Dk|] if k \u2208 I\u0303xi=d\n1 otherwise\nUBBM (Axi=d) = n\u220f k=1 BMfactors[|D\u2032k|] = n\u220f k=1 \u03b3k BMfactors[|Dk|]\n= UBBM (A) n\u220f k=1 \u03b3k\nNote that \u03b3k with k = i (i.e. we are computing UB BM (Axi=d)) does not depend on d; however I\u0303xi=d does depend on d because of the domain filtering. Algorithm 1 shows the pseudo code for computing UBBM (Axi=d) for all i = 1, . . . , n and d \u2208 Di. Initially, it computes the bound for matrix A (line 1); then, for a given i, it computes \u03b3i and the upper bound is modified accordingly (line 3). Afterwards, for each d \u2208 Di, \u03b8-consistency is enforced (line 7) and it iterates over the set of modified variables (line 9-10) to compute all the \u03b3k that are different from 1. We store the upper bound for variable i and value d in the structure V arV alUB[i][d]. Before computing the bound for the other variables-values the assignment xi = d needs to be undone (line 12). Finally, we normalize the upper bounds in order to correctly return solution densities (line 13-14). Let I be equal to maxi,d |I\u0303xi=d|, the time complexity is O(mP +mI).\nIf matrix A is dense we expect I ' n. Therefore most of the \u03b3k are different from 1 and need to be computed. As soon as the matrix becomes sparse enough then I n and only a small fraction of \u03b3k need to be computed, and that is where Algorithm 1 has an advantage.\n4. Stands for any form of consistency\nUB = UBBM (A);1 for i = 1, . . . , n do2 varUB = UB * BMfactors[1] / BMfactors[|Di|];3 total = 0;4 forall d \u2208 Di do5 set xi = d;6 enforce \u03b8-consistency;7 VarValUB[i][d] = varUB;8\nforall k \u2208 I\u0303xi=d \\ {i} do9 VarValUB[i][d] = VarValUB[i][d] * BMfactors[|D\u2032k|] / BMfactors[|Dk|];10 total = total + VarValUB[i][d];11 rollback xi = d;12 forall d \u2208 Di do13 SD[i][d] = VarValUB[i][d]/total;14\nreturn SD;15\nAlgorithm 1: Solution Densities\nThe sampling algorithm introduced by Zanarini and Pesant (2009) performed very well both in approximating the solution count and the solution densities, but this is not the case for upper bounds. The latter in fact produce weak approximations of the solution count but offer a very good trade-off between performance and accuracy for solution densities: taking the ratio of two solution counts appears to cancel out the weakness of the original approximations (see Zanarini & Pesant, 2010 for further details)."}, {"heading": "3.2 Symmetric Alldifferent", "text": "Re\u0301gin (1999) proposed the symmetric alldifferent constraint that is a special case of the alldifferent in which variables and values are defined from the same set. This is equivalent to a traditional alldifferent with an additional set of constraints stating that variable i is assigned to a value j iff variable j is assigned to value i. This constraint is useful in many real world problems in which a set of entities need to be paired up; particularly, in sport scheduling problems teams need to form a set of pairs that define the games.\nA symmetric alldifferent achieving domain consistency provides more pruning power than the equivalent decomposition given by the alldifferent constraint and the set of xi = j \u21d0\u21d2 xj = i constraints (Re\u0301gin, 1999). Its filtering algorithm is inspired from the one for alldifferent with the difference being that the matching is computed in a graph (not necessarily bipartite) called contracted value graph where vertices and values representing the same entity are collapsed into a single vertex (i.e. the vertex xi and the vertex i are merged into a single vertex i representing both the variable and the value). Re\u0301gin proved that there is a bijection between a matching in the contracted value graph and a solution of the symmetric alldifferent constraint. Therefore, counting the number of matchings on the contracted value graph corresponds to counting the number of solutions to the constraint.\nFriedland (2008) and Alon and Friedland (2008) extended the Bre\u0301gman-Minc upper bound to consider the number of matchings in general undirected graphs. Therefore, we can exploit the bound as in the previous section in order to provide an upper bound of the solution count and the solution densities for the symmetric alldifferent constraint. The upper bound for the number of matchings of a graph G = (V,E) representing the contracted value graph is the following:\n#matchings(G) \u2264 \u220f v\u2208V (deg(v))! 1 2deg(v) (4)\nwhere deg(v) is the degree of the vertex v and #matchings(G) denotes the number of matchings on the graph G. Note that in case of a bipartite graph, this bound is equivalent to the Bre\u0301gman-Minc upper bound.\nThe algorithm for counting the number of solutions and computing the solution densities can be easily derived from what we proposed for the alldifferent.\nExample 1. Consider a symmetric alldifferent defined on six variables x1, . . . , x6 each one having a domain equal to {1, . . . , 6}. In Figure 1 the associated contracted value graph is depicted (together with a possible solution to the constraint). In this case, the number of solutions of the symmetric alldifferent can be computed as 5 \u2217 3 = 15. In the contracted value graph each vertex is connected to each other vertex, forming a clique of size 6, therefore all the vertices have a degree equal to 5. The upper bound proposed by Friedland is equal to:\n#matchings(G) \u2264 \u220f v\u2208V (deg(v))! 1 2deg(v) = (5!1/10)6 \u2248 17.68\nIn the alldifferent formulation, the related value graph has variable vertices connected to each of the values (from 1 to 6) thus the ri\u2019s are equal to 6. If we consider to rule out all the edges causing degenerated assignments (xi = i) then we end up with a value graph in which all the ri\u2019s are equal to 5. The Bre\u0301gman-Minc upper bound would give:\nperm(A) \u2264 n\u220f i=1 (ri!) 1/ri = (5!(1/5))6 \u2248 312.62.\nThe result is obviously very far from the upper bound given by Formula 4 as well as from the exact value."}, {"heading": "4. Counting for Global Cardinality Constraints", "text": "We present in this section how to extend the results obtained in Section 3 to the Global Cardinality Constraint (gcc), which is a generalization of the alldifferent constraint.\nDefinition 5 (Global Cardinality Constraint). The set of solutions of constraint gcc(X, l, u) where X is a set of k variables, l and u respectively the lower and upper bounds for each value, is defined as:\nT (gcc(X, l, u)) = {(d1, . . . , dk) | di \u2208 Di, ld \u2264 |{di|di = d}| \u2264 ud \u2200d \u2208 DX = \u22c3 xj\u2208X Dj}\nWe will consider a gcc in which all the fixed variables are removed and the lower and upper bounds are adjusted accordingly (the semantics of the constraint is unchanged). We refer to the new set of variables as X \u2032 = {x \u2208 X | x is not bound}; lower bounds are l\u2032 where l\u2032d = ld\u2212 |{x \u2208 X | x = d}| and upper bounds u\u2032 are defined similarly; we assume the constraint maintains \u03b8-consistency so l\u2032d \u2265 0 and u\u2032d \u2265 0 for each d \u2208 DX .\nInspired by Quimper, Lopez-Ortiz, van Beek, and Golynski (2004) and Zanarini, Milano, and Pesant (2006), we define Gl the lower bound graph.\nDefinition 6. Let Gl(X \u2032 \u222aDl, El) be an undirected bipartite graph where X \u2032 is the set of unbounded variables and Dl the extended value set, that is for each d \u2208 DX the graph has l\u2032d vertices d1, d2, . . . representing d (l\u2032d possibly equal to zero). There is an edge (xi, d\nj) \u2208 El if and only if d \u2208 Di.\nNote that a maximum matching on Gl corresponds to a partial assignment of the variables in X that satisfies the gcc lower bound restriction on the number of occurrences of each value. This partial assignment may or may not be completed to a full assignment that satisfies both upper bound and lower bound restrictions (here we do not take into consideration augmenting paths as Zanarini et al., 2006 but instead we fix the variables to the values represented by the matching in Gl).\nExample 2. Suppose we have a gcc defined on X = {x1, . . . , x6} with domains D1 = D4 = {1, 2, 3}, D2 = {2}, D3 = D5 = {1, 2} and D6 = {1, 3}; lower and upper bounds for the values are respectively l1 = 1, l2 = 3, l3 = 0 and u1 = 2, u2 = 3, u3 = 2. Considering that x2 = 2, the lower and upper bounds for the value 2 are respectively l \u2032 2 = 2 and u \u2032 2 = 2. The lower bound graph is shown in Figure 2a: variable x2 is bounded and thus does not appear in the graph, value vertex 2 is represented by two vertices because it has l\u20322 = 2 (although l2 = 3); finally value vertex 3 does not appear because it has a lower bound equal to zero. The matching shown in the figure (bold edges) is maximum. However if we fix the assignments represented by it (x1 = 2, x4 = 2, x6 = 1) it is not possible to have a consistent solution since both x3 and x5 have to be assigned either to 1 or 2 hence exceeding the upper bound restriction. To compute the permanent two additional fake value vertices would be added to the graph and connected to all the variable vertices (not shown in the figure).\nEvery partial assignment that satisfies just the lower bound restriction might correspond to several maximum matchings in Gl due to the duplicated vertices. For each partial assignment satisfying the lower bound restriction there are exactly \u220f d\u2208DX l \u2032 d! maximum matchings corresponding to that particular partial assignment. If we take into consideration Example 2 shown in Figure 2a, variables x1 and x4 may be matched respectively to any permutation of the vertices 2 and 2\u2032, however no matter which is the permutation, this set of matchings represents always the assignment of both x2 and x4 to the value 2.\nLet Ml 5 be the set of maximum matchings in Gl. We define f : Ml \u2192 N, a function that counts the number of possible ways a maximum matching can be extended to a full gcc solution. As shown in Example 2, f can be possibly equal to zero. Note that the number of the remaining variables that need to be assigned starting from a matching m \u2208Ml is equal to K = |X \u2032| \u2212 \u2211 d\u2208DX l \u2032 d.\nThe total number of solutions satisfying the gcc is:\n#gcc(X, l, u) = \u2211 m\u2208Ml f(m)\u220f d\u2208DX l \u2032 d! \u2264 |Ml|maxm\u2208Ml(f(m))\u220f d\u2208DX l \u2032 d! \u2264 UB(Gl) maxm\u2208Ml(f(m))\u220f d\u2208DX l \u2032 d! (5)\nwhere UB(Gl) represents an upper bound on the permanent of the 0\u22121 matrix corresponding to graph Gl.\nNote that computing f(m) is as hard as computing the permanent. In fact if l and u are respectively equal to 0 and 1 for each value, the result is an alldifferent constraint and equation 5 simplifies to #gcc(X, l, u) = f(m) where m = {\u2205} and f(m) corresponds to the permanent.\nAs computing f(m) is a #P-complete problem on its own, we focus here on upper bounding f(m). In order to do that, we introduce the upper bound residual graph. Intuitively, it is similar to the lower bound graph but it considers the upper bound restriction.\nDefinition 7. Let Gu(X \u2032 \u222aDu, Eu) be an undirected bipartite graph where X \u2032 is the set of unbounded variables and Du the extended value set, that is for each d \u2208 DX the graph has 5. if \u2200d \u2208 DX , l\u2032d = 0 then Ml = {\u2205} and |Ml| = 1\nu\u2032d \u2212 l\u2032d vertices d1, d2, . . . representing d (if u\u2032d \u2212 l\u2032d is equal to zero then there is no vertex representing d). There is an edge (xi, d j) \u2208 Eu if and only if d \u2208 Di and u\u2032d \u2212 l\u2032d > 0.\nSimilarly to the lower bound matching, a matching on Gu that covers K variables may or may not be completed to a full assignment satisfying the complete gcc. Figure 2b shows the residual upper bound graph for Example 1: value 2 disappears from the graph since it has u\u20322 = l \u2032 2 i.e. starting from a matching in the lower bound graph, the constraints on value 2 are already satisfied.\nIn order to compute maxm\u2208Ml(f(m)), we should build (|X| K ) graphs each with a combination of K variables, and then choose the one that maximizes the permanent. More practically, given the nature of the UBMB and UBLB, it suffices to choose K variables which contribute with the highest factor in the computation of the upper bounds; this can be easily done in O(n logK) by iterating over the n variables and maintaining a heap with K entries with the highest factor. We write G\u0302u and G\u0303u for the graphs in which only the K variables that maximize respectively UBMB and UBLB are present; note that G\u0302u might be different from G\u0303u.\nWe recall here that although only K variables are chosen, the graphs G\u0302u and G\u0303u are completed with fake vertices in such a way to have an equal number of vertices on the two vertex partitions. As in the lower bound graph, the given upper bound has to be scaled down by a factor of \u220f d\u2208DX (u \u2032 d \u2212 l\u2032d)!. From Equation 5, the number of gcc solutions is bounded from above by:\n#gcc(X, l, u) \u2264 UB(Gl) min(UB MB(G\u0302u), UB LB(G\u0303u))\u220f d\u2208DX (l \u2032 d!(u \u2032 d \u2212 l\u2032d)!)\n(6)\nScaling and also fake vertices used with the permanent bounds are factors that degrade the quality of the upper bound. Nonetheless, solution densities are computed as a ratio between two upper bounds therefore these scaling factors are often attenuated.\nExample 3. We refer to the gcc described in Example 2. The exact number of solutions is 19. The UBMB and UBLB for the lower bound graph in Figure 2a are both 35 (the scaling for the two fake value vertices is already considered). In the upper bound only 2 variables need to be assigned and the one maximizing the bounds are x1 and x4 (or possibly x6): the resulting permanent upper bound is 6. An upper bound on the total number of gcc solutions is then b35\u221764 c = 52 where the division by 4 is due to l \u2032 2! = 2! and u \u2032 3! = 2!.\nFigure 3 shows the lower bound and residual upper bound graph for the same constraint where x1 = 1 and domain consistency is achieved. Vertex x1 has been removed and l \u2032 1 = 0 and u\u20321 = 1. The graph Gl has a permanent upper bound of 6. The number of unassigned variables in Gu is 2 and the ones maximizing the upper bounds are x4 and x6, giving an upper bound of 6. The total number of gcc solutions with x1 = 1 is then bounded above by b6\u221764 c = 9; the approximate solution density before normalizing it is thus 9/52. Note that after normalization, it turns out to be about 0.18 whereas the exact computation of it is 5/19 \u223c 0.26."}, {"heading": "5. Counting for Regular and Knapsack Constraints", "text": "The regular constraint is useful to express patterns that must be exhibited by sequences of variables.\nDefinition 8 (Regular Language Membership Constraint). The regular(X,\u03a0) constraint holds if the values taken by the sequence of finite domain variables X = \u3008x1, x2, . . . , xk\u3009 spell out a word belonging to the regular language defined by the deterministic finite automaton \u03a0 = (Q,\u03a3, \u03b4, q0, F ) where Q is a finite set of states, \u03a3 is an alphabet, \u03b4 : Q \u00d7 \u03a3 \u2192 Q is a partial transition function, q0 \u2208 Q is the initial state, and F \u2286 Q is the set of final (or accepting) states.\nLinear equalities and inequalities are expressed as knapsack constraints.\nDefinition 9 (Knapsack Constraint). The knapsack(x, c, `, u) constraint holds if\n` \u2264 cx \u2264 u\nwhere c = (c1, c2, . . . , ck) is an integer row vector, x is a column vector of finite domain variables (x1, x2, . . . , xk) T with xi \u2208 Di, and ` and u are integers.\nWe assume that l and u are finite as they can always be set to the smallest and largest value that cx can take. Strictly speaking to be interpreted as a knapsack, the integer values involved (including those in the finite domains) should be nonnegative but the algorithms proposed in this section can be easily adapted to lift the restriction of nonnegative coefficients and domain values, at the expense of a larger graph in the case of the algorithm of Section 5.1. So we are dealing here with general linear constraints.\nThe filtering algorithms for the regular constraint and the knapsack constraint (when domain consistency is enforced) are both based on the computation of paths in a layered acyclic directed graph (Pesant, 2004; Trick, 2003). This graph has the property that paths from the first layer to the last are in one-to-one correspondence with solutions of the constraint. An exact counting algorithm for the former constraint is derived by Zanarini and\nPesant (2009) \u2014 in the next section we describe an exact counting algorithm for knapsack constraints which is similar in spirit, while in Section 5.2 we present an approximate counting algorithm attuned to bounds consistency. 6"}, {"heading": "5.1 Domain Consistent Knapsacks", "text": "We start from the reduced graph described by Trick (2003), which is a layered directed graph G(V,A) with special vertex v0,0 and a vertex vi,b \u2208 V for 1 \u2264 i \u2264 k and 0 \u2264 b \u2264 u whenever\n\u2200 j \u2208 [1, i], \u2203 dj \u2208 Dj such that i\u2211\nj=1\ncjdj = b\nand\n\u2200 j \u2208 (i, n], \u2203 dj \u2208 Dj such that `\u2212 b \u2264 k\u2211\nj=i+1\ncjdj \u2264 u\u2212 b,\nand an arc (vi,b, vi+1,b\u2032) \u2208 A whenever\n\u2203 d \u2208 Di+1 such that ci+1d = b\u2032 \u2212 b.\nWe define the following two recursions to represent the number of incoming and outgoing paths at each node.\nFor every vertex vi,b \u2208 V , let #ip(i, b) denote the number of paths from vertex v0,0 to vi,b:\n#ip(0, 0) = 1 #ip(i+ 1, b\u2032) = \u2211\n(vi,b,vi+1,b\u2032 )\u2208A\n#ip(i, b), 0 \u2264 i < n\nLet #op(i, b) denote the number of paths from vertex vi,b to a vertex vk,b\u2032 with ` \u2264 b\u2032 \u2264 u.\n#op(n, b) = 1 #op(i, b) = \u2211\n(vi,b,vi+1,b\u2032 )\u2208A\n#op(i+ 1, b\u2032), 0 \u2264 i < k\nThe total number of paths (i.e. the solution count) is given by\n#knapsack(x, c, `, u) = #op(0, 0)\nin time linear in the size of the graph even though there may be exponentially many of them. The solution density of variable-value pair (xi, d) is given by\n\u03c3(xi, d, knapsack) =\n\u2211 (vi\u22121,b,vi,b+cid)\u2208A\n#ip(i\u2212 1, b) \u00b7#op(i, b+ cid) #op(0, 0) .\n6. This was originally introduced by Pesant and Quimper (2008).\nIn Figure 4, the left and right labels inside each vertex give the number of incoming and outgoing paths for that vertex, respectively. Table 1 reports the solution densities for every variable-value pair.\nThe time required to compute recursions #ip() and #op() is related to the number of arcs, which is in O(kumax1\u2264i\u2264k{|Di|}). Then each solution density computes a summation over a subset of the arcs but each arc of the graph is involved in at most one such summation, so the overall time complexity of computing every solution density is O(kumax1\u2264i\u2264k{|Di|}) as well."}, {"heading": "5.2 Bounds Consistent Knapsacks", "text": "Knapsack constraints, indeed most arithmetic constraints, have traditionally been handled by enforcing bounds consistency, a much cheaper form of inference. In some situations,\nwe may not afford to enforce domain consistency in order to get the solution counting information we need to guide our search heuristic. Can we still retrieve such information, perhaps not as accurately, from the weaker bounds consistency?\nConsider the variable x with domain D = [a, b]. Each value in D is equiprobable. We associate to x the discrete random variable X which follows a discrete uniform distribution with probability mass function f(v), mean \u00b5 = E[X], and variance \u03c32 = V ar[X].\nf(v) =\n{ 1\nb\u2212a+1 if a \u2264 v \u2264 b 0 otherwise\n(7)\n\u00b5 = a+ b\n2 (8)\n\u03c32 = (b\u2212 a+ 1)2 \u2212 1\n12 (9)\nTo find the distribution of a variable subject to a knapsack constraint, one needs to find the distribution of a linear combination of uniformly distributed random variables. Lyapunov\u2019s central limit theorem allows us to approximate the distribution of such a linear combination.\nTheorem 1 (Lyapunov\u2019s central limit theorem). Consider the independent random variables X1, . . . , Xn. Let \u00b5i be the mean of Xi, \u03c3 2 i be its variance, and r 3 i = E[|Xi \u2212 \u00b5i|3] be its third central moment. If\nlim n\u2192\u221e\n( \u2211n\ni=1 r 3 i )\n1 3 ( \u2211n\ni=1 \u03c3 2 i )\n1 2\n= 0,\nthen the random variable S = \u2211n\ni=1Xi follows a normal distribution with mean \u00b5S =\u2211n i=1 \u00b5i and variance \u03c3 2 S = \u2211n i=1 \u03c3 2 i .\nThe probability mass function of the normal distribution with mean \u00b5 and variance \u03c32\nis the Gaussian function:\n\u03d5(x) = e\u2212\n(x\u2212\u00b5)2\n2\u03c32\n\u03c3 \u221a 2\u03c0 (10)\nNote that Lyapunov\u2019s central limit theorem does not assume that the variables are taken from identical distributions. This is necessary since variables with different domains have different distributions.\nLemma 1 defines an upper bound on the third central moment of the expression kX where k is a positive coefficient and X is a uniformly distributed random variable.\nLemma 1. Let Y be a discrete random variable equal to kX such that k is a positive coefficient and X is a discrete random variable uniformly distributed over the interval [a, b]. The third central moment r3 = E[|Y \u2212 E[Y ]|3] is no greater than k3(b\u2212 a)3.\nProof. The case where a = b is trivial. We prove for b \u2212 a > 0. The proof involves simple algebraic manipulations from the definition of the expectation.\nr3 = kb\u2211 i=ka |i\u2212 E[Y ]|3f(i) (11)\n= b\u2211\nj=a\n|kj \u2212 kE[X]|3f(j) (12)\n= k3 b\u2211\nj=a\n\u2223\u2223\u2223\u2223j \u2212 a+ b2 \u2223\u2223\u2223\u22233 1b\u2212 a+ 1 since k > 0 (13)\n= k3\nb\u2212 a+ 1  a+b2\u2211 j=a ( a+ b 2 \u2212 j )3 + b\u2211 j=a+b\n2\n( j \u2212 a+ b\n2 )3 (14) = k3\nb\u2212 a+ 1  b\u2212a2\u2211 j=0 j3 + b\u2212a 2\u2211 j=0 j3  (15) \u2264 2k 3\nb\u2212 a b\u2212a 2\u2211 j=0 j3 since b\u2212 a > 0 (16)\nLet m = b\u2212a2 .\nr3 \u2264 k 3\nm m\u2211 j=0 j3 (17)\n\u2264 k 3\nm\n( 1\n4 (m+ 1)4 \u2212 1 2 (m+ 1)3 + 1 4 (m+ 1)2\n) (18)\n\u2264 k 3\nm\n( m4\n4 + m3 2 + m2 4\n) (19)\n\u2264 k 3\nm\n( m4\n4 +m4 +m4\n) since m \u2265 12 (20)\n\u2264 9 4 k3m3 (21)\nWhich confirms that r3 \u2264 932k 3(b\u2212 a)3 \u2264 k3(b\u2212 a)3.\nLemma 2 defines the distribution of a linear combination of uniformly distributed random variables.\nLemma 2. Let Y = \u2211n\ni=1 ciXi be a random variable where Xi is a discrete random variable uniformly chosen from the interval [ai, bi] and ci is a non-negative coefficient. When n tends to infinity, the distribution of Y tends to a normal distribution with mean \u2211n i=1 ci ai+bi 2 and\nvariance \u2211n\ni=1 c 2 i (bi\u2212ai+1)2\u22121 12 .\nProof. Let Yi = ciXi be a random variable. We want to characterize the distribution of\u2211n i=1 Yi. Let mi = bi\u2212ai 2 . The variance of the uniform distribution over the interval [ai, bi] is \u03c32i = (bi\u2212ai+1)2\u22121 12 = (mi+ 1 2 )2 3 \u2212 1 12 . We have V ar[Yi] = c 2 iV ar[Xi] = c 2 i\u03c3 2 i . Let r 3 i be the third central moment of Yi. By Lemma 1, we have r 3 i \u2264 c3i (bi \u2212 ai)3. Let L be the term mentioned in the condition of Lyapunov\u2019s central limit theorem:\nL = lim n\u2192\u221e\n(\u2211n i=1 r 3 i ) 1 3(\u2211n\ni=1 c 2 i\u03c3 2 i\n) 1 2\n(22)\nNote that the numerator and the denominator of the fraction are non-negative. This implies that L itself is non-negative. We prove that L \u2264 0 as n tends to infinity.\nL \u2264 lim n\u2192\u221e\n(\u2211n i=1 8c 3 im 3 i ) 1 3(\u2211n\ni=1 c 2 i\n( (mi+\n1 2 )2\n3 \u2212 1 12\n)) 1 2\n(23)\n\u2264 lim n\u2192\u221e\n( 8 \u2211n\ni=1 c 3 im 3 i ) 1 3(\n1 3 \u2211n i=1 c 2 im 2 i ) 1 2\n(24)\n\u2264 lim n\u2192\u221e\n2 \u221a 3 6 \u221a\u221a\u221a\u221a(\u2211ni=1 c3im3i )2(\u2211n i=1 c 2 im 2 i\n)3 (25) \u2264 lim\nn\u2192\u221e 2 \u221a 3 6\n\u221a \u2211n i=1 \u2211n j=1(cicjmimj)\n3\u2211n i=1 \u2211n j=1 \u2211n k=1(cicjckmimjmk) 2 (26)\nNote that in the last inequality, the terms (cicjmimj) 3 and (cicjckmimjmk) 2 are of the same order. However, there are n times more terms in the denominator than the numerator. Therefore, when n tends to infinity, the fraction tends to zero which proves that L = 0 as n tends to zero.\nBy Lyapunov\u2019s central limit theorem, as n tends to infinity, the expression Y = \u2211n\ni=1 Yi tends to a normal distribution with mean E[Y ] = \u2211n i=1 ciE[Xi] = \u2211n i=1 ci ai+bi 2 and variance\nV ar[Y ] = \u2211n\ni=1 c 2 iV ar[Xi] = \u2211n i=1 c 2 i (bi\u2212ai+1)2\u22121 12 .\nConsider the knapsack constraint ` \u2264 \u2211n\ni=1 cixi \u2264 u. Let xn+1 be a variable with domain Dn+1 = [`, u]. We obtain xj =\n1 cj\n(xn+1 \u2212 \u2211j\u22121 i=1 cixi \u2212 \u2211n i=j+1 cixi). Some coefficients in\nthis expression might be negative. They can be made positive by setting c\u2032i = \u2212ci and D\u2032i = [\u2212max(Di),\u2212min(Di)]. When n grows to infinity, the distribution of xj tends to a normal distribution as stated in Lemma 2. In practice, the normal distribution is a\ngood estimation even for small values of n. Figure 5.2 shows the actual distribution of the expression 3x+ 4y + 2z for x, y, z \u2208 [0, 5] and its approximation by a normal distribution.\nGiven a variable xi subject to a knapsack constraint, Algorithm 2 returns the assignment xi = ki with the highest solution density. The for loop computes the average mean \u00b5j and the variance \u03c32j of the uniform distribution associated to each variable xj . Lines 4 and 5 compute the mean and the variance of the distribution of xn+1 \u2212 \u2211n j=1 cjxj while Lines 6 and 7 compute the mean and the variance of xi = 1 ci (xn+1 \u2212 \u2211i\u22121 j=1 cjxj \u2212 \u2211n\nj=i+1 cjxj). Since this normal distribution is symmetric and unimodal, the most likely value ki in the domain Di is the one closest to the mean \u00b5i. The algorithm finds and returns this value as well as its density di. The density di is computed using the normal distribution. Since the variable xi must be assigned to a value in its domain, the algorithm normalizes on Line 9 the distribution over the values in the interval [min(Di),max(Di)].\nfor j \u2208 [1, n] do1 \u00b5j \u2190 min(Dj)+max(Dj)2 ;2 \u03c32j \u2190 (max(Dj)\u2212min(Dj)+1)2\u22121 12 ;3\nM \u2190 l+u2 \u2212 \u2211n j=1 cj\u00b5j ;4\nV \u2190 (u\u2212l+1) 2\u22121 12 + \u2211n j=1 c 2 j\u03c3 2 j ;5 m\u2190 M+ci\u00b5ici ;6 v \u2190 V\u2212c 2 i \u03c3 2 i\nc2i ;7 ki \u2190 arg mink\u2208Di |k \u2212m|;8\ndi \u2190 e\u2212 (ki\u2212m)\n2 2v / \u2211max(Di)\nk=min(Di) e\u2212\n(k\u2212m)2 2v ;9\nreturn \u3008xi = ki, di\u300910 Algorithm 2: xi = ki with the highest density as well as its density di for knapsack constraint ` \u2264 \u2211n i=1 cixi \u2264 u.\nLines 1 through 5 take O(n) time to execute. Line 8 depends on the data structure used by the solver to encode a domain. We assume that the line takes O(log |Di|) time to execute. The summation on Line 9 can be computed in constant time by approximating the summation with \u03a6m,v(max(Di) + 1 2) \u2212 \u03a6m,v(min(Di) + 1 2) where \u03a6m,v is the normal cumulative distribution function with average m and variance v. The constant 12 is added for the continuity correction. Other lines have a constant running time. The total complexity of Algorithm 2 is therefore O(n + log |Di|). Note that Line 1 to Line 5 do not depend on the value of i. Their computation can therefore be cached for subsequent calls to the function over the same knapsack constraint. Using this technique, finding the variable xi \u2208 {x1, . . . , xn} which has an assignment xi = ki of maximum density takes O( \u2211n i=1 log |Di|) time.\nA source of alteration of the distribution are values in the interval which are absent from the actual domain. Bounds consistency approximates the domain of a variable with its smallest covering interval. In order to reduce the error introduced by this approximation, one can compute the actual mean and actual variance of a domain Di on Lines 2 and 3\ninstead of using the mean and the variance of the covering interval, at a revised overall cost of O( \u2211n i=1 |Di|)."}, {"heading": "6. Generic Constraint-Centered Counting-based Heuristics", "text": "The previous sections provided algorithms to retrieve solution counting information from many of the most frequently used constraints. That information must then be exploited to guide search. The solving process alternates between propagating constraints to filter domains and branching by fixing a variable to a value in its domain. The crucial choice of variable and value is made through a search heuristic. We considered many search heuristics based on counting information, which we describe briefly in the next paragraph. We will experiment extensively with one of the most successful ones in Section 7, so we present it in more detail. In the following, we denote by C(xi) the set of constraints whose scope contains the variable xi. All the heuristics proposed assume a lexicographical ordering as tie breaking. Counting information is gathered at a search tree node once a propagation fixed point is reached: it is recomputed only on constraints for which a change occurred to the domain of a variable within its scope, and otherwise cached information is reused. That cached counting information is stored in trailing data structures (also known as reversible data structures) so that it can be retrieved upon backtracking. The heuristics considered fall into four broad categories:\nCombined choice of variable and value Those that select directly a variable-value pair without an explicit differentiation of variable and value ordering, based on the aggregation, through simple functions, of the counting information coming from different constraints. Such heuristics iterate over each variable-value pair, aggregating solution densities from the relevant constraints and selecting the pair exhibiting the maximum aggregated score. The type of aggregation used is e.g. the maximum, minimum, sum, or average. For instance:\n\u2022 maxSD: maxc\u2208C(xi)(\u03c3(xi, d, c)) \u2013 selects the maximum of the solution densities.\n\u2022 maxRelSD: maxc\u2208C(xi)(\u03c3(xi, d, c) \u2212 (1/|Di|)) \u2013 selects the maximum of the solution densities subtracting the average solution density for that given variable (i.e. 1/|Di|). It smoothes out the inherent solution densities differences due to domain cardinalities (as also the following aggregation function).\n\u2022 maxRelRatio: maxc\u2208C(xi)( \u03c3(xi,d,c) (1/|Di|) ) \u2013 selects the maximum of the ratio between the\nsolution density and the average solution density for that given variable.\n\u2022 aAvgSD: \u2211 c\u2208C(xi) \u03c3(xi,d,c)\n|C(xi)| \u2013 computes the arithmetic average of the solution densities.\n\u2022 wSCAvg: \u2211 c\u2208C(xi)\n(#c\u03c3(xi,d,c))\u2211 c\u2208C(xi) #c \u2013 computes the average of the solution densities weighted\nby the constraints\u2019 solution count. The weights tend to favor branchings on variablevalue pairs that keep a high percentage of solutions on constraints with a high solution count.\nChoice of constraint first Those that focus first on a specific constraint (e.g. based on its solution count) and then select a variable-value pair (as before) among the variables in the preselected constraint\u2019s scope. For instance, minSCMaxSD first selects the constraint with the lowest number of solutions and then restricts the choice of variable to those involved in this constraint, choosing the variable-value pair with the highest solution density. The rationale behind this heuristic is that the constraint with the fewest solutions is probably among the hardest to satisfy.\nRestriction of variables Those that preselect a subset of variables with minimum domain size and then choose among them the one with the best variable-value pair according to counting information.\nChoice of value only Those using some other generic heuristic for variable selection and solution densities for value selection.\nHeuristic maxSD The heuristic maxSD (Algorithm 3) simply iterates over all the variablevalue pairs and chooses the one that has the highest density; assuming that the \u03c3(xi, d, c) are precomputed, the complexity of the algorithm is O(qm) where q is the number of constraints and m is the sum of the cardinalities of the variables\u2019 domains. Interestingly, such a heuristic likely selects a variable with a small domain, in keeping with the fail-first principle, since its values have on average a higher density compared to a variable with many values (consider that the average density of a value is \u03c3(xi, d, c) =\n1 |Di|). Note that\neach constraint is considered individually.\nmax = 0;1 for each constraint c(x1, . . . , xk) do2 for each unbound variable xi \u2208 {x1, . . . , xk} do3 for each value d \u2208 Di do4 if \u03c3(xi, d, c) > max then5 (x?, d?) = (xi, d);6 max = \u03c3(xi, d, c);7 return branching decision \u201cx? = d?\u201d;8\nAlgorithm 3: The Maximum Solution Density search heuristic (maxSD)"}, {"heading": "7. Experimental Analysis", "text": "We performed a thorough experimental analysis in order to evaluate the performance of the proposed heuristics on eight different problems.7 All the problems expose sub-structures that can be encapsulated in global constraints for which counting algorithms are known. Counting-based heuristics are of no use for random problems as this class of problems do not expose any structure; nonetheless real-life problems usually do present structure therefore the performance of the heuristics proposed may have a positive impact in the quest to provide generic and efficient heuristics for structured problems. The problems on which we experimented have different structures and different constraints with possibly different\n7. The instances we used are available at www.crt.umontreal.ca/\u223cquosseca/fichiers/20-JAIRbenchs.tar.gz.\narities interconnected in different ways; thus, they can be considered as good representatives of the variety of problems that may arise in real life."}, {"heading": "7.1 Quasigroup Completion Problem with Holes (QWH)", "text": "Also referred to as the Latin Square problem, the QWH is defined on a n \u00d7 n grid whose squares each contain an integer from 1 to n such that each integer appears exactly once per row and column (problem 3 of the CSPLib maintained in Gent, Walsh, Hnich, & Miguel, 2009). The most common model uses a matrix of integer variables and an alldifferent constraint for each row and each column. So each constraint is defined on n variables and is of the same type; each variable is involved in two constraints and has the same domain (disregarding the clues). This is a very homogeneous problem. We tested on the 40 hard instances used by Zanarini and Pesant (2009) with n = 30 and 42% of holes (corresponding to the phase transition), generated following Gomes and Shmoys (2002)."}, {"heading": "7.2 Magic Square Completion Problem", "text": "The magic square completion problem (problem 19 of CSPLib) is defined on a n \u00d7 n grid and asks to fill the square with numbers from 1 to n2 such that each row, each column and each main diagonal sums up to the same value. In order to make them harder, the problem instances have been partially prefilled (half of the instances have 10% of the variables set and the other half, 50% of the variables set). The 40 instances (9 \u00d7 9) are taken from the work of Pesant and Quimper (2008). This problem is modeled with a matrix of integer variables, a single alldifferent constraint spanning over all the variables and a knapsack constraint for each row, column and main diagonal. The problem involves different constraints although the majority are equality knapsack with the same arity."}, {"heading": "7.3 Nonograms", "text": "A Nonogram (problem 12 of CSPLib) is built on a rectangular n\u00d7m grid and requires filling in some of the squares in the unique feasible way according to some clues given on each row and column. As a reward, one gets a pretty monochromatic picture. Each individual clue indicates how many sequences of consecutive filled-in squares there are in the row (column), with their respective size in order of appearance. For example, \u201c2 1 5\u201d indicates that there are two consecutive filled-in squares, then an isolated one, and finally five consecutive ones. Each sequence is separated from the others by at least one blank square but we know little about their actual position in the row (column). Such clues can be modeled with regular constraints. This is a very homogeneous problem, with constraints of identical type defined over m or n variables, and with each (binary) variable involved in two constraints. These puzzles typically require some amount of search, despite the fact that domain consistency is maintained on each clue. We experimented with 180 instances8 of sizes ranging from 16\u00d7 16 to 32\u00d7 32.\n8. Instances taken from http://www.blindchicken.com/\u223cali/games/puzzles.html"}, {"heading": "7.4 Multi Dimensional Knapsack Problem", "text": "The Multi dimensional knapsack problem was originally proposed as an optimization problem by the OR community. We followed the same approach as Refalo (2004) in transforming the optimization problem into a feasibility problem by fixing the objective function to its optimal value, thereby introducing a 0-1 equality knapsack constraint. The other constraints are upper bounded knapsack constraints on the same variables. We tested on three different set of instances for a total of 25 instances: the first set corresponds to the six instances used by Refalo, the second set and the third set come from the OR-Library (Weish[1-13] from Shi, 1979; PB[1,2,4] and HP[1,2] from Freville & Plateau, 1990). The first instance set have n, that is the number of variables, ranging from 6 to 50 and m, that is the number of constraints, from 5 to 10; in the second and third instance set n varies from 27 to 60 and m from 2 to 5. The problem involves only one kind of constraint and, differently from the previous problem classes, all the constraints are posted on the same set of variables."}, {"heading": "7.5 Market Split Problem", "text": "The market split problem was originally introduced by Cornue\u0301jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches. There exists both a feasibility and optimization version. The feasibility problem consists of m 0-1 equality knapsack constraints defined on the same set of 10(m\u22121) variables. Even small instances (4 \u2264 m \u2264 6) are surprisingly hard to solve by standard means. We used the 10 instances tested by Pesant and Quimper (2008) that were generated by Wassermann (2007). The Market Split Problem shares some characteristics with the Multi Dimensional Knapsack problem: the constraints are of the same type and they are posted on the same set of variables."}, {"heading": "7.6 Rostering Problem", "text": "The rostering problem was inspired by a rostering context. The objective is to schedule n employees over a span of n time periods. In each time period, n \u2212 1 tasks need to be accomplished and one employee out of the n has a break. The tasks are fully ordered 1 to n \u2212 1; for each employee the schedule has to respect the following rules: two consecutive time periods have to be assigned to either two consecutive tasks (in no matter which order i.e. (t, t+1) or (t+1, t)) or to the same task (i.e. (t, t)); an employee can have a break after no matter which task; after a break an employee cannot perform the task that precedes the task prior to the break (i.e. (t, break, t\u22121) is not allowed). The problem is modeled with one regular constraint per row and one alldifferent constraint per column. We generated 2 sets of 30 instances with n = 10 each with 5% preset assignments and respectively 0% and 2.5% of values removed."}, {"heading": "7.7 Cost-Constrained Rostering Problem", "text": "The cost-constrained rostering problem was borrowed from Pesant and Quimper (2008) and the 10 instances as well. It is inspired by a rostering problem where m employees (m = 4) have to accomplish a set of tasks in a n-day schedule (n = 25). No employee can perform the same task as another employee on the same day (alldifferent constraint on each day). Moreover, there is an hourly cost for making someone work, which varies both\nacross employees and days. For each employee, the total cost must be equal to a randomly generated value (equality knapsack constraint for each employee). Finally, each instance has about 10 forbidden shifts i.e. there are some days in which an employee cannot perform a given task. In the following, we refer to this problem also as KPRostering. This problem presents constraints of different types that have largely different arities."}, {"heading": "7.8 Traveling Tournament Problem with Predefined Venues (TTPPV)", "text": "The TTPPV was introduced by Melo, Urrutia, and Ribeiro (2009) and consists of finding an optimal single round robin schedule for a sport event. Given a set of n teams, each team has to play against every other team. In each game, a team is supposed to play either at home or away, however no team can play more than three consecutive times at home or away. The particularity of this problem resides on the venue of each game that is predefined, i.e. if team a plays against b it is already known whether the game is going to be held at a\u2019s home or at b\u2019s home. A TTPPV instance is said to be balanced if the number of home games and the number of away games differ by at most one for each team; otherwise it is referred to as non-balanced or random. The problem is modeled with one alldifferent and one regular constraint per row and one alldifferent constraint per column. The TTPPV was originally introduced as an optimization problem where the sum of the traveling distance of each team has to be minimized, however Melo et al. (2009) show that it is particularly difficult to find a single feasible solution employing traditional integer linear programming methods. Balanced instances of size 18 and 20 (the number of teams denotes the instance size) were taking from roughly 20 to 60 seconds to find a first feasible solution with Integer Linear Programming; non-balanced instances could take up to 5 minutes (or even time out after 2 hours of computation). Furthermore six non-balanced instances are infeasible but the ILP approach proposed by Melo et al. were unable to prove it. Hence, the feasibility version of this problem already represents a challenge.\nFor every problem (unless specified otherwise): domain consistency is maintained during search9, the counting algorithm for the alldifferent constraint is UB-FC (upper bounds with forward checking as the consistency level enforced), the search tree is binary (i.e. xi = j \u2228 xi 6= j), and traversed depth-first. All tests were performed on a AMD Opteron 2.2GHz with 1GB and Ilog Solver 6.6; the heuristics that involve some sort of randomization (either in the heuristic itself or in the counting algorithms employed) have been run 10 times and the average of the results has been taken into account. We set a timeout of 20 minutes for all problems and heuristics. We present the results by plotting the percentage of solved instances against time or backtracks."}, {"heading": "7.9 Comparing Counting-Based Search Heuristics", "text": "We first compare several of the proposed search heuristics based on counting with respect to how well they guide search, measured as the number of backtracks required to find a solution. The important issue of overall runtime will be addressed in the following sections.\n9. Even for knapsack constraints, comparative experimental results on the same benchmark instances, originally reported by Pesant and Quimper (2008), indicated that maxSD performed better with domain consistency and the associated counting algorithm.\nFigure 6 plots the number of solved instances against backtracks for our eight benchmark problems. On the Nonogram, Multi-Knapsack, and Market Split problems, maxSD, maxRelSD, and maxRelRatio correspond to the same heuristics because domains are binary. Restricting the use of solution densities to the choice of a value once the variable has been selected by the popular domain size over dynamic degree heuristic (domDeg;maxSD) generally achieves very poor performance compared to the others. One disappointment which came as a surprise is that selecting first the constraint with the fewest solutions left (minSCMaxSD) often behaves poorly as well. For the Multi-Knapsack Problem aAvgSD, which takes the arithmetic average of the solution densities, performs about one order of magnitude better than the others. We believe that this might be explained by the fact that all the constraints share the same variables (in the Latin Square and Nonogram problems constraints overlap on only one variable): therefore branching while considering all the constraint information pays off. The maxSD and maxRelSD search heuristics stand out as being more robust on these benchmarks. They are quite similar but each performs significantly better than the other on one problem domain. Because it is slightly simpler, we will restrict ourselves to the former in the remaining experiments."}, {"heading": "7.10 Comparing with Other Generic Search Heuristics", "text": "The experimental results of the previous section suggest that the relatively simple maxSD heuristic guides search at least as well as any of the others. We now compare it to the following ones (see Section 2 as a reference) which are good representatives of the state of the art for generic search heuristics:\n\u2022 dom - it selects among the variables with smallest remaining domain uniformly at random and then chooses a value uniformly at random;\n\u2022 domWDeg - it selects the variable according to the dom/wdeg heuristic and then the first value in lexicographic order;\n\u2022 IBS - Impact-based Search with full initialization of the impacts; it chooses a subset of 5 variables with the best approximated impact and then it breaks ties based on the node impacts while further ties are broken randomly; (ILOG, 2005)\nFigure 7 and 8 plot the number of solved instances against backtracks and time for our eight benchmark problems. For the moment we ignore the curves for the heuristics with restarts.\nThe maxSD heuristic significantly outperforms the other heuristics on the Latin Square, Magic Square, Multi Dimensional Knapsack, Cost-Constrained Rostering (KPRostering in the figure), and TTPPV problems (5 out of 8 problems), both in terms of number of backtracks and computation time. For the Nonogram Problem it is doing slightly worse than domWDeg and is eventually outperformed by IBS. The sharp improvement of the latter around 1000 backtracks suggests that singleton consistency is very powerful for this problem and not too time consuming since domains are binary. Indeed IBS\u2019s full initialization of the impacts at the root node achieves singleton consistency as a preprocessing step. This behavior is even more pronounced for the Rostering Problem (see the IBS curves). On that problem maxSD\u2019s performance is more easily compared to domWDeg, which dominates it.\nFor the Market Split Problem the differences in performance are not as striking: maxSD is doing slightly better in terms of backtracks but not enough to outperform domWDeg in terms of runtime.\nFor the Magic Square plot against time, there is a notable bend at about the 50% mark in most of the curves which can be explained by the fact that half of the instances only have 10% of their cells prefilled and present a bigger challenge. Interestingly, the simpler dom heuristics performs better than IBS and domWDeg, the latter being unable to solve half the instances in the allotted time. In contrast with the Nonogram Problem, here the full impact initialization is a very heavy procedure due to the high number of variable-value pairs to probe (\u2248 n4 that is in our instances 94 = 6561). It is also worth noting that on the Cost-Constrained Rostering Problem, maxSD solves seven out of the ten instances backtrack-free and is the only heuristic solving every instance. Similarly for the TTPPV Problem, almost 90% of the instances are solved backtrack-free by that heuristic. Moreover six instances happen to be infeasible and maxSD exhibits short proof trees for five of them, every other heuristic timing out on them."}, {"heading": "7.11 Adding Randomized Restarts", "text": "It has been remarked that some combinatorial search has a strictly positive probability to reach a subtree that requires exponentially more time than the other subtrees encountered so far (so called \u201cheavy-tail\u201d behavior). Nonetheless, heavy tails can be largely avoided by adding randomized restarts on top of the search procedure (Gomes, Selman, & Kautz, 1998). This technique is orthogonal to the search heuristic employed and it systematically restarts the search every time a limit (typically a bound on the number of backtracks) is reached; obviously, in order to be effective, randomized restarts must be employed along with a heuristic that presents some sort of randomization or learning such that at each restart different parts of the search tree are explored. We tested the same heuristics to assess their performance with randomized restarts. The maxSD and IBS heuristics have been randomized: particularly, one variable-value pair is chosen at random with equal probability between the best two provided by the heuristic. Note that, as pointed out by Refalo (2004), impact information is carried over different runs to improve the quality of the impact approximation. As for domWDeg, the learned weights are kept between restarts. We implemented a slow geometric restart policy (Walsh, 1999) (that is 1, r, r2, . . . with r = 2) with a scale parameter optimized experimentally and separately for each problem type and search heuristic.\nWe turn again to Figure 7 and 8 but this time we also consider the curves for the heuristics with restarts. Restarts generally help a less informed heuristic such as dom, sometimes spectacularly so as for the Rostering Problem, but not always as indicated by the results on the Market Split Problem. For the other heuristics their usefulness is mixed: it makes little difference for maxSD except for Market Split where it degrades performance and Rostering where it improves its performance very significantly, now solving every instance very easily; for IBS it helps on the most difficult instances for half of the problems but for three others it degrades performance; for domWDeg it is generally more positive but never spectacular. Note that heavy-tail behavior of runtime distribution is conjectured to depend both on the problem structure and on the search heuristic employed (Hulubei & O\u2019Sullivan,\n2006). The Market Split Problem stands out as one where randomized restarts hurt every search heuristic considered."}, {"heading": "7.12 Using Limited Discrepancy Search", "text": "Another way to avoid heavy tails is to change the order in which the search tree is traversed, undoing decisions made at the top of the search tree earlier in the traversal. A popular way of doing this is by applying limited discrepancy search (LDS) that visits branches in increasing order of their number of \u201cdiscrepancies\u201d, which correspond to branching decisions going against the search heuristic (Harvey & Ginsberg, 1995). As for restarts, it can be combined with any search heuristic and may cause dramatic improvements in some cases but this less natural traversal comes with a price. Figure 9 illustrates the impact of LDS on two of our benchmark problems, using maxSD as the search heuristic. Either the usual depth-first search traversal is used (\u201cmaxSD\u201d curve) or limited discrepancy search, grouping branches that have exactly the same number of discrepancies (\u201cLDS 1\u201d), by skips of 2 (\u201cLDS 2\u201d), or by skips of 4 (\u201cLDS 4\u201d) discrepancies. On the rostering problem LDS undoes bad early decisions made by our heuristic and now allows us to solve every instance very quickly. However on the Magic Square problem the impact of LDS on the number of backtracks is low and it actually significantly slows down the resolution because LDS must revisit internal nodes, thus repeating propagation steps: the smaller the skip, the larger the computational penalty.\nThe same behavior could have been observed on other search heuristics and other problems. So LDS does not necessarily add robustness to our search."}, {"heading": "7.13 Analyzing Variable and Value Selection Separately", "text": "One may wonder whether the success of counting-based search heuristics mostly depends on informed value selection, the accompanying variable selection being accessory. In order to investigate this, we introduce some hybrid heuristics:\n\u2022 maxSD; random - selects a variable as in maxSD but then selects a value in its domain uniformly at random;\n\u2022 IBS; maxSD - selects a variable as in IBS but then selects a value in its domain according to solution densities;\n\u2022 domWDeg; maxSD - selects a variable as in domWDeg but then selects a value in its domain according to solution densities;\nFigure 10 and 11 plot the number of solved instances against backtracks and time for our eight benchmark problems. Comparing maxSD and maxSD; random indicates that most of the time value selection according to solution densities is crucial, the Rostering Problem being an exception. Interestingly value selection by solution density improves the overall performance of IBS; for domWDeg it improves for the Latin Square and Magic Square problems but not for the rest, often decreasing performance. However such improvements do not really tip the balance in favor of other heuristics than maxSD, thus indicating that variable selection according to solution densities is also very important to its success."}, {"heading": "8. Conclusion", "text": "This paper described and evaluated counting-based search to solve constraint satisfaction problems. We presented some algorithms necessary to extract counting information from several of the main families of constraints in cp. We proposed a variety of heuristics based on that counting information and evaluated them. We then compared one outstanding representative, maxSD, to the state of the art on eight different problems from the literature and obtained very encouraging results. The next logical steps in this research include designing counting algorithms for some of the other common constraints and strengthening our empirical evaluation by considering new problems and comparing against applicationspecific heuristics. The next two paragraphs describe less obvious steps.\nUsers often need to introduce auxiliary variables or different views of the models that are linked together by channeling constraints. It is very important to provide all the counting information available at the level of the branching variables or at least at some level where direct comparison of solution densities is meaningful. For example in the case of the TTPPV an earlier model, in which two sets of variables each received solution densities from different constraints, did not perform nearly as well. Channeling constraints that express a one-tomany relation (such as the one present in the TTPPV) can be dealt with by considering value multiplicity in counting algorithms (Pesant & Zanarini, 2011). More complex channeling constraints represent however a limitation in the current framework.\nCombinatorial optimization problems have not been discussed in this paper but are very important in operations research. Heuristics with a strong emphasis on feasibility (such as counting-based heuristics) might not be well suited for problems with a strong optimization component, yet may be very useful when dealing with optimization problems that involve hard combinatorics. Ideally, counting algorithms should not be blind to cost reasoning. One possibility that we started investigating not only counts the number of solutions that involve a particular variable-value pair but also returns the average cost of all the solutions featuring that particular variable-value pair. Another has shown promise when the cost is linear and decomposable on the decision variables (Pesant & Zanarini, 2011).\nTo conclude, we believe counting-based search brings us closer to robust automated search in cp and also offers efficient building blocks for application-specific heuristics."}, {"heading": "Acknowledgments", "text": "Financial support for this research was provided in part by the Natural Sciences and Engineering Research Council of Canada and the Fonds que\u0301be\u0301cois de la recherche sur la nature et les technologies. We wish to thank Tyrel Russell who participated in the implementation and experimentation work. We also thank the anonymous referees for their constructive comments that allowed us to improve our paper."}], "references": [{"title": "The Maximum Number of Perfect Matchings in Graphs with a Given Degree Sequence", "author": ["N. Alon", "S. Friedland"], "venue": "The Electronic Journal of Combinatorics,", "citeRegEx": "Alon and Friedland,? \\Q2008\\E", "shortCiteRegEx": "Alon and Friedland", "year": 2008}, {"title": "Experimental evaluation of modern variable selection strategies in Constraint Satisfaction Problems. In Proceedings of the Fifteenth Knowledge Representation and Automated Reasoning Workshop on Experimental Evaluation of Algorithms for Solving Problems with Combinatorial Explosion, RCRA-08", "author": ["T. Balafoutis", "K. Stergiou"], "venue": null, "citeRegEx": "Balafoutis and Stergiou,? \\Q2008\\E", "shortCiteRegEx": "Balafoutis and Stergiou", "year": 2008}, {"title": "On Conflict-driven variable ordering heuristics", "author": ["T. Balafoutis", "K. Stergiou"], "venue": "In Proceedings of Thirteenth Annual ERCIM International Workshop on Constraint Solving and Constraint Logic Programming,", "citeRegEx": "Balafoutis and Stergiou,? \\Q2008\\E", "shortCiteRegEx": "Balafoutis and Stergiou", "year": 2008}, {"title": "MAC and Combined Heuristics: Two Reasons to Forsake FC (and CBJ?) on Hard Problems", "author": ["C. Bessi\u00e8re", "R\u00e9gin", "J.-C"], "venue": "In Proceedings of the Second International Conference on Principles and Practice of Constraint Programming, CP-96,", "citeRegEx": "Bessi\u00e8re et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bessi\u00e8re et al\\.", "year": 1996}, {"title": "Boosting Systematic Search by Weighting Constraints", "author": ["F. Boussemart", "F. Hemery", "C. Lecoutre", "L. Sais"], "venue": "In Proceedings of the Sixteenth Eureopean Conference on Artificial Intelligence,", "citeRegEx": "Boussemart et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boussemart et al\\.", "year": 2004}, {"title": "Efficient Generic Search Heuristics within the EMBP framework", "author": ["R.L. Bras", "A. Zanarini", "G. Pesant"], "venue": "In Proceedings of the Fifteenth International Conference on Principles and Practice of Constraint Programming, CP-04,", "citeRegEx": "Bras et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bras et al\\.", "year": 2009}, {"title": "Some Properties of Nonnegative Matrices and their Permanents", "author": ["L.M. Br\u00e9gman"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Br\u00e9gman,? \\Q1973\\E", "shortCiteRegEx": "Br\u00e9gman", "year": 1973}, {"title": "New Methods to Color the Vertices of a Graph", "author": ["D. Br\u00e9laz"], "venue": "Communications of the ACM,", "citeRegEx": "Br\u00e9laz,? \\Q1979\\E", "shortCiteRegEx": "Br\u00e9laz", "year": 1979}, {"title": "A Class of Hard Small 0-1 Programs", "author": ["G. Cornu\u00e9jols", "M. Dawande"], "venue": "INFORMS Journal of Computing,", "citeRegEx": "Cornu\u00e9jols and Dawande,? \\Q1999\\E", "shortCiteRegEx": "Cornu\u00e9jols and Dawande", "year": 1999}, {"title": "A Branch and Bound Method for the Multiconstraint Zero One Knapsack Problem", "author": ["A. Freville", "G. Plateau"], "venue": "Investigation Operativa,", "citeRegEx": "Freville and Plateau,? \\Q1990\\E", "shortCiteRegEx": "Freville and Plateau", "year": 1990}, {"title": "An Upper Bound for the Number of Perfect Matchings in Graphs. http://arxiv.org/abs/0803.0864", "author": ["S. Friedland"], "venue": null, "citeRegEx": "Friedland,? \\Q2008\\E", "shortCiteRegEx": "Friedland", "year": 2008}, {"title": "A Problem Library for Constraints. http://www.csplib.org", "author": ["I.P. Gent", "T. Walsh", "B. Hnich", "I. Miguel"], "venue": null, "citeRegEx": "Gent et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gent et al\\.", "year": 2009}, {"title": "Boosting Combinatorial Search Through Randomization", "author": ["C. Gomes", "B. Selman", "H. Kautz"], "venue": "In Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence,", "citeRegEx": "Gomes et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 1998}, {"title": "Completing Quasigroups or Latin Squares: A Structured Graph Coloring Problem", "author": ["C. Gomes", "D. Shmoys"], "venue": "In Proceedings of Computational Symposium on Graph Coloring and Generalizations,", "citeRegEx": "Gomes and Shmoys,? \\Q2002\\E", "shortCiteRegEx": "Gomes and Shmoys", "year": 2002}, {"title": "Learning to Identify Global Bottlenecks in Constraint Satisfaction Search. In Learning for Search: Papers from the AAAI-06", "author": ["D. Grimes", "R.J. Wallace"], "venue": null, "citeRegEx": "Grimes and Wallace,? \\Q2006\\E", "shortCiteRegEx": "Grimes and Wallace", "year": 2006}, {"title": "Sampling Strategies and Variable Selection in Weighted Degree Heuristics", "author": ["D. Grimes", "R.J. Wallace"], "venue": "In Proceedings of the Thirteenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Grimes and Wallace,? \\Q2007\\E", "shortCiteRegEx": "Grimes and Wallace", "year": 2007}, {"title": "Increasing Tree Seach Efficiency for Constraint Satisfaction Problems", "author": ["R.M. Haralick", "G.L. Elliott"], "venue": "Artificial Intelligence,", "citeRegEx": "Haralick and Elliott,? \\Q1980\\E", "shortCiteRegEx": "Haralick and Elliott", "year": 1980}, {"title": "Limited Discrepancy Search", "author": ["W.D. Harvey", "M.L. Ginsberg"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Harvey and Ginsberg,? \\Q1995\\E", "shortCiteRegEx": "Harvey and Ginsberg", "year": 1995}, {"title": "Using Expectation Maximization to Find Likely Assignments for Solving CSP\u2019s", "author": ["E.I. Hsu", "M. Kitching", "F. Bacchus", "S.A. McIlraith"], "venue": "In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hsu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2007}, {"title": "The Impact of Search", "author": ["T. Hulubei", "B. O\u2019Sullivan"], "venue": "Heuristics on Heavy-Tailed Behaviour. Constraints,", "citeRegEx": "Hulubei and O.Sullivan,? \\Q2006\\E", "shortCiteRegEx": "Hulubei and O.Sullivan", "year": 2006}, {"title": "Matrix Factorizations of Determinants and Permanents", "author": ["W. Jurkat", "H.J. Ryser"], "venue": "Journal of Algebra,", "citeRegEx": "Jurkat and Ryser,? \\Q1966\\E", "shortCiteRegEx": "Jurkat and Ryser", "year": 1966}, {"title": "Counting-Based Look-Ahead Schemes for Constraint Satisfaction", "author": ["K. Kask", "R. Dechter", "W. Gogate"], "venue": "In Springer-Verlag (Ed.), Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming, CP-04,", "citeRegEx": "Kask et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kask et al\\.", "year": 2004}, {"title": "An Upper Bound for the Permanent of (0,1)-Matrices", "author": ["H. Liang", "F. Bai"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Liang and Bai,? \\Q2004\\E", "shortCiteRegEx": "Liang and Bai", "year": 2004}, {"title": "The traveling tournament problem with predefined venues", "author": ["R. Melo", "S. Urrutia", "C. Ribeiro"], "venue": "Journal of Scheduling,", "citeRegEx": "Melo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2009}, {"title": "Upper Bounds for Permanents of (0, 1)-matrices", "author": ["H. Minc"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Minc,? \\Q1963\\E", "shortCiteRegEx": "Minc", "year": 1963}, {"title": "A Regular Language Membership Constraint for Finite Sequences of Variables", "author": ["G. Pesant"], "venue": "In Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Pesant,? \\Q2004\\E", "shortCiteRegEx": "Pesant", "year": 2004}, {"title": "Counting solutions of csps: A structural approach", "author": ["G. Pesant"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Pesant,? \\Q2005\\E", "shortCiteRegEx": "Pesant", "year": 2005}, {"title": "Counting solutions of knapsack constraints", "author": ["G. Pesant", "Quimper", "C.-G"], "venue": "In Proceedings of the Fifth International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems,", "citeRegEx": "Pesant et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pesant et al\\.", "year": 2008}, {"title": "Recovering indirect solution densities for counting-based branching heuristics", "author": ["G. Pesant", "A. Zanarini"], "venue": "CPAIOR, Vol. 6697 of Lecture Notes in Computer Science,", "citeRegEx": "Pesant and Zanarini,? \\Q2011\\E", "shortCiteRegEx": "Pesant and Zanarini", "year": 2011}, {"title": "Improved algorithms for the global cardinality constraint", "author": ["C. Quimper", "A. Lopez-Ortiz", "P. van Beek", "A. Golynski"], "venue": "In Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Quimper et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quimper et al\\.", "year": 2004}, {"title": "Impact-Based Search Strategies for Constraint Programming", "author": ["P. Refalo"], "venue": "In Proceedings of the Tenth International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "Refalo,? \\Q2004\\E", "shortCiteRegEx": "Refalo", "year": 2004}, {"title": "A Filtering Algorithm for Constraints of Difference in CSPs", "author": ["R\u00e9gin", "J.-C"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence, AAAI-94,", "citeRegEx": "R\u00e9gin and J..C.,? \\Q1994\\E", "shortCiteRegEx": "R\u00e9gin and J..C.", "year": 1994}, {"title": "The Symmetric Alldiff Constraint", "author": ["J. R\u00e9gin"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "R\u00e9gin,? \\Q1999\\E", "shortCiteRegEx": "R\u00e9gin", "year": 1999}, {"title": "A Branch and Bound Method for the Multiconstraint Zero One Knapsack Problem", "author": ["W. Shi"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "Shi,? \\Q1979\\E", "shortCiteRegEx": "Shi", "year": 1979}, {"title": "Trying Harder to Fail First", "author": ["B.M. Smith", "S.A. Grant"], "venue": "In Thirteenth European Conference on Artificial Intelligence,", "citeRegEx": "Smith and Grant,? \\Q1998\\E", "shortCiteRegEx": "Smith and Grant", "year": 1998}, {"title": "Permanental Bounds for Nonnegative Matrices via Decomposition", "author": ["G.W. Soules"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Soules,? \\Q2005\\E", "shortCiteRegEx": "Soules", "year": 2005}, {"title": "Guiding Search using Constraint-Level Advice", "author": ["R. Szymanek", "B. O\u2019Sullivan"], "venue": "In Proceeding of Seventeenth European Conference on Artificial Intelligence, ECAI06,", "citeRegEx": "Szymanek and O.Sullivan,? \\Q2006\\E", "shortCiteRegEx": "Szymanek and O.Sullivan", "year": 2006}, {"title": "A dynamic programming approach for consistency and propagation for knapsack constraints", "author": ["M.A. Trick"], "venue": "Annals of Operations Research,", "citeRegEx": "Trick,? \\Q2003\\E", "shortCiteRegEx": "Trick", "year": 2003}, {"title": "The Complexity of Computing the Permanent", "author": ["L. Valiant"], "venue": "Theoretical Computer Science,", "citeRegEx": "Valiant,? \\Q1979\\E", "shortCiteRegEx": "Valiant", "year": 1979}, {"title": "Search in a Small World", "author": ["T. Walsh"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Walsh,? \\Q1999\\E", "shortCiteRegEx": "Walsh", "year": 1999}, {"title": "The feasibility version of the market split problem. http://did.mat.uni-bayreuth.de/ alfred/marketsplit.html", "author": ["A. Wassermann"], "venue": null, "citeRegEx": "Wassermann,? \\Q2007\\E", "shortCiteRegEx": "Wassermann", "year": 2007}, {"title": "Improved algorithm for the soft global cardinality constraint", "author": ["A. Zanarini", "M. Milano", "G. Pesant"], "venue": "In Proceedings of the Third International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems, CPAIOR-06,", "citeRegEx": "Zanarini et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zanarini et al\\.", "year": 2006}, {"title": "Solution Counting Algorithms for Constraint-Centered", "author": ["A. Zanarini", "G. Pesant"], "venue": "Search Heuristics. Constraints,", "citeRegEx": "Zanarini and Pesant,? \\Q2009\\E", "shortCiteRegEx": "Zanarini and Pesant", "year": 2009}, {"title": "More robust counting-based search heuristics with alldifferent constraints", "author": ["A. Zanarini", "G. Pesant"], "venue": "CPAIOR, Vol. 6140 of Lecture Notes in Computer Science,", "citeRegEx": "Zanarini and Pesant,? \\Q2010\\E", "shortCiteRegEx": "Zanarini and Pesant", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "The concept of counting-based search heuristics has already been introduced, most recently by Zanarini and Pesant (2009). The specific contributions of this paper are: additional counting algorithms, including for other families of constraints, thus broadening the applicability of these heuristics; experiments that include the effect of some common features of search heuristics such as search tree traversal order, restarts and learning; considerable empirical evidence that counting-based search outperforms other generic heuristics.", "startOffset": 107, "endOffset": 121}, {"referenceID": 25, "context": "And now recall some definitions and notation from Pesant (2005) and Zanarini and Pesant (2009).", "startOffset": 50, "endOffset": 64}, {"referenceID": 25, "context": "And now recall some definitions and notation from Pesant (2005) and Zanarini and Pesant (2009).", "startOffset": 50, "endOffset": 95}, {"referenceID": 7, "context": "A similar heuristic, proposed by Br\u00e9laz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree ddeg 1 (that is, the one constraining the largest number of unbound variables).", "startOffset": 33, "endOffset": 47}, {"referenceID": 7, "context": "A similar heuristic, proposed by Br\u00e9laz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree ddeg 1 (that is, the one constraining the largest number of unbound variables). Bessi\u00e8re and R\u00e9gin (1996) and Smith and Grant (1998) combined the domain and degree information by minimizing the ratio dom/deg or dom/ddeg.", "startOffset": 33, "endOffset": 283}, {"referenceID": 7, "context": "A similar heuristic, proposed by Br\u00e9laz (1979), selects the variable with the smallest remaining domain and then breaks ties by choosing the one with the highest dynamic degree ddeg 1 (that is, the one constraining the largest number of unbound variables). Bessi\u00e8re and R\u00e9gin (1996) and Smith and Grant (1998) combined the domain and degree information by minimizing the ratio dom/deg or dom/ddeg.", "startOffset": 33, "endOffset": 310}, {"referenceID": 30, "context": "1 Impact-Based Heuristics Refalo (2004) proposed Impact Based Search (IBS), a heuristic that chooses the variable whose instantiation triggers the largest search space reduction (highest impact) that is approximated as the reduction of the product of the variable domain cardinalities.", "startOffset": 26, "endOffset": 40}, {"referenceID": 30, "context": "The variable impact is defined by Refalo (2004) as", "startOffset": 34, "endOffset": 48}, {"referenceID": 34, "context": "As an interesting connection with impact-based heuristics, Szymanek and O\u2019Sullivan (2006) proposed to query the model constraints to approximate the number of filtered values by each constraint individually.", "startOffset": 59, "endOffset": 90}, {"referenceID": 1, "context": "Balafoutis and Stergiou (2008b) proposed, among other improvements over the original dom/wdeg, weight aging, that is the constraint weights are periodically reduced.", "startOffset": 0, "endOffset": 32}, {"referenceID": 25, "context": "Hsu, Kitching, Bacchus, and McIlraith (2007) and later Bras, Zanarini, and Pesant (2009) apply a Belief Propagation algorithm within an Expectation Maximization framework (EMBP) in order to approximate variable biases (or marginals) i.", "startOffset": 75, "endOffset": 89}, {"referenceID": 25, "context": "Then as discussed by Zanarini and Pesant (2009), counting the number of solutions to an alldifferent constraint is equivalent to computing the permanent of A (or the number of maximum matchings in the value graph), formally defined as", "startOffset": 34, "endOffset": 48}, {"referenceID": 38, "context": "Because computing the permanent is well-known to be #P -complete (Valiant, 1979), Zanarini and Pesant (2009) developed an approach based on sampling which gave close approximations and led to very effective heuristics on hard instances.", "startOffset": 65, "endOffset": 80}, {"referenceID": 25, "context": "If p extra rows were added, the result must be divided by p! as shown by Zanarini and Pesant (2010). Because computing the permanent is well-known to be #P -complete (Valiant, 1979), Zanarini and Pesant (2009) developed an approach based on sampling which gave close approximations and led to very effective heuristics on hard instances.", "startOffset": 86, "endOffset": 100}, {"referenceID": 25, "context": "If p extra rows were added, the result must be divided by p! as shown by Zanarini and Pesant (2010). Because computing the permanent is well-known to be #P -complete (Valiant, 1979), Zanarini and Pesant (2009) developed an approach based on sampling which gave close approximations and led to very effective heuristics on hard instances.", "startOffset": 86, "endOffset": 210}, {"referenceID": 25, "context": "This was originally introduced by Zanarini and Pesant (2010).", "startOffset": 47, "endOffset": 61}, {"referenceID": 23, "context": "A first upper bound for the permanent was conjectured by Minc (1963) and later proved by Br\u00e9gman (1973): perm(A) \u2264 n \u220f", "startOffset": 57, "endOffset": 69}, {"referenceID": 6, "context": "A first upper bound for the permanent was conjectured by Minc (1963) and later proved by Br\u00e9gman (1973): perm(A) \u2264 n \u220f", "startOffset": 89, "endOffset": 104}, {"referenceID": 22, "context": "Recently Liang and Bai (2004) proposed a second upper bound (with qi = min{d ri+1 2 e, d i 2e}): perm(A) \u2264 n \u220f", "startOffset": 9, "endOffset": 30}, {"referenceID": 6, "context": "In the following we denote by UBBM (A) the Br\u00e9gman-Minc upper bound and by UBLB(A) the Liang-Bai upper bound. Jurkat and Ryser (1966) proposed another bound:", "startOffset": 43, "endOffset": 134}, {"referenceID": 25, "context": "The sampling algorithm introduced by Zanarini and Pesant (2009) performed very well both in approximating the solution count and the solution densities, but this is not the case for upper bounds.", "startOffset": 50, "endOffset": 64}, {"referenceID": 32, "context": "A symmetric alldifferent achieving domain consistency provides more pruning power than the equivalent decomposition given by the alldifferent constraint and the set of xi = j \u21d0\u21d2 xj = i constraints (R\u00e9gin, 1999).", "startOffset": 197, "endOffset": 210}, {"referenceID": 32, "context": "2 Symmetric Alldifferent R\u00e9gin (1999) proposed the symmetric alldifferent constraint that is a special case of the alldifferent in which variables and values are defined from the same set.", "startOffset": 25, "endOffset": 38}, {"referenceID": 0, "context": "Friedland (2008) and Alon and Friedland (2008) extended the Br\u00e9gman-Minc upper bound to consider the number of matchings in general undirected graphs.", "startOffset": 21, "endOffset": 47}, {"referenceID": 25, "context": "Inspired by Quimper, Lopez-Ortiz, van Beek, and Golynski (2004) and Zanarini, Milano, and Pesant (2006), we define Gl the lower bound graph.", "startOffset": 90, "endOffset": 104}, {"referenceID": 25, "context": "The filtering algorithms for the regular constraint and the knapsack constraint (when domain consistency is enforced) are both based on the computation of paths in a layered acyclic directed graph (Pesant, 2004; Trick, 2003).", "startOffset": 197, "endOffset": 224}, {"referenceID": 37, "context": "The filtering algorithms for the regular constraint and the knapsack constraint (when domain consistency is enforced) are both based on the computation of paths in a layered acyclic directed graph (Pesant, 2004; Trick, 2003).", "startOffset": 197, "endOffset": 224}, {"referenceID": 37, "context": "1 Domain Consistent Knapsacks We start from the reduced graph described by Trick (2003), which is a layered directed graph G(V,A) with special vertex v0,0 and a vertex vi,b \u2208 V for 1 \u2264 i \u2264 k and 0 \u2264 b \u2264 u whenever \u2200 j \u2208 [1, i], \u2203 dj \u2208 Dj such that i \u2211", "startOffset": 75, "endOffset": 88}, {"referenceID": 25, "context": "This was originally introduced by Pesant and Quimper (2008).", "startOffset": 34, "endOffset": 60}, {"referenceID": 24, "context": "We tested on the 40 hard instances used by Zanarini and Pesant (2009) with n = 30 and 42% of holes (corresponding to the phase transition), generated following Gomes and Shmoys (2002).", "startOffset": 56, "endOffset": 70}, {"referenceID": 13, "context": "We tested on the 40 hard instances used by Zanarini and Pesant (2009) with n = 30 and 42% of holes (corresponding to the phase transition), generated following Gomes and Shmoys (2002).", "startOffset": 160, "endOffset": 184}, {"referenceID": 25, "context": "The 40 instances (9 \u00d7 9) are taken from the work of Pesant and Quimper (2008). This problem is modeled with a matrix of integer variables, a single alldifferent constraint spanning over all the variables and a knapsack constraint for each row, column and main diagonal.", "startOffset": 52, "endOffset": 78}, {"referenceID": 30, "context": "We followed the same approach as Refalo (2004) in transforming the optimization problem into a feasibility problem by fixing the objective function to its optimal value, thereby introducing a 0-1 equality knapsack constraint.", "startOffset": 33, "endOffset": 47}, {"referenceID": 8, "context": "5 Market Split Problem The market split problem was originally introduced by Cornu\u00e9jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches.", "startOffset": 77, "endOffset": 107}, {"referenceID": 8, "context": "5 Market Split Problem The market split problem was originally introduced by Cornu\u00e9jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches. There exists both a feasibility and optimization version. The feasibility problem consists of m 0-1 equality knapsack constraints defined on the same set of 10(m\u22121) variables. Even small instances (4 \u2264 m \u2264 6) are surprisingly hard to solve by standard means. We used the 10 instances tested by Pesant and Quimper (2008) that were generated by Wassermann (2007).", "startOffset": 77, "endOffset": 483}, {"referenceID": 8, "context": "5 Market Split Problem The market split problem was originally introduced by Cornu\u00e9jols and Dawande (1999) as a challenge to LP-based branch-and-bound approaches. There exists both a feasibility and optimization version. The feasibility problem consists of m 0-1 equality knapsack constraints defined on the same set of 10(m\u22121) variables. Even small instances (4 \u2264 m \u2264 6) are surprisingly hard to solve by standard means. We used the 10 instances tested by Pesant and Quimper (2008) that were generated by Wassermann (2007). The Market Split Problem shares some characteristics with the Multi Dimensional Knapsack problem: the constraints are of the same type and they are posted on the same set of variables.", "startOffset": 77, "endOffset": 524}, {"referenceID": 25, "context": "7 Cost-Constrained Rostering Problem The cost-constrained rostering problem was borrowed from Pesant and Quimper (2008) and the 10 instances as well.", "startOffset": 94, "endOffset": 120}, {"referenceID": 23, "context": "The TTPPV was originally introduced as an optimization problem where the sum of the traveling distance of each team has to be minimized, however Melo et al. (2009) show that it is particularly difficult to find a single feasible solution employing traditional integer linear programming methods.", "startOffset": 145, "endOffset": 164}, {"referenceID": 25, "context": "Even for knapsack constraints, comparative experimental results on the same benchmark instances, originally reported by Pesant and Quimper (2008), indicated that maxSD performed better with domain consistency and the associated counting algorithm.", "startOffset": 120, "endOffset": 146}, {"referenceID": 39, "context": "We implemented a slow geometric restart policy (Walsh, 1999) (that is 1, r, r2, .", "startOffset": 47, "endOffset": 60}, {"referenceID": 30, "context": "Note that, as pointed out by Refalo (2004), impact information is carried over different runs to improve the quality of the impact approximation.", "startOffset": 29, "endOffset": 43}], "year": 2012, "abstractText": "Designing a search heuristic for constraint programming that is reliable across problem domains has been an important research topic in recent years. This paper concentrates on one family of candidates: counting-based search. Such heuristics seek to make branching decisions that preserve most of the solutions by determining what proportion of solutions to each individual constraint agree with that decision. Whereas most generic search heuristics in constraint programming rely on local information at the level of the individual variable, our search heuristics are based on more global information at the constraint level. We design several algorithms that are used to count the number of solutions to specific families of constraints and propose some search heuristics exploiting such information. The experimental part of the paper considers eight problem domains ranging from well-established benchmark puzzles to rostering and sport scheduling. An initial empirical analysis identifies heuristic maxSD as a robust candidate among our proposals. We then evaluate the latter against the state of the art, including the latest generic search heuristics, restarts, and discrepancy-based tree traversals. Experimental results show that counting-based search generally outperforms other generic heuristics.", "creator": "TeX"}}}