{"id": "1401.5699", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Text Relatedness Based on a Word Thesaurus", "abstract": "The computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. Without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. Such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. One of the reasons we are now aware of the role of relatedness between text segments is the general idea that a given word is one of several possible items in the collection of relatedness. Moreover, given the general concept of relatedness, this particular idea of relatedness may serve as a useful basis for some cognitive approaches to the topic of relatedness in general.\n\n\n\n\n\n\nThe following discussion was originally published on 8/11/2012 in the Journal of Neurosciences.\nThe following discussion was originally published on 7/10/2012 in the Journal of Neurosciences.\nThe following discussion was originally published on 7/10/2012 in the Journal of Neurosciences.\nA discussion by D.R. Martin was initially published on 7/10/2012 in the Journal of Neurosciences.\nA discussion by D.R. Martin was initially published on 7/10/2012 in the Journal of Neurosciences.\nThe following discussion was originally published on 7/10/2012 in the Journal of Neurosciences.\nD.R. Martin was originally published on 7/10/2012 in the Journal of Neurosciences.\nThe following discussion was originally published on 7/10/2012 in the Journal of Neurosciences.\nD.R. Martin was originally published on 7/10/2012 in the Journal of Neurosciences.\nD.R. Martin was originally published on 7/10/2012 in the Journal of Neurosciences.\nThe following discussion was originally published on 7/10/2012 in the Journal of Neurosciences.\nThe following discussion was originally published on 7/10/2012 in the Journal of Neurosciences.\nD.R. Martin was originally published on 7/10/2012 in the Journal of Neurosciences.\nD.R. Martin was originally published on 7/10/2012 in the Journal of Neurosciences.\nD.R. Martin was originally published on 7/10/2012 in the Journal of Neurosciences.", "histories": [["v1", "Wed, 15 Jan 2014 05:41:08 GMT  (324kb)", "http://arxiv.org/abs/1401.5699v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["george tsatsaronis", "iraklis varlamis", "michalis vazirgiannis"], "accepted": false, "id": "1401.5699"}, "pdf": {"name": "1401.5699.pdf", "metadata": {"source": "CRF", "title": "Text Relatedness Based on a Word Thesaurus", "authors": ["George Tsatsaronis", "Iraklis Varlamis", "Michalis Vazirgiannis"], "emails": ["GBT@IDI.NTNU.NO", "VARLAMIS@HUA.GR", "MVAZIRG@AUEB.GR"], "sections": [{"heading": null, "text": "taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. Without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. Such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. In this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links. The approach exploits only a word thesaurus in order to devise implicit semantic links between words. Based on this approach, we introduce Omiotis, a new measure of semantic relatedness between texts which capitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the relatedness between texts. We gradually validate our method: we first evaluate the performance of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy; then, we proceed with evaluating the performance of our method in measuring text-to-text semantic relatedness in two tasks, namely sentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the proposed method outperforms every lexicon-based method of semantic relatedness in the selected tasks and the used data sets, and competes well against corpus-based and hybrid approaches."}, {"heading": "1. Introduction", "text": "Relatedness between texts can be perceived in several different ways. Primarily, one can think of lexical relatedness or similarity between texts, which can be easily captured by a vectorial representation of texts (van Rijsbergen, 1979) and a standard similarity measure, like Cosine, Dice (Salton & McGill, 1983), and Jaccard (1901). Such models have had high impact in information retrieval over the past decades. Several improvements have been proposed for such techniques towards inventing more sophisticated weighting schemes for the text words, like for example TF-IDF and its variations (Aizawa, 2003). Other directions explore the need to capture the latent semantic relations between dimensions (words) in the created vector space model, by using techniques of latent semantic analysis (Landauer, Foltz, & Laham, 1998). Another aspect of text relatedness, probably of equal importance, is the semantic relatedness between two text segments. For example, the sentences \u201dThe shares of the company dropped 14 cents\u201d, and \u201dThe business institution\u2019s stock slumped 14 cents\u201d have an obvious semantic relatedness, which traditional measures of text similarity fail\nc\u00a92010 AI Access Foundation. All rights reserved.\nto recognize. The motivation of this work is to show that a measure of relatedness between texts, which takes into account both the lexical and the semantic relatedness of word elements, performs better than the traditional lexical matching models, and can handle cases like the one above.\nIn this paper we propose Omiotis1, a new measure of semantic relatedness between texts, which extends SR, a measure of semantic relatedness between words. The word-to-word relatedness measure, in its turn, is based on the construction of semantic links between individual words, according to a word thesaurus, which in our case is WordNet (Fellbaum, 1998). Each pair of words is potentially connected via one or more semantic paths, each one comprising one or more semantic relations (edges) that connect intermediate thesaurus concepts (nodes). For weighting the semantic path we consider three key factors: (a) the semantic path length, (b) the intermediate nodes\u2019 specificity denoted by the node depth in the thesaurus\u2019 hierarchy, and (c) the types of the semantic edges that compose the path. This triptych allows our measure to perform well in complex linguistic tasks, that require more than simple similarity, such as the SAT Analogy Test2 that is demonstrated in the experiments. To the best of our knowledge, Omiotis is the first measure of semantic relatedness between texts that considers in tandem all three factors for measuring the pairwise word-to-word semantic relatedness scores. Omiotis integrates the semantic relatedness in word level with words\u2019 statistical information in the text level to provide the final semantic relatedness score between texts.\nThe contributions of this work can be summarized in the following: 1) a new measure for computing semantic relatedness between words, namely SR, which exploits all of the semantic information a thesaurus can offer, including semantic relations crossing parts of speech (POS), while taking into account the relation weights and the depth of the thesaurus\u2019 nodes; 2) a new measure for computing semantic relatedness between texts, namely Omiotis, that does not require the use of external corpora or learning methods, supervised or unsupervised, 3) thorough experimental evaluation on benchmark data sets for measuring the performance on word-to-word similarity and relatedness tasks, as well as on word analogy; in addition, experimental evaluation on two text related tasks (sentence-to-sentence similarity and paraphrase recognition) for measuring the performance of our text-to-text relatedness measure. Additional contributions of this work are: a) the use of all semantic relations offered by WordNet, which increases the chances of finding a semantic path between any two words, b) the availability of pre-computed semantic relatedness scores between every pair of WordNet senses, which accelerates computation of semantic relatedness between texts and facilitates the incorporation of semantic relatedness in several applications (Tsatsaronis, Varlamis, N\u00f8rva\u030ag, & Vazirgiannis, 2009; Tsatsaronis & Panagiotopoulou, 2009).\nThe rest of the paper is organized as follows: Section 2 discusses preliminary concepts regarding word thesauri, semantic network construction, and semantic relatedness or similarity measures, and summarizes related work on these fields. Section 3 presents the key contributions of our work. Section 4 provides the experimental evaluation and the analysis of the results. Finally, Section 5 presents our conclusions and the next steps of our work."}, {"heading": "2. Preliminaries and Related Work", "text": "Our approach capitalizes on a word thesaurus in order to define a measure of semantic relatedness between words, and expands this measure to compute text relatedness using both semantic and\n1. Omiotis is the Greek word for relatedness or similarity. 2. http://www.aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questions\nlexical information. In order to facilitate the understanding of our methodology we elaborate on preliminary concepts in this section and present related research approaches."}, {"heading": "2.1 Word Thesauri and their use in Text Applications", "text": "Word thesauri, like WordNet (Fellbaum, 1998) or Roget\u2019s International Thesaurus (Morris & Hirst, 1991), constitute the knowledge base for several text-related research tasks. WordNet has been used successfully as a knowledge base in the construction of Generalized Vector Space Models (GVSM) and semantic kernels for document similarity with application to text classification, such as the works of Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald and Weikum (2005), and Basili, Cammisa and Moschitti (2005), and text retrieval, such as the works of Voorhees (1993), Stokoe, Oakes and Tait (2003), and our previous work regarding the definition of a new GVSM that uses word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009). Furthermore, the idea of using a thesaurus as a knowledge base in text retrieval has also been proven successful in the case of cross language information retrieval, like for example in the case of the CLIR system introduced by Clough and Stevenson (2004). Finally, the exploitation of word thesauri in linguistic tasks, such as Word Sense Disambiguation (WSD) (Ide & Veronis, 1998) has yielded interesting results (Mihalcea & Moldovan, 1999; Tsatsaronis, Vazirgiannis, & Androutsopoulos, 2007; Tsatsaronis, Varlamis, & Vazirgiannis, 2008).\nThe application of a text relatedness measure to text classification and retrieval tasks should first consider the impact of lexical ambiguity and WSD in the overall performance in these tasks. Sanderson (1994, 2008) concludes that ambiguity in words can take many forms, but new test collections are needed to realize the true importance of resolving ambiguity and embedding semantic relatedness and sense disambiguation in the text retrieval task. In the analysis of Barzilay and Elhadad (1997), and Barzilay, Elhadad and McKeown (2002) the impact of WSD in the performance of text summarization tasks is addressed by considering all possible interpretations of the lexical chains created from text. Similar to this methodology, we tackle word ambiguity by taking into account every possible type of semantic information that the thesaurus can offer, for any given sense of a text word.\nFrom the aforementioned approaches, it is clear that the use of a word thesaurus can offer much potential in the design of models that capture the semantic relatedness between texts, and consequently, it may improve the performance of existing retrieval and classification models under certain circumstances that are discussed in the respective research works (Mavroeidis et al., 2005; Basili et al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004). The word thesaurus employed in the development of Omiotis is WordNet. Its lexical database contains English nouns, verbs, adjectives and adverbs, organized in sets of synonym senses (synsets). Hereafter, the terms senses, synsets and concepts are used interchangeably. Synsets are connected with various links that represent semantic relations between them (i.e., hypernymy / hyponymy, meronymy / holonymy, synonymy / antonymy, entailment / causality, troponymy, domain / domain terms, derivationally related forms, coordinate terms, attributes, stem adjectives, etc.). Several relations cross parts of speech, like the domain terms relation, which connects senses pertaining to the same domain (e.g., light, as a noun meaning electromagnetic radiation producing a visual sensation, belongs to the domain of physics). To the best of our knowledge, the proposed approach is the first that utilizes all of the aforementioned semantic relations that exist in WordNet for the construction of a semantic relatedness measure."}, {"heading": "2.2 Creating Semantic Networks from Word Thesauri", "text": "Omiotis is based on the creation of semantic paths between words in a text using the thesaurus\u2019 concepts and relations. Early approaches in this field, used gloss words from the respective word definitions in order to build semantic networks from text (Veronis & Ide, 1990). The idea of representing text as a semantic network was initially introduced by Quilian (1969). The expansion of WordNet with semantic relations that cross parts of speech has added more possibilities of semantic network construction from text. More recent approaches to semantic network construction from word thesauri, by Mihalcea, Tarau and Figa (2004) and Navigli (2008), utilize a wide range of WordNet semantic relations instead of the gloss words. These methods outperformed previous methods that used semantic networks in the all words WSD tasks of Senseval 2 and 3 for the English language (Palmer, Fellbaum, & Cotton, 2001; Snyder & Palmer, 2004). In this work we adopt the semantic network construction method that we introduced in the past (Tsatsaronis et al., 2007). The method utilizes all of the available semantic relations in WordNet. In the WSD task, the respective method outperformed or matched previous methods that used semantic networks in the all words WSD tasks of Senseval 2 and 3 for the English language, and this was largely due to the rich representation that the semantic networks offered. Section 3.1 introduces our semantic relatedness measure."}, {"heading": "2.3 Measures of Semantic Relatedness", "text": "Semantic relatedness between words or concepts has been exploited, in the past, in text summarization (Barzilay et al., 2002), text retrieval (Stokoe et al., 2003; Smeaton, Kelledy, & O\u2019Donnell, 1995; Richardson & Smeaton, 1995) and WSD (Patwardhan, Banerjee, & Pedersen, 2003) tasks. Semantic relatedness measures can be widely classified to dictionary-based3, corpus-based and hybrid.\nAmong dictionary-based measures, the measure of Agirre and Rigau (1995) was one of the first measures developed to compute semantic relatedness between two or more concepts (i.e., for a set of concepts). Their measure was based on the density and depth of concepts in the set and on the length of the shortest path that connects them. However, they assume that all edges in the path are equally important.\nThe measure proposed by Leacock, Miller and Chodorow (1998) for computing the semantic similarity between a pair of concepts takes into account the length of the shortest path connecting them, measured as the number of nodes participating in the path, and the maximum depth of the taxonomy. The measure for two concepts s1 and s2 can be computed as follows:\nSim(s1, s2) = \u2212log length\n2 \u00b7 D (1)\nwhere length is the length of the shortest path connecting s1 and s2 and D is the maximum depth of the taxonomy used.\nRegarding hybrid measures, Resnik\u2019s (1995, 1999) measure for pairs of concepts is based on the Information Content (IC) of the deepest concept that can subsume both (least common subsumer), and can be considered as a hybrid measure, since it combines both the hierarchy of the used thesaurus, and statistical information for concepts measured in large corpora. More specifically, the\n3. Also found in the bibliography as knowledge-based, thesaurus-based, or lexicon-based.\nsemantic similarity for a given pair of concepts s1 and s2, which have s0 as their least common subsumer (i.e., least common ancestor), is defined in the following equation:\nSim(s1, s2) = IC(s0) (2)\nwhere the Information Content (IC) of a concept (i.e., s0) is defined as:\nIC(s0) = \u2212logP (s0) (3)\nand P (s0) is the probability of occurrence of the concept s0 in a large corpus. The measure proposed by Jiang and Conrath (1997), is also based on the concept of IC. Given two concepts s1 and s2, and their least common subsumer s0, their semantic similarity is defined as follows:\nSim(s1, s2) = 1\nIC(s1) + IC(s2) \u2212 2 \u00b7 IC(s0) (4)\nThe measure of Lin (1998) is also based on IC. Given, again, s1, s2, and s0, as before, the similarity between s1 and s2 is defined as follows:\nSim(s1, s2) = 2 \u00b7 IC(s0)\nIC(s1) + IC(s2) (5)\nHirst and St-Onge (1998) reexamine the idea of constructing lexical chains between words, based on their synsets and the respective semantic edges that connect them in WordNet. The initial idea of lexical chains was first introduced by Morris and Hirst (1991), who defined the lexical cohesion of a passage, based on the cohesion of the lexical chains between the passage\u2019s elements, which acted as an indicator for the continuity of the passage\u2019s lexical meaning.\nWe encourage the reader to consult the analysis of Budanitsky and Hirst (2006) for a detailed discussion on most of the aforementioned measures, as well as for more measures proposed prior to the aforementioned. While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet.\nMore recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words\u2019 definitions to create WordNet-based context vectors, the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness. Other recent hybrid measures of semantic similarity are: the measure proposed by Li et al. (2006), who use information from WordNet and corpus statistics collected from the Brown Corpus (Kucera,\nFrancis, & Caroll, 1967) to compute similarity between very short texts, and the measure for text distance proposed by Tsang (2008), that uses both distributional similarity and ontological/knowledge information to compute the distance between text fragments. Distributional similarity is also used in a supervised combination with WordNet-based approaches (Agirre, Alfonseca, Hall, Kravalova, Pasca, & Soroa, 2009), to produce a supervised measure of semantic relatedness. Li et al. (2006) have created a new data set for their experimental evaluation, which we also use in Section 4 to evaluate our Omiotis measure and compare against their approach.\nIn the following section we formally define Omiotis and provide its details, from the creation of the semantic links to the computation of relatedness between words and texts. We give evidence on the measure\u2019s complexity and justify our design choices. Finally, we discuss potential applications of the measure on text related tasks."}, {"heading": "3. Measuring Word-to-Word and Text-to-Text Semantic Relatedness", "text": "This section presents the details of Omiotis, our measure of text semantic relatedness. The measure capitalizes on the idea of semantic relatedness between WordNet senses, extends it to compute relatedness between words and finally between texts. Since the definition of semantic relatedness ranges from pairs of keyword senses to pairs of texts, Omiotis is defined in a way that captures relatedness in every granularity. As a result, it can be applied in a wide range of linguistic and text related tasks such as WSD, word similarity and word analogy, text similarity, and keyword ranking. The key points of the proposed measure are: (a) it constructs semantic links between all word senses in WordNet and pre-computes a relatedness score between every pair of WordNet senses, (b) it computes the semantic relatedness for a pair of words by taking into account the relatedness of their corresponding WordNet senses, and (c) it computes a semantic relatedness score for any two given text segments by extending word-to-word relatedness. Depending on the task, the computation of semantic relatedness can be modified to take into account all or some of the senses of each word, all or some of the words in each text, or to apply additional weights depending on the word importance or sense importance in context. This allows Omiotis to be adapted in various text related tasks, without modifying the main process of computing relatedness. In Section 3.1 that follows, we formally define our semantic relatedness measure and in Section 3.2 we provide a detailed justification of our design decisions."}, {"heading": "3.1 Construct Semantic Links between Words", "text": "The first step in measuring the semantic relatedness between two text fragments, is to find the implicit semantic links between the words of the two fragments. Thus, we present a definition of semantic relatedness for a pair of thesaurus concepts, which takes into account the semantic path connecting the concepts, and expands it to measure the relatedness between words. In order to solve the problem of constructing semantic paths between words, we base our approach on our previous method on how to construct semantic networks between words (Tsatsaronis et al., 2007)."}, {"heading": "3.1.1 SEMANTIC NETWORK CONSTRUCTION FROM WORD THESAURI", "text": "Figure 1 gives an example of the construction of a semantic network for two words ti and tj . For simplicity reasons, we assume the construction of a semantic path between senses S.i.2 and S.j.1 only (Initial Phase), though we could do the same for every possible combination of the two words\u2019\nsenses. Initially, the two sense nodes are expanded using all the semantic links offered by WordNet. The semantic links of the senses, as found in the thesaurus, become the edges and the pointed senses the nodes of the network (Network Expansion). The expansion process is repeated recursively until the shortest 4 path between S.i.2 and S.j.1 is found. When no path is found from S.i.2 to S.j.1 then the senses and consequently the words are not semantically related."}, {"heading": "3.1.2 SEMANTIC RELATEDNESS BETWEEN A PAIR OF CONCEPTS", "text": "The semantic relatedness for a pair of concepts is measured over the constructed semantic network. It considers the path length, captured by compactness, and the path depth, captured by semantic path elaboration, which are defined in the following. A measure for WSD based on the idea of compactness was initially proposed by Mavroeidis et al. (2005). The original measure used only nouns and the hypernym relation, and is extended in the current work to support all of WordNet\u2019s relations and the noun, verb and adjective parts of speech. Here we define a new compactness measure (Definition 1) as the core of the Omiotis measure.\nDefinition 1 Given a word thesaurus O, a weighting scheme for the edges that assigns a weight w \u2208 (0, 1) for each edge, a pair of senses S = (s1, s2), and a path P of length l connecting the two senses, the semantic compactness of S (SCM(S, O, P )) is defined as: SCM(S, O, P ) =\n\u220fl i=1 wi,\nwhere w1, w2, ..., wl are the path\u2019s edges\u2019 weights. If s1 = s2 then SCM(S, O, P ) = 1. If there is no path between s1 and s2 then SCM(S, O, P ) = 0.\nNote that compactness takes the path length into account and is bound in [0, 1]. Higher compactness between senses implies higher semantic relatedness. The intuition behind edge types\u2019 weighting is that certain types provide stronger semantic connections than others. Considering that the lexicographers of WordNet tend to use some relation types more often than others (we assume that the most used relation types are stronger than the types less used), a straightforward solution is to define edge types\u2019 weights in proportion to their frequency of occurrence in WordNet 2.0. The weights assigned to each type using this solution are shown in Table 1 and are in accordance to those found by Song et al. (2004). The table shows the probability of occurrence in WordNet 2.0 for every possible edge type in the thesaurus, in descending order of probability values. A detailed analysis of the choices we made in Definition 1 and in the definitions that follow is performed in Section 3.2.\nThe depth of nodes that belong to the path also affects term relatedness. A standard means of measuring depth in a word thesaurus is the hypernym/hyponym hierarchical relation for the noun and adjective POS and hypernym/troponym for the verb POS. For the adverb POS the related stem\n4. The details are presented in Algorithm 1.\nadjective sense can be used to measure its depth. A path with shallow sense nodes is more general compared to a path with deep nodes. This parameter of semantic relatedness between terms is captured by the measure of semantic path elaboration introduced in the following definition.\nDefinition 2 Given a word thesaurus O , a pair of senses S = (s1, s2), where s1,s2 \u2208 O and s1 6= s2, and a path P =< p1, p2, ..., pl > of length l, where either s1 = p1 and s2 = pl or s1 = pl and s2 = p1, the semantic path elaboration of the path (SPE(S, O, P )) is defined as: SPE(S, O, P ) =\n\u220fl i=1 2didi+1 di+di+1 \u00b7 1 dmax\n, where di is the depth of sense pi according to O, and dmax the maximum depth of O. If s1 = s2, then d1 = d2 = d and SPE(S, O, P ) = ddmax . If there is no path from s1 to s2 then SPE(S, O, P ) = 0.\nIt is obvious in Definition 2 that a path of length l comprises l+1 nodes, thus when i = l, di+1 is the last node in the path. Essentially, SPE is the harmonic mean of the two depths normalized to the maximum thesaurus depth. The harmonic mean is preferred over the average of depths, since it offers a lower upper bound and gives a more realistic estimation of the path\u2019s depth. Compactness and Semantic Path Elaboration measures capture the two most important parameters of measuring semantic relatedness between terms (Budanitsky & Hirst, 2006), namely path length and senses depth in the used thesaurus. We combine these two measures following the definition of the Semantic Relatedness between two terms:\nDefinition 3 Given a word thesaurus O, and a pair of senses S = (s1, s2) the semantic relatedness of S (SR(S, O)) is defined as maxP {SCM(S, O, P ) \u00b7 SPE(S, O, P )}.\nAlgorithm 1 Maximum-Semantic-Relatedness(G, u, v, w)\n1: INPUT: A directed weighted graph G, two nodes u, v and a weighting scheme w : E \u2192 (0..1). 2: OUTPUT: The path from u to v with the maximum product of the edges weights.\nInitialize-Single-Source(G, u) 3: for all vertices v \u2208 VG do 4: d[v] = \u2212\u221e 5: \u03c0[v] = NULL 6: end for 7: d[u] = 1\nRelax(u, v, w) 8: if d[v] < d[u] \u00b7 w(u, v) then 9: d[v] = d[u] \u00b7 w(u, v)\n10: \u03c0[v] = u 11: end if\nMaximum-Relatedness(G, u, v, w) 12: Initialize-Single-Source(G, u) 13: S = \u2205 14: Q = VG 15: while v \u2208 Q do 16: s = Extract from Q the vertex with the maximum d 17: S = S \u222a s 18: for all vertices k \u2208 Adjacency List of s do 19: Relax(s, k, w) 20: end for 21: end while 22: return the path following all the ancestors \u03c0 of v back to u\nGiven a word thesaurus, there can be more than one semantic path connecting two senses. The senses\u2019 compactness can take different values for all the different paths. In these cases, we use the path that maximizes the semantic relatedness. For its computation we introduce Algorithm 1, which is a modification of Dijkstra\u2019s algorithm (Cormen, Leiserson, & Rivest, 1990) for finding the shortest path between two nodes in a weighted directed graph. In the algorithm, G is the representation of the directed weighted graph given as input (e.g., using adjacency lists), and VG is the set of all the vertices of G. Also, two more sets are used; S, which contains all the vertices for which the maximum semantic relatedness has been computed from the source vertex (i.e., from u), and Q, which contains all the vertices for which the algorithm has not computed yet the maximum relatedness from the source vertex. Furthermore, three tables are used; d, which, for any vertex v stores the maximum semantic relatedness found at any given time of the algorithm execution from the source vertex, i.e., u in d[v]; \u03c0, which for any vertex v stores its predecessor in \u03c0[v]; and w, which stores the edge weights of the graph (e.g., w[k, m] stores the edge weight of the edge that starts from k and goes to m).\nThe algorithm comprises three functions: (a) Initialize-Single-Source(G, u), which initializes tables d and \u03c0, for every vertex v of the graph. More precisely, it sets d[v] = \u2212\u221e, since the semantic relatedness from the source is unknown at the beginning, and because the algorithm seeks for the maximum semantic relatedness this is initially set to the minimum value (i.e., \u2212\u221e). It also sets \u03c0[v] = NULL, since at the beginning of the algorithm execution we are not aware yet of the predecessor of any vertex v following the path from the source vertex u to v that results to the maximum semantic relatedness; (b) Relax(u, v, w), which given two vertices, u and v that are directly connected with an edge of weight w[u, v], it updates the value d[v], in case that if we follow the edge (u, v) this results to a higher semantic relatedness for vertex v from the source, compared to the value we have computed up to that time of the algorithm execution; and (c) Maximum-Relatedness(G, u, v, w), which uses the aforementioned functions and executes the Dijkstra\u2019s algorithm. The proof of the algorithm\u2019s correctness follows in the next theorem.\nTheorem 1 Given a word thesaurus O, an edges weighting function w : E \u2192 (0, 1), where a higher value declares a stronger edge, and a pair of senses S(ss, sf ) declaring source (ss) and destination (sf ) vertices, then the SCM(S, O, P ) \u00b7 SPE(S, O, P ) is maximized for the path returned by Algorithm 1, by using the weighting scheme w\u2032ij = wij \u00b7 2\u00b7di\u00b7dj dmax\u00b7(di+dj) , where w\u2032ij is the new weight of the edge connecting senses si and sj .\nProof 1 We will show that for each vertex sf \u2208 VG, d[sf ] is the maximum product of edges\u2019 weight through the selected path, starting from ss, at the time when sf is inserted into S. From now on, the notation \u03b4(ss, sf ) will represent this product. Path p connects a vertex in S, namely ss, to a vertex in VG \u2212 S, namely sf . Consider the first vertex sy along p such that sy \u2208 VG \u2212 S and let sx be y\u2019s predecessor. Now, path p can be decomposed as ss \u2192 sx \u2192 sy \u2192 sf . We claim that d[sy] = \u03b4(ss, sy) when sf is inserted into S. Observe that sx \u2208 S. Then, because sf is chosen as the first vertex for which d[sf ] 6= \u03b4(ss, sf ) when it is inserted into S, we had d[sx] = \u03b4(ss, sx) when sx was inserted into S.\nBecause sy occurs before sf on the path from ss to sf and all edge weights are nonnegative and in (0, 1) we have \u03b4(ss, sy) \u2265 \u03b4(ss, sf ), and thus d[sy] = \u03b4(ss, sy) \u2265 \u03b4(ss, sf ) \u2265 d[sf ]. But both sy and sf were in V \u2212 S when sf was chosen, so we have d[sf ] \u2265 d[sy]. Thus, d[sy] = \u03b4(ss, sy) = \u03b4(ss, sf ) = d[sf ]. Consequently, d[sf ] = \u03b4(ss, sf ) which contradicts our choice of sf . We conclude that at the time each vertex sf is inserted into S, d[sf ] = \u03b4(ss, sf ).\nNext, to prove that the returned maximum product is the SCM(S, O, P ) \u00b7 SPE(S, O, P ), let the path between ss and sf with the maximum edge weight product have k edges. Then, Algorithm 1 returns the maximum \u220fk\ni=1 w \u2032 i(i+1) = ws2 \u00b7 2\u00b7ds\u00b7d2 dmax\u00b7(ds+d2) \u00b7w23 \u00b7 2\u00b7d2\u00b7d3 dmax\u00b7(d2+d3) \u00b7 ... \u00b7wkf \u00b7 2\u00b7dk\u00b7df dmax\u00b7(dk+df )\n= \u220fk\ni=1 wi(i+1) \u00b7 \u220fk i=1 2didi+1 di+di+1 \u00b7 1 dmax = SCM(S, O, P ) \u00b7 SPE(S, O, P )."}, {"heading": "3.1.3 SEMANTIC RELATEDNESS FOR A PAIR OF TERMS", "text": "Based on Definition 3, which measures the semantic relatedness between a pair of senses S, we can define the semantic relatedness between a pair of terms T (t1, t2) as follows.\nDefinition 4 Let a word thesaurus O, let T = (t1, t2) be a pair of terms for which there are entries in O, let X1 be the set of senses of t1 and X2 be the set of senses of t2 in O. Let S1, S2, ..., S|X1|\u00b7|X2| be the set of pairs of senses, Sk = (si, sj), with si \u2208 X1 and sj \u2208 X2. Now the semantic relatedness of T (SR(T, S, O)) is defined as:\nmaxSk{maxP {SCM(Sk, O, P ) \u00b7 SPE(Sk, O, P )}} = maxSk{SR(Sk, O)} for all k = 1..|X1| \u00b7 |X2|. Semantic relatedness between two terms t1, t2 where t1 \u2261 t2 \u2261 t and t /\u2208 O is defined as 1. Semantic relatedness between t1, t2 when t1 \u2208 O and t2 /\u2208 O, or vice versa, is considered 0.\nFor the remaining of the paper, the SR(T, S, O) for a pair of terms will be denoted as SR(T ), to ease readability."}, {"heading": "3.2 Analysis of the SR Measure", "text": "In this section we present the rationale behind the Definitions 1, 2, and 3, by providing theoretical and/or experimental evidence for the decisions made on the design of the measure. We illustrate the advantages and disadvantages of the different alternatives using simple examples and argue for our decisions. Finally, we discuss on the advantages of SR against previous measures of semantic relatedness.\nThe list of decisions made for the design of our semantic relatedness measure comprises: a) use of senses in all POS, instead of noun senses only, b) use of all semantic edge types found in WordNet, instead of the IS-A relation only, c) use of edge weights, and d) use of senses\u2019 depth as a scaling factor. It is important to mention that measures of semantic relatedness differ from the measures of semantic similarity, which traditionally use hierarchical relations only and ignore all other type of semantic relations. In addition, both concepts differentiate from semantic distance, in the sense that the latter is a metric."}, {"heading": "3.2.1 USE ALL POS INFORMATION", "text": "Firstly, we shall argue on the fact that the use of all POS in designing a semantic relatedness measure is important, and can increase the coverage of such a measure. The rationale supporting this decision is fairly simple. Current data sets for evaluating semantic relatedness or even semantic similarity measures are restricted to nouns, like for example the Rubenstein and Goodenough 65 word pairs (1965), the Miller and Charles 30 word pairs (1991), and the Word-Similarity-353 collection (Finkelstein et al., 2002). Thus, the experimental evaluation in those data sets cannot pinpoint the caveat of omitting the remaining parts of speech. However, text similarity tasks and their benchmark data sets comprise more than nouns. Throughout the following analysis, the reader must consider that the resulting measure of semantic relatedness among words is destined to be embedded in a text-to-text semantic relatedness, as shown in the next section.\nThe following two sentences are a paraphrase example taken from the Microsoft Paraphrase Corpus (Dolan, Quirk, & Brockett, 2004) and show the importance of using other POS as well, such as verbs:\n\u201cThe charges of espionage and aiding the enemy can carry the death penalty.\u201d\n\u201cIf convicted of the spying charges he could face the death penalty.\u201d\nWords that appear in WordNet are written in bold and stopwords have been omitted for simplicity5. The two sentences have many nouns in common (charges, death, penalty), but there are also pairs of words between these two sentences that can contribute the evidence that these two sentences are\n5. The stopwords list that we used is available at http://www.db-net.aueb.gr/gbt/resources/stopwords.txt\na paraphrase. For example espionage and spying have an obvious semantic relatedness, as well as enemy and spying. Also, convicted and charges, as well as convicted and penalty. This type of evidence would have been disregarded by any measure of semantic relatedness or similarity that uses only the noun POS hierarchy of WordNet. Examples of such measures are: the measure of Sussna (1993), Wu and Palmer (1994), Jiang and Conrath (1997), Resnik (1995, 1999), and the WordNet-based component of the measure proposed by Finkelstein et al. (2002). From this point of view, the decision to use all POS information expands the potential matches found by the measure and allows the use of the measure in more complicated tasks, like paraphrase recognition, text retrieval, and text classification."}, {"heading": "3.2.2 USE EVERY TYPE OF SEMANTIC RELATIONS", "text": "The decision to use all parts of speech in the construction of the semantic graphs, as it was introduced in our previous work (Tsatsaronis et al., 2007), imposes the involvement of all semantic relations instead of merely taxonomic (IS-A) ones. Moreover, this decision was based on evidence from related literature. The work of Smeaton et al. (1995) provides experimental evidence that measuring semantic similarity by incorporating non-hierarchical link types (i.e. part meronym/holonym, member meronym/holonym, substance meronym/holonym) improves much the performance of such a measure. The experimental evaluation was conducted by adopting a small variation of the Resnik\u2019s measure (1995).\nHirst and St-Onge (1998) reported that they have discovered several limitations and missing connections in the set of WordNet relations during the construction of lexical chains from sentences for the detection and correction of malapropisms. They provided the following example using the pair of words in bold to report this caveat:\n\u201cSchool administrators say these same taxpayers expect the schools to provide child care and school lunches, to integrate immigrants into the community, to offer special classes for adult students,.\u201d\nThe intrinsic connection between the nouns child care and school, which both exist in WordNet, cannot be discovered by considering only hierarchical edge types. This connection is depicted in Figure 2, which shows the path in WordNet. Our rich semantic representation is able to detect such connections and address problems of the aforementioned type."}, {"heading": "3.2.3 USE WEIGHTS ON EDGES", "text": "The work of Resnik (1999) reports that simple edge counting, which implicitly assumes that links in the taxonomy represent uniform distances, is problematic and is not the best semantic distance measure for WordNet. In a similar direction lie the findings of Sussna (1993), who has performed thorough experimental evaluation by varying edge weights in order to measure semantic distance between concepts. Sussna\u2019s findings, revealed that weights on semantic edges are a non-negligible factor in the application of his measure for WSD, and that the best results were reported when an edge weighting scheme was used, instead of assigning each edge the same weight. For all these reasons, we decided to assign a weight on every edge type, and we chose the simple probability of occurrence for each edge type in WordNet, as our edge weighting scheme (see Table 1). This very important factor is absent in several similarity measures proposed in the past, such as in the measures of Leacock et al. (1998), Jarmasz and Szpakowicz (2003) and Banerjee and Pedersen (2003), which are outperformed in experimental evaluation by our measure."}, {"heading": "3.2.4 USE DEPTH SCALING FACTOR", "text": "Our decision to incorporate the depth scaling factor (SPE in Definition 2) in the edge weighting mechanism has been inspired by the thorough experimental evaluation conducted by Sussna (1993),\nwhich has provided evidence on the importance of the edge weighting factor in semantic network based measures. According to our experiments on the Miller and Charles data set the Spearman correlation with human judgements was much lower (7 percentage points) when omitting the depth scaling factor than when adopting the SPE factor (see Definition 3)."}, {"heading": "3.2.5 JUSTIFICATION OF SR DEFINITIONS", "text": "According to Definition 1, the semantic compactness for a pair of concepts is the product of depthscaled weights of the edges connecting the two concepts. The use of product instead of sum or normalized sum of edges\u2019 weights is explained in the following.\nSince there might be several paths connecting the two concepts, Definition 3 clearly selects the path that maximizes the product of semantic compactness (SC) and semantic path elaboration (SPE). For simplicity, we ignore the effect of the depth scaling factor (SPE in Definition 2) and consequently, our aim is to find the path that maximizes\n\u220fl i=1 ei, where e1, e2, ..., el are the\n(non depth-scaled) weights of edges in the path connecting two given concepts. Let us name this less elaborate version of our semantic relatedness measure after product relatedness (PR), where PR(S, O) = maxP {SCM(S, O, P )}. An alternative would have been to define semantic compactness as the normalized sum of the weights in the path, which is: \u2211l\ni=1 ei l\n. In this case, the semantic relatedness would be measured on the path that maximizes the latter formula, since by nature, semantic relatedness always seeks to find the path that maximizes the connectivity between two concepts. Let us name this alternative after normalized weighted path length (NWPL).\nIn the example of Figure 3, we show how PR and NWPL compute the semantic relatedness for the term pair car and accelerator (left) and car and autobus (right). The path that maximizes the respective formulas of PR and NWPL using Algorithm 1 and edge weights in Table 1, is illustrated in Figure 3 using black and white arrows respectively. For the pair car and accelerator the sum-based formula, normalized against the path length, selects a very large path in this example, with a final computed relatedness of 0.61, which is the weight of the hypernym/hyponym edges. PR finds that the path maximizing the product is the immediate part meronym relation from car to accelerator, with a computed relatedness of 0.0367, which is the weight of the part meronym edges. The main problem arising with NWPL is the fact that it cannot distinguish among the relatedness between any pair of concepts in the hypernym/hyponym hierarchy of WordNet. In this example, NWPL computes the same relatedness (0.61) between every possible concept pair shown in the top figure. In contrast, PR is able to distinguish most of these pairs in terms of relatedness. More precisely, this behavior of PR is due to the fact that it embeds the notion of the path length, since the computed relatedness decays by a factor in the range (0, 1) for every hop made following any type of semantic relation. Another example, that also shows the importance of considering all WordNet relations, is the one shown on the right part of Figure 3, where NWPL and PR paths have been computed for the term pair car and autobus. Again, NWPL selects a very large path, and does not incline from the hypernym/hyponym tree.\nClearly, NWPL would rather traverse through a huge path of hypernym/hyponym edges, than following any other less important edge type, which would decrease its average path importance. This behavior creates serious drawbacks: (a) lack of ability to distinguish relatedness among any pair of concepts in the same hierarchy, and (b) large increase of the actual computational cost of Algorithm 1, due to the fact that it will tend not to incline from the hypernym/hyponym hierarchy, even if there is a direct semantic edge (other than hypernym/hyponym) connecting the two concepts,\nlike shown in Figure 3. Furthermore, by conducting experiments with NWPL in the 30 word pairs of Miller and Charles, we discovered that in almost 40% of the cases, NWPL produces the same value of semantic relatedness, equal to 0.61, being unable to distinguish them and creating many ties. Thus, PR is a better option to use in our measure, as the semantic compactness factor.\nLast, but not least, regarding the overall design of SR, we should mention that the proposed measure is solely based on the use of WordNet, in contrast to measures of semantic relatedness that use large corpora, such as Wikipedia. Although, such measures, like the ones proposed by Gabrilovich and Markovitch (2007), and Ponzetto and Strube (2007a), provide a larger coverage regarding concepts that do not reside in WordNet, they require the processing of a very large corpora (Wikipedia), which also changes very fast and very frequently. Experimental evaluation in Section 4 shows that our measure competes well against the aforementioned word-to-word relatedness measures in the used data sets. In the following section, we introduce Omiotis, the extension of SR for measuring text-to-text relatedness."}, {"heading": "3.3 Omiotis", "text": "To quantify the degree to which two text segments semantically relate to each other, we build upon the SR measure, which we significantly extend in order to account not only for the terms\u2019 semantic relatedness but also for their lexical similarity. This is because texts may contain overly-specialized terms (e.g., an algorithm\u2019s name) that are not represented in WordNet. Therefore, relying entirely on the term semantics for identifying the degree to which texts relate to each other would hamper the performance of our approach. On the other hand, semantics serve as complement to our relevance estimations given that different text terms might refer to (nearly-) identical concepts.\nTo quantify the lexical similarity between two texts, e.g., text A and B, we begin with the estimation of the terms\u2019 importance weights as these are determined by the standard TF-IDF weighting scheme (Salton, Buckley, & Yu, 1982).\nThereafter, we estimate the lexical relevance, denoted as \u03bba,b between terms a \u2208 A and b \u2208 B based on the harmonic mean of the respective terms\u2019 TF-IDF values, given by:\n\u03bba,b = 2 \u00b7 TF IDF (a, A) \u00b7 TF IDF (b, B)\nTF IDF (a, A) + TF IDF (b, B) (6)\nHarmonic mean is preferred instead of average, since it provides a more tight upper bound (Li, 2008). This decision is based on the fact that TF IDF (a, A) and TF IDF (b, B) are two different quantities measuring the qualitative strength of a and b in the respective texts.\nHaving computed the lexical relevance between text terms a and b, we estimate their semantic relatedness, i.e. SR(a, b) as described previously. Based on the estimated lexical relevance and semantic relatedness between pairs of text terms, our next step is to find for every word a in text A the corresponding word b in text B that maximizes the product of semantic relatedness and lexical similarity values as given by Equation 7.\nb\u2217 = arg max b\u2208B (\u03bba,b \u00b7 SR(a, b)) (7)\nWhere b\u2217 corresponds to that term in text B, which entails the maximum lexical similarity and semantic relatedness with term a from text A.6 In a similar manner, we define a\u2217, which corresponds\n6. The function argmax selects the case from the examined ones, that maximizes the input formula of the function.\nto that term in text A, which entails the maximum lexical similarity and semantic relatedness with term b from text B.\na\u2217 = arg max a\u2208A (\u03bba,b \u00b7 SR(a, b)) (8)\nConsequently, we aggregate the lexical and semantic relevance scores for all terms in text A, with reference to their best match in text B denoted as shown in Equation 9.\n\u03b6(A, B) = 1\n|A|\n(\n\u2211\na\u2208A\n\u03bba,b\u2217 \u00b7 SR(a, b\u2217)\n)\n(9)\nWe do the same for the opposite direction (i.e. from the words of B to the words of A) to cover the cases where the two texts do not have an equal number of terms.\nFinally, we derive the degree of relevance between texts A and B by combining the values estimated for their terms that entail the maximum lexical and semantic relevance to one another, given by:\nOmiotis(A, B) = [\u03b6(A, B) + \u03b6(B, A)]\n2 (10)\nAlgorithm 2 summarizes the computation of Omiotis. Its computation entails a series of steps, the complexity of which is discussed in Section 3.5."}, {"heading": "3.4 Applications of Semantic Relatedness", "text": "In this section we describe the methodology of incorporating semantic relatedness between pairs of words or pairs of text segments, into several applications."}, {"heading": "3.4.1 WORD-TO-WORD SIMILARITY", "text": "Rubenstein and Goodenough (1965) obtained synonymy judgements from 51 human subjects on 65 pairs of words, in an effort to investigate the relationship between similarity of context and similarity of meaning (synonymy). Since then, the idea of evaluating computational measures of semantic relatedness by comparing against human judgments on a given set of word pairs, has been widely used, and even more data sets were developed. The proposed measure of semantic relatedness between words (SR), introduced in Definition 4, can be used directly in such a task, in order to evaluate the basis of Omiotis measure, which is the measurement of word-to-word semantic relatedness. The application is straightforward: Let n be all pairs of words in the used word similarity data set. Then, the semantic relatedness for every pair is computed, using SR(T, S, O) as defined in 4. The computed values are sorted in a descending order, and the produced ranking of similarities is compared against the \u201dgold standard\u201d ranking of humans, using Spearman correlation. The scores can be used to compute Pearson\u2019s product moment correlation. Additional measures of semantic relatedness can be compared against each other by examining the respective correlation values with human judgements."}, {"heading": "3.4.2 SAT ANALOGY TESTS", "text": "The problem of identifying similarities in word analogies among pairs of words is a difficult problem and it has been standardized as a test for assessing the human ability for language understanding,\nAlgorithm 2 Omiotis(A,B, Sem, Lex )\n1: INPUT: Two texts A and B, comprising m and n terms each (a and b are terms from A and B respectively), a semantic relatedness measure Sem : SR(a, b) \u2192 (0..1), a weighting scheme of term importance in a text Lex : TF IDF (a, A) \u2192 (0..1) 2: OUTPUT: Find the pair of terms that maximizes the product of Sem and Lex values.\nCompute-Zeta(A,B) 3: sum(A) := 0 4: for all terms a \u2208 A do 5: b\u2217 := NULL 6: TempZeta := 0 7: for all terms b \u2208 B do 8: \u03bba,b = 2\u00b7Lex(a,A)\u00b7Lex(b,B) Lex(a,A)+Lex(b,B)\n9: if TempZeta < \u03bba,b \u00b7 Sem(a, b) then 10: TempZeta = \u03bbi,j \u00b7 Sem(a, b) 11: b\u2217 = b 12: end if 13: end for 14: sum(A) := sum(A) + TempZeta 15: end for 16: Zeta(A, B) := sum(A)/|A|\nCompute-Omiotis(A,B)\n17: Omiotis(A, B) := Zeta(A,B)+Zeta(B,A)2\nunder the scope of the well known SAT analogy tests (Scholastic Aptitude Tests). SAT tests are used as admission tests by universities and colleges in the United States. The participants\u2019 aim is to locate out of the five given word pairs the one that presents the most similar analogy to the target pair.\nAlthough it is difficult for machines to model the human cognition of word analogy, several approaches exist in the bibliography that attempt to tackle this problem. Previous approaches can be widely categorized into: corpus-based, lexicon-based and hybrid. Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006). Hybrid approaches are applied in SAT, through the application of the measures of Resnik (1995) and Lin (1998) that can also be found in the work of Turney (2006).\nIn order for the reader to understand the difficulty of answering SAT questions, we must point out that the average US college applicant scores 57% (Turney & Littman, 2005), while the top corpus-based approach scores 56.1% (Turney, 2006), the top lexicon-based scores 42% (Veale, 2004) and the top hybrid scores 33.2% (Resnik, 1995).\nAnother way of categorizing the approaches that measure semantic similarity in analogy tasks is to distinguish among attributional and relational similarity measures (Gentner, 1983).7 Representative approaches of the first category are lexicon-based approaches, while paradigms of relational similarity measures can be found in approaches based on Latent Relational Analysis (LRA) (Turney, 2006). It is of great interest to point out that LRA-based approaches, like the LRME algorithm proposed recently by Turney (2008a), are superior to attributional similarity approaches in discovering word analogies. This fact is also supported by the experimental findings of Turney (2006). Without doubt, relational similarity approaches may perform better in the SAT analogy task, but still, as shown later in the experiments we conducted in other applications, like paraphrase recognition, the lexicon-based measures can outperform LRA-based approaches in such tasks.\nSemantic relatedness (SR) between words, as applied in Omiotis, can be exploited to solve the word analogy task. The aim of word analogy is, given a pair of words w1 and w2, to identify the series of semantic relations that lead from w1 to w2 (semantic path). In the SAT test, the target pair (w1,w2) and candidate word pairs (w1k,w2k), with k usually being from 1 to 5, are processed in order to find each pair\u2019s analogy. The aim is to locate the pair k, which exposes maximum similarity to w1 and w2. A straightforward method to choose among the 5 candidate pairs is to employ two criteria: At first, the k analogies to the analogy of the target pair can be compared, and then the candidate that shows by far the most similar analogy can be selected. However, when the most similar analogy is not obvious, all the 6 pairs may be examined together in order for the slightest differences that lead to the correct answer to be discovered. We attempt to model human cognition of this task using SR in a two fold manner: (a) we measure SR to capture the horizontal analogy between the given pair and the possible candidate pairs, and (b) we measure SR to capture the vertical analogy between the given pair and the possible candidate pairs. These two aspects are covered by the following Equations 11 to 13. To capture the horizontal analogy between a pair of words and a candidate pair, we measure the difference of the SR score between the two words respectively as shown:\ns1(w1k, w2k) = 1 \u2212 |SR(w1, w2) \u2212 SR(w1k, w2k)| (11)\nEssentially, s1 expresses the horizontal analogy of the candidate pair (w1k, w2k) with the given pair (w1, w2). Similarly, we capture the notion of the vertical analogy between the two pairs by computing the difference of the SR scores among the two pairs\u2019 words, as follows:\ns2(w1k, w2k) = 1 \u2212 |SR(w1, w1k) \u2212 SR(w2, w2k)| (12)\nFinally, we rank candidates depending on the combined vertical and horizontal analogy they have with the given pair, according to the following equation:\ns(w1k, w2k) = s1(w1k, w2k) + s2(w1k, w2k)\n2 (13)\nEventually, we select the candidate pair with the maximum combined score, taking into account both aspects (horizontal and vertical) of analogy between the given and the candidate pairs.\nThe intuition behind the selection of the these two scores for handling the SAT test, is the following. The order of the words in the pairs (both target and candidates) is not random. Usually, given a pair (w1, w2), and the candidate pairs (w1k, w2k) the test is solved if one can successfully\n7. Two objects, X and Y, are attributionally similar when the attributes of X are similar to the attributes of Y. Two pairs, A:B and C:D, are relationally similar when the relations between A and B are similar to the relations between C and D.\nfind the analogy: w1k is to w2k what w1 is to w2. From this perspective, s1 and s2 try to find the candidate pair that best aligns with the target pair. Figure 4 illustrates these two types of analogies (horizontal and vertical) for an example SAT question.\nIn order to motivate more our selection of s1 and s2 for answering SAT questions, we will discuss in more detail how these two quantities pertain to the concepts of strength and type of the relations between a pair of SAT words. Turney (2006) describes a method for comparing the relations between candidate word pairs and the stem word pair, in which he utilizes the type of the relation connecting the words in each pair and finally selects the pair that best matches the type of the relation connecting the words in the stem pair. Though we do not explicitly examine the label of the edges connecting the words in each pair, implicitly we do so by computing SR between them. Since our weighting of the WordNet edges is fine grained, and distinguishes every type of semantic relation in WordNet, instead of labels, we are using edge weights. SR definition can provide a fine grained distinguishment between two pairs of words, depending on the types of the edges connecting the words respectively, which is expressed by their weights, and also taking into account other factors, like the depth of the nodes comprising their connecting path inside the thesaurus. Besides s1, which attempts to capture the aforementioned properties between word pairs, s2 attempts the same between the words of the same order among two word pairs (i.e., the first word from the first pair, with the second word from the second pair). This forms an attempt to capture how aligned are two word pairs, according to their SR values between their words."}, {"heading": "3.4.3 PARAPHRASE RECOGNITION AND SENTENCE-TO-SENTENCE SIMILARITY", "text": "Performance of applications relying on natural language processing may suffer from the fact that the processed documents might contain lexically different, yet semantically related, text segments. The task of recognizing synonym text segments, which is better known as paraphrase recognition, or detection, is challenging and difficult to solve, as shown in the work of Pasca (2005). The task itself is important for many text related applications, like summarization (Hirao, Fukusima, Oku-\nmura, Nobata, & Nanba, 2005), information extraction (Shinyama & Sekine, 2003) and question answering (Pasca, 2003). We experimentally evaluate the application of Omiotis in the paraphrasing detection task (Section 4.2), using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The application of Omiotis in paraphrase detection is straightforward: given a pair of text segments, we compute the Omiotis score between them, using Equation 10 and Algorithm 2. Higher values of Omiotis for a given pair denote stronger semantic relation between the examined text segments. The task is now reduced to define a threshold, above which an Omiotis value can be considered as a determining sign of a paraphrasing pair. In the experimental evaluation of Omiotis, we explain in detail how we have selected this threshold for the paraphrase recognition task.\nIn a similar manner, by using Equation 10 and Algorithm 2, the semantic relatedness scores for pairs of sentences can be computed. For this task, we are using the data set of Li et al. (2006) to evaluate Omiotis, comprising 30 sentence pairs, for which human scores are provided. In Section 4 we describe in detail the experimental set up."}, {"heading": "3.5 Complexity and Implementation Issues", "text": "The computation of Omiotis entails a series of steps, the complexity of which is strongly related to its base measure of Semantic Relatedness (SR). Primarily, given two words, w1 and w2 the construction time of the semantic network used to compute SR according to Algorithm 1, has been proven to be O(2 \u00b7 kl+1) (Tsatsaronis et al., 2007), where k is the maximum branching factor of the used thesaurus nodes and l is the maximum semantic path length in the thesaurus. Once the semantic network is constructed, the complexity of Algorithm 1 is reduced to the standard time complexity cost of Dijkstra\u2019s algorithm. Using Fibonacci heaps, it is possible to alleviate the computational burden of Dijkstra and further improve time complexity. In the semantic network, Dijkstra takes O(nL + mD + nE), where n is the number of nodes in the network, m the number of edges, L is the time for insert, D the time for decrease-key and E the time for extract-min. If Fibonacci heaps are used then L = D = O(1) and the cost of extract-min is O(logn), thus significantly reducing the cost of execution. This whole procedure is repeated 2 \u00d7 n1 \u00d7 n2 times for the computation of Omiotis between two documents d1 and d2 having in total n1 and n2 distinct words respectively.\nFrom the aforementioned, it is obvious that the computation of Omiotis is not cheap in general. For this purpose, and in order to improve the system\u2019s scalability, we have pre-computed and stored all SR values between every possible pair of synsets in a RDBMS. This is a one-time computation cost, which dramatically decreases the computational complexity of Omiotis. The database schema has three entities, namely Node, Edge and Paths. Node contains all WordNet synsets. Edge indexes all edges of the WordNet graph adding weight information for each edge computed using the SR measure. Finally, Paths contains all pairs of WordNet synsets that are directly or indirectly connected in the WordNet graph and the computed relatedness. These pairs were found by running a Breadth First Search (BFS) starting from all WordNet roots for all POS. Table 2 provides statistical information for the RDBMS which exceeds 220 Gbytes in size. Column 1 indicates the number of distinct synsets examined, column 2 shows the total number of the edges, and column 3 depicts the number of the connected synsets (by at least one path following the offered WordNet edges). The current implementation takes advantage of the database structures (indices, stored procedures etc) in order to decrease the computational complexity of Omiotis. The following example is indicative of the complexity of SR computation. The average number of senses per term is between 5 and 7 (depending on the POS). For a pair of terms of known POS, we perform n 2\n2 (n \u2243 6) combinations\nand for each pair of synsets we compute the similarity as presented in Definition 3. When these similarities are pre-computed, the time required for processing 100 pairs of terms is \u2243 1 sec, which makes the computation of Omiotis feasible and scalable. As a proof of concept, we have developed an on-line version of the SR and the Omiotis measures8, where the user can test the term-to-term and sentence-to-sentence semantic relatedness measures (Tsatsaronis et al., 2009)."}, {"heading": "4. Experimental Evaluation", "text": "The experimental evaluation of Omiotis is two-fold. First, we test the performance of the wordto-word semantic relatedness measure (SR), in which Omiotis is based, in three types of tasks: (a) word-to-word similarity and relatedness, (b) synonym identification, and (c) Scholastic Aptitude Test (SAT). Second, we evaluate the performance of Omiotis in two tasks: (a) sentence-to-sentence similarity, and (b) the paraphrase recognition task."}, {"heading": "4.1 Evaluation of the Semantic Relatedness (SR) Measure", "text": "For the evaluation of the proposed semantic relatedness measure between two terms we experimented on three different categories of tests. The first category comprises data sets that contain word pairs, for which human subjects have provided similarity scores or relatedness scores. The provided scores create a ranking of the word pairs, from the most similar to the most irrelevant. We evaluate the performance of measures, by computing the correlation between the list of the human rankings and the list produced by the measures. In this task, we evaluate the performance of SR in three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (1965) (R&G), and the Miller and Charles 30 word pairs (1991) (M&C), for which humans have provided similarity scores, and, also, in the Word-Similarity-353 collection (Finkelstein et al., 2002) (353-C), which comprises 353 word pairs, for which humans have provided relatedness scores.\nThe second category of experiments comprises synonym identification tests. In these tests, given an initial word, the most appropriate synonym word must be identified among the given options. In this task we evaluate the performance of SR in the TOEFL data set, comprising 80 multiple choice synonym questions, and the ESL data set, comprising 50 multiple choice synonym questions questions.9\nThe third category of experiments is based on the Scholastic Aptitude Test (SAT) questions. In SAT, given a pair of words, the most relevant pair among five other given pairs must be selected. This task is based on word analogy identification. The evaluation data set comprises 374 test questions.\n8. Publicly available at http://omiotis.hua.gr 9. http://www.aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questions http://www.aclweb.org/aclwiki/index.php?title=ESL_Synonym_Questions_(State_of_the_art)"}, {"heading": "4.1.1 EVALUATION ON SEMANTIC SIMILARITY AND RELATEDNESS", "text": "For the first category of experiments, we compared our measure against ten known measures of semantic relatedness: Hirst and St-Onge (1998) (HS), Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS), Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al. (2002) (LSA), Hughes and Ramage (2007) (HR), and Strube and Ponzetto (2006, 2007a) (SP). For the measure of Strube and Ponzetto we have also included the results of a version of the measure that is only based on IS-A relations (Ponzetto & Strube, 2007b) (IS-A SP). For each measure, including our own measure (SR), we have computed both the Spearman rank order correlation coefficient (\u03c1) and the Pearson product-moment correlation coefficient (r), with \u03c1 being derived from r, since for the computation of \u03c1 the relatedness scores are transformed into rankings. Both correlation coefficients are computed based on the relatedness scores and rankings provided by humans in all three data sets (the relatedness scores create a ranking of the pairs of words, based on their similarity). For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007). For the WLM measure the \u03c1 values are given in the work of Milne and Witten (2008). For the LSA method the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007), only for the 353-C data set. For the HR measure the \u03c1 values are given in the work of Hughes and Ramage (2007). Finally, for the SP measure the r values are given in the work of Ponzetto and Strube (2007a), and for the IS-A SP are given in the work of Ponzetto and Strube (2007b).\nIn Table 3 we show the values of \u03c1 and r for the R&G and the M&C data sets and for SR and the compared measures. The human scores for all pairs of words for the two data sets can be found in the analysis of Budanitsky and Hirst (2006). Note that the M&C data set is a subset of the R&G data set. In some cases, the computation of \u03c1 or r was not feasible, due to missing information regarding the detailed rankings or relatedness scores for the respective measures. In these cases the table has the entry N/A. Also the LSA measure is omitted in this table because \u03c1 and r were not reported in the literature for these two data sets. We have also conducted a statistical significance test on the difference between SR correlations and the respective correlations of the compared measures, using Fisher\u2019s z-transformation (Fisher, 1915). For each reported number, the symbol \u00a7 indicates that the difference between the correlation produced by SR and the respective measure is statistically significant at the 0.99 confidence level (p < 0.01). The symbol \u2021 indicates the same at the 0.95 confidence level (p < 0.05) and, finally, the symbol \u2020 indicates statistical significance of the correlations\u2019 difference at the 0.90 confidence level (p < 0.10). In cases when the difference is not statistically significant in any of those confidence levels, there is no indicating symbol.\nIn Table 4 we show the values of \u03c1 and r for the 353-C data set. The reason we present the results of the experiments in the 353-C data set in another table than the respective results of the R&B and M&C data sets is that this collection focuses on the concept of semantic relatedness, rather than on the concept of semantic similarity (Gabrilovich & Markovitch, 2007). Relatedness is more general concept than similarity, as argued in the analysis of Budanitsky and Hirst (2006). Thus, it can be argued that the humans in the 353-C thought differently when scoring, compared to the case of the R&B and M&C data sets. The detailed human scores for the 353-C data set are made available with the collection10. The measures L, JC and HS are omitted, because no information was available for computing \u03c1 or r values. As a further remark regarding the 353-C collection, we need to add the fact that there are cases where the inter-judge correlations may fall below 65%, while R&B and M&C data sets have inter-judge correlations between 0.88 and 0.95. Again, statistical significance tests have been conducted using the Fisher\u2019s z-transformation, regarding the difference of SR correlations and the correlations of the compared measures. The used symbols that indicate the level of the statistical significance are the same as previously. With regards to the reported correlations for the R&G and M&C data sets, it is shown that SR performs very well, since in the majority of the cases SR has higher correlation compared to the other measures of semantic relatedness or similarity of any category (knowledge-based, corpus-based or hybrid). In the R&G data set SR reports the highest \u03c1 and r correlations. In the M&C data set SR has the second highest \u03c1 correlation. The HR measure has the highest \u03c1 correlation, but in the R&G and 353-C SR outperforms HR. The differences between SR and HR are not statistically significant in any of the two examined data sets. Also, in the M&C data set SR has the second r correlation with the JS reporting the highest, but JS is outperformed by SR in the R&G and 353-C data sets. In the case of the M&C data set, the difference between SR and JS is not statistically significant, but SR outperforms JS in the R&G and the 353-C data sets, with statistically significant difference in the reported correlations. Another important conclusion from the results, is the fact that the IS-A SP measure performs better than the SP measure. This is mainly due to the fact that for the computation of the similarity values in such data sets, the inclusion of only IS-A relations is much more reasonable (Ponzetto & Strube, 2007b). The differences in their results (SP and IS-A SP) motivate even more our SR measure, since we\n10. http://www.cs.technion.ac.il/\u02dcgabr/resources/data/wordsim353/\ntake the best of both worlds, i.e., we weigh IS-A relations high, and fall back to other relations if necessary.\nRegarding the 353-C data set, the results in Table 4 show that SR again performs well, with the top performers being the Wikipedia-based approaches (Gabrilovich & Markovitch, 2009; Milne & Witten, 2008). The difference between them is statistically significant, but we should note that SR outperforms both GM and WLM in the R&G and M&C data sets, with statistically significant difference as well. Partly, this difference in the performance of SR compared to GM and WLM can be explained as follows: the GM measure considers words in context (Gabrilovich & Markovitch, 2009), and thus inherently performs word sense disambiguation; in contrast, SR takes as input a pair of words, lacks context, and is based only on the information existing in WordNet, which, especially for several of the cases in the 353-C data set, creates a disadvantage (e.g., in the word pair Arafat and Jackson, there are 11 different entries for the second word in WordNet). The same holds for the WLM measure. Another reason for this difference in performance is the coverage of WordNet. In several cases, one or both of the two words in the 353-C data set comprising a pair, do not exist in WordNet (e.g., the football player Maradona). However, as expected, and also shown in the experimental analysis of Omiotis that follows, when context is considered, the proposed semantic relatedness measure performs better (the reader may wish to consult Table 9, where for a subset of the R&G data set that contains the full definitions of the words, the correlations of Omiotis with the human judgements are the top found among the compared approaches).\nTo visualize the performance of our measure in a more comprehensible manner, we also present in Figure 5 the relatedness values given by humans for all pairs in the R&G and M&C data sets, in increasing order of value (left side) and the respective values for these pairs produced using SR (right side). Note that the x-axis in both charts begins from the least related pair of terms, according to humans, and continues up to the most related pair of terms. The y-axis in the left chart is the respective humans\u2019 rating for each pair of terms. The right figure shows SR for each pair. A closer look on Figure 5 reveals that the values produced by SR (right figure) follow a pattern similar to that of the human ratings (left figure)."}, {"heading": "4.1.2 EVALUATION ON SYNONYM IDENTIFICATION", "text": "For the synonym identification task we are using the TOEFL 80 questions data set and the ESL 50 questions data set. For the TOEFL data set we compare with several other methods. More specifically, we examine: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L), Jiang and Conrath (1997) (JC), and Turney et al. (2003) (PR); and a Web-based method by RuizCasado et al. (2005) (RC). We also report the results of random guessing (RG) and the performance of the average college applicant (H). Table 5 shows the results on the 80 TOEFL questions. The table reports the number of the correct and the respective percentage given by all measures. In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990). As in the previous tables, the symbol \u00a7 indicates statistically significant difference at the 0.99 confidence level, \u2021 at the 0.95 confidence level, and \u2020 at the 0.90 confidence level. The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with the lexicon-based methods, SR reports better results, statistically significant at the confidence levels indicated.\nIn a similar manner, we have conducted experiments in the ESL 50 questions data set, and compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney (2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),\nLin (1998) (L), and Jiang and Conrath (1997) (JC). We report the results, together with random guessing, in Table 6. The results of Table 6 show that SR ranks first, having the same performance with JS in this data set, both outperforming all of the compared corpus-based methods. These\nresults are very interesting, since they indicate that lexicon-based methods are very promising in the synonym identification tasks."}, {"heading": "4.1.3 EVALUATION ON SAT ANALOGY QUESTIONS", "text": "The approach that we choose to evaluate SR in the analogy task is to use the typical benchmark test set employed in the related bibliography, namely the Scholastic Aptitude Test (SAT).11 It comprises of 374 words pairs and for each target pair 5 supplementary pairs of words. The average US college applicant answered correctly only the 57 percent of the questions, and no machine-based approach has yet surpassed the performance of the average college applicant.\nIn Table 7, we present the number of correct answers and the respective percentage (recall) on the 374 SAT questions, of the following methods: random guessing (RG), Jiang and Conrath (1997) (JC), Lin (1998) (L), Leacock et al. (1998) (LC), Hirst and St-Onge (1998) (HS), Resnik (1995) (R), Bollegala et al. (2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) and LRA (Turney, 2006). Furthermore, we present the results of s1 (Equation 11), s2 (Equation 12) and s (Equation 13). We also present, as before, the statistical significance of the differences in performance, conducting Fisher\u2019s exact test.\nTowards the direction of combining the answers of s1 and s2 in a different manner than the naive average, we also report the upper bound performance of such an attempt. This is computed by simply finding the union of the correct answers that s1 and s2 may provide. This is reported in the table as (UB). In an effort to design a learning mechanism that would learn when to select\n11. Many thanks to Peter Turney, for providing us with a standard set for experimentation, comprising of 374 SAT questions.\ns1 or s2 answers for each SAT question, with the goal to reach our upper-bound, we designed and implemented a simple representation of the SAT questions as training instances. For each SAT question, we created a training instance that has 6 features: the minimum s1 value found for this question (among the five computed values for all the possible pairs), the maximum s1 value, and their difference. We also added the same features regarding s2. We then trained and tested a Naive Bayes classifier using ten-fold cross validation in the 374 SAT questions. The results of this experiment are shown in the table as NB (Naive Bayes). Finally, we also present the top results ever reported in the literature for the specific data set, which is the LRA method by Turney (2006). This is reported in the table as (LRA).\nThe results presented in Table 7 show that S ranks second among the compared lexicon-based measures with the first being the measure of Veale (2004) (V). The method of Bollegala et al. (2008) (B) achieves higher score than SR, but needs training in SAT questions. At this point we have to note that the LRA method needs almost 8 days to process the 374 SAT questions (Turney, 2006), (B) needs around 6 hours (Bollegala et al., 2008), while S needs less than 3 minutes.\nFurthermore, the fact that combining s1 and s2 can reach 52.4% shows that S can produce very promising results, if a classifier learns successfully how to combine them. The NB results, which are a simple attempt to construct such a learner with few features, shows an important boost in performance of 4.1%. A proper feature engineering in the task, and more training SAT questions can potentially yield more promising results, as the gap between 38.1% and the upper bound of 52.4% is still large. In all, these results prove that our lexicon-based relatedness measure has a comparable performance to the state of the art measures for the SAT task, while it has smaller execution time than the majority of the methods which outperform it in recall."}, {"heading": "4.2 Evaluation of the Omiotis Measure", "text": "In order to evaluate the performance of the Omiotis measure, we performed two experiments which test the ability of the measure to capture the similarity between sentences. The first experiment is based on the data set produced by Li et al. (2006). The second experiment is based on the paraphrase recognition task, using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004)."}, {"heading": "4.2.1 EVALUATION ON SENTENCE SIMILARITY", "text": "The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set). The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001). For each sentence pair, similarity scores have been provided by 32 human participants, ranging from 0.0 (the sentences are unrelated in meaning), to 4.0 (the sentences are identical in meaning).12.\nFrom the 65 sentence pairs, Li et al. (2006) decided to keep a subset of 30 sentence pairs, similarly to the process applied by Miller and Charles (1991), in order to retain the sentences whose human ratings create a more even distribution across the similarity range. Thus, we apply Omiotis in this same subset of the 65 sentence pairs, described by Li et al. (2006). In this data set, we compare Omiotis against the STASIS measure of semantic similarity, proposed by Li et al. (2006), an LSA-based approach described by O\u2019Shea et al. (2008), and the STS measure proposed by Islam and Inkpen (2008). To the best of our knowledge, this data set has only been used by these three\n12. The data set is publicly available at http://www.docm.mmu.ac.uk/STAFF/J.Oshea/\nprevious works. In Table 8 we present the sentence pairs used, and the respective scores by humans, STASIS, LSA, STS, and Omiotis.\nIn Table 9 we present the results of the comparison, comprising the reporting of the Spearman\u2019s rank order correlation coefficient \u03c1 and the Pearson\u2019s product moment correlation coefficient r for STASIS, LSA, STS, and Omiotis. We have also included in the results, a version of Omiotis that does not take into account the inter-POS relations (i.e., relations that cross parts of speech). This version of Omiotis is indicated in the table as SimpleOmiotis. The objective of this experiment was to measure the contribution of the relations that cross parts of speech in the computation of text-to-\ntext semantic relatedness values, though these types of relations have been reported in the previous bibliography as advantageous (Jarmasz, 2003; Jarmasz & Szpakowicz, 2003), but their individual contribution had never been measured.\nWe also show the r correlation between the average participant (mean of individuals with group; n = 32, leave-one-out resampling and standard deviation 0.072), the worst participant (worst participant with group; n = 32, leave-one-out resampling) and the best participant (best participant with group; n = 32, leave-one-out resampling), taken from the work of O\u2019Shea et al. (2008). In addition, we have also conducted a z-test regarding the difference between Omiotis correlations and the compared measures\u2019 correlations. The symbols used in the previous tables indicate the confidence level of the statistical significance. Note, also, that the reported correlations (STASIS, LSA, STS, and Omiotis) individually constitute statistically significant positive correlations with the human scores (r) and rankings (\u03c1). As the results indicate, Omiotis has the best correlation, according to \u03c1 and r values, compared to STASIS, LSA, and STS. Furthermore, the contribution of the semantic relations that cross parts of speech is obvious, since the difference between the simple version of Omiotis that omits them and the defined Omiotis measure is large and statistically significant at the 0.99 confidence level. Overall, the results indicate that Omiotis can be applied successfully to the computation of similarities between small text segments, like sentences."}, {"heading": "4.2.2 EVALUATION ON PARAPHRASE RECOGNITION", "text": "In order to further evaluate the performance of Omiotis in measuring the semantic relatedness between small text segments, we conducted additional experiments on the paraphrase recognition task using the test pairs of the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). From the original data set, containing both training and test pairs, we run experiments only on the 1725 test pairs of text segments, which have been collected from news sources on the Web over a period of 18 months. For each pair, human subjects have determined whether any of the two texts in the pair consists a paraphrase of the other (direction is not an issue). The reported inter-judge agreement between annotators is 83%. The paraphrase recognition task has been widely studied in the past, since it is very important in many natural language applications, like question answering (Harabagiu\n& Hickl, 2006), and text summarization (Madnani, Zajic, Dorr, Fazil Ayan, & Lin, 2007). For this task we computed Omiotis between the sentences of every pair and marked as paraphrases only those pairs with Omiotis value greater than a threshold. The threshold was set to 0.2, after tuning in the training set. We used a simple approach for the tuning, namely forward hill-climbing and beam search (Guyon, Gunn, Nikravesh, & Zadeh, 2006).\nWe compare the performance of Omiotis against several other methods of various categories; more precisely, against: (a) two baseline methods, a random selection method that marks randomly each pair as being paraphrase of not (Random), and a vector-based similarity measure, using the cosine similarity measure and TF-IDF weighting for the features (VSM and Cosine) 13, (b) corpusbased methods; the PMI-IR proposed by Turney (2001), an LSA-based approach introduced by Mihalcea et al. (2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al. (2006) (Comb.), and (d) machine-learning based techniques, which also constitute the state of the art in paraphrase recognition, like the method of Wan et al. (2006), which trains a classifier with lexical and dependency similarity measures, the method of Zhang and Patrick (2005), who also build a feature vector with lexical similarities between the sentence pairs (e.g., edit distance, number of common words), the method of Qiu et al.\n13. The features are all words of the used data set.\n(2006), who use an SVM classifier (Vapnik, 1995) to decide whether or not a set of features for each sentence that has been created by parsing and semantic role labelling matches or not the respective set of the second sentence in the pair, and with what importance, and, finally, the method of Finch et al. (2005), who also train an SVM classifier based on machine translation evaluation metrics.\nThe results of the evaluation are shown in Table 10. The results indicate that Omiotis surpasses all the lexicon-based methods, and matches the combined method of Mihalcea et al. (2006). At this point we must mention that we also tuned Omiotis with a goal to maximize F-Measure in the test set, at the cost of dropping precision in favor of recall. This type of tuning reported an FMeasure of 81.7, which is larger than the F-Measures of the lexicon-based, the corpus-based and two of the machine learning-based approaches. Even though the reported results used a different and simpler tuning explained previously, still the results indicate that Omiotis manages very well in the paraphrase recognition task and produces comparable results with the state of the art. We believe that it can be used as part of a machine learning-based method, since it is one of the best choices in lexicon-based methods for paraphrase recognition, and this also constitutes part of our plan for future work in this application."}, {"heading": "5. Conclusions and Future Work", "text": "In this paper we presented a new measure of text semantic relatedness. The major strength of this measure lies in the formulation of the semantic relatedness between words. Experimental evaluation, proved that our measure approximates human understanding of semantic relatedness between words better than previous related measures. The combination of path length, nodes\u2019 depth and edges\u2019 type in a single formula allowed us to apply our semantic relatedness measure to different text-based tasks with very good performance. More specifically, the SR measure outperformed overall in the used data sets all state of the art measures in word-to-word tasks and the Omiotis measure performed very well in the sentence similarity and the paraphrase recognition tasks. Although, the results in the word analogy task are satisfactory, since no special tuning has been performed, we are confident that there is still place for improvement. The extensive evaluation of SR and Omiotis in several applications shows the capabilities of our measures and proves that both can be applied to several text related tasks. It is on our next plans to apply our relatedness measures to more applications, such as text classification and clustering, keyword and sentence extraction, and query expansion, and compare with state of the art techniques in each field. Finally, we are improving our supporting infrastructure in order to facilitate large scale tasks such as document clustering and text retrieval."}, {"heading": "Acknowledgments", "text": "Part of this work was done while George Tsatsaronis was at the Department of Informatics of Athens University of Economics and Business. We would like to thank Kjetil N\u00f8rva\u030ag for his constructive comments, and Ion Androutsopoulos for his feedback on the early stage of this work. We would also like to thank the anonymous reviewers for their detailed feedback."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pasca", "A. Soroa"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "A proposal for word sense disambiguation using conceptual distance", "author": ["E. Agirre", "G. Rigau"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP)", "citeRegEx": "Agirre and Rigau,? \\Q1995\\E", "shortCiteRegEx": "Agirre and Rigau", "year": 1995}, {"title": "Categorical Data Analysis", "author": ["A. Agresti"], "venue": "Wiley, Hoboken, NJ.", "citeRegEx": "Agresti,? 1990", "shortCiteRegEx": "Agresti", "year": 1990}, {"title": "An information-theoretic perspective of TF-IDF measures", "author": ["A. Aizawa"], "venue": "Information Processing and Management, 39(1), 45\u201365.", "citeRegEx": "Aizawa,? 2003", "shortCiteRegEx": "Aizawa", "year": 2003}, {"title": "Extended gloss overlaps as a measure of semantic relatedness", "author": ["S. Banerjee", "T. Pedersen"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Banerjee and Pedersen,? \\Q2003\\E", "shortCiteRegEx": "Banerjee and Pedersen", "year": 2003}, {"title": "Using lexical chains for text summarization", "author": ["R. Barzilay", "M. Elhadad"], "venue": "In Proceedings of the ACL 97/EACL 97 Workshop on Intelligent Scalable Text Summarization,", "citeRegEx": "Barzilay and Elhadad,? \\Q1997\\E", "shortCiteRegEx": "Barzilay and Elhadad", "year": 1997}, {"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["R. Barzilay", "M. Elhadad", "K. McKeown"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barzilay et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2002}, {"title": "A semantic kernel to exploit linguistic knowledge", "author": ["R. Basili", "M. Cammisa", "A. Moschitti"], "venue": "In Proceedings of Advances in Artificial Intelligence, Ninth Congress of the Italian Association for Artificial Intelligence (AI*IA),", "citeRegEx": "Basili et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Basili et al\\.", "year": 2005}, {"title": "Clustering word pairs to answer analogy questions", "author": ["E. Bicici", "D. Yuret"], "venue": "In Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks", "citeRegEx": "Bicici and Yuret,? \\Q2006\\E", "shortCiteRegEx": "Bicici and Yuret", "year": 2006}, {"title": "WWW sits the sat: Measuring relational similarity from the web", "author": ["D. Bollegala", "Y. Matsuo", "M. Ishizuka"], "venue": "In Proceedings of the Eighteenth European Conference on Artificial Intelligence (ECAI),", "citeRegEx": "Bollegala et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2008}, {"title": "Evaluating wordnet-based measures of lexical semantic relatedness", "author": ["A. Budanitsky", "G. Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Budanitsky and Hirst,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky and Hirst", "year": 2006}, {"title": "Cross-language information retrieval using EuroWordNet and word sense disambiguation", "author": ["P. Clough", "M. Stevenson"], "venue": "In Proceedings of the Twenty Sixth European Conference on Information Retrieval (ECIR),", "citeRegEx": "Clough and Stevenson,? \\Q2004\\E", "shortCiteRegEx": "Clough and Stevenson", "year": 2004}, {"title": "Introduction to Algorithms", "author": ["T. Cormen", "C. Leiserson", "R. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1990}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["W. Dolan", "C. Quirk", "C. Brockett"], "venue": "In Proceedings of the Twentieth International Conference on Computational Linguistics (COLING)", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "WordNet \u2013 an electronic lexical database", "author": ["C. Fellbaum"], "venue": "MIT Press.", "citeRegEx": "Fellbaum,? 1998", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Using machine translation evaluation techniques to determine sentence-level semantic equivalence", "author": ["A. Finch", "Y. Hwang", "E. Sumita"], "venue": "In Proceedings of the 3rd International Workshop on Paraphrasing,", "citeRegEx": "Finch et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finch et al\\.", "year": 2005}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Finkelstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Frequency distribution of the values of the correlation coefficient in samples of an indefinitely large population", "author": ["R. Fisher"], "venue": "Biometrika, 10, 507\u2013521.", "citeRegEx": "Fisher,? 1915", "shortCiteRegEx": "Fisher", "year": 1915}, {"title": "Computing semantic relatedness using Wikipedia-based explicit semantic analysis", "author": ["E. Gabrilovich", "R. Markovitch"], "venue": "In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Gabrilovich and Markovitch,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich and Markovitch", "year": 2007}, {"title": "Wikipedia-based semantic interpretation for natural language processing", "author": ["E. Gabrilovich", "R. Markovitch"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gabrilovich and Markovitch,? \\Q2009\\E", "shortCiteRegEx": "Gabrilovich and Markovitch", "year": 2009}, {"title": "Structure-mapping: A theoretical framework for analogy", "author": ["D. Gentner"], "venue": "Cognitive Science, 7(2), 155\u2013170.", "citeRegEx": "Gentner,? 1983", "shortCiteRegEx": "Gentner", "year": 1983}, {"title": "Feature Extraction, Foundations and Applications", "author": ["I. Guyon", "S. Gunn", "M. Nikravesh", "L. Zadeh"], "venue": null, "citeRegEx": "Guyon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2006}, {"title": "Methods for using textual entailment in open-domain question answering", "author": ["S. Harabagiu", "A. Hickl"], "venue": "In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL),", "citeRegEx": "Harabagiu and Hickl,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu and Hickl", "year": 2006}, {"title": "Corpus and evaluation measures for multiple document summarization with multiple sources", "author": ["T. Hirao", "T. Fukusima", "M. Okumura", "C. Nobata", "H. Nanba"], "venue": "In Proceedings of the Twentieth International Conference on Computational Linguistics (COLING),", "citeRegEx": "Hirao et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hirao et al\\.", "year": 2005}, {"title": "Lexical chains as representations of context for the detection and correction of malapropisms", "author": ["G. Hirst", "D. St-Onge"], "venue": "In WordNet: An Electronic Lexical Database,", "citeRegEx": "Hirst and St.Onge,? \\Q1998\\E", "shortCiteRegEx": "Hirst and St.Onge", "year": 1998}, {"title": "Lexical semantic relatedness with random graph walks", "author": ["T. Hughes", "D. Ramage"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Hughes and Ramage,? \\Q2007\\E", "shortCiteRegEx": "Hughes and Ramage", "year": 2007}, {"title": "Word Sense Disambiguation: The State of the Art", "author": ["N. Ide", "J. Veronis"], "venue": "Computational Linguistics,", "citeRegEx": "Ide and Veronis,? \\Q1998\\E", "shortCiteRegEx": "Ide and Veronis", "year": 1998}, {"title": "Semantic text similarity using corpus-based word similarity and string similarity", "author": ["A. Islam", "D. Inkpen"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "Islam and Inkpen,? \\Q2008\\E", "shortCiteRegEx": "Islam and Inkpen", "year": 2008}, {"title": "\u00c9tude comparative de la distribution florale dans une portion des alpes et des jura", "author": ["P. Jaccard"], "venue": "Bulletin del la Socie\u0301te\u0301 Vaudoise des Sciences Naturelles,", "citeRegEx": "Jaccard,? \\Q1901\\E", "shortCiteRegEx": "Jaccard", "year": 1901}, {"title": "Roget\u2019s thesaurus and semantic similarity", "author": ["M. Jarmasz"], "venue": "Master\u2019s Thesis, University of Ottawa.", "citeRegEx": "Jarmasz,? 2003", "shortCiteRegEx": "Jarmasz", "year": 2003}, {"title": "Roget\u2019s thesaurus and semantic similarity", "author": ["M. Jarmasz", "S. Szpakowicz"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP),", "citeRegEx": "Jarmasz and Szpakowicz,? \\Q2003\\E", "shortCiteRegEx": "Jarmasz and Szpakowicz", "year": 2003}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J. Jiang", "D. Conrath"], "venue": "In Proceedings of the International Conference Research on Computational Linguistics (ROCLING X),", "citeRegEx": "Jiang and Conrath,? \\Q1997\\E", "shortCiteRegEx": "Jiang and Conrath", "year": 1997}, {"title": "Computational Analysis of Present Day", "author": ["H. Kucera", "W. Francis", "J. Caroll"], "venue": null, "citeRegEx": "Kucera et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Kucera et al\\.", "year": 1967}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge", "author": ["T. Landauer", "S. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Introduction to latent semantc analysis", "author": ["T. Landauer", "P. Foltz", "D. Laham"], "venue": "Discourse Processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Using corpus statistics and WordNet relations for sense identification", "author": ["C. Leacock", "G. Miller", "M. Chodorow"], "venue": "Computational Linguistics,", "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Automated sense disambiguation using machine-readable dictionaries: How to tell a pine cone from an ice cream cone", "author": ["M. Lesk"], "venue": "Proceedings of the Fifth Annual International Conference on Systems Documentation (SIGDOC), pp. 24\u201326.", "citeRegEx": "Lesk,? 1986", "shortCiteRegEx": "Lesk", "year": 1986}, {"title": "Estimators and tail bounds for dimension reduction in l\u03b1 (0 < \u03b1 \u2264 2) using stable random projections", "author": ["P. Li"], "venue": "Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 10\u201319.", "citeRegEx": "Li,? 2008", "shortCiteRegEx": "Li", "year": 2008}, {"title": "Sentence similarity based on semantic nets and corpus statistics", "author": ["Y. Li", "D. McLean", "Z. Bandar", "J. O\u2019Shea", "K. Crockett"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Li et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Li et al\\.", "year": 2006}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning (ICML), pp. 296\u2013304.", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Multiple alternative sentence compressions for automatic text summarization", "author": ["N. Madnani", "D. Zajic", "B. Dorr", "N. Fazil Ayan", "J. Lin"], "venue": "In Proceedings of the HLT/NAACL Document Understanding Conference (DUC)", "citeRegEx": "Madnani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2007}, {"title": "Generalized latent semantic analysis for term representation", "author": ["I. Matveeva", "G. Levow", "A. Farahat", "C. Royer"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP)", "citeRegEx": "Matveeva et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Matveeva et al\\.", "year": 2005}, {"title": "Word sense disambiguation for exploiting hierarchical thesauri in text classification", "author": ["D. Mavroeidis", "G. Tsatsaronis", "M. Vazirgiannis", "M. Theobald", "G. Weikum"], "venue": "In Proceedings of the Ninth European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD),", "citeRegEx": "Mavroeidis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mavroeidis et al\\.", "year": 2005}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["R. Mihalcea", "C. Corley", "C. Strapparava"], "venue": "In Proceedings of the Twenty First Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Mihalcea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "A method for word sense disambiguation of unrestricted text", "author": ["R. Mihalcea", "D. Moldovan"], "venue": "In Proceedings of the 37th annual meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Mihalcea and Moldovan,? \\Q1999\\E", "shortCiteRegEx": "Mihalcea and Moldovan", "year": 1999}, {"title": "PageRank on semantic networks with application to word sense disambiguation", "author": ["R. Mihalcea", "P. Tarau", "E. Figa"], "venue": "In Proceedings of the Twentieth International Conference on Computational Linguistics (COLING)", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Contextual correlates of semantic similarity", "author": ["G. Miller", "W. Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "Miller and Charles,? \\Q1991\\E", "shortCiteRegEx": "Miller and Charles", "year": 1991}, {"title": "An effective, low-cost measure of semantic relatedness obtained from Wikipedia links", "author": ["D. Milne", "I. Witten"], "venue": "In Proceedings of the first AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI)", "citeRegEx": "Milne and Witten,? \\Q2008\\E", "shortCiteRegEx": "Milne and Witten", "year": 2008}, {"title": "Lexical cohesion computed by thesaural relations as an indicator of the structure of text", "author": ["J. Morris", "G. Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Morris and Hirst,? \\Q1991\\E", "shortCiteRegEx": "Morris and Hirst", "year": 1991}, {"title": "A structural approach to the automatic adjudication of word sense disagreements", "author": ["R. Navigli"], "venue": "Natural Language Engineering, 14(4), 547\u2013573.", "citeRegEx": "Navigli,? 2008", "shortCiteRegEx": "Navigli", "year": 2008}, {"title": "A comparative study of two short text semantic similarity measures", "author": ["J. O\u2019Shea", "Z. Bandar", "K. Crocket", "D. McLean"], "venue": "In Proceedings of the Agent and Multi-Agent Systems: Technologies and Applications, Second KES International Symposium (KES-AMSTA),", "citeRegEx": "O.Shea et al\\.,? \\Q2008\\E", "shortCiteRegEx": "O.Shea et al\\.", "year": 2008}, {"title": "Dependency-based construction of semantic space models", "author": ["S. Pado", "M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Pado and Lapata,? \\Q2007\\E", "shortCiteRegEx": "Pado and Lapata", "year": 2007}, {"title": "English tasks: All-words and verb lexical sample", "author": ["M. Palmer", "C. Fellbaum", "S. Cotton"], "venue": "In Proceedings of Senseval-2,", "citeRegEx": "Palmer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2001}, {"title": "Open-domain question answering from large text collections", "author": ["M. Pasca"], "venue": "CSLI Studies in Computational Linguistics. CSLI Publications, Distributed by the University of Chicago Press.", "citeRegEx": "Pasca,? 2003", "shortCiteRegEx": "Pasca", "year": 2003}, {"title": "Mining paraphrases from self-anchored web sentence fragments", "author": ["M. Pasca"], "venue": "Proceedings of the Ninth European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), pp. 193\u2013204.", "citeRegEx": "Pasca,? 2005", "shortCiteRegEx": "Pasca", "year": 2005}, {"title": "Using measures of semantic relatedness for word sense disambiguation", "author": ["S. Patwardhan", "S. Banerjee", "T. Pedersen"], "venue": "In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),", "citeRegEx": "Patwardhan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Patwardhan et al\\.", "year": 2003}, {"title": "Using WordNet based context vectors to estimate the semantic relatedness of concepts", "author": ["S. Patwardhan", "T. Pedersen"], "venue": "In Proceedings of the EACL 2006 Workshop Making Sense of Sense - Bringing Computational Linguistics and Psycholinguistics Together,", "citeRegEx": "Patwardhan and Pedersen,? \\Q2006\\E", "shortCiteRegEx": "Patwardhan and Pedersen", "year": 2006}, {"title": "Knowledge derived from Wikipedia for computing semantic relatedness", "author": ["S. Ponzetto", "M. Strube"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ponzetto and Strube,? \\Q2007\\E", "shortCiteRegEx": "Ponzetto and Strube", "year": 2007}, {"title": "Deriving a large-scale taxonomy from Wikipedia", "author": ["S. Ponzetto", "M. Strube"], "venue": "In Proceedings of the Twenty Second Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Ponzetto and Strube,? \\Q2007\\E", "shortCiteRegEx": "Ponzetto and Strube", "year": 2007}, {"title": "Paraphrase recognition via dissimilarity significance classification", "author": ["L. Qiu", "M. Kan", "T. Chua"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Qiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2006}, {"title": "The teachable language comprehender: a simulation program and theory of language", "author": ["R. Quilian"], "venue": "Communications of ACM, 12(8), 459\u2013476.", "citeRegEx": "Quilian,? 1969", "shortCiteRegEx": "Quilian", "year": 1969}, {"title": "Using information content to evaluate semantic similarity", "author": ["P. Resnik"], "venue": "Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI), pp. 448\u2013453.", "citeRegEx": "Resnik,? 1995", "shortCiteRegEx": "Resnik", "year": 1995}, {"title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "author": ["P. Resnik"], "venue": "Journal of Artificial Intelligence Research, 11, 95\u2013130.", "citeRegEx": "Resnik,? 1999", "shortCiteRegEx": "Resnik", "year": 1999}, {"title": "Using WordNet in a knowledge-based approach to information retrieval", "author": ["R. Richardson", "A. Smeaton"], "venue": "In Proceedings of the BCS-IRSG Colloquium", "citeRegEx": "Richardson and Smeaton,? \\Q1995\\E", "shortCiteRegEx": "Richardson and Smeaton", "year": 1995}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein and Goodenough,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough", "year": 1965}, {"title": "Using context-window overlapping in synonym discovery and ontology extension", "author": ["M. Ruiz-Casado", "E. Alfonseca", "P. Castells"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP)", "citeRegEx": "Ruiz.Casado et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ruiz.Casado et al\\.", "year": 2005}, {"title": "An evaluation of term dependence models in information retrieval", "author": ["G. Salton", "C. Buckley", "C. Yu"], "venue": "In Proceedings of the Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Salton et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1982}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M. McGill"], "venue": null, "citeRegEx": "Salton and McGill,? \\Q1983\\E", "shortCiteRegEx": "Salton and McGill", "year": 1983}, {"title": "Word sense disambiguation and information retrieval", "author": ["M. Sanderson"], "venue": "Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 142\u2013151.", "citeRegEx": "Sanderson,? 1994", "shortCiteRegEx": "Sanderson", "year": 1994}, {"title": "Ambiguous queries: Test collections need more sense", "author": ["M. Sanderson"], "venue": "Proceedings of the Thirty First Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 499\u2013506.", "citeRegEx": "Sanderson,? 2008", "shortCiteRegEx": "Sanderson", "year": 2008}, {"title": "Paraphrase acquisition for information extraction", "author": ["Y. Shinyama", "S. Sekine"], "venue": "In Proceedings of the ACL 2nd Workshop on Paraphrasing: Paraphrase Acquisition and Applications,", "citeRegEx": "Shinyama and Sekine,? \\Q2003\\E", "shortCiteRegEx": "Shinyama and Sekine", "year": 2003}, {"title": "Collins Cobuild English Dictionary for Advanced Learners, 3rd edn", "author": ["J. Sinclair"], "venue": "Harper Collins, New York.", "citeRegEx": "Sinclair,? 2001", "shortCiteRegEx": "Sinclair", "year": 2001}, {"title": "TREC-4 experiments at Dublin City University: Thresholding posting lists, query expansion with WordNet and POS tagging of Spanish", "author": ["A. Smeaton", "F. Kelledy", "R. O\u2019Donnell"], "venue": "In Proceedings of the Fourth Text REtrieval Conference (TREC)", "citeRegEx": "Smeaton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Smeaton et al\\.", "year": 1995}, {"title": "The English All-words task", "author": ["B. Snyder", "M. Palmer"], "venue": "In Proceedings of Senseval-3,", "citeRegEx": "Snyder and Palmer,? \\Q2004\\E", "shortCiteRegEx": "Snyder and Palmer", "year": 2004}, {"title": "A term weighting method based on lexical chain for automatic summarization", "author": ["Y. Song", "K. Han", "H. Rim"], "venue": "In Proceedings of the Fifth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),", "citeRegEx": "Song et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Song et al\\.", "year": 2004}, {"title": "Word sense disambiguation in information retrieval revisited", "author": ["C. Stokoe", "M. Oakes", "J. Tait"], "venue": "In Proceedings of the Twenty Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Stokoe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Stokoe et al\\.", "year": 2003}, {"title": "WikiRelate! Computing semantic relatedness using Wikipedia", "author": ["M. Strube", "S. Ponzetto"], "venue": "In Proceedings of the Twenty First Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Strube and Ponzetto,? \\Q2006\\E", "shortCiteRegEx": "Strube and Ponzetto", "year": 2006}, {"title": "Word sense disambiguation for free-text indexing using a massive semantic network", "author": ["M. Sussna"], "venue": "Proceedings of the Second International Conference on Information and Knowledge Management (CIKM), pp. 67\u201374.", "citeRegEx": "Sussna,? 1993", "shortCiteRegEx": "Sussna", "year": 1993}, {"title": "Frequency estimates for statistical word similarity measures", "author": ["E. Terra", "C. Clarke"], "venue": "In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies Conference (HLT/NAACL).,", "citeRegEx": "Terra and Clarke,? \\Q2003\\E", "shortCiteRegEx": "Terra and Clarke", "year": 2003}, {"title": "A Graph Approach to Measuring Text Distance", "author": ["V. Tsang"], "venue": "PhD Thesis, University of Toronto.", "citeRegEx": "Tsang,? 2008", "shortCiteRegEx": "Tsang", "year": 2008}, {"title": "A generalized vector space model for text retrieval based on semantic relatedness", "author": ["G. Tsatsaronis", "V. Panagiotopoulou"], "venue": "In Proceedings of the the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL - Student Research Workshop),", "citeRegEx": "Tsatsaronis and Panagiotopoulou,? \\Q2009\\E", "shortCiteRegEx": "Tsatsaronis and Panagiotopoulou", "year": 2009}, {"title": "Omiotis: A thesaurus-based measure of text relatedness", "author": ["G. Tsatsaronis", "I. Varlamis", "K. N\u00f8rv\u00e5g", "M. Vazirgiannis"], "venue": "In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD),", "citeRegEx": "Tsatsaronis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tsatsaronis et al\\.", "year": 2009}, {"title": "Word sense disambiguation with semantic networks", "author": ["G. Tsatsaronis", "I. Varlamis", "M. Vazirgiannis"], "venue": "In Proceedings of the 11th International Conference on Text, Speech and Dialogue (TSD),", "citeRegEx": "Tsatsaronis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tsatsaronis et al\\.", "year": 2008}, {"title": "Word sense disambiguation with spreading activation networks generated from thesauri", "author": ["G. Tsatsaronis", "M. Vazirgiannis", "I. Androutsopoulos"], "venue": "In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Tsatsaronis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tsatsaronis et al\\.", "year": 2007}, {"title": "Mining the Web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["P. Turney"], "venue": "Proceedings of the Twelfth European Conference on Machine Learning (ECML), pp. 491\u2013502.", "citeRegEx": "Turney,? 2001", "shortCiteRegEx": "Turney", "year": 2001}, {"title": "Similarity of semantic relations", "author": ["P. Turney"], "venue": "Computational Linguistics, 32(3), 379\u2013416.", "citeRegEx": "Turney,? 2006", "shortCiteRegEx": "Turney", "year": 2006}, {"title": "The latent relation mapping engine: Algorithm and experiments", "author": ["P. Turney"], "venue": "Journal of Artificial Intelligence Research, 33, 615\u2013655.", "citeRegEx": "Turney,? 2008a", "shortCiteRegEx": "Turney", "year": 2008}, {"title": "A uniform approach to analogies, synonyms, antonyms, and associations", "author": ["P. Turney"], "venue": "Proceedings of the Twenty Second International Conference on Computational Linguistics (COLING), pp. 905\u2013912.", "citeRegEx": "Turney,? 2008b", "shortCiteRegEx": "Turney", "year": 2008}, {"title": "Corpus-based learning of analogies and semantic relations", "author": ["P. Turney", "M. Littman"], "venue": "Machine Learning,", "citeRegEx": "Turney and Littman,? \\Q2005\\E", "shortCiteRegEx": "Turney and Littman", "year": 2005}, {"title": "Combining independent modules to solve multiple-choice synonym and analogy problems", "author": ["P. Turney", "M. Littman", "J. Bigham", "V. Shnayder"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP),", "citeRegEx": "Turney et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2003}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "Springer.", "citeRegEx": "Vapnik,? 1995", "shortCiteRegEx": "Vapnik", "year": 1995}, {"title": "WordNet sits the SAT: A knowledge-based approach to lexical analogy", "author": ["T. Veale"], "venue": "Proceedings of the Sixteenth European Conference on Artificial Intelligence (ECAI), pp. 606\u2013 612.", "citeRegEx": "Veale,? 2004", "shortCiteRegEx": "Veale", "year": 2004}, {"title": "Word sense disambiguation with very large neural networks extracted from machine readable dictionaries", "author": ["J. Veronis", "N. Ide"], "venue": "In Proceedings of the Thirteenth International Conference on Computational Linguistics (COLING),", "citeRegEx": "Veronis and Ide,? \\Q1990\\E", "shortCiteRegEx": "Veronis and Ide", "year": 1990}, {"title": "Using WordNet to disambiguate word sense for text retrieval", "author": ["E. Voorhees"], "venue": "Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 171\u2013180.", "citeRegEx": "Voorhees,? 1993", "shortCiteRegEx": "Voorhees", "year": 1993}, {"title": "Using dependency-based features to take the parafarce out of paraphrase", "author": ["S. Wan", "M. Dras", "R. Dale", "C. Paris"], "venue": "In Proceedings of the Australasian Language Technology Workshop,", "citeRegEx": "Wan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2006}, {"title": "Verb semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "In Proceedings of the Thirty Second Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Wu and Palmer,? \\Q1994\\E", "shortCiteRegEx": "Wu and Palmer", "year": 1994}, {"title": "Paraphrase identification by text canonicalization", "author": ["Y. Zhang", "J. Patrick"], "venue": "In Proceedings of the Australasian Language Technology Workshop,", "citeRegEx": "Zhang and Patrick,? \\Q2005\\E", "shortCiteRegEx": "Zhang and Patrick", "year": 2005}], "referenceMentions": [{"referenceID": 37, "context": "Journal of Artificial Intelligence Research 37 (2010) 1-39 Submitted 07/09; published 01/10", "startOffset": 27, "endOffset": 54}, {"referenceID": 3, "context": "Several improvements have been proposed for such techniques towards inventing more sophisticated weighting schemes for the text words, like for example TF-IDF and its variations (Aizawa, 2003).", "startOffset": 178, "endOffset": 192}, {"referenceID": 27, "context": "Primarily, one can think of lexical relatedness or similarity between texts, which can be easily captured by a vectorial representation of texts (van Rijsbergen, 1979) and a standard similarity measure, like Cosine, Dice (Salton & McGill, 1983), and Jaccard (1901). Such models have had high impact in information retrieval over the past decades.", "startOffset": 250, "endOffset": 265}, {"referenceID": 14, "context": "The word-to-word relatedness measure, in its turn, is based on the construction of semantic links between individual words, according to a word thesaurus, which in our case is WordNet (Fellbaum, 1998).", "startOffset": 184, "endOffset": 200}, {"referenceID": 14, "context": "Word thesauri, like WordNet (Fellbaum, 1998) or Roget\u2019s International Thesaurus (Morris & Hirst, 1991), constitute the knowledge base for several text-related research tasks.", "startOffset": 28, "endOffset": 44}, {"referenceID": 42, "context": "From the aforementioned approaches, it is clear that the use of a word thesaurus can offer much potential in the design of models that capture the semantic relatedness between texts, and consequently, it may improve the performance of existing retrieval and classification models under certain circumstances that are discussed in the respective research works (Mavroeidis et al., 2005; Basili et al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004).", "startOffset": 360, "endOffset": 453}, {"referenceID": 7, "context": "From the aforementioned approaches, it is clear that the use of a word thesaurus can offer much potential in the design of models that capture the semantic relatedness between texts, and consequently, it may improve the performance of existing retrieval and classification models under certain circumstances that are discussed in the respective research works (Mavroeidis et al., 2005; Basili et al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004).", "startOffset": 360, "endOffset": 453}, {"referenceID": 75, "context": "From the aforementioned approaches, it is clear that the use of a word thesaurus can offer much potential in the design of models that capture the semantic relatedness between texts, and consequently, it may improve the performance of existing retrieval and classification models under certain circumstances that are discussed in the respective research works (Mavroeidis et al., 2005; Basili et al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004).", "startOffset": 360, "endOffset": 453}, {"referenceID": 11, "context": "Word thesauri, like WordNet (Fellbaum, 1998) or Roget\u2019s International Thesaurus (Morris & Hirst, 1991), constitute the knowledge base for several text-related research tasks. WordNet has been used successfully as a knowledge base in the construction of Generalized Vector Space Models (GVSM) and semantic kernels for document similarity with application to text classification, such as the works of Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald and Weikum (2005), and Basili, Cammisa and Moschitti (2005), and text retrieval, such as the works of Voorhees (1993), Stokoe, Oakes and Tait (2003), and our previous work regarding the definition of a new GVSM that uses word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009).", "startOffset": 29, "endOffset": 465}, {"referenceID": 11, "context": "Word thesauri, like WordNet (Fellbaum, 1998) or Roget\u2019s International Thesaurus (Morris & Hirst, 1991), constitute the knowledge base for several text-related research tasks. WordNet has been used successfully as a knowledge base in the construction of Generalized Vector Space Models (GVSM) and semantic kernels for document similarity with application to text classification, such as the works of Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald and Weikum (2005), and Basili, Cammisa and Moschitti (2005), and text retrieval, such as the works of Voorhees (1993), Stokoe, Oakes and Tait (2003), and our previous work regarding the definition of a new GVSM that uses word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009).", "startOffset": 29, "endOffset": 507}, {"referenceID": 11, "context": "Word thesauri, like WordNet (Fellbaum, 1998) or Roget\u2019s International Thesaurus (Morris & Hirst, 1991), constitute the knowledge base for several text-related research tasks. WordNet has been used successfully as a knowledge base in the construction of Generalized Vector Space Models (GVSM) and semantic kernels for document similarity with application to text classification, such as the works of Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald and Weikum (2005), and Basili, Cammisa and Moschitti (2005), and text retrieval, such as the works of Voorhees (1993), Stokoe, Oakes and Tait (2003), and our previous work regarding the definition of a new GVSM that uses word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009).", "startOffset": 29, "endOffset": 565}, {"referenceID": 11, "context": "Word thesauri, like WordNet (Fellbaum, 1998) or Roget\u2019s International Thesaurus (Morris & Hirst, 1991), constitute the knowledge base for several text-related research tasks. WordNet has been used successfully as a knowledge base in the construction of Generalized Vector Space Models (GVSM) and semantic kernels for document similarity with application to text classification, such as the works of Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald and Weikum (2005), and Basili, Cammisa and Moschitti (2005), and text retrieval, such as the works of Voorhees (1993), Stokoe, Oakes and Tait (2003), and our previous work regarding the definition of a new GVSM that uses word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009).", "startOffset": 29, "endOffset": 596}, {"referenceID": 9, "context": "Furthermore, the idea of using a thesaurus as a knowledge base in text retrieval has also been proven successful in the case of cross language information retrieval, like for example in the case of the CLIR system introduced by Clough and Stevenson (2004). Finally, the exploitation of word thesauri in linguistic tasks, such as Word Sense Disambiguation (WSD) (Ide & Veronis, 1998) has yielded interesting results (Mihalcea & Moldovan, 1999; Tsatsaronis, Vazirgiannis, & Androutsopoulos, 2007; Tsatsaronis, Varlamis, & Vazirgiannis, 2008).", "startOffset": 228, "endOffset": 256}, {"referenceID": 5, "context": "In the analysis of Barzilay and Elhadad (1997), and Barzilay, Elhadad and McKeown (2002) the impact of WSD in the performance of text summarization tasks is addressed by considering all possible interpretations of the lexical chains created from text.", "startOffset": 19, "endOffset": 47}, {"referenceID": 5, "context": "In the analysis of Barzilay and Elhadad (1997), and Barzilay, Elhadad and McKeown (2002) the impact of WSD in the performance of text summarization tasks is addressed by considering all possible interpretations of the lexical chains created from text.", "startOffset": 19, "endOffset": 89}, {"referenceID": 83, "context": "In this work we adopt the semantic network construction method that we introduced in the past (Tsatsaronis et al., 2007).", "startOffset": 94, "endOffset": 120}, {"referenceID": 36, "context": "The idea of representing text as a semantic network was initially introduced by Quilian (1969). The expansion of WordNet with semantic relations that cross parts of speech has added more possibilities of semantic network construction from text.", "startOffset": 83, "endOffset": 95}, {"referenceID": 36, "context": "The idea of representing text as a semantic network was initially introduced by Quilian (1969). The expansion of WordNet with semantic relations that cross parts of speech has added more possibilities of semantic network construction from text. More recent approaches to semantic network construction from word thesauri, by Mihalcea, Tarau and Figa (2004) and Navigli (2008), utilize a wide range of WordNet semantic relations instead of the gloss words.", "startOffset": 83, "endOffset": 356}, {"referenceID": 36, "context": "The idea of representing text as a semantic network was initially introduced by Quilian (1969). The expansion of WordNet with semantic relations that cross parts of speech has added more possibilities of semantic network construction from text. More recent approaches to semantic network construction from word thesauri, by Mihalcea, Tarau and Figa (2004) and Navigli (2008), utilize a wide range of WordNet semantic relations instead of the gloss words.", "startOffset": 83, "endOffset": 375}, {"referenceID": 6, "context": "3 Measures of Semantic Relatedness Semantic relatedness between words or concepts has been exploited, in the past, in text summarization (Barzilay et al., 2002), text retrieval (Stokoe et al.", "startOffset": 137, "endOffset": 160}, {"referenceID": 75, "context": ", 2002), text retrieval (Stokoe et al., 2003; Smeaton, Kelledy, & O\u2019Donnell, 1995; Richardson & Smeaton, 1995) and WSD (Patwardhan, Banerjee, & Pedersen, 2003) tasks.", "startOffset": 24, "endOffset": 110}, {"referenceID": 1, "context": "Among dictionary-based measures, the measure of Agirre and Rigau (1995) was one of the first measures developed to compute semantic relatedness between two or more concepts (i.", "startOffset": 48, "endOffset": 72}, {"referenceID": 1, "context": "Among dictionary-based measures, the measure of Agirre and Rigau (1995) was one of the first measures developed to compute semantic relatedness between two or more concepts (i.e., for a set of concepts). Their measure was based on the density and depth of concepts in the set and on the length of the shortest path that connects them. However, they assume that all edges in the path are equally important. The measure proposed by Leacock, Miller and Chodorow (1998) for computing the semantic similarity between a pair of concepts takes into account the length of the shortest path connecting them, measured as the number of nodes participating in the path, and the maximum depth of the taxonomy.", "startOffset": 48, "endOffset": 466}, {"referenceID": 31, "context": "The measure proposed by Jiang and Conrath (1997), is also based on the concept of IC.", "startOffset": 24, "endOffset": 49}, {"referenceID": 37, "context": "The measure of Lin (1998) is also based on IC.", "startOffset": 15, "endOffset": 26}, {"referenceID": 34, "context": "(2002), who perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words\u2019 definitions to create WordNet-based context vectors, the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness.", "startOffset": 45, "endOffset": 68}, {"referenceID": 9, "context": "We encourage the reader to consult the analysis of Budanitsky and Hirst (2006) for a detailed discussion on most of the aforementioned measures, as well as for more measures proposed prior to the aforementioned.", "startOffset": 51, "endOffset": 79}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy.", "startOffset": 173, "endOffset": 202}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet. More recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al.", "startOffset": 173, "endOffset": 725}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet. More recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who perform Latent Semantic Analysis (Landauer et al.", "startOffset": 173, "endOffset": 888}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet. More recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words\u2019 definitions to create WordNet-based context vectors, the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness.", "startOffset": 173, "endOffset": 1072}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet. More recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words\u2019 definitions to create WordNet-based context vectors, the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness.", "startOffset": 173, "endOffset": 1260}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet. More recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words\u2019 definitions to create WordNet-based context vectors, the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness.", "startOffset": 173, "endOffset": 1289}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet. More recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words\u2019 definitions to create WordNet-based context vectors, the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness.", "startOffset": 173, "endOffset": 1452}, {"referenceID": 4, "context": "While all these measures use only the noun hierarchy (except from the measure of Hirst and St-Onge), the implementation of several of those measures provided by Patwardhan, Banerjee and Pedersen (2003) in the publicly available WordNet::Similarity package can also utilize the verb hierarchy. Still, the relations that cross parts of speech are not considered, as well as other factors discussed in detail in Section 3. In contrast, our measure defines the semantic relatedness between any two concepts, independently of their Part of Speech (POS), utilizing all of the available semantic links offered by WordNet. More recent works of interest on semantic relatedness, include: the measures of Jarmasz and Szpakowicz (2003), who use Roget\u2019s thesaurus to compute semantic similarity, by replicating a number of WordNet-based approaches, the LSA-based measure of Finkelstein et al. (2002), who perform Latent Semantic Analysis (Landauer et al., 1998) to capture text relatedness and can be considered as a corpus-based method, the measure of Patwardhan and Pedersen (2006), who utilize the gloss words found from the words\u2019 definitions to create WordNet-based context vectors, the methods of Strube and Ponzetto (2006, 2007a), Gabrilovich and Markovitch (2007), and Milne and Witten (2008) who use Wikipedia to compute semantic relatedness and can also be considered as corpus-based approaches, and the method of Mihalcea, Corley and Strappavara (2006), which is a hybrid method that combines knowledge-based and corpus-based measures of text relatedness. Other recent hybrid measures of semantic similarity are: the measure proposed by Li et al. (2006), who use information from WordNet and corpus statistics collected from the Brown Corpus (Kucera,", "startOffset": 173, "endOffset": 1653}, {"referenceID": 74, "context": "Francis, & Caroll, 1967) to compute similarity between very short texts, and the measure for text distance proposed by Tsang (2008), that uses both distributional similarity and ontological/knowledge information to compute the distance between text fragments.", "startOffset": 119, "endOffset": 132}, {"referenceID": 37, "context": "Li et al. (2006) have created a new data set for their experimental evaluation, which we also use in Section 4 to evaluate our Omiotis measure and compare against their approach.", "startOffset": 0, "endOffset": 17}, {"referenceID": 83, "context": "In order to solve the problem of constructing semantic paths between words, we base our approach on our previous method on how to construct semantic networks between words (Tsatsaronis et al., 2007).", "startOffset": 172, "endOffset": 198}, {"referenceID": 42, "context": "A measure for WSD based on the idea of compactness was initially proposed by Mavroeidis et al. (2005). The original measure used only nouns and the hypernym relation, and is extended in the current work to support all of WordNet\u2019s relations and the noun, verb and adjective parts of speech.", "startOffset": 77, "endOffset": 102}, {"referenceID": 37, "context": "Higher compactness between senses implies higher semantic relatedness. The intuition behind edge types\u2019 weighting is that certain types provide stronger semantic connections than others. Considering that the lexicographers of WordNet tend to use some relation types more often than others (we assume that the most used relation types are stronger than the types less used), a straightforward solution is to define edge types\u2019 weights in proportion to their frequency of occurrence in WordNet 2.0. The weights assigned to each type using this solution are shown in Table 1 and are in accordance to those found by Song et al. (2004). The table shows the probability of occurrence in WordNet 2.", "startOffset": 37, "endOffset": 631}, {"referenceID": 16, "context": "Current data sets for evaluating semantic relatedness or even semantic similarity measures are restricted to nouns, like for example the Rubenstein and Goodenough 65 word pairs (1965), the Miller and Charles 30 word pairs (1991), and the Word-Similarity-353 collection (Finkelstein et al., 2002).", "startOffset": 269, "endOffset": 295}, {"referenceID": 36, "context": "Current data sets for evaluating semantic relatedness or even semantic similarity measures are restricted to nouns, like for example the Rubenstein and Goodenough 65 word pairs (1965), the Miller and Charles 30 word pairs (1991), and the Word-Similarity-353 collection (Finkelstein et al.", "startOffset": 116, "endOffset": 184}, {"referenceID": 36, "context": "Current data sets for evaluating semantic relatedness or even semantic similarity measures are restricted to nouns, like for example the Rubenstein and Goodenough 65 word pairs (1965), the Miller and Charles 30 word pairs (1991), and the Word-Similarity-353 collection (Finkelstein et al.", "startOffset": 116, "endOffset": 229}, {"referenceID": 72, "context": "Examples of such measures are: the measure of Sussna (1993), Wu and Palmer (1994), Jiang and Conrath (1997), Resnik (1995, 1999), and the WordNet-based component of the measure proposed by Finkelstein et al.", "startOffset": 46, "endOffset": 60}, {"referenceID": 72, "context": "Examples of such measures are: the measure of Sussna (1993), Wu and Palmer (1994), Jiang and Conrath (1997), Resnik (1995, 1999), and the WordNet-based component of the measure proposed by Finkelstein et al.", "startOffset": 46, "endOffset": 82}, {"referenceID": 30, "context": "Examples of such measures are: the measure of Sussna (1993), Wu and Palmer (1994), Jiang and Conrath (1997), Resnik (1995, 1999), and the WordNet-based component of the measure proposed by Finkelstein et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 16, "context": "Examples of such measures are: the measure of Sussna (1993), Wu and Palmer (1994), Jiang and Conrath (1997), Resnik (1995, 1999), and the WordNet-based component of the measure proposed by Finkelstein et al. (2002). From this point of view, the decision to use all POS information expands the potential matches found by the measure and allows the use of the measure in more complicated tasks, like paraphrase recognition, text retrieval, and text classification.", "startOffset": 189, "endOffset": 215}, {"referenceID": 83, "context": "2 USE EVERY TYPE OF SEMANTIC RELATIONS The decision to use all parts of speech in the construction of the semantic graphs, as it was introduced in our previous work (Tsatsaronis et al., 2007), imposes the involvement of all semantic relations instead of merely taxonomic (IS-A) ones.", "startOffset": 165, "endOffset": 191}, {"referenceID": 36, "context": "Moreover, this decision was based on evidence from related literature. The work of Smeaton et al. (1995) provides experimental evidence that measuring semantic similarity by incorporating non-hierarchical link types (i.", "startOffset": 59, "endOffset": 105}, {"referenceID": 36, "context": "Moreover, this decision was based on evidence from related literature. The work of Smeaton et al. (1995) provides experimental evidence that measuring semantic similarity by incorporating non-hierarchical link types (i.e. part meronym/holonym, member meronym/holonym, substance meronym/holonym) improves much the performance of such a measure. The experimental evaluation was conducted by adopting a small variation of the Resnik\u2019s measure (1995). Hirst and St-Onge (1998) reported that they have discovered several limitations and missing connections in the set of WordNet relations during the construction of lexical chains from sentences for the detection and correction of malapropisms.", "startOffset": 59, "endOffset": 447}, {"referenceID": 24, "context": "Hirst and St-Onge (1998) reported that they have discovered several limitations and missing connections in the set of WordNet relations during the construction of lexical chains from sentences for the detection and correction of malapropisms.", "startOffset": 0, "endOffset": 25}, {"referenceID": 55, "context": "3 USE WEIGHTS ON EDGES The work of Resnik (1999) reports that simple edge counting, which implicitly assumes that links in the taxonomy represent uniform distances, is problematic and is not the best semantic distance measure for WordNet.", "startOffset": 35, "endOffset": 49}, {"referenceID": 33, "context": "3 USE WEIGHTS ON EDGES The work of Resnik (1999) reports that simple edge counting, which implicitly assumes that links in the taxonomy represent uniform distances, is problematic and is not the best semantic distance measure for WordNet. In a similar direction lie the findings of Sussna (1993), who has performed thorough experimental evaluation by varying edge weights in order to measure semantic distance between concepts.", "startOffset": 93, "endOffset": 296}, {"referenceID": 32, "context": "This very important factor is absent in several similarity measures proposed in the past, such as in the measures of Leacock et al. (1998), Jarmasz and Szpakowicz (2003) and Banerjee and Pedersen (2003), which are outperformed in experimental evaluation by our measure.", "startOffset": 117, "endOffset": 139}, {"referenceID": 28, "context": "(1998), Jarmasz and Szpakowicz (2003) and Banerjee and Pedersen (2003), which are outperformed in experimental evaluation by our measure.", "startOffset": 8, "endOffset": 38}, {"referenceID": 4, "context": "(1998), Jarmasz and Szpakowicz (2003) and Banerjee and Pedersen (2003), which are outperformed in experimental evaluation by our measure.", "startOffset": 42, "endOffset": 71}, {"referenceID": 37, "context": "4 USE DEPTH SCALING FACTOR Our decision to incorporate the depth scaling factor (SPE in Definition 2) in the edge weighting mechanism has been inspired by the thorough experimental evaluation conducted by Sussna (1993),", "startOffset": 15, "endOffset": 219}, {"referenceID": 18, "context": "Although, such measures, like the ones proposed by Gabrilovich and Markovitch (2007), and Ponzetto and Strube (2007a), provide a larger coverage regarding concepts that do not reside in WordNet, they require the processing of a very large corpora (Wikipedia), which also changes very fast and very frequently.", "startOffset": 51, "endOffset": 85}, {"referenceID": 18, "context": "Although, such measures, like the ones proposed by Gabrilovich and Markovitch (2007), and Ponzetto and Strube (2007a), provide a larger coverage regarding concepts that do not reside in WordNet, they require the processing of a very large corpora (Wikipedia), which also changes very fast and very frequently.", "startOffset": 51, "endOffset": 118}, {"referenceID": 37, "context": "Harmonic mean is preferred instead of average, since it provides a more tight upper bound (Li, 2008).", "startOffset": 90, "endOffset": 100}, {"referenceID": 63, "context": "1 WORD-TO-WORD SIMILARITY Rubenstein and Goodenough (1965) obtained synonymy judgements from 51 human subjects on 65 pairs of words, in an effort to investigate the relationship between similarity of context and similarity of meaning (synonymy).", "startOffset": 26, "endOffset": 59}, {"referenceID": 85, "context": "1% (Turney, 2006), the top lexicon-based scores 42% (Veale, 2004) and the top hybrid scores 33.", "startOffset": 3, "endOffset": 17}, {"referenceID": 91, "context": "1% (Turney, 2006), the top lexicon-based scores 42% (Veale, 2004) and the top hybrid scores 33.", "startOffset": 52, "endOffset": 65}, {"referenceID": 61, "context": "2% (Resnik, 1995).", "startOffset": 3, "endOffset": 17}, {"referenceID": 35, "context": "Although it is difficult for machines to model the human cognition of word analogy, several approaches exist in the bibliography that attempt to tackle this problem. Previous approaches can be widely categorized into: corpus-based, lexicon-based and hybrid. Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006).", "startOffset": 119, "endOffset": 325}, {"referenceID": 8, "context": "Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006).", "startOffset": 71, "endOffset": 95}, {"referenceID": 8, "context": "Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006).", "startOffset": 71, "endOffset": 160}, {"referenceID": 8, "context": "Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006).", "startOffset": 71, "endOffset": 237}, {"referenceID": 8, "context": "Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006). Hybrid approaches are applied in SAT, through the application of the measures of Resnik (1995) and Lin (1998) that can also be found in the work of Turney (2006).", "startOffset": 71, "endOffset": 292}, {"referenceID": 8, "context": "Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006). Hybrid approaches are applied in SAT, through the application of the measures of Resnik (1995) and Lin (1998) that can also be found in the work of Turney (2006).", "startOffset": 71, "endOffset": 388}, {"referenceID": 8, "context": "Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006). Hybrid approaches are applied in SAT, through the application of the measures of Resnik (1995) and Lin (1998) that can also be found in the work of Turney (2006).", "startOffset": 71, "endOffset": 403}, {"referenceID": 8, "context": "Some examples of corpus-based are the approaches of Turney (2008b) and Bicici and Yuret (2006). Examples of lexicon-based approaches, are those of Veale (2004) and the application of the lexicon-based measure by Hirst and St-Onge (1998) in SAT, that can be found in the work of Turney (2006). Hybrid approaches are applied in SAT, through the application of the measures of Resnik (1995) and Lin (1998) that can also be found in the work of Turney (2006). In order for the reader to understand the difficulty of answering SAT questions, we must point out that the average US college applicant scores 57% (Turney & Littman, 2005), while the top corpus-based approach scores 56.", "startOffset": 71, "endOffset": 455}, {"referenceID": 20, "context": "Another way of categorizing the approaches that measure semantic similarity in analogy tasks is to distinguish among attributional and relational similarity measures (Gentner, 1983).", "startOffset": 166, "endOffset": 181}, {"referenceID": 85, "context": "7 Representative approaches of the first category are lexicon-based approaches, while paradigms of relational similarity measures can be found in approaches based on Latent Relational Analysis (LRA) (Turney, 2006).", "startOffset": 199, "endOffset": 213}, {"referenceID": 20, "context": "Another way of categorizing the approaches that measure semantic similarity in analogy tasks is to distinguish among attributional and relational similarity measures (Gentner, 1983).7 Representative approaches of the first category are lexicon-based approaches, while paradigms of relational similarity measures can be found in approaches based on Latent Relational Analysis (LRA) (Turney, 2006). It is of great interest to point out that LRA-based approaches, like the LRME algorithm proposed recently by Turney (2008a), are superior to attributional similarity approaches in discovering word analogies.", "startOffset": 167, "endOffset": 521}, {"referenceID": 20, "context": "Another way of categorizing the approaches that measure semantic similarity in analogy tasks is to distinguish among attributional and relational similarity measures (Gentner, 1983).7 Representative approaches of the first category are lexicon-based approaches, while paradigms of relational similarity measures can be found in approaches based on Latent Relational Analysis (LRA) (Turney, 2006). It is of great interest to point out that LRA-based approaches, like the LRME algorithm proposed recently by Turney (2008a), are superior to attributional similarity approaches in discovering word analogies. This fact is also supported by the experimental findings of Turney (2006). Without doubt, relational similarity approaches may perform better in the SAT analogy task, but still, as shown later in the experiments we conducted in other applications, like paraphrase recognition, the lexicon-based measures can outperform LRA-based approaches in such tasks.", "startOffset": 167, "endOffset": 679}, {"referenceID": 37, "context": "From this perspective, s1 and s2 try to find the candidate pair that best aligns with the target pair. Figure 4 illustrates these two types of analogies (horizontal and vertical) for an example SAT question. In order to motivate more our selection of s1 and s2 for answering SAT questions, we will discuss in more detail how these two quantities pertain to the concepts of strength and type of the relations between a pair of SAT words. Turney (2006) describes a method for comparing the relations between candidate word pairs and the stem word pair, in which he utilizes the type of the relation connecting the words in each pair and finally selects the pair that best matches the type of the relation connecting the words in the stem pair.", "startOffset": 75, "endOffset": 451}, {"referenceID": 37, "context": "3 PARAPHRASE RECOGNITION AND SENTENCE-TO-SENTENCE SIMILARITY Performance of applications relying on natural language processing may suffer from the fact that the processed documents might contain lexically different, yet semantically related, text segments. The task of recognizing synonym text segments, which is better known as paraphrase recognition, or detection, is challenging and difficult to solve, as shown in the work of Pasca (2005). The task itself is important for many text related applications, like summarization (Hirao, Fukusima, Oku-", "startOffset": 79, "endOffset": 444}, {"referenceID": 53, "context": "mura, Nobata, & Nanba, 2005), information extraction (Shinyama & Sekine, 2003) and question answering (Pasca, 2003).", "startOffset": 102, "endOffset": 115}, {"referenceID": 13, "context": "2), using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004).", "startOffset": 51, "endOffset": 71}, {"referenceID": 13, "context": "2), using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The application of Omiotis in paraphrase detection is straightforward: given a pair of text segments, we compute the Omiotis score between them, using Equation 10 and Algorithm 2. Higher values of Omiotis for a given pair denote stronger semantic relation between the examined text segments. The task is now reduced to define a threshold, above which an Omiotis value can be considered as a determining sign of a paraphrasing pair. In the experimental evaluation of Omiotis, we explain in detail how we have selected this threshold for the paraphrase recognition task. In a similar manner, by using Equation 10 and Algorithm 2, the semantic relatedness scores for pairs of sentences can be computed. For this task, we are using the data set of Li et al. (2006) to evaluate Omiotis, comprising 30 sentence pairs, for which human scores are provided.", "startOffset": 52, "endOffset": 834}, {"referenceID": 83, "context": "Primarily, given two words, w1 and w2 the construction time of the semantic network used to compute SR according to Algorithm 1, has been proven to be O(2 \u00b7 kl+1) (Tsatsaronis et al., 2007), where k is the maximum branching factor of the used thesaurus nodes and l is the maximum semantic path length in the thesaurus.", "startOffset": 163, "endOffset": 189}, {"referenceID": 81, "context": "As a proof of concept, we have developed an on-line version of the SR and the Omiotis measures8, where the user can test the term-to-term and sentence-to-sentence semantic relatedness measures (Tsatsaronis et al., 2009).", "startOffset": 193, "endOffset": 219}, {"referenceID": 16, "context": "In this task, we evaluate the performance of SR in three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (1965) (R&G), and the Miller and Charles 30 word pairs (1991) (M&C), for which humans have provided similarity scores, and, also, in the Word-Similarity-353 collection (Finkelstein et al., 2002) (353-C), which comprises 353 word pairs, for which humans have provided relatedness scores.", "startOffset": 297, "endOffset": 323}, {"referenceID": 36, "context": "We evaluate the performance of measures, by computing the correlation between the list of the human rankings and the list produced by the measures. In this task, we evaluate the performance of SR in three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (1965) (R&G), and the Miller and Charles 30 word pairs (1991) (M&C), for which humans have provided similarity scores, and, also, in the Word-Similarity-353 collection (Finkelstein et al.", "startOffset": 82, "endOffset": 284}, {"referenceID": 36, "context": "We evaluate the performance of measures, by computing the correlation between the list of the human rankings and the list produced by the measures. In this task, we evaluate the performance of SR in three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (1965) (R&G), and the Miller and Charles 30 word pairs (1991) (M&C), for which humans have provided similarity scores, and, also, in the Word-Similarity-353 collection (Finkelstein et al.", "startOffset": 82, "endOffset": 339}, {"referenceID": 20, "context": "For the first category of experiments, we compared our measure against ten known measures of semantic relatedness: Hirst and St-Onge (1998) (HS), Jiang and Conrath (1997) (JC), Leacock et al.", "startOffset": 115, "endOffset": 140}, {"referenceID": 20, "context": "For the first category of experiments, we compared our measure against ten known measures of semantic relatedness: Hirst and St-Onge (1998) (HS), Jiang and Conrath (1997) (JC), Leacock et al.", "startOffset": 115, "endOffset": 171}, {"referenceID": 20, "context": "For the first category of experiments, we compared our measure against ten known measures of semantic relatedness: Hirst and St-Onge (1998) (HS), Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS), Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al.", "startOffset": 115, "endOffset": 199}, {"referenceID": 20, "context": "For the first category of experiments, we compared our measure against ten known measures of semantic relatedness: Hirst and St-Onge (1998) (HS), Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS), Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al.", "startOffset": 115, "endOffset": 216}, {"referenceID": 20, "context": "For the first category of experiments, we compared our measure against ten known measures of semantic relatedness: Hirst and St-Onge (1998) (HS), Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS), Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al.", "startOffset": 115, "endOffset": 276}, {"referenceID": 16, "context": "(1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS), Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al.", "startOffset": 90, "endOffset": 160}, {"referenceID": 15, "context": "(1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS), Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al. (2002) (LSA), Hughes and Ramage (2007) (HR), and Strube and Ponzetto (2006, 2007a) (SP).", "startOffset": 167, "endOffset": 193}, {"referenceID": 15, "context": "(1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz and Szpakowicz (2003) (JS), Gabrilovich and Markovitch (2007, 2009) (GM), Milne and Witten (2008) (WLM), Finkelstein et al. (2002) (LSA), Hughes and Ramage (2007) (HR), and Strube and Ponzetto (2006, 2007a) (SP).", "startOffset": 167, "endOffset": 225}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007).", "startOffset": 156, "endOffset": 184}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007).", "startOffset": 156, "endOffset": 271}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007).", "startOffset": 156, "endOffset": 380}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007). For the WLM measure the \u03c1 values are given in the work of Milne and Witten (2008).", "startOffset": 156, "endOffset": 472}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007). For the WLM measure the \u03c1 values are given in the work of Milne and Witten (2008). For the LSA method the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007), only for the 353-C data set.", "startOffset": 156, "endOffset": 555}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007). For the WLM measure the \u03c1 values are given in the work of Milne and Witten (2008). For the LSA method the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007), only for the 353-C data set.", "startOffset": 156, "endOffset": 645}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007). For the WLM measure the \u03c1 values are given in the work of Milne and Witten (2008). For the LSA method the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007), only for the 353-C data set. For the HR measure the \u03c1 values are given in the work of Hughes and Ramage (2007). Finally, for the SP measure the r values are given in the work of Ponzetto and Strube (2007a), and for the IS-A SP are given in the work of Ponzetto and Strube (2007b).", "startOffset": 156, "endOffset": 757}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007). For the WLM measure the \u03c1 values are given in the work of Milne and Witten (2008). For the LSA method the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007), only for the 353-C data set. For the HR measure the \u03c1 values are given in the work of Hughes and Ramage (2007). Finally, for the SP measure the r values are given in the work of Ponzetto and Strube (2007a), and for the IS-A SP are given in the work of Ponzetto and Strube (2007b).", "startOffset": 156, "endOffset": 852}, {"referenceID": 10, "context": "For the measures HS, JC, LC, L and R, the rankings and the relatedness scores of the word pairs for the R&G and the M&C data sets, are given in the work of Budanitsky and Hirst (2006). For the JS measure, the r value is given in the work of Jarmasz and Szpakowicz (2003) for the R&G and the M&C data sets, and the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007). For the GM measure the \u03c1 values are given in the work of Gabrilovich and Markovitch (2007). For the WLM measure the \u03c1 values are given in the work of Milne and Witten (2008). For the LSA method the \u03c1 value is given in the work of Gabrilovich and Markovitch (2007), only for the 353-C data set. For the HR measure the \u03c1 values are given in the work of Hughes and Ramage (2007). Finally, for the SP measure the r values are given in the work of Ponzetto and Strube (2007a), and for the IS-A SP are given in the work of Ponzetto and Strube (2007b).", "startOffset": 156, "endOffset": 926}, {"referenceID": 17, "context": "We have also conducted a statistical significance test on the difference between SR correlations and the respective correlations of the compared measures, using Fisher\u2019s z-transformation (Fisher, 1915).", "startOffset": 187, "endOffset": 201}, {"referenceID": 10, "context": "The human scores for all pairs of words for the two data sets can be found in the analysis of Budanitsky and Hirst (2006). Note that the M&C data set is a subset of the R&G data set.", "startOffset": 94, "endOffset": 122}, {"referenceID": 10, "context": "The human scores for all pairs of words for the two data sets can be found in the analysis of Budanitsky and Hirst (2006). Note that the M&C data set is a subset of the R&G data set. In some cases, the computation of \u03c1 or r was not feasible, due to missing information regarding the detailed rankings or relatedness scores for the respective measures. In these cases the table has the entry N/A. Also the LSA measure is omitted in this table because \u03c1 and r were not reported in the literature for these two data sets. We have also conducted a statistical significance test on the difference between SR correlations and the respective correlations of the compared measures, using Fisher\u2019s z-transformation (Fisher, 1915). For each reported number, the symbol \u00a7 indicates that the difference between the correlation produced by SR and the respective measure is statistically significant at the 0.99 confidence level (p < 0.01). The symbol \u2021 indicates the same at the 0.95 confidence level (p < 0.05) and, finally, the symbol \u2020 indicates statistical significance of the correlations\u2019 difference at the 0.90 confidence level (p < 0.10). In cases when the difference is not statistically significant in any of those confidence levels, there is no indicating symbol. In Table 4 we show the values of \u03c1 and r for the 353-C data set. The reason we present the results of the experiments in the 353-C data set in another table than the respective results of the R&B and M&C data sets is that this collection focuses on the concept of semantic relatedness, rather than on the concept of semantic similarity (Gabrilovich & Markovitch, 2007). Relatedness is more general concept than similarity, as argued in the analysis of Budanitsky and Hirst (2006). Thus, it can be argued that the humans in the 353-C thought differently when scoring, compared to the case of the R&B and M&C data sets.", "startOffset": 94, "endOffset": 1742}, {"referenceID": 2, "context": "In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990).", "startOffset": 128, "endOffset": 143}, {"referenceID": 89, "context": "The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003).", "startOffset": 118, "endOffset": 139}, {"referenceID": 28, "context": "More specifically, we examine: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al.", "startOffset": 13, "endOffset": 78}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al.", "startOffset": 13, "endOffset": 140}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al.", "startOffset": 13, "endOffset": 169}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al.", "startOffset": 13, "endOffset": 190}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al.", "startOffset": 13, "endOffset": 219}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L), Jiang and Conrath (1997) (JC), and Turney et al.", "startOffset": 13, "endOffset": 252}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L), Jiang and Conrath (1997) (JC), and Turney et al.", "startOffset": 13, "endOffset": 294}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L), Jiang and Conrath (1997) (JC), and Turney et al.", "startOffset": 13, "endOffset": 310}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L), Jiang and Conrath (1997) (JC), and Turney et al.", "startOffset": 13, "endOffset": 340}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L), Jiang and Conrath (1997) (JC), and Turney et al. (2003) (PR); and a Web-based method by RuizCasado et al.", "startOffset": 13, "endOffset": 371}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Landauer and Dumais (1997) (LD), Pado and Lapata (2007) (PL), Turney (2008b) (T), Terra and Clarke (2003) (TC), and Matveeva et al. (2005) (M); the hybrid measures of Resnik (1995) (R), Lin (1998) (L), Jiang and Conrath (1997) (JC), and Turney et al. (2003) (PR); and a Web-based method by RuizCasado et al. (2005) (RC).", "startOffset": 13, "endOffset": 428}, {"referenceID": 2, "context": "In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990). As in the previous tables, the symbol \u00a7 indicates statistically significant difference at the 0.99 confidence level, \u2021 at the 0.95 confidence level, and \u2020 at the 0.90 confidence level. The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with the lexicon-based methods, SR reports better results, statistically significant at the confidence levels indicated. In a similar manner, we have conducted experiments in the ESL 50 questions data set, and compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney (2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),", "startOffset": 129, "endOffset": 790}, {"referenceID": 2, "context": "In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990). As in the previous tables, the symbol \u00a7 indicates statistically significant difference at the 0.99 confidence level, \u2021 at the 0.95 confidence level, and \u2020 at the 0.90 confidence level. The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with the lexicon-based methods, SR reports better results, statistically significant at the confidence levels indicated. In a similar manner, we have conducted experiments in the ESL 50 questions data set, and compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney (2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),", "startOffset": 129, "endOffset": 820}, {"referenceID": 2, "context": "In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990). As in the previous tables, the symbol \u00a7 indicates statistically significant difference at the 0.99 confidence level, \u2021 at the 0.95 confidence level, and \u2020 at the 0.90 confidence level. The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with the lexicon-based methods, SR reports better results, statistically significant at the confidence levels indicated. In a similar manner, we have conducted experiments in the ESL 50 questions data set, and compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney (2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),", "startOffset": 129, "endOffset": 860}, {"referenceID": 2, "context": "In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990). As in the previous tables, the symbol \u00a7 indicates statistically significant difference at the 0.99 confidence level, \u2021 at the 0.95 confidence level, and \u2020 at the 0.90 confidence level. The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with the lexicon-based methods, SR reports better results, statistically significant at the confidence levels indicated. In a similar manner, we have conducted experiments in the ESL 50 questions data set, and compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney (2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),", "startOffset": 129, "endOffset": 909}, {"referenceID": 2, "context": "In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990). As in the previous tables, the symbol \u00a7 indicates statistically significant difference at the 0.99 confidence level, \u2021 at the 0.95 confidence level, and \u2020 at the 0.90 confidence level. The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with the lexicon-based methods, SR reports better results, statistically significant at the confidence levels indicated. In a similar manner, we have conducted experiments in the ESL 50 questions data set, and compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney (2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),", "startOffset": 129, "endOffset": 947}, {"referenceID": 2, "context": "In order to test the statistical significance of the differences in the measures\u2019 performance, we conducted Fisher\u2019s Exact Test (Agresti, 1990). As in the previous tables, the symbol \u00a7 indicates statistically significant difference at the 0.99 confidence level, \u2021 at the 0.95 confidence level, and \u2020 at the 0.90 confidence level. The results of Table 5 show that SR ranks second among all reported methods, with the best method being the hybrid PR (Turney et al., 2003). With regards to its comparison with the lexicon-based methods, SR reports better results, statistically significant at the confidence levels indicated. In a similar manner, we have conducted experiments in the ESL 50 questions data set, and compare our results with: the lexicon-based measures of Leacock et al. (1998) (LC), Hirst and StOnge (1998) (HS), and Jarmasz and Szpakowicz (2003) (JS); the corpus-based measures of Turney (2001) (PMI-IR), and Terra and Clarke (2003) (TC); and the hybrid measures of Resnik (1995) (R),", "startOffset": 129, "endOffset": 994}, {"referenceID": 31, "context": "Lin (1998) (L), and Jiang and Conrath (1997) (JC).", "startOffset": 20, "endOffset": 45}, {"referenceID": 84, "context": "(2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) and LRA (Turney, 2006).", "startOffset": 37, "endOffset": 51}, {"referenceID": 85, "context": "(2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) and LRA (Turney, 2006).", "startOffset": 60, "endOffset": 74}, {"referenceID": 28, "context": "In Table 7, we present the number of correct answers and the respective percentage (recall) on the 374 SAT questions, of the following methods: random guessing (RG), Jiang and Conrath (1997) (JC), Lin (1998) (L), Leacock et al.", "startOffset": 166, "endOffset": 191}, {"referenceID": 28, "context": "In Table 7, we present the number of correct answers and the respective percentage (recall) on the 374 SAT questions, of the following methods: random guessing (RG), Jiang and Conrath (1997) (JC), Lin (1998) (L), Leacock et al.", "startOffset": 166, "endOffset": 208}, {"referenceID": 28, "context": "In Table 7, we present the number of correct answers and the respective percentage (recall) on the 374 SAT questions, of the following methods: random guessing (RG), Jiang and Conrath (1997) (JC), Lin (1998) (L), Leacock et al. (1998) (LC), Hirst and St-Onge (1998) (HS), Resnik (1995) (R), Bollegala et al.", "startOffset": 166, "endOffset": 235}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), Resnik (1995) (R), Bollegala et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 22, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), Resnik (1995) (R), Bollegala et al.", "startOffset": 13, "endOffset": 58}, {"referenceID": 9, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), Resnik (1995) (R), Bollegala et al. (2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) and LRA (Turney, 2006).", "startOffset": 63, "endOffset": 87}, {"referenceID": 9, "context": "(1998) (LC), Hirst and St-Onge (1998) (HS), Resnik (1995) (R), Bollegala et al. (2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) and LRA (Turney, 2006).", "startOffset": 63, "endOffset": 105}, {"referenceID": 85, "context": "At this point we have to note that the LRA method needs almost 8 days to process the 374 SAT questions (Turney, 2006), (B) needs around 6 hours (Bollegala et al.", "startOffset": 103, "endOffset": 117}, {"referenceID": 9, "context": "At this point we have to note that the LRA method needs almost 8 days to process the 374 SAT questions (Turney, 2006), (B) needs around 6 hours (Bollegala et al., 2008), while S needs less than 3 minutes.", "startOffset": 144, "endOffset": 168}, {"referenceID": 36, "context": "We then trained and tested a Naive Bayes classifier using ten-fold cross validation in the 374 SAT questions. The results of this experiment are shown in the table as NB (Naive Bayes). Finally, we also present the top results ever reported in the literature for the specific data set, which is the LRA method by Turney (2006). This is reported in the table as (LRA).", "startOffset": 75, "endOffset": 326}, {"referenceID": 36, "context": "We then trained and tested a Naive Bayes classifier using ten-fold cross validation in the 374 SAT questions. The results of this experiment are shown in the table as NB (Naive Bayes). Finally, we also present the top results ever reported in the literature for the specific data set, which is the LRA method by Turney (2006). This is reported in the table as (LRA). The results presented in Table 7 show that S ranks second among the compared lexicon-based measures with the first being the measure of Veale (2004) (V).", "startOffset": 75, "endOffset": 516}, {"referenceID": 9, "context": "The method of Bollegala et al. (2008) (B) achieves higher score than SR, but needs training in SAT questions.", "startOffset": 14, "endOffset": 38}, {"referenceID": 13, "context": "The second experiment is based on the paraphrase recognition task, using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004).", "startOffset": 114, "endOffset": 134}, {"referenceID": 36, "context": "2 Evaluation of the Omiotis Measure In order to evaluate the performance of the Omiotis measure, we performed two experiments which test the ability of the measure to capture the similarity between sentences. The first experiment is based on the data set produced by Li et al. (2006). The second experiment is based on the paraphrase recognition task, using the Microsoft Research Paraphrase Corpus (Dolan et al.", "startOffset": 144, "endOffset": 284}, {"referenceID": 71, "context": "The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001).", "startOffset": 55, "endOffset": 71}, {"referenceID": 36, "context": "1 EVALUATION ON SENTENCE SIMILARITY The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set).", "startOffset": 61, "endOffset": 78}, {"referenceID": 36, "context": "1 EVALUATION ON SENTENCE SIMILARITY The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set). The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001). For each sentence pair, similarity scores have been provided by 32 human participants, ranging from 0.0 (the sentences are unrelated in meaning), to 4.0 (the sentences are identical in meaning).12. From the 65 sentence pairs, Li et al. (2006) decided to keep a subset of 30 sentence pairs, similarly to the process applied by Miller and Charles (1991), in order to retain the sentences whose human ratings create a more even distribution across the similarity range.", "startOffset": 61, "endOffset": 547}, {"referenceID": 36, "context": "1 EVALUATION ON SENTENCE SIMILARITY The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set). The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001). For each sentence pair, similarity scores have been provided by 32 human participants, ranging from 0.0 (the sentences are unrelated in meaning), to 4.0 (the sentences are identical in meaning).12. From the 65 sentence pairs, Li et al. (2006) decided to keep a subset of 30 sentence pairs, similarly to the process applied by Miller and Charles (1991), in order to retain the sentences whose human ratings create a more even distribution across the similarity range.", "startOffset": 61, "endOffset": 656}, {"referenceID": 36, "context": "1 EVALUATION ON SENTENCE SIMILARITY The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set). The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001). For each sentence pair, similarity scores have been provided by 32 human participants, ranging from 0.0 (the sentences are unrelated in meaning), to 4.0 (the sentences are identical in meaning).12. From the 65 sentence pairs, Li et al. (2006) decided to keep a subset of 30 sentence pairs, similarly to the process applied by Miller and Charles (1991), in order to retain the sentences whose human ratings create a more even distribution across the similarity range. Thus, we apply Omiotis in this same subset of the 65 sentence pairs, described by Li et al. (2006). In this data set, we compare Omiotis against the STASIS measure of semantic similarity, proposed by Li et al.", "startOffset": 61, "endOffset": 870}, {"referenceID": 36, "context": "1 EVALUATION ON SENTENCE SIMILARITY The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set). The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001). For each sentence pair, similarity scores have been provided by 32 human participants, ranging from 0.0 (the sentences are unrelated in meaning), to 4.0 (the sentences are identical in meaning).12. From the 65 sentence pairs, Li et al. (2006) decided to keep a subset of 30 sentence pairs, similarly to the process applied by Miller and Charles (1991), in order to retain the sentences whose human ratings create a more even distribution across the similarity range. Thus, we apply Omiotis in this same subset of the 65 sentence pairs, described by Li et al. (2006). In this data set, we compare Omiotis against the STASIS measure of semantic similarity, proposed by Li et al. (2006), an LSA-based approach described by O\u2019Shea et al.", "startOffset": 61, "endOffset": 988}, {"referenceID": 36, "context": "1 EVALUATION ON SENTENCE SIMILARITY The data set produced by Li et al. (2006) comprises 65 sentence pairs (each pair consists of two sentences that are the respective dictionary word definitions of the R&G 65 word pairs data set). The used dictionary was the Collins Cobuild dictionary (Sinclair, 2001). For each sentence pair, similarity scores have been provided by 32 human participants, ranging from 0.0 (the sentences are unrelated in meaning), to 4.0 (the sentences are identical in meaning).12. From the 65 sentence pairs, Li et al. (2006) decided to keep a subset of 30 sentence pairs, similarly to the process applied by Miller and Charles (1991), in order to retain the sentences whose human ratings create a more even distribution across the similarity range. Thus, we apply Omiotis in this same subset of the 65 sentence pairs, described by Li et al. (2006). In this data set, we compare Omiotis against the STASIS measure of semantic similarity, proposed by Li et al. (2006), an LSA-based approach described by O\u2019Shea et al. (2008), and the STS measure proposed by Islam and Inkpen (2008).", "startOffset": 61, "endOffset": 1045}, {"referenceID": 27, "context": "(2008), and the STS measure proposed by Islam and Inkpen (2008). To the best of our knowledge, this data set has only been used by these three", "startOffset": 40, "endOffset": 64}, {"referenceID": 29, "context": "text semantic relatedness values, though these types of relations have been reported in the previous bibliography as advantageous (Jarmasz, 2003; Jarmasz & Szpakowicz, 2003), but their individual contribution had never been measured.", "startOffset": 130, "endOffset": 173}, {"referenceID": 29, "context": "text semantic relatedness values, though these types of relations have been reported in the previous bibliography as advantageous (Jarmasz, 2003; Jarmasz & Szpakowicz, 2003), but their individual contribution had never been measured. We also show the r correlation between the average participant (mean of individuals with group; n = 32, leave-one-out resampling and standard deviation 0.072), the worst participant (worst participant with group; n = 32, leave-one-out resampling) and the best participant (best participant with group; n = 32, leave-one-out resampling), taken from the work of O\u2019Shea et al. (2008). In addition, we have also conducted a z-test regarding the difference between Omiotis correlations and the compared measures\u2019 correlations.", "startOffset": 131, "endOffset": 615}, {"referenceID": 13, "context": "In order to further evaluate the performance of Omiotis in measuring the semantic relatedness between small text segments, we conducted additional experiments on the paraphrase recognition task using the test pairs of the Microsoft Research Paraphrase Corpus (Dolan et al., 2004).", "startOffset": 259, "endOffset": 279}, {"referenceID": 33, "context": "& Hickl, 2006), and text summarization (Madnani, Zajic, Dorr, Fazil Ayan, & Lin, 2007). For this task we computed Omiotis between the sentences of every pair and marked as paraphrases only those pairs with Omiotis value greater than a threshold. The threshold was set to 0.2, after tuning in the training set. We used a simple approach for the tuning, namely forward hill-climbing and beam search (Guyon, Gunn, Nikravesh, & Zadeh, 2006). We compare the performance of Omiotis against several other methods of various categories; more precisely, against: (a) two baseline methods, a random selection method that marks randomly each pair as being paraphrase of not (Random), and a vector-based similarity measure, using the cosine similarity measure and TF-IDF weighting for the features (VSM and Cosine) 13, (b) corpusbased methods; the PMI-IR proposed by Turney (2001), an LSA-based approach introduced by Mihalcea et al.", "startOffset": 76, "endOffset": 869}, {"referenceID": 33, "context": "& Hickl, 2006), and text summarization (Madnani, Zajic, Dorr, Fazil Ayan, & Lin, 2007). For this task we computed Omiotis between the sentences of every pair and marked as paraphrases only those pairs with Omiotis value greater than a threshold. The threshold was set to 0.2, after tuning in the training set. We used a simple approach for the tuning, namely forward hill-climbing and beam search (Guyon, Gunn, Nikravesh, & Zadeh, 2006). We compare the performance of Omiotis against several other methods of various categories; more precisely, against: (a) two baseline methods, a random selection method that marks randomly each pair as being paraphrase of not (Random), and a vector-based similarity measure, using the cosine similarity measure and TF-IDF weighting for the features (VSM and Cosine) 13, (b) corpusbased methods; the PMI-IR proposed by Turney (2001), an LSA-based approach introduced by Mihalcea et al. (2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al.", "startOffset": 76, "endOffset": 929}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al.", "startOffset": 40, "endOffset": 64}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al.", "startOffset": 40, "endOffset": 116}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al.", "startOffset": 40, "endOffset": 144}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al.", "startOffset": 40, "endOffset": 161}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al.", "startOffset": 40, "endOffset": 203}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al.", "startOffset": 40, "endOffset": 232}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al. (2006) (Comb.", "startOffset": 40, "endOffset": 331}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al. (2006) (Comb.), and (d) machine-learning based techniques, which also constitute the state of the art in paraphrase recognition, like the method of Wan et al. (2006), which trains a classifier with lexical and dependency similarity measures, the method of Zhang and Patrick (2005), who also build a feature vector with lexical similarities between the sentence pairs (e.", "startOffset": 40, "endOffset": 490}, {"referenceID": 27, "context": "(2006), and the STS measure proposed by Islam and Inkpen (2008), (c) lexiconbased methods; Jiang and Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Lesk (1986) (Lesk), Wu and Palmer (1994) (WP), and a metric that combines the measures of this category, proposed by Mihalcea et al. (2006) (Comb.), and (d) machine-learning based techniques, which also constitute the state of the art in paraphrase recognition, like the method of Wan et al. (2006), which trains a classifier with lexical and dependency similarity measures, the method of Zhang and Patrick (2005), who also build a feature vector with lexical similarities between the sentence pairs (e.", "startOffset": 40, "endOffset": 605}, {"referenceID": 90, "context": "(2006), who use an SVM classifier (Vapnik, 1995) to decide whether or not a set of features for each sentence that has been created by parsing and semantic role labelling matches or not the respective set of the second sentence in the pair, and with what importance, and, finally, the method of Finch et al.", "startOffset": 34, "endOffset": 48}, {"referenceID": 15, "context": "(2006), who use an SVM classifier (Vapnik, 1995) to decide whether or not a set of features for each sentence that has been created by parsing and semantic role labelling matches or not the respective set of the second sentence in the pair, and with what importance, and, finally, the method of Finch et al. (2005), who also train an SVM classifier based on machine translation evaluation metrics.", "startOffset": 295, "endOffset": 315}, {"referenceID": 42, "context": "The results indicate that Omiotis surpasses all the lexicon-based methods, and matches the combined method of Mihalcea et al. (2006). At this point we must mention that we also tuned Omiotis with a goal to maximize F-Measure in the test set, at the cost of dropping precision in favor of recall.", "startOffset": 110, "endOffset": 133}], "year": 2009, "abstractText": "The computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. Without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. Such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. In this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links. The approach exploits only a word thesaurus in order to devise implicit semantic links between words. Based on this approach, we introduce Omiotis, a new measure of semantic relatedness between texts which capitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the relatedness between texts. We gradually validate our method: we first evaluate the performance of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy; then, we proceed with evaluating the performance of our method in measuring text-to-text semantic relatedness in two tasks, namely sentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the proposed method outperforms every lexicon-based method of semantic relatedness in the selected tasks and the used data sets, and competes well against corpus-based and hybrid approaches.", "creator": "gnuplot 4.2 patchlevel 2 "}}}