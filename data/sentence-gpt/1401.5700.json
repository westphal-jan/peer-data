{"id": "1401.5700", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora", "abstract": "This paper describes a method for the automatic inference of structural transfer rules to be used in a shallow-transfer machine translation (MT) system from small parallel corpora. The structural transfer rules are based on alignment templates, like those used in statistical MT. Alignment templates are extracted from sentence-aligned parallel corpora and extended with a set of restrictions which are derived from the bilingual dictionary of the MT system and control their application as transfer rules.\n\n\n\n\n\n\nA system of simple to apply generalizations for structures such as linear transversal and linear transversal in parallel. The paper identifies a method to achieve this. It shows that a method for creating a linear transversal or linear transversal structure that employs a linear Transversal-based transversal structure that uses a linear transversal structure that incorporates a linear transversal structure, by virtue of its modularity and complexity.\n\n\nIn this paper the technique of classification can be used in any form of a linear transversal structure, with the goal of developing a framework for an easy-to-use system to describe.\nTransposition of structural structures: a linear transversal structure\nTransverse of structures: a linear transversal structure\nTransverse of structures: a linear transversal structure\nTransverse of structures: a linear transversal structure\nTransverse of structures: a linear transversal structure\nTransverse of structures: a linear transversal structure\nTransverse of structures: a linear transversal structure\nIn particular, a linear transversal structure can be applied to the structures of the binary transversal structure in a linear transversal structure.\nThis example shows that the structural transversal structure can be described in more detail and to be applied more effectively to structures of the binary transversal structure as well as to the structures of the binary transversal structure as well as to the structures of the binary transversal structure.\nAn example of the mechanism is the use of a linear transversal structure that relies on a transversal structure, using a linear transversal structure that relies on a linear transversal structure, but also on the structural transversal structure that relies on a linear transversal structure.\nIn general, the structural transversal structure can be described as a linear transversal structure that uses a linear transversal structure.\nAn example of the mechanism is the use of a linear transversal structure that uses a linear", "histories": [["v1", "Wed, 15 Jan 2014 05:28:26 GMT  (282kb)", "http://arxiv.org/abs/1401.5700v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felipe s\\'anchez-mart\\'inez", "mikel l forcada"], "accepted": false, "id": "1401.5700"}, "pdf": {"name": "1401.5700.pdf", "metadata": {"source": "CRF", "title": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora", "authors": ["Felipe S\u00e1nchez-Mart\u0301\u0131nez", "Mikel L. Forcada"], "emails": ["fsanchez@dlsi.ua.es", "mlf@dlsi.ua.es"], "sections": [{"heading": "1. Introduction", "text": "Machine translation (MT) may be defined as the use of a computer to translate a text from one natural language, the source language (SL), into another, the target language (TL). MT is difficult mainly because natural languages are highly ambiguous and also because two languages do not always express the same content in the same way (Arnold, 2003).\nThe different ways in which the MT problem has been approached may be classified according to the nature of the knowledge used in the development of the MT system. From this point of view, one can distinguish between corpus-based and rule-based approaches; although, hybrid approaches are also possible.\nCorpus-based approaches to MT, such as example-based MT (EBMT; Nagao, 1984; Carl & Way, 2003) and statistical MT (SMT; Brown et al., 1993; Knight, 1999), use large collections of parallel texts as the source of knowledge from which the engine learns how to perform translations. A parallel text is a text in one language together with its translation into another language; a large collection of parallel texts is usually referred as a parallel corpus. Although corpus-based approaches to MT have grown in interest over the last years, they require large amounts, in the order of tens of millions of words, of parallel text to achieve reasonable translation quality (Och, 2005). Such a vast amount of parallel corpora is not available for many under-resourced language pairs demanding MT services.\nRule-based MT (RBMT) systems use knowledge in the form of rules explicitly coded by human experts that attempt to codify the translation process. RBMT systems heavily depend on linguistic knowledge, such as morphological and bilingual dictionaries (containing\nc\u00a92009 AI Access Foundation. All rights reserved.\nlexical, syntactic and even semantic information), part-of-speech disambiguation rules or manually disambiguated corpora, and a large set of rules. The process of building a RBMT system involves considerable human effort in order to develop the necessary linguistic resources (Arnold, 2003).\nGenerally, RBMT systems work by parsing (or analyzing) the SL text, usually creating an intermediate (symbolic) representation (IR), from which the text in the TL is generated (Hutchins & Somers, 1992). According to the nature of the IR used, an RBMT system may be said to be either interlingua or transfer-based. An interlingua MT system uses a single IR that is independent of the languages involved in the translation; the advantage of using a language-independent IR is that no transfer module needs to be developed for each new language pair; as a disadvantage such an IR used is difficult to design and hard to implement, even more so, for open-domain tasks. In contrast, a transfer-based MT system uses two IRs, one for each of the languages involved; this has the advantage of easing the design and development of the IRs used, but at the cost of having to develop a transfer module for each new language pair.\nTransfer-based MT systems usually work by applying, in addition to lexical transfer mappings, a set of structural transfer rules to the SL IR created during the analysis, in order to transform it into the TL IR from which the TL text is finally generated (see Figure 1). The level of analysis, and therefore the degree of abstraction provided by the IR, varies depending on how related the languages involved are. Translating between \u201cdistant\u201d languages (such as English and Japanese) requires deep analysis (syntactic and semantic), while the translation between related languages (for example between Romance languages) can be achieved with shallow parsing. We will call this last type of transfer-based systems shallow-transfer MT systems."}, {"heading": "1.1 Overview", "text": "This paper focuses on the automatic inference from small parallel corpora of the set of structural (shallow-)transfer rules that are used by shallow-transfer RBMT systems to convert a SL IR into the TL IR from which the TL text is generated. The development of such transfer rules requires qualified people to code them manually; therefore, their automatic inference may save part of this human effort. The method we present is entirely unsupervised and benefits from information in the rest of modules of the MT system in which the inferred rules are applied, in line with the method proposed by Sa\u0301nchez-Mart\u0301\u0131nez et al. (2008) to train part-of-speech taggers in an unsupervised way for their use in MT.\nIn our approach an existing bilingual dictionary is used to guide the inference of structural transfer rules (see below), and bilingual entries for that dictionary are not learned. This is because our approach is aimed at the inference of transfer rules from small parallel corpora1 for their application in open-domain tasks. Note that small parallel corpora may\n1. Small compared to the size of corpora commonly used to build corpus-based MT systems (Och, 2005).\nbe insufficient to obtain wide-coverage bilingual dictionaries, as demonstrated by the results obtained when translating through a state-of-the-art SMT system trained on the same small parallel corpora (see section 5). Notice that manually building a bilingual dictionary for a language pair is usually much easier than developing shallow structural transfer rules for it, moreover, the former task can be partially automated.\nThe method we propose for the automatic inference of shallow-transfer rules from parallel corpora is based on the alignment template (AT) approach initially proposed for its use in the SMT framework (Och, 2002; Och & Ney, 2004). An AT can be defined as a generalization performed over aligned phrase2 pairs (or translation units) by using word classes.\nTo adapt the AT approach to the RBMT framework, ATs are extended with a set of restrictions that control their application as structural shallow-transfer rules. To that end:\n\u2022 the bilingual dictionary of the RBMT system in which the inferred rules will be integrated is used to ensure that the lexical content of each bilingual phrase pair extracted from the training corpus (see section 2.2) can be reproduced by the MT system;\n\u2022 linguistically motivated word classes are used to generalize the extracted bilingual phrase pairs, deriving ATs from them; and,\n\u2022 a set of restrictions, derived from the bilingual dictionary of the RBMT system, is attached to each AT to control its application as part of a transfer rule; this extension of the definition of AT will be called extended AT.\nOnce these extended ATs have been extracted from the training corpora, transfer rules are generated from them. In the experiments reported in section 5, shallow-transfer rules to be used by the Apertium MT engine (see appendix A) are generated directly in Apertium\u2019s XML-based structural transfer language. An interesting property of the inferred rules is that they are human-readable and may, therefore, be edited by human experts to improve their performance or supplemented with new rules; MT developers can use this method to infer an initial set of rules and then improve them by focusing on the more difficult issues.\nBoth this method (Sa\u0301nchez-Mart\u0301\u0131nez & Forcada, 2007) and its predecessor (Sa\u0301nchezMart\u0301\u0131nez & Ney, 2006) have already been presented in conferences; here, we explain the method in more detail, test it on two additional language pairs and use training corpora of different sizes so as to evaluate the impact of size on translation quality. Moreover, in this paper we perform a more detailed analysis of the inferred rules and the results obtained; to that end, we provide confidence intervals, which allow for a better interpretation of the results achieved. We will also discuss the process followed to build the parallel corpora used to learn transfer rules.\n2. For the purpose of this paper, and to stick to the terminology used by Och and Ney (2004) in the definition of AT and by most SMT practitioners, by phrase we refer to any text segment, not necessarily a well-formed syntactic constituent."}, {"heading": "1.2 Related Work", "text": "There have been other attempts to learn automatically or semi-automatically the structural transformations needed to produce correct translations into the TL. Those approaches can be classified according to the translation framework to which the learned rules are applied.\nSome approaches learn transfer rules to be used in RBMT. Probst et al. (2002) and Lavie et al. (2004) developed a method to learn transfer rules for MT involving underresourced languages (such as Quechua) with very limited resources. To this end, a small parallel corpus (of a few thousand sentences) is built with the help of a small set of bilingual speakers of the two languages. The parallel corpus is obtained by translating a controlled corpus from the language with more resources (English or Spanish) into the under-resourced language by means of an elicitation tool. This tool is also used to graphically annotate the word alignments between the two sentences. Finally, hierarchical syntactic rules, which can be seen as constituting a context-free transfer grammar, are inferred from the aligned parallel corpus.\nMenezes and Richardson (2001) propose a method to infer transfer mappings (rules) between source and target languages. Prior to the acquisition of the transfer mappings, they align the nodes of the source and target parse trees by using an existing bilingual lexicon in which they look for word correspondences. Then, following a best-first strategy and using an small alignment grammar their method aligns the remaining (not-aligned) nodes. Once the alignments between the nodes of both parse trees have been obtained, frequencies are computed and sufficient context is retained to disambiguate between competing mappings at translation time. Our approach greatly differs from the one by Menezes and Richardson: (i) because they use a syntactic parser, a bilingual dictionary and an alignment grammar to obtain the word alignments from the sentence-aligned parallel corpus, while we only use statistical methods; (ii) because of how they use the bilingual dictionary, we use it to discard useless bilingual phrases and to derive restrictions to control the application of ATs, not for the computation of the word alignments; and (iii) because in our approach there is no ambiguity to solve at translation time.\nCaseli et al. (2006) propose a method to infer bilingual resources (structural transfer rules and bilingual dictionaries) to be used in shallow-transfer MT from aligned parallel corpora. Previously to the generation of transfer rules, alignment blocks (sequences of aligned words) are built from the translation examples found in the parallel corpus by considering three different types of word alignments according to their geometry (crossings, unaligned words, etc.). Then, shallow-transfer rules are built in a three-step procedure. In the first step, they identify the patterns in two phases, monolingual and bilingual; then in a second step their method generates shallow-transfer rules by deriving monolingual and bilingual constraints, that can also be seen as the rule itself; finally, in a third step the rules are filtered in order to solve the ambiguity caused by rules matching the same SL sequence of words. The inferred rules are human-readable, as are those inferred with the method we propose, and may therefore be also edited by human experts. Our approach differs from that of Caseli et al. in how rules are induced: while our approach uses bilingual phrase pairs without being concerned about the type of alignments between the words, the way in which Caseli et al. induce rules depends on the type of the alignment blocks. In addition, our approach does not ever produce more than one rule matching the same sequence of\nSL items, and therefore no ambiguity needs to be solved. Furthermore, we do not infer a bilingual dictionary; instead, we use an existing bilingual dictionary to guide the inference of shallow-transfer rules, and to control the application of the inferred rules.\nIn the EBMT framework, some researchers have dealt with the problem of inferring a kind of translation rules called translation templates (Kaji et al., 1992; Brown, 1999; Cicekli & Gu\u0308venir, 2001). A translation template can be defined as a bilingual pair of sentences in which corresponding units (words or phrases) are coupled and replaced by variables. Liu and Zong (2004) provide an interesting review of the different research works dealing with translation templates. Brown (1999) uses a parallel corpus and some linguistic knowledge in the form of equivalence classes (both syntactic and semantic) to perform a generalization over the bilingual examples collected. The method works by replacing each word by its corresponding equivalence class and then using a set of grammar rules to replace patterns of words and tokens by more general tokens. Cicekli and Gu\u0308venir formulate the acquisition of translation templates as a machine learning problem, in which the translation templates are learned from the differences and similarities observed in a set of different translation examples, using no morphological information at all. Kaji et al. use a bilingual dictionary and a syntactic parser to determine the correspondences between translation units while learning the translation templates. Our approach differs from those applied in the EBMT framework because, on the one hand, the transfer rules generated through the method we propose are mainly based on lexical forms (consisting of lemma, lexical category and morphological inflection information) and, on the other hand, because they are flatter, less structured and non-hierarchical, which makes them suitable for shallow-transfer MT. Moreover, the way in which translation rules are chosen for application greatly differs from how they are chosen in the EBMT framework.\nFinally, in the SMT framework the use of AT (Och & Ney, 2004) can be seen as an integration of translation rules into statistical translation models, since an AT is a generalization or an abstraction, of the transformations to apply when translating SL into TL by using word classes.\nThe rest of the paper is organized as follows: the next section reviews the alignment template (AT) approach; section 3 explains how ATs are extended with a set of restrictions in order to use them to generate shallow-transfer rules to be used in RBMT (section 4). Section 5 describes the experiments conducted and the results achieved. Finally, section 6 discusses the method described and outlines future research lines."}, {"heading": "2. The Alignment Template Approach", "text": "The alignment template (AT) approach (Och, 2002; Och & Ney, 2004) was introduced in the SMT framework as one of the feature functions in the maximum entropy model (Och & Ney, 2002) to try to generalize the knowledge learned for a specific phrase to similar phrases.\nAn AT performs a generalization over bilingual phrase pairs using word classes instead of words. An AT z = (Sm, Tn, A) consists of a sequence Sm of m SL word classes, a sequence Tn of n TL word classes, and a set of pairs A = {(i, j) : i \u2208 [1, n] \u2227 j \u2208 [1,m]} with the alignment information between the TL and SL word classes in the two sequences.\nLearning a set of ATs from a sentence-aligned parallel corpus consists of: (i) the computation of the word alignments, (ii) the extraction of bilingual phrase pairs, and (iii) the generalization of such bilingual phrase pairs by using word classes instead of the words themselves."}, {"heading": "2.1 Word Alignments", "text": "A variety of methods, statistical (Och & Ney, 2003) or hybrid (Caseli et al., 2005),3 may be used to compute word alignments from a (sentence-aligned) parallel corpus. In the experiments reported in section 5, word alignments are obtained by training classical statistical translation models to translate from language L1 to language L2 (and vice versa) and then computing the Viterbi alignments under the previously estimated translation models. The Viterbi alignment between SL and TL sentences is defined as the alignment whose probability is maximal under the translation models previously estimated. The resulting Viterbi alignments A1 and A2 (one for each translation direction) are symmetrized through the refined intersection method proposed by Och and Ney (2003, p. 33). Symmetrization is needed in order to allow a SL word to be aligned with more than one TL word; otherwise, wrong alignments are obtained when a SL word actually corresponds to more than one TL word.\nFigure 2 shows the word alignment in a Spanish\u2013English sentence pair. The alignment information is represented as a binary matrix in which a value of 1 (large black squares) means that the words at the corresponding positions are aligned; analogously, a value of 0 (small black squares) means that the words are not aligned.\n3. Caseli et al.\u2019s (2005) method is hybrid because prior to the application of heuristics, it uses a statistical tool (NATools) to obtain a probabilistic bilingual dictionary (Simo\u0303es & Almeida, 2003)."}, {"heading": "2.1.1 Training", "text": "In order to train the translation models and to calculate the Viterbi alignments of each pair of aligned sentences found in the training corpus the free/open-source GIZA++ toolkit4 (Och & Ney, 2003) is used with default parameters.\nThe computation of the word alignments consists of:\n1. training the IBM model 1 (Brown et al., 1993) for 5 iterations; in this model, word order does not affect the alignment probabilities;\n2. training the HMM alignment model (Vogel et al., 1996) for 5 iterations; this alignment model has the property of making alignment probabilities explicitly dependent on the alignment position of the previous word;\n3. training the IBM model 3 (Brown et al., 1993) for 5 iterations; in this model, the probability of an alignment depends on the positions of the aligned words and on the length of SL and TL sentences. In addition, IBM model 3 also introduces fertilities; the fertility of a word is defined as the number of aligned words in the other language. And finally,\n4. training the IBM model 4 (Brown et al., 1993) for 5 iterations; this model is identical to IBM model 3 except for the fact that it models the reordering of phrases that may be moved around as units.\nNote that after obtaining the Viterbi alignments these statistical translation models are no longer used."}, {"heading": "2.2 Extraction of Bilingual Phrase Pairs", "text": "Bilingual phrase pairs are automatically extracted from the word-aligned sentence pairs. Usually, the extraction of bilingual phrase pairs (Zens et al., 2002) is performed by considering all possible pairs below a certain length and ensuring that: (i) all words are consecutive, and (ii) words within the bilingual phrase pair are not aligned with words from outside.\nThe set BP(wSJ1 , wT I 1, A) of bilingual phrases that are extracted from the word-aligned\nsentence pair (wS1, . . . , wSJ), (wT1, . . . , wTI) may be formally expressed as follows:\nBP(wSJ1 , wT I 1, A) = {(wS j+m j , wT i+n i ) :\n\u2200(i\u2032, j\u2032) \u2208 A : j \u2264 j\u2032 \u2264 j +m\u21d4 i \u2264 i\u2032 \u2264 i+ n}.\nHowever, in our approach bilingual phrase pairs are also required to have their first and last words on both sides (source and target) aligned with at least one word in the other side.5 Integrating these additional constraints, previous equation may be rewritten as:\nBP(wSJ1 , wT I 1, A) = {(wS j+m j , wT i+n i ) :\n4. http://www.fjoch.com/GIZA++.html 5. Experiments conducted without such requirement show a significant degradation of the translation qual-\nity achieved with the inferred rules.\n(\u2200(i\u2032, j\u2032) \u2208 A : j \u2264 j\u2032 \u2264 j +m\u21d4 i \u2264 i\u2032 \u2264 i+ n) \u2227 (\u2203k \u2208 [i, i+ n] : (wSj , wTk) \u2208 A) \u2227 (\u2203k\u2032 \u2208 [i, i+ n] : (wSj+m, wTk\u2032) \u2208 A) \u2227 (\u2203l \u2208 [j, j +m] : (wSl, wTi) \u2208 A) \u2227 (\u2203l\u2032 \u2208 [j, j +m] : (wSl\u2032 , wTi+n) \u2208 A)}.\nFigure 3 shows the set bilingual phrase pairs with more than one SL word extracted from the word-aligned Spanish\u2013English sentence pair shown in Figure 2."}, {"heading": "2.3 Generalization", "text": "The generalization of the bilingual phrase pairs is simply done by using word classes instead of the words themselves; to that end, a function that maps single words into word classes is defined. The use of word classes allows the description of word reorderings, preposition changes and other divergences between SL and TL. Och and Ney (2004) use automatically obtained (Och, 1999) word classes to extract ATs for SMT. However, for RBMT, linguistically motivated word classes related to those used by the remaining modules in the MT system must be used (see section 3.1)."}, {"heading": "3. Alignment Templates for Shallow-Transfer Machine Translation", "text": "To apply the AT approach in a shallow-transfer MT system, the parallel corpus from which the ATs are learned must be in the intermediate representation (IR) used by the translation engine. In shallow-transfer MT the transformations to apply are mainly related to lexical forms; therefore, the IR used by the translation engine usually consists of lemma, lexical category and morphological inflection information for each word.\nIn order to convert the parallel corpus into the IR used by the engine, the analysis modules (morphological analyzers and part-of-speech taggers) of the engine are used to analyze both sides of the parallel corpus before computing the word alignments. After analyzing both sides of the parallel corpus we have, for each word, its lemma, lexical category and morphological inflection information. Note that generalizations are performed after word alignments and bilingual phrase pair extraction by using word classes based on that morphological information (see next section)."}, {"heading": "3.1 Word-Class Definition", "text": "As the transformations to apply are mainly based on the lexical category and inflection information of SL and TL words, the function that maps words into word classes will map each word into a word class representing its lexical category and morphological inflection information (such as verb, preterite tense, third person, plural).\nUsing the lexical category and morphological inflection information to define the set of word classes allows the method to learn general syntactic rules such as reordering and agreement rules, and verb tense changes, among others. However, in order to learn lexical changes, such as preposition changes or auxiliary verb usage, some words will be assigned single-word classes representing a lexical form, as discussed next."}, {"heading": "3.1.1 Lexicalized Categories", "text": "A set of (lexicalized) categories usually involved in lexical changes such as prepositions and auxiliary verbs may be provided. For those words whose lexical category is in the set of lexicalized categories (from now on, lexicalized words) the lemma is also used when defining the word class they belong to. In this way, lexicalized words are placed in single-word classes representing a particular lexical form. For example, if prepositions are considered lexicalized categories, words to and for would be in different word classes, even if they have the same lexical category and morphological inflection information, whereas words book and house would be in the same word class (noun, singular).\nTypically the set of lexicalized categories is a subset of the set of closed categories, that is, those that do not grow by addition of new words to the lexicon: pronouns, auxiliary verbs, prepositions, conjunctions, etc. The most typical lexicalized words are prepositions, as they usually have many different translations depending on the SL context.\nFigure 4 shows an example of a Spanish\u2013Catalan bilingual phrase and the generalization performed when each word is replaced by its corresponding word class; words in boldface correspond to lexicalized categories. The AT shown in Figure 4 generalizes, on the one hand, the use of the auxiliary Catalan verb anar to express the past perfect (preterite) tense and, on the other hand, the preposition change when it refers to a location name, such as the name of a city or a country. Note that lexicalized words (e.g. anar-(vaux.pres.3rd.pl), en-(pr)) coexist in the same AT with non-lexicalized categories (e.g. (verb.inf), (noun.loc)) without distinction."}, {"heading": "3.2 Extending the Definition of Alignment Template", "text": "In section 2 an AT was defined as a tuple z = (Sm, Tn, A) in which only the alignment A between SL and TL word classes was considered. Here the definition of AT is extended to z = (Sm, Tn, A,R), where a set of restrictions, R, over the TL inflection information of the non-lexicalized categories, is added to control its application as part of a transfer rule."}, {"heading": "3.2.1 TL Restrictions", "text": "When translating (see section 4.3.1), that is, when applying the inferred ATs, the TL inflection information of non-lexicalized words is taken from the corresponding (aligned) TL word class in the AT being applied, not from the bilingual dictionary; because of this, restrictions are needed in order to prevent an AT to be applied in certain conditions that would produce an incorrect translation.\nTo illustrate the need for such restrictions let us consider what would happen when translating the Spanish phrase la silla roja6 into Catalan by applying the extended AT shown in Figure 5, which should not be applied in this case. This AT generalizes the propagation of the masculine gender to the article and the adjective when translating a SL (Spanish) noun that is feminine singular in the SL (same with the article and the adjective) and has a masculine equivalent into Catalan, which is not the case of silla. After applying the extended AT in Figure 5, the morphological generator (see Appendix A) has to inflect the lexical form cadira-(noun.m.sg), which does not exist in Catalan,7 as cadira is feminine. By taking into account some restrictions over the TL inflection information, such as the one referring to w2 in the extended AT in Figure 5, we prevent the application of an AT if its application would produce an incorrect lexical form to inflect, as in the running example.\nTL restrictions are obtained from the bilingual dictionary of the MT system in which the inferred transfer rules will be integrated. Bilingual dictionaries may explicitly code all the inflection information of the translation of each SL lexical form, or only the inflection information that changes from one language to the other. TL restrictions could be derived from both kinds of bilingual dictionaries; however, their extraction is easier in the second case, that is, if only changes in the inflection information are explicitly coded.\nFor the experiments (see section 5) the Apertium MT platform has been used; in Apertium bilingual dictionaries, only changes in inflection information are explicitly coded. The following two examples show, on the one hand, a Spanish\u2013Catalan bilingual entry and, on the other hand, the restriction over the TL inflection information for the Spanish-to-Catalan translation derived for that bilingual entry:8\n6. Translated into English as the red chair. 7. Note that the lexical category and morphological inflection information of the TL lexical form to inflect\nhas been taken from the TL part of the AT. 8. Lemmas between tags <l> and </l> (left) correspond to Spanish words; analogously, lemmas between\ntags <r> and </r> (right) correspond to Catalan words. Lexical category and inflection information is coded through the tag <s> (symbol), the first one being the lexical category.\n\u2022 Bilingual entry without any change in inflection information\n<e><p> <l>castigo<s n=\"noun\"/></l> <r>ca\u0300stig<s n=\"noun\"/></r> </p></e>\nRestriction: w=noun.*\n\u2022 Bilingual entry in which the gender changes from feminine (Spanish) to masculine (Catalan)\n<e><p> <l>calle<s n=\"noun\"/><s n=\"f\"/></l> <r>carrer<s n=\"noun\"/><s n=\"m\"/></r> </p></e>\nRestriction: w=noun.m.*\nAs can be seen, restrictions provide the lexical category and morphological inflection information that the lexical form should have at translation time after looking it up in the bilingual dictionary; the star at the end of each restriction means that the rest of inflection information is not restricted. The second bilingual entry would be responsible of the restrictions attached to w2 in the AT shown in Figure 5. That AT can only be applied if the noun (w2) is masculine in the TL (see next section to know how ATs are applied); note that the inflection information of w3 is not restricted at all; this is because w3 refers to an adjective that can be both masculine and feminine, as its gender depends on the gender of the noun it qualifies."}, {"heading": "4. Generation of Apertium Transfer Rules", "text": "This section describes the automatic generation of Apertium structural shallow-transfer rules; note, however, that the generation of transfer rules for other shallow-transfer MT systems would also be feasible by following the approach presented here.\nThe structural transfer in Apertium (see appendix A) uses finite-state pattern matching to detect, in the usual left-to-right, longest-match way, fixed-length patterns of lexical forms to process and performs the corresponding transformations. A (generic) shallow-transfer rule consists of a sequence of lexical forms to detect and the transformations that need to be applied to them."}, {"heading": "4.1 Discarding Useless Bilingual Phrase Pairs", "text": "Not all bilingual phrase pairs are useful in the inference of transfer rules, since the generalization that would be performed from some of them cannot be used in RBMT; more precisely, bilingual phrase pairs satisfying one or both of the following conditions are useless, and therefore, discarded:\n\u2022 SL and TL non-lexicalized words are not aligned. When translating a SL nonlexicalized word (see next section) the inflection information is taken from the aligned TL word class, therefore the corresponding alignment must exist.\n\u2022 the bilingual phrase pair cannot be reproduced by the MT system in which the transfer rules will be used. This happens when the translation equivalent in the bilingual dictionary differs from the one observed in the bilingual phrase. Note that TL restrictions are extracted from the bilingual dictionary, and if translation equivalents do not agree the extracted AT could end up having a set of restrictions making no sense at all."}, {"heading": "4.2 Selecting the Alignment Templates to Use", "text": "To decide which ATs to take into account for the generation of rules, the method is provided with a frequency count threshold. ATs whose frequency count is below this threshold are discarded. In the experiments, two different ways of interpreting the frequency count have been tested:\n\u2022 to use directly the frequency count c, and\n\u2022 to use a modified frequency count c\u2032 = c(1 + log(l)), where l stands for the length of the SL part of the AT.\nThe second approach aims at solving the problem caused by the fact that longer ATs have lower frequency counts but may be more accurate as they take more context into account. A similar approach was used by (Mikheev, 1996) in his work on learning part-of-speech guessing rules to favor longer suffixes over shorter ones."}, {"heading": "4.3 Rule Generation", "text": "A rule consists of a set U of extended ATs with the same sequence of SL word classes, but different sequences of TL word classes, different alignment information or different set of TL restrictions. Formally this may be expressed as follows:\nU = {(Sm, Tn, A,R) \u2208 Z : Sm = SU}, (1)\nwhere Z refers to the whole set of ATs and SU to the sequence of SL word classes that all ATs in U have in common. Note that each rule matches a different sequence SU of SL word classes and, therefore, there is no ambiguity in the application of the shallow-transfer rules at translation time.\nEach rule U is coded in Apertium\u2019s XML-based transfer language. The code generated for each rule applies always the most frequent AT in U that satisfies the TL restrictions R; therefore, competing ATs are selected according to their frequency. A \u201cdefault\u201d AT, which translates word for word, is always added with the lowest frequency count. This AT has no TL restrictions and is the one applied when none of the remaining ATs can be applied because their TL restrictions are not met.\nTo check if the restrictions over the TL inflection information of an AT are met, the translation of each non-lexicalized word is retrieved from the bilingual dictionary; then, the\nretrieved morphological attributes (lexical category and inflection information) are compared with those specified by the corresponding restriction; the AT will be applicable if all restrictions hold."}, {"heading": "4.3.1 Application of an Alignment Template", "text": "The code generated in the Apertium\u2019s XML-based transfer language that applies an AT is guided by the sequence Tn of TL word classes. The actions to perform for each unit in Tn depend on the type of its word class:\n\u2022 if the word class corresponds to a non-lexicalized word, the translation of the lemma of the aligned SL (non-lexicalized) word is retrieved by looking it up in the bilingual dictionary; then, the lexical category and morphological inflection information provided by the TL word class are attached to the translated lemma;\n\u2022 if the word class corresponds to a lexicalized word, it is introduced as is; remember that word classes belonging to lexicalized words represent complete lexical forms consisting of lemma, lexical category and morphological inflection information.\nNote that the information about SL lexicalized words is not taken into account when applying a given AT (just when detecting it).\nThe following example illustrates how the AT shown in Figure 4 would be applied in order to translate from Spanish to Catalan the input text vivieron en Francia.9 This text segment, after morphological analysis and part-of-speech tagging, is transformed by the MT engine into the SL IR vivir-(verb.pret.3rd.pl) en-(pr) Francia-(noun.loc), which becomes the input to the structural transfer module. The AT is applied in the order specified in its TL part. For the word classes corresponding to non-lexicalized words, the aligned SL words are translated into TL (Catalan) by looking them up in the bilingual dictionary: vivir is translated as viure and Francia is translated as Franc\u0327a. Then, the inflection information provided by the TL part of the AT (see Figure 4) is attached to each translated lemma. Finally, word classes corresponding to lexicalized words are just copied to the output as they appear in the TL part of the AT. For the running example the structural transfer output would be the TL IR anar-(vaux.pres.3rd.pl) viure-(verb.inf) a-(pr) Franc\u0327a-(noun.loc), which the morphological generation module would transform into the Catalan phrase van viure a Franc\u0327a.\n9. Translated into English as They lived in France."}, {"heading": "5. Experiments", "text": "The approach presented in this paper has been tested on both translation directions of the Spanish\u2013Catalan (es-ca) and Spanish\u2013Galician (es-gl) language pairs, and on the Spanish-to-Portuguese (es-pt) translation.10,11\nThe parallel corpora used for training are from different sources. The Spanish\u2013Catalan parallel corpora come from El Perio\u0301dico de Catalunya,12 a daily newspaper published both in Catalan and Spanish; the Spanish\u2013Galician parallel corpora come from Diario Oficial de Galicia,13 the official publication of the autonomous government of Galicia published both in Galician and Spanish; the Spanish\u2013Portuguese parallel corpora come from The JRCAcquis Multilingual Parallel Corpus (Steinberger et al., 2006)14 which contains European Union (EU) law applicable in the EU member states.\nTo test the importance of the amount of parallel corpora available for training we have used corpora of different sizes. More precisely, we have used training corpora of around 0.25, 0.5, 1.0, 1.5, and 2.0 million words in each language. The corpora were built in such a way that, for the same language pair, the larger corpora include the shorter ones. Note that word alignments have been computed from each different training corpus in isolation before the extraction of the extended ATs that are then used in the inference of shallow-transfer rules.\nAs was explained in section 3.1, a set of categories usually involved in lexical changes needs to be provided for the definition of word classes so as to learn not only syntactic transformations, but also lexical transformations. To that end, a small set of eight to ten lexicalized categories is used for each language. The most common lexicalized categories are: prepositions, pronouns, determiners, subordinate conjunctions, relatives, modal verbs and auxiliary verbs.\nThe length of the bilingual phrase pairs extracted and used to obtain the ATs has been restricted to a maximum of 7 SL words for all the experiments. Remember from section 2.2 that to extract bilingual phrases from a pair of word-aligned sentences all possible pairs (within a certain length) are considered; by restricting that length we are making the problem computationally affordable.\nWith respect to the frequency count threshold used to select the set of ATs to take into account (see section 4.2), we have tested frequency count thresholds between 5 and 40 for all translation tasks and AT selection criteria. The frequency count used in the evaluation is the one giving the best translation edit rate (TER; Snover et al., 2006) when translating a corpus, similar to the one used for testing, with 1 000 sentences (see Table 1); in Table 5 (page 627) we provide the thresholds used when the rules are inferred from the corpus with 2.0 million words in each language.\n10. All linguistic data used can be freely downloaded from http://sf.net/projects/apertium, packages apertium-es-ca-1.0.2 (around 12 800 bilingual entries), apertium-es-gl-1.0.4 (around 10 800 bilingual entries) and apertium-es-pt-0.9.2 (around 11 000 bilingual entries); the number of bilingual entries reported correspond to lemma-based entries. 11. A possible criticism here is that we have not used a standard translation task to test our approach; we have not done so because the Apertium linguistic resources (morphological and bilingual dictionaries) necessary for those standard tasks were not available. 12. http://www.elperiodico.com 13. http://www.xunta.es/diario-oficial 14. http://wt.jrc.it/lt/Acquis/"}, {"heading": "5.1 Evaluation", "text": "The performance of the presented approach is compared to that of the same MT system when no transfer rules are used at all (word-for-word MT), to that of the same MT system when using the hand-coded transfer rules,15 and to that of using a state-of-the-art SMT system trained using the same parallel corpora. For the latter we have used the free/open-source SMT toolkit Moses (Koehn et al., 2007) and the SRILM language modelling toolkit (Stolcke, 2002). The training of the SMT system was done as follows:16 First, the translation model was trained using the 90% of the training corpus. Then, a 5-gram language model was trained using the SRILM toolkit with the whole training corpus. Finally, the minimum error \u201crate\u201d training algorithm (Och, 2003) used the remaining 10% of the training corpus to adjust the weight of each feature.17 The features used by the SMT system are those used by Moses by default: 5 phrase-table features (source-to-target and target-to-source phrase translation probabilities, source-to-target and target-to-source lexical weightings, and phrase penalty), a distance-based cost (total number of word movements), the sentence word count, and the TL model.\nTranslation performance is evaluated using two different measures; on the one hand, the translation edit rate (TER; Snover et al., 2006), and on the other hand, the bilingual evaluation understudy (BLEU; Papineni et al., 2002); in both cases the same evaluation corpora have been used and the confidence intervals of the measures being reported are given (see below)."}, {"heading": "5.1.1 Confidence intervals", "text": "Confidence intervals of MT quality measures are calculated through the bootstrap resampling method as described by Koehn (2004). In general, the bootstrap resampling method consists of estimating the precision of sample statistics (in our case, translation quality measures) by randomly resampling with replacement (that is, allowing repetitions) from the full set of samples (Efron & Tibshirani, 1994); in MT, sentences and their respective\n15. Those in the corresponding Apertium language packages. 16. For detailed training instructions visit http://www.statmt.org/wmt09/baseline.html. 17. The minimum error \u201crate\u201d training used BLEU as an evaluation measure.\nreference translations. This method has the property that no assumptions are made about the underlying distribution of the variable, in our case, the MT quality measure.\nThe calculation of the confidence intervals consists of the following steps:\n1. the translation performance is evaluated a large number of times, in our experiments 1 000 times, using randomly chosen sentences from the test corpus, and their counterpart sentences in the reference corpus;\n2. all the calculated measures are sorted in ascending order; and\n3. the top q% and the bottom q% elements are removed from that list.\nAfter that, the remaining values are in the interval [a, b]. This interval approximates with probability 1\u2212 2q/100 the range of values in which the quality measure being reported lies for test corpora with a number of sentences equal to that used to carry out the evaluation."}, {"heading": "5.1.2 Evaluation corpora", "text": "Table 2 shows the number of sentences and the number of SL and TL words of the different test corpora used for the evaluation of the inferred rules for each translation being considered. These test corpora come from independent parallel corpora, from a different source, with no relation to those used for training. More precisely, the test corpora for Spanish\u2013 Catalan and Spanish\u2013Galician comes from Revista Consumer Eroski (Alca\u0301zar, 2005),18 a magazine addressed to consumers published in Spanish, Catalan, Galician and Basque; the test corpora for Spanish\u2013Portuguese comes from the shared evaluation task of the 2008 workshop on SMT.19"}, {"heading": "5.2 Results", "text": "Figure 6 shows the TER and BLEU scores, together with their respective 95% confidence intervals, achieved for each translation direction of the Spanish\u2013Catalan language pair when using training corpora of different sizes. The error rates reported are: (a) the results when the frequency count is directly used to select the set of ATs to use for the rules generation, (b) the results achieved by a state-of-the-art SMT system trained on the same corpora, (c)\n18. http://revista.consumer.es 19. http://www.statmt.org/wmt08/wmt08-eval.tar.gz\nthe results achieved when using hand-coded transfer rules, and (d) the results of a wordfor-word translation (when no structural transformations are applied). The results achieved when the modified frequency count described in section 4.2 is used to select the set of ATs to use are not reported since they are indistinguishable in practice from those achieved by using directly the frequency count; for this reason, they will not be considered in the rest of the experiments. Notice that in all cases, except for the SMT results, the same linguistic data (morphological and bilingual dictionaries) have been used. Some Catalan-to-Spanish translations produced by the automatically inferred rules are shown in Table 3.\nResults in Figure 6 show that, as expected, the translation quality achieved by the inferred transfer rules is better than that of a word-for-word translation, even when a small parallel corpus with around 0.5 million words in each language is used; note however, that in the case of the Spanish-to-Catalan translation confidence intervals overlap for a training corpus of 0.25, 0.5 and 1.0 million words, the overlap being smaller for the latter.\nResults in Figure 6 also show that a SMT system performs worse than the rules automatically inferred from the same parallel corpus and even worse than a word-for-word translation. This is because the training corpora we have used are not large enough to learn a wide-coverage bilingual lexicon and, consequently, most of the words to translate are unknown to the SMT system. Remember that our approach only learns transfer rules from the parallel corpus, not bilingual entries, and that the same bilingual dictionary is used by the hand-coded rules, the automatically inferred rules and the word-for-word translation. In section 5.2.1 (page 626) we discuss the results achieved by the SMT system when the bilingual dictionary in the corresponding Apertium package is added to the SMT training data.\nFigure 7 shows, for each translation direction of the Spanish\u2013Galician language pair, the same MT quality measures and for the same translation setups reported for Spanish\u2013 Catalan in Figure 6.\nThe Spanish\u2013Galician language pair shows results in agreement to those obtained for Spanish\u2013Catalan; however, the improvement on the Galician-to-Spanish translation quality, compared to word-for-word translation, is smaller. In addition, the improvement obtained in the case of Spanish\u2013Catalan by increasing the amount of corpora used for training is greater\nSpanish to Catalan\nSpanish to Galician\nSpanish to Portuguese\nthan that of Spanish\u2013Galician, as shown by the slope of the curve. This is because the significant frequent patterns which can be learned from the training corpora are selected very early. Note that our method is unlikely to perform worse than word-for-word translation (when no rules are used).\nConcerning the Spanish-to-Portuguese translation, Figure 8 shows the TER and BLEU scores achieved for the different sizes of training corpora used. Notice that the automatically inferred rules perform better than the word-for-word translation, although their confidence intervals show a large overlap. It is worth mentioning that the confidence intervals obtained for the hand-coded transfer rules also overlap with those of the automatically inferred rules and the word-for-word translation. As in the rest of experiments the SMT system performs worse because the training corpus is not large enough to learn a wide-coverage bilingual lexicon.\nThe difference between the results achieved when using hand-coded transfer rules and that of using no rules at all (word-for-word translation) is very small compared to the rest of translation tasks considered in this paper. Moreover, the TER and BLEU scores obtained are very poor although Spanish and Portuguese are two related languages, and, therefore, translating between them should not be such a difficult task. Indeed, an evaluation of the hand-coded transfer rules performed using an evaluation corpus in which the reference translation is a post-edited (corrected) version of the MT output produced with the same hand-coded rules shows a TER below 10%.\nThe poor results obtained for the Spanish-to-Portuguese may be explained by the fact that the evaluation corpus, as well as the training corpora used, may have not been built by translating one language (say Spanish or Portuguese) into the other, but by translating\nfrom a third language (possibly English or French).20 This causes the reference translation to be very different compared to the translations automatically performed, thus giving very high TERs. On the other hand, this may also cause the alignments obtained from the training corpora to be unreliable, as shown by the percentage of discarded bilingual phrase pairs. This percentage is, for all training corpora, around 54% for the Spanish-to-Portuguese translation, about 22% for the Spanish\u2013Catalan language pairs, and around 20% for the Spanish\u2013Galician language pair."}, {"heading": "5.2.1 Adding the bilingual dictionary to the SMT training data", "text": "With the aim of testing whether the difference in the translation performance between the shallow-transfer rules and the SMT system is due to the fact that Apertium uses a wide-coverage, manually-built bilingual dictionary, we have added the bilingual dictionary in the corresponding Apertium package to the SMT training data (Tyers et al., 2009).21 It is worth noting that adding the bilingual dictionary to the training corpus does not only improve the vocabulary coverage of the SMT systems inferred, but also helps the word alignment process by adding word-to-word alignment, which gives an additional advantage to the SMT system with respect other systems; the bilingual dictionary has not been added to corpus used to learn the AT used for the automatic inference of shallow-transfer rules.\nTable 4 shows the 95% confidence intervals for the TER and the ratio of out-of-vocabulary (OOV) words when the test corpus is translated by means of Apertium with the shallowtransfer rules automatically obtained form the parallel corpus with 2.0 million words in each language (AT-count); when it is translated using a SMT system trained on this same parallel corpus (SMT); and, when it is translated with a SMT system trained with a par-\n20. Remember that the training corpora contains European Union law and that the evaluation corpus comes from the European Parliament proceedings. 21. Apertium bilingual dictionaries contain lemma-based bilingual entries which have been expanded to include all possible inflected forms before adding them to the SMT training data. After inflecting all the lemma-based bilingual entries the bilingual dictionary added to the SMT training data consists of (approximately) 1.8 million entries for Spanish\u2013Catalan, 1.2 million entries for Spanish\u2013Galician, and 0.9 million entries for Spanish\u2013Portuguese.\nallel corpus containing the original corpus with 2.0 million words in each language plus the corresponding Apertium bilingual dictionary (SMT+dictionary).\nThe results in Table 4 show that, as expected, the SMT results improve when the bilingual dictionary is added to the training corpus; note however, that the results obtained for es-ca, ca-es, es-gl, and es-pt are still worse than those achieved by the automatically inferred rules, although the ca-es \u201cSMT+dictionary\u201d confidence interval shows a large overlap with that of the automatically inferred rules. The only translation task in which the \u201cSMT+dictionary\u201d system provides better results than the automatically inferred rules is the gl-es task, although its confidence interval overlaps with that of the automatically inferred rules. In all cases the ratio of OOV words for \u201cSMT+dictionary\u201d is below that of the automatically inferred rules because some words not present in the bilingual dictionary do appear in the training corpus."}, {"heading": "5.2.2 Analysis of the inferred rules", "text": "Table 5 shows, for each translation task, the frequency count threshold used in the generation of rules, the number of rules obtained and the number of them that are used in the translation of the corresponding evaluation corpus; remember that the frequency count threshold used in each translation task is the one minimizing the TER when translating the corpora described in Table 1. The data reported in Table 5 correspond to the rules inferred with the largest training corpora (2.0 million word in each language). Note that the number of inferred rules varies depending on the translation task; for instance, the number of rules for es-ca is around twice the number of rules for ca-es, this is because to produce the minimum TER less rules happen to be needed in the case of ca-es.\nThe data in Table 5 reveal, on the one hand, that the percentage of rules finally used to translate the corresponding evaluation corpus varies depending on the translation task, and, on the other hand, that the percentage of rules that end up applying the \u201cdefault\u201d AT (which performs a word-for-word translation, see section 4) depends on the translation task, although it is always below 3%.\nFigure 9 shows for the Spanish-to-Catalan translation, on top, the number of rules obtained and the number of rules used in the translation of the evaluation corpus, all\ngrouped by rule length (number of SL word classes); and, at the bottom, the number of rule applications and the number of rule applications that end up performing a word-forword translation (apply the \u201cdefault\u201d AT); in the generation of the rules a frequency count threshold of 8 was used. Notice that there are rules of unit length, i.e. rules that process only a single SL word class: they are needed because the bilingual dictionary leaves some translation decisions open, such as the gender and number of some words that can be both masculine and feminine, or singular and plural, in the TL. The data in that figure correspond to the rules inferred form the largest training corpora used; in any case, with the rest of training corpora, a similar behaviour is obtained; the same happens with the remaining translation tasks.\nFigure 9 shows that most of the rules generated process SL patterns of only 3 or 4 word classes; the number of rules processing 7 SL word classes being very low. Remember that for the extraction of bilingual phrase pairs their length was restricted to 7 SL words.\nFinally, it is worth mentioning that the number of inferred rules is very high compared to the number of hand-coded rules. Note, however, that automatically inferred rules are more specific and lexicalized than hand-coded ones. Hand-coded rules use macros and complex control flow statements which allow them to treat more phenomena in the same rule."}, {"heading": "6. Discussion", "text": "This paper has focused on the inference of structural transfer rules to be used in MT, and more precisely on the inference of shallow-transfer rules. It describes how to extend the AT approach introduced in the SMT framework in order to use it to generate shallow-transfer rules to be used in RBMT. To this end, a very small amount of linguistic information, in addition to the linguistic data used by the MT engine, has been used in order to learn not only syntactic changes, but also lexical changes to apply when translating SL texts into TL. This linguistic information consists of a small set of lexical categories involved in lexical changes (prepositions, pronouns, etc.) and can easily be provided by an expert.\nThe approach has been tested using data from three existing language pairs of the free/open-source shallow-transfer MT engine Apertium; more precisely, the presented approach has been tested on both translation directions of the Spanish\u2013Catalan and Spanish\u2013 Galician languages pairs, and on the Spanish-to-Portuguese translation. For each language pair, training corpora of different sizes have been used so as to test the importance of the size of training corpora available.\nThe evaluation has been done, in all cases, by using independent parallel corpora, coming from an independent source, with no relation with the parallel corpora used for training. In the evaluation the translation quality achieved by the automatically inferred rules has been compared to that of using hand-coded shallow-transfer rules, to that of a word-for-word translation, and to that of using a state-of-the-art SMT system trained on the same parallel corpora. In all cases the automatically inferred rules perform better than the SMT system; moreover, when the Apertium bilingual dictionary is added to the SMT training data only one translation task performed slightly better than the automatically inferred rules. Notice that our approach, unlike that by Caseli et al. (2006), is aimed at learning shallow-transfer rules, not bilingual entries, and that we have used the bilingual dictionary provided by the corresponding Apertium language-pair package.\nThe evaluation of the inferred rules for both translation directions of the Spanish\u2013 Catalan and the Spanish\u2013Galician language pairs show an improvement in the translation quality as compared to word-for-word translation, even when a very small parallel corpus is used. In the case of the Spanish-to-Portuguese translation, there is a very small improvement: confidence intervals show a large overlap.\nTo our knowledge, this is the first time that the AT approach is extended for its use in RBMT; an important property of the inferred rules is that they can be edited by human experts so as to improve them. This means that developers of RBMT systems can use this method to obtain a set of initial transfer rules that can be then refined by linguists; proceeding in this way, human experts can focus on the more difficult issues of writing accurate transfer rules for MT, as most of the required rules are automatically obtained from parallel corpora. From our point of view, this is a great advantage over other corpusbased approaches to MT, such as SMT, because, in our approach, automatically generated rules can coexist with hand-coded ones.\nWith respect to the parallel corpus used for training, the results achieved by the inferred rules for the Spanish-to-Portuguese translation show that the procedure followed to build the parallel corpus, that is, the way in which the translation from one language into the other one is performed, deserves special attention. In our opinion, it may be concluded that parallel corpora that have been built by translating from a third language may not be appropriate for the task of inferring rules to be used in RBMT, especially if the languages involved are closely related and the third language is not.\nIt must be mentioned that software implementing the method described in this paper has been released as free/open-source software under the GNU GPL license22 and can be freely downloaded from http://apertium.sf.net, package name apertium-transfer-tools. The public availability of the source code ensures the reproducibility of all the experiments conducted and allows other researchers to improve the approach discussed here, saving them from having to implement the algorithms all over again. In addition, the method has been implemented in such a way that it integrates with the Apertium free/open-source MT platform (see appendix A); this benefits, on the one hand, other research that uses Apertium as a research platform, and on the other hand, people developing new language pairs for Apertium.\nWe plan to improve the generated rules by using linguistic criteria for the extraction of the bilingual phrase pairs that are generalized to ATs. Note that in the experiments reported in this paper bilingual phrase pairs are extracted from the training corpus without worrying whether they are well-formed syntactic constituents or not. We also plan to study how to use lexicalized categories in a more flexible way. It would be of interest to have context-dependent lexicalized categories, that is, categories which are lexicalized only in some contexts, while not in others; this would improve the generalization performed by the extended ATs and reduce the number of inferred rules.\nAnother improvement we plan to achieve is the extension of the present approach so that rules for translation between less-related language pairs can be inferred. Recently, the transfer in Apertium has been extended to translate between more divergent languages by splitting the structural transference phase into 3 stages: the first one detects word patterns\n22. http://www.gnu.org/licenses/gpl-2.0.html\ncalled chunks; the second one operates with sequences of chunks; finally, the third one makes some \u201cfinishing\u201d operations within the chunks detected in the first stage. Our approach could be extended by detecting chunks in the training parallel corpus using linguistic criteria as mentioned in the previous paragraph, or using the \u201cMarker Hypothesis\u201d (Green, 1979), as done by Gough and Way (2004), and then extracting ATs based on chunk classes instead of word classes, as it is done now. In any case, it would be worth testing the method in its present for the translation between less-related languages by using longer ATs and larger training corpora."}, {"heading": "Acknowledgments", "text": "Work funded by the Spanish Ministry of Education and Science and the European Social Fund through research grant BES-2004-4711, by the Spanish Ministry of Industry, Tourism and Commerce through projects TIC2003-08681-C02-01, FIT340101-2004-3 and FIT-350401-2006-5, and by the Spanish Ministry of Education and Science through project TIN2006-15071-C03-01. The authors thank the anonymous referees for suggesting significant improvements to this paper and Francis Tyers for proof-reading it."}, {"heading": "Appendix A. The Apertium Machine Translation Platform", "text": "This appendix briefly describes the free/open-source shallow-transfer MT engine Apertium23 (Armentano-Oller et al., 2006) used for the experiments. Apertium follows the shallow-transfer approach shown in Figure 10:\n\u2022 A morphological analyzer which tokenizes the text in surface forms and delivers, for each surface form, one or more lexical forms consisting of lemma, lexical category and morphological inflection information.\n\u2022 A part-of-speech tagger (categorial disambiguator) which chooses, using a first-order hidden Markov model (Cutting et al., 1992; Baum & Petrie, 1966), one of the lexical forms corresponding to an ambiguous surface form.\n\u2022 A lexical transfer module which reads each SL lexical form and delivers the corresponding TL lexical form by looking it up in a bilingual dictionary.\n23. The MT engine, documentation, and linguistic data for different language pairs can be downloaded from http://apertium.sf.net.\n\u2022 A structural transfer module (parallel to the lexical transfer) which uses a finite-state chunker to detect patterns, such as \u201carticle\u2013noun\u2013adjective\u201d, of lexical forms which need to be processed for word reorderings, agreement, etc., and then performs these operations. This is the module that applies the structural transfer rules automatically inferred from parallel corpora using the method in this paper.\n\u2022 A morphological generator which delivers a TL surface form for each TL lexical form, by suitably inflecting it.\n\u2022 A post-generator which performs orthographic operations such as contractions (e.g. Spanish de+el \u2192 del) and apostrophations (e.g. Catalan el+institut \u2192 l\u2019institut).\nThe Apertium MT engine is completely independent from the linguistic data used to translate for a given language pair. Linguistic data is coded using XML-based formats,24 which allows for easy data transformation and maintenance."}], "references": [{"title": "Towards linguistically searchable text", "author": ["A. Alc\u00e1zar"], "venue": "In Proceedings of BIDE (BilbaoDeusto) Summer School of Linguistics", "citeRegEx": "Alc\u00e1zar,? \\Q2005\\E", "shortCiteRegEx": "Alc\u00e1zar", "year": 2005}, {"title": "Open-source Portuguese-Spanish machine translation", "author": ["C. Armentano-Oller", "R.C. Carrasco", "A.M. Corb\u00c3-Bellot", "M.L. Forcada", "M. Ginest\u0301\u0131-Rosell", "S. Ortiz-Rojas", "J.A. P\u00e9rez-Ortiz", "G. Ram\u0131\u0301rez-S\u00e1nchez", "F. S\u00e1nchez-Mart\u0301\u0131nez", "M.A. Scalco"], "venue": "In Computational Processing of the Portuguese Language, Proceedings of the 7th International Workshop on Computational Processing of Written and Spoken Portuguese,", "citeRegEx": "Armentano.Oller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Armentano.Oller et al\\.", "year": 2006}, {"title": "Why translation is difficult for computers. In Computers and Translation: A translator\u2019s guide", "author": ["D. Arnold"], "venue": "Benjamins Translation Library", "citeRegEx": "Arnold,? \\Q2003\\E", "shortCiteRegEx": "Arnold", "year": 2003}, {"title": "Statistical inference for probabilistic functions of finite state Markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Baum and Petrie,? \\Q1966\\E", "shortCiteRegEx": "Baum and Petrie", "year": 1966}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "S.A.D. Pietra", "V.J.D. Pietra", "R.L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Adding linguistic knowledge to a lexical example-based translation system", "author": ["R.D. Brown"], "venue": "In Proceedings of the Eighth International Conference on Theoretical and Methodological Issues in Machine Translation", "citeRegEx": "Brown,? \\Q1999\\E", "shortCiteRegEx": "Brown", "year": 1999}, {"title": "LIHLA: A lexical aligner based on language-independent heuristics", "author": ["H.M. Caseli", "M.G.V. Nunes", "M.L. Forcada"], "venue": "In Anais do V Encontro Nacional de InteligA\u0303ancia Artificial (ENIA", "citeRegEx": "Caseli et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Caseli et al\\.", "year": 2005}, {"title": "Automatic induction of bilingual resources from aligned parallel corpora: application to shallow-transfer machine translation", "author": ["H.M. Caseli", "M.G.V. Nunes", "M.L. Forcada"], "venue": "Machine Translation,", "citeRegEx": "Caseli et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Caseli et al\\.", "year": 2006}, {"title": "Learning translation templates from bilingual translation examples", "author": ["I. Cicekli", "H.A. G\u00fcvenir"], "venue": "Applied Intelligence,", "citeRegEx": "Cicekli and G\u00fcvenir,? \\Q2001\\E", "shortCiteRegEx": "Cicekli and G\u00fcvenir", "year": 2001}, {"title": "A practical part-of-speech tagger", "author": ["D. Cutting", "J. Kupiec", "J. Pedersen", "P. Sibun"], "venue": "In Proceedings of the Third Conference on Applied Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Cutting et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Cutting et al\\.", "year": 1992}, {"title": "An introduction to the Bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1994\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1994}, {"title": "Robust large-scale EBMT with marker-based segmentation", "author": ["N. Gough", "A. Way"], "venue": "In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation,", "citeRegEx": "Gough and Way,? \\Q2004\\E", "shortCiteRegEx": "Gough and Way", "year": 2004}, {"title": "The necessity of syntax markers. Two experiments with artificial languages", "author": ["T. Green"], "venue": "Journal of Verbal Learning and Behavior,", "citeRegEx": "Green,? \\Q1979\\E", "shortCiteRegEx": "Green", "year": 1979}, {"title": "An Introduction to Machine Translation", "author": ["W.J. Hutchins", "H.L. Somers"], "venue": null, "citeRegEx": "Hutchins and Somers,? \\Q1992\\E", "shortCiteRegEx": "Hutchins and Somers", "year": 1992}, {"title": "Learning translation templates from bilingual text", "author": ["H. Kaji", "Y. Kida", "Y. Morimoto"], "venue": "In Proceedings of the 14th Conference on Computational Linguistics,", "citeRegEx": "Kaji et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kaji et al\\.", "year": 1992}, {"title": "A statistical machine translation tutorial workbook", "author": ["K. Knight"], "venue": null, "citeRegEx": "Knight,? \\Q1999\\E", "shortCiteRegEx": "Knight", "year": 1999}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Koehn,? \\Q2004\\E", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "A trainable transfer-based machine translation approach for languages with limited resources", "author": ["A. Lavie", "K. Probst", "E. Peterson", "S. Vogel", "L. Levin", "A. Font-Llitj\u00f3s", "J. Carbonell"], "venue": "In Proceedings of Workshop of the European Association for Machine Translation", "citeRegEx": "Lavie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lavie et al\\.", "year": 2004}, {"title": "The technical analysis on translation templates", "author": ["Y. Liu", "C. Zong"], "venue": "In Proceedings of the IEEE International Conference on Systems, Man & Cybernetics (SMC),", "citeRegEx": "Liu and Zong,? \\Q2004\\E", "shortCiteRegEx": "Liu and Zong", "year": 2004}, {"title": "A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora", "author": ["A. Menezes", "S.D. Richardson"], "venue": "In Proceedings of the ACL Workshop on data-driven machine translation,", "citeRegEx": "Menezes and Richardson,? \\Q2001\\E", "shortCiteRegEx": "Menezes and Richardson", "year": 2001}, {"title": "Unsupervised learning of word-category guessing rules", "author": ["A. Mikheev"], "venue": "In Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Mikheev,? \\Q1996\\E", "shortCiteRegEx": "Mikheev", "year": 1996}, {"title": "A framework of a mechanical translation between English and Japanese by analogy principle", "author": ["M. Nagao"], "venue": "Artifical and Human Intelligence,", "citeRegEx": "Nagao,? \\Q1984\\E", "shortCiteRegEx": "Nagao", "year": 1984}, {"title": "An efficient method for determining bilingual word classes", "author": ["F.J. Och"], "venue": "Ninth Conference of the European Chapter of the Association for Computational Lingustics,", "citeRegEx": "Och,? \\Q1999\\E", "shortCiteRegEx": "Och", "year": 1999}, {"title": "Statistical machine translation: From single-word models to alignment templates", "author": ["F.J. Och"], "venue": "Ph.D. thesis, RWTH Aachen University", "citeRegEx": "Och,? \\Q2002\\E", "shortCiteRegEx": "Och", "year": 2002}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Och,? \\Q2003\\E", "shortCiteRegEx": "Och", "year": 2003}, {"title": "Statistical machine translation: Foundations and recent advances", "author": ["F.J. Och"], "venue": "Tutorial at MT Summit X", "citeRegEx": "Och,? \\Q2005\\E", "shortCiteRegEx": "Och", "year": 2005}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Lingustics (ACL),", "citeRegEx": "Och and Ney,? \\Q2002\\E", "shortCiteRegEx": "Och and Ney", "year": 2002}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2003\\E", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "The alignment template approach to statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2004\\E", "shortCiteRegEx": "Och and Ney", "year": 2004}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "In Proceeding of 40th Annual meeting of the Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "MT for minority languages using elicitation-based learning of syntactic transfer rules", "author": ["K. Probst", "L. Levin", "E. Peterson", "A. Lavie", "J. Carbonell"], "venue": "Machine Translation,", "citeRegEx": "Probst et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Probst et al\\.", "year": 2002}, {"title": "Automatic induction of shallow-transfer rules for open-source machine translation", "author": ["F. S\u00e1nchez-Mart\u0301\u0131nez", "M.L. Forcada"], "venue": "In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI", "citeRegEx": "S\u00e1nchez.Mart\u0301\u0131nez and Forcada,? \\Q2007\\E", "shortCiteRegEx": "S\u00e1nchez.Mart\u0301\u0131nez and Forcada", "year": 2007}, {"title": "Using alignment templates to infer shallow-transfer machine translation rules", "author": ["F. S\u00e1nchez-Mart\u0301\u0131nez", "H. Ney"], "venue": "Science", "citeRegEx": "S\u00e1nchez.Mart\u0301\u0131nez and Ney,? \\Q2006\\E", "shortCiteRegEx": "S\u00e1nchez.Mart\u0301\u0131nez and Ney", "year": 2006}, {"title": "Using target-language information to train part-of-speech taggers for machine translation", "author": ["F. S\u00e1nchez-Mart\u0301\u0131nez", "J.A. P\u00e9rez-Ortiz", "M.L. Forcada"], "venue": "Machine Translation,", "citeRegEx": "S\u00e1nchez.Mart\u0301\u0131nez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "S\u00e1nchez.Mart\u0301\u0131nez et al\\.", "year": 2008}, {"title": "NATools - a statistical word aligner workbench", "author": ["A. Sim\u00f5es", "J. Almeida"], "venue": "Procesamiento del Lenguaje Natural,", "citeRegEx": "Sim\u00f5es and Almeida,? \\Q2003\\E", "shortCiteRegEx": "Sim\u00f5es and Almeida", "year": 2003}, {"title": "A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, \u201cVisions for the Future of Machine Translation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": null, "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages", "author": ["R. Steinberger", "B. Pouliquen", "A. Widiger", "C. Ignat", "T. Erjavec", "D. Tufis", "D. Varga"], "venue": "In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "Steinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinberger et al\\.", "year": 2006}, {"title": "SRILM \u2013 an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Rule-based augmentation of training data in Breton\u2013French statistical machine translation", "author": ["F.M. Tyers", "L. Dugast", "J. Park"], "venue": "In Proceedings of the 13th Annual Conference of the European Associtation for Machine Translation,", "citeRegEx": "Tyers et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tyers et al\\.", "year": 2009}, {"title": "HMM-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "In COLING \u201996: The 16th International Conference on Computational Linguistics,", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}, {"title": "Phrase-based statistical machine translation", "author": ["R. Zens", "F.J. Och", "H. Ney"], "venue": "In KI 2002: Advances in Artificial Intelligence: Proceedings 25th Annual German Conference on AI,", "citeRegEx": "Zens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zens et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "MT is difficult mainly because natural languages are highly ambiguous and also because two languages do not always express the same content in the same way (Arnold, 2003).", "startOffset": 156, "endOffset": 170}, {"referenceID": 21, "context": "Corpus-based approaches to MT, such as example-based MT (EBMT; Nagao, 1984; Carl & Way, 2003) and statistical MT (SMT; Brown et al.", "startOffset": 56, "endOffset": 93}, {"referenceID": 4, "context": "Corpus-based approaches to MT, such as example-based MT (EBMT; Nagao, 1984; Carl & Way, 2003) and statistical MT (SMT; Brown et al., 1993; Knight, 1999), use large collections of parallel texts as the source of knowledge from which the engine learns how to perform translations.", "startOffset": 113, "endOffset": 152}, {"referenceID": 15, "context": "Corpus-based approaches to MT, such as example-based MT (EBMT; Nagao, 1984; Carl & Way, 2003) and statistical MT (SMT; Brown et al., 1993; Knight, 1999), use large collections of parallel texts as the source of knowledge from which the engine learns how to perform translations.", "startOffset": 113, "endOffset": 152}, {"referenceID": 25, "context": "Although corpus-based approaches to MT have grown in interest over the last years, they require large amounts, in the order of tens of millions of words, of parallel text to achieve reasonable translation quality (Och, 2005).", "startOffset": 213, "endOffset": 224}, {"referenceID": 2, "context": "The process of building a RBMT system involves considerable human effort in order to develop the necessary linguistic resources (Arnold, 2003).", "startOffset": 128, "endOffset": 142}, {"referenceID": 33, "context": "The method we present is entirely unsupervised and benefits from information in the rest of modules of the MT system in which the inferred rules are applied, in line with the method proposed by S\u00e1nchez-Mart\u0301\u0131nez et al. (2008) to train part-of-speech taggers in an unsupervised way for their use in MT.", "startOffset": 194, "endOffset": 226}, {"referenceID": 25, "context": "Small compared to the size of corpora commonly used to build corpus-based MT systems (Och, 2005).", "startOffset": 85, "endOffset": 96}, {"referenceID": 23, "context": "The method we propose for the automatic inference of shallow-transfer rules from parallel corpora is based on the alignment template (AT) approach initially proposed for its use in the SMT framework (Och, 2002; Och & Ney, 2004).", "startOffset": 199, "endOffset": 227}, {"referenceID": 22, "context": "For the purpose of this paper, and to stick to the terminology used by Och and Ney (2004) in the definition of AT and by most SMT practitioners, by phrase we refer to any text segment, not necessarily a well-formed syntactic constituent.", "startOffset": 71, "endOffset": 90}, {"referenceID": 26, "context": "Probst et al. (2002) and Lavie et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 15, "context": "(2002) and Lavie et al. (2004) developed a method to learn transfer rules for MT involving underresourced languages (such as Quechua) with very limited resources.", "startOffset": 11, "endOffset": 31}, {"referenceID": 15, "context": "(2002) and Lavie et al. (2004) developed a method to learn transfer rules for MT involving underresourced languages (such as Quechua) with very limited resources. To this end, a small parallel corpus (of a few thousand sentences) is built with the help of a small set of bilingual speakers of the two languages. The parallel corpus is obtained by translating a controlled corpus from the language with more resources (English or Spanish) into the under-resourced language by means of an elicitation tool. This tool is also used to graphically annotate the word alignments between the two sentences. Finally, hierarchical syntactic rules, which can be seen as constituting a context-free transfer grammar, are inferred from the aligned parallel corpus. Menezes and Richardson (2001) propose a method to infer transfer mappings (rules) between source and target languages.", "startOffset": 11, "endOffset": 782}, {"referenceID": 6, "context": "Caseli et al. (2006) propose a method to infer bilingual resources (structural transfer rules and bilingual dictionaries) to be used in shallow-transfer MT from aligned parallel corpora.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "In the EBMT framework, some researchers have dealt with the problem of inferring a kind of translation rules called translation templates (Kaji et al., 1992; Brown, 1999; Cicekli & G\u00fcvenir, 2001).", "startOffset": 138, "endOffset": 195}, {"referenceID": 5, "context": "In the EBMT framework, some researchers have dealt with the problem of inferring a kind of translation rules called translation templates (Kaji et al., 1992; Brown, 1999; Cicekli & G\u00fcvenir, 2001).", "startOffset": 138, "endOffset": 195}, {"referenceID": 5, "context": ", 1992; Brown, 1999; Cicekli & G\u00fcvenir, 2001). A translation template can be defined as a bilingual pair of sentences in which corresponding units (words or phrases) are coupled and replaced by variables. Liu and Zong (2004) provide an interesting review of the different research works dealing with translation templates.", "startOffset": 8, "endOffset": 225}, {"referenceID": 5, "context": ", 1992; Brown, 1999; Cicekli & G\u00fcvenir, 2001). A translation template can be defined as a bilingual pair of sentences in which corresponding units (words or phrases) are coupled and replaced by variables. Liu and Zong (2004) provide an interesting review of the different research works dealing with translation templates. Brown (1999) uses a parallel corpus and some linguistic knowledge in the form of equivalence classes (both syntactic and semantic) to perform a generalization over the bilingual examples collected.", "startOffset": 8, "endOffset": 336}, {"referenceID": 23, "context": "The alignment template (AT) approach (Och, 2002; Och & Ney, 2004) was introduced in the SMT framework as one of the feature functions in the maximum entropy model (Och & Ney, 2002) to try to generalize the knowledge learned for a specific phrase to similar phrases.", "startOffset": 37, "endOffset": 65}, {"referenceID": 6, "context": "A variety of methods, statistical (Och & Ney, 2003) or hybrid (Caseli et al., 2005),3 may be used to compute word alignments from a (sentence-aligned) parallel corpus.", "startOffset": 62, "endOffset": 83}, {"referenceID": 6, "context": "Caseli et al.\u2019s (2005) method is hybrid because prior to the application of heuristics, it uses a statistical tool (NATools) to obtain a probabilistic bilingual dictionary (Sim\u00f5es & Almeida, 2003).", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "training the IBM model 1 (Brown et al., 1993) for 5 iterations; in this model, word order does not affect the alignment probabilities;", "startOffset": 25, "endOffset": 45}, {"referenceID": 39, "context": "training the HMM alignment model (Vogel et al., 1996) for 5 iterations; this alignment model has the property of making alignment probabilities explicitly dependent on the alignment position of the previous word;", "startOffset": 33, "endOffset": 53}, {"referenceID": 4, "context": "training the IBM model 3 (Brown et al., 1993) for 5 iterations; in this model, the probability of an alignment depends on the positions of the aligned words and on the length of SL and TL sentences.", "startOffset": 25, "endOffset": 45}, {"referenceID": 4, "context": "training the IBM model 4 (Brown et al., 1993) for 5 iterations; this model is identical to IBM model 3 except for the fact that it models the reordering of phrases that may be moved around as units.", "startOffset": 25, "endOffset": 45}, {"referenceID": 40, "context": "Usually, the extraction of bilingual phrase pairs (Zens et al., 2002) is performed by considering all possible pairs below a certain length and ensuring that: (i) all words are consecutive, and (ii) words within the bilingual phrase pair are not aligned with words from outside.", "startOffset": 50, "endOffset": 69}, {"referenceID": 22, "context": "Och and Ney (2004) use automatically obtained (Och, 1999) word classes to extract ATs for SMT.", "startOffset": 46, "endOffset": 57}, {"referenceID": 22, "context": "Och and Ney (2004) use automatically obtained (Och, 1999) word classes to extract ATs for SMT.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "A similar approach was used by (Mikheev, 1996) in his work on learning part-of-speech guessing rules to favor longer suffixes over shorter ones.", "startOffset": 31, "endOffset": 46}, {"referenceID": 36, "context": "The Spanish\u2013Catalan parallel corpora come from El Peri\u00f3dico de Catalunya,12 a daily newspaper published both in Catalan and Spanish; the Spanish\u2013Galician parallel corpora come from Diario Oficial de Galicia,13 the official publication of the autonomous government of Galicia published both in Galician and Spanish; the Spanish\u2013Portuguese parallel corpora come from The JRCAcquis Multilingual Parallel Corpus (Steinberger et al., 2006)14 which contains European Union (EU) law applicable in the EU member states.", "startOffset": 408, "endOffset": 434}, {"referenceID": 35, "context": "The frequency count used in the evaluation is the one giving the best translation edit rate (TER; Snover et al., 2006) when translating a corpus, similar to the one used for testing, with 1 000 sentences (see Table 1); in Table 5 (page 627) we provide the thresholds used when the rules are inferred from the corpus with 2.", "startOffset": 92, "endOffset": 118}, {"referenceID": 37, "context": ", 2007) and the SRILM language modelling toolkit (Stolcke, 2002).", "startOffset": 49, "endOffset": 64}, {"referenceID": 24, "context": "Finally, the minimum error \u201crate\u201d training algorithm (Och, 2003) used the remaining 10% of the training corpus to adjust the weight of each feature.", "startOffset": 53, "endOffset": 64}, {"referenceID": 35, "context": "Translation performance is evaluated using two different measures; on the one hand, the translation edit rate (TER; Snover et al., 2006), and on the other hand, the bilingual evaluation understudy (BLEU; Papineni et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 29, "context": ", 2006), and on the other hand, the bilingual evaluation understudy (BLEU; Papineni et al., 2002); in both cases the same evaluation corpora have been used and the confidence intervals of the measures being reported are given (see below).", "startOffset": 68, "endOffset": 97}, {"referenceID": 16, "context": "Confidence intervals of MT quality measures are calculated through the bootstrap resampling method as described by Koehn (2004). In general, the bootstrap resampling method consists of estimating the precision of sample statistics (in our case, translation quality measures) by randomly resampling with replacement (that is, allowing repetitions) from the full set of samples (Efron & Tibshirani, 1994); in MT, sentences and their respective", "startOffset": 115, "endOffset": 128}, {"referenceID": 0, "context": "More precisely, the test corpora for Spanish\u2013 Catalan and Spanish\u2013Galician comes from Revista Consumer Eroski (Alc\u00e1zar, 2005),18 a magazine addressed to consumers published in Spanish, Catalan, Galician and Basque; the test corpora for Spanish\u2013Portuguese comes from the shared evaluation task of the 2008 workshop on SMT.", "startOffset": 110, "endOffset": 125}, {"referenceID": 38, "context": "With the aim of testing whether the difference in the translation performance between the shallow-transfer rules and the SMT system is due to the fact that Apertium uses a wide-coverage, manually-built bilingual dictionary, we have added the bilingual dictionary in the corresponding Apertium package to the SMT training data (Tyers et al., 2009).", "startOffset": 326, "endOffset": 346}, {"referenceID": 6, "context": "Notice that our approach, unlike that by Caseli et al. (2006), is aimed at learning shallow-transfer rules, not bilingual entries, and that we have used the bilingual dictionary provided by the corresponding Apertium language-pair package.", "startOffset": 41, "endOffset": 62}, {"referenceID": 12, "context": "Our approach could be extended by detecting chunks in the training parallel corpus using linguistic criteria as mentioned in the previous paragraph, or using the \u201cMarker Hypothesis\u201d (Green, 1979), as done by Gough and Way (2004), and then extracting ATs based on chunk classes instead of word classes, as it is done now.", "startOffset": 182, "endOffset": 195}, {"referenceID": 11, "context": "Our approach could be extended by detecting chunks in the training parallel corpus using linguistic criteria as mentioned in the previous paragraph, or using the \u201cMarker Hypothesis\u201d (Green, 1979), as done by Gough and Way (2004), and then extracting ATs based on chunk classes instead of word classes, as it is done now.", "startOffset": 208, "endOffset": 229}, {"referenceID": 1, "context": "This appendix briefly describes the free/open-source shallow-transfer MT engine Apertium23 (Armentano-Oller et al., 2006) used for the experiments.", "startOffset": 91, "endOffset": 121}, {"referenceID": 9, "context": "\u2022 A part-of-speech tagger (categorial disambiguator) which chooses, using a first-order hidden Markov model (Cutting et al., 1992; Baum & Petrie, 1966), one of the lexical forms corresponding to an ambiguous surface form.", "startOffset": 108, "endOffset": 151}], "year": 2009, "abstractText": "This paper describes a method for the automatic inference of structural transfer rules to be used in a shallow-transfer machine translation (MT) system from small parallel corpora. The structural transfer rules are based on alignment templates, like those used in statistical MT. Alignment templates are extracted from sentence-aligned parallel corpora and extended with a set of restrictions which are derived from the bilingual dictionary of the MT system and control their application as transfer rules. The experiments conducted using three different language pairs in the free/open-source MT platform Apertium show that translation quality is improved as compared to word-for-word translation (when no transfer rules are used), and that the resulting translation quality is close to that obtained using hand-coded transfer rules. The method we present is entirely unsupervised and benefits from information in the rest of modules of the MT system in which the inferred rules are applied.", "creator": "TeX"}}}