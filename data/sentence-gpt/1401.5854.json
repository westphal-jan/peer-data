{"id": "1401.5854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Avoiding and Escaping Depressions in Real-Time Heuristic Search", "abstract": "Heuristics used for solving hard real-time search problems have regions with depressions. Such regions are bounded areas of the search space in which the heuristic function is inaccurate compared to the actual cost to reach a solution. Early real-time search algorithms, like LRTA*, easily become trapped in those regions since the heuristic values of their states may need to be updated multiple times, which results in costly solutions. State-of-the-art real-time search algorithms, like LSS-LRTA* or LRTA*(k), improve LRTA*s mechanism to update the heuristic, resulting in improved performance. Those algorithms, however, do not guide search towards avoiding depressed regions. This paper presents depression avoidance, a simple real-time search principle to guide search towards avoiding states that have been marked as part of a heuristic depression. We propose two ways in which depression avoidance can be implemented: mark-and-avoid and move-to-border. We implement these strategies on top of LSS-LRTA* and RTAA*, producing 4 new real-time heuristic search algorithms: aLSS-LRTA*, daLSS-LRTA*, aRTAA*, and daRTAA*. When the objective is to find a single solution by running the real-time search algorithm once, we show that daLSS-LRTA* and daRTAA* outperform their predecessors sometimes by one order of magnitude. Of the four new algorithms, daRTAA* produces the best solutions given a fixed deadline on the average time allowed per planning episode. We prove all our algorithms have good theoretical properties: in finite search spaces, they find a solution if one exists, and converge to an optimal after a number of trials. The results of each step are determined by the algorithm\u2019s parameters, which represent the total number of trials that are required to identify one solution. It then provides an opportunity for further computational simulations to consider all the effects of those parameters.\n\n\nAcknowledgments We thank our co-authors, Daniel de Viele, Peter Wiensi, Jens Weishelder, Jo\u00ebl Schulz, David Viele and Mark Piersz, Paul T.\n\n\n\n[1] The following was funded by the Institute of Research on Depression and Social Responsibility.\n[2]", "histories": [["v1", "Thu, 23 Jan 2014 02:45:02 GMT  (2140kb)", "http://arxiv.org/abs/1401.5854v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["carlos hern\\'andez", "jorge a baier"], "accepted": false, "id": "1401.5854"}, "pdf": {"name": "1401.5854.pdf", "metadata": {"source": "META", "title": "Avoiding and Escaping Depressions in Real-Time Heuristic Search", "authors": ["Carlos Hern\u00e1ndez", "Jorge A. Baier"], "emails": ["chernan@ucsc.cl", "jabaier@ing.puc.cl"], "sections": [{"heading": "1. Introduction", "text": "Many real-world applications require agents to act quickly in a possibly unknown environment. Such is the case, for example, of autonomous robots or vehicles moving quickly through initially unknown terrain (Koenig, 2001). It is also the case of virtual agents in games (e.g., Warcraft, Starcraft), in which the time dedicated by the game software to perform tasks such as path-finding for all virtual agents is very limited. Actually, companies impose limits on the order of 1 millisecond to perform these tasks (Bulitko, Bjo\u0308rnsson, Sturtevant, & Lawrence, 2011). Therefore, there is usually no time to plan for full trajectories in advance; rather, path-finding has to be carried out in a real-time fashion.\nReal-time search (e.g., Korf, 1990; Weiss, 1999; Edelkamp & Schro\u0308dl, 2011) is a standard paradigm for solving search problems in which the environment is not fully known in advance\nc\u00a92012 AI Access Foundation. All rights reserved.\nand agents have to act quickly. Instead of running a computationally expensive procedure to generate a conditional plan at the outset, real-time algorithms interleave planning and execution. As such, they usually run a computationally inexpensive lookahead-update-act cycle, in which search is carried out to select the next move (lookahead phase), then learning is carried out (update phase), and finally an action is executed which may involve observing the environment (act phase). Like standard A\u2217 search (Hart, Nilsson, & Raphael, 1968), they use a heuristic function to guide action selection. As the environment is unveiled, the algorithm updates its internal belief about the structure of the search space, updating (i.e. learning) the heuristic value for some states. The lookahead-update-act cycle is executed until a solution is found.\nEarly heuristic real-time algorithms like Learning Real-Time A\u2217 (LRTA\u2217) and RealTime A\u2217 (RTA\u2217) (Korf, 1990) are amenable for settings in which the environment is initially unknown. These algorithms will perform poorly in the presence of heuristic depressions (Ishida, 1992). Intuitively, a heuristic depression is a bounded region of the search space in which the heuristic is inaccurate with respect to the heuristic values of the states in the border of the region. When an agent controlled by LRTA\u2217 or RTA\u2217 enters a region of the search space that conforms a heuristic depression it will usually become \u201ctrapped\u201d. In order to leave the heuristically depressed region, the agent will need to visit and update many states in this region, potentially several times. Furthermore, in many applications, such as games, the behavior of the agent in a depression may look irrational and thus it is undesirable.\nState-of-the-art heuristic real-time search algorithms that are suitable for applications with initially unknown environments are capable of escaping heuristic depressions more quickly than LRTA\u2217 or RTA\u2217. They do so by performing more lookahead search, more learning, or a combination of both. More search involves selecting an action by looking farther away in the search space. More learning usually involves updating the heuristic of several states in a single iteration. There are many algorithms that use one or a combination of these techniques (e.g., Herna\u0301ndez & Meseguer, 2005; Bulitko & Lee, 2006; Koenig & Likhachev, 2006b; Herna\u0301ndez & Meseguer, 2007; Rayner, Davison, Bulitko, Anderson, & Lu, 2007; Bjo\u0308rnsson, Bulitko, & Sturtevant, 2009; Koenig & Sun, 2009). As a result, these algorithms perform better than LRTA\u2217, spending fewer moves trapped in depressions.\nTwo algorithms representative of the state of the art in real-time search for initially unknown environments are LSS-LRTA\u2217 (Koenig & Sun, 2009) and RTAA\u2217 (Koenig & Likhachev, 2006a). These algorithms generalize LRTA\u2217 by performing more search and more learning in each episode. Both algorithms have been shown to perform very well in practice. However, despite the use of more elaborate techniques, they may still perform poorly in the presence of heuristic depressions. This is because they may sometimes rely on increasing the heuristic value of states inside the depressions as a mechanism to exit them.\nIn this paper we study techniques that allow us to improve the performance of real-time search algorithms by making them explicitly aware of heuristic depressions, and then by guiding the search in order to avoid and, therefore, escape depressions. Specifically, the contributions of this paper are as follows.\n\u2022 We provide new empirical evidence that shows that RTAA\u2217 outperforms LSS-LRTA\u2217 in game map benchmarks in the first trial, which means that whenever there is a single chance to run one of those real-time heuristic search algorithms to solve a search\nproblem, RTAA\u2217 finds better solutions than LSS-LRTA\u2217 while making the same search effort. Before, Koenig and Likhachev (2006b) had shown similar performance results but in mazes. This is important since LSS-LRTA\u2217, and not RTAA\u2217, is the algorithm that has received more attention by the real-time heuristic search community. In this paper we consider incorporating our techniques to both LSS-LRTA\u2217 and RTAA\u2217.\n\u2022 We propose a definition for cost-sensitive heuristic depressions, which is a more general notion than Ishida\u2019s (1992) notion of heuristic depression since it incorporates action costs. We illustrate that our depressions better describe the regions of the search space in which real-time search algorithms get trapped.\n\u2022 We propose a simple principle to actively guide search towards avoiding cost-sensitive heuristic depressions that we call depression avoidance, together with two strategies to implement depression avoidance which can be incorporated into state-of-the-art real-time heuristic search algorithms: mark-and-avoid and move-to-border.\n\u2022 We propose four new real-time search algorithms; two based on mark-and-avoid, aLSSLRTA\u2217, aRTAA\u2217, and two based on move-to-border: daLSS-LRTA\u2217, and daRTAA\u2217. The algorithms are the result of implementing depression avoidance on top of RTAA\u2217\nand LSS-LRTA\u2217.\n\u2022 We prove that all our algorithms have desirable properties: heuristic consistency is preserved, they terminate if a solution exists, and they eventually converge to an optimal solution after running a sufficiently large, finite number of trials.\n\u2022 We carry out an extensive empirical evaluation of our algorithms over deployed game benchmarks and mazes. Our evaluation shows that our algorithms outperform existing algorithms in both game maps and mazes. When little time is allowed for the lookahead phase, two of our algorithms, daLSS-LRTA\u2217 and daRTAA\u2217, outperform existing ones by an order of magnitude.\nSome of the contributions of this paper have been published in conference papers (Herna\u0301ndez & Baier, 2011d, 2011c). This article includes new material that has not been presented before. In particular:\n\u2022 We describe and evaluate daLSS-LRTA\u2217, an algorithm that is presented in this article for the first time.\n\u2022 We include full proofs for the termination results (Theorem 6), and a new theoretical result (Theorem 7) on the convergence of all our algorithms.\n\u2022 We extend previously published empirical results by including maze benchmarks, which had not been previously considered, and by including more game domains and problems.\n\u2022 Finally, we discuss in detail some scenarios at which our techniques may not perform particularly good.\nThe rest of the paper is organized as follows. In Section 2 we explain basic concepts of real-time search. We continue presenting LSS-LRTA\u2217 and RTAA\u2217, and we extend the results available in the literature by comparing them over game maps. We continue elaborating on the concept of heuristic depression. We then describe our strategies for implementing depression avoidance and the algorithms that result from applying each of them to LSSLRTA\u2217 and RTAA\u2217. We continue with a detailed theoretical and experimental analysis. Then, we present a discussion of our approach and evaluation. We finish with a summary."}, {"heading": "2. Preliminaries", "text": "A search problem P is a tuple (S,A, c, s0, G), where (S,A) is a digraph that represents the search space. The set S represents the states and the arcs in A represent all available actions. A does not contain elements of form (x, x). In addition, the cost function c : A 7\u2192 R+ associates a cost to each of the available actions. Finally, s0 \u2208 S is the start state, and G \u2286 S is a set of goal states. In this paper we assume search spaces are undirected; i.e., whenever (u, v) is in A, then so is (v, u). Furthermore, c(u, v) = c(v, u), for all (u, v) \u2208 A. The successors of a state u are defined by Succ(u) = {v | (u, v) \u2208 A}. Two states are neighbors if they are successors of each other.\nA heuristic function h : S 7\u2192 [0,\u221e) associates to each state s an approximation h(s) of the cost of a path from s to a goal state. We denote by h\u2217(s) the cost of an optimal path to reach a solution from s.\nA heuristic h is consistent if and only if h(g) = 0 for all g \u2208 G and h(s) \u2264 c(s, s\u2032)+h(s\u2032) for all states s\u2032 \u2208 Succ(s). If h is consistent and C(s, s\u2032) is the cost of any path between two states s and s\u2032, then h(s) \u2264 C(s, s\u2032) +h(s\u2032). Furthermore, if h is consistent it is easy to prove that it is also admissible; i.e., h(s) underestimates h\u2217(s). For more details on these definitions, we refer the reader to the book authored by Pearl (1984).\nWe refer to h(s) as the h-value of s and assume familiarity with the A\u2217 algorithm (Hart et al., 1968): g(s) denotes the cost of the path from the start state to s, and f(s) is defined as g(s) + h(s). The f -value and g-value of s refer to f(s) and g(s) respectively."}, {"heading": "2.1 Real-Time Search", "text": "The objective of a real-time search algorithm is to make an agent travel from an initial state to a goal state performing, between moves, an amount of computation bounded by a constant. An example situation is path-finding in a priori unknown grid-like environments. There the agent has sufficient memory to store its current belief about the structure of the search space. In addition, the free-space assumption (Zelinsky, 1992; Koenig, Tovey, & Smirnov, 2003) is taken: the environment is initially assumed as obstacle-free. The agent is capable of a limited form of sensing: only obstacles in the neighbor states can be detected. When obstacles are detected, the agent updates its map accordingly.\nMany state-of-the-art real-time heuristic search algorithms can be described by the pseudo-code in Algorithm 1. The algorithm iteratively executes a lookahead-update-act cycle until the goal is reached. The lookahead phase (Line 4\u20136) determines the next state to move to, the update phase (Line 7) updates the heuristic, and the act phase (Line 8) moves the agent to its next position. The lookahead-update part of the cycle (Lines 4\u20137) is referred to as the planning episode throughout the paper.\nAlgorithm 1: A generic real-time heuristic search algorithm\nInput: A search problem P , and a heuristic function h. Side Effect: The agent is moved from the initial state to a goal state if a trajectory exists\n1 h0 \u2190 h 2 scurrent \u2190 s0 3 while scurrent 6\u2208 G do 4 LookAhead () 5 if Open = \u2205 then return no-solution 6 snext \u2190 Extract-Best-State() 7 Update () 8 move the agent from scurrent to snext through the path identified by LookAhead. Stop if an action cost along the path is updated. 9 scurrent \u2190 current agent position\n10 update action costs (if they have increased)\nThe generic algorithm has three local variables: scurrent stores the current position of the agent, c(s, s\u2032) contains the cost of moving from state s to a successor s\u2032, and h is such that h(s) contains the heuristic value for s. All three variables may change over time. In path-finding tasks, when the environment is initially unknown, the initial value of c is such that no obstacles are assumed; i.e., c(s, s\u2032) <\u221e for any two neighbor states s, s\u2032. The initial value of h(s), for every s, is given as a parameter.\nThe generic algorithm receives as input a search problem P , and starts off by initializing some useful variables (Lines 1\u20132). In h0 it records the initial value of h, for all states in P , and in scurrent it stores the initial position of the agent, s0. We assume the cost of an arc cannot decrease. In particular, arc costs increase to infinity when an obstacle is discovered.\nIn the lookahead phase (Lines 4\u20136), the algorithm determines where to proceed next. The Lookahead() procedure in Line 4 implements a bounded search procedure that expands states from the current state scurrent. The set of states generated by this call is referred to as local search space. Different choices can be made to implement this procedure. RealTime A\u2217 (RTA\u2217) and Learning Real-Time A\u2217 (LRTA\u2217)\u2014two early algorithms proposed by Korf (1990) \u2014and other modern real-time search algorithms run a search from the current state up to a fixed depth (e.g., Bulitko & Lee, 2006). Another common option is to run a bounded A\u2217 search; such a choice is taken by Local Search Space LRTA\u2217 (LSSLRTA\u2217) (Koenig & Sun, 2009), and Real-Time Adaptive A\u2217 (RTAA\u2217) (Koenig & Likhachev, 2006b). Algorithm 2 shows the pseudo-code for bounded A\u2217. Note that at most k states are expanded, where k is a parameter of the algorithm usually referred to as the lookahead parameter. The pseudo code of the generic real-time search algorithm assumes that the call to Lookahead() stores the frontier of the local search space in Open, and, moreover, that if a goal state is found during search, such a state is not removed from the frontier (in the bounded A\u2217 pseudo-code this is guaranteed by the condition in Line 7).\nIn the last step of the lookahead phase (Line 6, Algorithm 1), the variable containing the next state to move to, snext, is assigned. Here, most algorithms select the state in the search frontier that is estimated to be closest to a goal state. When A\u2217 lookahead is used, such a state usually corresponds to a state with minimum f -value in Open. Thus A\u2217-based lookahead algorithms use Algorithm 3 to implement the Extract-Best-State() function.\nAlgorithm 2: Bounded A\u2217 lookahead 1 procedure A\u2217 () 2 for each s \u2208 S do g(s)\u2190\u221e 3 g(scurrent)\u2190 0 4 Open\u2190 \u2205 5 Insert scurrent into Open 6 expansions\u2190 0 7 while each s\u2032 \u2208 Open with minimum f -value is such that s\u2032 6\u2208 G and expansions < k do 8 Remove state s with smallest f -value from Open 9 Insert s into Closed\n10 for each s\u2032 \u2208 Succ(s) do 11 if g(s\u2032) > g(s) + c(s, s\u2032) then 12 g(s\u2032)\u2190 g(s) + c(s, s\u2032) 13 s\u2032.back = s 14 if s\u2032 \u2208 Open then remove s\u2032 from Open 15 Insert s\u2032 in Open\n16 expansions\u2190 expansions+ 1\nAlgorithm 3: Selection of the Best State used by LSS-LRTA\u2217, RTAA\u2217, and other algorithms.\n1 procedure Extract-Best-State () 2 return argmins\u2032\u2208Openg(s \u2032) + h(s\u2032)\nIn the update phase (Line 7, Algorithm 1), the heuristic of some states in the search space is updated to a value that is a better estimate of the true cost to reach a solution, while staying consistent. After exploring states in the vicinity of scurrent, the algorithm gains information about the heuristic value of a number of states. Using this information, the h-value of scurrent\u2014and potentially that of other states in the search space\u2014can be updated in such a way that they reflect a better estimation of the cost to reach a solution. Since after the update the heuristic of some states are updated to a value closer to the true cost, this phase is also referred to as the learning phase.\nThe literature describes several ways in which one can implement the update of the heuristic, e.g., mini-min (e.g., Korf, 1990), max of mins (Bulitko, 2004), and heuristic bounded propagation (Herna\u0301ndez & Meseguer, 2005). The learning rules that are most relevant to this paper, however, are those implemented by LSS-LRTA\u2217 and RTAA\u2217. They are described in detail in the following subsections.\nFinally, after learning, the agent attempts to move to the state selected by the Extract-Best-State() function, snext. In most implementations, the path to the selected state has been computed already by the Lookahead() procedure (in the case of Algorithm 2, the path is reconstructed using the back pointer that is set in Line 13). When the environment is known in advance, the agent can always move to the destination. However, when the environment is not known in advance, this process can fail (in path-finding, this can occur due to the discovery of an obstacle). If such an obstacle is found, we assume the agent stops moving as soon as it has detected an obstacle. In such cases, the algorithm will\nupdate its memory regarding the environment, which typically involves updating the cost function. In our pseudo-code, this is reflected in Line 10."}, {"heading": "3. LSS-LRTA\u2217 and RTAA\u2217", "text": "Now we describe LSS-LRTA\u2217 and RTAA\u2217, the two state-of-the-art real-time heuristic search algorithms that are most relevant to this paper. We make two small contributions to the understanding of these two algorithms. First, we do an experimental comparison of them over benchmarks that had not been considered before. Second, we prove two theoretical results that aim at understanding the differences between their update mechanisms (Propositions 1 and 2). To our knowledge, none of these results appear in the literature."}, {"heading": "3.1 LSS-LRTA\u2217", "text": "Local search space LRTA\u2217 (LSS-LRTA\u2217) was first introduced by Koenig (2004), and later presented in detail by Koenig and Sun (2009). It is an instance of Algorithm 1. Its lookahead procedure is a bounded A\u2217 search (Algorithm 2). The next state to move to corresponds to a state in Open with the lowest f -value; i.e., it uses Algorithm 3 to implement Extract-Best-State().\nLSS-LRTA\u2217 updates the values of each state s in the local search space in such a way that h(s) is assigned the maximum possible value that guarantees consistency with the states in Open. It does so by implementing the Update() procedure as a modified Dijkstra\u2019s algorithm (Algorithm 4). Since the value of h is raised to the maximum, the update mechanism of LSS-LRTA\u2217 makes h as informed as it can get given the current knowledge about the search space, while maintaining consistency.\nAlgorithm 4: LSS-LRTA\u2217\u2019s Modified Dijkstra\u2019s Procedure. We assume Open list is a queue ordered by h-value.\n1 procedure ModifiedDijkstra () 2 for each state s in Closed do h(s)\u2190\u221e 3 while Closed 6= \u2205 do 4 Extract an s with minimum h-value from Open 5 if s \u2208 Closed then delete s from Closed 6 for each s\u2032 such that s \u2208 Succ(s\u2032) do 7 if s\u2032 \u2208 Closed and h(s\u2032) > c(s\u2032, s) + h(s) then 8 h(s\u2032)\u2190 c(s\u2032, s) + h(s) 9 if s\u2032 6\u2208 Open then Insert s\u2032 in Open\nAlgorithm 5: RTAA\u2217\u2019s Update Procedure\n1 procedure Update () 2 f \u2190 mins\u2208Open g(s) + h(s) 3 for each s \u2208 Closed do 4 h(s)\u2190 f \u2212 g(s)"}, {"heading": "3.2 RTAA\u2217", "text": "Real-Time Adaptive A\u2217 (RTAA\u2217) was proposed by Koenig and Likhachev (2006b). It is an instance of Algorithm 1. Its lookahead phase is identical to that of LSS-LRTA\u2217: a bounded A\u2217 followed by selecting a state with the lowest f -value in Open as the next state to move to. However it uses a simpler learning mechanism based on the update rule of the incremental A\u2217 search algorithm Adaptive A\u2217 (Koenig & Likhachev, 2006a). Thus, it updates the heuristic value of states in the interior of the local search space (i.e., those stored in A\u2217\u2019s variable Closed) using the f -value of the best state in Open. The procedure is shown in Algorithm 5.\nRTAA\u2217\u2019s update procedure is considerably faster in practice than that of LSS-LRTA\u2217. Obtaining the lowest f -value of a state in Open can be done in constant time if A\u2217 is implemented with binary heaps. After that, the algorithm simply iterates through the states in Closed. The worst-case performance is then O(|Closed|). On the other hand, LSS-LRTA\u2217\u2019s update procedure first needs to convert Open into a priority queue ordered by h and then may, in the worst case, need to extract |Open| + |Closed| elements from a binary heap. In addition, it expands each node that is ever extracted from the priority queue. The time to complete these operations, in the worst case is Texp \u00b7N + Tb \u00b7N logN , where N = |Open| + |Closed|, Texp is the time taken per expansion, and Tb is a constant factor associated to extraction from the binary heap. The worst-case asymptotic complexity of extraction is thus O(N logN). However, since we usually deal with a small N it may be the case that the term Texp \u00b7N dominates the expression for time.\nWe will prove that the heuristic values that RTAA\u2217 learns may be less accurate than those of LSS-LRTA\u2217. To state this formally, we introduce some notation. Let hn, for n > 0, denote the value of the h variable at the start of iteration n of the main algorithm, or, equivalently, right after the update phase of iteration n \u2212 1. We will also denote the heuristic function given as input as h0. Let kn(s, s\n\u2032) denote the cost of an optimal path from s to s\u2032 that traverses states only in Closed before ending in s\u2032.\nProposition 1 Let s be a state in Closed right after the call to A\u2217 has returned in the n-th iteration of LSS-LRTA\u2217. Then,\nhn+1(s) = min sb\u2208Open kn(s, sb) + hn(sb). (1)\nProof: We will show that the value h(s) computed by the modified Dijkstra algorithm for each state s corresponds to the minimum cost of reaching node s from a certain state in a particular graph G. The modified Dijkstra procedure can be seen as a run of the standard Dijkstra algorithm (e.g., Cormen, Leiserson, Rivest, & Stein, 2001) on such a graph.\nFirst we observe that our procedure differs from the standard Dijkstra algorithm in that a non-singleton set of states, namely those in Open, are initialized with a finite value for h. In the standard Dijkstra algorithm, on the other hand, only the source node is initialized with a cumulative cost of 0 whereas the remaining nodes are initialized to \u221e. With the facts above in mind, it is straightforward to see that a run of the modified Dijkstra can be interpreted as a run of the standard Dijkstra algorithm from node sstart of a directed graph G that is such that:\n\u2022 Its nodes are exactly those in Open \u222a Closed plus a distinguished node sstart.\n\u2022 It contains an arc (u, v) with cost c if there is an arc (v, u) with cost c in the search graph of P such that one of v or u is not in Open.\n\u2022 It contains an arc of the form (sstart, s) with cost h(s) for each s in Open.\n\u2022 It contains no other arcs.\nAfter running the Dijkstra algorithm from sstart over G, we obtain, for each node s in G the cost of an optimal path from sstart to s. If we interpret such a cost as h(s), for each s, Equation 1 holds, which finishes the proof.\nFor RTAA\u2217 we can prove a sightly different result.\nProposition 2 Right after the call to A\u2217 returns in the n-th iteration of RTAA\u2217, let s\u2217 be the state with lowest f -value in Open, and let s be a state in Closed. Then,\nhn+1(s) \u2264 min sb\u2208Open kn(s, sb) + hn(sb). (2)\nHowever, if hn is consistent and s is in the path found by A \u2217 from scurrent to s \u2217, then\nhn+1(s) = min sb\u2208Open kn(s, sb) + hn(sb). (3)\nProof: For (2), we use the fact that if the heuristic is consistent, it remains consistent after each RTAA\u2217 iteration (a fact proven by Koenig & Likhachev, 2006a), to write the inequality hn+1(s) \u2264 minsb\u2208Open kn(s, sb) + hn+1(sb). Now note that for every state sb in Open it holds that hn(s) = hn+1(s), since the heuristic values of states in Open are not updated. Substituting hn+1(s) in the inequality, we obtain the required result.\nFor (3), we use the a fact proven by Hart et al. (1968) about A\u2217: if consistent heuristics are used, g(s) contains the cost of the cheapest path from the start state to s right after s is extracted from Open (Line 8 in Algorithm 2).\nBecause A\u2217 is run with a consistent heuristic, for any state s\u2032 along the (optimal) path found by A\u2217 from scurrent to s \u2217,\ng(s\u2032) = kn(scurrent, s \u2032), and (4) g(s\u2217) = kn(scurrent, s \u2032) + kn(s \u2032, s\u2217). (5)\nRTAA\u2217\u2019s update rule states that:\nhn+1(s \u2032) = f(s\u2217)\u2212 g(s\u2032) = hn(s\u2217) + g(s\u2217)\u2212 g(s\u2032) (6)\nSubstituting with (4) and (5) in (6), we obtain hn+1(s \u2032) = kn(s \u2032, s\u2217) + hn(s \u2217). Finally, observe that kn(s\n\u2032, s\u2217) + h(s\u2217) = min sb\u2208Open kn(s \u2032, sb) + hn(sb).\nIndeed, if there were an s\u2212 in Open such that kn(s \u2032, s\u2217) + h(s\u2217) > kn(s \u2032, s\u2212) + hn(s \u2212), then by adding g(s\u2032) to both sides of the inequality, we would have that f(s\u2217) > f(s\u2212), which contradicts the fact that s\u2217 is the state with lowest f -value in Open. We conclude henceforth that hn+1(s \u2032) = minsb\u2208Open kn(s \u2032, sb) + hn(sb). This finishes the proof.\nProposition 2 implies that, when using consistent heuristics, RTAA\u2217\u2019s update may yield less informed h-values than those of LSS-LRTA\u2217. However, at least for some of the states in the local search space, the final h-values are equal to those of LSS-LRTA\u2217, and hence they are as informed as they can be given the current knowledge about the search space.\nKoenig and Likhachev (2006a) show that for a fixed value of the lookahead parameter, the quality of the solutions obtained by LSS-LRTA\u2217 are better on average than those obtained by RTAA\u2217 in path-finding tasks over mazes. This is due to the fact that LSSLRTA\u2217\u2019s heuristic is more informed over time than that of RTAA\u2217. However, they also showed that given a fixed time deadline per planning episode, RTAA\u2217 yields better solutions than LSS-LRTA\u2217. This is essentially due to the fact that RTAA\u2217\u2019s update mechanism is faster: for a fixed deadline, a higher lookahead parameter can be used with RTAA\u2217 than with LSS-LRTA\u2217.\nWe extend Koenig and Likhachev\u2019s experimental analysis by running a comparison of the two algorithms on game maps. Table 1 shows average results for LSS-LRTA\u2217 and RTAA\u2217 ran on 12 different game maps. For each map, we generated 500 random test cases. Observe, for example, that if a deadline of 0.0364 milliseconds is imposed per planning episode we can choose to run RTAA\u2217 with a lookahead k = 128, whereas we can choose to run LSS-LRTA\u2217 only with lookahead k = 64. With those parameters, RTAA\u2217 obtains a solution about 36% cheaper than LSS-LRTA\u2217 does. Figure 1 shows average solution cost versus time per episode. The slopes of the curves suggest that the rate at which RTAA\u2217 improves solutions is better than that of LSS-LRTA\u2217, as more time per episode is given. In conclusion RTAA\u2217 seems superior to LSS-LRTA\u2217 when time is actually important. We thus confirm for a wider range of tasks that, when time per episode matters, RTAA\u2217 is better than LSS-LRTA\u2217. These findings are important because mazes (for which previous evaluations existed) are problems with a very particular structure, and results over them do not necessarily generalize to other types of problems.\nAlthough we conclude that RTAA\u2217 is an algorithm superior to LSS-LRTA\u2217 when it comes to finding a good solution quickly, it is interesting to note that recent research on real-time heuristic search is focused mainly on extending or using LSS-LRTA\u2217 (see e.g., Bulitko, Bjo\u0308rnsson, & Lawrence, 2010; Bond, Widger, Ruml, & Sun, 2010; Herna\u0301ndez & Baier, 2011d; Sturtevant & Bulitko, 2011), while RTAA\u2217 is rarely considered. Since LSSLRTA\u2217 seems to be an algorithm under active study by the community, in this paper we apply our techniques to both algorithms."}, {"heading": "4. Heuristic Depressions", "text": "In real-time search problems heuristics usually contain depressions. The identification of depressions is central to our algorithm. Intuitively, a heuristic depression is a bounded region of the search space containing states whose heuristic value is too low with respect to the heuristic values of states in the border of the depression. Depressions exist naturally in heuristics used along with real-time heuristic search algorithms. As we have seen above, real-time heuristic algorithms build solutions incrementally, updating the heuristic values associated to certain states as more information is gathered from the environment.\nIshida (1992) gave a constructive definition for heuristic depressions. The construction starts with a node s such that its heuristic value is equal to or less than those of the\nsurrounding states. The region is then extended by adding a state of its border if all states in the resulting region have a heuristic value lower or equal than those of the states in the border. As a result, the heuristic depression D is a maximal connected component of states such that all states in the boundary of D have a heuristic value that is greater than or equal to the heuristic value of any state in D.\nIt is known that algorithms like LRTA\u2217 behave poorly in the presence of heuristic depressions (Ishida, 1992). To see this, assume that LRTA\u2217 is run with lookahead depth equal to 1, such that it only expands the current state, leaving its immediate successors in the search frontier. Assume further that it visits a state in a depression and that the solution node lies outside the depression. To exit the depressed region the agent must follow a path in the interior of the depressed region, say, s1 . . . sn, finally choosing a state in the border of the region, say se. While visiting sn, the agent chooses se as the next move, which means that se minimizes the estimated cost to reach a solution among all the neighbors of sn. In problems with uniform action costs, this can only happen if h(se) is lower or equal than the heuristic value of all other neighbors of sn. This fact actually means that the depression in that region of the search space no longer exists, which can only happen if the heuristic values of states in the originally depressed region have been updated (increased). For LRTA\u2217, the update process may be quite costly: in the worst case all states in the depression may need to be updated and each state may need to be updated several times.\nIshida\u2019s definition is, nonetheless, restrictive. In fact, it does not take into account the costs of the actions needed to move from the interior of the depression to the exterior. A closed region of states may have unrealistically low heuristic values even though the heuristic values in the interior are greater than the ones in the border. We propose a more intuitive notion of depression when costs are taken into account. The formal definition follows.\nDefinition 1 (Cost-sensitive heuristic depression) A connected component of states D is a cost-sensitive heuristic depression of a heuristic h iff for any state s \u2208 D and every state s\u2032 6\u2208 D that is a neighbor of a state in D, h(s) < k(s, s\u2032) +h(s\u2032), where k(s, s\u2032) denotes the cost of the cheapest path that starts in s, traverses states only in D, and ends in s\u2032.\nCost-sensitive heuristic depressions better reflect the regions in which an agent controlled by algorithms such as LRTA\u2217 get trapped. To illustrate this, consider the two 4-connected grid-world problems of Figure 2. Gray cells conform an Ishida depression. The union of yellow and gray cells conform a cost-sensitive heuristic depression. Suppose the agent\u2019s initial position is the lower-right corner of the Ishida depression (C4 in Figure 2(a), and C7 in Figure 2(b)). Assume further that ties are broken such that the priorities, given from higher to lower, are: down, left, up, and right. For such an initial state, both in situation (a) and situation (b), the agent controlled by LRTA\u2217 will visit every state in the cost-sensitive heuristic depression before reaching the goal. Indeed, cells in the costsensitive depression that are not adjacent to an obstacle are visited exactly 3 times, while cells adjacent to an obstacle are visited 2 times, before the agent escapes the depression, and thus the performance of LRTA\u2217 can be described as a linear function on the size of the cost-sensitive depression.\nIt is interesting to note that for problems like the ones shown in Figure 2, the size of the Ishida depression remains the same while the width of the grid varies. Thus, the size of\nthe Ishida depression is not correlated with the performance of LRTA\u2217. On the other hand, the size of the cost-sensitive heuristic depression is a predictor of the cost of the solution"}, {"heading": "5. Depression Avoidance", "text": "A major issue at solving real-time search problems is the presence of heuristic depressions. State-of-the-art algorithms are able to deal with this problem essentially by doing extensive learning and/or extensive lookahead. By doing more lookahead, chances are that a state outside of a depression is eventually selected to move to. On the other hand, by learning the heuristic values of several states at a time, fewer movements might be needed in order to raise the heuristic values of states in the interior of a depression high enough as to make it disappear. As such, LSS-LRTA\u2217, run with a high value for the lookahead parameter exits the depressions more quickly than LRTA\u2217 run with search depth equal to 1 for two reasons: (1) because the heuristic function increases for states in D more quickly and (2) because with a high value for the lookahead parameter it is sometimes possible to escape the depression in one step.\nBesides the already discussed LSS-LRTA\u2217 and RTAA\u2217, there are many algorithms described in the literature capable of doing extensive lookahead and learning. The lookahead ability of LRTS (Bulitko & Lee, 2006), and TBA\u2217 (Bjo\u0308rnsson et al., 2009) is parametrized. By using algorithms such as LRTA\u2217(k) (Herna\u0301ndez & Meseguer, 2005), PLRTA\u2217 (Rayner et al., 2007) and LRTA\u2217LS(k) (Herna\u0301ndez & Meseguer, 2007) one can increase the number of states updated based on a parameter. None of these algorithms however are aware of depressions; their design simply allows to escape them because of their ability to do lookahead, learning, or a combination of both. Later, in Section 9, we give a more detailed overview of other related work.\nTo improve search performance our algorithms avoid depressions, a principle we call depression avoidance. Depression avoidance is a simple principle that dictates that search\nshould be guided away from states identified as being in a heuristic depression. There are many ways in which one could conceive the implementation of this principle in a real-time heuristic search algorithm. Below we present two alternative realizations of the principle within the state-of-the-art RTAA\u2217 and LSS-LRTA\u2217 algorithms. As a result, we propose four new real-time search algorithms, each of which has good theoretical properties.\n5.1 Depression Avoidance via Mark-and-Avoid\nThis subsection presents a first possible realization of depression avoidance that we call mark-and-avoid. With this strategy, we extend the update phase to mark states that we can prove belong to a heuristic depression. We then modify the selection of the best state (i.e., the Extract-Best-State() function) to select states that are not marked; i.e., states that are not yet proven to be part of a depression.\naLSS-LRTA\u2217 is version of LSS-LRTA\u2217 that avoids depressions via mark-and-avoid. It is obtained by implementing the Update() function using Algorithm 6 and by implementing the Extract-Best() function with Algorithm 7. There are two differences between its update procedure and LSS-LRTA\u2217\u2019s. The first is the initialization of the updated flag in Lines 2\u20133. The second is Line 7, which sets s.updated to true if the heuristic value for h changes as a result of the update process. In the following section, we formally prove that this means that s was inside a cost-sensitive heuristic depression (Theorem 5).\nAlgorithm 6: Modified Dijkstra Procedure used by aLSS-LRTA\u2217.\n1 procedure ModifiedDijkstra () 2 if first run then 3 for each s \u2208 S do s.updated\u2190 false /* initialization of update flag */ 4 for each s \u2208 Closed do h(s)\u2190\u221e 5 while Closed 6= \u2205 do 6 Extract an s with minimum h-value from Open 7 if h(s) > h0(s) then s.updated = true 8 if s \u2208 Closed then delete s from Closed 9 for each s\u2032 such that s \u2208 Succ(s\u2032) do\n10 if s\u2032 \u2208 Closed and h(s\u2032) > c(s\u2032, s) + h(s) then 11 h(s\u2032)\u2190 c(s\u2032, s) + h(s) 12 if s\u2032 6\u2208 Open then Insert s\u2032 in Open\nTo select the next state snext, aLSS-LRTA \u2217 chooses the state with lowest f -value from Open that has not been marked as in a depression. If such a state does not exist, the algorithm selects the state with lowest f -value from Open, just like LSS-LRTA\u2217 would do. Depending on the implementation, the worst-case complexity of this new selection mechanism may be different from that of Algorithm 3. Indeed, if the Open list is implemented with a binary heap (as it is our case), the worst-case complexity of Algorithm 7 is O(N logN) where N is the size of Open. This is because the heap is ordered by f -value. On the other hand the worst-case complexity of Algorithm 3 using binary heaps is O(1). In our experimental results we do not observe, however, a significant degradation in performance due to this factor.\nAlgorithm 7: Selection of the next state used by aLSS-LRTA\u2217 and aRTAA\u2217\n1 function Extract-Best-State () 2 if Open contains an s such that s.updated = false then 3 s\u2190 argmins\u2032\u2208Open\u2227s\u2032.updated=falseg(s\u2032) + h(s\u2032) 4 else 5 s\u2190 argmins\u2032\u2208Openg(s\u2032) + h(s\u2032) 6 return s ;\nExample Figure 3 shows an example that illustrates the difference between LSS-LRTA\u2217 and aLSS-LRTA\u2217 with the lookahead parameter equal to two. After 4 search episodes, we observe that aLSS-LRTA\u2217 avoids the depression, leading the agent to a position that is 2 steps closer to the goal than LSS-LRTA\u2217.\nAlgorithm 8: aRTAA\u2217\u2019s Update Procedure\n1 procedure Update () 2 if first run then 3 for each s \u2208 S do s.updated\u2190 false /* initialization of update flag */ 4 f \u2190 f -value of the best state in Open 5 for each s \u2208 Closed do 6 h(s)\u2190 f \u2212 g(s) 7 if h(s) > h0(s) then s.updated\u2190 true\nWith aLSS-LRTA\u2217 as a reference, it is straightforward to implement the mark-and-avoid strategy into RTAA\u2217. The update phase of the resulting algorithm, aRTAA\u2217, is just like RTAA\u2217\u2019s but is extended to mark states in a depression (Algorithm 8). The selection of the best state to move to is done in the same way as aLSS-LRTA\u2217, i.e., with Algorithm 7. As a result aRTAA\u2217 is a version of RTAA\u2217 that aims at avoiding depressions using mark-and-avoid.\n5.2 Depression Avoidance via Move-to-Border\nMove-to-border is a more finely grained implementation of depression avoidance. To illustrate the differences, consider that, after lookahead, there is no state s in the frontier of the local search space such that s.updated is false. Intuitively, such is a situation in which the agent is \u201ctrapped\u201d in a heuristic depression. In this case, aLSS-LRTA\u2217 behaves exactly as LRTA\u2217 does since all states in the search frontier are marked. Nevertheless, in these cases, we would like the movement of the agent to still be guided away from the depression.\nIn situations in which all states in the frontier of the local search space are already proven as members of a depression, the move-to-border strategy attempts to move to a state that seems closer to the border of a depression. As a next state, this strategy chooses the state with best f -value among the states whose heuristic has changed the least. The intuition behind this behavior is as follows: assume \u2206(s) is the difference between the actual cost to reach a solution from a state s and the initial heuristic value of state s. Then, if s1 is a state close to the border of a depression D and s2 is a state farther away from the border and \u201cdeep\u201d in the interior of D, then \u2206(s2) \u2265 \u2206(s1), because the heuristic of s2 is\nLSS-LRTA\u2217 aLSS-LRTA\u2217\naLSS-LRTA\u2217 daLSS-LRTA\u2217\nmore imprecise than that of s1. At execution time, h is an estimate of the actual cost to reach a solution.\ndaLSS-LRTA\u2217 and daRTAA\u2217 differ, respectively, from LSS-LRTA\u2217 and RTAA\u2217 in that the selection of the next state to move to (i.e., function Extract-Best()) is implemented via Algorithm 9. Note that the worst case complexity of this algorithm is O(N logN), where N is the size of Open if binary heaps are used.\nAlgorithm 9: Selection of the next state by daRTAA\u2217 and daLSS-LRTA\u2217.\n1 function Extract-Best-State () 2 \u2206min \u2190\u221e 3 while Open 6= \u2205 and \u2206min 6= 0 do 4 Remove state sb with smallest f -value from Open 5 if h(sb)\u2212 h0(sb) < \u2206min then 6 s\u2190 sb 7 \u2206min \u2190 h(sb)\u2212 h0(sb)\n8 return s\nFigure 4 illustrates the differences between aLSS-LRTA\u2217 and daLSS-LRTA\u2217. Both algorithms execute in the same way if, after the lookahead phase, there is a state in Open whose heuristic value has not been updated. However, when this is not the case (i.e., when the algorithm is \u201ctrapped\u201d in a depression), daLSS-LRTA\u2217 will move to what seems to be closer to the border of the depression. In the example of Figure 4, at iteration 15, the algorithm chooses B4 instead of C3 since B4 is the state for which the h-value has changed the least. After iteration 18, daLSS-LRTA\u2217 will move to cells in which less learning has been carried out and thus will exit the depression more quickly.\nAll the new algorithms presented in this section are closely related. Table 2 shows a schematic view of the different components of each algorithm, and the complexity of the involved algorithms."}, {"heading": "6. Theoretical Analysis", "text": "In this section we analyze the theoretical properties of the algorithms that we propose. We prove that all of our algorithms also satisfy desirable properties that hold for their ancestors. We start off by presenting theoretical results that can be proven using existing proofs available in the literature; among them, we will show that the consistency of the heuristic is maintained by all our algorithms during run time. We continue with results that need different proofs; in particular, termination and convergence to an optimal solution.\nAs before, we use hn to refer to the value of variable h at the start of iteration n (h0, thus, denotes the heuristic function given as a parameter to the algorithm). Similarly, cn(s, s \u2032) is the cost of the arc between s and s\u2032. Finally, kn(s, s \u2032) denotes the cost of an optimal path between s and s\u2032 that traverses only nodes in Closed before ending in s\u2032 with respect to cost function cn.\nWe first establish that if h is initially consistent, then h is non-decreasing over time. This is an important property since it means that the heuristic becomes more accurate over time.\nTheorem 1 If hn is consistent with respect to cost function cn, then hn+1(s) \u2265 hn(s) for any n along an execution of aLSS-LRTA\u2217 or daLSS-LRTA\u2217.\nProof: Assume the contrary, i.e., that there is a state s such that hn(s) > hn+1(s). State s must be in Closed, since those are the only states whose h-value may be updated. As such, by Proposition 1, we have that hn+1(s) = kn(s, sb) + hn(sb), for some state sb in Open. However, since hn(s) > hn+1(s), we conclude that:\nhn(s) > kn(s, sb) + hn(sb),\nwhich contradicts the fact that hn is consistent. We thus conclude that the h-value of s cannot decrease.\nTheorem 2 If hn is consistent with respect to cost function cn, then hn+1(s) \u2265 hn(s) for any n along an execution of aRTAA\u2217 or daRTAA\u2217.\nProof: Assume the contrary, i.e., that there is a state s such that hn(s) > hn+1(s). State s must be in Closed, since those are the only states whose h-value may be updated. The update rule will set the value of hn+1(s) to f(s \u2032)\u2212 g(s) for some s\u2032 \u2208 Open, i.e.,\nhn+1(s) = f(s \u2032)\u2212 g(s) = g(s\u2032) + hn(s\u2032)\u2212 g(s).\nBut since hn(s) > hn+1(s), we have that:\nhn(s) > g(s \u2032) + hn(s \u2032)\u2212 g(s).\nReordering terms, we obtain that:\nhn(s) + g(s) > g(s \u2032) + hn(s \u2032),\nwhich means that the f -value of s is greater than the f -value of s\u2032. It is known however that A\u2217, run with a consistent heuristic, will expand nodes with non-decreasing f -values. We conclude, thus, that s\u2032 must have been expanded before s. Since s\u2032 is in Open, then s cannot be in Closed, which contradicts our initial assumption. We thus conclude that the h-value of s cannot decrease.\nTheorem 3 If hn is consistent with respect to cost function cn, then hn+1 is consistent with respect to cost function cn+1 along an execution aLSS-LRTA \u2217 or daLSS-LRTA\u2217.\nProof: Since the update procedure used by aLSS-LRTA\u2217, daLSS-LRTA\u2217 and LSS-LRTA\u2217 update variable h in exactly the same way, the proof by Koenig and Sun (2009) can be reused here. However, we provide a rather simpler proof in Section B.1.\nTheorem 4 If hn is consistent with respect to cost function cn, then hn+1 is consistent with respect to cost function cn+1 along an execution aRTAA \u2217 or daRTAA\u2217.\nProof: Since the update procedure used by aRTAA\u2217, daRTAA\u2217 and RTAA\u2217 update variable h in exactly the same way, we can re-use the proof of Theorem 1 by Koenig and Likhachev (2006b) to establish this result. We provide however a complete proof in Section B.2\nThe objective of the mark-and-avoid strategy is to stay away from depressions. The following theorems establish that, indeed, when a state is marked by the aLSS-LRTA\u2217 or aRTAA\u2217 then such a state is in a heuristic depression of the current heuristic.\nTheorem 5 Let s be a state such that s.updated switches from false to true between iterations n and n + 1 in an execution of aLSS-LRTA\u2217 or aRTAA\u2217 for which h was initially consistent. Then s is in a cost-sensitive heuristic depression of hn.\nProof: We first prove the result for the case of aLSS-LRTA\u2217. The proof for aRTAA\u2217 is very similar and can be found in Section B.3.\nLet D be the maximal connected component of states connected to s such that:\n1. All states in D are in Closed after the call to A\u2217 in iteration n, and\n2. Any state sd in D is such that hn+1(sd) > hn(sd).\nLet s\u2032 be a state in the boundary of D. We first show that hn(s \u2032) = hn+1(s \u2032). By definition s\u2032 is either in Closed or Open. If s\u2032 \u2208 Closed then, since s\u2032 6\u2208 D, it must be the case that s\u2032 does not satisfy condition 2 of the definition of D, and hence hn+1(s\n\u2032) \u2264 hn(s\u2032). However, since the heuristic is non-decreasing (Theorems 2 and 1), it must be that hn(s\n\u2032) = hn+1(s\n\u2032). On the other hand, if s\u2032 is in Open, its heuristic value is not changed and thus also hn(s \u2032) = hn+1(s \u2032). We have established, hence, that hn(s \u2032) = hn+1(s \u2032).\nNow we are ready to establish our result: that D is a cost-sensitive heuristic depression of hn.\nLet sd be a state in D. We distinguish two cases.\n\u2022 Case 1: s\u2032 \u2208 Closed. Then, by Proposition 1,\nhn(s \u2032) = kn(s \u2032, sb) + hn(sb), (7)\nfor some sb \u2208 Open. On the other hand, since the heuristic value has increased for sd, hn(sd) < hn+1(sd) = mins\u2032b\u2208Open kn(sd, s \u2032 b) + h(s \u2032 b); in particular, hn(sd) < kn(sd, sb)+hn(sb). Since kn(sd, sb) is the optimal cost to go from sd to sb, kn(sd, sb) \u2264 kn(sd, s \u2032) + kn(s \u2032, sb). Substituting kn(sd, sb) in the previous inequality we have:\nhn(sd) < kn(sd, s \u2032) + kn(s \u2032, sb) + hn(sb). (8)\nWe now substitute the right-hand side of (8) using (7), and we obtain\nhn(sd) < kn(sd, s \u2032) + hn(s \u2032).\n\u2022 Case 2: s\u2032 \u2208 Open. Because of Proposition 1 we have hn+1(sd) \u2264 kn(sd, s\u2032) + hn(s\u2032). Moreover, by definition of D, we have hn+1(sd) > hn(sd). Combining these two inequalities, we obtain:\nhn(sd) < kn(sd, s \u2032) + hn(s \u2032).\nIn both cases, we proved hn(sd) < kn(sd, s \u2032) + hn(s \u2032), for any sd in D and any s \u2032 in the boundary of D. We conclude D is a cost-sensitive heuristic depression of hn, which finishes the proof.\nNow we turn our attention to termination. We will prove that if a solution exists, then it will be found by any of our algorithms. To prove such a result, we need two intermediate lemmas. The first establishes that when the algorithm moves to the best state in Open, then the h-value of such a state has not changed more than the h-value of the current state. Formally,\nLemma 1 Let s\u2032 be the state with smallest f -value in Open after the lookahead phase of any of aLSS-LRTA\u2217, daLSS-LRTA\u2217, aRTAA\u2217, or daRTAA\u2217, when initialized with a consistent heuristic h. Then,\nhn+1(scurrent)\u2212 h0(scurrent) \u2265 hn(s\u2032)\u2212 h0(s\u2032).\nProof: Indeed, by Propositions 1 or 2:\nhn+1(scurrent) = kn(scurrent, s \u2032) + hn(s \u2032) (9)\nLet \u03c0 be an optimal path found by A\u2217 connecting scurrent and s \u2032. Let K\u03c00 denote the cost of this path with respect to cost function c0. Given that the heuristic h0 is consistent with respect to the graph with cost function c0, we have that h0(scurrent) \u2264 K\u03c00 + h0(s\u2032) which can be re-written as:\n\u2212h0(scurrent) \u2265 \u2212K\u03c00 \u2212 h0(s\u2032). (10)\nAdding (10) and (9), we obtain:\nhn+1(scurrent)\u2212 h0(s) \u2265 kn(scurrent, s\u2032)\u2212K\u03c00 + hn(s\u2032)\u2212 h0(s\u2032). (11)\nNow, because cn can only increase, the cost of \u03c0 at iteration n, kn(scurrent, s \u2032), is strictly greater than the cost of \u03c0 at iteration 0, K\u03c00 . In other words, the amount kn(scurrent, s \u2032)\u2212K\u03c00 is positive and can be removed from the right-hand side of (11) to produce:\nhn+1(scurrent)\u2212 h0(s) \u2265 hn(s\u2032)\u2212 h0(s\u2032),\nwhich is the desired result.\nThe second intermediate result to prove termination is the following lemma.\nLemma 2 Let n be an iteration of any of aLSS-LRTA\u2217, daLSS-LRTA\u2217, aRTAA\u2217, or daRTAA\u2217, when initialized with a consistent heuristic h. If snext is not set equal to the state s\u2032 with least f -value in Open, then:\nhn(s \u2032)\u2212 h0(s\u2032) > hn(snext)\u2212 h0(snext).\nProof: Indeed, if aRTAA\u2217 or aLSS-LRTA\u2217 are run, this means that snext is such that snext is not marked as updated, which means that hn(snext) = h0(snext), or equivalently, that hn(snext)\u2212 h0(snext) = 0. Moreover, the best state in Open, s\u2032, was not chosen and hence\nit must be that s\u2032.updated = true, which means that h(s\u2032) \u2212 h0(s\u2032) > 0. We obtain then that hn(s\n\u2032)\u2212 h0(s\u2032) > hn(snext)\u2212 h0(snext). The case of daRTAA\u2217 or daLSS-LRTA\u2217 is direct by the condition in Line 5 of Algo-\nrithm 9. Hence, it is also true that hn(s \u2032)\u2212 h0(s\u2032) > hn(snext)\u2212 h0(snext).\nNow we are ready to prove the main termination result.\nTheorem 6 Let P be an undirected finite real-time search problem such that a solution exists. Let h be a consistent heuristic for P . Then, any of aLSS-LRTA\u2217, daLSS-LRTA\u2217, aRTAA\u2217, or daRTAA\u2217, used with h, will find a solution for P .\nProof: Let us assume the contrary. There are two cases under which the algorithms do not return a solution: (a) they return \u201cno solution\u201d in Line 5 (Algorithm 1), and (b) the agent traverses an infinite path that never hits a solution node.\nFor (a) assume any of the algorithms is in state s before the call to A\u2217. When it reaches Line 5 (Algorithm 1), the open list is empty, which means the agent has exhausted the search space of states reachable from s without finding a solution; this is a contradiction with the fact that a solution node is reachable from s and the fact that the search problem is undirected.\nFor (b) assume that the agent follows an infinite path \u03c0. Observe that in such an infinite execution, after some iteration\u2014say, R\u2014the value of variable c does not increase anymore. This is because all states around states in \u03c0 have been observed in the past. As a consequence, in any iteration after R the agent traverses the complete path identified by the A\u2217 lookahead procedure (Line 8 in Algorithm 1).\nA second important observation is that, after iteration R, the value of h for the states in \u03c0 is finite and cannot increase anymore. Indeed, by Theorems 4 and 3, h remains consistent and hence admissible, which means that h(s) is bounded by the actual cost to reach a solution from s, for any s in \u03c0. Moreover, since c does not change anymore, the call to the update function will not change the value of h(s), for every s in \u03c0.\nNow we are ready to finish the proof. Consider the algorithm executes past iteration R. Since the path is infinite and the state space is finite, in some iteration after R the algorithm decides to go back to a previously visited state. As such, we are going to assume the agent visits state t0 and selects to move trough states t1t2 \u00b7 \u00b7 \u00b7 tr\u22121trt0 \u00b7 \u00b7 \u00b7 . Since the heuristic does not change anymore, we simply denote it by h, regardless of the iteration number. We distinguish two cases.\nCase 1 The agent always decides to move to the best state in Open, s\u2032, and hence\u2014 depending on the algorithm that is used\u2014by Proposition 1 or 2, h(s) = k(s, s\u2032)+h(s\u2032), which implies h(s) > h(s\u2032), since action costs are positive. This implies that:\nh(t0) > h(t1) > h(t2) > . . . > h(tn) > h(t0),\nwhich is a contradiction; it cannot be the case that h(t0) > h(t0).\nCase 2 At least once, the agent does not move to the best state in Open. Without loss of generality, we assume this happens only once, for a state ti for some i < r. Let t\n\u2217 be a state with the smallest f -value in Open after the lookahead is carried out from ti.\nBy Lemma 1, we can write the following inequalities.\nh(t0)\u2212 h0(t0) \u2265 h(t1)\u2212 h0(t1), ...\nh(ti\u22121)\u2212 h0(ti\u22121) \u2265 h(ti)\u2212 h0(ti), h(ti)\u2212 h0(ti) \u2265 h(t\u2217)\u2212 h0(t\u2217), h(ti+1)\u2212 h0(ti+1) \u2265 h(ti+2)\u2212 h0(ti+2), ...\nh(tr)\u2212 h0(tr) \u2265 h(t0)\u2212 h0(t0).\nLet I be a set containing these inequalities. Now since when in state ti the algorithm decides to move to ti+1 instead of t \u2217, we use Lemma 2 to write:\nhn(t \u2217)\u2212 h0(t\u2217) > hn(ti+1)\u2212 h0(ti+1). (12)\nThe inequalities in I together with (12) entail h(t0)\u2212 h0(t0) > h(t0)\u2212 h0(t0), which is a contradiction.\nIn both cases we derive contradictions and hence we conclude the algorithm cannot enter an infinite loop and thus finds a solution.\nWe now turn our attention to convergence. The literature often analyzes the properties of real-time heuristic search when they are run on a sequence of trials (e.g., Shimbo & Ishida, 2003). Each trial is characterized by running the algorithm from the start state until the problem is solved. The heuristic function h resulting from trial n is used to feed the algorithm\u2019s h variable in trial n+ 1.\nBefore stating the convergence theorem we prove a result related to how h increases between successive iterations or trials. Indeed, each iteration of our search algorithms potentially increases h, making it more informed. The following result implies that this improvement cannot be infinitesimal.\nLemma 3 Let P be a finite undirected search problem, and let Sol be a set of states in P from which a solution can be reached. Let n be an iteration of any of aLSS-LRTA\u2217, daLSSLRTA\u2217, aRTAA\u2217, or daRTAA\u2217. Then hn(s) can only take on a finite number of values, for every s in P .\nProof: Given Proposition 1, along an execution of any of the algorithms of the LSS-LRTA\u2217 family, it is simple to prove by induction on n that:\nhn(s) = K + h0(s \u2032\u2032\u2032),\nfor any n, where K is sum of the costs of 0 or more arcs in P under cost function cn. On the other hand, given the update rule of any of the algorithms of the RTAA\u2217 family (e.g., Line 6 in Algorithm 8),\nhn(s) = K \u2212K \u2032 + h0(s\u2032\u2032\u2032),\nfor any n, where K and K \u2032 correspond to the sum of the costs of some arcs in P under cost function cn.\nSince in finite problems there is a finite number of arcs, the quantities referred to by K and K \u2032 can only take on a finite number of values. This implies that hn(s), for any s in P , can only take on a finite number of values, which concludes the proof.\nBelow we show that if h converges after a sequence of trials, the solution found with h is optimal.\nTheorem 7 Let P be an undirected finite real-time search problem such that a solution exists. Let h be a consistent heuristic for P . When initialized with h, a sequence of trials of any of aLSS-LRTA\u2217, daLSS-LRTA\u2217, aRTAA\u2217, or daRTAA\u2217, converges to an optimal solution.\nProof: First, observe that since the heuristic is admissible, it remains admissible after a number of trials are run. This is a consequence of Theorems 3 and 4. Hence, for every state s from which a goal state can be reached, h(s) is bounded from above by the (finite amount) h\u2217(s).\nOn the other hand, by Lemma 3, the h-values of states from which a solution is reachable can only increase a finite number of times. After a sequence of trials the value of h thus converges; i.e., at least for one complete trial, h(s) is not changed, for every s in P . We can also assume that in such a trial, the value of c does not change either, since once h converges, the same path of states is always followed and thus no new cost increases are made.\nLet us focus on a run of any of our algorithms in which both h and c do not change. Observe that this means that hn(s) = h0(s) for any n (recall h0 is the heuristic given as input to the algorithm). Independent of the algorithm used, this implies the algorithm always moves to the best state in Open. Let s1 . . . sm be the sequence of states that were assigned to snext during the execution (sm is thus a goal state). Observe that since c does not change along the execution, states s1 . . . sm are actually visited by the agent. Depending on the algorithm that is used, by Proposition 1 or 2, we know:\nh(si) = k(si, si+1) + h(si+1), for all i \u2208 {0, . . . ,m\u2212 1}, (13)\nwhere k(si, si+1) is the cost of an optimal path between si and si+1. Since the heuristic is consistent h(sm) = 0, and thus with the family of equations in (13) we conclude h(s0) is equal to \u2211m\u22121 i=0 k(si, si+1), which corresponds to the cost of the path traversed by the agent. But we know that h is also admissible, so:\nh(s0) = m\u22121\u2211 i=0 k(si, si+1) \u2264 h\u2217(s0).\nSince h\u2217(s0) is the cost of an optimal solution, we conclude the path found has an optimal cost."}, {"heading": "7. Empirical Evaluation", "text": "We evaluated our algorithms at solving real-time navigation problems in unknown environments. LSS-LRTA\u2217 and RTAA\u2217 are used as a baseline for our comparisons. For fairness, we used comparable implementations that use the same underlying codebase. For example, all search algorithms use the same implementation for binary heaps as priority queues and break ties among cells with the same f -values in favor of cells with larger g-values, which is known to be a good tie-breaking strategy.\nWe carried out our experiments over two sets of benchmarks: deployed game maps and mazes. We used twelve maps from deployed video games to carry out the experiments. The first six are taken from the game Dragon Age, and the remaining six are taken from the game StarCraft. The maps were retrieved from Nathan Sturtevant\u2019s pathfinding repository.1 In addition, we used four maze maps taken from the HOG2 repository.2 They are shown in Figure 5. All results were obtained using a Linux machine with an Intel Xeon CPU running at 2GHz and 12 GB RAM.\nAll maps are regarded as undirected, eight-neighbor grids. Horizontal and vertical movements have cost 1, whereas diagonal movements have cost \u221a 2. We used the octile distance (Sturtevant & Buro, 2005) as heuristic.\n1. http://www.movingai.com/ and http://hog2.googlecode.com/svn/trunk/maps/. For Dragon Age we used the maps brc202d, orz702d, orz900d, ost000a, ost000t and ost100d of size 481\u00d7530, 939\u00d7718, 656 \u00d7 1491 969 \u00d7 487, 971 \u00d7 487, and 1025 \u00d7 1024 cells respectively. For StarCraft, we used the maps ArcticStation, Enigma, Inferno JungleSiege, Ramparts and WheelofWar of size 768 \u00d7 768, 768 \u00d7 768, 768\u00d7 768, 768\u00d7 768, 512\u00d7 512 and 768\u00d7 768 cells respectively. 2. http://hog2.googlecode.com/svn/trunk/maps/\nFor our evaluation we ran all algorithms for 10 different lookahead values. For each map, we generate 500 test cases. For each test case we choose the start and goal cells randomly.\nIn the presentation of our results we sometimes use the concept of improvement factor. When we say that the improvement factor of an algorithm A with respect to B in terms of average solution cost is n, it means that on average A produces solutions that are n times cheaper than the ones found by B.\nNext we describe the different views of the experimental data that is shown in plots and tables. We then continue to draw our experimental conclusions."}, {"heading": "7.1 An Analysis of the LSS-LRTA\u2217 Variants", "text": "This section analyzes the performance of LSS-LRTA\u2217, aLSS-LRTA\u2217 and daLSS-LRTA\u2217. Figure 6 shows two plots for the average solution costs versus the average planning time per episode for the three algorithms in games and mazes benchmarks. Planning time per planning episode is an accurate measure of the effort carried out by each of the algorithms. Thus these plots illustrate how solution quality varies depending on the effort that each algorithm carries out.\nRegardless of the search effort, we observe aLSS-LRTA\u2217 slightly but consistently outperforms LSS-LRTA\u2217 in solution cost. In games benchmarks we observe that for equal search effort, aLSS-LRTA\u2217 produces average improvement factors between 1.08 and 1.20 in terms of solution cost. In mazes, on the other hand, improvement factors are between 1.04 and 1.25. In games, the largest improvements are observed when the lookahead parameter (and hence the search time per episode) is rather small. Thus aLSS-LRTA\u2217\u2019s advantage over LSS-LRTA\u2217 is more clearly observed when tighter time constraints are imposed on planning episodes.\nOften times results in real-time search literature are presented in the form of tables, with search performance statistics reported per each lookahead value. We provide such tables the appendix of the paper (Tables 5 and 6). An important observation that can be drawn from the tables is that time per planning episode in LSS-LRTA\u2217 and aLSS-LRTA\u2217 are very similar for a fixed lookahead value; indeed, the time per planning episode of aLSSLRTA\u2217 is only slightly larger than that of LSS-LRTA\u2217. This is interesting since it shows that the worst-case asymptotic complexity does not seem to be achieved for aLSS-LRTA\u2217 (cf. Table 2).\nThe experimental results show that daLSS-LRTA\u2217\u2019s more refined mechanism for escaping depressions is better than that of aLSS-LRTA\u2217. For any given value of the search effort, daLSS-LRTA\u2217 consistently outperforms aLSS-LRTA\u2217 by a significant margin in solution cost in games and mazes. daLSS-LRTA\u2217 also outperforms aLSS-LRTA\u2217 in total search time, i.e., the overall time spent searching until a solution is found. Details can be found in Tables 5 and 6. When the search effort for each algorithm is small, daLSS-LRTA\u2217\u2019s average solution quality is substantially better than aLSS-LRTA\u2217\u2019s; the improvements are actually close to an order of magnitude.\ndaLSS-LRTA\u2217 consistently outperforms LSS-LRTA\u2217 by a significant margin in total search time and solution quality, independent of the search effort employed. In terms of solution cost daLSS-LRTA\u2217 produces average improvement factors with respect to LSSLRTA\u2217 between 1.66 and an order of magnitude in the game benchmarks, and produces average improvement factors between 1.49 and an order of magnitude in the mazes benchmarks. For a fixed lookahead (see Tables 5 and 6 for the specific numbers), the time spent per planning episode by daLSS-LRTA\u2217 is larger than time spent per planning episode by LSS-LRTA\u2217 because daLSS-LRTA\u2217 makes more heap percolations than LSS-LRTA\u2217. However, for small values of the lookahead parameter, daLSS-LRTA\u2217 obtains better solutions using less time per planning episode than LSS-LRTA\u2217 used with a much larger lookahead. For example, in game maps, with a lookahead parameter equal to 32, daLSS-LRTA\u2217 obtains better solutions than LSS-LRTA\u2217 with the lookahead parameter equal to 128, requiring, on average, 2.6 times less time per planning episode. In mazes, with a lookahead parameter equal to 16, daLSS-LRTA\u2217 obtains better solutions than LSS-LRTA\u2217 with the lookahead parameter equal to 64, requiring, on average, 2.4 times less time per planning episode.\nFor low values of the lookahead parameter (i.e. very limited search effort) daLSS-LRTA\u2217 obtains better solutions in less time per planning episode than aLSS-LRTA\u2217 used with a much larger lookahead. For example, in game maps, with a lookahead parameter equal to 1, daLSS-LRTA\u2217 obtains better solutions than aLSS-LRTA\u2217 with the lookahead parameter equal to 16, requiring, on average, 14.1 times less time per planning episode. On the other hand, in mazes with a lookahead parameter equal to 1, daLSS-LRTA\u2217 obtains better solutions than aLSS-LRTA\u2217 with the lookahead parameter equal to 16, requiring, on average, 11.6 times less time per planning episode.\nFor a fixed lookahead (see Tables 5 and 6), the time taken by daLSS-LRTA\u2217 per planning episode is larger than the time taken by aLSS-LRTA\u2217 per planning episode. This increase can be explained because, on average, daLSS-LRTA\u2217\u2019s open list grows larger than that of aLSS-LRTA\u2217. This is due to the fact that, in the benchmarks we tried, daLSS-LRTA\u2217 tends to expand cells that have less obstacles around than aLSS-LRTA\u2217 does. As a result,\ndaLSS-LRTA\u2217 expands more cells in the learning phase or makes more heap percolations in the lookahead phase than aLSS-LRTA\u2217.\nResults show that, among the LSS-LRTA\u2217 variants, daLSS-LRTA\u2217 is the algorithm with the best performance. In fact daLSS-LRTA\u2217 is clearly superior to LSS-LRTA\u2217. Of the 60,000 runs (12 maps \u00d7 500 test cases \u00d7 10 lookahead-values) in game benchmarks, daLSS-LRTA\u2217 obtains a better solution quality than LSS-LRTA\u2217 in 69.9% of the cases, they tie in 20.9% of the cases, and LSS-LRTA\u2217 obtains a better-quality solution in only 9.2% of the cases.\nOf the 20,000 (4 maps \u00d7 500 test cases \u00d7 10 lookahead-values) runs in mazes benchmarks, daLSS-LRTA\u2217 obtains a better solution quality than LSS-LRTA\u2217 in 75.1% of the cases, they tie in 3.3% of the cases, and LSS-LRTA\u2217 obtains a better-quality solution in 21.7% of the cases."}, {"heading": "7.2 An Analysis of the RTAA\u2217 Variants", "text": "In this section we analyze the relative performance of RTAA\u2217, aRTAA\u2217, and daRTAA\u2217. Figure 7 shows two plots of the average solution costs versus the average effort carried out per search episode.\nFor the same search effort, we do not observe significant improvements of aRTAA\u2217 over RTAA\u2217. Indeed, only for small values of the average time per search episode does aRTAA\u2217 improve the solution quality upon that of RTAA\u2217. In general, however, both algorithms seem to have very similar performance.\nOn the other hand, the results show that daRTAA\u2217\u2019s mechanism for escaping depressions is substantially better than that of aRTAA\u2217. For small values for the lookahead parameter (and hence reduced search effort), daRTAA\u2217 obtains better solutions than the other variants used with a much larger lookahead. Indeed, for limited search effort, daRTAA\u2217 is\napproximately an order of magnitude better than the two other algorithms. For example, in game maps, with a lookahead parameter equal to 1, daRTAA\u2217 obtains better solutions than aRTAA\u2217 with the lookahead parameter equal to 16, requiring, on average, 10.4 times less time per planning episode.\ndaRTAA\u2217 substantially improves RTAA\u2217, which is among the best real-time heuristic search algorithms known to date. In game maps, daRTAA\u2217 needs only a lookahead parameter of 16 to obtain solutions better than RTAA\u2217 with the lookahead parameter of 64. With those values, daRTAA\u2217 requires about 2.3 times less time per planning episode than RTAA\u2217.\nOur results show that daRTAA\u2217 is the best-performing algorithm of the RTAA\u2217 family. Of the 60,000 runs in game-map benchmarks, daRTAA\u2217 obtains a better solution quality than RTAA\u2217 in 71.2% of the cases, they tie in 20.5% of the cases, and RTAA\u2217 obtains a better-quality solution in only 8.3% of the cases. Of the 20,000 runs in mazes, daRTAA\u2217 obtains a better solution quality than RTAA\u2217 in 78.0% of the cases, they tie in 2.7% of the cases, and RTAA\u2217 obtains a better-quality solution in 19.4% of the cases.\n7.3 daLSS-LRTA\u2217 Versus daRTAA\u2217\ndaRTAA\u2217, the best performing algorithm among the RTAA\u2217 variants, is also superior to daLSS-LRTA\u2217, the best-performing algorithm of the LSS-LRTA\u2217 variants. Figure 8 shows average solution costs versus search effort, in game maps and mazes.\nAs can be seen in the figure, when the lookahead parameter is small (i.e., search effort is little), the performance of daRTAA\u2217 and daLSS-LRTA\u2217 is fairly similar. However, as more search is allowed per planning episode, daRTAA\u2217 outperforms daLSS-LRTA\u2217. For example, in games benchmarks, daRTAA\u2217, when allowed to spend 0.08 milliseconds per episode, will obtain solutions comparable to those of daLSS-LRTA\u2217 but when allowed do spend 0.18 millisecconds per episode.\nFurthermore, the slopes of the curves are significantly more favorable to daRTAA\u2217 over daLSS-LRTA\u2217. This can be verified in both types of benchmarks and is important since it speaks to an inherent superiority of the RTAA\u2217 framework when time per planning episode is the most relevant factor."}, {"heading": "7.4 An Analysis of Disaggregated Data", "text": "The performance of real-time algorithms usually varies depending on the map used. To illustrate how the algorithms perform in different maps, Figure 9 shows the improvement on solution cost of daLSS-LRTA\u2217 over LSS-LRTA\u2217 on 4 game and 4 maze benchmarks. They confirm that improvements can be observed in all domains thus showing that average values are representative of daLSS-LRTA\u2217\u2019s behavior in individual benchmarks. Although aLSSLRTA\u2217 and daLSS-LRTA\u2217 outperform LSS-LRTA\u2217 on average, there are specific cases in which the situation does not hold. Most notably, we observe that in one of the maze benchmarks daLSS-LRTA\u2217 does not improve significantly with respect to LSS-LRTA\u2217 for large values of the lookahead parameter. We discuss this further in the next section. Figure 10 shows also the improvement factors of daRTAA\u2217 over RTAA\u2217. In this plot, the different algorithms show a similar relative performance in relation to the LSS-LRTA\u2217 variants."}, {"heading": "7.5 A Worst-Case Experimental Analysis", "text": "Although all our algorithms perform a resource-bounded computation per planning episode, it is hard to tune the lookahead parameter in such a way that both LRTA\u2217 and daLSSLRTA\u2217 will incur the same worst-case planning effort. This is because the time spent in extracting the best state from the open list depends on the structure of the search space expanded in each lookahead phase.\nIn this section we set out to carry an experimental worst-case analysis based on a theoretical worst-case bound. This bound is obtained from the worst-case effort per planning step as follows. If RTAA\u2217 performs k expansions per planning episode, then the open list could contain up to 8k states. This is because each state has at most 8 neighbors. In the worst case, the effort spent in adding all such states to the open list would be 8k log 8k. On the other hand, daRTAA\u2217 would make the same effort to insert those states into the open list, but would incur an additional cost of 8k log 8k, in the worst-case, to remove all states from the open list. Therefore, in a worst-case scenario, given a lookahead parameter equal to k, daRTAA\u2217 will make double the effort than RTAA\u2217 makes for the same parameter.\nBased on that worst-case estimation, Figure 11 presents the performance of the RTAA\u2217 variants, displacing the RTAA\u2217 curve by a lookahead factor of 2. We conclude that in this worst-case scenario daRTAA\u2217 still clearly outperforms RTAA\u2217. Gains vary from one order of magnitude, for low values of the lookahead parameter, to very similar performance when the lookahead parameter is high.\nWe remark, however, that we never observed this worst-case in practice. For example, in our game benchmarks, RTAA\u2217, when used with a lookahead parameter 2k spends, on average 50% more time per planning episode than daRTAA\u2217 used with lookahead parameter k."}, {"heading": "8. Discussion", "text": "There are a number of aspects of our work that deserve a discussion. We focus on two of them. First, we discuss the setting in which we have evaluated our work, which focused on showing performance improvements in the first trial for a search in an a priori unknown domain, without considering other settings. Second, we discuss in which scenarios our algorithms may not exhibit average performance improvements that were shown in the previous section."}, {"heading": "8.1 The Experimental Setting: Unknown Environments, First Trial", "text": "Our algorithm is tailored to solving quickly a search problem in which the environment is initially unknown. This setting has several applications, including goal-directed navigation in unknown terrain (Koenig et al., 2003; Bulitko & Lee, 2006). It has also been widely used to evaluate real-time heuristic search algorithms (e.g., Koenig, 1998; Herna\u0301ndez & Meseguer, 2005; Bulitko & Lee, 2006; Herna\u0301ndez & Meseguer, 2007; Koenig & Sun, 2009).\nOn the other hand, we did not present an evaluation of our algorithm in environments that are known a priori. In a previous paper (Herna\u0301ndez & Baier, 2011d), however, we showed that aLSS-LRTA\u2217 obtains similar improvements over LSS-LRTA\u2217 when the environment is known. However, we omit results on known environments since RTAA\u2217 and LSS-LRTA\u2217 are not representative of the state of the art in those scenarios. Indeed, algorithms like TBA* (Bjo\u0308rnsson et al., 2009) outperform LSS-LRTA\u2217 significantly. It is not immediately obvious how to incorporate our techniques to algorithms like TBA*.\nWe did not present experimental results regarding convergence after several successive search trials. Recall that in this setting, the agent is \u201cteleported\u201d to the initial location\nand a new search trial is carried out. Most real-time search algorithms\u2014ours included\u2014are guaranteed to eventually find an optimal solution. Our algorithms do not particularly excel in this setting. This is because the heuristic value of fewer states is updated, and hence the heuristic values for states in the search space converges slowly to the correct value. As such, generally more trials are needed to converge.\nConvergence performance is important for problems that are solved offline and in which real-time approaches may be more adequate for computing an approximation of the optimal solution. This is the case of the problem of computing an optimal policy in MDPs using Real-Time Dynamic Programming (Barto, Bradtke, & Singh, 1995). We are not aware, however, of any application in deterministic search in which searching offline using realtime search would yield better performance than using other suboptimal search algorithms (e.g., Richter, Thayer, & Ruml, 2010; Thayer, Dionne, & Ruml, 2011). Indeed, Wilt, Thayer, and Ruml (2010) concluded that real-time algorithms, though applicable, should not be used for solving shortest path problems unless there is a need for real-time action."}, {"heading": "8.2 Bad Performance Scenarios", "text": "Although our algorithms clearly outperform its originators LSS-LRTA\u2217 and RTAA\u2217 on average, it is possible to contrive families of increasingly difficult path-finding tasks in which our algorithms perform worse than their respective predecessors.\nConsider for example the 4-connected grid-world scenario of size 7\u00d7n shown in Figure 12. The goal of the agent is to reach the state labeled with G, starting from S. Assume furthermore that to solve this problem we run aRTAA\u2217 or aLSS-LRTA\u2217, with lookahead parameter equal to 1, and that ties are broken such that the up movement has priority over the down movement. In the initial state both algorithms will determine that the initial state (cell E3) is in a heuristic depression and thus will update the heuristic of cell E3. Cell E3 is now marked as in a depression. Since both cells D3 and F3 have the same heuristic value and ties are broken in favor of upper cells, the agent is then moved to cell D3. In later iterations, the algorithm will not prefer to move to cells that have been updated and therefore the agent will not go back to state E3 unless it is currently in D3 and (at least) C3 is also marked. However, the agent will not go back to D3 quickly. Indeed, it will visit all states to the right of Wall 1 and Wall 2 before coming back to E3. This happens because, as the algorithm executes, it will update and mark all visited states, and will never prefer to go back to a previously marked position unless all current neighbors are also marked.\nIn the same situation, RTAA\u2217 and LSS-LRTA\u2217, run with lookahead parameter 1 will behave differently depending on the tie-breaking rules. Indeed, if the priority is given by up (highest), down, right, and left (lowest), then both RTAA\u2217 and LSS-LRTA\u2217 find the goal fairly quickly as they do not have to visit states to the right of the walls. Indeed, since the tie-breaking rules prefer a move up, the agent reaches cell A3 after 4 moves, and then proceeds straight to the goal. In such situations, the performance of aRTAA\u2217 or aLSS-LRTA\u2217 can be made arbitrarily worse than that of RTAA\u2217 or LSS-LRTA\u2217, as n is increased.\nA quite different situation is produced if the tie-breaking follows the priorities given by up (highest), right, down, and left (lowest). In this case all four algorithms have to visit the states to the right of both walls. Indeed, once A3 is reached, there is a tie between\nthe h-value of B3 and A4. The agent prefers moving to A4, and from there on it continues moving to the right of the grid in a zig-zag fashion.\nAfter investigating executions of our \u201cda-\u201d algorithms in the maze512-4-0 benchmark (performance is shown in Figures 9 and 10), we believe that the lack of improvement in this particular benchmark can be explained by the situation just described. This benchmark is a 512 \u00d7 512 maze in which corridors have a 4-cell width. For low lookahead values, the number of updates is not high enough to \u201cblock\u201d the corridors. As such, for low values of the lookahead parameter the increase in performance is still reasonably good. As the lookahead increases, the algorithm updates more states in one single iteration, and, as a result, chances are that good paths may become blocked.\nInterestingly, however, we do not observe this phenomenon on mazes with wider corridors or on game maps. A necessary condition to \u201cblock\u201d a corridor that leads to a solution is that the agent has sufficient knowledge about the borders of the corridor. In mazes with narrow corridors this may happen with relative ease, as the agent only needs a few moves to travel between opposite walls. In grids in which corridors are wide however, knowledge about the existence of obstacles (walls) is hard to obtain by the agent, and, thus, the chances of updating and blocking, a corridor that leads to a solution are lower.\nWe believe that it is possible to prove that our algorithms are always better or always worse for specific search space topologies. We think, nevertheless, that such an analysis may be hard to carry out, and that its practical significance may be limited. Therefore we decided to exclude it from the scope of this work. On the other hand, we think that the impressive performance exhibited by our algorithms in many benchmarks is sufficiently strong in favor of using our algorithms in domains that do not contain narrow corridors."}, {"heading": "9. Related Work", "text": "Besides LSS-LRTA\u2217 and RTAA\u2217, there are a number of real-time search algorithms that can be used in a priori unknown environments. LRTA\u2217(k) and LRTA\u2217LS(k) (Herna\u0301ndez & Meseguer, 2005, 2007) are two algorithms competitive with LSS-LRTA\u2217 that are capable of\nlearning the heuristic of several states at the same time; the states for which the heuristic is learned is independent from those expanded in the lookahead phase. They may escape heuristic depressions more quickly than LRTA\u2217, but its action selection mechanism is not aware of heuristic depressions. eLSS-LRTA\u2217 is a preliminary version of aLSS-LRTA\u2217 we presented in an extended abstract (Herna\u0301ndez & Baier, 2011a). It is outperformed by aLSS-LRTA\u2217 on average, as it usually becomes too focused on avoiding depressions.\nOur algorithms have been designed in order to find good-quality solutions on the first search trial. Other algorithms described in the literature have been designed with different objectives in mind. For example, RIBS (Sturtevant, Bulitko, & Bjo\u0308rnsson, 2010) is a realtime algorithm specifically designed to converge quickly to an optimal solution. It will move the agent as if an iterative-deepening A\u2217 search was carried out. As such the first solution it finds is optimal. As a consequence, RIBS potentially requires more time to find one solution than LSS-LRTA\u2217 does, but if an optimal solution is required RIBS will likely outperform LSS-LRTA\u2217 run to convergence. f -LRTA* (Sturtevant & Bulitko, 2011) is another recent real-time search algorithm which builds upon ideas introduced by RIBS, in which the gcost of states is learned through successive trials. It has good convergence performance, but needs to do more computation per planning step than LSS-LRTA\u2217.\nIncremental A\u2217 methods, like D* (Stentz, 1995), D*Lite (Koenig & Likhachev, 2002), Adaptive A* (Koenig & Likhachev, 2006a), and Tree Adaptive A* (Herna\u0301ndez, Sun, Koenig, & Meseguer, 2011), are search methods that also allow solving goal-directed navigation problems in unknown environments. If the first-move delay is required to be short, incremental A* methods cannot be used since they require to compute a complete solution before starting to move. Real-time search remains the only applicable strategy for this task when limited time is allowed per planning episode.\nLess related to our work are algorithms that abide to real-time search constraints but that assume the environment is known in advance and that sufficient time is given prior to solving the problem, allowing preprocessing. Examples are D LRTA\u2217 (Bulitko, Lus\u030ctrek, Schaeffer, Bjo\u0308rnsson, & Sigmundarson, 2008) and kNN-LRTA\u2217 (Bulitko et al., 2010), tree subgoaling (Herna\u0301ndez & Baier, 2011b), or real-time search via compressed path databases (Botea, 2011).\nFinally, the concept of cost-sensitive depression in real-time search could be linked to other concepts used to describe the poor performance of planning algorithms. For example, Hoffmann (2005, 2011) analyzed the existence of plateaus in h+, an effective admissible domain-independent planning heuristic, and how this negatively affects the performance of otherwise fast planning algorithms. Cushing, Benton, and Kambhampati (2011) introduced the concept of \u03b5-traps that is related to poor performance of best-first search in problems in which action costs have a high variance. \u03b5-traps are areas of the search space connected by actions of least cost. As such, the h-values of states in \u03b5-traps is not considered in their analysis. Although we think that the existence of cost-sensitive heuristic depressions does affect the performance of A\u2217, the exact relation between the performance of A\u2217 and heuristic depressions does not seem to be obvious."}, {"heading": "10. Summary and Future Work", "text": "We have presented a simple principle for guiding real-time search algorithms away from heuristic depressions. We proposed two alternative approaches for implementing the principle: mark-and-avoid and move-to-border. In the first approach, states that are proven to be in a depression are marked in the update phase, and then avoided, if possible, when deciding the next move. In the second approach, the algorithm selects as the next move the state that seems closer to the border of a depression.\nBoth approaches can be implemented efficiently. Mark-and-avoid requires very little overhead, which results in an almost negligible increment in time per planning episode. Move-to-border, on the other hand, requires more overhead per planning episode, but, given a time deadline per planning episode, it is able to obtain the best-quality solutions.\nExperimentally, we have shown that in goal-directed navigation tasks in unknown terrain, our algorithms outperform their predecessors RTAA\u2217 and LSS-LRTA\u2217. Indeed, the algorithms based on move-to-border\u2014daLSS-LRTA\u2217 and daRTAA\u2217\u2014are significantly more efficient than LSS-LRTA\u2217 and RTAA\u2217, especially when the lookahead parameter is a small value.\nThe four algorithms proposed have good properties: in undirected, finite search spaces, they are guaranteed to find a solution if such a solution exists. Moreover, they converge to an optimal solution after running a number of search trials.\nDepression avoidance is a principle applicable to other real-time heuristic search algorithms. Indeed, we think it could be easily incorporated into LRTA\u2217(k), LRTA\u2217LS(k), and P-LRTA* (Rayner et al., 2007). All those algorithms have specialized mechanisms for updating the heuristic, but the mechanism to select the next state is just like LSS-LRTA\u2217\u2019s run with lookahead parameter equal to 1. We think significant improvements could be achieved if the procedure to select the next movement was changed by daLSS-LRTA\u2217\u2019s. We also believe depression avoidance could be incorporated into multi-agent real-time search algorithms (e.g., Knight, 1993; Yokoo & Kitamura, 1996; Kitamura, Teranishi, & Tatsumi, 1996)."}, {"heading": "11. Acknowledgments", "text": "We thank the JAIR reviewers who provided extensive feedback that helped improve this article significantly. We also thank the IJCAI-11, SoCS-11, and AIIDE-11 anonymous reviewers for their thoughtful insights on earlier versions of this work. We are very grateful to Cristhian Aguilera, who helped out running some of the experiments. Carlos Herna\u0301ndez was partly funded a by Fondecyt project #11080063. Jorge Baier was partly funded by the VRI-38-2010 grant from Pontificia Universidad Cato\u0301lica de Chile and the Fondecyt grant number 11110321."}, {"heading": "Appendix A. Additional Experimental Data", "text": "Tables 3\u20136 show average statistics for LSS-LRTA\u2217, RTAA\u2217, and our 4 algorithms."}, {"heading": "Appendix B. Additional Proofs for Theorems", "text": "B.1 Proof of Theorem 3\nWe establish that, for any pair of neighbor states, s and s\u2032, hn+1(s) \u2264 cn+1(s, s\u2032)+hn+1(s\u2032). We divide the rest of the argument in three cases. Case 1. Both s and s\u2032 are in Closed. Then, by Proposition 1,\nhn+1(s \u2032) = kn(s \u2032, s\u2032\u2032) + hn(s \u2032\u2032), (14)\nfor some s\u2032\u2032 \u2208 Open. On the other hand, again by Proposition 1,\nhn+1(s) = min sb\u2208Open kn(s, sb) + hn(sb),\nand thus hn+1(s) \u2264 kn(s, s\u2032\u2032) + hn(s\u2032\u2032), (15) since s\u2032\u2032 is an element of Open. However, because kn(s, s \u2032\u2032) is the cost of the shortest path between s and s\u2032\u2032, we know that\nkn(s, s \u2032\u2032) \u2264 cn(s, s\u2032) + kn(s\u2032, s\u2032\u2032) (16)\nAdding up (15) and (16), we obtain\nhn+1(s) \u2264 cn(s, s\u2032) + kn(s\u2032, s\u2032\u2032) + hn(s\u2032\u2032) (17)\nUsing Equation 14 we substitute kn(s \u2032, s\u2032\u2032) + hn(s \u2032\u2032) in Inequality 17, obtaining:\nhn+1(s) \u2264 cn(s, s\u2032) + hn(s\u2032). (18)\nSince the cost function can only increase, we have that cn(s, s \u2032) \u2264 cn+1(s, s\u2032), and hence:\nhn+1(s) \u2264 cn+1(s, s\u2032) + hn(s\u2032), (19)\nFinally, since h is non-decreasing (Theorem 1), we have hn(s \u2032) \u2264 hn+1(s\u2032), which allows us to write hn+1(s) \u2264 cn+1(s, s\u2032) + hn+1(s\u2032), (20)\nwhich finishes the proof for this case. Case 2. One state among s and s\u2032 is in Closed, and the other state is not in Closed. Without loss of generality, assume s \u2208 Closed. Since s\u2032 is not in Closed, it must be in Open, because s was expanded by A\u2217 and s\u2032 is a neighbor of s. By Proposition 1 we know:\nhn+1(s) = min sb\u2208Open kn(s, sb) + hn(sb),\nbut since s\u2032 is a particular state in Open, we have:\nhn+1(s) \u2264 cn(s, s\u2032) + hn(s\u2032).\nSince cn \u2264 cn+1, we obtain:\nhn+1(s) \u2264 cn+1(s, s\u2032) + hn(s\u2032),\nwhich concludes the proof for this case. Case 3. Both s and s\u2032 are not in Closed. Since hn is consistent:\nhn(s) \u2264 cn(s, s\u2032) + hn(s\u2032) (21)\nNow we use that the h-value of s and s\u2032 are not updated (hn(s) = hn+1(s) and hn(s \u2032) = hn+1(s \u2032)), and the fact that the cost function increases to write:\nhn+1(s) \u2264 cn+1(s, s\u2032) + hn+1(s\u2032), (22)\nwhich finishes the proof for this case. In all three cases we proved the desired inequality and therefore we conclude the heuristic hn+1 is consistent with respect to cost function cn+1.\nB.2 Proof of Theorem 4\nWe establish that, for any pair of neighbor states, s and s\u2032, hn+1(s) \u2264 cn+1(s, s\u2032)+hn+1(s\u2032). We divide the rest of the argument in three cases. Case 1. Both s and s\u2032 are in Closed. We have that\nhn+1(s) = f(s \u2217)\u2212 g(s), (23)\nhn+1(s \u2032) = f(s\u2217)\u2212 g(s\u2032), (24)\nfor some s\u2217 in Open. Subtracting (24) from (23), we obtain:\nhn+1(s)\u2212 hn+1(s\u2032) = g(s\u2032)\u2212 g(s). (25)\nSince hn is consistent g(s) and g(s \u2032) correspond to the cost of the shortest path between scurrent and, respectively, s and s \u2032. Thus g(s\u2032) = kn(scurrent, s\n\u2032) and g(s) = kn(scurrent, s), and therefore:\nhn+1(s)\u2212 hn+1(s\u2032) = kn(scurrent, s\u2032)\u2212 kn(scurrent, s). (26)\nLet us consider a path from scurrent to s \u2032 that goes optimally to s, and then goes from s to s\u2032. The cost of such a path must be at least kn(scurrent, s \u2032). In other words:\nkn(scurrent, s \u2032) \u2264 kn(scurrent, s) + cn(s, s\u2032),\nwhich directly implies:\nkn(scurrent, s \u2032)\u2212 kn(scurrent, s) \u2264 cn(s, s\u2032). (27)\nNow we combine (27) and (26) to obtain:\nhn+1(s) \u2264 cn(s, s\u2032) + hn+1(s\u2032). (28)\nAnd, finally, since cn \u2264 cn+1 we conclude that:\nhn+1(s) \u2264 cn+1(s, s\u2032) + hn+1(s\u2032), (29)\nwhich finishes the proof for this case.\nCase 2. One state among s and s\u2032 is in Closed, and the other state is not in Closed. Without loss of generality, assume s \u2208 Closed. Since s\u2032 is not in Closed, it must be in Open, because s was expanded by A\u2217 and s\u2032 is a neighbor of s.\nFor some state s\u2217 in Open, we have that\nhn+1(s) = f(s \u2217)\u2212 g(s) (30)\nAgain we use the fact that, with the consistent heuristic hn, A \u2217 expands nodes with increasing f -values. Note that s\u2217 is the state that would have been expanded next by A\u2217, and that s\u2032 would have been expanded later on. Moreover, as soon as s\u2032 would have been expanded the g-value for s\u2032 is the optimal cost of the path from scurrent to s \u2032, kn(scurrent, s \u2032). Therefore, we can write: f(s\u2217) \u2264 kn(scurrent, s\u2032) + hn(s\u2032), (31)\nas kn(scurrent, s \u2032) +hn(s \u2032) is the f -value of s\u2032 upon expansion. Adding up (30) and (31), we obtain:\nhn+1(s) \u2264 kn(scurrent, s\u2032)\u2212 g(s) + hn(s\u2032)\nHowever, since s is in Closed, g(s) is the cost of an optimal path from scurrent to s, and thus:\nhn+1(s) \u2264 kn(scurrent, s\u2032)\u2212 kn(scurrent, s) + hn(s\u2032) (32)\nWe use now the same argument of the previous case to conclude that:\nkn(scurrent, s \u2032)\u2212 kn(scurrent, s) \u2264 cn(s, s\u2032). (33)\nCombining (31) and (33) we obtain:\nhn+1(s) \u2264 cn(s, s\u2032) + hn(s\u2032) (34)\nSince s\u2032 is not in closed, hn+1(s \u2032) = hn(s). Furthermore, we know that cn \u2264 cn+1. Substituting in (34), we obtain:\nhn+1(s) \u2264 cn+1(s, s\u2032) + hn+1(s\u2032), (35)\nwhich allows us to conclude the proof for this case. Case 3. Both s and s\u2032 are not in Closed. The proof is the same as that for Case 3 in Theorem 3.\nIn all three cases we proved the desired inequality and therefore we conclude the heuristic hn+1 is consistent with respect to cost function cn+1.\nB.3 An Appendix for the Proof of Theorem 5\nThis section describes the proof of Theorem 5 for the specific case of aRTAA\u2217. Let D be the maximal connected component of states connected to s such that (1) all states in D are in Closed after the call to A\u2217 in iteration n, and (2) any state sd in D is such that hn+1(sd) > hn(sd). We prove that D is a cost-sensitive heuristic depression of hn.\nLet s\u2032 be a state in the boundary of D; as argued for the case of aLSS-LRTA\u2217, we can show that hn(s \u2032) = hn+1(s \u2032). Now, let sd be a state in D. We continue the proof by showing\nthat hn(sd) is too low with respect to hn(s \u2032), which means that D is a heuristic depression of hn. For this final part of the proof, we distinguish two cases: (Case 1) s \u2032 \u2208 Closed, and (Case 2) s\u2032 \u2208 Open. For Case 1, given that hn+1(s \u2032) = hn(s \u2032), we know hn(s\n\u2032) = f\u2217 \u2212 g(s\u2032), where f\u2217 is the lowest f -value in the open list after the algorithm is run, and hence:\nf\u2217 = hn(s \u2032) + g(s\u2032) (36)\nOn the other hand, since by definition of D the heuristic value has increased for sd,\nhn(sd) < hn+1(sd) = f \u2217 \u2212 g(sd). (37)\nSubstituting f\u2217 in Eq. 37 with the right-hand-side of Eq. 36, we get:\nhn(sd) < hn(s \u2032) + g(s\u2032)\u2212 g(sd). (38)\nBecause the heuristic is consistent and both s\u2032 and sd are in Closed, g(s \u2032) and g(sd) actually correspond to the cost of the cheapest path to reach, respectively, s\u2032 and sd from s; i.e., g(s\u2032) = k(s, s\u2032) and g(sd) = kn(s, sd). In addition, the triangular inequality kn(s, sd) + kn(sd, s \u2032) \u2265 kn(s, s\u2032), can be re-written as:\ng(s\u2032)\u2212 g(sd) \u2264 kn(sd, s\u2032). (39)\nInequalities 38 and 39 imply hn(sd) < kn(sd, s \u2032) + hn(s \u2032).\nFinally, for Case 2, if s\u2032 \u2208 Open, by Proposition 2 and the fact that hn+1(sd) > hn(sd), we also have that hn(sd) < kn(sd, s \u2032) + hn(s \u2032).\nIn both cases, we proved hn(sd) < kn(sd, s \u2032) + hn(s \u2032), for any sd in D and any s \u2032 in the\nboundary of D. We conclude D is a cost-sensitive heuristic depression of hn."}], "references": [{"title": "Learning to act using real-time dynamic programming", "author": ["A.G. Barto", "S.J. Bradtke", "S.P. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Barto et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1995}, {"title": "TBA*: Time-bounded A", "author": ["Y. Bj\u00f6rnsson", "V. Bulitko", "N.R. Sturtevant"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Bj\u00f6rnsson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bj\u00f6rnsson et al\\.", "year": 2009}, {"title": "Real-time search in dynamic worlds", "author": ["D.M. Bond", "N.A. Widger", "W. Ruml", "X. Sun"], "venue": "In Proceedings of the 3rd Symposium on Combinatorial Search (SoCS),", "citeRegEx": "Bond et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bond et al\\.", "year": 2010}, {"title": "Ultra-fast Optimal Pathfinding without Runtime Search", "author": ["A. Botea"], "venue": "Proceedings of the 7th Annual International AIIDE Conference (AIIDE), Palo Alto, California.", "citeRegEx": "Botea,? 2011", "shortCiteRegEx": "Botea", "year": 2011}, {"title": "Learning in real time search: a unifying framework", "author": ["V. Bulitko", "G. Lee"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bulitko and Lee,? \\Q2006\\E", "shortCiteRegEx": "Bulitko and Lee", "year": 2006}, {"title": "Learning for adaptive real-time search", "author": ["V. Bulitko"], "venue": "Computing Research Repository, cs.AI/0407016.", "citeRegEx": "Bulitko,? 2004", "shortCiteRegEx": "Bulitko", "year": 2004}, {"title": "Case-based subgoaling in real-time heuristic search for video game pathfinding", "author": ["V. Bulitko", "Y. Bj\u00f6rnsson", "R. Lawrence"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bulitko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bulitko et al\\.", "year": 2010}, {"title": "Real-time Heuristic Search for Game Pathfinding", "author": ["V. Bulitko", "Y. Bj\u00f6rnsson", "N. Sturtevant", "R. Lawrence"], "venue": "Applied Research in Artificial Intelligence for Computer Games. Springer", "citeRegEx": "Bulitko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bulitko et al\\.", "year": 2011}, {"title": "Dynamic control in real-time heuristic search", "author": ["V. Bulitko", "M. Lu\u0161trek", "J. Schaeffer", "Y. Bj\u00f6rnsson", "S. Sigmundarson"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bulitko et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bulitko et al\\.", "year": 2008}, {"title": "Introduction to Algorithms, Second Edition", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 2001}, {"title": "Cost based satisficing search considered harmful. CoRR, abs/1103.3687", "author": ["W. Cushing", "J. Benton", "S. Kambhampati"], "venue": null, "citeRegEx": "Cushing et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cushing et al\\.", "year": 2011}, {"title": "Heuristic Search: Theory and Applications", "author": ["S. Edelkamp", "S. Schr\u00f6dl"], "venue": null, "citeRegEx": "Edelkamp and Schr\u00f6dl,? \\Q2011\\E", "shortCiteRegEx": "Edelkamp and Schr\u00f6dl", "year": 2011}, {"title": "A formal basis for the heuristic determination of minimal cost paths", "author": ["P.E. Hart", "N. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "Improving LRTA*(k)", "author": ["C. Hern\u00e1ndez", "P. Meseguer"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Hern\u00e1ndez and Meseguer,? \\Q2007\\E", "shortCiteRegEx": "Hern\u00e1ndez and Meseguer", "year": 2007}, {"title": "Escaping heuristic depressions in real-time heuristic search (extended abstract)", "author": ["C. Hern\u00e1ndez", "J.A. Baier"], "venue": "In Proceedings of the 10th International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS),", "citeRegEx": "Hern\u00e1ndez and Baier,? \\Q2011\\E", "shortCiteRegEx": "Hern\u00e1ndez and Baier", "year": 2011}, {"title": "Fast subgoaling for pathfinding via real-time search", "author": ["C. Hern\u00e1ndez", "J.A. Baier"], "venue": "In Proceedings of the 21th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Hern\u00e1ndez and Baier,? \\Q2011\\E", "shortCiteRegEx": "Hern\u00e1ndez and Baier", "year": 2011}, {"title": "Real-time adaptive A* with depression avoidance", "author": ["C. Hern\u00e1ndez", "J.A. Baier"], "venue": "In Proceedings of the 7th Annual International AIIDE Conference (AIIDE),", "citeRegEx": "Hern\u00e1ndez and Baier,? \\Q2011\\E", "shortCiteRegEx": "Hern\u00e1ndez and Baier", "year": 2011}, {"title": "Real-time heuristic search with depression avoidance", "author": ["C. Hern\u00e1ndez", "J.A. Baier"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Hern\u00e1ndez and Baier,? \\Q2011\\E", "shortCiteRegEx": "Hern\u00e1ndez and Baier", "year": 2011}, {"title": "Tree adaptive A", "author": ["C. Hern\u00e1ndez", "X. Sun", "S. Koenig", "P. Meseguer"], "venue": "In Proceedings of the 10th International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS),", "citeRegEx": "Hern\u00e1ndez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hern\u00e1ndez et al\\.", "year": 2011}, {"title": "Where ignoring delete lists works, part II: Causal graphs", "author": ["J. Hoffmann"], "venue": "Proceedings of the 21th International Conference on Automated Planning and Scheduling (ICAPS), pp. 98\u2013105.", "citeRegEx": "Hoffmann,? 2011", "shortCiteRegEx": "Hoffmann", "year": 2011}, {"title": "Where \u2019ignoring delete lists\u2019 works: Local search topology in planning benchmarks", "author": ["J. Hoffmann"], "venue": "Journal of Artificial Intelligence Research, 24, 685\u2013758.", "citeRegEx": "Hoffmann,? 2005", "shortCiteRegEx": "Hoffmann", "year": 2005}, {"title": "Moving target search with intelligence", "author": ["T. Ishida"], "venue": "Proceedings of the 10th National Conference on Artificial Intelligence (AAAI), pp. 525\u2013532.", "citeRegEx": "Ishida,? 1992", "shortCiteRegEx": "Ishida", "year": 1992}, {"title": "Organizational strategies for multiagent real-time search", "author": ["Y. Kitamura", "Teranishi", "K.-i", "S. Tatsumi"], "venue": "In Proceedings of the 2nd International Conference on Multiagent Systems (ICMAS),", "citeRegEx": "Kitamura et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kitamura et al\\.", "year": 1996}, {"title": "Are many reactive agents better than a few deliberative ones", "author": ["K. Knight"], "venue": "Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI), pp. 432\u2013437.", "citeRegEx": "Knight,? 1993", "shortCiteRegEx": "Knight", "year": 1993}, {"title": "Exploring unknown environments with real-time search or reinforcement learning", "author": ["S. Koenig"], "venue": "Proceedings of the 11th Conference on Advances in Neural Information Processing Systems (NIPS), pp. 1003\u20131009.", "citeRegEx": "Koenig,? 1998", "shortCiteRegEx": "Koenig", "year": 1998}, {"title": "Agent-centered search", "author": ["S. Koenig"], "venue": "Artificial Intelligence Magazine, 22 (4), 109\u2013131.", "citeRegEx": "Koenig,? 2001", "shortCiteRegEx": "Koenig", "year": 2001}, {"title": "A comparison of fast search methods for real-time situated agents", "author": ["S. Koenig"], "venue": "Proceedings of the 3rd International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS), pp. 864\u2013871.", "citeRegEx": "Koenig,? 2004", "shortCiteRegEx": "Koenig", "year": 2004}, {"title": "A new principle for incremental heuristic search: Theoretical results", "author": ["S. Koenig", "M. Likhachev"], "venue": "In Proceedings of the 16th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Koenig and Likhachev,? \\Q2006\\E", "shortCiteRegEx": "Koenig and Likhachev", "year": 2006}, {"title": "Real-time adaptive A", "author": ["S. Koenig", "M. Likhachev"], "venue": "In Proceedings of the 5th International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS),", "citeRegEx": "Koenig and Likhachev,? \\Q2006\\E", "shortCiteRegEx": "Koenig and Likhachev", "year": 2006}, {"title": "Comparing real-time and incremental heuristic search for real-time situated agents", "author": ["S. Koenig", "X. Sun"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Koenig and Sun,? \\Q2009\\E", "shortCiteRegEx": "Koenig and Sun", "year": 2009}, {"title": "Performance bounds for planning in unknown terrain", "author": ["S. Koenig", "C.A. Tovey", "Y.V. Smirnov"], "venue": "Artificial Intelligence,", "citeRegEx": "Koenig et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koenig et al\\.", "year": 2003}, {"title": "Real-time heuristic search", "author": ["R.E. Korf"], "venue": "Artificial Intelligence, 42 (2-3), 189\u2013211.", "citeRegEx": "Korf,? 1990", "shortCiteRegEx": "Korf", "year": 1990}, {"title": "Heuristics: preintelligent search strategies for computer problem solving", "author": ["J. Pearl"], "venue": "Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.", "citeRegEx": "Pearl,? 1984", "shortCiteRegEx": "Pearl", "year": 1984}, {"title": "Real-time heuristic search with a priority queue", "author": ["D.C. Rayner", "K. Davison", "V. Bulitko", "K. Anderson", "J. Lu"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Rayner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rayner et al\\.", "year": 2007}, {"title": "The joy of forgetting: Faster anytime search via restarting", "author": ["S. Richter", "J.T. Thayer", "W. Ruml"], "venue": "In Proceedings of the 20th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Richter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Richter et al\\.", "year": 2010}, {"title": "Controlling the learning process of real-time heuristic search", "author": ["M. Shimbo", "T. Ishida"], "venue": "Artificial Intelligence,", "citeRegEx": "Shimbo and Ishida,? \\Q2003\\E", "shortCiteRegEx": "Shimbo and Ishida", "year": 2003}, {"title": "The focussed D* algorithm for real-time replanning", "author": ["A. Stentz"], "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), pp. 1652\u20131659.", "citeRegEx": "Stentz,? 1995", "shortCiteRegEx": "Stentz", "year": 1995}, {"title": "Learning where you are going and from whence you came: h- and g-cost learning in real-time heuristic search", "author": ["N.R. Sturtevant", "V. Bulitko"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Sturtevant and Bulitko,? \\Q2011\\E", "shortCiteRegEx": "Sturtevant and Bulitko", "year": 2011}, {"title": "On learning in agent-centered search", "author": ["N.R. Sturtevant", "V. Bulitko", "Y. Bj\u00f6rnsson"], "venue": "In Proceedings of the 9th International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS),", "citeRegEx": "Sturtevant et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sturtevant et al\\.", "year": 2010}, {"title": "Partial pathfinding using map abstraction and refinement", "author": ["N.R. Sturtevant", "M. Buro"], "venue": "In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Sturtevant and Buro,? \\Q2005\\E", "shortCiteRegEx": "Sturtevant and Buro", "year": 2005}, {"title": "Learning inadmissible heuristics during search", "author": ["J.T. Thayer", "A.J. Dionne", "W. Ruml"], "venue": "In Proceedings of the 21th International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Thayer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Thayer et al\\.", "year": 2011}, {"title": "Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence", "author": ["G. Weiss"], "venue": null, "citeRegEx": "Weiss,? \\Q1999\\E", "shortCiteRegEx": "Weiss", "year": 1999}, {"title": "A comparison of greedy search algorithms", "author": ["C.M. Wilt", "J.T. Thayer", "W. Ruml"], "venue": "In Proceedings of the 3rd Symposium on Combinatorial Search (SoCS)", "citeRegEx": "Wilt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wilt et al\\.", "year": 2010}, {"title": "Multiagent real-time A* with selection: Introducing competition in cooperative search", "author": ["M. Yokoo", "Y. Kitamura"], "venue": "In Proceedings of the 2nd International Conference on Multiagent Systems (ICMAS),", "citeRegEx": "Yokoo and Kitamura,? \\Q1996\\E", "shortCiteRegEx": "Yokoo and Kitamura", "year": 1996}, {"title": "A mobile robot exploration algorithm", "author": ["A. Zelinsky"], "venue": "IEEE Transactions on Robotics and Automation, 8 (6), 707\u2013717.", "citeRegEx": "Zelinsky,? 1992", "shortCiteRegEx": "Zelinsky", "year": 1992}], "referenceMentions": [{"referenceID": 25, "context": "Such is the case, for example, of autonomous robots or vehicles moving quickly through initially unknown terrain (Koenig, 2001).", "startOffset": 113, "endOffset": 127}, {"referenceID": 41, "context": "Real-time search (e.g., Korf, 1990; Weiss, 1999; Edelkamp & Schr\u00f6dl, 2011) is a standard paradigm for solving search problems in which the environment is not fully known in advance", "startOffset": 17, "endOffset": 74}, {"referenceID": 31, "context": "Early heuristic real-time algorithms like Learning Real-Time A\u2217 (LRTA\u2217) and RealTime A\u2217 (RTA\u2217) (Korf, 1990) are amenable for settings in which the environment is initially unknown.", "startOffset": 95, "endOffset": 107}, {"referenceID": 21, "context": "These algorithms will perform poorly in the presence of heuristic depressions (Ishida, 1992).", "startOffset": 78, "endOffset": 92}, {"referenceID": 23, "context": "Before, Koenig and Likhachev (2006b) had shown similar performance results but in mazes.", "startOffset": 8, "endOffset": 37}, {"referenceID": 21, "context": "\u2022 We propose a definition for cost-sensitive heuristic depressions, which is a more general notion than Ishida\u2019s (1992) notion of heuristic depression since it incorporates action costs.", "startOffset": 104, "endOffset": 120}, {"referenceID": 12, "context": "We refer to h(s) as the h-value of s and assume familiarity with the A\u2217 algorithm (Hart et al., 1968): g(s) denotes the cost of the path from the start state to s, and f(s) is defined as g(s) + h(s).", "startOffset": 82, "endOffset": 101}, {"referenceID": 31, "context": "For more details on these definitions, we refer the reader to the book authored by Pearl (1984). We refer to h(s) as the h-value of s and assume familiarity with the A\u2217 algorithm (Hart et al.", "startOffset": 83, "endOffset": 96}, {"referenceID": 44, "context": "In addition, the free-space assumption (Zelinsky, 1992; Koenig, Tovey, & Smirnov, 2003) is taken: the environment is initially assumed as obstacle-free.", "startOffset": 39, "endOffset": 87}, {"referenceID": 27, "context": "RealTime A\u2217 (RTA\u2217) and Learning Real-Time A\u2217 (LRTA\u2217)\u2014two early algorithms proposed by Korf (1990) \u2014and other modern real-time search algorithms run a search from the current state up to a fixed depth (e.", "startOffset": 86, "endOffset": 98}, {"referenceID": 5, "context": ", Korf, 1990), max of mins (Bulitko, 2004), and heuristic bounded propagation (Hern\u00e1ndez & Meseguer, 2005).", "startOffset": 27, "endOffset": 42}, {"referenceID": 24, "context": "1 LSS-LRTA\u2217 Local search space LRTA\u2217 (LSS-LRTA\u2217) was first introduced by Koenig (2004), and later presented in detail by Koenig and Sun (2009).", "startOffset": 73, "endOffset": 87}, {"referenceID": 24, "context": "1 LSS-LRTA\u2217 Local search space LRTA\u2217 (LSS-LRTA\u2217) was first introduced by Koenig (2004), and later presented in detail by Koenig and Sun (2009). It is an instance of Algorithm 1.", "startOffset": 73, "endOffset": 143}, {"referenceID": 24, "context": "2 RTAA\u2217 Real-Time Adaptive A\u2217 (RTAA\u2217) was proposed by Koenig and Likhachev (2006b). It is an instance of Algorithm 1.", "startOffset": 54, "endOffset": 83}, {"referenceID": 12, "context": "For (3), we use the a fact proven by Hart et al. (1968) about A\u2217: if consistent heuristics are used, g(s) contains the cost of the cheapest path from the start state to s right after s is extracted from Open (Line 8 in Algorithm 2).", "startOffset": 37, "endOffset": 56}, {"referenceID": 23, "context": "Koenig and Likhachev (2006a) show that for a fixed value of the lookahead parameter, the quality of the solutions obtained by LSS-LRTA\u2217 are better on average than those obtained by RTAA\u2217 in path-finding tasks over mazes.", "startOffset": 0, "endOffset": 29}, {"referenceID": 21, "context": "Ishida (1992) gave a constructive definition for heuristic depressions.", "startOffset": 0, "endOffset": 14}, {"referenceID": 21, "context": "It is known that algorithms like LRTA\u2217 behave poorly in the presence of heuristic depressions (Ishida, 1992).", "startOffset": 94, "endOffset": 108}, {"referenceID": 1, "context": "The lookahead ability of LRTS (Bulitko & Lee, 2006), and TBA\u2217 (Bj\u00f6rnsson et al., 2009) is parametrized.", "startOffset": 62, "endOffset": 86}, {"referenceID": 33, "context": "By using algorithms such as LRTA\u2217(k) (Hern\u00e1ndez & Meseguer, 2005), PLRTA\u2217 (Rayner et al., 2007) and LRTALS(k) (Hern\u00e1ndez & Meseguer, 2007) one can increase the number of states updated based on a parameter.", "startOffset": 74, "endOffset": 95}, {"referenceID": 24, "context": "Proof: Since the update procedure used by aLSS-LRTA\u2217, daLSS-LRTA\u2217 and LSS-LRTA\u2217 update variable h in exactly the same way, the proof by Koenig and Sun (2009) can be reused here.", "startOffset": 136, "endOffset": 158}, {"referenceID": 24, "context": "Proof: Since the update procedure used by aRTAA\u2217, daRTAA\u2217 and RTAA\u2217 update variable h in exactly the same way, we can re-use the proof of Theorem 1 by Koenig and Likhachev (2006b) to establish this result.", "startOffset": 151, "endOffset": 180}, {"referenceID": 30, "context": "This setting has several applications, including goal-directed navigation in unknown terrain (Koenig et al., 2003; Bulitko & Lee, 2006).", "startOffset": 93, "endOffset": 135}, {"referenceID": 1, "context": "Indeed, algorithms like TBA* (Bj\u00f6rnsson et al., 2009) outperform LSS-LRTA\u2217 significantly.", "startOffset": 29, "endOffset": 53}, {"referenceID": 36, "context": "Incremental A\u2217 methods, like D* (Stentz, 1995), D*Lite (Koenig & Likhachev, 2002), Adaptive A* (Koenig & Likhachev, 2006a), and Tree Adaptive A* (Hern\u00e1ndez, Sun, Koenig, & Meseguer, 2011), are search methods that also allow solving goal-directed navigation problems in unknown environments.", "startOffset": 32, "endOffset": 46}, {"referenceID": 6, "context": "Examples are D LRTA\u2217 (Bulitko, Lu\u0161trek, Schaeffer, Bj\u00f6rnsson, & Sigmundarson, 2008) and kNN-LRTA\u2217 (Bulitko et al., 2010), tree subgoaling (Hern\u00e1ndez & Baier, 2011b), or real-time search via compressed path databases (Botea, 2011).", "startOffset": 98, "endOffset": 120}, {"referenceID": 3, "context": ", 2010), tree subgoaling (Hern\u00e1ndez & Baier, 2011b), or real-time search via compressed path databases (Botea, 2011).", "startOffset": 103, "endOffset": 116}, {"referenceID": 19, "context": "For example, Hoffmann (2005, 2011) analyzed the existence of plateaus in h+, an effective admissible domain-independent planning heuristic, and how this negatively affects the performance of otherwise fast planning algorithms. Cushing, Benton, and Kambhampati (2011) introduced the concept of \u03b5-traps that is related to poor performance of best-first search in problems in which action costs have a high variance.", "startOffset": 13, "endOffset": 267}, {"referenceID": 33, "context": "Indeed, we think it could be easily incorporated into LRTA\u2217(k), LRTALS(k), and P-LRTA* (Rayner et al., 2007).", "startOffset": 87, "endOffset": 108}], "year": 2012, "abstractText": "Heuristics used for solving hard real-time search problems have regions with depressions. Such regions are bounded areas of the search space in which the heuristic function is inaccurate compared to the actual cost to reach a solution. Early real-time search algorithms, like LRTA\u2217, easily become trapped in those regions since the heuristic values of their states may need to be updated multiple times, which results in costly solutions. State-of-the-art real-time search algorithms, like LSS-LRTA\u2217 or LRTA\u2217(k), improve LRTA\u2217\u2019s mechanism to update the heuristic, resulting in improved performance. Those algorithms, however, do not guide search towards avoiding depressed regions. This paper presents depression avoidance, a simple real-time search principle to guide search towards avoiding states that have been marked as part of a heuristic depression. We propose two ways in which depression avoidance can be implemented: mark-and-avoid and move-to-border. We implement these strategies on top of LSS-LRTA\u2217 and RTAA\u2217, producing 4 new real-time heuristic search algorithms: aLSS-LRTA\u2217, daLSS-LRTA\u2217, aRTAA\u2217, and daRTAA\u2217. When the objective is to find a single solution by running the real-time search algorithm once, we show that daLSS-LRTA\u2217 and daRTAA\u2217 outperform their predecessors sometimes by one order of magnitude. Of the four new algorithms, daRTAA\u2217 produces the best solutions given a fixed deadline on the average time allowed per planning episode. We prove all our algorithms have good theoretical properties: in finite search spaces, they find a solution if one exists, and converge to an optimal after a number of trials.", "creator": "TeX"}}}