{"id": "1401.6024", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Matrix factorization with binary components", "abstract": "Motivated by an application in computational biology, we consider low-rank matrix factorization with $\\{0,1\\}$-constraints on one of the factors and optionally convex constraints on the second one. In addition to the non-convexity shared with other matrix factorization schemes, our problem is further complicated by a combinatorial constraint set of size $2^{m \\cdot r}$, where $m$ is the dimension of the data points and $r$ the rank of the factorization. Despite apparent intractability, we provide - in the line of recent work on non-negative matrix factorization by Arora et al. (2012), which is not only useful for computational science but also as an application of the \"friction of the information matrix\" that the graph model presents as an ideal example of a simple vectorization. In the example below, we use the \"friction of the data points\" in a way that can be used in a computational model to describe the matrix's structure, and to show how we can incorporate the geometry of the data points into a graph model to describe the matrix's structure.\n\n\nTo explore the mathematical modeling of the matrices, we present a simple model based on three steps:\n(1) In one of our matrix model classes, we use a function called matrix2(m)-1, called matrices. The matrices are the same for matrix1 and matrix2 and matrix2. Our graph models represent a set of elements that are of particular types and have a set of attributes:\n(2) A matrix class is a set of attributes, for the graph. The matrix classes are all unique: the data points are just \"type,\" \"type,\" \"size,\" \"count,\" \"width,\" and \"height.\" The data points contain all attributes of a given matrix:\n(3) An object type of matrix is the same as the matrix (data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points that are the same as data points", "histories": [["v1", "Thu, 23 Jan 2014 16:02:19 GMT  (368kb)", "http://arxiv.org/abs/1401.6024v1", "appeared in NIPS 2013"]], "COMMENTS": "appeared in NIPS 2013", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["martin slawski", "matthias hein 0001", "pavlo lutsik"], "accepted": true, "id": "1401.6024"}, "pdf": {"name": "1401.6024.pdf", "metadata": {"source": "CRF", "title": "Matrix factorization with Binary Components", "authors": ["Martin Slawski", "Matthias Hein"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 1.\n60 24\nv1 [\nst at\n.M L\n] 2\n3 Ja\nn 20"}, {"heading": "1 Introduction", "text": "Low-rank matrix factorization techniques like the singular value decomposition (SVD) constitute an important tool in data analysis yielding a compact representation of data points as linear combinations of a comparatively small number of \u2019basis elements\u2019 commonly referred to as factors, components or latent variables. Depending on the specific application, the basis elements may be required to fulfill additional properties, e.g. non-negativity [1, 2], smoothness [3] or sparsity [4, 5]. In the present paper, we consider the case in which the basis elements are constrained to be binary, i.e. we aim at factorizing a real-valued data matrix D into a product TA with T \u2208 {0, 1}m\u00d7r and A \u2208 Rr\u00d7n, r \u226a min{m,n}. Such decomposition arises e.g. in blind source separation in wireless communication with binary source signals [6]; in network inference from gene expression data [7, 8], where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures from DNA methylation signatures [9] in which case T represents presence/absence of methylation; or in clustering with overlapping clusters with T as a matrix of cluster assignments [10, 11]. Several other matrix factorizations involving binary matrices have been proposed in the literature. In [12] and [13] matrix factorization for binary input data, but non-binary factors T and A is discussed, whereas a factorization TWA with both T and A binary and real-valued W is proposed in [14], which is more restrictive than the model of the present paper. The model in [14] in turn encompasses binary matrix factorization as proposed in [15], where all of D, T and A are constrained to be binary. It is important to note that this ine of research is fundamentally different from Boolean matrix factorization [16], which is sometimes also referred to as binary matrix factorization. A major drawback of matrix factorization schemes is non-convexity. As a result, there is in general no algorithm that is guaranteed to compute the desired factorization. Algorithms such as block coordinate descent, EM, MCMC, etc. commonly employed in practice lack theoretical guarantees beyond convergence to a local minimum. Substantial progress in this regard has been achieved recently for non-negative matrix factorization (NMF) by Arora et al. [17] and follow-up work in [18], where it is shown that under certain additional conditions, the NMF problem can be solved globally optimal by means of linear programming. Apart from being a non-convex problem, the matrix factorization studied in the present paper is further complicated by the {0, 1}-constraints imposed on the left factor T , which yields a combinatorial optimization problem that appears to be computationally intractable except for tiny dimensions m and r even in case the right factor A were\nalready known. Despite the obvious hardness of the problem, we present as our main contribution an algorithm that provably provides an exact factorization D = TA whenever such factorization exists. Our algorithm has exponential complexity only in the rank r of the factorization, but scales linearly in m and n. In particular, the problem remains tractable even for large values of m as long as r remains small. We extend the algorithm to the approximate case D \u2248 TA and empirically show superior performance relative to heuristic approaches to the problem. Moreover, we establish uniqueness of the exact factorization under the separability condition from the NMF literature [17, 19], or alternatively with high probability for T drawn uniformly at random. As a corollary, we obtain that at least for these two models, the suggested algorithm continues to be fully applicable if additional constraints e.g. non-negativity, are imposed on the right factor A. We demonstrate the practical usefulness of our approach in unmixing DNA methylation signatures of blood samples [9]. Notation. For a matrix M and index sets I, J , MI,J denotes the submatrix corresponding to I and J ; MI,: and M:,J denote the submatrices formed by the rows in I respectively columns in J . We write [M ;M \u2032] and [M,M \u2032] for the row- respectively column-wise concatenation of M and M \u2032. The affine hull generated by the columns of M is denoted by aff(M). The symbols 1/0 denote vectors or matrices of ones/zeroes and I denotes the identity matrix. We use | \u00b7 | for the cardinality of a set. Appendix. The appendix contains all proofs, additional comments and experimental results."}, {"heading": "2 Exact case", "text": "We start by considering the exact case, i.e. we suppose that a factorization having the desired properties exists. We first discuss the geometric ideas underlying our basic approach for recovering such factorization from the data matrix before presenting conditions under which the factorization is unique. It is shown that the question of uniqueness as well as the computational performance of our approach is intimately connected to the Littlewood-Offord problem in combinatorics [20].\n2.1 Problem formulation. Given D \u2208 Rm\u00d7n, we consider the following problem.\nfind T \u2208 {0, 1}m\u00d7r and A \u2208 Rr\u00d7n, A\u22a41r = 1n such that D = TA. (1)\nThe columns {T:,k}rk=1 of T , which are vertices of the hypercube [0, 1] m, are referred to as components. The requirement A\u22a41r = 1n entails that the columns of D are affine instead of linear combinations of the columns of T . This additional constraint is not essential to our approach; it is imposed for reasons of presentation, in order to avoid that the origin is treated differently from the other vertices of [0, 1]m, because otherwise the zero vector could be dropped from T , leaving the factorization unchanged. We further assume w.l.o.g. that r is minimal, i.e. there is no factorization of the form (1) with r\u2032 < r, and in turn that the columns of T are affinely independent, i.e. \u2200\u03bb \u2208 Rr, \u03bb\u22a41r = 0, T\u03bb = 0 implies that \u03bb = 0. Moreover, it is assumed that rank(A) = r. This ensures the existence of a submatrix A:,C of r linearly independent columns and of a corresponding submatrix of D:,C of affinely independent columns, when combined with the affine independence of the columns of T :\n\u2200\u03bb \u2208 Rr, \u03bb\u22a41r = 0 : D:,C\u03bb = 0 \u21d0\u21d2 T (A:,C\u03bb) = 0 =\u21d2 A:,C\u03bb = 0 =\u21d2 \u03bb = 0, (2)\nusing at the second step that 1\u22a4r A:,C\u03bb = 1 \u22a4 r \u03bb = 0 and the affine independence of the {T:,k} r k=1. Note that the assumption rank(A) = r is natural; otherwise, the data would reside in an affine subspace of lower dimension so that D would not contain enough information to reconstruct T .\n2.2 Approach. Property (2) already provides the entry point of our approach. From D = TA, it is obvious that aff(T ) \u2287 aff(D). Since D contains the same number of affinely independent columns as T , it must also hold that aff(D) \u2287 aff(T ), in particular aff(D) \u2287 {T:,k}rk=1. Consequently, (1) can in principle be solved by enumerating all vertices of [0, 1]m contained in aff(D) and selecting a maximal affinely independent subset thereof (see Figure 1). This procedure, however, is exponential in the dimension m, with 2m vertices to be checked for containment in aff(D) by solving a linear system. Remarkably, the following observation along with its proof, which prompts Algorithm 5 below, shows that the number of elements to be checked can be reduced to 2r\u22121 irrespective of m.\nProposition 1. The affine subspace aff(D) contains no more than 2r\u22121 vertices of [0, 1]m. Moreover, Algorithm 5 provides all vertices contained in aff(D).\nAlgorithm 1 FINDVERTICES EXACT 1. Fix p \u2208 aff(D) and compute P = [D:,1 \u2212 p, . . . , D:,n \u2212 p]. 2. Determine r \u2212 1 linearly independent columns C of P , obtaining P:,C and subsequently\nr \u2212 1 linearly independent rows R, obtaining PR,C \u2208 Rr\u22121\u00d7r\u22121. 3. Form Z = P:,C(PR,C)\u22121 \u2208 Rm\u00d7r\u22121 and T\u0302 = Z(B(r\u22121) \u2212 pR1\u22a42r\u22121) + p1 \u22a4 2r\u22121 \u2208\nR m\u00d72r\u22121 , where the columns of B(r\u22121) correspond to the elements of {0, 1}r\u22121.\n4. Set T = \u2205. For u = 1, . . . , 2r\u22121, if T\u0302:,u \u2208 {0, 1}m set T = T \u222a {T\u0302:,u}. 5. Return T = {0, 1}m \u2229 aff(D).\nAlgorithm 2 BINARYFACTORIZATION EXACT 1. Obtain T as output from FINDVERTICES EXACT(D) 2. Select r affinely independent elements of T to be used as columns of T . 3. Obtain A as solution of the linear system [1\u22a4r ;T ]A = [1 \u22a4 n ;D].\n4. Return (T,A) solving problem (1).\nFigure 1: Illustration of the geometry underlying our approach in dimension m = 3. Dots represent data points and the shaded areas their affine hulls aff(D) \u2229 [0, 1]m. Left: aff(D) intersects with r + 1 vertices of [0, 1]m. Right: aff(D) intersects with precisely r vertices.\nComments. In step 2 of Algorithm 5, determining the rank of P and an associated set of linearly independent columns/rows can be done by means of a rank-revealing QR factorization [21, 22]. The crucial step is the third one, which is a compact description of first solving the linear systems PR,C\u03bb = b\u2212pR for all b \u2208 {0, 1}r\u22121 and back-substituting the result to compute candidate vertices P:,C\u03bb+ p stacked into the columns of T\u0302 ; the addition/subtraction of p is merely because we have to deal with an affine instead of a linear subspace, in which p serves as origin. In step 4, the pool of 2r\u22121 \u2019candidates\u2019 is filtered, yielding T = aff(D) \u2229 {0, 1}m. Determining T is the hardest part in solving the matrix factorization problem (1). Given T , the solution can be obtained after few inexpensive standard operations. Note that step 2 in Algorithm 2 is not necessary if one does not aim at finding a minimal factorization, i.e. if it suffices to have D = TA with T \u2208 {0, 1}m\u00d7r \u2032\nbut r\u2032 possibly being larger than r. As detailed in the appendix, the case without sum-to-one constraints on A can be handled similarly, as can be the model in [14] with binary left and right factor and real-valued middle factor. Computational complexity. The dominating cost in Algorithm 5 is computation of the candidate matrix T\u0302 and checking whether its columns are vertices of [0, 1]m. Note that\nT\u0302R,: = ZR,:(B (r\u22121)\u2212pR1 \u22a4 2r\u22121)+pR1 \u22a4 2r\u22121 = Ir\u22121(B (r\u22121)\u2212pR1 \u22a4 2r\u22121)+pR1 \u22a4 2r\u22121 = B (r\u22121), (3)\ni.e. the r \u2212 1 rows of T\u0302 corresponding to R do not need to be taken into account. Forming the matrix T\u0302 would hence require O((m\u2212 r+1)(r\u2212 1)2r\u22121) and the subsequent check for vertices in the fourth step O((m \u2212 r + 1)2r\u22121) operations. All other operations are of lower order provided e.g. (m \u2212 r + 1)2r\u22121 > n. The second most expensive operation is forming the matrix PR,C in step 2 with the help of a QR decomposition requiring O(mn(r \u2212 1)) operations in typical cases [21]. Computing the matrix factorization (1) after the vertices have been identified (steps 2 to 4 in Algorithm 2) has complexity O(mnr + r3 + r2n). Here, the dominating part is the solution of a linear system in r variables and n right hand sides. Altogether, our approach for solving (1) has exponential complexity in r, but only linear complexity in m and n. Later on, we will argue that under additional assumptions on T , the O((m\u2212r+1)2r\u22121) terms can be reduced to O((r\u22121)2r\u22121).\n2.3 Uniqueness. In this section, we study uniqueness of the matrix factorization problem (1) (modulo permutation of columns/rows). First note that in view of the affine independence of the columns of T , the factorization is unique iff T is, which holds iff\naff(D) \u2229 {0, 1}m = aff(T ) \u2229 {0, 1}m = {T:,1, . . . , T:,r}, (4)\ni.e. if the affine subspace generated by {T:,1, . . . , T:,r} contains no other vertices of [0, 1]m than the r given ones (cf. Figure 1). Uniqueness is of great importance in applications, where one aims at\nan interpretation in which the columns of T play the role of underlying data-generating elements. Such an interpretation is not valid if (4) fails to hold, since it is then possible to replace one of the columns of a specific choice of T by another vertex contained in the same affine subspace. Solution of a non-negative variant of our factorization. In the sequel, we argue that property (4) plays an important role from a computational point of view when solving extensions of problem (1) in which further constraints are imposed onA. One particularly important extension is the following.\nfind T \u2208 {0, 1}m\u00d7r and A \u2208 Rr\u00d7n+ , A \u22a4 1r = 1n such that D = TA. (5)\nProblem (5) is a special instance of non-negative matrix factorization. Problem (5) is of particular interest in the present paper, leading to a novel real world application of matrix factorization techniques as presented in Section 4.2 below. It is natural to ask whether Algorithm 2 can be adapted to solve problem (5). A change is obviously required for the second step when selecting r vertices from T , since in (5) the columns D now have to be expressed as convex instead of only affine combinations of columns of T : picking an affinely independent collection from T does not take into account the non-negativity constraint imposed on A. If, however, (4) holds, we have |T | = r and Algorithm 2 must return a solution of (5) provided that there exists one.\nCorollary 1. If problem (1) has a unique solution, i.e. if condition (4) holds and if there exists a solution of (5), then it is returned by Algorithm 2.\nTo appreciate that result, consider the converse case |T | > r. Since the aim is a minimal factorization, one has to find a subset of T of cardinality r such that (5) can be solved. In principle, this can be achieved by solving a linear program for (|T | r ) subsets of T , but this is in general not computationally feasible: the upper bound of Proposition 3 indicates that |T | = 2r\u22121 in the worst case. For the example below, T consists of all 2r\u22121 vertices contained in an r\u22121-dimensional face of [0, 1]m:\nT =   0m\u2212r\u00d7r Ir\u22121 0r\u22121\n0\u22a4r\n  with T = { T\u03bb : \u03bb1 \u2208 {0, 1}, . . . , \u03bbr\u22121 \u2208 {0, 1}, \u03bbr = 1\u2212 r\u22121\u2211\nk=1\n\u03bbk\n} . (6)\nUniqueness under separability. In view of the negative example (6), one might ask whether uniqueness according to (4) can be achieved under additional conditions on T . We prove uniqueness under separability, a condition introduced in [19] and imposed recently in [17] to show solvability of the NMF problem by linear programming. We say that T is separable if there exists a permutation \u03a0 such that \u03a0T = [M ; Ir], where M \u2208 {0, 1}m\u2212r\u00d7r.\nProposition 2. If T is separable, condition (4) holds and thus problem (1) has a unique solution.\nUniqueness under generic random sampling. Both the negative example (6) as well as the positive result of Proposition 2 are associated with special matrices T . This raises the question whether uniqueness holds respectively fails for broader classes of binary matrices. In order to gain insight into this question, we consider random T with i.i.d. entries from a Bernoulli distribution with parameter 12 and study the probability of the event {aff(T ) \u2229 {0, 1}\nm = {T:,1, . . . , T:,r}}. This question has essentially been studied in combinatorics [23], with further improvements in [24]. The results therein rely crucially on Littlewood-Offord theory (see Section 2.4 below).\nTheorem 1. Let T be a random m \u00d7 r-matrix whose entries are drawn i.i.d. from {0, 1} with probability 12 . Then, there is a constant C so that if r \u2264 m\u2212 C,\nP ( aff(T )\u2229{0, 1}m = {T:,1, . . . , T:,r ) \u2265 1\u2212(1+o(1)) 4\n( r\n3\n)( 3\n4\n)m \u2212 ( 3\n4 + o(1)\n)m as m \u2192 \u221e.\nTheorem 3 suggests a positive answer to the question of uniqueness posed above. For m large enough and r small compared to m (in fact, following [24] one may conjecture that Theorem 3 holds with C = 1), the probability that the affine hull of r vertices of [0, 1]m selected uniformly at random contains some other vertex is exponentially small in the dimension m. We have empirical evidence that the result of Theorem 3 continues to hold if the entries of T are drawn from a Bernoulli distribution with parameter in (0, 1) sufficiently far away from the boundary points (cf. appendix). As a byproduct, these results imply that also the NMF variant of our matrix factorization problem (5) can in most cases be reduced to identifying a set of r vertices of [0, 1]m (cf. Corollary 1).\n2.4 Speeding up Algorithm 5. In Algorithm 5, an m \u00d7 2r\u22121 matrix T\u0302 of potential vertices is formed (Step 3). We have discussed the case (6) where all candidates must indeed be vertices, in which case it seems to be impossible to reduce the computational cost of O((m \u2212 r)r2r\u22121), which becomes significant once m is in the thousands and r \u2265 25. On the positive side, Theorem 3 indicates that for many instances of T , only r out of 2r\u22121 candidates are in fact vertices. In that case, noting that columns of T\u0302 cannot be vertices if a single coordinate is not in {0, 1} (and that the vast majority of columns of T\u0302 must have one such coordinate), it is computationally more favourable to incrementally compute subsets of rows of T\u0302 and then to discard already those columns with coordinates not in {0, 1}. We have observed empirically that this scheme rapidly reduces the candidate set \u2212 already checking a single row of T\u0302 eliminates a substantial portion (see Figure 2). Littlewood-Offord theory. Theoretical underpinning for the last observation can be obtained from a result in combinatorics, the Littlewood-Offord (L-O)-lemma. Various extensions of that result have been developed until recently, see the survey [25]. We here cite the L-O-lemma in its basic form. Theorem 2. [20] Let a1, . . . , a\u2113 \u2208 R \\ {0} and y \u2208 R.\n(i) \u2223\u2223{b \u2208 {0, 1}\u2113 : \u2211\u2113 i=1 aibi = y} \u2223\u2223 \u2264 ( \u2113 \u230a\u2113/2\u230b ) .\n(ii) If |ai| \u2265 1, i = 1, . . . , \u2113, \u2223\u2223{b \u2208 {0, 1}\u2113 : \u2211\u2113 i=1 aibi \u2208 (y, y + 1)} \u2223\u2223 \u2264 ( \u2113 \u230a\u2113/2\u230b ) .\nThe two parts of Theorem 2 are referred to as discrete respectively continuous L-O lemma. The discrete L-O lemma provides an upper bound on the number of {0, 1}-vectors whose weighted sum with given weights {ai}\u2113i=1 is equal to some given number y, whereas the stronger continuous version, under a more stringent condition on the weights, upper bounds the number of {0, 1}-vectors whose weighted sum is contained in some interval (y, y+1). In order to see the relation of Theorem 2 to Algorithm 5, let us re-inspect the third step of that algorithm. To obtain a reduction of candidates by checking a single row of T\u0302 = Z(B(r\u22121)\u2212pR1\u22a42r\u22121)+p1 \u22a4 2r\u22121 , pick i /\u2208 R (recall that coordinates in R do not need to be checked, cf. (3)) and u \u2208 {1, . . . , 2r\u22121} arbitrary. The u-th candidate can be a vertex only if T\u0302i,u \u2208 {0, 1}. The condition T\u0302i,u = 0 can be written as\nZi,:\ufe38\ufe37\ufe37\ufe38 {ak}rk=1 B(r\u22121):,u\ufe38 \ufe37\ufe37 \ufe38 =b = Zi,:pR \u2212 pi\ufe38 \ufe37\ufe37 \ufe38 =y . (7)\nA similar reasoning applies when setting T\u0302i,u = 1. Provided none of the entries of Zi,: = 0, the discrete L-O lemma implies that there are at most 2 ( r\u22121 \u230a(r\u22121)/2\u230b ) out of 2r\u22121 candidates for which the i-th coordinate is in {0, 1}. This yields a reduction of the candidate set by 2 (\nr\u22121 \u230a(r\u22121)/2\u230b\n) /2r\u22121 =\nO (\n1\u221a r\u22121\n) . Admittedly, this reduction may appear insignificant given the total number of candi-\ndates to be checked. The reduction achieved empirically (cf. Figure 2) is typically larger. Stronger reductions have been proven under additional assumptions on the weights {ai}\u2113i=1: e.g. for distinct weights, one obtains a reduction of O((r\u2212 1)\u22123/2) [25]. Furthermore, when picking successively d rows of T\u0302 and if one assumes that each row yields a reduction according to the discrete L-O lemma, one would obtain the reduction (r \u2212 1)\u2212d/2 so that d = r \u2212 1 would suffice to identify all vertices provided r \u2265 4. Evidence for the rate (r \u2212 1)\u2212d/2 can be found in [26]. This indicates a reduction in complexity of Algorithm 5 from O((m \u2212 r)r2r\u22121) to O(r22r\u22121). Achieving further speed-up with integer linear programming. The continuous L-O lemma (part (ii) of Theorem 2) combined with the derivation leading to (7) allows us to tackle even the case r = 80 (280 \u2248 1024). In view of the continuous L-O lemma, a reduction in the number of candidates can still be achieved if the requirement is weakened to T\u0302i,u \u2208 [0, 1]. According to (7) the candidates satisfying the relaxed constraint for the i-th coordinate can be obtained from the feasibility problem\nfind b \u2208 {0, 1}r\u22121 subject to 0 \u2264 Zi,:(b \u2212 pR) + pi \u2264 1, (8)\nwhich is an integer linear program that can be solved e.g. by CPLEX. The L-O- theory suggests that the branch-bound strategy employed therein is likely to be successful. With the help of CPLEX, it is affordable to solve problem (8) with all m \u2212 r + 1 constraints (one for each of the rows of T\u0302 to be checked) imposed simultaneously. We always recovered directly the underlying vertices in our experiments and only these, without the need to prune the solution pool (which could be achieved by Algorithm 1, replacing the 2r\u22121 candidates by a potentially much smaller solution pool)."}, {"heading": "3 Approximate case", "text": "In the sequel, we discuss an extension of our approach to handle the approximate case D \u2248 TA with T and A as in (1). In particular, we have in mind the case of additive noise i.e. D = TA+ E with \u2016E\u2016F small. While the basic concept of Algorithm 5 can be adopted, changes are necessary because D may have full rank min{m,n} and second aff(D) \u2229 {0, 1}m = \u2205, i.e. the distances of aff(D) and the {T:,k}rk=1 may be strictly positive (but are at least assumed to be small). As dis-\nAlgorithm 3 FINDVERTICES APPROXIMATE 1. Let p = D1n/n and compute P = [D:,1 \u2212 p, . . . , D:,n \u2212 p]. 2. Compute U (r\u22121) \u2208 Rm\u00d7r\u22121, the left singular vectors corresponding to the r \u2212 1 largest\nsingular values of P . Select r \u2212 1 linearly independent rows R of U (r\u22121), obtaining U\n(r\u22121) R,: \u2208 R r\u22121\u00d7r\u22121.\n3. Form Z = U (r\u22121)(U (r\u22121)R,: ) \u22121 and T\u0302 = Z(B(r\u22121) \u2212 pR1\u22a42r\u22121) + p1 \u22a4 2r\u22121 . 4. Compute T\u0302 01 \u2208 Rm\u00d72 r\u22121\n: for u = 1, . . . , 2r\u22121, i = 1, . . . ,m, set T\u0302 01i,u = I(T\u0302i,u > 1 2 ).\n5. For u = 1, . . . , 2r\u22121, set \u03b4u = \u2016T\u0302:,u\u2212 T\u0302 01:,u\u20162. Order increasingly s.t. \u03b4u1 \u2264 . . . \u2264 \u03b42r\u22121 . 6. Return T = [T\u0302 01:,u1 . . . T\u0302 01 :,ur ]\ntinguished from the exact case, Algorithm B.6 requires the number of components r to be specified in advance as it is typically the case in noisy matrix factorization problems. Moreover, the vector p subtracted from all columns of D in step 1 is chosen as the mean of the data points, which is in particular a reasonable choice if D is contaminated with additive noise distributed symmetrically around zero. The truncated SVD of step 2 achieves the desired dimension reduction and potentially reduces noise corresponding to small singular values that are discarded. The last change arises in step 5. While in the exact case, one identifies all columns of T\u0302 that are in {0, 1}m, one instead only identifies columns close to {0, 1}m. Given the output of Algorithm B.6, we solve the approximate matrix factorization problem via least squares, obtaining the right factor from minA\u2016D \u2212 TA\u20162F . Refinements. Improved performance for higher noise levels can be achieved by running Algorithm B.6 multiple times with different sets of rows selected in step 2, which yields candidate matrices {T (l)}sl=1, and subsequently using T = argmin{T (l)} minA\u2016D\u2212T (l)A\u20162F , i.e. one picks the candidate yielding the best fit. Alternatively, we may form a candidate pool by merging the {T (l)}sl=1 and then use a backward elimination scheme, in which successively candidates are dropped that yield the smallest improvement in fitting D until r candidates are left. Apart from that, T returned by Algorithm B.6 can be used for initializing the block optimization scheme of Algorithm B.7 below. Algorithm B.7 is akin to standard block coordinate descent schemes proposed in the matrix factorization literature, e.g. [27]. An important observation (step 3) is that optimization of T is separable along the rows of T , so that for small r, it is feasible to perform exhaustive search over all 2r possibilities (or to use CPLEX). However, Algorithm B.7 is impractical as a stand-alone scheme, because without proper initialization, it may take many iterations to converge, with each single iteration being more expensive than Algorithm B.6. When initialized with the output of the latter, however, we have observed convergence of the block scheme only after few steps.\nAlgorithm 4 Block optimization scheme for solving minT\u2208{0,1}m\u00d7r , A \u2016D \u2212 TA\u20162F 1. Set k = 0 and set T (k) equal to a starting value. 2. A(k) \u2190 argminA\u2016D \u2212 T\n(k)A\u20162F and set k = k + 1. 3. T (k) \u2190 argminT\u2208{0,1}m\u00d7r\u2016D\u2212TA (k)\u20162F = argmin{Ti,:\u2208{0,1}r}mi=1 \u2211m i=1\u2016Di,:\u2212Ti,:A (k)\u201622 (9) 4. Alternate between steps 2 and 3."}, {"heading": "4 Experiments", "text": "In Section 4.1 we demonstrate with the help of synthetic data that the approach of Section 3 performs well on noisy datasets. In the second part, we present an application to a real dataset."}, {"heading": "4.1 Synthetic data.", "text": "Setup. We generate D = T \u2217A\u2217 + \u03b1E, where the entries of T \u2217 are drawn i.i.d. from {0, 1} with probability 0.5, the columns of A are drawn i.i.d. uniformly from the probability simplex and the entries of E are i.i.d. standard Gaussian. We let m = 1000, r = 10 and n = 2r and let the noise level \u03b1 vary along a grid starting from 0. Small sample sizes n as considered here yield more challenging problems and are motivated by the real world application of the next subsection. Evaluation. Each setup is run 20 times and we report averages over the following performance measures: the normalized Hamming distance \u2016T \u2217 \u2212 T \u20162F/(mr) and the two RMSEs \u2016T \u2217A\u2217 \u2212 TA\u2016F/(mn)1/2 and \u2016TA \u2212 D\u2016F/(mn)1/2, where (T,A) denotes the output of one of the following approaches that are compared. FindVertices: our approach in Section 3. oracle: we solve problem (9) with A(k) = A\u2217. box: we run the block scheme of Algorithm B.7, relaxing the integer constraint into a box constraint. Five random initializations are used and we take the result yielding the best fit, subsequently rounding the entries of T to fulfill the {0, 1}-constraints and refitting A. quad pen: as box, but a (concave) quadratic penalty \u03bb \u2211 i,k Ti,k(1\u2212Ti,k) is added to push the entries of T towards {0, 1}. D.C. programming [28] is used for the block updates of T .\nComparison to HOTTOPIXX [18]. HOTTOPIXX (HT) is a linear programming approach to NMF equipped with guarantees such as correctness in the exact and robustness in the non-exact case as long as T is (nearly) separable (cf. Section 2.3). HT does not require T to be binary, but applies to the generic NMF problem D \u2248 TA, T \u2208 Rm\u00d7r+ and A \u2208 R r\u00d7n + . Since separability is crucial to the performance of HT, we restrict our comparison to separable T = [M ; Ir], generating the entries of M i.i.d. from a Bernoulli distribution with parameter 0.5. For runtime reasons, we lower the dimension to m = 100. Apart from that, the experimental setup is as above. We\nuse an implementation of HT from [29]. We first pre-normalize D to have unit row sums as required by HT, and obtain A as first output. Given A, the non-negative least squares problem minT\u2208Rm\u00d7r+ \u2016D \u2212 TA\u20162F is solved. The entries of T are then re-scaled to match the original scale of D, and thresholding at 0.5 is applied to obtain a binary matrix. Finally, A is re-optimized by solving the above fitting problem with respect to A in place of T . In the noisy case, HT needs a tuning parameter to be specified that depends on the noise level, and we consider a grid of 12 values for that parameter. The range of the grid is chosen based on knowledge of the noise matrix E. For each run, we pick the parameter that yields best performance in favour of HT. Results. From Figure H.1, we find that unlike the other approaches, box does not always recover T \u2217 even if the noise level \u03b1 = 0. FindVertices outperforms box and quad pen throughout. For \u03b1 \u2264 0.06, its performance closely matches that of the oracle. In the separable case, our approach performs favourably as compared to HT, a natural benchmark in this setting."}, {"heading": "4.2 Analysis of DNA methylation data.", "text": "Background. Unmixing of DNA methylation profiles is a problem of high interest in cancer research. DNA methylation is a chemical modification of the DNA occurring at specific sites, so-called CpGs. DNA methylation affects gene expression and in turn various processes such as cellular differentiation. A site is either unmethylated (\u20190\u2019) or methylated (\u20191\u2019). DNA methylation microarrays allow one to measure the methylation level for thousands of sites. In the dataset considered here, the measurements D (the rows corresponding to sites, the columns to samples) result from a mixture of cell types. The methylation profiles of the latter are in {0, 1}m, whereas, depending on the mixture proportions associated with each sample, the entries of D take values in [0, 1]m. In other words, we have the model D \u2248 TA, with T representing the methylation of the cell types and the columns of A being elements of the probability simplex. It is often of interest to recover the mixture proportions of the samples, because e.g. specific diseases, in particular cancer, can be associated with shifts in these proportions. The matrix T is frequently unknown, and determining it experimentally is costly. Without T , however, recovering the mixing matrix A is challenging, in particular since the number of samples in typical studies is small. Dataset. We consider the dataset studied in [9], with m = 500 CpG sites and n = 12 samples of blood cells composed of four major types (B-/T-cells, granulocytes, monocytes), i.e. r = 4. Ground truth is partially available: the proportions of the samples, denoted by A\u2217, are known.\nAnalysis. We apply our approach to obtain an approximate factorization D \u2248 T A, T \u2208 {0, 1}m\u00d7r, A \u2208 Rr\u00d7n+ and A \u22a4 1r = 1n. We first obtained T as outlined in Section 3, replacing {0, 1} by {0.1, 0.9} in order to account for measurement noise in D that slightly pushes values towards 0.5. This can be accomodated re-scaling T\u0302 01 in step 4 of Algorithm B.6 by 0.8 and then adding 0.1. Given T , we solve the quadratic program A = argminA\u2208Rr\u00d7n+ ,A\u22a41r=1n\u2016D \u2212 TA\u2016 2 F and compare A to the ground truth A\u2217. In order to judge the fit as well as the matrix T returned by our method, we compute T \u2217 = argminT\u2208{0,1}m\u00d7r\u2016D \u2212 TA \u2217\u20162F as in (9). We obtain 0.025 as average mean squared difference of T and T \u2217, which corresponds to an agreement of 96 percent. Figure 4 indicates at least a qualitative agreement of A\u2217 and A. In the rightmost plot, we compare the RMSEs of our approach for different choices of r relative to the RMSE of (T \u2217, A\u2217). The error curve flattens after r = 4, which suggests that with our approach, we can recover the correct number of cell types."}, {"heading": "A Proof of Proposition 1", "text": "Proposition 1 is about Algorithm 5, which we re-state here.\nAlgorithm 5 FINDVERTICES EXACT 1. Fix p \u2208 aff(D) and compute P = [D:,1 \u2212 p, . . . , D:,n \u2212 p]. 2. Determine r \u2212 1 linearly independent columns C of P , obtaining P:,C and subsequently\nr \u2212 1 linearly independent rows R, obtaining PR,C \u2208 Rr\u22121\u00d7r\u22121. 3. Form Z = P:,C(PR,C)\u22121 \u2208 Rm\u00d7r\u22121 and T\u0302 = Z(B(r\u22121) \u2212 pR1\u22a42r\u22121) + p1 \u22a4 2r\u22121 \u2208\nR m\u00d72r\u22121 , where the columns of B(r\u22121) correspond to the elements of {0, 1}r\u22121.\n4. Set T = \u2205. For u = 1, . . . , 2r\u22121, if T\u0302:,u \u2208 {0, 1}m set T = T \u222a {T\u0302:,u}. 5. Return T = {0, 1}m \u2229 aff(D).\nProposition 3. The affine subspace aff(D) contains no more than 2r\u22121 vertices of [0, 1]m. Moreover, Algorithm 5 provides all vertices contained in aff(D).\nProof. Consider the first part of the statement. Let b \u2208 {0, 1}m and p \u2208 aff(D) arbitrary. We have b \u2208 aff(D) iff there exists \u03b8 \u2208 Rn s.t.\nD\u03b8 = b, \u03b8\u22a41n = 1 \u21d0\u21d2 [D:,1 \u2212 p, . . . , D:,n \u2212 p]\ufe38 \ufe37\ufe37 \ufe38 =P \u03b8 + p = b \u21d0\u21d2 P\u03b8 = b\u2212 p. (9)\nNote that rank(P ) = r \u2212 1. Hence, if there exists \u03b8 s.t. P\u03b8 = b \u2212 p, such \u03b8 can be obtained from the unique \u03bb \u2208 Rr\u22121 solving PR,C\u03bb = bR \u2212 pR, where R \u2282 {1, . . . ,m} and C \u2282 {1, . . . , n} are subsets of rows respectively columns of P s.t. rank(PR,C) = r\u22121. Finally note that bR \u2208 {0, 1}r\u22121 so that there are no more than 2r\u22121 distinct right hand sides bR \u2212 pR. Turning to the second part of the statement, observe that for each b \u2208 {0, 1}m, there exists a unique \u03bb s.t. PR,C\u03bb = bR \u2212 pR \u21d4 \u03bb = (PR,C)\u22121(bR \u2212 pR). Repeating the argument preceding (9), if b \u2208 {0, 1}m \u2229 aff(D), it must hold that\nb = P:,C\u03bb+ p \u21d0\u21d2 b = P:,C(PR,C) \u22121\n\ufe38 \ufe37\ufe37 \ufe38 =Z\n(bR \u2212 pR) + p \u21d0\u21d2 b = Z(bR \u2212 pR) + p. (10)\nAlgorithm 5 generates all possible right hand sides T\u0302 = Z(B(r\u22121) \u2212 pR1\u22a42r\u22121) + p1 \u22a4 2r\u22121 , where B(r\u22121) contains all elements of {0, 1}r\u22121 as its columns. Consequently if b \u2208 {0, 1}m \u2229 aff(D), it must appear as a column of T\u0302 . Conversely, if the leftmost equality in (10) does not hold, b /\u2208 aff(D) and the column of T\u0302 corresponding to bR cannot be a binary vector.\nB The matrix factorization problem without the constraint A\u22a41 r = 1 n\nIn the paper, we have provided Algorithm 2 to solve the matrix factorization problem\nfind T \u2208 {0, 1}m\u00d7r and A \u2208 Rr\u00d7n, A\u22a41r = 1n such that D = TA. (11)\nWe here provide variants of Algorithms 1 and 2 to solve the corresponding problem without the constraint A\u22a41r = 1n, that is\nfind T \u2208 {0, 1}m\u00d7r and A \u2208 Rr\u00d7n such that D = TA. (12)\nThe following Algorithm B.6 is the analog of Algorithm 1. Algorithm B.6 yields span(D)\u2229{0, 1}m, which can be proved along the lines of the proof of Proposition 1 under the stronger assumption that T has r linearly independent in place of only r affinely independent columns, which together with the assumption rank(A) = r implies that also rank(D) = r (cf. Section 2.1 of the paper). Algorithm B.6 results from Algorithm 1 by setting p = 0 and replacing r \u2212 1 by r.\nThe following Algorithm B.7 solves problem (12) given the output of Algorithm B.6. For the sake of completness, we provide Algorithm B.8 as a counterpart to Algorithm 3 regarding the approximate case. An additional modification is necessary to eliminate the zero vector, which is always contained in span(D) and hence would be returned as a column of T if we used B(r) in place of B(r)\\0 in step 2. below, whose columns correspond to the elements of {0, 1} r \\ {0r}.\nAlgorithm B.6 FINDVERTICES EXACT LINEAR 1. Determine r linearly independent columns C of D, obtaining D:,C and subsequently r\nlinearly independent rows R, obtaining DR,C \u2208 Rr\u00d7r. 2. Form Z = D:,C(DR,C)\u22121 \u2208 Rm\u00d7r and T\u0302 = ZB(r) \u2208 Rm\u00d72 r\n, where the columns of B(r) correspond to the elements of {0, 1}r\n3. Set T = \u2205. For u = 1, . . . , 2r, if T\u0302:,u \u2208 {0, 1}m set T = T \u222a {T\u0302:,u}. 4. Return T = {0, 1}m \u2229 span(D).\nAlgorithm B.7 BINARYFACTORIZATION EXACT LINEAR 1. Obtain T as output from FINDVERTICES EXACT LINEAR(D) 2. Select r linearly independent elements of T to be used as columns of T . 3. Obtain A as solution of the linear system TA = D. 4. Return (T,A) solving problem (12)."}, {"heading": "C Matrix factorization with left and right binary factor and real-valued middle factor", "text": "We here sketch how our approach can be applied to obtain a matrix factorization considered in [14], which is of the form TWA\u22a4 with both T and A binary and W real-valued in the exact case; the noisy case be tackled similarly with the help of Algorithm B.8 and is thus omitted. Consider the matrix factorization problem\nfind T \u2208 {0, 1}m\u00d7r, A \u2208 {0, 1}n\u00d7r and W \u2208 Rr\u00d7r such that D = TWA\u22a4, (13)\nand suppose that rank(D) = r. Then the following Algorithm C.9 solves problem (13)."}, {"heading": "D Proof of Corollary 1", "text": "Corollary 1 follows directly from Proposition 1."}, {"heading": "E Proof of Proposition 2", "text": "Before re-stating Proposition 2 below, let us recall problem (1) and property (4) of the paper.\nfind T \u2208 {0, 1}m\u00d7r and A \u2208 Rr\u00d7n, A\u22a41r = 1n such that D = TA. (1)\naff(D) \u2229 {0, 1}m = aff(T ) \u2229 {0, 1}m = {T:,1, . . . , T:,r} (4)\nLet us also recall that T is said to be separable if there exists a permutation \u03a0 such that \u03a0T = [M ; Ir], where M \u2208 {0, 1}m\u2212r\u00d7r.\nProposition 2. If T is separable, condition (4) holds and thus problem (1) has a unique solution.\nAlgorithm B.8 FINDVERTICES APPROXIMATE LINEAR\n1. Compute U (r) \u2208 Rm\u00d7r, the left singular vectors corresponding to the r largest singular values of D. Select r linearly independent rows R of U (r), obtaining U (r)R,: \u2208 R r\u00d7r. 2. Form Z = U (r)(U (r)R,:) \u22121 and T\u0302 = ZB(r)\\0 . 4. Compute T\u0302 01 \u2208 Rm\u00d72 r\n: for u = 1, . . . , 2r, i = 1, . . . ,m, set T\u0302 01i,u = I(T\u0302i,u > 1 2 ).\n5. For u = 1, . . . , 2r, set \u03b4u = \u2016T\u0302:,u \u2212 T\u0302 01:,u\u20162. Order increasingly s.t. \u03b4u1 \u2264 . . . \u2264 \u03b42r . 6. Return T = [T\u0302 01:,u1 . . . T\u0302 01 :,ur ]\nAlgorithm C.9 THREEWAYBINARYFACTORIZATION 1. Obtain T as output from FINDVERTICES EXACT LINEAR(D) 2. Obtain A as output from FINDVERTICES EXACT LINEAR(D\u22a4) 3. Select r linearly independent elements of T and A to be used as columns of T respec-\ntively A. 4. Obtain W = (T\u22a4T )\u22121T\u22a4DA(A\u22a4A)\u22121. 5. Return (T,A,W ) solving problem (13).\nProof. We have aff(T ) \u220b b \u2208 {0, 1}m iff there exists \u03bb \u2208 Rr, \u03bb\u22a41r = 1 such that\nT\u03bb = b \u21d0\u21d2 \u03a0T\u03bb = \u03a0b \u21d0\u21d2 [M ; Ir]\u03bb = \u03a0b.\nSince \u03a0b \u2208 {0, 1}m, for the bottom r block of the linear system to be fulfilled, it is necessary that \u03bb \u2208 {0, 1}r. The condition \u03bb\u22a41r = 1 then implies that \u03bb must be one of the r canonical basis vectors of Rr. We conclude that aff(T ) \u2229 {0, 1}m = {T:,1, . . . , T:,r}."}, {"heading": "F Proof of Theorem 1", "text": "Our proof of Theorem 1 relies on two seminal results on random \u00b11-matrices.\nTheorem F.1. [24] Let M be a random m\u00d7 r-matrix whose entries are drawn i.i.d. from {\u22121, 1} each with probability 12 . There is a constant C so that if r \u2264 m\u2212 C,\nP (span(M) \u2229 {\u22121, 1}m = {\u00b1M:,1, . . . ,\u00b1M:,r}) \u2265 1\u2212 (1 + o(1)) 4\n( r\n3\n)( 3\n4\n)m as m \u2192 \u221e.\n(14)\nTheorem F.2. [30] Let M be a random m\u00d7 r-matrix, r \u2264 m, whose entries are drawn i.i.d. from {\u22121, 1} each with probability 12 . Then\nP ( M has linearly independent columns ) \u2265 1\u2212\n( 3\n4 + o(1)\n)m as m \u2192 \u221e. (15)\nWe are now in position to re-state and prove Theorem 1.\nTheorem 3. Let T be a random m\u00d7 r-matrix whose entries are drawn i.i.d. from {0, 1} each with probability 12 . Then, there is a constant C so that if r \u2264 m\u2212 C,\nP ( aff(T )\u2229{0, 1}m = {T:,1, . . . , T:,r ) \u2265 1\u2212(1+o(1)) 4\n( r\n3\n)( 3\n4\n)m \u2212 ( 3\n4 + o(1)\n)m as m \u2192 \u221e.\nProof. Note that T = 12 (M + 1m\u00d7r), where M is a random \u00b11-matrix as in Theorem F.1. Let \u03bb \u2208 Rr, \u03bb\u22a41r = 1 and b \u2208 {0, 1}m. Then\nT\u03bb = b \u21d0\u21d2 1\n2 (M\u03bb+ 1m) = b \u21d0\u21d2 M\u03bb = 2b\u2212 1m \u2208 {\u22121, 1}\nm. (16)\nNow note that with the probability given in (14),\nspan(M) \u2229 {\u22121, 1}m = {\u00b1M:,1, . . . ,\u00b1M:,r} =\u21d2 aff(M) \u2229 {\u22121, 1} m \u2286 {\u00b1M:,1, . . . ,\u00b1M:,r}\nOn the other hand, with the probability given in (15), the columns of M are linearly independent. If this is the case,\naff(M) \u2229 {\u22121, 1}m \u2286 {\u00b1M:,1, . . . ,\u00b1M:,r}\n=\u21d2 aff(M) \u2229 {\u22121, 1}m = {M:,1, . . . ,M:,r}. (17)\nTo verify this, first note the obvious inclusion aff(M) \u2229 {\u22121, 1}m \u2287 {M:,1, . . . ,M:,r}. Moreover, suppose by contradiction that there exists j \u2208 {1, . . . , r} and \u03b8 \u2208 Rr, \u03b8\u22a41r = 1 such that M\u03b8 = \u2212M:,j . Writing ej for the j-th canonical basis vector, this would imply M(\u03b8 + ej) = 0 and in turn by linear independence \u03b8 = \u2212ej , which contradicts \u03b8\u22a41r = 1. Under the event (17), M\u03bb = 2b\u2212 1m is fulfilled iff \u03bb is equal to one of the canonical basis vectors and 2b\u22121m equals the corresponding column of M . We conclude the assertion in view of (16)."}, {"heading": "G Theorem 1: empirical evidence", "text": "It is natural to ask whether a result similar to Theorem 3 holds if the entries of T are drawn from a Bernoulli distribution with parameter p in (0, 1) sufficiently far away from the boundary points. We have conducted an experiment whose outcome suggests that the answer is positive. For this experiment, we consider the grid {0.01, 0.02, . . . , 0.99} for p and generate random binary matrices T \u2208 Rm\u00d7r with m = 500 and r \u2208 {8, 16, 24} whose entries are i.i.d. Bernoulli with parameter p. For each value of p and r, 100 trials are considered, and for each of these trials, we compute the number of vertices of [0, 1]m contained in aff(T ). In Figure G.5, we report the maximum number of vertices over these trials. One observes that except for a small set of values of p very close to 0 or 1, exactly r vertices are returned in all trials. On the other hand, for extreme values of p the number of vertices can be as large as 220 in the worst case.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n5\n10\n15\n20\n25\nProbability p\nN um\nbe r\nof r\net ur\nne d\nve rt\nic es\n( lo\ng 2)\nMaximum number of returned vertices over 100 trials\nr=8 r=16 r=24\nFigure G.5: Number of vertices contained in aff(T ) over 100 trials for T drawn entry-wise from a Bernoulli distribution with parameter p."}, {"heading": "H Entire set of experiments with synthetic data", "text": "In section 4.1 of the paper, we have presented only a subset of all synthetic data experiments that we have performed. We here present the entire set. For the first set of experiments, we have considered three different setups concerning the generation of T and A and two choices of r (10 and 20), out of which only the results of the first one (\u2019T0.5\u2019) for r = 10 are reported in the paper. Setups. \u2019T0.5\u2019: We generate D = T \u2217A\u2217 + \u03b1E, where the entries of T \u2217 are drawn i.i.d. from {0, 1} with probability 0.5, the columns of A are drawn i.i.d. uniformly from the probability simplex and the entries of E are i.i.d. standard Gaussian. We let m = 1000, r \u2208 {10, 20}, n = 2r, and let the noise level \u03b1 vary along a grid starting from 0. \u2019Tsparse+dense\u2019: The matrix T is now generated by drawing the entries of one half of the columns of T i.i.d. from a Bernoulli distribution with probability 0.1 (\u2019sparse\u2019 part), and the second half from a Bernoulli distribution with parameter 0.9 (\u2019dense\u2019 part). The rest is as for the first setup. \u2019T0.5,Adense\u2019: As for \u2019T0.5\u2019 apart from the following modification: after random generation of A as above, we compute its Euclidean projection on {A \u2208 Rr\u00d7n+ : A \u22a4 1r = 1n, maxk,i Ak,i \u2264 2/r}, thereby constraining the columns of A to be roughly constant. With such A, all data points are situated near the barycentre T1r/r of the simplex generated by the columns of T . Given that the goal is to recover vertices, this setup is hence potentially more difficult.\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),T0.5,r=10\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n)\nRMSE(TA, T*A*),T0.5,r=10\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 D\n| F /s\nq rt\n(m *n\n)\nRMSE(TA, D),T0.5,r=10\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),T0.5,r=20\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.02\n0.04\n0.06\n0.08\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n)\nRMSE(TA, T*A*),T0.5,r=20\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\nalpha\n|T A\n\u2212 D\n| F /s\nq rt\n(m *n\n)\nRMSE(TA, D),T0.5,r=20\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.1\n0.2\n0.3\n0.4\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),Tsparse+dense,r=10\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n)\nRMSE(TA, T*A*),Tsparse+dense,r=10\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha |T\nA \u2212\nD | F\n/s q rt\n(m *n\n)\nRMSE(TA, D),Tsparse+dense,r=10\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.1\n0.2\n0.3\n0.4\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),Tsparse+dense,r=20\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.02\n0.04\n0.06\n0.08\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n)\nRMSE(TA, T*A*),Tsparse+dense,r=20\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.02\n0.04\n0.06\n0.08\nalpha\n|T A\n\u2212 D\n| F /s\nq rt\n(m *n\n)\nRMSE(TA, D),Tsparse+dense,r=20\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),T0.5,Adense,r=10\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n)\nRMSE(TA, T*A*),T0.5,Adense,r=10\nbox quad pen oracle FindVertices\n0 0.05 0.1 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 D\n| F /s\nq rt\n(m *n\n)\nRMSE(TA, D),T0.5,Adense,r=10\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.1\n0.2\n0.3\n0.4\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),T0.5,Adense,r=20\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n)\nRMSE(TA, T*A*),T0.5,Adense,r=20\nbox quad pen oracle FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\nalpha\n|T A\n\u2212 D\n| F /s\nq rt\n(m *n\n)\nRMSE(TA, D),T0.5,Adense,r=20\nbox quad pen oracle FindVertices\nFigure H.1: Results of the synthetic data experiments separated according to the setups \u2019T.05\u2019, \u2019Tsparse+dense\u2019 and \u2019T0.5,Adense\u2019. Bottom/top: r = 10, r = 20. Left/Middle/Right: \u2016T \u2217 \u2212 T \u20162F/(mr), \u2016T \u2217A\u2217 \u2212 TA\u2016F/(mn)1/2 and \u2016TA\u2212D\u2016F /(mn)1/2.\nRegarding the comparison against HOTTOPIXX, only the results for r = 10 are reported in the paper. We here display the results for r = 20 as well.\n0 0.02 0.04 0.06 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),r=10\nHotTopixx FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n)\nRMSE(TA, T*A*),r=10\nHotTopixx FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 D\n| F /s\nq rt\n(m *n\n)\nRMSE(TA, D),r=10\nHotTopixx FindVertices\n0 0.02 0.04 0.06 0\n0.1\n0.2\n0.3\n0.4\nalpha\n|T \u2212\nT *|\nF2 /(\nm *r\n)\nHamming(T,T*),r=20\nHotTopixx FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 T\n*A *|\nF /s\nq rt\n(m *n\n) RMSE(TA, T*A*),r=20\nHotTopixx FindVertices\n0 0.02 0.04 0.06 0\n0.05\n0.1\n0.15\n0.2\nalpha\n|T A\n\u2212 D\n| F /s\nq rt\n(m *n\n)\nRMSE(TA, D),r=20\nHotTopixx FindVertices\nFigure H.2: Results of the experimental comparison against HOTTOPIXX."}], "references": [{"title": "Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data", "author": ["P. Paatero", "U. Tapper"], "venue": "values. Environmetrics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Functional Data Analysis", "author": ["J. Ramsay", "B. Silverman"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Convex Sparse Matrix Factorization", "author": ["F. Bach", "J. Mairal", "J. Ponce"], "venue": "Technical report, ENS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D. Witten", "R. Tibshirani", "T. Hastie"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Analytical Method for Blind Binary Signal Separation", "author": ["A-J. van der Veen"], "venue": "IEEE Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Network component analysis: reconstruction of regulatory signals in biological systems", "author": ["J. Liao", "R. Boscolo", "Y. Yang", "L. Tran", "C. Sabatti", "V. Roychowdhury"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Transcription Network Analysis by a Sparse Binary Factor Analysis Algorithm", "author": ["S. Tu", "R. Chen", "L. Xu"], "venue": "Journal of Integrative Bioinformatics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "DNA methylation arrays as surrogate measures of cell mixture distribution", "author": ["E. Houseman"], "venue": "BMC Bioinformatics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Model-based overlapping clustering", "author": ["A. Banerjee", "C. Krumpelman", "J. Ghosh", "S. Basu", "R. Mooney"], "venue": "In KDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Decomposing gene expression into cellular processes", "author": ["E. Segal", "A. Battle", "D. Koller"], "venue": "In Proceedings of the 8th Pacific Symposium on Biocomputing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "A generalized linear model for principal component analysis of binary data", "author": ["A. Schein", "L. Saul", "L. Ungar"], "venue": "In AISTATS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Factorisation and denoising of 0-1 data: a variational approach", "author": ["A. Kaban", "E. Bingham"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Modeling dyadic data with binary latent factors", "author": ["E. Meeds", "Z. Gharamani", "R. Neal", "S. Roweis"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Binary matrix factorization with applications", "author": ["Z. Zhang", "C. Ding", "T. Li", "X. Zhang"], "venue": "In IEEE ICDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "The discrete basis problem", "author": ["P. Miettinen", "T. Mielik\u00e4inen", "A. Gionis", "G. Das", "H. Mannila"], "venue": "In PKDD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Computing a nonnegative matrix factorization \u2013 provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Factoring nonnegative matrices with linear programs", "author": ["V. Bittdorf", "B. Recht", "C. Re", "J. Tropp"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "On a lemma of Littlewood and Offord", "author": ["P. Erd\u00f6s"], "venue": "Bull. Amer. Math. Soc,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1951}, {"title": "Efficient algorithms for computing a strong rank-revealing QR factorization", "author": ["M. Gu", "S. Eisenstat"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "Matrix Computations", "author": ["G. Golub", "C. Van Loan"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "On Subspaces Spanned by Random Selections of \u00b11 vectors", "author": ["A. Odlyzko"], "venue": "Journal of Combinatorial Theory A,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "On the Probability that a \u00b11 matrix is singular", "author": ["J. Kahn", "J. Komlos", "E. Szemeredi"], "venue": "Journal of the American Mathematical Society,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}, {"title": "The Littlewoord-Offord problem in high-dimensions and a conjecture of Frankl and F\u00fcredi", "author": ["T. Tao", "V. Vu"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Projected gradient methods for non-negative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Convex analysis approach to D.C. programming: theory, algorithms and applications", "author": ["P. Tao", "L. An"], "venue": "Acta Mathematica Vietnamica,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "On the singularity problem of random Bernoulli matrices", "author": ["T. Tao", "V. Vu"], "venue": "Journal of the American Mathematical Society,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "non-negativity [1, 2], smoothness [3] or sparsity [4, 5].", "startOffset": 15, "endOffset": 21}, {"referenceID": 1, "context": "non-negativity [1, 2], smoothness [3] or sparsity [4, 5].", "startOffset": 15, "endOffset": 21}, {"referenceID": 2, "context": "non-negativity [1, 2], smoothness [3] or sparsity [4, 5].", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "non-negativity [1, 2], smoothness [3] or sparsity [4, 5].", "startOffset": 50, "endOffset": 56}, {"referenceID": 4, "context": "non-negativity [1, 2], smoothness [3] or sparsity [4, 5].", "startOffset": 50, "endOffset": 56}, {"referenceID": 5, "context": "in blind source separation in wireless communication with binary source signals [6]; in network inference from gene expression data [7, 8], where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures from DNA methylation signatures [9] in which case T represents presence/absence of methylation; or in clustering with overlapping clusters with T as a matrix of cluster assignments [10, 11].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "in blind source separation in wireless communication with binary source signals [6]; in network inference from gene expression data [7, 8], where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures from DNA methylation signatures [9] in which case T represents presence/absence of methylation; or in clustering with overlapping clusters with T as a matrix of cluster assignments [10, 11].", "startOffset": 132, "endOffset": 138}, {"referenceID": 7, "context": "in blind source separation in wireless communication with binary source signals [6]; in network inference from gene expression data [7, 8], where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures from DNA methylation signatures [9] in which case T represents presence/absence of methylation; or in clustering with overlapping clusters with T as a matrix of cluster assignments [10, 11].", "startOffset": 132, "endOffset": 138}, {"referenceID": 8, "context": "in blind source separation in wireless communication with binary source signals [6]; in network inference from gene expression data [7, 8], where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures from DNA methylation signatures [9] in which case T represents presence/absence of methylation; or in clustering with overlapping clusters with T as a matrix of cluster assignments [10, 11].", "startOffset": 266, "endOffset": 269}, {"referenceID": 9, "context": "in blind source separation in wireless communication with binary source signals [6]; in network inference from gene expression data [7, 8], where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures from DNA methylation signatures [9] in which case T represents presence/absence of methylation; or in clustering with overlapping clusters with T as a matrix of cluster assignments [10, 11].", "startOffset": 415, "endOffset": 423}, {"referenceID": 10, "context": "in blind source separation in wireless communication with binary source signals [6]; in network inference from gene expression data [7, 8], where T encodes connectivity of transcription factors and genes; in unmixing of cell mixtures from DNA methylation signatures [9] in which case T represents presence/absence of methylation; or in clustering with overlapping clusters with T as a matrix of cluster assignments [10, 11].", "startOffset": 415, "endOffset": 423}, {"referenceID": 11, "context": "In [12] and [13] matrix factorization for binary input data, but non-binary factors T and A is discussed, whereas a factorization TWA with both T and A binary and real-valued W is proposed in [14], which is more restrictive than the model of the present paper.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [12] and [13] matrix factorization for binary input data, but non-binary factors T and A is discussed, whereas a factorization TWA with both T and A binary and real-valued W is proposed in [14], which is more restrictive than the model of the present paper.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "In [12] and [13] matrix factorization for binary input data, but non-binary factors T and A is discussed, whereas a factorization TWA with both T and A binary and real-valued W is proposed in [14], which is more restrictive than the model of the present paper.", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "The model in [14] in turn encompasses binary matrix factorization as proposed in [15], where all of D, T and A are constrained to be binary.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "The model in [14] in turn encompasses binary matrix factorization as proposed in [15], where all of D, T and A are constrained to be binary.", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "It is important to note that this ine of research is fundamentally different from Boolean matrix factorization [16], which is sometimes also referred to as binary matrix factorization.", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "[17] and follow-up work in [18], where it is shown that under certain additional conditions, the NMF problem can be solved globally optimal by means of linear programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[17] and follow-up work in [18], where it is shown that under certain additional conditions, the NMF problem can be solved globally optimal by means of linear programming.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "Moreover, we establish uniqueness of the exact factorization under the separability condition from the NMF literature [17, 19], or alternatively with high probability for T drawn uniformly at random.", "startOffset": 118, "endOffset": 126}, {"referenceID": 18, "context": "Moreover, we establish uniqueness of the exact factorization under the separability condition from the NMF literature [17, 19], or alternatively with high probability for T drawn uniformly at random.", "startOffset": 118, "endOffset": 126}, {"referenceID": 8, "context": "We demonstrate the practical usefulness of our approach in unmixing DNA methylation signatures of blood samples [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 19, "context": "It is shown that the question of uniqueness as well as the computational performance of our approach is intimately connected to the Littlewood-Offord problem in combinatorics [20].", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "The columns {T:,k}k=1 of T , which are vertices of the hypercube [0, 1] , are referred to as components.", "startOffset": 65, "endOffset": 71}, {"referenceID": 0, "context": "This additional constraint is not essential to our approach; it is imposed for reasons of presentation, in order to avoid that the origin is treated differently from the other vertices of [0, 1], because otherwise the zero vector could be dropped from T , leaving the factorization unchanged.", "startOffset": 188, "endOffset": 194}, {"referenceID": 0, "context": "Consequently, (1) can in principle be solved by enumerating all vertices of [0, 1] contained in aff(D) and selecting a maximal affinely independent subset thereof (see Figure 1).", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "The affine subspace aff(D) contains no more than 2r\u22121 vertices of [0, 1].", "startOffset": 66, "endOffset": 72}, {"referenceID": 0, "context": "Dots represent data points and the shaded areas their affine hulls aff(D) \u2229 [0, 1].", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "Left: aff(D) intersects with r + 1 vertices of [0, 1].", "startOffset": 47, "endOffset": 53}, {"referenceID": 20, "context": "In step 2 of Algorithm 5, determining the rank of P and an associated set of linearly independent columns/rows can be done by means of a rank-revealing QR factorization [21, 22].", "startOffset": 169, "endOffset": 177}, {"referenceID": 21, "context": "In step 2 of Algorithm 5, determining the rank of P and an associated set of linearly independent columns/rows can be done by means of a rank-revealing QR factorization [21, 22].", "startOffset": 169, "endOffset": 177}, {"referenceID": 13, "context": "As detailed in the appendix, the case without sum-to-one constraints on A can be handled similarly, as can be the model in [14] with binary left and right factor and real-valued middle factor.", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "The dominating cost in Algorithm 5 is computation of the candidate matrix T\u0302 and checking whether its columns are vertices of [0, 1].", "startOffset": 126, "endOffset": 132}, {"referenceID": 20, "context": "The second most expensive operation is forming the matrix PR,C in step 2 with the help of a QR decomposition requiring O(mn(r \u2212 1)) operations in typical cases [21].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": ", T:,r} contains no other vertices of [0, 1] than the r given ones (cf.", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": "For the example below, T consists of all 2r\u22121 vertices contained in an r\u22121-dimensional face of [0, 1]:", "startOffset": 95, "endOffset": 101}, {"referenceID": 18, "context": "We prove uniqueness under separability, a condition introduced in [19] and imposed recently in [17] to show solvability of the NMF problem by linear programming.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "We prove uniqueness under separability, a condition introduced in [19] and imposed recently in [17] to show solvability of the NMF problem by linear programming.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "This question has essentially been studied in combinatorics [23], with further improvements in [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "This question has essentially been studied in combinatorics [23], with further improvements in [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "For m large enough and r small compared to m (in fact, following [24] one may conjecture that Theorem 3 holds with C = 1), the probability that the affine hull of r vertices of [0, 1] selected uniformly at random contains some other vertex is exponentially small in the dimension m.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "For m large enough and r small compared to m (in fact, following [24] one may conjecture that Theorem 3 holds with C = 1), the probability that the affine hull of r vertices of [0, 1] selected uniformly at random contains some other vertex is exponentially small in the dimension m.", "startOffset": 177, "endOffset": 183}, {"referenceID": 0, "context": "As a byproduct, these results imply that also the NMF variant of our matrix factorization problem (5) can in most cases be reduced to identifying a set of r vertices of [0, 1] (cf.", "startOffset": 169, "endOffset": 175}, {"referenceID": 19, "context": "[20] Let a1, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Evidence for the rate (r \u2212 1)\u2212d/2 can be found in [26].", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "In view of the continuous L-O lemma, a reduction in the number of candidates can still be achieved if the requirement is weakened to T\u0302i,u \u2208 [0, 1].", "startOffset": 141, "endOffset": 147}, {"referenceID": 25, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "programming [28] is used for the block updates of T .", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "Comparison to HOTTOPIXX [18].", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "The methylation profiles of the latter are in {0, 1}, whereas, depending on the mixture proportions associated with each sample, the entries of D take values in [0, 1].", "startOffset": 161, "endOffset": 167}, {"referenceID": 8, "context": "We consider the dataset studied in [9], with m = 500 CpG sites and n = 12 samples of blood cells composed of four major types (B-/T-cells, granulocytes, monocytes), i.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "The affine subspace aff(D) contains no more than 2r\u22121 vertices of [0, 1].", "startOffset": 66, "endOffset": 72}, {"referenceID": 13, "context": "We here sketch how our approach can be applied to obtain a matrix factorization considered in [14], which is of the form TWA\u22a4 with both T and A binary and W real-valued in the exact case; the noisy case be tackled similarly with the help of Algorithm B.", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "[24] Let M be a random m\u00d7 r-matrix whose entries are drawn i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] Let M be a random m\u00d7 r-matrix, r \u2264 m, whose entries are drawn i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For each value of p and r, 100 trials are considered, and for each of these trials, we compute the number of vertices of [0, 1] contained in aff(T ).", "startOffset": 121, "endOffset": 127}], "year": 2014, "abstractText": "Motivated by an application in computational biology, we consider low-rank matrix factorization with {0, 1}-constraints on one of the factors and optionally convex constraints on the second one. In addition to the non-convexity shared with other matrix factorization schemes, our problem is further complicated by a combinatorial constraint set of size 2m\u00b7r, where m is the dimension of the data points and r the rank of the factorization. Despite apparent intractability, we provide \u2212 in the line of recent work on non-negative matrix factorization by Arora et al. (2012)\u2212 an algorithm that provably recovers the underlying factorization in the exact case with O(mr2r +mnr + rn) operations for n datapoints. To obtain this result, we use theory around the Littlewood-Offord lemma from combinatorics.", "creator": "LaTeX with hyperref package"}}}