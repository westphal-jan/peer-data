{"id": "1401.6050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection", "abstract": "Semantic parsing, i.e., the automatic derivation of meaning representation such as an instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language constructs, and can be used to create, execute, and write, many other kinds of computer programs.\n\n\n\n\n\nThe main difference between this distinction is the use of this distinction in human language, since the language has the distinction of making and interpreting in order to describe its concepts. In this case, it is much more important that language writers use this distinction to understand the language's context. This difference is because the language has the distinction of describing the language's semantic properties and its semantic concepts. The language has its semantic properties, and the semantic terms are different. In such cases, the language is more closely related to the context of the sentences used by its readers, since they represent the concepts and sentences used by its readers. In a sentence-oriented language, the semantic terms are not only semantic terms, but also the phrases used by its readers. The semantic terms and phrases they represent are used by their readers. For example, in the French sentence-oriented language, a sentence is defined as having a definite meaning in the sentence. But in English, the concept of meaning does not even make sense in English.\nThe distinction between the semantic terms and the semantic terms is important to human language design because they represent the concepts and sentences used by its readers. The distinction between the semantic terms and the semantic terms is important to human language design because they represent the concepts and sentences used by its readers. The distinction between the semantic terms and the semantic terms is important to human language design because they represent the concepts and sentences used by its readers.\nA new distinction is called the syntactic distinction between the semantic terms and the semantic terms. When two words are separated by one, the semantic terms are similar to those used by the two words. For example, in the Spanish sentence-oriented language, the semantic terms are similar to those used by the two words.\nThere are three main differences between the syntactic distinctions between the semantic terms and the semantic terms:\nFor example, in the Spanish sentence-oriented language, a sentence is defined as having a definite meaning in the sentence. But in the Spanish sentence-oriented language, the semantic terms are different than that used by the two words. For example, in the Spanish sentence-oriented language, a sentence is defined as having a definite meaning in the sentence.\nFor example, in the", "histories": [["v1", "Thu, 23 Jan 2014 16:45:39 GMT  (265kb)", "http://arxiv.org/abs/1401.6050v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hai zhao", "xiaotian zhang", "chunyu kit"], "accepted": false, "id": "1401.6050"}, "pdf": {"name": "1401.6050.pdf", "metadata": {"source": "CRF", "title": "Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection", "authors": ["Hai Zhao", "Xiaotian Zhang"], "emails": ["zhaohai@cs.sjtu.edu.cn", "xtian.zh@gmail.com", "ctckit@cityu.edu.hk"], "sections": [{"heading": "1. Introduction", "text": "The purpose of semantic parsing is to derive the meaning representation for a sentence, usually taking a syntactic parse as input. A popular formalism to represent this kind of meaning is predicate-argument structure and, accordingly, the parsing is to instantiate the predicate and argument(s) in such a structure properly with actual words or phrases from a given sentence. In the context of dependency parsing, it becomes semantic dependency parsing, which takes a syntactic dependency tree as input and outputs a filled predicateargument structure for a predicate, with each argument word properly labeled with its semantic role in relation to the predicate.\nSemantic role labeling (SRL) is one of the core tasks in semantic dependency parsing, be it dependency or constituent based. Conventionally, it is tackled mainly through two subtasks, namely, argument identification and classification. Conceptually, the former determines whether a word is a true argument of a predicate, and the latter what semantic role it plays in relation to the predicate (or which argument it instantiates in a predicateargument structure). When no predicate is given, two other indispensable subtasks are predicate identification and disambiguation, one to identify which word is a predicate in a sentence and the other to determine the predicate-argument structure for an identified predicate in a particular context.\nc\u00a92013 AI Access Foundation. All rights reserved.\nA pipeline framework was adopted in almost all previous researches to handle these subtasks one after another. The main reason for dividing the whole task of semantic dependency parsing into multiple stages in this way is twofold: maintaining computational efficiency and adopting different favorable features for each subtask. In general, a joint learning system of multiple components is slower than a pipeline system, especially in training. It is also reported by Xue and Palmer (2004) that different features do favor different subtasks of SRL, especially argument identification and classification. The results from the CoNLL shared tasks in 2005 and 2008 (Carreras & Ma\u0300rquez, 2005; Koomen, Punyakanok, Roth, & Yih, 2005; Surdeanu, Johansson, Meyers, Ma\u0300rquez, & Nivre, 2008; Johansson & Nugues, 2008) seem to suggest that the pipeline strategy has been the benchmark of technology for the state-of-the-art performance on this specific NLP task.\nWhen most SRL systems are pipeline, an integrated SRL system holds its unique merits, e.g., integrity of implementation, practicality for real applications, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all expected semantic role information, and so on. In particular, it takes into account the interactive effect of features favoring different subtasks and hence holds a more comprehensive view of all features working together as a whole. This article is intended to present our recent research to explore the feasibility of constructing an effective integrated system for semantic dependency parsing that melds all subtasks together into one, including predicate identification/disambiguation and argument identification/classification, for both verbal and nominal predicates, and uses the same feature set for all these subtasks. The core of our research is to verify, through practical implementation and then empirical evaluation, the methodological soundness and effectiveness of this approach. Its success, however, has to be rooted in a solid technical foundation, i.e., a large-scale engineering procedure for efficient mining of effective feature templates from a huge set of feature candidates, a feature space far richer than others ever used before. It is this piece of engineering that brings the potentials of this integrative approach into full play. Another focus of this article is hence to illustrate its technical essentials.\nNevertheless, it is worth pointing out that the term integrative, when used in opposite to pipeline, can be misleading to mean that all subtasks are carried out jointly in a single run. Instead, it is used to highlight the integrity of our model and its implementation that uses a single representation and feature set to accommodate all these subtasks. Although this approach has its unique advantages in simplifying system engineering and feature selection, the model we have implemented and will present below is not a joint one to accomplish the whole semantic parsing through synchronous determination of both predicates and arguments. These two types of indispensable objects in a semantic parse tree are recognized in succession through decoding using the same trained model.\nThe rest of the article is organized as follows. Section 2 gives a brief overview of related work, providing the background of our research. Section 4 presents our approach of adaptive pruning of argument candidates to generate head-dependent word pairs for both training and decoding, which underlies the whole process of semantic parsing. The other two key procedures to optimize the parsing, namely, feature selection and decoding, are presented in Section 5 and 6, respectively. The details of evaluation, including evaluation data, experimental results and a comprehensive comparative analysis of the results, are presented\nin Section 7. Finally, Section 8 concludes our research, highlighting its contributions and the practicality and competitiveness of this approach."}, {"heading": "2. Related Work", "text": "Note that SRL has almost become a surrogate for semantic dependency parsing in the literature of recent years. Most recent research efforts in this field, including the CoNLL shared tasks in 2004 and 2005, have been focused on verbal predicates, thanks to the availability of PropBank (Palmer, Gildea, & Kingsbury, 2005). As a complement to PropBank, NomBank (Meyers, Reeves, Macleod, Szekely, Zielinska, Young, & Grishman, 2004) annotates nominal predicates and their correspondent semantic roles using a similar semantic framework. Although offering more challenges, SRL for nominal predicates has drawn relatively little attention (Jiang & Ng, 2006). The issue of merging various treebanks, including PropBank, NomBank and others, was once discussed in the work of Pustejovsky, Meyers, Palmer, and Poesio (2005). The idea of merging these two treebanks was put into practice for the CoNLL-2008 shared task (Surdeanu et al., 2008). The best system in CoNLL-2008 used two different subsystems to cope with verbal and nominal predicates, respectively (Johansson & Nugues, 2008). Unfortunately, however, there has been no other integrative approach than ours to illustrate a performance so close to that of this system.\nIn fact, there have been few research efforts in this direction, except a recent one on joint identification of predicates, arguments and senses by Meza-Ruiz and Riedel (2009). They formulate the problem into a Markov Logic Network, with weights learnt via 1-best MIRA (Crammer & Singer, 2003) Online Learning method, and use Cutting Plane Inference (Riedel, 2008) with Integer Linear Programming (ILP) as the base solver for efficient joint inference of the best choice of predicates, frame types, arguments and role labels with maximal a posteriori probability. Using CoNLL-2008 data, their system achieves its best semantic F1 80.16% on the WSJ test set. This is 0.75 percentage point lower than ours, to be reported below, on the whole WSJ+Brown test set. Note that when trained on CoNLL2008 training corpus, a subset of WSJ corpus, an SRL system has a performance at least 10 percentage points higher on the WSJ than on the Brown test set (Surdeanu et al., 2008).\nBoth CoNLL-2008 and -2009 shared tasks1 are devoted to the joint learning of syntactic and semantic dependencies, aimed at testing whether SRL can be well performed using only dependency syntax input. The research reported in this article focuses on semantic dependency parsing. To conduct a valid and reliable evaluation, we will use the data set and evaluation settings of CoNLL-2008 and compare our integrated system, which is the best SRL system in CoNLL-2009 (Zhao, Chen, Kit, & Zhou, 2009), against the top systems in CoNLL shared tasks (Surdeanu et al., 2008; Hajic\u030c, Ciaramita, Johansson, Kawahara, Mart\u0301\u0131, Ma\u0300rquez, Meyers, Nivre, Pado\u0301, S\u030cte\u030cpa\u0301nek, Stran\u030ca\u0301k, Surdeanu, Xue, & Zhang, 2009).2 Note that these systems achieved higher performance scores in CoNLL-2008 than in CoNLL-2009.\nAn integrative approach to dependency semantic parsing has its own pros and cons. To deal with its main drawbacks, two key techniques need to be applied for the purpose of\n1. Henceforth referred to as CoNLL-2008 and -2009, respectively. 2. CoNLL-2008 is an English-only task, while CoNLL-2009 is a multilingual one. Although both use the\nsame English corpus, except some more-sophisticated structures for the former (Surdeanu et al., 2008), their main difference is that semantic predicate identification is not required for the latter.\nefficiency enhancement. One is to bring in auxiliary argument labels that enable further improvement of argument candidate pruning. This significantly facilitates the development of a fast and lightweight SRL system. The other is to apply a greedy feature selection algorithm to perform the task of feature selection from a given set of feature templates. This helps find as many features as possible that are of benefit to the overall process of the parsing. Many individual optimal feature template sets are reported in the literature to have achieved an excellent performance on specific subtasks of SRL. This is the first time that an integrated SRL system is reported to produce a result so close to the state of the art of SRL achieved by those pipelines with individual sub-systems each highly specialized for a specific subtask or a specific type of predicate."}, {"heading": "3. System Architecture", "text": "Dependencies between words in a sentence, be they syntactic or semantic, can be formulated as individual edges in an abstract graph structure. In practice, a dependency edge has to be built, and its type (usually referred to as its label) to be identified, through proper learning and then decoding. Most conventional syntactic parsing makes use of a property of projectiveness stipulated by the well-formedness of a syntactic tree. In contrast, in dependency parsing, new dependencies have to be built with regard to existing ones. However, this is not the case for semantic parsing, for most semantic parsing results are not projective trees. Instead, they are actually directed acyclic graphs, because the same word can serve as an argument for multiple predicates. Inevitably, a learning model for semantic parsing has to take all word pairs into account when exploring possible dependent relationships.\nSRL as a specific task of semantic dependency parsing can be formulated as a word pair classification problem and tackled with various machine learning models, e.g., the Maximum Entropy (ME) model as used by Zhao and Kit (2008). The ME model is also used in this work but only for probability estimation to support the global decoding given below in Section 6, which extends our model beyond a sequential model. Without any constraint, a classifier for this task has to deal with all word pairs in an input sequence and is thus inevitably prone to poor computational efficiency and also unsatisfactory performance. A straightforward strategy to alleviate these problems is to perform proper pruning on both the training sample and test data.\nA word pair consists of a word as semantic head and another as semantic dependent, which are conventionally denoted as p (for predicate) and a (for argument), respectively. We will follow this convention in the feature representation below. Since our approach unifies the two tasks of SRL, namely, predicate identification/disambiguation and argument identification/classification, into one classification framework, there is no need to differentiate between verbal and non-verbal heads, because they are all handled in the same way. This is one of the unique characteristics of our integrated system.\nThe overall architecture of our system is depicted in Figure 1. An input sentence from a data set in use, be it a training, a development or a test set, is parsed into a word pair sequence by a word pair generator using a pruning algorithm, e.g., the adaptive pruning described below, to eliminate useless pairs. Word pairs so generated from each sentence of the training set are used to train a word pair classifier, which then supports the decoding formulated in Section 6 to search for an optimal set of word pairs from a test sentence to\nform a semantic parse tree. The decoding first recognizes all predicates in a sentence and then determines the arguments for each predicate by a beam search for their argument role labels. The features used in the classifier are selected from a predefined feature space by a greedy selection procedure using the training and the development set for repeated training and testing to refine a candidate feature set until no more performance gain is achievable (see Section 5). Then the classifier obtained this way with the selected features is tested on the test set."}, {"heading": "4. Adaptive Argument Pruning", "text": "Word pairs are derived from a sentence for the classifier in the following ways. (1) For predicate identification/disambiguation, each word pair consists of the virtual root (VR) of a semantic parse tree under construction (whose root is virtually preset), as head, and a predicate candidate as its dependent. Theoretically, all words in the sentence in question can be a predicate candidate. To reduce their number, we opt for a simple POS tag pruning strategy that only verbs and nouns are allowed as predicate candidates. (2) For argument identification/classification, each word pair consists of an identified predicate, as head, and another word as its dependent (or its argument, in conventional term). Potentially, any other word in the same sentence can be its argument candidate. Pruning off as many argument candidates as possible is thus particularly significant in improving the efficiency and performance of the classifier.\nThere are two ways to collect argument candidates for a given predicate, one from the syntactic dependency tree and the other from the linear path of an input sentence. For the former (referred to as synPth hereafter), we use a dependency version of the pruning algorithm by Xue and Palmer (2004), which is given as follows with a necessary modification to allow a predicate itself also to be included in its own argument candidate list, because a nominal predicate sometimes takes itself as its own argument.\nInitialization: Given a predicate as the current node in a syntactic dependency tree.\n1. Collect all its syntactic children as argument candidates, by traversing the children from left to right.\n2. Reset the current node to its syntactic head and repeat Step 1 till the root of the tree.\n3. Collect the root and stop.\nThis algorithm is effective in collecting both words in the path from a given predicate to the root and their children as argument candidates. However, a more efficient one is still needed to lend stronger support to our SRL system that is designed to tackle argument identification/classification in a single stage. Following the observation that arguments usually tend to surround their predicate in a close distance, the auxiliary label noMoreArg is introduced to signify where the pruning stops collecting argument candidates. For training sample generation, this label is assigned to the next word as soon as the arguments of the current predicate have been saturated with previously collected words, in light of the original training data as illustrated in Table 1. Accordingly, the pruning process stops collecting any more candidates. For decoding, it signals the decoder to stop searching, along a similar traverse as the pruning, for any more arguments for an identified predicate. This adaptive technique improves the pruning efficiency significantly, saving about 1/3 training time and memory at the cost of missing very few more true arguments than the pruning without this label, according to our experiments. The training sample generated this way from the sentence in Table 1, by means of both POS pruning and the above pruning algorithm, is illustrated in Table 2, with a few class labels in the third column.\nTo collect argument candidates along the linear path (referred to as linPth hereafter) instead of the syntactic tree of a sentence, the classifier will search through all words around a given predicate. In a way similar to how the pruning along synPth is improved, two auxiliary labels, namely, noMoreLeftArg and noMoreRightArg, are introduced to signify where the adaptive pruning along linPth stops, skipping those words too far away from the predicate. Given below is an example to illustrate how these two labels are used, where e in\nthe input sequence is a predicate with two arguments, labeled with A0 and A1, respectively. The two labels are assigned to the next two words c and g, respectively, indicating no more arguments farther than them from the predicate. Accordingly, the word sequence from c to g are taken as training sample.\na b c d e f g h . noMoreLeftArg A1 A0 noMoreRightArg\nThe total list of class labels in our model, including those from the CoNLL-2008 data set and a few auxiliary ones newly introduced on purpose, is provided in Table 9 in Appendix A. These labels are in three categories, namely, 22 PropBank sense labels as predicate classes, 54 argument classes, and 2\u20133 auxiliary labels as extra classes, for a total of 78-79. Pruning along linPth needs one more label than that along synPth. Note that our work does not assume whether the same sense label in the training and the test set means the same for different words. The tendency of a particular word form to associate with its senses in a statistically significant way throughout the data set allows our classifier to predict sense labels using word form features.\nIn principle, an auxiliary label is assigned to the last item in the sample that is generated for a predicate via pruning along a traversal order, be it syntactic or linear. That is, it is assigned to the first item immediately after the last argument of the predicate has been seen during the pruning. An auxiliary label is treated in exactly the same way as all other argument labels during training and decoding, except its extra utility to signal where to stop a search."}, {"heading": "5. Feature Generation and Selection", "text": "Following many previous works (Gildea & Jurafsky, 2002; Carreras & Ma\u0300rquez, 2005; Koomen et al., 2005; Ma\u0300rquez, Surdeanu, Comas, & Turmo, 2005; Dang & Palmer, 2005; Pradhan, Ward, Hacioglu, Martin, & Jurafsky, 2005; Toutanova, Haghighi, & Manning,\n2005; Jiang & Ng, 2006; Liu & Ng, 2007; Surdeanu, Marquez, Carreras, & Comas, 2007; Johansson & Nugues, 2008; Che, Li, Hu, Li, Qin, Liu, & Li, 2008), we carefully examine the factors involved in a wide range of features that have been or can be used to facilitate the undertaking of the two SRL subtasks, for both verbal and nominal predicates. Our endeavor is to further decompose these factors into some more fundamental elements, so that the largest possible space of feature templates can be explored for more effective and novel combinations of them into features."}, {"heading": "5.1 Feature Element", "text": "All features adopted for this work are intended to make full use of these elements, which are mainly drawn from the word property and syntactic connection of a node in the syntactic parse tree of an input sentence. The sequences or sets of tree nodes, whose basic elements are drawn to form features via feature generation by means of many predefined feature templates, are identified through the path and family relations as stipulated below.\nWord Property This type of elements include word form (denoted as form and its split form as spForm),3 lemma (as lemma and spLemma), part-of-speech tag (as pos and spPos), and syntactic and semantic dependency labels (as dprel and semdprel).4\nSyntactic Connection This includes syntactic head (as h), left/right farthest/nearest child (as slm, ln, rm and rn), and high/low support verb or noun. Note that along the path from a given word to the root of a syntactic tree, the first/last verb is called its low/high support verb, respectively. This notion is widely adopted in the field (Toutanova et al., 2005; Xue, 2006; Jiang & Ng, 2006).5 In this work, we extend it to both nouns and prepositions. Besides, we also introduce another syntactic head feature pphead for a given word in question, to retain its left most sibling if headed by a preposition, or its original head otherwise, aimed at drawing utility from the fact that a preposition usually carries little semantic information. The positive effect of this new feature is confirmed by our experiments.\nPath There are two basic types of path from an argument candidate a to a given predicate p, namely, the linear path linePath as the sequence of input words between them (inclusive) and the other path dpPath between them (inclusive) as in their syntactic dependency tree. Given the two paths from them to the root r of the tree that meet at a node r\u2032, we have their common part dpPathShare from r\u2032 to r, their different parts dpPathArgu and dpPathPred from a and p to r\u2032, respectively, and the path dpPath between a and p. Similarly, we have a dpPath between any two nodes in a syntactic tree.\nFamily Two child sets are differentiated for a given predicate or argument candidate, one (as children) including all syntactic children and the other (as noFarChildren) excluding only the leftmost and the rightmost one. The latter is introduced as a feature to differentiate the modifiers (i.e., children) close to the head from those far away.\n3. Note that in CoNLL-2008, many treebank tokens are split at the position of a hyphen (-) or a forward slash (/), resulting in two types of form for each, namely, non-split and split. 4. The lemma and pos, for both training and test, are directly from the pre-analyzed columns of an input file, automatically generated by the organizer of CoNLL shared tasks. 5. Note that the notion of the term support verb is slightly different in these works. It is used here to refer to a verb that introduces a long-distance argument to a nominal predicate from outside of the noun phrase headed by the nominal predicate.\nOthers There are also a number of other elements, besides those in the above categories, that play a significant role in feature generation. Many of them are derived from inter-word relationships. Listed below are a number of representative ones.\ndpTreeRelation It returns the relationship of a and p in an input syntactic tree. The possible values for this feature include parent, sibling, etc.\nisCurPred It checks whether a word in question is the current predicate, and returns the predicate itself if yes, or a default value otherwise.\nexistCross It checks if a potential dependency relation between a given pair of words may cross any existing relation in the semantic tree under construction.\ndistance It returns the distance between two words along a given path, be it dpPath or linePath, in number of words.\nexistSemdprel It checks whether a given argument label under a predicate has been assigned to any other word.\nvoice It returns either Active or Passive for a verb and a default value for a noun. baseline A small set of simple rules6 are used to generate SRL output as the baseline for CoNLL evaluation (Carreras & Ma\u0300rquez, 2005). This baseline output can be selectively used as features, in two categories: baseline Ax tags the head of the first NP before and after a predicate as A0 and A1, respectively, and baseline Mod tags the modal verb dependent of a predicate as AM-MOD.\nA number of features such as existCross and existSemdprel have to depend on the semantic dependencies or dependency labels in the existing part of a semantic parse tree under (re)construction for a sentence, be it for training or decoding. Note that both training and decoding first take the candidate word pairs from a given sentence as input, as illustrated in Table 2, and then undergo a process of selecting a subset of the candidates to (re)construct a semantic parse tree, which consists of a root, some predicate(s) as its child(ren), and the argument(s) of the predicate(s) as its grandchild(ren). The decoding infers an optimal semantic tree for a sentence with the aid of a trained ME model (see Section 6). The training reconstructs the gold standard semantic tree of an input sentence when scanning through its word pairs in sequence and differentiating the true ones in the tree from the others. The true ones rebuild the tree part by part. All features (including existCross and existSemdprel) extracted from both the true ones, as in the partially (re)built parts of the tree, and the others in the current context are fed to the ME model for training. In other words, the feature generation is based on gold standard argument labels during training and on predicted ones during decoding."}, {"heading": "5.2 Feature Generation", "text": "Sequences of syntactic tree nodes are first collected by means of the paths and/or the family relations defined above. Three strategies are then applied to combine elements of the same type (e.g., form, spPos) from these nodes into a feature via string concatenation. The three strategies of concatenation are: (1) sequencing (as seq), which concatenates given element strings in their original order in the path, (2) unduplicating (as noDup), which further frees\n6. Developed by Erik T K Sang, of the University of Antwerp, Belgium.\nseq from adjacent duplicates, and (3) bagging (as bag), which concatenates unique element strings in alphabetical order.\nGiven below are a number of typical feature templates to illustrate how individual features are derived in the ways as described above, with the aid of the following operators: x+y (the concatenation of x and y), x.y (the attribute y of x), x:y (the path from x to y), and x:y|z (the collection of all instances of attribute z along the path from x to y).\na.lm.lemma The lemma of the leftmost child of the argument candidate a.\np.h.dprel The dependency label of the syntactic head of predicate candidate p.\np-1.pos + p.pos The concatenation of the POS tags of two consecutive predicates. a:p|dpPath.lemma.bag The bag of all lemmas along the dpPath from a to p. a:p.highSupportNoun|linePath.dprel.seq The seq of all dependency labels along the\nlinePath from a to the high support noun of p.\nIn this way, a set of 781 feature templates,7 henceforth referred to as FT , is generated to specify the allowable feature space for feature selection. Many of them are generated by analogy to existing feature templates in the literature. For example, given a feature template like a.lm.lemma which has been used in some previous works, its analogous ones such as a.rm.lemma, a.rn.lemma and a.ln.lemma are included in the FT .\nPredicate sense labels in the data set are also utilized as a type of element in various feature templates in the FT . However, it is worth noting that the same sense label associated with different words, e.g., 02 in take.02 and in say.02, is not assumed to have anything in common or anything to do with each other. For predicate disambiguation, however, these features always combine a predicate sense with a word form, and hence naturally differentiate between the same sense label for different words. To predict a predicate sense label is always to predict it in association with a word form. That is, a sense label is never used in separation from a word form. In this way, our model gives a very high precision for sense label prediction according to our empirical results."}, {"heading": "5.3 Feature Template Selection", "text": "It is a complicated and hence computationally expensive task to extract an optimal subset of feature templates from a large feature space. For the sake of efficiency, a greedy procedure for feature selection has to be applied towards this goal, as illustrated in many previous works, e.g., by Jiang and Ng (2006), and Ding and Chang (2008). The algorithm that we implemented for this purpose is presented in Algorithm 1 below, which imposes fewer assumptions than those in previous works, aiming at a higher efficiency. It repeats two main steps until no further performance gain is achievable on the given development set:\n1. Include any template from the rest of FT into the current set of candidate templates if its inclusion would lead to a performance gain.\n2. Exclude any template from the current set of candidate templates if its exclusion would lead to no deterioration in performance. By repeatedly adding/removing the\n7. Available at http://bcmi.sjtu.edu.cn/\u223czhaohai/TSRLENAllT.txt, in a macro language as used in our implementation, far not as readable as the notation of the illustrations given here.\nmost/least useful template, the algorithm aims to return a better or smaller candidate set for next round.\nGiven n candidate feature templates, the algorithm by Ding and Chang (2008) requires O(n2) time to execute a training/test routine, whereas the one by Jiang and Ng (2006) requires O(n) time, assuming that the initial set of feature templates is \u201cgood\u201d enough and the others can be handled in a strictly incremental way. The time complexity of our algorithm can also be analyzed in terms of the execution time of the training-and-test routine scr(M(.)), for all other subroutines such as sorting are negligible while compared against its execution time. In Algorithm 1, recruitMore first calls this routine |FT \u2212 S| \u2264 n times in the for loop, and then shakeOff calls it |Smax| \u2264 n times to prepare for the sorting, followed by at most another |Smax| times in the inner while loop. Assuming that the first while loop and the outer while in shakeOff iterate k1 and k2 times, respectively, the algorithm is of O(k1(|FT \u2212 S|+ k2(|Smax|+ |Smax|))) = O(k1k2n) time.\nEmpirically, however, we have k1, k2 << n, in that our experiments seldom show any k1 > 5 or k2 > 10, especially when running with 1/10 FT randomly chosen as the initial S. In particular, the first while loop often iterates only 2-3 times, and after its first iteration k2 drops rapidly. The observation that k1k2 varies only in a very limited range suggests that we may have O(k1k2n) = O(n) as an empirical estimation of the efficiency of the algorithm in this particular context. A reasonable account for this is that as the first while loop comprises of only two functions, namely, recruitMore to recruit positive feature templates and shakeOff to filter out negative ones, so as to improve the model in either case, it is likely that the positive/negative ones remain positive/negative consistently throughout the looping. As a result, only very few of them remain outside/inside the candidate set for further recruiting/filtering after a couple of iterations of the loop.\nThis efficiency allows a large-scale engineering of feature selection to be accomplished at a reasonable cost of time. In our experiments with 1/10 FT randomly selected as the initial S, the greedy selection procedure was performed along one of the two argument candidate traverse schemes (i.e., the synPth and linPth) on NomBank, PropBank or their combination, and output six feature template sets SsN , S s P , S s N+P , S l N , S l P and S l N+P , of 186, 87, 246, 120, 80 and 118 selected templates, respectively, for performance evaluation and comparison. About 5500 machine learning routines ran for the synPth scheme and nearly 7000 routines for the linPth. A contrastive analysis of these template sets, with a focus on the top 100 or so most important templates from each of them, is presented in Appendix A through Tables 9-17, where the rank columns present the rankings of feature templates in terms of their importance in respective feature template sets. The importance of a feature template in a template set is measured in terms of the performance change by adding or removing that template, and the performance of a model using a template set is measured by its labeled F1 score on a given test set, following the conventional practice of SRL evaluation in CoNLL shared tasks.\nIt is interesting to note that the six template sets have a tiny intersection of only 5 templates, as listed in Table 10, each manifesting a notable variance of importance ranking in different sets. Excluding these five, the rest of the overlap of the top 100 of the synPth sets SsN , S s P and S s N+P is also very small, of only 11 templates, in contrast to that of the linPth sets SlN , S l P and S l N+P , which is about 4 times larger, of 46 templates; as listed in\nAlgorithm 1 Greedy Feature Selection Input A training data set: T A development data set: D The set of all feature templates: FT\nDenotation M(S) = M(S, T ), a model using feature template set S, trained on T ; scr(M) = scr(M,D), the evaluation score of model M on D; Since T and D are fixed, let scr(M(S)) = scr(M(S, T ), D) for brevity.\nAlgorithm\n1: S = {f0, f1, ..., fk}, a random subset of FT ; FT : a globally accessible constant 2: while do 3: Cr = recruitMore(S); 4: if Cr == {} then return S; 5: S\u2032 = shakeOff(S + Cr); 6: if scr(M(S)) \u2265 scr(M(S\u2032)) then return S; 7: S = S\u2032; 8: end while 1: function recruitMore(S) Retrieve more positive templates from FT \u2212 S 2: Cr = {}, and p = scr(M(S)); 3: for each f \u2208 FT \u2212 S do 4: if p < scr(M(S + {f})) then Cr = Cr + {f}; 5: end for 6: return Cr; 7: end function\n1: function shakeOff(Smax) Shake off useless templates from Smax 2: while do 3: S = S0 = Smax; 4: sort S in the descending ordera of scr(M(S \u2212 {f})) for each f \u2208 S; 5: while (S = S \u2212 {f0}) = {} do 6: Smax = argmaxx\u2208{Smax, S}scr(M(x)); Drop f0 \u2208 S if it is useless 7: end while 8: if S0 == Smax then return S0; If none dropped 9: end while\n10: end function\na. Namely in the ascending order of the importance of f in S, estimated by scr(M(S))\u2212 scr(M(S\u2212{f})).\nTables 11 and 12, respectively. Besides these shared templates, these six sets hold 84, 71, 84, 69, 29 and 67 others in their top 100, as listed in Tables 13-18, respectively, where a negative/positive subscript denotes a preceding/following word. For example, a.lm -1.lemma returns the lemma of the previous word of a\u2019s left most child.\nThe rather small overlap of the six sets suggests that the greedy feature selection algorithm maintains a stable efficiency while working out these template sets of huge divergence, lending evidence to support the empirical estimation above. Despite this divergence, each of these template sets enables our SRL model to achieve a state-of-the-art performance on the CoNLL-2008 data set,8 indicating the effectiveness of this approach, for which more details of evaluation will be provided in Section 7 below."}, {"heading": "6. Decoding", "text": "Following exactly the same procedure of generating the training sample, our ME classifier, after training, outputs a series of labels for the sequence of word pairs generated from an input sentence, inferring its predicates and their arguments one after another. Different from most existing SRL systems, it instantiates an integrative approach that conducts all predication with the same trained model. However, following the common practice of incorporating task-specific constraints into a global inference (Roth & Yih, 2004; Punyakanok, Roth, Yih, & Zimak, 2004), we opt for further developing a decoding algorithm to infer the optimal argument structure for any predicate that is identified this way by the classifier. The main differences of our work from Punyakanok et al. (2004) are that (1) they use ILP for joint inference, which is exact, and we use beam search, which is greedy and approximate, and (2) the constraints (e.g., no duplicate argument label is allowed) that they impose on arguments through individual linear (in)equalities are realized through our constraint fulfillment features (e.g., existCross and existSemdprel).\nSpecifically, the decoding is to identify the arguments among candidate words by inferring the best semantic role label for each candidate (cf. the training sample in Table 2 with one label per word). Let A = {a0, a1, ..., an\u22121} be the candidates for a predicate, where each ai embodies all available properties of a word, including a candidate label, and let A\u2032i = a0 a1 ... ai\u22121 be a partial argument structure (of our target under search) that has been determined and ready for use as the context for inferring the next argument. Instead of counting on best-first search, which simply keeps picking the next best argument according the conditional probability p(ai|A\u2032i), we resort to a beam search for a better approximation of the global optimization for the maximal probability in\nA\u0303 = argmax A\u2032\u2286A\nn\u220f\ni=0\np(ai|A\u2032i), (1)\nwhere A\u2032i consists of the first i elements of A \u2032. Ideally, the beam search returns the most probable subset of A as arguments for the predicate in question. It rests on a conditional maximum entropy sequential model incorporating global features into the decoding to infer the arguments that are not necessarily in a sequential order. As in previous practice, our\n8. Note that an early version of this model also illustrated a top-ranking performance on CoNLL-2009 multilingual data sets (Zhao, Chen, Kit, & Zhou, 2009).\nME model adopts a tunable Gaussian prior (Chen & Rosenfeld, 1999) to estimate p(ai|A\u2032i) and applies the L-BFGS algorithm (Nocedal, 1980; Nash & Nocedal, 1991) for parameter optimization."}, {"heading": "7. Evaluation", "text": "The evaluation of our SRL approach is conducted with various feature template sets on the official training/development/test corpora of CoNLL-2008 (Surdeanu et al., 2008). This data set is derived by merging a dependency version of the Penn Treebank 3 (Marcus, Santorini, & Marcinkiewicz, 1993) with PropBank and NomBank. Note that CoNLL-2008 is essentially a joint learning task on both syntactic and semantic dependencies. The research presented in this article is focused on semantic dependencies, for which the primary evaluation measure is the semantic labeled F1 score (Sem-F1). Other scores, including the macro labeled F1 score (Macro-F1), which was used to rank the participating systems in CoNLL-2008, and Sem-F1/LAS, the ratio between labeled F1 score for semantic dependencies and the labeled attachment score (LAS) for syntactic dependencies, are also provided for reference."}, {"heading": "7.1 Syntactic Input", "text": "Two types of syntactic input are used to examine the effectiveness of our integrative SRL approach. One is the gold standard syntactic input available from the official data set and the other is the parsing results of the same data set by two state-of-the-art syntactic parsers, namely, the MSTparser9 (McDonald, Pereira, Ribarov, & Hajic\u030c, 2005; McDonald & Pereira, 2006) and the parser of Johansson and Nugues (2008). However, instead of using the original MSTparser, we have it substantially enriched with additional features, following Chen, Kawahara, Uchimoto, Zhang, and Isahara (2008), Koo, Carreras, and Collins (2008), and Nivre and McDonald (2008). The latter one, henceforth referred to as J&N for short, is a second-order graph-based dependency parser that takes advantage of pseudo-projective techniques and resorts to syntactic-semantic reranking for further refining its final outputs. However, only its 1-best outputs before the reranking are used for our evaluation, even thought the reranking can slightly improve its parsing performance. Note that this reward of reranking through joint-learning for syntactic and semantic parsing is gained at a huge computational cost. On the contrary, our approach is intended to show that highly comparable results can be achieved at much lower cost."}, {"heading": "7.2 Experimental Results", "text": "The effectiveness of the proposed adaptive approach to pruning argument candidates is examined with the above three syntactic inputs, and the results are presented in Table 3,10 where a coverage rate is the proportion of true arguments in pruning output. Note that using auxiliary labels does not affect this rate, which has to be accounted for by the choice of traverse path and the quality of syntactic input, as suggested by its difference in the synPth rows. The results show that the pruning reduces more than 50% candidates along\n9. Available at http://mstparser.sourceforge.net. 10. Decimal figures in all tables herein are percentages unless otherwise specified.\nsynPth, at the cost of losing 1.6-4.6% true ones, and more than 70% along linPth without any loss. Nevertheless, the candidate set so resulted from synPth is 1/3 smaller in size than that from linPth.\nThe number of times that the training-and-test routine is executed in the greedy selection of all six feature sets are presented in Table 4, showing that synPth saves 21%-24% execution times. Given the estimation of the time complexity of the selection algorithm as O(k1k2n) for executing the routine, empirically we have 7<k1k2<10 on a feature space of size n=781 for our experiments, verifying the very high efficiency of the algorithm.\nAs pointed out by Pradhan, Ward, Hacioglu, Martin, and Jurafsky (2004), argument identification (before classification) is a bottleneck problem in the way of improving SRL performance. Narrowing down the set of predicate candidates as much as possible in a reliable way has been shown to be a feasible means to alleviate this problem. The effectiveness of our adaptive pruning for this purpose can be examined through comparative experiments in terms of time reduction and performance enhancement. The results from a series of such experiments are presented in Table 5, showing that the adaptive pruning saves the training and test time by about 30% and 60%, respectively, while enhancing the performance (in Sem-F1 score) by 23.9%\u201324.8%, nearly a quarter. These results also confirm a significant improvement upon its non-adaptive origin (Xue & Palmer, 2004) and the twofold benefit of pruning off arguments far away from their predicates, which follows from the assumption that true arguments tend to be close to their predicates. It is straightforward that using the noMoreArg label reduces more training samples than not using (see Section 4) and hence leads to a greater reduction of training time. Using this label also decreases the test time remarkably. During decoding, a noMoreArg label, once assigned a probability higher than all other possible role labels for the current word pair, hints the decoder to stop working on the next word pair, resulting in a further test time reduction by 18.5\u201321.0 percentage points upon the non-adaptive pruning. The particularly low performance without pruning also reflects the soundness of the motivation for candidate pruning from both\nthe machine learning and linguistic perspective. The pruning provides a more balanced training dataset for classifier training than without pruning. Note that without pruning, most word pairs generated for the training are irrelevant and far away from the current predicate, inevitably interfering with the informative features from the truly relevant ones in the very small minority and, hence, leading to an unsatisfactory performance. Although the pruning, especially its adaptive version, is rooted in a linguistic insight gained from empirical observations on real data, most previous works on semantic parsing simply took the pruning as an indispensable step towards a good parsing performance, seldom paying much attention to the poor performance without pruning nor comparing it with the performance by different pruning strategies.\nTable 6 presents a comprehensive results of our semantic dependency parsing on the three syntactic inputs aforementioned of different quality. A number of observations can be made from these results. (1) The greedy feature selection, as encoded in Algorithm 1 above, boosts the SRL performance drastically, raising the Sem-F1 scores in the synPth rows from 54.79%\u201357.77% of the initial feature sets, the baseline, to 80.88%\u201386.02% of the\nselected feature sets, by an increment of 46.73%\u201348.90%. The rise in corresponding linPth rows is even larger. Among the three inputs, the largest increment is on the gold standard, suggesting that the feature selection has a greater effect on an input of better quality. (2) The traverse scheme synPth leads to a better model than linPth, as reflected in the difference of Sem-F1 and Sem-F1/LAS scores between them, indicating that this integrative SRL approach is sensitive to the path along which argument candidates are traversed. The difference of their Sem-F1/LAS scores, for instance, is in the range of 7.14%\u20138.75% and 0.91%\u20131.21% for the initial and the selected feature sets, respectively. The significant advantage of synPth is confirmed consistently, even though an optimized feature set narrows down the performance discrepancy between the two so radically. (3) The result that both Nomi-F1 x N and Verb-F1 x P are higher than corresponding F1 x N+P consistently throughout almost all experimental settings except one shows that the feature selection separately on Nombank or PropBank (for verbal or nominal predicates, respectively) gives a better performance than that on the combination Nombank+PropBank for both. This has to be explained by the interference between the two data sets due to their heterogeneous nature, namely, the interference between the nominal and verbal predicate samples. Hence, optimizing a feature set specifically for a particular type of predicates is more effective than for both. (4) An overall comparison of our system\u2019s SRL performance on the three syntactic inputs of different quality (as reflected in their LAS) shows that the performance as a whole varies in accord with the quality of input. This is exhibited in the contrast of the Sem-F1 scores on these inputs, even though a small LAS difference may not necessarily lead to a significant performance difference (for instance, MST has a LAS of 0.89 percentage point lower than J&N but gives a Sem-F1 score as high in one of the four experimental settings). The table also shows that a LAS difference of 11.61 percentage points, from 88.39% to 100%, corresponds to a Sem-F1 score difference of at most 5.14 percentage points, from 80.88% to 86.02%, in the best setting (i.e., using the selected feature set and taking synPth).\nHowever, Sem-F1 scores cannot be trusted to faithfully reflect the competence of a semantic parser, because the quality of syntactic input is also a decisive factor to decide such scores. For this reason, we have the Sem-F1/LAS ratio as an evaluation metric. Interestingly, our parser\u2019s scores of this ratio on the two syntactic inputs of a LAS 10.82\u2013 11.61 percentage points below the gold standard are, contrarily, 4.57\u20135.52 percentage points higher. This is certainly not to mean that the parser is able to rescue, in a sense, some true semantic parses from an erroneous syntactic input. Instead, it can only be explained by the parser\u2019s high tolerance of imperfections in the syntactic input.\nTable 7 further presents experimental results on feature ablation and feature set combination. The former is to examine the effect of sense features and the latter that of feature\noptimization. Along synPth, both the ablation of sense feature and the mix of two feature sets respectively optimized (through the greedy selection) on the NomBank and PropBank lead to a significant performance loss of 0.75%\u20130.83%, in comparison with the performance of the feature set SsN+P optimized on the combination of the two treebanks as given in Table 6. Along linPth, they lead to a much less significant and an insignificant loss, respestively. These results show that both sense features and the greedy selection of features are more significant in joining with the adaptive pruning along synPth to achieve a performance gain."}, {"heading": "7.3 Comparison and Analysis", "text": "In order to evaluate the parser impartially in a comparative manner, its performance along synPth is compared with that of the other state-of-the-art systems in CoNLL-2008. They are chosen for this comparison because of being ranked among top four among all participants in the shared task or using some sophisticated joint learning techniques. The one of Titov, Henderson, Merlo, and Musillo (2009) that adopts a similar joint learning approach as Henderson, Merlo, Musillo, and Titov (2008) is also included, because of their significant methodological difference from the others. In particular, the former has attained the best performance to date in the direction of genuine joint learning. The reported performance of all these systems on the CoNLL-2008 test set in terms of a series of F1 scores is presented in Table 8 for comparison. Ours is significantly better (t = 14.6, P < 0.025) than all the others except the post-evaluation result of Johansson and Nugues (2008). Contrary to the best three systems in CoNLL-2008 (Johansson & Nugues, 2008; Ciaramita, Attardi, Dell\u2019Orletta, & Surdeanu, 2008; Che et al., 2008) that use SRL pipelines, our current work is intended to integrate them into one. Another baseline, namely, our current model using the feature set from the work of Zhao and Kit (2008), instead of a random set, is also included in the table for comparison, showing a significant performance enhancement on top of the previous model and, then, a further enhancement by the greedy feature selection.\nAlthough this work draws necessary support from the basic techniques (especially those for traverse along synPth) underlying our previous systems for CoNLL-2008 and -2009 (Zhao & Kit, 2008; Zhao, Chen, Kit, & Zhou, 2009; Zhao, Chen, Kazama, Uchimoto, & Torisawa, 2009), what marks its uniqueness is that all SRL sub-tasks are performed by one integrative model with one selected feature set. Our previous systems dealt with predicate disambiguation as a separate sub-task. This is our first attempt at a fully integrated SRL system.\nThe fact that our integrated system is yet to give a performance on a par with the postevaluation result of Johansson and Nugues (2008) seems attributable to a number of factors, including the ad hoc features adopted in their work to handle linguistic constructions such as raising/control and coordination. However, the most noticeable ones are the following discrepancies between the two systems, in addition to pipeline vs. all-in-one integration. (1) They have the n-best syntactic candidates as input, which without doubt provide more useful information than the 1-best that we use. (2) Then, they exploit reranking as a joint learning strategy to make fuller use of the n-best candidates and any intermediate semantic result once available, resulting in a gain of 0.5% increment of Sem-F1 score. (3) They use respective sub-systems to deal with verbal and nominal predicates in a more specific manner, following the observation that adaptive optimization of feature sets for nominal\nor verbal predicates respectively is more likely to give a better performance than that for a mix of both. This observation is also confirmed by evidence in our experimental results: F1 x N and F1 x P scores are consistently higher than respective F1 x N+P ones in Table 6 above.\nBecause of the integrative nature of our approach, however, our priority has to be given to optimizing the whole feature set for both verbal and nominal predicates. It is nevertheless understood that all these point to potential ways to further enhance our system, e.g., by taking advantage of specialized feature sets for various kinds of words and/or utilizing some joint learning techniques such as syntactic-semantic reranking, in a way that the integrity of the system can be maintained properly.\nThe difference between the joint learning in the work of Johansson and Nugues (2008) and that of Titov et al. (2009) is worth noting. The former is a kind of cascade-style joint learning that first has a syntactic submodel to provide the n-best syntactic trees and a semantic submodel to infer correspondent semantic structures, and then a reranking model, with the log probabilities of the syntactic trees and semantic structures as its features, to find the best joint syntactic-semantic analysis, resulting in an improvement on top of individual submodels. In contrast to the former with a non-synchronous pipeline from syntactic to semantic parsing, the latter adopts a stricter all-in-one strategy of joint learning, where syntactic and semantic dependencies are learnt and decoded synchronously, based on an augmented version of the transition-based shift-reduce parsing strategy (Henderson et al., 2008). Regrettably, however, the performance of this approach is still far from the top of the ranked list in Table 8, indicating the particular significance of our current work.\nWhether it is worth integrating some form of joint-learning into an integrative system such as ours depends on the cost-effectiveness of doing so. It has been illustrated that such joint learning does lead to certain performance improvement, as in CoNLL shared task on SRL and successive works, e.g., by Johansson and Nugues (2008). However, a great deal of computational cost has to be paid in order to enable such a reranking procedure to handle multiple syntactic inputs. This certainly makes it impractical for real applications, not to mention that an integrative system is born with a particularly strong demand for integrity to preclude itself from accommodating such a stand-alone submodel."}, {"heading": "8. Conclusion", "text": "Semantic parsing, which aims to derive and instantiate the semantic structure of a sentence via identifying semantic relations between words, plays a critical role in deep processing of natural language. In this article, we have presented an integrative approach to semantic dependency parsing in the form of semantic role labeling, its implementation as an all-inone word pair classifier, and a comprehensive evaluation of it using three syntactic inputs of different quality. The evaluation results confirm the effectiveness and practicality of this approach. The major contributions of this research are the following. It exhibits a significant success for the first time that an integrative SRL system has achieved a performance next only to that of the best pipeline system, indicating the potentials of the integrative approach besides its practicality for real applications. The large-scale feature selection engineering underlying the success of this work also demonstrates (1) how the largest feature space ever in use in this field is formed by allowing a wide range of flexible (re)combinations of basic elements extracted from the known features and properties of input words and (2) how a speedy adaptive feature selection procedure is formulated and applied to select the most effective set of features from the allowable feature space.\nThe core techniques that have contributed to this success are developed based on the two types of traverse path, along syntactic tree branches vs. linear input word sequence. Both argument candidate pruning and feature selection are performed along an identical path. The strategy of using auxiliary labels to facilitate argument candidate pruning, following the observation that true arguments tend to be close to their predicates, works well with both traverse schemes. Interestingly, although the feature selection procedure outputs two very different feature sets for each of NomBank, PropBank and their combination whilst working along the two paths, both feature sets lead the SRL system to a very close performance on the same test data, a competitive performance on top of all but one best pipeline system, confirming the robustness and effectiveness of the feature selection procedure.\nEvidence is also presented in our evaluation results to reconfirm the finding in the previous works of semantic parsing that feature sets optimized specifically for verbal or nominal predicates outperform a collective one for both. However, the competitive performance of the collective one that we have arrived at also suggests that a harmonious rival feature set for both types of predicate as a whole is reachable and its slight performance difference from the specific sets is fairly acceptable as the unavoidable small cost for exchange for the higher integrity and practicality of an integrative SRL system. This competitiveness is attributable at least to two main factors. One is the very large feature space in use, which provides about a dozen times as many feature templates as those in the previous\nworks (e.g., see Xue & Palmer, 2004; Xue, 2006). The other is the ME classifier that can accommodate so many features in one model. According to our experience in this piece of work, the ME model is not vulnerable to the use of many overlapping features, from which SVM and other margin-based learners usually suffer a lot."}, {"heading": "Acknowledgments", "text": "The research reported in this article was partially supported by the Department of Chinese, Translation and Linguistics, City University of Hong Kong, through a post-doctorate research fellowship to the first author and a research grant (CTL UNFD-GRF-144611) to the third and corresponding author, the National Natural Science Foundation of China (Grants 60903119 and 61170114), the National Basic Research Program of China (Grant 2009CB320901), the National High-Tech Research Program of China (Grant 2008AA02Z315), the Research Grants Council of HKSAR, China (Grant CityU 144410), and the City University of Hong Kong (Grant 7002796). Special thanks are owed to Richard Johansson for kindly providing his syntactic output for the CoNLL-2008 shared task, to three anonymous reviewers for their insightful comments and to John S. Y. Lee for his help."}, {"heading": "Appendix A. Feature Templates and their Importance Rankings", "text": "Template Rank in: SsN+P S s N S s P p\u22121.pos + p.pos 2 37 79 p\u22121.spLemma 27 13 59 p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPos 7 45 63 a.isCurPred.lemma 83 94 75 a.isCurPred.spLemma 36 38 86 a:p|existCross 48 77 82 a:p|dpPath.dprel.bag 47 14 85 a:p|dpPathPred.spForm.bag 97 24 5 a:p|dpPath.spLemma.seq 67 59 71 a:p|linePath.spForm.bag 85 48 61 a.semdprel = A0 ? 50 86 40\na.rn.dprel + a.spPos 28 33 72 a\u22121.lemma + a.lemma 27 46 37 a:p|dpPathArgu.dprel.seq 3 96 1 a:p|dpPathArgu.pos.seq 75 79 9 a:p|dpPathPred.dprel.seq 12 64 35 a.form 53 94 78 a.form + a.pos 32 93 32 a.form + a1.form 94 31 38 a.spForm + a.spPos 16 73 48 a.spForm + a1.spForm 79 38 52 a.spLemma + a.dprel 43 118 7 a.spLemma + a.h.spForm 110 2 51\na1.form 54 a1.spForm 83 a1.spPos 33 (a:p|dpTreeRelation) + p.form 25 (a:p|dpTreeRelation) + p.spPos 29 (a:p|dpTreeRelation) + a.spPos 30 (a:p|dpPath.dprel.seq) + p.spForm 36 a\u22121.isCurPred.spLemma + a.isCurPred.spLemma 17 a.noFarChildren.spPos.bag + a.rm.spPos 21 a.children.spPos.seq + p.children.spPos.seq 34 a.highSupportNoun:p|dpPath.dprel.seq 89 (a.highSupportNoun:p|dpTreeRelation) + p.form 66 (a.highSupportVerb:p|dpTreeRelation) + a.spForm 72 (a.lowSupportVerb:p|dpTreeRelation) + a.spForm 42\na.lowSupportVerb:p|dpPathShared.spPos.seq 37 a.lowSupportVerb:p|dpPathPred.dprel.seq 38 a.lowSupportVerb:p|dpPathPred.spPos.seq 39 a.highSupportNoun:p|dpPath.dprel.seq 83 a.lowSupportVerb:p|dpPath.dprel.seq 30 (a.highSupportVerb:p|dpTreeRelation) + a.spPos 44\nTemplate Rank Template Rank p.rm.dprel 88 p.children.dprel.seq 27 p.lowSupportNoun.spForm 16 p.lowSupportProp:p|dpTreeRelation 72 p\u22121.form + p.form 103 p\u22121.lemma + p.lemma 91 p\u22121.pos+p.pos 32 p\u22121.spForm + p.spForm 40 p\u22121.spLemma 13 p\u22122.form + p\u22121.form 99 p\u22122.pos 18 p\u22122.spForm 39 p.dprel = OBJ ? 59 p.form + p.dprel 95 p.lemma + p.h.form 42 p.pos + p.dprel 1 p.spPos + p1.spPos 34 p1.spForm 86 a.voice + (a:p|direction) 75 a.isCurPred.lemma 43 a.isCurPred.spLemma 29 a.lm.dprel + a.dprel 98\nTemplate Rank Template Rank p.currentSense + a.spPos 69 p.rm.dprel 117 p.lm.form 101 p.lm.spForm 51 p.lowSupportNoun.spForm 99 p.lowSupportProp:p|dpTreeRelation 74 p\u22121.form + p.form 106 p\u22121.pos+p.pos 1 p\u22121.spForm + p.spForm 98 p\u22122.form + p\u22121.form 40 p\u22122.pos 87 p\u22122.spForm 54 p.form + p.dprel 114 p.spForm + p.dprel 115 p.spPos + p1.spPos 45 p1.spForm 37 p1.spPos 102 a.voice + (a:p|direction) 10 a.isCurPred.lemma 52 a.isCurPred.spLemma 66 a1.isCurPred.Lemma 41 a1.isCurPred.spLemma 64 a.children.dprel.bag 48 a.lm.dprel + a.dprel 70 a.lm\u22121.lemma 20 a.lm\u22121.spLemma 17 a.lm.Lemma 84 a.lm.pos + a.pos 8 a.lm.spForm 34 a.lm.spPos 59 a.ln.dprel + a.pos 63 a.rm1.spPos 111 a.lowSupportNoun:p|dpTreeRelation 93 a.lowSupportVerb.spLemma 15 a\u22121.lemma 81 a\u22121.spLemma+a.spLemma 31 a\u22121.spPos 109 a\u22121.spPos + a1.spPos 92"}], "references": [{"title": "Introduction to the CoNLL-2005 shared task: Semantic role labeling", "author": ["X. Carreras", "L. M\u00e0rquez"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras and M\u00e0rquez,? \\Q2005\\E", "shortCiteRegEx": "Carreras and M\u00e0rquez", "year": 2005}, {"title": "A cascaded syntactic and semantic dependency parsing system", "author": ["W. Che", "Z. Li", "Y. Hu", "Y. Li", "B. Qin", "T. Liu", "S. Li"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Che et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Che et al\\.", "year": 2008}, {"title": "A Gaussian prior for smoothing maximum entropy models", "author": ["S.F. Chen", "R. Rosenfeld"], "venue": "Technical report CMU-CS-99-108,", "citeRegEx": "Chen and Rosenfeld,? \\Q1999\\E", "shortCiteRegEx": "Chen and Rosenfeld", "year": 1999}, {"title": "Dependency parsing with short dependency relations in unlabeled data", "author": ["W. Chen", "D. Kawahara", "K. Uchimoto", "Y. Zhang", "H. Isahara"], "venue": "In Proceedings of the Third International Joint Conference on Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "DeSRL: A lineartime semantic role labeling system", "author": ["M. Ciaramita", "G. Attardi", "F. Dell\u2019Orletta", "M. Surdeanu"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Ciaramita et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2008}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer,? \\Q2003\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2003}, {"title": "The role of semantic roles in disambiguating verb senses", "author": ["H.T. Dang", "M. Palmer"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Dang and Palmer,? \\Q2005\\E", "shortCiteRegEx": "Dang and Palmer", "year": 2005}, {"title": "Improving Chinese semantic role classification with hierarchical feature selection strategy", "author": ["W. Ding", "B. Chang"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ding and Chang,? \\Q2008\\E", "shortCiteRegEx": "Ding and Chang", "year": 2008}, {"title": "Automatic labeling of semantic roles", "author": ["D. Gildea", "D. Jurafsky"], "venue": "Computational Linguistics,", "citeRegEx": "Gildea and Jurafsky,? \\Q2002\\E", "shortCiteRegEx": "Gildea and Jurafsky", "year": 2002}, {"title": "The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages", "author": ["J. Haji\u010d", "M. Ciaramita", "R. Johansson", "D. Kawahara", "M.A. Mart\u0301\u0131", "L. M\u00e0rquez", "A. Meyers", "J. Nivre", "S. Pad\u00f3", "J. \u0160t\u011bp\u00e1nek", "P. Stra\u0148\u00e1k", "M. Surdeanu", "N. Xue", "Y. Zhang"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Haji\u010d et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Haji\u010d et al\\.", "year": 2009}, {"title": "A latent variable model of synchronous parsing for syntactic and semantic dependencies", "author": ["J. Henderson", "P. Merlo", "G. Musillo", "I. Titov"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Henderson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2008}, {"title": "Semantic role labeling of NomBank: A maximum entropy approach", "author": ["Z.P. Jiang", "H.T. Ng"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Jiang and Ng,? \\Q2006\\E", "shortCiteRegEx": "Jiang and Ng", "year": 2006}, {"title": "Dependency-based syntactic\u2013semantic analysis with PropBank and NomBank", "author": ["R. Johansson", "P. Nugues"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Johansson and Nugues,? \\Q2008\\E", "shortCiteRegEx": "Johansson and Nugues", "year": 2008}, {"title": "Simple semi-supervised dependency parsing", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Generalized inference with multiple semantic role labeling systems", "author": ["P. Koomen", "V. Punyakanok", "D. Roth", "Yih", "W.-T"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning,", "citeRegEx": "Koomen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Koomen et al\\.", "year": 2005}, {"title": "Learning predictive structures for semantic role labeling of NomBank", "author": ["C. Liu", "H.T. Ng"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Liu and Ng,? \\Q2007\\E", "shortCiteRegEx": "Liu and Ng", "year": 2007}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "Computational Linguistics, Special Issue on Using Large Corpora: II,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "A robust combination strategy for semantic role labeling", "author": ["L. M\u00e0rquez", "M. Surdeanu", "P. Comas", "J. Turmo"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "M\u00e0rquez et al\\.,? \\Q2005\\E", "shortCiteRegEx": "M\u00e0rquez et al\\.", "year": 2005}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["R. McDonald", "F. Pereira"], "venue": "In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "McDonald and Pereira,? \\Q2006\\E", "shortCiteRegEx": "McDonald and Pereira", "year": 2006}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["R. McDonald", "F. Pereira", "K. Ribarov", "J. Haji\u010d"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "The NomBank project: An interim report", "author": ["A. Meyers", "R. Reeves", "C. Macleod", "R. Szekely", "V. Zielinska", "B. Young", "R. Grishman"], "venue": "Workshop: Frontiers in Corpus Annotation,", "citeRegEx": "Meyers et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Meyers et al\\.", "year": 2004}, {"title": "Jointly identifying predicates, arguments and senses using Markov logic", "author": ["I. Meza-Ruiz", "S. Riedel"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Meza.Ruiz and Riedel,? \\Q2009\\E", "shortCiteRegEx": "Meza.Ruiz and Riedel", "year": 2009}, {"title": "A numerical study of the limited memory BFGS method and truncated-Newton method for large scale optimization", "author": ["S.G. Nash", "J. Nocedal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nash and Nocedal,? \\Q1991\\E", "shortCiteRegEx": "Nash and Nocedal", "year": 1991}, {"title": "Integrating graph-based and transition-based dependency parsers", "author": ["J. Nivre", "R. McDonald"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Nivre and McDonald,? \\Q2008\\E", "shortCiteRegEx": "Nivre and McDonald", "year": 2008}, {"title": "Updating quasi-Newton matrices with limited storage", "author": ["J. Nocedal"], "venue": "Mathematics of Computation,", "citeRegEx": "Nocedal,? \\Q1980\\E", "shortCiteRegEx": "Nocedal", "year": 1980}, {"title": "The Proposition Bank: An annotated corpus of semantic roles", "author": ["M. Palmer", "D. Gildea", "P. Kingsbury"], "venue": "Computational Linguistics,", "citeRegEx": "Palmer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "Semantic role labeling using different syntactic views", "author": ["S. Pradhan", "W. Ward", "K. Hacioglu", "J. Martin", "D. Jurafsky"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Pradhan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "Shallow semantic parsing using support vector machines", "author": ["S.S. Pradhan", "W.H. Ward", "K. Hacioglu", "J.H. Martin", "D. Jurafsky"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Pradhan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2004}, {"title": "Semantic role labeling via integer linear programming inference", "author": ["V. Punyakanok", "D. Roth", "W. Yih", "D. Zimak"], "venue": "In Proceedings of the 20th International Conference on Computational Linguistics,", "citeRegEx": "Punyakanok et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2004}, {"title": "Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and coreference", "author": ["J. Pustejovsky", "A. Meyers", "M. Palmer", "M. Poesio"], "venue": "In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky,", "citeRegEx": "Pustejovsky et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2005}, {"title": "Improving the accuracy and efficiency of map inference for markov logic", "author": ["S. Riedel"], "venue": "In Proceedings of the Twenty-Fourth Conference Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Riedel,? \\Q2008\\E", "shortCiteRegEx": "Riedel", "year": 2008}, {"title": "A linear programming formulation for global inference in natural language tasks", "author": ["D. Roth", "W. Yih"], "venue": "In Proceedings of the Eighth Conference on Computational Natural Language Learning,", "citeRegEx": "Roth and Yih,? \\Q2004\\E", "shortCiteRegEx": "Roth and Yih", "year": 2004}, {"title": "The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies", "author": ["M. Surdeanu", "R. Johansson", "A. Meyers", "L. M\u00e0rquez", "J. Nivre"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Surdeanu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2008}, {"title": "Combination strategies for semantic role labeling", "author": ["M. Surdeanu", "L. Marquez", "X. Carreras", "P.R. Comas"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Surdeanu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2007}, {"title": "Online graph planarisation for synchronous parsing of semantic and syntactic dependencies", "author": ["I. Titov", "J. Henderson", "P. Merlo", "G. Musillo"], "venue": "In Proceedings of the 21st International Jont Conference on Artifical Intelligence,", "citeRegEx": "Titov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2009}, {"title": "Joint learning improves semantic role labeling", "author": ["K. Toutanova", "A. Haghighi", "C.D. Manning"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Toutanova et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2005}, {"title": "Semantic role labeling of nominalized predicates in Chinese", "author": ["N. Xue"], "venue": "In Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Main Conference,", "citeRegEx": "Xue,? \\Q2006\\E", "shortCiteRegEx": "Xue", "year": 2006}, {"title": "Calibrating features for semantic role labeling", "author": ["N. Xue", "M. Palmer"], "venue": "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Xue and Palmer,? \\Q2004\\E", "shortCiteRegEx": "Xue and Palmer", "year": 2004}, {"title": "Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies", "author": ["H. Zhao", "W. Chen", "J. Kazama", "K. Uchimoto", "K. Torisawa"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing", "author": ["H. Zhao", "W. Chen", "C. Kit", "G. Zhou"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "Parsing syntactic and semantic dependencies with two singlestage maximum entropy models", "author": ["H. Zhao", "C. Kit"], "venue": "In Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Zhao and Kit,? \\Q2008\\E", "shortCiteRegEx": "Zhao and Kit", "year": 2008}], "referenceMentions": [{"referenceID": 36, "context": "It is also reported by Xue and Palmer (2004) that different features do favor different subtasks of SRL, especially argument identification and classification.", "startOffset": 23, "endOffset": 45}, {"referenceID": 32, "context": "The idea of merging these two treebanks was put into practice for the CoNLL-2008 shared task (Surdeanu et al., 2008).", "startOffset": 93, "endOffset": 116}, {"referenceID": 30, "context": "They formulate the problem into a Markov Logic Network, with weights learnt via 1-best MIRA (Crammer & Singer, 2003) Online Learning method, and use Cutting Plane Inference (Riedel, 2008) with Integer Linear Programming (ILP) as the base solver for efficient joint inference of the best choice of predicates, frame types, arguments and role labels with maximal a posteriori probability.", "startOffset": 173, "endOffset": 187}, {"referenceID": 32, "context": "Note that when trained on CoNLL2008 training corpus, a subset of WSJ corpus, an SRL system has a performance at least 10 percentage points higher on the WSJ than on the Brown test set (Surdeanu et al., 2008).", "startOffset": 184, "endOffset": 207}, {"referenceID": 32, "context": "To conduct a valid and reliable evaluation, we will use the data set and evaluation settings of CoNLL-2008 and compare our integrated system, which is the best SRL system in CoNLL-2009 (Zhao, Chen, Kit, & Zhou, 2009), against the top systems in CoNLL shared tasks (Surdeanu et al., 2008; Haji\u010d, Ciaramita, Johansson, Kawahara, Mart\u0301\u0131, M\u00e0rquez, Meyers, Nivre, Pad\u00f3, \u0160t\u011bp\u00e1nek, Stra\u0148\u00e1k, Surdeanu, Xue, & Zhang, 2009).", "startOffset": 264, "endOffset": 413}, {"referenceID": 21, "context": "In fact, there have been few research efforts in this direction, except a recent one on joint identification of predicates, arguments and senses by Meza-Ruiz and Riedel (2009). They formulate the problem into a Markov Logic Network, with weights learnt via 1-best MIRA (Crammer & Singer, 2003) Online Learning method, and use Cutting Plane Inference (Riedel, 2008) with Integer Linear Programming (ILP) as the base solver for efficient joint inference of the best choice of predicates, frame types, arguments and role labels with maximal a posteriori probability.", "startOffset": 148, "endOffset": 176}, {"referenceID": 32, "context": "Although both use the same English corpus, except some more-sophisticated structures for the former (Surdeanu et al., 2008), their main difference is that semantic predicate identification is not required for the latter.", "startOffset": 100, "endOffset": 123}, {"referenceID": 40, "context": ", the Maximum Entropy (ME) model as used by Zhao and Kit (2008). The ME model is also used in this work but only for probability estimation to support the global decoding given below in Section 6, which extends our model beyond a sequential model.", "startOffset": 44, "endOffset": 64}, {"referenceID": 36, "context": "For the former (referred to as synPth hereafter), we use a dependency version of the pruning algorithm by Xue and Palmer (2004), which is given as follows with a necessary modification to allow a predicate itself also to be included in its own argument candidate list, because a nominal predicate sometimes takes itself as its own argument.", "startOffset": 106, "endOffset": 128}, {"referenceID": 35, "context": "This notion is widely adopted in the field (Toutanova et al., 2005; Xue, 2006; Jiang & Ng, 2006).", "startOffset": 43, "endOffset": 96}, {"referenceID": 36, "context": "This notion is widely adopted in the field (Toutanova et al., 2005; Xue, 2006; Jiang & Ng, 2006).", "startOffset": 43, "endOffset": 96}, {"referenceID": 10, "context": ", by Jiang and Ng (2006), and Ding and Chang (2008).", "startOffset": 5, "endOffset": 25}, {"referenceID": 7, "context": ", by Jiang and Ng (2006), and Ding and Chang (2008). The algorithm that we implemented for this purpose is presented in Algorithm 1 below, which imposes fewer assumptions than those in previous works, aiming at a higher efficiency.", "startOffset": 30, "endOffset": 52}, {"referenceID": 7, "context": "Given n candidate feature templates, the algorithm by Ding and Chang (2008) requires O(n2) time to execute a training/test routine, whereas the one by Jiang and Ng (2006) requires O(n) time, assuming that the initial set of feature templates is \u201cgood\u201d enough and the others can be handled in a strictly incremental way.", "startOffset": 54, "endOffset": 76}, {"referenceID": 7, "context": "Given n candidate feature templates, the algorithm by Ding and Chang (2008) requires O(n2) time to execute a training/test routine, whereas the one by Jiang and Ng (2006) requires O(n) time, assuming that the initial set of feature templates is \u201cgood\u201d enough and the others can be handled in a strictly incremental way.", "startOffset": 54, "endOffset": 171}, {"referenceID": 28, "context": "The main differences of our work from Punyakanok et al. (2004) are that (1) they use ILP for joint inference, which is exact, and we use beam search, which is greedy and approximate, and (2) the constraints (e.", "startOffset": 38, "endOffset": 63}, {"referenceID": 24, "context": "ME model adopts a tunable Gaussian prior (Chen & Rosenfeld, 1999) to estimate p(ai|Ai) and applies the L-BFGS algorithm (Nocedal, 1980; Nash & Nocedal, 1991) for parameter optimization.", "startOffset": 120, "endOffset": 157}, {"referenceID": 32, "context": "The evaluation of our SRL approach is conducted with various feature template sets on the official training/development/test corpora of CoNLL-2008 (Surdeanu et al., 2008).", "startOffset": 147, "endOffset": 170}, {"referenceID": 12, "context": "One is the gold standard syntactic input available from the official data set and the other is the parsing results of the same data set by two state-of-the-art syntactic parsers, namely, the MSTparser9 (McDonald, Pereira, Ribarov, & Haji\u010d, 2005; McDonald & Pereira, 2006) and the parser of Johansson and Nugues (2008). However, instead of using the original MSTparser, we have it substantially enriched with additional features, following Chen, Kawahara, Uchimoto, Zhang, and Isahara (2008), Koo, Carreras, and Collins (2008), and Nivre and McDonald (2008).", "startOffset": 290, "endOffset": 318}, {"referenceID": 12, "context": "One is the gold standard syntactic input available from the official data set and the other is the parsing results of the same data set by two state-of-the-art syntactic parsers, namely, the MSTparser9 (McDonald, Pereira, Ribarov, & Haji\u010d, 2005; McDonald & Pereira, 2006) and the parser of Johansson and Nugues (2008). However, instead of using the original MSTparser, we have it substantially enriched with additional features, following Chen, Kawahara, Uchimoto, Zhang, and Isahara (2008), Koo, Carreras, and Collins (2008), and Nivre and McDonald (2008).", "startOffset": 290, "endOffset": 491}, {"referenceID": 12, "context": "One is the gold standard syntactic input available from the official data set and the other is the parsing results of the same data set by two state-of-the-art syntactic parsers, namely, the MSTparser9 (McDonald, Pereira, Ribarov, & Haji\u010d, 2005; McDonald & Pereira, 2006) and the parser of Johansson and Nugues (2008). However, instead of using the original MSTparser, we have it substantially enriched with additional features, following Chen, Kawahara, Uchimoto, Zhang, and Isahara (2008), Koo, Carreras, and Collins (2008), and Nivre and McDonald (2008).", "startOffset": 290, "endOffset": 526}, {"referenceID": 12, "context": "One is the gold standard syntactic input available from the official data set and the other is the parsing results of the same data set by two state-of-the-art syntactic parsers, namely, the MSTparser9 (McDonald, Pereira, Ribarov, & Haji\u010d, 2005; McDonald & Pereira, 2006) and the parser of Johansson and Nugues (2008). However, instead of using the original MSTparser, we have it substantially enriched with additional features, following Chen, Kawahara, Uchimoto, Zhang, and Isahara (2008), Koo, Carreras, and Collins (2008), and Nivre and McDonald (2008). The latter one, henceforth referred to as J&N for short, is a second-order graph-based dependency parser that takes advantage of pseudo-projective techniques and resorts to syntactic-semantic reranking for further refining its final outputs.", "startOffset": 290, "endOffset": 557}, {"referenceID": 36, "context": "The original pruning as in Xue and Palmer (2004), not using noMoreArg.", "startOffset": 27, "endOffset": 49}, {"referenceID": 1, "context": "Contrary to the best three systems in CoNLL-2008 (Johansson & Nugues, 2008; Ciaramita, Attardi, Dell\u2019Orletta, & Surdeanu, 2008; Che et al., 2008) that use SRL pipelines, our current work is intended to integrate them into one.", "startOffset": 49, "endOffset": 145}, {"referenceID": 11, "context": "025) than all the others except the post-evaluation result of Johansson and Nugues (2008). Contrary to the best three systems in CoNLL-2008 (Johansson & Nugues, 2008; Ciaramita, Attardi, Dell\u2019Orletta, & Surdeanu, 2008; Che et al.", "startOffset": 62, "endOffset": 90}, {"referenceID": 1, "context": "Contrary to the best three systems in CoNLL-2008 (Johansson & Nugues, 2008; Ciaramita, Attardi, Dell\u2019Orletta, & Surdeanu, 2008; Che et al., 2008) that use SRL pipelines, our current work is intended to integrate them into one. Another baseline, namely, our current model using the feature set from the work of Zhao and Kit (2008), instead of a random set, is also included in the table for comparison, showing a significant performance enhancement on top of the previous model and, then, a further enhancement by the greedy feature selection.", "startOffset": 128, "endOffset": 330}, {"referenceID": 12, "context": "The fact that our integrated system is yet to give a performance on a par with the postevaluation result of Johansson and Nugues (2008) seems attributable to a number of factors, including the ad hoc features adopted in their work to handle linguistic constructions such as raising/control and coordination.", "startOffset": 108, "endOffset": 136}, {"referenceID": 10, "context": "In contrast to the former with a non-synchronous pipeline from syntactic to semantic parsing, the latter adopts a stricter all-in-one strategy of joint learning, where syntactic and semantic dependencies are learnt and decoded synchronously, based on an augmented version of the transition-based shift-reduce parsing strategy (Henderson et al., 2008).", "startOffset": 326, "endOffset": 350}, {"referenceID": 11, "context": "The difference between the joint learning in the work of Johansson and Nugues (2008) and that of Titov et al.", "startOffset": 57, "endOffset": 85}, {"referenceID": 11, "context": "The difference between the joint learning in the work of Johansson and Nugues (2008) and that of Titov et al. (2009) is worth noting.", "startOffset": 57, "endOffset": 117}, {"referenceID": 12, "context": ", by Johansson and Nugues (2008). However, a great deal of computational cost has to be paid in order to enable such a reranking procedure to handle multiple syntactic inputs.", "startOffset": 5, "endOffset": 33}, {"referenceID": 36, "context": "works (e.g., see Xue & Palmer, 2004; Xue, 2006).", "startOffset": 6, "endOffset": 47}], "year": 2013, "abstractText": "Semantic parsing, i.e., the automatic derivation of meaning representation such as an instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language. Unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask, the one presented in this article integrates everything into one model, in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance. This integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier. We leverage adaptive pruning of argument candidates and large-scale feature selection engineering to allow the largest feature space ever in use so far in this field, it achieves a state-of-the-art performance on the evaluation data set for CoNLL-2008 shared task, on top of all but one top pipeline system, confirming its feasibility and effectiveness.", "creator": "dvips(k) 5.99 Copyright 2010 Radical Eye Software"}}}