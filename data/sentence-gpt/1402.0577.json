{"id": "1402.0577", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "A Survey on Latent Tree Models and Applications", "abstract": "In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships and patterns of reasoning and cognition and predict other variables that affect our behavior.\n\n\n\n\nFor example, this tree was constructed by creating a matrix of variables that represent a set of variables, and each variable is assigned to a set of variables that contain more than a certain threshold. The linear model is shown to have no influence on the accuracy of the model, while the linear model (the linear model) produces some information that can be extrapolated from the model, such as the values of the linear model and the variables' expected response.\nBy using the latent tree model, the tree also contains information that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can be extrapolated from the model and the data that can", "histories": [["v1", "Tue, 4 Feb 2014 01:40:28 GMT  (1125kb)", "http://arxiv.org/abs/1402.0577v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rapha\\\"el mourad", "christine sinoquet", "nevin l zhang", "tengfei liu", "philippe leray"], "accepted": false, "id": "1402.0577"}, "pdf": {"name": "1402.0577.pdf", "metadata": {"source": "CRF", "title": "A Survey on Latent Tree Models and Applications", "authors": ["Rapha\u00ebl Mourad", "Christine Sinoquet", "Nevin L. Zhang", "Tengfei Liu", "Philippe Leray"], "emails": ["raphael.mourad@aliceadsl.fr", "christine.sinoquet@univ-nantes.fr", "lzhang@cse.ust.hk", "liutf@cse.ust.hk", "philippe.leray@univ-nantes.fr"], "sections": [{"heading": "1. Introduction", "text": "In statistics, latent variables (LVs), as opposed to observed variables (OVs), are random variables which are not directly measured. A wide range of statistical models, called latent variable models, relate a set of OVs to a set of LVs. In these models, LVs explain dependences among OVs and hence offer compact and intelligible insights of data. Moreover LVs allow to reduce data dimensionality and to generate conditionally independent variables, which considerably simplifies downstream analysis. Applications are numerous and cover many scientific fields. This is typically the case in domains such as psychology, sociology, economics, but also biological sciences and artificial intelligence, to cite some examples. Such fields may need complex constructs that cannot be observed directly. For instance, human personality in psychology and social class in socio-economics refer to higher level abstractions than observed reality.\nc\u00a92013 AI Access Foundation. All rights reserved."}, {"heading": "1.1 Context", "text": "Latent tree model (LTM)1 is a class of latent variable models which has received considerable attention. LTM is a probabilistic tree-structured graphical model where leaf nodes are observed while internal nodes can be either observed or latent. This model is appealing since its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships.\nA subclass of LTMs was first developed in the phylogenetic community (Felsenstein, 2003). In this context, leaf nodes are observed taxa while internal nodes represent unobserved taxum ancestors. For instance, in molecular genetic evolution, which is the process of evolution at DNA scale, it is obviously hopeless to study DNA sequences in some dead species from collecting their DNA. Nevertheless, evolutionary latent models can be used to infer the most probable ancestral sequences knowing contemporary living species sequences. Neighbor joining represents one of the first algorithms developed for LTM learning in the phylogenetic context (Saitou & Nei, 1987; Gascuel & Steel, 2006). It is still very popular as it is quick to compute and it allows to find the optimal model in polynomial time under certain assumptions.\nDuring the past decade, LTMs in their general form have been under extensive investigation and have been applied to many fields. For instance they have been applied in human interaction recognition. Human interaction recognition is a challenging task, because of multiple body parts and concomitant inclusions (Aggarwal & Cai, 1999). For this purpose, the use of LTM allows to segment the interaction in a multi-level fashion (Park & Aggarwal, 2003): body part positions are estimated through low-level LVs, while overall body position is estimated by a high-level LV. LTMs have also been used in medical diagnosis (Zhang, Yuan, Chen, & Wang, 2008). In this context, LTMs provide a way to identify, through the LVs, the different syndrome factors which cannot be directly observed by the physician."}, {"heading": "1.2 Contributions", "text": "In this paper, we present a comprehensive study of LTM and a broad-brush view of its recent theoretical and methodological developments. LTM must be paid attention because (i) it offers deep insights for latent structure discovery (Saitou & Nei, 1987), (ii) it can be applied to multidimensional clustering (Chen, Zhang, Liu, Poon, & Wang, 2012) and (iii) it allows efficient probabilistic inference (Wang, Zhang, & Chen, 2008). Somewhat surprisingly, no extensive review on this research area has been published.\nIn addition to the reviewing of the LTM research area, we also contribute an analysis and a perspective that advance our understanding of the subject. We establish a categorization of learning methods. We present generic learning algorithms implementing fundamental principles. These generic algorithms are partly different from those of the literature because they have been adapted to a broader context. Besides, the performances of all the algorithms of the literature are compared in the context of small, large and very large simulated and real datasets. Finally, we discuss future directions, such as the adaptation of LTM for continuous data.\n1. LTM has been previously called \u201chierarchical latent class model\u201d (Zhang, 2004), but this name has been discarded because the model does not inherently reveal a hierarchy."}, {"heading": "1.3 Paper Organization", "text": "This paper is organized as follows. Section 2 presents the latent tree model and related theoretical developments. In Section 3, we review methods developed to learn latent tree models for the two main situations: learning when structure is known and learning when it is not the case. Then, Section 4 presents and details three types of applications of latent tree models: latent structure discovery, multidimensional clustering and probabilistic inference. Other applications such as classification are also discussed. Finally, the last two sections 5 and 6 conclude and point out future directions."}, {"heading": "2. Theory", "text": "In this section, we first introduce graph terminology and then present LTM. Latent classes and probabilistic inference for clustering are next presented. Scoring LTMs is discussed. We also present the concepts of marginal equivalence, equivalence and model parsimony, useful for LTM learning. Then, we explain the necessity of a trade-off between latent variable complexity and partial structure complexity.\nVariables are denoted by capital letters, e.g. A, B and C, whereas lower-case letters refer to values that variables can take, e.g. a, b and c. Bold-face letters represent sets of objects, that is A, B and C are sets of variables while a, b and c are value sets. An observed variable is denoted X whereas a latent variable is denoted H. A variable about which we do not know if it is observed or latent is denoted V ."}, {"heading": "2.1 Graph Theory Terminology", "text": "Before presenting LTM, we first need to define graph-related terms, which are illustrated in Figure 1. A graph G(V,E) is composed of a set of nodes V and a set of edges E \u2282 V\u00d7V. An edge is a pair of nodes (Va, Vb) \u2208 E. The edge is undirected (noted Va\u2212Vb) if (Vb, Va) \u2208 E and directed (noted Va \u2192 Vb) if not.\nA directed graph is a graph whose all edges are directed. In a directed graph, a node Va is a parent of a node Vb if and only if there exists an edge from Va to Vb. The node Vb is then called the child of node Va. Nodes are siblings if they share the same parent. A node Vc is a root if it has no parent. A directed path from a node Vd to a node Va is a sequence of nodes such that for each node except the last one, there is an edge to the next node in the sequence. A node Va is a descendant of a node Vd if and only if there is a directed path from Vd to Va. The node Vd is then called an ancestor of node Va.\nAn undirected graph only contains undirected edges. In an undirected graph, a node Va is a neighbor of another node Vb if and only if there is an edge between them. A leaf is a node having only one neighbor. An internal node is a node having at least two neighbors. An undirected path is a path for which the edges are not all oriented in the same direction.\nA clique is a set of pairwise connected nodes (in a tree, a clique is simply an edge). A separator is a set of nodes whose removal disconnect two or more cliques (in a tree, a separator is simply an internal node). A tree T is a graph for which any two nodes are connected by exactly one path."}, {"heading": "2.2 Latent Tree Model", "text": "LTM is a tree-structured graphical model with latent variables. It is composed of a tree - the structure - T (V,E), and a set of parameters, \u03b8. The tree can be either directed (i.e. a Bayesian network; Zhang, 2004) or undirected (i.e. a Markov random field; Choi, Tan, Anandkumar, andWillsky, 2011). Both representations are described in Figure 2. The set of nodes V = {V1, ..., Vn+m} represents n+m observed and latent variables. X = {X1, ...,Xn} is the set of observed variables and H = {H1, ...,Hm} is the set of latent variables. Leaf nodes are OVs while internal nodes can be either observed or latent. Variables can be either discrete or continuous. The set of k edges E = {E1, ..., Ek} captures the direct dependences between these variables.\nIn the directed setting (Figure 2a), the set of parameters \u03b8 consists of probability distributions, one for each variable. Given a variable Vi with parents PaVi , a conditional distribution P (Vi|PaVi) is defined. For a variable Vi that has no parent, a marginal distribution P (Vi) is defined instead. The joint probability distribution (JPD) of this model is\nformulated as:\nP (V) = \u03a0n+mi=1 P (Vi|PaVi). (1)\nTo illustrate the model, let us take the example in Figure 2a. It is composed of a set of OVs {X1, ...,X7} and a set of LVs {H1,H2}. Its JPD writes as:\nP (X1, ...,X7,H1,H2) = P (X1|H1) P (X2|H1) P (X3|H1) P (H1|X7) P (X7) \u00d7 P (X4|X7) P (H2|X7) P (X5|H2) P (X6|H2).\n(2)\nIn the undirected setting (Figure 2b), the set of parameters \u03b8 consists of probability distributions, one for each clique and separator. In LTM, cliques are edges while separators are internal nodes. Let {I1, ..., Ij} be the separators. The JPD of this model is formulated as:\nP (V) = \u03a0(Va,Vb)\u2208E P (Va, Vb)\n\u03a0mj=1 P (Ij) (d(Ij )\u22121)\n, (3)\nwhere d(Ij) is the degree of internal node Ij. The JPD of the undirected model in Figure 2b writes as:\nP (X1, ...,X7,H1,H2) = P (X1,H1) P (X2,H1) P (X3,H1) P (H1,X7) P (X4,X7)\nP (H1) P (H1) P (H1) P (X7)\n\u00d7 P (X7,H2) P (X5,H2) P (X6,H2) P (X7) P (H2) P (H2) .\n(4)\nIn the following, for the seek of simplicity, we restrain the study to categorical variables, i.e. random variables which have a finite number of states. We also mainly focus on LTM whose internal nodes are all LVs. Most works on LTM have been developed for these two model settings."}, {"heading": "2.3 Latent Classes and Clustering", "text": "An LV has a number of states, each of them representing a latent class. All latent classes together represent a soft partition of the data and define a finite mixture model (FMM). LTM can be seen as multiple FMMs connected to form a tree. Given a data point, the probability of belonging to a particular class can be computed using the Bayes formula. This computation is called class assignment.\nIn an LTM, each LV Hj represents a partition of the data. For an observation ` and the vector of its values x` = {x`1, ..., x`n} on the set of OVs X, the probability of its membership to a class c of an LV Hj can be computed as follows:\nP (Hj = c|x`) = P (x`|Hj = c) P (Hj = c)\nP (x`)\n=\n\u2211\nH \u2032 P (x`,H\u2032|Hj = c) P (Hj = c)\n\u2211k c=1 \u2211 H \u2032 P (x`,H\u2032|Hj = c) P (Hj = c)\n(5)\nwith H\u2032 = H\\{Hj} and k the cardinality of Hj. From the last formula, the reader might get the impression that the complexity of class assignment is exponential. However, in trees,\nlinear2 - thus efficient - probabilistic inference, using message passing (Kim & Pearl, 1983), can be used to compute P (Hj = c|x`).\nProbabilistic inference of LV values has two applications: clustering and latent data imputation. Clustering using LTMs will be illustrated and seen in detail in Section 4.2. Latent data imputation is the process of inferring, for each observation `, the values of LVs. These variables are called imputed LVs. In Section 3.2.2, we will see that latent data imputation is at the basis of fast variable clustering-based LTM learning methods."}, {"heading": "2.4 Scoring Latent Tree Models", "text": "In theory, every score, such as Akaike information criterion (AIC) (Akaike, 1970) and Bayesian information criterion (BIC) (Schwartz, 1978), could be used for scoring LTMs. In practice, the BIC score is often used for LTMs. Let consider a set of n OVs X = {X1, ...,Xn} and a collection of N identical and independently distributed (i.i.d.) observations Dx = {x1, ...,xN}. BIC is composed of two terms:\nBIC(T,Dx) = log P (Dx|\u03b8ML, T )\u2212 1\n2 dim(T ) log N, (6)\nwith \u03b8ML the maximum likelihood parameters, dim(T ) the model dimension and N the number of observations. The first term evaluates the fit of the model to the data. It is computed through probabilistic inference of P (x`) for each observation `. The second term of the score penalizes the model according to its dimension, to prevent overfitting.\nIn models without latent variables, the dimension is simply calculated as the number of free parameters. This is sometimes called standard dimension. When LVs are present, standard dimension is no longer an appropriate measure of model complexity, and effective dimension should be used instead (Geiger, Heckerman, & Meek, 1996). Effective dimension is computed as the rank of the Jacobian matrix of the mapping from the model parameters to the OV joint distribution."}, {"heading": "2.5 Model Parsimony", "text": "Let us consider two LTMs, M = (T, \u03b8) and M\u2032 = (T \u2032, \u03b8\u2032), built on the same set of n OVs, X = {X1, ...,Xn}. We say thatM andM\u2032 are marginally equivalent if their joint distributions on OVs are equal:\nP (X1, ...,Xn|T, \u03b8) = P (X1, ...,Xn|T \u2032, \u03b8\u2032). (7)\nIf two marginally equivalent models have the same dimension, they are equivalent models.\nA model M is parsimonious3 if there does not exist another model M\u2032 that is marginally equivalent and has a smaller dimension. A parsimonious model has the best possible score. It does not contain any redundant LVs or any redundant latent classes. It represents the model to infer from data. Two conditions ensure that an LTM does not include any redundant LVs (Pearl, 1988):\n2. Actually, in trees, inference is linear with the number of edges |E|, and is thus also linear with the number n+m of observed and latent variables, because |E| \u2264 n+m\u2212 1. 3. The notion of parsimony is also called minimality by Pearl (1988).\n\u2212 An LV must have at least three (observed or latent) neighbors. If it has only two neighbors, it can simply be replaced by a direct link between the two.\n\u2212 Any two variables connected by an edge in the LTM are neither perfectly dependent nor independent.\nThere is also a condition ensuring that an LTM does not include any redundant latent classes. Let H be an LV in an LTM. The set of k variables Z = {Z1, ..., Zk} are the neighbors of H. An LTM is regular (Zhang, 2004) if for any LV H:\n|H| \u2264 \u03a0 k i=1|Zi|\nmaxki=1 |Zi| . (8)\nZhang (2004) showed that all parsimonious models are necessarily regular. Thus the model search can be restricted to the space of regular models. Zhang also demonstrated that the space of regular models is upper bounded by 23n 2 , where n is the number of OVs."}, {"heading": "2.6 Trade-off between Latent Variable Complexity and Partial Structure Complexity", "text": "Zhang and Kocka (2004b) distinguished two kinds of model complexity in LTM: latent variable complexity refers to LV cardinalities while partial structure complexity4 is\n4. In their paper, Zhang and Kocka (2004b) called it structure complexity. For a better understanding, we prefer to make the distinction between (complete) structure which includes LV cardinalities and partial structure which does not.\nthe edges and number of LVs in the graph. The trade-off between these two complexities has an important role to play when one wants to choose a model. This trade-off is illustrated in Figure 3. For instance, let us consider a latent class model (i.e. a model with only one LV, abbreviated LCM) versus an LTM having the same marginal likelihood (marginally equivalent models). The LCM is the model showing the highest LV complexity and the lowest partial structure complexity. It might have a low score if local dependences are present between OVs. At the opposite, the model with a low LV complexity and a high partial structure complexity, a binary tree with binary LVs, would also have a low score, because some LVs would be unnecessary. Depending on the application, a model showing a good trade-off between the two complexities should be preferred, because it would present a better score and might be easier to interpret."}, {"heading": "3. Statistical Learning", "text": "In this section, we present generic algorithms implementing fundamental principles for learning LTMs. These algorithms are partly different from those proposed in the literature, because they have been adapted to a broader context. Moreover we provide a unified presentation of algorithms, in the context of this survey. When learning a model from data, two main situations have to be distinguished: when structure is known and only parameters have to be learned, and the more complicated situation where both are unknown."}, {"heading": "3.1 Known Structure", "text": "In the simplest situation, structure is known, i.e. not only the dependences between variables but also the number of LVs and their respective cardinalities (i.e. the numbers of latent classes). The problem is to estimate probability parameters. To solve the problem, one can use expectation-maximization (EM), the most popular algorithm for learning parameters in the face of LVs (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995). Because EM leads to computational burden for large LTMs, a more efficient procedure, that we call LCM-based EM, can be used. Other methods different from EM, such as spectral techniques, have also been developed."}, {"heading": "3.1.1 Expectation-maximization", "text": "Ideally, when learning parameters, we would like to maximize the log-likelihood for a set of N i.i.d. observed data Dx = {x1, ...,xN}:\nL(\u03b8;Dx) = log P (Dx|\u03b8) = log \u2211\nH\nP (Dx,H|\u03b8). (9)\nHowever, directly maximizing L(\u03b8;Dx) in Equation (9) is often intractable because it involves the logarithm of a (large) sum. To overcome the difficulty, EM implements an iterative approach. At each iteration, it optimizes instead the following expected log-likelihood conditional on current parameters \u03b8t:\nQ(\u03b8; \u03b8t) = EDh|Dx,\u03b8t [log P (Dx,Dh|\u03b8)] (10) where Dx is completed by the missing data Dh = {h1, ...,hN} inferred using \u03b8t. Note that by completing the missing data, EM can easily deal with partially observed variables. An\nAlgorithm 1 LCM-based EM parameter learning (LCMB-EM, adapted from Harmeling and Williams, 2011)"}, {"heading": "INPUT:", "text": "T , the tree structure of the LTM."}, {"heading": "OUTPUT:", "text": "\u03b8, the parameters of the LTM.\n1: T \u2032 \u2190 graph rooting(T ) /* choose an LV as a root of T */ 2: Ho \u2190 \u2205 /* initialization of the set of imputed latent variables */ 3: \u03b8\u2032 \u2190 \u2205 4: loop 5: TLCM = {TLCM1 , ..., TLCMk} \u2190 identify LCMs of graph(T \u2032,Ho) 6: \u0398LCM = {\u03b8LCM1 , ..., \u03b8LCMk} \u2190 EM(TLCM) 7: if |TLCM| > 1 then 8: \u03b8\u2032 \u2190 \u03b8\u2032 \u222a children parameters(\u0398LCM) 9: else\n10: \u03b8\u2032 \u2190 \u03b8\u2032 \u222a children and parent parameters(\u0398LCM) 11: break 12: end if 13: Ho \u2190 Ho \u222a impute LV data(\u0398LCM) /* now parents are observed */ 14: end loop 15: \u03b8 \u2190 EM(\u03b8\u2032) /* global EM using \u03b8\u2032 as a starting point */\nimportant drawback of EM is that it does not guarantee to reach the global optimum. To reduce the probability of getting trapped into a local maximum, random restarts (multiple starts with different random initial parameters) or simulated annealing represent well-used solutions. Wang and Zhang (2006) showed that a few random restarts suffice when the LTM is small and variables are strongly dependent with each other. In Supplemental material B.1, we also present our experiments on random restarts for EM. It appears that it is not possible to give a simple answer to how many restarts should be used because it depends on the model. Besides, convergence is sometimes not reached even after a very large number of restarts."}, {"heading": "3.1.2 LCM-based EM", "text": "Although inference is linear in LTMs, running EM might be prohibitive. One solution to speed up EM computations consists in chaining two steps: a first step of divide-andconquer strategy through local - LCM - learning, followed by a final step carrying out global learning. This LCM-based EM (LCMB-EM) parameter learning method5 is presented in Algorithm 1 and illustrated in Figure 4. In the first step, parameters are locally learned through a bottom-up LCM-based learning procedure explained as follows. An LV is first\n5. This learning procedure is very similar to the one proposed for binary trees by Harmeling and Williams (2011).\nchosen as a root of the LTM (line 1; note that this point will be discussed in detail in Section 3.2.5). In the rooted LTM, every LCM {X,H} is identified (line 5), i.e. every LV H whose all children are OVs X. Then, LCM parameters can be quickly learned through EM (line 6) and used to update current LTM parameters (lines 7 to 12). Once parameters have been learned for such an LCM, the distributions of unknown values of LV H can be probabilistically inferred for each observation6 (line 13; for more details, see Section 2.3, paragraph 2). These distributions can then be used as weighted observations. In their turn, these weighted observations will seed the learning processes for the LCMs on the (now observed) LVs Ho and remaining OVs which have not been used for LCM learning yet. Iterating the two operations (LCM learning and latent data imputation) leads to a bottomup LCM-based EM procedure enabling local and fast parameter learning of LTMs. Finally, LTM parameters are refined through global EM using as starting point the locally learned parameters (line 15). As with EM, we also provide the results of experiments evaluating the efficiency of random restarts (see Supplemental material B.1). Our results show that LCMB-EM converges better than EM. However convergence is not achieved for the largest dataset studied."}, {"heading": "3.1.3 Spectral Methods", "text": "Recently, Parikh et al. (2011) applied spectral techniques to LTM parameter learning. The method directly estimates the joint distribution of OVs without explicitly recovering the\n6. We recall that this process is named latent data imputation.\nLTM parameters. Their algorithm is useful when LTM parameters are not required, for instance, for probabilistic inference over OVs only. The work of Parikh et al. alleviates the restriction of the approach of Mossel et al. (2006) requiring that all conditional probability tables should be invertible, and generalizes the method of Hsu et al. (2009) specific to hidden Markov models.\nParikh et al. (2011) reformulated the message passing algorithm using an algebraic formulation:\nP (x) = \u2211\nH\n\u03a0n+mi=1 P (vi|Pavi)\n= r>(Mj1Mj2 ... MjJ1r), (11)\nwhere x = {x1, ..., xn} is an observation, Xr is a root, r is the marginal probability vector of the root and Mj1 ,Mj2 , ...,MjJ are the incoming message matrices from the root\u2019s children. Children message matrices are calculated in a similar manner:\nMi = Ti\u00d7\u03041(Mj1Mj2 ... MjJ1i), (12)\nwhere Xi is a root child, Mi is the outgoing message matrix from Xi, Ti is a third order tensor related to the conditional probability matrix between Xi and PaXi (i.e. Xr), Mj1 ,Mj2 , ...,MjJ are the Xi\u2019s children incoming messages and \u00d7\u03041 is the mode-1 vector product. Such as in the original message passing algorithm, all messages are recursively calculated starting from the leaves and going up to the root.\nThe drawback of the previous representation is that message passing still needs model parameters. To tackle this issue, the key is to recover P (x) using invertible transformations. Message matrices are then calculated by transforming each message Mj by two invertible matrices Lj and Rj (LjR \u22121 j = I):\nMi = Ti\u00d7\u03041(Lj1L\u22121j1 Mj1Rj1L \u22121 j2 Mj2Rj2 ... L \u22121 jJ MjJRjJR \u22121 jJ 1i). (13)\nThe matrices Lj, Mj and Rj can be recovered from singular vectors Uj of the empirical probability matrices P (X\u03bbj ,Xj) of OVs Xj and their left neighbor OV X\u03bbj . This leads to a very efficient computation of message passing only involving a sequence of singular value decompositions of empirical pairwise joint probability matrices. We refer to the work of Parikh et al. (2011) for more details about the singular value decomposition and spectral algorithm. Compared to EM, this spectral method does not entail the problem of getting trapped in local maxima. Moreover, it performs comparable to or better than EM while being orders of magnitude faster."}, {"heading": "3.1.4 Other Methods", "text": "Other methods exist for parameter learning. Gradient descent (Kwoh & Gillies, 1996; Binder, Koller, Russel, & Kanazawa, 1997) and variations of the Gauss-Newton method (Xu & Jordan, 1996) help accelerate the sometimes slow convergence of EM. However, they require the evaluation of first and/or second derivatives of the likelihood function. For a Bayesian learning, variational Bayes (Attias, 1999) offers a counterpart of EM."}, {"heading": "3.2 Unknown Structure", "text": "Regrettably, most of the time, there is no a priori information on the LTM structure. This compels to learn every part of the model, i.e. the number of LVs, their cardinalities, the dependences and the parameters. This learning task represents a challenging issue, for which various methods have been conceived. In this section, we provide a survey of those algorithms. The determination of LV cardinalities, as well as the time complexity and scalability of algorithms are also discussed. We end by establishing a summary relative to these learning methods.\nStructure learning approaches fall into three categories. The first one is comprised of search-based methods, inspired from standard Bayesian network learning. The second one is based on variable clustering and is related to hierarchical procedures. The last category relies on the notion of distances and comes from phylogenetics."}, {"heading": "3.2.1 Search-based Methods", "text": "Search-based methods aim at finding the model that is optimal according to some scoring metric. For BNs without LVs, the BIC score is often used. In the context of LTM, BIC suffers from a theoretical shortcoming as pointed out in Section 2.4. However, empirical results indicate that the shortcoming does not seem to compromise model quality in practice (Zhang & Kocka, 2004a). So, researchers still use BIC when it comes to learning LTM. Many search procedures have been proposed. They all explore the space of regular LTMs. Here we focus on: (i) the most naive one which is conceptually simple but very computationally expensive and (ii) the most advanced one which reduces the search space and implements fast parameter learning through local EM."}, {"heading": "Naive Greedy Search", "text": "Naive greedy search (NGS) consists in starting from an LCM and then visiting the space of regular LTM partial structures. The neighborhood of the current model is explored by greedy search through operations such as addition or removal of a latent node, and node relocation7. For each partial structure neighbor, the cardinalities of all LVs are optimized through addition or dismissal of a state relative to an LV. During the model search (partial structure and LV cardinality), candidate models are learned with EM and evaluated through a score. If the best candidate model shows a score superior to the current model score, then the former is used as a seed for the next step. Otherwise, NGS stops and the current model is considered as the best model. Therefore, at each step of the search, the learning approach necessitates to evaluate the score of a very large number of candidate models. This leads to a huge computational burden because, for each candidate model evaluation, the likelihood has to be computed through EM.\n7. Node relocation picks a child of an LV and then grafts it as a child of another LV which is connected to the former LV."}, {"heading": "Advanced Greedy Search", "text": "Advanced greedy search (AGS) relies on three strategies to reduce the complexity. Advanced greedy search is presented in Algorithm 2. First, AGS focuses on a smaller space of models to explore than NGS (Zhang & Kocka, 2004b). The algorithm performs partial structure search and LV cardinality exploration simultaneously. For this purpose, two additional operators are used: addition and removal of a latent state for an LV.\nSecond, AGS follows a grow-restructure-thin strategy to reduce again the complexity of the search space (Chen, Zhang, & Wang, 2008; Chen et al., 2012). The strategy consists in dividing the five operators into three groups. Each group is applied at a given step of the model search. First, latent node and latent state introduction (NI and SI, respectively) are used to make the current model more complex8 (grow, line 3). Then, node relocation (NR) rearranges connections between variables (restructure, line 4). Finally, latent node and latent state deletion (ND and SD, respectively) make the current model simpler (thin, line 5).\nThird, one needs to assess the BIC score of candidate models. Learning parameters of the new models is thus required. To achieve fast learning, Chen et al. (2008, 2012) do not compute likelihood but instead the so-called restricted likelihood through the local EM procedure (line 12). The principle relies only on optimizing parameters of variables whose connection or cardinality was changed in the candidate model. Parameters of remaining variables are kept identical as in the current model."}, {"heading": "Operation Granularity", "text": "When starting from the simplest solution (an LCM), Zhang and Kocka (2004b) observed that the comparison of the BIC scores between the candidate model T \u2032 and the current one T might not be a relevant criterion. The problem is that this strategy always leads to increase the cardinality of the LCM, without introducing LVs in the model (see trade-off between LV complexity and partial structure complexity in Section 2.6). To tackle this issue, they propose instead to assess the so-called improvement ratio during the grow step:\nIRBIC(T \u2032, T |DX) = BIC(T \u2032,DX)\u2212BIC(T,DX)\ndim(T \u2032)\u2212 dim(T ) , (14)\nthat is the difference of the BIC scores between candidate model T \u2032 and current model T divided by the difference of their respective dimensions."}, {"heading": "3.2.2 Methods Based on Variable Clustering", "text": "The major drawback of search-based methods is that the evaluation of maximum likelihood in presence of LVs, as well as the large space to explore through local search, still entails computational burden. Approaches relying on variable clustering represent efficient and much faster alternatives. All of them rely on two key points: grouping variables to identify LVs and constructing model through a bottom-up strategy. Three main categories have been developed, depending on the structures learned: binary trees, non-binary trees and\n8. Note that node relocation is used locally after each NI to increase the number of its children.\nAlgorithm 2 Advanced greedy search for LTM learning (AGS, adapted from EAST, Chen et al., 2012)"}, {"heading": "INPUT:", "text": "X, a set of n observed variables {X1, ...,Xn}."}, {"heading": "OUTPUT:", "text": "T and \u03b8, respectively the tree structure and the parameters of the LTM constructed.\n1: (T 0, \u03b80)\u2190 latent class model(X) /* LCM learning using EM */ 2: loop for i = 0, 1,... until convergence 3: (T i \u2032 , \u03b8i \u2032\n)\u2190 local search(NI \u222a SI, T i, \u03b8i) /* grow */ 4: (T i \u2032\u2032 , \u03b8i \u2032\u2032\n)\u2190 local search(NR,T i\u2032 , \u03b8i\u2032) /* restructure */ 5: (T i+1, \u03b8i+1)\u2190 local search(ND \u222a SD, T i\u2032\u2032 , \u03b8i\u2032\u2032) /* thin */ 6: end loop 7: 8: /* description of the function local search(operators, T, \u03b8) */ 9: (T 0, \u03b80)\u2190 (T, \u03b8)\n10: loop for j = 0, 1,... until convergence 11: T = {T1, ..., T`} \u2190 neighborhood(operators, T j) \u22c3 T j 12: \u0398 = {\u03b81, ..., \u03b8`} \u2190 local EM(T, \u03b8j) /* except for T j */ 13: T j+1 \u2190 argmaxT {\nBIC(T,\u0398)/dim(T,\u0398) if operators = NI \u222a SI BIC(T,\u0398) otherwise\n14: \u03b8j+1 \u2190 EM(T j+1) 15: if T j+1 \u2208 neighborhood(NI, T j) 16: (T j+1, \u03b8j+1)\u2190 local search(NR,T j+1, \u03b8j+1) 17: end if 18: end loop\nforests."}, {"heading": "Binary Trees", "text": "Binary tree learning represents the most simple situation. For instance, one can simply compute an agglomerative hierarchical clustering (Xu & Wunsch, 2005) to learn the LTM structure. This algorithm is called agglomerative hierarchical cluster-based learning (AHCB). For the purpose of clustering, pairwise mutual information (MI) represents a well-suited similarity measure between variables (Cover & Thomas, 1991; Kraskov & Grassberger, 2009). The MI between two variables Vi and Vj can be defined as follows:\nI(Vi;Vj) = \u2211\nvi\u2208Vi\n\u2211\nvj\u2208Vj\np(vi, vj) log p(vi, vj)\np(vi) p(vj) . (15)\nSingle, complete or average linkage is used, depending on the cluster compactness required. The inferred hierarchy provides a binary tree (the partial structure) where leaf nodes are\nOVs and internal nodes are LVs (Wang et al., 2008; Harmeling & Williams, 2011). Then, LV cardinalities and parameters are learned (for details, see Section 3.2.4 and Section 3.1, respectively).\nIn AHCB, each cluster of the hierarchy represents an LV. The MI between an LV and the other variables (observed or latent) is approximated through a linkage criterion. Instead of this approximation, another solution consists in directly computing the MI between variables, whatever their status, i.e. observed or latent (Hwang, Kim, & Zhang, 2006; Harmeling & Williams, 2011). To compute MI, values of LVs are imputed through LCM-based learning. Algorithm 3 presents the LCM-based LTM learning (LCMB-LTM) algorithm9 implementing this solution. First, the working node set W is initialized with the set of OVs X = {X1, ...,Xn} (line 1). An empty graph is created on W (line 2). Then the pair of variables showing the highest MI, {Wi,Wj}, is selected (line 5). An LCM ({Wi,Wj},H) is learned (line 6), allowing to locally estimate the LV cardinality (through greedy search) and parameters, and then to impute values for H (line 7). Once values of H are known, it can be used as an observed variable (line 8). A new step of clustering, followed by LCM learning and LV value imputation, can then be performed. Iterating this step into a loop allows to construct an LTM through a bottom-up recursive procedure. At each step of the procedure, the parameters \u03b8lcm of the current LCM are used to update the current parameters \u03b8\n\u2032 of the LTM (line 10). If only two nodes are remaining, the two nodes are connected10 (line 12), the corresponding parameters are learned using maximum likelihood (line 13) and the loop is broken. After constructing the LTM, a final step globally learns LTM parameters using \u03b8\u2032 as a starting point (line 17). LCMB-LTM yields slightly better BIC results than AHCB for large datasets (Harmeling & Williams, 2011). LCMB-LTM is illustrated for a set of 4 variables {X1,X2,X3,X4} in Figure 5.\nHarmeling and Williams (2011) justified selecting the pair of variables showing the highest MI at each step of LCMB-LTM. Let us consider a working node set of (observed or imputed latent) variables W = {W1, ...,W`}. At each step of LCMB-LTM, the unknown\n9. This algorithm is called LCMB-LTM to distinguish it from LCMB-EM for parameter learning (Algorithm 1). Both algorithms are similar and rely on LCM-based learning.\n10. It prevents the introduction of a redundant LV, see Section 2.5, second paragraph.\nAlgorithm 3 LCM-based LTM learning (LCMB-LTM, adapted from BIN-T, Harmeling and Williams, 2011)"}, {"heading": "INPUT:", "text": "X, a set of n observed variables {X1, ...,Xn}."}, {"heading": "OUTPUT:", "text": "T (V,E) and \u03b8, respectively the tree structure and the parameters of the LTM constructed.\n1: W\u2190 X /* Initialization of the working set of variables */ 2: T (V,E)\u2190 empty tree(W) /* tree on W with no edges */ 3: \u03b8\u2032 \u2190 \u2205 4: loop 5: {Wi,Wj} \u2190 pair with highest MI(W) 6: lcm\u2190 latent class model({Wi,Wj}) 7: H \u2190 impute LV data(lcm) 8: W\u2190W\\{Wi,Wj} \u222aH /* remove children and add imputed parent */ 9: E\u2190 E \u222a edges(lcm);V \u2190 V \u222aH\n10: \u03b8\u2032 \u2190 \u03b8\u2032 \u222a children parameters(\u03b8lcm) 11: if |W| = 2 then 12: E\u2190 E \u222a edge between the two remaining nodes(W) 13: \u03b8\u2032 \u2190 \u03b8\u2032 \u222a learn remaining parameters(W) /* max likelihood estimation */ 14: break 15: end if 16: end loop 17: \u03b8 \u2190 EM(\u03b8\u2032) /* global EM using \u03b8\u2032 as a starting point */\nJPD P (W) is approximated by a JPD Q(W):\nQ(W) = P (Wi,Wj) \u03a0Wk\u2208W\\{Wi,Wj} P (Wk)\n= P (Wi,Wj)\nP (Wi) P (Wj) \u03a0Wk\u2208W P (Wk), (16)\nwhere only Wi and Wj are dependent. A proper measure to assess the divergence between P (W) and Q(W) is the Kullback-Leibler (KL) divergence, which is easy to calculate in this situation:\nKL(P ||Q) = \u2211\nW\nP (W) log P (W)\u2212 \u2211\nW\nP (W) log Q(W)\n= \u2212I(Wi;Wj) + \u2211\nW\nP (W) log P (W)\n\u03a0Wk\u2208WP (Wk) , (17)\nwhere I(Wi;Wj) is the mutual information between Wi and Wj. As the last term is constant, the maximization of the KL between P and Q simply consists in selecting the pair of variables with the highest MI before introducing an LV into the model under construction."}, {"heading": "Non-binary Trees", "text": "Although modeling with binary trees performs well in practice (Harmeling & Williams, 2011), it would be worth alleviating the binarity restriction. Indeed it might provide a better model faithfulness and interpretation because less LVs would be required. There are several ways to learn non-binary trees without necessitating too much additional computational cost. For instance, Wang et al. (2008) first learn a binary tree. Then, they check each pair of neighbor LVs in the tree. If the information is redundant between the two LVs (i.e. the model is not parsimonious), then the LV node which is the child of the other LV node is removed and the remaining node is connected to every child of the removed node. Although the approach of Wang et al. is rigorous, it can lead in practice to find trees which are very close to binary trees.\nAnother solution consists in identifying cliques of pairwise dependent variables to detect the presence of LVs (Martin & Vanlehn, 1995; Mourad, Sinoquet, & Leray, 2011). For this purpose, Mourad et al. propose to alternate two main steps: (i) at each agglomerative step, a clique partitioning method is used to identify disjoint cliques of variables; (ii) each such clique, containing at least two nodes, is connected to an LV through an LCM. For each LCM, parameters are learned using EM and LV data are imputed through probabilistic inference. In Algorithm 3 (line 5), it is possible to replace the selection of the pair of variables having the highest MI by a clique partitioning step where each clique leads to construct an LCM and to impute corresponding LV values."}, {"heading": "Flat Trees", "text": "All the algorithms discussed so far in this section are based on the idea of hierarchical variable clustering. The Bridged Island (BI) algorithm by Liu et al. (2012) takes a slightly different approach. It first partitions the set of all observed variables into subsets that are called sibling clusters. Then it creates an LCM for each sibling cluster by introducing a latent variable and optimizing its cardinality as well as the model parameters. After that, it imputes the values of the latent variables and links the latent variables up to form a tree structure using Chow-Liu\u2019s algorithm. The EM algorithm is run once at the end to optimize the parameters of the whole model. To highlight the difference between BI and the other variable clustering algorithms, we call the models it produces flat LTMs. Sibling cluster determination is the key step in BI. BI determines the sibling clusters one by one. To determine the first sibling cluster, it starts with the pair of variables with the maximum empirical MI. The cluster is expanded by adding other variables one after another. At each step, the variable that is the most dependent with the variables already in the cluster is added to the cluster. After that, a unidimensionality test (UD test) determines whether the dependences among all the variables in the cluster can be properly modeled with one latent variable. If the test fails, the expansion process is terminated and the first sibling cluster is determined. Thereafter, the same process repeats on the remaining observed variables until they are all grouped into sibling clusters."}, {"heading": "Forests", "text": "When the number of variables to analyze is very large (e.g. 1000 variables), it might be more reasonable to learn a forest instead of a tree because many variables might not be significantly dependent of each other (see Figure 6). We call this model \u201clatent forest model\u201d (LFM). It has many advantages over LTM, such as reducing the complexity of probabilistic inference (which depends on the number of edges). To learn LFM, there exists multiple approaches. For instance, in AHCB, one can use a cluster validation criterion to decide where to cut the hierarchy. Regarding LCMB-LTM (Algorithm 3), there are two options. On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this means that the LV is not useful to the model under construction and the algorithm stops. On the other hand, after partitioning variables in cliques (which replaces the step in line 5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are discovered.\nTo build an LFM, one can also first construct an LTM and then use the independence testing method of Tan et al. (2011) for pruning non-significant edges. This method provides guarantees to satisfy structural and risk consistencies. Similar works for non-parametric analysis have also been developed by Liu et al. (2011). It is worth mentioning that, to ensure model parsimony, the pruning of non-significant edges in an LTM should be followed by the removal of latent nodes which are no longer connected to a minimum of three nodes."}, {"heading": "3.2.3 Distance-based Methods", "text": "This class of methods has been originally developed for phylogenetics (Felsenstein, 2003). A phylogenetic tree is a binary LTM showing the evolutionary relations among a set of taxa. Compared to the other LTM learning methods, distance-based ones provide strong guarantees for the inference of the optimal model. In this section, we first define distances and then present learning algorithms: neighbor joining (phylogenetic tree inference), distance-based\nmethod dedicated to general LTM learning and recent spectral methods."}, {"heading": "Distances between Variables", "text": "Distances are restricted to LTMs whose all variables share the same state space X , e.g. binary variables (Lake, 1994). Distances are functions of pairwise distributions. For a discrete tree model G(V,E) (e.g. an LTM), the distance between two variables Vi and Vj is:\ndij = \u2212 log |det(Jij)| \u221a\ndet(Mi) det(Mj) , (18)\nwith Jij the joint probability matrix between Vi and Vj (i.e. J ij ab = p(Vi = a, Vj = b), a, b \u2208 X ), and Mi the diagonal marginal probability matrix of Vi (i.e. M iaa = p(Vi = a)). For a special case of discrete tree models, called symmetric discrete tree models (Choi et al., 2011), distance has a simpler form. Symmetric discrete tree models are characterized by the fact that every variable has a uniform marginal distribution and that any pair of variables Vi and Vj connected by an edge in E verifies the following property:\np(vi|vj) = { 1\u2212 (K \u2212 1) \u03b8ij if vi = vj \u03b8ij otherwise,\nwith K the cardinality common to Vi and Vj , and \u03b8ij \u2208 (0, 1/K), known as the crossover probability. For symmetric discrete tree models, the distance between two variables Vi and Vj is then: dij = \u2212(K \u2212 1) log (1\u2212K\u03b8ij). (19) Note that there is a one-to-one correspondence between distances and model parameters for symmetric discrete tree models (for more details, see Choi et al., 2011).\nThe aforementioned distances are additive tree metrics (Erdos, Szekely, Steel, &Warnow, 1999):\ndk` = \u2211\n(Vi,Vj)\u2208Path((k,`);E)\ndij , \u2200k, ` \u2208 V. (20)\nChoi et al. (2011) showed that additive tree distances allow to ascertain child-parent and sibling relationships between variables in a parsimonious LTM. Let us consider any three variables Vi, Vj, Vk \u2208 V. Choi et al. define \u03a6ijk as the difference between distances dik and djk (\u03a6ijk = dik \u2212 djk). For any distance dij between Vi and Vj , the following two properties on \u03a6ijk hold:\n\u2212 \u03a6ijk = dij ,\u2200Vk \u2208 V\\{Vi, Vj}, if and only if Vi is a leaf node and Vj is its parent node;\n\u2212 \u03a6ijk = \u2212dij,\u2200Vk \u2208 V\\{Vi, Vj}, if and only if Vj is a leaf node and Vi is its parent node;\n\u2212 \u2212dij < \u03a6ijk = \u03a6ijk\u2032 < dij ,\u2200Vk, Vk\u2032 \u2208 V\\{Vi, Vj}, if and only if Vi and Vj are leaf nodes and they share the same parent node, i.e. they belong to the same sibling group."}, {"heading": "Neighbor Joining", "text": "The principle of neighbor joining (NJ) is quite simple (Saitou & Nei, 1987; Gascuel & Steel, 2006). NJ starts with a star-shaped tree. Then it iteratively selects two taxa i and j, and it creates a new taxum u to connect them. The selection of a pair seeks to optimize the following Q criterion:\nQ(i, j) = (n\u2212 2) dij \u2212 n \u2211\nk=1\ndik \u2212 n \u2211\nk=1\ndjk, (21)\nwhere n is the number of taxa and dij is the additive tree distance between i and j. The distance between i and the new taxum u is estimated as follows:\ndiu = 1\n2 dij +\n1\n2(n \u2212 2)\n(\nn \u2211\nk=1\ndik \u2212 n \u2211\nk=1\ndjk\n)\n, (22)\nand dju is calculated by symmetry. The distances between the new taxum u and the other taxa in the tree are computed as:\nduk = 1\n2 (dik \u2212 diu) +\n1 2 (djk \u2212 dju) . (23)\nThe success of distance methods such as NJ comes from the fact that they have been proved very efficient in terms of sample complexity. Under the Cavender-Farris model of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to guarantee that maxi,j |dij \u2212 d\u0302ij| < with a probability at least \u03b4 if:\nN \u2265 2 ln ( 2n \u03b4 ) (1\u2212 exp(\u2212 ))2 ( exp ( max i,j dij ))2 , (24)\nwith N the number of mutation sites (i.e. the number of observations) and n the number of taxa (i.e. the number of OVs). Erdo\u0308s et al. (1999) then demonstrated that under any evolutionary model and for any reconstruction method, N grows at least as fast as log n, and for any model assuming i.i.d. observations, it grows at least as n log n. Erdo\u0308s et al. also proposed a new algorithm, called Dyadic Closure Method, with a sample complexity of a power of log n, when the mutation probabilities lie in a fixed interval. Daskalakis et al. (2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n. Recently, Mossel et al. (2011) proved that log2 n suffices when discretization is not assumed.\nIn phylogenetics, the scientist is often faced with a set of different trees11 and the construction of a consensus tree is thus required. The computational complexity of this construction has been studied and a polynomial algorithm has been proposed by Steel et al. (1992)."}, {"heading": "Learning Dedicated to General LTM", "text": "In this subsection, we present the latest developments of Choi et al. (2011) for general LTM learning, i.e. learning not restricted to phylogenetic trees. It is restricted to the analysis of data whose all variables share the same state space, for instance binary data. Assuming data generated by a parsimonious LTM, the additive tree metric property allows to exactly recover child-parent and sibling relations from distances (see Subsection Distances between Variables, last paragraph). Another advantage is that OVs are not necessarily constrained to be leaf nodes (this will be seen in the next paragraph).\nThe distance-based general LTM learning (DBG) is implemented in Algorithm 4, detailed as follows. First, the working node set W is initialized with the set of observed variables X = {X1, ...,Xn} (line 1). Distances are computed for any three variables in W (line 2). An empty tree is created on W (line 3). Then the following steps are successively iterated (lines 4 to 16):\n\u2212 A procedure (based on properties described in Subsection Distances between Variables, last paragraph) allows to identify nodes that have a parent-child relation and nodes that are siblings (line 5). This procedure generates three different sets of node groups: a set of parent-child groups, PC = {PC1, ..., PCp}, a set of sibling groups, S = {S1, ..., Sq} and a set of remaining single nodes, R = {R1, ..., Rr};\n\u2212 The content of the working node set W is replaced with parent nodes belonging to parents(PC) and remaining single nodes belonging to R (line 6);\n\u2212 For each group of sibling nodes \u2208 S, a new parent LV H is created and added to W (lines 7 and 8);\n\u2212 To update the distances of the working node set (line 9), the distances between the new LVs and the remaining variables in W are calculated (the calculation is easily derived from previously computed distances; for more details, see Choi et al., 2011);\n11. For instance when different genes are used to infer trees.\nAlgorithm 4 Distance-based general LTM learning (DBG, adapted from RG, Choi et al., 2011)"}, {"heading": "INPUT:", "text": "X, a set of n observed variables {X1, ...,Xn}."}, {"heading": "OUTPUT:", "text": "T and \u03b8, respectively the tree structure and the parameters of the LTM constructed.\n1: W\u2190 X /* Initialization of the working set of variables */ 2: D\u2190 info dist computation(W) /* for any three variables in W */ 3: T (V,E)\u2190 empty tree(W) /* tree on W with no edges */ 4: loop 5: (PC,S,R)\u2190 test relations(D,W) /* see paragraph 3 of the section */ 6: W\u2190 (parents(PC),R) /* parents and singles */ 7: LCMT\u2190 LCM trees(S) /* an LCM tree for each sibling group */ 8: W\u2190W \u222a latent variables(LCMT) 9: D\u2190 D \u222a info dist computation(W) /* only for LVs \u2208W */\n10: E\u2190 E \u222a edges(PC) \u222a edges(LCMT) 11: V\u2190 V \u222a latent variables(LCMT) 12: if |W| = 2 then 13: E\u2190 E \u222a edge between the two remaining nodes(W) 14: break 15: else if |W| = 1 then break end if 16: end loop 17: \u03b8 \u2190 EM(T ) /* See Section 3.1 */\n\u2212 If the working node set W contains strictly more than two nodes, then a new step is started. Otherwise, there are two possible situations: if the working node set W contains two nodes, then these nodes are connected12 and the procedure is completed (lines 12 to 14); if the working node set W only contains one node, then the iteration stops (line 15).\nAfter learning the partial structure, model parameters (line 17) are learned (see Section 3.1).\nIn practice, Choi et al. (2011) restricted the learning to symmetric discrete distributions13 (see definition in Subsection Distances between Variables). This restriction presents a major advantage: it allows to derive model parameters through the use of distances previously learned by structure recovery. In other words, after learning the structure, there is no need to recover parameters through EM. This is due to the one-to-one correspondence between distance and model parameters (for more details, see Choi et al., 2011).\n12. It prevents the introduction of a redundant LV, see Section 2.5, second paragraph. 13. Nevertheless, the algorithm can be applied to non-symmetric discrete distributions. The only requirement\nis that all variables share the same state space.\nTo diminish the computational complexity of DBG, Choi et al. (2011) propose to first learn a minimum spanning tree (MST) based on distances between OVs. Then, in the tree, they identify the set of internal nodes. For each internal node Vi and its neighbor nodes nbd(Vi), they apply DBG which outputs a latent tree. In the global model, each subtree {Vi, nbd(Vi)} is then replaced by the corresponding latent tree. This strategy allows to reduce the computational complexity of latent tree construction because MST is fast to compute and DBG is only applied to a restricted number of variables. In term of sample complexity, DBG and its derived algorithms only require log n observations for recovering the model with high probability. This sample complexity is equal to the one of the Chow-Liu algorithm (Tan et al., 2011).\nAnother version of the DBG algorithm was developed when it is not assumed that observations have been generated by a genuine LTM. To prevent the incorporation of irrelevant LVs, after applying DBG on subtrees {Vi, nbd(Vi)}, only are integrated in the model those latent trees that increase the BIC score."}, {"heading": "Spectral Methods", "text": "Recent works extended previous distance methods following a spectral approach. On the one hand, Anandkumar et al. (2011) addressed the multivariate setting where observed and latent nodes are random vectors rather than scalars. Their approach can deal with general linear models containing both categorical and continuous variables. Another important improvement of their method is that sample complexity bound is given in terms of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011). The proposed extension consists in replacing the step in line 5 of Algorithm 4 by a quartet test14 relying on spectral techniques (more specifically, canonical correlation analysis; Hair, Black, Babin, and Anderson, 2009). Given four observed variables {X1,X2,X3,X4}, the spectral quartet test distinguishes between the four possible tree topologies (see Figure 8). The correct topology is {{Xi,Xj}, {Xi\u2032 ,Xj\u2032}} if and only if:\n|E[XiX>j ]|\u2217 |E[Xi\u2032X>j\u2032 ]|\u2217 > |E[Xi\u2032X>j ]|\u2217 |E[XiX>j\u2032 ]|\u2217, (25) where |M |\u2217 := \u03a0k`=1\u03c3`(M) is the product of the k largest singular values of matrix M and E[M ] is the expectation of M (estimated using the covariance matrix).\nOn the other hand, Song et al. (2011) proposed a non-parametric learning based on kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962). KDE is particularly relevant for model learning, in the case of non-Gaussian continuous variables showing multimodality and skewness. Given a set of N i.i.d. observed data Dx = {x1, ...,xN}, the joint distribution is modeled as :\nP (x) = 1\nN\nN \u2211\ni=1\nn \u220f\nj=1\nk(xj , x i j), (26)\nwith x = {x1, ..., xn} an observation, k(x, x\u2032) the Gaussian radial basis function and N the number of observations. Kernels k(x, x\u2032) are represented as inner products \u3008\u03c6(x), \u03c6(x\u2032)\u3009F 14. Quartet tests are widely used for phylogenetic tree inference. The first authors to adapt them to LTM\nlearning were Chen and Zhang (2006).\nthrough a feature map \u03c6 : R \u2192 F . As products of kernels are also kernels, the product \u03a0nj=1k(xj , x \u2032 j) is written as a single inner product \u3008\u2297nj=1\u03c6(xj),\u2297nj=1\u03c6(x\u2032j)\u3009Fn , where \u2297 is the tensor product. Let CX := EX[\u2297nj=1\u03c6(Xj)] be the Hilbert space embedding of the KDE distribution P (X). The expectation of P (X) can be formulated as \u3008CX,\u2297nj=1\u03c6(xj)\u3009Fn . By exploiting a given latent tree structure, the key is that Hilbert space embedding allows to decompose CX into simpler tensors. Similarly to the work of Parikh et al. (2011), message passing and parameter estimation are reformulated through tensor notation (see the work of Song et al., 2011, for more details). To learn the structure, a non-parametric additive tree metric is used. For two variables Vi and Vj, the distance is:\ndij = \u2212 1\n2 log |CijC>ij |? +\n1 4 log |CiiC>ii |? + 1 4 log |CjjC>jj|?. (27)\nThen, given the distances between variables, NJ or DBG can be used to learn the structure."}, {"heading": "3.2.4 Determination of Latent Variable Cardinalities", "text": "During LCM learning, LV cardinality can be determined through the examination of all possible values (up to a maximum) and then by choosing the one which maximizes a criterion, such as BIC (Raftery, 1986). For each cardinality value, parameters are required to calculate the likelihood appearing in the optimization criterion. For this purpose, random restarts of EM are generally used to learn parameters with a low probability of getting trapped in local maxima. The drawback is that this method cannot be applied to LTM learning, because EM becomes time-consuming when there are several LVs. A better solution consists in using a greedy search approach, starting with a preset value of LV cardinality (generally equal to 2) and incrementing it to meet the optimal criterion. However, this solution still remains computationally demanding (Zhang, 2004).\nTo tackle the issue of computational burden, several strategies have been proposed. For instance, one can simply set a small value for the LV cardinalities. Following this idea, Hwang and collaborators (2006) constrain LVs to binary variables. Because they worked on binary trees whose OVs are also binary, this restriction is not severe in practice. Nevertheless, in the case of non-binary OVs and/or non-binary trees, this very fast method presents several drawbacks: on the one hand, a too small cardinality can lead to an important loss\nof information; on the other hand, a too large cardinality can entail model overfitting and unnecessary computational burden.\nMore rigorous, Wang et al.\u2019s approach (2008) relies on regularity (see Section 2.5). Knowing the cardinality of its neighbor variables Zi, the cardinality of an LV H is determined as follows:\n|H| = \u03a0 k i=1|Zi|\nmaxki=1 |Zi| . (28)\nThis very fast approach is efficient for LVs owning a few number of neighbors. Thus this is only practicable for binary trees or trees whose LV degrees are small (close to 3). In the context of large scale data analysis (several thousands of variables), Mourad et al. (2011) proposed to estimate the cardinality of an LV given the number of its children. The rationale underlying this approach is the following: the more child nodes an LV has, the larger the number of combinations is for the values of the child variables. Therefore, the cardinality of a latent variable should depend on the number of its child nodes. Nonetheless, to keep the model complexity within reasonable limits, a maximum cardinality is fixed.\nTwo additional methods have been proposed to offer a better trade-off between accuracy and computational cost. The first one uses a search-based agglomerative state-clustering procedure (Elidan & Friedman, 2001). The idea relies on the Markov blanket of an LV. In LTMs, the Markov blanket of an LV H, noted MBH , is composed of its parent and its children. The Markov blanket represents the set of variables that directly interact with H. Elidan and Friedman\u2019s method sets the initial cardinality of H based on the empirical joint distribution ofMBH , noted P\u0302 (MBH). H is initialized to have a state for each configuration found in P\u0302 (MBH). Then the cardinality is repeatedly decreased through successive merging operations: states hi and hj, whose merging entails the best optimization of a given score, are merged. After repeating these operations till H has only one state, the cardinality value leading to the best score is selected. The second method relies on local and fast parameter estimation through LCM learning (Harmeling & Williams, 2011). As presented in the first paragraph of this section, a greedy search approach can be used. It starts with a preset value and increments it to meet an optimal criterion. This greedy search becomes efficient because, for each cardinality value to test, parameters are quickly learned in constant time."}, {"heading": "3.2.5 Choosing a Root", "text": "The LTM root cannot be learned from data. However there is sometimes a need to determine the root. For instance, LCM-based parameter learning (see Algorithm 1) can easily be performed when a root is chosen.\nThe root can be determined from a priori knowledge on data. For instance, we can consider that the latent structure of LTM represents a hierarchy of concepts (i.e. a taxonomy in ontology). Thus, the LV root corresponds to the highest abstract level, whereas an LV node only having OVs as children is interpreted as the lowest abstract level. Actually, variable clustering-based algorithms implicitly implement this a priori knowledge."}, {"heading": "3.2.6 Time Complexity and Scalability", "text": "The time complexity of generic LTM learning algorithms is summarized in Table 1. In the table, we compare algorithms, approaches, models and time complexities. We also\ngive examples of instantiations for generic algorithms. Online resources are summarized in Appendix A. In order to simplify the comparison of time complexities, we only consider the number n of variables (input data), the number N of observations and the number s of steps (for search-based algorithms). The LTM learning algorithms are compared with the Chow-Liu algorithm for learning a tree without LVs. Details about the complexity calculation of LTM learning algorithms are provided in Supplemental material B.2.\nWhen the tree does not contain any LV, learning the model can be done efficiently in O(n2N) using the Prim\u2019s algorithm (1957). The situation is more complicated when the tree contains LVs. The complexity of finding the regular LTM with the lowest score is O(23n 2\n). Search-based methods implement heuristics to reduce this large complexity. Their overall complexity can be decomposed into a product of three main terms: number of steps, structure learning complexity and parameter learning complexity. At the opposite, in variable clustering- and distance-based methods, the overall complexity can be decomposed into a sum. Nevertheless, the development of new operators for greedy search and the application of local EM have led to significant improvements (from O(sn5N) to O(sn2N)). Variable clustering-based methods are computationally more efficient for multiple reasons. They rely on pairwise dependence computation to identify LVs and their connections, and on LCM-based learning to determine LV cardinality. Regarding distance-based methods, NJ provides a reasonable complexity of O(n3N), whereas DBG presents a high complexity of O(n3N+n4). However, this last complexity corresponds to the worst case, when the tree to learn is a hidden Markov model. Besides, Choi et al. provide a modified DBG which reduces the complexity to O(n2N + n4).\nSome of the algorithms proposed in the literature differ from the generic algorithms presented. In Tables 2 and 3, we compare the algorithms from the literature on small, large and very large simulated and real datasets15. Moreover we provide results for standard algorithms: CL model-based and LCM-based approaches. For each dataset, we learned the model from training data and evaluated the BIC score on test data. We repeated the experiments 10 times. The programs were allowed to run in maximum 6 hours. Datasets are described in Supplemental material B.316. We report the BIC score with its standard deviation and the running time.\nOn small datasets (n \u2264 10 variables), search-based methods lead to the best BIC values, except for the Asia dataset for which a distance-based method, regCLRG, is the best one. This is not surprising since search-based methods evaluate a large number of models to find the optimal one. Nevertheless, on large datasets (10 \u2264 n \u2264 100), search-based methods require long running times and thus cannot be used for some datasets such as the Coil-42 and NewsGroup ones. In this context, variable clustering-based and distance-based methods are much more efficient while yielding accurate results. Regarding the very large dataset context (n > 100), only variable clustering-based and distance-based methods can learn LTMs17. CFHLC18 is the only approach able to process the HapMap data containing 117 observations for 10k variables. For all datasets, we observe that using CL model and LCM predominantly leads to lower BIC scores than when using LTM, except for large and very large datasets."}, {"heading": "3.2.7 Summary", "text": "LTM learning has been subject to many methodological developments. When structure is known, EM is often preferred. Nevertheless, for large LTMs, EM leads to considerable running times and to local maxima. To address this problem, LCM-based EM allows to quickly learn parameters, while spectral methods help find the optimal solution when LTM parameters are not required. When structure is unknown, search-based approaches represent standard methods from Bayesian network learning. However they are only suitable for learning small LTMs. To tackle this issue, variable clustering-based methods represent efficient alternatives. These methods are based on the idea of grouping variables to identify LVs in a bottom-up manner. Recently, phylogenetic algorithms have been adapted to general LTM learning. Compared to the other methods, they guarantee to exactly recover the generative LTM structure under some conditions.\n15. For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011). 16. Although algorithms NJ, RG, CLRG and regCLRG can process any kind of data with shared state space\n(binary data, ternary data, ...), the implementation provided by Choi et al. (2011) can only process binary data. Hence the algorithms have not been applied to some datasets. We recall that RG, CLRG and regCLRG do not exactly learn an LTM but instead a tree whose all internal nodes are not compelled to be latent. 17. Although it is not shown in Tables 2 and 3, NJ, RG, CLRG and regCLRG were able to process 1000 binary variables in our experiments. 18. In the work of Mourad et al. (2011), CFHLC implements a window-based approach to scale very large datasets (n \u2265 100k variables). Here for a fair comparison, the window-based approach has not been used."}, {"heading": "4. Applications", "text": "In this section, we discuss and illustrate three types of applications of LTMs: latent structure discovery, multidimensional clustering and probabilistic inference. At the end of the section, we also briefly present other applications such as classification."}, {"heading": "4.1 Latent Structure Discovery", "text": "Latent structure discovery aims at revealing: (i) latent information underlying data, i.e. unobservable variables or abstract concepts which have a role to play in data analysis, and (ii) latent relationships, i.e. relationships existing between observed and latent information, and also between pieces of latent information themselves. For this purpose, LTM analysis represents a powerful tool where latent information and latent relationships are modeled by LVs and graph edges, respectively. Thanks to LTMs, latent structure discovery has been applied to several fields: marketing (Zhang, Wang, & Chen, 2008), medicine (Zhang, Nielsen, & Jensen, 2004; Zhang et al., 2008), genetics (Hwang et al., 2006; Mourad et al., 2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Pe\u2019er, & Pupko, 2002). Let us take the example of phylogenetics which is the major application of LTMs in structure discovery.\nIn phylogenetics, the purpose is to infer the tree representing the evolutionary connections between observed species. Let us consider human and its closest living relatives: orangutan, gorilla, bonobo and chimpanzee. From their DNA sequences, it is possible to reconstruct the phylogenetic tree. DNA sequences are sequences of letters A, T , G and C. During the evolution of species, DNA sequences are modified by mutational processes. Each\nspecies can then be characterized by its DNA sequence. One of the most popular algorithms for phylogenetic tree reconstruction is NJ (described in Section 3.2.3, Neighbor Joining). It starts by considering the tree as a star linking all species (see illustration in Figure 9). Then the distances between all species are calculated based on the DNA sequences. Chimpanzee and bonobo present the shortest distance and are thus regrouped under a new latent node. Then distances are updated and the last previous step is reiterated until the construction of the final phylogenetic tree. The tree first links chimpanzee and bonobo, then human, gorilla and orangutan. The success of NJ comes from the fact that, compared to previous hierarchical clustering methods such as UPGMA (Unweighted Pair Group Method with Arithmetic Mean), it does not assume all species evolve at the same rate. The length of an edge represents the time separating two species. Moreover, assuming that the distances are correct, NJ outputs the correct tree."}, {"heading": "4.2 Multidimensional Clustering", "text": "Cluster analysis, also called clustering, aims at assigning a set of observations to several groups (called clusters) so that observations belonging to the same cluster are similar in some sense (Xu & Wunsch, 2008). LTMs are particular tools which can produce multiple clusterings: each LV represents a partition of data, which is most related to a specific subset of variables. This application is called \u201cmultidimensional clustering\u201d and has been mainly explored by Chen et al. (2012).\nLet us illustrate LTM-based multidimensional clustering using dataset from a survey on Danish beer market consumption. For this purpose, we use the user-friendly software Lantern. The dataset consists of 11 variables and 463 consumers. Each variable represents a beer brand which is evaluated through the four possible answers to a survey questionnaire: never seen the brand before (s0); seen before, but never tasted (s1); tasted, but do not drink regularly (s2) and drink regularly (s3).\nThe learned model is presented in Figure 10. It has a BIC score of \u22124851.99. The model contains 3 LVs: H0, H1 and H2. These LVs have 2, 3 and 2 latent classes, respectively. Let us start withH1 which is the simplest interpretable LV. Let X1,X2, ...,Xn be the OVs sorted by decreasing values of pairwise MI between H1 and each OV Xi. Two different information curves are analyzed in Figure 11: (i) the curve of pairwise mutual information I(H1;Xi) between H1 and each OV Xi, and (ii) the curve of cumulative information I(H1;X1, ...,Xi) representing MI between H1 and the first i OVs X1, ...,Xi. The curve of pairwise mutual information shows that TuborgClas, followed by CarlSpec and Heineken, are the beers most related to H1. The curve of cumulative information presents complementary information. The cumulative information curve increases monotonically with i and reaches the maximum for n. The ratio I(H1;X1, ...,Xi)/I(H1;X1, ...,Xn) is the information coverage of the first i OVs. If this ratio is equal to 1, it means that H1 is conditionally independent of Xi+1, ...,Xn given the first i OVs. In practice only the first OVs whose information coverage is less than 95% are considered relevant. Using the cumulative information curve, we observe that H1 is only related to TuborgClas, CarlSpec and Heineken, which represent a group of beers different from the others. TuborgClas and CarlSpec are frequent beers, being a bit darker in color and more different in taste than the two main mass-market beers, GronTuborg and Carlsberg. Although not Danish, Heineken is one of the largest brand in the world that most Danes would have tasted sometimes during travels abroad. Results for other LVs are discussed but not shown (interpretation remains the same as for H1). H0 is more related to CeresTop, CeresRoyal, Pokal, Fuglsang, CarlsSpec and FaxeFad (i.e. minor local beers), whereas H2 is more connected to GronTuborg and Carlsberg (i.e., the two main mass-market beers).\nClass conditional probability distributions (CCPDs) of H1 are presented in Table 4. Using this table, it is easy to interpret latent classes. For instance, the first class (Class1) represents 36% of the consumers (prior = 0.36). For this class, all conditional probabilities of s2 are higher than 0.5. This means that these consumers tasted the beers, but do not drink them regularly. The second class (Class2) contains 27% of the consumers and represents people who saw the beers before or only tasted them. The last class (Class3) includes 37% of the consumers and represents people who just tasted the beers or drink them regularly. Results for other LVs are discussed but not shown. The CCPDs relative to H0 show a division into a group of consumers who just tasted the beers (Class1) and a more complicated group of consumers who never saw the brands, just saw them or just tasted them (Class2). Regarding H2, the CCPDs report consumers who just tasted the beers (Class1) and consumers who drink them regularly (Class2). Using the LTM, we can also analyze relations between the different partitions. The conditional probability distribution P (H2|H1) is given in Table 5. We observe that consumer behaviors are similar for the two groups of beers. For instance, consumers who just tasted or regularly drink the H1 group of beers (Class3 of H1) generally also drink regularly the H2 group of beers (Class2 of H2).\nIn this example, we were able to find consumer profiles specific to beer brands. Multidimensional clustering thanks to LTMs helps discover multiple facets of data (i.e. LVs) and partition data along each facet (i.e. latent class). Moreover, general relations between multiple facets are highlighted through connections between LVs."}, {"heading": "4.3 Probabilistic Inference", "text": "Probabilistic inference is the process of answering probabilistic queries of the form p(x|y), for an event x given some knowledge y (Koller & Friedman, 2009), using the Bayes formula:\np(x|y) = p(y|x)p(x) p(y) . (29)\nProbabilistic inference is used in many circumstances, such as in credit card fraud detection (Ezawa & Norton, 1996) or disease diagnostic (McKendrick, Gettinbya, Gua, Reidb, & Revie, 2000).\nProbabilistic inference in a general BN is known to be an NP-hard task (Cooper, 1990). To tackle this issue, one can approximate the original BN using a maximum weight spanning tree learned relying on Chow and Liu\u2019s algorithm (1968). The drawback is the risk of inaccuracy in inference results. In this context, LTM provides an efficient and simple solution, because: (i) thanks to its tree structure, the model allows linear computations with respect to the number of OVs, and at the same time, (ii) it can represent complex relations between OVs through multiple LVs. Because learning LTM before performing inference can be time-consuming Wang et al. (2008) propose the following strategy: first, offline model learning is performed, then answers to probabilistic queries are quickly computed online. However, recent spectral methods (Parikh et al., 2011) considerably reduced model learning phase, because they do not require to learn the model structure. This makes inference through large LTMs possible (around several hundred OVs) and thus renders them even more attractive.\nInferential complexity does not only depend on the number of OVs, but also on LV cardinalities: the higher the cardinality, the higher the complexity. Hence Wang et al. (2008) propose a tradeoff between inferential complexity and model approximation accuracy by fixing a maximal cardinality C for LVs.\nWang et al. (2008) empirically demonstrated the high performance of LTM-based inference on 8 well-known Bayesian networks from the literature. The principle consists in learning the LTM which will provide the best approximation of the original BN. For this purpose, data are sampled from the original BN and then an LTM is learned from the data. To illustrate inference, let us only consider two examples, the ALARM and the BARLEY networks which show the lowest and highest inferential complexities among the aforementioned networks, respectively. The ALARM network contains 37 nodes, and is characterized by an average indegree of 1.24 (max: 4) and an average cardinality of 2.84 (max: 4). The BARLEY network contains 48 nodes; its average indegree is 1.75 (max: 4) and its average cardinality is 8.77 (max: 67). Two parameters are central for the user: (i) N , the sample size which entails long model learning running times but leads to better model accuracies, and (ii) C, the maximal cardinality of LVs which entails long model learning and inference running times but leads to higher model accuracies.\nIn Figure 12, the LTM-based method is compared to other standard inference approaches: the LCM-based approach, the Chow-Liu (CL) model-based approach and loopy belief propagation (LBP) (Pearl, 1988). Exact inference through clique tree propagation (CTP) (Lauritzen & Spiegelhalter, 1988) on the original BN is considered as the reference. Figure 12a reports running times for LTM learning using the LTAB algorithm (Wang et al., 2008). Running times almost linearly increase with N and C. Regarding inference accuracy (Figure 12b), the LTM-based method outperforms other methods when N and C are high, e.g. N = 100k and C = 8 for the ALARM network. As regards inference running times (Figure 12c), the benefits of using the LTM-based method are high for the ALARM network, a high inferential complexity network. In these experiments, we note that CL is also very interesting, and the choice between the latter and the LTM will depend on the online inference time allowed. If time is very limited, CL would be preferred. In the other case, the LTM would be chosen."}, {"heading": "4.4 Other Applications", "text": "Beside latent structure discovery, multidimensional clustering and probabilistic inference, there are other interesting applications of LTMs.\nA simple but efficient classifier is naive Bayes. This model assumes that OVs are independent conditional on the class variable. This assumption is often violated by data and hence numerous adaptations have been developed to improve the classifier performance. Naive Bayes has been generalized by introducing latent nodes as internal discrete nodes (Zhang et al., 2004) or continuous nodes (Langseth & Nielsen, 2009), mediating the relation between leaves and the class variable. The model is identical to an LTM except that the root is observed. Recently, Wang et al. (2011) proposed a classifier based on LTM. For each class, a specific LTM is learned and a latent tree classifier is built by aggregating all LTMs. This classifier outperforms naive Bayes and many other successful classifiers such as tree augmented naive Bayes (Friedman, Geiger, & Goldszmidt, 1997) and averaged one-dependence estimator (Webb, Boughton, & Wang, 2005).\nMore specifically to some research fields, LTM has been used for human interaction recognition, haplotype inference in genetics and diagnosis in traditional medicine. Human interaction recognition is a challenging task, because of multiple body parts and concomitant inclusions (Aggarwal & Cai, 1999). For this purpose, the use of LTM allows to segment the interaction in a multi-level fashion (Park & Aggarwal, 2003): body part positions are estimated through low-level LVs, while overall body position is estimated by a highlevel LV. In genetics, there is a need for inferring haplotypic data (i.e. latent genetic DNA sequences) from genotypic data (observed data). Kimmel and Shamir (2005) perform efficient haplotypic inference using a two-layer LFM. Finally, Zhang et al. (2008) applied LTMs to traditional Chinese medicine (TCM). They discovered that the latent structure obtained matches TCM theories. The model provides an objective justification for the ancient theories."}, {"heading": "5. Discussion", "text": "In data analysis, LTM represents an emerging popular topic as it offers several advantages:\n\u2212 The model allows to discover interpretable latent structure.\n\u2212 Each latent variable is intended to represent a way to cluster categorical data, and connections between latent variables are meant to express relations between the multiple clustering ways.\n\u2212 Multiple latent variables organized into a tree greatly improve the flexibility of probabilistic modeling while, at the same time, ensuring linear - thus fast - probabilistic inference.\nApplications of LTMs are summarized in Table 6, which recapitulates three types of applications with details, examples, references to generic algorithms, scalability to large datasets, software and bibliographical references.\nIn the past decade, extensive research efforts have been done in LTM learning. When structure is known, standard EM and LCM-based EM or spectral methods can be used.\nWhen structure is unknown, three classes of methods have been proposed: search-based, variable clustering-based and distance-based methods. The first one is slow but leads to accurate models. The second one drastically decreases running times. Finally, the last class guarantees to exactly recover the generative LTM structure under the assumption that all LVs have the number of states and this number is known.\nIn spite of the aforementioned advances, the use of LTM presents some drawbacks. For example, when the data dimension is large or very large, model learning still remains prohibitive. Regarding probabilistic inference, LTM provides better results than the standard Chow-Liu\u2019s approach, but leads to more computational burden."}, {"heading": "6. Future Directions", "text": "Progress on LTM has been made, but there is still much to be done. There are multiple promising directions. For instance, a recent work developed LTM for continuous data analysis (Poon, Zhang, Chen, & Wang, 2010; Choi et al., 2011). Other authors investigated the relationships between LTM and ontology (Hwang et al., 2006), and LTM-based dependence visualization (Mourad, Sinoquet, Dina, & Leray, 2011). Although no research has been carried out on the application to causal discovery and latent trait analysis, we argue that LTM might represent interesting avenues of research.\nLTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012). For instance, Poon et al. (2010) proposed a new model, called pouch latent tree model (PLTM). PLTM is identical to LTM, except that each observed node in an LTM is replaced with a \u201cpouch\u201d node representing an aggregation of multiple continuous OVs. PLTM represents a generalization of the Gaussian mixture model (GMM) when more than one LV is allowed. The proposal of Poon et al. originates from the fact that model-based clustering of continuous data is sensitive to the selected variables. Similarly to categorical clustering (Section 4.2), in high-dimensional continuous data, there are multiple ways to partition the data, and the multiple LVs of PLTM are able to take this into account. Poon et al.\ndeveloped a search-based algorithm for PLTM learning. The latter algorithm is closely related to the EAST algorithm (Zhang & Kocka, 2004b) dedicated to LTM learning and is thus quite slow. Hence, the development of new methods dedicated to efficient PLTM learning certainly represents interesting perspectives of research. Besides, a next step would also be the development of LTM dedicated to mixed data analysis, i.e. combining categorical and continuous data.\nLTM Structure and Ontology: It is possible to relate LTM to ontology, in particular taxonomy (tree-structured ontology). For instance, when applying LTM to a microarray dataset of yeast cell-cycle, Hwang et al. (2006) showed that some LVs are significantly related to specific gene ontology terms, such as organelle organization or cellular physiological process. Thus taxonomy could help interpret LVs. Moreover the taxonomy structure could be used as a priori structure.\nDependence Visualization: LTM provide a compact and interpretable view of dependences between variables, thanks to its graphical nature and its latent variables (Mourad et al., 2011). Compared to heat map (Barrett, Fry, Maller, & Daly, 2005) which can only display pairwise dependences between variables, LTM helps visualize both pairwise and higher-order dependences. Pairwise dependence can be represented by the chain length linking two leaf variables, whereas higher-order dependences are simply represented by a set of leaf variables connected to a common LV.\nCausal Discovery: We argue that LTM represents simple but efficient model for causal discovery, for the following reasons:\n\u2212 If the LTM root is known, then the model can be interpreted as a hierarchy. Into this hierarchy, LVs are distributed into multiple layers. This multiple LV layers represent different degrees of information compactness (i.e. data dimension reduction), since each LV captures the patterns of its child variables. The connexion of variables through parent-child relationships allows easy and natural moves from general (top layers) to specific (bottom layers) causes, and vice-versa. Thus, causal discovery can be guided by the hierarchical model feature.\n\u2212 After constructing the model on variables X = {X1, ...,Xn}, if one wants to test the direct dependence between Xi and another variable Y not present in the network, it can be easily computed through a test for independence between Xi and Y conditional on the parent of Xi. A practical advantage of this conditional test meant to assess direct dependence is that the number of degrees of freedom required is low (because only the parent of Xi is used to condition the test), which ensures a good power.\nLatent Trait Analysis: Similarly to the generalization of LCM by LTM, it would be worth developing an extension of the latent trait model by a tree-structured model where internal nodes are continuous LVs. For instance, this would alleviate the drawbacks of local independence in the latent trait model and would provide multiple facets thanks to LVs when dealing with high-dimensional data."}, {"heading": "Acknowledgments", "text": "The authors are deeply indebted to four anonymous reviewers for their invaluable comments and for helping to improve the manuscript. This work was supported by the BIL Bioinformatics Research Project of Pays de la Loire Region, France. The authors are also grateful to Carsten Poulsen (Aalborg University, Denmark) for providing the Danish beer data, Yi Wang (National University of Singapore) for the LTAB algorithm, Tao Chen (EMC Corporation, Beijing, China) for the EAST algorithm, Stefan Harmeling (Max Planck Institute, Germany) for the BIN-A and BIN-G algorithms, and Myung Jin Choi (Two Sigma Investments, USA) and Vincent Tan (University of Wisconsin-Madison, USA) for the RG, CLRG and regCLRG algorithms."}, {"heading": "Appendix A. Online Resources Mentioned", "text": "Software:\n\u2212 BIN-A, BIN-G, CL and LCM: http://people.kyb.tuebingen.mpg.de/harmeling/code/ltt-1.4.tar\n\u2212 CFHLC: https://sites.google.com/site/raphaelmouradeng/home/programs\n\u2212 DHC, SHC and HSHC: http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (hlcm-distribute.zip) http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (toolBox.zip)\n\u2212 EAST: http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (EAST.zip)\n\u2212 Lantern: http://www.cse.ust.hk/faculty/lzhang/ltm/index.htm (Lantern2.0-beta.exe)\n\u2212 NJ, RG, CLRG and regCLRG: http://people.csail.mit.edu/myungjin/latentTree.html\n\u2212 NJ (fast implementation): http://nimbletwist.com/software/ninja\nAll datasets used for the algorithm comparison are available at : https://sites.google.com/site/raphaelmouradeng/home/programs"}, {"heading": "Appendix B. Supplemental Material", "text": ""}, {"heading": "B.1 Experiments on Parameter Learning with EM", "text": "We studied the number of random restarts required, in practice, to obtain the convergence of EM (Section 3.1.1) and LCMB-EM (Section 3.1.2) to the optimal solution. BIC scores are presented in Figure 13. For EM, we used the method of Chickering and Heckerman (1997) which is implemented in the software LTAB (Wang et al., 2008). We analyzed three\ndatasets: two small ones (BinTree and Asia) and a large one (Tree). For the first two, convergence was achieved after 20 and 2000 restarts, respectively. For the large dataset, convergence is never achieved, even after 5000 restarts. For LCMB-EM, we used the software BIN-A (Harmeling & Williams, 2011). We analyzed three datasets: one small one (Asia) and two large ones (Tree, Alarm). Convergence is achieved for the first two datasets with only one parameter initialization, whereas for the later which is the largest, convergence is never achieved (we were not able to assess with more than 1000 restarts because of a prohibitive running time)."}, {"heading": "B.2 Time Complexity", "text": "We recall the reader that n is the number of variables (input data), N the number of observations and s the number of steps (for search-based algorithms). The time complexities of generic algorithms for LTM learning are detailed as follows:\n\u2212 Naive greedy search (NGS). There are O(s) steps needed for the convergence of search-based methods. For each step, there are O(n2) new models generated through the use of 3 operators: addition/removal of an LV and node relocation (Zhang, 2004). For each model, the cardinality is optimized for each LV, so that O(n2) new models are generated (Zhang, 2004). For each model, parameters are learned using EM, which is\nachieved in O(nN). Thus, the overall complexity is : O(s) \u2217O(n2) \u2217O(n2) \u2217O(nN) = O(sn5N).\n\u2212 Advanced greedy search (AGS), Algorithm 2. There are O(s) steps needed for the convergence of search-based methods. For each step, there are O(n2) new models generated through the use of 5 operators: addition/removal of an LV, node relocation and addition/dismissal of a state relative to an LV (Zhang & Kocka, 2004b). For each model, model evaluation is realized through local EM in O(N). After choosing the best model at each step, parameters are learned using EM, which is achieved in O(nN). Thus, the overall complexity is : O(s)\u2217(O(n2)\u2217O(N)+O(nN)) = O(sn2N).\n\u2212 Agglomerative hierarchical clustering-based learning (AHCB). The agglomerative hierarchical clustering is achieved in O(n2N)19. LV cardinality and parameters can be learned in O(N) thanks to LCM parameter learning. There are O(n) LVs, thus the complexity is (O(N) \u2217 O(n)). A final global EM parameter learning is done in O(nN). Thus, the overall complexity is : O(n2N)+(O(N)\u2217O(n))+O(nN) = O(n2N).\n\u2212 Latent class model-based LTM learning (LCMB-LTM), Algorithm 3. Pairwise mutual information values are computed in O(n2N). LCM is learned in O(N). LCM learning is called O(n) times, i.e. for each new LV added to the model. LV cardinality and parameters are learned during LCM learning. A final global EM parameter learning is done in O(nN). Thus, the overall complexity is : O(n2N) + (O(N) \u2217O(n)) +O(nN) = O(n2N).\n\u2212 Neighbor joining (NJ). To learn the structure, there are O(n) steps. At each step, pairwise distances are computed in O(n2N). Structure learning thus requires O(n3N) computations. After learning the structure, parameters can be learned with EM or LCMB-EM in O(nN). Thus, the overall complexity is : O(n3N)+O(nN) = O(n3N).\n\u2212 Distance-based general LTM learning (DBG), Algorithm 4. First the distances are computed in O(n3N). If the minimum spanning tree is learned before, the complexity is reduced to O(n2N). Then, to learn the structure, testing child-parent and sibling relations necessitates O(n4) operations in the worst case, i.e. when the tree is a hidden Markov model. Parameters can be learned with EM or LCMB-EM in O(nN). Thus, the overall complexity is : O(n3N) +O(n4) +O(nN) = O(n3N + n4) or O(n2N) +O(n4) +O(nN) = O(n2N + n4)."}, {"heading": "B.3 Description of Datasets Used for Literature Algorithm Comparison", "text": "Small datasets (n \u2264 10 variables):\n\u2212 BinTree. Datasets generated using a binary tree on 7 variables (4 leaves and 3 internal nodes), each having eight states. Only leaf data are used. The train and test datasets both consist of 500 observations. The model comes from the work of Harmeling and Williams (2011).\n19. The complexity of agglomerative hierarchical clustering is O(n2N) using the single linkage criterion. The complexity is higher for other criteria.\n\u2212 BinForest. Datasets generated from a binary forest composed of two trees. One tree has 3 variables (2 leaves and 1 internal node), the other one has 5 variables (3 leaves and 2 internal nodes). Only leaf data are used. The train and test datasets both consist of 500 observations. The model comes from the work of Harmeling and Williams (2011).\n\u2212 Asia. Datasets generated using the well-known Asia network containing 8 binary OVs. The train and test datasets both consist of 100 observations.\n\u2212 Hannover. Real dataset containing 5 binary variables. The dataset has been split into a train dataset and a test dataset. They consist of 3573 and 3589 observations, respectively. The dataset comes from the work of Zhang (2004).\n\u2212 Car. Real dataset containing 7 variables. The dataset has been split into a train dataset and a test dataset. They consist of 859 and 869 observations, respectively. The dataset is available at : http://archive.ics.uci.edu/ml/.\nLarge datasets (10 \u2264 n \u2264 100 variables): \u2212 Tree. Datasets generated using a tree on 50 variables (19 leaves and 31 internal\nnodes). Only leaf data are used. The train and test datasets both consist of 500 observations.\n\u2212 Forest. Datasets generated using a tree on 50 variables (20 leaves and 30 internal nodes). Only leaf data are used. The train and test datasets both consist of 500 observations.\n\u2212 Alarm. Datasets generated using the well-known Alarm network containing 37 OVs. The train and test datasets both consist of 1000 observations.\n\u2212 Coil-42. Real dataset containing 42 variables. The dataset has been split into a train dataset and a test dataset. They consist of 5822 and 4000 observations, respectively. The dataset comes from the work of Zhang and Kocka (2004b).\n\u2212 NewsGroup. Real dataset containing 100 binary variables. The dataset has been split into a train dataset and a test dataset. They both consist of 8121 observations. The dataset is available at : http://cs.nyu.edu/roweis/data/20news w100.mat.\nVery large datasets (n > 100 variables):\n\u2212 HapGen. Datasets generated over 1000 genetic variables using the HAPGEN software (Spencer, Su, Donnelly, & Marchini, 2009). The train and test datasets both consist of 1000 observations.\n\u2212 HapMap. Real dataset containing 10000 binary variables. The dataset has been split into a train dataset and a test dataset. They consist of 118 and 116 observations, respectively. The dataset comes from HapMap phase III (The International HapMap Consortium, 2007) and concerns Utah residents with Northern and Western European ancestry (CEU)."}], "references": [{"title": "Human motion analysis: A review", "author": ["J. Aggarwal", "Q. Cai"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Aggarwal and Cai,? \\Q1999\\E", "shortCiteRegEx": "Aggarwal and Cai", "year": 1999}, {"title": "Statistical predictor identification", "author": ["H. Akaike"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Akaike,? \\Q1970\\E", "shortCiteRegEx": "Akaike", "year": 1970}, {"title": "Spectral methods for learning multivariate latent tree structure", "author": ["A. Anandkumar", "K. Chaudhuri", "D. Hsu", "S.M. Kakade", "L. Song", "T. Zhang"], "venue": "In Twenty-Fifth Conference in Neural Information Processing Systems (NIPS-11)", "citeRegEx": "Anandkumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2011}, {"title": "The performance of neighbor-joining methods of phylogenetic", "author": ["K. Atteson"], "venue": "reconstruction. Algorithmica,", "citeRegEx": "Atteson,? \\Q1999\\E", "shortCiteRegEx": "Atteson", "year": 1999}, {"title": "Inferring parameters and structure of latent variable models by variational Bayes", "author": ["H. Attias"], "venue": "In Proceedings of the 15th Conference on Uncertainty and Artificial Intelligence", "citeRegEx": "Attias,? \\Q1999\\E", "shortCiteRegEx": "Attias", "year": 1999}, {"title": "Haploview: analysis and visualization of LD and haplotype", "author": ["J.C. Barrett", "B. Fry", "J. Maller", "M.J. Daly"], "venue": "maps. Bioinformatics,", "citeRegEx": "Barrett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2005}, {"title": "Adaptive probabilistic networks with hidden variables", "author": ["J. Binder", "D. Koller", "S. Russel", "K. Kanazawa"], "venue": "Machine Learning,", "citeRegEx": "Binder et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Binder et al\\.", "year": 1997}, {"title": "Taxonomy with confidence", "author": ["J.A. Cavender"], "venue": "Mathematical Biosciences,", "citeRegEx": "Cavender,? \\Q1978\\E", "shortCiteRegEx": "Cavender", "year": 1978}, {"title": "Quartet-based learning of hierarchical latent class models: Discovery of shallow latent variables", "author": ["T. Chen", "N.L. Zhang"], "venue": "In Proceedings of 9th International Symposium on Artificial Intelligence and Mathematics", "citeRegEx": "Chen and Zhang,? \\Q2006\\E", "shortCiteRegEx": "Chen and Zhang", "year": 2006}, {"title": "Search-based learning of latent tree models", "author": ["T. Chen"], "venue": "Ph.D. thesis,", "citeRegEx": "Chen,? \\Q2008\\E", "shortCiteRegEx": "Chen", "year": 2008}, {"title": "Model-based multidimensional clustering of categorical data", "author": ["T. Chen", "N.L. Zhang", "T. Liu", "K.M. Poon", "Y. Wang"], "venue": "Artificial Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Efficient model evaluation in the searchbased approach to latent structure discovery", "author": ["T. Chen", "N.L. Zhang", "Y. Wang"], "venue": "In Proceedings of the Fourth European Workshop on Probabilistic Graphical Models", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables", "author": ["D.M. Chickering", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "Chickering and Heckerman,? \\Q1997\\E", "shortCiteRegEx": "Chickering and Heckerman", "year": 1997}, {"title": "Learning latent tree graphical models", "author": ["M.J. Choi", "V.Y. Tan", "A. Anandkumar", "A.S. Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chow and Liu,? \\Q1968\\E", "shortCiteRegEx": "Chow and Liu", "year": 1968}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks", "author": ["G.F. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "Cooper,? \\Q1990\\E", "shortCiteRegEx": "Cooper", "year": 1990}, {"title": "Evolutionary trees and the Ising model on the Bethe lattice: A proof of Steel\u2019s conjecture", "author": ["C. Daskalakis", "E. Mossel", "S. Roch"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Daskalakis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daskalakis et al\\.", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Learning the dimensionality of hidden variables", "author": ["G. Elidan", "N. Friedman"], "venue": "In Proceedings of the 17th Conference on Uncertainty and Artificial Intelligence", "citeRegEx": "Elidan and Friedman,? \\Q2001\\E", "shortCiteRegEx": "Elidan and Friedman", "year": 2001}, {"title": "A few logs suffice to build (almost) all trees: Part II", "author": ["P.L. Erdos", "L.A. Szekely", "M.A. Steel", "T.J. Warnow"], "venue": "Theoretical Computer Science,", "citeRegEx": "Erdos et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Erdos et al\\.", "year": 1999}, {"title": "Constructing Bayesian networks to predict uncollectible telecommunications accounts", "author": ["K.J. Ezawa", "S.W. Norton"], "venue": "IEEE Expert,", "citeRegEx": "Ezawa and Norton,? \\Q1996\\E", "shortCiteRegEx": "Ezawa and Norton", "year": 1996}, {"title": "A probability model for inferring evolutionary trees", "author": ["J.S. Farris"], "venue": "Systematic Zoology,", "citeRegEx": "Farris,? \\Q1973\\E", "shortCiteRegEx": "Farris", "year": 1973}, {"title": "Inferring phylogenies (2 edition)", "author": ["J. Felsenstein"], "venue": "Sinauer Associates", "citeRegEx": "Felsenstein,? \\Q2003\\E", "shortCiteRegEx": "Felsenstein", "year": 2003}, {"title": "A structural EM algorithm for phylogenetic inference", "author": ["N. Friedman", "M. Ninio", "I. Pe\u2019er", "T. Pupko"], "venue": "Journal of Computational Biology,", "citeRegEx": "Friedman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2002}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning,", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Asymptotic model selection for directed networks with hidden variables", "author": ["D. Geiger", "D. Heckerman", "C. Meek"], "venue": "In Proceedings of Twelfth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Geiger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 1996}, {"title": "Multivariate data analysis (7 edition)", "author": ["J.F. Hair", "W.C. Black", "B.J. Babin", "R.E. Anderson"], "venue": null, "citeRegEx": "Hair et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hair et al\\.", "year": 2009}, {"title": "Greedy learning of binary latent trees", "author": ["S. Harmeling", "C.K.I. Williams"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Harmeling and Williams,? \\Q2011\\E", "shortCiteRegEx": "Harmeling and Williams", "year": 2011}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In The 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Learning hierarchical Bayesian networks for large-scale data analysis", "author": ["Hwang", "K.-B", "Kim", "B.-H", "Zhang", "B.-T"], "venue": "In International Conference on Neural Information Processing", "citeRegEx": "Hwang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2006}, {"title": "A computation model for causal and diagnostic reasoning in inference systems", "author": ["J.H. Kim", "J. Pearl"], "venue": "In Proceedings of the 8th International Joint Conference on Artificial Intelligence", "citeRegEx": "Kim and Pearl,? \\Q1983\\E", "shortCiteRegEx": "Kim and Pearl", "year": 1983}, {"title": "GERBIL: Genotype resolution and block identification using likelihood", "author": ["G. Kimmel", "R. Shamir"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Kimmel and Shamir,? \\Q2005\\E", "shortCiteRegEx": "Kimmel and Shamir", "year": 2005}, {"title": "Latent tree copulas", "author": ["S. Kirshner"], "venue": "In Proceedings of the Sixth European Workshop on Probabilistic Graphical Models (PGM-12)", "citeRegEx": "Kirshner,? \\Q2012\\E", "shortCiteRegEx": "Kirshner", "year": 2012}, {"title": "Probabilistic graphical models: Principles and techniques (adaptive computation and machine learning)", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Information theory and statistical learning, chap. MIC: Mutual information based hierarchical clustering", "author": ["A. Kraskov", "P. Grassberger"], "venue": null, "citeRegEx": "Kraskov and Grassberger,? \\Q2009\\E", "shortCiteRegEx": "Kraskov and Grassberger", "year": 2009}, {"title": "Using hidden nodes in Bayesian networks", "author": ["Kwoh", "C.-K", "D.F. Gillies"], "venue": "Artificial Intelligence,", "citeRegEx": "Kwoh et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kwoh et al\\.", "year": 1996}, {"title": "Reconstructing evolutionary trees from DNA and protein sequences: Paralinear distances", "author": ["J.A. Lake"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Lake,? \\Q1994\\E", "shortCiteRegEx": "Lake", "year": 1994}, {"title": "Latent classification models for binary data", "author": ["H. Langseth", "T.D. Nielsen"], "venue": "Pattern Recognition,", "citeRegEx": "Langseth and Nielsen,? \\Q2009\\E", "shortCiteRegEx": "Langseth and Nielsen", "year": 2009}, {"title": "The EM algorithm for graphical association models with missing data", "author": ["S.L. Lauritzen"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Lauritzen,? \\Q1995\\E", "shortCiteRegEx": "Lauritzen", "year": 1995}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Forest density estimation", "author": ["H. Liu", "M. Xu", "H. Gu", "A. Gupta", "J. Lafferty", "L. Wasserman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "A novel LTM-based method for multidimensional clustering", "author": ["T.F. Liu", "N.L. Zhang", "A.H. Liu", "L.K.M. Poon"], "venue": "In Proceedings of the Sixth European Workshop on Probabilistic Graphical Models (PGM-12)", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Discrete factor analysis: Learning hidden variables in Bayesian network", "author": ["J. Martin", "K. Vanlehn"], "venue": "Tech. rep.,", "citeRegEx": "Martin and Vanlehn,? \\Q1995\\E", "shortCiteRegEx": "Martin and Vanlehn", "year": 1995}, {"title": "Using a Bayesian belief network to aid differential diagnosis of tropical bovine diseases", "author": ["I.J. McKendrick", "G. Gettinbya", "Y. Gua", "S.W.J. Reidb", "C.W. Revie"], "venue": "Preventive Veterinary Medicine,", "citeRegEx": "McKendrick et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McKendrick et al\\.", "year": 2000}, {"title": "Learning nonsingular phylogenies and hidden Markov models", "author": ["E. Mossel", "S. Roch"], "venue": "The Annals of Applied Probability,", "citeRegEx": "Mossel and Roch,? \\Q2006\\E", "shortCiteRegEx": "Mossel and Roch", "year": 2006}, {"title": "Robust estimation of latent tree graphical models: Inferring hidden states with inexact parameters", "author": ["E. Mossel", "S. Roch", "A. Sly"], "venue": null, "citeRegEx": "Mossel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mossel et al\\.", "year": 2011}, {"title": "Visualization of pairwise and multilocus linkage disequilibrium structure using latent forests", "author": ["R. Mourad", "C. Sinoquet", "C. Dina", "P. Leray"], "venue": "PLoS ONE,", "citeRegEx": "Mourad et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2011}, {"title": "A hierarchical Bayesian network approach for linkage disequilibrium modeling and data-dimensionality reduction prior to genomewide association studies", "author": ["R. Mourad", "C. Sinoquet", "P. Leray"], "venue": "BMC Bioinformatics,", "citeRegEx": "Mourad et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2011}, {"title": "A spectral algorithm for latent tree graphical models", "author": ["A.P. Parikh", "L. Song", "E.P. Xing"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-2011)", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "Recognition of two-person interactions using a hierarchical Bayesian network", "author": ["S. Park", "J.K. Aggarwal"], "venue": "In The first ACM International Workshop on Video Surveillance", "citeRegEx": "Park and Aggarwal,? \\Q2003\\E", "shortCiteRegEx": "Park and Aggarwal", "year": 2003}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Parzen,? \\Q1962\\E", "shortCiteRegEx": "Parzen", "year": 1962}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Variable selection in modelbased clustering: To do or to facilitate", "author": ["L.K.M. Poon", "N.L. Zhang", "T. Chen", "Y. Wang"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-2010)", "citeRegEx": "Poon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2010}, {"title": "Shortest connection networks and some generalizations", "author": ["R.C. Prim"], "venue": "Bell System Technical Journal,", "citeRegEx": "Prim,? \\Q1957\\E", "shortCiteRegEx": "Prim", "year": 1957}, {"title": "Choosing models for cross-classifications", "author": ["A.E. Raftery"], "venue": "American Sociological Review,", "citeRegEx": "Raftery,? \\Q1986\\E", "shortCiteRegEx": "Raftery", "year": 1986}, {"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Rosenblatt,? \\Q1956\\E", "shortCiteRegEx": "Rosenblatt", "year": 1956}, {"title": "The neighbor-joining method: A new method for reconstructing phylogenetic trees", "author": ["N. Saitou", "M. Nei"], "venue": "Molecular Biology and Evolution,", "citeRegEx": "Saitou and Nei,? \\Q1987\\E", "shortCiteRegEx": "Saitou and Nei", "year": 1987}, {"title": "Estimating the dimension of a model", "author": ["G. Schwartz"], "venue": "The Annals of Statistics,", "citeRegEx": "Schwartz,? \\Q1978\\E", "shortCiteRegEx": "Schwartz", "year": 1978}, {"title": "Kernel embeddings of latent tree graphical models", "author": ["L. Song", "A. Parikh", "E. Xing"], "venue": "In Twenty-Fifth Conference in Neural Information Processing Systems (NIPS-11)", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Designing genome-wide association studies: sample size, power, imputation, and the choice of genotyping chip", "author": ["C.C. Spencer", "Z. Su", "P. Donnelly", "J. Marchini"], "venue": "PLoS Genetics,", "citeRegEx": "Spencer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spencer et al\\.", "year": 2009}, {"title": "My favourite conjecture. http://www.math.canterbury.ac.nz/m.steel/files/misc/conjecture.pdf", "author": ["M. Steel"], "venue": null, "citeRegEx": "Steel,? \\Q2001\\E", "shortCiteRegEx": "Steel", "year": 2001}, {"title": "The complexity of reconstructing trees from qualitative characters and subtrees", "author": ["M. Steel"], "venue": "Journal of Classification,", "citeRegEx": "Steel,? \\Q1992\\E", "shortCiteRegEx": "Steel", "year": 1992}, {"title": "Learning high-dimensional Markov forest distributions: Analysis of error rates", "author": ["V.Y.F. Tan", "A. Anandkumar", "A. Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2011}, {"title": "Latent tree classifier. In European Conferences on Symbolic and Quantitative Approaches to Reasoning with Uncertainty", "author": ["Y. Wang", "N.L. Zhang", "T. Chen", "L.K.M. Poon"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Severity of local maxima for the EM algorithm: Experiences with hierarchical latent class models", "author": ["Y. Wang", "N.L. Zhang"], "venue": "In Proceedings of the Third European Workshop on Probabilistic Graphical Models (PGM-06)", "citeRegEx": "Wang and Zhang,? \\Q2006\\E", "shortCiteRegEx": "Wang and Zhang", "year": 2006}, {"title": "Latent tree models and approximate inference in Bayesian networks", "author": ["Y. Wang", "N.L. Zhang", "T. Chen"], "venue": "Journal of Articial Intelligence Research,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Not so naive Bayes: Aggregating onedependence estimators", "author": ["G.I. Webb", "J.R. Boughton", "Z. Wang"], "venue": "Machine Learning,", "citeRegEx": "Webb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2005}, {"title": "Large-scale neighbor-joining with NINJA", "author": ["T.J. Wheeler"], "venue": "In Proceedings of the 9th Workshop on Algorithms in Bioinformatics", "citeRegEx": "Wheeler,? \\Q2009\\E", "shortCiteRegEx": "Wheeler", "year": 2009}, {"title": "On convergence properties of the EM algorithm for Gaussian mixtures", "author": ["L. Xu", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Xu and Jordan,? \\Q1996\\E", "shortCiteRegEx": "Xu and Jordan", "year": 1996}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Xu and Wunsch,? \\Q2005\\E", "shortCiteRegEx": "Xu and Wunsch", "year": 2005}, {"title": "Clustering (illustrated edition)", "author": ["R. Xu", "D.C. Wunsch"], "venue": null, "citeRegEx": "Xu and Wunsch,? \\Q2008\\E", "shortCiteRegEx": "Xu and Wunsch", "year": 2008}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Effective dimensions of hierarchical latent class models", "author": ["N.L. Zhang", "T. Kocka"], "venue": "Journal of Articial Intelligence Research,", "citeRegEx": "Zhang and Kocka,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Kocka", "year": 2004}, {"title": "Efficient learning of hierarchical latent class models", "author": ["N.L. Zhang", "T. Kocka"], "venue": "In Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "Zhang and Kocka,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Kocka", "year": 2004}, {"title": "Latent variable discovery in classification models", "author": ["N.L. Zhang", "T.D. Nielsen", "F.V. Jensen"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Zhang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2004}, {"title": "Discovery of latent structures: Experience with the CoIL Challenge 2000 data set", "author": ["N.L. Zhang", "Y. Wang", "T. Chen"], "venue": "Journal of Systems Science and Complexity,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Latent tree models and diagnosis in traditional Chinese medicine", "author": ["N.L. Zhang", "S. Yuan", "T. Chen", "Y. Wang"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 22, "context": "A subclass of LTMs was first developed in the phylogenetic community (Felsenstein, 2003).", "startOffset": 69, "endOffset": 88}, {"referenceID": 71, "context": "LTM has been previously called \u201chierarchical latent class model\u201d (Zhang, 2004), but this name has been discarded because the model does not inherently reveal a hierarchy.", "startOffset": 65, "endOffset": 78}, {"referenceID": 71, "context": "The tree can be either directed (i.e. a Bayesian network; Zhang, 2004) or undirected (i.", "startOffset": 32, "endOffset": 70}, {"referenceID": 1, "context": "In theory, every score, such as Akaike information criterion (AIC) (Akaike, 1970) and Bayesian information criterion (BIC) (Schwartz, 1978), could be used for scoring LTMs.", "startOffset": 67, "endOffset": 81}, {"referenceID": 57, "context": "In theory, every score, such as Akaike information criterion (AIC) (Akaike, 1970) and Bayesian information criterion (BIC) (Schwartz, 1978), could be used for scoring LTMs.", "startOffset": 123, "endOffset": 139}, {"referenceID": 51, "context": "Two conditions ensure that an LTM does not include any redundant LVs (Pearl, 1988):", "startOffset": 69, "endOffset": 82}, {"referenceID": 51, "context": "The notion of parsimony is also called minimality by Pearl (1988).", "startOffset": 53, "endOffset": 66}, {"referenceID": 71, "context": "An LTM is regular (Zhang, 2004) if for any LV H:", "startOffset": 18, "endOffset": 31}, {"referenceID": 71, "context": "6 Trade-off between Latent Variable Complexity and Partial Structure Complexity Zhang and Kocka (2004b) distinguished two kinds of model complexity in LTM: latent variable complexity refers to LV cardinalities while partial structure complexity4 is", "startOffset": 80, "endOffset": 104}, {"referenceID": 71, "context": "In their paper, Zhang and Kocka (2004b) called it structure complexity.", "startOffset": 16, "endOffset": 40}, {"referenceID": 38, "context": "To solve the problem, one can use expectation-maximization (EM), the most popular algorithm for learning parameters in the face of LVs (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995).", "startOffset": 135, "endOffset": 184}, {"referenceID": 64, "context": "Wang and Zhang (2006) showed that a few random restarts suffice when the LTM is small and variables are strongly dependent with each other.", "startOffset": 0, "endOffset": 22}, {"referenceID": 27, "context": "This learning procedure is very similar to the one proposed for binary trees by Harmeling and Williams (2011).", "startOffset": 80, "endOffset": 110}, {"referenceID": 48, "context": "Recently, Parikh et al. (2011) applied spectral techniques to LTM parameter learning.", "startOffset": 10, "endOffset": 31}, {"referenceID": 44, "context": "alleviates the restriction of the approach of Mossel et al. (2006) requiring that all conditional probability tables should be invertible, and generalizes the method of Hsu et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 28, "context": "(2006) requiring that all conditional probability tables should be invertible, and generalizes the method of Hsu et al. (2009) specific to hidden Markov models.", "startOffset": 109, "endOffset": 127}, {"referenceID": 28, "context": "(2006) requiring that all conditional probability tables should be invertible, and generalizes the method of Hsu et al. (2009) specific to hidden Markov models. Parikh et al. (2011) reformulated the message passing algorithm using an algebraic formulation:", "startOffset": 109, "endOffset": 182}, {"referenceID": 48, "context": "We refer to the work of Parikh et al. (2011) for more details about the singular value decomposition and spectral algorithm.", "startOffset": 24, "endOffset": 45}, {"referenceID": 4, "context": "For a Bayesian learning, variational Bayes (Attias, 1999) offers a counterpart of EM.", "startOffset": 43, "endOffset": 57}, {"referenceID": 10, "context": "Second, AGS follows a grow-restructure-thin strategy to reduce again the complexity of the search space (Chen, Zhang, & Wang, 2008; Chen et al., 2012).", "startOffset": 104, "endOffset": 150}, {"referenceID": 71, "context": "When starting from the simplest solution (an LCM), Zhang and Kocka (2004b) observed that the comparison of the BIC scores between the candidate model T \u2032 and the current one T might not be a relevant criterion.", "startOffset": 51, "endOffset": 75}, {"referenceID": 65, "context": "OVs and internal nodes are LVs (Wang et al., 2008; Harmeling & Williams, 2011).", "startOffset": 31, "endOffset": 78}, {"referenceID": 27, "context": "Harmeling and Williams (2011) justified selecting the pair of variables showing the highest MI at each step of LCMB-LTM.", "startOffset": 0, "endOffset": 30}, {"referenceID": 61, "context": "For instance, Wang et al. (2008) first learn a binary tree.", "startOffset": 14, "endOffset": 33}, {"referenceID": 40, "context": "The Bridged Island (BI) algorithm by Liu et al. (2012) takes a slightly different approach.", "startOffset": 37, "endOffset": 55}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6).", "startOffset": 17, "endOffset": 47}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this means that the LV is not useful to the model under construction and the algorithm stops. On the other hand, after partitioning variables in cliques (which replaces the step in line 5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are discovered.", "startOffset": 17, "endOffset": 394}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this means that the LV is not useful to the model under construction and the algorithm stops. On the other hand, after partitioning variables in cliques (which replaces the step in line 5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are discovered. To build an LFM, one can also first construct an LTM and then use the independence testing method of Tan et al. (2011) for pruning non-significant edges.", "startOffset": 17, "endOffset": 570}, {"referenceID": 27, "context": "On the one hand, Harmeling and Williams (2011) check the optimal cardinality of the current LV H (additional step after line 6). If its optimal cardinality equals 1, this means that the LV is not useful to the model under construction and the algorithm stops. On the other hand, after partitioning variables in cliques (which replaces the step in line 5), the algorithm of Mourad et al. (2011) terminates when only single-node cliques are discovered. To build an LFM, one can also first construct an LTM and then use the independence testing method of Tan et al. (2011) for pruning non-significant edges. This method provides guarantees to satisfy structural and risk consistencies. Similar works for non-parametric analysis have also been developed by Liu et al. (2011). It is worth mentioning that, to ensure model parsimony, the pruning of non-significant edges in an LTM should be followed by the removal of latent nodes which are no longer connected to a minimum of three nodes.", "startOffset": 17, "endOffset": 771}, {"referenceID": 22, "context": "This class of methods has been originally developed for phylogenetics (Felsenstein, 2003).", "startOffset": 70, "endOffset": 89}, {"referenceID": 36, "context": "binary variables (Lake, 1994).", "startOffset": 17, "endOffset": 29}, {"referenceID": 13, "context": "For a special case of discrete tree models, called symmetric discrete tree models (Choi et al., 2011), distance has a simpler form.", "startOffset": 82, "endOffset": 101}, {"referenceID": 7, "context": "Under the Cavender-Farris model of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to guarantee that maxi,j |dij \u2212 d\u0302ij| < with a probability at least \u03b4 if: N \u2265 2 ln ( 2n \u03b4 )", "startOffset": 45, "endOffset": 75}, {"referenceID": 21, "context": "Under the Cavender-Farris model of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to guarantee that maxi,j |dij \u2212 d\u0302ij| < with a probability at least \u03b4 if: N \u2265 2 ln ( 2n \u03b4 )", "startOffset": 45, "endOffset": 75}, {"referenceID": 3, "context": "Under the Cavender-Farris model of evolution (Cavender, 1978; Farris, 1973), Atteson (1999) showed that it is possible to guarantee that maxi,j |dij \u2212 d\u0302ij| < with a probability at least \u03b4 if: N \u2265 2 ln ( 2n \u03b4 )", "startOffset": 77, "endOffset": 92}, {"referenceID": 60, "context": "(2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n.", "startOffset": 37, "endOffset": 50}, {"referenceID": 16, "context": "Daskalakis et al. (2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Daskalakis et al. (2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n. Recently, Mossel et al. (2011) proved that log n suffices when discretization is not assumed.", "startOffset": 0, "endOffset": 270}, {"referenceID": 16, "context": "Daskalakis et al. (2009) proved the Steel\u2019s conjecture (Steel, 2001) which states that if the mutation probabilities on all edges of the tree are less than p\u2217 = ( \u221a 2 \u2212 1)/23/2 and are discretized, then the tree can be recovered in log n. Recently, Mossel et al. (2011) proved that log n suffices when discretization is not assumed. In phylogenetics, the scientist is often faced with a set of different trees11 and the construction of a consensus tree is thus required. The computational complexity of this construction has been studied and a polynomial algorithm has been proposed by Steel et al. (1992).", "startOffset": 0, "endOffset": 606}, {"referenceID": 13, "context": "In this subsection, we present the latest developments of Choi et al. (2011) for general LTM learning, i.", "startOffset": 58, "endOffset": 77}, {"referenceID": 13, "context": "In practice, Choi et al. (2011) restricted the learning to symmetric discrete distributions13 (see definition in Subsection Distances between Variables).", "startOffset": 13, "endOffset": 32}, {"referenceID": 62, "context": "This sample complexity is equal to the one of the Chow-Liu algorithm (Tan et al., 2011).", "startOffset": 69, "endOffset": 87}, {"referenceID": 13, "context": "To diminish the computational complexity of DBG, Choi et al. (2011) propose to first learn a minimum spanning tree (MST) based on distances between OVs.", "startOffset": 49, "endOffset": 68}, {"referenceID": 19, "context": "Another important improvement of their method is that sample complexity bound is given in terms of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011).", "startOffset": 212, "endOffset": 251}, {"referenceID": 13, "context": "Another important improvement of their method is that sample complexity bound is given in terms of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011).", "startOffset": 212, "endOffset": 251}, {"referenceID": 55, "context": "(2011) proposed a non-parametric learning based on kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962).", "startOffset": 83, "endOffset": 115}, {"referenceID": 50, "context": "(2011) proposed a non-parametric learning based on kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962).", "startOffset": 83, "endOffset": 115}, {"referenceID": 2, "context": "On the one hand, Anandkumar et al. (2011) addressed the multivariate setting where observed and latent nodes are random vectors rather than scalars.", "startOffset": 17, "endOffset": 42}, {"referenceID": 2, "context": "On the one hand, Anandkumar et al. (2011) addressed the multivariate setting where observed and latent nodes are random vectors rather than scalars. Their approach can deal with general linear models containing both categorical and continuous variables. Another important improvement of their method is that sample complexity bound is given in terms of natural correlation conditions that generalize the more restrictive effective depth conditions of previous works (Erdos et al., 1999; Choi et al., 2011). The proposed extension consists in replacing the step in line 5 of Algorithm 4 by a quartet test14 relying on spectral techniques (more specifically, canonical correlation analysis; Hair, Black, Babin, and Anderson, 2009). Given four observed variables {X1,X2,X3,X4}, the spectral quartet test distinguishes between the four possible tree topologies (see Figure 8). The correct topology is {{Xi,Xj}, {Xi\u2032 ,Xj\u2032}} if and only if: |E[XiX j ]|\u2217 |E[Xi\u2032X j ]|\u2217 > |E[Xi\u2032X j ]|\u2217 |E[XiX j ]|\u2217, (25) where |M |\u2217 := \u03a0`=1\u03c3`(M) is the product of the k largest singular values of matrix M and E[M ] is the expectation of M (estimated using the covariance matrix). On the other hand, Song et al. (2011) proposed a non-parametric learning based on kernel density estimation (KDE) (Rosenblatt, 1956; Parzen, 1962).", "startOffset": 17, "endOffset": 1195}, {"referenceID": 8, "context": "The first authors to adapt them to LTM learning were Chen and Zhang (2006).", "startOffset": 53, "endOffset": 75}, {"referenceID": 48, "context": "Similarly to the work of Parikh et al. (2011), message passing and parameter estimation are reformulated through tensor notation (see the work of Song et al.", "startOffset": 25, "endOffset": 46}, {"referenceID": 54, "context": "During LCM learning, LV cardinality can be determined through the examination of all possible values (up to a maximum) and then by choosing the one which maximizes a criterion, such as BIC (Raftery, 1986).", "startOffset": 189, "endOffset": 204}, {"referenceID": 71, "context": "However, this solution still remains computationally demanding (Zhang, 2004).", "startOffset": 63, "endOffset": 76}, {"referenceID": 54, "context": "During LCM learning, LV cardinality can be determined through the examination of all possible values (up to a maximum) and then by choosing the one which maximizes a criterion, such as BIC (Raftery, 1986). For each cardinality value, parameters are required to calculate the likelihood appearing in the optimization criterion. For this purpose, random restarts of EM are generally used to learn parameters with a low probability of getting trapped in local maxima. The drawback is that this method cannot be applied to LTM learning, because EM becomes time-consuming when there are several LVs. A better solution consists in using a greedy search approach, starting with a preset value of LV cardinality (generally equal to 2) and incrementing it to meet the optimal criterion. However, this solution still remains computationally demanding (Zhang, 2004). To tackle the issue of computational burden, several strategies have been proposed. For instance, one can simply set a small value for the LV cardinalities. Following this idea, Hwang and collaborators (2006) constrain LVs to binary variables.", "startOffset": 190, "endOffset": 1065}, {"referenceID": 63, "context": "More rigorous, Wang et al.\u2019s approach (2008) relies on regularity (see Section 2.", "startOffset": 15, "endOffset": 45}, {"referenceID": 45, "context": "In the context of large scale data analysis (several thousands of variables), Mourad et al. (2011) proposed to estimate the cardinality of an LV given the number of its children.", "startOffset": 78, "endOffset": 99}, {"referenceID": 71, "context": "NGS Score Tree O(sn5N) DHC (Zhang, 2004)", "startOffset": 27, "endOffset": 40}, {"referenceID": 10, "context": "2 Score Tree O(sn2N) HSHC (Zhang & Kocka, 2004b) EAST (Chen et al., 2012)", "startOffset": 54, "endOffset": 73}, {"referenceID": 65, "context": "AHCB Variable Forest O(n2N) LTAB (Wang et al., 2008) clustering BIN-A (Harmeling & Williams, 2011)", "startOffset": 33, "endOffset": 52}, {"referenceID": 46, "context": "clustering CFHLC (Mourad et al., 2011) BI (Liu et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 41, "context": ", 2011) BI (Liu et al., 2012)", "startOffset": 11, "endOffset": 29}, {"referenceID": 67, "context": "NJ Information Tree O(n3N) NINJA (Saitou & Nei, 1987; Wheeler, 2009) distance", "startOffset": 33, "endOffset": 68}, {"referenceID": 13, "context": "4 Information Tree O(n3N + n4) RG (Choi et al., 2011)", "startOffset": 34, "endOffset": 53}, {"referenceID": 13, "context": "distance O(n2N + n4) CLRG (Choi et al., 2011) regCLRG (Choi et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 13, "context": ", 2011) regCLRG (Choi et al., 2011)", "startOffset": 16, "endOffset": 35}, {"referenceID": 52, "context": "When the tree does not contain any LV, learning the model can be done efficiently in O(n2N) using the Prim\u2019s algorithm (1957). The situation is more complicated when the tree contains LVs.", "startOffset": 102, "endOffset": 126}, {"referenceID": 9, "context": "3: results from the work of Chen (2008).", "startOffset": 28, "endOffset": 40}, {"referenceID": 9, "context": "3: results from the work of Chen (2008).", "startOffset": 28, "endOffset": 40}, {"referenceID": 13, "context": "For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011). 16.", "startOffset": 68, "endOffset": 87}, {"referenceID": 13, "context": "For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011). 16. Although algorithms NJ, RG, CLRG and regCLRG can process any kind of data with shared state space (binary data, ternary data, ...), the implementation provided by Choi et al. (2011) can only process binary data.", "startOffset": 68, "endOffset": 274}, {"referenceID": 13, "context": "For a fair comparison, we used the implementation of NJ provided by Choi et al. (2011). 16. Although algorithms NJ, RG, CLRG and regCLRG can process any kind of data with shared state space (binary data, ternary data, ...), the implementation provided by Choi et al. (2011) can only process binary data. Hence the algorithms have not been applied to some datasets. We recall that RG, CLRG and regCLRG do not exactly learn an LTM but instead a tree whose all internal nodes are not compelled to be latent. 17. Although it is not shown in Tables 2 and 3, NJ, RG, CLRG and regCLRG were able to process 1000 binary variables in our experiments. 18. In the work of Mourad et al. (2011), CFHLC implements a window-based approach to scale very large datasets (n \u2265 100k variables).", "startOffset": 68, "endOffset": 681}, {"referenceID": 75, "context": "Thanks to LTMs, latent structure discovery has been applied to several fields: marketing (Zhang, Wang, & Chen, 2008), medicine (Zhang, Nielsen, & Jensen, 2004; Zhang et al., 2008), genetics (Hwang et al.", "startOffset": 127, "endOffset": 179}, {"referenceID": 29, "context": ", 2008), genetics (Hwang et al., 2006; Mourad et al., 2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Pe\u2019er, & Pupko, 2002).", "startOffset": 18, "endOffset": 59}, {"referenceID": 46, "context": ", 2008), genetics (Hwang et al., 2006; Mourad et al., 2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Pe\u2019er, & Pupko, 2002).", "startOffset": 18, "endOffset": 59}, {"referenceID": 22, "context": ", 2011) and phylogenetics (Felsenstein, 2003; Friedman, Ninio, Pe\u2019er, & Pupko, 2002).", "startOffset": 26, "endOffset": 84}, {"referenceID": 9, "context": "This application is called \u201cmultidimensional clustering\u201d and has been mainly explored by Chen et al. (2012). Let us illustrate LTM-based multidimensional clustering using dataset from a survey on Danish beer market consumption.", "startOffset": 89, "endOffset": 108}, {"referenceID": 65, "context": "Figure 12: Experiments on ALARM and BARLEY networks: a) running times for LTM learning using the LTAB algorithm (Wang et al., 2008), b) approximation accuracy of probabilistic inference and c) running time for inference.", "startOffset": 112, "endOffset": 131}, {"referenceID": 63, "context": "Figure 12: Experiments on ALARM and BARLEY networks: a) running times for LTM learning using the LTAB algorithm (Wang et al., 2008), b) approximation accuracy of probabilistic inference and c) running time for inference. Approximation accuracy is measured by the Kullback-Leibler divergence between approximate inferred distributions and exact inferred distributions obtained from clique tree propagation on the original BN. N and C designate the sample size and the maximal cardinality of latent variables, respectively. These results come from the work of Wang et al. (2008).", "startOffset": 113, "endOffset": 577}, {"referenceID": 15, "context": "Probabilistic inference in a general BN is known to be an NP-hard task (Cooper, 1990).", "startOffset": 71, "endOffset": 85}, {"referenceID": 48, "context": "However, recent spectral methods (Parikh et al., 2011) considerably reduced model learning phase, because they do not require to learn the model structure.", "startOffset": 33, "endOffset": 54}, {"referenceID": 14, "context": "To tackle this issue, one can approximate the original BN using a maximum weight spanning tree learned relying on Chow and Liu\u2019s algorithm (1968). The drawback is the risk of inaccuracy in inference results.", "startOffset": 114, "endOffset": 146}, {"referenceID": 14, "context": "To tackle this issue, one can approximate the original BN using a maximum weight spanning tree learned relying on Chow and Liu\u2019s algorithm (1968). The drawback is the risk of inaccuracy in inference results. In this context, LTM provides an efficient and simple solution, because: (i) thanks to its tree structure, the model allows linear computations with respect to the number of OVs, and at the same time, (ii) it can represent complex relations between OVs through multiple LVs. Because learning LTM before performing inference can be time-consuming Wang et al. (2008) propose the following strategy: first, offline model learning is performed, then answers to probabilistic queries are quickly computed online.", "startOffset": 114, "endOffset": 573}, {"referenceID": 63, "context": "Hence Wang et al. (2008) propose a tradeoff between inferential complexity and model approximation accuracy by fixing a maximal cardinality C for LVs.", "startOffset": 6, "endOffset": 25}, {"referenceID": 51, "context": "In Figure 12, the LTM-based method is compared to other standard inference approaches: the LCM-based approach, the Chow-Liu (CL) model-based approach and loopy belief propagation (LBP) (Pearl, 1988).", "startOffset": 185, "endOffset": 198}, {"referenceID": 65, "context": "Figure 12a reports running times for LTM learning using the LTAB algorithm (Wang et al., 2008).", "startOffset": 75, "endOffset": 94}, {"referenceID": 74, "context": "Naive Bayes has been generalized by introducing latent nodes as internal discrete nodes (Zhang et al., 2004) or continuous nodes (Langseth & Nielsen, 2009), mediating the relation between leaves and the class variable.", "startOffset": 88, "endOffset": 108}, {"referenceID": 62, "context": "Recently, Wang et al. (2011) proposed a classifier based on LTM.", "startOffset": 10, "endOffset": 29}, {"referenceID": 31, "context": "Kimmel and Shamir (2005) perform efficient haplotypic inference using a two-layer LFM.", "startOffset": 0, "endOffset": 25}, {"referenceID": 31, "context": "Kimmel and Shamir (2005) perform efficient haplotypic inference using a two-layer LFM. Finally, Zhang et al. (2008) applied LTMs to traditional Chinese medicine (TCM).", "startOffset": 0, "endOffset": 116}, {"referenceID": 13, "context": "For instance, a recent work developed LTM for continuous data analysis (Poon, Zhang, Chen, & Wang, 2010; Choi et al., 2011).", "startOffset": 71, "endOffset": 123}, {"referenceID": 29, "context": "Other authors investigated the relationships between LTM and ontology (Hwang et al., 2006), and LTM-based dependence visualization (Mourad, Sinoquet, Dina, & Leray, 2011).", "startOffset": 70, "endOffset": 90}, {"referenceID": 52, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 13, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 58, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 32, "context": "LTM for Continuous Data: Recently, LTM modeling has been applied to continuous data analysis (Poon et al., 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012).", "startOffset": 93, "endOffset": 166}, {"referenceID": 13, "context": ", 2010; Choi et al., 2011; Song et al., 2011; Kirshner, 2012). For instance, Poon et al. (2010) proposed a new model, called pouch latent tree model (PLTM).", "startOffset": 8, "endOffset": 96}, {"referenceID": 29, "context": "For instance, when applying LTM to a microarray dataset of yeast cell-cycle, Hwang et al. (2006) showed that some LVs are significantly related to specific gene ontology terms, such as organelle organization or cellular physiological process.", "startOffset": 77, "endOffset": 97}, {"referenceID": 46, "context": "Dependence Visualization: LTM provide a compact and interpretable view of dependences between variables, thanks to its graphical nature and its latent variables (Mourad et al., 2011).", "startOffset": 161, "endOffset": 182}, {"referenceID": 65, "context": "For EM, we used the method of Chickering and Heckerman (1997) which is implemented in the software LTAB (Wang et al., 2008).", "startOffset": 104, "endOffset": 123}, {"referenceID": 12, "context": "For EM, we used the method of Chickering and Heckerman (1997) which is implemented in the software LTAB (Wang et al.", "startOffset": 30, "endOffset": 62}, {"referenceID": 71, "context": "For each step, there are O(n2) new models generated through the use of 3 operators: addition/removal of an LV and node relocation (Zhang, 2004).", "startOffset": 130, "endOffset": 143}, {"referenceID": 71, "context": "For each model, the cardinality is optimized for each LV, so that O(n2) new models are generated (Zhang, 2004).", "startOffset": 97, "endOffset": 110}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011).", "startOffset": 33, "endOffset": 63}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011). \u2212 Asia.", "startOffset": 33, "endOffset": 63}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011). \u2212 Asia. Datasets generated using the well-known Asia network containing 8 binary OVs. The train and test datasets both consist of 100 observations. \u2212 Hannover. Real dataset containing 5 binary variables. The dataset has been split into a train dataset and a test dataset. They consist of 3573 and 3589 observations, respectively. The dataset comes from the work of Zhang (2004). \u2212 Car.", "startOffset": 33, "endOffset": 442}, {"referenceID": 27, "context": "The model comes from the work of Harmeling and Williams (2011). \u2212 Asia. Datasets generated using the well-known Asia network containing 8 binary OVs. The train and test datasets both consist of 100 observations. \u2212 Hannover. Real dataset containing 5 binary variables. The dataset has been split into a train dataset and a test dataset. They consist of 3573 and 3589 observations, respectively. The dataset comes from the work of Zhang (2004). \u2212 Car. Real dataset containing 7 variables. The dataset has been split into a train dataset and a test dataset. They consist of 859 and 869 observations, respectively. The dataset is available at : http://archive.ics.uci.edu/ml/. Large datasets (10 \u2264 n \u2264 100 variables): \u2212 Tree. Datasets generated using a tree on 50 variables (19 leaves and 31 internal nodes). Only leaf data are used. The train and test datasets both consist of 500 observations. \u2212 Forest. Datasets generated using a tree on 50 variables (20 leaves and 30 internal nodes). Only leaf data are used. The train and test datasets both consist of 500 observations. \u2212 Alarm. Datasets generated using the well-known Alarm network containing 37 OVs. The train and test datasets both consist of 1000 observations. \u2212 Coil-42. Real dataset containing 42 variables. The dataset has been split into a train dataset and a test dataset. They consist of 5822 and 4000 observations, respectively. The dataset comes from the work of Zhang and Kocka (2004b). \u2212 NewsGroup.", "startOffset": 33, "endOffset": 1451}], "year": 2013, "abstractText": "In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure a tree allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}