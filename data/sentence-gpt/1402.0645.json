{"id": "1402.0645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Local Gaussian Regression", "abstract": "Locally weighted regression was created as a nonparametric learning method that is computationally efficient, can learn from very large amounts of data and add data incrementally. An interesting feature of locally weighted regression is that it can work with spatially varying length scales, a beneficial property, for instance, in control problems. However, it does not provide a generative model for function values and requires training and test data to be generated identically, independently and explicitly to avoid bias. This is in contrast to an initialist model for linear regression, which uses a probabilistic model (see Appendix 2).\n\n\nIn the simplest way, in which linear regression can be constructed (e.g., the probability of success or failure is determined, as predicted, by a linear regression, or by an estimation of probabilities by a regression method, using the Bayesian Bayesian Bayesian method). The Bayesian Bayesian model uses the Bayesian version of linear regression to estimate the probability of failure. This approach has been called an optimization (BDS) since it is one of the first to use BDS to classify a function from a set of Bayesian regressors. In this particular case, the Bayesian model is based on an explicit version of Bayesian regression using the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesian version of the Bayesis not strictly necessary (in theory, this model is not the only way that results in error.) Instead, in practice, the Bayesian model cannot be used to estimate the likelihood of failure.\nFor example, in the last example, in the first case, when a function in the second case is not true, a", "histories": [["v1", "Tue, 4 Feb 2014 07:35:48 GMT  (418kb,D)", "http://arxiv.org/abs/1402.0645v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["franziska meier", "philipp hennig", "stefan schaal"], "accepted": false, "id": "1402.0645"}, "pdf": {"name": "1402.0645.pdf", "metadata": {"source": "CRF", "title": "Local Gaussian Regression", "authors": ["Franziska Meier", "Philipp Hennig", "Stefan Schaal"], "emails": [], "sections": [{"heading": null, "text": "Locally weighted regression was created as a nonparametric learning method that is computationally efficient, can learn from very large amounts of data and add data incrementally. An interesting feature of locally weighted regression is that it can work with spatially varying length scales, a beneficial property, for instance, in control problems. However, it does not provide a generative model for function values and requires training and test data to be generated identically, independently. Gaussian (process) regression, on the other hand, provides a fully generative model without significant formal requirements on the distribution of training data, but has much higher computational cost and usually works with one global scale per input dimension. Using a localising function basis and approximate inference techniques, we take Gaussian (process) regression to increasingly localised properties and toward the same computational complexity class as locally weighted regression."}, {"heading": "1 Introduction", "text": "Besides expressivity and sample efficiency, computational cost is a crucial design criterion for machine learning algorithms in real-time settings, such as control problems. An example is the problem of building a model for robot dynamics: The sensors in a robot\u2019s limbs can produce thousands of datapoints per second, quickly amassing a local coverage of the input domain. In such settings, fast local learning and generalization can be more important than a globally optimized model. A learning method should rapidly produce a good model from the large number N of datapoints, using a comparably small number M of parameters.\nLocally weighted regression (LWR) [1] makes use of the popular and well-studied idea of local learning [2] to address the task of compressing large amounts of data into a small number of parameters. In the spirit of a Taylor expansion, the idea of LWR is that simple models with few parameters may locally be precise,\nwhile it may be difficult to find good nonlinear features to capture the entire function globally \u2013 lots of good local models may form a good global one.\nThe key to LWRs low computational cost (linear,O(NM)) is that each local model is trained independently. The resulting speed has made LWR popular in robot learning. The downside is that LWR requires several tuning parameters, whose optimal values can be highly data dependent. This is at least partly a result of the strongly localized training strategy, which does not allow models to \u2018coordinate\u2019, or to benefit from other local models in their vicinity.\nHere, we explore a probabilistic alternative to LWR that alleviates the need for parameter tuning, but retains potential for fast training. An initial candidate could be the mixture of experts model (ME) [3]. Indeed, it has been argued [1], that LWR can be thought of as a mixture of experts model in which experts are trained independently of each other. The advantage of ME is that it comes with a full generative model [4, 5] allowing for principled learning of parameters and expert allocations, while LWR does not (see Figure 1). However, in this work we emulate LWRs assumption that the data is a continous function as opposed to a mixture of linear models, for instance. When the underlying data is not assumed to be generated by a mixture model, utilizing ME to fit the data makes inference (unneccesarily) complicated. Hence, we are interested in a probabilistic model that captures the best of both worlds without making the mixture assumption: A generative model that has the ability to localize computations to speed up learning.\nOur proposed solution is local Gaussian regression (LGR), a linear regression model that explicitly encodes localisation. Starting from the well-known probabilistic, Gaussian formulation of least-squares generalized linear regression, we first re-interpret the well known radial basis feature functions as localisers of constant feature functions. This allows us to introduce more expressive features, capturing the idea of local models within the Gaussian regression framework. In its exact form, this model has the cubic cost in M typical of\nar X\niv :1\n40 2.\n06 45\nv1 [\ncs .L\nG ]\n4 F\neb 2\n01 4\nGaussian models, arising because observations induce correlation between all local models in the posterior. To decouple the local models, we propose a variational approximation that gives essentially linear cost in the number of models M . The core of this work revolves around fitting the model parameters using maximum likelihood (Section 3.1). As a final note, we show how the novel feature function representation allows us to readily extend our probabilistic model to a local nonparametric formulation (Section 4).\nPrevious work on probabilistic formulations of local regression [6, 7] has been focussed on bottom-up constructions, trying to find generative models for one local model at a time. To our knowledge, this is the first top-down approach, starting from a globally optimal training procedure, to find approximations giving a localized regression algorithm similar in spirit to LWR."}, {"heading": "2 Background", "text": "Both LWR and Gaussian regression have been studied extensively before, so we only give brief introductions here. Generalized linear regression maps weights w \u2208 RF to the nonlinear function f \u2236 RD _R via F feature functions \u03c6i(x) \u2236 RD _R:\nf(x) = F\u2211 i=1\u03c6i(x)wi = \u03c6\u22baw. (1)\nUsing the feature matrix \u03a6 \u2208 RN\u00d7F whose elements are \u03a6nf = \u03c6f(xn), the function values at N locations xn \u2208 RD, subsumed in the matrix X \u2208 RN\u00d7D, are f(X) = \u03a6w. Locally weighted regression (LWR) trains M local regression models. We will assume each local model has K local feature functions \u03bemk(x), so that the m-th model\u2019s prediction at x is\nfm(x) = K\u2211 k=1 \u03bemk(x)wmk = \u03bem(x)wm (2)\nK = 2 and \u03bem1(x) = 1, \u03bem2(x) = (x \u2212 cm) gives a linear model around cm. The models are localized by a nonnegative, symmetric and integrable weighting \u03b7m(x), typically the radial basis function\n\u03b7m(x) = exp [\u2212(x \u2212 cm)2 2\u03bb2m\n] , or, for x \u2208 RD, (3) \u03b7m(x) = exp [\u22121\n2 (x \u2212 cm)\u039b\u22121(x \u2212 cm)\u22ba] . (4)\nwith center cm and length scale \u03bb or positive definite metric \u039b. In LWR, each local model is trained independently of the others, by minimizing a quadratic\nFranziska Meier1, Philipp Hennig2, Stefan Schaal1,2\nloss\nL(w) = N\u2211 n=1 M\u2211 m=1\u03b7m(xn)(yn \u2212 \u03bem(xn)wm)2 (5)\n= N\u2211 n=1 M\u2211 m=1 (\n\u221a \u03b7m(xn)yn \u2212\u221a\u03b7m(xn)\u03bem(xn)wm)2\nover the N observations (yn, xn). At test time, local predictions (2) are combined into a joint prediction at input x as a normalised weighted sum f(x) = \u2211m \u03b7m(x)fm(x)\u2211m\u2032 \u03b7m\u2032(x) = \u2211m,k\u03c6mk(x)wmk (6) with the features\n\u03c6mk(x) = \u03b7m(x)\u03bemk(x)\u2211m\u2032 \u03b7m\u2032(x) . (7) An important observation is that the objective (5) cannot be interpreted as least-squares estimation of the linear model \u03a6 from Eq. (6). Eq. (5) is effectively training\nM linear models with \u03c6mk(xn) = \u221a\u03b7m(xn)\u03bemk(xn) on M separate datasets ym(xn) = \u221a\u03b7m(xn)yn, but that model differs from the one in Equation (6). Thus, LWR can not be cast probabilistically as one generative model for training and test data simultaneously. The factor graph in Figure 1, right, shows this broken symmetry.\nTraining LWR is linear in N and M , and cubic in K, since it involves a least-squares problem in the K weights wm. But this low cost profile also means absence of mixing terms between the M models, thus no \u2018coordination\u2019 between the local models. One problem this causes is that model-specific hyperparameters are not sufficiently identified and tend to over-fit. To pick one example among the hyperparameters: when learning the length scale parameters \u03bbm, there is no sense of global improvement. Each local model focusses only on fitting data points within a \u03bbm-neighborhood around cm. So the optimal choice is actually \u03bb_0, which gives a useless regression model. To counteract this behaviour, implementations of LWR use regularisation of \u03bb, but of course this introduces free parameters, whose best values are not obvious. Most works in LWR address this problem via leave-one-out cross-validation, which works well if the data is sparse. But when the data is dense, like in robotics, the kernel tends to shrink to only \u2018activate\u2019 points close by the left-out data point.\nGaussian regression [8, \u00a72] is a probabilistic framework for inference on the weights w given the features \u03c6. Using a Gaussian prior p(w) = N (w;\u00b50,\u03a30) and a Gaussian likelihood p(y \u2223\u03c6,w) = N (y;\u03c6\u22baw, \u03b2\u22121I),\nthe posterior distribution is itself Gaussian:\np(w \u2223y,\u03c6) = N (w;\u00b5N ,\u03a3N) with (8) \u00b5N = (\u03a3\u221210 + \u03b2\u03a6\u22ba\u03a6)\u22121(\u03b2\u03a6\u22bay \u2212\u03a3\u221210 \u00b50) (9) \u03a3N = (\u03a3\u221210 + \u03b2\u03a6\u22ba\u03a6)\u22121 (10)\nThe mean of this Gaussian posterior is identical with the \u03a3-regularised least-squares estimator for w, so it could alternatively be derived as the minimiser of the loss function\nL(w) =w\u22ba\u03a3\u22121w + N\u2211 n=1(yn \u2212\u03c6(xn)w)2. (11)\nBut the probabilistic interpretation of Equation (8) has additional value over (11) because it is a generative model for all (training and test) data points y, which can be used to learn hyperparameters of the feature functions. The prediction for f(x\u2217) with features \u03c6(x\u2217) =\u2236 \u03c6\u2217 is also Gaussian, with\np(f(x\u2217) \u2223y,\u03c6) = N (f(x\u2217);\u03c6\u2217\u00b5N ,\u03c6\u2217\u03a3N\u03c6\u22ba\u2217) (12) As is widely known, this framework can be extended to the nonparametric case by a limit which replaces all inner products \u03c6(x1)\u03a30\u03c6(x2)\u22ba with a Mercer (positive semi-definite) kernel function k(x1, x2). The corresponding priors are Gaussian processes. This generalisation will only play a role in Section 4 of this paper, but the direct connection between Gaussian regression and the elegant theory of Gaussian processes is often cited in favour of this framework. Its main downside, relative to LWR, is computational cost: Calculating the posterior (12) requires solving the least-squares problem for all F parameters w jointly, by inverting the Gram matrix (\u03a3\u221210 + \u03b2\u03a6\u22ba\u03a6). In the general case, this inversion requires \u00d8(F 3) operations. The point of the construction in the following sections is to use approximations to lower the computation cost of this operation such that the resulting algorithm is comparable in cost to LWR, while retaining the probabilistic interpretation, and the modelling robustness of the full Gaussian model.\nThe Gaussian regression framework puts virtually no limit on the choice of basis functions \u03c6, even allowing discontinuous and unbounded functions, but the radial basis function (RBF, aka. Gaussian, squareexponential) features from Equation (3), \u03c6i(x) = \u03b7i(x), (for F =M) enjoy particular popularity for various reasons, including algebraic conveniences and the fact that their associated reproducing kernel Hilbert space lies dense in the space of continuous functions [9]. A downside of this covariance function is that it uses one global length scale for the entire input domain. There are some special kernels of locally varying regularity [10], and mixture descriptions offer a more discretely varying model class [11]. Both, however, require computationally demanding training.\nLocal Gaussian Regression"}, {"heading": "3 Local Parametric Gaussian Regression", "text": "In Gaussian regression with RBF features as described above, without changing anything, the features \u03c6m(x) = \u03b7m(x) can be interpreted as M constant function models \u03bem(x) = 1, localised by the RBF function, \u03c6(x) = [\u03be1(x)\u03b71(x), . . . , \u03beM(x)\u03b7M(x)]. This representation extends to more elaborate local models \u03be(x). For example \u03be(x) = x gives a local weighted regression with linear local models. Extending to M local models consisting of K parameters each, feature function \u03c6nmk combines the k\nth component of the local model \u03bekm(xn), localised by the m-th weighting function \u03b7m(x) \u03c6nmk \u2236= \u03c6mk(xn) = \u03b7m(xn)\u03bekm(xn). (13) For these features, treating mk as the index set of a vector with MK elements, the results from Equations (8) and (12) apply, giving a localised linear Gaussian regression algorithm.\nThe choice of local parametric model is essentially arbitrary. But regular features are an obvious choice. A scalar polynomial model of order K \u2212 1 arises from \u03bekm(xn) = (xn\u2212cm)k\u22121. Local linear regression in a Kdimensional input space takes the form \u03bekm(xn) = xnk\u2212 cm. We will adopt this latter model in the remainder, because it is also the most widely used model in LWR. An illustration of LWR regression and the proposed local Gaussian regression is given in Figure 2.\nSince it will become necessary to prune out unnecessary parts of the model, we adopt the classic idea of automatic relevance determination [12, 13] using a\nfactorizing prior\np(w\u2223A) = M\u220f m=1N (wm; 0,A\u22121m ) with (14)\nAm = diag(\u03b1m1, . . . , \u03b1mK). (15) So every component k of each local model m has its own precision, and can thus be pruned out by setting \u03b1mk_\u221e. Section 3.1 assumes a fixed number M of local models with fixed centers cm. Thus the parameters are \u03b8 = {\u03b2,{\u03b1mk},{\u03bbmk}}. We propose an approximation for estimating these parameters. Section 3.2 then describes placing the local models incrementally to adapt M and cm."}, {"heading": "3.1 Learning in Local Bayesian Linear Regression", "text": "Exact inference in Gaussian regression with localised feature functions comes with the cubic cost of its nonlocal origins. However, because of the localised feature functions, correlation between far away local models is approximately 0, hence inference is approximately independent between local models. In this section we aim to make use of this \u201calmost independence\u201d to derive a localised approximate inference scheme of low cost, similar in spirit to LWR. To arrive at this localised learning algorithm we first introduce a latent variable fnm for each local model m and data point xn, similar to probabilistic backfitting [14]. Intuitively, the f form approximate targets, one for each local model, against which the local regression parameters fit (see also Figure 1, middle).\nThis modified model motivates a factorizing variational bound constructed in Section 3.1.1, rendering the the\nFranziska Meier1, Philipp Hennig2, Stefan Schaal1,2\nlocal models computationally independent, which allows for fast approximate inference in the local Gaussian model. Hyperparameters will be learnt by approximate maximum likelihood (3.1.2), i.e. iterating between constructing a bound q(z \u2223\u03b8) on the posterior over variables z given current parameter estimates \u03b8 and optimising q with respect to \u03b8."}, {"heading": "3.1.1 Variational Bound", "text": "The complete data likelihood of the modified model is\np(y,f ,w \u2223\u03a6, \u03b8) = N\u220f n=1p(yn \u2223fn, \u03b2y) (16)\nN\u220f n=1 M\u220f m=1p(fnm \u2223\u03c6nmwm, \u03b2fm) M\u220f m=1p(wm \u2223Am)\n(c.f. Figure 1, left). Our Gaussian model involves the latent variables w and f , the precision \u03b2 and the model parameters \u03bbm, cm. We treat w and f as probabilistic variables and estimate \u03b8 = {\u03b2,\u03bb,c}. On w,f , we construct a variational bound q(w,f) imposing factorisation q(w,f) = q(w)q(f). The variational free energy is a lower bound on the log evidence for the observations y:\nlog p(y \u2223 \u03b8) \u2265 \u222b q(w,f) log p(y,w,f \u2223 \u03b8) q(w,f) . (17)\nThis bound is maximized by the q(w,f) minimizing the relative entropy DKL[q(w,f)\u2225p(w,f \u2223y, \u03b8)], the distribution for which log q(w) = Ef [log p(y \u2223f ,w)p(w,f)] and log q(f) = Ew[log p(y \u2223f ,w)p(w,f)]. It is relatively straightforward to show (e.g. [15]) that these distributions are Gaussian in both w and f .The approximation on w is\nlog q(w) = Ef [log p(fn \u2223\u03c6(xn),w) + log p(w \u2223A)] = log M\u220f\nm=1N (wm;\u00b5wm ,\u03a3wm) (18) where\n\u03a3wm = (\u03b2fm N\u2211 n=1\u03c6 n m\u03c6\nn m T +Am)\u22121 \u2208 RK\u00d7K (19)\n\u00b5wm = \u03b2fm\u03a3wm ( N\u2211 n=1\u03c6 n mE [fnm]) \u2208 RK\u00d71 (20)\nThe posterior update equations for the weights are local: each of the local models updates its parameters independently. This comes at the cost of having to update the belief over the variables fnm, which achieves a coupling between the local models. The Gaussian variational bound on f is\nlog q(fn) = Ew [log p(yn \u2223fn, \u03b2y) + log p(fn \u2223\u03c6nm,w)]= N (fn;\u00b5fn,\u03a3f) (21)\nwhere\n\u03a3f = B\u22121 \u2212B\u221211(\u03b2\u22121y + 1TB\u221211)\u221211TB\u22121 = B\u22121 \u2212 B\u2212111TB\u22121\n\u03b2\u22121y + 1TB\u221211 (22) \u00b5fn = M\u2211\nm=1Ew [wTm]\u03c6(xn)+ (23) 1\n\u03b2\u22121y + 1\u22baB\u221211B\u221211(yn \u2212 M\u2211 m=1E [wm]T \u03c6nm)\nwhere \u00b5fn \u2208 RM and using B = diag (\u03b2f1, . . . , \u03b2fM). These updates can be performed in O(MK). Inspecting Equations (23) and (20), one can see that the optimal assignment of all \u00b5w actually amounts to a joint linear problem of size MK \u00d7MK, which could also be solved as a linear program. However, this would come at additional computational cost."}, {"heading": "3.1.2 Optimizing Hyperparameters", "text": "We set the model parameters \u03b8 ={\u03b2y,{\u03b2fm, \u03bbm}Mm=1,{\u03b1mk}} to maximize the expected complete log likelihood under the variational bound,\nEf ,w[log p(y,f ,w \u2223\u03a6, \u03b8)] = (24) Ef ,w{ N\u2211\nn=1 [ logN (yn; M\u2211 m=1 fnm, \u03b2 \u22121 y )\n+ M\u2211 m=1 logN (fnm;wTm\u03c6nm, \u03b2\u22121fm)] + M\u2211 m=1 logN (wm; 0,A\u22121m )}\nSetting the gradient of this expression to zero leads to the following update equations for the variances\n\u03b2\u22121y = 1N N\u2211 n=1(yn \u2212 1\u00b5fn)2 + 1T\u03a3f1 (25)\n\u03b2\u22121fm = 1N N\u2211 n=1 [(\u00b5fnm \u2212\u00b5wm\u03c6nm)2 +\u03c6nmT\u03a3wm\u03c6nm] + \u03c32fm\n(26)\n\u03b1\u22121mk = \u00b52wmk +\u03a3w,kk (27) The gradient with respect to the scales of each local model is completely localized\n\u2202Ef ,w [log p(y,f ,w \u2223\u03a6, \u03b8)] \u2202 log\u03bbmk\n= \u2202Ef ,w [\u2211Nn=1N (fnm;wTm\u03c6m(xn), \u03b2\u22121fm)] \u2202\u03bbmk (28)\nWe use gradient ascent to optimize the length scales \u03bbmk. All necessary equations are of low cost and, with\nLocal Gaussian Regression\nthe exception of the variance 1 \u03b2y , all hyper-parameter updates are solved independently for each local model, similar to LWR. In contrast to LWR, however, these local updates do not cause a shrinking in the length scales: In LWR, both inputs and outputs are weighted by the localizing function, thus reducing the length scale improves the fit. The localization of Equation (13) only affects the influence of regression model m, but the targets still need to be fit accordingly. Shrinking of local models only happens if it actually improves the fit against the unweighted targets fnm."}, {"heading": "3.1.3 Prediction", "text": "Prediction at a test point x\u2217 arise from marginalizing over both w and f , using\n\u222b N (y\u2217;1Tf\u2217, \u03b2\u22121y )N (f\u2217;WT\u03c6(x\u2217),B\u22121)df\u2217 = N (y\u2217;\u2211\nm\nwTm\u03c6 \u2217 m, \u03b2 \u22121 y + 1TB\u221211) (29)\nand\n\u222b N (y\u2217;wT\u03c6\u2217, \u03b2\u22121y + 1TB\u221211)N (w;\u00b5w,\u03a3w)dw = N (y\u2217; M\u2211\nm\n\u00b5Twm\u03c6 \u2217 m, \u03c3 2(x\u2217)) (30) where\n\u03c32(x\u2217) = \u03b2\u22121y + M\u2211 m=1\u03b2 \u22121 fm + M\u2211 m=1\u03c6 \u2217 m T \u03a3wm\u03c6 \u2217 m (31)\nwhich is linear in M and K."}, {"heading": "3.2 Incremental Adding of Local Models", "text": "Up to here, the number M and locations c of local models were fixed. An extension analogous to the incremental learning of the relevance vector machine [16] can be used to iteratively add local models at new, greedily selected locations cM+1. The resulting algorithm starts with one local model, and per iteration adds one local model in the variational step. Conversely, existing local models for which all components \u03b1mk_\u221e are pruned out. This works well in practice, with the caveat that the number local models M can grow fast initially before the pruning becomes effective. Thus we check for each selected location cM+1 whether any of the existing local models c1\u2236M produces a localizing weight \u03b7m(cM+1) \u2265 wgen, where wgen is a parameter between 0 and 1 and regulates how many parameters are added. An overview of the final algorithm is given in 1.\nAlgorithm 1 Incremental LGR Require: {xn, yn}Nn=1// initialize first local model 1: M = 1; c1 = x1;C = {c1}// iterate over all data points 2: for n = 2 . . .N do 3: if \u03b7m(xn) < wgen,\u2200m = 1, . . . ,M then 4: cm^xn 5: C^C \u222a {cm}, M =M + 1 6: end if\n// E-Step: equations (19),(20),(22),(23) 7: {\u00b5wm ,\u03a3wm , \u00b5fm,\u03a3fm}m^ 8: e-step({\u03b2y, \u03b2fm,{\u03b1mk, \u03bbmk}k}m)// M-Step: equations (25),(26),(27),(28) 9: {\u03b2y, \u03b2fm,{\u03b1mk, \u03bbmk}k}m^\n10: m-step ({\u00b5wm ,\u03a3wm , \u00b5fm,\u03a3fm}m)// pruning 11: for all m do 12: if \u03b1mk > 1e3 \u2200k = 1, . . . ,K then 13: M ^M \u2212 1;C^C \u2216 cm 14: end if 15: end for 16: end for"}, {"heading": "4 Extension to Finitely Many Local Nonparametric Models", "text": "An interesting component of local Gaussian regression is that it easily extends to a model with finitely many local nonparametric, Gaussian process models. Marginalising out the weights wm in Eq. (17) gives the marginal\np(y,f \u2223X,\u03b8) = N (y;1\u22baf , \u03b2\u22121y IN) M\u220f m=1N (fm; 0,Cm)\n(32) where p(fm) = N (fm; 0,Cm) is a Gaussian process prior over function fm with a (degenerate) covariance function Cm = \u03b2\u22121fmIN + \u03c6TmA\u22121m\u03c6m. Replacing the finitely many local features \u03bem with infinitely many features results in the local nonparametric model with covariance function \u03bam(x,x\u2032) = \u03b2\u22121fm\u03b4ij + \u03b7m(x)\u03ba\u0302m(x,x\u2032)\u03b7m(x\u2032). The exact posterior over fx \u2236= f(x) is p(fx \u2223y) = N (fx;\u00b5x,\u03a3xx) (33) with\n\u03a3mrxx\u2032 = cov(fm(x), fr(x\u2032)) (34) = \u03b4mrkmxx\u2032 \u2212 kmxX(M\u2211\ns ksXX + \u03b2\u22121y I)\u22121krXx\u2032 (35) \u00b5mx = kmxX(M\u2211\ns\nksXX + \u03b2\u22121y I)\u22121y (36)\nSo computing the posterior covariance \u03a3mrxx\u2032 requires one inversion of an N \u00d7N matrix. It remains to be seen to what extend variational bounds on the parametric forms presented in the previous sections can be transported to a local nonparametric Gaussian regression framework."}, {"heading": "5 Experiments", "text": "We evaluate and compare to mixture of experts and locally weighted projection regression (LWPR) \u2013 an extension of LWR suitable for regression in high dimensional space [17] \u2013 on two data sets. The first is data drawn from the \u201ccross function\u201d in Figure 3, often used to demonstrate locally weighted learning. For the second comparison we learn inverse dynamics of a SARCOS anthropomorphic robot arm with seven degrees of freedom [8].\nIn order to compare to mixture of experts we assume linear expert models. A prior of the form N (wm; 0,A\u22121m ) on each experts regression parameters and normalized gaussian kernels for the mixture components are used in our implementation to make it as comparable as possible. To compute the posterior in the e-step a mean field approximation is employed.\nIn both experiments, LWPR performed multiple cycles through the data sets. Both the local Gaussian regression and the mixture of experts implementation are executed based on Algorithm 1 and are allowed an additional 1000 iterations to reach convergence."}, {"heading": "5.1 Data from the \u2018Cross Function\u2019", "text": "We used 2,000 uniformly distributed training inputs, with zero mean Gaussian noise of variance (0.2)2. The test set is a regular grid of 1641 edges without noise and is used to evaluate how well the underlying function is captured. The initial length scale was set to \u03bbm = 0.3,\u2200m, and we ran each method with and without lengthscale learning (LSL) for wgen = 0.1,0.2, . . . ,1.0. All results presented here are results averaged over 5 randomly seeded runs. Table 1 presents the top performance achieved by each method, including what the optimal setting for wgen was and how many local models were used. In both settings (with or without LSL) LGR outperforms both LWPR and ME in terms of accuracy as well as number of local models used. To understand the role of parameter wgen better, we also summarize the normalized mean squared error as a function of parameter wgen for all 3 methods in Figure 4 with (right) and without (left) LSL. The key message of this graph is that the parameter wgen does not affect the performance of LGR greatly. While accuracy slightly improves with increasing wgen it is not dramatic. Thus for LGR wgen can be thought of as a trade-off parameter, for smaller wgen the algorithm has to consider less potential local models, for a slight performance decrease. For LWPR and ME this relationship to wgen is not that clear. Furthermore, although LGR has to consider more local models for larger wgen (up to 2000 for wgen = 1), we only see a slight increase in the number of local models in the final model, indicating that the pruning mechanism works very well.\nFinally, we show representative results of the shape of learned local models for LWPR, LGR and ME In Figure 3, nicely illustrating the key difference between the three methods: In LWPR local models don\u2019t know of each other and thus aim to find the best linear approximation to the function. In both ME and LGR, the local models know of each other and collaborate to fit the function."}, {"heading": "5.2 Inverse Dynamics Learning Task", "text": "The SARCOS data contains 44, 484 training data points and 4,449 test data points. The 21 input variables represent joint positions, velocities and accelerations for the 7 joints. The task is to predict the 7 joint torques. In Table 2 we show the predictive performance of LWPR, ME and LGR when trained with lengthscale learning and with wgen = 0.3. LGR outperforms LWPR and ME in terms of accuracy for almost all joints.However, the true advantage of LGR lies in the fact the number of hand tuned parameters is reduced to setting the learning rate for the gradient descent updates and setting the parameter wgen."}, {"heading": "6 Conclusion", "text": "We have taken a top-down approach to developing a probabilistic localised regression algorithm: We start with the generative model of Gaussian generalized linear regression, which amounts to a fully connected graph and thus has cubic inference cost. To break down the computational cost of inference, we first introduce the idea of localised feature functions as local models, which can be extended to local nonparametric models. In as second step, we argue that because of the localisation these local models are approximately independent . We exploit that fact through a variational approximation that reduced computational complexity to local computations. Empirical evaluation suggests that LGR successfully addresses the problem of fitting hyperparameters inherent in locally weighted regression. A final step left for future work is to re-formulate our algorithm into an incremental version that can deal with a continuous stream of incoming data.\nFranziska Meier1, Philipp Hennig2, Stefan Schaal1,2"}], "references": [{"title": "Constructive incremental learning from only local information", "author": ["Stefan Schaal", "Christopher G Atkeson"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Local learning algorithms", "author": ["L\u00e9on Bottou", "Vladimir Vapnik"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Adaptive mixtures of local experts", "author": ["Robert A Jacobs", "Michael I Jordan", "Steven J Nowlan", "Geoffrey E Hinton"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Bayesian methods for mixtures of experts", "author": ["Steve Waterhouse", "David MacKay", "Tony Robinson"], "venue": "Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Dirichlet process mixtures of generalized linear models", "author": ["Lauren Hannah", "David M Blei", "Warren B Powell"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Bayesian Kernel Shaping for Learning Control", "author": ["Jo-Anne Ting", "Mrinal Kalakrishnan", "Sethu Vijayakumar", "Stefan Schaal"], "venue": "In Neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Kernel carpentry for online regression using randomly varying coefficient model", "author": ["Narayanan U Edakunni", "Stefan Schaal", "Sethu Vijayakumar"], "venue": "In Proceedings of the international joint conference on artificial intelligence (IJCAI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Universal kernels", "author": ["C.A. Micchelli", "Y. Xu", "H. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Bayesian Gaussian processes for regression and classification", "author": ["M. N Gibbs"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Infinite mixtures of Gaussian process experts. In Advances in neural information processing systems", "author": ["C.E. Rasmussen", "Z. Ghahramani"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Bayesian learning for neural networks", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "The bayesian backfitting relevance vector machine", "author": ["Aaron D\u2019Souza", "Sethu Vijayakumar", "Stefan Schaal"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["M.K. Titsias"], "venue": "In Artificial Intelligence & Statistics (AISTATS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Incremental gaussian processes", "author": ["Joaquin Quinonero-Candela", "Ole Winther"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Locally weighted projection regression: An o (n) algorithm for incremental real time learning in high dimensional space", "author": ["Sethu Vijayakumar", "Stefan Schaal"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Locally weighted regression (LWR) [1] makes use of the popular and well-studied idea of local learning [2] to address the task of compressing large amounts of data into a small number of parameters.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Locally weighted regression (LWR) [1] makes use of the popular and well-studied idea of local learning [2] to address the task of compressing large amounts of data into a small number of parameters.", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "An initial candidate could be the mixture of experts model (ME) [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "Indeed, it has been argued [1], that LWR can be thought of as a mixture of experts model in which experts are trained independently of each other.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "The advantage of ME is that it comes with a full generative model [4, 5] allowing for principled learning of parameters and expert allocations, while LWR does not (see Figure 1).", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "The advantage of ME is that it comes with a full generative model [4, 5] allowing for principled learning of parameters and expert allocations, while LWR does not (see Figure 1).", "startOffset": 66, "endOffset": 72}, {"referenceID": 5, "context": "Previous work on probabilistic formulations of local regression [6, 7] has been focussed on bottom-up constructions, trying to find generative models for one local model at a time.", "startOffset": 64, "endOffset": 70}, {"referenceID": 6, "context": "Previous work on probabilistic formulations of local regression [6, 7] has been focussed on bottom-up constructions, trying to find generative models for one local model at a time.", "startOffset": 64, "endOffset": 70}, {"referenceID": 8, "context": "Gaussian, squareexponential) features from Equation (3), \u03c6i(x) = \u03b7i(x), (for F =M) enjoy particular popularity for various reasons, including algebraic conveniences and the fact that their associated reproducing kernel Hilbert space lies dense in the space of continuous functions [9].", "startOffset": 281, "endOffset": 284}, {"referenceID": 9, "context": "There are some special kernels of locally varying regularity [10], and mixture descriptions offer a more discretely varying model class [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "There are some special kernels of locally varying regularity [10], and mixture descriptions offer a more discretely varying model class [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "Since it will become necessary to prune out unnecessary parts of the model, we adopt the classic idea of automatic relevance determination [12, 13] using a factorizing prior p(w\u2223A) = M \u220f m=1N (wm; 0,A\u22121 m ) with (14) Am = diag(\u03b1m1, .", "startOffset": 139, "endOffset": 147}, {"referenceID": 12, "context": "Since it will become necessary to prune out unnecessary parts of the model, we adopt the classic idea of automatic relevance determination [12, 13] using a factorizing prior p(w\u2223A) = M \u220f m=1N (wm; 0,A\u22121 m ) with (14) Am = diag(\u03b1m1, .", "startOffset": 139, "endOffset": 147}, {"referenceID": 13, "context": "To arrive at this localised learning algorithm we first introduce a latent variable fnm for each local model m and data point xn, similar to probabilistic backfitting [14].", "startOffset": 167, "endOffset": 171}, {"referenceID": 14, "context": "[15]) that these distributions are Gaussian in both w and f .", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "An extension analogous to the incremental learning of the relevance vector machine [16] can be used to iteratively add local models at new, greedily selected locations cM+1.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "We evaluate and compare to mixture of experts and locally weighted projection regression (LWPR) \u2013 an extension of LWR suitable for regression in high dimensional space [17] \u2013 on two data sets.", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "For the second comparison we learn inverse dynamics of a SARCOS anthropomorphic robot arm with seven degrees of freedom [8].", "startOffset": 120, "endOffset": 123}], "year": 2014, "abstractText": "Locally weighted regression was created as a nonparametric learning method that is computationally efficient, can learn from very large amounts of data and add data incrementally. An interesting feature of locally weighted regression is that it can work with spatially varying length scales, a beneficial property, for instance, in control problems. However, it does not provide a generative model for function values and requires training and test data to be generated identically, independently. Gaussian (process) regression, on the other hand, provides a fully generative model without significant formal requirements on the distribution of training data, but has much higher computational cost and usually works with one global scale per input dimension. Using a localising function basis and approximate inference techniques, we take Gaussian (process) regression to increasingly localised properties and toward the same computational complexity class as locally weighted regression.", "creator": "LaTeX with hyperref package"}}}