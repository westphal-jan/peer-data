{"id": "1402.2324", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2014", "title": "Universal Matrix Completion", "abstract": "The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem. However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled afresh. This process is not a trivial operation, and the resulting results were extremely difficult to measure. For example, by using the \"new matrix\" function, the resulting results showed that the \"new matrix\" (1/2) corresponds to that observed index that the current matrix has been sampled.\n\n\nThe problem with this approach is that it also suffers from a lot of problems. The real problem is that it is not really an \"interesting\" problem. It is just not interesting. The problem with this approach is that it is not really an interesting problem. It is just not interesting. The problem with this approach is that it is not really an interesting problem. It is just not interesting. The problem with this approach is that it is not really an interesting problem. It is just not interesting.\nThere are many techniques to implement these methods (for example:\nA) some of the more complicated problems can be solved by the above methods. For example, the methods simply take random values from a random source that is only very small. However, this doesn't have a big impact on the results because it is only very small. Thus, the result in the \"new matrix\" function is often very small.\nOne such approach is to use a function called a simple function called a \"new matrix\" function (for instance:\nA) a simple function called a simple function called a \"new matrix\" function (for instance:\nA) a simple function called a simple function called a simple function called a simple function called a simple function called a simple function called a simple function called a simple function called a simple function called a simple function\nIn contrast, it has been argued that even the simplest simple function that has been implemented by the methods can be implemented in a very different way. This is a major point in this blog post, but this has been repeated to the point where the simple and complex methods need to be implemented in a different way.\nWhat are some of the more complicated problems that the method does? In the previous post we pointed out that these types of methods, called \"super\" and \"super\"", "histories": [["v1", "Mon, 10 Feb 2014 22:53:15 GMT  (138kb)", "https://arxiv.org/abs/1402.2324v1", "23 pages, 2 figures"], ["v2", "Fri, 11 Jul 2014 15:21:56 GMT  (137kb)", "http://arxiv.org/abs/1402.2324v2", "22 pages, 2 figures"]], "COMMENTS": "23 pages, 2 figures", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["srinadh bhojanapalli", "prateek jain 0002"], "accepted": true, "id": "1402.2324"}, "pdf": {"name": "1402.2324.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["bsrinadh@utexas.edu", "prajain@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n23 24\nv2 [\nst at\n.M L\n] 1\n1 Ju\nl 2 01"}, {"heading": "1 Introduction", "text": "In this paper, we study the problem of universal low-rank matrix completion. Low-rank matrix completion is an important problem with several applications in areas such as recommendation systems, sketching, and quantum tomography (Recht et al., 2010; Cande\u0300s & Recht, 2009; Gross et al., 2010). The goal in matrix completion is to recover a rank-r matrix, given a small number of entries of the matrix. That is, to find a matrix M given values {Mij , (i, j) \u2208 \u2126}, where \u2126 is the set of observed indices.\nRecently, several methods with provable guarantees have been proposed for solving the problem under the following two assumptions: a) M is incoherent, b) \u2126 is sampled uniformly and |\u2126| \u2265 cnr log n. Moreover, \u2126 needs to be re-sampled for each matrix M that is to be recovered, i.e., the same \u2126 cannot be re-used without worsening the guarantees significantly.\nWhile the first assumption can be shown to be necessary for any matrix oblivious sampling, the second assumption is relatively restrictive and might not hold in several practical settings. The main goal of this work is to develop a general result that can handle other sampling schemes as well. Moreover, we aim to develop a universal method where one fixed \u2126 would be enough to recover any low-rank matrix M . Such a universal recovery result is highly desirable in several signal processing applications, where the goal is to design one \u2126 that can recover any low-rank signal matrix M by observing M over \u2126 alone.\nTo this end, we reduce the problem of recoverability using an index set \u2126 to the spectral gap (gap between largest and second largest singular values) of G, where G is a bipartite graph whose biadjacency matrix G \u2208 Rn\u00d7n is given by: Gij = 1 iff (i, j) \u2208 \u2126 and Gij = 0 otherwise. In particular, we show that if G has a large enough spectral gap and if the rank-r matrix M satisfies the standard incoherence property, then the best rank-r approximation of P\u2126(M) (see (1)) itself is enough to get a \u201creasonable\u201d approximation to M (see Theorem 4.1 for details).\nNote that our approximation result is similar to Theorem 1.1 of (Keshavan et al., 2010), but our result holds for any \u2126 with large spectral gap unlike (Keshavan et al., 2010) that requires uniform sampling. On the other hand, we require explicit incoherence condition on singular vectors of M , while the result of (Keshavan et al., 2010) only requires a bound on Mmax = maxij Mij . The later assumption is strictly weaker assumption; the assumptions coincide for PSD matrices.\nNext, we show that by assuming certain stronger incoherence properties, the number of samples required by the popular nuclear-norm minimization method (Cande\u0300s & Recht, 2009) to recover back M depends only on n, r and the spectral gap of the d-regular bipartite graph G. In particular, we require d \u2265 \u03c32(G) \u00b7 r, where \u03c32(G) is the second largest singular value of G. Hence, if \u03c32(G) = O(\n\u221a d), i.e., if G is an expander, then |\u2126| = nd = O(nr2) samples suffice for exact recovery. Our recovery results applies to any low rank matrix M that satisfies the stronger incoherence property, given that the fixed graph G has a large spectral gap. To the best of our knowledge, this is the first universal guarantee for matrix completion. Furthermore, using recent results by (Feige & Ofek, 2005) we show that for the standard uniform sampling of \u2126, only O(nr2) samples suffice for exact recovery of a rank-r matrix M that satisfies a stronger incoherence condition (see A2 in Section 3).\nNext, we discuss the stronger incoherence property that we require for our universal recovery guarantees. In particular, we show that the standard incoherence condition alone cannot provide universal recovery with any graph G and hence a stronger incoherence property is required.\nFinally, we empirically demonstrate our observation that, instead of the number of samples, the spectral gap of G is what really governs recoverability of the true matrix. In particular, we construct a family of graphs based on the stochastic block model and show that the probability of success grows linearly with the spectral gap, irrespective of the number of samples.\nNotation: We denote matrices by capital letters (e.g. U) and vectors by small letters (e.g. u). UT denotes the transpose of the matrix U . Uij represents the (i, j)-th element of U . Ui represents the i-th column of U and U i represents the i-th row of U (but in column format). \u2016u\u2016 represents the L2 norm of u and \u2016U\u2016 represents the spectral norm of U , i.e., \u2016U\u2016 = maxx:\u2016x\u2016\u22641 \u2016Ux\u2016. \u2016U\u2016F represents the Frobenius norm and \u2016U\u2016\u221e is the absolute maximum element of U . C = A.B represents the Hadamard product of A,B, i.e., Cij = AijBij . Similarly, (u.v)i = uivi. 1 denotes the all 1\u2019s vector. 1\u22a5 represents a unit vector that is perpendicular to 1 and is determined appropriately by the context.\nPaper Organization: In the next section, we discuss some related works. Then, in Section 3, we define the problem of matrix completion and the bipartite graph G that we use. We present our main results in Section 4 and discuss the additional incoherence assumption in Section 5. In Section 6, we present observations from our empirical study. Finally in Section 7, we provide the proof of our exact recovery result."}, {"heading": "2 Related work", "text": "Matrix completion: In a seminal paper on matrix completion, (Cande\u0300s & Recht, 2009) showed that any n \u00d7 n incoherent matrix of rank r can be recovered from Cn1.2r log(n) uniform random samples using nuclear norm minimization. Later, assuming the matrix to be strongly incoherent, (Cande\u0300s & Tao, 2010) improved the sample complexity for nuclear norm minimization method to O(nr log6(n)). Subsequently, (Recht, 2009; Gross, 2011) generalized this result for any incoherent matrix using matrix Bernstein inequalities and presented significantly simpler proofs. Concurrently other algorithms were shown to recover incoherent matrices using O(nr log(n)) (or worse) samples such as: SVD followed by descent on Grassmanian manifold (Keshavan et al., 2010), alternating minimization (Jain et al., 2012). We note that all the above mentioned results need to assume a rather restrictive sampling scheme, i.e., each entry is sampled uniformly at random and furthermore require a fresh set of samples for each new matrix. Moreover, the number of samples required is at least O(nr log n).\nOther sampling schemes: Recently, there has been some results for different type of sampling schemes such as power-law distributions (Meka et al., 2009), but here again universal results are not known and furthermore the proposed algorithms are not robust to noise. Another line of work has been to devise sampling schemes dependent on the data matrix (Chen et al., 2013), (Kira\u0301ly & Tomioka, 2012). Naturally, these schemes cannot be universal as the sampling scheme itself is dependent on the data matrix. Furthermore, practicality of such schemes is not clear a priori.\nUniversality: Universality is an important property in signal processing or sketching applications, as the goal there is to have one fixed sampling operator that performs well for all the given signals. While, universality results are well known for several other sensing problems, such as sparse vector recovery (Candes & Tao, 2005), one-bit compressive sensing (Gopi et al., 2013), similar results for low-rank matrix sensing are mostly restricted to RIP-type operators (Recht et al., 2010; Liu, 2011). Unfortunately, RIP-type of operators are typically dense, requiring knowledge of all elements of matrix to get observations, and also require large storage/computational complexity. On the other hand sampling individual elements is a sparse operator and hence computationally efficient. Hence, for several signal processing applications, universal matrix completion results are critical.\nIn fact, several recent works have studied problems similar to that of universal matrix completion. For example, (Kira\u0301ly & Tomioka, 2012; Heiman et al., 2013) and (Lee & Shraibman, 2013). However, there are critical differences in our results/approaches that we now highlight. In (Kira\u0301ly & Tomioka, 2012) authors consider an algebraic approach to analyze sufficient conditions for matrix completion. While they propose interesting deterministic sufficient conditions, the algorithm analyzed in the paper requires solving an NP hard problem. In contrast we analyze the nuclear norm minimization method which is known to have several efficient implementations. In (Heiman et al., 2013) and (Lee & Shraibman, 2013) authors consider sampling based on expanders but only provide generalization error bounds rather than exact recovery guarantee. Moreover, the recovered matrix using their algorithm need not have a low-rank."}, {"heading": "3 Problem Definition & Assumptions", "text": "Let M \u2208 Rn1\u00d7n2 be a rank-r matrix and let n1 \u2265 n2. Define n = max{n1, n2} = n1. Let M = U\u03a3V T be the SVD of M and let \u03c31 \u2265 \u03c32 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r be the singular values of M . We observe a small number of entries of M indexed by a set \u2126 \u2208 [n1]\u00d7 [n2]. That is, we observe Mij ,\u2200(i, j) \u2208 \u2126. Define the sampling operator P\u2126 : R n1\u00d7n2 \u2192 Rn1\u00d7n2 as:\nP\u2126(M) =\n{\nMij , if (i, j) \u2208 \u2126, 0, if (i, j) 6\u2208 \u2126.\n(1)\nNext, we define a bipartite graph associated with the sampling operator P\u2126. That is, let G = (V,E) be a bipartite graph where V = {1, 2, . . . , n1} \u222a {1, 2, . . . , n2} and (i, j) \u2208 E iff (i, j) \u2208 \u2126. Let G \u2208 Rn1\u00d7n2 be the biadjacency matrix of the bipartite graph G with Gij = 1 iff (i, j) \u2208 \u2126. Note that, P\u2126(M) = M.G, where . denotes the Hadamard product.\nNow, the goal in universal matrix completion is to design a set \u2126 and a recovery algorithm, s.t., all rank-r matrices M can be recovered using only P\u2126(M). In the next section, we present two results for this problem. Our first result gives an approximate solution to the matrix completion problem and our second result gives exact recovery guarantees.\nFor our results, we require G, that is associated with \u2126, to be a d-regular bipartite graph with large spectral gap. More concretely, we require the following two properties from the sampling graph G:\nAssumptions on G/\u2126:\n\u2022 G1 Top singular vectors of G are all 1\u2019s vector. \u2022 G2 \u03c31(G) = d and \u03c32(G) \u2264 C \u221a d.\nNote that as the graph is d-regular, hence |\u2126| = nd. The eigenvalues of the adjacency matrix of the bipartite graph G are {\u03c3i(G),\u2212\u03c3i(G)}, i = 1, ..n. We state all the definitions in terms of singular values of G instead of the eigenvalues of the adjacency matrix. The above two properties are satisfied by a class of expander graphs called Ramanujan graphs; in fact, Ramanujan graphs are defined by using this spectral gap property:\nDefinition 3.1 (Ramanujan graph (Hoory et al., 2006)). Let \u03c31(G), \u03c32(G), ..., \u03c3n(G) be the singular values of G in decreasing order. Then, a d-regular bipartite graph G is a Ramanujan graph if \u03c32(G) \u2264 2 \u221a d\u2212 1.\nRamanujan graphs G are well-studied in literature and there exists several randomized/deterministic methods to generate such graphs. We briefly discuss a couple of popular constructions in Section 4.2.\nIncoherence assumptions: Now, we present incoherence assumptions that we impose on M :\nA1 ||U i||2 \u2264 \u00b50r n1 ,\u2200i and ||V j ||2 \u2264 \u00b50r n2 ,\u2200j (2) A2 \u2016 \u2211\nk\u2208S\nn1 d UkUk T \u2212 I\u2016 \u2264 \u03b4d,\u2200S \u2282 [n1], |S| = d and\n\u2016 \u2211\nk\u2208S\nn2 d\u2032 V kV k T \u2212 I\u2016 \u2264 \u03b4d,\u2200S \u2282 [n2], |S| = d\u2032. (3)\nd\u2032 = dn2/n1. Note that A1 is the standard incoherence assumption required by most of the existing matrix completion results. However, A2 is a stricter assumption than A1 and is similar to the stronger incoherence property introduced by (Cande\u0300s & Tao, 2010). We discuss necessity of such assumption for universal matrix completion in Section 5."}, {"heading": "4 Main Results", "text": "We now present our main results for the matrix completion problem. We assume that \u2126 is generated using a bipartite d-regular expander and satisfies G1 and G2 (see Section 3). Our first result shows that, if M satisfies A1, then the best rank-r approximation of P\u2126(M) is \u201cclose\u201d to M and hence serves as a good approximation for M that can also be used for initialization of other methods like alternating least squares. Our second results shows that if M satisfies both A1 and A2, then using nuclear-norm minimization based method, P\u2126(M) can be used to recover back M exactly."}, {"heading": "4.1 Matrix approximation", "text": "Theorem 4.1. Let G be a d-regular bipartite graph satisfying G1 and G2. Let M be a rank-r matrix that satisfies assumption A1. Then,\n\u2225 \u2225 \u2225 n\nd P\u2126(M)\u2212M\n\u2225 \u2225 \u2225 \u2264 C\u00b50r\u221a\nd ||M ||.\nThat is, \u2016n d Pk(P\u2126(M))\u2212M\u2016 \u2264 2C\u00b50r\u221ad ||M ||, for any k \u2265 r, where Pk(A) is the best rank-k approx-\nimation of A and can be obtained using top-k singular vectors of A.\nNow, ifM is a PSDmatrix then the above result is exactly same as the Theorem 1.1 of (Keshavan et al., 2010). For non-PSD matrices, our result requires a bound on norm of each row of singular vectors of M , while the result of (Keshavan et al., 2010) only requires a bound on the largest element of M , hence is similar to our requirement but is strictly weaker as well.\nOn the other hand, our result holds for all M for a given \u2126, if \u2126\u2019s associated graph G satisfies both G1 and G2. Moreover, if G is generated using an Erdos-Renyi graph then, after a standard trimming step, the above theorem directly implies(for PSD matrices) Theorem 1.1 of (Keshavan et al., 2010). Finally, we would like to stress that our proof is significantly simpler and is able to exploit the fact that Erdo\u0308s-Re\u0301nyi graphs have good spectral gap in a fairly straightforward and intuitive manner.\nWe now present a detailed proof of the above theorem.\nProof. Let M = U\u03a3V T , U, V \u2208 Rn\u00d7r. Note that,\n\u2016n d P\u2126(M)\u2212M\u2016 = max\n{x,y:\u2016x\u20162=1,\n\u2016y\u20162=1}\nyT ( n\nd P\u2126(M)\u2212M)x.\nNow,\nyT ( n\nd P\u2126(M)\u2212M)x =\nr \u2211\ni=1\n(n\nd \u03c3i(y.Ui)\nTG(x.Vi)\u2212 \u03c3i(yTUi)(xTVi) ) . (4)\nLet y.Ui = \u03b1i1+ \u03b2i1 i \u22a5. Then, \u03b1i = 1T (y.Ui) n = y TUi n . Hence,\nyT ( n\nd P\u2126(M)\u2212M)x =\nr \u2211\ni=1\n(\n\u03c3i(y TUix TVi + n\nd \u03b2i1\ni \u22a5 T G(x.Vi))\u2212 \u03c3iyTUixTVi\n)\n\u03b61 \u2264Cn\u221a\nd\n\u2211\ni\n\u03c3i\u03b2i\u2016x.Vi\u20162 \u03b62 \u2264 \u03c31 Cn\u221a d\n\u221a\n\u2211\ni\n\u03b22i\n\u221a\n\u2211\ni\n\u2016x.Vi\u201622. (5)\nwhere \u03b61 follows from assumption G2 and \u03b62 follows from the Cauchy-Schwarz inequality. Now,\nr \u2211\ni=1\n\u03b22i \u2264 r \u2211\ni=1\n\u2016y.Ui\u20162 = n \u2211\nj=1\nr \u2211\ni=1\ny2jU 2 ji\n\u03b61 \u2264\nn \u2211\nj=1\ny2j \u00b50r\nn\n\u03b62 =\n\u00b50r\nn , (6)\nwhere \u03b61 follows from A1 and \u03b62 follows by using \u2016y\u20162 = 1. Using similar argument as above, \u2211r\ni=1 \u2016x.Vi\u20162 \u2264 \u00b50rn . Theorem now follows by using (5), (6), and the above inequality. The proof of the second part is given in appendix A."}, {"heading": "4.2 Nuclear norm minimization", "text": "We now present our result for exact recovery of the matrix M using P\u2126(M) alone. For recovery, we use the standard nuclear norm minimization algorithm, i.e., we obtain a matrix X by solving the following convex optimization problem:\nmin \u2016X\u2016\u2217 s. t. P\u2126(X) = P\u2126(M),\n(7)\nwhere \u2016X\u2016\u2217 denotes the nuclear norm of X; nuclear norm of X is equal to the sum of its singular values.\nAs mentioned in Section 2, nuclear norm minimization technique is a popular technique for the low-rank matrix completion problem and has been shown to provably recover the true matrix, assuming that \u2126 is sampled uniformly at random and |\u2126| \u2265 cnr log n (Cande\u0300s & Tao, 2010).\nBelow, we provide a universal recovery result for the nuclear-norm minimization method as long as the samples \u2126 come from G that satisfies G1 and G2.\nTheorem 4.2. Let M be an n1 \u00d7 n2 matrix of rank r satisfying assumptions (A1) and (A2) with \u03b4d \u2264 16 , and \u2126 is generated from a d-regular graph G that satisfies the assumptions (G1) and (G2). Also, let d \u2265 36C2\u00b520r2, i.e., |\u2126| = nd \u2265 36C2\u00b520r2max{n1, n2}. Then M is the unique optimum of problem (7).\nNote that the above result requires only deterministic constraints on the sampling operator P\u2126 and guarantees exact recovery for any matrix M that satisfies A1, A2. As mentioned earlier, A2 is a stronger assumption than A1. But as we show in Section 5, universal recovery is not possible with assumption A1 alone.\nWe can use the above theorem to derive results for several interesting sampling schemes such as random d-regular graphs. Using Theorem 1 in (Friedman, 2003), the second singular value of a random d-regular graph is \u2264 2 \u221a d\u2212 1 + \u01eb, for every \u01eb > 0, with high probability. Hence, a random d-regular graph, with high probability, obeys G1 and G2 which implies the following exact recovery result:\nCorollary 4.3. Let M be an n1 \u00d7 n2 matrix of rank r satisfying assumptions (A1) and (A2) with \u03b4d \u2264 16 , and \u2126 is generated from a random d-regular graph, then M is the unique optimum of program (7) when d \u2265 36 \u2217 4\u00b520r2, with high probability.\nNote that the standard completion results such as (Cande\u0300s & Recht, 2009), (Keshavan et al., 2010) generate \u2126 using Erdo\u0308s-Re\u0301nyi graph, which are slightly different than the random d-regular graph we considered above. However, (Feige & Ofek, 2005) showed that the second largest singular value of the Erdo\u0308s-Re\u0301nyi graph, G(n1, n2, p), is O( \u221a d) when p is \u0398(log(n1)/n2). Interestingly, even when p = c/n2, i.e. n2 \u00b7 p is a constant, trimming the graph (i.e., removing few nodes with high degree) gives a graph G s.t. \u03c32(G) = O( \u221a d). Hence, we can again apply Theorem 4.2 to obtain the following result:\nCorollary 4.4. Let M be an n1 \u00d7n2 matrix of rank r satisfying assumptions (A1) and (A2) with \u03b4d \u2264 16 , and \u2126 is generated from a G(n, p) graph after trimming, then M is the unique optimum of program (7) when p \u2265 36c\u00b5 2 0 r2\nmin{n1,n2} , with high probability.\nWhile the above two results exploit the fact that a random graph is almost a Ramanujan expander and hence our general recovery result can be applied, the graph construction is still randomized. Interestingly, (Lubotzky et al., 1988; Margulis, 1988; Morgenstern, 1994) proposed explicit deterministic constructions of Ramanujan graphs when d\u2212 1 is a prime power. Moreover, (Marcus et al., 2013) showed that bipartite Ramanujan graphs exist for all n and d. However, explicit construction for all n and d still remains an open problem."}, {"heading": "5 Discussion", "text": "In this section, we discuss the two assumptions A1 and A2 that are mentioned in Section 3. Note that A1 is a standard assumption that is used by most of the existing approaches (Cande\u0300s & Recht, 2009), (Keshavan et al., 2010). Moreover, it is easy to show that for any matrix \u201coblivious\u201d sampling approach, this assumption is necessarily required for exact recovery. For example, if Gij = 0, i.e., (i, j)-th element is not observed then we cannot recover M = eie T j .\nHowever, A2 is a slightly non-standard assumption and intuitively it requires the singular vectors of M to satisfy RIP. Note that A2 is similar to the strong incoherence property introduced by (Cande\u0300s & Tao, 2010). Below we show the connection between strong incoherence property (SIP) assumed in (Cande\u0300s & Tao, 2010) and assumption A2.\nClaim 5.1. Let M \u2208 Rn1\u00d7n2 be a rank-r matrix. Let M = U\u03a3V T satisfy SIP i.e.,\n|\u3008ei, UUTej\u3009 \u2212 r\nn1 1i=j | \u2264 \u00b51\n\u221a r n1 ,\u22001 \u2264 i, j \u2264 n1,\n|\u3008ei, V V Tej\u3009 \u2212 r\nn1 1i=j | \u2264 \u00b51\n\u221a r n2 ,\u22001 \u2264 i, j \u2264 n2. (8)\nThen, M satisfies A2 for all d \u2265 r and \u03b4d \u2264 \u00b51 \u221a r.\nNote that the above claim holds with \u03b4d = \u00b51 \u221a r, \u2200d \u2265 r. This bound is independent of d and\nhence weak; as d becomes close to n1, \u2225 \u2225 \u2225 n1 d \u2211 k\u2208S U kUk T \u2212 I \u2225 \u2225 \u2225 gets close to 0 since UTU = I. We leave the task of obtaining a stronger bound as an open problem.\nIn the context of universal recovery, a natural question is if any additional assumption is required or the standard A1 assumption alone suffices. Here, we answer this question in negative. Specifically, we show that if M satisfies A1 only, then universal recovery guarantee for even rank-2 matrices cannot be provided by using as many as n1n2/4 observations.\nClaim 5.2. Let \u2126 be a fixed set of indices and let P\u2126 be the sampling operator as defined in (1). Let n1 = n2 = n and let |\u2126| = n2/4. Then, there exists a rank-2 matrix M that cannot be recovered exactly from P\u2126(M).\nNow, another question is if we require a property as strong as A2 and if just a lower-bound on \u2016U i\u20162, \u2016V j\u20162 is enough for universal recovery. The proof of the above claim (in appendix) shows that even if \u2016U i\u20162, \u2016V j\u20162 are lower-bounded, then also exact recovery is not possible."}, {"heading": "6 Simulations", "text": "In this section, we will present a few empirical results on both synthetic and real data sets. The goal of this section is to demonstrate effect of the spectral gap of the sampling graph G (associated with \u2126) on successful recovery of a matrix.\nFirst, we use synthetic data sets generated in the following manner. We first sample U, V \u2208 R 500\u00d710 using standard normal distribution. We then generate rank-10 matrix M , using M = UV T . As U, V are sampled from the normal distribution, hence w.h.p., M satisfies incoherence assumptions A1, A2 mentioned in Section 3. Next, we generate a sequence of sampling operators P\u2126 (and the associated graph G) with varying (relative) spectral gap(1\u2212 \u03c32(G)/\u03c31(G)) by using a stochastic block model. In the basic stochastic block model, the nodes can be thought of as being divided into two clusters. Now, each intra-cluster edge is sampled uniformly with probability p and an inter-cluster edge is sampled uniformly with probability q. Note that, when p = q, then the spectral gap is largest and when p = 1, q = 0, then spectral gap is smaller as there are two distinct clusters in that case (Nadakuditi & Newman, 2012).\nNote that number of samples generated in this model depends only on the value of p + q. To generate \u2126 (i.e., G), we first fix a value for p+q, hence fixing the number of samples, and then vary p, q, which gives graphs of different spectral gap. As value of p goes from 0 to p+q2 , the spectral gap goes up and from p+q2 to p+ q, the spectral gap goes down. Figure 1(a) clearly demonstrates this trend.\nWe use an Augmented Lagrangian Method (ALM) based method (Lin et al., 2010) to solve the nuclear norm minimization (7) problem. A trial is considered to be successful if the relative error (in Frobenius norm) is less than 0.01. We average over 50 such trials to determine the success ratio.\nFigure 1(a) plots the (relative) spectral gap (dotted lines) and the success ratio (solid lines) as p varies. Lines of different colors indicate different number of samples (p + q). As expected, the spectral gap increases initially, as p varies from 0 to p+q2 and then it decreases. Moreover, the trend of successful recovery also follows a similar trajectory and hence, is more or less independent of the number of samples (given a particular spectral gap).\nFigure 1(b) shows fraction of successful recoveries as the spectral gap increases. Here again, lines of different colors indicate different number of samples (p + q). Clearly, success ratio is positively correlated with the spectral gap of the sampling operator and in fact exhibits a phase transition type of phenomenon. We expect the difference in success ratio for different p+ q values to decrease with increasing problem size, i.e, dimensionality of M .\nNow, we conduct an experiment to show that spectral gap of G helps in reducing effect of noise as well. To this end, we generated noisy input matrix (M + Z), where Z is a random Gaussian matrix and let \u03c3 = ||Z||F /||M ||F . We consider two values of \u03c3, i.e., \u03c3 = 0.1 and 0.2. Figure 2(a) plots the error(in Frobenius norm) in the recovered matrix against the relative spectral gap in the noisy setting. Solid lines represent \u03c3 = 0.1 and dotted lines represent \u03c3 = 0.2. Clearly, larger spectral gap leads to smaller error in recovery. Moreover, the \u201cmatrix completion denoising\u201d effect (Candes & Plan, 2010) can also be observed. For example, when ||Z||F = 0.1 and p + q = 0.4, output error is less than 0.05, and when ||Z||F = 0.2, error is less than 0.1\nTemperature prediction: Finally we take a real dataset of temperature values(T ) for 365 days at 316 different locations from (NCDC), which has been used to test matrix completion algorithms (Candes & Plan, 2010) before. Note that T is approximately rank-1 matrix with\n\u03c31(T )/||T ||F = 0.98(\u03c3i(T ) are singular values of T ). We use the block model sampling scheme to sample entries from T , and let the output of (7) be T\u0302 . In figure 2(b) we plot the error ||T\u0302 \u2212T ||/||T || for different values of spectral gap of G and for different number of samples (p + q). Note that ||X \u2212 T ||/||T || \u2265 \u03c32(T )/\u03c31(T ) for any rank-1 matrix X, and we see that for large enough spectral gap we achieve this bound.\nFinally in figure 2(c) we compare running times of the r-closure algorithm proposed in (Kira\u0301ly & Tomioka, 2012) with the nuclear norm minimization algorithm. While it is noted that this algorithm has better error guarantees, it is combinatorial and takes exponential time to compute."}, {"heading": "7 Proof of Theorem 4.2", "text": "In this section, we present the proof of our main result (Theorem 4.2). The main steps of our proof are similar to the proof given by (Recht, 2009). The main difference is that the bounds in the existing proof assume that \u2126 is independent of M and hence is not adversarial and holds with high probability. In contrast, for our proofs, bounds are deterministic and our proofs have to work under the assumption that M is adversarially selected for a given \u2126.\nThe key steps in the proof are: a) provide conditions that an optimal dual solution (or dual certificate) of problem (7) should satisfy, so that the true matrix M is the unique optimum of (7), b) construct such a dual certificate and hence guarantee that M is the unique optimum of (7).\nWe first introduce a few notations required by our proof. For simplicity, we assume that n1 = n2 = n. Note that, our proof easily generalizes to case when n1 6= n2. Define T which is a subspace of Rn\u00d7n, and is span of all matrices of form UXT and Y V T , i.e. all matrices with either row space in V or column space in U . Hence, the projection operator PT is defined as follows:\nPT (Z) = UUTZ + ZV V T \u2212 UUTZV V T = UUTZ + (I \u2212 UUT )ZV V T .\nHence any matrix in T can be written as UXT + Y V T , for some X and Y such that Y and U are orthogonal to each other. Similarly, we can define the projection operator onto T\u22a5, the orthogonal complement of T : PT\u22a5(Z) = (I \u2212 UUT )Z(I \u2212 V V T ). Now, before presenting conditions on the dual certificate and construction of the dual certificate, we provide a few structural lemmas that show \u201cgoodness\u201d of operators PT and P\u2126. We would like to stress that the key differences of our proof from that of (Recht, 2009) is in fact proofs of these structural lemmas and also in the way we apply Lemma 7.3. We specifically show (using Lemma 7.3) that each matrix in the series used in the construction of dual certificate Y (discussed later in the section) is incoherent and has small infinity norm.\nThe first lemma proves injectivity of operator P\u2126 on the subspace T :\nLemma 7.1. Let M = U\u03a3V T satisfies A1, A2 and let the graph G that generates \u2126 satisfies G1, G2 (see Section 3). Then, for any matrix Z \u2208 T ,\n\u2016n d PTP\u2126(Z)\u2212 Z\u2016F \u2264\n\u221a\n2(\u03b42d + C2\u00b520r 2\nd )\u2016Z\u2016F .\nNext, we provide a lemma that characterizes the \u201cdifference\u201d between P\u2126(Z) and Z, for any incoherent-type Z \u2208 T :\nLemma 7.2. Let Z \u2208 T , i.e., Z = UXT + Y V T and Y is orthogonal to U , and X and Y be incoherent, i.e.,\n\u2016Xi\u20162 \u2264 c 2 1\u00b50r n , \u2016Y j\u20162 \u2264 c 2 2\u00b50r n .\nLet \u2126 satisfy the assumptions G1 and G2, then:\n\u2016n d P\u2126(Z)\u2212 Z\u2016 \u2264 (c1 + c2) C\u00b50r\u221a d .\nThe next lemma is a stronger version of Lemma 7.1 for special incoherent-type matrices Z \u2208 T .\nLemma 7.3. Let Z \u2208 T , i.e., Z = UXT + Y V T and Y is orthogonal to U . Let X and Y be incoherent, i.e.,\n\u2016Xi\u20162 \u2264 c 2 1\u00b50r n , \u2016Y j\u20162 \u2264 c 2 2\u00b50r n .\nLet Z\u0303 = Z \u2212 n d PTP\u2126(Z). Then, the following holds for M,\u2126 that satisfy conditions given in Lemma 7.1:\n\u2022 \u2016Z\u0303\u2016\u221e \u2264 (c1+c2)\u00b50rn (\u03b4d + C\u00b50r\u221a d ).\n\u2022 Z\u0303 = UX\u0303T + Y\u0303 V T and X\u0303 and Y\u0303 are incoherent. \u2016X\u0303i\u20162 \u2264 \u00b50r n\n(\n\u03b4dc1 + 2c2 C\u00b50r\u221a\nd\n)2 and \u2016Y\u0303 j\u20162 \u2264\n\u00b50r n (\u03b4dc2 + (c1 + c2) C\u00b50r\u221a d )2.\nThe proof of the above three lemmas is provided in the appendix. Conditions on the dual certificate: We now present the lemma that characterizes the conditions a dual certificate should satisfy so that M is the unique optimum of (7):\nLemma 7.4. Let M,\u2126 satisfy A1, A2 and G1, G2, respectively. Then, M is the unique optimum of (7), if there exists a Y \u2208 Rn\u00d7n that satisfies the following:\n\u2022 P\u2126(Y ) = Y\n\u2022 ||PT (Y )\u2212 UV T ||F \u2264 \u221a d 8n\n\u2022 ||PT\u22a5(Y )|| < 12 Having specified the conditions on dual certificate and also the key structural lemma, we are\nnow ready to present the proof of Theorem 4.2.\nProof of Theorem 4.2. We prove the theorem by constructing a dual certificate Y that satisfies conditions in lemma 7.4 and hence guarantee that M is exactly recovered by (7). Our construction of Y is similar to the golfing scheme based construction given in (Gross, 2011; Recht, 2009). In particular, Y is obtained as the p-th term of the series given below:\nWk+1 = Wk \u2212 n\nd PTP\u2126Wk, Yk =\nk\u22121 \u2211\ni=0\nn d P\u2126Wi,\nwhere W0 = UV T . That is, Y = Yp where p = \u230812 log3( n18C2\u00b52 0 r )\u2309. Also, define \u03b1 = C\u00b50r\u221a d . As d \u2265 36C2\u00b520r2, we get: \u03b1 \u2264 16 .\nNow, the first condition of Lemma 7.4 is satisfied trivially by construction as Yp is a sum of P\u2126(Wi) terms. Bounding ||PT (Y )\u2212 UV T ||F : By construction:\nPT (Y )\u2212 UV T = p\u22121 \u2211\ni=0\nn d PTP\u2126Wi \u2212 UV T = \u2212Wp. (9)\nNow, note that each Wk \u2208 T . Hence, using Lemma 7.1,\n\u2016Wk+1\u2016F = \u2016Wk \u2212 n\nd PTP\u2126Wk\u2016F (10)\n\u2264 \u221a 2(\u03b42d + C2\u00b520r 2\nd )\u2016Wk\u2016F \u2264\n1 3 \u2016Wk\u2016F , (11)\nwhere the last inequality follows by using assumption on \u03b4d and by using \u03b1 \u2264 1/6. Hence, using (9), (11), we have:\n\u2016PT (Y )\u2212 UV T \u2016F = \u2016Wp\u2016F \u2264 ( 1\n3\n)p \u2016W0\u2016F \u2264 \u221a d\n8n , (12)\nwhere the last inequality follows by using p = \u230812 log3( n18C2\u00b52 0 r )\u2309 and by using \u2016W0\u2016F =\n\u221a r.\nBounding \u2016PT\u22a5(Y )\u2016: Recall that, Wk \u2208 T . Now, let Wk = UXTWk + YWkV T with YWk perpendicular to U . Moreover, let,\n\u2016XiWk\u2016 \u2264 cWk1 \u221a \u00b50r\u221a n , \u2016Y iWk\u2016 \u2264 cWk2 \u221a \u00b50r\u221a n .\nNote that, for W0 = UV T , cW01 = 1 and c W0 2 = 0. Hence, using Lemma 7.3: c W1 1 \u2264 16 , c W1 2 \u2264 16 . Similarly, applying Lemma 7.3 k-times, we get:\ncWk1 = c Wk 2 \u2264 3k\u22121\n1\n6k . (13)\nNow, by using construction of Y , by triangle inequality, and by using the fact that PT\u22a5 is a contraction operator:\n\u2016PT\u22a5(Y )\u2016 \u2264 p \u2211\nk=1\n\u2016n d P\u2126(Wk\u22121)\u2212Wk\u22121\u2016 \u2264\np \u2211\nk=1\n(c Wk\u22121 1 + c Wk\u22121 2 ) C\u00b50r\u221a d , (14)\nwhere the last inequality follows by using Lemma 7.2. Now, using (13), (14), and Lemma 7.3, we have:\n\u2016PT\u22a5(Y )\u2016 \u2264 C\u00b50r\u221a d (1 + 2 \u00b7 1 6 (\np\u22121 \u2211\nk=2\n3k\u22122 1 6k\u22122 )) \u2264 \u03b1(1 + 2 \u00b7 1 6 \u00b7 1 1\u2212 12 ) < 1/2.\nHence, proved."}, {"heading": "8 Conclusions", "text": "In this paper, we provided the first (to the best of our knowledge) universal recovery guarantee for matrix completion. The main observation of the paper is that the spectral gap of G (that generates \u2126) is the key property that governs recoverability of M using P\u2126(M) alone. For example, if G is a Ramanujan expander (i.e., \u03c32(G) = O( \u221a d)), then we have universal recovery guarantees for matrices with strong incoherence property. For uniformly sampled \u2126, our main result implies exact recovery of constant rank matrices using O(n) entries, in contrast to the O(n log n) entries required by the existing analyses. One caveat is that we require stronger incoherence property to obtain the above given sample complexity. Our results also provide a recipe to determine if a given index set \u2126 is enough to recover a low-rank matrix. That is, given \u2126 and its associated graph G, we can measure the spectral gap of G and if it is large enough then our results guarantee exact recovery of strongly incoherent matrices.\nIn Section 5, we showed that the standard incoherence assumption alone is not enough for universal recovery and a property similar to A2 (see Section 3) is required. However, it is an open problem to obtain precise information theoretic limits on \u03b4d (see A2) for universal recovery guarantees. Another interesting research direction is to study the alternating minimization method under assumptions given in Section 3. Finally, we also plan to apply our universal recovery guarantees to specific applications in the domains of sketching and signal-processing."}, {"heading": "A Proof of second part of Theorem 4.1", "text": "Proof. From the first part of Theorem 4.1 we get that \u2225 \u2225 n d P\u2126(M)\u2212M \u2225 \u2225 \u2264 C\u00b50r\u221a d ||M ||. Hence by Weyl\u2019s inequalities we get that\n\u2223 \u2223 \u2223 n\nd \u03c3k(P\u2126(M)) \u2212 \u03c3k(M)\n\u2223 \u2223 \u2223 \u2264 C\u00b50r\u221a\nd ||M ||,\nfor all i. Since M is rank-r matrix, for any k \u2265 r+1, n d \u03c3k(P\u2126(M)) \u2264 C\u00b50r\u221a d ||M ||. Hence by triangle inequality we get for any k \u2265 r, \u2225\n\u2225 \u2225\nn d Pk(P\u2126(M))\u2212M\n\u2225 \u2225 \u2225 \u2264 \u2225 \u2225 \u2225 n\nd Pk(P\u2126(M))\u2212\nn d P\u2126(M)\n\u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 n\nd P\u2126(M)\u2212M\n\u2225 \u2225 \u2225\n\u2264 n d \u03c3k+1(P\u2126(M)) +\n\u2225 \u2225 \u2225 n\nd P\u2126(M)\u2212M\n\u2225 \u2225 \u2225\n\u2264 2C\u00b50r\u221a d ||M ||."}, {"heading": "B Proof of Claim 5.1", "text": "Proof. Let S be a set of size |S| = d. Since n1 d\n\u2211\nk\u2208S U kUk T \u2212 I is a Hermitian matrix, \u2225 \u2225 \u2225 \u2225 \u2225 n1 d \u2211\nk\u2208S UkUk\nT \u2212 I \u2225 \u2225 \u2225\n\u2225 \u2225\n= \u2225 \u2225\n\u2225 n1 d US\n\u2225 \u2225 \u2225 2 \u2212 1, (15)\nwhere US is a matrix whose columns are U k, k \u2208 S. Now we will use equation (8) to bound \u2016US\u2016.\n\u2016US\u20162 = max x:\u2016x\u2016=1 \u2016USx\u20162 = max x:\u2016x\u2016=1\nd \u2211\ni,j=1\n\u3008U i, U j\u3009xixj\n= max x:\u2016x\u2016=1\nd \u2211\ni=1\n\u2016U i\u20162x2i + \u2211 i 6=j \u3008U i, U j\u3009xixj\n\u03b61 \u2264 max x:\u2016x\u2016=1 \u2016x\u20162 r + \u00b51\n\u221a r\nn + (d\u2212 1)\u2016x\u20162\u00b51\n\u221a r\nn\n= r + d\u00b51\n\u221a r\nn . (16)\n\u03b61 follows from (8). Hence from (15) and (16) we get,\n\u03b4d \u2264 r\nd + \u00b51\n\u221a r \u2212 1 \u2264 \u00b51 \u221a r."}, {"heading": "C Proof of Claim 5.2", "text": "Proof. Let M = UV T where U \u2208 Rn\u00d72 and V \u2208 Rn\u00d72 are both orthonormal matrices. Now, let S = {j s.t., (1, j) \u2208 \u2126 or (2, j) \u2208 \u2126} be the set of all the columns of M that have an observed entry in any of the first two rows.\nAs |\u2126| = n2/4, hence wlog we can assume that |S| \u2264 n/2. Let S\u2032 = S \u222a S1, where S1 is any set of columns s.t. |S\u2032| = n/2. Now, construct U, V as follows:\nV j =\n{\n[ 1\u221a n 1\u221a n ], \u2200j \u2208 S\u2032, [ 1\u221a n \u22121\u221a n ], \u2200j 6\u2208 S\u2032,\n(17)\nU i =\n\n    \n    \n[ 1\u221a n 1\u221a n ], \u2200 3 \u2264 i \u2264 n/2, [ 1\u221a n \u22121\u221a n ], \u2200 n/2 + 1 \u2264 i \u2264 n, [a \u2212 a], i = 1, [b \u2212 b], i = 2.\n(18)\nNote that by construction, Mij = 0,\u22001 \u2264 i \u2264 2, j \u2208 S\u2032. That is, the first two rows of P\u2126(M) are all zeros. Since, U1, U2 participate in only those rows. Hence, even if V is known exactly, one cannot obtain any information about a, b from the observed entries. Only other constraints on a, b comes from orthonormality of U , which reduces to a2 + b2 = 2/n. Now, without violating incoherence assumptions, we can have multiple solutions to the above given equation that cannot be distinguished from each other. For example, a = 1\u221a 2n and b = \u221a 3 2n , or vice-versa, i.e., a = \u221a 3 2n and b = 1\u221a 2n .\nHence, exact recovery is not possible for the above given M for any \u2126 s.t. |\u2126| \u2264 n2/4."}, {"heading": "D Proofs of Lemmas used to prove Theorem 4.2", "text": "In this section we present the proofs of all the lemmas used to prove theorem 4.2.\nLemma (7.1). Let M = U\u03a3V T satisfy A1, A2 and let the graph G that generates \u2126 satisfy G1, G2 (see Section 3). Then, for any matrix Z \u2208 T ,\n\u2016n d PTP\u2126(Z)\u2212 Z\u2016F \u2264\n\u221a\n2(\u03b42d + C2\u00b520r 2\nd )\u2016Z\u2016F .\nProof of Lemma 7.1. Since Z \u2208 T , we can write Z = UXT + Y V T , such that Y and U are orthogonal. \u2016n\nd PTP\u2126(Z) \u2212 Z\u2016F \u2264 \u2016ndPTP\u2126(UXT ) \u2212 UXT \u2016F + \u2016ndPTP\u2126(Y V T ) \u2212 Y V T \u2016F . Since\nboth the above summands are similar we will bound the first term and then extend the results to the second one. Now,\n\u2016n d PTP\u2126(UXT )\u2212 UXT \u20162F =\u2016UUT ( n d P\u2126(UX T )\u2212 UXT )\u20162F + \u2016(I \u2212 UUT ) n d P\u2126(UX T )V V T \u20162F ,\nas both the terms on RHS are orthogonal to each other. Next, we bound both of these terms\nindividually.\n1)\u2016UUT (n d P\u2126(UX T )\u2212 UXT )\u20162F = \u2211\nij\n(\nU i T ( n\nd\n\u2211\nk\nUkUk T Gkj \u2212 I)Xj )2\n= \u2211\nj\n\u2211\ni\n(\nU i T ( n\nd\n\u2211\nk\nUkUk T Gkj \u2212 I)Xj )2\n\u03b61 =\n\u2211\nj\n\u2225 \u2225 \u2225 \u2225 \u2225 ( n d \u2211\nk\nUkUk T Gkj \u2212 I)Xj \u2225 \u2225 \u2225 \u2225\n\u2225\n2 \u03b62 \u2264 \u2211\nj\n\u03b42d\u2016Xj\u20162 = \u03b42d\u2016X\u20162F ,\nwhere \u03b62 follows from the assumption A2 and from the fact that G is d-regular, and \u03b61 follows by using:\nn \u2211\ni=1\n(U i T x)2 =\nn \u2211\ni=1\nxTU iU i T x = xT\n(\nn \u2211\ni=1\nU iU i T\n)\nx = xTUTUx = \u2016x\u20162.\n2)\u2016(I \u2212 UUT )n d P\u2126(UX T )V V T \u20162F =\u2016(I \u2212 UUT ) n d P\u2126(UX\nT )V \u20162F = r \u2211\ni=1\n\u2016(I \u2212 UUT )n d P\u2126(UX T )Vi\u201622\n\u03b61 =\nr \u2211\ni=1\nmax u\u0303:\u2016u\u0303\u2016\u22641 & u\u0303TU=0 \u2016u\u0303T n d P\u2126(UX T )Vi\u20162,\nwhere \u03b61 follows from the definition of the spectral norm. Now, we bound \u2016u\u0303T ndP\u2126(UXT )Vi\u20162 over {u\u0303 : \u2016u\u0303\u2016 \u2264 1 & u\u0303TU = 0}. Note that u\u0303TUk = 0 implies that u\u0303.Uk is orthogonal to all ones vector.\nr \u2211\ni=1\n\u2016u\u0303T n d P\u2126(\nr \u2211\nk=1\nUkX T k )Vi\u20162 =\nr \u2211\ni=1\n\u2016n d\nr \u2211\nk=1\n(Uk.u\u0303) TG(Xk.Vi)\u20162 \u03b61 \u2264\nr \u2211\ni=1\nn2C2\nd\n(\nr \u2211\nk=1\n\u2016Uk.u\u0303\u2016\u2016Xk.Vi\u2016 )2\n\u2264 r \u2211\ni=1\nn2C2\nd\n(\nr \u2211\nk=1\n\u2016Uk.u\u0303\u20162 )( r \u2211\nk=1\n\u2016Xk.Vi\u20162 ) \u03b62 \u2264 C 2\u00b520r 2\nd \u2016u\u0303\u20162\u2016X\u20162F .\n\u03b61 follows from the assumption G2 and \u03b62 from incoherence property A1. Using the above two bounds we get \u2016n d PTP\u2126(UXT ) \u2212 UXT \u20162F \u2264 \u2016X\u20162F (\u03b42d + C2\u00b52 0 r2 d ). Similarly \u2016n d PTP\u2126(Y V T )\u20162F \u2264 \u2016Y \u20162F (\u03b42d + C2\u00b52 0 r2 d ). Hence\n\u2016n d PTP\u2126(Z)\u2212 Z\u2016F \u2264\n\u221a\n2(\u03b42d + C2\u00b520r 2\nd )\u2016Z\u2016F .\nLemma (7.2). Let Z \u2208 T , i.e., Z = UXT + Y V T and Y is orthogonal to U , and X and Y be incoherent, i.e.,\n\u2016Xi\u20162 \u2264 c 2 1\u00b50r n , \u2016Y j\u20162 \u2264 c 2 2\u00b50r n .\nLet \u2126 satisfy the assumptions G1 and G2, then:\n\u2016n d P\u2126(Z)\u2212 Z\u2016 \u2264 (c1 + c2) C\u00b50r\u221a d .\nProof of Lemma 7.2. Note that \u2016n d P\u2126(Z)\u2212Z\u2016 \u2264 \u2016ndP\u2126(UXT )\u2212UXT \u2016+ \u2016ndP\u2126(Y V T )\u2212Y V T \u2016 by triangle inequality. First we will bound \u2016n d P\u2126(UX\nT )\u2212 UXT \u2016. The proof follows the same line as proof of Theorem 4.1.\n\u2016n d P\u2126(UX T )\u2212 UXT \u2016 = max {a,b:\u2016a\u2016=1,\u2016b\u2016=1} aT ( n d P\u2126(UX T )\u2212 UXT )b\n= max {a,b:\u2016a\u2016=1,\u2016b\u2016=1}\nr \u2211\ni=1\n(n\nd (a.Ui)\nTG(Xi.b)\u2212 (aTUi)(XTi b) )\n(19)\nLet a.Ui = \u03b1i1+ \u03b2i1 i \u22a5. Then \u03b1i = (aTUi) n and \u03b22i \u2264 \u2016a.Ui\u20162. Hence,\naT ( n\nd P\u2126(UX\nT )\u2212 UXT )b = r \u2211\ni=1\n(\n(aTUi)(X T i b) +\nn d \u03b2i1 i \u22a5 T G(Xi.b)\u2212 (aTUi)(XTi b) )\n\u03b61 \u2264\nr \u2211\ni=1\nCn\u221a d \u03b2i\u2016Xi.b\u2016 \u03b62 \u2264 Cn\u221a d\n\u221a \u221a \u221a \u221a r \u2211\ni=1\n\u03b22i\n\u221a \u221a \u221a \u221a r \u2211\ni=1\n\u2016Xi.b\u20162, (20)\nwhere \u03b61 follows from assumption G2 and \u03b62 from Cauchy-Schwarz inequality. Now\nr \u2211\ni=1\n\u03b22i \u2264 r \u2211\ni=1\n\u2016a.Ui\u20162 = n \u2211\nj=1\nr \u2211\ni=1\na2jU 2 ji \u2264\n\u00b50r\nn\nn \u2211\nj=1\na2j = \u00b50r\nn .\nSimilarly \u2211r i=1 \u2016Xi.b\u20162 \u2264 c2 1 \u00b50r n . Hence using (19), (20) and above two inequalities we get\n\u2016n d P\u2126(UX T )\u2212 UXT \u2016 \u2264 c1C\u00b50r\u221a d .\nSimilarly we can show that \u2016n d P\u2126(Y V T ) \u2212 Y V T \u2016 \u2264 c2C\u00b50r\u221a d . Hence the lemma follows from the above two bounds.\nLemma (7.3). Let Z \u2208 T , i.e., Z = UXT + Y V T and Y is orthogonal to U . Let X and Y be incoherent, i.e.,\n\u2016Xi\u20162 \u2264 c 2 1\u00b50r n , \u2016Y j\u20162 \u2264 c 2 2\u00b50r n .\nLet Z\u0303 = Z \u2212 n d PTP\u2126(Z). Then, the following holds for all M,\u2126 that satisfy the conditions given in Lemma 7.1:\n\u2022 \u2016Z\u0303\u2016\u221e \u2264 (c1+c2)\u00b50rn (\u03b4d + C\u00b50r\u221a d ).\n\u2022 Z\u0303 = UX\u0303T + Y\u0303 V T and X\u0303 and Y\u0303 are incoherent. \u2016X\u0303i\u20162 \u2264 \u00b50r n\n(\n\u03b4dc1 + 2c2 C\u00b50r\u221a\nd\n)2 and \u2016Y\u0303 j\u20162 \u2264\n\u00b50r n (\u03b4dc2 + (c1 + c2) C\u00b50r\u221a d )2.\nProof of Lemma 7.3.\nZ\u0303ij = (Z \u2212 n\nd PTP\u2126(Z))ij =\n( UUT (UXT \u2212 n d P\u2126(UX T ))\u2212 (I \u2212 UUT )(n d P\u2126(UX T ))V V T ) ij\n+ ( (Y V T \u2212 n d P\u2126(Y V T ))V V T \u2212 UUT n d P\u2126(Y V T )(I \u2212 V V )T ) ij ,\nwhere the last equality follows by using the definition of PT and the fact that Z = PT (Z). Now, we bound the first term in the RHS of the above equation. To do this we individually bound (UUT (UXT \u2212 n\nd P\u2126(UX T )))ij and ((I \u2212 UUT )(ndP\u2126(UXT ))V V T )ij :\n(UUT (UXT \u2212 n d P\u2126(UX T )))ij = U iT\n(\nI \u2212 n d\nn \u2211\nk=1\nUkUk T\nGkj\n)\nXj \u03b61 \u2264 \u03b4d\u2016U i\u2016\u2016Xj\u2016 \u03b62 \u2264 \u03b4dc1\u00b50r\nn ,\nwhere \u03b61 follows from A2 and \u03b62 from the incoherence property A1 and the hypothesis of the lemma. Similarly,\n\u2223 \u2223 \u2223 ((I \u2212 UUT )(n\nd P\u2126(UX\nT ))V V T )ij\n\u2223 \u2223 \u2223 = \u2223 \u2223 \u2223 U i T\n\u22a5 U T \u22a5(\nn d P\u2126(UX T ))V V j \u2223 \u2223 \u2223 =\n\u2223 \u2223 \u2223 \u2223 \u2223 u\u0302 n d r \u2211\nk=1\nP\u2126(UkX T k )v\u0302\n\u2223 \u2223 \u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 n d r \u2211\nk=1\n(u\u0302.Uk) TG(Xk.v\u0302)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u03b61 \u2264 Cn\u221a\nd\nr \u2211\nk=1\n\u2016u\u0302.Uk\u2016\u2016Xk.v\u0302\u2016,\nwhere u\u0302 = U\u22a5U i\u22a5, v\u0302 = V V j, 1T (u\u0302.Uk) = 0 and \u03b61 follows from G2. Now note that,\nr \u2211\nk=1\n\u2016u\u0302.Uk\u20162 = r \u2211\nk=1\nn \u2211\nl=1\n\u3008U i\u22a5, U l\u22a5\u3009 2 U2lk =\nn \u2211\nl=1\n\u3008U i, U l\u30092\u2016U l\u20162 \u2264 (\u00b50r n )2.\nUsing this we can finish the bound as follows:\nCn\u221a d\nr \u2211\nk=1\n\u2016u\u0302.Uk\u2016\u2016Xk.v\u0302\u2016 \u03b61 \u2264 Cn\u221a\nd\n\u221a\n\u00b50r\nn\nc21\u00b50r\nn \u2016U i\u2016\u2016V j\u2016\n\u03b62 \u2264 Cn\u221a\nd\n\u00b50r\nn\nc1\u00b50r\nn =\nC\u00b520r 2c1\nn \u221a d ,\nwhere \u03b61 follows from hypothesis of the lemma and \u03b62 from the incoherence property A1. Putting the two bounds together we get\n( UUT (UXT \u2212 n d P\u2126(UX T ))\u2212 (I \u2212 UUT )(n d P\u2126(UX T ))V V T ) ij \u2264 c1\u00b50r n (\u03b4d + C\u00b50r\u221a d ).\nSimilarly,\n( (Y V T \u2212 n d P\u2126(Y V T ))V V T \u2212 UUT n d P\u2126(Y V T )(I \u2212 V V )T ) ij \u2264 c2\u00b50r n (\u03b4d + C\u00b50r\u221a d ).\nHence each element of Z is bounded by,\nZ\u0303ij \u2264 (c1 + c2)\u00b50r\nn (\u03b4d + C\u00b50r\u221a d ).\nNow,\n(UX\u0303T )ij = (UU T Z\u0303)ij = (UU T (Z \u2212 n d PTP\u2126(Z)))ij = (UUT (UXT \u2212 n d P\u2126(UX T )\u2212 n d P\u2126(Y V T )))ij .\nNote that (UUT n d P\u2126(Y V T ))ij = (UU T n d P\u2126(Y V T )V V T + UUT n d P\u2126(Y V T )(I \u2212 V V T ))ij . Hence,\n\u2016X\u0303j\u20162 = n \u2211\ni=1\n(UX\u0303T )2ij\n=\nn \u2211\ni=1\n(\nU i T\n(\nI \u2212 n d\nn \u2211\nk=1\nUkUk T\nGkj\n)\nXj \u2212 U iTUT n d P\u2126\n(\nr \u2211\nk=1\nYkV T k\n)\nV V j \u2212 U iTUT n d P\u2126\n(\nr \u2211\nk=1\nYkV T k\n)\nV\u22a5V j \u22a5\n)2\n\u03b61 =\n\u2225 \u2225 \u2225 \u2225 \u2225 ( I \u2212 n d n \u2211\nk=1\nUkUk T\nGkj\n)\nXj \u2212 UT n d P\u2126\n(\nr \u2211\nk=1\nYkV T k\n)\nV V j \u2212 UT n d P\u2126\n(\nr \u2211\nk=1\nYkV T k\n)\nV\u22a5V j \u22a5\n\u2225 \u2225 \u2225 \u2225 \u2225 2\n\u2264 (\u2225 \u2225 \u2225\n\u2225 \u2225\n(\nI \u2212 n d\nn \u2211\nk=1\nUkUk T\nGkj\n)\nXj\n\u2225 \u2225 \u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 \u2225 \u2225 UT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV V j\n\u2225 \u2225 \u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 \u2225 \u2225 UT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV\u22a5V j \u22a5\n\u2225 \u2225 \u2225 \u2225 \u2225 )2 ,\nwhere, \u03b61 follows by the following:\nn \u2211\ni=1\n(U i T x)2 = n \u2211\ni=1\nxTU iU i T x = xT\n(\nn \u2211\ni=1\nU iU i T\n)\nx = xTUTUx = \u2016x\u20162.\nNext, we bound each of the above three terms individually. First term \u2225 \u2225\n\u2225\n(\nI \u2212 n d \u2211n k=1 U kUk T Gkj\n) Xj \u2225 \u2225\n\u2225\nis bounded by \u03b4d\n\u221a\nc2 1 \u00b50r\nn , which follows from the assumption A2 and the hypothesis of the lemma.\nNext, we consider the second and third terms.\n2)\n\u2225 \u2225 \u2225 \u2225 \u2225 UT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV V j\n\u2225 \u2225 \u2225 \u2225 \u2225 = max a:\u2016a\u2016\u22641 aTUT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV V j = max a:\u2016a\u2016\u22641\nn\nd\nr \u2211\nk=1\n(\n(Ua.Yk) TG(Vk.v\u0302)\n)\n\u03b61 \u2264 max\na:\u2016a\u2016\u22641 Cn\u221a d\nr \u2211\nk=1\n\u2016Ua.Yk\u2016\u2016Vk.v\u0302\u2016 \u2264 max a:\u2016a\u2016\u22641 Cn\u221a d\n\u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016Ua.Yk\u20162 \u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016Vk.v\u0302\u20162 \u03b62 \u2264 max\na:\u2016a\u2016\u22641 Cn\u221a d\n\u221a\nc22\u00b50r\nn \u2016a\u2016\u00b50r n\n=\n\u221a\nC2c22\u00b5 3 0r 3\nnd ,\nwhere v\u0302 = V V j and aTUTYk = 0. \u03b61 follows from the assumption G2 and \u03b62 from the assumption A1 and the hypothesis of the lemma.\n3)\n\u2225 \u2225 \u2225 \u2225 \u2225 UT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV\u22a5V j \u22a5\n\u2225 \u2225 \u2225 \u2225 \u2225 = max a:\u2016a\u2016\u22641 aTUT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV\u22a5V j \u22a5 = max\na:\u2016a\u2016\u22641\nn\nd\nr \u2211\nk=1\n(\n(Ua.Yk) TG(Vk.v\u0302)\n)\n\u03b61 \u2264 max\na:\u2016a\u2016\u22641 Cn\u221a d\nr \u2211\nk=1\n\u2016Ua.Yk\u2016\u2016Vk.v\u0302\u2016 \u2264 max a:\u2016a\u2016\u22641 Cn\u221a d\n\u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016Ua.Yk\u20162 \u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016Vk.v\u0302\u20162 \u03b62 \u2264 max\na:\u2016a\u2016\u22641 Cn\u221a d\n\u221a\nc22\u00b50r\nn \u2016a\u2016\u00b50r n\n=\n\u221a\nC2c22\u00b5 3 0r 3\nnd ,\nwhere v\u0302 = V\u22a5V j \u22a5 and a TUTYk = 0. \u03b61 follows from G2 and \u03b62 from A1 and the hypothesis of the lemma. Using all the three bounds we can finally bound \u2016X\u0303j\u20162.\n\u2016X\u0303j\u20162 \u2264 (\n\u03b4d\n\u221a\nc21\u00b50r\nn +\n\u221a\nC2c22\u00b5 3 0r 3\nnd +\n\u221a\nC2c22\u00b5 3 0r 3\nnd\n)2\n= \u00b50r\nn\n(\n\u03b4dc1 + 2c2 C\u00b50r\u221a\nd\n)2\n.\nNow, we bound the norm of rows of Y\u0303 .\n\u2016Y\u0303 i\u20162 = n \u2211\nj=1\n(Y\u0303 V T )2ij\n= n \u2211\nj=1\n(\nY i T\n(\nI \u2212 n d\nn \u2211\nk=1\nV kV k T\nGik\n)\nV j + U i T UT n\nd P\u2126\n(\nr \u2211\nk=1\nYkV T k\n)\nV V j \u2212 U iT\u22a5 UT\u22a5 (n\nd P\u2126(UX\nT ) ) V V j\n)2\n=\n\u2225 \u2225 \u2225 \u2225 \u2225 Y i T ( I \u2212 n d n \u2211\nk=1\nV kV k T\nGik\n)\n+ U i T UT n\nd P\u2126\n(\nr \u2211\nk=1\nYkV T k\n)\nV \u2212 U iT\u22a5 UT\u22a5 (n\nd P\u2126(UX\nT ) ) V\n\u2225 \u2225 \u2225 \u2225 \u2225 2\n\u2264 ( \u2225 \u2225 \u2225\n\u2225 \u2225\nY i T\n(\nI \u2212 n d\nn \u2211\nk=1\nV kV k T\nGik\n) \u2225\n\u2225 \u2225 \u2225 \u2225 +\n\u2225 \u2225 \u2225 \u2225 \u2225 U i T UT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV\n\u2225 \u2225 \u2225 \u2225 \u2225 + \u2225 \u2225 \u2225 U i T \u22a5 U T \u22a5 (n d P\u2126(UX T ) ) V \u2225 \u2225 \u2225 )2\nNext, we bound each of the above three terms individually. First term \u2225 \u2225 \u2225 Y i T (\nI \u2212 n d \u2211n k=1 V kV k T Gik\n)\u2225\n\u2225 \u2225\nis bounded by \u03b4d\n\u221a\nc2 2 \u00b50r\nn , which follows from A2 and the hypothesis of the lemma. Now, we bound\nthe second and third terms.\n2)\n\u2225 \u2225 \u2225 \u2225 \u2225 U i T UT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV\n\u2225 \u2225 \u2225 \u2225 \u2225 2 = max b:\u2016b\u2016\u22641 U i T UT n d P\u2126 ( r \u2211\nk=1\nYkV T k\n)\nV b = max b:\u2016b\u2016\u22641\nn\nd\nr \u2211\nk=1\n(\n(u\u0302.Yk) TG(Vk.V b)\n)\n\u03b61 \u2264 max\nb:\u2016b\u2016\u22641 Cn\u221a d\nr \u2211\nk=1\n\u2016u\u0302.Yk\u2016\u2016Vk.V b\u2016 \u2264 max b:\u2016b\u2016\u22641 Cn\u221a d\n\u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016u\u0302.Yk\u20162 \u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016Vk.V b\u20162 \u03b62 \u2264 max\nb:\u2016b\u2016\u22641 Cn\u221a d c2\u00b50r n\n\u221a\n\u00b50r\nn \u2016b\u2016\n=\n\u221a\nC2c22\u00b5 3 0r 3\nnd ,\nwhere u\u0302 = UU i and 1T (u\u0303.Yk) = 0. \u03b61 follows from G2 and \u03b62 from A1 and the hypothesis of the lemma.\n3) \u2225 \u2225 \u2225 U i T \u22a5 U T \u22a5 (n\nd P\u2126(UX\nT ) ) V \u2225 \u2225 \u2225 = max\nb:\u2016b\u2016\u22641 U i\nT \u22a5 U T \u22a5 (n\nd P\u2126(UX\nT ) )\nV b = max b:\u2016b\u2016\u22641\nn\nd\nr \u2211\nk=1\n(\n(u\u0302.Uk) TG(Xk.V b)\n)\n\u03b61 \u2264 max\nb:\u2016b\u2016\u22641 Cn\u221a d\nr \u2211\nk=1\n\u2016u\u0302.Uk\u2016\u2016Xk.V b\u2016 \u2264 max b:\u2016b\u2016\u22641 Cn\u221a d\n\u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016u\u0302.Uk\u20162 \u221a \u221a \u221a \u221a r \u2211\nk=1\n\u2016Xk.V b\u20162 \u03b62 \u2264 max\nb:\u2016b\u2016\u22641 Cn\u221a d \u00b50r n\n\u221a\nc21\u00b50r\nn \u2016b\u2016\n=\n\u221a\nC2c21\u00b5 3 0r 3\nnd ,\nwhere u\u0302 = U\u22a5U i\u22a5 and 1 T (u\u0303.Uk) = 0. \u03b61 follows from G2 and \u03b62 from A1 and the hypothesis of the lemma. Using all the three bounds we can finally bound \u2016Y\u0303 i\u20162.\n\u2016Y\u0303 i\u20162 \u2264 (\n\u03b4d\n\u221a\nc22\u00b50r\nn +\n\u221a\nC2c22\u00b5 3 0r 3\nnd +\n\u221a\nC2c21\u00b5 3 0r 3\nnd\n)2\n= \u00b50r\nn\n(\n\u03b4dc2 + (c1 + c2) C\u00b50r\u221a\nd\n)2\n.\nLemma (7.4). Let M,\u2126 satisfy A1, A2 and G1, G2, respectively. Then, M is the unique optimum of (7), if there exists a Y \u2208 Rn\u00d7n that satisfies the following:\n\u2022 P\u2126(Y ) = Y\n\u2022 ||PT (Y )\u2212 UV T ||F \u2264 \u221a d 8n\n\u2022 ||PT\u22a5(Y )|| < 12 Proof of Lemma 7.4. For any Z such that, P\u2126(Z) = 0, implies \u2016P\u2126PT\u22a5(Z)\u2016 = \u2016P\u2126PT (Z)\u2016. Also let \u03b4d =\nC\u00b50r\u221a d = \u03b1.\n\u2016P\u2126PT (Z)\u2016F = \u3008PT (Z), P\u2126PT (Z)\u3009 \u03b61 \u2265 d\nn (1\u2212\n\u221a\n2(\u03b42d + C2\u00b520r 2\nd ))||PT (Z)||2F =\nd n (1\u2212 2\u03b1)||PT (Z)||2F\n> d\n2n ||PT (Z)||2F ,\nfor \u03b1 < 14 . \u03b61 follows from Lemma 7.1. Also note that \u2016P\u2126PT\u22a5(Z)\u2016F \u2264 \u2016PT\u22a5(Z)\u2016F . Hence,\n\u2016PT\u22a5(Z)\u2016\u2217 \u2265 \u2016PT\u22a5(Z)\u2016F > \u221a d\n2n ||PT (Z)||F .\nNow choose U\u22a5 and V\u22a5 from the SVD of PT\u22a5(Z), which ensures that \u3008U\u22a5V T\u22a5 ,PT\u22a5(Z)\u3009 = \u2016PT\u22a5(Z)\u2016\u2217. Now,\n\u2016M + Z\u2016\u2217 \u03b61 \u2265\u3008UV T + U\u22a5V T\u22a5 ,M + Z\u3009 =\u2016M\u2016\u2217 + \u3008UV T + U\u22a5V T\u22a5 , Z\u3009 \u03b62 =\u2016M\u2016\u2217 + \u3008UV T + U\u22a5V T\u22a5 , Z\u3009 \u2212 \u3008Y,Z\u3009 =\u2016M\u2016\u2217 + \u3008UV T \u2212 PT (Y ),PT (Z)\u3009+ \u3008U\u22a5V T\u22a5 \u2212 PT\u22a5(Y ),PT\u22a5(Z)\u3009 \u03b63 \u2265\u2016M\u2016\u2217 \u2212 \u2016UV T \u2212 PT (Y )\u2016F \u2016PT (Z)\u2016F + \u2016PT\u22a5(Z)\u2016\u2217 \u2212 \u2016PT\u22a5(Y )\u2016\u2016PT\u22a5(Z)\u2016\u2217\n>\u2016M\u2016\u2217 \u2212 \u2016UV T \u2212 PT (Y )\u2016F \u2016PT (Z)\u2016F + (1\u2212 \u2016PT\u22a5(Y )\u2016) \u221a d\n2n ||PT (Z)||F\n\u03b64 >\u2016M\u2016\u2217.\n\u03b61 follows from the Holder\u2019s inequality and the fact that \u2016UV T + U\u22a5V T\u22a5 \u2016 = 1; \u03b62 from \u3008Y,Z\u3009 = \u3008P\u2126(Y ), Z\u3009 = 0; \u03b63 again from the Holder\u2019s inequality; and \u03b64 from the hypothesis of lemma."}], "references": [{"title": "Matrix completion with noise", "author": ["Candes", "Emmanuel J", "Plan", "Yaniv"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Cand\u00e8s", "Emmanuel J", "Recht", "Benjamin"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2009}, {"title": "Decoding by linear programming", "author": ["Candes", "Emmanuel J", "Tao", "Terence"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Candes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2005}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Cand\u00e8s", "Emmanuel J", "Tao", "Terence"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2010}, {"title": "Coherent matrix completion", "author": ["Chen", "Yudong", "Bhojanapalli", "Srinadh", "Sanghavi", "Sujay", "Ward", "Rachel"], "venue": "arXiv preprint arXiv:1306.2979,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Spectral techniques applied to sparse random graphs", "author": ["Feige", "Uriel", "Ofek", "Eran"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Feige et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Feige et al\\.", "year": 2005}, {"title": "A proof of alon\u2019s second eigenvalue conjecture", "author": ["Friedman", "Joel"], "venue": "In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Friedman and Joel.,? \\Q2003\\E", "shortCiteRegEx": "Friedman and Joel.", "year": 2003}, {"title": "One-bit compressed sensing: Provable support and vector recovery", "author": ["Gopi", "Sivakant", "Netrapalli", "Praneeth", "Jain", "Prateek", "Nori", "Aditya"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Gopi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gopi et al\\.", "year": 2013}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["Gross", "David"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Gross and David.,? \\Q2011\\E", "shortCiteRegEx": "Gross and David.", "year": 2011}, {"title": "tomography via compressed sensing", "author": ["Heiman", "Eyal", "Schechtman", "Gideon", "Shraibman", "Adi"], "venue": "Physical review letters,", "citeRegEx": "Heiman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heiman et al\\.", "year": 2010}, {"title": "Expander graphs and their applications", "author": ["Hoory", "Shlomo", "Linial", "Nathan", "Wigderson", "Avi"], "venue": null, "citeRegEx": "Hoory et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoory et al\\.", "year": 2013}, {"title": "Low-rank matrix completion", "author": ["Prateek", "Netrapalli", "Praneeth", "Sanghavi", "Sujay"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Prateek et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Prateek et al\\.", "year": 2006}, {"title": "Matrix completion from a few", "author": ["Keshavan", "Raghunandan H", "Montanari", "Andrea", "Oh", "Sewoong"], "venue": null, "citeRegEx": "Keshavan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2012}, {"title": "A combinatorial algebraic approach for the identifiability", "author": ["Kir\u00e1ly", "Franz J", "Tomioka", "Ryota"], "venue": null, "citeRegEx": "Kir\u00e1ly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kir\u00e1ly et al\\.", "year": 2010}, {"title": "low-rank matrix completion", "author": ["Troy", "Shraibman", "Adi"], "venue": null, "citeRegEx": "Troy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Troy et al\\.", "year": 2012}, {"title": "The augmented lagrange multiplier method for exact", "author": ["Zhouchen", "Chen", "Minming", "Ma", "Yi"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Zhouchen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhouchen et al\\.", "year": 2013}, {"title": "Interlacing families i: Bipartite", "author": ["1988. Marcus", "Adam", "Spielman", "Daniel A", "Srivastava", "Nikhil"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1988}, {"title": "Matrix completion from power-law distributed", "author": ["Meka", "Raghu", "Jain", "Prateek", "Dhillon", "Inderjit S"], "venue": "informatsii,", "citeRegEx": "Meka et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Meka et al\\.", "year": 1988}, {"title": "Existence and explicit constructions of q+ 1 regular ramanujan graphs for", "author": ["Morgenstern", "Moshe"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Morgenstern and Moshe.,? \\Q2009\\E", "shortCiteRegEx": "Morgenstern and Moshe.", "year": 2009}, {"title": "Graph spectra and the detectability of community", "author": ["Nadakuditi", "Raj Rao", "Newman", "Mark EJ"], "venue": "Journal of Combinatorial Theory Series B,", "citeRegEx": "Nadakuditi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Nadakuditi et al\\.", "year": 1994}, {"title": "NCDC. National climatic data center", "author": ["Recht", "Benjamin"], "venue": "Physical review letters,", "citeRegEx": "Recht and Benjamin.,? \\Q2012\\E", "shortCiteRegEx": "Recht and Benjamin.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Another line of work has been to devise sampling schemes dependent on the data matrix (Chen et al., 2013), (Kir\u00e1ly & Tomioka, 2012).", "startOffset": 86, "endOffset": 105}, {"referenceID": 7, "context": "While, universality results are well known for several other sensing problems, such as sparse vector recovery (Candes & Tao, 2005), one-bit compressive sensing (Gopi et al., 2013), similar results for low-rank matrix sensing are mostly restricted to RIP-type operators (Recht et al.", "startOffset": 160, "endOffset": 179}], "year": 2014, "abstractText": "The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem. However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled afresh. In this work, we address these issues by providing a universal recovery guarantee for matrix completion that works for a variety of sampling schemes. In particular, we show that if the set of sampled indices come from the edges of a bipartite graph with large spectral gap (i.e. gap between the first and the second singular value), then the nuclear norm minimization based method exactly recovers all low-rank matrices that satisfy certain incoherence properties. Moreover, we also show that under certain stricter incoherence conditions, O(nr) uniformly sampled entries are enough to recover any rank-r n\u00d7n matrix, in contrast to the O(nr logn) sample complexity required by other matrix completion algorithms as well as existing analyses of the nuclear norm method.", "creator": "LaTeX with hyperref package"}}}