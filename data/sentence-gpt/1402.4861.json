{"id": "1402.4861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2014", "title": "A Quasi-Newton Method for Large Scale Support Vector Machines", "abstract": "This paper adapts a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the solution of support vector machine classification problems. The proposed method is shown to converge almost surely to the optimal classifier at a rate that is linear in expectation. Numerical results show that the proposed method exhibits a convergence rate that degrades smoothly with the dimensionality of the feature vectors. We are therefore able to demonstrate that the model is able to reduce the probability of the prediction that an important feature is correct for the prediction of a given feature, but that this is not an appropriate way to assess the value of the particular feature vector in this instance.\n\n\nThe method provides a simple function to resolve the problem of classification problems in the standard, non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear way that is a popular way to resolve the problem of classification problems in the standard and non-linear manner that is a popular way to resolve the problem of classification problems in the standard and non-linear way that is a popular way to resolve the problem of classification problems in the standard and non-linear", "histories": [["v1", "Thu, 20 Feb 2014 01:44:33 GMT  (90kb,D)", "http://arxiv.org/abs/1402.4861v1", "5 pages, To appear in International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2014"]], "COMMENTS": "5 pages, To appear in International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aryan mokhtari", "alejandro ribeiro"], "accepted": false, "id": "1402.4861"}, "pdf": {"name": "1402.4861.pdf", "metadata": {"source": "CRF", "title": "A QUASI-NEWTON METHOD FOR LARGE SCALE SUPPORT VECTOR MACHINES", "authors": ["Aryan Mokhtari", "Alejandro Ribeiro"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "Given a training set with points whose class is known the goal of a support vector machine (SVM) is to find a hyperplane that best separates the training set. If future samples are statistically identical to the training set this hyperplane provides the best classification accuracy. Computation of the separating hyperplane entails solution of a convex optimization problem that can be implemented without much difficulty in problems of moderate size [1]. Large scale problems in which the dimension of the points to be classified is large require a commensurably large training set. In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].\nHowever practical, stochastic gradient descent methods need a large number of iterations to converge. This translates into the need of very large training sets, or, since the size of the training set is in general limited by data collection, in the computation of hyperplanes that are not as good classifiers as they could be given the available data. In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set. In particular, we adapt a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) method [9] for the solution of SVM classification problems (Section 2). The proposed method is shown to converge almost surely over realizations of the training set to the optimal classifier (Theorem 1) at a rate that is linear in expectation (Theorem 2). Numerical results show that the method exhibits a convergence time that degrades smoothly with the dimensionality of the feature vectors. (Section 4)."}, {"heading": "2. STOCHASTIC QUASI-NEWTON METHOD", "text": "Consider a training set S = {(xi, yi)}Ni=1 containing N pairs of the form (xi, yi), where xi \u2208 Rn is a feature vector and yi \u2208 {\u22121, 1} the corresponding vector\u2019s class. We want to find a hyperplane supported by a vector w \u2208 Rn which separates the training set so that wTxi > 0 for all points with yi = 1 and wTxi < 0 for all points with yi = \u22121. Since this vector may not exist if the data is not perfectly separable we introduce the loss function l((x, y);w) measuring the distance between the point xi and the hyperplane supported by w and proceed to select the hyperplane supporting vector as the one with minimum aggregate loss\nw\u2217 := argmin \u03bb 2 \u2016w\u20162 + 1 N N\u2211 i=1 l((xi, yi);w), (1)\nwhere we also added the regularization term \u03bb\u2016w\u20162/2 for some constant \u03bb > 0. The vector w\u2217 in (1) balances the minimization of the sum of\nSupported by NSF CAREER CCF-0952867 and ONR N00014-12-1-0997.\ndistances to the separating hyperplane, as measured by the loss function l((x, y);w), with the minimization of the L2 norm \u2016w\u20162 to enforce desirable properties in w\u2217 [13]. Common selections for the loss function are the squared hinge loss l((x, y);w) = max(0, 1\u2212 y(wTx))2 and the log loss l((x, y);v) = log(1 + exp(\u2212y(wTx))), e.g. [1].\nTo model (1) as a stochastic optimization problem let \u03b8i := (xi, yi) be a given training point and consider a uniform probability distribution on the training set S = {(xi, yi)}Ni=1 = {\u03b8i}Ni=1. Upon defining the function f(w,\u03b8) := \u03bb\u2016w\u20162/2 + l((xi, yi);w) we can rewrite (1) as\nw\u2217 := argmin w E\u03b8[f(w,\u03b8)] := argmin w F (w). (2)\nIn (2), we (re-)interpret the sum in (1) as an expectation over the uniform discrete distribution on the set S. We refer to f(w,\u03b8) as the instantaneous functions and to F (w) := E\u03b8[f(w,\u03b8)] as the average function.\nSince the loss functions l((xi, yi);w) are convex, the functions f(w,\u03b8) := \u03bb\u2016w\u20162/2 + l((xi, yi);w) are strongly convex. Thus, the average objective F (w) in (2) is also strongly convex and the optimal separating hyperplane w\u2217 can be found by stochastic gradient descent algorithms. However, the number of iterations required to run these algorithms, which translates to the number of training features (xi, yi) that need to be acquired, becomes prohibitive for large dimensional problems. To reduce the number of iterations required for convergence we develop a regularized stochastic version of the BFGS method.\nTo be precise let t \u2265 0 be an iteration index and assume that at time t we are given a sample of L realizations of the random variables \u03b8. Group these samples in the vector \u03b8\u0303t := [\u03b8t1; ...;\u03b8tL] and let wt denote the current hyperplane normal vector iterate. We then define the stochastic gradient of F (w) associated with samples \u03b8\u0303t at point wt as\ns\u0302(wt, \u03b8\u0303t) = 1\nL L\u2211 l=1 \u2207f(wt,\u03b8tl). (3)\nFurther introduce a step size sequence t, a positive definite curvature approximation matrix B\u0302t, and a regularization constant \u0393 > 0. The regularized stochastic BFGS algorithm is then defined by the iteration\nwt+1 = wt \u2212 t ( B\u0302\u22121t + \u0393I ) s\u0302(wt, \u03b8\u0303t). (4)\nThe update in (4) proceeds along the negative stochastic gradient direction \u2212s\u0302(wt, \u03b8\u0303t) premultiplied by the positive definite matrix B\u0302\u22121t + \u0393I and modulated by the step size t.\nFor the algorithm in (4) to have better convergence properties than gradient descent we need the matrix B\u0302t to approximate the Hessian of the objective function H(wt) := \u22072F (wt) so that (4) approximates an stochastic version of Newton\u2019s method \u2013 the role of \u0393I is to provide a guarantee of minimum progress as we discuss in the convergence analysis in Section 3. To define such approximation we use a stochastic version of the secant condition used in deterministic BFGS. Start by defining the variable and stochastic gradient variations at time t as\nvt := wt+1 \u2212wt, r\u0302t := s\u0302(wt+1, \u03b8\u0303t)\u2212 s\u0302(wt, \u03b8\u0303t), (5)\nrespectively, and select the matrix B\u0302t+1 to be used in the next time step so that it satisfies the secant condition B\u0302t+1vt = r\u0302t. The rationale for this selection is that the Hessian H(wt) satisfies this condition for wt+1 tending to wt. Notice however that the secant condition B\u0302t+1vt = r\u0302t\nar X\niv :1\n40 2.\n48 61\nv1 [\ncs .L\nG ]\n2 0\nFe b\n20 14\nis not enough to completely specify B\u0302t+1. To resolve this indeterminacy, matrices B\u0302t+1 in BFGS are also required to be as close as possible to B\u0302t in terms of minimizing the Gaussian differential entropy,\nB\u0302t+1 = argmin Z\ntr [ B\u0302\u22121t Z ] \u2212 log det [ B\u0302\u22121t Z ] \u2212 n,\ns. t. Zvt = r\u0302t, Z 0. (6)\nThe constraint Z 0 restricts the feasible space to positive semidefinite matrices whereas the constraint Zvt = r\u0302t requires Z to satisfy the secant condition. The objective tr(B\u0302\u22121t Z)\u2212 log det(B\u0302\u22121t Z)\u2212n is the differential entropy between Gaussian variables with covariances B\u0302t and Z.\nObserve that B\u0302t+1 stays positive definite as long as the matrix B\u0302t 0 is positive definite, e.g. [10]. However, it is possible for the smallest eigenvalue of B\u0302t to become arbitrarily close to zero which means that the largest eigenvalue of B\u0302\u22121t becomes very large. To avoid this problem we introduce a regularization of (6) that requires the smallest eigenvalue of B\u0302t+1 to be larger than a positive constant \u03b4,\nB\u0302t+1 = argmin Z\ntr [ B\u0302\u22121t (Z\u2212 \u03b4I) ] \u2212 log det [ B\u0302\u22121t (Z\u2212 \u03b4I) ] \u2212 n,\ns. t. Zvt = r\u0302t, Z 0. (7)\nSince the logarithm determinant log det[B\u0302\u22121t (Z \u2212 \u03b4I)] diverges as the smallest eigenvalue of Z approaches \u03b4, the smallest eigenvalue of the Hessian approximation matrices B\u0302t+1 computed as solutions of (7) exceeds the lower bound \u03b4. Thus, the largest eigenvalue of B\u0302\u22121t+1 is bounded above by 1/\u03b4. The following lemma shows that solutions of (7) can be computed by a simple algebraic formula (see [14] for proofs of results in this paper).\nLemma 1 Consider the semidefinite program in (7) where the matrix B\u0302t 0 is positive definite and define the corrected gradient variation\nr\u0303t := r\u0302t \u2212 \u03b4vt, (8)\nIf r\u0303Tt vt = (r\u0302t\u2212 \u03b4vt)Tvt > 0, the solution B\u0302t+1 of (7) can be written as\nB\u0302t+1 = B\u0302t + r\u0303tr\u0303\nT t\nvTt r\u0303t \u2212 B\u0302tvtv\nT t B\u0302t\nvTt B\u0302tvt + \u03b4I. (9)\nWhen \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15]. Therefore, the differences between BFGS and regularized BFGS are the replacement of the gradient variation r\u0302t by the corrected variation r\u0303t := r\u0302t\u2212 \u03b4vt and the addition of the regularization term \u03b4I. Notice that the expression in (9) is the solution to (7) only when the inner product r\u0303Tt vt = (r\u0302t \u2212 \u03b4vt)Tvt > 0."}, {"heading": "2.1. Regularized stochastic BFGS support vector machines", "text": "To solve the SVM problem in (1) using regularized stochastic BFGS we need the stochastic gradient in (3). For that, select a sample of L feature vectors x\u0303 = [x1; ...;xL] and corresponding classes y\u0303 = [y1; ...; yL] from the training set and compute the stochastic gradient as [cf. (3)]\ns\u0302(w, (x\u0303, y\u0303)) = \u03bbw + 1\nL L\u2211 i=1 \u2207w l((xi, yi);w). (10)\nStart at time t with current iterate wt and recall that B\u0302t stands for the Hessian approximation computed by stochastic BFGS in the previous iteration. Proceed to collect feature vectors x\u0303t = [xt1; ...;xtL] and their corresponding class vectors y\u0303t = [yt1; ...; ytL] and for each pair (x\u0303t, y\u0303t) determine the stochastic gradients s\u0302(wt, (x\u0303t, y\u0303t)) as per (10). Descend then along the direction (B\u0302\u22121t + \u0393I) s\u0302(wt, (x\u0303t, y\u0303t)) as per (4). This leads to the next iterate wt+1, but to complete the iteration we still need to compute the updated Hessian approximation B\u0302t+1. To do so compute the stochastic gradient s\u0302(wt+1, (x\u0303t, y\u0303t)) associated with the same set of\nAlgorithm 1 Regularized stochastic BFGS support vector machines Require: Variable w0. Hessian approximation B\u03020 \u03b4I. 1: for t = 0, 1, 2, . . . do 2: Collect L training points x\u0303t = [xt1, . . . ,xtL] and y\u0303t = [yt1, . . . , ytL] 3: Compute stochastic gradient s\u0302(wt, (x\u0303t, y\u0303t)) [cf. (10)].\ns\u0302(wt, (x\u0303t, y\u0303t)) = \u03bbwt + 1\nL L\u2211 i=1 \u2207wl((xti, yti);wt).\n4: Descend along direction (B\u0302\u22121t + \u0393I) s\u0302(wt, (x\u0303t, y\u0303t)) [cf. (4)]\nwt+1 = wt \u2212 t (B\u0302\u22121t + \u0393I) s\u0302(wt, (x\u0303t, y\u0303t)). 5: Compute s\u0302(wt+1, (x\u0303t, y\u0303t)) [cf. (10)]\ns\u0302(wt+1, (x\u0303t, y\u0303t)) = \u03bbwt+1 + 1\nL L\u2211 i=1 \u2207w l((xti, yti);wt+1).\n6: Variable and modified stochastic gradient variations [cf. (5) and (8)]\nvt = wt+1 \u2212wt, r\u0303t = s\u0302(wt+1, (x\u0303t, y\u0303t))\u2212 s\u0302(wt, (x\u0303t, y\u0303t))\u2212 \u03b4vt\n7: Update Hessian approximation matrix [cf. (9)]\nB\u0302t+1 = B\u0302t + r\u0303tr\u0303Tt vTt r\u0303t \u2212 B\u0302tvtvTt B\u0302t vTt B\u0302tvt + \u03b4I.\n8: end for\nrandom data points samples (x\u0303t, y\u0303t) used to compute the stochastic gradient s\u0302(wt, (x\u0303t, y\u0303t)). The stochastic gradient variation r\u0302t, the variable variation vt, and the modified stochastic gradient variation r\u0303t at time t are now computed using (5) and (8). The Hessian approximation B\u0302t+1 for the next iteration is defined as the matrix that satisfies the stochastic secant condition B\u0302t+1vt = r\u0302t and is closest to B\u0302t in the sense of (7). As per Lemma 1 we can compute B\u0302t+1 using (9).\nThe solution of (1) using regularized stochastic BFGS is summarized in Algorithm 1. The two core steps in each iteration are the descent in Step 4 and the update of the Hessian approximation B\u0302t in Step 8. Step 2 comprises the observation of L pairs of data points and feature vectors that are required to compute the stochastic gradients in steps 3 and 5. The stochastic gradient s\u0302(wt, (x\u0303t, y\u0303t)) in Step 3 is used in the descent iteration in Step 4. The stochastic gradient of Step 3 along with the stochastic gradient s\u0302(wt+1, (x\u0303t, y\u0303t)) of Step 5 are used to compute the variations in steps 6 and 7 that permit carrying out the update of the Hessian approximation B\u0302t in Step 8. Iterations are initialized with arbitrary vector w0 and matrix B\u03020 having all eigenvalues larger than \u03b4.\n3. CONVERGENCE ANALYSIS\nOur goal here is to show that as time progresses the sequence of classifiers wt approaches the optimal classifier w\u2217. In proving this result we make the following assumptions.\nAssumption 1 For any set of samples \u03b8\u0303 = [\u03b81, . . . ,\u03b8L] the instantaneous functions f\u0302(w, \u03b8\u0303) := (1/L) \u2211L l=1 f(w,\u03b8l) are twice differentiable and their Hessians H\u0302(w, \u03b8\u0303) = \u22072wf\u0302(w, \u03b8\u0303) have lower and upper bounded eigenvalues,\nm\u0303I H\u0302(w, \u03b8\u0303) M\u0303I. (11)\nAssumption 2 There exists a constant S2 such that for all variables w the second moment of the norm of the stochastic gradient satisfies\nE\u03b8 [ \u2016s\u0302(wt, \u03b8\u0303t)\u20162 ] \u2264 S2, (12)\nAssumption 3 The regularization constant \u03b4 is smaller than the smallest Hessian eigenvalue m\u0303, i.e., \u03b4 < m\u0303.\nRecall that according to Lemma 1 the update in (9) is a solution to (7) as long as the inner product (r\u0302t \u2212 \u03b4vt)Tvt = r\u0303Tt vt > 0 is positive. Our first result is to show that selecting \u03b4 < m\u0303 as required by Assumption 3 guarantees that this inequality is satisfied for all times t.\nLemma 2 Consider the modified stochastic gradient variation r\u0303t defined in (8) and the variable variation vt defined in (5). If assumptions 1 and 3 are true, then, for all times t it holds\nr\u0303Tt vt = (r\u0302t \u2212 \u03b4vt)Tvt \u2265 (m\u0303\u2212 \u03b4)\u2016vt\u20162 > 0. (13)\nThe result in Lemma 2 guarantees that the regularized stochastic BFGS algorithm as defined by recursive application of (4), (5), (8), and (9) results in matrices B\u0302t that solve (7). In particular, this implies that B\u0302t is positive definite with smallest eigenvalue not smaller than \u03b4, i.e., B\u0302t \u03b4I. This implies that all the eigenvalues of B\u0302\u22121t are between 0 and 1/\u03b4 and that, as a consequence, the matrix B\u0302\u22121t + \u0393I is such that\n\u0393I B\u0302\u22121t + \u0393I (\u0393 + 1\n\u03b4 )I. (14)\nHaving matrices B\u0302\u22121t + \u0393I that are strictly positive definite with eigenvalues uniformly upper bounded by \u0393 + (1/\u03b4) leads to the conclusion that if s\u0302(wt, \u03b8\u0303t) is a descent direction, the same holds true of (B\u0302\u22121t + \u0393I) s\u0302(wt, \u03b8\u0303t). The stochastic gradient s\u0302(wt, \u03b8\u0303t) is not a descent direction in general, but we know that this is true for its conditional expectation E[s\u0302(wt, \u03b8\u0303t)\n\u2223\u2223wt] = \u2207wF (wt). Therefore, we conclude that (B\u0302\u22121t + \u0393I)s\u0302(wt, \u03b8\u0303t) is an average descent direction because E[(B\u0302 \u22121 t +\n\u0393I) s\u0302(wt, \u03b8\u0303t) \u2223\u2223wt] = (B\u0302\u22121t + \u0393I)\u2207wF (wt). Having a displacement wt+1 \u2212 wt that is a descent direction on average implies convergence towards optimal arguments as we claim in the following theorem.\nTheorem 1 Consider the regularized stochastic BFGS algorithm as defined by (4), (5), (8), and (9). If assumptions 1-3 hold true and the sequence of stepsizes satisfies is nonsummable but square summable, i.e., if \u2211\u221e t=0 t = \u221e, and \u2211\u221e t=0 2 t < \u221e, the limit infimum of the squared Euclidean distance to optimality \u2016wt \u2212w\u2217\u20162 satisfies\nlim inf t\u2192\u221e\n\u2016wt \u2212w\u2217\u20162 = 0 a.s. (15)\nover realizations of the random samples {\u03b8\u0303t}\u221et=1.\nTheorem 1 establishes convergence of the stochastic regularized BFGS algorithm summarized in Algorithm 1. In the proof of this result the lower bound in the eigenvalues of B\u0302t enforced by the regularization in (9) plays a fundamental role. Roughly speaking, the lower bound in\nthe eigenvalues of B\u0302t results in an upper bound on the eigenvalues of B\u0302\u22121t which limits the effect of random variations on the stochastic gradient s\u0302(wt, \u03b8\u0303t). If this regularization is not implemented, i.e., if we keep \u03b4 = 0, we may observe catastrophic amplification of random variations of the stochastic gradient. This effect is indeed observed in the numerical experiments in Section 4. The addition of the identity matrix bias \u0393I in (4) is also instrumental in the proof of Theorem 1. This bias limits the effects of randomness in the curvature estimate B\u0302t. If random variations in the curvature estimate B\u0302t result in a matrix B\u0302\u22121t with small eigenvalues the term \u0393I dominates and (4) reduces to stochastic gradient descent. This ensures continued progress towards the optimal argument w\u2217.\nThe convergence claim in Theorem 1is complemented by a expected convergence rate result which we state in the following theorem.\nTheorem 2 Consider the regularized stochastic BFGS algorithm as defined by (4)-(9) and let the sequence of stepsizes be given by t = 0\u03c4/(\u03c4 + t) with the parameter 0 sufficiently small and the parameter \u03c4 sufficiently large so as to satisfy the inequality\n2 0\u03c4\u0393 > 1 . (16)\nIf assumptions 1 and 2 hold true the difference between the expected objective value E [F (wt)] at time t and the optimal objective F (w\u2217) satisfies\nE [F (wt)]\u2212 F (w\u2217) \u2264 \u03be\n\u03c4 + t , (17)\nwhere the constant \u03be satisfies\n\u03be = max\n{ 20 \u03c4 2K\n2 0\u03c4\u0393\u2212 1 , (1 + \u03c4)(F (w0)\u2212 F (w\u2217))\n} . (18)\nTheorem 2 shows the convergence rate of regularized stochastic BFGS is at least linear in terms of the expectation of the objective function. This rate is typical of stochastic optimization algorithms and, in that sense, no better than stochastic gradient descent. While the convergence rate doesn\u2019t change, improvements in convergence time are marked as we illustrate with the numerical experiments of the following section.\n4. NUMERICAL ANALYSIS\nWe test Algorithm 1 when using the squared hinge loss l((x, y);w) = max(0, 1\u2212y(xTw))2 in (1). The training set S = {(xi, yi)}Ni=1 contains N = 104 feature vectors half of which belong to the class yi = \u22121 with the other half belonging to the class yi = 1. For the class yi = \u22121 each of the n components of each of the feature vectors xi \u2208 Rn is chosen uniformly at random from the interval [\u22120.8, 0.2]. Likewise, each of the n components of each of the feature vectors xi \u2208 Rn is chosen uniformly\nat random from the interval [\u22120.2, 0.8] for the class yi = 1. The overlap in the range of the feature vectors is such that the classification accuracy expected from a clairvoyant classifier that knows the statistic model of the data set is less than 100%. Exact values can be computed from the Irwin-Hall distribution [16]. For n = 4 this amounts to 98%.\nWe set the parameter \u03bb in (1) to \u03bb = 10\u22123. Since the Hessian eigenvalues of f(w,\u03b8) := \u03bb\u2016w\u20162/2 + l((xi, yi);w) are, at least, equal to \u03bb this implies that the eigenvalue lower bound m\u0303 is such that m\u0303 \u2265 \u03bb = 10\u22123. Thus, we set the BFGS regularization parameter to \u03b4 = \u03bb = 10\u22123. Further set the minimum progress parameter in (3) to \u0393 = 10\u22124 and the sample size for computation of stochastic gradients to L = 5. Stepsizes are of the form t = 0\u03c4/(\u03c4 + t) with 0 = 3 \u00d7 10\u22122 and \u03c4 = 102. We compare the behavior of stochastic gradient descent and stochastic BFGS for a small dimensional problem with n = 4 and a large problem with n = 40. For stochastic gradient descent the sample size in (3) is L = 1 and we use the same stepsize sequence used for stochastic BFGS.\nAn illustration of the relative performances of stochastic gradient descent and BFGS for n= 4 is presented in Fig. 1. The value of the objective function F (wt) is represented with respect to the number of feature vectors processed, which is given by the product Lt between the iteration index and the sample size used to compute stochastic gradients. This is done because the sample sizes in stochastic BFGS (L = 5) and stochastic gradient descent (L = 1) are different. The curvature correction of stochastic BFGS results in significant reductions in convergence time. E.g., Stochastic BFGS achieves an objective value of F (wt) = 6.5\u00d7 10\u22122 upon processing of Lt = 315 feature vectors. To achieve the same objective value F (wt) = 6.5\u00d710\u22122 stochastic gradient descent processes 1.74\u00d7103 feature vectors. Conversely, after processing Lt = 2.5\u00d7 103 feature vectors the objective values achieved by stochastic BFGS and gradient descent are F (wt) = 4.14\u00d7 10\u22122 and F (wt) = 6.31\u00d7 10\u22122, respectively.\nThe performance difference between the two methods is larger for feature vectors of larger dimension n. The plot of the value of the objective function F (wt) with respect to the number of feature vectors processed Lt is shown in Fig. 2 for n = 40. The convergence time of stochastic BFGS increases but is still acceptable. For stochastic gradient descent the algorithm becomes unworkable. After processing 3.5\u00d7103 stochastic BFGS reduces the objective value to F (wt) = 5.55\u00d710\u22124 while stochastic gradient descent has barely made progress at F (wt) = 1.80\u00d7 10\u22122.\nDifferences in convergence times translate into differences in classification accuracy when we process all N vectors in the training set. This is shown for dimension n = 4 and training set sizeN = 2.5\u00d7103 in Fig. 3. To build Fig. 3 we process N = 2.5\u00d7 103 feature vectors with stochastic BFGS and stochastic gradient descent with the same parameters used in Fig. 1. We then use these vectors to classify 104 observations in the test set and record the percentage of samples that are correctly classified. The\nprocess is repeated 103 times to estimate the probability distribution of the correct classification percentage represented by the histograms shown. The dominance of stochastic BFGS with respect to stochastic gradient descent is almost uniform. The vector wt computed by stochastic gradient descent classifies correctly at most 65% of the of the feature vectors in the test set. The vector wt computed by stochastic BFGS exceeds this accuracy with probability 0.98. Perhaps more relevant, the classifier computed by stochastic BFGS achieves a mean classification accuracy of 82.2% which is not far from the clairvoyant classification accuracy of 98%. Although performance is markedly better in general, stochastic BFGS fails to compute a working classifier with probability 0.02.\nWe also investigate the difference between regularized and nonregularized versions of stochastic BFGS for feature vectors of dimension n = 10. Observe that non-regularized stochastic BFGS corresponds to making \u03b4 = 0 and \u0393 = 0 in Algorithm 1. To illustrate the advantage of the regularization induced by the proximity requirement in (7), as opposed to the non regularized proximity requirement in (6), we keep a constant stepsize t = 10\u22121. The corresponding evolutions of the objective function values F (wt) with respect to the number of feature vectors processed Lt are shown in Fig. 4 along with the values associated with stochastic gradient descent. As we reach convergence the likelihood of having small eigenvalues appearing in B\u0302t becomes significant. In regularized stochastic BFGS this results in recurrent jumps away from the optimal classifier w\u2217. However, the regularization term limits the size of the jumps and further permits the algorithm to consistently recover a reasonable curvature estimate. In Fig. 4 we process 104 feature vectors and observe many occurrences of small eigenvalues. However, the algorithm always recovers and heads back to a good approximation of w\u2217. In the absence of regularization small eigenvalues in B\u0302t result in larger jumps away from w\u2217. This not only sets back the algorithm by a much larger amount than in the regularized case but also results in a catastrophic deterioration of the curvature approximation matrix B\u0302t. In Fig. 4 we observe recovery after the first two occurrences of small eigenvalues but eventually there is a catastrophic deviation after which non-regularized stochastic BFSG behaves not better than stochastic gradient descent.\n5. CONCLUSIONS\nWe considered the problem of determining the separating hyperplane of a support vector machine using stochastic optimization. In order to handle large scale problems with reasonable convergence times we adapted a regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method [9]. We derived theoretical convergence guarantees that are customary of stochastic optimization and illustrated improvements in convergence time through numerical analysis."}, {"heading": "6. REFERENCES", "text": "[1] L. Bottou, \u201cLarge-scale machine learning with stochastic gradient descent,\u201d In Proceedings of COMPSTAT\u20192010, pp. 177\u2013186, Physica-Verlag HD, 2010.\n[2] S. Shalev-Shwartz, Y. Singer, and N. Srebro, \u201cPegasos: Primal estimated sub-gradient solver for svm,\u201d In Proceedings of the 24th international conference on Machine learning, pp. 807\u2013814, ACM, 2007.\n[3] T. Zhang, \u201cSolving large scale linear prediction problems using stochastic gradient descent algorithms,\u201d In Proceedings of the twenty-first international conference on Machine learning, p. 919926, ACM, 2004.\n[4] N. LeRoux, M. Schmidt, and F. Bach, \u201cA stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets,\u201d arXiv preprint arXiv, 1202.6258, 2012.\n[5] C. G. Broyden, J. E. D. Jr., Wang, and J. J. More, \u201cOn the local and superlinear convergence of quasi-newton methods,\u201d IMA J. Appl. Math, vol. 12, no. 3, pp. 223\u2013245, June 1973.\n[6] A. Bordes, L. Bottou, and P. Gallinari, \u201cSgd-qn: Careful quasinewton stochastic gradient descent,\u201d The Journal of Machine Learning Research, vol. 10, pp. 1737\u20131754, 2009.\n[7] R. H. Byrd, J. Nocedal, and Y. Yuan, \u201cGlobal convergence of a class of quasi-newton methods on convex problems,\u201d SIAM J. Numer. Anal., vol. 24, no. 5, pp. 1171\u20131190, October 1987.\n[8] A. Mokhtari and A. Ribeiro, \u201cA dual stochastic dfp algorithm for optimal resource allocation in wireless systems,\u201d in Proc. IEEE 14th Workshop on Signal Process. Advances in Wireless Commun. (SPAWC). pp. 21-25, Darmstadt Germany, June 16-19 2013.\n[9] \u2014\u2014, \u201cRegularized stochastic bfgs algorithm,\u201d in Proc. IEEE Global Conf. on Signal and Inform. Process. pp. 1109-1112, Austin Texas, Dec. 3-5 2013.\n[10] J. Nocedal and S. J. Wright, Numerical optimization, 2nd ed. New York, NY: Springer-Verlag, 1999.\n[11] M. J. D. Powell, Some global convergence properties of a variable metric algorithm for minimization without exact line search, 2nd ed. London, UK: Academic Press, 1971.\n[12] N. N. Schraudolph, J. Yu, and S. Gnter, \u201cA stochastic quasi-newton method for online convex optimization,\u201d In Proc. 11th Intl. Conf. on Artificial Intelligence and Statistics (AIstats), p. 433 440, Soc. for Artificial Intelligence and Statistics, 2007.\n[13] V. Vapnik, The nature of statistical learning theory, 2nd ed. springer, 1999.\n[14] A. Mokhtari and A. Ribeiro, \u201cRes: Regularized stochastic bfgs algorithm,\u201d arXiv preprint arXiv, 1401.7625, 2014.\n[15] J. J. E. Dennis and J. J. More, \u201cA characterization of super linear convergence and its application to quasi-newton methods,\u201d Mathematics of computation, vol. 28, no. 126, pp. 549\u2013560, 1974.\n[16] N. L. Johnson, S. Kotz, and N. Balakrishnan, Continuous Univariate Distributions, vol. 2, 2nd ed. Wiley-Interscience, 1995."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pp. 177\u2013186, Physica-Verlag HD, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "Proceedings of the 24th international conference on Machine learning, pp. 807\u2013814, ACM, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the twenty-first international conference on Machine learning, p. 919926, ACM, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets", "author": ["N. LeRoux", "M. Schmidt", "F. Bach"], "venue": "arXiv preprint arXiv, 1202.6258, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "On the local and superlinear convergence of quasi-newton methods", "author": ["C.G. Broyden", "J.E.D. Jr.", "Wang", "J.J. More"], "venue": "IMA J. Appl. Math, vol. 12, no. 3, pp. 223\u2013245, June 1973.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1973}, {"title": "Sgd-qn: Careful quasinewton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1737\u20131754, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Global convergence of a class of quasi-newton methods on convex problems", "author": ["R.H. Byrd", "J. Nocedal", "Y. Yuan"], "venue": "SIAM J. Numer. Anal., vol. 24, no. 5, pp. 1171\u20131190, October 1987.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1987}, {"title": "A dual stochastic dfp algorithm for optimal resource allocation in wireless systems", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "Proc. IEEE 14th Workshop on Signal Process. Advances in Wireless Commun. (SPAWC). pp. 21-25, Darmstadt Germany, June 16-19 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized stochastic bfgs algorithm", "author": ["\u2014\u2014"], "venue": "Proc. IEEE Global Conf. on Signal and Inform. Process. pp. 1109-1112, Austin Texas, Dec. 3-5 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Numerical optimization, 2nd ed", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Some global convergence properties of a variable metric algorithm for minimization without exact line search, 2nd ed", "author": ["M.J.D. Powell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1971}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N.N. Schraudolph", "J. Yu", "S. Gnter"], "venue": "Proc. 11th Intl. Conf. on Artificial Intelligence and Statistics (AIstats), p. 433 440, Soc. for Artificial Intelligence and Statistics, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "The nature of statistical learning theory, 2nd ed", "author": ["V. Vapnik"], "venue": "springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "arXiv preprint arXiv, 1401.7625, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A characterization of super linear convergence and its application to quasi-newton methods", "author": ["J.J.E. Dennis", "J.J. More"], "venue": "Mathematics of computation, vol. 28, no. 126, pp. 549\u2013560, 1974.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1974}], "referenceMentions": [{"referenceID": 0, "context": "Computation of the separating hyperplane entails solution of a convex optimization problem that can be implemented without much difficulty in problems of moderate size [1].", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 1, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 2, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 3, "context": "In these situations, computing the gradients that are required for numerical determination of the separating hyperplanes becomes infeasible and motivates the use of stochastic gradient descent methods which build unbiased gradient estimates based on small data subsamples [1\u20134].", "startOffset": 272, "endOffset": 277}, {"referenceID": 4, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 5, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 9, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 10, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 11, "context": "In this paper we resort to quasi-Newton methods [5\u201312] to make better use of the provided training set.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "In particular, we adapt a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) method [9] for the solution of SVM classification problems (Section 2).", "startOffset": 137, "endOffset": 140}, {"referenceID": 12, "context": "distances to the separating hyperplane, as measured by the loss function l((x, y);w), with the minimization of the L2 norm \u2016w\u20162 to enforce desirable properties in w\u2217 [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The following lemma shows that solutions of (7) can be computed by a simple algebraic formula (see [14] for proofs of results in this paper).", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 9, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 10, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 14, "context": "When \u03b4 = 0 the update in (9) coincides with standard non-regularized BFGS [7, 10, 11, 15].", "startOffset": 74, "endOffset": 89}, {"referenceID": 8, "context": "In order to handle large scale problems with reasonable convergence times we adapted a regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method [9].", "startOffset": 192, "endOffset": 195}], "year": 2014, "abstractText": "This paper adapts a recently developed regularized stochastic version of the Broyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the solution of support vector machine classification problems. The proposed method is shown to converge almost surely to the optimal classifier at a rate that is linear in expectation. Numerical results show that the proposed method exhibits a convergence rate that degrades smoothly with the dimensionality of the feature vectors.", "creator": "LaTeX with hyperref package"}}}