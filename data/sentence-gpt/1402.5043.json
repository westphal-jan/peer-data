{"id": "1402.5043", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2014", "title": "A logical model of Theory of Mind for virtual agents in the context of job interview simulation", "abstract": "Job interview simulation with a virtual agents aims at improving people's social skills and supporting professional inclusion. In such simulators, the virtual agent must be capable of representing and reasoning about the user's mental state based on social cues that inform the system about his/her affects and social attitude. In this paper, we propose a formal model of Theory of Mind (ToM) for virtual agent in the context of human-agent interaction that focuses on the affective dimension. It relies on a hybrid ToM that combines the two major paradigms of the domain. Our framework is based on modal logic and inference rules about the mental states, emotions and social relations of both actors. Finally, we present preliminary results regarding the impact of such a model on natural interaction in the context of job interviews simulation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 20 Feb 2014 15:40:08 GMT  (102kb,D)", "http://arxiv.org/abs/1402.5043v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marwen belkaid", "nicolas sabouret"], "accepted": false, "id": "1402.5043"}, "pdf": {"name": "1402.5043.pdf", "metadata": {"source": "CRF", "title": "A logical model of Theory of Mind for virtual agents in the context of job interview simulation", "authors": ["Marwen Belkaid", "Nicolas Sabouret"], "emails": ["marwen.belkaid@ensea.fr", "nicolas.sabouret@limsi.fr"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Intelligent agents\nGeneral Terms Theory, Experimentation\nKeywords Theory of Mind, Cognitive Models, Logic-Based Approaches, Human-Agent Interaction, Affective Computing, Serious Games for Inclusion"}, {"heading": "1. INTRODUCTION AND POSITIONING", "text": "During the last decade, several projects have proposed to use intelligent virtual agents in digital games for user empowerment [19, 22, 23, 2, 5]. The work presented in this paper considers the use of virtual agents in job interview simulation games for young unemployed peoples, a.k.a NEETs1. Current research reveals that NEETs often lack self-confidence\n1NEET is a government acronym for young people not in employment, education or training. According to Eurostat,\nand the essential social skills needed to seek and secure employment [9]. Training with a virtual agent can help them acquire self-confidence and improve their social skills. Indeed, it has already been proven that training at job interviews with a virtual agent could improve the performance [17].\nThe role of the virtual agent in such training games is to be able to react in a coherent manner: based on the non-verbal inputs (smiles, emotion expressions, body movements), the agent must select relevant verbal and non-verbal responses. In this context, several work illustrated the role of emotion regulation in the context of job interviews. For instance, in [29], a study shows that people who tried to suppress or hide negative emotions during a job interview are considered more competent by evaluators. Similarly, Tiedens [30] shows that anger and sadness play an important role in job interviews. For this reason, credible simulation of emotions appears as a key issue when it comes to using virtual agents in job interview simulations.\nMost existing models for virtual agents rely on a reactive approach, in which the system does not manipulate or reason on the mental states of the interlocutor [17, 22, 23]. However, in human psychology, Theory of Mind (ToM) refers to the ability of human beings and primates to interpret, predict and even influence others\u2019 behavior [4]. Such an ability is a key feature in the development of intelligent virtual agents in the context of tutoring and training systems. In this paper, we propose a new model of ToM for virtual agent in the context of job interview simulation.\nThe next section briefly discusses existing research that serves as a basis to our work. Sections 3 and 4 present the general architecture and the logical framework for our ToM. Section 5 describes our implementation of this model in the context of job interviews simulation. An outline of the preliminary evaluation we conducted is given in Section 6. Finally, results and perspectives are discussed in Section 6.3."}, {"heading": "2. RELATED WORK", "text": "In order to be able to reason on the affective dimension of the interaction, conveyed by the non-verbal behaviour of both\nin march 2012, 5.5 million of European youngster (16 to 25 years old) were unemployed meaning that 22.6% of the youngster global population in European union is unemployed. This unemployment percentage is 10 points superior to the whole population showing that the employment of NEETs is a real problem in Europe.\nar X\niv :1\n40 2.\n50 43\nv1 [\ncs .A\nI] 2\n0 Fe\nb 20\n14\ninterlocutors, several models rely on the cognitive structure of emotions and appraisal theories such as CPM [27] or OCC [21]. These theories provide domain-independant descriptions of triggering conditions of emotions, that are required for the development the affective aspect of the ToM reasoner. For instance, [1, 11] are BDI-based implementation of the OCC theory. FAtiMA\u2019s double appraisal model [3], although not implemented using a BDI framework, also encodes the OCC model. However, in these models, the inference mechanism itself encodes the chosen Appraisal Theory. On the contrary, in our model, we propose a theory-independant ToM reasoner. While our experiments were conducted using an OCC-based model, the corresponding rules (described in equation 20) could be easily replaced by another theory.\nTo support such adaptability, we propose to rely on the BDI model. Several computational models of emotions have already been proposed (e.g. [16, 11]) that show that BDI is a good basis to represent and to reason about the interlocutor\u2019s mental state. Our aim is thus to define a logical model of emotions and ToM in BDI.\nFrom the philosophical point of view, a debate about how ToM is processed by human adults opposes two theories. The theory-theory(TT) argues for a folk-psychology reasoning, i.e. a set of rules one acquires regarding human mind functioning. [8]. The simulation-theory (ST) [13] defends a mirroring or projection process allowing for taking someone else\u2019s perspective. Various research demonstrated that neither pure TT nor pure ST were realistic [31] and both theorists and simulationists turn toward more hybrid models [8][13].\nExisting computational ToM models either imply a choice between the TT and ST theories (e.g. [3] that relies on a ST approach, or [7] and [25] that position in the TT) or implement them separately as in [15]. In our work, we propose a hybrid approach that relies on theory-theory to model the agent\u2019s mental states and commonsense rules, but also on simulation-theory to others\u2019 perspective by projecting attributed mental states on its own inference engine. Both models are integrated in the same reasoner."}, {"heading": "3. ARCHITECTURE OVERVIEW", "text": "Our ToM reasoning architecture consists of two main components, as presented on Figure 1.\nThe agent\u2019s mental states contains beliefs, attitudes, goals and intentions. Beliefs represent knowledge about general facts, rules of the world (i.e. commonsense knowledge from the theory-theory) and mental states of self or other\u2019s (i.e. attributed mental states). Attitudes represent appreciations of the current state of affairs and, by extension, desires (what the agent wants to be true in the future) and ideals (what the agents would like to be always true). Goals and intentions form the deliberative aspect, limited to immediate actions.\nThe agent\u2019s inference engine contains three parts. The folk-psychology deliberative reasoner is responsible for intention generation (according to the agent\u2019s beliefs and attitudes) and updating mental state. The Commonsense reasoner\u2019s role it to enrich the agent\u2019s beliefs base using commonsense rules and facts. Finally, the emotional inference engine computes emotion based on the appraisal theory.\nOur hybrid ToM modeling relies on: 1) a TT approach based on folk-psychology and commonsense to reason about others, and 2) a ST approach consisting in projecting their attributed mental states on the agent\u2019s own inference engine. The following section details this logical model."}, {"heading": "4. LOGICAL FRAMEWORK", "text": "In the following, def = and def =\u21d2 respectively mean equals by definition and implies by definition. The former is used to define new operators as functions of others and the latter to express inference rules."}, {"heading": "4.1 Syntax", "text": "Assume finite sets of atomic propositions ATM , physical actions ACT , illocutionary (speech) acts ILL, agents AGT , emotions EMO (which is a subset of the twenty two OCC emotions in our model), and the intervals of real numbers DEG = [\u22121, 1] and DEG+ = [0, 1]. ATM describes facts or assertions (e.g. salary is bad, picnic is fun) or external events such as rain starts falling. ACT describes actions that the agents or humans (AGT ) may perform, e.g. introduce itself or have a picnic.\nOur model defines events as acts in which at least one of the actors of the interaction take part. Elements in EV T are tuples in AGT \u00d7 AGT \u00d7 (ACT \u222a ILL(ATM)) where the first element is the actor that performs the action, the second is a passive agent and the act can be either an actions (ACT ) or a speech act (ILL). This representation is similar to the one in [20] except we associate a subjective degree of plausibility as is usually done in BDI models and we do not distinguish actions from communcation. Illocutionnary speech acts have the form \u03c2(\u03d5) and mean \u201cactor utters \u03d5 to recipient through the illocutionary act \u03c2\u201d.\nThe language we define is the set of formulas described by the following BNF (Backus-Naur-Form):\nEvt : ::= \u3008a, (a|\u2205), \u03b1\u3009 | \u3008a, a, Spk(\u03c2, \u03d5)\u3009\nPrp : \u03c0 ::= p | |Likeka,b |Domka,b Fml : \u03d5 ::= \u03c0 |Bella(\u03d5) |Attka(\u03d5) | Inta(\u03d5) |Emoia,(b|\u2205)(\u03b5, \u03d5) |\nN(\u03d5) |U(\u03d5,\u03d5) | \u00ac\u03d5 |\u03d5 \u2227 \u03d5 (1)\nwhere a, b \u2208 AGT , \u03b1 \u2208 ACT , p \u2208 ATM , \u2208 EV T , \u03b5 \u2208 EMO, \u03c2 \u2208 ILL, l, i \u2208 DEG+, k \u2208 DEG. Like, Dom, Bel, Att and Int are modal operators and N , and U are temporal operators Next and Until from LTL and CTL* [24]. The other temporal operators F and G and boolean conditions >, \u22a5, \u2228 and \u21d2 are defined in the standard way. Moreover, in the events\u2019 representation, we use \u201c\u2212\u201d as the any operator.\nFor the representation of social relation [18], Likeka,b determines the level of liking agent a has for agent b, whileDomka,b represents the degree of dominance.\nBella(\u03d5) is a graded belief, in a similar manner to [11], and has to be read \u201ca believes that \u03d5 with certainty l\u201d. For instance, Bel1a(\u03d5) means \u201ca is sure that \u03d5\u201d and Bel 0 a(\u03d5) can be read \u201cFor a, \u03d5 is not plausible at all\u201d.\nSimilarly, Attka(\u03d5) is a graded attitude that has to be read \u201ca appreciates/values the fact that \u03d5 with a degree l\u201d. In our context, this operator will be used to cover various notions, such as Desires, Ideals and Goals that are represented with distinct modal operators in other work such as [1] and [14]. We define desires as a positive attitude toward future facts and ideals as what the agents would like to be always true:\nDeska(\u03d5) def = Attka(F (\u03d5))\nIdealk>0a (\u03d5) def = Attk>0a (G(\u03d5)) = Des \u2212k<0 a (\u00ac\u03d5)\n(2)\nThe definition of goals through attitudes will be presented in the next subsection.\nNote that in our model, the subject of an attitude can as well be preserving forest, being nice to others, hiring new employee or Bellb(\u3008a, c, give sandwich\u3009), eventually encapsulated in temporal operators.\nAs in classical BDI, Inta(\u03d5) represents an agent\u2019s plan [26] and has to be read \u201da intends to make \u03d5 true\u201d (with \u03d5 being an event in the general case).\nEmoia,(b|\u2205)(\u03b5, \u03d5) represent emotions. Following classical literature [12], our emotions are related to facts and can be directed toward an agent. Emoia,(b|\u2205)(\u03b5, \u03d5) has to be read \u201ca feels \u03b5, eventually for/towards b, with intensity i, regarding the fact that \u03d5\u201d with \u03b5 \u2208 EMO. In the following sections, it will be simplified into \u03b5ia,(b|\u2205)(\u03d5).\nFor the sake of readability, we introduce new operators to represent agents\u2019 involvement in an event. Respa expresses a direct responsibility. Unlike [1, 14], we do not consider an agent responsible for a situation it could have avoided. Wita means that the agent witnessed the occurrence of the event:\nRespa( ) def = ( = \u3008a,\u2212,\u2212\u3009)\nWita( ) def = ( = \u3008a,\u2212,\u2212\u3009) \u2228 ( = \u3008\u2212, a,\u2212\u3009)\n(3)"}, {"heading": "4.2 Semantics", "text": "Based on possible world semantics, we define a frame F = \u3008W,B,D, I, E\u3009 as a tuple where:\n\u2022 W is a nonempty set of possible worlds,\n\u2022 B : AGT \u2192 (W \u2192 2W ) is the function that associates each agent a \u2208 AGT and possible world w \u2208W to the set of belief-accessible worlds Ba(w),\n\u2022 D : AGT \u2192 (W \u00d7DEG+ \u2192 2W ) is the function that associates each agent a \u2208 AGT and possible world w \u2208 W with a level of desirability l \u2208 DEG+ to the set of desire-accessible worlds Da(w, l),\n\u2022 I : AGT \u2192 (W \u2192 2W ) is the function that associates each agent a \u2208 AGT and possible world w \u2208W to the set of intention-accessible worlds Ia(w), and\n\u2022 E : EV T \u2192 W is the function that associates each event \u2208 EV T to the resulting possible world.\nThen, a model M = \u3008F ,V\u3009 is a couple where F is a frame and V : W \u2192 ATM a valuation function.\nGiven a model M we note M, w |= \u03d5 a formula \u03d5 that is true in a world w. Truth conditions of formulas are defined by induction in the classical way:\n\u2022 M, w |= p iff p \u2208 V(w);\n\u2022 M, w |= \u00ac\u03d5 iff not M, w |= \u03d5;\n\u2022 M, w |= \u03d5 \u2227 \u03c8 iff M, w |= \u03d5 and M, w |= \u03c8 ;\n\u2022 M, w |= Bella(\u03d5) iff card(GBa(w))card(Ba(w)) = l where GBa(w) = {v \u2208 Ba(w) ; M, v |= \u03d5} ;\n\u2022 M, w |= Desla(\u03d5) iff M, v |= \u03d5 \u2200v \u2208 Da(w, l);\n\u2022 M, w |= Inta(\u03d5) iff M, v |= \u03d5 \u2200v \u2208 Ia(w);\n\u2022 M, w |= iff M, v |= > \u2200v \u2208 E( );\nThe truth condition ofBella(\u03d5) states that the level of plausibility of \u03d5 is the proportion of belief-accessible worlds where \u03d5 is true. The next subsections describe the rules for the inference engines presented in section 3. When required, the computation of believability, desirability and intensity degrees will be represented by a f function that is part of the implementation and will not be detailed in this section (see section 5 instead)."}, {"heading": "4.3 Folk-psychology reasoner", "text": "4.3.1 Graded beliefs Following [11] and [1], all accessibility relations B are transitive and euclidean, which ensures that the agent is aware of its own beliefs2:\nBella(\u03d5) def =\u21d2 Bel1a(Bella(\u03d5)) (4)\nWe generalize (4) so that agents are aware of their own mental states, social relations and involvement.\nHowever, unlike other models [1] [11], B is not serial3. Only GB is. This represents the fact that the agent generally has uncertainty about states of affairs. Intuitively:\nBella(\u03d5) def =\u21d2 Bel1\u2212la (\u00ac\u03d5) (5)\n2If wRv and vRu, then successively by transitivity, euclidianity and transitivity again: wRv and vRv. 3A relation R is serial iff \u2200w, \u2203v so that wRv.\nFor convenience, we define two thresholds mod th and str th wich 0.5 < mod th < str th. They correspond to situations where the agent moderately (Bell>mod tha (\u03d5)) and strongly (Bell>str tha (\u03d5)) believes something.\nFinally, if an agent believes a state of affairs to possibly cause another, it will deduce a belief about it:\nBella(\u03c8) \u2227Bell \u2032 a (\u03c8 \u21d2 \u03d5) def =\u21d2 Belf(l,l \u2032) a (\u03d5) (6)\n4.3.2 Graded attitudes Attitudes can be positive or negative and we assume that agents hold consistent desires:\nM, w |= (Attka(\u03d5) \u2227Attk \u2032 a (\u00ac\u03d5)) iff k = \u2212k\u2032 (7)\nHowever, indirect inconsistency is still possible: an agent might want something that can possibly lead to or be caused by (the occurrence of) the negation of another desire of his. The consistency is then preserved at the level of desire adoption:\nDeska(\u03d5) \u2227Bell>str tha (\u03c8 \u21d2 F (\u03d5)) \u2227 \u00acIncDeska(\u03c8) def =\u21d2 N(Deska(\u03c8)) (8)\nwith IncDeska(\u03d5) representing inconsistent desires:\nIncDeska(\u03d5) def = (Bell>str tha (\u03d5\u21d2 \u00ac\u03c8) \u2227Desk \u2032>0 a (\u03c8))\n\u2228(Bell>str tha (\u03d5\u21d2 \u03c8) \u2227Desk \u2032<0 a (\u03c8)) (9)\nThis means that desiring \u03d5 is inconsistent when the agent strongly beliefs it might lead to an undesirable \u03c8. We allow for adopting indirectly inconsistent desires only when the agent only believes moderately that there can be a certain incompatibility with existing ones.\nWe also define a weaker case of inconsistency where \u03d5 leads to an undesirable state of affairs of a higher level:\nWIncDeska(\u03d5) def = Bell>str tha (\u03d5\u21d2 \u00ac\u03c8)\n\u2227Desk \u2032;|k\u2032|>|k| a (\u03c8) (10)\n4.3.3 Goals Following the BDI model [26], goals are defined as desires that are consistent \u2013 at least weakly, in our case \u2013 and believed to be achievable. To this purpose, we introduce a new threshold des th:\nGoalk>0a (\u03d5) def = Desk>des tha (\u03d5)\u2227Bella(F (\u03d5))\u2227\u00acWIncDeska(\u03d5)\n(11)\nGoals are then turned into intentions either because the agent can achieve it:\nGoalk>0a ( ) \u2227Respa( ) def =\u21d2 N(Inta( )) (12)\nor, similarly to [7], because the agent strongly believes there is \u2013 at least \u2013 one mean to achieve it:\nGoalk>0a (\u03d5) \u2227Bell>str tha (\u03c8 \u21d2 F (\u03d5)) \u2227 \u00acWIncDeska(\u03c8)\n\u2227Bell \u2032 a (F (\u03c8)) def =\u21d2 N(Inta(\u03c8))\n(13)\nWe leave it to the implementation phase (section 5) to decide how intentions are ordered when several possible known \u03c8 can be used to achieve a goal.\n4.3.4 Intentions and acts Since intentions are generated from desires all accessibility relations I are serial: M, w 6|= Inta(\u00ac\u03d5) ifM, w |= Inta(\u03d5)\nIf an agent intends a state of affairs and knows a means to achieve it, it will also intend the latter:\nInta(\u03d5) \u2227Bell>str tha (\u03c8 \u21d2 F (\u03d5)) def =\u21d2 Inta(\u03c8) (14)\nAdditionally, if an agent intends an act which it is responsible for, it will perform it in the next step:\nInta( ) \u2227Respa( ) def =\u21d2 N( ) (15)\nFurthermore, when an event occurs, we propagate responsibility to all the states of affairs it is believed to have caused:\nBelda(\u03c8) \u2227Bella(Respb(\u03c8)) \u2227Bell \u2032 a (\u03d5) \u2227Bell \u2032\u2032 a (\u03c8 \u21d2 F (\u03d5))\ndef =\u21d2 Belf(l,l \u2032,l\u2032\u2032) a (Respb(\u03d5))\n(16)\nFinally, as far as accessibility relations E are concerned, we consider any witness believes with degree 1 that the event happened and that the other witness also believes it. Note that when an event occurs, the belief that it happened remains true afterwards.\n4.3.5 Updating attitudes Beliefs are updated as new events occur (except for ideals that are constant and hold globally). In order for the agent to react to situation change, attitudes about new states of affairs have to be triggered. In our model, following [20, 10, 3], the attitude is influenced not only by new beliefs, but also by the attitude of others and the social relation. Formally:\nBell>str tha (\u03d5) \u2227Attka(F (\u03d5)) \u2227Bell \u2032 a (Att k\u2032 b (F (\u03d5)))\n\u2227Likeha,b \u2227Domh \u2032 a,b def =\u21d2 Attf(k,k \u2032,h,h\u2032) a (\u03d5)\nBell>str tha (Des k b (\u03d5)) \u2227 Likek \u2032>0 a,b def =\u21d2 N(Desf(k,k \u2032) a (\u03d5)))\n(17)\n4.3.6 Speech acts and social interaction Beliefs can also be updated through communication. Although our work mostly focus on non-verbal communication, we consider a limited set of illocutionnary acts [28] ILL = {Assert, Request, Commit, Express}.\nBased on similar work in speech acts formalization [16, 14], we define trigering rules for our speech acts. For instance:\n\u00acBel1a(Intb(\u03d5)) \u2227 Inta(Intb(\u03d5)) def =\u21d2 Requesta,b(\u03d5) (18)\nIn turn, these events will lead to new mental states for the recipient agent, similarly to classical FIPA semantics and existing work on social interaction modeling [16, 10]. For\nthe sake of conciseness, we only present these two examples here:\nAssertb,a(\u03d5) \u2227 Likeka,b \u2227Domk \u2032 a,b def =\u21d2 N(Belf(k,k \u2032) a (\u03d5))\nRequestb,a(\u03d5) \u2227Domk<0a,b def =\u21d2 N(Inta(\u03d5)))\n(19)"}, {"heading": "4.4 Emotional inference engine", "text": "The emotional inference engine consists of a set of appraisal rules for emotion categories EMO. In this implementation, we have used an OCC-based model, highly inspired by [1, 14, 11]. Here are some examples of triggering conditions for each group of emotions.\nBella(\u03b3) \u2227Attk>0a (\u03b3) def =\u21d2 N(Joyi=f(l,k)a (\u03b3))\nBella(F (\u03b3)) \u2227Desk<0a (\u03b3) def =\u21d2 N(Feari=f(l,k)a (\u03b3))\nBelda(\u03b3) \u2227Bella(Attk<0b (\u03b3)) \u2227 Like k\u2032<0 a,b\ndef =\u21d2 N(Gloatingi=f(l,k,k \u2032,d) a,b (\u03b3))\nBella(\u03b3) \u2227 Idealka(\u03b3)\u2227Bell \u2032 a (Rspb(\u03b3))\ndef =\u21d2 N(Admirationi=f(l,l \u2032,k) a,b (\u03b3))\nBella(\u03b3) \u2227 Idealka(\u03b3)\u2227Bell \u2032 a (Rspb(\u03b3)) \u2227Goalk \u2032 a (\u03b3)\ndef =\u21d2 N(Gratitudei=f(l,l \u2032,k,k\u2032) a,b (\u03b3))\n(20)\nPlease note that \u03b3 is a proposition that do not involve any temporal operator. Besides, the intensity of an emotion is a combination of the degree of certainty of beliefs and the degree of desirability in attitudes. Depending on the appraisal model, the degree of certainty can represent the sense of reality, the unexpectedness, the likelihood and the realization. The degree of desirability can correspond to desirability-forself but also to praiseworthiness [1].\nIn OCC [21], Gratification, Remorse, Gratitude and Anger are defined as Well-being/Attribution emotions, triggered when one focuses both on the praiseworthiness of an action and on its desirability. However, in our model, these two notions overlap since ideals are deduced from attitude. Nevertheless, similarly to [14] we think that one might distinguish Gratitude and Anger from Admiration and Reproach if the triggering state of affairs corresponds to a goal, that is to say it is not only praiseworthy but is also desirable and consistent enough to generate an intention of achievement."}, {"heading": "4.5 Commonsense reasoner", "text": "The commonsence reasoner allows the agent to acquire new beliefs based on a set of commonsense rules. It is mostly domain-dependent. Section 5 describes how we used it to implement a job interview simulation scenario. Here is a simple example of how this reasoner can combine with the folk-psychology inference engines presented above.\n4.5.1 Example Consider two friends John (J) and Mary (M) having a conversation about their holidays. Mary is going to her home-\ntown (ht). The fact that she is going to visit her father is a detail she could either mention or not:\nDes0.77M (talking about holidays) (input) Bel0.8M (\u3008M,J, visiting ht and dad\u3009 =\u21d2 F (talk about holidays)) (input) Bel0.8M (\u3008M,J, visiting ht\u3009\n=\u21d2 F (talk about holidays)) (input)\nNevertheless Mary remembers John recently lost his father and thus supposes it is a sensitive topic:\nBel1M (J lost his dad) (input) Bel0.76M (J lost his dad =\u21d2 Ideal0.8J (\u00ac\u3008\u2212, J, dad\u3009))(input)\n(=\u21d2)BellM (Ideal0.8J (\u00ac\u3008\u2212, J, dad\u3009)) (6)\nOf course, Mary knows that saying she is going to visit her father implies actually talking about her father:\nBel0.8M (\u3008M,\u2212, visiting ht and dad\u3009 =\u21d2 \u3008M,\u2212, dad\u3009)(input)\nAnd, knowing that John wants to avoid this topic, she does too. Hence, she is will not mention the fact that she is visiting her father when talking about her holidays:\n(=\u21d2)IdealkM (\u00ac\u3008\u2212, J, dad\u3009) (8) (=\u21d2)WIncDes0.77M (\u3008M,J, visiting ht and dad\u3009) (10) (=\u21d2)Goal0.77M (\u3008M,J, visiting ht\u3009) (11)"}, {"heading": "5. IMPLEMENTATION", "text": "The theoretical model we presented in previous sections is aimed to be domain-independent. Yet, the purpose of our current work in the TARDIS project [2] is to develop a training game in order to facilitate NEETs\u2019 access to employment. Therefore, we propose to implement it in the context of job interview simulation. Indeed, this sort of application appears as a promising way to increase applicant\u2019s self-confidence [17]. Additionally, job interviews are a good example of semi-structured dyadic interactions where recruiters have several opportunities to reason about candidates\u2019 mental and affective states. Nevertheless, to our knowledge, existing models do not include a Theory of Mind.\nOur implementation was done in SWI-Prolog for the inference engine and the logical framework. This reasoner was embedded in a C++ program that handles the reasoning loop and the communication between the modules.\nReasoning loop Following the classical BDI interpreter, at every cycle, the agent interpret external events to generate a list of potential actions, deliberate to select one of them, update its intentions and then execute them:\nThresholds and level functions The implementation of the model requires to instanciate all thresholds (th) and combination function (f) for degrees of believability and desirability of new mental state or the intensity of emotions.\nIn our implementation, mod th = 0.5, str th = 0.75 and des th = 0.7.\nThe combination function cannot be given in detail in this paper but we consider two families:\nAlgorithm 1 ToM Reasoning loop\nloop Execute intentions() Simulate others emotions() Update beliefs and attitudes()\nUpdate beliefs with new SoA() Handle operators equivalence() Adopt new desires() Order goals()\nAdopt new intentions() Adopt new intentions from goals() Adopt new intentions from intentions()\nend loop\n\u2013 For attitude dynamics and credibility (e.g. equation 17), we use simple average functions on the relevant interval:\nf(k, k\u2032) = ((k + k\u2032)/4) + 0.5\n\u2013 For emotions (see equation 20), we combine the linear influence from attitude (for instance, joy has been chosen to be linearly correlated to the attitude toward the fact) with a logarithmic influence of the degree of certainty. This way, we get to trigger more salient emotions even with relatively weak beliefs. Nevertheless, let us remind here that we only consider beliefs which levels are greater than a certain threshold (mod thld = 0.5 in our implementation).\nf(l, k) = k 2 \u00d7 Log(2l \u2212 1)\u2212min min + 0.5\nwhere min represents the smallest value coded by the machine (i.e. the value of Log(x) when x\u2192 0). The 2l\u22121 facto is used to adjust the value in [0,1] before we compute the intensity, which is then readjusted in [0.5,1] to get significant values.\nJob interview simulation The course of the job interview is handled in the commonsense module: we define a series of topics that must be adressed by the agent through speech acts (e.g. questions about the salary, the experience...). Moreover, each topic is associated with some expectations about the impact of the question. Based on the current goals (in terms of affective state for the interlocutor), the agent will select a question (a speech act) or another.\nMoreover, the agent computes beliefs about the interlocutor\u2019s self-confidence, motivation and qualification, based on its reaction to the questions and simple TT-rules. For instance, hesitating in the job description topic can indicate they are not qualified enough while being focused when introducing themselves denotes a good self-confidence level. The perception of \u201chesitation\u201d and \u201cfocused\u201d is done by another module of the TARDIS platform which is not part of this paper (see [2])."}, {"heading": "6. PRELIMINARY EVALUATION", "text": "In this section, we describe a preliminary evaluation aiming to assess the functioning of our model and its possible contribution in the context of job interview simulation.\nSubjects play the role of an unemployed youngster lacking work experience and applying for the job of sales department secretary. The virtual recruiter utterances are predefined for each possible speech act and situation in the model. No constraint were given about a supposed personality, level of education of professional background of the role-played interviewee."}, {"heading": "6.1 Method", "text": "We recruited 30 volunteers \u2013 11 females and 19 males \u2013, 19 of them working or having an internship at our university. All the subjects were aged over 24, had gone to university and were familiar with computers. 18 of the participants are native speakers and the remaining have at least an intermediate level. Since we are not interested in verbal communication, this is sufficient so that the participants understand what the recruiter says.\nThe recruiter\u2019s utterances were given in a very simple Graphical User Interface (GUI). The valence of the agent\u2019s affective state and its runtime evaluation of the candidate\u2019s self-confidence, motivation and qualification (values in [-1,1]) were represented by slide-bars. A text field allows the subject to type his/her answer to the virtual recruiter\u2019s questions. Besides, a series of 8 sliders (values in [0,1]) gives them the possibility to express their affective states to the recruiter as combinations of the following affects: relieved (REL), embarrassed (EMB), hesitating (HES), stressed (STR), ill at ease (IAE), focused (FOC), aggressive (AGG) and bored (BOR).4\nSubjects faced one agent out of 3 possible recruiter profiles: one that tries to make the candidates feel at ease (PROFILE A), one asking regular questions, with no specific goal on the user\u2019s mental state (PROFILE B) and one that, asks embarassing questions (PROFILE C). This is simply done by varying their goals regarding the emotional reaction they want to elicit in our model. All three agents use the same ToM reasoner described in previous sections.\nHypothesis: The profile variation will have an impact on the participants\u2019 emotional states as expressed through the slidebars.\nMeasures: In this paper, we focus on measures extracted from the interaction history. They refer to the average intensity of relief, embarrassment, hesitation, stress, uneasiness, concentration, aggressiveness and boredom expressed by the participants as well as the total emotional expressiveness (TOT). More specifically, we measure the mean amount of information the candidates gave about their affective states."}, {"heading": "6.2 Results", "text": "Shapiro-Wilks test shows that none of our measures follows a normal distribution. Besides, Kruskal-Wallis test reveals a main effect of PROFILE on TOT (Chi2(2, 629) = 11.435; p < 0.01) and particularly EMB (Chi2(2, 629) = 6.231; p < 0.05) and FOC (Chi2(2, 629) = 9.218; p < 0.01). This means that the profile of the recruiter (comprehensive,\n4In the full TARDIS project\u2019s setting, these sliders are replaced by automatic recognition of user affects using the SSI system [32].\nneutral or challenging) has an effect on the affects assessed (and possibly expressed) by the user, and that this effect is particularly important for embarassement and concentration.\nA Mann-Whitney test then shows that participants that interact with PROFILE A (comprehensive recruiter) express more affects in general (U = 20; p < 0.05), more embarrassment (U = 20; p < 0.05) and more concentration (U = 21; p < 0.05) than those who interact with PROFILE B. Likewise, PROFILE C (challenging recruiter) elicits more affects (U = 6; p < 0.01) and in particular stress (U = 18; p < 0.05), uneasiness (U = 24; p < 0.05) and concentration (U = 10; p < 0.01) than PROFILE B. We also note that in this case, no effect appears regarding embarrassment (U = 26; p = 0.069). Finally, no significant effect is revealed between PROFILE A and PROFILE C. See Figure 2."}, {"heading": "6.3 Discussion", "text": "Theory of Mind is a complex process that relies on various other cognitive and perceptual processes. It is not only hard to model but also to assess. Thus, a simple protocol such as the one we used in this study is not sufficient to fully evaluate the impact of our ToM model on the quality of the training. First, the GUI we used is not user-friendly and does not allow for user\u2019s immersion in the scenario. We assume that using the full TARDIS project\u2019s setting [2] would enhance the interaction credibility and help highlight the virtual agent\u2019s reasoning and reactivity. Besides, the evaluation process should be based on richer measures (e.g. thorough post-hoc questionnaire) in order to evaluate the effect of our model. Yet, such a specific evaluation protocol for affective and interaction-oriented ToM still has to be defined.\nIn the litterature, there are validated methods to evaluate whether subjects \u2013 generally children \u2013 have ToM abilities and use it [6]. Nevertheless, there is no such test that integrates a strong interactional aspect to our knowledge. From the computational point of view, [15] points out the issue of evaluating a ToM model. In this work, the course of events and the agent\u2019s actions and explanations are specified in advance for different scenarios. Thus, the ToM models are evaluated based on whether they match these specifications. Similarly, [25] builds expectations about user\u2019s\nactions \u2013 based on formal models in the specific context of wartime negotiations \u2013 in order to model a simplified theory of mind and then compare them with the actual user\u2019s behavior. These two approaches are not applicable in our human/agent interaction situation, because they rely on a model of the task which is difficult to describe when it comes to afective non-verbal behaviour.\nNevertheless, the study we present in this paper shows promising results regarding the contribution of our ToM model in the context of a training game. Although all recruiters\u2019 profiles benefit from the ToM reasoner, only PROFILE A and PROFILE C use it to select questions according to a reasoning about the mental and emotional states they could induce. The more recruters ask such questions, the more mindreading they perform. The study shows that this kind of ToM-based behavior indeed has an impact on the users\u2019 reactions. It also demonstrates the benefit of implementing several profiles in the enrichment of the coaching scenarios."}, {"heading": "7. CONCLUSION & PERSPECTIVES", "text": "In this paper, we proposed an affective and interactionoriented Theory of Mind model to support the development of intelligent agents that are able to represent and reason about their human interlocutors\u2019 mental states. It relies on a hybrid mindreading approach mixing theory-theory and simulation-theory paradigms. This model is domainindependent, which means it can potentially be used in different context of application, including social coaching.\nIt has been implemented and evaluated in the context of job interview simulation in which the virtual recruiter both evaluates the human candidates based on their affective reactions, and reacts emotionally according to its desires and ideals. This study demonstrates the influence of the implementation of various recruiter profiles on the enrichment of the system\u2019s efficiency. In addition, the explanatory capability of our reasoning model is a key feature for the users to benefit from a rich post-interview feedback. While evaluating such a complex cognitive process as ToM remains a difficult task, we are currently working on the integration of this model in the TARDIS platform in order to perform mental states evaluation using signal processing. We plan to evaluate the impact of such a ToM model on the credibility of the virtual recruiter."}, {"heading": "8. REFERENCES", "text": "[1] C. Adam, A. Herzig, and D. Longin. A logical\nformalization of the OCC theory of emotions. Synthese, 168(2):201\u2013248, Feb. 2009.\n[2] K. Anderson, E. Andre\u0301, T. Baur, S. Bernardini, M. Chollet, E. Chryssafidou, I. Damian, C. Ennis, A. Egges, P. Gebhard, H. Jones, M. Ochs, C. Pelachaud, K. Porayska-Pomsta, R. Paola, and N. Sabouret. The TARDIS framework: intelligent virtual agents for social coaching in job interviews. Proceedings of the Tenth International Conference on Advances in Computer Entertainment Technology (ACE-13). Enschede, the Netherlands. LNCS 8253, page in press, 2013.\n[3] R. Aylett and S. Louchart. If I were you: double appraisal in affective agents. In Proceedings of the 7th\ninternational joint conference on Autonomous Agents and MultiAgent Systems, pages 1233\u20131236, 2008.\n[4] S. Baron-Cohen. Mindblindness: An essay on autism and theory of mind. MIT press, 1997.\n[5] L. M. Batrinca, G. Stratou, A. Shapiro, L.-P. Morency, and S. Scherer. Cicero - towards a multimodal virtual audience platform for public speaking training. In Proc. 2013 International Conference on Intelligent Virtual Agents, pages 116\u2013128, 2013.\n[6] E. M. A. Blijd-Hoogewys, P. L. C. Van Geert, M. Serra, and R. B. Minderaa. Measuring theory of mind in children. Psychometric properties of the ToM storybooks. Journal of Autism and Developmental Disorders, 38(10):1907\u20131930, 2008.\n[7] T. Bosse, Z. A. Memon, and J. Treur. A recursive BDI agent model for Theory of Mind and its applications. Applied Artificial Intelligence, 25(1):1\u201344, 2011.\n[8] G. Botterill and P. Carruthers. The philosophy of psychology. Cambridge University Press, 1999.\n[9] J. Bynner and S. Parsons. Social Exclusion and the Transition from School to Work: The Case of Young People Not in Education, Employment, or Training (NEET). Journal of Vocational Behavior, 60(2):289\u2013309, Apr. 2002.\n[10] C. Castelfranchi. Modelling social action for AI agents. IJCAI\u201997 Proceedings of the Fifteenth international joint conference on Artifical intelligence - Volume 2, 103(1):1567\u20131576, 1997.\n[11] M. Dastani and E. Lorini. A logic of emotions : from appraisal to coping. In Proceedings of the 11thiInternational conference on Autonomous Agents and Multiagent Systems, pages 1133\u20131140, 2012.\n[12] N. H. Frijda. The emotions. Cambridge University Press, 1986.\n[13] A. I. Goldman. Simulating minds: The philosophy, psychology, and neuroscience of mindreading. Oxford University Press, 2006.\n[14] N. Guiraud, D. Longin, E. Lorini, S. Pesty, and J. Rivie\u0300re. The face of emotions : a logical formalization of expressive speech acts. In The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 3, pages 1031\u20131038, 2011.\n[15] M. Harbers. Explaining agent behavior in virtual training. SIKS dissertation series, 2011(35), 2011.\n[16] A. Herzig and D. Longin. A logic of intention with cooperation principles and with assertive speech acts as communication primitives. In Proceedings of the first international joint conference on Autonomous agents and multiagent systems: part 2, pages 920\u2013927. ACM, 2002.\n[17] M. Hoque, M. Courgeon, J.-C. Martin, B. Mutlu, and R. Picard. MACH: My Automated Conversation coacH. In Proc. 2013 ACM international joint conference on Pervasive and ubiquitous computing. ACM Press, 2013.\n[18] T. F. Leary et al. Interpersonal diagnosis of personality. Ronald Press New York, 1957.\n[19] S. Marsella, J. Gratch, and J. Rickel. Expressive behaviors for virtual worlds. Lifelike Characters Tools Affective Functions and Applications, pages 317\u2013360,\n2003.\n[20] M. Ochs, N. Sabouret, and V. Corruble. Simulation of the Dynamics of Nonplayer Characters\u2019 Emotions and Social Relations in Games. Computational Intelligence and AI in Games, IEEE Transactions on, 1(4):281\u2013297, 2009.\n[21] A. Ortony, G. L. Clore, and A. Collins. The Cognitive Structure of Emotions, 1990.\n[22] A. Paiva, J. Dias, D. Sobral, R. Aylett, P. Sobreperez, S. Woods, C. Zoll, and L. Hall. Caring for agents and agents that care: Building empathic relations with synthetic agents. In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems-Volume 1, pages 194\u2013201, Washington, DC, USA, 2004. IEEE Computer Society.\n[23] L. Pareto, D. Schwartz, and L. Svensson. Learning by guiding a teachable agent to play an educational game. in Education Building Learning, pages 1\u20133, 2009.\n[24] A. Pnueli. The temporal logic of programs. In Foundations of Computer Science, 1977., 18th Annual Symposium on, pages 46\u201357. IEEE, 1977.\n[25] D. V. Pynadath, N. Wang, and S. C. Marsella. Are you thinking what I\u2019m thinking? An Evaluation of a Simplified Theory of Mind. In Intelligent Virtual Agents, pages 44\u201357. Springer, 2013.\n[26] A. S. Rao and M. P. Georgeff. Modeling Rational Agents within a BDI-Architecture. In Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning, 1991.\n[27] K. R. Scherer. Emotion and emotional competence: conceptual and theoretical issues for modelling agents. Blueprint for Affective Computing, pages 3\u201320, 2010.\n[28] J. R. Searle. Speech acts: An essay in the philosophy of language, volume 626. Cambridge university press, 1969.\n[29] M. Sieverding. \u2019Be Cool!\u2019: Emotional costs of hiding feelings in a job interview. International Journal of Selection and Assessment, 17(4), 2009.\n[30] L. Z. Tiedens. Anger and advancement versus sadness and subjugation: The effect of negative emotion expressions on social status conferral. Journal of personality and social psychology, 80(1):86\u2014-94, 2001.\n[31] K. Vogeley, P. Bussfeld, A. Newen, S. Herrmann, F. Happe, P. Falkai, W. Maier, N. J. Shah, G. R. Fink, and K. Zilles. Mind reading: neural mechanisms of theory of mind and self-perspective. Neuroimage, 14(1):170\u2013181, 2001.\n[32] J. Wagner, F. Lingenfelser, T. Baur, I. Damian, F. Kistler, and E. Andre\u0301. The social signal interpretation (ssi) framework-multimodal signal processing and recognition in real-time. In Proceedings of the 21st ACM International Conference on Multimedia, Barcelona, Spain, 2013."}], "references": [{"title": "A logical formalization of the OCC theory of emotions", "author": ["C. Adam", "A. Herzig", "D. Longin"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The TARDIS framework: intelligent virtual agents for social coaching in job interviews", "author": ["K. Anderson", "E. Andr\u00e9", "T. Baur", "S. Bernardini", "M. Chollet", "E. Chryssafidou", "I. Damian", "C. Ennis", "A. Egges", "P. Gebhard", "H. Jones", "M. Ochs", "C. Pelachaud", "K. Porayska-Pomsta", "R. Paola", "N. Sabouret"], "venue": "Proceedings of the Tenth International Conference on Advances in Computer Entertainment Technology (ACE-13). Enschede, the Netherlands. LNCS 8253, page in press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "If I were you: double appraisal in affective agents", "author": ["R. Aylett", "S. Louchart"], "venue": "Proceedings of the 7th  international joint conference on Autonomous Agents and MultiAgent Systems, pages 1233\u20131236", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Mindblindness: An essay on autism and theory of mind", "author": ["S. Baron-Cohen"], "venue": "MIT press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Cicero - towards a multimodal virtual audience platform for public speaking training", "author": ["L.M. Batrinca", "G. Stratou", "A. Shapiro", "L.-P. Morency", "S. Scherer"], "venue": "Proc. 2013 International Conference on Intelligent Virtual Agents, pages 116\u2013128", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Measuring theory of mind in children", "author": ["E.M.A. Blijd-Hoogewys", "P.L.C. Van Geert", "M. Serra", "R.B. Minderaa"], "venue": "Psychometric properties of the ToM storybooks. Journal of Autism and Developmental Disorders, 38(10):1907\u20131930", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "A recursive BDI agent model for Theory of Mind and its applications", "author": ["T. Bosse", "Z.A. Memon", "J. Treur"], "venue": "Applied Artificial Intelligence, 25(1):1\u201344", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "The philosophy of psychology", "author": ["G. Botterill", "P. Carruthers"], "venue": "Cambridge University Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Social Exclusion and the Transition from School to Work: The Case of Young People Not in Education, Employment, or Training (NEET)", "author": ["J. Bynner", "S. Parsons"], "venue": "Journal of Vocational Behavior,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Modelling social action for AI agents", "author": ["C. Castelfranchi"], "venue": "IJCAI\u201997 Proceedings of the Fifteenth international joint conference on Artifical intelligence - Volume 2, 103(1):1567\u20131576", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "A logic of emotions : from appraisal to coping", "author": ["M. Dastani", "E. Lorini"], "venue": "Proceedings of the 11thiInternational conference on Autonomous Agents and Multiagent Systems, pages 1133\u20131140", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "The emotions", "author": ["N.H. Frijda"], "venue": "Cambridge University Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1986}, {"title": "Simulating minds: The philosophy", "author": ["A.I. Goldman"], "venue": "psychology, and neuroscience of mindreading. Oxford University Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "The face of emotions : a logical formalization of expressive speech acts", "author": ["N. Guiraud", "D. Longin", "E. Lorini", "S. Pesty", "J. Rivi\u00e8re"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems - Volume 3, pages 1031\u20131038", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Explaining agent behavior in virtual training", "author": ["M. Harbers"], "venue": "SIKS dissertation series, 2011(35)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "A logic of intention with cooperation principles and with assertive speech acts as communication primitives", "author": ["A. Herzig", "D. Longin"], "venue": "Proceedings of the first international joint conference on Autonomous agents and multiagent systems: part 2, pages 920\u2013927. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "MACH: My Automated Conversation coacH", "author": ["M. Hoque", "M. Courgeon", "J.-C. Martin", "B. Mutlu", "R. Picard"], "venue": "Proc. 2013 ACM international joint conference on Pervasive and ubiquitous computing. ACM Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Interpersonal diagnosis of personality", "author": ["T.F. Leary"], "venue": "Ronald Press New York,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1957}, {"title": "Expressive behaviors for virtual worlds", "author": ["S. Marsella", "J. Gratch", "J. Rickel"], "venue": "Lifelike Characters Tools Affective Functions and Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Simulation of the Dynamics of Nonplayer Characters\u2019 Emotions and Social Relations in Games", "author": ["M. Ochs", "N. Sabouret", "V. Corruble"], "venue": "Computational Intelligence and AI in Games, IEEE Transactions on, 1(4):281\u2013297", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "and A", "author": ["A. Ortony", "G.L. Clore"], "venue": "Collins. The Cognitive Structure of Emotions", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Caring for agents and agents that care: Building empathic relations with synthetic agents", "author": ["A. Paiva", "J. Dias", "D. Sobral", "R. Aylett", "P. Sobreperez", "S. Woods", "C. Zoll", "L. Hall"], "venue": "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems-Volume 1, pages 194\u2013201, Washington, DC, USA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning by guiding a teachable agent to play an educational game", "author": ["L. Pareto", "D. Schwartz", "L. Svensson"], "venue": "Education Building Learning, pages 1\u20133", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "The temporal logic of programs", "author": ["A. Pnueli"], "venue": "Foundations of Computer Science, 1977., 18th Annual Symposium on, pages 46\u201357. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1977}, {"title": "Are you thinking what I\u2019m thinking? An Evaluation of a Simplified Theory of Mind", "author": ["D.V. Pynadath", "N. Wang", "S.C. Marsella"], "venue": "Intelligent Virtual Agents, pages 44\u201357. Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling Rational Agents within a BDI-Architecture", "author": ["A.S. Rao", "M.P. Georgeff"], "venue": "Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1991}, {"title": "Emotion and emotional competence: conceptual and theoretical issues for modelling agents", "author": ["K.R. Scherer"], "venue": "Blueprint for Affective Computing, pages 3\u201320", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech acts: An essay in the philosophy of language", "author": ["J.R. Searle"], "venue": "volume 626. Cambridge university press", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1969}, {"title": "Be Cool!\u2019: Emotional costs of hiding feelings in a job interview", "author": ["M. Sieverding"], "venue": "International Journal of Selection and Assessment, 17(4)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Anger and advancement versus sadness and subjugation: The effect of negative emotion expressions on social status conferral", "author": ["L.Z. Tiedens"], "venue": "Journal of personality and social psychology, 80(1):86\u2014-94", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Mind reading: neural mechanisms of theory of mind and self-perspective", "author": ["K. Vogeley", "P. Bussfeld", "A. Newen", "S. Herrmann", "F. Happe", "P. Falkai", "W. Maier", "N.J. Shah", "G.R. Fink", "K. Zilles"], "venue": "Neuroimage, 14(1):170\u2013181", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "The social signal interpretation (ssi) framework-multimodal signal processing and recognition in real-time", "author": ["J. Wagner", "F. Lingenfelser", "T. Baur", "I. Damian", "F. Kistler", "E. Andr\u00e9"], "venue": "Proceedings of the 21st ACM International Conference on Multimedia, Barcelona, Spain", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "INTRODUCTION AND POSITIONING During the last decade, several projects have proposed to use intelligent virtual agents in digital games for user empowerment [19, 22, 23, 2, 5].", "startOffset": 156, "endOffset": 174}, {"referenceID": 21, "context": "INTRODUCTION AND POSITIONING During the last decade, several projects have proposed to use intelligent virtual agents in digital games for user empowerment [19, 22, 23, 2, 5].", "startOffset": 156, "endOffset": 174}, {"referenceID": 22, "context": "INTRODUCTION AND POSITIONING During the last decade, several projects have proposed to use intelligent virtual agents in digital games for user empowerment [19, 22, 23, 2, 5].", "startOffset": 156, "endOffset": 174}, {"referenceID": 1, "context": "INTRODUCTION AND POSITIONING During the last decade, several projects have proposed to use intelligent virtual agents in digital games for user empowerment [19, 22, 23, 2, 5].", "startOffset": 156, "endOffset": 174}, {"referenceID": 4, "context": "INTRODUCTION AND POSITIONING During the last decade, several projects have proposed to use intelligent virtual agents in digital games for user empowerment [19, 22, 23, 2, 5].", "startOffset": 156, "endOffset": 174}, {"referenceID": 8, "context": "According to Eurostat, and the essential social skills needed to seek and secure employment [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "Indeed, it has already been proven that training at job interviews with a virtual agent could improve the performance [17].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "For instance, in [29], a study shows that people who tried to suppress or hide negative emotions during a job interview are considered more competent by evaluators.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "Similarly, Tiedens [30] shows that anger and sadness play an important role in job interviews.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "Most existing models for virtual agents rely on a reactive approach, in which the system does not manipulate or reason on the mental states of the interlocutor [17, 22, 23].", "startOffset": 160, "endOffset": 172}, {"referenceID": 21, "context": "Most existing models for virtual agents rely on a reactive approach, in which the system does not manipulate or reason on the mental states of the interlocutor [17, 22, 23].", "startOffset": 160, "endOffset": 172}, {"referenceID": 22, "context": "Most existing models for virtual agents rely on a reactive approach, in which the system does not manipulate or reason on the mental states of the interlocutor [17, 22, 23].", "startOffset": 160, "endOffset": 172}, {"referenceID": 3, "context": "However, in human psychology, Theory of Mind (ToM) refers to the ability of human beings and primates to interpret, predict and even influence others\u2019 behavior [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 26, "context": "interlocutors, several models rely on the cognitive structure of emotions and appraisal theories such as CPM [27] or OCC [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "interlocutors, several models rely on the cognitive structure of emotions and appraisal theories such as CPM [27] or OCC [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "For instance, [1, 11] are BDI-based implementation of the OCC theory.", "startOffset": 14, "endOffset": 21}, {"referenceID": 10, "context": "For instance, [1, 11] are BDI-based implementation of the OCC theory.", "startOffset": 14, "endOffset": 21}, {"referenceID": 2, "context": "FAtiMA\u2019s double appraisal model [3], although not implemented using a BDI framework, also encodes the OCC model.", "startOffset": 32, "endOffset": 35}, {"referenceID": 15, "context": "[16, 11]) that show that BDI is a good basis to represent and to reason about the interlocutor\u2019s mental state.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[16, 11]) that show that BDI is a good basis to represent and to reason about the interlocutor\u2019s mental state.", "startOffset": 0, "endOffset": 8}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "The simulation-theory (ST) [13] defends a mirroring or projection process allowing for taking someone else\u2019s perspective.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "Various research demonstrated that neither pure TT nor pure ST were realistic [31] and both theorists and simulationists turn toward more hybrid models [8][13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "Various research demonstrated that neither pure TT nor pure ST were realistic [31] and both theorists and simulationists turn toward more hybrid models [8][13].", "startOffset": 152, "endOffset": 155}, {"referenceID": 12, "context": "Various research demonstrated that neither pure TT nor pure ST were realistic [31] and both theorists and simulationists turn toward more hybrid models [8][13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 2, "context": "[3] that relies on a ST approach, or [7] and [25] that position in the TT) or implement them separately as in [15].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[3] that relies on a ST approach, or [7] and [25] that position in the TT) or implement them separately as in [15].", "startOffset": 37, "endOffset": 40}, {"referenceID": 24, "context": "[3] that relies on a ST approach, or [7] and [25] that position in the TT) or implement them separately as in [15].", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "[3] that relies on a ST approach, or [7] and [25] that position in the TT) or implement them separately as in [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "1 Syntax Assume finite sets of atomic propositions ATM , physical actions ACT , illocutionary (speech) acts ILL, agents AGT , emotions EMO (which is a subset of the twenty two OCC emotions in our model), and the intervals of real numbers DEG = [\u22121, 1] and DEG = [0, 1].", "startOffset": 262, "endOffset": 268}, {"referenceID": 19, "context": "This representation is similar to the one in [20] except we associate a subjective degree of plausibility as is usually done in BDI models and we do not distinguish actions from communcation.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "Like, Dom, Bel, Att and Int are modal operators and N , and U are temporal operators Next and Until from LTL and CTL* [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "For the representation of social relation [18], Likea,b determines the level of liking agent a has for agent b, whileDoma,b represents the degree of dominance.", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "Bel a(\u03c6) is a graded belief, in a similar manner to [11], and has to be read \u201ca believes that \u03c6 with certainty l\u201d.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "In our context, this operator will be used to cover various notions, such as Desires, Ideals and Goals that are represented with distinct modal operators in other work such as [1] and [14].", "startOffset": 176, "endOffset": 179}, {"referenceID": 13, "context": "In our context, this operator will be used to cover various notions, such as Desires, Ideals and Goals that are represented with distinct modal operators in other work such as [1] and [14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 25, "context": "As in classical BDI, Inta(\u03c6) represents an agent\u2019s plan [26] and has to be read \u201da intends to make \u03c6 true\u201d (with \u03c6 being an event in the general case).", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "Following classical literature [12], our emotions are related to facts and can be directed toward an agent.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Unlike [1, 14], we do not consider an agent responsible for a situation it could have avoided.", "startOffset": 7, "endOffset": 14}, {"referenceID": 13, "context": "Unlike [1, 14], we do not consider an agent responsible for a situation it could have avoided.", "startOffset": 7, "endOffset": 14}, {"referenceID": 10, "context": "1 Graded beliefs Following [11] and [1], all accessibility relations B are transitive and euclidean, which ensures that the agent is aware of its own beliefs: Bel a(\u03c6) def =\u21d2 Bel a(Bel a(\u03c6)) (4)", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "1 Graded beliefs Following [11] and [1], all accessibility relations B are transitive and euclidean, which ensures that the agent is aware of its own beliefs: Bel a(\u03c6) def =\u21d2 Bel a(Bel a(\u03c6)) (4)", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "However, unlike other models [1] [11], B is not serial.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "However, unlike other models [1] [11], B is not serial.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "Following the BDI model [26], goals are defined as desires that are consistent \u2013 at least weakly, in our case \u2013 and believed to be achievable.", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "or, similarly to [7], because the agent strongly believes there is \u2013 at least \u2013 one mean to achieve it:", "startOffset": 17, "endOffset": 20}, {"referenceID": 19, "context": "In our model, following [20, 10, 3], the attitude is influenced not only by new beliefs, but also by the attitude of others and the social relation.", "startOffset": 24, "endOffset": 35}, {"referenceID": 9, "context": "In our model, following [20, 10, 3], the attitude is influenced not only by new beliefs, but also by the attitude of others and the social relation.", "startOffset": 24, "endOffset": 35}, {"referenceID": 2, "context": "In our model, following [20, 10, 3], the attitude is influenced not only by new beliefs, but also by the attitude of others and the social relation.", "startOffset": 24, "endOffset": 35}, {"referenceID": 27, "context": "Although our work mostly focus on non-verbal communication, we consider a limited set of illocutionnary acts [28] ILL = {Assert, Request, Commit, Express}.", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Based on similar work in speech acts formalization [16, 14], we define trigering rules for our speech acts.", "startOffset": 51, "endOffset": 59}, {"referenceID": 13, "context": "Based on similar work in speech acts formalization [16, 14], we define trigering rules for our speech acts.", "startOffset": 51, "endOffset": 59}, {"referenceID": 15, "context": "In turn, these events will lead to new mental states for the recipient agent, similarly to classical FIPA semantics and existing work on social interaction modeling [16, 10].", "startOffset": 165, "endOffset": 173}, {"referenceID": 9, "context": "In turn, these events will lead to new mental states for the recipient agent, similarly to classical FIPA semantics and existing work on social interaction modeling [16, 10].", "startOffset": 165, "endOffset": 173}, {"referenceID": 0, "context": "In this implementation, we have used an OCC-based model, highly inspired by [1, 14, 11].", "startOffset": 76, "endOffset": 87}, {"referenceID": 13, "context": "In this implementation, we have used an OCC-based model, highly inspired by [1, 14, 11].", "startOffset": 76, "endOffset": 87}, {"referenceID": 10, "context": "In this implementation, we have used an OCC-based model, highly inspired by [1, 14, 11].", "startOffset": 76, "endOffset": 87}, {"referenceID": 0, "context": "The degree of desirability can correspond to desirability-forself but also to praiseworthiness [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 20, "context": "In OCC [21], Gratification, Remorse, Gratitude and Anger are defined as Well-being/Attribution emotions, triggered when one focuses both on the praiseworthiness of an action and on its desirability.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "Nevertheless, similarly to [14] we think that one might distinguish Gratitude and Anger from Admiration and Reproach if the triggering state of affairs corresponds to a goal, that is to say it is not only praiseworthy but is also desirable and consistent enough to generate an intention of achievement.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "Yet, the purpose of our current work in the TARDIS project [2] is to develop a training game in order to facilitate NEETs\u2019 access to employment.", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "Indeed, this sort of application appears as a promising way to increase applicant\u2019s self-confidence [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "The 2l\u22121 facto is used to adjust the value in [0,1] before we compute the intensity, which is then readjusted in [0.", "startOffset": 46, "endOffset": 51}, {"referenceID": 1, "context": "The perception of \u201chesitation\u201d and \u201cfocused\u201d is done by another module of the TARDIS platform which is not part of this paper (see [2]).", "startOffset": 131, "endOffset": 134}, {"referenceID": 0, "context": "The valence of the agent\u2019s affective state and its runtime evaluation of the candidate\u2019s self-confidence, motivation and qualification (values in [-1,1]) were represented by slide-bars.", "startOffset": 146, "endOffset": 152}, {"referenceID": 0, "context": "Besides, a series of 8 sliders (values in [0,1]) gives them the possibility to express their affective states to the recruiter as combinations of the following affects: relieved (REL), embarrassed (EMB), hesitating (HES), stressed (STR), ill at ease (IAE), focused (FOC), aggressive (AGG) and bored (BOR).", "startOffset": 42, "endOffset": 47}, {"referenceID": 31, "context": "In the full TARDIS project\u2019s setting, these sliders are replaced by automatic recognition of user affects using the SSI system [32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "We assume that using the full TARDIS project\u2019s setting [2] would enhance the interaction credibility and help highlight the virtual agent\u2019s reasoning and reactivity.", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": "In the litterature, there are validated methods to evaluate whether subjects \u2013 generally children \u2013 have ToM abilities and use it [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 14, "context": "From the computational point of view, [15] points out the issue of evaluating a ToM model.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "Similarly, [25] builds expectations about user\u2019s actions \u2013 based on formal models in the specific context of wartime negotiations \u2013 in order to model a simplified theory of mind and then compare them with the actual user\u2019s behavior.", "startOffset": 11, "endOffset": 15}], "year": 2014, "abstractText": "Job interview simulation with a virtual agents aims at improving people\u2019s social skills and supporting professional inclusion. In such simulators, the virtual agent must be capable of representing and reasoning about the user\u2019s mental state based on social cues that inform the system about his/her affects and social attitude. In this paper, we propose a formal model of Theory of Mind (ToM) for virtual agent in the context of human-agent interaction that focuses on the affective dimension. It relies on a hybrid ToM that combines the two major paradigms of the domain. Our framework is based on modal logic and inference rules about the mental states, emotions and social relations of both actors. Finally, we present preliminary results regarding the impact of such a model on natural interaction in the context of job interviews simulation.", "creator": "LaTeX with hyperref package"}}}