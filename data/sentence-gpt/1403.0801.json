{"id": "1403.0801", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2014", "title": "Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring", "abstract": "Developments in the educational landscape have spurred greater interest in the problem of automatically scoring short answer questions. A recent shared task on this topic revealed a fundamental divide in the modeling approaches that have been applied to this problem, with the best-performing systems split between those that employ a knowledge engineering approach and those that almost solely leverage lexical information (as opposed to higher-level syntactic information) in assigning a score to a given response. This paper aims to introduce the NLP community to the largest corpus currently available for short-answer scoring, provide an overview of methods used in the shared task using this data, and explore the extent to which more syntactically-informed features can contribute to the short answer scoring task in a way that avoids the question-specific manual effort of the knowledge engineering approach. This paper will be updated by authors in the following days.", "histories": [["v1", "Tue, 4 Mar 2014 14:45:56 GMT  (33kb,D)", "https://arxiv.org/abs/1403.0801v1", null], ["v2", "Wed, 5 Mar 2014 14:50:06 GMT  (33kb,D)", "http://arxiv.org/abs/1403.0801v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["derrick higgins", "chris brew", "michael heilman", "ramon ziai", "lei chen", "aoife cahill", "michael flor", "nitin madnani", "joel tetreault", "daniel blanchard", "diane napolitano", "chong min lee", "john blackmore"], "accepted": false, "id": "1403.0801"}, "pdf": {"name": "1403.0801.pdf", "metadata": {"source": "CRF", "title": "Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring", "authors": ["Derrick Higgins", "Chris Brew", "Michael Heilman", "Ramon Ziai", "Lei Chen", "Aoife Cahill", "Michael Flor", "Nitin Madnani", "Joel Tetreault", "Daniel Blanchard", "Diane Napolitano", "Chong Min Lee", "John Blackmore"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This paper aims to demonstrate that \u201chigher-level\u201d linguistic features that encode information such as syntactic relations, topics referenced, and response structure can make a contribution to the accuracy and validity of automated methods of short answer scoring. Although the results of a recent shared task on short answer scoring seem to indicate that lexical features alone cannot be improved upon, a more thorough examination of the performance of models using different sorts of features tells a different story. In support of this goal, we also provide an\noverview of the ASAP short-answer scoring competition, which has gone largely unnoticed in the community of NLP researchers working on educational applications.\nResearch on using computers to score open-ended student responses has a long history, dating back to Ellis Page\u2019s work on automated scoring of essays (Page, 1966; Page, 1968). Since the very beginning of this research field, there has been an awareness that agreement with human raters is a limited evaluation measure. Page\u2019s work demonstrated that the length of an essay correlates strongly with human ratings. Such superficial measures can sometimes do surprisingly well as predictive mechanisms, despite the fact that they are only marginally related to the skills and attributes we aim to measure with a writing task (the test construct).\nGiven a sample of scored test-taker responses it is possible to identify many potentially measurable linguistic features that correlate well with score. Some of these features rely on advanced natural language processing, but many do not. Given the redundancy of information encoded in many of these features, and the difficulty of reliably measuring features that depend on advanced NLP, it is tempting to focus attention on superficial features that are easy to extract, and to hope that the redundancy will allow good prediction. However, a system that relies on superficial features as proxies for important underlying attributes will fail when it begins to see answers in which the measureable surface features are no longer correlated with the underlying attributes. Unfortunately, such answers are exactly what is to be expected when a sophisticated test-taking comar X\niv :1\n40 3.\n08 01\nv2 [\ncs .C\nL ]\n5 M\nar 2\nmunity begins to analyse the test in the search for simple ways to get good scores. Therefore, it is important to understand the potential of deeper features even when their predictive contribution to scoring in a research setting is limited.\nThe field of automated essay scoring has made advances in the intervening years, allowing the development of features related to various aspects of the writing construct, including lexical sophistication, discourse structure, syntactic variety, and grammatical accuracy (cf. Landauer et al. (2003); Elliot (2003); Attali and Burstein (2006); Attali et al. (2010); Yannakoudakis et al. (2011); Foltz et al. (2011)). The addition of these features has not only improved the conceptual basis of scoring but also improved the accuracy of these systems according to traditional evaluation metrics.\nOther automated scoring tasks have not yet progressed to the same level of maturity. In particular, not as much work has been focused to date on automated scoring of short-answer questions. Such questions are distinguished from essays by their brevity (eliciing responses of only a few words or a few sentences), and by the fact that they are scored according to response content, rather than quality of written expression. The scoring rubrics for shortanswer questions often require specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) to be included in a response for it to receive credit.\nThe task of short-answer scoring has received more attention recently, however, because shortanswer questions are expected to figure prominently in new, computerized state tests currently under development with Race To the Top funding from the US Department of Education. As proved to be the case for the automated essay scoring task, results on the recent ASAP short answer scoring task (described later in Section 2.1) have demonstrated that superficial features (in this case, features related to the use of particular words in a response) are strongly predictive. We aim to re-examine the contribution of different sorts of predictive features on the same dataset of short-answer tasks on which these results were achieved, and demonstrate that attention to linguistic structure is empirically valuable in automated scoring of short answers."}, {"heading": "2 Previous Work", "text": "Like the field of automated essay scoring, research on methods for automated scoring of short answer questions has a history that spans multiple decades. As early as 1988, Carlson & Ward examined the potential use of natural language processing for the \u201cformulating hypotheses\u201d task, a new item type under consideration for the GRE test that would ask students to list all of the possible explanations they could think of that would account for some observed phenomenon (for example, a steady reduction in the mortality rate for a particular population). While this is a somewhat unique item type, but it is quite similar in its fundamental scoring characteristics (the fact that it is scored according to the correctness or semantic appropriateness of a short, textual unit) to many other \u201cshort answer\u201d tasks that have been considered more recently.\nResearch on automated scoring of short-answer tasks continued at the Educational Testing Service during the early 1990s (Kaplan, 1991; Kaplan and Bennett, 1994; Burstein and Kaplan, 1995), and received broader attention in the early 2000s, when a number of short answer scoring systems were developed, including ETS\u2019 c-rater (Leacock and Chodorow, 2003), AutoMark (Mitchell et al., 2002), the Intelligent Essay Assessor (Landauer et al., 2003), the Oxford-UCLES system (Sukkarieh and Pulman, 2005), and applications developed at the University of Portsmouth (Callear et al., 2001) and the University of Manchester (Sargeant et al., 2004). Some approaches to the task have relied heavily on knowledge engineering, involving manual creation of patterns to encapsulate correct answer types for particular questions (Callear et al., 2001; Sukkarieh and Pulman, 2005). Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; Pe\u0301rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012). Hybrid systems have also been developed, in which some human involvement is required for task-specific pattern creation or annotation, but other components of the system use automatically-constructed features and statistical calibration (Mitchell et al., 2002; Leacock and\nChodorow, 2003; Nielsen et al., 2008). There has been a shift over time towards more fully-automated and statistically-based systems, and away from those relying on manual knowledge engineering, but the selection of methodology also depends on the exact type of short answer questions targeted by each system. For instance, the tasks addressed by Mitchell et al. (2002) required answers to include specific welldefined concepts (see Figure 1), and were therefore more amenable to a knowledge engineering approach, whereas those addressed by Foltz et al. (2011) elicited longer, less-constrained responses (see Figure 2), and were scored according to the evidence students gave of their \u201cdepth of knowledge\u201d, rather than for specific, correct concepts.\nSome of these systems have recently seen operational use for scoring consequential tests. Foltz (2010) reported that Pearson\u2019s Intelligent Essay Assessor was being used to score science questions on the Maryland State Assessment. Leacock and Chodorow (2003) also cite the use of ETS\u2019 c-rater in a state assessment context. More opportunities for the use of such systems in consequential testing systems are likely to emerge in coming years, as well, as more state tests move from paper-and-pencil administration to online formats, and as new multi-state tests are developed. Two state consortia (known\nas PARCC1 and Smarter Balanced2) have received funding from the US Department of Education to develop next-generation tests that can be used in multiple states, and incorporate innovative technology to address a new set of standards for what children at different grades should know and be able to do (the Common Core State Standards3). These tests are slated to be launched in the 2014-2015 school year, and have explicitly included the automated scoring of open-ended tasks as one of their design desiderata.\nPartly as a result of this increased commercial interest in the automated scoring of short-answer questions, recent efforts have arisen to empirically assess the state of the art in this field, and to compare the performance of available systems. One of these is the \u201cJoint Student Response Analysis and 8th Recognizing Textual Entailment Challenge\u201d at SEMEVAL-2013, which added a new corpus of student answers from the tutorial dialog context to the set of textual entailment evaluation corpora (Dzikovska et al., 2012).\nThe second organized effort to compare methods for short-answer scoring is the The Automated Student Assessment Prize, the outcomes of which directly motivate the current study, and from which the data used here are drawn."}, {"heading": "2.1 The Automated Student Assessment Prize", "text": "The Automated Student Assessment Prize (ASAP) is an effort funded by the Hewlett Foundation, and organized by Open Educational Solutions, to assess the current state of capabilities for automated scoring of a wide range of open-ended student response tasks. The short-answer scoring competition conducted in 2012 is the second in a series of such evaluations, with the first phase having focused on essay-length responses (Shermis and Hamner, 2012), and future phases under discussion. For both phases of the ASAP competition, states participating in the multi-state assessment consortia mentioned above have cooperated by providing student responses to use as data. The set of responses these states have shared through the ASAP competition is the largest publicly-available dataset for develop-\n1http://smarterbalanced.org/ 2http://parcconline.org/ 3http://corestandards.org/\ning and evaluating automated methods for scoring of short-answer questions to date.\nThe organizers elected to host the competition through Kaggle4, an online platform for posting and running competitions (which typically involve computational modeling). The Hewlett Foundation provided a $100,000 prize fund for the competition, and the contest rules required that participants release all model submissions under open-source licenses in order to be eligible for the cash prizes. The contest training data was released in late June, 2012, and the 256 participating teams had approximately 9 weeks in which to develop their scoring methods. Once the test data was released (without human scores), participants had approximately one week to compute and submit their final score predictions. All score predictions were required to be rounded to integers. Most individuals or teams participating in the competition did so under pseudonyms, so it is not entirely clear what areas of expertise might have been reflected in the field. However, since the tasks hosted on Kaggle typically do not involve text processing, it is likely that many participants had more experience with machine learning and applied statistics training than with natural language processing.\nThe final rankings of prize-winning teams participating in the ASAP short-answer scoring challenge are listed in Table 1, ranked by final weighted kappa on the test set averaged across questions. The top prize winner, Luis Tandalla, invested a great deal of manual effort in developing regular expressions to cover many key concepts that were referenced in student responses, and then trained a top-level regression model to score responses based on the presence of these concepts and the predictions of base models using lexical features only (Tandalla, 2012). The remaining four prize winners developed models that were quite similar in structure to one another, also using a stacked prediction architecture. Each of these models used low-level classifiers to model the scores of responses based on lexical features (the presence of particular words, stems, ngrams, or character sequences), and the predictions of these classifiers were combined using one or more higher-level models (Zbontar, 2012; Conort, 2012; Jesensky, 2012; Peters and Jankiewicz, 2012). Some\n4http://kaggle.com\nmodels incorporated other features in addition to the standard base learners using lexical information, as indicated in Table 1.\nThese results demonstrate a number of points about the short answer scoring task, at least as presented in ASAP:\n1. Lexical features\u2014which specific words and word sequences are used in a response\u2014are strongly predictive of human scores, and can predict these scores even without other features.\n2. A predictive model architecture using stacked classifiers (Wolpert, 1992) seems to work well for this task\u2014perhaps because providing different \u201cviews\u201d of the feature set helps to address the sparsity of lexical features.\n3. Model performance can be improved somewhat by investing a great deal of effort to model specific response patterns (with regular expressions, for example), although such an approach would not scale well to large numbers of questions.\nIn sum, the results of the ASAP competition suggest that relatively superficial methods of automated short response scoring (leveraging lexical features alone), can do quite well, and that while it is possible to improve somewhat on such models, the methods needed to do so are not scalable in any case. The purpose of this paper is to more closely examine the potential contribution of syntactically informed features, that attempt to model response patterns above the lexical level, and to investigate whether such features can be developed in a generic way, so that manual work is not required to support scoring responses to each new question."}, {"heading": "3 Data", "text": "The data set used in this study was the same one used in the ASAP short answer scoring challenge described above. The questions and student responses in this data set were contributed by multiple state education departments, although the names of the participating states were not provided by the organizers of the competition. The organizers did report that students\u2019 responses to some questions were entered on a computer, while responses to other questions were provided in handwritten form (and later transcribed).\nThe data set contains ten different short-answer questions, differing in characteristics such as subject area, average response length, and scoring scale, as shown in Table 2. All of the ASAP questions were administered at Grade 10 in the US, except for question 10, which was administered in Grade 8. For each question, a rubric is also provided, which outlines the criteria for assigning scores to responses.5\n5More detailed information about the ASAP questions and scoring guidelines is posted on the Kaggle site for the competition: http://kaggle.com/c/asap-sas/data/.\nThe ASAP short answer data set is an important new resource for researchers working on analyzing short-answer questions responses. However, a few points are worth mentioning as context for the results of any studies using the corpus. First, the data capture process for one question resulted in an artifact into the responses that cannot be straightforwardly reversed. Space characters have been inserted, seemingly at random, into the responses to question 10, often creating artificial breaks within words. Second, a small number of responses seem to have been truncated, leaving only a short initial substring in place of the complete student response. (This speculation is based upon the observation that certain very short, seemingly incomplete responses received very high scores from both\nThe organizers report that each response was scored by two independent human raters, and Table 2 also reports the agreement between these raters (on the training set) using the quadratic weighted kappa measure widely used in studies of open-ended response scoring (cf. Attali et al. (2010); Williamson et al. (2012)).\nThe responses to each ASAP task were divided by the organizers into three partitions. The training partition contains approximately 60% of the responses to each question, and was intended to be used for direct parametrization of automated scoring models developed for the challenge. Participants were provided with both sets of human scores to all responses in the training partition. The leaderboard partition contains another 20% of the responses. The texts of the leaderboard responses were available to participants throughout the challenge, but the human scores for these responses were not. However, participants could submit their score estimates for leaderboard responses through a web interface (at most, twice per day) to learn how well these estimates agreed with human scores on the leaderboard set, and to be ranked on a publicly visible \u201cleaderboard\u201d on this basis. Finally, the remaining 20% of the responses made up the test partition, which was provided to participants without human scores at the very end of the competition, and which was used as the basis for final rankings of systems."}, {"heading": "4 Model", "text": "The features described in the following section were used to train a variety of different regression models to estimate the score to be assigned to each response:\n\u2022 simple least-squares linear regression,\n\u2022 ridge regression,\n\u2022 support vector regression (with an RBF kernel),\n\u2022 random forests, and\n\u2022 gradient boosting. raters.) Finally, note that the human agreement reported here is substantially higher for some tasks than has been observed in previously-reported studies using short-answer questions. More information about states\u2019 scoring practices would help to clarify whether the two human ratings of these responses were truly provided independently of one another in all cases.\nThe parameters used in model training were determined by cross-validation within the training set.\nThis setup is consistent with the popular modeling approach of stacking used in the ASAP competition, in which individual predictive features used by the top-level model are themselves the outputs of classification or regression models (as are many of the features described in Section 5). And as many participants chose to do in the ASAP competition, we have also included a model variant which uses an ensemble as the top-level model, in which the unrounded outputs of all five of the simple regression models are averaged to produce the final ensemble prediction. For all models, rounding of scores is done as the final step."}, {"heading": "5 Features", "text": ""}, {"heading": "5.1 BASE Features", "text": "The four classes of features described in this section constitute the \u201cBASE\u201d set, which is intended to be representative of the features commonly included in high-performing models from the ASAP challenge."}, {"heading": "5.1.1 Bag of Words Features", "text": "The first set of base features included in our stacking classifier is a feature that is itself the output of a model trained to predict human scores based on the presence of specific words. Such a bag-of-words model using random forest regression was provided by the competition organizers as a baseline for the ASAP challenge.\nUsing the same methodology as the simple bag-\nof-words models, we also included two features based on bags of character ngrams. Character ngrams tend to be useful for this task, because they capture lexical information in a way that is insensitive to spelling errors.\nFinally, we include two bag-of-stems features, trained using the same method as above, but with all words pre-processed using the Porter stemming algorithm.\nThese three feature sets together will be referred to below as Bag of Words (BOW) features."}, {"heading": "5.1.2 LDA Features", "text": "For each question, two LDA topic spaces were constructed using the MALLET toolkit (McCallum, 2002), one with 30 dimensions, and one with only ten. The weight of each of these 40 topics for a given response was used as a feature. These features are meant to be roughly comparable to the reduceddimensionality lexical features used by ASAP participants."}, {"heading": "5.1.3 Well-formedness Features", "text": "Five features were used to represent the degree to which each response consisted of well-edited, grammatical English text. These features were based on the count of errors identified in four different categories, as identified automatically by a system for grammatical error detection (Attali and Burstein, 2006), as well as a feature representing the sum across error categories."}, {"heading": "5.1.4 Length Features", "text": "Three features were used to represent length: the number of characters, words, and sentences in each response."}, {"heading": "5.2 Syntactically-informed Features", "text": "The four additional categories of features described in this section are more \u201csyntactically-informed\u201d than those in the BASE set, because they encode information about word sequences, syntactic units or discourse units."}, {"heading": "5.2.1 Ngram Features", "text": "This feature class included three \u201cbag-of-ngram\u201d features analogous to the bag-of-words features described above. Each included the top 1000 most frequent unagrams, bigrams and trigrams in a regression model trained to predict the human score. The three feature variants used random forests, support vector machines, and ridge regression."}, {"heading": "5.2.2 Language Model Features", "text": "Three features were included based on language models built using the IRST Language Modeling Toolkit (Federico et al., 2008). Language models were trained on responses to each question that received the highest score, one of the two highest scores, and the score of zero. The perplexity of a response with respect to each of these models was used as a feature."}, {"heading": "5.2.3 Dependency Features", "text": "This feature class included six \u201cbag-ofdependency\u201d features. Each included the top 1000 most frequent dependency triples (and possibly other items; see below) extracted from responses in the training set using the Stanford parser (Klein and Manning, 2003) in a regression model trained to predict the human score. The feature variants included models incorporating dependencies only, dependencies and single words, and dependencies combined with \u201cpartial\u201d dependencies (a lexical head associated with a dependency relation, but not the other head with which it is associated). Each of the three variants was implemented with both random forests and support vector machines, yielding a total of six features.\n5.2.4 k-Nearest Neighbors features Two features were developed that relate to the similarity of word sequences used to respond to a question by different examinees. To represent the similarity between responses, we used a symmetric version of the BLEU score ( BLEU(x,y)+BLEU(y,x)\n2\n) .\nWe then defined two features based on the set of eight training responses most similar to a given response: the average score of these eight nearest neighbors, and the distance-weighted average score."}, {"heading": "5.2.5 Discourse Segment Features", "text": "This final feature class included five features based on a system for segmenting short-answers into meaningful sub-units based on regular expressions. This model parses out sentences or clauses set off using bullets, numbering, or discourse connectives such as furthermore or however. The features used are the count of the total number of discourse units identified, the length of any numbered list identified, the highest number identified associated with a numbered bullet, and the number of discourse units headed by discourse markers in two categories (the finally category indicating a conclusion, and the furthermore category indicating supplemental information)."}, {"heading": "6 Experiments", "text": "First, in order to demonstrate the strong performance of the ensemble model on this data set relative to single top-level regression models, we trained each model on the BASE feature set (cf. Section 5.1). As Table 3 indicates, the ensemble of regressors outperforms any individual model on two of the three metrics, and is barely edged by gradient boosting on to the third metric.\nThe results reported in Table 3 are aggregated across all ten ASAP questions, using three different metrics. The average correlation of unrounded predicted scores with human scores provides the most precise measure of model performance, since information is lost in the rounding process, and in many testing contexts an unrounded item score could be used directly to construct total test scores. The average quadratic weighted kappa, computed using rounded predictions, is provided for comparison. Finally, we also report the \u201cofficial\u201d metric used in the\nASAP short-answer scoring competition, in which weighted kappas for each task are manipulated with the Fisher transform before averaging:\nz =\n\u2211 i=1...10 1 2 ln 1+\u03bai 1\u2212\u03bai\n10\n\u03baaggregate = e2z \u2212 1 e2z + 1\nIn order to determine the effect of different feature categories on overall performance, we then compared variants of the ensemble model using the BASE feature set with variants that employed more features, individually adding in each of the other feature categories described above. These results, as well as the performance results for a model including ALL available features, are indicated in Table 4.\nAs Table 4 shows, each of the new features added to the BASE set adds some incremental predictive\npower to the model, except for the bag-of-ngrams features, which slightly degrade performance vs. the baseline. The greatest increase in performance for a single feature class comes from the inclusion of dependency-based features, which yields a modest increase in all three measures. The addition of all five features to the BASE feature set increases the overall weighted kappa using the ASAP contest metric by approximately 0.009 to 0.768. This increase may be small, but it is not negligible, considering the magnitude of differences between top-ranked participants. (The difference between places 1 and 10 in the final ranking of participants on the public leaderboard was only 0.011.) Note that the BASE feature set performs slightly better than a model using only bag-of-words features (including features based on stems and character ngrams).\nFigure 3 shows the performance of the ensemble model across all ten ASAP questions, as well as the cross-task performance of two top-ranked systems from the competition7. The agreement of predicted scores and human scores varies both according to the reliability of human scoring (cf. Table 2) and according to the difficulty of modeling the distinctions made in the scoring rubrics for each question. For example, questions 7 and 8 call for students to draw conclusions based on a reading passage, and to support those conclusions with examples. In part because of the supra-lexical semantic requirements this imposes on responses, the reliability of automated scoring lags behind that of human scoring for these items. By contrast, questions 5 and 6 ask students to list specific biological concepts or processes, and for these items automated scoring performance is higher both in absolute terms and relative to human scoring."}, {"heading": "7 Conclusions and Discussion", "text": ""}, {"heading": "7.1 Performance relative to ASAP systems", "text": "The first point to note in connection with the results reported above is that the final ensemble model\u2019s prediction accuracy on the ASAP leaderboard data is lower than the highest scores posted during the competition itself. The top final weighted kappa score on the ASAP public leaderboard was 0.772, compared\n7Since participants made their code available as open source, we were able to reproduce their models and generate more detailed statistical evaluations.\nQuadratic Weighted Kappa by Question\nto our result of 0.768. There are a number of reasons for this.\nFirstly, stacking models such as our ensemble model and the models applied by the top-ranked ASAP participants have a number of free parameters that can be iteratively manipulated in order to improve the models\u2019 performance. Learning parameters of the base-level and top-level regression models can be tweaked, features can be added or removed, and feature variants can be introduced in order to optimize prediction results. Since ASAP participants had the opportunity to submit predictions to the Kaggle server twice a day for two months (receiving performance measures on the leaderboard set each time), there was ample opportunity to fit these parameters to the leaderboard data. In contrast, the models described here were not optimized in any way using information from the leaderboard set. While the ASAP challenge also had an independent test set (the \u201cprivate leaderboard\u201d), the human scores on that set have not been released publicly, so it could not be used for the current study.\nThe second reason why the ensemble model\u2019s per-\nformance trails that of the top ASAP models is that it uses exactly the same features and parameterization across all ten questions. Because our aim was to examine the contribution of different feature categories to overall scoring accuracy, rather than to achieve the highest possible scoring accuracy through model optimization, we did not optimize the configuration of the model to each question individually (as most ASAP participants did).\nFinally, in contrast to some ASAP systems, our ensemble model does not use any special method for rounding of scores; scores are simply truncated to the allowable range, and rounded to the nearest integer. As noted above, unrounded scores are more appropriate for most testing purposes."}, {"heading": "7.2 Role of syntactically-informed features", "text": "The question this paper set out to address was to reexamine the seeming lesson of the ASAP short answer scoring challenge, that lexical features alone could not be improved upon except by investing considerable effort in task-specific knowledge engineering. The results presented in Table 4 above indi-\ncate that more syntactically-informed features can, indeed, contribute to improved short-answer scoring performance. The use of all new features together improves the aggregate weighted kappa by about 0.009, with the greatest increase coming from the incorporation of syntactic dependency information, the feature class with the clearest link to syntax. This is particularly remarkable given that many student answers contain numerous misspellings and grammatical errors, making them difficult to parse reliably.\nThe incremental predictive value of these features was modest, but there may be motivation for including such features in short-answer scoring systems beyond their empirical effects on scoring accuracy. Since the scoring rubrics for these items claim to be sensitive to characteristics of responses beyond simple word choice, including higher-level features will improve the systems\u2019 validity, the degree to which it will actually measure the skills and knowledge that items are supposed to test. If students are not able to get a high score simply by producing a response that includes relevant vocabulary, this will reduce the risk of negative washback, or unproductive learning strategies directed solely toward optimizing test performance."}, {"heading": "7.3 Future work", "text": "There are a number of areas in which this work could be extended. First, of course, the use of syntactic dependencies does not come close to exhausting the space of feature types that more directly reflect the syntactico-semantic relationships that short answers are supposed to encode. Features based on semantic role labeling and paraphrase detection, for example, may offer additional benefits. There may also be some gains to be had from tailoring features to a particular content area (for example, Biology or reading comprehension).\nAnother important question to address is how the particular setting of the ASAP task affects the results achieved. Parameters such as training sample size, the number of tasks to be modeled concurrently, and the content areas to be scored, could significantly influence the findings."}], "references": [{"title": "Automated essay scoring with e-rater v.2", "author": ["Attali", "Burstein2006] Yigal Attali", "Jill Burstein"], "venue": "Journal of Technology, Learning, and Assessment,", "citeRegEx": "Attali et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Attali et al\\.", "year": 2006}, {"title": "Performance of a generic approach in automated essay scoring", "author": ["Attali et al.2010] Yigal Attali", "Brent Bridgeman", "Cathy Trapani"], "venue": "Journal of Technology, Learning, and Assessment,", "citeRegEx": "Attali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Attali et al\\.", "year": 2010}, {"title": "GE FRST evaluation report: How well does a statistically-based natural language processing system score natural language constructed responses", "author": ["Burstein", "Kaplan1995] Jill Burstein", "Randy M. Kaplan"], "venue": "ETS Research Report No. RR-95-29", "citeRegEx": "Burstein et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Burstein et al\\.", "year": 1995}, {"title": "CAA of short non-MCQ answers", "author": ["Jenny Jerrams-Smith", "Victor Soh"], "venue": "In Proceedings of the 5th International CAA Conference,", "citeRegEx": "Callear et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Callear et al\\.", "year": 2001}, {"title": "A new look at formulating hypotheses items", "author": ["Carlson", "Ward1988] Sybil B. Carlson", "William C. Ward"], "venue": "GRE Board Professional Report No. 85-14P; ETS Research Report", "citeRegEx": "Carlson et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 1988}, {"title": "Towards effective tutorial feedback for explanation questions: A dataset and baselines", "author": ["Rodney D. Nielsen", "Chris Brew"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter", "citeRegEx": "Dzikovska et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dzikovska et al\\.", "year": 2012}, {"title": "Intellimetric: From here to validity", "author": ["Scott Elliot"], "venue": null, "citeRegEx": "Elliot.,? \\Q2003\\E", "shortCiteRegEx": "Elliot.", "year": 2003}, {"title": "IRSTLM: an open source toolkit for handling large scale language models", "author": ["Nicola Bertoldi", "Mauro Cettolo"], "venue": "In Proceedings of Interspeech", "citeRegEx": "Federico et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Federico et al\\.", "year": 2008}, {"title": "AI scoring MSA science. Paper presented at the meeting of the Maryland Assessment Group. Ocean City, MD", "author": ["Foltz", "Lochbaum2010] Peter Foltz", "Karen Lochbaum"], "venue": null, "citeRegEx": "Foltz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 2010}, {"title": "Analysis of student writing for a large scale implementation", "author": ["Foltz et al.2011] Peter Foltz", "Karen Lochbaum", "Mark Rosenstein"], "venue": null, "citeRegEx": "Foltz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 2011}, {"title": "Evaluating the meaning of answers to reading comprehension questions: A semantics-based approach", "author": ["Hahn", "Meurers2012] Michael Hahn", "Detmar Meurers"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,", "citeRegEx": "Hahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hahn et al\\.", "year": 2012}, {"title": "Using the free-response scoring tool to automatically score the formulatinghypotheses item", "author": ["Kaplan", "Bennett1994] Randy M. Kaplan", "Randy E. Bennett"], "venue": "ETS Research Report No", "citeRegEx": "Kaplan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kaplan et al\\.", "year": 1994}, {"title": "Using a trainable pattern-directed computer program to score natural language item responses", "author": ["Randy M. Kaplan"], "venue": "ETS Research Report No", "citeRegEx": "Kaplan.,? \\Q1991\\E", "shortCiteRegEx": "Kaplan.", "year": 1991}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Automated scoring and annotation of essays with the Intelligent Essay Assessor", "author": ["Darrell Laham", "Peter W. Foltz"], "venue": null, "citeRegEx": "Landauer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 2003}, {"title": "C-rater: Scoring of short-answer questions", "author": ["Leacock", "Chodorow2003] Claudia Leacock", "Martin Chodorow"], "venue": "Computers and the Humanities,", "citeRegEx": "Leacock et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 2003}, {"title": "Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu", "author": ["Andrew Kachites McCallum"], "venue": null, "citeRegEx": "McCallum.,? \\Q2002\\E", "shortCiteRegEx": "McCallum.", "year": 2002}, {"title": "Evaluating answers to reading comprehension questions in context: Results for German and the role of information structure", "author": ["Ramon Ziai", "Niels Ott", "Janina Kopp"], "venue": "In Proceedings of the TextInfer 2011 Workshop on Tex-", "citeRegEx": "Meurers et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Meurers et al\\.", "year": 2011}, {"title": "Towards robust computerized marking of free-text responses", "author": ["Terry Russell", "Peter Broomhead", "Nicola Aldridge"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2002}, {"title": "Learning to grade short answer questions using semantic similarity measures and dependency graph alignments", "author": ["Razvan Bunescu", "Rada Mihalcea"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Compu-", "citeRegEx": "Mohler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohler et al\\.", "year": 2011}, {"title": "Classification errors in a domain-independent assessment system", "author": ["Wayne Ward", "James H. Martin"], "venue": "In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Nielsen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2008}, {"title": "The imminence of grading essays by computer", "author": ["Ellis B. Page"], "venue": "Phi Delta Kappan,", "citeRegEx": "Page.,? \\Q1966\\E", "shortCiteRegEx": "Page.", "year": 1966}, {"title": "The use of the computer in analyzing student essays", "author": ["Ellis B. Page"], "venue": "International Review of Education,", "citeRegEx": "Page.,? \\Q1968\\E", "shortCiteRegEx": "Page.", "year": 1968}, {"title": "About the effects of combining latent semantic analysis with natural language processing techniques for free-text assessment", "author": ["P\u00e9rez et al.2005] Diana P\u00e9rez", "Enrique Alfonseca", "Pilar Rodr\u0131\u0301guez", "Alfio Gliozzo", "Carlo Strapparava", "Bernardo Magnini"], "venue": null, "citeRegEx": "P\u00e9rez et al\\.,? \\Q2005\\E", "shortCiteRegEx": "P\u00e9rez et al\\.", "year": 2005}, {"title": "The William and Flora Hewlett Foundation Automated Student Assessment Prize (ASAP). ASAP Short Answer Scoring Competition System Description", "author": ["Peters", "Jankiewicz2012] Jonathan Peters", "Pawe\u0142 Jankiewicz"], "venue": null, "citeRegEx": "Peters et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2012}, {"title": "A human-computer collaborative approach to the marking of free text answers", "author": ["Mary McGee Wood", "Stuart M. Anderson"], "venue": "In Proceedings of the 8th International CAA Conference,", "citeRegEx": "Sargeant et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sargeant et al\\.", "year": 2004}, {"title": "Contrasting state-of-the-art automated scoring of essays: Analysis", "author": ["Shermis", "Hamner2012] Mark D. Shermis", "Ben Hamner"], "venue": "In Paper presented at Annual Meeting of the National Council on Measurement in Education,", "citeRegEx": "Shermis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shermis et al\\.", "year": 2012}, {"title": "Information extraction and machine learning: Automarking short free text responses to science questions", "author": ["Sukkarieh", "Pulman2005] Jana Sukkarieh", "Stephen Pulman"], "venue": "In Proceedings of the 12th International Conference on Artificial", "citeRegEx": "Sukkarieh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sukkarieh et al\\.", "year": 2005}, {"title": "A framework for evaluation and use of automated scoring", "author": ["Xiaoming Xi", "F. Jay Breyer"], "venue": "Educational Measurement: Issues and Practice,", "citeRegEx": "Williamson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2012}, {"title": "A new dataset and method for automatically grading ESOL texts", "author": ["Ted Briscoe", "Ben Medlock"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Yannakoudakis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "Research on using computers to score open-ended student responses has a long history, dating back to Ellis Page\u2019s work on automated scoring of essays (Page, 1966; Page, 1968).", "startOffset": 150, "endOffset": 174}, {"referenceID": 22, "context": "Research on using computers to score open-ended student responses has a long history, dating back to Ellis Page\u2019s work on automated scoring of essays (Page, 1966; Page, 1968).", "startOffset": 150, "endOffset": 174}, {"referenceID": 9, "context": "Landauer et al. (2003); Elliot (2003); Attali and Burstein (2006); Attali et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 4, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 0, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al. (2010); Yannakoudakis et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 0, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al. (2010); Yannakoudakis et al. (2011); Foltz et al.", "startOffset": 51, "endOffset": 101}, {"referenceID": 0, "context": "(2003); Elliot (2003); Attali and Burstein (2006); Attali et al. (2010); Yannakoudakis et al. (2011); Foltz et al. (2011)).", "startOffset": 51, "endOffset": 122}, {"referenceID": 12, "context": "tasks continued at the Educational Testing Service during the early 1990s (Kaplan, 1991; Kaplan and Bennett, 1994; Burstein and Kaplan, 1995), and received broader attention in the early 2000s, when a number of short answer scoring systems were developed, including ETS\u2019 c-rater (Leacock", "startOffset": 74, "endOffset": 141}, {"referenceID": 18, "context": "and Chodorow, 2003), AutoMark (Mitchell et al., 2002), the Intelligent Essay Assessor (Landauer et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 14, "context": ", 2002), the Intelligent Essay Assessor (Landauer et al., 2003), the Oxford-UCLES system (Sukkarieh and Pulman, 2005), and applications developed at the University of Portsmouth (Callear et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 3, "context": ", 2003), the Oxford-UCLES system (Sukkarieh and Pulman, 2005), and applications developed at the University of Portsmouth (Callear et al., 2001)", "startOffset": 122, "endOffset": 144}, {"referenceID": 25, "context": "and the University of Manchester (Sargeant et al., 2004).", "startOffset": 33, "endOffset": 56}, {"referenceID": 14, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 23, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 19, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 17, "context": "Other approaches have aimed to use more generic text similarity features to determine the distance between students\u2019 responses and some \u201cgold standard\u201d answer or answers (Landauer et al., 2003; P\u00e9rez et al., 2005; Mohler et al., 2011; Meurers et al., 2011; Hahn and Meurers, 2012).", "startOffset": 170, "endOffset": 280}, {"referenceID": 16, "context": "For instance, the tasks addressed by Mitchell et al. (2002) required answers to include specific welldefined concepts (see Figure 1), and were therefore more amenable to a knowledge engineering approach, whereas those addressed by Foltz et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 8, "context": "(2002) required answers to include specific welldefined concepts (see Figure 1), and were therefore more amenable to a knowledge engineering approach, whereas those addressed by Foltz et al. (2011) elicited longer, less-constrained responses (see Figure 2), and were scored according to the evidence students gave of their \u201cdepth of knowledge\u201d,", "startOffset": 178, "endOffset": 198}, {"referenceID": 5, "context": "to the set of textual entailment evaluation corpora (Dzikovska et al., 2012).", "startOffset": 52, "endOffset": 76}, {"referenceID": 0, "context": "Attali et al. (2010); Williamson et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Attali et al. (2010); Williamson et al. (2012)).", "startOffset": 0, "endOffset": 47}, {"referenceID": 16, "context": "constructed using the MALLET toolkit (McCallum, 2002), one with 30 dimensions, and one with only ten.", "startOffset": 37, "endOffset": 53}, {"referenceID": 7, "context": "Three features were included based on language models built using the IRST Language Modeling Toolkit (Federico et al., 2008).", "startOffset": 101, "endOffset": 124}], "year": 2014, "abstractText": "Developments in the educational landscape have spurred greater interest in the problem of automatically scoring short answer questions. A recent shared task on this topic revealed a fundamental divide in the modeling approaches that have been applied to this problem, with the best-performing systems split between those that employ a knowledge engineering approach and those that almost solely leverage lexical information (as opposed to higher-level syntactic information) in assigning a score to a given response. This paper aims to introduce the NLP community to the largest corpus currently available for short-answer scoring, provide an overview of methods used in the shared task using this data, and explore the extent to which more syntactically-informed features can contribute to the short answer scoring task in a way that avoids the question-specific manual effort of the knowledge engineering approach.", "creator": "LaTeX with hyperref package"}}}