{"id": "1403.1329", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Integer Programming Relaxations for Integrated Clustering and Outlier Detection", "abstract": "In this paper we present methods for exemplar based clustering with outlier selection based on the facility location formulation. Given a distance function and the number of outliers to be found, the methods automatically determine the number of clusters and outliers. We formulate the problem as an integer program to which we present relaxations that allow for solutions that scale to large data sets. We use the approach of the same technique to generate data for each variable, and define a method to use in this paper.\n\n\n\n\nThe main component of the new paper, as used for the last one, is the algorithm in which we are required to consider clustering. In terms of the data set we include the new features that have been incorporated, the number of outliers to be found (as in the previous paper), and how those clusters are constructed, as well as how they are calculated (through the method). In this paper we present the methods to illustrate how they work.\nThe method is available in the repository at: http://www.rpi.org\nThe new features are included in the package for the original paper. You can see the documentation for the package for this paper.", "histories": [["v1", "Thu, 6 Mar 2014 02:42:22 GMT  (1636kb,D)", "http://arxiv.org/abs/1403.1329v1", "10 pages, 10 figures"]], "COMMENTS": "10 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lionel ott", "linsey pang", "fabio ramos", "david howe", "sanjay chawla"], "accepted": false, "id": "1403.1329"}, "pdf": {"name": "1403.1329.pdf", "metadata": {"source": "CRF", "title": "Integer Programming Relaxations for Integrated Clustering and Outlier Detection", "authors": ["Lionel Ott", "Linsey Pang", "Fabio Ramos", "David Howe", "Sanjay Chawla"], "emails": ["lott4241@uni.sydney.edu.au"], "sections": [{"heading": "1. INTRODUCTION", "text": "Clustering and outlier detection are often studied and investigated as two separate problems [Chandola et al., 2009]. However, it is natural to consider them simultaneously. For example, outliers can have a disproportionate impact on the location and shape of clusters which in turn can help identify, contextualise and interpret the outliers.\nA branch of statistics known as \u201crobust statistics\u201d studies the design of statistical methods which are less sensitive to the presence of outliers [Huber and Ronchetti, 2008]. For example, the median and trimmed mean estimators are far less sensitive to outliers than the mean. Similarly, versions of Principal Component Analysis (PCA) have been proposed [Croux and Ruiz-Gazen, 1996, Wright et al., 2009] which are more robust against model mis-specification. An impor-\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\ntant primitive in the area of robust statistics is the notion of Minimum Covariance Determinant (MCD): Given a set of n multivariate data points and a parameter `, the objective is to identify a subset of points which minimises the determinant of the variance-covariance matrix over all subsets of size n\u2212 `. The resulting variance-covariance matrix can be integrated into the Mahalanobis distance and used as part of a chi-square test to identify multivariate outliers [Rousseeuw and Driessen, 1999].\nIn the theoretical computer science literature, similar problems have been studied in the context of clustering and facility location. For example, Chen [2008] has considered and proposed a constant factor approximation algorithm for the k-median with outliers problem: Given n data points and parameters k and `, the objective is to remove a set of ` points such that the cost of k-median clustering on the remaining n \u2212 ` points is minimised. Earlier Charikar et al. [2001], have proposed a bi-criteria approximation algorithm for the facility location with outliers problem. While of the-\nar X\niv :1\n40 3.\n13 29\nv1 [\ncs .L\nG ]\n6 M\nar 2\n01 4\noretical interest, none of these algorithms are amenable to a practical implementation on large data sets.\nRobustness of clustering methods in general is discussed in [Garc\u0301\u0131a-Escudero et al., 2010, Hennig, 2008]. They provide a good theoretical overview about the conditions under which clustering methods can deal with noise or outliers. However, it is difficult to determine a-priori if data exhibits the required properties.\nMore recently, Chawla and Gionis [2013] have proposed kmeans-- a practical and scalable algorithm for the k-means with outlier problem. k-means-- is a simple extension of the k-means algorithm and is guaranteed to converge to a local optima. However, the algorithm inherits the weaknesses of the classical k-means algorithm. These are: (i) the requirement of setting the number of clusters k and (ii) initial specification of the k centroids. It is well known that the choice of k and initial set of centroids can have a large impact on the result. Trimmed k-means [Cuesta-Albertos et al., 1997] is a special case of k-means-- with k = 1.\nIn this paper we present methods that approach the problem of joint clustering and outlier detection as an integer programming optimisation task. The resulting algorithms find the number of cluster on their own and require as sole input the distance between pairs of points as well as the number ` of outliers to select. We propose three methods to solve this, each with their benefits and drawbacks: (i) affinity propagation [Frey and Dueck, 2007] extension to outlier detection, (ii) linear programming and (iii) Lagrangian duality relaxation. The main contributions of the paper are as follows:\n\u2022 formulation of the clustering with outlier detection problem as an integer program;\n\u2022 modification of the affinity propagation model and derivation of new update rules;\n\u2022 scalable algorithm based on Lagrangian duality;\n\u2022 approximation analysis of real data based on linear programming solutions;\n\u2022 evaluation on synthetic and real-world data sets.\nThe remainder of the paper is structured as follows. In Section 2 we describe the problem in detail. Following that in Section 3 we describe the different methods. In Section 4 we evaluate the methods on both synthetic and real datasets. Section 5 provides some additional information on related work before we conclude in Section 6."}, {"heading": "2. PROBLEM FORMULATION", "text": "Given the assignment cost matrix dij and cluster creation costs ci we define the task of clustering and outlier selection as the problem of finding the assignments to the binary exemplar indicators yi, outlier indicators oi and point assignments xij that minimises the following energy function:\nmin \u2211 i ciyi + \u2211 i \u2211 x dijxij , (1)\nsubject to\nxij \u2264 yj (2) oi + \u2211 j\nxij = 1 (3)\u2211 i oi = ` (4) xij , yj , oi \u2208 {0, 1}. (5)\nIn order to obtain a valid solution a set of constraints have been imposed:\n\u2022 points can only be assigned to valid exemplars Eq. (2)\n\u2022 every point must be assigned to exactly one other point or declared an outlier Eq. (3)\n\u2022 exactly ` outliers have to be selected Eq. (4)\n\u2022 only integer solutions are allowed Eq. (5)\nThese constraints describe the facility location problem with outlier selection (FLO). This allows the algorithm to select the number of clusters automatically and implicitly defines outliers as those points whose presence in the dataset has the biggest negative impact on the overall solution.\nIn the following we explore three different methods of formulating and solving the above generic problem. In the experiments we evaluate each of the methods under different criteria such as quality of the solution, complexity of the method and applicability to large datasets"}, {"heading": "3. METHODS", "text": "In the following we will describe three ways of solving the problem stated in Section 2. We start with a linear programming formulation which is known to provide the optimal answer if the solution is integer. Next we propose an extension to affinity propagation which is a conceptually nice method. Finally, we propose a method based on Lagrangian duality which is highly scalable while achieving results very close to the optimum found by LP."}, {"heading": "3.1 Linear Programming Relaxation", "text": "The first method we present is based on linear programming and will serve as ground truth for the other two methods. In order to solve the integer program described in Section 2 we have to relax it. If the solution to this relaxed formulation is integer, i.e. all assignments are either 0 or 1, we have found the optimal solution. The relaxed linear program has the following form:\nmin \u2211 i \u2211 j xijdij (6)\nsubject to\nxij \u2264 xii (7)\u2211 j xij + \u2211 k\noik = 1 (8)\u2211 i oik = 1 (9)\n0 \u2264 xij , oik \u2264 1 \u2200i, j, k (10)\nwhere dij is again the distance between point i and j. This follows Komodakis et al. [2008] with additional constraints\nto enforce the outlier selection. We note here that Eq. (6) uses the diagonal to indicate exemplar selection as it maps more easily to affinity propagation. The constraint in Eq. (7) enforces the condition that points are only assigned to valid exemplars. Eq. (8) enforces that a point is either assigned to a single cluster or is declared as an outlier. Eq. (9) ensures that every outlier point is used exactly once thus enforcing that exactly ` outliers are selected. Finally, Eq. (10) is the relaxation of the original integer program."}, {"heading": "3.2 Affinity Propagation Outlier Clustering", "text": "The extension to affinity propagation, based on the binary variable model [Givoni and Frey, 2009], solves the integer program of Section 2 by representing it as a factor graph, shown in Figure 2. This factor graph is solved using belief propagation and is based on the following energy function:\nmax \u2211 ij Sij(xij)+ \u2211 j Ej(x:j)+ \u2211 i Ii(xi:, oi:)+ \u2211 k Pk(o:k),\n(11)\nwhere\nSij(xij) = { \u2212ci if i = j \u2212dij otherwise\n(12)\nIi(xi:, oi:) =\n{ 0 if \u2211 j xij + \u2211 k oik = 1\n\u2212\u221e otherwise (13)\nEj(x:j) =\n{ 0 if xjj = maxi cij\n\u2212\u221e otherwise (14)\nPk(o:k) =\n{ 0 if \u2211 i oik = 1\n\u2212\u221e otherwise (15)\nwith xi: = xi1, . . . , xiN . Since we use the max-sum algorithm we maximise the energy function and use negative distances. The three constraints can be interpreted as follows:\n1. 1-of-N Constraint (Ii). Each data point has to choose exactly one exemplar or be declared as an outlier.\n2. Exemplar Consistency Constraint (Ej). For point i to select point j as its exemplar, point j must declare itself an exemplar.\n3. Select ` Outliers Constraint (Pk). For every outlier selection exactly one point is assigned.\nThese constraints are enforced by associating an infinite cost with invalid configurations,thus resulting in an obviously suboptimal solution.\nThe energy function is optimised with the max-sum algorithm [Kschischang et al., 2001], which allows the recovery of the maximum a posteriori (MAP) assignments of the xij and oik variables. The algorithm works by exchanging messages between nodes in the factor graph. In their most general form these messages are defined as follows:\n\u00b5v\u2192f (xv) = \u2211\nf\u2217\u2208ne(v)\\f \u00b5f\u2217\u2192v(xv), (16)\n\u00b5f\u2192v(xv) = max x1,...,xM\n[ f(xv, x1, . . . , xM )\n+ \u2211\nv\u2217\u2208ne(f)\\v \u00b5v\u2217\u2192f (xv\u2217)\n] ,\n(17)\nxij Sij\nEj\nIisij\n\u03b1ij\u03c1ij\n\u03b2ij \u03b7ij oik\nPk\n\u03c9ik\u03c4ik\n\u03bbik\n\u03c7ik\nFigure 3: Messages exchanged by the APOC graphical model. xij represents the clustering choice whereas oik represents the outlier choice.\nwhere \u00b5v\u2192f (x) is the message sent from node v to factor f , \u00b5f\u2192v(xv) is the message from factor f sent to node v, ne() is the set of neighbours of the given factor or node and xv is the value of node v.\nThe messages exchanged by APOC are shown in Figure 3. We can see that each node xij is connected to three factors: Sij , Ii and Ej whereas outlier nodes oik are connected to only two, Ii and Pk. Messages \u03c1ij , \u03b2ij , \u03c4ik and \u03beik are sent from nodes to factors and derived using Eq. (16). The other five messages sij , \u03b1ij , \u03b7ij , \u03bbik and \u03c9ik are derived with Eq. (17) since they are sent from a factor to a node. Since only binary variables are involved it is sufficient to compute the difference between the two variable settings. Combining these messages we obtain the final set of update equations as:\n\u03c1ij = sij + min [ \u2212max\nt6=j (\u03b1it + sit),\u2212max t (\u03c9it)\n] (18)\n\u03b1ij =\n{\u2211 t6=j max(0, \u03c1tj) i = j\nmin [ 0, \u03c1jj + \u2211 t/\u2208{i,j}max(0, \u03c1tj) ] i 6= j (19)\n\u03bbik = min [ \u2212max\nt (\u03b1it + sit),\u2212max t 6=k (\u03c9ti)\n] (20)\n\u03c9ik = \u2212max t 6=i (\u03bbtk) (21)\nThe above equations show how to update the messages, however, we still need to explain how to initialise the messages, determine convergence and extract the MAP solution. First, it is important to set the diagonal entries of S properly. Typically using Sii = \u03b8 \u2217median(S) is a good choice, with \u03b8 \u2208 [1, 30]. The messages \u03b1ij , \u03c1ij and \u03bbik are initialised to 0 and \u03c9ik to the median of S. Once the messages are initialised we update them in turn with damping until we achieve convergence. Convergence is achieved when the en-\nAlgorithm 1: apoc(S, `)\n1 foreach i, j \u2208 {1, . . . , N} do 2 \u03b1ij \u2190 0; 3 \u03c1ij \u2190 0; 4 end 5 foreach i \u2208 {1, . . . , N}, k \u2208 {1, . . . , `} do 6 \u03bbik \u2190 0; 7 \u03c9ik \u2190 median(S); 8 end 9 repeat\n10 update \u03c1 according to Eq. (18); 11 update \u03b1 according to Eq. (19); 12 update \u03bb according to Eq. (20); 13 update \u03c9 according to Eq. (21);\n14 until convergence; 15 O \u2190 extract outliers; 16 E \u2190 extract exemplars; 17 A\u2190 find exemplar assignments;\nergy of the solution is not changing drastically over a few iterations. The outliers are determined as the ` points with the largest values of maxk(\u03bbik + \u03c9ik). From the remaining points the exemplars are then selected as the points for which (\u03b1ii + \u03c1ii) > 0 is true. All other points i are assigned to the exemplar e satisfying arg maxe(\u03b1ie+\u03c1ie). This entire process is shown in Algorithm 1, where we first initialise the messages, then update them until convergence and finally extract the MAP solution."}, {"heading": "3.3 Lagrangian Duality", "text": "The final method is based on Lagrangian duality. The basic idea is to relax the original problem by introducing Lagrange multipliers \u03bb. The result is a dual problem which is now concave with a unique maximum, making it possible to use gradient ascent based methods. However, the resulting function is not differentiable and requires the use of subgradients.\nWe restate the original problem here for convenience: min \u2211 j yjcj + \u2211 i \u2211 j xijdij (22)\nsubject to\nxij \u2264 yj (23) oi + \u2211 j\nxij = 1 (24)\u2211 i oi = ` (25)\n0 \u2264 xij , yj , oi \u2264 1 \u2200i, j, (26)\nwhere oi = 1 indicates that point i is an outlier and ` is the number of outliers we wish to find. Eq. (23) encodes that a point can only be assigned to a valid exemplar. The second constraint Eq. (24) enforces that a point is either assigned to a single cluster or selected as an outlier. The final constraint, Eq. (25), ensures that exactly ` outliers are selected. Relaxing Eq. (24) yields:\nmin \u2211 i\n(1\u2212 oi)\u03bbi\ufe38 \ufe37\ufe37 \ufe38 outliers\n+ \u2211 j cjyj + \u2211 i \u2211 j\n(dij \u2212 \u03bbi)xij\ufe38 \ufe37\ufe37 \ufe38 clustering . (27)\nsubject to\nxij \u2264 yi (28)\u2211 k ok = ` (29)\n0 \u2264 xij , yj , ok \u2264 1 \u2200i, j, k (30)\nWe now solve this relaxed problem using the idea of Bertsimas and Weismantel [2005] by finding valid assignments that attempt to minimise Eq. (27) without optimality guarantees. The Lagrange multipliers \u03bb act as a penalty incurred for constraint violation which we try to minimise. From Eq. (27) we see that the penalty influences two parts: outlier selection and clustering. We select good outliers by designating the ` points with largest \u03bb as outliers, as this removes a large part of the penalty. For the remaining N\u2212` points we determine clustering assignments by setting xij = 0 for all pairs for which dij \u2212 \u03bbi \u2265 0. To select the exemplars we compute\n\u00b5j = cj + \u2211\ni:dij\u2212\u03bbi<0 (dij \u2212 \u03bbi), (31)\nwhich represents the amortised cost of selecting point j as exemplar and assigning points to it. Thus, if \u00b5j < 0 we select point j as an exemplar and set yj = 1, otherwise we set yj = 0. Finally, we set xij = yj if dij\u2212\u03bbi < 0. From this complete assignment we then compute a new subgradient st and update the Lagrangian multipliers \u03bbt as follows:\nstj = 1\u2212 \u2211 j xij (32) \u03bbtj = max(\u03bb t\u22121 j + \u03b8 tsj , 0), (33)\nwhere \u03b8t is the step size at time t computed as\n\u03b8t = \u03b80 pow(\u03b1, t) \u03b1 \u2208 (0, 1), (34)\nwhere pow(a, b) = ab. To obtain the final solution we repeat the above steps until the changes become small enough, at which point we extract a feasible solution. This is guaranteed to converge [Bertsimas and Weismantel, 2005] if a step function is used for which the following holds:\n\u221e\u2211 t=1 \u03c6t =\u221e and lim t\u2192\u221e \u03b8t = 0. (35)\n3.3.1 Scalable Implementation Considerations In order to enable the algorithm to scale to large datasets\nwe have to consider the limited availability of main memory. First we cannot assume that the complete distance matrix can fit into main memory. Therefore, we compute the distances on the fly. Since this involves N2 evaluations per iteration it is the most costly part of the method. However, the evaluation of the distance function can be easily parallelised. In practice with simple distance functions, such as the Euclidean distance, approximately 75% of the computational time is spent evaluating the distance function. Another important point is that just as storing the full distance matrix is not possible, neither is storing the full assignment matrix x. However, we are only interested in the values where xij = 1, which is a small portion of the full matrix. Thus we can use standard sparse matrix implementations to manage the assignment matrix.\nThe pseudo code in Algorithm 2 shows how to compute the assignment matrix x using the above mentioned im-\nAlgorithm 2: LD-Iteration(\u03bb)"}, {"heading": "1 O \u2190 0; // Outlier indicators", "text": "2 y\u2190 0; // Exemplar indicators 3 L \u2190 \u2205; // Set of (i, \u03bbi) pairs 4 S \u2190 \u2205; // Assignment pairs (i, j) 5 x\u2190 0; // Assignments // Selecting outliers 6 foreach i \u2208 {1, . . . , N} do 7 L \u2190 L \u222a (i, \u03bbi); 8 end 9 L \u2190 sort(L);\n10 foreach i \u2208 {1, . . . , L} do 11 OLi,1 \u2190 1; 12 end\n// Compute exemplar and outlier scores\n13 foreach j \u2208 {1, . . . , N} do 14 \u00b5j \u2190 cj ; 15 foreach i \u2208 {1, . . . , N} do 16 q \u2190 dist(i, j)\u2212 \u03bbi; 17 if q < 0 then 18 \u00b5j \u2190 \u00b5j + q; 19 S \u2190 S \u222a (i, j); 20\n21 end\n22 end // Select exemplars 23 foreach j \u2208 {1, . . . , N} do 24 if \u00b5j < 0 then 25 yj \u2190 1; 26 else 27 yj \u2190 0; 28 end\n29 end // Perform cluster assignments 30 foreach s \u2208 S do 31 if \u00b5s2 = 1 then 32 xp1,p2 \u2190 1; 33 end\n34 end 35 return x,y,O;\nprovements. Lines 1 through 5 initialise the required storage. In lines 6 to 12 we sort the points in descending order according to their \u03bb values, by first creating pairs of point index and value and then sorting these. The computationally intensive but also parallelisable part of the algorithm is located in lines 13 to 21. There we compute the exemplar score \u00b5j and at the same time remember point pairs (i, j) for which dij \u2212 \u03bbi < 0. Lines 22 to 27 then perform the exemplar assignments based on \u00b5j . Finally, in lines 28 to 31 we set the assignment xij = 1 if yj = 1. Lastly, we return assignments, exemplars and outliers."}, {"heading": "3.4 Comparisons", "text": "Now that we have presented the different methods we want to give an overview of the advantages and disadvantages of these. Table 1 presents a quick overview of different properties of the proposed methods discussed in more detail next. First, the speed of APOC and LD clearly outperform LP by a large margin, however, other methods such as k-means--\nstill perform better. With regards to optimality we know that\nFLOLP \u2264 FLOLD \u2264 FLOIP (36)\n[Bertsimas and Weismantel, 2005]. Additionally, if LP finds a solution that is integer it is equivalent to the IP one, i.e. FLOLP = FLOIP . In the experiments we see that even though LP finds the optimal solution, LD fails to do so. This contradicts theory, but is explained by the fact that for performance gains we sacrifice some optimality. More specifically, we use discounted updates of the solution matrix x as well as stop before we converge to a pure integer solution. APOC has no theoretical guarantees on either convergence or optimality bounds of the solution. While such guarantees can be given for certain types of structures with belief propagation [Weiss et al., 2007] it is unclear how they apply to the special case of APOC. However, in practice both APOC and LD achieve scores very close to the optimum.\nThe speed of LP clearly makes it impossible to scale for large datasets which leaves us with the comparison of APOC and LD. APOC requires storage for messages and similarities and operations which cannot be supported by standard sparse matrix implementations. As such APOC cannot be made truly scalable. LD on the other hand spends the majority of its runtime computing distances between pairs of points, a task that can easily be parallelised. Furthermore the actual assignment matrix can easily be stored using standard sparse matrix implementations. Thus LD is a much better candidate for large scale datasets. Finally, with regards to extensibility LP is the easiest as the relaxation only touches the variables. LD is more involved as there is more freedom in the relaxation of the constraints and a way to solve the dual problem efficiently has to be devised. Affinity propagation is the hardest one as it requires very careful choice of constraints which can be modelled by the graphical model and laborious derivation of update rules, which makes it much harder to come up with solutions to new problems."}, {"heading": "4. EXPERIMENTS", "text": "In this section the proposed methods are evaluated on both synthetic and real data. We first present experiments using synthetic data to show different properties of the presented methods and a quantitative analysis. We then process GPS traces of hurricanes to show the applicability of this method to real data. Finally, we present results on two image datasets visually showing the quality of the clusters and outliers found.\nBoth APOC and LD require a cost for creating clusters. In all experiments we obtain this value as \u03b8 \u2217 median(xij), i.e. the median of all distances multiplied by a scaling factor \u03b8 which is typically in the range [1, 30].\n4.1 Synthetic Data\nWe use synthetic datasets to evaluate the performance of the proposed methods in a controlled setting. We randomly sample k clusters with m points each from d-dimensional normal distributions N (\u00b5,\u03a3) with randomly selected \u00b5 and \u03a3. To these clusters we add ` additional outlier points that have a low probability of belonging to any of the selected clusters.\nWe compare APOC and LD against k-means-- using kmeans++ [Arthur and Vassilvitskii, 2007] to select the initial centres. Euclidean distance is used as the metric for all methods. Unless mentioned otherwise k-means-- is provided with the exact number of clusters generated, while APOC and LD are required to determine this automatically.\nTo assess the performance of the methods we use the following three metrics:\n1. Normalised Jaccard index, measures how accurately a method selects the ground truth outliers. It is a coefficient computed between selected outliers O and ground-truth outliers O\u2217. The final coefficient is normalised with regards to the best possible coefficient obtainable in the following way:\nJ(O,O\u2217) = |O \u2229O\u2217| |O \u222aO\u2217|/ min(|O|, |O\u2217|) max(|O|, |O\u2217|) . (37)\n2. Local outlier factor [Breunig et al., 2000] (LOF) measures the outlier quality of a point. We compute the ratio between the average LOF of O and O\u2217, which indicates the quality of the set of selected outliers.\n3. V-Measure [Rosenberg and Hirschberg, 2007] indicates the quality of the overall clustering solution. The outliers are considered as an additional class for this measure.\nFor all the metrics a value of 1 is the optimal outcome. In Figure 1 we present clustering results obtained by the different methods. Both APOC and LD manage to identify the correct number of clusters and select accurate outliers. k-means-- on the other hand, even with good initialisation and correct value for k specified, fails to find the correct clusters and as a result finds a suboptimal solution. The errors made by k-means-- are highlighted by the red circles.\nWe first investigate the influence of the data dimensionality on the results. From Figure 4 it is clear that in general the quality of the solution increases with higher dimensions. This can be explained by the fact that in higher dimensional spaces the points are farther apart and hence easier to cluster. Looking at k-means-- we can see that it struggles more then the other two methods even though it is provided with the correct number of clusters. In higher dimensions it achieves perfect scores for the two outlier centric measures but is unable to always find the correct solution to the whole clustering problem. Looking at APOC and LD we can see that both have little trouble finding perfect solutions in high dimensions. In lower dimensions LD shows a bit more variability compared to APOC. In general, the two dimensional data is the most challenging one and thus will be used for all further experiments.\nThe number of outliers ` is a parameter that all methods require. Typically the correct value of ` is unknown and it is therefore important to know how the algorithms react when the user\u2019s estimate is incorrect. We generate 2D datasets with 2000 inliers and 200 outliers and vary the number of\noutliers ` selected by the methods. The results in Figure 5 show that in general no method breaks completely if the correct value for ` is not provided. Looking at the Jaccard index we see that if ` is smaller then the true number of outliers all methods pick only outliers. When ` is greater then the true number of outliers we can see a difference in performance, namely that LD picks the largest outliers which APOC does to some extent as well, while k-means-- does not seem to be very specific about which points to select. This difference in behaviours stems from the fact that APOC and LD optimise a cost function in which removing the biggest outliers is the most beneficial. The LOF ratio shows a similar picture as the Jaccard index. For ` = 100 the values are much lower as not all outliers can be picked. For the other cases APOC and LD perform similarly while k-means-- shows higher variability. Finally, V-Measure shows that the overall clustering results remain accurate even if the number of outliers is misspecified. For large differences between actual and specified outliers a drop in clustering performance can be observed.\nSince both APOC and LD are not guaranteed to find the optimal solution we investigate how close the solutions obtained are to the optimum. The ground truth needed for this is obtained by solving the LP formulation of Section 3.1 with CPLEX. This comparison indicates what quality can be typically expected from the two methods. Additionally, we can evaluate the speed of these approximations. We evaluate 100 datasets, consisting of 2D Gaussian clusters and outliers, with varying number of points. On average APOC obtains an energy that is 96%\u00b1 4% of the optimal solution found by LP, LD obtains 94%\u00b1 5% of the LP energy while k-means--, with correct k, only obtains 86%\u00b112% of the optimum. These results reinforce the previous analysis; APOC and LD perform similarly while outperforming k-means--. We now look at the speedup of APOC and LD over LP, as shown in Figure 6 a). Both methods outperform LP by a large margin which only grows the more points are involved. Overall for a small price in quality the two methods obtain a solution significantly faster.\nNext we compare the performance between APOC and LD. Figure 6 b) shows the overall runtime of both methods for varying number of data points. Here we observe that APOC takes less time then LD. However, by observing the time a single iteration takes, shown in Figure 6 c), we see that LD is much faster on a per iteration basis compared to APOC. In practice LD requires several times the number of iterations of APOC, which is affected by the step size function used. Using a more sophisticated method of computing the step size will provide large gains to LD. Finally, the biggest difference between APOC and LD is that the former requires all messages and distances to be held in memory. This obviously scales poorly to large datasets. Conversely, LD computes the distances during running time and only needs to store indicator vectors and a sparse assignment matrix, thus using much less memory. This makes LD amenable to processing large scale datasets. For example, with single precision floating point numbers, dense matrices and 10 000 points APOC requires around 2200 MB of memory while LD only needs 370 MB. Further gains can be obtained by using sparse matrices which is straight forward in the case of LD but complicated for APOC."}, {"heading": "4.2 Hurricane Data", "text": "We use hurricane data from 1970 to 2010 provided by the\nNational Oceanic and Atmospheric Administration (NOAA) in this experiment. The data provides a time series of longitude and latitude coordinates for each storm, effectively forming a GPS trace. In order to compare the overall shape of these traces we use the discrete Fre\u0301chet distance [Eiter and Mannila, 1994]. Intuitively this measures the minimal distance required to connect points on two curves being compared, i.e. structurally similar curves have a low score. Before computing the Fre\u0301chet distance between two curves we move their starting points to the same location. This means that we are comparing their shapes and not their global location. Clustering the 700 traces using the LD method with ` = 20 we obtain clusters that move in a similar direction, as shown in Figure 7 b) and c). Analysing the outliers shown in Figure 7 a) we find that half of them are category 4 and 5 Cape Verde-type hurricanes, shown by the thick stroke. The other half of the outliers were selected due to either their long life time, unusual motion, or high destructive power and are shown with dashed stroke. This demonstrates that the outliers found by the proposed methods find interesting patterns in spatial temporal data which are not directly apparent.\nTo better understand the behaviour of LD we plot the value of the \u03bb values associated with representative outliers and exemplars in Figure 8. One can see how after about 100 iterations the values level out and remain stable. Interestingly the outliers have higher \u03bb values compared to the exemplars which allows the method to distinguish between them more easily. Lastly, the outliers have smoother curves, i.e. less jumps, when compared to the exemplars. This indicates that outliers do not change significantly whereas exem-\nplars will change during the optimisation process. Overall, stable results are achieved after about a third of the runtime suggesting that a more sophisticated termination criterion can be used for faster convergence."}, {"heading": "4.3 MNIST Data", "text": "The MNIST dataset, introduced by LeCun et al. [1998], contains 28 \u00d7 28 pixel images of handwritten digits. We extract features from these images by representing them as 768 dimensional vectors which is reduced to 25 dimensions using PCA. L2 norm is used to compute the distance between these vectors. In Figure 9 we show exemplary results obtained when processing 10 000 digits with the LD method with \u03b8 = 5 and ` = 500. Each row in Figure 9 a) and b) shows examples of clusters representing the digits 1 and 4 respectively. This illustrates how different the same digit can appear and the separation induced by the clusters. Figure 9 c) contains a subset of the outliers selected by the method. These outliers have different characteristics that make them sensible outliers, such as: thick stroke, incomplete, unrecognisable or ambiguous meaning.\nTo investigate the influence the cluster creation cost has we run the experiment with different values of \u03b8. In Table 2 we show results for values of cost scaling factor \u03b8 = {5, 15, 25}. The V-Measure score does not change drastically between the different runs. However, the other measures, especially the number of clusters changes. We can see that, as expected, by increasing the cost the number of clusters decreases. This results in an increased completeness score, i.e. a larger percentage of the same ground truth labels are contained in a smaller number of clusters. Meanwhile, the\nhomogeneity score drops which indicates that there are more cases of different digits that look similar, such as 1 and 7, being placed into a single cluster. Overall, we can say that changes to the cluster creation cost has a predictable impact on the results with no apparent sudden changes."}, {"heading": "4.4 Natural Scenes Data", "text": "In this section we apply APOC to outlier detection in image collections. Our dataset is composed of images from mountain ranges with outliers in the form of cars and coffee cups. The distance between images is computed from colour histograms and local binary pattern [Ojala et al., 2002] histograms using the Bhattacharyya distance. When we specify the correct number of outliers APOC and LD find all of the images belonging to our two outlier groups and cluster the remaining images according to their appearance. Figure 10 shows outliers in the first two rows and examples belong-\ning to the three clusters found in subsequent rows as found by APOC. The three clusters contain images which mainly differ in their colour and mood. The first cluster contains images with muted greens and browns, the second cluster has images with cold white and blue colours and finally the last cluster is dominated by vibrant colours. For comparison we also process this data using LP, LD and k-means--. The results in Table 3 show how close the different methods come to the optimal energy, the number of clusters and the quality of the selected outliers. This shows that APOC and LD perform nearly as well as the optimal LP solution, while k-means-- depends on a good choice of k and still fails to\nrecover all the outliers. This experiment also highlights the capability of APOC and LD to pick communities of points as outliers in a global scope."}, {"heading": "5. RELATED WORK", "text": "In Section 1, we provided some context for combining clustering and outlier detection. Here we elaborate further on other works which are germane to our problem. The problem of outlier detection has been extensively researched in both statistics and data mining [Chandola et al., 2009, Hawkins, 1980]. However, both communities have different perspectives. The statistical perspective is to design models which are robust against outliers. In the process, outliers are discovered and evaluated. The data mining perspective is to directly mine for outliers and then determine their usefulness. The study of outlier detection in data mining was pioneered by the work of Knorr and Ng [1997] who proposed a definition of distance-based outliers which relaxed strict distributional assumptions and was readily generalisable to multi-dimensional data sets. Following Knorr and Ng, several variations and algorithms have been proposed to detect distance-based outliers [Bay and Schwabacher, 2003, Ramaswamy et al., 2000]. However, the outliers detected by these methods are global outliers, i.e., the \u201coutlierness\u201d is with respect to the whole dataset. Breunig et al. [2000] have argued that in some situations local outliers are more important than global outliers and cannot be easily detected by standard distance-based techniques. They introduced the\nconcept of local outlier factor (LOF ), which captures how isolated an object is with respect to its surrounding neighbourhood. The concept of local outliers has subsequently been extended in several directions [Chandola et al., 2009, Chawla and Sun, 2006, Papadimitriou et al., 2003].\nDespite the observation that clustering and outlier detection techniques should naturally complement each other, research work in integrating the two is limited. The MCD (as noted in the Introduction), is one prominent approach where the problem of outliers was tightly integrated in an optimisation framework [Rousseeuw and Driessen, 1999]. The k-means-- approach extended the idea of MCD to include clustering [Chawla and Gionis, 2013]. The theoretical research in combining clustering and outlier detection has been investigated in the context of robust versions of the k-median and facility location problems [Chen, 2008, Charikar et al., 2001]. However, as noted before, the focus has exclusively been on deriving theoretical approximation bounds. To the best of our knowledge the use of belief propagation ideas to investigate outliers is entirely novel."}, {"heading": "6. CONCLUSION", "text": "In this paper we presented a novel approach to joint clustering and outlier detection formulated as an integer program. The proposed optimisation problem enforces valid clusters as well as the selection of a fixed number of outliers. We then described three ways of solving the optimisation problem using (i) linear programming, (ii) affinity propagation with outlier detection and (iii) Lagrangian duality. Experiments on synthetic and real data show how the joint optimisation outperforms two stage approaches such as k-means--. Additionally, experimental results suggest that results obtained via APOC and LD are very close to the optimal solution at a fraction of the computational time. Finally, we detail the modifications of the LD method, needed to process large scale datasets."}], "references": [{"title": "k-means++: The Advantages of Careful Seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "Mining distance-based outliers in near linear time with randomization and a simple pruning rule", "author": ["S. Bay", "M. Schwabacher"], "venue": "In Int. Conf. on Knowledge Discovery and Data Mining,", "citeRegEx": "Bay and Schwabacher.,? \\Q2003\\E", "shortCiteRegEx": "Bay and Schwabacher.", "year": 2003}, {"title": "Optimization over Integers", "author": ["D. Bertsimas", "R. Weismantel"], "venue": "Dynamic Ideas Belmont,", "citeRegEx": "Bertsimas and Weismantel.,? \\Q2005\\E", "shortCiteRegEx": "Bertsimas and Weismantel.", "year": 2005}, {"title": "LOF: Identifying Density-Based Local Outliers", "author": ["M. Breunig", "H. Kriegel", "R. Ng", "J. Sander"], "venue": "In Int. Conf. on Management of Data,", "citeRegEx": "Breunig et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Breunig et al\\.", "year": 2000}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys,", "citeRegEx": "Chandola et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chandola et al\\.", "year": 2009}, {"title": "Algorithms for Facility Location Problems with Outliers", "author": ["M. Charikar", "S. Khuller", "D.M. Mount", "G. Narasimhan"], "venue": "In Proc. of the ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Charikar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2001}, {"title": "k-means\u2013: A Unified Approach to Clustering and Outlier Detection", "author": ["S. Chawla", "A. Gionis"], "venue": "In SIAM International Conference on Data Mining,", "citeRegEx": "Chawla and Gionis.,? \\Q2013\\E", "shortCiteRegEx": "Chawla and Gionis.", "year": 2013}, {"title": "SLOM: A new measure for local spatial outliers", "author": ["S. Chawla", "P. Sun"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Chawla and Sun.,? \\Q2006\\E", "shortCiteRegEx": "Chawla and Sun.", "year": 2006}, {"title": "A constant factor approximation algorithm for k-median clustering with outliers", "author": ["K. Chen"], "venue": "In Proc. of the ACMSIAM Symposium on Discrete Algorithms,", "citeRegEx": "Chen.,? \\Q2008\\E", "shortCiteRegEx": "Chen.", "year": 2008}, {"title": "A Fast Algorithm for Robust Principal Components Based on Projection Pursuit", "author": ["C. Croux", "A. Ruiz-Gazen"], "venue": "In Proc. in Computational Statistics,", "citeRegEx": "Croux and Ruiz.Gazen.,? \\Q1996\\E", "shortCiteRegEx": "Croux and Ruiz.Gazen.", "year": 1996}, {"title": "Trimmed k-means: an attempt to robustify quantizers", "author": ["J. Cuesta-Albertos", "A. Gordaliza", "C. Matr\u00e1n"], "venue": "The Annals of Statistics,", "citeRegEx": "Cuesta.Albertos et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cuesta.Albertos et al\\.", "year": 1997}, {"title": "Computing Discrete Fr\u00e9chet Distance", "author": ["T. Eiter", "H. Mannila"], "venue": "Technical report, Technische Universita\u0308t Wien,", "citeRegEx": "Eiter and Mannila.,? \\Q1994\\E", "shortCiteRegEx": "Eiter and Mannila.", "year": 1994}, {"title": "Clustering by Passing Messages Between Data Points", "author": ["B. Frey", "D. Dueck"], "venue": "Science,", "citeRegEx": "Frey and Dueck.,? \\Q2007\\E", "shortCiteRegEx": "Frey and Dueck.", "year": 2007}, {"title": "MayoIscar. A Review of Robust Clustering Methods", "author": ["L. Gar\u0107\u0131a-Escudero", "A. Gordaliza", "C. Matr\u00e1n"], "venue": "Advances in Data Analysis Classification,", "citeRegEx": "Gar\u0107\u0131a.Escudero et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gar\u0107\u0131a.Escudero et al\\.", "year": 2010}, {"title": "A Binary Variable Model for Affinity Propagation", "author": ["I. Givoni", "B. Frey"], "venue": "Neural Computation,", "citeRegEx": "Givoni and Frey.,? \\Q2009\\E", "shortCiteRegEx": "Givoni and Frey.", "year": 2009}, {"title": "Identification of Outliers", "author": ["D. Hawkins"], "venue": null, "citeRegEx": "Hawkins.,? \\Q1980\\E", "shortCiteRegEx": "Hawkins.", "year": 1980}, {"title": "Dissolution point and isolation robustness: Robustness criteria for general cluster analysis methods", "author": ["C. Hennig"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Hennig.,? \\Q2008\\E", "shortCiteRegEx": "Hennig.", "year": 2008}, {"title": "A Unified Notion of Outliers: Properties and Computation", "author": ["E. Knorr", "R. Ng"], "venue": "In Int. Conf. on Knowledge Discovery and Data Mining,", "citeRegEx": "Knorr and Ng.,? \\Q1997\\E", "shortCiteRegEx": "Knorr and Ng.", "year": 1997}, {"title": "Clustering via LP-based Stabilities", "author": ["N. Komodakis", "N. Paragios", "G. Tziritas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Komodakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Komodakis et al\\.", "year": 2008}, {"title": "Factor Graphs and the Sum-Product Algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.-A. Loeliger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns", "author": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ojala et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ojala et al\\.", "year": 2002}, {"title": "LOCI: Fast outlier detection using the local correlation integral", "author": ["S. Papadimitriou", "H. Kitagawa", "P. Gibbons", "C. Faloutsos"], "venue": "In Int. Conf. on Data Engineering,", "citeRegEx": "Papadimitriou et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 2003}, {"title": "Efficient Algorithms for Mining Outliers from Large Data Sets", "author": ["S. Ramaswamy", "R. Rastogi", "K. Shim"], "venue": "In Int. Conf. on Management of Data,", "citeRegEx": "Ramaswamy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ramaswamy et al\\.", "year": 2000}, {"title": "V-Measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Rosenberg and Hirschberg.,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg and Hirschberg.", "year": 2007}, {"title": "A fast algorithm for the minimum covariance determinant estimator", "author": ["P. Rousseeuw", "K. Driessen"], "venue": null, "citeRegEx": "Rousseeuw and Driessen.,? \\Q1999\\E", "shortCiteRegEx": "Rousseeuw and Driessen.", "year": 1999}, {"title": "MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies", "author": ["Y. Weiss", "C. Yanover", "T. Meltzer"], "venue": "In Proc. of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Weiss et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2007}, {"title": "Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices by Convex Optimization", "author": ["J. Wright", "A. Ganesh", "S. Rao", "Y. Peng", "Y. Ma"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "INTRODUCTION Clustering and outlier detection are often studied and investigated as two separate problems [Chandola et al., 2009].", "startOffset": 106, "endOffset": 129}, {"referenceID": 25, "context": "The resulting variance-covariance matrix can be integrated into the Mahalanobis distance and used as part of a chi-square test to identify multivariate outliers [Rousseeuw and Driessen, 1999].", "startOffset": 161, "endOffset": 191}, {"referenceID": 7, "context": "For example, Chen [2008] has considered and proposed a constant factor approximation algorithm for the k-median with outliers problem: Given n data points and parameters k and `, the objective is to remove a set of ` points such that the cost of k-median clustering on the remaining n \u2212 ` points is minimised.", "startOffset": 13, "endOffset": 25}, {"referenceID": 5, "context": "Earlier Charikar et al. [2001], have proposed a bi-criteria approximation algorithm for the facility location with outliers problem.", "startOffset": 8, "endOffset": 31}, {"referenceID": 10, "context": "Trimmed k-means [Cuesta-Albertos et al., 1997] is a special case of k-means-- with k = 1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 12, "context": "We propose three methods to solve this, each with their benefits and drawbacks: (i) affinity propagation [Frey and Dueck, 2007] extension to outlier detection, (ii) linear programming and (iii) Lagrangian duality relaxation.", "startOffset": 105, "endOffset": 127}, {"referenceID": 6, "context": "More recently, Chawla and Gionis [2013] have proposed kmeans-- a practical and scalable algorithm for the k-means with outlier problem.", "startOffset": 15, "endOffset": 40}, {"referenceID": 18, "context": "This follows Komodakis et al. [2008] with additional constraints", "startOffset": 13, "endOffset": 37}, {"referenceID": 14, "context": "2 Affinity Propagation Outlier Clustering The extension to affinity propagation, based on the binary variable model [Givoni and Frey, 2009], solves the integer program of Section 2 by representing it as a factor graph, shown in Figure 2.", "startOffset": 116, "endOffset": 139}, {"referenceID": 19, "context": "The energy function is optimised with the max-sum algorithm [Kschischang et al., 2001], which allows the recovery of the maximum a posteriori (MAP) assignments of the xij and oik variables.", "startOffset": 60, "endOffset": 86}, {"referenceID": 2, "context": "We now solve this relaxed problem using the idea of Bertsimas and Weismantel [2005] by finding valid assignments that attempt to minimise Eq.", "startOffset": 52, "endOffset": 84}, {"referenceID": 2, "context": "This is guaranteed to converge [Bertsimas and Weismantel, 2005] if a step function is used for which the following holds: \u221e \u2211", "startOffset": 31, "endOffset": 63}, {"referenceID": 2, "context": "[Bertsimas and Weismantel, 2005].", "startOffset": 0, "endOffset": 32}, {"referenceID": 26, "context": "While such guarantees can be given for certain types of structures with belief propagation [Weiss et al., 2007] it is unclear how they apply to the special case of APOC.", "startOffset": 91, "endOffset": 111}, {"referenceID": 0, "context": "We compare APOC and LD against k-means-- using kmeans++ [Arthur and Vassilvitskii, 2007] to select the initial centres.", "startOffset": 56, "endOffset": 88}, {"referenceID": 3, "context": "Local outlier factor [Breunig et al., 2000] (LOF) measures the outlier quality of a point.", "startOffset": 21, "endOffset": 43}, {"referenceID": 24, "context": "V-Measure [Rosenberg and Hirschberg, 2007] indicates the quality of the overall clustering solution.", "startOffset": 10, "endOffset": 42}, {"referenceID": 11, "context": "In order to compare the overall shape of these traces we use the discrete Fr\u00e9chet distance [Eiter and Mannila, 1994].", "startOffset": 91, "endOffset": 116}, {"referenceID": 20, "context": "3 MNIST Data The MNIST dataset, introduced by LeCun et al. [1998], contains 28 \u00d7 28 pixel images of handwritten digits.", "startOffset": 46, "endOffset": 66}, {"referenceID": 21, "context": "The distance between images is computed from colour histograms and local binary pattern [Ojala et al., 2002] histograms using the Bhattacharyya distance.", "startOffset": 88, "endOffset": 108}, {"referenceID": 2, "context": "The problem of outlier detection has been extensively researched in both statistics and data mining [Chandola et al., 2009, Hawkins, 1980]. However, both communities have different perspectives. The statistical perspective is to design models which are robust against outliers. In the process, outliers are discovered and evaluated. The data mining perspective is to directly mine for outliers and then determine their usefulness. The study of outlier detection in data mining was pioneered by the work of Knorr and Ng [1997] who proposed a definition of distance-based outliers which relaxed strict distributional assumptions and was readily generalisable to multi-dimensional data sets.", "startOffset": 101, "endOffset": 526}, {"referenceID": 1, "context": "Following Knorr and Ng, several variations and algorithms have been proposed to detect distance-based outliers [Bay and Schwabacher, 2003, Ramaswamy et al., 2000]. However, the outliers detected by these methods are global outliers, i.e., the \u201coutlierness\u201d is with respect to the whole dataset. Breunig et al. [2000] have argued that in some situations local outliers are more important than global outliers and cannot be easily detected by standard distance-based techniques.", "startOffset": 112, "endOffset": 317}, {"referenceID": 25, "context": "The MCD (as noted in the Introduction), is one prominent approach where the problem of outliers was tightly integrated in an optimisation framework [Rousseeuw and Driessen, 1999].", "startOffset": 148, "endOffset": 178}, {"referenceID": 6, "context": "The k-means-- approach extended the idea of MCD to include clustering [Chawla and Gionis, 2013].", "startOffset": 70, "endOffset": 95}], "year": 2014, "abstractText": "In this paper we present methods for exemplar based clustering with outlier selection based on the facility location formulation. Given a distance function and the number of outliers to be found, the methods automatically determine the number of clusters and outliers. We formulate the problem as an integer program to which we present relaxations that allow for solutions that scale to large data sets. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable, i.e. it is easier to distinguish between outliers which are the result of data errors from those that may be indicative of a new pattern emergent in the data. We present and contrast three relaxations to the integer program formulation: (i) a linear programming formulation (LP) (ii) an extension of affinity propagation to outlier detection (APOC) and (iii) a Lagrangian duality based formulation (LD). Evaluation on synthetic as well as real data shows the quality and scalability of these different methods.", "creator": "LaTeX with hyperref package"}}}