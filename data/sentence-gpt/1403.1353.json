{"id": "1403.1353", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Collaborative Representation for Classification, Sparse or Non-sparse?", "abstract": "Sparse representation based classification (SRC) has been proved to be a simple, effective and robust solution to face recognition. As it gets popular, doubts on the necessity of enforcing sparsity starts coming up, and primary experimental results showed that simply changing the $l_1$-norm based regularization to the computationally much more efficient $l_2$-norm based non-sparse version would lead to a similar or even better performance. However, that's not always the case, the results also show that sparsity is still much simpler to model and test.\n\n\nThe first practical step in sparsity is to apply an \"integration matrix\" to the program (the model can be applied in sparsity), one that can be derived from a non-sparse representation, such as the sparsity matrix. In sparsity we use multiple-valued representations and then a single-valued representation, which can be obtained by modeling the program. Since Sparsity only works with a single-valued representation, we can use the $s_1$-norm (or $l_2$-norm based non-sparse version of the program) to derive sparsity.\nIn the same way, the second approach is to use an input matrix. This can be used to model the program, but it does not actually represent the program at all. Sparsity is essentially the same as the current \"regularization,\" where you are able to determine how many variables in the program are valid.\nAs it turns out, the standard sparsity-style \"matrix\" is actually just a representation of the program. When sparsity and the $l_2$-norm based \"matrix\" are defined in sparsity, we get an input matrix of the program. A valid input matrix with a fixed input value (or a value that is not yet valid), is then used to represent the program at all. There are several other ways to calculate the input matrix. Sparsity does not only have an input matrix, but it also also has an input matrix that can be obtained using a single-valued representation, such as the sparsity matrix. Sparsity is only possible to model the program, but it also has a input matrix that can be obtained using a single-valued representation, such as the sparsity matrix.\nWhile Sparsity is only possible to model the program, it also has an input matrix that can be obtained using a single-valued representation, such as the sparsity matrix. We will assume", "histories": [["v1", "Thu, 6 Mar 2014 05:44:32 GMT  (107kb)", "http://arxiv.org/abs/1403.1353v1", "8 pages, 1 figure"]], "COMMENTS": "8 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["yang wu", "vansteenberge jarich", "masayuki mukunoki", "michihiko minoh"], "accepted": false, "id": "1403.1353"}, "pdf": {"name": "1403.1353.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 3.\n13 53\nv1 [\ncs .C\nV ]\n6 M\nar 2\n01 4\nIndex Terms\u2014Sparse representation, collaborative representation, regularization, dictionary learning, pattern classification\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "Recently, a simple approach called sparse representation based classification (SRC) [1], has shown quite impressive results on face recognition and also some other classification tasks [2]. It minimizes the l2-norm based error on reconstructing a test sample with a linear combination of all the training samples whilst limiting the sparsity of reconstruction coefficients. The sparsity term tends to force larger coefficients to be assigned to training samples in the same class as which the test sample belongs to, making such coefficients discriminative for classification. Since the ideal l0-norm for modeling sparsity leads to a computationally expensive or even infeasible combinatorial optimization problem, SRC adopts l1-norm to approximate the l0-norm, though it is still a bit time consuming due to its unavoidable iterative optimization. The main weakness of SRC is that it has two preconditions for ensuring a good performance [2]: the training samples need to be carefully controlled and the number of samples per class has to be sufficiently large, and there is a lack of quantitative criteria for verifying whether they are satisfied or not.\nLater research argued that SRC\u2019s success lies in the collaborative representation using all the training samples, but not the l1-norm based regularization which makes the representation coefficients sparse [3]. It has shown that the l1-norm can be replaced by the computationally much more efficient l2norm, without sacrificing the performance. Therefore, for a better understanding and comparison, these two models were both treated as collaborative representation based classification\n\u2022 Y. Wu, M. Mukunoki, and M. Minoh are with the Academic Center for Computing and Media Studies, Kyoto University, Kyoto, 606-8501, Japan. E-mail: {yangwu, mukunoki, minoh}@mm.media.kyoto-u.ac.jp. \u2022 V. Jarich is with the Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Kyoto, 606-8501, Japan. E-mail: vansteenberge@mm.media.kyoto-u.ac.jp.\n(CRC), and the regularization term was used to differentiate them. Here we follow the same notation and name them CRC l1 and CRC l2, standing for sparse representation and non-sparse representation, respectively, because the representation coefficients regularized by l1-norm are widely-regarded to be sparse while the ones regularized by l2-norm are generally non-sparse.\nSince the birth of CRC l2, more and more attention has been paid to l2-norm based regularization for collaborative representation due to its attractive effectiveness and efficiency. Though experiments on both CRC l2 itself [4] and its extensions [5] have shown their superiority to the sparse representation competitors, there are counterexamples reported as well [4], [6]. The uncertain relative superiority between sparse and non-sparse CRC models confuses people who have limited research experiences on them, and there is still a lack of an in-depth and reliable criterion for preselecting the more promising model for a given task. This paper presents our study for solving this problem. Specifically, we contribute in two aspects:\n\u2022 We propose an analytic scoring function, depending on only the given feature vectors and the size of the dataset, for predicting whether the CRC model should be sparse or non-sparse. Extensive and representative experiments on various classification tasks have demonstrated the effectiveness of this function. \u2022 We further discuss the important direction of extending collaborative representation with dictionary learning whilst proposing a very simple dictionary learning approach for non-sparse collaborative representation (named DL-NSCR), which is as far as we are aware the first of its kind. Extensive experiments have shown that DL-NSCR is generally superior to the most similar as well as other state-of-the-art dictionary learning models\nfor sparse representation in term of effectiveness, robustness and efficiency.\nThe rest parts of the paper are organized as follows. A brief introduction of the background knowledge for collaborative representation is given in section 3 after commenting on the related work in section 2. Section 4 presents the details of DLNSCR. All the experiments and results are stated and analyzed in section 5, while the conclusions and future work are given in section 6."}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 Comparison between CRC l1 and CRC l2", "text": "After proposing CRC l2, Zhang et.al. [4] have done more experiments on comparing CRC l1 with CRC l2, along with their robust versions for handling occlusions/corruptions. They concluded that the relative superiority between them depends on the feature dimensionality of data. It was supposed that high dimensionality corresponds to high discrimination power and in this case the coefficients tend to be naturally and passively sparse even without sparse regularization, so CRC l2 can do a better job than CRC l1. If the dimension is very low, it will lead to the opposite result. While we agree with the point that CRC l2 favors higher dimension, we don\u2019t think the relative superiority only depends on feature dimensionality. Note that the assumption of correspondence between feature dimensionality and data discrimination power in Zhang et.al.\u2019s work [4] is unreliable, because you may have arbitrarily different features with quite different discrimination abilities for a given dimensionality. Even when the same feature vectors are projected into different spaces using certain dimension reduction approach, there are still counterexamples to the effectiveness of using dimensionality as the indicator, as shown in [6]. Besides this issue, there is another drawback of Zhang et.al.\u2019s experiments: they are limited to only 3 face datasets, though different subsets/variations of them have been tested on."}, {"heading": "2.2 Comparison on extended CRC models", "text": "The debate between sparse and non-sparse collaborative representation is not only limited to the simplest CRC l1 and CRC l2 models. Recently, a model called Extended SRC (ESRC) [7] added another generic dictionary based on a thirdparty dataset for handling the within-class variations. Since the additional dictionary may be able to cover the possibly large changes between the test sample and the corresponding training samples from the same class, ESRC can be applied to single-shot recognition problems where only a single training sample is available for each class. Later on, a non-sparse version Extended CRC (ECRC) [5] was published. The only difference between ECRC and ESRC is that ECRC uses l2norm instead of l1-norm for coefficients regularization. Experimental results on several widely used face recognition datasets showed that ECRC is much faster and more effective than ESRC. Though being interesting and valuable, the comparison has a limitation that both of these two models depend on the third-party data which brings two new problems: the relative\nsuperiority between ESRC and ECRC may depend on the third-party data and the selection and evaluation of this thirdparty data is not a trivial task."}, {"heading": "2.3 An important unexplored area: dictionary learning", "text": "There is another more important and influential direction for enhancing the collaborative representation models \u2014 dictionary learning (DL), i.e., learning a discriminative dictionary from the training data instead of directly using it as the dictionary. Generally speaking, dictionary learning can significantly improve the discrimination and generalization abilities of CRC models without relying on any other additional data.\nQuite a few publications on DL can be found for sparse representation, but as far as we are aware no such model has ever been proposed for non-sparse representation. The existing DL models can be roughly grouped into three categories: making the dictionary discriminative, learning a discriminative model for classification with the coefficients, and simultaneously optimizing the dictionary and the coefficients-based classification model.\nThe first group [8], [9] follow the classification model of CRC l1 in using only the class-specific reconstruction error, directly targeting a discriminative dictionary. The second group try to learn discriminative classification models on the sparse representation coefficients, including logistic regression [10] and linear regression (D-KSVD [11] and LC-KSVD [12] which adds one more linear regression term to D-KSVD to further enhance the label consistency within each class). The third group so far contain only one representative work called Fisher discrimination dictionary learning (FDDL) [13]. It has a discriminative fidelity term which minimizes the reconstruction error using both a global dictionary and classspecific sub-dictionaries, while at the same time minimizes the ability of the sub-dictionaries on reconstructing samples from different classes. Besides that, FDDL also has a discriminative coefficient term which utilizes the Fisher discriminant to make the coefficients discriminative. Very recently, a new model called DL-COPAR [14] developed the idea proposed in DLSI [9] on exploring the common bases of subdictionaries by explicitly separating the particularity (classspecific sub-dictionaries) and commonality (a common subdictionary) in dictionary learning. Meanwhile, it also inherited the incoherence term from [9] and the third part of the fidelity term in FDDL to make the class-specific sub-dictionaries as discriminative as possible.\nDespite their differences in learning the discriminative model, all these approaches enforce the sparsity of the coefficients using either l0-norm or l1-norm, which is usually computationally expensive."}, {"heading": "3 COLLABORATIVE REPRESENTATION AND DICTIONARY LEARNING", "text": ""}, {"heading": "3.1 Sparse representation", "text": "Suppose a training dataset X = [X1, . . . , XL] \u2208 Rd\u00d7n is given, where n denotes the total number of samples; d denotes\ntheir feature dimension; L is the number of classes; and Xi denotes the ni samples belonging to class i. SRC (i.e., CRC l1) seeks a reconstruction of test sample y \u2208 Rd via a linear combination of all the training samples X , while at the same time minimizes the l1-norm sparsity of the reconstruction coefficients. The coding model can be formulated as:\n\u03b1\u0302 = argmin \u03b1\n\u2016y \u2212X\u03b1\u201622 +\u03bb1\u2016\u03b1\u2016 2 1, (1)\nwhere \u03b1\u0302 = [\u03b1\u03021, . . . , \u03b1\u0302L] are the concatenated reconstruction coefficients corresponding to training samples of different classes, and \u03bb1 is a regulatory parameter for weighting the regularization of \u03b1. For classification, CRC l1 computes the representation residual for each class:\nri (y) = \u2016y \u2212Xi\u03b1\u0302i\u2016 2 2, \u2200i \u2208 {1, . . . , L} , (2)\nand then classifies y by C (y) = argminiri (y)."}, {"heading": "3.2 Non-sparse representation", "text": "According to the recent arguments from [3], in the SRC (i.e., CRC l1) model, it is the collaborative representation with all classes but not the l1-norm regularization term that truly contributes to the good face recognition performance. Therefore, they proposed the following non-sparse scheme (CRC l2 in this paper) which replaces Equation 1 by\n\u03b1\u0302 = argmin \u03b1\n\u2016y \u2212X\u03b1\u201622 +\u03bb1\u2016\u03b1\u2016 2 2. (3)\nThe biggest benefit of this replacement is the dramatic reduction of computational cost, because this new convex optimization problem has a closed-form solution\n\u03b1\u0302 = ( XTX + \u03bb1 \u00b7 I )\u22121 XTy. (4)\nMore attractively, this solution is just a linear projection of y, and the projector P = ( XTX + \u03bb1 \u00b7 I )\u22121\nXT is independent of y. P can be pre-computed given the training data, so it doesn\u2019t require a separate optimization process for each test sample as CRC l1 demands.\nThough the l2-norm based regularizer in Equation 3 is no longer a sparsity constraint on the coefficients, it still has the potential to induce the competition among training samples from all candidate classes, which may cause the right class to have relatively smaller reconstruction error and larger l2norm values of coefficients. Therefore, CRC l2 computes the normalized residuals\nri (y) = \u2016y \u2212Xi\u03b1\u0302i\u2016 2 2 /\u2016\u03b1\u0302i\u20162, \u2200i, (5)\nand then classifies y by C (y) = argminiri (y)."}, {"heading": "3.3 Dictionary learning", "text": "Instead of using the training data itself as the reconstruction dictionary, dictionary learning techniques seek to learn a compact yet over-complete dictionary from the training data, so that it can scale up to large amounts of training samples\nwhile at the same time being as discriminative as possible. A general dictionary learning model is:\n\u3008D\u2217,W \u2217, A\u2217\u3009\n= arg min D,W,A\n{\nr (X,D,A) + \u03bb1 \u2016A\u2016 2 p + \u03bb2f (W,A)\n}\n,\n(6) where D = [D1, . . . , DL] \u2208 Rd\u00d7K is the learned dictionary from X (usually K \u2264 n); A denotes the reconstruction coefficients over D for X ; W denotes the learned parameters of the discriminative model f(W,A) for classification with A; r(X,D,A) is the discriminative reconstruction model defined over D (called the discriminative fidelity in [13]); and \u03bb1 and \u03bb2 are trade-off parameters.\nThough most of the existing dictionary learning models can be covered by the above general model (some models do not have the f(W,A) term), they vary in their detailed design of r(X,D,A) and f(W,A), resulting in different performances and speeds. However, all the proposed dictionary learning models have to iteratively optimize the model parameters D (and W ) and the coefficients A due to the difficulty on optimizing them simultaneously. As mentioned before, so far only p = 0 and p = 1 have been explored."}, {"heading": "4 DICTIONARY LEARNING FOR NON-SPARSE REPRESENTATION", "text": "The proposed DL-NSCR model inherits the design of the r(X,D,A) term from FDDL [13], but discards its timeconsuming f(W,A) term to keep the model light. Note that there is a big difference between it and other existing dictionary learning models: it adopts the computationally efficient l2-norm (\u2016 \u00b7 \u2016F in matrix format) to regularize A."}, {"heading": "4.1 Learning model", "text": "In DL-NSCR, r(X,D,A) is designed as follows.\nr (X,D,A) = \u2016X \u2212DA\u2016 2 F +\nL \u2211\ni=1\n\u2225 \u2225Xi \u2212DiA i i \u2225 \u2225\n2\nF\n+\nL \u2211\ni=1\nL \u2211\nj=1,j 6=i\n\u2225 \u2225DiA i j \u2225 \u2225\n2 F , (7)\nwhere \u2016\u00b7\u2016F denotes the Frobenius norm 1, and Aij denotes the coefficients corresponding to the sub-dictionary Di of class i for those samples from Xj (i.e. the columns of Aij correspond to class j). In this model, the first term is the overall reconstruction error (ensuring that D can well represent X); the second term is the class-specific reconstruction error (forcing Di to be able to well represent Xi); and the third term is the confusion factor (restricting Di\u2019s ability on reconstructing samples from any other class rather than i). It is easy to tell that such a definition of r(X,D,A) will force D to be discriminative.\nWith the term of r(X,D,A), the overall learning model of DL-NSCR is as simple as\n\u3008D\u2217, A\u2217\u3009 = argmin D,A\n{\nr (X,D,A) + \u03bb1 \u2016A\u2016 2 F\n}\n. (8)\n1. Frobenius norm is a generalization of l2-norm (squared root of sum of squares) from dealing with vectors to operating on matrices."}, {"heading": "4.2 Optimization", "text": "Similar to other dictionary learning algorithms, the optimization of DL-NSCR is done iteratively between optimizing A and optimizing D until the iteration converges."}, {"heading": "4.2.1 Initialization", "text": "We use principle component analysis for initializing Di with samples from class i. However, it is also acceptable to initialize with random numbers, which will only cost a few more iterations."}, {"heading": "4.2.2 Optimizing A given a fixed D", "text": "Given D, thanks to the fact that Frobenius norm is decomposable, optimizing A is equivalent to optimizing Ai for each i \u2208 {1, . . . , L} independently as follows.\nA\u2217i = argmin Ai\n\n\n\n\u2016Xi \u2212DAi\u2016 2 F +\n\u2225 \u2225 \u2225 DS\\iS T \\iAi \u2225 \u2225 \u2225 2\nF\n+ \u2225\n\u2225Xi \u2212DSiS T i Ai\n\u2225 \u2225 2\nF + \u03bb1 \u2016Ai\u2016 2 F\n\n\n\n,\n(9) where\nSi =\n\n\nO\u2211i\u22121 m=1 Km\u00d7Ki\nIKi\u00d7Ki O\u2211L\nm=i+1 Km\u00d7Ki\n\n ,\nS\\i = [S1, \u00b7 \u00b7 \u00b7 , Si\u22121, Si+1, \u00b7 \u00b7 \u00b7 , SL] ,\n(10)\nwith Ki, i \u2208 {1, . . . , L} denoting the dictionary size of Di. Here Si and S\\i are matrices for selecting the specific subdictionaries, while O and I denote zero matrix and identity matrix, respectively. This optimization problem can be rewritten into a simpler form\nA\u2217i = argmin Ai\n{\n\u2016Ri \u2212 ZiAi\u2016 2 F + \u03bb1 \u2016Ai\u2016 2 F\n}\n, (11)\nwhere\nRi =\n\n Xi Xi\nOd\u00d7ni\n\n , Zi =\n\n D DSiS T i\nDS\\iS T \\i\n\n . (12)\nWith only the Frobenius norm in its optimization objective function, Ai has a closed-form solution\nA\u2217i = ( ZTi Zi + \u03bb1 \u00b7 I )\u22121 ZTi Ri. (13)\nEquation 11 has exactly the same form as the CRC l2 model, so it is computationally very efficient."}, {"heading": "4.2.3 Optimizing D given a fixed A", "text": "When fixing A, the term \u03bb1 \u2016A\u2016 2 F becomes a constant, however, D is still impossible to be optimized as a whole because the objective function in Equation 8 has two terms which are functions of sub-dictionaries Di, i \u2208 {1, . . . , L} but not the overall dictionary D. Therefore, we optimize Dis one-by-one, assuming the others are fixed. Concretely,\nD\u2217i = argmin Di\n{\n\u2016Ui \u2212DiVi\u2016 2 F\n}\n, (14)\nwhere Ui = [ X \u2212D\\iA \\i, Xi,Od\u00d7(n\u2212ni) ] ,\nVi = [ Ai, Aii, A i \\i ] . (15)\nIn Equation 15, D\\i denotes all the Djs with j 6= i and A\\i denotes the corresponding coefficients (i.e. without Ai), where \u201c\\i\u201d means without class i. Od\u00d7(n\u2212ni) denotes a d\u00d7(n\u2212 ni) dimensional zero matrix, where ni is the number of samples in class i and n = \u2211L\ni=1 ni. Equation 14 has a closed-form solution\nD\u2217i = UiV T i\n(\nViV T i\n)\u22121 . (16)\nNote that optimizing Di depends on a given D\\i, which means once Di is updated, it should be used to update each Dj, j 6= i in D\\i. This is a chicken-and-egg problem, so a straightforward solution consists in updating all the Dis iteratively until convergence. However, since we are iterating between optimizing A and updating D, a converged D will soon been changed once A is recomputed. Therefore, in our implementation we ignored the inner-iteration in D\u2019s optimization, and found it still worked quite well."}, {"heading": "4.3 Classification model", "text": "After learning D, we can use it for solving both single-sample based and set based classification problems, all of which will be covered in our experiments.\nFor single-sample based classification, we follow CRC on reconstructing an arbitrary test sample y by D with its reconstruction coefficients obtained by solving\n\u03b1\u0302 = argmin \u03b1\n\u2016y \u2212D\u03b1\u2016 2 2 +\u03bb1\u2016\u03b1\u20162, (17)\nwhose closed-form solution is \u03b1\u0302 = ( DTD + \u03bb1 \u00b7 I )\u22121\nDTy. Then y is classified by C (y) = argminiri (y) with\nri (y) = \u2016y \u2212Di\u03b1\u0302i\u2016 2 2 /\u2016\u03b1\u0302i\u20162. (18)\nFor set based classification, we have a similar reconstruction model which just replaces y by a set of test samples Y , and the reconstruction coefficients for Y are A\u0302 = (\nDTD + \u03bb1 \u00b7 I )\u22121\nDTY . Then, we classify Y by C (Y ) = argminiri (Y ) with\nri (Y ) = \u2225 \u2225Y \u2212DiA i \u2225 \u2225\n2 F +\nL \u2211\nj=1,j 6=i\n\u2225 \u2225DiA i j \u2225 \u2225\n2 F . (19)"}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "We conduct our experiments on five visual recognition tasks using nine public benchmark datasets. They are chosen to cover different scenarios: appearance-based face recognition in controlled environment, texture recognition focusing on texture information, leaf categorization using shape information, food categorization with the stuffs rich of highly varying color, texture and shape information, and appearancebased across-camera person re-identification in uncontrolled environment. Besides that, we follow [15], [16] on varying the number of samples per class for 2 of the 3 adopted reidentification datasets with 3 different values for each of them to investigate how this factor influences the performance of the tested models. Therefore, there are totally 13 different data settings for experiments.\nSuch extensive and diverse experiments extend the scope of collaborative representation from face recognition to general recognition/classification tasks with various statistics, properties, and feature representations. They are different from many other experiments in the literature which use artificially generated versions of the same dataset (for example, changing the feature dimension by applying dimension reduction methods), because the artificial data may override the true factors we\u2019re looking for.\nFor all the methods to be compared in each experiment, we used exactly the same features and data splits (10 splits for averaging if allowed). The regulatory parameter for the sparse/non-sparse regularization term in all the concerned models are set to be the same for a fair comparison. Specifically, we have it set to 0.5 for person re-identification datasets and 0.0001 for all the others, which was proved to be a good choice. We notice that finding the best value for this regulatory parameter and analyzing the sensitivity of it for each method are important open issues, but they are not the focus of this paper."}, {"heading": "5.1 Datasets and settings", "text": "Face Recognition. For face recognition, we choose two widely-used datasets: Extended Yale B [17] and AR [18]. The Extended Yale B dataset contains 2414 frontal-face images belonging to 38 individuals. These images were captured under different lighting conditions with various facial expressions. We preprocessed the data according to [1]: using cropped images with a size of 198 \u00d7 168 pixels; randomly selecting half of the samples for training and testing (about 32 samples per person); projecting each image into a 504-dimensional feature space using a random matrix generated from a zeromean normal distribution with its rows normalized (by l2norm). The size of the class-specific dictionary Ki for all the dictionary learning models was set to 15, so K = 570, as suggested by [12]. For DL-COPAR, which will be compared with, we had Ki = 14 and the common sub-dictionary size set to be 38. The AR dataset contains more variations than the Extended Yale B dataset, including illumination, expression and disguises changes. Following [1] and [13], we use a subset of 100 subjects (50 male and 50 female) with only 14 images containing illumination and expression changes for each of them (7 from session 1 for training and another 7 from session 2 for testing). We use 300-dimensional Eigenfaces for feature representation. Ki was set to 7. For DL-COPAR, Ki = 6 and the common sub-dictionary is assigned a size of 2. Since the training data and the test data are all fixed, only one round of experiment was conducted. Texture Recognition. We work on two representative datasets: the KTH-TIPS dataset and the CUReT dataset, because both of them have many samples for each class (may satisfy SRC\u2019s first precondition) and great within-class variations including illumination, viewpoint and scale changes. They are also different from each other in the sense that KTHTIPS has greater within-class variations while CUReT has significantly more classes. We adopt the 1180-dimensional PRI-CoLBP0 feature descriptor proposed in [19] due to its high performances. Ki was chosen to be 10 for both datasets, and the suggested common sub-dictionary size for DL-COPAR is 5. Leaf Categorization. The popular Swedish leaf dataset is used here for leaf recognition. It was carefully built with all the leaves well-aligned, open, flat, and complete. Only one side of each leaf is photographed against a clean background. Though its strict settings make the problem much easier than it might be in real applications, it has the advantage of making the problem clean and focused, i.e., distinguishing different leaf species mainly by their flat shapes. This dataset contains 15 species of leaves, with 75 images for each of them. Following the state-of-the-art model spatial PACT [20], we have 25 images per class sampled for training and the rest left for testing. Ki was set to 12, and the common sub-dictionary size for DL-COPAR was set to 2 (which was better than others). Again, we use PRI-CoLBP0 as the feature descriptor. Food Categorization. It is a relatively less popular visual recognition problem due to its difficulty and the lack of good benchmark datasets. The recently released Pittsburgh Food Image Dataset(PFID) might be a good starting point. It owns fast food images and videos collected from 13 chain restaurants and acquired under lab and realistic settings. Following Yang et.al. [21], we focus on the same set of 61 categories of specific food items with background removed. This set has three different instances for each category, which were bought from different chain stores on different days. Each instance has six images taken from different viewpoints. We follow the standard experimental protocol in using 12 images from two instances for training and the other 6 images from the third instance for testing. This allows a 3-fold cross-validation. The 1180-dimensional PRI-CoLBP0 feature is extracted for each color channel and thus the whole feature vector is 3540- dimensional. We set Ki = 6 for all dictionary learning models and the common sub-dictionary for DL-COPAR had a size of 3. Person re-identification. We experiment on three recently built datasets \u201ciLIDS-MA\u201d, \u201ciLIDS-AA\u201d [15] and CAVIAR4REID[22]. They are representatives of cross-camera re-identification with non-overlapping views in real scenarios. The first two were collected from the i-LIDS video surveillance data captured at an airport, while the third one consists of several sequences filmed in a shopping centre. The iLIDSMA dataset has 40 persons with exactly 46 manually cropped images per camera for each person, resulting in 3680 images in total. Unlike iLIDS-MA, the iLIDS-AA dataset was extracted automatically using a HOG-based detector instead of manual annotation. Such a property simulates the localization errors of human detection and tracking in real systems. Moreover, iLIDS-AA is also much bigger than iLIDS-MA. It contains as many as 100 individuals totaling 10754 images. Since it was automatically annotated, the number of images for each person varies from 21 to 243. For both datasets, a certain number of samples (10, 23, or 46) are randomly sampled from one camera (Camera 3) for each person to serve as the training data (i.e. the gallery), and the same amount of samples are randomly sampled from the other camera (Camera 1) for testing (i.e. as queries). It results\nin three different versions for each dataset, named as \u201ciLIDSMA10\u201d, \u201ciLIDS-MA23\u201d, etc. Note that for iLIDS-MA46 there is only one data sampling result. Compared with iLIDSMA and iLIDS-AA datasets, CAVIAR4REID has broader resolution changes and larger pose variations. We follow [22] on training with 22 specified subjects and testing on the other 50 subjects. Each set (either for gallery or query) contains 5 randomly sampled images. Like [16], we perform multipleshot re-identification and treat it as a set-based classification problem. Therefore, the set-based classification model of DLNSCR is used.\nThe person re-identification problem is usually treated as a ranking problem, and it is desired that the correct match for a given querying individual appears in the top-ranked candidates, so we used the cumulative recognition rate at rank top 10% as the effectiveness measure. We used exactly the same 400-dimensional color and texture histograms based features as adopted in [23] for all the methods. We had Ki, \u2200i chosen to be the same as the number of samples per class for the iLIDS-MA dataset and the CAVIAR4REID dataset, but for the iLIDS-AA dataset they were set to be Ki = 8, \u2200i. The common sub-dictionary size for DL-COPAR got the same value as Ki for all the datasets."}, {"heading": "5.2 Sparse or non-sparse representation?", "text": "For justifying whether collaborative representation should be sparse or non-sparse, we compare the l1-norm regularization with the l2-norm regularization, i.e., compare CRC l1 with CRC l2. The results are shown in Table 1. Clearly, none of them completely outperforms the other, though CRC l1 only wins on 2 of the 13 datasets. Therefore, the question becomes when shall we choose which one if we want to get a better performance, and we expect to get the answer before applying them to the data. For an easy comparison between them, we propose a relative superiority measure called Error Reduction Rate (ERR), which is defined as\nERR = Err(CRC l1)\u2212 Err(CRC l2)\nErr(CRC l1) (20)\n= Acc(CRC l2)\u2212Acc(CRC l1)\n1\u2212Acc(CRC l1) , (21)\nwhere Err(\u00b7) and Acc(\u00b7) denote the error rate and accuracy rate of the concerned model, respectively. ERR shows how much performance improvement can be got from replacing CRC l1 by CRC l2. Therefore, positive ERR values indicate CRC l2 performs better than CRC l1, while negative ones stand for the opposite. The larger the ERR value is, the more the data favors non-sparse representation.\nIn order to predict how much a specific dataset might favor non-sparse representation before knowing the ERR value, it is necessary to design some scoring function (S(l2, l1)) which coincides with ERR. It should be function of some statistics and properties of the dataset. Therefore, we list the representative statistics for each of the 13 datasets as shown in Table 1, including the feature dimensionality d, the number of classes C, the number of training samples per class ni, and the total number of training samples n. Besides\nthat, we believe that datasets should have some properties which are independent from these simple statistics. One of the most important properties is the quality of features, which directly influences the performance of a classification model. An intuitive feeling is that larger ERR may be related to better features, because better features generally enable that samples in the same class stay relatively closer to each other and thus make it easier to generate a discriminative collaborative representation even with non-sparse coefficients. Therefore, we design a Feature Discrimination Rate (FDR) as\nFDR = Acc(MPD)\nAcc(chance) , (22)\nwhere MPD is the simplest Minimum Point-wise Distance based classifier, and \u201cchance\u201d is the method of randomly guessing the class label for the test sample(s). In greater details, MPD uses Euclidean distance for point-wise distance measurement and treats the minimum point-wise distance between the test sample(s) and the training samples belong to a specific class as the dissimilarity between them. MPD directly uses such a dissimilarity for classification. Since its performance purely relies on the feature space, it makes FDR a good measure of the features\u2019 discriminative power. Note that for FDR, Acc(\u00b7) uses the top-1 recognition accuracy, which is different from that for ERR.\nBoth MPD\u2019s accuracy rates and the FDR values for all datasets are given in Table 1. However, FDR itself is not proportional to ERR. Recall that there are many evidences in the literature showing that a larger d can lead to a higher ERR, so we also test the effectiveness of \u201cFDR\u00d7d\u201d. Unfortunately, it still does not coincide with ERR. To enhance it, we propose to have the total number of training samples n involved in an inversely proportional way as\nS(l2, l1) = FDR \u00d7 d/n. (23)\nThe reason is quite simple: a smaller n generally means a lower redundancy of training samples so that the representation coefficients are likely to be denser but not sparser. To prove that, we simply test \u201cFDR/n\u201d and find it can already differentiate negative ERR values from positive ones. However, we insist that \u201cFDR\u00d7d/n\u201d is a better choice. When the four special datasets (marked with a star in Table 1 because they contain only 1 to 3 trials or have large within-class variations with few samples per class) are not considered, \u201cFDR\u00d7d/n\u201d clearly has a more definite relationship w.r.t to ERR in terms of a linear or a power function with low SSE (Sum of Squared Errors) than \u201cFDR/n\u201d, as shown in Figure 1. Note that the threshold of \u201cFDR\u00d7d/n\u201d in our experiments (say 5.0) for differentiating positive ERR values from negative ones may not be accurate enough for generalization to other datasets, but we believe that it can be a good reference."}, {"heading": "5.3 Dictionary learning", "text": "We compare the simple dictionary learning model for nonsparse representation (DL-NSCR) with CRC l2, CRC l1, and the most influential dictionary learning models for sparse representation which are generally more complex than DL-NSCR,\nScore\nScore\nincluding FDDL [13], LC-KSVD [12], and DL-COPAR [14]. The results shown in Table 2 clearly show that even DL-NSCR can significantly promote CRC l2\u2019s performance on 12 of the 13 datasets, and it also outperforms all its competitors on 4 of the 5 classification tasks, except on the person re-identification task. The relatively lower performance boost of DL-NSCR on the re-identification datasets is probably due to the difficulty of the data which limits the room for improvement (all the MPD rates on these datasets are lower than those for the other datasets). Even though, it is interesting to see that DLNSCR performs the best on the two datasets which favors CRC l1 than CRC l2. Moreover, when we focus on FDDL as it is most similar to DL-NSCR, we can see that it only wins DL-NSCR on 2 of the 13 datasets (iLIDS-MA23 and iLIDS-MA46). Therefore, generally speaking, when dictionary learning is taken into account, non-sparse representation looks more promising than sparse representation. Note that DLNSCR is just a simple example, and it will be promising and valuable to explore better dictionary learning models for nonsparse collaborative representation."}, {"heading": "5.4 Complexity and running time", "text": "DL-NSCR takes an alternative optimization model which has theoretical guarantee for global convergence and local qlinear (faster than linear) convergence speed [24]. In our experiments, it always converges within several steps. Considering that each iteration only contains basic matrix op-\nerations, it is easy to get the complexity for training and testing DL-NSCR. Concretely, the complexity for training is O ( (K + n)dKL+K3L+K2n )\n, and that for testing is O ((d+K + L)KL). It can be seen that DL-NSCR scales less than linearly with d, L, and n, but nearly proportionally to K3. Thus, when the K is predetermined, it scales well with d and n. Table 3 presents the actual running time for all the concerned methods on four representative datasets. Clearly, DL-NSCR is the fastest dictionary learning model, and its testing time is also comparable to the fastest dictionary learning based sparse representation model."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "We have shown a promising scoring function for pre-selecting sparse or non-sparse collaborative representation models. DLNSCR, the simple dictionary learning model for non-sparse collaborative representation, has demonstrated its superiority to both sparse and non-sparse collaborative representation models and those state-of-the-art dictionary learning models for sparse representation. There is still a large room for further enhancing it with better dictionary learning models, and it will be very interesting to have a more comprehensive comparison between them and existing dictionary learning models."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by \u201cR&D Program for Implementation of Anti-Crime and Anti-Terrorism Technologies for a Safe and Secure Society\u201d, Funds for integrated promotion of social system reform and research and development of the Ministry of Education, Culture, Sports, Science and Technology, the Japanese Government."}], "references": [{"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Anal. Machine Intell, vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse representation for computer vision and pattern recognition", "author": ["J. Wright", "Y. Ma", "J. Mairal", "G. Spairo", "T. Huang", "S. Yan"], "venue": "Proceedings of the IEEE, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse representation or collaborative representation: which helps face recognition?", "author": ["L. Zhang", "M. Yang", "X. Feng"], "venue": "in Proc. Internat. Conf. on Computer Vision (ICCV),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Collaborative representation based classification for face recognition", "author": ["L. Zhang", "M. Yang", "X. Feng", "Y. Ma", "D. Zhang"], "venue": "CoRR, vol. abs/1204.2358, 2012, arXiv: 1204.2358.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Extended crc: Face recognition with a single training image per person via intraclass variant dictionary", "author": ["G. Lin", "M. Xie", "L. Mao"], "venue": "IEICE TRANS. on Information and Systems, vol. E96-D, no. 10, pp. 2290\u20132293, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive and weighted collaborative representations for image classification", "author": ["R. Timofte", "L.V. Gool"], "venue": "Pattern Recog. Lett, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Extended src: Undersampled face recognition via intraclass variant dictionary", "author": ["W. Deng", "J. Hu", "J. Guo"], "venue": "IEEE Trans. Pattern Anal. Machine Intell, vol. 34, no. 9, pp. 1864\u20131870, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1864}, {"title": "Metaface learning for sparse representation based face recognition", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": "IEEE Internat. Conf. on Image Processing (ICIP), 2010, pp. 1601 \u20131604.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "IEEE Conf. On Computer Vision And Pattern Recognition (CVPR), 2010, pp. 3501 \u20133508.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Neural Information Processing Systems (NIPS), 2009, pp. 1033\u20131040.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative k-svd for dictionary learning in face recognition", "author": ["Q. Zhang", "B. Li"], "venue": "IEEE Conf. On Computer Vision And Pattern Recognition (CVPR), 2010, pp. 2691 \u20132698.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent k-svd", "author": ["Z. Jiang", "Z. Lin", "L. Davis"], "venue": "IEEE Conf. On Computer Vision And Pattern Recognition (CVPR), 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Fisher discrimination dictionary learning for sparse representation", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": "Proc. Internat. Conf. on Computer Vision (ICCV), 2011, pp. 543 \u2013550.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "A dictionary learning approach for classification: Separating the particularity and the commonality", "author": ["S. Kong", "D. Wang"], "venue": "European Conf. on Computer Vision (ECCV), 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Boosted human reidentification using riemannian manifolds", "author": ["S. Bak", "E. Corvee", "F. Bremond", "M. Thonnat"], "venue": "Image and Vision Computing, vol. 30, no. 6-7, pp. 443 \u2013 452, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Collaborative sparse approximation for multiple-shot across-camera person reidentification", "author": ["Y. Wu", "M. Minoh", "M. Mukunoki", "W. Li", "S. Lao"], "venue": "IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS), 2012, pp. 209 \u2013214.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "From few to many: illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Machine Intell, pp. 643 \u2013660, 2001.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "The ar face database", "author": ["A. Martinez", "R. Benavente"], "venue": "CVC Technical Report 24, June 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Pairwise rotation invariant cooccurrence local binary pattern", "author": ["X. Qi", "R. Xiao", "J. Guo", "L. Zhang"], "venue": "European Conf. on Computer Vision (ECCV), 2012, pp. 158\u2013171.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Centrist: A visual descriptor for scene categorization", "author": ["J. Wu", "J. Rehg"], "venue": "IEEE Trans. Pattern Anal. Machine Intell, vol. 33, no. 8, pp. 1489\u20131501, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Food recognition using statistics of pairwise local features", "author": ["S. Yang", "M. Chen", "D. Pomerleau", "R. Sukthankar"], "venue": "IEEE Conf. On Computer Vision And Pattern Recognition (CVPR), 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Custom pictorial structures for re-identification", "author": ["D.S. Cheng", "M. Cristani", "M. Stoppa", "L. Bazzani", "V. Murino"], "venue": "British Machine Vision Conference (BMVC), 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust object recognition via third-party collaborative representation", "author": ["Y. Wu", "M. Minoh", "M. Mukunoki", "S. Lao"], "venue": "Internat. Conf. on Pattern Recognition (ICPR), November 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Convergence of alternating optimization", "author": ["J.C. Bezdek", "R.J. Hathaway"], "venue": "Neural, Parallel Sci. Comput., vol. 11, no. 4, pp. 351\u2013368, 2003.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Recently, a simple approach called sparse representation based classification (SRC) [1], has shown quite impressive results on face recognition and also some other classification tasks [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Recently, a simple approach called sparse representation based classification (SRC) [1], has shown quite impressive results on face recognition and also some other classification tasks [2].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "The main weakness of SRC is that it has two preconditions for ensuring a good performance [2]: the training samples need to be carefully controlled and the number of samples per class has to be sufficiently large, and there is a lack of quantitative criteria for verifying whether they are satisfied or not.", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "Later research argued that SRC\u2019s success lies in the collaborative representation using all the training samples, but not the l1-norm based regularization which makes the representation coefficients sparse [3].", "startOffset": 206, "endOffset": 209}, {"referenceID": 3, "context": "Though experiments on both CRC l2 itself [4] and its extensions [5] have shown their superiority to the sparse representation competitors, there are counterexamples reported as well [4], [6].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "Though experiments on both CRC l2 itself [4] and its extensions [5] have shown their superiority to the sparse representation competitors, there are counterexamples reported as well [4], [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "Though experiments on both CRC l2 itself [4] and its extensions [5] have shown their superiority to the sparse representation competitors, there are counterexamples reported as well [4], [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "Though experiments on both CRC l2 itself [4] and its extensions [5] have shown their superiority to the sparse representation competitors, there are counterexamples reported as well [4], [6].", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "[4] have done more experiments on comparing CRC l1 with CRC l2, along with their robust versions for handling occlusions/corruptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "\u2019s work [4] is unreliable, because you may have arbitrarily different features with quite different discrimination abilities for a given dimensionality.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "Even when the same feature vectors are projected into different spaces using certain dimension reduction approach, there are still counterexamples to the effectiveness of using dimensionality as the indicator, as shown in [6].", "startOffset": 222, "endOffset": 225}, {"referenceID": 6, "context": "Recently, a model called Extended SRC (ESRC) [7] added another generic dictionary based on a thirdparty dataset for handling the within-class variations.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Later on, a non-sparse version Extended CRC (ECRC) [5] was published.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "The first group [8], [9] follow the classification model of CRC l1 in using only the class-specific reconstruction error, directly targeting a discriminative dictionary.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "The first group [8], [9] follow the classification model of CRC l1 in using only the class-specific reconstruction error, directly targeting a discriminative dictionary.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "The second group try to learn discriminative classification models on the sparse representation coefficients, including logistic regression [10] and linear regression (D-KSVD [11] and LC-KSVD [12] which adds one more linear regression term to D-KSVD to further enhance the label consistency within each class).", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "The second group try to learn discriminative classification models on the sparse representation coefficients, including logistic regression [10] and linear regression (D-KSVD [11] and LC-KSVD [12] which adds one more linear regression term to D-KSVD to further enhance the label consistency within each class).", "startOffset": 175, "endOffset": 179}, {"referenceID": 11, "context": "The second group try to learn discriminative classification models on the sparse representation coefficients, including logistic regression [10] and linear regression (D-KSVD [11] and LC-KSVD [12] which adds one more linear regression term to D-KSVD to further enhance the label consistency within each class).", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "The third group so far contain only one representative work called Fisher discrimination dictionary learning (FDDL) [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 13, "context": "Very recently, a new model called DL-COPAR [14] developed the idea proposed in DLSI [9] on exploring the common bases of subdictionaries by explicitly separating the particularity (classspecific sub-dictionaries) and commonality (a common subdictionary) in dictionary learning.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "Very recently, a new model called DL-COPAR [14] developed the idea proposed in DLSI [9] on exploring the common bases of subdictionaries by explicitly separating the particularity (classspecific sub-dictionaries) and commonality (a common subdictionary) in dictionary learning.", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Meanwhile, it also inherited the incoherence term from [9] and the third part of the fidelity term in FDDL to make the class-specific sub-dictionaries as discriminative as possible.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "According to the recent arguments from [3], in the SRC (i.", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": ", DL] \u2208 R is the learned dictionary from X (usually K \u2264 n); A denotes the reconstruction coefficients over D for X ; W denotes the learned parameters of the discriminative model f(W,A) for classification with A; r(X,D,A) is the discriminative reconstruction model defined over D (called the discriminative fidelity in [13]); and \u03bb1 and \u03bb2 are trade-off parameters.", "startOffset": 318, "endOffset": 322}, {"referenceID": 12, "context": "The proposed DL-NSCR model inherits the design of the r(X,D,A) term from FDDL [13], but discards its timeconsuming f(W,A) term to keep the model light.", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "Besides that, we follow [15], [16] on varying the number of samples per class for 2 of the 3 adopted reidentification datasets with 3 different values for each of them to investigate how this factor influences the performance of the tested models.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "Besides that, we follow [15], [16] on varying the number of samples per class for 2 of the 3 adopted reidentification datasets with 3 different values for each of them to investigate how this factor influences the performance of the tested models.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "For face recognition, we choose two widely-used datasets: Extended Yale B [17] and AR [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "For face recognition, we choose two widely-used datasets: Extended Yale B [17] and AR [18].", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "We preprocessed the data according to [1]: using cropped images with a size of 198 \u00d7 168 pixels; randomly selecting half of the samples for training and testing (about 32 samples per person); projecting each image into a 504-dimensional feature space using a random matrix generated from a zeromean normal distribution with its rows normalized (by l2norm).", "startOffset": 38, "endOffset": 41}, {"referenceID": 11, "context": "The size of the class-specific dictionary Ki for all the dictionary learning models was set to 15, so K = 570, as suggested by [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "Following [1] and [13], we use a subset of 100 subjects (50 male and 50 female) with only 14 images containing illumination and expression changes for each of them (7 from session 1 for training and another 7 from session 2 for testing).", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "Following [1] and [13], we use a subset of 100 subjects (50 male and 50 female) with only 14 images containing illumination and expression changes for each of them (7 from session 1 for training and another 7 from session 2 for testing).", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "We adopt the 1180-dimensional PRI-CoLBP0 feature descriptor proposed in [19] due to its high performances.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Following the state-of-the-art model spatial PACT [20], we have 25 images per class sampled for training and the rest left for testing.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "[21], we focus on the same set of 61 categories of specific food items with background removed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We experiment on three recently built datasets \u201ciLIDS-MA\u201d, \u201ciLIDS-AA\u201d [15] and CAVIAR4REID[22].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "We experiment on three recently built datasets \u201ciLIDS-MA\u201d, \u201ciLIDS-AA\u201d [15] and CAVIAR4REID[22].", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "We follow [22] on training with 22 specified subjects and testing on the other 50 subjects.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "Like [16], we perform multipleshot re-identification and treat it as a set-based classification problem.", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "We used exactly the same 400-dimensional color and texture histograms based features as adopted in [23] for all the methods.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "including FDDL [13], LC-KSVD [12], and DL-COPAR [14].", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "including FDDL [13], LC-KSVD [12], and DL-COPAR [14].", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "including FDDL [13], LC-KSVD [12], and DL-COPAR [14].", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "DL-NSCR takes an alternative optimization model which has theoretical guarantee for global convergence and local qlinear (faster than linear) convergence speed [24].", "startOffset": 160, "endOffset": 164}], "year": 2014, "abstractText": "Sparse representation based classification (SRC) has been proved to be a simple, effective and robust solution to face recognition. As it gets popular, doubts on the necessity of enforcing sparsity starts coming up, and primary experimental results showed that simply changing the l1-norm based regularization to the computationally much more efficient l2-norm based non-sparse version would lead to a similar or even better performance. However, that\u2019s not always the case. Given a new classification task, it\u2019s still unclear which regularization strategy (i.e., making the coefficients sparse or non-sparse) is a better choice without trying both for comparison. In this paper, we present as far as we know the first study on solving this issue, based on plenty of diverse classification experiments. We propose a scoring function for pre-selecting the regularization strategy using only the dataset size, the feature dimensionality and a discrimination score derived from a given feature representation. Moreover, we show that when dictionary learning is taking into account, non-sparse representation has a more significant superiority to sparse representation. This work is expected to enrich our understanding of sparse/non-sparse collaborative representation for classification and motivate further research activities.", "creator": "LaTeX with hyperref package"}}}