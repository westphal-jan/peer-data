{"id": "1403.1497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Active Learning for Autonomous Intelligent Agents: Exploration, Curiosity, and Interaction", "abstract": "In this survey we present different approaches that allow an intelligent agent to explore autonomous its environment to gather information and learn multiple tasks. Different communities proposed different solutions, that are in many cases, similar and/or complementary. These solutions include active learning, exploration/exploitation, online-learning and social learning. The common aspect of all these approaches is that it is the agent to selects and decides what information to gather next. Applications for these approaches already include tutoring systems, autonomous grasping learning, navigation and mapping and human-robot interaction. We discuss how these approaches are related, explaining their similarities and their differences in terms of problem assumptions and metrics of success. We consider that such an integrated discussion will improve inter-disciplinary research and applications. In general, we are interested in the need to develop new approaches to inter-disciplinary and collaborative research.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 6 Mar 2014 17:12:30 GMT  (2786kb,D)", "http://arxiv.org/abs/1403.1497v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["manuel lopes", "luis montesano"], "accepted": false, "id": "1403.1497"}, "pdf": {"name": "1403.1497.pdf", "metadata": {"source": "CRF", "title": "Active Learning for Autonomous Intelligent Agents: Exploration, Curiosity, and Interaction", "authors": ["Manuel Lopes", "Luis Montesano"], "emails": ["manuel.lopes@inria.fr", "luis.montesano@unizar.es"], "sections": [{"heading": null, "text": "In this survey we present different approaches that allow an intelligent agent to explore autonomous its environment to gather information and learn multiple tasks. Different communities proposed different solutions, that are in many cases, similar and/or complementary. These solutions include active learning, exploration/exploitation, online-learning and social learning. The common aspect of all these approaches is that it is the agent to selects and decides what information to gather next. Applications for these approaches already include tutoring systems, autonomous grasping learning, navigation and mapping and human-robot interaction. We discuss how these approaches are related, explaining their similarities and their differences in terms of problem assumptions and metrics of success. We consider that such an integrated discussion will improve inter-disciplinary research and applications.1"}, {"heading": "1 Introduction", "text": "One of the most remarkable aspects of human intelligence is its adaptation to new situations, new tasks and new environments. To fulfill the dream of Artificial Intelligence and to build truly Autonomous Intelligent Agents (Robots included), it is necessary to develop systems that can adapt to new situations by learning fast how to behave or how to modify their previous knowledge. Consequently, learning has taken an important role in the development of such systems. This paradigm shift has been motivated by the limitations\n1Draft v0.7 18Dec2013\nof other approaches to cope with complex open-ended problems and fostered by the progress achieved in the fields of statistics and machine learning. Since tasks to be learned are becoming increasingly complex, have to be executed in ever changing environments and may involve interactions with people or other agents, learning agents are faced with situations that require either a lot of data to model and cover high dimensional spaces and/or a continuous acquisition of new information to adapt to novel situations. Unfortunately, data is not always easy and cheap, but often requires a lot of time, energy, computational or human resources and can be argued to be a limiting factor in the deployment of systems where learning is a key factor.\nConsider for instance a robot learning from data obtained during operation. It is common to decouple the acquisition of training data from the learning process. However, the embodiment in this type of systems provides a unique opportunity to exploit an active learning (AL) 2 approach (AL)(Angluin, 1988; Thrun, 1995; Settles, 2009) to guide the robot actions towards a more efficient learning and adaptation and, consequently, to achieve a better performance more rapidly.\nThe robot example illustrates the main particularity of learning for autonomous agents: the abstract learning machine is embodied in a (cyber) physical environment and so it needs to find the relevant information for the task at hand by itself. Although these ideas have been around for more than twenty years (Schmidhuber, 1991b; Thrun, 1992; Dorigo and Colombetti, 1994; Aloimonos et al., 1988), in the last decade there\n2Active learning can also be used to describe situations where the student is involved in the learning process as opposed to passively listening to lectures, see for instance (Linder et al., 2001).\nar X\niv :1\n40 3.\n14 97\nv1 [\ncs .A\nI] 6\nM ar\n2 01\nhas been a renewed interest from different perspectives in actively gathering data during autonomous learning. Broadly speaking, the idea of AL is to use the current knowledge the system has about the task that is currently being learned to select the most informative data to sample. In the field of machine learning this idea has been envigorated by the existence of huge amounts of unlabeled data freely available on the internet or from other sources. Labeling such data is expensive as it requires the use of experts or costly procedures. If similar accuracy can be obtained with less labeled data then huge savings, monetary and/or computational, could be made.\nIn the context of intelligent system, another line of motivation and inspiration comes from the field of artificial development (Schmidhuber, 1991b; Weng et al., 2001; Asada et al., 2001; Lungarella et al., 2003; Oudeyer, 2011). This field, inspired by developmental psychology, tries to understand biological development by creating computational models of the process that biological agents go through their lifetimes. In such process there is no clearly defined tasks and the agents have to create their own representations, decide what to learn and create their own learning experiments.\nA limiting factor on active approaches is the limited theoretical understanding of some of its processes. Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011). The first intuition on why AL might required a smaller number of labeled data, is to note that the system will only ask for data that might changes its hypothesis and so uninformative examples will not be used. Nevertheless, previous research provides an optimistic perspective on the applicability of AL for real application, and indeed there already many examples: image classification (Qi et al., 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a review). Active learning can also be used to plan experiments in genetics research, e.g. the robot scientist (King et al., 2004) eliminates redundant experiments based on inductive logic programming. Also, most algorithms already have an active extension: logistic regression (Schein and Ungar, 2007), support vector machines (Tong and Koller, 2001), GP (Kapoor et al., 2007), neural networks (Cohn et al., 1996), mixture models (Cohn et al., 1996), inverse reinforcement learning (Lopes et al., 2009b), among many others.\nIn this paper we take a very broad perspective on the meaning of AL: any situation where an agent (or a team) actively looks for data instead of passively waiting to receive it. The previous description rules out those cases where a learning process uses data previously obtained in any possible way (e.g. by random movements, or with a predefined paths; or by receiving data from people or other agents). Thus, the key property of such algorithms is the involvement of the agent to decide what information suits better its learning task. There are multiple intances of this wide definition of AL with sometimes unexplored links. We structured them in three big groups: a) exploration where an agent explores its environment to learn; b) curiosity where the agent discovers and creates its own goals; and c) interaction where the existence of a human-inthe-loop is taken explicitly into account."}, {"heading": "1.1 Exploration", "text": "Exploration by an agent (or a team of agents) is at the core of rover missions, search and rescue operations, environmental monitoring, surveillance and security, best teaching strategies, online publicity, among others. In all these situations the amount of time and resources for completing a task is limited or unknown. Also, there are often trade-offs to be made between different tasks such as surviving in a hostile environment, communicating with other agents, gathering more information to minimize risk, collecting and analyzing samples. All these tasks must be accomplished in the end but the order is relevant inasmuch as is it helps subsequent tasks. For instance collecting geological samples for analysis and communicating the results will be easier if the robot has already a map of the environment. Active strategies are of paramount importance to select the right tasks and actively execute the task maximizing the operation utility while minimizing the required resources or the time to accomplish the goal."}, {"heading": "1.2 Curiosity", "text": "A more open-ended perspective on learning should consider cases where the task itself is not defined. Humans develop and grow in an open-ended environment without pre-defined goals. Due to this uncertainty we cannot assume that all situations are considered apriori and the agent itself has to adapt and learn new\ntasks. Even more problematic is that the tasks faced are so complex that learning them might require the acquisition of new skills.\nRecent results from neuroscience have given several insights into visual attention and general information seeking in humans and other animals. Results seem to indicate that curiosity is an intrinsic drive in most animals (Gottlieb et al., 2013). Similarly to animals with complex behaviors, an initial period of immaturity dedicated to play and learning might allow to develop such skills. This is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman, 1997; Lungarella et al., 2003; Oudeyer, 2011) where the complexity of the problems that the agent is able to solve increases with time. During this period the agent is not solving a task but learning for the sake of learning. This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al., 2010b)."}, {"heading": "1.3 Interaction", "text": "Learning agents have intensively tackled the problem of acquiring robust and adaptable skills and behaviors for complex tasks from two different perspectives: programming by demonstration (a.k.a. imitation learning) and learning through experience. From an AL perspective, the main difference between these two approaches is the source of the new data. Programming by demonstration is based on examples provided by some external agent (usually a human). Learning through experience exploits the embodiment of the agent to gather examples by itself by acting on the world. In the abstract AL from machine learning the new data/labels used to come from an oracle and no special regard is given to what exactly the oracle is besides well behaved properties such as no bias and consistency. More recently, data and labels may come from ratings and tagging provided by humans resulting in bias and inconsistencies. This is also the case for agents interacting with humans in which applications had taken into account where that information comes from and what other sources of information might be exploited. For instance, sometimes humans may pro-\nvide more easily information other than labels that can further guide exploration3."}, {"heading": "1.4 Organization", "text": "This review will consider AL in this general setting. We will first clarify the AL principles for autonomous intelligent agents in Sec. 2. Then the core review will be organized in three main parts: Sec 3 AL during selfexploration; Sec. 4 autonomous discovery/creation of goals; and finally Sec 5 AL with humans."}, {"heading": "2 Active Learning for Autonomous", "text": "Intelligent Agents\nIn this Section we provide an integrated perspective on the many approaches for active learning. The name active learning has mostly been used in machine learning but here we consider any situation where a learning agent uses its current hypothesis about the learning task to select what/where/how to learn next. Different communities formulated problems with similar ideas and all of them can be useful for autonomous intelligent agents. Different approaches are able to reduce the time, or samples, required to learn but they consider different fitness functions, learning algorithms and choices of what can be selected. Figure 1 shows the three main perspectives on for single task active learning. Exploration in reinforcement learning (Sutton and Barto, 1998), bayesian optimization (Brochu et al., 2010), multi-armed bandits (Bubeck and CesaBianchi, 2012), curiosity (Oudeyer and Kaplan, 2007), interactive machine learning (Breazeal et al., 2004) or active learning for classification and regression problems (Settles, 2009), all these share many properties and face similar challenges. Interestingly, a better understanding of the different approaches from the various communities can lead to more powerful algorithms. Also, in some cases to solve the problem of one community, it is necessary to rely on the formalism of another. For instance, active learning for regression methods that can synthesize queries need to find the most informative point. This is, in general, an optimization problem in high-dimension and it is not possible to solve it exactly. Bayesian optimization methods can\n3I don\u2019t like the last sentence with the last changes in the section\nthen be used to find the best point with a minimum of function evaluations (Brochu et al., 2010). Another example, still for regression, is to decompose complex regression functions to a set of local regressions and then rely on multi-armed bandit algorithms to balance exploration in a more efficient way (Maillard, 2012).\nEach of these topics would benefit from a dedicated survey and we do not aim at a definite discussion on all the methods. In this section we will discuss all these approaches with the goal of understanding the similarities, strengths and domains of application. Due to the large variety of methods and formalism we can not describe the full details and mathematical theory but we will provide references for most methods. This Section can be seen as a cookbook of active learning methods where all the design choices and tradeoffs are explained jointly with links for the theory and for examples of application (see Figure 2 for a summary)."}, {"heading": "2.1 Optimal Exploration Problem", "text": "To ground the discussion, let us consider a robot whose mission is to build a map of some physical quantities of interest over a region (e.g. obstacles, air pollution, density of traffic, presence of diamonds...). The agent will have a set of on-board capabilities for acting in the environment that will include moving along a path or to a specific position and using its sensors to obtain measurements about the quantity of interest. In addition to this, it may be possible to make decisions about other issues such as what algorithms should be used to process the obtained measurements or to fit the model of the environment. The set of all possible decisions will define the space of exploration policies4 \u03a0. To derive an active algorithm for this task, we need to model the costs and the loss function associated to the actions of a specific exploration policy \u03c0. The most common costs include the cost of using each of the onboard sensors (e.g. energy consumption, time required to acquire the measurement or changes in the payload) and the cost of moving from one location to another (e.g. energy and the associated autonomy constraints). Regarding the loss function, it has to capture the error of the learned model w.r.t. the unknown true model. For instance, one may consider the uncertainty of the\n4The concept is similar to the policy for reinforcement learning, but here the policy is not optimizing total reward but, instead, exploration gain (to be defined latter)\npredictions at each point or the uncertainty on the locations of the objects of interest.\nThe optimal exploration policy is the one that simultaneously gives the best learned model but with the smallest possible cost:\n\u03c0\u2217t = argmax\u03c0\u2208\u03a0f(\u03c0,C(\u03c0),Lx\u2208X (g\u0302(x;D \u223c \u03c0))), (1)\nwhere \u03c0 is an exploration policy (i.e. a sequence of actions possibly conditioned on the history of states and/or actions taken by the agent), \u03a0 denotes the space of possible policies, f() is a function that summarizes the utility of the policy5, and X is a space of\n5Note that \u03c0 might have different semantics depend on the\npoints that can be sampled. Function f depends on the policy itself, the cost C(\u03c0) of executing this policy and the loss of the policy Lx(g\u0302(x;D \u223c \u03c0)). The loss depends on a function g\u0302() learned with the dataset D acquired following policy \u03c0. Equation 1 selects the best way to act, taking into account the task uncertainty along time. Clearly this problem is, in general, intractable and the following sections describe particular instantiations, approximations and models of this optimal exploration problem (S\u0327ims\u0327ek and Barto, 2006).\nEquation 1 is intentionally vague with respect to several crucial aspects of the optimization process. For instance, time is not included in any way, and just the abstract policy \u03c0 and the corresponding policy space \u03a0 are explicit. Also, many different costs and loss models can be fed into the function f(), with the different choices resulting in different problems and algorithms. It is the aim of this work to build bridges between this general formulation and the problems and solutions proposed in different fields. However, before delving into the different instances of this problem, we briefly describe the three most common frameworks for active learning and then discuss possible choices for the policy space \u03a0 and the role of the terms C and L in the context of autonomous agents.\ntask at hand. It can be an exploration policy used to learn a model g() in a pure learning problem, or it can be an exploitation policy in an optimization setting. For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; S\u0327ims\u0327ek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012)."}, {"heading": "2.2 Learning Setups", "text": ""}, {"heading": "2.2.1 Function approximation", "text": "Regression, and classification, problems are the most common problems in machine learning methods. In both cases given a dataset of points D = {(x, y)} the goal is to find an approximation of the input output relation g : x \u2192 y. Typical loss functions are the squared mean error L = |h\u0302 \u2212 y|2 = |g(x) \u2212 y|2 for regression and the 0 \u2212 1 loss L0\u22121 = I(g\u0302(x) = y) for classification, with I denoting the indicator function. In this setup the cost function directly measures the cost of obtaining measurements (e.g. collecting the measurement or moving to the next spot), if it exists. The active learning perspective corresponds to deciding for which input x it is more relevant to ask for the corresponding label y. Some other restrictions can be included such as being restricted to a finite set of input points (pool-based active learning) or having the points arriving sequentially and having to decide to query or not (online learning)(see (Settles, 2009) for a comprehensive discussion on the different settings)."}, {"heading": "2.2.2 Multi-Armed Bandits", "text": "An alternative formalism that is usually applied to discrete selection problems is the multi-armed bandit (MAB) formalism (Gittins, 1979; Bubeck and CesaBianchi, 2012). Multi-arm bandits define a problem where a player, at each round, can choose an arm among a set of n possible ones. After playing the selected arm the player receives a reward. In the most common setting the goal of the player is to find a strategy that allows it to get the maximum possible cumulative reward. The loss in bandit problems is usually based on the concept of regret, that is, the difference between the reward that was collected and the reward that would have been collect if the player knew which was the best arm since the beginning (Auer et al., 2003). Many algorithms have been proposed for different variants of the problems where instead of regret the player is tested after a learning period and it has either to declare what is the best arm (Victor Gabillon et al., 2011) or the value of all the arms (Carpentier et al., 2011)."}, {"heading": "2.2.3 MDP", "text": "The most general, and well known, formalism to model sequential decision processes are markov-decision process (MDP)(Bellman, 1952). When there is no knowledge about the model of the environment and an agent has to optimize a reward function while interacting with the environment the problem is called reinforcement learning (RL) (Sutton and Barto, 1998). A sequential problem is modeled as a set of states S, actions that allow the system to change between state A and the rewards that the system receives at each time step R. The time evolution of the system is considered to depend on the current state st and the chosen action at, i.e. p(st+1|st, at). The goal of the agent is to find a policy, i.e. \u03c0(s, a) = p(a|s), that maximizes the total discounted reward J(s0) = \u2211\u221e t=0 \u03b3\ntrt. For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesva\u0301ri, 2011; Kober et al., 2013). As the agent does not know the dynamics and the reward function it can not act optimally with respect to the cost function without first exploring the environment for that information. Then it can explicitly create a model of the environment and exploit it (Hester and Stone, 2011; Nguyen-Tuong and Peters, 2011) and directly try to find a policy that optimizes the behavior (Deisenroth et al., 2013). The balance between the amount of exploration necessary to learn the model and the exploitation of the latter to collect reward is, in general, an intractable problem and is usually called the exploitation-exploration dilemma.\nPartial-observable markov decision processes (POMDP) generalize the concept for cases where the state is not directly observable (Kaelbling et al., 1998)."}, {"heading": "2.3 Space of Exploration Policies", "text": "The policy space \u03a0 is defined by all possible sequences of actions that can be taken by the agent or, alternatively, by all the different closed-loop policies that generate such sequences. The simplest approach is to select a single data point from the environment database and use it to improve the model g\u0302(). In this case, \u03a0 is defined by the set of all possible sequences of data points (or the algorithm, or sensor, that is used to select them). Another case is when autonomous agents gather information by moving in the environ-\nment. Here, the actions usually include all the trajectories necessary to sample particular locations (or the motion commands that take the agent to them).\nHowever, the formulation of Eq. 1 is much more general and can incorporate any other possible decision to be made by the agent. An agent might try to select particular locations to maximize information or could select at a more abstract level between different regions, e.g. starting to map the beach or the forest. This idea can be pushed further. The agent might decide among different exploration types and request a helicopter survey of a particular location instead of measuring with its own sensors. In this case the robot selects among different exploration types. The agent might even decide between learning methods and representations that, in view of the current data, will behave better, produce more accurate models or result in better performance (see Section 3). This choice modifies the function g\u0302() used to compute the loss and can be changed several times during the learning process.\nThe following list summarizes the possible choices that have been considered in the literature in the context of active learning:\n\u2022 next location, or next locations\n\u2022 among a pre-defined partition of the space\n\u2022 among different exploration algorithms\n\u2022 learning methods\n\u2022 representations\n\u2022 others"}, {"heading": "2.4 Cost", "text": "The term C(\u03c0) represents the cost of the policy and we will assume that each action at taken following \u03c0 incurs a cost which is independent of future actions. However, the cost of an action may depend on the history of actions and states of the agent. Indeed, modeling this dependency is an important design decision, specially for autonomous agents. Figure 3 illustrates the implications of this dependency. In the first example, the cost of an action C(at) depends only on the action. This is usually the case of costs associated to sensing the environment. In the second case, the cost C(at | at\u22121) depends on the previous action since it implies a non-zero cost motion. This type of cost appears naturally for autonomous agents that need to move from one location to another6. In many cases, the cost will consist of a combination of different costs that can individually depend or not on previous actions."}, {"heading": "2.5 Loss and Active Learning Tasks", "text": "The term Lx\u2208X (g\u0302(x;D \u223c \u03c0)) represents the loss incurred by the exploration policy. Recall that the agent\u2019s objective is to learn model g(). The loss is therefore defined as a function of the goodness of the learned model. Obviously, the function g() varies with the task. It can be a discriminant function for classification, a function approximation for some quantity of interest or a policy mapping states to actions. In any case, the learned function g\u0302() will be determined by the flow of observations D induced by the policy \u03c0 (e.g. training examples for a classifier or measurements of the environment to build a map).\nAnother important aspect that must be considered is when the loss is evaluated. One possibility is that only the final learned model g\u0302() is used to obtain the expected loss. In this case, mistakes made during training are not taken into account. Alternatively, one may consider the accumulated loss during the whole lifetime of the agent, where even the cost and errors made\n6Action at is not precisely defined yet. The previous distinction abuses notation by abstracting over the specific action definition (e.g. local displacements or global coordinates). The important thing is that moving incurs a cost that depends on previous actions.\nduring the learning phase are taken into account. We can also think that no explicit learning phase exists in this setting. In the MAB literature these measures are known as the simple regret and average regret. The latter tells, in hindsight, how much was lost by not pulling always the best arm. And the former tells how good is the arm estimated as being the best.\nEarlier on, we did not make explicit what the loss function aims to capture during the learning process. Again, there are two possible generic options to consider: learn the whole environment (what we consider to be a pure learning problem); or find a location of the environment that provides the highest value (optimization problem). Note that in both cases, it is necessary to learn a model g(). However, in the first case we are interested in minimizing the error of the learned model \u222b\nX L(g(x), g\u0302(x))dx (2)\nwhile in the second case we are just interested on fitting a function g() that helps us to find the maximum of\nmaxxg(x)\u2212 g (argmaxxg\u0302(x)) (3)\nirrespectively of what the function g\u0302() is actually approximating. In a multi-armed bandit setting this amounts to just detect which is the best arm, or learn the payoff of all the arms. Table 2 summarizes this perspective. In this pure learning problem of multi-armed bandits regret bounds on the simple regret can also be made (Carpentier et al., 2011; Victor Gabillon et al., 2011). For the general RL problem regret bounds have also been established (Jaksch et al., 2010)."}, {"heading": "2.6 Measures of Information", "text": "The utility of the policy in Eq. 1 is measured using a function f(). Computing the information gain of a given sample is a difficult task which can be computationally very expensive or intractable. Furthermore, it can be implemented in multiple different ways depending on how the information is defined and on the\nassumptions and decisions done in terms of loss, cost and representation. Also, we note that in some cases, due to interdependencies between all the points, the order in which the samples are obtained might be relevant. The classification below follows the one proposed in (Settles, 2009) (also refer to (MacKay, 1992; Settles, 2009) for further details) and completes it by including empirical measures as a different way of assessing the information gain of a sample. The latter class of measures aims to consider those cases where there is no single model that covers the whole state-space, or if the agents lacks the knowledge to select which is the best one (Schmidhuber, 1991b; Oudeyer and Kaplan, 2007)."}, {"heading": "2.6.1 Uncertainty sampling and Entropy", "text": "The simplest way to select the new sample is to select the one we are currently more uncertain about. Formally, this can be modeled as the entropy of the output. Uncertainty sampling where the query is made where the classifier is most uncertain about (Lewis and Gale, 1994), still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Ungar, 2007), among others."}, {"heading": "2.6.2 Minimizing the version space", "text": "The version space defines the subset of all possible models (or parameters of a model) that are consistent with the current samples and, therefore, provides the set of hypotheses we are still undecided about. This space cannot in general be computed. It has been approximated in many different ways. An initial model considered Selective Sampling (Cohn et al., 1994) where a pool, or stream, of unlabeled examples exists and the learner may request the labels to an oracle. The goal was to minimize the amount of labeled data to learn the concepts to a fixed accuracy. Query by committee (Seung et al., 1992; Freund et al., 1997) considers a committee of classifiers and measures the degree of disagreement between the committee. An-\nother perspective was proposed by (Angluin, 1988) to find the correct hypothesis using membership queries. In this method the learner as a class of hypothesis and has to identify the correct hypothesis exactly. Perhaps the best-studied approach of this kind is learning by queries (Angluin, 1988; Cohn et al., 1994; Baum, 1991). Under this setting approaches have generalized methods based on binary search (Nowak, 2011; Melo and Lopes, 2013). Also, active learning in support vector machines can be seen in a version space perspective or as the uncertainty of the classifier (Tong and Koller, 2001)."}, {"heading": "2.6.3 Variance reduction", "text": "Variance reduction aims to select the sample(s) that will minimize the variance of the estimation for unlabeled samples (Cohn et al., 1996). There exist closed form solutions for some specific regression problems (e.g. linear regression or Gaussian mixture models). In other cases, the variance is computed over a set of possible unlabeled examples which may be computationally expensive. Finally, there are other decisiontheoretic based measures such as the expected model change (Settles et al., 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al., 2007) which select the sample that, in expectation, will result in the largest change in the model parameters or in the largest reduction in the generalization error, respectively."}, {"heading": "2.6.4 Empirical Measures", "text": "Empirical measures make less assumptions on the data-generating process and instead estimate empirically the expected quality of each data-points/region (Schmidhuber, 1991b; Schmidhuber, 2006; Oudeyer and Kaplan, 2007; Oudeyer et al., 2007; Lopes et al., 2012). This type of measures consider problems where (parts of-) the state space have properties that change over time, can not be learned accurately, or require much more data than other parts given a particular learning algorithm. Efficient learning in those situations will require to balance exploration so that resources are assigned according to the difficulty of the task. In those cases where this prior information is available, it can be directly incorporated in the previous methods. The increase in complexity may result in\ncomputationally expensive algorithms. When the underlying structure is completely unknown, it might be difficult to find proper models to take into account all the uncertainty. And even for the case where there is a generative model that explains the data, its complexity will be very high.\nLet us use a simple analogy to illustrate the main idea behind empirical measures. Signal theory tells us what is the sampling rate required to accurately reconstruct a signal with a limited bandwidth. To estimate several signals, an optimal allocation of sampling resources would require the knowledge of each signal bandwidth. Without this knowledge, it is necessary to estimate simultaneously the signal and the optimal sampling rate, see Figure 6. Although for this simple case one can imagine how to create such an algorithm, the formalization of more complex problems might be difficult. Indeed, in real applications it is quite common to encounter similar problems. For instance, a robot might be able to recover the map in most parts of the environment but fail in the presence of mirrors. Or, a visual attention system might end up spending most of its time looking at a tv set showing static.\nThe first attempt to develop empirical measures was made by (Schmidhuber, 1991a; Schmidhuber, 1991b) in which an agent could model its own expectation about how future experiences can improve model learning. After this seminal paper, several measures to empirically estimate how can data improve task learning have been proposed and a integrated view can be seen in (Oudeyer et al., 2007). To note that if there is an accurate generative model of the data, then empirical measures reduce to standard methods, see for instance the generalization of Rmax method (Brafman and Tennenholtz, 2003) to the use of empirical measures in (Lopes et al., 2012).\nIn more concrete terms empirical measure rely not on the statistical properties of a generative data model, but on tracking the evolution of the quality of estimation, see Figure 4.\nA simple empirical measure of learning progress \u03b6 can be made by estimating the variation of the estimated prediction error. If we consider a loss model L for the learning problem as: L(T\u0302 ;D), where T is the true model and D is the observed data. Putting an absolute threshold directly on the loss is hard. Note that the predictive error has the entropy of the true distribution as a lower bound, which is unknown (Cohn\net al., 1996). Therefore, these methods drive exploration based on the learning progress instead of the current learner accuracy. Using the change in loss they may gain robustness by becoming independent of the loss\u2019 absolute value and can potentially detect timevarying conditions (Oudeyer et al., 2007; Lopes et al., 2012).\nWe can define \u03b6 in terms of the change in the (empirically estimated) loss as follows. Let D\u2212k denote the experiences in D except the last k and T\u0302 \u2212k is the transition model learned from the reduced data-set D\u2212ks,a . We define \u03b6\u0302 \u2248 L(T\u0302 \u2212k;D)\u2212L(T\u0302 ;D). This estimates to which extent the last k experiences help to learn a better model as evaluated over the complete data. Thus, if \u03b6\u0302 is small, then the last k visitations in the data-set D did not have a significant effect on improving T\u0302 . To note that finding a good estimator for the expected loss is not trivial and resampling methods might be required (Lopes et al., 2012). See also (Oudeyer et al., 2007) for different definitions of learning progress."}, {"heading": "2.7 Solving strategies", "text": "The optimal exploration problem defined in Eq. 1 is in its most general case computationally intractable. Note that we aim at finding a exploration policy, or an algorithm, that is able to minimize the amount of data required while minimizing the loss. In Fig. 1 that would amount to choose among all the possible trajectories, of equivalent cost, the ones that provide the best fit. Furthermore, common statistical learning theory does not directly apply to most active learning algorithms and it is difficult to obtain theoretical guarantees about their properties. The main reason is that most theory on learning relies on the assumption that data is acquired randomly, i.e. the training data comes from the some distribution as the real data, while in active learning the agents itself chooses the next data point."}, {"heading": "2.7.1 Theoretical guarantees for binary search", "text": "Despite previous remarks, there are several cases where it is possible to show that active learning provides a gain and obtain some guarantees. (Castro and Novak, 2008; Balcan et al., 2008) identify the expected gains that active learning can give in different classes of problems. For instance, (Dasgupta, 2005; Dasgupta, 2011) studied the problem of actively finding the optimal threshold on a line for a separable classification problem. A binary search applied to this problem yields an exponential gain in sample efficiency. In what conditions, and for which problems this gain still hold is currently under study. As discussed by the authors, in the worst case it might still be necessary to classify the whole dataset to identify the best possible classifier. However, if we consider the average case and consider the expected learning quality for finite sample sizes, results show that we can get exponential improvements over random exploration. Indeed, other authors have shown that generalized binary search algorithms can be derived for more complex learning problems (Nowak, 2011; Melo and Lopes, 2013)."}, {"heading": "2.7.2 Greedy methods", "text": "Many practical solutions are greedy, i.e. they only look at maximizing directly a function. We note the difference between a greedy approach that directly maximizes a function an a myopic approach that ignores\nthe long-term effects of those choices. As we discuss now, there are cases where greedy methods are not myopic. The question is how far are greedy solutions from the optimal exploration strategy. This is in general a complex combinatorial problem. If the loss function being minimized has some structural properties, then some guarantees can be found that relate the sample complexity of a given algorithm with the possible best polynomial time algorithm. Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012). Submodular functions are functions that observe the diminishing return property, i.e. if B \u2282 A then F (A \u222a {x})\u2212 F (A) \u2265 F (B \u222a {x})\u2212 F (B). This means that choosing a datapoint sooner during the optimization will always provide equal or more information than the same point later on.\nA theorem from (Nemhauser et al., 1978) says that for monotonic submodular functions, the value of the function for the set obtained with the greedy algorithm G(Dg) is close, (1 \u2212 1/e), to the value of the optimal set G(DOPT ). This means that if we would solve the combinatorial problem, the solution we get with the greedy algorithm is at most 33% below the true optimal solution.\nUnfortunately not all problems are submodular. First, some target functions are not submodular. Second, online learning methods introduce bias since the order of the data changes the active learning results. Third, some problems cannot be solved using a greedy approach. For these problems a greedy algorithm can be exponentially bad (worst than random exploration). Also, a common situation is to have submodular problems given some unknown parameters without which it is not possible to use a the greedy algorithm. In this situation it is necessary to take an exploration/exploitation strategy to explore the parameter space to gather information about the properties of the loss function and and then exploit it."}, {"heading": "2.7.3 Approximate Exploration", "text": "The most general case as shown in Figure 1 is not submodular and the best solution rely of PAC-bounds. Two of the most influential works on the topics are: E3 (Kearns and Singh, 2002) and R-max (Brafman and Tennenholtz, 2003). Both take into account how often\na state-action pair has been visited to decide if further exploration is needed or if the model can be trusted enough (in a PAC setting) to be used for planning purposes. With different technical details both algorithms guaranty that with high-probability the system learns a policy whose value is close to the optimal one. Some other approaches consider limited look-ahead planning to approximately solve this problem (Sim and Roy, 2005; Krause and Guestrin, 2007)."}, {"heading": "2.7.4 No-regret", "text": "In the domain of multi-armed bandits several algorithms have been developed that can solve the optimization (Victor Gabillon et al., 2011) or the learning (Carpentier et al., 2011) problem with the best possible regret sometime taking into account specific knowledge about the statistical properties of each arm, but in many cases taken a distribution free approach (Auer et al., 2003)."}, {"heading": "3 Exploration", "text": "In this section we present the main approaches of active learning, particularly focused in systems with physical restrictions, i.e. where the cost depends on the state. This section organizes the literature according to what is being selected as policy for exploration. The distinctions are not clear in some cases, and some works include aspects of more than one problem, or can be seen in different perspectives. We consider three different parts: greedy selection of points where C(at|at\u22121) = C(at) and considering a selection among an infinite set of points or among a finite set, the last part considers the cases where the selection takes explicitly into account C(at|at\u22121) and longer time horizons. There is already a great variety of approaches but mainly the division corresponds to classical active learning, multi-armed bandits and explorationexploitation in reinforcement learning. We are interested in applications related to general autonomous agents and will only consider approaches focused on the classical active learning methods if they provide a novel idea."}, {"heading": "3.1 Single-Point Exploration", "text": "This section describes works that, at each time step, choose which is the single best observation point to explore without any explicit long term planning. This is the most common setting in active learning for function approximation problems (Settles, 2009), with examples ranging from vehicle detection (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al., 2007) among others. Note that, as seen in Section 2.7, in some cases information measures were defined where a greedy choice is (quasi-) optimal. Figure 5 provides an example of this setting where a robot is able to try to grasp an object at any point to learn the probability of success, at each new trial the robot can still choose amongst the same (infinite) set of grasping points."}, {"heading": "3.1.1 Learning reliability of actions", "text": "An example of the use of active learning under this setting, and with particular interest for physical systems, is to learn the reliability of actions. For instance, it has been suggested that grasping could be addressed by learning a function that relates a set of visual features with the probability of grasp success when a robot tries to grasp at those points (Saxena et al., 2006). This process requires a large database of synthetically generated grasping points (as initially suggested by (Saxena et al., 2006)), or alternatively to actively search and select where to apply grasping actions to esti-\nmate their success (Salganicoff et al., 1996; Morales et al., 2004). Another approach, proposed by (Montesano and Lopes, 2009; Montesano and Lopes, 2012) (see also Figure 5), derived a kernel based algorithm to predict the probability of a successful grasp together with its uncertainty based on Beta priors. Another approach used Gaussian process to model directly probability densities of successful grasps (Detry et al., 2009). Clearly such success probabilities depend on the grasping policy is being applied, and a combination of the two will be required to learn the best grasping strategy (Kroemer et al., 2009; Kroemer et al., 2010).\nAnother example is to learn several terrain properties in mobile robots such as obstacle detection and terrain classification. (Dima et al., 2004) use active learning to request human users the correct labels of extensive datasets acquired by robots using density measures. Also using multiview approaches (Dima and Hebert, 2005). Another property exploited by other authors is the traversability of given regions (Ugur et al., 2007).\nA final example considers how to optimize the parameters of a controller whose results can only be evaluated as success or failure (Tesch et al., 2013). The authors rely on Bayesian optimization to select which parameters are still expected to provide higher probabilities of success."}, {"heading": "3.1.2 Learning general input-output relations", "text": "Several works explore different ways to learn inputouputs maps. A simple case is to learn forwardbackward kinematic or dynamical models of robots but it can also be the effects of time extended policies such as walking.\nTo learn the dynamical model of a robot, (MartinezCantin et al., 2010) considered how to select which measure to gather next to improve the model. The authors consider a model parameterized by the location and orientation of a rigid body and their goal is to learn such parameters as fast as possible. They rely on uncertainty measures such as a-optimality.\nFor non-parametric models several works learn different models of the robotic kinematic, using either nearest-neighbors (Baranes and Oudeyer, 2012) or local-linear maps (Rolf et al., 2011). Empirical measures of learning progress were used by (Baranes and Oudeyer, 2012) and (Rolf et al., 2011)."}, {"heading": "3.1.3 Policies", "text": "Another example is to learn what action to apply in any given situation. In many cases this is learned from user input. This setting will be discussed in detail in Section 5.3.\n(Chernova and Veloso, 2009) considering support vector machines as the classification method. The authors consider the confidence on the prediction of the SVM and while the robot is moving it will query the teacher when that confidence is low.\nUnder the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012). Initial sample complexity results show that this approaches can indeed provide gains on the average case (Melo and Lopes, 2013)."}, {"heading": "3.2 Multi-Armed Bandits", "text": "This section discusses works that, similarly to the previous section, solely choose a single exploration point. The main difference is that we consider here the setting where this choice is discrete, or categorical. There are several learning problems that fall under this setting: environmental sensing and online sensor selection, multi-task learning, online selection of learning/exploration strategy, among others (see Table 3).\nThere are two main origins for this different set of choices. One is that the problem is intrinsically discrete. For instance the system can either be able to select among a set of different sensors, different learning algorithms (Baram et al., 2004; Hoffman et al., 2011; Hester et al., 2013), or being interested in learning from among a set of discrete tasks (Barto et al., 2004). Another case is when the discretization is made to simplify the exploration problem in a continuous space, reducing the cases presented in Section 3.1 to a MAB problem. Examples include environmental sensing where the state is partitioned for computational purposes (Krause et al., 2008), or learning dynamical models of robots where the partition is created online based on the similarities of the function properties at each location (Oudeyer et al., 2005; Barane\u0300s and Oudeyer, 2009) (see Figure 6). In all cases the goal is to learn a function in all domain by learning a function in each partial domain. Or to learn the relation of all the choices with their outputs. For a limited time\nhorizon the best overall learning must be obtained.\nIn the recently introduced strategic student problem (Lopes and Oudeyer, 2012), the authors provide an unified view of these problems, following a computational approach similar to (Baram et al., 2004; Hoffman et al., 2011; Baranes and Oudeyer, 2012). After having a finite set of different possible choices that can be explored, both problems can be approached in the same way and relying on variants of the EXP4 algorithm (Auer et al., 2003). This algorithm considers adversarial bandit settings and relies on a collection of experts. The algorithm has zero regret on the choice of experts and each expert will track the recent quality of each choice.\nWe note that most algorithms for MAB were defined for the exploration-exploitation setting, but there are cases where there is a pure-exploration problem. The main difference is that if we define the learning improvement as reward, this reward will change with time, as sampling the same location will reduce its value. It is worth to note that if the reward function were known then most of these cases could be reduced to a submodular optimization where a greedy heuristic is quasi-optimal. When this is not the case then a MAB algorithm must be used to ensure proper exploration of all the arms (Lopes and Oudeyer, 2012; Golovin et al., 2010a).\nOne interesting aspect to note is that, in most of cases, the optimal strategy is non-stationary. That is, for different time instants, the percentage of time applied to each choice is different. We can see that there is a developmental progression from learning simpler topics to more complex ones. Even at the extreme cases where with little amount of time some choices are not studied at all. These results confirms the heuristics of learning progress given by (Schmidhuber, 1991b; Oudeyer et al., 2007). Both works considered that at any time instant the learner must sample the task that has given a larger benefit in the recent past. For the case at hand we can see that the solution is to probe, at any time instant, the task whose learning curve has an higher derivative, and for smooth learning curves both criteria are equivalent.\nWe will now present some works that do active exploration by selecting among a finite set of choices. We divide the approaches in terms of choosing different (sub-) tasks or different strategies to explore, or learn a single task. Clearly this division depends on\ndifferent nomenclatures and on how the problems are formulated."}, {"heading": "3.2.1 Multiple (Sub-)Tasks", "text": "In this case we considered that there is a set of possible choices to be made that correspond to learning a different (sub-)task. This set can be pre-defined, or acquired autonomously (see Section 4), to have a large dictionary of skills that can be used in different situations or to create complex hierarchical controllers (Barto et al., 2004; Byrne, 2002)\nMulti-task problems have been considered in classification tasks (Qi et al., 2008; Reichart et al., 2008). Here active learning methods are used to improve not only one task, but the overall quality of the different tasks.\nMore interestingly for our discussion are the works from (Singh et al., 2005; Oudeyer et al., 2007). The authors divide the problem of learning complex agentenvironment tasks into learning a set of macro-action, or predictive models, in an autonomous way (see Section 4). These initial problems took very naive approaches and were latter improved with more efficient methods. (Oudeyer et al., 2007) initially considered that each parameter region gave a different learning gain, and the one that were given the highest gain was selected. Taking into account the previous discussion we know that a better exploration strategy must be applied and the authors considered more robust measures and created a stochastic policy to provide ef-\nficient results in high-dimensional problems (Baranes and Oudeyer, 2012). More recently (Maillard, 2012) introduce a new formulation of the problem and a new algorithm with specific regret bounds. The initial work of (Singh et al., 2005) lead to further improvements. The measures of progress that guide the selection of the macro action that is to be chosen started to consider the change in value function during learning (S\u0327ims\u0327ek and Barto, 2006). Similar ideas were applied to learn affordances (Hart and Grupen, 2013) where different controllers and their validity regions are learned following their learning progress.\nIn distributed sensing it is required to estimate which sensors provide the most information about a environmental quantity. Typically this quantity is time varying and the goal is to actively estimate which sensors provide more information. When using a gaussian process as function approximation it is important to consider exploration to find the property of the kernel and then, for known parameters of the kernel, a simple offline policy provides optimal results (Krause and Guestrin, 2007). This partition in a finite set of choices allows to derive more efficient exploration/sensing strategies and still ensure tight bounds (Krause et al., 2008; Golovin and Krause, 2010; Golovin et al., 2010a)."}, {"heading": "3.2.2 Multiple Strategies", "text": "The other big perspective is to consider that the choices are the different methods that can be used to learn from the task, in this case a single-task is often considered. This learning how to learn approach makes explicit that a learning problem is extremely depending on the method to collect the data and the algorithm used to learn the task.\nOther approaches include the choice among the different teachers that are available to be observed (Price and Boutilier, 2003) where some of them might not even be cooperative (Shon et al., 2007), or even choose between looking/asking for a teacher demonstration or doing self-exploration (Nguyen and Oudeyer, 2012).\nAnother approach considers the problem of having different representation and selecting the best one. The representation that gives more progress will be used more frequently (Konidaris and Barto, 2008; Maillard et al., 2011).\nThe previous mentioned work of (Lopes and\nOudeyer, 2012) showed also that the same algorithm can be used to select online which exploration strategy was best to learn faster the transition probability model of an MDP. The authors compared R-Max, \u2212 greedy and random. A similar approach was suggested by (Castronovo et al., 2012) where a list of possible exploration reward is proposed and a given arm bandit is assigned to each one. Both works took a simplified approach by considering that reset actions were available and the choices were only made at the beginning of each episode. This limitation was recently improved by considering that the agent can evaluate and select online the best exploration strategies (Hester et al., 2013). In this work the authors relied on a factored representation of an MDP (Hester and Stone, 2012) and using many different exploration bonuses they were able to define a large set of exploration strategies. The new algorithm at each instant computes the gain in reward for the selected exploration strategy and simultaneously the expected gain for all the other strategies using an importance sampling idea. Using such expected gains the system can select online the best strategy given better results than any single exploration strategy would do."}, {"heading": "3.3 Long-term exploration", "text": "We now consider active exploration strategies in which the whole trajectory is considered within the optimization criteria instead of planning only a single step ahead. A real world example is the one of selecting informative paths for environmental monitoring, see Figure 7.\nWe divide this section in two parts. A first part entitled Exploration in Dynamical Systems considering exploration where the dynamical constraints of the system are taken into account and another, that considers similar aspects, specific to Reinforcement Learning and Markov Decision Processes. We make this distinction due to the different communities, formalisms and metrics commonly used in each domain."}, {"heading": "3.3.1 Exploration in Dynamical Systems", "text": "The most representative example of such a problem is one of the best studied problems in robotics: simultaneous localization and mapping (SLAM). The goal is to build a map of an unknown environment while keeping track of the robot position within it. Early works\nfocused on active localization given an a priori map. In this case, the objective is to actively move the robot to obtain a better localization. In (Fox et al., 1998) the belief over the robot position and orientation was obtained using a Monte Carlo algorithm. Actions are chosen based on a utility function based on the expected entropy of the robot location. A set of predefined relative motions are considered and only moving costs are considered.\nThe first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005). The implementation details depend on the onboard sensors (e.g. sonar or laser), the SLAM representation (feature based or grid maps) and the technique (EKF, Monte Carlo). For instance, in (Feder et al., 1999) an EKF was used to represent the robot location and the map features measured using sonar. Actions were selected to minimize the total area of error ellipses for the robot and each landmark, by reducing the expected covariance matrix at the next time step. For grid maps, similar ideas have been developed using mutual information (Stachniss and Burgard, 2003) and it is even possible to combine both representations (Bourgault et al., 2002) using a weighted criteria. Most of the previous approaches consider just a single step ahead, have to discretize the action space or ignore the information that will be obtained during the path and its effect in the quality of the map. A more elaborated strategy was proposed in (Sim and\nRoy, 2005) where an a-optimality criterion over the whole trajectory was used. To make the problem computationally tractable, only a set of predefined trajectories is considered using breadth-first search. The work in (Martinez-Cantin et al., 2007) directly aims to estimate the trajectory (i.e. a policy) in a continuous action-state space taking into account the cost to go there and all the information gathered in the path (Martinez-Cantin et al., 2007). The policies are parameterized using way-points and the optimization is done over the latter. Some works explore similar ideas in the context of navigation and obstacle avoidance. For instance, (Kneebone and Dearden, 2009) uses a POMDP framework to incorporate uncertainty into Rapid Random Trees planning. The resulting policy takes into account the information the robot will obtain while executing the plan. Hence, the map is implicitly refined during the plan resulting in an improved model of the environment.\nThe active mapping approaches described above deal mainly with mapping environments with obstacles. However, similar ideas have been used to map other phenomena such as rough terrain, gas concentration or other environmental monitoring tasks. In this setting, robots allow to cover larger areas and to reconfigure the sensor network dynamically during operation. This makes active strategies even more relevant than in traditional mapping. Robots must decide where, when and what to sample to accurately monitor the quantities of interest. In this domain it is important to consider learning non-stationary spacetime models (Krause and Guestrin, 2007; Garg et al., 2012). By exploiting submodularity it is possible to compute efficient paths for multiple robots assuring that they will gather information in a set of regions (Singh et al., 2007). Without relying on a particular division into regions, but without any proven bounds,\n(Marchant and Ramos, 2012) used Bayesian optimization tools to find an informative path in a space-time model."}, {"heading": "3.3.2 Exploration / Exploitation", "text": "Another setting where the learner actively plans its actions to improve learning is in reinforcement learning (see an early review on the topic (Thrun, 1992)). In this general setting the agent is not just learning but is simultaneously being evaluated on its actions. This means that the errors made during learning count towards the global evaluation. In the Reinforcement learning (RL) approaches this is the most common setting. Under our taxonomy here the problem is also the one more challenging as the choice of where to explore next depends on the current location and the system has to take into account the way to travel to such locations.\nAs discussed before, this most general case, as shown in Figure 1, is not submodular and there is not hope to find a computationally efficient method to solve it exactly. Initial proposals considered the uncertainty in the models and guided exploration based on this uncertainty and other measures such as recency of visits. The authors then proposed that a never-ending exploration strategy could be made that incorporates knowledge about already well know states and novel ones. (Schmidhuber et al., 1997; Wiering and Schmidhuber, 1998).\nThe best solutions, with theoretical guarantees, aim at finding efficient algorithms that have an highprobability of finding a solution that is approximately correct, following the standard probably approximately correct learning (PAC) (Strehl and Littman, 2008; Strehl et al., 2009). Two of the most influential works on the topic are: E3 (Kearns and Singh,\n2002) and R-max (Brafman and Tennenholtz, 2003). Both take into account how often a state-action pair has been visited to decide how much further exploration must be done. Specifically, for the case of Rmax (Brafman and Tennenholtz, 2003), the algorithm divides the states into known and unknown based on the number of visits made. This number is defined based on general bounds for having a high certainty on the correct transition and reward model. Then the algorithm proceeds by considering a surrogate reward function that is R-max in unknown states and the observed reward in known states. For a further analysis an more recent algorithm see the discussion in (Strehl and Littman, 2008).\nPAC-RL measures consider that most of the times the agent will be executing a policy that is close to the optimal one. An alternative view is to see if the cumulative reward is close to the best one, as in the notion of regret. Such regret measure have been already generate some RL algorithms (Salganicoff and Ungar, 1995; Ortner, 2007; Jaksch et al., 2010).\nYet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c). In this formalism the agents aims at finding a policy that is (close to) optimal taking into account the model uncertainty. The resulting policies solve implicitly the exploration-exploitation problem. Bayesian RL exploits prior knowledge about the transition dynamics to reason explicitly about the uncertainty of the estimated model. Bayesian exploration bonus (BEB) approach (Kolter and Ng, 2009) mixes the ideas of Bayesian RL with R-max where the state are not explicitly separated between known and unknown but instead each state get a bonus proportionally to the uncertainty in the model. The authors were able to show that this algorithm approximates the - hard to compute - bayesian optimal solution.\nA recent approach considered how can R-max be generalized for the case where each different state might have different statistical properties (Lopes et al., 2012). Specially in the case where the different properties are not known, empirical measures of learning progress have been proposed to allow the system to balance online the exploration necessary to verify the PAC-MDP conditions.\nAs a generalization of exploration methods in reinforcement learning, such as (Brafman and Tennenholtz, 2003), ideas have been suggested such as plan-\nning to be surprised (Sun et al., 2011) or the combination of empirical learning progress with visit counts (Hester and Stone, 2012). This aspect will be further explored in Section 4.\nWe note also that the ideas and algorithms for exploration/exploitation are not limited to finite state representations, there have been recent results extending them to to POMDPs (Fox and Tennenholtz, 2007; Jaulmes et al., 2005; Doshi et al., 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al., 2010).\nMost of the previous approaches are optimistic in the face of uncertainty. In the real world most of the times exploration must be done in incremental and safe ways due to the physical limits and security issues. In most cases process are not ergodic and care must be made. Safe exploration techniques have started to be developed (Moldovan and Abbeel, 2012). In this work the system is able to know if an exploration step can be reversed. This means that the robot can see ahead and estimate if it can return to the previous location. Results show that the exploration trajectory followed is different from other methods but allows the system to explore only the safe parts of the environment."}, {"heading": "3.4 Others", "text": "There are other exploration methods that do not fit well in the previously defined structure, in most cases because they do not model explicitly the uncertainty. Relevant examples consider policy search and active vision. Other cases combine different methods to accomplish different goals."}, {"heading": "3.4.1 Mixed Approaches", "text": "There are several methods that include several levels of active learning to accomplish complex tasks, see Figure 8.\nIn (Martinez-Cantin et al., 2009; Martinez-Cantin et al., 2010) the authors want to learn a dynamical model of a robot arm, or a good map of the environment, with the minimum amount of data. For this it is necessary to find a trajectory, consisting of a sequence of via-points, that reduces the uncertainty on the estimator as fast as possible. The main difficulty is that this is in itself a computationally expensive\nproblem, and if it is to be used in real time, then efficient Bayesian optimization techniques must be used (Brochu et al., 2010).\nAnother examples is the SAGG-RIAC architecture (Baranes and Oudeyer, 2012). In this system a hierarchy of forward models are learned and for this it actively makes choices at two levels: in a goal space, it chooses what topic/region to sample (i.e. which goal to set), and in a control space, it chooses which motor commands to sample to improve its know-how towards goals chosen at the higher level.\nWe can also view the works of (Kroemer et al., 2009; Kroemer et al., 2010) as having a level of active exploration of good grasping points and another level of implicit exploration to find the best grasping strategies."}, {"heading": "3.4.2 Implicit exploration", "text": "Learning in robots and data collection are always intertwined. Even if such data collection process is explicit in many cases, other situations, even if strongly dependent on that same process, address it only in an implicit way or as a side-effect of an optimization process (Deisenroth et al., 2013). The most noteworthy example are all policy gradient methods and similar\napproaches (Sutton et al., 2000; Kober et al., 2013). In these methods the learner tries to directly optimize a policy given experiments and the corresponding associated reward. Some methods consider stochastic policies and the noise on the policy is used to perform exploration and collect data (Peters et al., 2005). The exploration reduces under the same process that adjust the parameters to improve the expected reward. Another line of research is to use more classical methods of optimization to find the best set of parameters that maximize a reward function (Stulp and Sigaud, 2012). Recently, and using a more accurate model of uncertainty it is possible to use Bayesian optimization methods to search for the best policy parameters that result in the highest success rate (Tesch et al., 2013)."}, {"heading": "3.4.3 Active Perception", "text": "Another common use of the word active is in active perception. Initially it was introduced because many computer vision problems become easier if more than one images is available or even a stream of video. An active motion of the camera can make such extra information much easier to discover. More recently it was motivated by the possibilities opened by having a robot acting in the environment to discover world properties.\nThis idea has been applied to segment object and learn about their properties (Fitzpatrick et al., 2003), disambiguate and model articulated objects (Katz et al., 2008), disambiguate sound (Berglund and Sitte, 2005), among others. Attention can also be seen as an instance of active perception, (Meger et al., 2008) presents an attention system and learning in a real environment to learn about object using SIFTs and finally, in highly cluttered environments active approach can also provide significant gains (van Hoof et al., 2012)."}, {"heading": "3.5 Open Challenges", "text": "Under the label of exploration we considered several domains that include standard active learning, exploration and exploitation problems, multi-armed bandits and general online learning problems. All these problems have already a large research body but there are still many open challenges.\nClearly a great deal of work is still necessary to expand the classes of problem that can be actively sam-\npled in an efficient way. In all the settings we described there exist already many different approaches, many of them with formal guarantees. Nevertheless, for any particular instance of a problem it is not clear what method is the most efficient in practice, or how to synthesize the exploration strategies from a problem domain description.\nSome of the heuristics and methods, and also many of the hypothesis and models, proposed in the developmental communities can be natural extensions to the active learning setting. For instance there is a very limited research on active learning for more complex models such as time-variant problems, domains with heteroscedastic noise and properties (see many of the differences in Table 4)."}, {"heading": "4 Curiosity", "text": "Most active approaches for learning address the problem of learning a single, well defined, task as fast as possible. Some of the examples given, such as safe exploration, already showed that in many cases there is a multi-criteria goal to be fulfilled. In a truly autonomous and intelligent system knowing what tasks are worth exploring or even which tasks are to be learned is a ill-defined problem.\nIn the 50s and 60s researchers started to be amazed by the amount of time children and primates spend in tasks that do not have a clear objective return. This spontaneous motivation to explore and intrinsic curiosity to novelty (Berlyne, 1960) challenged utilitarian perspectives on behavior. The main question is why do so many animals have a long period of playing and are curious, activities that in many perspectives can be considered risky and useless? One important reason seems to be that is this intrinsic motivation that will create situations for learning that will be useful in future situations (Baldassarre, 2011; Singh et al., 2009), only after going through school will that knowledge have some practical benefit. Intelligent agents are not myopically optimizing their behavior but also gathering a large set of perceptual, motor, and cognitive skills that will have a benefit in a large set of possible future tasks. A major problem is how to define a criteria of what a successful learning is if the task is just to explore for the sake of pure exploration. Some hypothesis can be made that this stage results from an evolutionary process that leads to a better performance in a\nclass of problems (Singh et al., 2010b). Or that intrinsic motivation is a way to deal with bounded agents where maximizing the objective reward would be too difficult (Singh et al., 2010a; Sorg et al., 2010a). Even for very limited time spans where an agent wants to select a single action, there are many somewhat contradictory mechanisms for attention and curiosity (Gottlieb, 2012). An agent might have preferences for: specific stimuli; actions to promise bigger learning gains; selecting actions that provide the required information for reward prediction/gathering.\nThe idea of assuming that the future will bring new unknown tasks can be operationalized even in a single domain. Consider a dynamical environment (defined as a MDP) where there is a training phase of unknown length. In one approach the agent progressively learns how to reach all the states that can be reached in 1 step. After being sufficiently sure that it found all such states and has a good enough policy to reach them the system increases the number of steps and starts the process. This work, suggested by (Auer et al., 2011; Lim and Auer, 2012), shows that it is possible to address such problem and still ensure formal regret bounds. Under different formalisms we can also see the POWERPLAY system as a way to increasingly augment the complexity of already explained problems (Schmidhuber, 2011). The approach from (Baranes and Oudeyer, 2012) can also be seen in this perspective where the space of policy parameters is explored in an increasing order of complexity.\nOne of the earliest works that tried to operationalize these concepts was made by (Schmidhuber, 1991b). More recently several researchers have extended the study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007). Research in this field has considered new problems such as: situations where parts of the state space are unlearnable (Barane\u0300s and Oudeyer, 2009; Baranes and Oudeyer, 2012); guide exploration in different spaces (Baranes and Oudeyer, 2012); environmental changes (Lopes et al., 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Barane\u0300s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012); limited agents (Singh et al., 2010a; Sorg et al., 2010a; Sequeira et al., 2011); open-ended problems (Singh et al., 2005; Oudeyer et al., 2007); autonomous discovery of good representations (Luciw et al., 2011);\nand selecting efficient exploration policies (Lopes and Oudeyer, 2012; Hester et al., 2013).\nSome of these ideas are natural extensions to the active learning setting, e.g. time-variant problems, heteroscedastic domains but, usually due to limited formal understanding, theoretical results have been limited. Table 4 shows a comparison of the main qualitative differences between the traditional perspective and this more recent generalizations."}, {"heading": "4.1 Creating Representations", "text": "A very important aspect in any learning machine is to be able to create, or at least select, its own representations. In many cases (most?) the success of a learning algorithm is critically dependent on the selected representations. Any variant of feature selection is the most common approach for the problem and it is assumed that a large bank of features exist and the learning algorithm chooses a good sub-set of them, considering sparsity, or any other criteria. Nevertheless, the problem is not trivial and most heuristics are bound to fail in most cases (Guyon and Elisseeff, 2003).\nSome works focused just on the perceptual capabilities of agents. For instance, (Meng and Lee, 2008) grows radial basis functions to learn mappings between sensory modalities by sampling locations with an high error. For the discussion on this document, particularly in this section, the most relevant works are those that not consider just what is the best representation for a particular task, but those that have a co-adaptation perspective and co-select the representation and the behavior. For instance (Ruesch and Bernardino, 2009; Schatz and Oudeyer, 2009; Rothkopf et al., 2009) study what is the relation between the behavior of an agent and the most representative retinal distribution.\nSeveral works consider how to learn a good representations of the state-space of an agent while exploring an environment. These learned representations are not only good to classify regions but also to navigate and create hierarchies of behavior (Luciw et al., 2011; Bakker and Schmidhuber, 2004). Early works considered how a finite-automaton and an hierarchy could be learned from data (Pierce and Kuipers, 1995).\nGeneralizations of those ideas consider how to detect regularities that identify non-static world objects and thus allowing to infer actions that change the world in\nthe desired ways (Modayil and Kuipers, 2007)."}, {"heading": "4.2 Bounded Rationality", "text": "There are several models of artificial curiosity, or intrinsic motivation systems, that, in general, guide the behavior of the agent to novel situations. These models provide exploration bonuses, sometimes called intrinsic rewards, to focus attention on such novel situations. The advantages of such models for an autonomous agents are, in many situations, not clear.\nAn interesting perspective can be that of bounded rationality. Even if agents were able to see all the environment they might lack the reasoning and planning capabilities to behave optimally. Another way to see these works is to consider that the agent lives in a POMDP problem and, for some cases, it is possible to find a different reward function that mitigate some of the partial observability problem.\nA very interesting perspective was approached with the definition of the optimal reward problem (Sorg et al., 2010a). In here the authors consider that the learning agent is limited in its reasoning capabilities. If it tries to optimize the observed reward signal it will be sub-optimal in the task, and so another reward is found that allows the agent to learn the task. The authors have extended their initial approach to have a more practical algorithm using reward gradient (Sorg et al., 2010b) and by comparing different search methods (Sorg et al., 2011). Recently the authors considered how the computational resources must be taken into account when choosing between optimizing a new reward or planning the next actions. Such search for an extra reward signal can also be used to improve coordination in a multi-agent scenario (Sequeira et al., 2011)."}, {"heading": "4.3 Creating Skills", "text": "When an animal is faced with a new environment there are an infinite number of different tasks that it might try to achieve, e.g. learn the properties of all objects or understand its own dynamics in this new environment. It can be argued that there is the single goal of survival and that any sub-division is an arbitrary construct. We agree with this view but we consider that such sub-division will create a set of reusable sub-goals that might provide advantages in the single main goal.\nTable 4: Active Learning vs Artificial Curiosity\nActive Learning Artificial Curiosity\nLearn with reduced time/data Learn with reduced time/data Fixed tasks Tasks change and are selected by the agent\nLearnable everywhere Parts might not be learnable Everything can be learned in the limit Not everything can be learned during a lifetime\nReduce uncertainty Improve progress\nThis perspective on (sub) goal creation motivated one of the earliest computational models on intrinsic motivated systems (Barto et al., 2004; Singh et al., 2005), see Figure 9. There the authors, using the theory of options (Sutton et al., 1999), construct new goals (as options) every time the agent finds a new \u201dsalient\u201d stimuli. In this toy example turning on a light, ringing a bell are considered reusable skills that might have an interest on latter stages and so if a skill is learned that reaches such state efficiently it will be able to learn complex hierarchical skills by combining the basic actions and the new learned skills.\nThe main criticism of those works is that the hierarchical nature of the problem was pre-designed and the saliency of novelty measures were tuned to the problem. To solve such limitations many authors have explored ways to autonomously define which skills much be created. Next we will discuss different approaches that have been proposed to create new skills in various problems.\nIn regression problems several authors reduced the problem of learning a single complex task into learning a set of multiple simpler tasks. In problems modeled as MDPs authors have considered how to create macrostate or macro actions that can be reused in different problems of allow to create a complex hierarchical control system. After such division of a problem into a set of smaller problems it is necessary to decide what to learn at each time instant. For this, results from multiarmed bandits can be used, see (Lopes and Oudeyer, 2012) and Section 3.2."}, {"heading": "4.3.1 Regression Models", "text": "In problems that consist in learning forward and backward maps among spaces (e.g. to learn dynamical models of systems), authors have considered how to incrementally create a partition of the space into regions of consistent properties (Oudeyer et al., 2007;\nFigure 9: The playroom domain where a set of motor skills is incrementally created and learned resulting in a set of reusable, and hierarchical, repertoire of skills. (a) Playroom domain; (b) Speed of learning of various skills; (c) The effect of intrinsically motivated learning when extrinsic reward is present. From (Singh et al., 2005).\nBarane\u0300s and Oudeyer, 2009). An initial theoretical study frames such model as a multi-armed bandits over a pre-defined hierarchical partition of the space (Maillard, 2012).\nThe set of skills that is created by the system might represent many different problems. Either an hierarchical decomposition of skills, but we can also see it as a decomposition of a problem in several, simpler, local problems. An example is the optimization setting of (Krause et al., 2008). Here the authors try to find which regions of a given area must be sampled to provide more information about one of several environmental conditions. It considers an already known sub-division and learns the properties of each one. Yet, in real world applications, the repertoire of topics to choose from might not be provided initially or might evolve dynamically. The aforementioned works of (Oudeyer et al., 2007; Baranes and Oudeyer, 2012) consider initially a single region (a prediction task in the former and a control task in the latter) but then automatically and continuously constructs new region, by sub-dividing or joining previous existing ones.\nIn order to discover affordances of objects and new\nways to manipulate them, (Hart et al., 2008) introduces an intrinsic reward that motivates the system to explore changes in the perceptual space. These changes are related to different motions of the objects upon contact from the robot arm.\nA different perspective on regression methods is considering that the input space is a space of policy parameters and the output is whatever time-extended results of applying such policy. Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al., 2011; Lim and Auer, 2012), explores the policy space in an increasing order of complexity of learning each behavior."}, {"heading": "4.3.2 MDP", "text": "In the case of problems formulated as MDPs several researchers have defined automatic measures to create options or other equivalent state-action abstractions, see (Barto and Mahadevan, 2003) for an early discussion. (Mannor et al., 2004) considered approaches such as online clustering of the state-action space using measures of connectivity, and variance of reward values. One such connectivity measure was introduced by (McGovern and Barto, 2001) where states that are present in multiple paths to the goals are considered sub-goals and an option is initiated to reach them. These states can be seen as \u201ddoors\u201d connecting between high-connected parts of the state-space. Other measures of connectivity have been suggested by (Menache et al., 2002; S\u0327ims\u0327ek and Barto, 2004; S\u0327ims\u0327ek et al., 2005; Simsek and Barto, 2008). Even before the introduction of the options formalism, (Digney, 1998) introduced a method that would create skills based on reward gradients. (Hengst, 2002) exploited the factored structure of the problem to create the hierarchy, by measuring which factors are more predictable and connecting that to the different levels of the hierarchy. A more recent approach models the problem as a dynamic bayesian network that explains the relation between different tasks (Jonsson and Barto, 2006). Another perspective considers how to simultaneously learn different representations for the high-level and the lower level. By ensuring that neighbor states at the lower level are clustered in the higher level, it is possible to create efficient hierarchies of behavior (Bakker\nand Schmidhuber, 2004).\nAn alternative perspective on the creation of a set of reusable macro actions is to exploit commonalities in collections of policies (Thrun et al., 1995; Pickett and Barto, 2002)."}, {"heading": "4.4 Diversity and Competence", "text": "For many learning problems we can define several spaces of parameters, usually the input parameters and the resulting behaviors are trivial concepts. Most of the previous concepts can be applied in different spaces and in many cases, and dependent on the metric of learning, there is a decision to be made on which of these spaces is better to use when guiding exploration. The robot might detect salient events in perceptual space, or generate new references, in the control space of a robot or on the environment space. Although coming from different perspectives: developmental robotics (Baranes and Oudeyer, 2012) and evolutionary development (Lehman and Stanley, 2011) argue that exploration in the behavior space might be more efficient and relevant than in the space of the parameters that generate that behavior.\nThe first perspective proposed by (Lehman and Stanley, 2011) is that many different genetic controller encodings might lead to very similar behaviors, and when considering also the morphological and environmental restrictions, the space of behaviors is much smaller than the space of controller encodings. The notion of diversity is not clear due to the redundancy in the control parameters, see (Mouret and Doncieux, 2011) for a discussion. It is interesting to note that in a more computational perspective, particle filters tend to also consider diversity criteria to detect convergence and improve efficiency (Gilks and Berzuini, 2002).\nFrom a robot controller point of view we can see a similar idea as proposed by (Baranes and Oudeyer, 2010), see Figure 10. In this case we consider the case of redundant robots where many different joint position lead to the same task space position of the robot. And so a dramatic reduction of the size of the exploration space is achieved. Also the authors introduced the concept of competence where, and again for the case of redundant robots, the robot might prefer to be able to reach a larger volume of the task space, even without knowing all the possible solution to reach each point, than being able to use all the dexterity in a small\npart of the task space and not knowing how to reach the rest.\nOther authors have considered also exploration in task space, e.g. (Jamone et al., 2011) and (Rolf et al., 2011). We can refer again to the works of (Schmidhuber, 2011; Lim and Auer, 2012) and see that they also consider as criteria having access to the more diversified set of policies possible."}, {"heading": "4.5 Development", "text": "The previous discussion might lead us to think that a pure data-driven approach might be sufficient to address all the real world complexity. Several authors consider that data-driven approaches must be combined with pre-structured information. For examples artificial development considers that the learning process is guided not only by the environment and the data it is collect but also by the \u201dgenetic information\u201d of the system (Elman, 1997; Lungarella et al., 2003).\nIn living organism, it is believed that maturational constraints help reduce the complexity of learning in early stages thus resulting in better and more efficient learning in the longer term. It does this by structuring the perceptual and motor space (Nagai et al., 2006; Lee et al., 2007; Lopes and Santos-Victor, 2007; Lapeyre et al., 2011; Baranes and Oudeyer, 2011; Oudeyer et al., 2013) or by developing intrinsic rewards that focus attention to informative experiences (Baldassarre, 2011; Singh et al., 2010b), pre-dispositions to detect meaningful salient events, among many other aspects."}, {"heading": "4.6 Open Challenges", "text": "In a broad perspective, open-ended learning and curiosity is still far from being a problem well understood, or even well formulated. Evolutionary models (Singh et al., 2010b) and recent studies in neurosciences (Gottlieb et al., 2013) are starting to provide a more clear picture on if, and why, curiosity is an intrinsic drive in many animals. A clear understanding on why this drive exist, what triggers the drive to learn new tasks, and why agents seek complex situations will provide many insights on human cognition and on the development of autonomous and robust agents.\nA related discussion is that a purely data-driven approach will not be able to consider such long-term learning problems. If we consider large domain problems, time-varying, the need for prior information that provide exploration constraints will be a fundamental aspect on any algorithm. This developmental constraints, and all genetic information, will be fundamental to any of such endeavor. We note that during learning and development it is required to co-develop representations, exploration strategies, learning methods, and hierarchical organization of behavior will require the introduction of novel theoretical frameworks."}, {"heading": "5 Interaction", "text": "The previous sections considered active learning where the agents act, or make queries, and either the environment or an oracle provides more data. Such abstract formalism might not be the best model when the oracle is a human with specific reasoning capabilities. Humans have a tremendous amount of prior knowledge, inference capabilities that allows them to solve very complex problems and so a benevolent teacher might guide exploration and provide information for learning. Feedback from a teacher takes the form of: initial condition for further self-exploration in robotics (Nicolescu and Mataric, 2003), information about the task solution (Calinon et al., 2007), information about affordances (Ekvall and Kragic, 2004), information about the task representation (Lopes et al., 2007), among others. Figure 11 explains this process where the world state, the signals produced by the teacher and the signal required to the learning algorithms are not in the same representation and an explicit mechanism of translation is required. An active learning ap-\nproach can also allow a robot to inquire a user about adequate state representations, see Fig. 12.\nIt has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004). Under this approach the teacher interacts with the robot and provides extra feedback. Approaches have considered extra reinforcement signals (Thomaz and Breazeal, 2008), action requests (Grollman and Jenkins, 2007a; Lopes et al., 2009b), disambiguation among actions (Chernova and Veloso, 2009), preferences among states (Mason and Lopes, 2011), iterations between practice and user feedback sessions (Judah et al., 2010; Korupolu et al., 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).\nIn this document we are more focused in active perspective and so it is the learner that has to ask for such information. Having a human on the loop we have to consider the cost in terms of tiredness of making many queries. Studies and algorithms have considered such aspect and addressed the problem of deciding when to ask. Most approaches will just ask to user when-\never the information is needed (Nicolescu and Mataric, 2001) or when there is high uncertainty (Chernova and Veloso, 2009). A more advanced situation considers making queries only when it is too risky to try experiments (Doshi et al., 2008). (Cakmak et al., 2010a) compare the results when the robot has the option of asking or not the teacher for feedback and in a more recent work they study how can the robot make different types of queries including: label, features and demonstrations (Cakmak and Thomaz, 2011; Cakmak and Thomaz, 2012).\nMost of these systems have been developed to speedup learning or to provide a more intuitive way to program robots. There are reasons to believe that an interactive perspective on learning from demonstration might lead to better results (even for the same amount of data). The theoretical aspects of these interactive systems have not been explored, besides the directly applied results from active learning. One justification for the need and expected gain of using such systems is discussed by (Ross and Bagnell, 2010). Even if an agent learns from a good demonstration then, when executing that learned policy, its error will grow with T 2 (where T is the horizon of the task). The reason being that any deviation from the correct policy moves the learner to a region where the policy has a worse fit. If a new demonstration is requested in that new region then the system learns not only how to execute a good policy but also how to correct from small mistakes. Such observation, as the authors refer, was already given by (Pomerleau, 1992) without a proof.\nAnother reason to use interactive systems is that when the users train the system they might become more comfortable with using it and accept it. See the work from (Ogata et al., 2003) for a study on this subject. The queries of the robot will have the dual goal of allowing the robot to deal with its own limitations and give the user information about the robot\u2019s uncertainty on the task being learned (Fong et al., 2003; Chao et al., 2010).\nThere are many cases where the learning data comes directly from humans but no special uncertainty models are used. Such system either have an intuitive interface to provide information to the system during teleoperation (Kristensen et al., 1999), or it is the system that initiates questions based on perceptual saliency (Lutkebohle et al., 2009). There is also the case where the authors just follow the standard active learning\nsetting (e.g. to learn a better gesture classification the system is able to ask the user to provide more examples of a given class (Francke et al., 2007) even if for human-robot interfaces (Lee and Xu, 1996)).\nThis section will start by presenting a perspective on the behavior of humans when they teach machines and the various ways in which a human can help a learning system. We then divide our review into systems for active learning from demonstration where the learner makes questions to the user and a second part where the teacher intervenes whenever it is required. Finally we show that sometimes it is important to try to learn explicit the teaching behavior of the teacher."}, {"heading": "5.1 Interactive Learning Scenarios", "text": "The type of feedback/guidance that an human can provide depends on the task, the human knowledge, how easy it is to provide each type of information, the communication channels between the system and the user, among many other factors. For instance in a financial situation it is straightforward to attribute values to the outcomes of a policy but in some tasks, dancing for instance, it is much easier to provide trajectory information. In some tasks a combination of both is also required, for instance when teaching how to play tennis it is easy to provide a numeric evaluation of the different policies, but only by showing particular motions can a learner really improve its game.\nThe presence of other agents in the environment creates diverse opportunities for different learning and exploration scenarios. We can view the other agents as teachers that can behave in different ways. They can provide:\n\u2022 guidance on exploration\n\u2022 examples\n\u2022 task goals\n\u2022 task solutions\n\u2022 example trajectories\n\u2022 quantitative or qualitative evaluation on behavior\n\u2022 information about their preferences\nBy guiding exploration we consider that the agent is able to learn by itself but the extra feedback, or guidance, provided by the teacher will improve its learning\nspeed. The teacher can be demonstrating new tasks and from this the learner might get several extra elements: the goal of the task, how to solve the task, or simply environment trajectories. Another perspective puts the teacher in a jury perspective of evaluating the behavior of the system, either providing directly an evaluation on the learner\u2019s behavior or by reveling his preferences. Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstro\u0308m, 2010).\nWe can describe interactive learning system along another axis, and that is what type of participation the human has in the process. Table 5 provides a nonexhaustive list of the different positions of a teacher during learning. First, the demonstrator can be completely unaware that a learner is observing him and collecting data for learning. Many systems are like this and use the observation as a dataset to learn. Most interesting cases are those where the teacher is aware of the situation and provides the learner with a batch of data; this is the more common setting. In the active approach the teacher is passive and only answers the questions of the learner (refer to Section 5.3), while in the teaching setting it is the teacher that actively selects the best demonstration examples, taking into account the task and the learner\u2019s progress. Recent examples exist of human on-the-loop setting where the teacher observes the actions of the robot and only acts when it is required to make a correction or provide more data.\nAs usual all these approaches are not pure and many combine different perspectives. There are situations where different teachers are available to be observed and the learner chooses which one to observe (Price and Boutilier, 2003) where some of them might not even be cooperative (Shon et al., 2007), and even choose between looking at a demonstrator or just learn by self-exploration (Nguyen et al., 2011)."}, {"heading": "5.2 Human Behavior", "text": "Humans change the way they act when they are demonstrating actions to others (Nagai and Rohlfing, 2009). This might help the learner by attracting attention to the relevant parts of the actions, but it also\nshows that humans will change the way a task is executed, see (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012).\nIt is clear now that when teaching robots there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011). An important aspect is that, many times, the feedback is ambiguous and deviates from the mathematical interpretation of a reward or a sample from a policy. For instance, in the work of (Thomaz and Breazeal, 2008) the teachers frequently gave a reward to exploratory actions even if the signal was used as a standard reward. Also, in some problems we can define an optimal teaching sequence but humans do not behave according to those strategies (Cakmak and Thomaz, 2010).\n(Kaochar et al., 2011) developed a GUI to observe the teaching patterns of humans when teaching an electronic learner to achieve a complex sequential task ( e.g. search and detect scenario ). The more interesting finding is that humans use all available channels of communication, including demonstration; examples; reinforcement; and testing. The use of testing varies a lot among users and without a fixed protocol many users will create very complex forms of interaction."}, {"heading": "5.3 Active Learning by Demonstration", "text": "Social learning, that is learning how to solve a task after seeing it being done has been suggested has an efficient way to program robots. Typically, the burden of selecting informative demonstrations has been completely on the side of the teacher. Active learning approaches endow the learner with the power to select which demonstrations the teacher should perform. Several criteria have been proposed: game theoretic approaches (Shon et al., 2007), entropy (Lopes et al., 2009b; Melo and Lopes, 2010), query by committee (Judah et al., 2012), membership queries (Melo and\nLopes, 2013), maximum classifier uncertainty (Chernova and Veloso, 2009), expected myopic gain (Cohn et al., 2010; Cohn et al., 2011) and risk minimization (Doshi et al., 2008).\nOne common goal is to find the correct behavior, defined as the one that matches the teacher, by repeatedly asking for the correct behavior in a given situation. Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al., 2009b)."}, {"heading": "5.3.1 Learning Policies", "text": "Another learning task of interest is to acquire policies by querying an oracle. (Chernova and Veloso, 2009) used support-vector machine classifiers to make queries to the teacher when it is uncertain about the action to execute as measured by the uncertainty of the classifier. They apply this uncertainty sampling perspective online, and thus only make queries in states that are actually encountered by the robot. A problem with this approach is that the information on the dynamics of the environment is not taken into account when learning the policy. To address this issue, (Melo and Lopes, 2010) proposed a method that computes a kernel based on MDP metrics (Taylor et al., 2008) that includes the information of the environment dynamics. In this way the topology of the dynamics is better preserved and thus better results can be obtained then with just a simple classifier with a naive kernel. They use the method proposed by (Montesano and Lopes, 2012) to make queries where there is lower confidence of the estimated policy.\nDirectly under the inverse reinforcement learning formalism, one of the first approaches were proposed by (Lopes et al., 2009b). After a set of demonstration it is possible to compute the posterior distribution of reward that explain the teacher behavior. By seeing each sample of the posterior distribution as a different expert, the authors took a query by committee perspective allowing the learner to ask the teacher what is the correct action in the state where there is higher disagreement among the experts (or more precisely where the predicted policies are more different). This work was latter extended by considering not just the uncertainty on the policy but the expected reduc-\ntion in the global uncertainty (Cohn et al., 2010; Cohn et al., 2011).\nThe teacher can directly ask about the reward value at a given location (Regan and Boutilier, 2011) and it has been shown that reward queries can be combined with action queries (Melo and Lopes, 2013).\nThe previous works on active inverse reinforcement learning can be seen as a way to infer the preferences of the teacher. This problem of preference elicitation has been addressed in several domains (Fu\u0308rnkranz and Hu\u0308llermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007)."}, {"heading": "5.4 Online Feedback and Guidance", "text": "Another approach is to consider that the robot is always executing and that a teacher/user might interrupt it at any time and assume the command of the robot. These corrections will act as new demonstrations to be incorporated in the learning process.\nThe TAMMER framework, and its extensions, considers how signals from humans can speed up exploration and learning in reinforcement learning tasks (Knox and Stone, 2009; Knox and Stone, 2010). The interesting aspect is that MDP reward is informational poor but it is sampled from the process while the human reinforcement is rich in information but might have stronger biases. Knox (Knox and Stone, 2009; Knox and Stone, 2010) presented the initial framework where the agent learns to predict the human feedback and then selects actions to maximize the expected reward from the human. After learning to predict such behavior during learning the agent will also observe the reward from the environment. The combination of both allows the robot to learn better using information given by the user will shape the reward function (Ng et al., 1999) improving the learning rate of the agent. Recently this process was improved to allow both processes to occur simultaneously (Knox and Stone, 2012).\nIt is important to take care to ensure that the shaping made by a human does not change the task. (Zhang et al., 2009) introduced a method were the teacher is able to provide extra rewards to change the behavior of the learner but, at the same time, considering that there is a limited budget on such extra rewards. Results showed that there are some tasks that are not possible to teach under a limited budget.\nOther approaches considered that the learner can train by self-exploration and have several periods where the teacher is able to criticize its progress (Manoonpong et al., 2010; Judah et al., 2010).\nSeveral work consider that initially the system will not show any initiative and will be operated by the user. Then as learning progresses the system will start acting according to the learned model while the teacher will act when a correction, or an exception, is needed. For instance, in the dogged learning approach suggested in (Grollman and Jenkins, 2007a; Grollman and Jenkins, 2007b; Grollman and Jenkins, 2008) an AIBO robot is teleoperated and learns a policy from the user to dribble a ball towards a goal. After that training period the robot starts executing the learned policy but, at any time, the user has the possibility of resuming the teleoperation to provide eventual corrections. With this process a policy, encoded with a gaussian process, can be learned with better quality. A similar approach was followed in the work of (Mason and Lopes, 2011). The main difference is that here the robot does not learn a policy and instead learns the preferences of the user and the interaction is done with a natural language interface. The authors consider a cleaning robot that is able to move objects in a room. Initially the robot as only a generic user profile that consider desired object locations, then after several interactions the robot moves the objects to the requested location. Every time the user says that the room is clean/tidy, the robot memorizes the configuration and through a kernel method is able to generalize what is a clean of not clean robot to different contexts. With the advent of compliant robots the same approach can be made where the corrections are provided directly by moving the robot arm (Sauser et al., 2011).\nAn interesting aspect that was not explored much is to consider delays in the user\u2019s feedback. If such asynchronous behavior exist then the agent must decide how to act while waiting for the feedback (Cohn et al., 2012)."}, {"heading": "5.5 Ambiguous Protocols and Teacher Adaptation", "text": "In most of the previous discussion we considered that the feedback signals provided by the teacher have a semantic meaning that is known to the learner. Nevertheless, in many cases the signals provided by the\nteacher might be too noisy or have unknown meaning. Several of these works fall under the learning from communication framework (Klingspor et al., 1997), where a shared understanding between the robot and the teacher is fundamental to allow good interactive learning sessions.\nThe system in (Mohammad and Nishida, 2010) automatically learns different interaction protocols for navigation tasks where the robot learns the actions it should make and which gestures correspond to those actions. In (Lopes et al., 2011; Grizou et al., 2013) the authors introduce a new algorithm for inverse reinforcement learning under multiple instructions with unknown symbols. At each step the learner executes an action and waits for the feedback from the user. This feedback can be understood as a correct/incorrect action, the name of the action itself or a silence. The main difficulty is that the user uses symbols that have an unknown correspondence with such feedback meanings. The learner assumes that the teacher feedback protocol and simultaneously estimates the reward function, the protocols being used and the meaning of the symbols used by the teacher. An early work consider such process in isolation and considered that learning the meaning of communication can be simplified by using the expectation from the already known task model (Kozima and Yano, 2001).\nOther works, such as (Lauria et al., 2002; Kollar et al., 2010), consider the case of learning new instructions and guidance signals for already known tasks, thus providing more efficient commands for instructing the robot. This algorithm is different from typical learning by demonstration systems because data is acquired in an interactive and online setting. It is different from previous learning by interaction systems in the sense that the feedback signals received are unknown.\nThe shared understanding between the teacher and the agents needs also to include a shared knowledge of the names of states. In (Kulick et al., 2013) the authors take an active learning approach allowing the robot to learn state descriptions that are meaningful for the teacher, see Fig. 12."}, {"heading": "5.6 Open Challenges", "text": "There are two big challenges in interactive systems. A first one is to clearly understand the theoretical prop-\nerties of such systems. Empirical results seem to indicate that an interactive approach is more sample efficient than any specific approach taken in isolation. Another aspect is the relation between active learning and optimal teaching, where does not exist yet a clear understanding on the problems that can be learned efficiently but not taught and vice-versa. The second challenge is to model accurately the human, or in general the cognitive/representational differences between the teacher and the learner, during the interactive learning process. This challenge include how to create a shared representation of the problem, how to create interaction protocols, and physical interfaces, that enables such shared understanding, and how to exploit the multi-modal cues that humans provides during instruction and interaction."}, {"heading": "6 Final Remarks", "text": "In this document we presented a general perspective on agents that, aiming at learning fast, look for the most important information required. To our knowledge it is the first time that a unifying look on methods and goals of different communities was made. Several further developments are still necessary in all these domains, but there is already the opportunity to a more multidisciplinary perspective that can give rise to more advanced methods."}, {"heading": "1 Introduction 1", "text": "1.1 Exploration . . . . . . . . . . . . . . . . 2 1.2 Curiosity . . . . . . . . . . . . . . . . . 2 1.3 Interaction . . . . . . . . . . . . . . . . 3 1.4 Organization . . . . . . . . . . . . . . . 4"}, {"heading": "2 Active Learning for Autonomous Intelligent Agents 4", "text": "2.1 Optimal Exploration Problem . . . . . . 4 2.2 Learning Setups . . . . . . . . . . . . . . 6\n2.2.1 Function approximation . . . . . 6 2.2.2 Multi-Armed Bandits . . . . . . 6 2.2.3 MDP . . . . . . . . . . . . . . . 6\n2.3 Space of Exploration Policies . . . . . . 6 2.4 Cost . . . . . . . . . . . . . . . . . . . . 7 2.5 Loss and Active Learning Tasks . . . . . 7 2.6 Measures of Information . . . . . . . . . 8\n2.6.1 Uncertainty sampling and Entropy 8 2.6.2 Minimizing the version space . . 8 2.6.3 Variance reduction . . . . . . . . 9 2.6.4 Empirical Measures . . . . . . . 9\n2.7 Solving strategies . . . . . . . . . . . . . 10\n2.7.1 Theoretical guarantees for binary search . . . . . . . . . . . . 10 2.7.2 Greedy methods . . . . . . . . . 10 2.7.3 Approximate Exploration . . . . 11 2.7.4 No-regret . . . . . . . . . . . . . 11"}, {"heading": "3 Exploration 11", "text": "3.1 Single-Point Exploration . . . . . . . . . 12\n3.1.1 Learning reliability of actions . . 12 3.1.2 Learning general input-output relations . . . . . . . . . . . . . . 12 3.1.3 Policies . . . . . . . . . . . . . . 13\n3.2 Multi-Armed Bandits . . . . . . . . . . 13\n3.2.1 Multiple (Sub-)Tasks . . . . . . . 14 3.2.2 Multiple Strategies . . . . . . . . 14\n3.3 Long-term exploration . . . . . . . . . . 15\n3.3.1 Exploration in Dynamical Systems 15 3.3.2 Exploration / Exploitation . . . 16\n3.4 Others . . . . . . . . . . . . . . . . . . . 17\n3.4.1 Mixed Approaches . . . . . . . . 17 3.4.2 Implicit exploration . . . . . . . 18\n3.4.3 Active Perception . . . . . . . . 18 3.5 Open Challenges . . . . . . . . . . . . . 18"}, {"heading": "4 Curiosity 19", "text": "4.1 Creating Representations . . . . . . . . 20 4.2 Bounded Rationality . . . . . . . . . . . 20 4.3 Creating Skills . . . . . . . . . . . . . . 20\n4.3.1 Regression Models . . . . . . . . 21 4.3.2 MDP . . . . . . . . . . . . . . . 22\n4.4 Diversity and Competence . . . . . . . . 22 4.5 Development . . . . . . . . . . . . . . . 23 4.6 Open Challenges . . . . . . . . . . . . . 23"}, {"heading": "5 Interaction 23", "text": "5.1 Interactive Learning Scenarios . . . . . . 25 5.2 Human Behavior . . . . . . . . . . . . . 25 5.3 Active Learning by Demonstration . . . 26\n5.3.1 Learning Policies . . . . . . . . . 27 5.4 Online Feedback and Guidance . . . . . 27 5.5 Ambiguous Protocols and Teacher Adaptation . . . . . . . . . . . . . . . . 28 5.6 Open Challenges . . . . . . . . . . . . . 28\n6 Final Remarks 29"}], "references": [{"title": "Models for autonomously motivated exploration in reinforcement learning", "author": ["P. Auer", "S.H. Lim", "C. Watkins"], "venue": "Proceedings of the 22nd international conference on Algorithmic learning theory, ALT\u201911, pages 14\u201317, Berlin, Heidelberg. Springer-Verlag.", "citeRegEx": "Auer et al\\.,? 2011", "shortCiteRegEx": "Auer et al\\.", "year": 2011}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["B. Bakker", "J. Schmidhuber"], "venue": "Proc. of the 8-th Conf. on Intelligent Autonomous Systems, pages 438\u2013445.", "citeRegEx": "Bakker and Schmidhuber,? 2004", "shortCiteRegEx": "Bakker and Schmidhuber", "year": 2004}, {"title": "The true sample complexity of active learning", "author": ["M.F. Balcan", "S. Hanneke", "J. Wortman"], "venue": "Conf. on Learning Theory (COLT).", "citeRegEx": "Balcan et al\\.,? 2008", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "What are intrinsic motivations? a biological perspective", "author": ["G. Baldassarre"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Baldassarre,? 2011", "shortCiteRegEx": "Baldassarre", "year": 2011}, {"title": "Online choice of active learning algorithms", "author": ["Y. Baram", "R. El-Yaniv", "K. Luz"], "venue": "The Journal of Machine Learning Research, 5:255\u2013291.", "citeRegEx": "Baram et al\\.,? 2004", "shortCiteRegEx": "Baram et al\\.", "year": 2004}, {"title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study", "author": ["A. Baranes", "P. Oudeyer"], "venue": "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ Inter. Conf. on, pages 1766\u20131773.", "citeRegEx": "Baranes and Oudeyer,? 2010", "shortCiteRegEx": "Baranes and Oudeyer", "year": 2010}, {"title": "The interaction of maturational constraints and intrinsic motivations in active motor development", "author": ["A. Baranes", "P. Oudeyer"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Baranes and Oudeyer,? 2011", "shortCiteRegEx": "Baranes and Oudeyer", "year": 2011}, {"title": "Active learning of inverse models with intrinsically motivated goal exploration in robots", "author": ["A. Baranes", "P. Oudeyer"], "venue": "Robotics and Autonomous Systems.", "citeRegEx": "Baranes and Oudeyer,? 2012", "shortCiteRegEx": "Baranes and Oudeyer", "year": 2012}, {"title": "R-iac: Robust intrinsically motivated exploration and active learning", "author": ["A. Baran\u00e8s", "Oudeyer", "P.-Y."], "venue": "Autonomous Mental Development, IEEE Transactions on, 1(3):155\u2013169.", "citeRegEx": "Baran\u00e8s et al\\.,? 2009", "shortCiteRegEx": "Baran\u00e8s et al\\.", "year": 2009}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems, 13(4):341\u2013379.", "citeRegEx": "Barto and Mahadevan,? 2003", "shortCiteRegEx": "Barto and Mahadevan", "year": 2003}, {"title": "Intrinsically motivated learning of hierarchical collections of skills", "author": ["A. Barto", "S. Singh", "N. Chentanez"], "venue": "Inter. Conf. on development and learning (ICDL\u201904), San Diego, USA.", "citeRegEx": "Barto et al\\.,? 2004", "shortCiteRegEx": "Barto et al\\.", "year": 2004}, {"title": "Neural net algorithms that learn in polynomial time from examples and queries", "author": ["E.B. Baum"], "venue": "Neural Networks, IEEE Transactions on, 2(1):5\u201319.", "citeRegEx": "Baum,? 1991", "shortCiteRegEx": "Baum", "year": 1991}, {"title": "On the theory of dynamic programming", "author": ["R. Bellman"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 38(8):716. 29", "citeRegEx": "Bellman,? 1952", "shortCiteRegEx": "Bellman", "year": 1952}, {"title": "Sound source localisation through active audition", "author": ["E. Berglund", "J. Sitte"], "venue": "Intelligent Robots and Systems, 2005.(IROS 2005). 2005 IEEE/RSJ Inter. Conf. on, pages 653\u2013658.", "citeRegEx": "Berglund and Sitte,? 2005", "shortCiteRegEx": "Berglund and Sitte", "year": 2005}, {"title": "Conflict, arousal, and curiosity", "author": ["D. Berlyne"], "venue": "McGraw-Hill Book Company.", "citeRegEx": "Berlyne,? 1960", "shortCiteRegEx": "Berlyne", "year": 1960}, {"title": "A formalism for learning from demonstration", "author": ["E. Billing", "T. Hellstr\u00f6m"], "venue": "Paladyn. Journal of Behavioral Robotics, 1(1):1\u201313.", "citeRegEx": "Billing and Hellstr\u00f6m,? 2010", "shortCiteRegEx": "Billing and Hellstr\u00f6m", "year": 2010}, {"title": "Information based adaptive robotic exploration", "author": ["F. Bourgault", "A. Makarenko", "S. Williams", "B. Grocholsky", "H. Durrant-Whyte"], "venue": "IEEE/RSJ Conf. on Intelligent Robots and Systems (IROS).", "citeRegEx": "Bourgault et al\\.,? 2002", "shortCiteRegEx": "Bourgault et al\\.", "year": 2002}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research, 3:213\u2013231.", "citeRegEx": "Brafman and Tennenholtz,? 2003", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 2003}, {"title": "Local utility elicitation in gai models", "author": ["D. Braziunas", "C. Boutilier"], "venue": "Twenty-first Conf. on Uncertainty in Artificial Intelligence, pages 42\u201349.", "citeRegEx": "Braziunas and Boutilier,? 2005", "shortCiteRegEx": "Braziunas and Boutilier", "year": 2005}, {"title": "Tutelage and collaboration for humanoid robots", "author": ["C. Breazeal", "A. Brooks", "J. Gray", "G. Hoffman", "J. Lieberman", "H. Lee", "A.L. Thomaz", "D. Mulanda."], "venue": "Inter. Journal of Humanoid Robotics, 1(2).", "citeRegEx": "Breazeal et al\\.,? 2004", "shortCiteRegEx": "Breazeal et al\\.", "year": 2004}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V. Cora", "N. De Freitas"], "venue": "Arxiv preprint arXiv:1012.2599.", "citeRegEx": "Brochu et al\\.,? 2010", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Active preference learning with discrete choice data", "author": ["E. Brochu", "N. de Freitas", "A. Ghosh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Brochu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2007}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends R", "citeRegEx": "Bubeck and Cesa.Bianchi,? 2012", "shortCiteRegEx": "Bubeck and Cesa.Bianchi", "year": 2012}, {"title": "Seeing actions as hierarchically organised structures: great ape manualskills", "author": ["R.W. Byrne"], "venue": "The Imitative Mind. Cambridge University Press.", "citeRegEx": "Byrne,? 2002", "shortCiteRegEx": "Byrne", "year": 2002}, {"title": "Designing interactions for robot active learners", "author": ["M. Cakmak", "C. Chao", "A. Thomaz"], "venue": "IEEE Transactions on Autonomous Mental Development, 2(2):108\u2013 118.", "citeRegEx": "Cakmak et al\\.,? 2010a", "shortCiteRegEx": "Cakmak et al\\.", "year": 2010}, {"title": "Exploiting social partners in robot learning", "author": ["M. Cakmak", "N. DePalma", "R. Arriaga", "A. Thomaz"], "venue": "Autonomous Robots.", "citeRegEx": "Cakmak et al\\.,? 2010b", "shortCiteRegEx": "Cakmak et al\\.", "year": 2010}, {"title": "Algorithmic and human teaching of sequential decision tasks", "author": ["M. Cakmak", "M. Lopes"], "venue": "AAAI Conference on Artificial Intelligence (AAAI\u201912), Toronto, Canada.", "citeRegEx": "Cakmak and Lopes,? 2012", "shortCiteRegEx": "Cakmak and Lopes", "year": 2012}, {"title": "Optimality of human teachers for robot learners", "author": ["M. Cakmak", "A. Thomaz"], "venue": "Inter. Conf. on Development and Learning (ICDL).", "citeRegEx": "Cakmak and Thomaz,? 2010", "shortCiteRegEx": "Cakmak and Thomaz", "year": 2010}, {"title": "Active learning with mixed query types in learning from demonstration", "author": ["M. Cakmak", "A. Thomaz"], "venue": "Proc. of the ICML Workshop on New Developments in Imitation Learning.", "citeRegEx": "Cakmak and Thomaz,? 2011", "shortCiteRegEx": "Cakmak and Thomaz", "year": 2011}, {"title": "Designing robot learners that ask good questions", "author": ["M. Cakmak", "A. Thomaz"], "venue": "7th ACM/IEE Inter. Conf. on Human-Robot Interaction.", "citeRegEx": "Cakmak and Thomaz,? 2012", "shortCiteRegEx": "Cakmak and Thomaz", "year": 2012}, {"title": "On learning, representing and generalizing a task in a humanoid robot", "author": ["S. Calinon", "F. Guenter", "A. Billard"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part B. Special issue on robot learning by observation, demonstration and imitation, 37(2):286\u2013", "citeRegEx": "Calinon et al\\.,? 2007", "shortCiteRegEx": "Calinon et al\\.", "year": 2007}, {"title": "Upper confidence bounds algorithms for active learning in multi-armed bandits", "author": ["A. Carpentier", "M. Ghavamzadeh", "A. Lazaric", "R. Munos", "P. Auer"], "venue": "Algorithmic Learning Theory.", "citeRegEx": "Carpentier et al\\.,? 2011", "shortCiteRegEx": "Carpentier et al\\.", "year": 2011}, {"title": "Minimax bounds for active learning", "author": ["R. Castro", "R. Novak"], "venue": "IEEE Trans. on Information Theory, 54(5):2339\u20132353.", "citeRegEx": "Castro and Novak,? 2008", "shortCiteRegEx": "Castro and Novak", "year": 2008}, {"title": "Learning exploration/exploitation strategies for single trajectory reinforcement learning", "author": ["M. Castronovo", "F. Maes", "R. Fonteneau", "D. Ernst"], "venue": "10th European Workshop on Reinforcement Learning (EWRL 2012).", "citeRegEx": "Castronovo et al\\.,? 2012", "shortCiteRegEx": "Castronovo et al\\.", "year": 2012}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["U. Chajewska", "D. Koller", "R. Parr"], "venue": "National Conf. on Artificial Intelligence, pages 363\u2013 369. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.", "citeRegEx": "Chajewska et al\\.,? 2000", "shortCiteRegEx": "Chajewska et al\\.", "year": 2000}, {"title": "Transparent active learning for robots", "author": ["C. Chao", "M. Cakmak", "A. Thomaz"], "venue": "Human-Robot Interaction (HRI), 2010 5th ACM/IEEE Inter. Conf. on, pages 317\u2013324.", "citeRegEx": "Chao et al\\.,? 2010", "shortCiteRegEx": "Chao et al\\.", "year": 2010}, {"title": "Interactive policy learning through confidence-based autonomy", "author": ["S. Chernova", "M. Veloso"], "venue": "J. Artificial Intelligence Research, 34:1\u201325.", "citeRegEx": "Chernova and Veloso,? 2009", "shortCiteRegEx": "Chernova and Veloso", "year": 2009}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning, 15(2):201\u2013221. 30", "citeRegEx": "Cohn et al\\.,? 1994", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research, 4:129\u2013145.", "citeRegEx": "Cohn et al\\.,? 1996", "shortCiteRegEx": "Cohn et al\\.", "year": 1996}, {"title": "Comparing action-query strategies in semi-autonomous agents", "author": ["R. Cohn", "E. Durfee", "S. Singh"], "venue": "Inter. Conf. on Autonomous Agents and Multiagent Systems.", "citeRegEx": "Cohn et al\\.,? 2011", "shortCiteRegEx": "Cohn et al\\.", "year": 2011}, {"title": "Planning delayed-response queries and transient policies under reward uncertainty", "author": ["R. Cohn", "E. Durfee", "S. Singh"], "venue": "Seventh Annual Workshop on Multiagent Sequential Decision Making Under Uncertainty (MSDM-2012), page 17.", "citeRegEx": "Cohn et al\\.,? 2012", "shortCiteRegEx": "Cohn et al\\.", "year": 2012}, {"title": "Selecting Operator Queries using Expected Myopic Gain", "author": ["R. Cohn", "M. Maxim", "E. Durfee", "S. Singh"], "venue": "2010 IEEE/WIC/ACM Inter. Conf. on Web Intelligence and Intelligent Agent Technology, pages 40\u201347.", "citeRegEx": "Cohn et al\\.,? 2010", "shortCiteRegEx": "Cohn et al\\.", "year": 2010}, {"title": "Using relative novelty to identify useful temporal abstractions in reinforcement learning", "author": ["O. \u015eim\u015fek", "A.G. Barto"], "venue": "Inter. Conf. on Machine Learning.", "citeRegEx": "\u015eim\u015fek and Barto,? 2004", "shortCiteRegEx": "\u015eim\u015fek and Barto", "year": 2004}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 337\u2013344.", "citeRegEx": "Dasgupta,? 2005", "shortCiteRegEx": "Dasgupta", "year": 2005}, {"title": "Two faces of active learning", "author": ["S. Dasgupta"], "venue": "Theoretical computer science, 412(19):1767\u20131781.", "citeRegEx": "Dasgupta,? 2011", "shortCiteRegEx": "Dasgupta", "year": 2011}, {"title": "Bayesian q-learning", "author": ["R. Dearden", "N. Friedman", "S. Russell"], "venue": "AAAI Conf. on Artificial Intelligence, pages 761\u2013768.", "citeRegEx": "Dearden et al\\.,? 1998", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "A survey on policy search for robotics", "author": ["M. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics, 21.", "citeRegEx": "Deisenroth et al\\.,? 2013", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Learning object-specific grasp affordance densities", "author": ["R. Detry", "E. Baseski", "M. P", "Y. Touati", "N. Kruger", "O. Kroemer", "J. Peters", "J. Piater"], "venue": "In IEEE 8TH Inter. Conf. on Development and Learning", "citeRegEx": "Detry et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Detry et al\\.", "year": 2009}, {"title": "Learning hierarchical control structures for multiple tasks and changing environments", "author": ["B. Digney"], "venue": "fifth Inter. Conf. on simulation of adaptive behavior on From animals to animats, volume 5, pages 321\u2013330.", "citeRegEx": "Digney,? 1998", "shortCiteRegEx": "Digney", "year": 1998}, {"title": "Teaching and learning of robot tasks via observation of human performance", "author": ["R. Dillmann"], "venue": "Robotics and Autonomous Systems, 47(2):109\u2013116.", "citeRegEx": "Dillmann,? 2004", "shortCiteRegEx": "Dillmann", "year": 2004}, {"title": "Learning robot behaviour and skills based on human demonstration and advice: the machine learning paradigm", "author": ["R. Dillmann", "O. Rogalla", "M. Ehrenmann", "R. Zollner", "M. Bordegoni"], "venue": "Inter. Symposium on Robotics Research (ISRR), volume 9, pages 229\u2013238.", "citeRegEx": "Dillmann et al\\.,? 2000", "shortCiteRegEx": "Dillmann et al\\.", "year": 2000}, {"title": "Interactive natural programming of robots: Introductory overview", "author": ["R. Dillmann", "R. Z\u00f6llner", "M. Ehrenmann", "O Rogalla"], "venue": "In Tsukuba Research Center, AIST. Citeseer", "citeRegEx": "Dillmann et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dillmann et al\\.", "year": 2002}, {"title": "Active learning for outdoor obstacle detection", "author": ["C. Dima", "M. Hebert"], "venue": "Robotics Science and Systems Conf., Cambridge, MA.", "citeRegEx": "Dima and Hebert,? 2005", "shortCiteRegEx": "Dima and Hebert", "year": 2005}, {"title": "Enabling learning from large datasets: Applying active learning to mobile robotics", "author": ["C. Dima", "M. Hebert", "A. Stentz"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE Inter. Conf. on, volume 1, pages 108\u2013114.", "citeRegEx": "Dima et al\\.,? 2004", "shortCiteRegEx": "Dima et al\\.", "year": 2004}, {"title": "Robot shaping: Developing autonomous agents through learning", "author": ["M. Dorigo", "M. Colombetti"], "venue": "Artificial intelligence, 71(2):321\u2013370.", "citeRegEx": "Dorigo and Colombetti,? 1994", "shortCiteRegEx": "Dorigo and Colombetti", "year": 1994}, {"title": "Reinforcement learning with limited reinforcement: using bayes risk for active learning in pomdps", "author": ["F. Doshi", "J. Pineau", "N. Roy"], "venue": "25th Inter. Conf. on Machine learning (ICML\u201908), pages 256\u2013263.", "citeRegEx": "Doshi et al\\.,? 2008", "shortCiteRegEx": "Doshi et al\\.", "year": 2008}, {"title": "Design for an optimal probe", "author": ["M. Duff"], "venue": "Inter. Conf. on Machine Learning.", "citeRegEx": "Duff,? 2003", "shortCiteRegEx": "Duff", "year": 2003}, {"title": "Interactive grasp learning based on human demonstration", "author": ["S. Ekvall", "D. Kragic"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE Inter. Conf. on, volume 4, pages 3519\u20133524.", "citeRegEx": "Ekvall and Kragic,? 2004", "shortCiteRegEx": "Ekvall and Kragic", "year": 2004}, {"title": "Rethinking innateness: A connectionist perspective on development, volume 10", "author": ["J. Elman"], "venue": "The MIT press.", "citeRegEx": "Elman,? 1997", "shortCiteRegEx": "Elman", "year": 1997}, {"title": "Interactive machine learning", "author": ["J. Fails", "D. Olsen Jr"], "venue": "8th Inter. Conf. on Intelligent user interfaces, pages 39\u201345.", "citeRegEx": "Fails and Jr,? 2003", "shortCiteRegEx": "Fails and Jr", "year": 2003}, {"title": "Adaptive mobile robot navigation and mapping", "author": ["H.J.S. Feder", "J.J. Leonard", "C.M. Smith"], "venue": "International Journal of Robotics Research, 18(7):650\u2013668.", "citeRegEx": "Feder et al\\.,? 1999", "shortCiteRegEx": "Feder et al\\.", "year": 1999}, {"title": "Learning about objects through action: Initial steps towards artificial cognition", "author": ["P. Fitzpatrick", "G. Metta", "L. Natale", "S. Rao", "G. Sandini."], "venue": "IEEE Inter. Conf. on Robotics and Automation, Taipei, Taiwan.", "citeRegEx": "Fitzpatrick et al\\.,? 2003", "shortCiteRegEx": "Fitzpatrick et al\\.", "year": 2003}, {"title": "Robot, asker of questions", "author": ["T. Fong", "C. Thorpe", "C. Baur"], "venue": "Robotics and Autonomous systems, 42(3):235\u2013243.", "citeRegEx": "Fong et al\\.,? 2003", "shortCiteRegEx": "Fong et al\\.", "year": 2003}, {"title": "Active markov localization for mobile robots", "author": ["D. Fox", "W. Burgard", "S. Thrun"], "venue": "Robotics and Autonomous Systems, 25(3):195\u2013207.", "citeRegEx": "Fox et al\\.,? 1998", "shortCiteRegEx": "Fox et al\\.", "year": 1998}, {"title": "A reinforcement learning algorithm with polynomial interaction complexity for only-costly-observable mdps", "author": ["R. Fox", "M. Tennenholtz"], "venue": "National Conf. on Artificial Intelligence (AAAI).", "citeRegEx": "Fox and Tennenholtz,? 2007", "shortCiteRegEx": "Fox and Tennenholtz", "year": 2007}, {"title": "Real-time hand gesture detection and recognition using boosted classifiers and active learning", "author": ["H. Francke", "J. Ruiz-del Solar", "R. Verschae"], "venue": "Advances in Image and Video Technology, pages 533\u2013547.", "citeRegEx": "Francke et al\\.,? 2007", "shortCiteRegEx": "Francke et al\\.", "year": 2007}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine learning, 28(2):133\u2013168.", "citeRegEx": "Freund et al\\.,? 1997", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Preference learning: An introduction", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier"], "venue": "Preference Learning, page 1.", "citeRegEx": "F\u00fcrnkranz and H\u00fcllermeier,? 2010", "shortCiteRegEx": "F\u00fcrnkranz and H\u00fcllermeier", "year": 2010}, {"title": "Efficient spacetime modeling for informative sensing", "author": ["S. Garg", "A. Singh", "F. Ramos"], "venue": "Sixth Inter. Workshop on Knowledge Discovery from Sensor Data, pages 52\u201360.", "citeRegEx": "Garg et al\\.,? 2012", "shortCiteRegEx": "Garg et al\\.", "year": 2012}, {"title": "Following a moving target?onte carlo inference for dynamic bayesian models", "author": ["W. Gilks", "C. Berzuini"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(1):127\u2013146.", "citeRegEx": "Gilks and Berzuini,? 2002", "shortCiteRegEx": "Gilks and Berzuini", "year": 2002}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J. Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 148\u2013177.", "citeRegEx": "Gittins,? 1979", "shortCiteRegEx": "Gittins", "year": 1979}, {"title": "Online distributed sensor selection", "author": ["D. Golovin", "M. Faulkner", "A. Krause"], "venue": "Proc. ACM/IEEE Inter. Conf. on Information Processing in Sensor Networks (IPSN).", "citeRegEx": "Golovin et al\\.,? 2010a", "shortCiteRegEx": "Golovin et al\\.", "year": 2010}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "Proc. Inter. Conf. on Learning Theory (COLT).", "citeRegEx": "Golovin and Krause,? 2010", "shortCiteRegEx": "Golovin and Krause", "year": 2010}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "Proc. Neural Information Processing Systems (NIPS).", "citeRegEx": "Golovin et al\\.,? 2010b", "shortCiteRegEx": "Golovin et al\\.", "year": 2010}, {"title": "Attention, learning, and the value of information", "author": ["J. Gottlieb"], "venue": "Neuron, 76(2):281\u2013295.", "citeRegEx": "Gottlieb,? 2012", "shortCiteRegEx": "Gottlieb", "year": 2012}, {"title": "Information seeking, curiosity and attention: computational and empirical mechanisms", "author": ["J. Gottlieb", "Oudeyer", "P.-Y.", "M. Lopes", "A. Baranes"], "venue": "Trends in Cognitive Sciences.", "citeRegEx": "Gottlieb et al\\.,? 2013", "shortCiteRegEx": "Gottlieb et al\\.", "year": 2013}, {"title": "Robot Learning Simultaneously a Task and How to Interpret Human Instructions", "author": ["J. Grizou", "M. Lopes", "Oudeyer", "P.-Y."], "venue": "Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics (ICDL-EpiRob), Osaka, Japan.", "citeRegEx": "Grizou et al\\.,? 2013", "shortCiteRegEx": "Grizou et al\\.", "year": 2013}, {"title": "Dogged learning for robots", "author": ["D. Grollman", "O. Jenkins"], "venue": "Robotics and Automation, 2007 IEEE Inter. Conf. on, pages 2483\u20132488.", "citeRegEx": "Grollman and Jenkins,? 2007a", "shortCiteRegEx": "Grollman and Jenkins", "year": 2007}, {"title": "Learning robot soccer skills from demonstration", "author": ["D. Grollman", "O. Jenkins"], "venue": "Development and Learning, 2007. ICDL 2007. IEEE 6th Inter. Conf. on, pages 276\u2013281.", "citeRegEx": "Grollman and Jenkins,? 2007b", "shortCiteRegEx": "Grollman and Jenkins", "year": 2007}, {"title": "Sparse incremental learning for interactive robot control policy estimation", "author": ["D. Grollman", "O. Jenkins"], "venue": "Robotics and Automation, 2008. ICRA 2008. IEEE Inter. Conf. on, pages 3315\u20133320.", "citeRegEx": "Grollman and Jenkins,? 2008", "shortCiteRegEx": "Grollman and Jenkins", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research, 3:1157\u20131182.", "citeRegEx": "Guyon and Elisseeff,? 2003", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "Intrinsically motivated affordance discovery and modeling", "author": ["S. Hart", "R. Grupen"], "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems, pages 279\u2013300. Springer.", "citeRegEx": "Hart and Grupen,? 2013", "shortCiteRegEx": "Hart and Grupen", "year": 2013}, {"title": "Intrinsically motivated hierarchical manipulation", "author": ["S. Hart", "S. Sen", "R. Grupen"], "venue": "2008 IEEE Conf. on Robots and Automation (ICRA), Pasadena, California.", "citeRegEx": "Hart et al\\.,? 2008", "shortCiteRegEx": "Hart et al\\.", "year": 2008}, {"title": "Discovering hierarchy in reinforcement learning with hexq", "author": ["B. Hengst"], "venue": "MACHINE LEARNING-Inter. WORKSHOP THEN Conf.-, pages 243\u2013250. Citeseer.", "citeRegEx": "Hengst,? 2002", "shortCiteRegEx": "Hengst", "year": 2002}, {"title": "Learning exploration strategies", "author": ["T. Hester", "M. Lopes", "P. Stone"], "venue": "AAMAS, USA.", "citeRegEx": "Hester et al\\.,? 2013", "shortCiteRegEx": "Hester et al\\.", "year": 2013}, {"title": "Reinforcement Learning: State-of-the-Art, chapter Learning and Using Models", "author": ["T. Hester", "P. Stone"], "venue": "Springer.", "citeRegEx": "Hester and Stone,? 2011", "shortCiteRegEx": "Hester and Stone", "year": 2011}, {"title": "Intrinsically motivated model learning for a developing curious agent", "author": ["T. Hester", "P. Stone"], "venue": "AAMAS Workshop on Adaptive Learning Agents.", "citeRegEx": "Hester and Stone,? 2012", "shortCiteRegEx": "Hester and Stone", "year": 2012}, {"title": "Portfolio allocation for bayesian optimization", "author": ["M. Hoffman", "E. Brochu", "N. de Freitas"], "venue": "In Uncertainty in artificial intelligence,", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "J. Mach. Learn. Res., 11:1563\u20131600.", "citeRegEx": "Jaksch et al\\.,? 2010", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Learning task space control through goal directed exploration", "author": ["L. Jamone", "L. Natale", "K. Hashimoto", "G. Sandini", "A. Takanishi"], "venue": "Inter. Conf. on Robotics and Biomimetics (ROBIO\u201911).", "citeRegEx": "Jamone et al\\.,? 2011", "shortCiteRegEx": "Jamone et al\\.", "year": 2011}, {"title": "Active learning in partially observable markov decision processes", "author": ["R. Jaulmes", "J. Pineau", "D. Precup"], "venue": "NIPS Workshop on Value of Information in Inference, Learning and Decision-Making.", "citeRegEx": "Jaulmes et al\\.,? 2005", "shortCiteRegEx": "Jaulmes et al\\.", "year": 2005}, {"title": "Causal graph based decomposition of factored mdps", "author": ["A. Jonsson", "A. Barto"], "venue": "The Journal of Machine Learning Research, 7:2259\u20132301.", "citeRegEx": "Jonsson and Barto,? 2006", "shortCiteRegEx": "Jonsson and Barto", "year": 2006}, {"title": "Active imitation learning via reduction to iid active learning", "author": ["K. Judah", "A. Fern", "T. Dietterich"], "venue": "UAI. 32", "citeRegEx": "Judah et al\\.,? 2012", "shortCiteRegEx": "Judah et al\\.", "year": 2012}, {"title": "Reinforcement learning via practice and critique advice", "author": ["K. Judah", "S. Roy", "A. Fern", "T. Dietterich"], "venue": "AAAI Conf. on Artificial Intelligence (AAAI-10).", "citeRegEx": "Judah et al\\.,? 2010", "shortCiteRegEx": "Judah et al\\.", "year": 2010}, {"title": "Gaussian processes for sample efficient reinforcement learning with rmax-like exploration", "author": ["T. Jung", "P. Stone"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 601\u2013616.", "citeRegEx": "Jung and Stone,? 2010", "shortCiteRegEx": "Jung and Stone", "year": 2010}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence, 101(1):99\u2013 134.", "citeRegEx": "Kaelbling et al\\.,? 1998", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "J. Artificial Intelligence Research, 4:237\u2013285.", "citeRegEx": "Kaelbling et al\\.,? 1996", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Towards understanding how humans teach robots", "author": ["T. Kaochar", "R. Peralta", "C. Morrison", "I. Fasel", "T. Walsh", "P. Cohen"], "venue": "User Modeling, Adaption and Personalization, pages 347\u2013352.", "citeRegEx": "Kaochar et al\\.,? 2011", "shortCiteRegEx": "Kaochar et al\\.", "year": 2011}, {"title": "Active learning with gaussian processes for object categorization", "author": ["A. Kapoor", "K. Grauman", "R. Urtasun", "T. Darrell"], "venue": "IEEE 11th Inter. Conf. on Computer Vision.", "citeRegEx": "Kapoor et al\\.,? 2007", "shortCiteRegEx": "Kapoor et al\\.", "year": 2007}, {"title": "Interactive classifier system for real robot learning", "author": ["D. Katagami", "S. Yamada"], "venue": "Robot and Human Interactive Communication, 2000. RO-MAN 2000. Proceedings. 9th IEEE Inter. Workshop on, pages 258\u2013 263.", "citeRegEx": "Katagami and Yamada,? 2000", "shortCiteRegEx": "Katagami and Yamada", "year": 2000}, {"title": "Learning to manipulate articulated objects in unstructured environments using a grounded relational representation", "author": ["D. Katz", "Y. Pyuro", "O. Brock"], "venue": "RSS - Robotics Science and Systems IV, Zurich, Switzerland.", "citeRegEx": "Katz et al\\.,? 2008", "shortCiteRegEx": "Katz et al\\.", "year": 2008}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning, 49(2):209\u2013232.", "citeRegEx": "Kearns and Singh,? 2002", "shortCiteRegEx": "Kearns and Singh", "year": 2002}, {"title": "Functional genomic hypothesis generation and experimentation by a robot scientist", "author": ["R. King", "K. Whelan", "F. Jones", "P. Reiser", "C. Bryant", "S. Muggleton", "D. Kell", "S. Oliver"], "venue": "Nature, 427(6971):247\u2013252.", "citeRegEx": "King et al\\.,? 2004", "shortCiteRegEx": "King et al\\.", "year": 2004}, {"title": "Humanrobot communication and machine learning", "author": ["V. Klingspor", "J. Demiris", "M. Kaiser"], "venue": "Applied Artificial Intelligence, 11(7):719\u2013746.", "citeRegEx": "Klingspor et al\\.,? 1997", "shortCiteRegEx": "Klingspor et al\\.", "year": 1997}, {"title": "Navigation planning in probabilistic roadmaps with uncertainty", "author": ["M. Kneebone", "R. Dearden"], "venue": "ICAPS. AAAI.", "citeRegEx": "Kneebone and Dearden,? 2009", "shortCiteRegEx": "Kneebone and Dearden", "year": 2009}, {"title": "How humans teach agents: A new experimental perspective", "author": ["W. Knox", "B. Glass", "B. Love", "W. Maddox", "P. Stone"], "venue": "Inter. Journal of Social Robotics, Special Issue on Robot Learning from Demonstration.", "citeRegEx": "Knox et al\\.,? 2012", "shortCiteRegEx": "Knox et al\\.", "year": 2012}, {"title": "Interactively shaping agents via human reinforcement: The tamer framework", "author": ["W. Knox", "P. Stone"], "venue": "fifth Inter. Conf. on Knowledge capture, pages 9\u201316.", "citeRegEx": "Knox and Stone,? 2009", "shortCiteRegEx": "Knox and Stone", "year": 2009}, {"title": "Combining manual feedback with subsequent mdp reward signals for reinforcement learning", "author": ["W. Knox", "P. Stone"], "venue": "9th Inter. Conf. on Autonomous Agents and Multiagent Systems (AAMAS\u201910), pages 5\u201312.", "citeRegEx": "Knox and Stone,? 2010", "shortCiteRegEx": "Knox and Stone", "year": 2010}, {"title": "Reinforcement learning from simultaneous human and mdp reward", "author": ["W. Knox", "P. Stone"], "venue": "11th Inter. Conf. on Autonomous Agents and Multiagent Systems.", "citeRegEx": "Knox and Stone,? 2012", "shortCiteRegEx": "Knox and Stone", "year": 2012}, {"title": "Reinforcement learning in robotics: a survey", "author": ["J. Kober", "D. Bagnell", "J. Peters"], "venue": "Inter. Journal of Robotics Research, 32(11):12361272.", "citeRegEx": "Kober et al\\.,? 2013", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Grounding verbs of motion in natural language commands to robots", "author": ["T. Kollar", "S. Tellex", "D. Roy", "N. Roy"], "venue": "Inter. Symposium on Experimental Robotics (ISER), New Delhi, India.", "citeRegEx": "Kollar et al\\.,? 2010", "shortCiteRegEx": "Kollar et al\\.", "year": 2010}, {"title": "Near-bayesian exploration in polynomial time", "author": ["J. Kolter", "A. Ng"], "venue": "26th Annual Inter. Conf. on Machine Learning, pages 513\u2013520.", "citeRegEx": "Kolter and Ng,? 2009", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Sensorimotor abstraction selection for efficient, autonomous robot skill acquisition", "author": ["G. Konidaris", "A. Barto"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201908).", "citeRegEx": "Konidaris and Barto,? 2008", "shortCiteRegEx": "Konidaris and Barto", "year": 2008}, {"title": "Instructing a reinforcement learner", "author": ["V.N. Korupolu", "P.", "M. Sivamurugan", "B. Ravindran"], "venue": "TwentyFifth Inter. FLAIRS Conf.", "citeRegEx": "Korupolu et al\\.,? 2012", "shortCiteRegEx": "Korupolu et al\\.", "year": 2012}, {"title": "A robot that learns to communicate with human caregivers", "author": ["H. Kozima", "H. Yano"], "venue": "First Inter. Workshop on Epigenetic Robotics, pages 47\u201352.", "citeRegEx": "Kozima and Yano,? 2001", "shortCiteRegEx": "Kozima and Yano", "year": 2001}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "Uncertainty in AI.", "citeRegEx": "Krause and Guestrin,? 2005", "shortCiteRegEx": "Krause and Guestrin", "year": 2005}, {"title": "Nonmyopic active learning of gaussian processes: an explorationexploitation approach", "author": ["A. Krause", "C. Guestrin"], "venue": "24th Inter. Conf. on Machine learning.", "citeRegEx": "Krause and Guestrin,? 2007", "shortCiteRegEx": "Krause and Guestrin", "year": 2007}, {"title": "Nearoptimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research, 9:235\u2013284.", "citeRegEx": "Krause et al\\.,? 2008", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Interactive learning of world model information for a service robot", "author": ["S. Kristensen", "V. Hansen", "S. Horstmann", "J. Klandt", "K. Kondak", "F. Lohnert", "A. Stopp"], "venue": "Sensor Based Intelligent Robots, pages 49\u201367. 33", "citeRegEx": "Kristensen et al\\.,? 1999", "shortCiteRegEx": "Kristensen et al\\.", "year": 1999}, {"title": "Active learning using mean shift optimization for robot grasping", "author": ["O. Kroemer", "R. Detry", "J. Piater", "J. Peters"], "venue": "Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ Inter. Conf. on, pages 2610\u2013 2615.", "citeRegEx": "Kroemer et al\\.,? 2009", "shortCiteRegEx": "Kroemer et al\\.", "year": 2009}, {"title": "Combining active learning and reactive control for robot grasping", "author": ["O. Kroemer", "R. Detry", "J. Piater", "J. Peters"], "venue": "Robotics and Autonomous Systems, 58(9):1105\u20131116.", "citeRegEx": "Kroemer et al\\.,? 2010", "shortCiteRegEx": "Kroemer et al\\.", "year": 2010}, {"title": "Active learning for teaching a robot grounded relational symbols", "author": ["J. Kulick", "M. Toussaint", "T. Lang", "M. Lopes"], "venue": "Inter. Joint Conference on Artificial Intelligence (IJCAI\u201913), Beijing, China.", "citeRegEx": "Kulick et al\\.,? 2013", "shortCiteRegEx": "Kulick et al\\.", "year": 2013}, {"title": "Exploration in relational worlds", "author": ["T. Lang", "M. Toussaint", "K. Kersting"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 178\u2013194.", "citeRegEx": "Lang et al\\.,? 2010", "shortCiteRegEx": "Lang et al\\.", "year": 2010}, {"title": "Maturational constraints for motor learning in high-dimensions: the case of biped walking", "author": ["M. Lapeyre", "O. Ly", "P. Oudeyer"], "venue": "Inter. Conf. on Humanoid Robots (Humanoids\u201911), pages 707\u2013714.", "citeRegEx": "Lapeyre et al\\.,? 2011", "shortCiteRegEx": "Lapeyre et al\\.", "year": 2011}, {"title": "Mobile robot programming using natural language", "author": ["S. Lauria", "G. Bugmann", "T. Kyriacou", "E. Klein"], "venue": "Robotics and Autonomous Systems, 38(34):171\u2013181.", "citeRegEx": "Lauria et al\\.,? 2002", "shortCiteRegEx": "Lauria et al\\.", "year": 2002}, {"title": "Online, interactive learning of gestures for human/robot interfaces", "author": ["C. Lee", "Y. Xu"], "venue": "Robotics and Automation, 1996. Proceedings., 1996 IEEE Inter. Conf. on, volume 4, pages 2982\u20132987.", "citeRegEx": "Lee and Xu,? 1996", "shortCiteRegEx": "Lee and Xu", "year": 1996}, {"title": "Staged competence learning in developmental robotics", "author": ["M. Lee", "Q. Meng", "F. Chao"], "venue": "Adaptive Behavior, 15(3):241\u2013255.", "citeRegEx": "Lee et al\\.,? 2007", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Abandoning objectives: Evolution through the search for novelty alone", "author": ["J. Lehman", "K. Stanley"], "venue": "Evolutionary Computation, 19(2):189\u2013223.", "citeRegEx": "Lehman and Stanley,? 2011", "shortCiteRegEx": "Lehman and Stanley", "year": 2011}, {"title": "A sequential algorithm for training text classifiers", "author": ["D. Lewis", "W. Gale"], "venue": "17th annual Inter. ACM SIGIR Conf. on Research and development in information retrieval, pages 3\u201312. Springer-Verlag New York, Inc.", "citeRegEx": "Lewis and Gale,? 1994", "shortCiteRegEx": "Lewis and Gale", "year": 1994}, {"title": "Autonomous exploration for navigating in mdps", "author": ["S. Lim", "P. Auer"], "venue": "JMLR.", "citeRegEx": "Lim and Auer,? 2012", "shortCiteRegEx": "Lim and Auer", "year": 2012}, {"title": "Facilitating active learning with inexpensive mobile robots", "author": ["S. Linder", "B. Nestrick", "S. Mulders", "C. Lavelle"], "venue": "Journal of Computing Sciences in Colleges, 16(4):21\u201333.", "citeRegEx": "Linder et al\\.,? 2001", "shortCiteRegEx": "Linder et al\\.", "year": 2001}, {"title": "Tutelage and socially guided robot learning", "author": ["A. Lockerd", "C. Breazeal"], "venue": "Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ Inter. Conf. on, volume 4, pages 3475\u2013 3480.", "citeRegEx": "Lockerd and Breazeal,? 2004", "shortCiteRegEx": "Lockerd and Breazeal", "year": 2004}, {"title": "Simultaneous acquisition of task and feedback models", "author": ["M. Lopes", "T. Cederborg", "Oudeyer", "P.-Y."], "venue": "IEEE - International Conference on Development and Learning (ICDL\u201911), Frankfurt, Germany.", "citeRegEx": "Lopes et al\\.,? 2011", "shortCiteRegEx": "Lopes et al\\.", "year": 2011}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["M. Lopes", "T. Lang", "M. Toussaint", "P.Y. Oudeyer"], "venue": "Neural Information Processing Systems (NIPS\u201912), Tahoe, USA.", "citeRegEx": "Lopes et al\\.,? 2012", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "A computational model of social-learning mechanisms", "author": ["M. Lopes", "F. Melo", "B. Kenward", "J. Santos-Victor"], "venue": "Adaptive Behavior, 467(17).", "citeRegEx": "Lopes et al\\.,? 2009a", "shortCiteRegEx": "Lopes et al\\.", "year": 2009}, {"title": "Abstraction levels for robotic imitation: Overview and computational approaches", "author": ["M. Lopes", "F. Melo", "L. Montesano", "J. Santos-Victor"], "venue": "Sigaud, O. and Peters, J., editors, From Motor to Interaction Learning in Robots, volume 264 of Studies in Compu-", "citeRegEx": "Lopes et al\\.,? 2010", "shortCiteRegEx": "Lopes et al\\.", "year": 2010}, {"title": "Affordance-based imitation learning in robots", "author": ["M. Lopes", "F.S. Melo", "L. Montesano"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\u201907), USA.", "citeRegEx": "Lopes et al\\.,? 2007", "shortCiteRegEx": "Lopes et al\\.", "year": 2007}, {"title": "Active learning for reward estimation in inverse reinforcement learning", "author": ["M. Lopes", "F.S. Melo", "L. Montesano"], "venue": "Machine Learning and Knowledge Discovery in Databases (ECML/PKDD\u201909).", "citeRegEx": "Lopes et al\\.,? 2009b", "shortCiteRegEx": "Lopes et al\\.", "year": 2009}, {"title": "The strategic student approach for life-long exploration and learning", "author": ["M. Lopes", "Oudeyer", "P.-Y."], "venue": "IEEE International Conference on Development and Learning (ICDL), San Diego, USA.", "citeRegEx": "Lopes et al\\.,? 2012", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "A developmental roadmap for learning by imitation in robots", "author": ["M. Lopes", "J. Santos-Victor"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics, 37(2).", "citeRegEx": "Lopes and Santos.Victor,? 2007", "shortCiteRegEx": "Lopes and Santos.Victor", "year": 2007}, {"title": "Artificial curiosity with planning for autonomous perceptual and cognitive development", "author": ["M. Luciw", "V. Graziano", "M. Ring", "J. Schmidhuber"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Luciw et al\\.,? 2011", "shortCiteRegEx": "Luciw et al\\.", "year": 2011}, {"title": "Developmental robotics: a survey", "author": ["M. Lungarella", "G. Metta", "R. Pfeifer", "G. Sandini"], "venue": "Connection Science, 15(40):151\u2013190.", "citeRegEx": "Lungarella et al\\.,? 2003", "shortCiteRegEx": "Lungarella et al\\.", "year": 2003}, {"title": "The curious robot-structuring interactive robot learning", "author": ["I. Lutkebohle", "J. Peltason", "L. Schillingmann", "B. Wrede", "S. Wachsmuth", "C. Elbrechter", "R. Haschke"], "venue": "Robotics and Automation, 2009. ICRA\u201909. IEEE Inter. Conf. on, pages 4156\u20134162.", "citeRegEx": "Lutkebohle et al\\.,? 2009", "shortCiteRegEx": "Lutkebohle et al\\.", "year": 2009}, {"title": "Information-based objective functions for active data selection", "author": ["D. MacKay"], "venue": "Neural computation, 4(4):590\u2013604.", "citeRegEx": "MacKay,? 1992", "shortCiteRegEx": "MacKay", "year": 1992}, {"title": "Hierarchical optimistic region selection driven by curiosity", "author": ["O. Maillard"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Maillard,? 2012", "shortCiteRegEx": "Maillard", "year": 2012}, {"title": "Selecting the state-representation in reinforcement learning", "author": ["O.A. Maillard", "R. Munos", "D. Ryabko"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Maillard et al\\.,? 2011", "shortCiteRegEx": "Maillard et al\\.", "year": 2011}, {"title": "Dynamic abstraction in reinforcement learning via clustering", "author": ["S. Mannor", "I. Menache", "A. Hoze", "U. Klein"], "venue": "Inter. Conf. on Machine Learning, page 71.", "citeRegEx": "Mannor et al\\.,? 2004", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Extraction of reward-related feature space using correlation-based and reward-based learning methods", "author": ["P. Manoonpong", "F. W\u00f6rg\u00f6tter", "J. Morimoto"], "venue": "17th Inter. Conf. on Neural information processing: theory and algorithms - Volume Part I, ICONIP\u201910,", "citeRegEx": "Manoonpong et al\\.,? 2010", "shortCiteRegEx": "Manoonpong et al\\.", "year": 2010}, {"title": "Bayesian optimisation for intelligent environmental monitoring", "author": ["R. Marchant", "F. Ramos"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ Inter. Conf. on, pages 2242\u20132249.", "citeRegEx": "Marchant and Ramos,? 2012", "shortCiteRegEx": "Marchant and Ramos", "year": 2012}, {"title": "A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot", "author": ["R. Martinez-Cantin", "N. de Freitas", "E. Brochu", "J. Castellanos", "A. Doucet"], "venue": "Autonomous Robots - Special Issue on Robot", "citeRegEx": "Martinez.Cantin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martinez.Cantin et al\\.", "year": 2009}, {"title": "Active policy learning for robot planning and exploration under uncertainty", "author": ["R. Martinez-Cantin", "N. de Freitas", "A. Doucet", "J. Castellanos"], "venue": "In Robotics: Science and Systems (RSS)", "citeRegEx": "Martinez.Cantin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Martinez.Cantin et al\\.", "year": 2007}, {"title": "Body schema acquisition through active learning", "author": ["R. Martinez-Cantin", "M. Lopes", "L. Montesano"], "venue": "IEEE International Conference on Robotics and Automation (ICRA\u201910), Alaska, USA.", "citeRegEx": "Martinez.Cantin et al\\.,? 2010", "shortCiteRegEx": "Martinez.Cantin et al\\.", "year": 2010}, {"title": "Robot self-initiative and personalization by learning through repeated interactions", "author": ["M. Mason", "M. Lopes"], "venue": "6th ACM/IEEE International Conference on Human-Robot Interaction (HRI\u201911).", "citeRegEx": "Mason and Lopes,? 2011", "shortCiteRegEx": "Mason and Lopes", "year": 2011}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["A. McGovern", "A.G. Barto"], "venue": "Inter. Conf. on Machine Learning (ICML\u201901), San Francisco, CA, USA.", "citeRegEx": "McGovern and Barto,? 2001", "shortCiteRegEx": "McGovern and Barto", "year": 2001}, {"title": "Curious george: An attentive semantic robot", "author": ["D. Meger", "P. Forss\u00e9n", "K. Lai", "S. Helmer", "S. McCann", "T. Southey", "M. Baumann", "J. Little", "D. Lowe"], "venue": "Robotics and Autonomous Systems, 56(6):503\u2013511.", "citeRegEx": "Meger et al\\.,? 2008", "shortCiteRegEx": "Meger et al\\.", "year": 2008}, {"title": "A unified framework for imitation-like behaviors", "author": ["F. Melo", "M. Lopes", "J. Santos-Victor", "M.I. Ribeiro"], "venue": "4th International Symposium on Imitation in Animals and Artifacts, Newcastle, UK.", "citeRegEx": "Melo et al\\.,? 2007", "shortCiteRegEx": "Melo et al\\.", "year": 2007}, {"title": "Learning from demonstration using mdp induced metrics", "author": ["F.S. Melo", "M. Lopes"], "venue": "Machine learning and knowledge discovery in databases (ECML/PKDD\u201910).", "citeRegEx": "Melo and Lopes,? 2010", "shortCiteRegEx": "Melo and Lopes", "year": 2010}, {"title": "Multi-class generalized binary search for active inverse reinforcement learning", "author": ["F.S. Melo", "M. Lopes"], "venue": "submitted to Machine Learning.", "citeRegEx": "Melo and Lopes,? 2013", "shortCiteRegEx": "Melo and Lopes", "year": 2013}, {"title": "Qcutdynamic discovery of sub-goals in reinforcement learning", "author": ["I. Menache", "S. Mannor", "N. Shimkin"], "venue": "Machine Learning: ECML 2002, pages 187\u2013 195.", "citeRegEx": "Menache et al\\.,? 2002", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Error-driven active learning in growing radial basis function networks for early robot learning", "author": ["Q. Meng", "M. Lee"], "venue": "Neurocomputing, 71(7):1449\u20131461.", "citeRegEx": "Meng and Lee,? 2008", "shortCiteRegEx": "Meng and Lee", "year": 2008}, {"title": "Autonomous development of a grounded object ontology by a learning robot", "author": ["J. Modayil", "B. Kuipers"], "venue": "National Conf. on Artificial Intelligence (AAAI).", "citeRegEx": "Modayil and Kuipers,? 2007", "shortCiteRegEx": "Modayil and Kuipers", "year": 2007}, {"title": "Learning interaction protocols using Augmented Bayesian Networks applied to guided navigation", "author": ["Y. Mohammad", "T. Nishida"], "venue": "Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ Inter. Conf. on, pages 4119\u20134126.", "citeRegEx": "Mohammad and Nishida,? 2010", "shortCiteRegEx": "Mohammad and Nishida", "year": 2010}, {"title": "Safe exploration in markov decision processes", "author": ["T.M. Moldovan", "P. Abbeel"], "venue": "CoRR, abs/1205.4810.", "citeRegEx": "Moldovan and Abbeel,? 2012", "shortCiteRegEx": "Moldovan and Abbeel", "year": 2012}, {"title": "Learning grasping affordances from local visual descriptors", "author": ["L. Montesano", "M. Lopes"], "venue": "IEEE International Conference on Development and Learning (ICDL\u201909), China.", "citeRegEx": "Montesano and Lopes,? 2009", "shortCiteRegEx": "Montesano and Lopes", "year": 2009}, {"title": "Active learning of visual descriptors for grasping using nonparametric smoothed beta distributions", "author": ["L. Montesano", "M. Lopes"], "venue": "Robotics and Autonomous Systems, 60(3):452\u2013462.", "citeRegEx": "Montesano and Lopes,? 2012", "shortCiteRegEx": "Montesano and Lopes", "year": 2012}, {"title": "An active learning approach for assessing robot grasp reliability", "author": ["A. Morales", "E. Chinellato", "A. Fagg", "A. del Pobil"], "venue": "In IEEE/RSJ Inter. Conf. on Intelligent Robots and Systems (IROS", "citeRegEx": "Morales et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Morales et al\\.", "year": 2004}, {"title": "Improving the detection of unknown computer worms activity using active learning", "author": ["R. Moskovitch", "N. Nissim", "D. Stopel", "C. Feher", "R. Englert", "Y. Elovici"], "venue": "KI 2007: Advances in Artificial Intelligence, pages 489\u2013493. Springer.", "citeRegEx": "Moskovitch et al\\.,? 2007", "shortCiteRegEx": "Moskovitch et al\\.", "year": 2007}, {"title": "Encouraging behavioral diversity in evolutionary robotics: an empirical study", "author": ["J. Mouret", "S. Doncieux"], "venue": "Evolutionary Computation.", "citeRegEx": "Mouret and Doncieux,? 2011", "shortCiteRegEx": "Mouret and Doncieux", "year": 2011}, {"title": "Learning for joint attention helped by functional development", "author": ["Y. Nagai", "M. Asada", "K. Hosoda"], "venue": "Advanced Robotics, 20(10):1165\u20131181.", "citeRegEx": "Nagai et al\\.,? 2006", "shortCiteRegEx": "Nagai et al\\.", "year": 2006}, {"title": "Computational analysis of motionese toward scaffolding robot action learning", "author": ["Y. Nagai", "K. Rohlfing"], "venue": "Autonomous Mental Development, IEEE Transactions on, 1(1):44\u201354.", "citeRegEx": "Nagai and Rohlfing,? 2009", "shortCiteRegEx": "Nagai and Rohlfing", "year": 2009}, {"title": "Nine billion correspondence problems", "author": ["C.L. Nehaniv"], "venue": "Cambridge University Press.", "citeRegEx": "Nehaniv,? 2007", "shortCiteRegEx": "Nehaniv", "year": 2007}, {"title": "An analysis of approximations for maximizing submodular set functions", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming, 14(1):265\u2013294.", "citeRegEx": "Nemhauser et al\\.,? 1978", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "Inter. Conf. on Machine Learning.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Interactive learning gives the tempo to an intrinsically motivated robot learner", "author": ["M. Nguyen", "Oudeyer", "P.-Y."], "venue": "IEEE-RAS Inter. Conf. on Humanoid Robots.", "citeRegEx": "Nguyen et al\\.,? 2012", "shortCiteRegEx": "Nguyen et al\\.", "year": 2012}, {"title": "Bootstrapping intrinsically motivated learning with human demonstration", "author": ["S. Nguyen", "A. Baranes", "P. Oudeyer"], "venue": "Inter. Conf. on Development and Learning (ICDL\u201911).", "citeRegEx": "Nguyen et al\\.,? 2011", "shortCiteRegEx": "Nguyen et al\\.", "year": 2011}, {"title": "Model learning for robot control: a survey", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": "Cognitive Processing, 12(4):319\u2013340.", "citeRegEx": "Nguyen.Tuong and Peters,? 2011", "shortCiteRegEx": "Nguyen.Tuong and Peters", "year": 2011}, {"title": "Learning and interacting in human-robot domains", "author": ["M. Nicolescu", "M. Mataric"], "venue": "Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 31(5):419\u2013430.", "citeRegEx": "Nicolescu and Mataric,? 2001", "shortCiteRegEx": "Nicolescu and Mataric", "year": 2001}, {"title": "Natural methods for robot task learning: Instructive demonstrations, generalization and practice", "author": ["M. Nicolescu", "M. Mataric"], "venue": "second Inter. joint Conf. on Autonomous agents and multiagent systems, pages 241\u2013248.", "citeRegEx": "Nicolescu and Mataric,? 2003", "shortCiteRegEx": "Nicolescu and Mataric", "year": 2003}, {"title": "Social learning mechanisms compared in a simple environment", "author": ["J. Noble", "D.W. Franks"], "venue": "Artificial Life VIII: Eighth Inter. Conf.on the Simulation and Synthesis of Living Systems, pages 379\u2013385. MIT Press.", "citeRegEx": "Noble and Franks,? 2002", "shortCiteRegEx": "Noble and Franks", "year": 2002}, {"title": "Dimension reduction and its application to model-based exploration in continuous spaces", "author": ["A. Nouri", "M. Littman"], "venue": "Machine learning, 81(1):85\u201398.", "citeRegEx": "Nouri and Littman,? 2010", "shortCiteRegEx": "Nouri and Littman", "year": 2010}, {"title": "The geometry of generalized binary search", "author": ["R. Nowak"], "venue": "Information Theory, Transactions on, 57(12):7893\u20137906.", "citeRegEx": "Nowak,? 2011", "shortCiteRegEx": "Nowak", "year": 2011}, {"title": "Interactive learning in human-robot collaboration", "author": ["T. Ogata", "N. Masago", "S. Sugano", "J. Tani"], "venue": "Intelligent Robots and Systems, 2003.(IROS 2003). Proceedings. 2003 IEEE/RSJ Inter. Conf. on, volume 1, pages 162\u2013167.", "citeRegEx": "Ogata et al\\.,? 2003", "shortCiteRegEx": "Ogata et al\\.", "year": 2003}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["P.A.R. Ortner"], "venue": "Advances in Neural Information Processing Systems (NIPS).", "citeRegEx": "Ortner,? 2007", "shortCiteRegEx": "Ortner", "year": 2007}, {"title": "What is intrinsic motivation? a typology of computational approaches", "author": ["P. Oudeyer", "F. Kaplan"], "venue": "Frontiers in Neurorobotics, 1.", "citeRegEx": "Oudeyer and Kaplan,? 2007", "shortCiteRegEx": "Oudeyer and Kaplan", "year": 2007}, {"title": "Developmental Robotics", "author": ["Oudeyer", "P.-Y."], "venue": "Seel, N., editor, Encyclopedia of the Sciences of Learning, Springer Reference Series. Springer.", "citeRegEx": "Oudeyer and P..Y.,? 2011", "shortCiteRegEx": "Oudeyer and P..Y.", "year": 2011}, {"title": "Intrinsically motivated learning of real world sensorimotor skills with developmental constraints", "author": ["Oudeyer", "P.-Y.", "A. Baranes", "F. Kaplan"], "venue": "Baldassarre, G. and Mirolli, M., editors, Intrinsically Motivated Learning in Natural and Artificial Systems. Springer.", "citeRegEx": "Oudeyer et al\\.,? 2013", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2013}, {"title": "Intrinsic motivation systems for autonomous mental development", "author": ["Oudeyer", "P.-Y.", "F. Kaplan", "V. Hafner"], "venue": "IEEE Transactions on Evolutionary Computation, 11(2):265\u2013286.", "citeRegEx": "Oudeyer et al\\.,? 2007", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2007}, {"title": "The playground experiment: Taskindependent development of a curious robot", "author": ["Oudeyer", "P.-Y.", "F. Kaplan", "V. Hafner", "A. Whyte"], "venue": "AAAI Spring Symposium on Developmental Robotics, pages 42\u201347.", "citeRegEx": "Oudeyer et al\\.,? 2005", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2005}, {"title": "Natural Actor-Critic", "author": ["J. Peters", "S. Vijayakumar", "S. Schaal"], "venue": "Proc. 16th European Conf. Machine Learning, pages 280\u2013291.", "citeRegEx": "Peters et al\\.,? 2005", "shortCiteRegEx": "Peters et al\\.", "year": 2005}, {"title": "Policyblocks: An algorithm for creating useful macro-actions in reinforcement learning", "author": ["M. Pickett", "A. Barto"], "venue": "MACHINE LEARNING-Inter. WORKSHOP THEN Conf.-, pages 506\u2013513.", "citeRegEx": "Pickett and Barto,? 2002", "shortCiteRegEx": "Pickett and Barto", "year": 2002}, {"title": "Learning to explore and build maps", "author": ["D. Pierce", "B. Kuipers"], "venue": "National Conf. on Artificial Intelligence, pages 1264\u20131264.", "citeRegEx": "Pierce and Kuipers,? 1995", "shortCiteRegEx": "Pierce and Kuipers", "year": 1995}, {"title": "Neural network perception for mobile robot guidance", "author": ["D. Pomerleau"], "venue": "Technical report, DTIC Document.", "citeRegEx": "Pomerleau,? 1992", "shortCiteRegEx": "Pomerleau", "year": 1992}, {"title": "An analytic solution to discrete bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Inter. Conf. on Machine learning, pages 697\u2013704.", "citeRegEx": "Poupart et al\\.,? 2006", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Accelerating reinforcement learning through implicit imitation", "author": ["B. Price", "C. Boutilier"], "venue": "J. Artificial Intelligence Research, 19:569\u2013629.", "citeRegEx": "Price and Boutilier,? 2003", "shortCiteRegEx": "Price and Boutilier", "year": 2003}, {"title": "Two-dimensional active learning for image classification", "author": ["G. Qi", "X. Hua", "Y. Rui", "J. Tang", "H. Zhang"], "venue": "Computer Vision and Pattern Recognition (CVPR\u201908).", "citeRegEx": "Qi et al\\.,? 2008", "shortCiteRegEx": "Qi et al\\.", "year": 2008}, {"title": "Eliciting additive reward functions for markov decision processes", "author": ["K. Regan", "C. Boutilier"], "venue": "Inter. Joint Conf. on Artificial Intelligence (IJCAI\u201911), Barcelona, Spain.", "citeRegEx": "Regan and Boutilier,? 2011", "shortCiteRegEx": "Regan and Boutilier", "year": 2011}, {"title": "Multi-task active learning for linguistic annotations", "author": ["R. Reichart", "K. Tomanek", "U. Hahn", "A. Rappoport"], "venue": "ACL08.", "citeRegEx": "Reichart et al\\.,? 2008", "shortCiteRegEx": "Reichart et al\\.", "year": 2008}, {"title": "Online goal babbling for rapid bootstrapping of inverse models in high dimensions", "author": ["M. Rolf", "J. Steil", "M. Gienger"], "venue": "Development and Learning (ICDL), 2011 IEEE Inter. Conf. on.", "citeRegEx": "Rolf et al\\.,? 2011", "shortCiteRegEx": "Rolf et al\\.", "year": 2011}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "J.A.D. Bagnell"], "venue": "13th Inter. Conf. on Artificial Intelligence and Statistics (AISTATS).", "citeRegEx": "Ross and Bagnell,? 2010", "shortCiteRegEx": "Ross and Bagnell", "year": 2010}, {"title": "Learning independent causes in natural images explains the spacevariant oblique effect", "author": ["C.A. Rothkopf", "T.H. Weisswange", "J. Triesch"], "venue": "Development and Learning, 2009. ICDL 2009. IEEE 8th Inter. Conf. on, pages 1\u20136.", "citeRegEx": "Rothkopf et al\\.,? 2009", "shortCiteRegEx": "Rothkopf et al\\.", "year": 2009}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "ICML, Williamstown.", "citeRegEx": "Roy and McCallum,? 2001", "shortCiteRegEx": "Roy and McCallum", "year": 2001}, {"title": "Evolving predictive visual motion detectors", "author": ["J. Ruesch", "A. Bernardino"], "venue": "Development and Learning, 2009. ICDL 2009. IEEE 8th Inter. Conf. on, pages 1\u2013", "citeRegEx": "Ruesch and Bernardino,? 2009", "shortCiteRegEx": "Ruesch and Bernardino", "year": 2009}, {"title": "Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices", "author": ["M. Salganicoff", "L. Ungar"], "venue": "MACHINE LEARNINGInter. WORKSHOP THEN Conf.-, pages 480\u2013487.", "citeRegEx": "Salganicoff and Ungar,? 1995", "shortCiteRegEx": "Salganicoff and Ungar", "year": 1995}, {"title": "Active learning for vision-based robot grasping", "author": ["M. Salganicoff", "L.H. Ungar", "R. Bajcsy"], "venue": "Machine Learning, 23(2).", "citeRegEx": "Salganicoff et al\\.,? 1996", "shortCiteRegEx": "Salganicoff et al\\.", "year": 1996}, {"title": "Iterative learning of grasp adaptation through human corrections", "author": ["E. Sauser", "B. Argall", "G. Metta", "A. Billard"], "venue": "Robotics and Autonomous Systems.", "citeRegEx": "Sauser et al\\.,? 2011", "shortCiteRegEx": "Sauser et al\\.", "year": 2011}, {"title": "Robotic grasping of novel objects", "author": ["A. Saxena", "J. Driemeyer", "J. Kearns", "A.Y. Ng"], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Saxena et al\\.,? 2006", "shortCiteRegEx": "Saxena et al\\.", "year": 2006}, {"title": "Learning motor dependent crutchfield\u2019s information distance to anticipate changes in the topology of sensory body maps", "author": ["T. Schatz", "Oudeyer", "P.-Y."], "venue": "Development and Learning, 2009. ICDL 2009. IEEE 8th Inter. Conf. on, pages 1\u20136.", "citeRegEx": "Schatz et al\\.,? 2009", "shortCiteRegEx": "Schatz et al\\.", "year": 2009}, {"title": "Active learning for logistic regression: an evaluation", "author": ["A. Schein", "L.H. Ungar"], "venue": "Machine Learning, 68:235\u2013265.", "citeRegEx": "Schein and Ungar,? 2007", "shortCiteRegEx": "Schein and Ungar", "year": 2007}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "Inter. Joint Conf. on Neural Networks, pages 1458\u20131463.", "citeRegEx": "Schmidhuber,? 1991a", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "From Animals to Animats: First Inter. Conf. on Simulation of Adaptive Behavior, pages 222 \u2013 227, Cambridge, MA, USA.", "citeRegEx": "Schmidhuber,? 1991b", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "On learning how to learn learning strategies", "author": ["J. Schmidhuber"], "venue": "Technical Report FKI-198-94, Fakultaet fuer Informatik, Technische Universitaet Muenchen.", "citeRegEx": "Schmidhuber,? 1995", "shortCiteRegEx": "Schmidhuber", "year": 1995}, {"title": "Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts", "author": ["J. Schmidhuber"], "venue": "Connection Science, 18(2):173 \u2013 187.", "citeRegEx": "Schmidhuber,? 2006", "shortCiteRegEx": "Schmidhuber", "year": 2006}, {"title": "Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem", "author": ["J. Schmidhuber"], "venue": "Technical report, http://arxiv.org/abs/1112.5309.", "citeRegEx": "Schmidhuber,? 2011", "shortCiteRegEx": "Schmidhuber", "year": 2011}, {"title": "Reinforcement learning with self-modifying policies", "author": ["J. Schmidhuber", "J. Zhao", "N. Schraudolph"], "venue": "Learning to learn, 293:309.", "citeRegEx": "Schmidhuber et al\\.,? 1997", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 1997}, {"title": "Global versus local search in constrained optimization of computer models", "author": ["M. Schonlau", "W. Welch", "D. Jones"], "venue": "Flournoy, N., Rosenberger, W., and Wong, W., editors, New Developments and Applications in Experimental Design, volume 34, pages 11\u201325.", "citeRegEx": "Schonlau et al\\.,? 1998", "shortCiteRegEx": "Schonlau et al\\.", "year": 1998}, {"title": "Emerging social awareness: Exploring intrinsic motivation in multiagent learning", "author": ["P. Sequeira", "F. Melo", "R. Prada", "A. Paiva"], "venue": "IEEE Inter. Conf. on Developmental Learning.", "citeRegEx": "Sequeira et al\\.,? 2011", "shortCiteRegEx": "Sequeira et al\\.", "year": 2011}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical Report CS Tech. Rep. 1648, University of Wisconsin-Madison.", "citeRegEx": "Settles,? 2009", "shortCiteRegEx": "Settles", "year": 2009}, {"title": "Multipleinstance active learning", "author": ["B. Settles", "M. Craven", "S. Ray"], "venue": "Advances in neural information processing systems, pages 1289\u20131296.", "citeRegEx": "Settles et al\\.,? 2007", "shortCiteRegEx": "Settles et al\\.", "year": 2007}, {"title": "Query by committee", "author": ["H. Seung", "M. Opper", "H. Sompolinsky"], "venue": "fifth annual workshop on Computational learning theory, pages 287\u2013294.", "citeRegEx": "Seung et al\\.,? 1992", "shortCiteRegEx": "Seung et al\\.", "year": 1992}, {"title": "Active imitation learning", "author": ["A.P. Shon", "D. Verma", "R.P.N. Rao"], "venue": "AAAI Conf. on Artificial Intelligence (AAAI\u201907).", "citeRegEx": "Shon et al\\.,? 2007", "shortCiteRegEx": "Shon et al\\.", "year": 2007}, {"title": "Global a-optimal robot exploration in slam", "author": ["R. Sim", "N. Roy"], "venue": "IEEE Inter. Conf. on Robotics and Automation (ICRA).", "citeRegEx": "Sim and Roy,? 2005", "shortCiteRegEx": "Sim and Roy", "year": 2005}, {"title": "An intrinsic reward mechanism for efficient exploration", "author": ["\u00d6. \u015eim\u015fek", "A. Barto"], "venue": "Inter. Conf. on Machine learning, pages 833\u2013840.", "citeRegEx": "\u015eim\u015fek and Barto,? 2006", "shortCiteRegEx": "\u015eim\u015fek and Barto", "year": 2006}, {"title": "Skill characterization based on betweenness", "author": ["O. Simsek", "A. Barto"], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Simsek and Barto,? 2008", "shortCiteRegEx": "Simsek and Barto", "year": 2008}, {"title": "Identifying useful subgoals in reinforcement learning by local graph partitioning", "author": ["\u00d6. \u015eim\u015fek", "A. Wolfe", "A. Barto"], "venue": "Inter. Conf. on Machine learning, pages 816\u2013823.", "citeRegEx": "\u015eim\u015fek et al\\.,? 2005", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "Efficient planning of informative paths for multiple robots", "author": ["A. Singh", "A. Krause", "C. Guestrin", "W. Kaiser", "M. Batalin"], "venue": "Proc. of the Int. Joint Conf. on Artificial Intelligence.", "citeRegEx": "Singh et al\\.,? 2007", "shortCiteRegEx": "Singh et al\\.", "year": 2007}, {"title": "Intrinsically motivated reinforcement learning", "author": ["S. Singh", "A. Barto", "N. Chentanez"], "venue": "Advances in neural information processing systems (NIPS), volume 17, pages 1281\u20131288.", "citeRegEx": "Singh et al\\.,? 2005", "shortCiteRegEx": "Singh et al\\.", "year": 2005}, {"title": "Where do rewards come from? In Annual Conf", "author": ["S. Singh", "R. Lewis", "A. Barto"], "venue": "of the Cognitive Science Society.", "citeRegEx": "Singh et al\\.,? 2009", "shortCiteRegEx": "Singh et al\\.", "year": 2009}, {"title": "On Separating Agent Designer Goals from Agent Goals: Breaking the Preferences\u2013Parameters Confound", "author": ["S. Singh", "R. Lewis", "J. Sorg", "A. Barto", "A. Helou"], "venue": "Citeseer.", "citeRegEx": "Singh et al\\.,? 2010a", "shortCiteRegEx": "Singh et al\\.", "year": 2010}, {"title": "Intrinsically motivated reinforcement learning: an evolutionary perspective", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto", "J. Sorg"], "venue": "IEEE Transactions on Autonomous Mental Development, 2(2).", "citeRegEx": "Singh et al\\.,? 2010b", "shortCiteRegEx": "Singh et al\\.", "year": 2010}, {"title": "A general activelearning framework for on-road vehicle recognition and tracking", "author": ["S. Sivaraman", "M. Trivedi"], "venue": "Intelligent Transportation Systems, IEEE Transactions on, 11(2):267\u2013276.", "citeRegEx": "Sivaraman and Trivedi,? 2010", "shortCiteRegEx": "Sivaraman and Trivedi", "year": 2010}, {"title": "Internal rewards mitigate agent boundedness", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "Int. Conf. on Machine Learning (ICML).", "citeRegEx": "Sorg et al\\.,? 2010a", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Reward design via online gradient ascent", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "Advances of Neural Information Processing Systems, volume 23.", "citeRegEx": "Sorg et al\\.,? 2010b", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Variance-based rewards for approximate bayesian reinforcement learning", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "26th Conf. on Uncertainty in Artificial Intelligence.", "citeRegEx": "Sorg et al\\.,? 2010c", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Optimal rewards versus leaf-evaluation heuristics in planning agents", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "Twenty-Fifth AAAI Conf. on Artificial Intelligence.", "citeRegEx": "Sorg et al\\.,? 2011", "shortCiteRegEx": "Sorg et al\\.", "year": 2011}, {"title": "Exploring unknown environments with mobile robots using coverage maps", "author": ["C. Stachniss", "W. Burgard"], "venue": "AAAI Conference on Artificial Intelligence.", "citeRegEx": "Stachniss and Burgard,? 2003", "shortCiteRegEx": "Stachniss and Burgard", "year": 2003}, {"title": "Information gain-based exploration using rao-blackwellized particle filters", "author": ["C. Stachniss", "G. Grisetti", "W. Burgard"], "venue": "Robotics: Science and Systems.", "citeRegEx": "Stachniss et al\\.,? 2005", "shortCiteRegEx": "Stachniss et al\\.", "year": 2005}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A.L. Strehl", "L. Li", "M. Littman"], "venue": "J. of Machine Learning Research.", "citeRegEx": "Strehl et al\\.,? 2009", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "J. Comput. Syst. Sci., 74(8):1309\u20131331.", "citeRegEx": "Strehl and Littman,? 2008", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "Policy improvement methods: Between black-box optimization and episodic reinforcement learning", "author": ["F. Stulp", "O. Sigaud"], "venue": "ICML.", "citeRegEx": "Stulp and Sigaud,? 2012", "shortCiteRegEx": "Stulp and Sigaud", "year": 2012}, {"title": "Planning to be surprised: Optimal bayesian exploration in dynamic environments", "author": ["Y. Sun", "F. Gomez", "J. Schmidhuber"], "venue": "Artificial General Intelligence, pages 41\u201351.", "citeRegEx": "Sun et al\\.,? 2011", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "Adv. Neural Information Proc. Systems (NIPS), volume 12, pages 1057\u20131063.", "citeRegEx": "Sutton et al\\.,? 2000", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R. Sutton", "D. Precup", "S Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Reinforcement learning algorithms for mdps", "author": ["C. Szepesv\u00e1ri"], "venue": "Wiley Encyclopedia of Operations Research and Management Science.", "citeRegEx": "Szepesv\u00e1ri,? 2011", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2011}, {"title": "Bounding performance loss in approximate mdp homomorphisms", "author": ["J. Taylor", "D. Precup", "P. Panagaden"], "venue": "Advances in Neural Information Processing Systems, pages 1649\u20131656.", "citeRegEx": "Taylor et al\\.,? 2008", "shortCiteRegEx": "Taylor et al\\.", "year": 2008}, {"title": "Expensive function optimization with stochastic binary outcomes", "author": ["M. Tesch", "J. Schneider", "H. Choset"], "venue": "Inter. Conf. on Machine Learning (ICML\u201913).", "citeRegEx": "Tesch et al\\.,? 2013", "shortCiteRegEx": "Tesch et al\\.", "year": 2013}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A. Thomaz", "C. Breazeal"], "venue": "Artificial Intelligence, 172(67):716\u2013737.", "citeRegEx": "Thomaz and Breazeal,? 2008", "shortCiteRegEx": "Thomaz and Breazeal", "year": 2008}, {"title": "Learning about objects with human teachers", "author": ["A. Thomaz", "M. Cakmak"], "venue": "ACM/IEEE Inter. Conf. on Human robot interaction, pages 15\u201322.", "citeRegEx": "Thomaz and Cakmak,? 2009", "shortCiteRegEx": "Thomaz and Cakmak", "year": 2009}, {"title": "Reinforcement learning with human teachers: Understanding how people want to teach robots", "author": ["A. Thomaz", "G. Hoffman", "C. Breazeal"], "venue": "Robot and Human Interactive Communication, 2006. ROMAN 2006. The 15th IEEE Inter. Symposium on, pages 352\u2013357.", "citeRegEx": "Thomaz et al\\.,? 2006", "shortCiteRegEx": "Thomaz et al\\.", "year": 2006}, {"title": "Efficient exploration in reinforcement learning", "author": ["S. Thrun"], "venue": "Technical Report CMU-CS-92-102, CarnegieMellon University.", "citeRegEx": "Thrun,? 1992", "shortCiteRegEx": "Thrun", "year": 1992}, {"title": "Exploration in active learning", "author": ["S. Thrun"], "venue": "Handbook of Brain Science and Neural Networks, pages 381\u2013384.", "citeRegEx": "Thrun,? 1995", "shortCiteRegEx": "Thrun", "year": 1995}, {"title": "Finding structure in reinforcement learning", "author": ["S. Thrun", "A Schwartz"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Thrun and Schwartz,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Schwartz", "year": 1995}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research, 2:45\u201366.", "citeRegEx": "Tong and Koller,? 2001", "shortCiteRegEx": "Tong and Koller", "year": 2001}, {"title": "Theory and Principled Methods for Designing Metaheuristics, chapter The Bayesian Search Game", "author": ["M. Toussaint"], "venue": "Springer.", "citeRegEx": "Toussaint,? 2012", "shortCiteRegEx": "Toussaint", "year": 2012}, {"title": "Curiosity-driven learning of traversability affordance on a mobile robot", "author": ["E. Ugur", "M.R. Dogar", "M. Cakmak", "E. Sahin"], "venue": "Development and Learning, 2007. ICDL 2007. IEEE 6th Inter. Conf. on, pages 13\u201318.", "citeRegEx": "Ugur et al\\.,? 2007", "shortCiteRegEx": "Ugur et al\\.", "year": 2007}, {"title": "Maximally informative interaction learning for scene exploration", "author": ["H. van Hoof", "O. Kr\u00f6mer", "H. Amor", "J. Peters"], "venue": "In IEEE/RSJ Inter. Conf. on Intelligent Robots and Systems (IROS)", "citeRegEx": "Hoof et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoof et al\\.", "year": 2012}, {"title": "Optimal bayesian recommendation sets and myopically optimal choice query sets", "author": ["P. Viappiani", "C. Boutilier"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Viappiani and Boutilier,? 2010", "shortCiteRegEx": "Viappiani and Boutilier", "year": 2010}, {"title": "Multibandit best arm identification", "author": ["Victor Gabillon", "Alessandro Lazaric", "Mohammad Ghavamzadeh", "S. Bubeck"], "venue": "Neural Information Processing Systems (NIPS\u201911).", "citeRegEx": "Gabillon et al\\.,? 2011", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Bayesian reinforcement learning", "author": ["N. Vlassis", "M. Ghavamzadeh", "S. Mannor", "P. Poupart"], "venue": "Reinforcement Learning, pages 359\u2013386.", "citeRegEx": "Vlassis et al\\.,? 2012", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}, {"title": "Active learning in multimedia annotation and retrieval: A survey", "author": ["M. Wang", "X. Hua"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(2):10.", "citeRegEx": "Wang and Hua,? 2011", "shortCiteRegEx": "Wang and Hua", "year": 2011}, {"title": "Autonomous mental development by robots and animals", "author": ["J. Weng", "J. McClelland", "A. Pentland", "O. Sporns", "I. Stockman", "M. Sur", "E. Thelen"], "venue": "Science, 291:599 \u2013 600.", "citeRegEx": "Weng et al\\.,? 2001", "shortCiteRegEx": "Weng et al\\.", "year": 2001}, {"title": "Efficient modelbased exploration", "author": ["M. Wiering", "J. Schmidhuber"], "venue": "Inter. Conf. on Simulation of Adaptive Behavior: From Animals to Animats 6, pages 223\u2013228.", "citeRegEx": "Wiering and Schmidhuber,? 1998", "shortCiteRegEx": "Wiering and Schmidhuber", "year": 1998}, {"title": "Policy teaching through reward function learning", "author": ["H. Zhang", "D. Parkes", "Y. Chen"], "venue": "ACM Conf. on Electronic commerce, pages 295\u2013304.", "citeRegEx": "Zhang et al\\.,? 2009", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 250, "context": "However, the embodiment in this type of systems provides a unique opportunity to exploit an active learning (AL) 2 approach (AL)(Angluin, 1988; Thrun, 1995; Settles, 2009) to guide the robot actions towards", "startOffset": 128, "endOffset": 171}, {"referenceID": 216, "context": "However, the embodiment in this type of systems provides a unique opportunity to exploit an active learning (AL) 2 approach (AL)(Angluin, 1988; Thrun, 1995; Settles, 2009) to guide the robot actions towards", "startOffset": 128, "endOffset": 171}, {"referenceID": 130, "context": "Active learning can also be used to describe situations where the student is involved in the learning process as opposed to passively listening to lectures, see for instance (Linder et al., 2001).", "startOffset": 174, "endOffset": 195}, {"referenceID": 209, "context": "In the context of intelligent system, another line of motivation and inspiration comes from the field of artificial development (Schmidhuber, 1991b; Weng et al., 2001; Asada et al., 2001; Lungarella et al., 2003; Oudeyer, 2011).", "startOffset": 128, "endOffset": 227}, {"referenceID": 260, "context": "In the context of intelligent system, another line of motivation and inspiration comes from the field of artificial development (Schmidhuber, 1991b; Weng et al., 2001; Asada et al., 2001; Lungarella et al., 2003; Oudeyer, 2011).", "startOffset": 128, "endOffset": 227}, {"referenceID": 141, "context": "In the context of intelligent system, another line of motivation and inspiration comes from the field of artificial development (Schmidhuber, 1991b; Weng et al., 2001; Asada et al., 2001; Lungarella et al., 2003; Oudeyer, 2011).", "startOffset": 128, "endOffset": 227}, {"referenceID": 216, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 43, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 44, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 180, "context": "Most theoretical results on AL are recent (Settles, 2009; Dasgupta, 2005; Dasgupta, 2011; Nowak, 2011).", "startOffset": 42, "endOffset": 102}, {"referenceID": 194, "context": "perspective on the applicability of AL for real application, and indeed there already many examples: image classification (Qi et al., 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 122, "endOffset": 139}, {"referenceID": 252, "context": ", 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 29, "endOffset": 52}, {"referenceID": 259, "context": ", 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 65, "endOffset": 85}, {"referenceID": 216, "context": ", 2008), text classification (Tong and Koller, 2001), multimedia (Wang and Hua, 2011), among many others (see (Settles, 2009) for a", "startOffset": 110, "endOffset": 125}, {"referenceID": 102, "context": "the robot scientist (King et al., 2004) eliminates redundant experiments based on inductive logic programming.", "startOffset": 20, "endOffset": 39}, {"referenceID": 207, "context": "gistic regression (Schein and Ungar, 2007), support vector machines (Tong and Koller, 2001), GP (Kapoor et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 252, "context": "gistic regression (Schein and Ungar, 2007), support vector machines (Tong and Koller, 2001), GP (Kapoor et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 98, "context": "gistic regression (Schein and Ungar, 2007), support vector machines (Tong and Koller, 2001), GP (Kapoor et al., 2007), neural networks (Cohn et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 38, "context": ", 2007), neural networks (Cohn et al., 1996), mixture models (Cohn et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 38, "context": ", 1996), mixture models (Cohn et al., 1996), inverse reinforcement learning (Lopes et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 137, "context": ", 1996), inverse reinforcement learning (Lopes et al., 2009b), among many others.", "startOffset": 40, "endOffset": 61}, {"referenceID": 216, "context": "Classical Active Learning(AL), refers to a set of approaches in which a learning algorithm is able to interactively query a source of information to obtain the desired outputs at new data points (Settles, 2009).", "startOffset": 195, "endOffset": 210}, {"referenceID": 214, "context": "Optimal Experimental Design(OED), an early perspective on active learning where the design of the experiments is optimal according to some statistical criteria (Schonlau et al., 1998).", "startOffset": 160, "endOffset": 183}, {"referenceID": 20, "context": "Bayesian Optimization, class of methods to solve an optimization problem that use statistical measures of uncertainty about the target function to guide exploration (Brochu et al., 2010).", "startOffset": 165, "endOffset": 186}, {"referenceID": 56, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 221, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 72, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 253, "context": "See a discussion at (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 20, "endOffset": 99}, {"referenceID": 75, "context": "Results seem to indicate that curiosity is an intrinsic drive in most animals (Gottlieb et al., 2013).", "startOffset": 78, "endOffset": 101}, {"referenceID": 260, "context": "This is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman, 1997; Lungarella et al., 2003; Oudeyer, 2011) where the complexity of the problems that the agent is able to solve increases with time.", "startOffset": 48, "endOffset": 140}, {"referenceID": 58, "context": "This is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman, 1997; Lungarella et al., 2003; Oudeyer, 2011) where the complexity of the problems that the agent is able to solve increases with time.", "startOffset": 48, "endOffset": 140}, {"referenceID": 141, "context": "This is the main idea of developmental robotics (Weng et al., 2001; Asada et al., 2001; Elman, 1997; Lungarella et al., 2003; Oudeyer, 2011) where the complexity of the problems that the agent is able to solve increases with time.", "startOffset": 48, "endOffset": 140}, {"referenceID": 10, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 209, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 187, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 225, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 211, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 186, "context": "This early stage is guided by curiosity and intrinsic motivation (Barto et al., 2004; Schmidhuber, 1991b; Oudeyer et al., 2005; Singh et al., 2005; Schmidhuber, 2006; Oudeyer et al., 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al.", "startOffset": 65, "endOffset": 188}, {"referenceID": 228, "context": ", 2007) and its justification is that it is a skill that will lead to a better adaptation to a large distribution of problems (Singh et al., 2010b).", "startOffset": 126, "endOffset": 147}, {"referenceID": 240, "context": "Exploration in reinforcement learning (Sutton and Barto, 1998), bayesian optimization (Brochu et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 20, "context": "Exploration in reinforcement learning (Sutton and Barto, 1998), bayesian optimization (Brochu et al., 2010), multi-armed bandits (Bubeck and CesaBianchi, 2012), curiosity (Oudeyer and Kaplan, 2007), interactive machine learning (Breazeal et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 183, "context": ", 2010), multi-armed bandits (Bubeck and CesaBianchi, 2012), curiosity (Oudeyer and Kaplan, 2007), interactive machine learning (Breazeal et al.", "startOffset": 71, "endOffset": 97}, {"referenceID": 19, "context": ", 2010), multi-armed bandits (Bubeck and CesaBianchi, 2012), curiosity (Oudeyer and Kaplan, 2007), interactive machine learning (Breazeal et al., 2004) or", "startOffset": 128, "endOffset": 151}, {"referenceID": 216, "context": "active learning for classification and regression problems (Settles, 2009), all these share many properties and face similar challenges.", "startOffset": 59, "endOffset": 74}, {"referenceID": 20, "context": "I don\u2019t like the last sentence with the last changes in the section then be used to find the best point with a minimum of function evaluations (Brochu et al., 2010).", "startOffset": 143, "endOffset": 164}, {"referenceID": 144, "context": "Another example, still for regression, is to decompose complex regression functions to a set of local regressions and then rely on multi-armed bandit algorithms to balance exploration in a more efficient way (Maillard, 2012).", "startOffset": 208, "endOffset": 224}, {"referenceID": 221, "context": "Clearly this problem is, in general, intractable and the following sections describe particular instantiations, approximations and models of this optimal exploration problem (\u015eim\u015fek and Barto, 2006).", "startOffset": 174, "endOffset": 198}, {"referenceID": 56, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 221, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 72, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 253, "context": "For a more detailed description on the relation of the exploration policy with the learning task see (Duff, 2003; \u015eim\u015fek and Barto, 2006; Golovin and Krause, 2010; Toussaint, 2012).", "startOffset": 101, "endOffset": 180}, {"referenceID": 216, "context": "Some other restrictions can be included such as being restricted to a finite set of input points (pool-based active learning) or having the points arriving sequentially and having to decide to query or not (online learning)(see (Settles, 2009) for a comprehensive discussion on the different settings).", "startOffset": 228, "endOffset": 243}, {"referenceID": 70, "context": "An alternative formalism that is usually applied to discrete selection problems is the multi-armed bandit (MAB) formalism (Gittins, 1979; Bubeck and CesaBianchi, 2012).", "startOffset": 122, "endOffset": 167}, {"referenceID": 31, "context": ", 2011) or the value of all the arms (Carpentier et al., 2011).", "startOffset": 37, "endOffset": 62}, {"referenceID": 12, "context": "The most general, and well known, formalism to model sequential decision processes are markov-decision process (MDP)(Bellman, 1952).", "startOffset": 116, "endOffset": 131}, {"referenceID": 240, "context": "When there is no knowledge about the model of the environment and an agent has to optimize a reward function while interacting with the environment the problem is called reinforcement learning (RL) (Sutton and Barto, 1998).", "startOffset": 198, "endOffset": 222}, {"referenceID": 96, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 240, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 243, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 109, "context": "For a complete treatment on the topic refer to (Kaelbling et al., 1996; Sutton and Barto, 1998; Szepesv\u00e1ri, 2011; Kober et al., 2013).", "startOffset": 47, "endOffset": 133}, {"referenceID": 85, "context": "Then it can explicitly create a model of the environment and exploit it (Hester and Stone, 2011; Nguyen-Tuong and Peters, 2011) and directly try to find a policy that optimizes the behavior (Deisenroth et al.", "startOffset": 72, "endOffset": 127}, {"referenceID": 175, "context": "Then it can explicitly create a model of the environment and exploit it (Hester and Stone, 2011; Nguyen-Tuong and Peters, 2011) and directly try to find a policy that optimizes the behavior (Deisenroth et al.", "startOffset": 72, "endOffset": 127}, {"referenceID": 46, "context": "Then it can explicitly create a model of the environment and exploit it (Hester and Stone, 2011; Nguyen-Tuong and Peters, 2011) and directly try to find a policy that optimizes the behavior (Deisenroth et al., 2013).", "startOffset": 190, "endOffset": 215}, {"referenceID": 95, "context": "Partial-observable markov decision processes (POMDP) generalize the concept for cases where the state is not directly observable (Kaelbling et al., 1998).", "startOffset": 129, "endOffset": 153}, {"referenceID": 20, "context": "Optimization Learning Point Bayesian Optimization (Brochu et al., 2010) Classical Active Learning (Settles, 2009) Discrete tasks Multi-armed bandits (Auer et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 216, "context": ", 2010) Classical Active Learning (Settles, 2009) Discrete tasks Multi-armed bandits (Auer et al.", "startOffset": 34, "endOffset": 49}, {"referenceID": 31, "context": ", 2003) AL for MAB (Carpentier et al., 2011) Trajectory Exploration/Exploitation (Kaelbling et al.", "startOffset": 19, "endOffset": 44}, {"referenceID": 96, "context": ", 2011) Trajectory Exploration/Exploitation (Kaelbling et al., 1996) Exploration", "startOffset": 44, "endOffset": 68}, {"referenceID": 31, "context": "In this pure learning problem of multi-armed bandits regret bounds on the simple regret can also be made (Carpentier et al., 2011; Victor Gabillon et al., 2011).", "startOffset": 105, "endOffset": 160}, {"referenceID": 88, "context": "also been established (Jaksch et al., 2010).", "startOffset": 22, "endOffset": 43}, {"referenceID": 216, "context": "The classification below follows the one proposed in (Settles, 2009) (also refer to (MacKay, 1992; Settles, 2009) for further details) and completes it by including empirical measures as a different way of assessing the information gain of a sample.", "startOffset": 53, "endOffset": 68}, {"referenceID": 143, "context": "The classification below follows the one proposed in (Settles, 2009) (also refer to (MacKay, 1992; Settles, 2009) for further details) and completes it by including empirical measures as a different way of assessing the information gain of a sample.", "startOffset": 84, "endOffset": 113}, {"referenceID": 216, "context": "The classification below follows the one proposed in (Settles, 2009) (also refer to (MacKay, 1992; Settles, 2009) for further details) and completes it by including empirical measures as a different way of assessing the information gain of a sample.", "startOffset": 84, "endOffset": 113}, {"referenceID": 209, "context": "The latter class of measures aims to consider those cases where there is no single model that covers the whole state-space, or if the agents lacks the knowledge to select which is the best one (Schmidhuber, 1991b; Oudeyer and Kaplan, 2007).", "startOffset": 193, "endOffset": 239}, {"referenceID": 183, "context": "The latter class of measures aims to consider those cases where there is no single model that covers the whole state-space, or if the agents lacks the knowledge to select which is the best one (Schmidhuber, 1991b; Oudeyer and Kaplan, 2007).", "startOffset": 193, "endOffset": 239}, {"referenceID": 128, "context": "Uncertainty sampling where the query is made where the classifier is most uncertain about (Lewis and Gale, 1994), still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Ungar, 2007), among others.", "startOffset": 90, "endOffset": 112}, {"referenceID": 252, "context": "Uncertainty sampling where the query is made where the classifier is most uncertain about (Lewis and Gale, 1994), still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Ungar, 2007), among others.", "startOffset": 152, "endOffset": 175}, {"referenceID": 207, "context": "Uncertainty sampling where the query is made where the classifier is most uncertain about (Lewis and Gale, 1994), still used in support vector machines (Tong and Koller, 2001), logistic regression (Schein and Ungar, 2007), among others.", "startOffset": 197, "endOffset": 221}, {"referenceID": 37, "context": "An initial model considered Selective Sampling (Cohn et al., 1994) where a pool, or stream, of unlabeled examples exists and the learner may request the labels to an or-", "startOffset": 47, "endOffset": 66}, {"referenceID": 218, "context": "Query by committee (Seung et al., 1992; Freund et al., 1997) considers a committee of classifiers and measures the degree of disagreement between the committee.", "startOffset": 19, "endOffset": 60}, {"referenceID": 66, "context": "Query by committee (Seung et al., 1992; Freund et al., 1997) considers a committee of classifiers and measures the degree of disagreement between the committee.", "startOffset": 19, "endOffset": 60}, {"referenceID": 37, "context": "Perhaps the best-studied approach of this kind is learning by queries (Angluin, 1988; Cohn et al., 1994; Baum, 1991).", "startOffset": 70, "endOffset": 116}, {"referenceID": 11, "context": "Perhaps the best-studied approach of this kind is learning by queries (Angluin, 1988; Cohn et al., 1994; Baum, 1991).", "startOffset": 70, "endOffset": 116}, {"referenceID": 180, "context": "Under this setting approaches have generalized methods based on binary search (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 78, "endOffset": 113}, {"referenceID": 157, "context": "Under this setting approaches have generalized methods based on binary search (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 78, "endOffset": 113}, {"referenceID": 252, "context": "Also, active learning in support vector machines can be seen in a version space perspective or as the uncertainty of the classifier (Tong and Koller, 2001).", "startOffset": 132, "endOffset": 155}, {"referenceID": 38, "context": "Variance reduction aims to select the sample(s) that will minimize the variance of the estimation for unlabeled samples (Cohn et al., 1996).", "startOffset": 120, "endOffset": 139}, {"referenceID": 217, "context": "Finally, there are other decisiontheoretic based measures such as the expected model change (Settles et al., 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al.", "startOffset": 92, "endOffset": 114}, {"referenceID": 200, "context": ", 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al., 2007) which select the sample that, in expectation, will result in the largest change in the model parameters or in the largest reduction in the generalization error, respectively.", "startOffset": 40, "endOffset": 89}, {"referenceID": 166, "context": ", 2007) or the expected error reduction (Roy and McCallum, 2001; Moskovitch et al., 2007) which select the sample that, in expectation, will result in the largest change in the model parameters or in the largest reduction in the generalization error, respectively.", "startOffset": 40, "endOffset": 89}, {"referenceID": 208, "context": "The first attempt to develop empirical measures was made by (Schmidhuber, 1991a; Schmidhuber, 1991b) in which an agent could model its own expectation about how future experiences can improve model learning.", "startOffset": 60, "endOffset": 100}, {"referenceID": 209, "context": "The first attempt to develop empirical measures was made by (Schmidhuber, 1991a; Schmidhuber, 1991b) in which an agent could model its own expectation about how future experiences can improve model learning.", "startOffset": 60, "endOffset": 100}, {"referenceID": 186, "context": "After this seminal paper, several measures to empirically estimate how can data improve task learning have been proposed and a integrated view can be seen in (Oudeyer et al., 2007).", "startOffset": 158, "endOffset": 180}, {"referenceID": 133, "context": "nenholtz, 2003) to the use of empirical measures in (Lopes et al., 2012).", "startOffset": 52, "endOffset": 72}, {"referenceID": 186, "context": "From (Oudeyer et al., 2007).", "startOffset": 5, "endOffset": 27}, {"referenceID": 186, "context": "Using the change in loss they may gain robustness by becoming independent of the loss\u2019 absolute value and can potentially detect timevarying conditions (Oudeyer et al., 2007; Lopes et al., 2012).", "startOffset": 152, "endOffset": 194}, {"referenceID": 133, "context": "Using the change in loss they may gain robustness by becoming independent of the loss\u2019 absolute value and can potentially detect timevarying conditions (Oudeyer et al., 2007; Lopes et al., 2012).", "startOffset": 152, "endOffset": 194}, {"referenceID": 133, "context": "To note that finding a good estimator for the expected loss is not trivial and resampling methods might be required (Lopes et al., 2012).", "startOffset": 116, "endOffset": 136}, {"referenceID": 186, "context": "See also (Oudeyer et al., 2007) for different definitions of learning progress.", "startOffset": 9, "endOffset": 31}, {"referenceID": 32, "context": "(Castro and Novak, 2008; Balcan et al., 2008) identify the expected gains that active learning can give in different classes of problems.", "startOffset": 0, "endOffset": 45}, {"referenceID": 2, "context": "(Castro and Novak, 2008; Balcan et al., 2008) identify the expected gains that active learning can give in different classes of problems.", "startOffset": 0, "endOffset": 45}, {"referenceID": 43, "context": "For instance, (Dasgupta, 2005; Dasgupta, 2011) studied the problem of actively finding the optimal threshold on a line for a separable classification problem.", "startOffset": 14, "endOffset": 46}, {"referenceID": 44, "context": "For instance, (Dasgupta, 2005; Dasgupta, 2011) studied the problem of actively finding the optimal threshold on a line for a separable classification problem.", "startOffset": 14, "endOffset": 46}, {"referenceID": 180, "context": "authors have shown that generalized binary search algorithms can be derived for more complex learning problems (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 111, "endOffset": 146}, {"referenceID": 157, "context": "authors have shown that generalized binary search algorithms can be derived for more complex learning problems (Nowak, 2011; Melo and Lopes, 2013).", "startOffset": 111, "endOffset": 146}, {"referenceID": 115, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 73, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 72, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 144, "context": "Under this approach the submodular property has been extensively used (Krause and Guestrin, 2005; Golovin et al., 2010b; Golovin and Krause, 2010; Maillard, 2012).", "startOffset": 70, "endOffset": 162}, {"referenceID": 171, "context": "A theorem from (Nemhauser et al., 1978) says that for monotonic submodular functions, the value of the function for the set obtained with the greedy algorithm G(Dg) is close, (1 \u2212 1/e), to the value of the optimal set G(DOPT ).", "startOffset": 15, "endOffset": 39}, {"referenceID": 101, "context": "Two of the most influential works on the topics are: E3 (Kearns and Singh, 2002) and R-max (Brafman and Tennenholtz, 2003).", "startOffset": 56, "endOffset": 80}, {"referenceID": 17, "context": "Two of the most influential works on the topics are: E3 (Kearns and Singh, 2002) and R-max (Brafman and Tennenholtz, 2003).", "startOffset": 91, "endOffset": 122}, {"referenceID": 220, "context": "Some other approaches consider limited look-ahead planning to approximately solve this problem (Sim and Roy, 2005; Krause and Guestrin, 2007).", "startOffset": 95, "endOffset": 141}, {"referenceID": 116, "context": "Some other approaches consider limited look-ahead planning to approximately solve this problem (Sim and Roy, 2005; Krause and Guestrin, 2007).", "startOffset": 95, "endOffset": 141}, {"referenceID": 31, "context": ", 2011) or the learning (Carpentier et al., 2011) problem with the best possible regret sometime taking into account specific knowledge about the statistical properties of each arm, but in many cases taken a distribution free approach (Auer et al.", "startOffset": 24, "endOffset": 49}, {"referenceID": 164, "context": "From (Montesano and Lopes, 2012).", "startOffset": 5, "endOffset": 32}, {"referenceID": 216, "context": "This is the most common setting in active learning for function approximation problems (Settles, 2009), with examples ranging from vehicle detection (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al.", "startOffset": 87, "endOffset": 102}, {"referenceID": 229, "context": "This is the most common setting in active learning for function approximation problems (Settles, 2009), with examples ranging from vehicle detection (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al.", "startOffset": 149, "endOffset": 178}, {"referenceID": 98, "context": "This is the most common setting in active learning for function approximation problems (Settles, 2009), with examples ranging from vehicle detection (Sivaraman and Trivedi, 2010), object recognition (Kapoor et al., 2007) among others.", "startOffset": 199, "endOffset": 220}, {"referenceID": 205, "context": "with the probability of grasp success when a robot tries to grasp at those points (Saxena et al., 2006).", "startOffset": 82, "endOffset": 103}, {"referenceID": 205, "context": "This process requires a large database of synthetically generated grasping points (as initially suggested by (Saxena et al., 2006)), or alternatively to actively search and select where to apply grasping actions to estimate their success (Salganicoff et al.", "startOffset": 109, "endOffset": 130}, {"referenceID": 203, "context": ", 2006)), or alternatively to actively search and select where to apply grasping actions to estimate their success (Salganicoff et al., 1996; Morales et al., 2004).", "startOffset": 115, "endOffset": 163}, {"referenceID": 165, "context": ", 2006)), or alternatively to actively search and select where to apply grasping actions to estimate their success (Salganicoff et al., 1996; Morales et al., 2004).", "startOffset": 115, "endOffset": 163}, {"referenceID": 163, "context": "Another approach, proposed by (Montesano and Lopes, 2009; Montesano and Lopes, 2012) (see also Figure 5), derived a kernel based algorithm to predict the probability of a successful grasp together with its uncertainty based on Beta priors.", "startOffset": 30, "endOffset": 84}, {"referenceID": 164, "context": "Another approach, proposed by (Montesano and Lopes, 2009; Montesano and Lopes, 2012) (see also Figure 5), derived a kernel based algorithm to predict the probability of a successful grasp together with its uncertainty based on Beta priors.", "startOffset": 30, "endOffset": 84}, {"referenceID": 47, "context": "Another approach used Gaussian process to model directly probability densities of successful grasps (Detry et al., 2009).", "startOffset": 100, "endOffset": 120}, {"referenceID": 119, "context": "Clearly such success probabilities depend on the grasping policy is being applied, and a combination of the two will be required to learn the best grasping strategy (Kroemer et al., 2009; Kroemer et al., 2010).", "startOffset": 165, "endOffset": 209}, {"referenceID": 120, "context": "Clearly such success probabilities depend on the grasping policy is being applied, and a combination of the two will be required to learn the best grasping strategy (Kroemer et al., 2009; Kroemer et al., 2010).", "startOffset": 165, "endOffset": 209}, {"referenceID": 53, "context": "(Dima et al., 2004) use active learning to request human users the correct labels of extensive datasets acquired by robots using density measures.", "startOffset": 0, "endOffset": 19}, {"referenceID": 52, "context": "Also using multiview approaches (Dima and Hebert, 2005).", "startOffset": 32, "endOffset": 55}, {"referenceID": 254, "context": "Another property exploited by other authors is the traversability of given regions (Ugur et al., 2007).", "startOffset": 83, "endOffset": 102}, {"referenceID": 245, "context": "A final example considers how to optimize the parameters of a controller whose results can only be evaluated as success or failure (Tesch et al., 2013).", "startOffset": 131, "endOffset": 151}, {"referenceID": 7, "context": "ferent models of the robotic kinematic, using either nearest-neighbors (Baranes and Oudeyer, 2012) or local-linear maps (Rolf et al.", "startOffset": 71, "endOffset": 98}, {"referenceID": 197, "context": "ferent models of the robotic kinematic, using either nearest-neighbors (Baranes and Oudeyer, 2012) or local-linear maps (Rolf et al., 2011).", "startOffset": 120, "endOffset": 139}, {"referenceID": 7, "context": "Empirical measures of learning progress were used by (Baranes and Oudeyer, 2012) and (Rolf et al.", "startOffset": 53, "endOffset": 80}, {"referenceID": 197, "context": "Empirical measures of learning progress were used by (Baranes and Oudeyer, 2012) and (Rolf et al., 2011).", "startOffset": 85, "endOffset": 104}, {"referenceID": 36, "context": "(Chernova and Veloso, 2009) considering support vector machines as the classification method.", "startOffset": 0, "endOffset": 27}, {"referenceID": 137, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 156, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 41, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 39, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 92, "context": "Under the formalism of inverse reinforcement learning, queries are made to a user that allow to infer the correct reward (Lopes et al., 2009b; Melo and Lopes, 2010; Cohn et al., 2010; Cohn et al., 2011; Judah et al., 2012).", "startOffset": 121, "endOffset": 222}, {"referenceID": 157, "context": "Initial sample complexity results show that this approaches can indeed provide gains on the average case (Melo and Lopes, 2013).", "startOffset": 105, "endOffset": 127}, {"referenceID": 4, "context": "For instance the system can either be able to select among a set of different sensors, different learning algorithms (Baram et al., 2004; Hoffman et al., 2011; Hester et al., 2013), or being interested in learning from among a set of discrete tasks (Barto et al.", "startOffset": 117, "endOffset": 180}, {"referenceID": 87, "context": "For instance the system can either be able to select among a set of different sensors, different learning algorithms (Baram et al., 2004; Hoffman et al., 2011; Hester et al., 2013), or being interested in learning from among a set of discrete tasks (Barto et al.", "startOffset": 117, "endOffset": 180}, {"referenceID": 84, "context": "For instance the system can either be able to select among a set of different sensors, different learning algorithms (Baram et al., 2004; Hoffman et al., 2011; Hester et al., 2013), or being interested in learning from among a set of discrete tasks (Barto et al.", "startOffset": 117, "endOffset": 180}, {"referenceID": 10, "context": ", 2013), or being interested in learning from among a set of discrete tasks (Barto et al., 2004).", "startOffset": 76, "endOffset": 96}, {"referenceID": 117, "context": "Examples include environmental sensing where the state is partitioned for computational purposes (Krause et al., 2008), or learning dynamical models of robots where the partition is created online based on the similarities of the function properties", "startOffset": 97, "endOffset": 118}, {"referenceID": 187, "context": "at each location (Oudeyer et al., 2005; Baran\u00e8s and Oudeyer, 2009) (see Figure 6).", "startOffset": 17, "endOffset": 66}, {"referenceID": 4, "context": "In the recently introduced strategic student problem (Lopes and Oudeyer, 2012), the authors provide an unified view of these problems, following a computational approach similar to (Baram et al., 2004; Hoffman et al., 2011; Baranes and Oudeyer, 2012).", "startOffset": 181, "endOffset": 250}, {"referenceID": 87, "context": "In the recently introduced strategic student problem (Lopes and Oudeyer, 2012), the authors provide an unified view of these problems, following a computational approach similar to (Baram et al., 2004; Hoffman et al., 2011; Baranes and Oudeyer, 2012).", "startOffset": 181, "endOffset": 250}, {"referenceID": 7, "context": "In the recently introduced strategic student problem (Lopes and Oudeyer, 2012), the authors provide an unified view of these problems, following a computational approach similar to (Baram et al., 2004; Hoffman et al., 2011; Baranes and Oudeyer, 2012).", "startOffset": 181, "endOffset": 250}, {"referenceID": 71, "context": "When this is not the case then a MAB algorithm must be used to ensure proper exploration of all the arms (Lopes and Oudeyer, 2012; Golovin et al., 2010a).", "startOffset": 105, "endOffset": 153}, {"referenceID": 209, "context": "These results confirms the heuristics of learning progress given by (Schmidhuber, 1991b; Oudeyer et al., 2007).", "startOffset": 68, "endOffset": 110}, {"referenceID": 186, "context": "These results confirms the heuristics of learning progress given by (Schmidhuber, 1991b; Oudeyer et al., 2007).", "startOffset": 68, "endOffset": 110}, {"referenceID": 183, "context": "From (Oudeyer and Kaplan, 2007).", "startOffset": 5, "endOffset": 31}, {"referenceID": 10, "context": "This set can be pre-defined, or acquired autonomously (see Section 4), to have a large dictionary of skills that can be used in different situations or to create complex hierarchical controllers (Barto et al., 2004; Byrne, 2002)", "startOffset": 195, "endOffset": 228}, {"referenceID": 23, "context": "This set can be pre-defined, or acquired autonomously (see Section 4), to have a large dictionary of skills that can be used in different situations or to create complex hierarchical controllers (Barto et al., 2004; Byrne, 2002)", "startOffset": 195, "endOffset": 228}, {"referenceID": 194, "context": "Multi-task problems have been considered in classification tasks (Qi et al., 2008; Reichart et al., 2008).", "startOffset": 65, "endOffset": 105}, {"referenceID": 196, "context": "Multi-task problems have been considered in classification tasks (Qi et al., 2008; Reichart et al., 2008).", "startOffset": 65, "endOffset": 105}, {"referenceID": 225, "context": "More interestingly for our discussion are the works from (Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 57, "endOffset": 99}, {"referenceID": 186, "context": "More interestingly for our discussion are the works from (Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 57, "endOffset": 99}, {"referenceID": 186, "context": "(Oudeyer et al., 2007) initially considered that each parameter region gave a different learning", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "Taking into account the previous discussion we know that a better exploration strategy must be applied and the authors considered more robust measures and created a stochastic policy to provide efficient results in high-dimensional problems (Baranes and Oudeyer, 2012).", "startOffset": 241, "endOffset": 268}, {"referenceID": 144, "context": "More recently (Maillard, 2012) introduce a new formulation of the problem and a new algorithm with specific regret bounds.", "startOffset": 14, "endOffset": 30}, {"referenceID": 225, "context": "The initial work of (Singh et al., 2005) lead to further improvements.", "startOffset": 20, "endOffset": 40}, {"referenceID": 221, "context": "The measures of progress that guide the selection of the macro action that is to be chosen started to consider the change in value function during learning (\u015eim\u015fek and Barto, 2006).", "startOffset": 156, "endOffset": 180}, {"referenceID": 81, "context": "Similar ideas were applied to learn affordances (Hart and Grupen, 2013) where different controllers and their validity regions are learned following their learning progress.", "startOffset": 48, "endOffset": 71}, {"referenceID": 116, "context": "When using a gaussian process as function approximation it is important to consider exploration to find the property of the kernel and then, for known parameters of the kernel, a simple offline policy provides optimal results (Krause and Guestrin, 2007).", "startOffset": 226, "endOffset": 253}, {"referenceID": 117, "context": "This partition in a finite set of choices allows to derive more efficient exploration/sensing strategies and still ensure tight bounds (Krause et al., 2008; Golovin and Krause, 2010; Golovin et al., 2010a).", "startOffset": 135, "endOffset": 205}, {"referenceID": 72, "context": "This partition in a finite set of choices allows to derive more efficient exploration/sensing strategies and still ensure tight bounds (Krause et al., 2008; Golovin and Krause, 2010; Golovin et al., 2010a).", "startOffset": 135, "endOffset": 205}, {"referenceID": 71, "context": "This partition in a finite set of choices allows to derive more efficient exploration/sensing strategies and still ensure tight bounds (Krause et al., 2008; Golovin and Krause, 2010; Golovin et al., 2010a).", "startOffset": 135, "endOffset": 205}, {"referenceID": 219, "context": "and Boutilier, 2003) where some of them might not even be cooperative (Shon et al., 2007), or even choose between looking/asking for a teacher demonstration or doing self-exploration (Nguyen and Oudeyer, 2012).", "startOffset": 70, "endOffset": 89}, {"referenceID": 112, "context": "The representation that gives more progress will be used more frequently (Konidaris and Barto, 2008; Maillard et al., 2011).", "startOffset": 73, "endOffset": 123}, {"referenceID": 145, "context": "The representation that gives more progress will be used more frequently (Konidaris and Barto, 2008; Maillard et al., 2011).", "startOffset": 73, "endOffset": 123}, {"referenceID": 33, "context": "A similar approach was suggested by (Castronovo et al., 2012) where a list of possible exploration reward is proposed and a given arm bandit is assigned to each one.", "startOffset": 36, "endOffset": 61}, {"referenceID": 84, "context": "This limitation was recently improved by considering that the agent can evaluate and select online the best exploration strategies (Hester et al., 2013).", "startOffset": 131, "endOffset": 152}, {"referenceID": 86, "context": "In this work the authors relied on a factored representation of an MDP (Hester and Stone, 2012) and using many different exploration bonuses they were able to define a large set of exploration strategies.", "startOffset": 71, "endOffset": 95}, {"referenceID": 148, "context": "Courtesy from (Marchant and Ramos, 2012).", "startOffset": 14, "endOffset": 40}, {"referenceID": 63, "context": "In (Fox et al., 1998) the belief over the robot position and orientation was obtained using a Monte Carlo algorithm.", "startOffset": 3, "endOffset": 21}, {"referenceID": 60, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 16, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 234, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 235, "context": "The first attempts to actively explore the environment during SLAM aimed to maximize the expected information gain (Feder et al., 1999; Bourgault et al., 2002; Stachniss and Burgard, 2003; Stachniss et al., 2005).", "startOffset": 115, "endOffset": 212}, {"referenceID": 60, "context": "For instance, in (Feder et al., 1999) an EKF was used to represent the robot location and the map features measured using sonar.", "startOffset": 17, "endOffset": 37}, {"referenceID": 234, "context": "For grid maps, similar ideas have been developed using mutual information (Stachniss and Burgard, 2003) and it is even possible to combine both representations (Bourgault et al.", "startOffset": 74, "endOffset": 103}, {"referenceID": 16, "context": "For grid maps, similar ideas have been developed using mutual information (Stachniss and Burgard, 2003) and it is even possible to combine both representations (Bourgault et al., 2002) using a weighted crite-", "startOffset": 160, "endOffset": 184}, {"referenceID": 5, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al.", "startOffset": 22, "endOffset": 76}, {"referenceID": 7, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al.", "startOffset": 22, "endOffset": 76}, {"referenceID": 10, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al., 2004; Oudeyer et al., 2005; Oudeyer et al., 2007) reg.", "startOffset": 110, "endOffset": 174}, {"referenceID": 187, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al., 2004; Oudeyer et al., 2005; Oudeyer et al., 2007) reg.", "startOffset": 110, "endOffset": 174}, {"referenceID": 186, "context": "n Regions n Functions (Baranes and Oudeyer, 2010; Baranes and Oudeyer, 2012) mdp n Environment n Environments (Barto et al., 2004; Oudeyer et al., 2005; Oudeyer et al., 2007) reg.", "startOffset": 110, "endOffset": 174}, {"referenceID": 7, "context": "Model (Baranes and Oudeyer, 2012; Jamone et al., 2011; Rolf et al., 2011) mdp Exploration strategies 1 Environment (Baram et al.", "startOffset": 6, "endOffset": 73}, {"referenceID": 89, "context": "Model (Baranes and Oudeyer, 2012; Jamone et al., 2011; Rolf et al., 2011) mdp Exploration strategies 1 Environment (Baram et al.", "startOffset": 6, "endOffset": 73}, {"referenceID": 197, "context": "Model (Baranes and Oudeyer, 2012; Jamone et al., 2011; Rolf et al., 2011) mdp Exploration strategies 1 Environment (Baram et al.", "startOffset": 6, "endOffset": 73}, {"referenceID": 4, "context": ", 2011) mdp Exploration strategies 1 Environment (Baram et al., 2004; Krause et al., 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al.", "startOffset": 49, "endOffset": 115}, {"referenceID": 117, "context": ", 2011) mdp Exploration strategies 1 Environment (Baram et al., 2004; Krause et al., 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al.", "startOffset": 49, "endOffset": 115}, {"referenceID": 193, "context": ", 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al., 2007) reg.", "startOffset": 62, "endOffset": 108}, {"referenceID": 219, "context": ", 2008; Lopes and Oudeyer, 2012) mdp n Teachers 1 Environment (Price and Boutilier, 2003; Shon et al., 2007) reg.", "startOffset": 62, "endOffset": 108}, {"referenceID": 112, "context": "Teacher,self-exploration 1 Function (Nguyen and Oudeyer, 2012) mdp n Representations 1 Environment (Konidaris and Barto, 2008; Maillard et al., 2011)", "startOffset": 99, "endOffset": 149}, {"referenceID": 145, "context": "Teacher,self-exploration 1 Function (Nguyen and Oudeyer, 2012) mdp n Representations 1 Environment (Konidaris and Barto, 2008; Maillard et al., 2011)", "startOffset": 99, "endOffset": 149}, {"referenceID": 150, "context": "The work in (Martinez-Cantin et al., 2007) directly aims to estimate the trajectory (i.", "startOffset": 12, "endOffset": 42}, {"referenceID": 150, "context": "a policy) in a continuous action-state space taking into account the cost to go there and all the information gathered in the path (Martinez-Cantin et al., 2007).", "startOffset": 131, "endOffset": 161}, {"referenceID": 104, "context": "For instance, (Kneebone and Dearden, 2009) uses a POMDP framework to incorporate uncertainty into Rapid Random Trees planning.", "startOffset": 14, "endOffset": 42}, {"referenceID": 116, "context": "time models (Krause and Guestrin, 2007; Garg et al., 2012).", "startOffset": 12, "endOffset": 58}, {"referenceID": 68, "context": "time models (Krause and Guestrin, 2007; Garg et al., 2012).", "startOffset": 12, "endOffset": 58}, {"referenceID": 224, "context": "By exploiting submodularity it is possible to compute efficient paths for multiple robots assuring that they will gather information in a set of regions (Singh et al., 2007).", "startOffset": 153, "endOffset": 173}, {"referenceID": 148, "context": "Without relying on a particular division into regions, but without any proven bounds, (Marchant and Ramos, 2012) used Bayesian optimization tools to find an informative path in a space-time model.", "startOffset": 86, "endOffset": 112}, {"referenceID": 249, "context": "Another setting where the learner actively plans its actions to improve learning is in reinforcement learning (see an early review on the topic (Thrun, 1992)).", "startOffset": 144, "endOffset": 157}, {"referenceID": 237, "context": "probability of finding a solution that is approximately correct, following the standard probably approximately correct learning (PAC) (Strehl and Littman, 2008; Strehl et al., 2009).", "startOffset": 134, "endOffset": 181}, {"referenceID": 236, "context": "probability of finding a solution that is approximately correct, following the standard probably approximately correct learning (PAC) (Strehl and Littman, 2008; Strehl et al., 2009).", "startOffset": 134, "endOffset": 181}, {"referenceID": 17, "context": "2002) and R-max (Brafman and Tennenholtz, 2003).", "startOffset": 16, "endOffset": 47}, {"referenceID": 17, "context": "Specifically, for the case of Rmax (Brafman and Tennenholtz, 2003), the algorithm divides the states into known and unknown based on the number of visits made.", "startOffset": 35, "endOffset": 66}, {"referenceID": 237, "context": "For a further analysis an more recent algorithm see the discussion in (Strehl and Littman, 2008).", "startOffset": 70, "endOffset": 96}, {"referenceID": 202, "context": "Such regret measure have been already generate some RL algorithms (Salganicoff and Ungar, 1995; Ortner, 2007; Jaksch et al., 2010).", "startOffset": 66, "endOffset": 130}, {"referenceID": 182, "context": "Such regret measure have been already generate some RL algorithms (Salganicoff and Ungar, 1995; Ortner, 2007; Jaksch et al., 2010).", "startOffset": 66, "endOffset": 130}, {"referenceID": 88, "context": "Such regret measure have been already generate some RL algorithms (Salganicoff and Ungar, 1995; Ortner, 2007; Jaksch et al., 2010).", "startOffset": 66, "endOffset": 130}, {"referenceID": 45, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 192, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 258, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 232, "context": "Yet another approach considers Bayesian RL (Dearden et al., 1998; Poupart et al., 2006; Vlassis et al., 2012; Sorg et al., 2010c).", "startOffset": 43, "endOffset": 129}, {"referenceID": 111, "context": "Bayesian exploration bonus (BEB) approach (Kolter and Ng, 2009) mixes the ideas of Bayesian RL with R-max where the state are not explicitly separated between known and un-", "startOffset": 42, "endOffset": 63}, {"referenceID": 133, "context": "generalized for the case where each different state might have different statistical properties (Lopes et al., 2012).", "startOffset": 96, "endOffset": 116}, {"referenceID": 17, "context": "As a generalization of exploration methods in reinforcement learning, such as (Brafman and Tennenholtz, 2003), ideas have been suggested such as planning to be surprised (Sun et al.", "startOffset": 78, "endOffset": 109}, {"referenceID": 239, "context": "As a generalization of exploration methods in reinforcement learning, such as (Brafman and Tennenholtz, 2003), ideas have been suggested such as planning to be surprised (Sun et al., 2011) or the combination of empirical learning progress with visit counts (Hester and Stone, 2012).", "startOffset": 170, "endOffset": 188}, {"referenceID": 86, "context": ", 2011) or the combination of empirical learning progress with visit counts (Hester and Stone, 2012).", "startOffset": 76, "endOffset": 100}, {"referenceID": 64, "context": "We note also that the ideas and algorithms for exploration/exploitation are not limited to finite state representations, there have been recent results extending them to to POMDPs (Fox and Tennenholtz, 2007; Jaulmes et al., 2005; Doshi et al., 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 180, "endOffset": 249}, {"referenceID": 90, "context": "We note also that the ideas and algorithms for exploration/exploitation are not limited to finite state representations, there have been recent results extending them to to POMDPs (Fox and Tennenholtz, 2007; Jaulmes et al., 2005; Doshi et al., 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 180, "endOffset": 249}, {"referenceID": 55, "context": "We note also that the ideas and algorithms for exploration/exploitation are not limited to finite state representations, there have been recent results extending them to to POMDPs (Fox and Tennenholtz, 2007; Jaulmes et al., 2005; Doshi et al., 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 180, "endOffset": 249}, {"referenceID": 94, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 44, "endOffset": 66}, {"referenceID": 86, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 87, "endOffset": 136}, {"referenceID": 179, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al.", "startOffset": 87, "endOffset": 136}, {"referenceID": 122, "context": ", 2008), Gaussian Process Dynamical Systems (Jung and Stone, 2010), structured domains (Hester and Stone, 2012; Nouri and Littman, 2010), and relational problems (Lang et al., 2010).", "startOffset": 162, "endOffset": 181}, {"referenceID": 162, "context": "Safe exploration techniques have started to be developed (Moldovan and Abbeel, 2012).", "startOffset": 57, "endOffset": 84}, {"referenceID": 149, "context": "In (Martinez-Cantin et al., 2009; Martinez-Cantin et al., 2010) the authors want to learn a dynamical model of a robot arm, or a good map of the environment, with the minimum amount of data.", "startOffset": 3, "endOffset": 63}, {"referenceID": 151, "context": "In (Martinez-Cantin et al., 2009; Martinez-Cantin et al., 2010) the authors want to learn a dynamical model of a robot arm, or a good map of the environment, with the minimum amount of data.", "startOffset": 3, "endOffset": 63}, {"referenceID": 7, "context": "From (Baranes and Oudeyer, 2012).", "startOffset": 5, "endOffset": 32}, {"referenceID": 20, "context": "problem, and if it is to be used in real time, then efficient Bayesian optimization techniques must be used (Brochu et al., 2010).", "startOffset": 108, "endOffset": 129}, {"referenceID": 7, "context": "Another examples is the SAGG-RIAC architecture (Baranes and Oudeyer, 2012).", "startOffset": 47, "endOffset": 74}, {"referenceID": 119, "context": "We can also view the works of (Kroemer et al., 2009; Kroemer et al., 2010) as having a level of active explo-", "startOffset": 30, "endOffset": 74}, {"referenceID": 120, "context": "We can also view the works of (Kroemer et al., 2009; Kroemer et al., 2010) as having a level of active explo-", "startOffset": 30, "endOffset": 74}, {"referenceID": 46, "context": "in many cases, other situations, even if strongly dependent on that same process, address it only in an implicit way or as a side-effect of an optimization process (Deisenroth et al., 2013).", "startOffset": 164, "endOffset": 189}, {"referenceID": 241, "context": "The most noteworthy example are all policy gradient methods and similar approaches (Sutton et al., 2000; Kober et al., 2013).", "startOffset": 83, "endOffset": 124}, {"referenceID": 109, "context": "The most noteworthy example are all policy gradient methods and similar approaches (Sutton et al., 2000; Kober et al., 2013).", "startOffset": 83, "endOffset": 124}, {"referenceID": 188, "context": "Some methods consider stochastic policies and the noise on the policy is used to perform exploration and collect data (Peters et al., 2005).", "startOffset": 118, "endOffset": 139}, {"referenceID": 238, "context": "Another line of research is to use more classical methods of optimization to find the best set of parameters that maximize a reward function (Stulp and Sigaud, 2012).", "startOffset": 141, "endOffset": 165}, {"referenceID": 245, "context": "Recently, and using a more accurate model of uncertainty it is possible to use Bayesian optimization methods to search for the best policy parameters that result in the highest success rate (Tesch et al., 2013).", "startOffset": 190, "endOffset": 210}, {"referenceID": 61, "context": "This idea has been applied to segment object and learn about their properties (Fitzpatrick et al., 2003), disambiguate and model articulated objects (Katz et al.", "startOffset": 78, "endOffset": 104}, {"referenceID": 100, "context": ", 2003), disambiguate and model articulated objects (Katz et al., 2008), disambiguate sound (Berglund and Sitte, 2005), among others.", "startOffset": 52, "endOffset": 71}, {"referenceID": 13, "context": ", 2008), disambiguate sound (Berglund and Sitte, 2005), among others.", "startOffset": 28, "endOffset": 54}, {"referenceID": 154, "context": "Attention can also be seen as an instance of active perception, (Meger et al., 2008) presents an attention system and learning in a real environment to learn about object using SIFTs and finally, in highly cluttered environments active approach can also provide significant gains (van Hoof et al.", "startOffset": 64, "endOffset": 84}, {"referenceID": 14, "context": "This spontaneous motivation to explore and intrinsic curiosity to novelty (Berlyne, 1960) challenged utilitarian perspectives on behavior.", "startOffset": 74, "endOffset": 89}, {"referenceID": 3, "context": "ture situations (Baldassarre, 2011; Singh et al., 2009), only after going through school will that knowledge have some practical benefit.", "startOffset": 16, "endOffset": 55}, {"referenceID": 226, "context": "ture situations (Baldassarre, 2011; Singh et al., 2009), only after going through school will that knowledge have some practical benefit.", "startOffset": 16, "endOffset": 55}, {"referenceID": 228, "context": "Some hypothesis can be made that this stage results from an evolutionary process that leads to a better performance in a class of problems (Singh et al., 2010b).", "startOffset": 139, "endOffset": 160}, {"referenceID": 227, "context": "Or that intrinsic motivation is a way to deal with bounded agents where maximizing the objective reward would be too difficult (Singh et al., 2010a; Sorg et al., 2010a).", "startOffset": 127, "endOffset": 168}, {"referenceID": 230, "context": "Or that intrinsic motivation is a way to deal with bounded agents where maximizing the objective reward would be too difficult (Singh et al., 2010a; Sorg et al., 2010a).", "startOffset": 127, "endOffset": 168}, {"referenceID": 74, "context": "Even for very limited time spans where an agent wants to select a single action, there are many somewhat contradictory mechanisms for attention and curiosity (Gottlieb, 2012).", "startOffset": 158, "endOffset": 174}, {"referenceID": 0, "context": "This work, suggested by (Auer et al., 2011; Lim and Auer, 2012), shows that it is possible to address such problem and still ensure formal regret bounds.", "startOffset": 24, "endOffset": 63}, {"referenceID": 129, "context": "This work, suggested by (Auer et al., 2011; Lim and Auer, 2012), shows that it is possible to address such problem and still ensure formal regret bounds.", "startOffset": 24, "endOffset": 63}, {"referenceID": 212, "context": "Under different formalisms we can also see the POWERPLAY system as a way to increasingly augment the complexity of already explained problems (Schmidhuber, 2011).", "startOffset": 142, "endOffset": 161}, {"referenceID": 7, "context": "The approach from (Baranes and Oudeyer, 2012) can also be seen in this perspective where the space of policy parameters is explored in an increasing order of complexity.", "startOffset": 18, "endOffset": 45}, {"referenceID": 209, "context": "One of the earliest works that tried to operationalize these concepts was made by (Schmidhuber, 1991b).", "startOffset": 82, "endOffset": 102}, {"referenceID": 210, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 211, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 225, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 186, "context": "study to many other domains (Schmidhuber, 1995; Schmidhuber, 2006; Singh et al., 2005; Oudeyer et al., 2007).", "startOffset": 28, "endOffset": 108}, {"referenceID": 7, "context": "Research in this field has considered new problems such as: situations where parts of the state space are unlearnable (Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012); guide exploration in different", "startOffset": 118, "endOffset": 172}, {"referenceID": 7, "context": "spaces (Baranes and Oudeyer, 2012); environmental changes (Lopes et al.", "startOffset": 7, "endOffset": 34}, {"referenceID": 133, "context": "spaces (Baranes and Oudeyer, 2012); environmental changes (Lopes et al., 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 211, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 186, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 7, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 84, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 133, "context": ", 2012); empirical measures of learning progress (Schmidhuber, 2006; Oudeyer et al., 2007; Baran\u00e8s and Oudeyer, 2009; Baranes and Oudeyer, 2012; Hester et al., 2013; Lopes et al., 2012);", "startOffset": 49, "endOffset": 185}, {"referenceID": 227, "context": "limited agents (Singh et al., 2010a; Sorg et al., 2010a; Sequeira et al., 2011); open-ended problems (Singh et al.", "startOffset": 15, "endOffset": 79}, {"referenceID": 230, "context": "limited agents (Singh et al., 2010a; Sorg et al., 2010a; Sequeira et al., 2011); open-ended problems (Singh et al.", "startOffset": 15, "endOffset": 79}, {"referenceID": 215, "context": "limited agents (Singh et al., 2010a; Sorg et al., 2010a; Sequeira et al., 2011); open-ended problems (Singh et al.", "startOffset": 15, "endOffset": 79}, {"referenceID": 225, "context": ", 2011); open-ended problems (Singh et al., 2005; Oudeyer et al., 2007); autonomous discovery of good representations (Luciw et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 186, "context": ", 2011); open-ended problems (Singh et al., 2005; Oudeyer et al., 2007); autonomous discovery of good representations (Luciw et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 140, "context": ", 2007); autonomous discovery of good representations (Luciw et al., 2011);", "startOffset": 54, "endOffset": 74}, {"referenceID": 84, "context": "and selecting efficient exploration policies (Lopes and Oudeyer, 2012; Hester et al., 2013).", "startOffset": 45, "endOffset": 91}, {"referenceID": 80, "context": "Nevertheless, the problem is not trivial and most heuristics are bound to fail in most cases (Guyon and Elisseeff, 2003).", "startOffset": 93, "endOffset": 120}, {"referenceID": 159, "context": "For instance, (Meng and Lee, 2008) grows radial basis functions to learn mappings between sensory modalities by sampling locations with an high error.", "startOffset": 14, "endOffset": 34}, {"referenceID": 201, "context": "For instance (Ruesch and Bernardino, 2009; Schatz and Oudeyer, 2009; Rothkopf et al., 2009) study what is the relation be-", "startOffset": 13, "endOffset": 91}, {"referenceID": 199, "context": "For instance (Ruesch and Bernardino, 2009; Schatz and Oudeyer, 2009; Rothkopf et al., 2009) study what is the relation be-", "startOffset": 13, "endOffset": 91}, {"referenceID": 190, "context": "Early works considered how a finite-automaton and an hierarchy could be learned from data (Pierce and Kuipers, 1995).", "startOffset": 90, "endOffset": 116}, {"referenceID": 160, "context": "Generalizations of those ideas consider how to detect regularities that identify non-static world objects and thus allowing to infer actions that change the world in the desired ways (Modayil and Kuipers, 2007).", "startOffset": 183, "endOffset": 210}, {"referenceID": 230, "context": "A very interesting perspective was approached with the definition of the optimal reward problem (Sorg et al., 2010a).", "startOffset": 96, "endOffset": 116}, {"referenceID": 231, "context": "The authors have extended their initial approach to have a more practical algorithm using reward gradient (Sorg et al., 2010b) and by comparing different search methods (Sorg et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 233, "context": ", 2010b) and by comparing different search methods (Sorg et al., 2011).", "startOffset": 51, "endOffset": 70}, {"referenceID": 215, "context": "Such search for an extra reward signal can also be used to improve coordination in a multi-agent scenario (Sequeira et al., 2011).", "startOffset": 106, "endOffset": 129}, {"referenceID": 10, "context": "This perspective on (sub) goal creation motivated one of the earliest computational models on intrinsic motivated systems (Barto et al., 2004; Singh et al., 2005), see Figure 9.", "startOffset": 122, "endOffset": 162}, {"referenceID": 225, "context": "This perspective on (sub) goal creation motivated one of the earliest computational models on intrinsic motivated systems (Barto et al., 2004; Singh et al., 2005), see Figure 9.", "startOffset": 122, "endOffset": 162}, {"referenceID": 242, "context": "There the authors, using the theory of options (Sutton et al., 1999), construct new goals (as options) every time the agent finds a new \u201dsalient\u201d stimuli.", "startOffset": 47, "endOffset": 68}, {"referenceID": 225, "context": "From (Singh et al., 2005).", "startOffset": 5, "endOffset": 25}, {"referenceID": 144, "context": "An initial theoretical study frames such model as a multi-armed bandits over a pre-defined hierarchical partition of the space (Maillard, 2012).", "startOffset": 127, "endOffset": 143}, {"referenceID": 117, "context": "An example is the optimization setting of (Krause et al., 2008).", "startOffset": 42, "endOffset": 63}, {"referenceID": 186, "context": "The aforementioned works of (Oudeyer et al., 2007; Baranes and Oudeyer, 2012) consider initially a single region (a prediction task in the former and a control task in the latter) but then automatically and continuously constructs new region,", "startOffset": 28, "endOffset": 77}, {"referenceID": 7, "context": "The aforementioned works of (Oudeyer et al., 2007; Baranes and Oudeyer, 2012) consider initially a single region (a prediction task in the former and a control task in the latter) but then automatically and continuously constructs new region,", "startOffset": 28, "endOffset": 77}, {"referenceID": 82, "context": "ways to manipulate them, (Hart et al., 2008) introduces an intrinsic reward that motivates the system to explore changes in the perceptual space.", "startOffset": 25, "endOffset": 44}, {"referenceID": 7, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al.", "startOffset": 56, "endOffset": 83}, {"referenceID": 212, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al.", "startOffset": 108, "endOffset": 127}, {"referenceID": 0, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al., 2011; Lim and Auer, 2012), explores the policy space in an increasing order of complexity of learning each behavior.", "startOffset": 150, "endOffset": 189}, {"referenceID": 129, "context": "Taking into account this perspective, the approach from (Baranes and Oudeyer, 2012), similarly to POWERPLAY (Schmidhuber, 2011) and the approach from (Auer et al., 2011; Lim and Auer, 2012), explores the policy space in an increasing order of complexity of learning each behavior.", "startOffset": 150, "endOffset": 189}, {"referenceID": 9, "context": "In the case of problems formulated as MDPs several researchers have defined automatic measures to create options or other equivalent state-action abstractions, see (Barto and Mahadevan, 2003) for an early discussion.", "startOffset": 164, "endOffset": 191}, {"referenceID": 146, "context": "(Mannor et al., 2004) considered approaches such as online clustering of the state-action space using measures of connectivity, and variance of reward values.", "startOffset": 0, "endOffset": 21}, {"referenceID": 153, "context": "One such connectivity measure was introduced by (McGovern and Barto, 2001) where states that are present in multiple paths to the goals are considered sub-goals and an option is initiated to reach them.", "startOffset": 48, "endOffset": 74}, {"referenceID": 48, "context": "Even before the introduction of the options formalism, (Digney, 1998) introduced a method that would create skills based on reward gradients.", "startOffset": 55, "endOffset": 69}, {"referenceID": 83, "context": "(Hengst, 2002) exploited the factored structure of the problem to create the hierar-", "startOffset": 0, "endOffset": 14}, {"referenceID": 91, "context": "A more recent approach models the problem as a dynamic bayesian network that explains the relation between different tasks (Jonsson and Barto, 2006).", "startOffset": 123, "endOffset": 148}, {"referenceID": 1, "context": "By ensuring that neighbor states at the lower level are clustered in the higher level, it is possible to create efficient hierarchies of behavior (Bakker and Schmidhuber, 2004).", "startOffset": 146, "endOffset": 176}, {"referenceID": 189, "context": "An alternative perspective on the creation of a set of reusable macro actions is to exploit commonalities in collections of policies (Thrun et al., 1995; Pickett and Barto, 2002).", "startOffset": 133, "endOffset": 178}, {"referenceID": 7, "context": "Although coming from different perspectives: developmental robotics (Baranes and Oudeyer, 2012) and evolutionary development (Lehman and Stanley, 2011) argue that exploration in the behavior space might be more efficient and relevant than in the space of the parameters that generate that behavior.", "startOffset": 68, "endOffset": 95}, {"referenceID": 127, "context": "Although coming from different perspectives: developmental robotics (Baranes and Oudeyer, 2012) and evolutionary development (Lehman and Stanley, 2011) argue that exploration in the behavior space might be more efficient and relevant than in the space of the parameters that generate that behavior.", "startOffset": 125, "endOffset": 151}, {"referenceID": 127, "context": "The first perspective proposed by (Lehman and Stanley, 2011) is that many different genetic controller encodings might lead to very similar behaviors, and when considering also the morphological and environmental restrictions, the space of behaviors is much smaller than the space of controller encodings.", "startOffset": 34, "endOffset": 60}, {"referenceID": 167, "context": "The notion of diversity is not clear due to the redundancy in the control parameters, see (Mouret and Doncieux, 2011) for a discussion.", "startOffset": 90, "endOffset": 117}, {"referenceID": 69, "context": "and improve efficiency (Gilks and Berzuini, 2002).", "startOffset": 23, "endOffset": 49}, {"referenceID": 5, "context": "From a robot controller point of view we can see a similar idea as proposed by (Baranes and Oudeyer, 2010), see Figure 10.", "startOffset": 79, "endOffset": 106}, {"referenceID": 89, "context": "(Jamone et al., 2011) and (Rolf et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 197, "context": ", 2011) and (Rolf et al., 2011).", "startOffset": 12, "endOffset": 31}, {"referenceID": 212, "context": "We can refer again to the works of (Schmidhuber, 2011; Lim and Auer, 2012) and see that they also consider as criteria having access to the more diversified set of policies possible.", "startOffset": 35, "endOffset": 74}, {"referenceID": 129, "context": "We can refer again to the works of (Schmidhuber, 2011; Lim and Auer, 2012) and see that they also consider as criteria having access to the more diversified set of policies possible.", "startOffset": 35, "endOffset": 74}, {"referenceID": 58, "context": "For examples artificial development considers that the learning process is guided not only by the environment and the data it is collect but also by the \u201dgenetic information\u201d of the system (Elman, 1997; Lungarella et al., 2003).", "startOffset": 189, "endOffset": 227}, {"referenceID": 141, "context": "For examples artificial development considers that the learning process is guided not only by the environment and the data it is collect but also by the \u201dgenetic information\u201d of the system (Elman, 1997; Lungarella et al., 2003).", "startOffset": 189, "endOffset": 227}, {"referenceID": 3, "context": ", 2013) or by developing intrinsic rewards that focus attention to informative experiences (Baldassarre, 2011; Singh et al., 2010b), pre-dispositions to detect meaningful salient events, among many other aspects.", "startOffset": 91, "endOffset": 131}, {"referenceID": 228, "context": ", 2013) or by developing intrinsic rewards that focus attention to informative experiences (Baldassarre, 2011; Singh et al., 2010b), pre-dispositions to detect meaningful salient events, among many other aspects.", "startOffset": 91, "endOffset": 131}, {"referenceID": 228, "context": "Evolutionary models (Singh et al., 2010b) and recent studies in neurosciences (Gottlieb et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 75, "context": ", 2010b) and recent studies in neurosciences (Gottlieb et al., 2013) are starting to provide a more clear picture on if, and why, curiosity is an intrinsic drive in many animals.", "startOffset": 45, "endOffset": 68}, {"referenceID": 177, "context": "(Nicolescu and Mataric, 2003), information about the task solution (Calinon et al.", "startOffset": 0, "endOffset": 29}, {"referenceID": 30, "context": "(Nicolescu and Mataric, 2003), information about the task solution (Calinon et al., 2007), information about affordances (Ekvall and Kragic, 2004), information about the task representation (Lopes et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 57, "context": ", 2007), information about affordances (Ekvall and Kragic, 2004), information about the task representation (Lopes et al.", "startOffset": 39, "endOffset": 64}, {"referenceID": 136, "context": ", 2007), information about affordances (Ekvall and Kragic, 2004), information about the task representation (Lopes et al., 2007), among others.", "startOffset": 108, "endOffset": 128}, {"referenceID": 76, "context": "From (Grizou et al., 2013).", "startOffset": 5, "endOffset": 26}, {"referenceID": 50, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 51, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 177, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 19, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 131, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 49, "context": "It has been suggested that interactive learning, human-guided machine learning or learning with human in-the-loop, might be a new perspective on robot learning that combines the ideas of learning by demonstration, learning by exploration, active learning and tutor feedback (Dillmann et al., 2000; Dillmann et al., 2002; Fails and Olsen Jr, 2003; Nicolescu and Mataric, 2003; Breazeal et al., 2004; Lockerd and Breazeal, 2004; Dillmann, 2004).", "startOffset": 274, "endOffset": 442}, {"referenceID": 246, "context": "Approaches have considered extra reinforcement signals (Thomaz and Breazeal, 2008), action requests (Grollman and Jenkins, 2007a; Lopes et al.", "startOffset": 55, "endOffset": 82}, {"referenceID": 77, "context": "Approaches have considered extra reinforcement signals (Thomaz and Breazeal, 2008), action requests (Grollman and Jenkins, 2007a; Lopes et al., 2009b), disambiguation among actions (Chernova and Veloso, 2009), preferences among states (Mason and", "startOffset": 100, "endOffset": 150}, {"referenceID": 137, "context": "Approaches have considered extra reinforcement signals (Thomaz and Breazeal, 2008), action requests (Grollman and Jenkins, 2007a; Lopes et al., 2009b), disambiguation among actions (Chernova and Veloso, 2009), preferences among states (Mason and", "startOffset": 100, "endOffset": 150}, {"referenceID": 36, "context": ", 2009b), disambiguation among actions (Chernova and Veloso, 2009), preferences among states (Mason and", "startOffset": 39, "endOffset": 66}, {"referenceID": 93, "context": "Lopes, 2011), iterations between practice and user feedback sessions (Judah et al., 2010; Korupolu et al., 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 69, "endOffset": 112}, {"referenceID": 113, "context": "Lopes, 2011), iterations between practice and user feedback sessions (Judah et al., 2010; Korupolu et al., 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 69, "endOffset": 112}, {"referenceID": 106, "context": ", 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 61, "endOffset": 105}, {"referenceID": 107, "context": ", 2012) and choosing actions that maximize the user feedback (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 61, "endOffset": 105}, {"referenceID": 176, "context": "Most approaches will just ask to user whenever the information is needed (Nicolescu and Mataric, 2001) or when there is high uncertainty (Chernova and Veloso, 2009).", "startOffset": 73, "endOffset": 102}, {"referenceID": 36, "context": "Most approaches will just ask to user whenever the information is needed (Nicolescu and Mataric, 2001) or when there is high uncertainty (Chernova and Veloso, 2009).", "startOffset": 137, "endOffset": 164}, {"referenceID": 55, "context": "A more advanced situation considers making queries only when it is too risky to try experiments (Doshi et al., 2008).", "startOffset": 96, "endOffset": 116}, {"referenceID": 24, "context": "(Cakmak et al., 2010a) compare the results when the robot has the option of asking or not the teacher for feedback and in a more recent work they study how can the robot make different types of queries including: label, features and demonstrations (Cakmak and Thomaz, 2011; Cakmak and Thomaz, 2012).", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": ", 2010a) compare the results when the robot has the option of asking or not the teacher for feedback and in a more recent work they study how can the robot make different types of queries including: label, features and demonstrations (Cakmak and Thomaz, 2011; Cakmak and Thomaz, 2012).", "startOffset": 234, "endOffset": 284}, {"referenceID": 29, "context": ", 2010a) compare the results when the robot has the option of asking or not the teacher for feedback and in a more recent work they study how can the robot make different types of queries including: label, features and demonstrations (Cakmak and Thomaz, 2011; Cakmak and Thomaz, 2012).", "startOffset": 234, "endOffset": 284}, {"referenceID": 198, "context": "One justification for the need and expected gain of using such systems is discussed by (Ross and Bagnell, 2010).", "startOffset": 87, "endOffset": 111}, {"referenceID": 191, "context": "Such observation, as the authors refer, was already given by (Pomerleau, 1992) without a proof.", "startOffset": 61, "endOffset": 78}, {"referenceID": 181, "context": "See the work from (Ogata et al., 2003) for a study on this subject.", "startOffset": 18, "endOffset": 38}, {"referenceID": 62, "context": "certainty on the task being learned (Fong et al., 2003; Chao et al., 2010).", "startOffset": 36, "endOffset": 74}, {"referenceID": 35, "context": "certainty on the task being learned (Fong et al., 2003; Chao et al., 2010).", "startOffset": 36, "endOffset": 74}, {"referenceID": 118, "context": "face to provide information to the system during teleoperation (Kristensen et al., 1999), or it is the system that initiates questions based on perceptual saliency (Lutkebohle et al.", "startOffset": 63, "endOffset": 88}, {"referenceID": 142, "context": ", 1999), or it is the system that initiates questions based on perceptual saliency (Lutkebohle et al., 2009).", "startOffset": 83, "endOffset": 108}, {"referenceID": 65, "context": "to learn a better gesture classification the system is able to ask the user to provide more examples of a given class (Francke et al., 2007) even if for human-robot interfaces (Lee and Xu, 1996)).", "startOffset": 118, "endOffset": 140}, {"referenceID": 125, "context": ", 2007) even if for human-robot interfaces (Lee and Xu, 1996)).", "startOffset": 43, "endOffset": 61}, {"referenceID": 178, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 155, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 170, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 134, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 25, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 15, "context": "Several authors provided studies on how to model the different sources of information during social learning in artificial agents (Noble and Franks, 2002; Melo et al., 2007; Nehaniv, 2007; Lopes et al., 2009a; Cakmak et al., 2010b; Billing and Hellstr\u00f6m, 2010).", "startOffset": 130, "endOffset": 260}, {"referenceID": 193, "context": "and the learner chooses which one to observe (Price and Boutilier, 2003) where some of them might not even be cooperative (Shon et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 219, "context": "and the learner chooses which one to observe (Price and Boutilier, 2003) where some of them might not even be cooperative (Shon et al., 2007), and even choose between looking at a demonstrator or just learn by self-exploration (Nguyen et al.", "startOffset": 122, "endOffset": 141}, {"referenceID": 174, "context": ", 2007), and even choose between looking at a demonstrator or just learn by self-exploration (Nguyen et al., 2011).", "startOffset": 93, "endOffset": 114}, {"referenceID": 169, "context": "Humans change the way they act when they are demonstrating actions to others (Nagai and Rohlfing, 2009).", "startOffset": 77, "endOffset": 103}, {"referenceID": 193, "context": "Table 5: Interactive Learning Teachers Teacher Examples unaware (Price and Boutilier, 2003) batch (Argall et al.", "startOffset": 64, "endOffset": 91}, {"referenceID": 135, "context": "Table 5: Interactive Learning Teachers Teacher Examples unaware (Price and Boutilier, 2003) batch (Argall et al., 2009; Lopes et al., 2010; Calinon et al., 2007) active Section 5.", "startOffset": 98, "endOffset": 161}, {"referenceID": 30, "context": "Table 5: Interactive Learning Teachers Teacher Examples unaware (Price and Boutilier, 2003) batch (Argall et al., 2009; Lopes et al., 2010; Calinon et al., 2007) active Section 5.", "startOffset": 98, "endOffset": 161}, {"referenceID": 29, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al.", "startOffset": 11, "endOffset": 60}, {"referenceID": 26, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al.", "startOffset": 11, "endOffset": 60}, {"referenceID": 99, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al., 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 67, "endOffset": 141}, {"referenceID": 93, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al., 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 67, "endOffset": 141}, {"referenceID": 246, "context": "3 teaching (Cakmak and Thomaz, 2012; Cakmak and Lopes, 2012) mixed (Katagami and Yamada, 2000; Judah et al., 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 67, "endOffset": 141}, {"referenceID": 77, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 47, "endOffset": 121}, {"referenceID": 106, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 47, "endOffset": 121}, {"referenceID": 152, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al.", "startOffset": 47, "endOffset": 121}, {"referenceID": 76, "context": ", 2010; Thomaz and Breazeal, 2008) on-the-loop (Grollman and Jenkins, 2007a; Knox and Stone, 2009; Mason and Lopes, 2011) ambiguous protocols (Grizou et al., 2013)", "startOffset": 142, "endOffset": 163}, {"referenceID": 121, "context": "From (Kulick et al., 2013).", "startOffset": 5, "endOffset": 26}, {"referenceID": 247, "context": "shows that humans will change the way a task is executed, see (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012).", "startOffset": 62, "endOffset": 128}, {"referenceID": 97, "context": "shows that humans will change the way a task is executed, see (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012).", "startOffset": 62, "endOffset": 128}, {"referenceID": 105, "context": "shows that humans will change the way a task is executed, see (Thomaz and Cakmak, 2009; Kaochar et al., 2011; Knox et al., 2012).", "startOffset": 62, "endOffset": 128}, {"referenceID": 248, "context": "It is clear now that when teaching robots there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011).", "startOffset": 77, "endOffset": 147}, {"referenceID": 246, "context": "It is clear now that when teaching robots there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011).", "startOffset": 77, "endOffset": 147}, {"referenceID": 97, "context": "It is clear now that when teaching robots there is also a change in behavior (Thomaz et al., 2006; Thomaz and Breazeal, 2008; Kaochar et al., 2011).", "startOffset": 77, "endOffset": 147}, {"referenceID": 246, "context": "For instance, in the work of (Thomaz and Breazeal, 2008) the teachers frequently gave a reward to exploratory actions even if the signal was used as a standard reward.", "startOffset": 29, "endOffset": 56}, {"referenceID": 27, "context": "Also, in some problems we can define an optimal teaching sequence but humans do not behave according to those strategies (Cakmak and Thomaz, 2010).", "startOffset": 121, "endOffset": 146}, {"referenceID": 97, "context": "(Kaochar et al., 2011) developed a GUI to observe the teaching patterns of humans when teaching an electronic learner to achieve a complex sequential task ( e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 219, "context": "Several criteria have been proposed: game theoretic approaches (Shon et al., 2007), entropy (Lopes et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 137, "context": ", 2007), entropy (Lopes et al., 2009b; Melo and Lopes, 2010), query by committee (Judah et al.", "startOffset": 17, "endOffset": 60}, {"referenceID": 156, "context": ", 2007), entropy (Lopes et al., 2009b; Melo and Lopes, 2010), query by committee (Judah et al.", "startOffset": 17, "endOffset": 60}, {"referenceID": 92, "context": ", 2009b; Melo and Lopes, 2010), query by committee (Judah et al., 2012), membership queries (Melo and", "startOffset": 51, "endOffset": 71}, {"referenceID": 36, "context": "Lopes, 2013), maximum classifier uncertainty (Chernova and Veloso, 2009), expected myopic gain (Cohn et al.", "startOffset": 45, "endOffset": 72}, {"referenceID": 41, "context": "Lopes, 2013), maximum classifier uncertainty (Chernova and Veloso, 2009), expected myopic gain (Cohn et al., 2010; Cohn et al., 2011) and risk minimization (Doshi et al.", "startOffset": 95, "endOffset": 133}, {"referenceID": 39, "context": "Lopes, 2013), maximum classifier uncertainty (Chernova and Veloso, 2009), expected myopic gain (Cohn et al., 2010; Cohn et al., 2011) and risk minimization (Doshi et al.", "startOffset": 95, "endOffset": 133}, {"referenceID": 55, "context": ", 2011) and risk minimization (Doshi et al., 2008).", "startOffset": 30, "endOffset": 50}, {"referenceID": 137, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 41, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 39, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 156, "context": "Such idea as been applied in situations as different as navigation (Lopes et al., 2009b; Cohn et al., 2010; Cohn et al., 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 67, "endOffset": 148}, {"referenceID": 36, "context": ", 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al.", "startOffset": 53, "endOffset": 80}, {"referenceID": 137, "context": ", 2011; Melo and Lopes, 2010), simulated car driving (Chernova and Veloso, 2009) or object manipulation (Lopes et al., 2009b).", "startOffset": 104, "endOffset": 125}, {"referenceID": 36, "context": "(Chernova and Veloso, 2009) used support-vector machine classifiers to make queries to the teacher when it is uncertain about the action to execute as measured by the uncertainty of the classifier.", "startOffset": 0, "endOffset": 27}, {"referenceID": 156, "context": "To address this issue, (Melo and Lopes, 2010) proposed a method that computes a kernel based on MDP metrics (Taylor et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 244, "context": "To address this issue, (Melo and Lopes, 2010) proposed a method that computes a kernel based on MDP metrics (Taylor et al., 2008) that includes the information of the environment dynamics.", "startOffset": 108, "endOffset": 129}, {"referenceID": 164, "context": "They use the method proposed by (Montesano and Lopes, 2012) to make queries where there is lower confidence of the estimated policy.", "startOffset": 32, "endOffset": 59}, {"referenceID": 137, "context": "by (Lopes et al., 2009b).", "startOffset": 3, "endOffset": 24}, {"referenceID": 41, "context": "This work was latter extended by considering not just the uncertainty on the policy but the expected reduction in the global uncertainty (Cohn et al., 2010; Cohn et al., 2011).", "startOffset": 137, "endOffset": 175}, {"referenceID": 39, "context": "This work was latter extended by considering not just the uncertainty on the policy but the expected reduction in the global uncertainty (Cohn et al., 2010; Cohn et al., 2011).", "startOffset": 137, "endOffset": 175}, {"referenceID": 195, "context": "The teacher can directly ask about the reward value at a given location (Regan and Boutilier, 2011) and it has been shown that reward queries can be combined with action queries (Melo and Lopes, 2013).", "startOffset": 72, "endOffset": 99}, {"referenceID": 157, "context": "The teacher can directly ask about the reward value at a given location (Regan and Boutilier, 2011) and it has been shown that reward queries can be combined with action queries (Melo and Lopes, 2013).", "startOffset": 178, "endOffset": 200}, {"referenceID": 67, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 34, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 18, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 256, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 21, "context": "This problem of preference elicitation has been addressed in several domains (F\u00fcrnkranz and H\u00fcllermeier, 2010; Chajewska et al., 2000; Braziunas and Boutilier, 2005; Viappiani and Boutilier, 2010; Brochu et al., 2007).", "startOffset": 77, "endOffset": 217}, {"referenceID": 106, "context": "The TAMMER framework, and its extensions, considers how signals from humans can speed up exploration and learning in reinforcement learning tasks (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 146, "endOffset": 190}, {"referenceID": 107, "context": "The TAMMER framework, and its extensions, considers how signals from humans can speed up exploration and learning in reinforcement learning tasks (Knox and Stone, 2009; Knox and Stone, 2010).", "startOffset": 146, "endOffset": 190}, {"referenceID": 106, "context": "Knox (Knox and Stone, 2009; Knox and Stone, 2010) presented the initial framework where the agent learns to predict the human feedback and then selects actions to maximize the expected reward from the human.", "startOffset": 5, "endOffset": 49}, {"referenceID": 107, "context": "Knox (Knox and Stone, 2009; Knox and Stone, 2010) presented the initial framework where the agent learns to predict the human feedback and then selects actions to maximize the expected reward from the human.", "startOffset": 5, "endOffset": 49}, {"referenceID": 108, "context": "Recently this process was improved to allow both processes to occur simultaneously (Knox and Stone, 2012).", "startOffset": 83, "endOffset": 105}, {"referenceID": 262, "context": "(Zhang et al., 2009) introduced a method were the teacher is able to provide extra rewards to change the behavior", "startOffset": 0, "endOffset": 20}, {"referenceID": 147, "context": "Other approaches considered that the learner can train by self-exploration and have several periods where the teacher is able to criticize its progress (Manoonpong et al., 2010; Judah et al., 2010).", "startOffset": 152, "endOffset": 197}, {"referenceID": 93, "context": "Other approaches considered that the learner can train by self-exploration and have several periods where the teacher is able to criticize its progress (Manoonpong et al., 2010; Judah et al., 2010).", "startOffset": 152, "endOffset": 197}, {"referenceID": 77, "context": "For instance, in the dogged learning approach suggested in (Grollman and Jenkins, 2007a; Grollman and Jenkins, 2007b; Grollman and Jenkins, 2008) an AIBO robot is teleoperated and learns a policy from the user to dribble a ball towards a goal.", "startOffset": 59, "endOffset": 145}, {"referenceID": 78, "context": "For instance, in the dogged learning approach suggested in (Grollman and Jenkins, 2007a; Grollman and Jenkins, 2007b; Grollman and Jenkins, 2008) an AIBO robot is teleoperated and learns a policy from the user to dribble a ball towards a goal.", "startOffset": 59, "endOffset": 145}, {"referenceID": 79, "context": "For instance, in the dogged learning approach suggested in (Grollman and Jenkins, 2007a; Grollman and Jenkins, 2007b; Grollman and Jenkins, 2008) an AIBO robot is teleoperated and learns a policy from the user to dribble a ball towards a goal.", "startOffset": 59, "endOffset": 145}, {"referenceID": 152, "context": "A similar approach was followed in the work of (Mason and Lopes, 2011).", "startOffset": 47, "endOffset": 70}, {"referenceID": 204, "context": "be made where the corrections are provided directly by moving the robot arm (Sauser et al., 2011).", "startOffset": 76, "endOffset": 97}, {"referenceID": 40, "context": "asynchronous behavior exist then the agent must decide how to act while waiting for the feedback (Cohn et al., 2012).", "startOffset": 97, "endOffset": 116}, {"referenceID": 103, "context": "Several of these works fall under the learning from communication framework (Klingspor et al., 1997), where a shared understanding between the robot and the teacher is fundamental to allow good interactive learning sessions.", "startOffset": 76, "endOffset": 100}, {"referenceID": 161, "context": "The system in (Mohammad and Nishida, 2010) automatically learns different interaction protocols for navigation tasks where the robot learns the actions it should make and which gestures correspond to those actions.", "startOffset": 14, "endOffset": 42}, {"referenceID": 132, "context": "In (Lopes et al., 2011; Grizou et al., 2013) the authors introduce a new algorithm for inverse reinforcement learning under multiple instructions with unknown symbols.", "startOffset": 3, "endOffset": 44}, {"referenceID": 76, "context": "In (Lopes et al., 2011; Grizou et al., 2013) the authors introduce a new algorithm for inverse reinforcement learning under multiple instructions with unknown symbols.", "startOffset": 3, "endOffset": 44}, {"referenceID": 114, "context": "An early work consider such process in isolation and considered that learning the meaning of communication can be simplified by using the expectation from the already known task model (Kozima and Yano, 2001).", "startOffset": 184, "endOffset": 207}, {"referenceID": 124, "context": "Other works, such as (Lauria et al., 2002; Kollar et al., 2010), consider the case of learning new instructions and guidance signals for already known tasks, thus providing more efficient commands for instructing the robot.", "startOffset": 21, "endOffset": 63}, {"referenceID": 110, "context": "Other works, such as (Lauria et al., 2002; Kollar et al., 2010), consider the case of learning new instructions and guidance signals for already known tasks, thus providing more efficient commands for instructing the robot.", "startOffset": 21, "endOffset": 63}, {"referenceID": 121, "context": "In (Kulick et al., 2013) the authors take an active learning approach allowing the", "startOffset": 3, "endOffset": 24}], "year": 2014, "abstractText": "In this survey we present different approaches that allow an intelligent agent to explore autonomous its environment to gather information and learn multiple tasks. Different communities proposed different solutions, that are in many cases, similar and/or complementary. These solutions include active learning, exploration/exploitation, online-learning and social learning. The common aspect of all these approaches is that it is the agent to selects and decides what information to gather next. Applications for these approaches already include tutoring systems, autonomous grasping learning, navigation and mapping and human-robot interaction. We discuss how these approaches are related, explaining their similarities and their differences in terms of problem assumptions and metrics of success. We consider that such an integrated discussion will improve inter-disciplinary research and applications.1", "creator": "TeX"}}}