{"id": "1403.3741", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2014", "title": "Near-optimal Reinforcement Learning in Factored MDPs", "abstract": "Any learning algorithm over Markov decision processes (MDPs) will have worst-case regret $\\Omega(\\sqrt{SAT})$ where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces. In many settings of interest $S$ and $A$ may be so huge that it is impossible to guarantee good performance for an arbitrary MDP on any practical timeframe $T$. We show that, if we know the true system can be represented as a \\emph{factored} MDP, we can obtain regret bounds which scale polynomially in the number of \\emph{parameters} of the MDP, which may be exponentially smaller than $S$ or $A$ in terms of an estimate for the real state (and the actual state of its state). The number of these dimensions is given by the \\mathbb{A} parameter that gives us an approximate value of $\\emph{parameters} at its surface in an exponential way in the number of $\\emph{parameters} of the MDP, which is the real state of the \\(proj_{s}.\\} $\\emph{parameters} \\) where $\\Omega(\\sqrt{SAT})$ is the time in which the actual state of the \\(proj_{s}.\\} \\) in the $\\frac{A} \\) is the true system, with the \\frac{A} \\) not shown. In other words, $S$ is just the time in which the actual state of the \\(proj_{s}.\\} \\) is the true system. We also show that this is the condition that all the \\(\\emph{parameters} is a constant and that the \\(proj_{s}.\\} \\) is the mean and the \\(proj_{s}.\\} \\) is the state of the \\(proj_{s}.\\} \\) and $S$ is the average state of the \\(proj_{s}.\\} The first value, $\\Omega(\\sqrt{SAT})$, is the correct value, \\(\\Omega(\\sqrt{SAT})$. \\) is the true state of the \\(proj_{s}.\\} \\) and $\\Omega(\\sqrt{SAT})$. We can use a linear equation to apply it.\n\nIn fact, if we consider the $\\sum{", "histories": [["v1", "Sat, 15 Mar 2014 01:56:02 GMT  (18kb)", "http://arxiv.org/abs/1403.3741v1", null], ["v2", "Fri, 6 Jun 2014 23:17:54 GMT  (18kb)", "http://arxiv.org/abs/1403.3741v2", null], ["v3", "Fri, 31 Oct 2014 23:34:32 GMT  (21kb)", "http://arxiv.org/abs/1403.3741v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian osband", "benjamin van roy"], "accepted": true, "id": "1403.3741"}, "pdf": {"name": "1403.3741.pdf", "metadata": {"source": "CRF", "title": "Near-optimal Regret Bounds for Reinforcement Learning in Factored MDPs", "authors": ["Ian Osband", "Benjamin Van Roy"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 3.\n37 41\nv1 [\nst at\n.M L\n] 1\n5 M\nar 2\nAny learning algorithm over Markov decision processes (MDPs) will have worst-case regret \u2126( \u221a SAT ) where T is the elapsed time and S and A are the cardinalities of the state and action spaces. In many settings of interest S and A may be so huge that it is impossible to guarantee good performance for an arbitrary MDP on any practical timeframe T . We show that, if we know the true system can be represented as a factored MDP, we can obtain regret bounds which scale polynomially in the number of parameters of the MDP, which may be exponentially smaller than S or A. Assuming an algorithm for approximate planning and knowledge of the graphical structure of the underlying MDP, we demonstrate that posterior sampling reinforcement learning (PSRL) and an algorithm based upon optimism in the face of uncertainty (UCRL-Factored) both satisfy near-optimal regret bounds."}, {"heading": "1 Introduction", "text": "The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2]. The environment is modeled as a Markov decision process (MDP) but the agent is uncertain of the true dynamics of the MDP. The agent must plan actions to maximize rewards based upon its imperfect knowledge, but also learns about its environment through experience. Efficient reinforcement learning manages this tradeoff between exploration and exploitation in such a way that the deviation from the optimal policy given perfect information is controlled.\nFactored MDPs [3] allow us to represent large structured MDPs compactly. A state is described by a selection of state variables, whose transitions can be represented by a dynamic Bayesian network (DBN) [4]. This is particularly beneficial when the transition of a state variable depends only on a small subset of other variables. For example, consider a large production line with m machines in sequence, each with K possible states. We write s = (s1, .., sm) with each si \u2208 {1, ..,K}. It may be that, over a single time-step, machine i can only be influenced by the states of i \u2212 1, i and i + 1. If so, any single si can still influence the entire system eventually, but the dimensionality of the learning problem is reduced exponentially from O(Km) to O(mK3).\nThere has been some success in establishing efficient reinforcement learning in factored MDPs (FMDPs). Kearns and Koller extend the E3 algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity. There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9]. These algorithms require the graph structure of the FMDP as a fixed prior. Some algorithms do seek to learn this structure from experience [10], but we will assume this structure is known.\nAnother form of efficiency guarantees for reinforcement learning are given by regret bounds. These bound the difference in accumulated rewards of a learning algorithm and the optimal policy over T steps [11]. Regret bounds naturally give rise to PAC bounds as a corollary, but also give a guarantee on the algorithm\u2019s performance during the learning phase. Jaksch et al. [12] present UCRL2, which attains near-optimal regret of O\u0303(S \u221a AT ) with high probability. Recently Osband et al. [13] analyze PSRL, which also provides bounds on the expected regret of O\u0303(S \u221a AT ). Unlike the algorithms mentioned so far, PSRL does not use \u201coptimism in the face of uncertainty\u201d (OFU) to guide exploration, but instead the variance in posterior sampling. There has been no algorithm with efficient regret bounds in FMDPs so far.\nWe present two algorithms, PSRL and UCRL-Factored, with efficient regret bounds for FMDPs. These algorithms are described fully in Section 6. UCRL-Factored is a minor modification to UCRL2 that allows us to exploit the DBN structure while PSRL is unchanged. UCRL-Factored is guided by the OFU principle whereas PSRL is guided by posterior (also known as Thompson) sampling [14]. Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13]. We believe that posterior sampling will be simpler to implement, computationally cheaper and statistically more efficient than the optimistic alternative in FMDPs as well.\nBoth algorithms make use of approximate FMDP planner in internal steps. However, even where an FMDP is able to represented concisely, solving for the optimal policy may still be intractable in the most general case [18]. Our focus in this paper is upon the statistical aspect of the learning problem and like earlier discussions [5] we do not specify which computational methods are used. Our results serve as a reduction of the reinforcement learning problem to finding an approximate solution for a given FMDP. In many cases of interest, effective approximate planning methods for FMDPs do exist. Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].\nWe believe that dimensionality reduction in large MDPs is essential for practical reinforcement learning. Factored MDPs are an approach with successful applications in many fields [3] but they are not the only one. There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as Ho\u0308der continuous [23] or linear quadratic control [24]. These results are interesting, but each have some undesirable properties, the former has regret bounds which approach O(T ) for high dimensions and the latter retains an exponential dependence on the dimension. Perhaps the most popular approach in the literature is to assume the value function can be well-approximated by a low-dimensional (usually linear) representation of basis functions. Value-based approaches typically struggle to plan efficient exploration and so cannot obtain efficient learning guarantees, although there has been interesting progress in this field as well [25]."}, {"heading": "2 Problem formulation", "text": "We consider the problem of learning to optimize a random finite horizon MDP M = (S,A, RM , PM , \u03c4, \u03c1) in repeated finite episodes of interaction. This is the same formulation as earlier work [13], which we reproduce here for completeness. S is the state space, A is the action space, RM (s, a) is a probability distribution over R when selecting action a while in state s, PM (s\u2032|s, a) is the probability of transitioning to state s\u2032 if action a is selected while at state s, \u03c4 is the time horizon, and \u03c1 the initial state distribution. We define the MDP and all other random variables we will consider with respect to a probability space (\u2126,F ,P).\nA deterministic policy \u00b5 is a function mapping each state s \u2208 S and i = 1, . . . , \u03c4 to an action a \u2208 A. For each MDP M = (S,A, RM , PM , \u03c4, \u03c1) and policy \u00b5, we define a value function\nV M\u00b5,i(s) := EM,\u00b5\n\n\n\u03c4 \u2211\nj=i\nR M (sj , aj)\n\u2223 \u2223 \u2223 si = s\n\n ,\nwhere R M (s, a) denotes the expected reward realized when action a is selected while in state s, and the subscripts of the expectation operator indicate that aj = \u00b5(sj , j), and sj+1 \u223c PM (\u00b7|sj , aj) for j = i, . . . , \u03c4 . A policy \u00b5 is said to be optimal for MDP M if V M\u00b5,i(s) = max\u00b5\u2032 V M \u00b5\u2032,i(s) for all s \u2208 S and i = 1, . . . , \u03c4 . We will associate with each MDP M a policy \u00b5M that is optimal for M . The reinforcement learning agent interacts with the MDP over episodes that begin at times tk = (k\u2212 1)\u03c4 +1, k = 1, 2, . . .. At each time t, the agent selects an action at, observes a scalar reward rt, and then transitions to st+1. If an agent follows a policy \u00b5 then when in state s at time t during episode k, it selects an action at = \u00b5(s, t\u2212 tk). Let Ht = (s1, a1, r1, . . . , st\u22121, at\u22121, rt\u22121) denote the history of observations made prior to time t. A reinforcement learning algorithm is a deterministic sequence {\u03c0k|k = 1, 2, . . .} of functions, each mapping Htk to a probability distribution \u03c0k(Htk) over policies. At the start of the kth episode, the algorithm samples a policy \u00b5k from the distribution \u03c0k(Htk). The algorithm then selects actions at = \u00b5k(st, t\u2212 tk) at times t during the kth episode.\nWe define the regret incurred by a reinforcement learning algorithm \u03c0 up to time T to be\nRegret(T, \u03c0,M\u2217) :=\n\u2308T/\u03c4\u2309 \u2211\nk=1\n\u2206k,\nwhere \u2206k denotes regret over the kth episode, defined with respect to the MDP M \u2217 by\n\u2206k := \u2211\nS\n\u03c1(s)(V M \u2217 \u00b5\u2217,1(s)\u2212 V M \u2217 \u00b5k,1(s))\nwith \u00b5\u2217 = \u00b5M \u2217 and \u00b5k \u223c \u03c0k(Htk). Note that regret is not deterministic since it can depend on the random MDP M\u2217, the algorithm\u2019s internal random sampling and, through the history Htk , on previous random transitions and random rewards. We will assess and compare algorithm performance in terms of regret and its expectation."}, {"heading": "3 Factored MDPs", "text": "To formalize our definition of a factored MDP we introduce some notation common to the literature [9].\nDefinition 1 ( Scope operation for factored sets X = X1 \u00d7 ..\u00d7Xn). For any subset of indices Z \u2286 {1, 2, .., n} let us define the scope set X [Z] := \u2297\ni\u2208Z\nXi. Further, for any x \u2208 X define\nthe scope variable x[Z] \u2208 X [Z] to be the value of the variables xi \u2208 Xi with indices i \u2208 Z. For singleton sets Z we will write x[i] for x[{i}] in the natural way.\nLet PX ,Y be the set of functions mapping elements of a finite set X to probability mass functions over a finite set Y. PC,\u03c3X ,R will denote the set of functions mapping elements of a finite set X to \u03c3-sub gaussian probability measures over (R,B(R)) with mean bounded in [0, C]. We will consider factored reward and factored transition functions which are drawn from within these families.\nDefinition 2 ( Factored reward functions R \u2208 R \u2286 PC,\u03c3X ,R). The reward function class R is factored over X = X1 \u00d7 .. \u00d7 Xn with scopes Z1, ..Zl if and only if, for all R \u2208 R, x \u2208 X there exist functions {Ri \u2208 PC,\u03c3X [Zi],R} l i=1 such that,\nE[r] = l \u2211\ni=1\nE [ ri ]\nwhere the observed reward r \u223c R(x) is equal to \u2211li=1 ri with each ri \u223c Ri(x[Zi]) and individually observed.\nDefinition 3 ( Factored transition functions P \u2208 P \u2286 PX ,S ). The transition function class P is factored over X = X1 \u00d7 ..\u00d7 Xn and S = S1 \u00d7 .. \u00d7 Sm with scopes Z1, ..Zm if and only if, for all P \u2208 P , x \u2208 X , s \u2208 S there exist some {Pi \u2208 PX [Zi],Si}mi=1 such that,\nP (s|x) = m \u220f\ni=1\nPi\n(\ns[i]\n\u2223 \u2223 \u2223 \u2223 x[Zi] )\nA factored MDP (FMDP) is then defined to be an MDP with both factored rewards and factored transition functions. If we write X = S \u00d7A, then an FMDP is fully characterized by the tuple\nM = ( {Si}mi=1; {Xi}ni=1; {ZRi }li=1; {Ri}li=1; {ZPi }mi=1; {Pi}mi=1; \u03c4 ; \u03c1 ) ,\nwhere ZRi and Z P i are the scopes for the reward and transition functions respectively \u2286 {1, .., n} which refer to Xi. We assume that the size of all scopes |Zi| \u2264 \u03b6 \u226a n and factors |Xi| \u2264 K so that the domains of Ri and Pi are of size at most K\u03b6 ."}, {"heading": "4 Results", "text": "We present two algorithms, PSRL and UCRL-Factored with efficient regret bounds over factored MDPs. PSRL is guided by posterior sampling while UCRL-Factored uses optimism in the face of uncertainty. Full details of these algorithms are available in Section 6.\nOur first result shows that we can bound the expected regret of PSRL.\nTheorem 1 (Expected regret for PSRL in factored MDPs). Let M\u2217 be factored with graph structure G = (\n{Si}mi=1; {Xi}ni=1; {ZRi }li=1; {ZPi }mi=1; \u03c4 )\n. If \u03c6 is the distribution of M\u2217 and \u03a8 is the span of the optimal value function then we can bound the regret of PSRL:\nE [ Regret(T, \u03c0PS\u03c4 ,M \u2217) ] \u2264 4 + 2 \u221a T + l \u2211\ni=1\n{\n4(\u03c4C|X [ZRi ]|+ 1) + 8\u03c3 \u221a 2|X [ZRi ]|T log (4l|X [ZRi ]|kT ) }\n+E[\u03a8] ( 1 + 4\nT \u2212 4\n) m \u2211\nj=1\n{\n4(\u03c4 |X [ZPj ]|+ 1) + 8 \u221a 2|X [ZPj ]||Sj |T log ( 4m|X [ZPj ]|kT )\n}\n(1)\nWe also show that using UCRL-Factored in a factored MDP we can bound the regret with high probability.\nTheorem 2 (High probability regret for UCRL-Factored in factored MDPs). Let M\u2217 be factored with graph structure G = (\n{Si}mi=1; {Xi}ni=1; {ZRi }li=1; {ZPi }mi=1; \u03c4 )\n. If D is the diameter of M\u2217, then for any M\u2217 can bound the regret of UCRL-Factored:\nRegret(T, \u03c0UC\u03c4 ,M \u2217) \u2264 CD\n\u221a 2T log(6/\u03b4) + 2 \u221a T + l \u2211\ni=1\n{\n4(\u03c4C|X [ZRi ]|+ 1) + 8\u03c3 \u221a 2|X [ZRi ]|T log (12l|X [ZRi ]|kT/\u03b4) }\n+ CD\nm \u2211\nj=1\n{\n4(\u03c4 |X [ZPj ]|+ 1) + 8 \u221a 2|X [ZPj ]||Sj |T log ( 12m|X [ZPj ]|kT/\u03b4 )\n}\n(2)\nwith probability at least 1\u2212 \u03b4\nFor clarity, we present a symmetric problem instance for which we can produce a cleaner single-term upper bound. Let Q be shorthand for the structure G such that l + 1 = m, C = \u03c3 = 1, |Si| = |Xi| = K and |ZRi | = |ZPi | = \u03b6 for all suitable i and write J = K\u03b6 . In this case \u03a8, D \u2264 \u03c4 trivially.\nCorollary 1 (Clean bounds for PSRL in a symmetric problem). For an MDP with structure Q, if \u03c6 is the distribution of M\u2217 then we can bound the regret of PSRL:\nE [ Regret(T, \u03c0PS\u03c4 ,M \u2217) ] \u2264 15m\u03c4 \u221a JKT log(2mJT ) (3)\nCorollary 2 (Clean bounds for UCRL-Factored in a symmetric problem). For an MDP with structure Q, then for any M\u2217 we can bound the regret of UCRL-Factored:\nRegret(T, \u03c0UC\u03c4 ,M \u2217) \u2264 15m\u03c4\n\u221a\nJKT log(12mJT/\u03b4) (4)\nwith probability at least 1\u2212 \u03b4.\nThese simply follow from the theorems above with loose upper bounds upon constant and logarithmic factors. The derivations are available in the Appendix B. Both algorithms satisfy bounds of O\u0303(\u03c4m \u221a JKT ) whereas a Q-naive algorithm gives O\u0303(\u03c4 \u221a Jm/\u03b6KmT ). We see that these new bounds are improved exponentially. These results are near optimal since for a factored MDP with m independent components with S states and A actions we obtain regret bounds O\u0303(mS \u221a AT ), which is close to the lower bound of \u2126(m \u221a SAT )."}, {"heading": "4.1 Interpretting regret bounds", "text": "The bounds for PSRL and UCRL-Factored are qualitatively similar and share much of the same analysis. For each algorithm, the regret is O\u0303 ( \u039e \u2211m\nj=1\n\u221a |X [ZPj ]||Sj |T ) where \u039e is a measure of MDP connectedness, expected\nspan E[\u03a8] for PSRL and scaled diameter CD for UCRL-Factored. The span of an MDP is defined \u03a8(M\u2217) := maxs,s\u2032\u2208S{VM \u2217 \u00b5\u2217,1(s) \u2212 VM \u2217 \u00b5\u2217,1(s \u2032)} which is the maximum difference in expected value of any two states under the optimal policy. The diameter of an MDP D(M\u2217) = maxs6=s\u2032 min\u00b5 T \u00b5 s\u2192s\u2032 , where T \u00b5 s\u2192s\u2032 is the expected number of steps to get from s to s\n\u2032 under policy \u00b5. It is always the case that \u03a8(M) \u2264 CD(M), otherwise one might improve the optimal policy from s\u2032 to follow simply by taking the fastest policy to the s with highest value. In some cases the span may be exponentially smaller than the diameter. In this sense PSRL satisfies a tighter bound.\nHowever, UCRL-Factored has stronger probabilistic guarantees than PSRL since its bounds hold with high probability for any MDP M\u2217 not just in expectation. There is an optimistic algorithm REGAL [26] which formally\nreplaces the UCRL2 D with \u03a8 and retains the high probability guarantees. However, no practical implementation of that algorithm exists, even when given access to an MDP planner. An analogous extension to the analysis of REGAL-Factored is possible.\nWe should also note that UCRL2 was designed to obtain regret bounds even in MDPs without episodic reset. This is accomplished by imposing artificial episodes which end whenever the number of visits to a state-action pair is doubled [12]. Using a similar modification, it is possible to extend UCRL-Factored to this setting without trouble and retain similar regret bounds. However, this doubling trick in PSRL does not retain provable regret bounds, since the episode length is no longer independent of the sampled MDP. Nevertheless, there has been good empirical performance using this method for non-factored MDPs without episodic reset in simulation [13]."}, {"heading": "5 Confidence sets", "text": "Our analysis will rely upon the construction of confidence sets based around the empirical estimates for the underlying reward and transition functions. These confidence sets are chosen so that at the beginning of any episode k the true and sampled functions are contained within the confidence set with high probability. We will then bound the deviation between the true function and elements in the confidence set by the maximal deviation within the confidence set. This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].\nConsider a family of functions F \u2286 MX ,(Y,\u03a3Y) which takes x \u2208 X to a probability distribution over (Y,\u03a3Y) measurable space. We will write this as MX ,Y unless we wish to stress the dependence on a particular \u03c3-algebra which is not obvious.\nDefinition 4 (Set widths). Let X be a finite set, and let (Y,\u03a3Y) be a measurable space. The width of a set F \u2208 MX ,Y at x \u2208 X with respect to a norm \u2016 \u00b7 \u2016 is\nwF (x) := sup f,f\u2208F\n\u2016(f \u2212 f)(x)\u2016\nOur confidence set sequence {Ft \u2286 F : t \u2208 N} is initialized with a set F . We adapt our confidence set to the observations yt \u2208 Y which are drawn from the true function f\u2217 \u2208 F at measurement points xt \u2208 X so that yt \u223c f\u2217(xt). Each confidence set is then centered around an empirical estimate f\u0302t \u2208 MX ,Y at time t, defined by\nf\u0302t(x) = 1\nnt(x)\n\u2211\n\u03c4<t:x\u03c4=x\n\u03b4y\u03c4 ,\nwhere nt(x) is the number of time x appears in (x1, .., xt\u22121) and \u03b4yt is the probability mass function over Y that assigns all probability to the outcome yt. If at any time t, nt(x) = 0 then we will let f\u0302t(x) be any arbitrary function \u2208 MX ,Y . Our sequence of confidence sets depends on our choice of norm \u2016 \u00b7 \u2016 and a non-decreasing sequence {dt : t \u2208 N}. For each t, the confidence set is defined by:\nFt = Ft(\u2016 \u00b7 \u2016, xt\u221211 , dt) := { f \u2208 F \u2223 \u2223 \u2223\n\u2223\n\u2016(f \u2212 f\u0302t)(xi)\u2016 \u2264 \u221a\ndt nt(xi)\n\u2200i = 1, .., t\u2212 1 } .\nWhere xt\u221211 is shorthand for (x1, .., xt\u22121) and we interpret nt(xi) = 0 as a null constraint which is satisfied \u2200f \u2208 F . The following result shows that we can bound the sum of confidence widths through time.\nTheorem 3 (Bounding the sum of widths). Let us write Fk for Ftk and associate times within episodes of length \u03c4 , t = tk + i for i = 1, .., \u03c4 and T = L\u00d7 \u03c4 . For all finite sets X , measurable spaces (Y,\u03a3Y), function classes F \u2286 MX ,Y with uniformly bounded widths wF (x) \u2264 CF \u2200x \u2208 X and non-decreasing sequences {dt : t \u2208 N}:\nL \u2211\nk=1\n\u03c4 \u2211\ni=1\nwFk(xtk+i) \u2264 4 ( \u03c4CF |X |+ 1 ) + 4 \u221a 2dT |X |T (5)\nProof. The proof follows from elementary considerations of nt(x) and the pigeonhole principle. We omit the details for brevity but refer the reader to Appendix A for a full derivation."}, {"heading": "6 Algorithms", "text": "Both algorithms require prior knowledge of G = ( {Si}mi=1; {Xi}ni=1; {ZRi }li=1; {ZPi }mi=1; \u03c4 )\n, the graphical structure of the FMDP. They also assume access to a \u201cblack box\u201d that performs approximate dynamic programming for FMDPs. PSRL requires \u0393(\u00b7, \u01eb) which takes a single MDP M and output an \u01eb-optimal policy for M . UCRLFactored requires \u0393\u0303(\u00b7, \u01eb) which takes in a family of MDPs M and outputs an \u01eb-optimal with respect to the most optimistic M \u2208 M. In general, it will be much more difficult to obtain an approximate solver \u0393\u0303 than \u0393.\nPSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior \u03c6. UCRL-Factored is essentially UCRL2 [12] modified to exploit G in graph and episodic structure. We write Rit(dRit ) and Pjt (d Pj t ) as shorthand for these confidence sets Rit(|E[\u00b7]|, xt\u221211 [ZRi ], dRit ) and P it (\u2016 \u00b7 \u20161, xt\u221211 [ZPj ], d Pj t ) generated from initial sets Ri1 = PC,\u03c3X [ZR i ],R and Pj1 = PX [ZPj ],Sj .\nAlgorithm 1 PSRL (Posterior Sampling)\n1: Input: Prior \u03c6 encoding G, t = 1 2: for episodes k = 1, 2, .. do 3: sample Mk \u223c \u03c6(\u00b7|Ht) 4: compute \u00b5k = \u0393(Mk, \u221a\n\u03c4/k) 5: for timesteps j = 1, .., \u03c4 do 6: sample and apply at = \u00b5k(st, j) 7: observe r1t , .., r l t and s 1 t+1, .., s m t+1 8: t = t+ 1 9: end for\n10: end for\nAlgorithm 2 UCRL-Factored (Optimism)\n1: Input: Graph structure G, confidence \u03b4, t = 1 2: for episodes k = 1, 2, .. do 3: dRit = 4\u03c3 2 log ( 4l|X [ZRi ]|k/\u03b4 ) for i = 1, .., l 4: d Pj t = 4|Sj | log ( 4m|X [ZPj ]|k/\u03b4 ) for j = 1, ..,m 5: Mk = {M |G, Ri \u2208 Rit(dRit ), Pj \u2208 Pjt (d Pj t ) \u2200i, j} 6: compute \u00b5k = \u0393\u0303(Mk, \u221a\n\u03c4/k) 7: for timesteps u = 1, .., \u03c4 do 8: sample and apply at = \u00b5k(st, u) 9: observe r1t , .., r l t and s 1 t+1, .., s m t+1\n10: t = t+ 1 11: end for\n12: end for\nThe parameters dRit and d Pj t are chosen to satisfy concentration inequalities so that the true MDP M \u2217 lies within Mk for all k with high probability. Although PSRL makes no mention of confidence sets, Mk will also be useful in the analysis of PSRL."}, {"heading": "7 Analysis", "text": "We will now piece together the necessary analysis for our main results. First we recap the analysis of PSRL and UCRL2 which allow us to the regret to the bellman error. Next we show that, for factored MDPs, it is possible to bound this estimation error by the error in each factored component separately. From here we will use concentration inequalities upon the individual factors to show that, with high probability, the true MDP M\u2217 lies within Mk for all k. The final results will then be obtained through an application of Theorem 3."}, {"heading": "7.1 From regret to Bellman error", "text": "A key difficulty in providing regret bounds for reinforcement learning is that it depends upon the rewards of the optimal policy \u00b5\u2217. For many reinforcement learning algorithms there is no clean way to relate the unknown optimal policy to the states and actions observed by the agent. Using the OFU principle, we can guarantee with high probability that the optimal rewards of the true MDP are upper bounded by the optimal rewards of the optimistic MDP [11]. In the case of posterior sampling, we make use of the posterior sampling lemma [17]\nLemma 1 (Posterior Sampling). If \u03c6 is the distribution of M\u2217 then, for any \u03c3(Htk)-measurable function g,\nE[g(M\u2217)|Htk ] = E[g(Mk)|Htk ]. (6)\nNote that taking the expectation of (6) shows E[g(M\u2217)] = E[g(Mk)] through the tower property. We introduce the Bellman operator T M\u00b5 , which for any MDP M = (S,A, RM , PM , \u03c4, \u03c1), stationary policy \u00b5 : S \u2192 A and value function V : S \u2192 R, is defined by\nT M\u00b5 V (s) := R M (s, \u00b5(s)) + \u2211\ns\u2032\u2208S\nPM (s\u2032|s, \u00b5(s))V (s\u2032).\nThis returns the expected value of state s where we follow the policy \u00b5 under the laws ofM , for one time step. The following lemma gives a concise form for the dynamic programming paradigm in terms of the Bellman operator.\nLemma 2 (Dynamic programming equation). For any MDP M = (S,A, RM , PM , \u03c4, \u03c1) and policy \u00b5 : S \u00d7 {1, . . . , \u03c4} \u2192 A, the value functions V M\u00b5 satisfy\nV M\u00b5,i = T M\u00b5(\u00b7,i)V M\u00b5,i+1 (7)\nfor i = 1 . . . \u03c4 , with V M\u00b5,\u03c4+1 := 0.\nIn order to streamline our common analysis of PSRL and UCRL2 we will let M\u0303k refer generally to either the sampled MDP used in PSRL or the optimistic MDP chosen from Mk with associated near-optimal policy \u00b5\u0303k). We will streamline our discussion of PM , RM , V M\u00b5,i and T M\u00b5 by simply writing \u2217 in place of M\u2217 or \u00b5\u2217 and k in place of M\u0303k or \u00b5\u0303k where appropriate; for example V \u2217 k,i := V M\u2217 \u00b5\u0303k,i . We will also write xk,i := (stk+i, \u00b5k(stk+i)).\nWe now break down the regret by adding and subtracting the imagined near optimal reward of policy \u00b5\u0303K , which is known to the agent. For clarity of analysis we consider only the case of \u03c1(s\u2032) = 1{s\u2032 = s} but this changes nothing for our consideration of finite S.\n\u2206k = V \u2217 \u2217,1(s)\u2212 V \u2217k,1(s) =\n( V kk,1(s)\u2212 V \u2217k,1(s) ) + ( V \u2217\u2217,1(s)\u2212 V kk,1(s) )\n(8)\nThe second term V \u2217\u2217,1 \u2212 V \u2217\u2217,1 relates the optimal rewards of the MDP M\u2217 to those near optimal for M\u0303k. We can bound this difference by \u221a\n1/k for PSRL in expectation, and for UCRL-Factored with high probability. This follows for PSRL by Lemma 1, and for UCRL-Factored whenever the true MDP lies within the confidence set Mk by the principle of OFU and the approximate MDP planner \u0393.\nWe decompose the first term through repeated applications of the dynamic programming equation,\n( V kk,1 \u2212 V \u2217k,1 ) (stk+1) = \u03c4 \u2211\ni=1\n( T kk,i \u2212 T \u2217k,i ) V kk,i+1(stk+i) + \u03c4 \u2211\ni=1\ndtk+1. (9)\nWhere dtk+i := \u2211 s\u2208S\n{ P \u2217(s|xk,i)(V \u2217k,i+1 \u2212 V kk,i+1)(s) }\n\u2212 (V \u2217k,i+1 \u2212 V kk,i+1)(stk+i). The second term captures the randomness in the transitions of the true MDP M\u2217 under policy \u00b5k. The expected value of (V \u2217 k,i+1\u2212V kk,i+1)(stk+i) given Htk+i is precisely \u2211 s\u2208S { P \u2217(s|xk,i)(V \u2217k,i+1 \u2212 V kk,i+1)(s) } so that E[dtk+i] = 0 for all i. To obtain high probability bounds for UCRL-Factored we note that dtk+i martingale difference bounded by \u03a8k equal to the span of V kk,i. By the OFU principle we know that \u03a8k \u2264 CD [12]. We apply the Azuma-Hoeffding inequality to say that:\nP\n(\nm \u2211\nk=1\n\u03c4 \u2211\ni=1\ndtk+i > CD \u221a 2T log(2/\u03b4)\n)\n\u2264 \u03b4 (10)\nThe remaining term is the one step Bellman error of the imagined MDP M\u0303k. Crucially this term only depends on xk,i which are actually observed. We can now use the Ho\u0308lder inequality to bound\n\u03c4 \u2211\ni=1\n( T kk,i \u2212 T \u2217k,i ) V kk,i+1(stk+i) \u2264 \u03c4 \u2211\ni=1\n|Rk(xk,i)\u2212 R \u2217 (xk,i)|+\n1 2 \u03a8k\u2016P k(\u00b7|xk,i)\u2212 P \u2217(\u00b7|xk,i)\u20161 (11)"}, {"heading": "7.2 Factorization decomposition", "text": "So far our analysis has made no mention of the factorized structure of the MDP. We will now show how we can further bound equation (11) by the sums of errors in each factor of the reward and transition functions. It is quite clear that we may upper bound the deviations of R \u2217 , R k by the sum of deviations of their factors using the triangle inequality. In fact, as we show in Lemma 3 we can also do this for the transition functions P \u2217 and P k. This result really is the key insight for our results in this paper.\nLemma 3 (Bounding factored deviations). Let the transition function class P \u2286 PX ,S be factored over X = X1 \u00d7 .. \u00d7 Xn and S = S1 \u00d7 .. \u00d7 Sm with scopes Z1, ..Zm. Then, for any P, P\u0303 \u2208 P we may bound their L1 distance by the sum of the differences of their factorizations:\n\u2016P (x)\u2212 P\u0303 (x)\u20161 \u2264 m \u2211\ni=1\n\u2016Pi(x[Zi])\u2212 P\u0303i(x[Zi])\u20161\nProof. In order to prove this lemma we begin with the simple claim that for any \u03b11, \u03b12, \u03b21, \u03b22 \u2208 (0, 1]:\n|\u03b11\u03b12 \u2212 \u03b21\u03b22| = \u03b12 \u2223 \u2223 \u2223\n\u2223 \u03b11 \u2212 \u03b21\u03b22 \u03b12\n\u2223 \u2223 \u2223 \u2223\n\u2264 \u03b12 ( |\u03b11 \u2212 \u03b21|+ \u2223 \u2223 \u2223\n\u2223 \u03b21 \u2212 \u03b21\u03b22 \u03b12\n\u2223 \u2223 \u2223 \u2223 )\n\u2264 \u03b12 |\u03b11 \u2212 \u03b21|+ \u03b21 |\u03b12 \u2212 \u03b22|\nThis result also holds for any \u03b11, \u03b12, \u03b21, \u03b22 \u2208 [0, 1], where including 0 can be verified case by case. We now consider the probability distributions p, p\u0303 over {1, .., d1} and q, q\u0303 over {1, .., d2}. We let Q = pqT , Q\u0303 = p\u0303q\u0303T be the joint probability distribution over {1, .., d1} \u00d7 {1, .., d2}. Using the claim above we will be able to bound the L1 deviation \u2016Q\u2212 Q\u0303\u20161 by the deviations of their factors:\n\u2016Q\u2212 Q\u0303\u20161 = d1 \u2211\ni=1\nd2 \u2211\nj=1\n|piqj \u2212 p\u0303iq\u0303j |\n\u2264 d1 \u2211\ni=1\nd2 \u2211\nj=1\nqj |pi \u2212 p\u0303i|+ p\u0303i|qj \u2212 q\u0303j |\n= \u2016p\u2212 p\u0303\u20161 + \u2016q \u2212 q\u0303\u20161\nApplying this result m times to the factored transition functions P and P\u0303 we recover our desired result."}, {"heading": "7.3 Concentration guarantees for Mk", "text": "We now want to show that the true MDP lies within Mk with high probability. Note that posterior sampling will also allow us to then say that the sampled Mk is within Mk with high probability too. In order to show this, we first present a concentration result for the L1 deviation of empirical probabilities.\nLemma 4 (L1 bounds for the empirical transition function). For all finite sets X , finite sets Y, function classes P \u2286 PX ,Y then for any x \u2208 X , \u01eb > 0:\nP\n( \u2016P \u2217(x)\u2212 P\u0302t(x)\u20161 \u2265 \u01eb ) \u2264 exp ( |Y| log(2)\u2212 nt(x)\u01eb 2\n2\n)\nProof. This is a relaxation of the result proved by Weissman [28].\nLemma 4 ensures that for any x \u2208 X , j = 1, ..,m P ( \u2016P \u2217j (x) \u2212 P\u0302jt(x)\u20161 \u2265 \u221a 2|Sj| nt(x) log ( 2 \u03b4\u2032 ) ) \u2264 \u03b4\u2032. We then\ndefine d Pj tk = 2|Si| log\n(\n2/\u03b4\u2032k,j\n)\nwith \u03b4\u2032k,j = \u03b4/(2m|X [ZPj ]|k2). Now using a union bound, together with the fact that\n\u2211\u221e n=1 1/n 2 = \u03c02/6 < 2:\nP\n(\nP \u2217j \u2208 Pjt (d Pj tk ) \u2200k \u2208 N, j = 1, ..,m\n)\n\u2265 1\u2212 \u03b4\nThe proof for sub \u03c3-gaussian random variables follows from the definition and resultant tail bounds. \u01eb \u2208 R is a sub \u03c3-gaussian random variable \u21d0\u21d2 \u2200t \u2208 R, E [exp(t\u01eb)] \u2264 exp ( \u03c32t2\n2\n)\n.\nLemma 5 (Tail bounds for sub \u03c3-gaussian random variables). If {\u01ebi} are all independent and sub \u03c3-gaussian then \u2200\u03b2 \u2265 0: P ( 1 n | \u2211n i=1 \u01ebi| > \u03b2 ) \u2264 exp ( log(2)\u2212 n\u03b222\u03c32 ) .\nLemma 5 ensures that for any x \u2208 X , i = 1, .., l P ( |R\u2217i (x)\u2212 R\u0302it(x)| \u2265 \u221a 2\u03c32\nnt(x) log\n(\n2 \u03b4\u2032\n)\n)\n\u2264 \u03b4\u2032. A similar\nargument from here ensures that: P ( R \u2217 i \u2208 Rit(dRitk ) \u2200k \u2208 N, i = 1, .., l )\n\u2265 1 \u2212 \u03b4. This allows us to present our important result that\nP\n( M\u2217 \u2208 Mk \u2200k \u2208 N ) \u2265 1\u2212 2\u03b4 (12)"}, {"heading": "7.4 Regret bounds", "text": "We now have all the necessary intermediate results to complete our proof. We begin with the analysis of PSRL. Using equation (12) and the posterior sampling lemma we can say that P(M\u2217,Mk \u2208 Mk\u2200k \u2208 N) \u2265 1\u2212 4\u03b4. The contributions from near-optimal regret in planning function \u0393 are bounded by\n\u2211L k=1\n\u221a \u03c4/k \u2264 2 \u221a T . From here\nwe take equation (11), Lemma 3 and Theorem 3 to say that for any \u03b4 > 0:\nE [ Regret(T, \u03c0PS\u03c4 ,M \u2217) ]\n\u2264 4\u03b4T + 2 \u221a T + l \u2211\ni=1\n{\n4(\u03c4C|X [ZRi ]|+ 1) + 4 \u221a 2dRiT |X [ZRi ]|T }\n+ sup k=1,..,L\n( E[\u03a8k|Mk,M\u2217 \u2208 Mk] ) \u00d7 m \u2211\nj=1\n{\n4(\u03c4 |X [ZPj ]|+ 1) + 4 \u221a 2d Pj T |X [ZPj ]|T\n}\nLet A = {M\u2217,Mk \u2208 Mk}, since \u03a8k \u2265 0 and E[\u03a8k] = E[\u03a8] via posterior sampling we can say that for all k:\nE[\u03a8k|A] \u2264 P(A)\u22121E[\u03a8] \u2264 ( 1\u2212 4\u03b4 k2\n)\u22121\nE[\u03a8] =\n(\n1 + 4\u03b4\nk2 \u2212 4\u03b4\n) E[\u03a8] \u2264 ( 1 + 4\u03b4\n1\u2212 4\u03b4\n)\nE[\u03a8].\nPlugging in the values of dRiT and d Pj T and setting \u03b4 = 1/T completes the proof of Theorem 1\nThe analysis of UCRL-Factored and Theorem 2 follows in a similar manner. We use equation (10) to bound the contributions of dt with probability 1\u2212 \u03b4, and equation (12) to bound missed confidence regions. From here we take equation (11), Lemma 3 and Theorem 3 to say that, with probability \u2265 1\u2212 3\u03b4:\nRegret(T, \u03c0UC\u03c4 ,M \u2217) \u2264 CD\n\u221a 2T log(2/\u03b4) + 2 \u221a T + l \u2211\ni=1\n{\n4(\u03c4C|X [ZRi ]|+ 1) + 4 \u221a 2dRiT |X [ZRi ]|T }\n+ CD \u00d7 m \u2211\nj=1\n{\n4(\u03c4 |X [ZPj ]|+ 1) + 4 \u221a 2d Pj T |X [ZPj ]|T\n}\nand 2. The Corollaries 1 and 2 follow from simply plugging in these values in the symmetric case and upper bounding the constant and logarithmic terms. This is presented in more detail in Appendix B."}, {"heading": "A Bounding the widths of confidence sets", "text": "We present elementary arguments which culminate in a proof of Theorem 3.\nLemma 6 (Concentration results for \u221a\ndT /nt(x)). For all finite sets X and any dT , \u01eb \u2265 0:\nT \u2211\nt=1\n1\n{\n\u221a dT /nt(xt) > h(dT , \u01eb) } \u2264 T \u2211\nt=1\n1\n{\n\u221a dT /nt(xt) > \u01eb } + |X |,\nWhere h(dT , \u01eb) := \u221a dT \u01eb2/(dT + \u01eb2).\nProof. Let (xs1 , .., xsK ) be the largest subsequence of x T 1 such that\n\u221a\ndT /nsi(xsi) \u2208 (h(dT , \u01eb), \u01eb] \u2200i. Now for any x \u2208 X , let Tx = {si | xsi = x}. Suppose there exist two distinct elements \u03c3, \u03c1 \u2208 Tx with \u03c3 < \u03c1 so that n\u03c1(x) \u2265 n\u03c3(x) + 1. We note that for any n \u2208 R+, h(dT , \u221a dT /n) = \u221a dT /(n+ 1) so that:\n\u01eb \u2265 \u221a dT /n\u03c3(x) =\u21d2 h(dT , \u01eb) \u2265 \u221a dT /(n\u03c3(x) + 1) \u2265 \u221a dT /n\u03c1(x)\nThis contradicts our assumption \u221a\ndT /n\u03c1(x) \u2208 (h(d, \u01eb), \u01eb] and so we must conclude that |Tx| \u2264 1 for all x \u2208 X . This means that (xs1 , .., xsK ) forms a subsequence of unique elements in X , the total length of which must be bounded by |X |.\nWe now provide a corollary of this result which allows for episodic delays in updating visit counts nt(x). We imagine that we will only update our counts every \u03c4 steps.\nCorollary 3 (Concentration results for \u221a\ndT /ntk (x) in the episodic setting). Let us associate times within episodes of length \u03c4 , t = tk + i for i = 1, .., \u03c4 and T = M \u00d7 \u03c4 . For all finite sets X and any dT , \u01eb \u2265 0:\nM \u2211\nk=1\n\u03c4 \u2211\ni=1\n1\n{\n\u221a\ndT /ntk (xtk+i) > h (\u03c4)(dT , \u01eb)\n} \u2264 M \u2211\nk=1\n\u03c4 \u2211\ni=1\n1\n{\n\u221a dT /ntk (xtk+i) > \u01eb } + 2\u03c4 |X |,\nWhere h(\u03c4)(dT , \u01eb) is the \u03c4 -fold composition of h(dT , \u00b7) acting on \u01eb. Proof. By an argument of visiting times similar to lemma 6 we can see that the worst case scenario for the episodic case \u2211M\nk=1\n\u2211\u03c4\ni=1 1\n{\n\u221a\ndT /ntk (xtk+i) > h (\u03c4)(dT , \u01eb)\n}\nis to visit each x exactly \u03c4 \u2212 1 times before the start of an episode, and then spend the entirety of the following episode within the state. Here we have upper bounded 2\u03c4 \u2212 1 by 2\u03c4 and |X | \u2212 1 by |X | to complete our result.\nIt will be useful to define notion of radius for each confidence set at each x \u2208 X , rFt(x) := supf\u2208Ft \u2016(f \u2212 f\u0302t)(x)\u2016. By the triangle inequality, we have wFt(x) \u2264 2rFt(x) for all x \u2208 X . Lemma 7 (Bounding the number of large radii). Let us write Fk for Ftk and associate times within episodes of length \u03c4 , t = tk + i for i = 1, .., \u03c4 and T = M \u00d7 \u03c4 . For all finite sets X , measurable spaces (Y,\u03a3Y), function classes F \u2286 MX ,Y , non-decreasing sequences {dt : t \u2208 N}, any T \u2208 N and \u01eb > 0:\nM \u2211\nk=1\n\u03c4 \u2211\ni=1\n1{rFk(xtk+i) > \u01eb} < ( dT \u03c4\u01eb2 + 1 ) 2\u03c4 |X |\nProof. By construction of Ft and noting that dt is non-decreasing in t, we can say that rFk(xt) \u2264 \u221a\ndT /ntk(xt) for all t = 1, .., T so that\nM \u2211\nk=1\n\u03c4 \u2211\ni=1\n1{rFk(xt+k+1) > \u01eb} \u2264 M \u2211\nk=1\n\u03c4 \u2211\ni=1\n1\n{\n\u221a dT /ntk (xtk+i) > \u01eb } .\nNow let g(\u01eb) = \u221a\ndT \u01eb2/(dT \u2212 \u03c4\u01eb2) be the \u01eb-inverse of h(\u03c4)(dT , \u01eb) such that g(h(\u03c4)(dT , \u01eb)) = \u01eb. Applying Corollary 3 to our expression n times repeatedly we can say:\nM \u2211\nk=1\n\u03c4 \u2211\ni=1\n1\n{\n\u221a dT /ntk (xtk+i) > \u01eb } \u2264 M \u2211\nk=1\n\u03c4 \u2211\ni=1\n1\n{\n\u221a\ndT /ntk (xtk+i) > g (n)(\u01eb)\n}\n+ 2n\u03c4 |X |.\nWhere g(n)(\u01eb) denotes the composition of g(\u00b7) n-times acting on \u01eb. If we take n to be the lowest integer such that g(n)(\u01eb) > \u221a dT /\u03c4 then, \u2211M\nk=1\n\u2211\u03c4\ni=1 1\n{\n\u221a\ndT /ntk(xtk+i) > g (n)(\u01eb)\n}\n\u2264 2\u03c4 |X | so that the whole expression is bounded by (n+ 1) 2\u03c4 |X |. Note that for all N \u2208 R+, g( \u221a dT /N) = \u221a dT /(N \u2212 \u03c4 ), if we write \u01eb = \u221a\ndT /N1 then n \u2264 N1/\u03c4 = dT\u03c4\u01eb2 , which completes the proof.\nUsing these results we are finally able to complete our proof of Theorem 3 We first note that, via the triangle inequality \u2211M\nk=1\n\u2211\u03c4\ni=1 wFk (xtk+i) \u2264 2\n\u2211M\nk=1\n\u2211\u03c4\ni=1 rFk(xtk+i). We streamline our notation by letting rk,i = rFk(xtk+i). Reordering\nthe sequence (r1,1, .., rM,\u03c4 ) \u2192 (ri1 , .., riT ) such ri1 \u2265 .. \u2265 riT we have that: M \u2211\nk=1\n\u03c4 \u2211\ni=1\nrFk(xtk+i) =\nT \u2211\nt=1\nrit \u2264 1 + T \u2211\ni=1\nrit1{rit \u2265 T\u22121}.\nWe can see that rit > \u01eb \u2265 T\u22121 \u21d0\u21d2 \u2211T i=1 1{rit \u2265 \u01eb} \u2265 t. From Lemma 7 this means that t \u2264\n(\ndT \u03c4\u01eb2 + 1 ) 2\u03c4 |X |, so that \u01eb \u2264 \u221a\n2|X|dT t\u22122\u03c4 |X| . This means that rit \u2264 min{CF , \u221a 2|X|dT t\u22122\u03c4 |X| }. Therefore,\nT \u2211\ni=1\nrit1{rit \u2265 T\u22121} \u2264 2\u03c4CF |X |+ T \u2211\nt=2\u03c4 |X|+1\n\u221a\n2dT |X | t\u2212 \u03c4 |X |\n\u2264 2\u03c4CF |X |+ \u222b T\n0\n\u221a\n2dT |X | t dt\n\u2264 2\u03c4CF |X |+ 2 \u221a 2dT |X |T\nWhich completes the proof of Theorem 3."}, {"heading": "B Clean bounds for the symmetric problem", "text": "We now provide concrete clean upper bounds for Theorems 1 and 2 in the simple symmetric case l + 1 = m, C = \u03c3 = 1, |Si| = |Xi| = K and |ZRi | = |ZPi | = \u03b6 for all suitable i and write J = K\u03b6 . For a non-trivial problem setting we assume that K \u2265 2, m \u2265 2, \u03c4 \u2265 2.\nFrom Section 7.4 we have that\nE\n[ Regret(T, \u03c0PS\u03c4 ,M \u2217) ] \u2264 4 + 2 \u221a T +m { 4(\u03c4J + 1) + 4 \u221a 8 log(4mJT 2/\u03c4 )JT }\n+ E[\u03a8] ( 1 + 4\nT \u2212 4\n) m { 4(\u03c4J + 1) + 4 \u221a 8K log(4mJT 2/\u03c4 )JT }\nThrough looking at the constant term we know that the bounds are trivially satisfied for all T \u2264 56, from here we can certainly upper bound 4/(T \u2212 4) \u2264 1/13. From here we can say that:\nE [ Regret(T, \u03c0PS\u03c4 ,M \u2217) ] \u2264 { 4 + 4m ( 1 + 14\n13 E[\u03a8]\n) (\u03c4J + 1) } + \u221a T { 2 + 4 \u221a 8J log(4mJT 2/\u03c4 ) + 4 \u221a 8JK log(4mJT 2/\u03c4 ) 14\n13 E[\u03a8]\n}\n\u2264 5 (1 +E[\u03a8])m\u03c4J + \u221a T { 12 \u221a J log(2mJT ) + 12E[\u03a8] \u221a JK log(2mJT ) }\n\u2264 5 (1 +E[\u03a8])m\u03c4J + 12m ( 1 +E[\u03a8] \u221a K ) \u221a JT log(2mJT )\n\u2264 min(5m\u03c4 2J, T ) + 12m\u03c4 \u221a JKT log(2mJT )\n\u2264 15m\u03c4 \u221a JKT log(2mJT )\nWhere in the last steps we have used that \u03a8 \u2264 \u03c4 and min(a, b) \u2264 \u221a ab. We now repeat a similar procedure of upper bounds for UCRL-Factored, immediately replicating D by \u03c4 in our analysis to say that with probability \u2265 1\u2212 3\u03b4:\nRegret(T, \u03c0UC\u03c4 ,M \u2217) \u2264 \u03c4\n\u221a 2T log(2/\u03b4) + 2 \u221a T +m { 4(\u03c4J + 1) + 4 \u221a 8 log(4mJT/\u03b4)JT }\n+ \u03c4m { 4(\u03c4J + 1) + 4 \u221a 8K log(4mJT/\u03b4)JT }\n\u2264 (1 + \u03c4 )m4(\u03c4J + 1) + \u221a T { \u03c4 \u221a 2 log(2/\u03b4) + 2 +m4 \u221a 8 log(4mJT/\u03b4)J + \u03c4m4 \u221a 8 log(4mJT/\u03b4)JK } \u2264 5(1 + \u03c4 )m\u03c4J + 12m(1 + \u03c4 \u221a K) \u221a JT log(4mJT/\u03b4)\n\u2264 15m\u03c4 \u221a JKT log(4mJT/\u03b4)\nWhere in the last step we used a similar argument"}], "references": [{"title": "Optimal adaptive policies for Markov decision processes", "author": ["A.N. Burnetas", "M.N. Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Stochastic systems: estimation, identification and adaptive control", "author": ["P.R. Kumar", "P. Varaiya"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Stochastic dynamic programming with factored representations", "author": ["Craig Boutilier", "Richard Dearden", "Mois\u00e9s Goldszmidt"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Learning dynamic bayesian networks. In Adaptive processing of sequences and data structures, pages 168\u2013197", "author": ["Zoubin Ghahramani"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Efficient reinforcement learning in factored MDPs", "author": ["Michael Kearns", "Daphne Koller"], "venue": "In IJCAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Model-based reinforcement learning in factored-state MDPs", "author": ["Alexander Strehl"], "venue": "In Approximate Dynamic Programming and Reinforcement Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Optimistic initialization and greediness lead to polynomial time learning in factored MDPs", "author": ["Istv\u00e1n Szita", "Andr\u00e1s L\u0151rincz"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Efficient structure learning in factored-state MDPs", "author": ["Alexander Strehl", "Carlos Diuk", "Michael Littman"], "venue": "In AAAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "arXiv preprint cs/9605103,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "More) Efficient Reinforcement Learning via Posterior Sampling", "author": ["Ian Osband", "Daniel Russo", "Benjamin Van Roy"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1933}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Thompson sampling: an asymptotically optimal finite time analysis", "author": ["E. Kauffmann", "N. Korda", "R. Munos"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "CoRR, abs/1301.2609,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Policy iteration for factored MDPs", "author": ["Daphne Koller", "Ronald Parr"], "venue": "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Max-norm projections for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr"], "venue": "In IJCAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Efficient solutions to factored MDPs with imprecise transition probabilities", "author": ["Karina Valdivia Delgado", "Scott Sanner", "Leliane Nunes De Barros"], "venue": "Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Approximate linear programming for first-order MDPs", "author": ["Scott Sanner", "Craig Boutilier"], "venue": "arXiv preprint arXiv:1207.1415,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ronald Ortner", "Daniil Ryabko"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Regret bounds for the adaptive control of linear quadratic systems", "author": ["Yasin Abbasi-Yadkori", "Csaba Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Generalization and exploration via randomized value functions", "author": ["Benjamin Van Roy", "Zheng Wen"], "venue": "arXiv preprint arXiv:1402.0635,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Regal: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["P.L. Bartlett", "A. Tewari"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2].", "startOffset": 195, "endOffset": 201}, {"referenceID": 1, "context": "1 Introduction The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2].", "startOffset": 195, "endOffset": 201}, {"referenceID": 2, "context": "Factored MDPs [3] allow us to represent large structured MDPs compactly.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "A state is described by a selection of state variables, whose transitions can be represented by a dynamic Bayesian network (DBN) [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "Kearns and Koller extend the E algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity.", "startOffset": 41, "endOffset": 47}, {"referenceID": 5, "context": "Kearns and Koller extend the E algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity.", "startOffset": 41, "endOffset": 47}, {"referenceID": 6, "context": "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].", "startOffset": 59, "endOffset": 65}, {"referenceID": 7, "context": "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].", "startOffset": 59, "endOffset": 65}, {"referenceID": 8, "context": "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 9, "context": "Some algorithms do seek to learn this structure from experience [10], but we will assume this structure is known.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "These bound the difference in accumulated rewards of a learning algorithm and the optimal policy over T steps [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "[12] present UCRL2, which attains near-optimal regret of \u00d5(S \u221a AT ) with high probability.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] analyze PSRL, which also provides bounds on the expected regret of \u00d5(S \u221a AT ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "UCRL-Factored is guided by the OFU principle whereas PSRL is guided by posterior (also known as Thompson) sampling [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 79, "endOffset": 91}, {"referenceID": 15, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 79, "endOffset": 91}, {"referenceID": 16, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 79, "endOffset": 91}, {"referenceID": 12, "context": "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "However, even where an FMDP is able to represented concisely, solving for the optimal policy may still be intractable in the most general case [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "Our focus in this paper is upon the statistical aspect of the learning problem and like earlier discussions [5] we do not specify which computational methods are used.", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 19, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 20, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 21, "context": "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].", "startOffset": 77, "endOffset": 93}, {"referenceID": 2, "context": "Factored MDPs are an approach with successful applications in many fields [3] but they are not the only one.", "startOffset": 74, "endOffset": 77}, {"referenceID": 22, "context": "There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as H\u00f6der continuous [23] or linear quadratic control [24].", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as H\u00f6der continuous [23] or linear quadratic control [24].", "startOffset": 207, "endOffset": 211}, {"referenceID": 24, "context": "Value-based approaches typically struggle to plan efficient exploration and so cannot obtain efficient learning guarantees, although there has been interesting progress in this field as well [25].", "startOffset": 191, "endOffset": 195}, {"referenceID": 12, "context": "This is the same formulation as earlier work [13], which we reproduce here for completeness.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "3 Factored MDPs To formalize our definition of a factored MDP we introduce some notation common to the literature [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 25, "context": "There is an optimistic algorithm REGAL [26] which formally 4", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "This is accomplished by imposing artificial episodes which end whenever the number of visits to a state-action pair is doubled [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "Nevertheless, there has been good empirical performance using this method for non-factored MDPs without episodic reset in simulation [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].", "startOffset": 117, "endOffset": 129}, {"referenceID": 25, "context": "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].", "startOffset": 117, "endOffset": 129}, {"referenceID": 12, "context": "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].", "startOffset": 117, "endOffset": 129}, {"referenceID": 26, "context": "PSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior \u03c6.", "startOffset": 44, "endOffset": 52}, {"referenceID": 12, "context": "PSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior \u03c6.", "startOffset": 44, "endOffset": 52}, {"referenceID": 11, "context": "UCRL-Factored is essentially UCRL2 [12] modified to exploit G in graph and episodic structure.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Using the OFU principle, we can guarantee with high probability that the optimal rewards of the true MDP are upper bounded by the optimal rewards of the optimistic MDP [11].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "In the case of posterior sampling, we make use of the posterior sampling lemma [17] Lemma 1 (Posterior Sampling).", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "By the OFU principle we know that \u03a8k \u2264 CD [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "\u2264 \u03b12 |\u03b11 \u2212 \u03b21|+ \u03b21 |\u03b12 \u2212 \u03b22| This result also holds for any \u03b11, \u03b12, \u03b21, \u03b22 \u2208 [0, 1], where including 0 can be verified case by case.", "startOffset": 77, "endOffset": 83}], "year": 2017, "abstractText": "Any learning algorithm over Markov decision processes (MDPs) will have worst-case regret \u03a9( \u221a SAT ) where T is the elapsed time and S and A are the cardinalities of the state and action spaces. In many settings of interest S and A may be so huge that it is impossible to guarantee good performance for an arbitrary MDP on any practical timeframe T . We show that, if we know the true system can be represented as a factored MDP, we can obtain regret bounds which scale polynomially in the number of parameters of the MDP, which may be exponentially smaller than S or A. Assuming an algorithm for approximate planning and knowledge of the graphical structure of the underlying MDP, we demonstrate that posterior sampling reinforcement learning (PSRL) and an algorithm based upon optimism in the face of uncertainty (UCRL-Factored) both satisfy near-optimal regret bounds.", "creator": "LaTeX with hyperref package"}}}