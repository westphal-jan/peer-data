{"id": "1404.3368", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2014", "title": "Near-optimal sample compression for nearest neighbors", "abstract": "We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds in metric spaces and to improve the accuracy and accuracy of our own algorithms.\n\n\n\n\n\nWe can already observe the limits of all parameters as they exist. The largest bounds are the expected dimension of the smallest, most likely to be 1 (1) at the root of all dimensions, and 2 (1) at the root of all dimensions, including the most likely to be 1 (2). The smallest bounds are the expected dimension of the smallest, most likely to be 1 (2) at the root of all dimensions, including the most likely to be 1 (2) at the root of all dimensions, including the most likely to be 1 (2) at the root of all dimensions, including the most likely to be 1 (3) at the root of all dimensions, including the most likely to be 1 (4) at the root of all dimensions, including the most likely to be 1 (5) at the root of all dimensions, including the most likely to be 1 (6) at the root of all dimensions, including the most likely to be 1 (7) at the root of all dimensions, including the most likely to be 1 (8) at the root of all dimensions, including the most likely to be 1 (9) at the root of all dimensions, including the most likely to be 1 (10) at the root of all dimensions, including the most likely to be 1 (11) at the root of all dimensions, including the most likely to be 1 (12) at the root of all dimensions, including the most likely to be 1 (13) at the root of all dimensions, including the most likely to be 1 (14) at the root of all dimensions, including the most likely to be 1 (15) at the root of all dimensions, including the most likely to be 1 (16) at the root of all dimensions, including the most likely to be 1 (17) at the root of all dimensions, including the most likely to be 1 (18) at the root of all dimensions, including the most likely to be 1 (19) at the root of all dimensions, including the most likely to be 1 (20) at the root of all", "histories": [["v1", "Sun, 13 Apr 2014 11:13:02 GMT  (54kb)", "https://arxiv.org/abs/1404.3368v1", null], ["v2", "Thu, 4 Dec 2014 15:23:49 GMT  (96kb)", "http://arxiv.org/abs/1404.3368v2", null], ["v3", "Fri, 5 Dec 2014 10:38:21 GMT  (92kb)", "http://arxiv.org/abs/1404.3368v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CC", "authors": ["lee-ad gottlieb", "aryeh kontorovich", "pinhas nisnevitch"], "accepted": true, "id": "1404.3368"}, "pdf": {"name": "1404.3368.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["leead@ariel.ac.il", "karyeh@cs.bgu.ac.il", "pinhasn@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 4.\n33 68\nv3 [\ncs .L\nG ]\n5 D\nec 2\n01 4\nNear-optimal sample compression for nearest neighbors\nLee-Ad Gottlieb Department of Computer Science and Mathematics, Ariel University\nAriel, Israel. leead@ariel.ac.il\nAryeh Kontorovich Computer Science Department, Ben Gurion University\nBeer Sheva, Israel. karyeh@cs.bgu.ac.il\nPinhas Nisnevitch Department of Computer Science and Mathematics, Ariel University\nAriel, Israel. pinhasn@gmail.com\nDecember 8, 2014\nWe present the first sample compression algorithm for nearest neighbors with nontrivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented."}, {"heading": "1 Introduction", "text": "The nearest neighbor classifier for non-parametric classification is perhaps the most intuitive learning algorithm. It is apparently the earliest, having been introduced by Fix and Hodges in 1951 (technical report reprinted in [1]). In this model, the learner observes a sample S of labeled points (X,Y ) = (Xi, Yi)i\u2208[n], where Xi is a point in some metric space X and Yi \u2208 {1,\u22121} is its label. Being a metric space, X is equipped with a distance function d : X \u00d7 X \u2192 R. Given a new unlabeled point x \u2208 X to be classified, x is assigned the same label as its nearest neighbor in S, which is argminYi\u2208Y d(x,Xi). Under mild regularity assumptions, the nearest neighbor classifier\u2019s expected error is asymptotically bounded by twice the Bayesian error, when the sample size tends to infinity [2].1 These results have inspired a vast body of research on proximity-based classification (see [4, 5] for extensive background and\n1A Bayes-consistent modification of the 1-NN classifier was recently proposed in [3].\n[6] for a recent refinement of classic results). More recently, strong margin-dependent generalization bounds were obtained in [7], where the margin is the minimum distance between opposite labeled points in S.\nIn addition to provable generalization bounds, nearest neighbor (NN) classification enjoys several other advantages. These include simple evaluation on new data, immediate extension to multiclass labels, and minimal structural assumptions \u2014 it does not assume a Hilbertian or even a Banach space. However, the naive NN approach also has disadvantages. In particular, it requires storing the entire sample, which may be memory-intensive. Further, information-theoretic considerations show that exact NN evaluation requires \u0398(|S|) time in high-dimensional metric spaces [8] (and possibly Euclidean space as well [9]) \u2014 a phenomenon known as the algorithmic curse of dimensionality. Lastly, the NN classifier has infinite VC-dimension [5], implying that it tends to overfit the data. This last problem can be mitigated by taking the majority vote among k > 1 nearest neighbors [10, 11, 5], or by deleting some sample points so as to attain a larger margin [12].\nShortcomings in the NN classifier led Hart [13] to pose the problem of sample compression. Indeed, significant compression of the sample has the potential to simultaneously address the issues of memory usage, NN search time, and overfitting. Hart considered the minimum Consistent Subset problem \u2014 elsewhere called the Nearest Neighbor Condensing problem \u2014 which seeks to identify a minimal subset S\u2217 \u2282 S that is consistent with S, in the sense that the nearest neighbor in S\u2217 of every x \u2208 S possesses the same label as x. This problem is known to be NP-hard [14, 15], and Hart provided a heuristic with runtime O(n3). The runtime was recently improved by [16] to O(n2), but neither paper gave performance guarantees.\nThe Nearest Neighbor Condensing problem has been the subject of extensive research since its introduction [17, 18, 19]. Yet surprisingly, there are no known approximation algorithms for it \u2014 all previous results on this problem are heuristics that lack any non-trivial approximation guarantees. Conversely, no strong hardness-ofapproximation results for this problem are known, which indicates a gap in the current state of knowledge. Main results. Our contribution aims at closing the existing gap in solutions to the Nearest Neighbor Condensing problem. We present a simple near-optimal approximation algorithm for this problem, where our only structural assumption is that the points lie in some metric space. Define the scaled margin \u03b3 < 1 of a sample S as the ratio of the minimum distance between opposite labeled points in S to the diameter of S. Our algorithm produces a consistent set S\u2032 \u2282 S of size \u23081/\u03b3\u2309ddim(S)+1 (Theorem 1), where ddim(S) is the doubling dimension of the space S. This result can significantly speed up evaluation on test points, and also yields sharper and simpler generalization bounds than were previously known (Theorem 3).\nTo establish optimality, we complement the approximation result with an almost matching hardness-of-approximation lower-bound. Using a reduction from the Label Cover problem, we show that the Nearest Neighbor Condensing problem is NP-hard to approximate within factor 2(ddim(S) log(1/\u03b3)) 1\u2212o(1)\n(Theorem 2). Note that the above upper-bound is an absolute size guarantee, and stronger than an approximation guarantee.\nAdditionally, we present a simple heuristic to be applied in conjunction with the algorithm of Theorem 1, that achieves further sample compression. The empirical performances of both our algorithm and heuristic seem encouraging (see Section 4). Related work. A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions). This problem is monotone in the sense that adding a conjunction to the solution set can only increase the sample accuracy of the solution. In contrast, in our problem the addition of a point of S to S\u2217 can cause S\u2217 to be inconsistent \u2014 and this distinction is critical to the hardness of our problem.\nRemoval of points from the sample can also yield lower dimensionality, which itself implies faster nearest neighbor evaluation and better generalization bounds. For metric spaces, [24] and [25] gave algorithms for dimensionality reduction via point removal (irrespective of margin size).\nThe use of doubling dimension as a tool to characterize metric learning has appeared several times in the literature, initially by [26] in the context of nearest neighbor classification, and then in [27] and [28]. A series of papers by Gottlieb, Kontorovich and Krauthgamer investigate doubling spaces for classification [12], regression [29], and dimension reduction [25]. k-nearest neighbor. A natural question is whether the Nearest Neighbor Condensing problem of [13] has a direct analogue when the 1-nearest neighbor rule is replaced by a (k > 1)-nearest neighbor \u2013 that is, when the label of a point is determined by the majority vote among its k nearest neighbors. A simple argument shows that the analogy breaks down. Indeed, a minimal requirement for the condensing problem to be meaningful is that the full (uncondensed) set S is feasible, i.e. consistent with itself. Yet even for k = 3 there exist self-inconsistent sets. Take for example the set S consisting of two positive points at (0, 1) and (0,\u22121) and two negative points at (1, 0) and (\u22121, 0). Then the 3-nearest neighbor rule misclassifies every point in S, hence S itself is inconsistent. Paper outline. This paper is organized as follows. In Section 2, we present our algorithm and prove its performance bound, as well as the reduction implying its near optimality (Theorem 2). We then highlight the implications of this algorithm for learning in Section 3. In Section 4 we describe a heuristic which refines our algorithm, and present empirical results."}, {"heading": "1.1 Preliminaries", "text": "Metric spaces. A metric d on a set X is a positive symmetric function satisfying the triangle inequality d(x, y) \u2264 d(x, z)+d(z, y); together the two comprise the metric space (X , d). The diameter of a set A \u2286 X , is defined by diam(A) = supx,y\u2208A d(x, y). Throughout this paper we will assume that diam(S) = 1; this can always be achieved by scaling. Doubling dimension. For a metric (X , d), let \u03bb be the smallest value such that every ball in X of radius r (for any r) can be covered by \u03bb balls of radius r2 . The doubling dimension of X is ddim(X ) = log2 \u03bb. A metric is doubling when its doubling dimension is bounded. Note that while a low Euclidean dimension implies a low doubling\ndimension (Euclidean metrics of dimension d have doubling dimension O(d) [30]), low doubling dimension is strictly more general than low Euclidean dimension. The following packing property can be demonstrated via a repetitive application of the doubling property: For set S with doubling dimension ddim(X ) and diam(S) \u2264 \u03b2, if the minimum interpoint distance in S is at least \u03b1 < \u03b2 then\n|S| \u2264 \u2308\u03b2/\u03b1\u2309ddim(X )+1 (1)\n(see, for example [8]). The above bound is tight up to constant factors, meaning there exist sets of size (\u03b2/\u03b1)\u2126(ddim(X )). Nearest Neighbor Condensing. Formally, we define the Nearest Neighbor Condensing (NNC) problem as follows: We are given a set S = S\u2212\u222aS+ of points, and distance metric d : S \u00d7 S \u2192 R. We must compute a minimal cardinality subset S\u2032 \u2282 S with the property that for any p \u2208 S, the nearest neighbor of p in S\u2032 comes from the same subset {S+, S\u2212} as does p. If p has multiple exact nearest neighbors in S\u2032, then they must all be of the same subset. Label Cover. The Label Cover problem was first introduced by [31] in a seminal paper on the hardness of computation. Several formulations of this problem have appeared the literature, and we give the description forwarded by [32]: The input is a bipartite graph G = (U, V,E), with two sets of labels: A for U and B for V . For each edge (u, v) \u2208 E (where u \u2208 U , v \u2208 V ), we are given a relation \u03a0u,v \u2282 A\u00d7B consisting of admissible label pairs for that edge. A labeling (f, g) is a pair of functions f : U \u2192 2A and g : V \u2192 2B\\{\u2205} assigning a set of labels to each vertex. A labeling covers an edge (u, v) if for every label b \u2208 g(v) there is some label a \u2208 f(u) such that (a, b) \u2208 \u03a0u,v. The goal is to find a labeling that covers all edges, and which minimizes the sum of the number of labels assigned to each u \u2208 U , that is \u2211 u\u2208U |f(u)|. It was shown in [32] that it is NP-hard to approximate Label Cover to within a factor 2(logn) 1\u2212o(1)\n, where n is the total size of the input. Learning. We work in the agnostic learning model [33, 5]. The learner receives n labeled examples (Xi, Yi) \u2208 X \u00d7{\u22121, 1} drawn iid according to some unknown probability distribution P. Associated to any hypothesis h : X \u2192 {\u22121, 1} is its empirical error e\u0302rr(h) = n\u22121 \u2211 i\u2208[n] 1{h(Xi) 6=Yi} and generalization error err(h) = P(h(X) 6= Y )."}, {"heading": "2 Near-optimal approximation algorithm", "text": "In this section, we describe a simple approximation algorithm for the Nearest Neighbor Condensing problem. In Section 2.1 we provide almost tight hardness-of-approximation bounds. We have the following theorem:\nTheorem 1. Given a point set S and its scaled margin \u03b3 < 1, there exists an algorithm that in time\nmin{n2, 2O(ddim(S))n log(1/\u03b3)}\ncomputes a consistent set S\u2032 \u2282 S of size at most \u23081/\u03b3\u2309ddim(S)+1.\nRecall that an \u03b5-net of point set S is a subset S\u03b5 \u2282 S with two properties:\n(i) Packing. The minimum interpoint distance in S\u03b5 is at least \u03b5.\n(ii) Covering. Every point p \u2208 S has a nearest neighbor in S\u03b5 strictly within distance \u03b5.\nWe make the following observation: Since the margin of the point set is \u03b3, a \u03b3-net of S is consistent with S. That is, every point p \u2208 S has a neighbor in S\u03b3 strictly within distance \u03b3, and since the margin of S is \u03b3, this neighbor must be of the same label set as p. By the packing property of doubling spaces (Equation 1), the size of S\u03b3 is at most \u23081/\u03b3\u2309ddim(S)+1. The solution returned by our algorithm is S\u03b3 , and satisfies the guarantees claimed in Theorem 1.\nIt remains only to compute the net S\u03b3 . A brute-force greedy algorithm can accomplish this in time O(n2): For every point p \u2208 S, we add p to S\u03b3 if the distance from p to all points currently in S\u03b3 is \u03b3 or greater, d(p, S\u03b3) \u2265 \u03b3. See Algorithm 1.\nAlgorithm 1 Brute-force net construction Require: S\n1: S\u03b3 \u2190 arbitrary point of S 2: for all p \u2208 S do 3: if d(p, S\u03b3) \u2265 \u03b3 then 4: S\u03b3 = S\u03b3 \u222a {p} 5: end if 6: end for\nThe construction time can be improved by building a net hierarchy, similar to the one employed by [8], in total time 2O(ddim(S))n log(1/\u03b3). (See also [34, 35, 36].) A hierarchy consists of all nets S2i for i = 0,\u22121, . . . , \u230alog \u03b3\u230b, where S2i \u2282 S2i\u22121 for all i > \u230alog \u03b3\u230b. Two points p, q \u2208 S2i are neighbors if d(p, q) < 4 \u00b72i. Further, each point q \u2208 S is a child of a single nearby parent point p \u2208 S2i satisfying d(p, q) < 2i. By the definition of a net, a parent point must exist. If two points p, q \u2208 S2i are neighbors (d(p, q) < 4 \u00b7 2i) then their respective parents p\u2032, q\u2032 \u2208 S2i+1 are necessarily neighbors as well: d(p\u2032, q\u2032) \u2264 d(p\u2032, p) + d(p, q) + d(q, q\u2032) < 2i+1 + 4 \u00b7 2i + 2i+1 = 4 \u00b7 2i+1.\nThe net S20 = S1 consists of a single arbitrary point. Having constructed S2i , it is an easy matter to construct S2i\u22121 : Since we require S2i\u22121 \u2283 S2i , we will initialize S2i\u22121 = S2i . For each q \u2208 S, we need only to determine whether d(q, S2i\u22121) \u2265 2i\u22121, and if so add q to S2i\u22121 . Crucially, we need not compare q to all points of S2i\u22121 : If there exists a point p \u2208 S2i with d(q, p) < 2i, then the respective parents p\u2032, q\u2032 \u2208 S2i of p, q must be neighbors. Let set T include only the children of q\u2032 and of q\u2032\u2019s neighbors. To determine the inclusion of every q \u2208 S in S2i\u22121 , it suffices to compute whether d(q, T ) \u2265 2i\u22121, and so n such queries are sufficient to construct S2i\u22121 . The points of T have minimum distance 2i\u22121 and are all contained in a ball of radius 4 \u00b7 2i + 2i\u22121 centered at T , so by the packing property (Equation 1) |T | = 2O(ddim(S)). It follows that the above query d(q, T ) can be answered in time 2O(ddim(S)). For each point in S we execute O(log(1/\u03b3)) queries, for a total runtime of 2O(ddim(S))n log(1/\u03b3). The above procedure is illustrated in the Appendix."}, {"heading": "2.1 Hardness of approximation of NNC", "text": "In this section, we prove almost matching hardness results for the NNC problem.\nTheorem 2. Given a set S of labeled points with scaled margin \u03b3, it is NP-hard to approximate the solution to the Nearest Neighbor Condensing problem on S to within a factor 2(ddim(S) log(1/\u03b3)) 1\u2212o(1) .\nTo simplify the proof, we introduce an easier version of NNC called Weighted Nearest Neighbor Condensing (WNNC). In this problem, the input is augmented with a function assigning weight to each point of S, and the goal is to find a subset S\u2032 \u2282 S of minimum total weight. We will reduce Label Cover to WNNC and then reduce WNNC to NNC (with some mild assumptions on the admissible range of weights), all while preserving hardness of approximation. The theorem will follow from the hardness of Label Cover [32]. First reduction. Given a Label Cover instance of size m = |U |+ |V | + |A| + |B|+ |E| + \u2211 e\u2208E |\u03a0E |, fix large value c to be specified later, and an infinitesimally small constant \u03b7. We create an instance of WNNC as follows (see Figure 1).\nLabel Cover Nearest Neighbor Condensing\n1. We first create a point p+ \u2208 S+ of weight 1.\nWe introduce set SE \u2282 S\u2212 representing edges in E: For each edge e \u2208 E, create point pe of weight \u221e. The distance from pe to p+ is 3 + \u03b7.\n2. We introduce set SV,B \u2282 S\u2212 representing pairs in V \u00d7 B: For each vertex v \u2208 V and label b \u2208 B, create point pv,b of weight 1. If edge e is incident to v and there exists a label (a, b) \u2208 \u03a0e for any a \u2208 A, then the distance from pv,b to pe is 3.\nFurther add a point p\u2212 \u2208 S\u2212 of weight 1, at distance 2 from all points in SV,B .\n3. We introduce set SL \u2282 S+ representing labels in \u03a0e. For each edge e = (u, v) and label b \u2208 B for which (a, b) \u2208 \u03a0e (for any a \u2208 A), we create point pe,b \u2282 SL of\nweight \u221e. pe,b represents the set of labels (a, b) \u2208 \u03a0e over all a \u2208 A. pe,b is at distance 2 + \u03b7 from pv,b.\nFurther add a point p\u2032+ \u2208 S+ of weight 1, at distance 2 + 2\u03b7 from all points in SL.\n4. We introduce set SU,A \u2282 S+ representing pairs in U \u00d7 A: For each vertex u \u2208 U and label a \u2208 A, create point pu,a of weight c. For any edge e = (u, v) and label b \u2208 B, if (a, b) \u2208 \u03a0e then the distance from pe,b \u2208 SL to pu,a is 2.\nThe points of each set SE , SV,B, SL and SU,A are packed into respective balls of diameter 1. Fixing any target doubling dimension D = \u2126(1) and recalling that the cardinality of each of these sets is less than m2, we conclude that the minimum interpoint distance in each ball is m\u2212O(1/D). All interpoint distances not yet specified are set to their maximum possible value. The diameter of the resulting set is constant, so its scaled margin is \u03b3 = m\u2212O(1/D). We claim that a solution of WNNC on the constructed instance implies some solution of the Label Cover Instance:\n1. p+ must appear in any solution: The nearest neighbors of p+ are the negative points of SE , so if p+ is not included the nearest neighbor of set SE is necessarily the nearest neighbor of p+, which is not consistent.\n2. Points in SE have infinite weight, so no points of SE appear in the solution. All points of SE are at distance exactly 3 + \u03b7 from p+, hence each point of SE must be covered by some point of SV,B to which it is connected \u2013 other points in SV,B are farther than 3+ \u03b7. (Note that SV,B itself can be covered by including the single point p\u2212.)\nChoosing covering points in SV,B corresponds to assigning labels in B to vertices of V in the Label Cover instance.\n3. Points in SL have infinite weight, so no points of SL appear in the solution. Hence, either p\u2032+ or some points of SU,A must be used to cover points of SL. Specifically, a point in SL \u2208 S+ incident on an included point of SV,B \u2208 S\u2212 is at distance exactly 2+\u03b7 from this point, and so it must be covered by some point of SU,A to which it is connected, at distance 2 \u2013 other points in SU,A are farther than 2 + \u03b7. Points of SL not incident on an included point of SV,B can be covered by p\u2032+, which at distance 2 + 2\u03b7 is still closer than any point in SV,B . (Note that SU,A itself can be covered by including a single arbitrary point of SU,A, which at distance 1 is closer than all other point sets.)\nChoosing the covering point in SU,A corresponds to assigning labels inA to vertices of U in the Label Cover instance, thereby inducing a valid labeling for some edge and solving the Label Cover problem.\nNow, a trivial solution to this instance of WNNC is to take all points of SU,A, SV,B and the single point p+: then SE and p\u2212 are covered by SV,B , and SL and p\u2032+ by SU,A. The size of the resulting set is c|SU,A|+ |SU,B|+ 1, and this provides an upper bound on the optimal solution. By setting c = m4 \u226b m3 > m(|SU,B| + 1), we ensure that the solution cost of WNNC is asymptotically equal to the number of points\nof SU,A included in its solution. This in turn is exactly the sum of labels of A assigned to each vertex of U in a solution to the Label Cover problem. Label Cover is hard to approximate within a factor 2(logm) 1\u2212o(1)\n, implying that WNNC is hard to approximate within a factor of 2(logm) 1\u2212o(1) = 2(D log(1/\u03b3)) 1\u2212o(1)\n. Before proceeding to the next reduction, we note that to rule out the inclusion of points of SE , SL in the solution set, infinite weight is not necessary: It suffices to give each heavy point weight c2, which is itself greater than the weight of the optimal solution by a factor of at least m2. Hence, we may assume all weights are restricted to the range [1,mO(1)], and the hardness result for WNNC still holds. Second reduction. We now reduce WNNC to NNC, assuming that the weights of the n points are in the range [1,mO(1)]. Let \u03b3 be the scaled margin of the WNNC instance. To mimic the weight assignment of WNNC using the unweighted points of NNC, we introduce the following gadget graph G(w,D): Given parameter w and doubling dimension D, create a point set T of size w whose interpoint distances are the same as those realized by a set of contiguous points on the D-dimensional \u21131-grid of side-length \u2308w1/D\u2309. Now replace each point p \u2208 T by twin positive and negative points at mutual distance \u03b32 , so that the distance from each twin replacing p to each twin replacing any q \u2208 T is the same as the distance from p to q. G(w,D) consists of T , as well as a single positive point at distance \u2308w1/D\u2309 from all positive points of T , and \u2308w1/D\u2309+ \u03b32 from all negative points of T , and a single negative point at distance \u2308w1/D\u2309 from all negative points of T , and \u2308w1/D\u2309+ \u03b32 from all positive points of T .\nClearly, the optimal solution to NNC on the gadget instance is to choose the two points not in T . Further, if any single point in T is included in the solution, then all of T must be included in the solution: First the twin of the included point must also be included in the solution. Then, any point at distance 1 from both twins must be included as well, along with its own twin. But then all points within distance 1 of the new twins must be included, etc., until all points of T are found in the solution.\nTo effectively assign weight to a positive point of NNC, we add a gadget to the point set, and place all negative points of the gadget at distance \u2308w1/D\u2309 from this point. If the point is not included in the NNC solution, then the cost of the gadget is only 2.2 But if this point is included in the NNC solution, then it is the nearest neighbor of the negative gadget points, and so all the gadget points must be included in the solution, incurring a cost of w. A similar argument allows us to assign weight to negative points of NNC. The scaled margin of the NNC instance is of size \u2126(\u03b3/w1/D) = \u2126(\u03b3m\u2212O(1/D)), which completes the proof of Theorem 2."}, {"heading": "3 Learning", "text": "In this section, we apply Theorem 1 to obtain improved generalization bounds for binary classification in doubling spaces. Working in the standard agnostic PAC setting, we take the labeled sample S to be drawn iid from some unknown distribution over X \u00d7 {\u22121, 1}, with respect to which all of our probabilities will be defined. In a slight\n2By scaling up all weights by a factor of n2, we can ensure that the cost of all added gadgets (2n) is asymptotically negligible.\nabuse of notation, we will blur the distinction between S \u2282 X as a collection of points in a metric space and S \u2208 (X \u00d7 {\u22121, 1})n as a sequence of point-label pairs. As mentioned in the preliminaries, there is no loss of generality in taking diam(S) = 1. Partitioning the sample S = S+ \u222a S\u2212 into its positively and negatively labeled subsets, the margin induced by the sample is given by \u03b3(S) = d(S+, S\u2212), where d(A,B) := minx\u2208A,x\u2032\u2208B d(x, x\n\u2032) for A,B \u2282 X . Any labeled sample S induces the nearest-neighbor classifier \u03bdS : X \u2192 {\u22121, 1} via\n\u03bdS(x) =\n{ +1 if d(x, S+) < d(x, S\u2212)\n\u22121 else.\nWe say that S\u0303 \u2282 S is \u03b5-consistent with S if 1n \u2211 x\u2208S 1{\u03bdS(x) 6=\u03bdS\u0303(x)} \u2264 \u03b5. For \u03b5 = 0, an \u03b5-consistent S\u0303 is simply said to be consistent (which matches our previous notion of consistent subsets). A sample S is said to be (\u03b5, \u03b3)-separable (with witness S\u0303) if there is an \u03b5-consistent S\u0303 \u2282 S with \u03b3(S\u0303) \u2265 \u03b3.\nWe begin by invoking a standard Occam-type argument to show that the existence of small \u03b5-consistent sets implies good generalization. The generalizing power of sample compression was independently discovered by [37, 38], and later elaborated upon by [39].\nTheorem 3. For any distribution P, any n \u2208 N and any 0 < \u03b4 < 1, with probability at least 1\u2212 \u03b4 over the random sample S \u2208 (X \u00d7 {\u22121, 1})n, the following holds:\n(i) If S\u0303 \u2282 S is consistent with S, then err(\u03bdS\u0303) \u2264 1\nn\u2212 |S\u0303|\n( |S\u0303| logn+ logn+ log 1\n\u03b4\n) .\n(ii) If S\u0303 \u2282 S is \u03b5-consistent with S, then err(\u03bdS\u0303) \u2264 \u03b5n\nn\u2212 |S\u0303| +\n\u221a |S\u0303| logn+ 2 logn+ log 1\u03b4\n2(n\u2212 |S\u0303|) .\nProof. Finding a consistent (resp., \u03b5-consistent) S\u0303 \u2282 S constitutes a sample compression scheme of size |S\u0303|, as stipulated in [39]. Hence, the bounds in (i) and (ii) follow immediately from Theorems 1 and 2 ibid.\nCorollary 1. With probability at least 1 \u2212 \u03b4, the following holds: If S is (\u03b5, \u03b3)separable with witness S\u0303, then\nerr(\u03bdS\u0303) \u2264 \u03b5n\nn\u2212 \u2113 +\n\u221a \u2113 logn+ 2 logn+ log 1\u03b4\n2(n\u2212 \u2113) ,\nwhere \u2113 = \u23081/\u03b3\u2309ddim(S)+1.\nProof. Follows immediately from Theorems 1 and 3(ii).\nRemark. It is instructive to compare the bound above to [12, Corollary 5]. Stated in the language of this paper, the latter upper-bounds the NN generalization error in terms of the sample margin \u03b3 and ddim(X ) by\n\u03b5+\n\u221a 2\nn (d\u03b3 ln(34en/d\u03b3) log2(578n) + ln(4/\u03b4)), (2)\nwhere d\u03b3 = \u230816/\u03b3\u2309 ddim(X )+1 and \u03b5 is the fraction of the points in S that violate the margin condition (i.e., opposite-labeled point pairs less than \u03b3 apart in d). Hence, Corollary 1 is a considerable improvement over (2) in at least three aspects. First, the data-dependent ddim(S) may be significantly smaller than the dimension of the ambient space, ddim(X ).3 Secondly, the factor of 16ddim(X )+1 is shaved off. Finally, (2) relied on some fairly intricate fat-shattering arguments [40, 41], while Corollary 1 is an almost immediate consequence of much simpler Occam-type results.\nOne limitation of Theorem 1 is that it requires the sample to be (0, \u03b3)-separable. The form of the bound in Corollary 1 suggests a natural Structural Risk Minimization (SRM) procedure: minimize the right-hand size over (\u03b5, \u03b3). A solution to this problem was (essentially) given in [12, Theorem 7]:\nTheorem 4. Let R(\u03b5, \u03b3) denote the right-hand size of the inequality in Corollary 1 and put (\u03b5\u2217, \u03b3\u2217) = argmin\u03b5,\u03b3 R(\u03b5, \u03b3). Then (i) One may compute (\u03b5\n\u2217, \u03b3\u2217) in O(n4.376) randomized time. (ii) One may compute (\u03b5\u0303, \u03b3\u0303) satisfying R(\u03b5\u0303, \u03b3\u0303) \u2264 4R(\u03b5\u2217, \u03b3\u2217) in O(ddim(S)n2 logn) deterministic time. Both solutions yield a witness S\u0303 \u2282 S of (\u03b5, \u03b3)-separability as a by-product.\nHaving thus computed the optimal (or near-optimal) \u03b5\u0303, \u03b3\u0303 with the corresponding witness S\u0303, we may now run the algorithm furnished by Theorem 1 on the sub-sample S\u0303 and invoke the generalization bound in Corollary 1. The latter holds uniformly over all \u03b5\u0303, \u03b3\u0303."}, {"heading": "4 Experiments", "text": "In this section we discuss experimental results. First, we will describe a simple heuristic built upon our algorithm. The theoretical guarantees in Theorem 1 feature a dependence on the scaled margin \u03b3, and our heuristic aims to give an improved solution in the problematic case where \u03b3 is small. Consider the following procedure for obtaining a smaller consistent set. We first extract a net S\u03b3 satisfying the guarantees of Theorem 1. We then remove points from S\u03b3 using the following rule: for all i \u2208 {0, . . . \u2308log \u03b3\u2309}, and for each p \u2208 S\u03b3 , if the distance from p to all opposite labeled points in S\u03b3 is at least 2 \u00b7 2i, then remove from S\u03b3 all points strictly within distance 2i \u2212 \u03b3 of p (see Algorithm 2). We can show that the resulting set is consistent:\nLemma 5. The above heuristic produces a consistent solution.\nProof. Consider a point p \u2208 S\u03b3 , and assume without loss of generality that p is positive. If d(p, S\u2212\u03b3 ) \u2265 2 \u00b7 2\ni, then the positive net-points strictly within distance 2i of p are closer to p than to any negative point in S\u03b3 , and are \u201ccovered\u201d by p. The removed positive net-points at distance 2i\u2212\u03b3 themselves cover other positive points of S within distance \u03b3, but p covers these points of S as well. Further, p cannot be removed at a later stage in the algorithm, since p\u2019s distance from all remaining points is at least 2i \u2212 \u03b3.\n3 In general, ddim(S) \u2264 cddim(X ) for some universal constant c, as shown in [24].\nAlgorithm 2 Consistent pruning heuristic 1: S\u03b3 is produced by Algorithm 1 or its fast version (Appendix) 2: for all i \u2208 {0, . . . , \u2308log \u03b3\u2309} do 3: for all p \u2208 S\u03b3 do 4: if p \u2208 S\u00b1\u03b3 and d(p, S \u2213 \u03b3 ) \u2265 2 \u00b7 2\ni then 5: for all q 6= p \u2208 S\u03b3 with d(p, q) < 2i \u2212 \u03b3 do 6: S\u03b3 \u2190 S\u03b3\\{q} 7: end for 8: end if 9: end for\n10: end for\nAs a proof of concept, we tested our sample compression algorithms on several data sets from the UCI Machine Learning Repository. These included the Skin Segmentation, Statlog Shuttle, and Covertype sets.4 The final dataset features 7 different label types, which we treated as 21 separate binary classification problems; we report results for labels 1 vs. 4, 4 vs. 6, and 4 vs. 7, and these typify the remaining pairs. We stress that the focus of our experiments is to demonstrate that (i) a significant amount of consistent sample compression is often possible and (ii) the compression does not adversely affect the generalization error.\nFor each data set and experiment, we sampled equal sized learning and test sets, with equal representation of each label type. The L1 metric was used for all data sets. We report (i) the initial sample set size, (ii) the percentage of points retained after the net extraction procedure of Algorithm 1, (iii) the percentage retained after the pruning heuristic of Algorithm 2, and (iv) the change in prediction accuracy on test data, when comparing the heuristic to the uncompressed sample. The results, averaged over 500 trials, are summarized in Figure 2."}, {"heading": "A Fast net construction", "text": "In this section we provide an illustration of the fast net algorithm of Section 2. See the algorithm in the Appendix, where for each p \u2208 S2i we maintain lists N(p, i) of neighbors in S2i and C(p, i) of children in S2i+1 . Further, for each point q \u2208 S we maintain a point P (q, i) in S2i which is either its parent or an arbitrary covering point. Although we have assumed there that \u03b3 is known a priori, knowledge of \u03b3 is not actually necessary: We may terminate the algorithm when we encounter a net S2i where for all p \u2208 S2i and q \u2208 S, if d(p, q) < 2i then p and q are of the same label set. Clearly, the net i = \u230alog \u03b3\u230b satisfies this property (as may some other consistent net with larger i). It is an easy matter to check the stopping condition during the run of the algorithm, during the query for d(q, T ).\nAlgorithm 3 Fast net construction Require: S\n1: p \u2190 arbitrary point of S 2: S1 \u2190 {p} \u22b2 Top level contains a single point 3: C(p, 0) \u2190 \u2205, N(p, 0) \u2190 {p} \u22b2 Initialize child and neighbor lists of p 4: for all q \u2208 S do 5: P (q, 0) \u2190 p \u22b2 p covers all points 6: end for 7: for i = 0,\u22121, . . . , \u230alog \u03b3\u230b+ 1 do 8: S2i\u22121 \u2190 S2i \u22b2 All points of level i are present in level i\u2212 1 9: for all p \u2208 S2i\u22121 do 10: C(p, i\u2212 1) \u2190 \u2205 \u22b2 Initialize child list of p 11: for all r \u2208 N(p, i) with d(p, r) < 4 \u00b7 2i\u22121 do 12: N(p, i\u2212 1) \u2190 N(p, i\u2212 1) \u222a {r} 13: N(r, i \u2212 1) \u2190 N(r, i\u2212 1) \u222a {p} 14: end for 15: end for 16: for all q \u2208 S do 17: T \u2190 \u222ar\u2208N(P (q,i),i)C(r, i) \u22b2 Potential neighbors of q in level i\u2212 1 18: if d(q, T ) < 2i\u22121 then 19: P (q, i \u2212 1) \u2190 point r \u2208 T with d(r, q) < 2i\u22121 20: else 21: S2i\u22121 \u2190 S2i\u22121 \u222a {q} \u22b2 q is placed in level i\u2212 1 22: C(q\u2032, i) \u2190 C(q\u2032, i) \u222a {q} \u22b2 Update child list of q\u2019s parent 23: C(q, i \u2212 1) \u2190 \u2205, N(q, i\u2212 1) \u2190 {p} \u22b2 Initialize lists of p: 24: for all r \u2208 T with d(q, r) < 4 \u00b7 2i\u22121 do 25: N(q, i\u2212 1) \u2190 N(q, i\u2212 1) \u222a {r} 26: N(r, i\u2212 1) \u2190 N(r, i\u2212 1) \u222a {q} 27: end for 28: end if 29: end for 30: end for"}], "references": [{"title": "Discriminatory analysis. nonparametric discrimination: Consistency properties", "author": ["E. Fix", "J.L. Hodges"], "venue": "International Statistical Review / Revue Internationale de Statistique,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1967}, {"title": "A Bayes consistent 1-NN classifier (arXiv:1407.0208)", "author": ["A. Kontorovich", "R. Weiss"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Open problems in geometric methods for instance-based learning", "author": ["G. Toussaint"], "venue": "In Discrete and computational geometry,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Rates of Convergence for Nearest Neighbor Classification", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Distance-based classification with Lipschitz functions", "author": ["U. von Luxburg", "O. Bousquet"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Navigating nets: Simple algorithms for proximity search", "author": ["R. Krauthgamer", "J.R. Lee"], "venue": "In SODA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "An algorithm for approximate closest-point queries", "author": ["K.L. Clarkson"], "venue": "In SCG,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "On the strong universal consistency of nearest neighbor regression function estimates", "author": ["L. Devroye", "L. Gy\u00f6rfi", "A. Krzy\u017cak", "G. Lugosi"], "venue": "Ann. Statist.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Asymptotic expansions of the k nearest neighbor risk", "author": ["R.R. Snapp", "S.S. Venkatesh"], "venue": "Ann. Statist.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Efficient classification for metric data", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "In COLT,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "The condensed nearest neighbor rule", "author": ["P.E. Hart"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1968}, {"title": "Nearest neighbor problems", "author": ["G. Wilfong"], "venue": "In SCG,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "NP-completeness of the problem of prototype selection in the nearest neighbor method", "author": ["A.V. Zukhba"], "venue": "Pattern Recognit. Image Anal.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Fast condensed nearest neighbor rule", "author": ["F. Angiulli"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "The reduced nearest neighbor rule", "author": ["W. Gates"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1972}, {"title": "An algorithm for a selective nearest neighbor decision rule", "author": ["G.L. Ritter", "H.B. Woodruff", "S.R. Lowry", "T.L. Isenhour"], "venue": "IEEE Trans. Info. Theo.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1975}, {"title": "Reduction techniques for instance-based learning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Mach. Learn.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Commun. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1984}, {"title": "Quantifying inductive bias: AI learning algorithms and valiant\u2019s learning framework", "author": ["D. Haussler"], "venue": "Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "Learning the set covering machine by bound minimization and margin-sparsity trade-off", "author": ["F. Laviolette", "M. Marchand", "M. Shah", "S. Shanian"], "venue": "Mach. Learn.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "The set covering machine", "author": ["M. Marchand", "J. Shawe-Taylor"], "venue": "JMLR, 3:723\u2013746,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Proximity algorithms for nearly doubling spaces", "author": ["L. Gottlieb", "R. Krauthgamer"], "venue": "SIAM J. on Discr. Math.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Adaptive metric dimensionality reduction", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "ALT,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Learnability and the doubling dimension", "author": ["Y. Li", "P.M. Long"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Using the doubling dimension to analyze the generalization of learning algorithms", "author": ["N.H. Bshouty", "Y. Li", "P.M. Long"], "venue": "J. Comp. Sys. Sci.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Efficient regression in metric spaces via approximate Lipschitz extension", "author": ["L. Gottlieb", "A. Kontorovich", "R. Krauthgamer"], "venue": "In SIMBAD,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Bounded geometries, fractals, and low-distortion embeddings", "author": ["A. Gupta", "R. Krauthgamer", "J.R. Lee"], "venue": "In FOCS,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "The hardness of approximate optima in lattices, codes, and systems of linear equations", "author": ["S. Arora", "L. Babai", "J. Stern", "Z. Sweedyk"], "venue": "In FOCS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1993}, {"title": "On the hardness of approximating label-cover", "author": ["I. Dinur", "S. Safra"], "venue": "Info. Proc. Lett.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Cover trees for nearest neighbor", "author": ["A. Beygelzimer", "S. Kakade", "J. Langford"], "venue": "ICML", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Fast construction of nets in low-dimensional metrics and their applications", "author": ["S. Har-Peled", "M. Mendel"], "venue": "SIAM J. on Comput.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Searching dynamic point sets in spaces with bounded doubling dimension", "author": ["R. Cole", "L. Gottlieb"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Relating data compression and learnability, unpublished", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1986}, {"title": "A probabilistic theory of pattern recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1996}, {"title": "Pac-bayesian compression bounds on the prediction error of learning algorithms for classification", "author": ["T. Graepel", "R. Herbrich", "J. Shawe-Taylor"], "venue": "Mach. Learn.,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Scale-sensitive dimensions, uniform convergence, and learnability", "author": ["N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler"], "venue": "J. ACM,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "It is apparently the earliest, having been introduced by Fix and Hodges in 1951 (technical report reprinted in [1]).", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Under mild regularity assumptions, the nearest neighbor classifier\u2019s expected error is asymptotically bounded by twice the Bayesian error, when the sample size tends to infinity [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 3, "context": "1 These results have inspired a vast body of research on proximity-based classification (see [4, 5] for extensive background and", "startOffset": 93, "endOffset": 99}, {"referenceID": 2, "context": "1A Bayes-consistent modification of the 1-NN classifier was recently proposed in [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "[6] for a recent refinement of classic results).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "More recently, strong margin-dependent generalization bounds were obtained in [7], where the margin is the minimum distance between opposite labeled points in S.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Further, information-theoretic considerations show that exact NN evaluation requires \u0398(|S|) time in high-dimensional metric spaces [8] (and possibly Euclidean space as well [9]) \u2014 a phenomenon known as the algorithmic curse of dimensionality.", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "Further, information-theoretic considerations show that exact NN evaluation requires \u0398(|S|) time in high-dimensional metric spaces [8] (and possibly Euclidean space as well [9]) \u2014 a phenomenon known as the algorithmic curse of dimensionality.", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "This last problem can be mitigated by taking the majority vote among k > 1 nearest neighbors [10, 11, 5], or by deleting some sample points so as to attain a larger margin [12].", "startOffset": 93, "endOffset": 104}, {"referenceID": 9, "context": "This last problem can be mitigated by taking the majority vote among k > 1 nearest neighbors [10, 11, 5], or by deleting some sample points so as to attain a larger margin [12].", "startOffset": 93, "endOffset": 104}, {"referenceID": 10, "context": "This last problem can be mitigated by taking the majority vote among k > 1 nearest neighbors [10, 11, 5], or by deleting some sample points so as to attain a larger margin [12].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Shortcomings in the NN classifier led Hart [13] to pose the problem of sample compression.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "This problem is known to be NP-hard [14, 15], and Hart provided a heuristic with runtime O(n).", "startOffset": 36, "endOffset": 44}, {"referenceID": 13, "context": "This problem is known to be NP-hard [14, 15], and Hart provided a heuristic with runtime O(n).", "startOffset": 36, "endOffset": 44}, {"referenceID": 14, "context": "The runtime was recently improved by [16] to O(n), but neither paper gave performance guarantees.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "The Nearest Neighbor Condensing problem has been the subject of extensive research since its introduction [17, 18, 19].", "startOffset": 106, "endOffset": 118}, {"referenceID": 16, "context": "The Nearest Neighbor Condensing problem has been the subject of extensive research since its introduction [17, 18, 19].", "startOffset": 106, "endOffset": 118}, {"referenceID": 17, "context": "The Nearest Neighbor Condensing problem has been the subject of extensive research since its introduction [17, 18, 19].", "startOffset": 106, "endOffset": 118}, {"referenceID": 18, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 181, "endOffset": 185}, {"referenceID": 19, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 199, "endOffset": 203}, {"referenceID": 20, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 247, "endOffset": 255}, {"referenceID": 21, "context": "A well-studied problem related to the Nearest Neighbor Condensing problem is that of extracting a small set of simple conjunctions consistent with much of the sample, introduced by [20] and shown by [21] to be equivalent to minimum Set Cover (see [22, 23] for further extensions).", "startOffset": 247, "endOffset": 255}, {"referenceID": 22, "context": "For metric spaces, [24] and [25] gave algorithms for dimensionality reduction via point removal (irrespective of margin size).", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "For metric spaces, [24] and [25] gave algorithms for dimensionality reduction via point removal (irrespective of margin size).", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "The use of doubling dimension as a tool to characterize metric learning has appeared several times in the literature, initially by [26] in the context of nearest neighbor classification, and then in [27] and [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 25, "context": "The use of doubling dimension as a tool to characterize metric learning has appeared several times in the literature, initially by [26] in the context of nearest neighbor classification, and then in [27] and [28].", "startOffset": 199, "endOffset": 203}, {"referenceID": 26, "context": "The use of doubling dimension as a tool to characterize metric learning has appeared several times in the literature, initially by [26] in the context of nearest neighbor classification, and then in [27] and [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 10, "context": "A series of papers by Gottlieb, Kontorovich and Krauthgamer investigate doubling spaces for classification [12], regression [29], and dimension reduction [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "A series of papers by Gottlieb, Kontorovich and Krauthgamer investigate doubling spaces for classification [12], regression [29], and dimension reduction [25].", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "A series of papers by Gottlieb, Kontorovich and Krauthgamer investigate doubling spaces for classification [12], regression [29], and dimension reduction [25].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "A natural question is whether the Nearest Neighbor Condensing problem of [13] has a direct analogue when the 1-nearest neighbor rule is replaced by a (k > 1)-nearest neighbor \u2013 that is, when the label of a point is determined by the majority vote among its k nearest neighbors.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "dimension (Euclidean metrics of dimension d have doubling dimension O(d) [30]), low doubling dimension is strictly more general than low Euclidean dimension.", "startOffset": 73, "endOffset": 77}, {"referenceID": 6, "context": "(see, for example [8]).", "startOffset": 18, "endOffset": 21}, {"referenceID": 29, "context": "The Label Cover problem was first introduced by [31] in a seminal paper on the hardness of computation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "Several formulations of this problem have appeared the literature, and we give the description forwarded by [32]: The input is a bipartite graph G = (U, V,E), with two sets of labels: A for U and B for V .", "startOffset": 108, "endOffset": 112}, {"referenceID": 30, "context": "It was shown in [32] that it is NP-hard to approximate Label Cover to within a factor 2 1\u2212o(1) , where n is the total size of the input.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "The construction time can be improved by building a net hierarchy, similar to the one employed by [8], in total time 2n log(1/\u03b3).", "startOffset": 98, "endOffset": 101}, {"referenceID": 31, "context": "(See also [34, 35, 36].", "startOffset": 10, "endOffset": 22}, {"referenceID": 32, "context": "(See also [34, 35, 36].", "startOffset": 10, "endOffset": 22}, {"referenceID": 33, "context": "(See also [34, 35, 36].", "startOffset": 10, "endOffset": 22}, {"referenceID": 30, "context": "The theorem will follow from the hardness of Label Cover [32].", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "The generalizing power of sample compression was independently discovered by [37, 38], and later elaborated upon by [39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 35, "context": "The generalizing power of sample compression was independently discovered by [37, 38], and later elaborated upon by [39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 36, "context": "The generalizing power of sample compression was independently discovered by [37, 38], and later elaborated upon by [39].", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": ", \u03b5-consistent) S\u0303 \u2282 S constitutes a sample compression scheme of size |S\u0303|, as stipulated in [39].", "startOffset": 94, "endOffset": 98}, {"referenceID": 37, "context": "Finally, (2) relied on some fairly intricate fat-shattering arguments [40, 41], while Corollary 1 is an almost immediate consequence of much simpler Occam-type results.", "startOffset": 70, "endOffset": 78}, {"referenceID": 22, "context": "3 In general, ddim(S) \u2264 cddim(X ) for some universal constant c, as shown in [24].", "startOffset": 77, "endOffset": 81}], "year": 2014, "abstractText": "We present the first sample compression algorithm for nearest neighbors with nontrivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.", "creator": "LaTeX with hyperref package"}}}