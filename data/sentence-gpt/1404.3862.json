{"id": "1404.3862", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2014", "title": "Optimizing the CVaR via Sampling", "abstract": "Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains such as finance. In this work we present a new formula for the gradient of the CVaR in the form of a conditional expectation. Our result is similar to policy gradients in the reinforcement learning literature. Based on this formula, we propose novel sampling-based estimators for the CVaR gradient, and a corresponding gradient descent procedure for CVaR optimization. We evaluate our approach in learning a risk-sensitive controller for the game of Tetris, and propose an importance sampling procedure that is suitable for such domains.\n\n\n\n\nThe current approach uses a novel sampling model. Based on this concept, the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters of the algorithm and model parameters", "histories": [["v1", "Tue, 15 Apr 2014 10:32:05 GMT  (79kb,D)", "https://arxiv.org/abs/1404.3862v1", null], ["v2", "Sun, 29 Jun 2014 15:35:36 GMT  (91kb,D)", "http://arxiv.org/abs/1404.3862v2", null], ["v3", "Tue, 16 Sep 2014 15:32:48 GMT  (97kb,D)", "http://arxiv.org/abs/1404.3862v3", null], ["v4", "Sat, 22 Nov 2014 14:44:54 GMT  (115kb,D)", "http://arxiv.org/abs/1404.3862v4", "To appear in AAAI 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["aviv tamar", "yonatan glassner", "shie mannor"], "accepted": true, "id": "1404.3862"}, "pdf": {"name": "1404.3862.pdf", "metadata": {"source": "CRF", "title": "Optimizing the CVaR via Sampling", "authors": ["Aviv Tamar", "Yonatan Glassner", "Shie Mannor"], "emails": ["yglasner}@tx.technion.ac.il,", "shie@ee.technion.ac.il"], "sections": [{"heading": null, "text": "1 Introduction Conditional Value at Risk (CVaR; Rockafellar and Uryasev, 2000) is an established risk measure that has found extensive use in finance among other fields. For a random payoff R, whose distribution is parameterized by a controllable parameter \u03b8, the \u03b1-CVaR is defined as the expected payoff over the \u03b1% worst outcomes of Z:\n\u03a6(\u03b8) = E\u03b8 [R\u2223R \u2264 \u03bd\u03b1(\u03b8)] ,\nwhere \u03bd\u03b1(\u03b8) is the \u03b1-quantile ofR. CVaR optimization aims to find a parameter \u03b8 that maximizes \u03a6(\u03b8).\nWhen the payoff is of the structure R = f\u03b8(X), where f\u03b8 is a deterministic function, and X is random but does not depend on \u03b8, CVaR optimization may be formulated as a stochastic program, and solved using various approaches (Rockafellar and Uryasev 2000; Hong and Liu 2009; Iyengar and Ma 2013). Such a payoff structure is appropriate for certain domains, such as portfolio optimization, in which the investment strategy generally does not affect the asset prices. However, in many important domains, for example queueing systems, resource allocation, and reinforcement learning, the tunable parameters also control the distribution of the random outcomes. Since existing CVaR optimization methods are not suitable for such cases, and due to increased interest in risk-sensitive optimization recently in these domains (Tamar, Di Castro, and Mannor 2012;\nCopyright \u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nPrashanth and Ghavamzadeh 2013), there is a strong incentive to develop more general CVaR optimization algorithms.\nIn this work, we propose a CVaR optimization approach that is applicable when \u03b8 also controls the distribution of X . The basis of our approach is a new formula that we derive for the CVaR gradient \u2202\u03a6(\u03b8)\n\u2202\u03b8 in the form of a conditional\nexpectation. Based on this formula, we propose a samplingbased estimator for the CVaR gradient, and use it to optimize the CVaR by stochastic gradient descent.\nIn addition, we analyze the bias of our estimator, and use the result to prove convergence of the stochastic gradient descent algorithm to a local CVaR optimum. Our method allows us to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risk-sensitive controller for the game of Tetris. To our knowledge, CVaR optimization for such a domain is beyond the reach of existing approaches. Considering Tetris also allows us to easily interpret our results, and show that we indeed learn sensible policies.\nWe remark that in certain domains, CVaR is often not maximized directly, but used as a constraint in an optimization problem of the form max\u03b8 E\u03b8[R] s.t. \u03a6(\u03b8) \u2265 b. Extending our approach to such problems is straightforward, using standard penalty method techniques (see, e.g., Tamar, Di Castro, and Mannor, 2012, and Prashanth and Ghavamzadeh, 2013, for a such an approach with a varianceconstrained objective), since the key component for these methods is the CVaR gradient estimator we provide here. Another appealing property of our estimator is that it naturally incorporates importance sampling, which is important when \u03b1 is small, and the CVaR captures rare events.\nRelated Work Our approach is similar in spirit to the likelihood-ratio method (LR; Glynn, 1990), that estimates the gradient of the expected payoff. The LR method has been successfully applied in diverse domains such as queueing systems, inventory management, and financial engineering (Fu 2006), and also in reinforcement learning (RL; Sutton and Barto, 1998), where it is commonly known as the policy gradient method (Baxter and Bartlett 2001; Peters and Schaal 2008). Our work extends the LR method to estimating the gradient of the CVaR of the payoff.\nClosely related to our work are the studies of Hong and Liu (2009) and Scaillet (2004), who proposed perturbation\nar X\niv :1\n40 4.\n38 62\nv4 [\nst at\n.M L\n] 2\n2 N\nov 2\n01 4\nanalysis style estimators for the gradient of the CVaR, for the setting mentioned above, in which \u03b8 does not affect the distribution of X . Indeed, their gradient formulae are different than ours, and do not apply in our setting.\nLR gradient estimators for other risk measures have been proposed by Borkar (2001) for exponential utility functions, and by Tamar, Di Castro, and Mannor (2012) for mean\u2013 variance. These measures, however, consider a very different notion of risk than the CVaR. For example, the mean\u2013 variance measure is known to underestimate the risk of rare, but catastrophic events (Agarwal and Naik 2004).\nRisk-sensitive optimization in RL is receiving increased interest recently. A mean-variance criterion was considered by Tamar, Di Castro, and Mannor (2012) and Prashanth and Ghavamzadeh (2013). Morimura et al. (2010) consider the expected return, with a CVaR based risk-sensitive policy for guiding the exploration while learning. Their method, however, does not scale to large problems. Borkar and Jain (2014) optimize a CVaR constrained objective using dynamic programming, by augmenting the state space with the accumulated reward. As such, that method is only suitable for a finite horizon and a small state-space, and does not scale-up to problems such as the Tetris domain we consider. A function approximation extension of (Borkar and Jain 2014) is mentioned, using a three time scales stochastic approximation algorithm. In that work, three different learning rates are decreased to 0, and convergence is determined by the slowest one, leading to an overall slow convergence. In contrast, our approach requires only a single learning rate. Recently, Prashanth (2014) used our gradient formula of Proposition 2 (from a preliminary version of this paper) in a two time-scale stochastic approximation scheme to show convergence of CVaR optimization. Besides providing the theoretical basis for that work, our current convergence result (Theorem 5) obviates the need for the extra time-scale, and results in a simpler and faster algorithm.\n2 A CVaR Gradient Formula In this section we present a new LR-style formula for the gradient of the CVaR. This gradient will be used in subsequent sections to optimize the CVaR with respect to some parametric family. We start with a formal definition of the CVaR, and then present a CVaR gradient formula for 1- dimensional random variables. We then extend our result to the multi-dimensional case.\nLet Z denote a random variable with a cumulative distribution function (C.D.F.) FZ(z) = Pr(Z \u2264 z). For convenience, we assume that Z is a continuous random variable, meaning that FZ(z) is everywhere continuous. We also assume that Z is bounded. Given a confidence level \u03b1 \u2208 (0,1), the \u03b1-Value-at-Risk, (VaR; or \u03b1-quantile) of Z is denoted \u03bd\u03b1(Z), and given by\n\u03bd\u03b1(Z) = F \u22121 Z (\u03b1) \u2250 inf {z \u2236 FZ(z) \u2265 \u03b1} . (1)\nThe \u03b1-Conditional-Value-at-Risk of Z is denoted by \u03a6\u03b1(Z) and defined as the expectation of the \u03b1 fraction of the worst outcomes of Z\n\u03a6\u03b1(Z) = E [Z \u2223Z \u2264 \u03bd\u03b1(Z)] . (2)\nWe next present a formula for the sensitivity of \u03a6\u03b1(Z) to changes in FZ(z)."}, {"heading": "2.1 CVaR Gradient of a 1-Dimensional Variable", "text": "Consider again a random variable Z, but now let its probability density function (P.D.F.) fZ(z; \u03b8) be parameterized by a vector \u03b8 \u2208 Rk. We let \u03bd\u03b1(Z; \u03b8) and \u03a6\u03b1(Z; \u03b8) denote the VaR and CVaR of Z as defined in Eq. (1) and (2), when the parameter is \u03b8, respectively.\nWe are interested in the sensitivity of the CVaR to the parameter vector, as expressed by the gradient \u2202\n\u2202\u03b8j \u03a6\u03b1(Z; \u03b8).\nIn all but the most simple cases, calculating the gradient analytically is intractable. Therefore, we derive a formula in which \u2202\n\u2202\u03b8j \u03a6\u03b1(Z; \u03b8) is expressed as a conditional expecta-\ntion, and use it to calculate the gradient by sampling. For technical convenience, we make the following assumption: Assumption 1. Z is a continuous random variable, and bounded in [\u2212b, b] for all \u03b8.\nWe also make the following smoothness assumption on \u03bd\u03b1(Z; \u03b8) and \u03a6\u03b1(Z; \u03b8) Assumption 2. For all \u03b8 and 1 \u2264 j \u2264 k, the gradients \u2202\u03bd\u03b1(Z;\u03b8) \u2202\u03b8j and \u2202\u03a6\u03b1(Z;\u03b8) \u2202\u03b8j exist and are bounded.\nNote that since Z is continuous, Assumption 2 is satisfied whenever \u2202\n\u2202\u03b8j fZ(z; \u03b8) is bounded. Relaxing Assumptions 1\nand 2 is possible, but involves technical details that would complicate the presentation, and is left to future work. The next assumption is standard in LR gradient estimates Assumption 3. For all \u03b8, z, and 1 \u2264 j \u2264 k, we have that \u2202fZ(z;\u03b8) \u2202\u03b8j /fZ(z; \u03b8) exists and is bounded.\nIn the next proposition we present a LR-style sensitivity formula for \u03a6\u03b1(Z; \u03b8), in which the gradient is expressed as a conditional expectation. In Section 3 we shall use this formula to suggest a sampling algorithm for the gradient. Proposition 1. Let Assumptions 1, 2, and 3 hold. Then\n\u2202\u03a6\u03b1(Z;\u03b8)\n\u2202\u03b8j =E\u03b8[\n\u2202logfZ(Z; \u03b8)\n\u2202\u03b8j (Z\u2212\u03bd\u03b1(Z; \u03b8))\u2223Z \u2264\u03bd\u03b1(Z;\u03b8)].\nProof. Define the level-setD\u03b8 ={z \u2208 [\u2212b, b] \u2236 z \u2264 \u03bd\u03b1(Z; \u03b8)} . By definition,D\u03b8 \u2261 [\u2212b, \u03bd\u03b1(Z; \u03b8)], and \u222bz\u2208D\u03b8 fZ (z; \u03b8)dz = \u03b1. Taking a derivative and using the Leibniz rule we obtain\n0 = \u2202\n\u2202\u03b8j \u222b\n\u03bd\u03b1(Z;\u03b8)\n\u2212b fZ (z; \u03b8)dz\n= \u222b\n\u03bd\u03b1(Z;\u03b8)\n\u2212b\n\u2202fZ (z; \u03b8)\n\u2202\u03b8j dz +\n\u2202\u03bd\u03b1(Z; \u03b8)\n\u2202\u03b8j fZ (\u03bd\u03b1(Z; \u03b8); \u03b8) .\n(3)\nBy definition (2) we have \u03a6\u03b1(Z; \u03b8) = \u222bz\u2208D\u03b8 fZ(z;\u03b8)z \u03b1 dz = \u03b1\u22121 \u222b \u03bd\u03b1(Z;\u03b8) \u2212b fZ (z; \u03b8) zdz. Now, taking a derivative and using the Leibniz rule we obtain\n\u2202\n\u2202\u03b8j \u03a6\u03b1(Z; \u03b8) =\u03b1\n\u22121 \u222b\n\u03bd\u03b1(Z;\u03b8)\n\u2212b\n\u2202fZ (z; \u03b8)\n\u2202\u03b8j zdz\n+\u03b1\u22121 \u2202\u03bd\u03b1(Z; \u03b8)\n\u2202\u03b8j fZ(\u03bd\u03b1(Z; \u03b8); \u03b8) \u03bd\u03b1(Z; \u03b8).\n(4)\nRearranging, and plugging (3) in (4) we obtain \u2202 \u2202\u03b8j \u03a6\u03b1(Z; \u03b8) = \u03b1 \u22121 \u222b \u03bd\u03b1(Z;\u03b8) \u2212b \u2202fZ(z;\u03b8) \u2202\u03b8j (z \u2212 \u03bd\u03b1(Z; \u03b8))dz. Finally, using the likelihood ratio trick \u2013 multiplying and dividing by fZ (z; \u03b8) inside the integral, which is justified due to Assumption 3, we obtain the required expectation.\nLet us contrast the CVaR LR formula of Proposition 1 with the standard LR formula for the expectation (Glynn 1990) \u2202\n\u2202\u03b8j E\u03b8[Z] = E\u03b8 [\u2202log fZ(Z;\u03b8) \u2202\u03b8j (Z \u2212 b)], where the\nbaseline b could be any arbitrary constant. Note that in the CVaR case the baseline is specific, and, as seen in the proof, accounts for the sensitivity of the level-set D\u03b8. Quite surprisingly, this specific baseline turns out to be exactly the VaR, \u03bd\u03b1(Z; \u03b8), which, as we shall see later, also leads to an elegant sampling based estimator.\nIn a typical application, Z would correspond to the performance of some system, such as the profit in portfolio optimization, or the total reward in RL. Note that in order to use Proposition 1 in a gradient estimation algorithm, one needs access to \u2202\n\u2202\u03b8j log fZ(Z; \u03b8): the sensitiv-\nity of the performance distribution to the parameters. Typically, the system performance is a complicated function of a high-dimensional random variable. For example, in RL and queueing systems, the performance is a function of a trajectory from a stochastic dynamical system, and calculating its probability distribution is usually intractable. The sensitivity of the trajectory distribution to the parameters, however, is often easy to calculate, since the parameters typically control how the trajectory is generated. We shall now generalize Proposition 1 to such cases. The utility of this generalization is further exemplified in Section 5, for the RL domain."}, {"heading": "2.2 CVaR Gradient Formula \u2013 General Case", "text": "Let X = (X1,X2, . . . ,Xn) denote an n\u2212dimensional random variable with a finite support [\u2212b, b]n, and let Y denote a discrete random variable taking values in some countable set Y . Let fY (y; \u03b8) denote the probability mass function of Y , and let fX\u2223Y (x\u2223y; \u03b8) denote the probability density function of X given Y . Let the reward function r be a bounded mapping from [\u2212b, b]n \u00d7 Y to R, and consider the random variable R \u2250 r(X, Y ). We are interested in a formula for \u2202 \u2202\u03b8j\n\u03a6\u03b1(R; \u03b8). We make the following assumption, similar to Assumptions 1, 2, and 3. Assumption 4. The reward R is a continuous random variable for all \u03b8. Furthermore, for all \u03b8 and 1 \u2264 j \u2264 k, the gradients \u2202\n\u2202\u03b8j \u03bd\u03b1(R; \u03b8) and \u2202\u2202\u03b8j \u03a6\u03b1(R; \u03b8) are well defined and\nbounded. In addition \u2202log fX\u2223Y (x\u2223y;\u03b8) \u2202\u03b8j and \u2202log fY (y;\u03b8) \u2202\u03b8j\nexist and are bounded for all x, y, and \u03b8.\nDefine the level-set Dy;\u03b8 = {x \u2208 [\u2212b, b]n \u2236 r(x, y) \u2264 \u03bd\u03b1(R; \u03b8)} . We require some smoothness of the function r, that is captured by the following assumption on Dy;\u03b8. Assumption 5. For all y and \u03b8, the set Dy;\u03b8 may be written as a finite sum of Ly;\u03b8 disjoint, closed, and connected componentsDiy;\u03b8, each with positive measure:Dy;\u03b8 =\u2211 Ly;\u03b8 i=1 D i y;\u03b8.\nAssumption 5 may satisfied, for example, when r(x, y) is Lipschitz in x for all y \u2208 Y . We now present a sensitivity formula for \u03a6\u03b1(R; \u03b8). Proposition 2. Let Assumption 4 and 5 hold. Then\n\u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8) = E\u03b8 [(\n\u2202log fY (Y ; \u03b8)\n\u2202\u03b8j +\n\u2202log fX\u2223Y (X\u2223Y ; \u03b8)\n\u2202\u03b8j )(R \u2212 \u03bd\u03b1(R; \u03b8))\u2223R \u2264 \u03bd\u03b1(R; \u03b8)] .\nThe proof of Proposition 2 is similar in spirit to the proof of Proposition 1, but involves some additional difficulties of applying the Leibnitz rule in a multidimensional setting. It is given in (?). We reiterate that relaxing Assumptions 4 and 5 is possible, but is technically involved, and left for future work. In the next section we show that the formula in Proposition 2 leads to an effective algorithm for estimating \u2202 \u2202\u03b8j \u03a6\u03b1(R; \u03b8) by sampling.\n3 A CVaR Gradient Estimation Algorithm The sensitivity formula in Proposition 2 suggests a natural Monte\u2013Carlo (MC) estimation algorithm. The method, which we label GCVaR (Gradient estimator for CVaR), is described as follows. Let x1, y1 . . . ,xN , yN be N samples drawn i.i.d. from fX,Y (x, y; \u03b8), the joint distribution of X and Y . We first estimate \u03bd\u03b1(R; \u03b8) using the empirical \u03b1quantile1 v\u0303\nv\u0303 = inf z F\u0302 (z) \u2265 \u03b1, (5)\nwhere F\u0302 (z) is the empirical C.D.F. of R: F\u0302 (z) \u2250 1 N \u2211 N i=1 1r(xi,yi)\u2264z. The MC estimate of the gradient \u2206j;N \u2248\n\u2202 \u2202\u03b8j \u03a6\u03b1(R; \u03b8) is given by\n\u2206j;N = 1\n\u03b1N\nN\n\u2211 i=1\n( \u2202log fY (yi; \u03b8) \u2202\u03b8j + \u2202log fX\u2223Y (xi\u2223yi; \u03b8) \u2202\u03b8j )\u00d7\n\u00d7(r(xi, yi) \u2212 v\u0303)1r(xi,yi)\u2264v\u0303. (6)\nIt is known that the empirical \u03b1-quantile is a biased estimator of \u03bd\u03b1(R; \u03b8). Therefore, \u2206j;N is also a biased estimator of \u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8). In the following we analyze and bound\nthis bias. We first show that \u2206j;N is a consistent estimator. The proof is similar to the proof of Theorem 4.1 in (Hong and Liu 2009), and given in the supplementary material. Theorem 3. Let Assumption 4 and 5 hold. Then \u2206j;N \u2192 \u2202 \u2202\u03b8j \u03a6\u03b1(R; \u03b8) w.p. 1 as N \u2192\u221e.\nWith an additional smoothness assumption we can explicitly bound the bias. Let fR(\u22c5; \u03b8) denote the P.D.F. of R, and define the function g(\u03b2; \u03b8) \u2250 E\u03b8[(\u2202log fY (Y ;\u03b8)\n\u2202\u03b8j + \u2202log fX\u2223Y (X\u2223Y ;\u03b8) \u2202\u03b8j )(R\u2212\u03bd\u03b1(R; \u03b8))\u2223R = \u03b2].\nAssumption 6. For all \u03b8, fR(\u22c5; \u03b8) and g(\u22c5; \u03b8) are continuous at \u03bd\u03b1(R; \u03b8), and fR(\u03bd\u03b1(R; \u03b8); \u03b8) > 0.\n1Algorithmically, this is equivalent to first sorting the r(xi, yi)\u2019s in ascending order, and then selecting v\u0303 as the \u2308\u03b1N\u2309 term in the sorted list.\nAlgorithm 1 GCVaR"}, {"heading": "1: Given:", "text": "\u2022 CVaR level \u03b1 \u2022 A reward function r(x, y) \u2236 Rn \u00d7Y \u2192 R \u2022 Derivatives \u2202\n\u2202\u03b8j of the probability mass function fY (y; \u03b8)\nand probability density function fX\u2223Y (x\u2223y; \u03b8)\n\u2022 An i.i.d. sequence x1, y1, . . . ,xN , yN \u223c fX,Y (x, y; \u03b8). 2: Set rs1, . . . , r s N = Sort (r(x1, y1), . . . , r(xN , yN)) 3: Set v\u0303 = rs\u2308\u03b1N\u2309 4: For j = 1, . . . , k do\n\u2206j;N = 1\n\u03b1N\nN\n\u2211 i=1\n( \u2202log fY (yi; \u03b8) \u2202\u03b8j + \u2202log fX\u2223Y (xi\u2223yi; \u03b8) \u2202\u03b8j )\u00d7\n\u00d7(r(xi, yi) \u2212 v\u0303)1r(xi,yi)\u2264v\u0303\n5: Return:\u22061;N , . . . ,\u2206k;N\nAssumption 6 is similar to Assumption 4 of (Hong and Liu 2009), and may be satisfied, for example, when \u2202log fX\u2223Y (x\u2223y;\u03b8)\n\u2202\u03b8j is continuous and r(x, y) is Lipschitz in x.\nThe next theorem shows that the bias is O(N\u22121/2). The proof, given in the supplementary material, is based on separating the bias to a term that is bounded using a result of Hong and Liu (2009), and an additional term that we bound using well-known results for the bias of empirical quantiles.\nTheorem 4. Let Assumptions 4, 5, and 6 hold. Then E [\u2206j;N ] \u2212 \u2202\u2202\u03b8j \u03a6\u03b1(R; \u03b8) is O(N \u22121/2).\nAt this point, let us again contrast GCVaR with the standard LR method. One may naively presume that applying a standard LR gradient estimator to the \u03b1% worst samples would work as a CVaR gradient estimator. This corresponds to applying the GCVaR algorithm without subtracting the v\u0303 baseline from the reward in (6). Theorems 3 and 4 show that such an estimator would not be consistent. In fact, in the supplementary material we give an example where the gradient error of such an approach may be arbitrarily large.\nIn the sequel, we use GCVaR as part of a stochastic gradient descent algorithm for CVaR optimization. An asymptotically decreasing gradient bias, as may be established from Theorem 3, is necessary to guarantee convergence of such a procedure. Furthermore, the bound of Theorem 4 will allow us to quantify how many samples are needed at each iteration for such convergence to hold."}, {"heading": "Variance Reduction by Importance Sampling", "text": "For very low quantiles, i.e., \u03b1 close to 0, the GCVaR estimator would suffer from a high variance, since the averaging is effectively only over \u03b1N samples. This is a well-known issue in sampling based approaches to VaR and CVaR estimation, and is often mitigated using variance reduction techniques such as Importance Sampling (IS; Rubinstein and Kroese, 2011; Bardou, Frikha, and Page\u0300s, 2009). In IS, the variance of a MC estimator is reduced by using samples\nfrom a different sampling distribution, and suitably modifying the estimator to keep it unbiased. It is straightforward to incorporate IS into LR gradient estimators in general, and to our GCVaR estimator in particular. Due to space constraints, and since this is fairly standard textbook material (e.g., Rubinstein and Kroese, 2011), we provide the full technical details in the supplementary material. In our empirical results we show that using IS indeed leads to significantly better performance.\n4 CVaR Optimization In this section, we consider the setting of Section 2.2, and aim to solve the CVaR optimization problem:\nmax \u03b8\u2208Rk \u03a6\u03b1(R; \u03b8). (7)\nFor this goal we propose CVaRSGD: a stochastic gradient descent algorithm, based on the GCVaR gradient estimator. We now describe the CVaRSGD algorithm in detail, and show that it converges to a local optimum of (7).\nIn CVaRSGD, we start with an arbitrary initial parameter \u03b80 \u2208 Rk. The algorithm proceeds iteratively as follows. At each iteration i of the algorithm, we first sample ni i.i.d. realizations x1, y1, . . . , xni , yni of the random variables X and Y , from the distribution fX,Y (x, y; \u03b8i). We then apply the GCVaR algorithm to obtain an estimate \u2206j;ni of \u2202 \u2202\u03b8j \u03a6\u03b1(R; \u03b8 i), using the samples x1, y1, . . . , xni , yni . Finally, we update the parameter according to\n\u03b8i+1j = \u0393 (\u03b8 i j + i\u2206j;ni) , (8)\nwhere i is a positive step size, and \u0393 \u2236 Rk \u2192 Rk is a projection to some compact set \u0398 with a smooth boundary. The purpose of the projection is to facilitate convergence of the algorithm, by guaranteeing that the iterates remain bounded (this is a common stochastic approximation technique; Kushner and Yin, 2003). In practice, if \u0398 is chosen large enough so that it contains the local optima of \u03a6\u03b1(R; \u03b8), the projection would rarely occur, and would have a negligible effect on the algorithm. Let \u0393\u0302\u03b8(\u03bd) \u2250 lim\u03b4\u21920 \u0393(\u03b8+\u03b4\u03bd)\u2212\u03b8 \u03b4\ndenote an operator that, given a direction of change \u03bd to the parameter \u03b8, returns a modified direction that keeps \u03b8 within \u0398. Consider the following ordinary differential equation:\n\u03b8\u0307 = \u0393\u0302\u03b8 (\u2207\u03a6\u03b1(R; \u03b8)) , \u03b8(0) \u2208 \u0398. (9)\nLet K denote the set of all asymptotically stable equilibria of (9). The next theorem shows that under suitable technical conditions, the CVaRSGD algorithm converges to K almost surely. The theorem is a direct application of Theorem 5.2.1 of Kushner and Yin (2003), and given here without proof.\nTheorem 5. Consider the CVaRSGD algorithm (8). Let Assumptions 4, 5, and 6 hold, and assume that \u03a6\u03b1(R; \u03b8) is continuously differentiable in \u03b8. Also, assume that \u2211\u221ei=1 i = \u221e, \u2211 \u221e i=1 2 i < \u221e, and that \u2211 \u221e i=1 i \u2223E [\u2206j;ni] \u2212 \u2202\u2202\u03b8j \u03a6\u03b1(R; \u03b8 i)\u2223 < \u221e w.p. 1 for all j. Then \u03b8i \u2192 K almost surely.\nNote that from the discussion in Section 3, the requirement \u2211\u221ei=1 i \u2223E [\u2206j;ni] \u2212 \u2202\u2202\u03b8j \u03a6\u03b1(R; \u03b8\ni)\u2223 < \u221e implies that we must have limi\u2192\u221e ni =\u221e. However, the rate of ni could be very slow, for example, using the bound of Theorem 4 the requirement may be satisfied by choosing i = 1/i and ni = (log i) 4.\n5 Application to Reinforcement Learning In this section we show that the CVaRSGD algorithm may be used in an RL policy-gradient type scheme, for optimizing performance criteria that involve the CVaR of the total return. We first describe some preliminaries and our RL setting, and then describe our algorithm.\nWe consider an episodic2 Markov Decision Problem (MDP) in discrete time with a finite state space S and a finite action space A. At time t \u2208 {0,1,2, . . .} the state is st, and an action at is chosen according to a parameterized policy \u03c0\u03b8, which assigns a distribution over actions fa\u2223h(a\u2223h; \u03b8) according to the observed history of states ht = s0, . . . , st. Then, an immediate random reward \u03c1t \u223c f\u03c1\u2223s,a(\u03c1\u2223s, a) is received, and the state transitions to st+1 according to the MDP transition probability fs\u2032\u2223s,a(s\u2032\u2223s, a). We denote by \u03b60 the initial state distribution and by s\u2217 a terminal state, and we assume that for all \u03b8, s\u2217 is reached w.p. 1.\nFor some policy \u03c0\u03b8, let s0, a0, \u03c10, s1, a1, \u03c11, . . . , s\u03c4 denote a state-action-reward trajectory from the MDP under that policy, that terminates at time \u03c4 , i.e., s\u03c4 = s\u2217. The trajectory is a random variable, and we decompose3 it into a discrete part Y \u2250 s0, a0, s1, a1, . . . , s\u2217 and a continuous part X \u2250 \u03c10, \u03c11, . . . , \u03c1\u03c4\u22121. Our quantity of interest is the total reward along the trajectory R \u2250 \u2211\u03c4t=0 \u03c1t. In standard RL, the objective is to find the parameter \u03b8 that maximizes the expected return V (\u03b8) = E\u03b8 [R]. Policy gradient methods (Baxter and Bartlett 2001; Marbach and Tsitsiklis 1998; Peters and Schaal 2008) use simulation to estimate \u2202V (\u03b8)/\u2202\u03b8j , and then perform stochastic gradient ascent on the parameters \u03b8. In this work we are risk-sensitive, and our goal is to maximize the CVaR of the total return J(\u03b8) \u2250 \u03a6\u03b1(R; \u03b8). In the spirit of policy gradient methods, we estimate \u2202J(\u03b8)/\u2202\u03b8j from simulation, using GCVaR, and optimize \u03b8 using CVaRSGD. We now detail our approach.\nFirst, it is well known (Marbach and Tsitsiklis 1998) that by the Markov property of the state transitions:\n\u2202 log fY (Y ; \u03b8) /\u2202\u03b8 = \u03c4\u22121 \u2211 t=0 \u2202 log fa\u2223h(at\u2223ht; \u03b8)/\u2202\u03b8. (10)\nAlso, note that in our formulation we have\n\u2202 log fX\u2223Y (xi\u2223yi; \u03b8) /\u2202\u03b8 = 0, (11)\nsince the reward does not depend on \u03b8 directly. To apply CVaRSGD in the RL setting, at each iteration i of the algorithm we simulate ni trajectories x1, y1, . . . , xni , yni 2Also known as a stochastic shortest path (Bertsekas 2012). 3This decomposition is not restrictive, and used only to illustrate the definitions of Section 2. One may alternatively consider a continuous state space, or discrete rewards, so long as Assumptions 4, 5, and 6 hold.\nof the MDP using policy \u03c0\u03b8i (each xk and yk here together correspond to a single trajectory, as realizations of the random variables X and Y defined above). We then apply the GCVaR algorithm to obtain an estimate \u2206j;ni of \u2202J(\u03b8)/\u2202\u03b8j , using the simulated trajectories x1, y1, . . . , xni , yni , Eq. (10), and Eq. (11). Finally, we update the policy parameter according to Eq. (8). Note that due to Eq. (10), the transition probabilities of the MDP, which are generally not known to the decision maker, are not required for estimating the gradient using GCVaR. Only policy-dependent terms are required.\nWe should remark that for the standard RL criterion V (\u03b8), a Markov policy that depends only on the current state suffices to achieve optimality (Bertsekas 2012). For the CVaR criterion this is not necessarily the case. Ba\u0308uerle and Ott (2011) show that under certain conditions, an augmentation of the current state with a function of the accumulated reward suffices for optimality. In our simulations, we used a Markov policy, and still obtained useful and sensible results.\nAssumptions 4, 5, and 6, that are required for convergence of the algorithm, are reasonable for the RL setting, and may be satisfied, for example, when f\u03c1\u2223s,a(\u03c1\u2223s, a) is smooth, and \u2202 log fa\u2223h(a\u2223h; \u03b8)/\u2202\u03b8j is well defined and bounded. This last condition is standard in policy gradient literature, and a popular policy representation that satisfies it is softmax action selection (Sutton et al. 2000; Marbach and Tsitsik-\nlis 1998), given by fa\u2223h(a\u2223h; \u03b8) = exp(\u03c6(h,a)\n\u22ba \u03b8)\n\u2211a\u2032 exp(\u03c6(h,a\u2032) \u22ba\u03b8) , where\n\u03c6(h, a) \u2208 Rk are a set of k features that depend on the history and action.\nIn some RL domains, the reward takes only discrete values. While this case is not specifically covered by the theory in this paper, one may add an arbitrarily small smooth noise to the total reward for our results to hold. Since such a modification has negligible impact on performance, this issue is of little importance in practice. In our experiments the reward was discrete, and we did not observe any problem."}, {"heading": "5.1 Experimental Results", "text": "We examine Tetris as a test case for our algorithms. Tetris is a popular RL benchmark that has been studied extensively. The main challenge in Tetris is its large state space, which necessitates some form of approximation in the solution technique. Many approaches to learning controllers for Tetris are described in the literature, among them are approximate value iteration (Tsitsiklis and Van Roy 1996), policy gradients (Kakade 2001; Furmston and Barber 2012), and modified policy iteration (Gabillon, Ghavamzadeh, and Scherrer 2013). The standard performance measure in Tetris is the expected number of cleared lines in the game. Here, we are interested in a risk-averse performance measure, captured by the CVaR of the total game score. Our goal in this section is to compare the performance of a policy optimized for the CVaR criterion versus a policy obtained using the standard policy gradient method. As we will show, optimizing the CVaR indeed produces a different policy, characterized by a risk-averse behavior. We note that at present, the best results in the literature (for the standard performance measure) were obtained using a modified policy iteration approach (Gabillon, Ghavamzadeh, and Scherrer 2013), and\nnot using policy gradients. We emphasize that our goal here is not to compete with those results, but rather to illustrate the application of CVaRSGD. We do point out, however, that whether the approach of Gabillon, Ghavamzadeh, and Scherrer (2013) could be extended to handle a CVaR objective is currently not known.\nWe used the regular 10 \u00d7 20 Tetris board with the 7 standard shapes (a.k.a. tetrominos). In order to induce risksensitive behavior, we modified the reward function of the game as follows. The score for clearing 1,2,3 and 4 lines is 1,4,8 and 16 respectively. In addition, we limited the maximum number of steps in the game to 1000. These modifications strengthened the difference between the risk-sensitive and nominal policies, as they induce a tradeoff between clearing many \u2019single\u2019 lines with a low profit, or waiting for the more profitable, but less frequent, \u2019batches\u2019.\nWe used the softmax policy, with the feature set of Thiery and Scherrer (2009). Starting from a fixed policy parameter \u03b80, which was obtained by running several iterations of standard policy gradient (giving both methods a \u2019warm start\u2019), we ran both CVaRSGD and standard policy gradient4 for enough iterations such that both algorithms (approximately) converged. We set \u03b1 = 0.05 and N = 1000.\nIn Fig. 1A and Fig. 1B we present the average return V (\u03b8) and CVaR of the return J(\u03b8) for the policies of both algorithms at each iteration (evaluated by MC on independent trajectories). Observe that for CVaRSGD, the average return has been compromised for a higher CVaR value.\nThis compromise is further explained in Fig. 1C, where we display the reward distribution of the final policies. It may be observed that the left-tail distribution of the CVaR policy is significantly lower than the standard policy. For the risk-sensitive decision maker, such results are very important, especially if the left-tail contains catastrophic outcomes, as is common in many real-world domains, such as finance. To better understand the differences between the policies, we compare the final policy parameters \u03b8 in\n4Standard policy gradient is similar to CVaRSGD when \u03b1 = 1. However, it is common to subtract a baseline from the reward in order to reduce the variance of the gradient estimate. In our experiments, we used the average return < r > as a baseline, and our gradient estimate was 1\nN \u2211 N i=1 \u2202log fY (yi;\u03b8) \u2202\u03b8j (r(xi, yi)\u2212 < r >).\nFig. 1D. The most significant difference is in the parameter that corresponds to the Board Well feature. A well is a succession of unoccupied cells in a column, such that their left and right cells are both occupied. The controller trained by CVaRSGD has a smaller negative weight for this feature, compared to the standard controller, indicating that actions which create deep-wells are repressed. Such wells may lead to a high reward when they get filled, but are risky as they heighten the board.\nTo demonstrate the importance of IS in optimizing the CVaR when \u03b1 is small, we chose\u03b1 = 0.01, andN = 200, and compared CVaRSGD against its IS version, IS CVaRSGD, described in the supplementary material. As Fig. 1E shows, IS GCVaRSGD converged significantly faster, improving the convergence rate by more than a factor of 2. The full details are provided in the supplementary material.\n6 Conclusion and Future Work We presented a novel LR-style formula for the gradient of the CVaR performance criterion. Based on this formula, we proposed a sampling-based gradient estimator, and a stochastic gradient descent procedure for CVaR optimization that is guaranteed to converge to a local optimum. To our knowledge, this is the first extension of the LR method to the CVaR performance criterion, and our results extend CVaR optimization to new domains.\nWe evaluated our approach empirically in an RL domain: learning a risk-sensitive policy for Tetris. To our knowledge, such a domain is beyond the reach of existing CVaR optimization approaches. Moreover, our empirical results show that optimizing the CVaR indeed results in useful risksensitive policies, and motivates the use of simulation-based optimization for risk-sensitive decision making.\nAcknowledgments The authors thank Odalric-Ambrym Maillard for many helpful discussions. The research leading to these results has received funding from the European Research Council under the European Union\u2019s Seventh Framework Program (FP/2007-2013) / ERC Grant Agreement n. 306638.\nReferences [2004] Agarwal, V., and Naik, N. Y. 2004. Risks and portfolio deci-\nsions involving hedge funds. Review of Financial Studies 17(1):63\u2013 98.\n[2009] Bardou, O.; Frikha, N.; and Page\u0300s, G. 2009. Computing var and cvar using stochastic approximation and adaptive unconstrained importance sampling. Monte Carlo Methods and Applications 15(3):173\u2013210.\n[2011] Ba\u0308uerle, N., and Ott, J. 2011. Markov decision processes with average-value-at-risk criteria. Mathematical Methods of Operations Research 74(3):361\u2013379.\n[2001] Baxter, J., and Bartlett, P. L. 2001. Infinite-horizon policygradient estimation. JAIR 15:319\u2013350.\n[2012] Bertsekas, D. P. 2012. Dynamic Programming and Optimal Control, Vol II. Athena Scientific, 4th edition.\n[2014] Borkar, V., and Jain, R. 2014. Risk-constrained Markov decision processes. IEEE TAC PP(99):1\u20131.\n[2001] Borkar, V. S. 2001. A sensitivity formula for risk-sensitive cost and the actor\u2013critic algorithm. Systems & Control Letters 44(5):339\u2013346.\n[2002] Boyan, J. A. 2002. Technical update: Least-squares temporal difference learning. Machine Learning 49(2):233\u2013246.\n[1981] David, H. 1981. Order Statistics. A Wiley publication in applied statistics. Wiley.\n[1973] Flanders, H. 1973. Differentiation under the integral sign. The American Mathematical Monthly 80(6):615\u2013627.\n[2006] Fu, M. C. 2006. Gradient estimation. In Henderson, S. G., and Nelson, B. L., eds., Simulation, volume 13 of Handbooks in Operations Research and Management Science. Elsevier. 575 \u2013 616.\n[2012] Furmston, T., and Barber, D. 2012. A unifying perspective of parametric policy search methods for Markov decision processes. In NIPS.\n[2013] Gabillon, V.; Ghavamzadeh, M.; and Scherrer, B. 2013. Approximate dynamic programming finally performs well in the game of tetris. In NIPS.\n[1990] Glynn, P. W. 1990. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM 33(10):75\u201384.\n[1996] Glynn, P. W. 1996. Importance sampling for monte carlo estimation of quantiles. In Mathematical Methods in Stochastic Simulation and Experimental Design: Proceedings of the 2nd St. Petersburg Workshop on Simulation, 180\u2013185.\n[2009] Hong, L. J., and Liu, G. 2009. Simulating sensitivities of conditional value at risk. Management Science.\n[2013] Iyengar, G., and Ma, A. 2013. Fast gradient descent method for mean-cvar optimization. Annals of Operations Research 205(1):203\u2013212.\n[2001] Kakade, S. 2001. A natural policy gradient. In NIPS. [2003] Kushner, H., and Yin, G. 2003. Stochastic approximation\nand recursive algorithms and applications. Springer Verlag. [1998] Marbach, P., and Tsitsiklis, J. N. 1998. Simulation-based\noptimization of Markov reward processes. IEEE Transactions on Automatic Control 46(2):191\u2013209.\n[2010] Morimura, T.; Sugiyama, M.; Kashima, H.; Hachiya, H.; and Tanaka, T. 2010. Nonparametric return distribution approximation for reinforcement learning. In ICML, 799\u2013806.\n[2008] Peters, J., and Schaal, S. 2008. Reinforcement learning of motor skills with policy gradients. Neural Networks 21(4):682\u2013 697.\n[2013] Prashanth, L., and Ghavamzadeh, M. 2013. Actor-critic algorithms for risk-sensitive mdps. In NIPS.\n[2014] Prashanth, L. 2014. Policy gradients for CVaR-constrained MDPs. In International Conference on Algorithmic Learning Theory.\n[2000] Rockafellar, R. T., and Uryasev, S. 2000. Optimization of conditional value-at-risk. Journal of risk 2:21\u201342.\n[2011] Rubinstein, R. Y., and Kroese, D. P. 2011. Simulation and the Monte Carlo method. John Wiley & Sons.\n[2004] Scaillet, O. 2004. Nonparametric estimation and sensitivity analysis of expected shortfall. Mathematical Finance.\n[1998] Sutton, R. S., and Barto, A. G. 1998. Reinforcement learning: An introduction. Cambridge Univ Press.\n[2000] Sutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y. 2000. Policy gradient methods for reinforcement learning with function approximation. In NIPS.\n[2012] Tamar, A.; Di Castro, D.; and Mannor, S. 2012. Policy gradients with variance related risk criteria. In ICML.\n[2009] Thiery, C., and Scherrer, B. 2009. Improvements on learning tetris with cross entropy. International Computer Games Association Journal 32.\n[1996] Tsitsiklis, J. N., and Van Roy, B. 1996. Feature-based methods for large scale dynamic programming. Machine Learning 22(1- 3):59\u201394.\nA Proof of Proposition 2 Proof. The main difficulty in extending the proof of Proposition 1 to this case is in applying the Leibnitz rule in a multidimensional case. Such an extension is given by (Flanders 1973), which we now state.\nWe are given an n\u2212dimensional \u03b8\u2212dependent chain (field of integration) D\u03b8 in Rn. We also have an exterior differential n\u2212form whose coefficients are \u03b8-dependent:\n\u03c9 = f(x, \u03b8)dx1 \u2227\u22ef \u2227 dxn\nThe general Leibnitz rule5 is given by \u2202\n\u2202\u03b8 \u222b D\u03b8 \u03c9 = \u222b \u2202D\u03b8 v \u231f \u03c9 + \u222b D\u03b8\n\u2202\u03c9 \u2202\u03b8 (12)\nwhere v denotes the vector field of velocities \u2202 \u2202\u03b8 x of D\u03b8, and v\u231f\u03c9 denotes the interior product between v and \u03c9 (see (Flanders 1973) for more details). We now write the CVaR explicitly as\n\u03a6\u03b1(R; \u03b8) = 1\n\u03b1 \u2211 y\u2208Y fY (y; \u03b8)\u222b x\u2208Dy;\u03b8 fX\u2223Y (x\u2223y; \u03b8)r(x, y)dx\n= 1\n\u03b1 \u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1 \u222b x\u2208Di y;\u03b8\nfX\u2223Y (x\u2223y; \u03b8)r(x, y)dx,\ntherefore\n\u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8) =\n1 \u03b1 \u2211 y\u2208Y \u2202fY (y; \u03b8) \u2202\u03b8j\nLy;\u03b8\n\u2211 i=1 \u222b x\u2208Di y;\u03b8\nfX\u2223Y (x\u2223y; \u03b8)r(x, y)dx\n+ 1\n\u03b1 \u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1\n\u2202\n\u2202\u03b8j \u222b x\u2208Di\ny;\u03b8\nfX\u2223Y (x\u2223y; \u03b8)r(x, y)dx\n(13)\nWe now treat each Diy;\u03b8 in the last sum separately. Let X denote the set [\u2212b, b] n over which X is defined. Obviously, Diy;\u03b8 \u2282 X .\nWe now make an important observation. By definition of the level-set Diy;\u03b8, and since it is closed by Assumption 5, for every x \u2208 \u2202Diy;\u03b8 we have that either (a) r(x, y) = \u03bd\u03b1(R; \u03b8), (14) or\n(b) x \u2208 \u2202X , and r(x, y) < \u03bd\u03b1(R; \u03b8). (15)\nWe write \u2202Diy;\u03b8 = \u2202D i,a y;\u03b8 + \u2202D i,b y;\u03b8 where the two last terms correspond to the two possibilities in (14) and (15).\nWe now claim that for the boundary term \u2202Di,by;\u03b8, we have\n\u222b \u2202Di,b\ny;\u03b8\nv \u231f \u03c9 = 0. (16)\nTo see this, first note that by definition of X , the boundary \u2202X is smooth and has a unique normal vector at each point, except for a set of measure zero (the corners of X ). Let \u2202D\u0303i,by;\u03b8 denote the set of all points in \u2202D i,b y;\u03b8 for which a unique normal vector exists. For each x \u2208 \u2202D\u0303i,by;\u03b8 we let v\u22a5 and v\u2225 denote the normal and tangent (with respect to \u2202X ) elements of the velocity \u2202 \u2202\u03b8 x at x, respectively. Thus, v = v\u22a5 + v\u2225.\nFor some > 0 let d denote the set {x \u2208 \u2202D i,b y;\u03b8 \u2236 r(x, y) < \u03bd\u03b1(R; \u03b8) \u2212 }. From Assumption 4 we have that \u2202 \u2202\u03b8j \u03bd\u03b1(R; \u03b8) is bounded, therefore there exists \u03b4( ) > 0 such that for all \u03b8\u2032 that satisfy \u2225\u03b8 \u2212 \u03b8\u2032\u2225 < \u03b4( ) we have \u2223\u03bd\u03b1(R; \u03b8\u2032) \u2212 \u03bd\u03b1(R; \u03b8)\u2223 < , and therefore d \u2208 \u2202D\ni,b y;\u03b8\u2032 . Since this holds for every > 0, we conclude that a small change in \u03b8 does not change \u2202D i,b y;\u03b8, and\ntherefore we have v\u22a5 = 0, \u2200x \u2208 \u2202D\u0303 i,b y;\u03b8.\nFurthermore, by definition of the interior product we have\nv\u2225 \u231f \u03c9 = 0.\n5The formula in (Flanders 1973) is for a more general case whereD\u03b8 is not necessarily n\u2212dimensional. That formula includes an additional term \u222bD\u03b8 v \u231f dx\u03c9, where dx is the exterior derivative, which cancels in our case.\nTherefore we have\n\u222b \u2202Di,b\ny;\u03b8\nv \u231f \u03c9 = \u222b \u2202D\u0303i,b\ny;\u03b8\nv \u231f \u03c9 = \u222b \u2202D\u0303i,b\ny;\u03b8\nv\u2225 \u231f \u03c9 = 0,\nand the claim follows. Now, let \u03c9y = fX\u2223Y (x\u2223y; \u03b8)r(x, y)dx1 \u2227\u22ef \u2227 dxn. Using (12), we have\n\u2202\n\u2202\u03b8j \u222b x\u2208Di\ny;\u03b8\n\u03c9y = \u222b \u2202Di\ny;\u03b8 v \u231f \u03c9y + \u222b Di y;\u03b8\n\u2202\u03c9y\n\u2202\u03b8\n= \u222b \u2202Di,a\ny;\u03b8 v \u231f \u03c9y + \u222b Di y;\u03b8\n\u2202\u03c9y\n\u2202\u03b8\n(17)\nwhere the last equality follows from (16) and the definition of v. Let \u03c9\u0303y = fX\u2223Y (x\u2223y; \u03b8)dx1 \u2227\u22ef \u2227 dxn. By the definition of Dy;\u03b8 we have that for all \u03b8\n\u03b1 = \u2211 y\u2208Y fY (y; \u03b8)\u222b Dy;\u03b8 \u03c9\u0303y,\ntherefore, by taking a derivative, and using (16) we have\n0 = \u2202\n\u2202\u03b8j\n\u239b \u239d \u2211 y\u2208Y fY (y; \u03b8)\u222b Dy;\u03b8 \u03c9\u0303y \u239e \u23a0 =\u2211 y\u2208Y \u2202fY (y; \u03b8) \u2202\u03b8j \u222b Dy;\u03b8 \u03c9\u0303y\n+ \u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1 (\u222b \u2202Di,a y;\u03b8 v \u231f \u03c9\u0303y + \u222b Di y;\u03b8\n\u2202\u03c9\u0303y\n\u2202\u03b8 )\n(18)\nFrom (14), and linearity of the interior product we have\n\u222b \u2202Di,a\ny;\u03b8\nv \u231f \u03c9y = \u03bd\u03b1(R; \u03b8)\u222b \u2202Di,a\ny;\u03b8\nv \u231f \u03c9\u0303y,\ntherefore, plugging in (18) we have\n\u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1 \u222b \u2202Di,a y;\u03b8 v \u231f \u03c9y = \u2212 \u03bd\u03b1(R; \u03b8)\u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1 \u222b Di y;\u03b8\n\u2202\u03c9\u0303y\n\u2202\u03b8\n\u2212 \u03bd\u03b1(R; \u03b8)\u2211 y\u2208Y\n\u2202fY (y; \u03b8)\n\u2202\u03b8j \u222b Dy;\u03b8\n\u03c9\u0303y\n(19)\nNow, note that from (13) and (17) we have\n\u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8) =\n1 \u03b1 \u2211 y\u2208Y \u2202fY (y; \u03b8) \u2202\u03b8j\nLy;\u03b8\n\u2211 i=1 \u222b x\u2208Di y;\u03b8\n\u03c9y\n+ 1\n\u03b1 \u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1 \u222b Di y;\u03b8\n\u2202\u03c9y\n\u2202\u03b8\n+ 1\n\u03b1 \u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1 \u222b \u2202Di,a y;\u03b8\nv \u231f \u03c9y,\nand by plugging in (19) we obtain\n\u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8) =\n1 \u03b1 \u2211 y\u2208Y \u2202fY (y; \u03b8) \u2202\u03b8j\nLy;\u03b8\n\u2211 i=1 \u222b Di y;\u03b8\n\u03c9y \u2212 \u03bd\u03b1(R; \u03b8)\u03c9\u0303y\n+ 1\n\u03b1 \u2211 y\u2208Y\nfY (y; \u03b8) Ly;\u03b8\n\u2211 i=1 \u222b Di y;\u03b8\n\u2202\u03c9y\n\u2202\u03b8 \u2212 \u03bd\u03b1(R; \u03b8)\n\u2202\u03c9\u0303y\n\u2202\u03b8 .\nFinally, using the standard likelihood ratio trick \u2013 multiplying and dividing by fY (y; \u03b8) inside the first sum, and multiplying and dividing by fX\u2223Y (x\u2223y; \u03b8) inside the second integral we obtain the required expectation.\nB Proof of Theorem 3 Proof. Let \u03bd = \u03bd\u03b1(R; \u03b8). To simplify notation, we also introduce the functions h1(x, y) \u2250 ( \u2202log fY (y;\u03b8)\n\u2202\u03b8j + \u2202log fX\u2223Y (x\u2223y;\u03b8) \u2202\u03b8j ) r(x, y), and h2(x, y) \u2250 ( \u2202log fY (y;\u03b8) \u2202\u03b8j + \u2202log fX\u2223Y (x\u2223y;\u03b8) \u2202\u03b8j ). Thus we have\n\u2206j;N = 1\n\u03b1N\nN\n\u2211 i=1 (h1(xi, yi) \u2212 h2(xi, yi)v\u0303)1r(xi,yi)\u2264v\u0303\n= 1\n\u03b1N\nN\n\u2211 i=1 (h1(xi, yi) \u2212 h2(xi, yi)\u03bd)1r(xi,yi)\u2264\u03bd\n+ 1\n\u03b1N\nN\n\u2211 i=1 (h1(xi, yi) \u2212 h2(xi, yi)\u03bd) (1r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd)\n+ (\u03bd \u2212 v\u0303) 1\n\u03b1N\nN \u2211 i=1 h2(xi, yi) (1r(xi,yi)\u2264v\u0303)\n(20)\nWe furthermore let D(x, y) \u2250 h1(x, y) \u2212 h2(x, y)\u03bd. Note that by Assumption 4, D is bounded. By Proposition 2, and the strong law of large numbers, we have that w.p. 1\n1\n\u03b1N\nN\n\u2211 i=1\n(h1(xi, yi) \u2212 h2(xi, yi)\u03bd)1r(xi,yi)\u2264\u03bd \u2192 \u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8). (21)\nWe now show that the two additional terms in (20) vanish as N \u2192\u221e. By Ho\u0308lder\u2019s inequality\n\u2223 1\nN\nN \u2211 i=1 D(xi, yi) (1r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd)\u2223 \u2264 ( 1 N N \u2211 i=1 \u2223D(xi, yi)\u2223 2 )\n0.5\n\u22c5 ( 1\nN\nN\n\u2211 i=1\n\u22231r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd \u2223 2 )\n0.5\n, (22)\nand ( 1 N \u2211 N i=1 \u2223D(xi, yi)\u2223 2 )\n0.5 is bounded. Also, note that\n1\nN\nN\n\u2211 i=1\n\u22231r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd \u2223 2 =\n1\nN\nN\n\u2211 i=1 \u22231r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd \u2223\n= (1v\u0303\u2264\u03bd \u2212 1\u03bd\u2264v\u0303) 1\nN\nN\n\u2211 i=1 (1r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd)\nBy Proposition 4.1 of (Hong and Liu 2009), we have that w.p. 1\n(1v\u0303\u2264\u03bd \u2212 1\u03bd\u2264v\u0303) 1\nN\nN\n\u2211 i=1 (1r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd)\u2192 0.\nBy the continuous mapping theorem, we thus have that w.p. 1\n( 1\nN\nN\n\u2211 i=1\n\u22231r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd \u2223 2 )\n0.5\n\u2192 0,\ntherefore, using Eq. (22) we have that w.p. 1\n1\nN\nN \u2211 i=1 D(xi, yi) (1r(xi,yi)\u2264v\u0303 \u2212 1r(xi,yi)\u2264\u03bd)\u2192 0. (23)\nWe now turn to the last sum in (20). by Assumption 4, h2 is bounded, and therefore 1\u03b1N \u2211 N i=1 h2(xi, yi) (1r(xi,yi)\u2264v\u0303) is bounded. It is well-known (David 1981) that the sample-quantile is a consistent estimator, thus \u03bd \u2212 v\u0303 \u2192 0, and therefore\n(\u03bd \u2212 v\u0303) 1\n\u03b1N\nN \u2211 i=1 h2(xi, yi) (1r(xi,yi)\u2264v\u0303)\u2192 0. (24)\nPlugging (21), (23), and (24) in (20) gives the stated result.\nC Proof of Theorem 4 We follow the notation of Section B.\nIn our analysis we use a result of (Hong and Liu 2009), which we now state. Let x1, y1, . . . ,xN , yN be N samples drawn i.i.d. from fX,Y (x, y; \u03b8).\nTheorem 6. (Theorem 4.2 of (Hong and Liu 2009)) Let Assumption 6, and the assumptions required for Proposition 2 hold. Let\n\u2206\u0304N = 1\n\u03b1N\nN \u2211 i=1 D(xi, yi) \u22c5 1r(xi,yi)\u2264v\u0303.\nThen E [\u2206\u0304N ] \u2212 \u2202\u2202\u03b8j \u03a6\u03b1(R; \u03b8) is o(N \u22121/2).\nIn the original theorem of (Hong and Liu 2009), D is defined differently, corresponding to the perturbation analysis type gradient estimator. However, the proof of the theorem follows through also with our definition of D, and using Proposition 2.\nWe are now ready to prove Theorem 4.\nProof. From Eq. (20) we have\nE [\u2206j;N ] \u2212 \u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8) =E [\n1\n\u03b1N\nN \u2211 i=1 D(xi, yi) \u22c5 1r(xi,yi)\u2264v\u0303]\n+E [(\u03bd \u2212 v\u0303) 1\n\u03b1N\nN \u2211 i=1 h2(xi, yi) (1r(xi,yi)\u2264v\u0303)] .\n(25)\nThe first term in the r.h.s. of Eq. (25) is o(N\u22121/2) by Theorem 6. We now bound the second term. Let h\u03042 denote a bound on h2, which, by Assumption 4, is finite. Note that we have \u2223 1N \u2211 N i=1 h2(xi, yi) (1r(xi,yi)\u2264v\u0303)\u2223 \u2264 h\u03042 with probability 1. Therefore,\nE [(\u03bd \u2212 v\u0303) 1\n\u03b1N\nN \u2211 i=1 h2(xi, yi) (1r(xi,yi)\u2264v\u0303)] \u2264 E [\u2223\u03bd \u2212 v\u0303\u2223 \u2223 1 \u03b1N N \u2211 i=1 h2(xi, yi) (1r(xi,yi)\u2264v\u0303)\u2223] \u2264 h\u03042 \u03b1 E [\u2223\u03bd \u2212 v\u0303\u2223] .\nWe will show that E [\u2223\u03bd \u2212 v\u0303\u2223] is O(N\u22121/2). It is well-known (David 1981) that the empirical \u03b1\u2212quantile may be written as follows:\nv\u0303 = \u03bd \u2212 F\u0302R(\u03bd) \u2212 \u03b1\nfR(\u03bd) + R\u0303, (26)\nwhere F\u0302R(\u22c5) is the empirical C.D.F. of R, and R\u0303 is O(N\u22121/2) in probability. Thus, we have\nE [\u2223\u03bd \u2212 v\u0303\u2223] \u2264 fR(\u03bd)\u22121 (E [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223] +E [\u2223R\u0303\u2223]) . (27)\nNote that since R is bounded, v\u0303 is also bounded, and it is clear from Eq. (26) that R\u0303 is bounded, and therefore uniformly integrable. Since R\u0303 is also O(N\u22121/2) in probability, we conclude that E [\u2223R\u0303\u2223] is O(N\u22121/2). Let yi \u2250 1r(xi,yi)\u2264\u03bd . Then by definition, the empirical C.D.F. satisfies\nF\u0302R(\u03bd) = 1\nN\nN \u2211 i=1 yi,\nand the yi\u2019s are i.i.d., and satisfy E[yi] = \u03b1, and Var[yi] = \u03b1(1 \u2212 \u03b1). Observe that\n0 \u2264 Var [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223] = E [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223 2 ] \u2212 (E [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223]) 2 ,\ntherefore\nE [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223] \u2264 \u221a E [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223 2 ],\nbut\nE [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223 2 ] = Var [F\u0302R(\u03bd)] =\n\u03b1(1 \u2212 \u03b1)\nN ,\ntherefore E [\u2223F\u0302R(\u03bd) \u2212 \u03b1\u2223] is O(N\u22121/2). From Eq. (27) we thus have that E [\u2223\u03bd \u2212 v\u0303\u2223] is O(N\u22121/2), which completes the proof.\nD Example: the Importance of the VaR Baseline in GCVaR Here we show that the subtraction of the VaR baseline from the reward in GCVaR (Eq. (6)) is crucial, and without it the error in the gradient estimate may be arbitrarily large.\nConsider the following example, in the setting of proposition 1: Z \u223c Normal(\u03b8,1), and \u03b1 = 0.5. The true CVaR gradient is constant:\n\u2202\n\u2202\u03b8j \u03a6\u03b1(Z;\u03b8) = E\u03b8 [\n\u2202logfZ(Z; \u03b8)\n\u2202\u03b8j (Z \u2212 \u03bd\u03b1(Z; \u03b8))\u2223Z \u2264\u03bd\u03b1(Z;\u03b8)] = 1,\nwhile the term due to the baseline is\nE\u03b8 [ \u2202logfZ(Z; \u03b8)\n\u2202\u03b8j (\u2212\u03bd\u03b1(Z; \u03b8))\u2223Z \u2264\u03bd\u03b1(Z;\u03b8)] = \u2212\n\u221a 2\n\u03c0 \u03b8,\nwhich is unbounded in \u03b8. Thus, we have that E\u03b8 [\u2202logfZ(Z;\u03b8)\n\u2202\u03b8j (Z)\u2223Z \u2264\u03bd\u03b1(Z;\u03b8)] = 1 + \u221a 2 \u03c0 \u03b8, meaning that a naive estimator without the baseline may\nhave an arbitrarily large error, and, for \u03b8 < \u2212 \u221a \u03c0 2 , would even point in the opposite direction!\nE Importance Sampling For very low quantiles, i.e., \u03b1 close to 0, the estimator GCVaR of Eq. (6) would have a high variance, since the averaging is effectively only over \u03b1N samples. In order to mitigate this problem, we now propose an importance sampling procedure for estimating \u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8).\nImportance sampling (IS; (Rubinstein and Kroese 2011)) is a general procedure for reducing the variance of Monte\u2013Carlo (MC) estimates. We first describe it in a general context, and then give the specific implementation for the CVaR sensitivity estimator."}, {"heading": "E.1 Background", "text": "Consider the following general problem. We wish to estimate the expectation l = E [H(X)] where X is a random variable with P.D.F. f(x), and H(x) is some function. The MC solution is given by l\u0302 = 1\nN \u2211 N i=1H(xi), where xi \u223c f are drawn i.i.d.\nThe IS method aims to reduce the variance of the MC estimator by using a different sampling distribution for the samples xi. Assume we are given a sampling distribution g(x), and that g dominates f in the sense that g(x) = 0\u21d2 f(x) = 0. We let Ef and Eg denote expectations w.r.t. f and g, respectively. Observe that l = Ef [H(X)] = Eg [H(X) f(X)g(X) ] ,and we thus define the IS estimator l\u0302IS as\nl\u0302IS = 1\nN\nN \u2211 i=1 H(xi) f(xi) g(xi) , (28)\nwhere the xi\u2019s are drawn i.i.d., and now xi \u223c g. Obviously, selecting an appropriate g such that l\u0302IS indeed has a lower variance than l\u0302 is the heart of the problem. One approach is by the variance minimization method (Rubinstein and Kroese 2011). Here, we are given a family of distributions g(x;\u03c9) parameterized by \u03c9, and we aim to find an \u03c9 that minimizes the variance V (\u03c9) = Varxi\u223cg(\u22c5;\u03c9) (l\u0302IS). A straightforward calculation shows that V (\u03c9) = Ef [H(X)2 f(X) g(X;\u03c9)] \u2212 l 2, and since l does not depend on \u03c9, we are left with the optimization problem min\u03c9 Ef [H(X)2 f(X)g(X;\u03c9)] , which is typically solved approximately, by solving the sampled average approximation (SAA)\nmin \u03c9\n1\nNSAA\nNSAA\n\u2211 i=1\n[H(xi) 2 f(xi)\ng(xi;\u03c9) ] , (29)\nwhere xi \u223c f are i.i.d. Numerically, the SAA may be solved using (deterministic) gradient descent, by noting that \u2202\u2202\u03c9 ( f(xi) g(xi;\u03c9)) = \u2212 f(xi) g(xi;\u03c9) \u2202 \u2202\u03c9 log g(xi;\u03c9).\nThus, in order to find an IS distribution g from a family of distributions g(x;\u03c9), we draw NSAA samples from the original distribution f , and solve the SAA (29) to obtain the optimal \u03c9. We now describe how this procedure is applied for estimating the CVaR sensitivity \u2202\n\u2202\u03b8j \u03a6\u03b1(R; \u03b8).\nE.2 IS Estimate for CVaR Sensitivity We recall the setting of Proposition 2, and assume that in addition to fX,Y (x, y; \u03b8) we have access to a family of distributions gX,Y (x, y; \u03b8,\u03c9) parameterized by \u03c9. We follow the procedure outlined above and, using Proposition 2, set\nHj(X, Y ) = 1 \u03b1 ( \u2202log fY (Y ; \u03b8) \u2202\u03b8j + \u2202log fX\u2223Y (X\u2223Y ; \u03b8) \u2202\u03b8j )(R \u2212 \u03bd\u03b1(R; \u03b8))1R\u2264\u03bd\u03b1(R;\u03b8).\nHowever, since \u03bd\u03b1(R; \u03b8) is not known in advance, we need a procedure for estimating it in order to plug it into Eq. (28). The empirical quantile v\u0303 of Eq. (5) is not suitable since it uses samples from fX,Y (x, y; \u03b8). Thus, we require an IS estimator for \u03bd\u03b1(R; \u03b8) as well. Such was proposed by Glynn (1996). Let F\u0302IS(z) denote the IS empirical C.D.F. of R: F\u0302IS(z) \u2250 1 N \u2211 N i=1 fX,Y (xi,yi;\u03b8) gX,Y (xi,yi;\u03b8,\u03c9)1r(xi,yi)\u2264z. Then, the IS empirical VaR is given by\nv\u0303IS = inf z F\u0302IS(z) \u2265 \u03b1. (30)\nWe also need to modify the variance minimization method, as we are not estimating a scalar function but a gradient in Rk. We assume independence between the elements, and replace H(xi)2 in Eq. (29) with \u2211kj=1Hj(xi)\n2. Let us now state the estimation procedure explicitly. We first drawNSAA i.i.d. samples from fX,Y (x, y; \u03b8), and find a suitable\n\u03c9 by solving the following equivalent of (29)\nmin \u03c9\n1\nNSAA\nNSAA\n\u2211 i=1 \u23a1 \u23a2 \u23a2 \u23a2 \u23a3 k \u2211 j=1 Hj(xi, yi) 2 fX,Y (xi, yi; \u03b8) gX,Y (xi, yi; \u03b8,\u03c9) \u23a4 \u23a5 \u23a5 \u23a5 \u23a6 , (31)\nwith Hj(X, Y ) = 1\u03b1 ( \u2202log fY (Y ;\u03b8) \u2202\u03b8j + \u2202log fX\u2223Y (X\u2223Y ;\u03b8) \u2202\u03b8j ) (r(X, Y ) \u2212 v\u0303)1r(X,Y )\u2264v\u0303 , where v\u0303 is given in (5).\nWe then run the IS GCVaR algorithm, as follows. We draw N i.i.d. samples x1, y1, . . . ,xN , yN from gX,Y (x, y; \u03b8,\u03c9). The IS estimate of the CVaR gradient \u2206ISj;N is given by\n\u2206ISj;N = 1\n\u03b1N\nN\n\u2211 i=1\n( \u2202log fY (yi; \u03b8) \u2202\u03b8j + \u2202log fX\u2223Y (xi\u2223yi; \u03b8) \u2202\u03b8j ) fX,Y (xi, yi; \u03b8)(r(xi, yi) \u2212 v\u0303IS)1r(xi,yi)\u2264v\u0303IS gX,Y (xi, yi; \u03b8,\u03c9) , (32)\nwhere v\u0303IS is given in (30).\nAlgorithm 2 IS GCVaR"}, {"heading": "1: Given:", "text": "\u2022 CVaR level \u03b1 \u2022 A reward function r(x, y) \u2236 Rn\u2297Y \u2192 R\u2192 R \u2022 A density function fX,Y (x, y; \u03b8) \u2022 A density function gX,Y (x, y; \u03b8) \u2022 A sequence x1, y1, . . . ,xN , yN \u223c gX,Y , i.i.d. 2: Set xs1, y s 1 . . . ,x s N , y s N = Sort (x1, y1, . . . ,xN , yN) by r(x, y) 3: For i = 1, . . . ,N do\nL(i) = i\n\u2211 j=1\nfX,Y (x s j , y s j ; \u03b8) /gX,Y (x s j , y s j ; \u03b8)\n4: Set l = arg miniL(i) \u2265 \u03b1 5: Set v\u0303IS = r(xsl , y s l ) 6: For j = 1, . . . , k do\n\u2206ISj;N = 1\n\u03b1N\nN\n\u2211 i=1\n( \u2202log fY (yi; \u03b8) \u2202\u03b8j + \u2202log fX\u2223Y (xi\u2223yi; \u03b8) \u2202\u03b8j ) fX,Y (xi, yi; \u03b8)(r(xi, yi) \u2212 v\u0303IS)1r(xi,yi)\u2264v\u0303IS gX,Y (xi, yi; \u03b8) ,\n7: Return:\u2206IS1;N , . . . ,\u2206 IS k;N\nNote that in our SAA program for finding \u03c9, we estimate \u03bd\u03b1 using crude Monte Carlo. In principle, IS may also be used for that estimate as well, with an additional optimization process for finding a suitable sampling distribution. However, a typical application of the CVaR gradient is in optimization of \u03b8 by stochastic gradient descent. There, one only needs to update \u03c9 intermittently, therefore a large sample size NSAA is affordable and IS is not needed.\nSo far, we have not discussed how the parameterized distribution family gX,Y (x, y; \u03b8,\u03c9) is obtained. While there are some standard approaches such as exponential tilting (Rubinstein and Kroese 2011), this task typically requires some domain knowledge. For the RL domain, we present a heuristic method for selecting gX,Y (x, y; \u03b8,\u03c9)."}, {"heading": "E.3 CVaR Policy Gradient with Importance Sampling", "text": "As explained earlier, when dealing with small values of \u03b1, an IS scheme may help reduce the variance of the CVaR gradient estimator. In this section, we apply the IS estimator to the RL domain. As is typical in IS, the main difficulty is finding a suitable sampling distribution, and actually sampling from it. In RL, a natural method for modifying the trajectory distribution is by modifying the MDP transition probabilities. We note, however, that by such our method actually requires access to a simulator of this modified MDP. In many applications a simulator of the original system is available anyway, thus modifying it should not be a problem.\nConsider the RL setting of Section 5, and denote the original MDP by M . The P.D.F. of a trajectory {X,Y } from the MDP M , where, as defined in the main text Y \u2250 s0, a0, s1, a1, . . . , s\u03c4 ,X \u2250 \u03c10, \u03c11, . . . , \u03c1\u03c4\u22121 is given by\nfX,Y (x, y; \u03b8) = \u03b60(s0) \u03c4\u22121 \u220f t=0 fa\u2223s(at\u2223st; \u03b8)f\u03c1\u2223s,a(\u03c1\u2223st, at)fs\u2032\u2223s,a(st+1\u2223st, at).\nConsider now an MDP M\u0302 that is similar to the original MDP M but with transition probabilities f\u0302s\u2032\u2223s,a(s\u2032\u2223s, a;\u03c9), where \u03c9 is some controllable parameter. We will later specify f\u0302s\u2032\u2223s,a(s\u2032\u2223s, a;\u03c9) explicitly, but for now, observe that the P.D.F. of a trajectory {X,Y } from the MDP M\u0302 is given by\ngX,Y (x, y; \u03b8,\u03c9) = \u03b60(s0) \u03c4\u22121 \u220f t=0 fa\u2223s(at\u2223st; \u03b8)f\u03c1\u2223s,a(\u03c1\u2223st, at)f\u0302s\u2032\u2223s,a(st+1\u2223st, at;\u03c9).\nand therefore fX,Y (x, y; \u03b8)\ngX,Y (x, y; \u03b8,\u03c9) = \u03c4\u22121 \u220f t=0 fs\u2032\u2223s,a(st+1\u2223st, at) f\u0302s\u2032\u2223s,a(st+1\u2223st, at;\u03c9) . (33)\nUsing Eq. (10), Eq. (33), and the fact that \u2202 log fX\u2223Y (xi\u2223yi; \u03b8)/\u2202\u03b8=0 in our formulation, the IS GCVaR algorithm may be used to obtain the IS estimated gradient \u2206ISj;N , which may then be used instead of \u2206j;N in the parameter update equation (8).\nWe now turn to the problem of choosing the transition probabilities f\u0302s\u2032\u2223s,a(s\u2032\u2223s, a;\u03c9) in the MDP M\u0302 , and propose a heuristic approach that is suitable for the RL domain. We first observe that by definition, the CVaR takes into account only the \u2018worst\u2019 trajectories for a given policy, therefore a suitable IS distribution should give more weight to such bad outcomes in some sense. The difficulty is how to modify the transition probabilities, which are defined per state, such that the whole trajectory will be \u2018bad\u2019. We note that this difficulty is in a sense opposite to the action selection problem: how to choose an action at each state such that the long-term reward is high. Action selection is a fundamental task in RL, and has a very elegant solution, which inspires our IS approach.\nA standard approach to action selection is through the value-function V (s) (Sutton and Barto 1998), which assigns to each state s its expected long term outcome E [B\u2223s0 = s] under the current policy. Once the value function is known, the \u2018greedy selection\u2019 rule selects the action that maximizes the expected value of the next state. The intuition behind this rule is that since V (s) captures the long-term return from s, states with higher values lead to better trajectories, and should be preferred. By a similar reasoning, we expect that encouraging transitions to low-valued states will produce worse trajectories. We thus propose the following heuristic for the transition probabilities f\u0302s\u2032\u2223s,a(s\u2032\u2223s, a;\u03c9). Assume that we have access to an approximate value function V\u0303 (s) for each state. We propose the following IS transitions for M\u0302\nf\u0302s\u2032\u2223s,a(s \u2032 \u2223s, a;\u03c9) =\nfs\u2032\u2223s,a(s \u2032\u2223s, a) exp (\u2212\u03c9V\u0303 (s\u2032; \u03b8))\n\u2211y fs\u2032\u2223s,a(y\u2223s, a) exp (\u2212\u03c9V\u0303 (y; \u03b8)) . (34)\nNote that increasing \u03c9 encourages transitions to low value states, thus increasing the probability of \u2018bad\u2019 trajectories. Obtaining an approximate value function for a given policy has been studied extensively in RL literature, and many efficient solutions for this task are known, such as LSTD (Boyan 2002) and TD(\u03bb) (Sutton and Barto 1998). Here, we don\u2019t restrict ourselves to a specific method."}, {"heading": "E.4 Empirical Results with Importance Sampling", "text": "We report the full details about the experimental results with importance sampling mentioned in the main text.\nFig. 2 demonstrates the importance of IS in optimizing the CVaR when \u03b1 is small. We chose \u03b1 = 0.01, and N = 200, and compared the naive GCVaR against IS GCVaR. As our value function approximation, we exploited the fact that the soft-max policy uses \u03c6(s, a) \u22ba \u03b8 as a sort of state-action value function, and therefore set V\u0303 (s) = maxa \u03c6(s, a) \u22ba\n\u03b8. We chose \u03c9 using SAA, with trajectories from the initial policy \u03b80. We observe that IS GCVaR converges significantly faster than GCVaR, due to the lower variance in gradient estimation."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risksensitive controller for the game of Tetris.", "creator": "LaTeX with hyperref package"}}}