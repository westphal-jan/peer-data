{"id": "1404.4032", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2014", "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit", "abstract": "The recently established RPCA method provides us a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA may be not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank matrices. It may not be a problem in the long term.\n\n\n\n\nThe simplest and simplest way to fix this problem is to try to provide an algorithm that works by allowing us to generate low-rank matrices with zero-order integer-order integers, which we can easily control via the method's output method, which can be called from the \"p\" command, or from the \"r\" command (or from the \"p\" command). The simplest way to solve this problem is to run the \"r\" command (or from the \"r\" command). This way, our matrices are computed with a few values of the following values:\n$matrices = (1) * 3 * 3 $matrices = (2) * 3\nThe simplest way to solve this problem is to compute the value of a few values of the following values:\n$matrices = (1) * 3 * 3 $matrices = (2) * 3\nThe simplest way to solve this problem is to use the \"r\" command, which returns a value of the same value:\n$matrices = (1) * 3 * 3 $matrices = (2) * 3\nThis way, our matrices are computed using the \"r\" command, which returns a value of the same value:\n$matrices = (1) * 3 * 3 $matrices = (2) * 3\nUsing the \"r\" command gives us a very simple way to compute the value of a few values of the following values:\n$matrices = (2) * 3 * 3 $matrices = (3) * 3 $matrices = (3) * 3\nThis method can easily be used to compute the value of a few values of the following values:\n$matrices = (1) * 3 * 3 $matrices = (3) * 3 $matrices = (4) * 3 $matrices = (4) * 3 $matrices = (5) * 3 $matrices = (6) * 3 $matrices = (7) * 3 $matrices = (8) * 3 $matrices", "histories": [["v1", "Tue, 15 Apr 2014 19:35:15 GMT  (329kb)", "https://arxiv.org/abs/1404.4032v1", null], ["v2", "Wed, 16 Jul 2014 17:57:02 GMT  (454kb)", "http://arxiv.org/abs/1404.4032v2", null]], "reviews": [], "SUBJECTS": "stat.ME cs.IT cs.LG math.IT math.ST stat.TH", "authors": ["guangcan liu", "ping li 0001"], "accepted": true, "id": "1404.4032"}, "pdf": {"name": "1404.4032.pdf", "metadata": {"source": "CRF", "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit", "authors": ["Guangcan Liu", "Ping Li"], "emails": ["guangcan.liu@rutgers.edu", "pingli@stat.rutgers.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 4.\n40 32\nv2 [\nst at\n.M E\n] 1\n6 Ju"}, {"heading": "1. Introduction", "text": "Nowadays our data is often high-dimensional, massive and full of gross errors (e.g., corruptions, outliers and missing measurements). In the presence of gross errors, the classical Principal Component Analysis (PCA) method, which is probably the most widely used tool for data analysis and dimensionality reduction, becomes brittle \u2014 A single gross error could render the estimate produced by PCA arbitrarily far from the desired estimate. As a consequence, it is crucial to develop new statistical tools for robustifying PCA. A variety of methods have been proposed and explored in the literature over several decades, e.g., (Cande\u0300s and Plan, 2010; Cande\u0300s and Recht, 2009; Cande\u0300s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010). One of the most exciting methods is probably the so-called RPCA (Robust Principal Component Analysis) method by Cande\u0300s et al. (2011), built upon the exploration of the following low-rank matrix recovery problem:\nProblem 1 (Low-Rank Matrix Recovery) Suppose we have a data matrix X \u2208 Rm\u00d7n and we know it can be decomposed as\nX = L0 + S0, (1.1)\nwhere L0 \u2208 Rm\u00d7n is a low-rank matrix in which each column is a data point drawn from some low-dimensional subspace, and S0 \u2208 Rm\u00d7n is a sparse matrix supported on \u2126 \u2286 {1, \u00b7 \u00b7 \u00b7 ,m} \u00d7 {1, \u00b7 \u00b7 \u00b7 , n}. Except these mild restrictions, both components are arbitrary. The rank of L0 is unknown, the support set \u2126 (i.e., the locations of the nonzero entries of S0) and its cardinality (i.e., the amount of the nonzero entries of S0) are unknown either. In particular, the magnitudes of the nonzero entries in S0 may be arbitrarily large. Given X, can we recover both L0 and S0, in a scalable and exact fashion?\nThe theory of RPCA tells us that, very generally, when the low-rank matrix L0 satisfies some incoherent conditions (i.e., the coherence parameters of L0 are small), both the low-rank and the sparse matrices can be exactly recovered by using the following convex, potentially scalable program:\nmin L,S\n\u2016L\u2016\u2217 + \u03bb\u2016S\u20161, s.t. X = L+ S, (1.2)\nwhere \u2016 \u00b7 \u2016\u2217 is the nuclear norm (Fazel, 2002) of a matrix, \u2016 \u00b7 \u20161 denotes the \u21131 norm of a matrix seen as a long vector, and \u03bb > 0 is a parameter. Besides of its elegance in theory, RPCA also has good empirical performance in many practical areas, e.g., image processing (Zhang et al., 2012), computer vision (Peng et al., 2012), radar imaging (Borcea et al., 2012), magnetic resonance imaging (Otazo et al., 2012), etc.\nWhile complete in theory and powerful in reality, RPCA cannot be an ultimate solution to the low-rank matrix recovery Problem 1. Indeed, the method might not produce perfect recovery even when the latent matrix L0 is strictly low-rank. This is because, seen from the aspect of mathematics, RPCA requires L0 to satisfy some incoherent conditions, which, however, might not hold in reality. In a physical sense, the reason is that RPCA captures only the low-rankness property, which should not be the only property of our data, but essentially ignores the extra structures (beyond low-rankness) widely existed in data: Given the situation that L0 is low-rank, i.e., the column vectors of L0 locate on a low-dimensional subspace, it is quite normal that L0 may have some extra structures, which specify in more detail how the data points (i.e., the column vectors of L0) locate on the subspace.\nFigure 1 demonstrates a typical example of extra structures; that is, the clustering structure which is ubiquitous in modern applications (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Soltanolkotabi et al., 2013). Whenever the data is exhibiting some clustering structure, the coherence parameters might be large and therefore RPCA might be unsatisfactory. More precisely, as will be shown in this paper, while the rank of L0 is fixed and the underlying cluster number goes large, the coherence of L0 keeps heightening and thus, arguably, the performance of RPCA drops.\nTo well handle coherent data1, a straightforward approach is to avoid the coherence parameters of L0. Nevertheless, as explained in (Cande\u0300s et al., 2011; Cande\u0300s and Recht, 2009), the coherence parameters are indeed necessary for matrix recovery (if there is no additional condition available). Even more, this paper shall further indicate that the coherence parameters are related in nature to some extra structures intrinsically existed in L0 and therefore cannot be discarded simply. Interestingly, we show that it is possible to avoid the coherence parameters by imposing some additional conditions, which are easy to obey in supervised environments and can also be approximately satisfied in unsupervised environments. Our study is based on the following convex program termed Low-Rank Representation (LRR) (Liu et al., 2013):\nmin Z,S\n\u2016Z\u2016\u2217 + \u03bb\u2016S\u20161, s.t. X = AZ + S, (1.3)\nwhere A \u2208 Rm\u00d7d is a size-d dictionary matrix constructed in advance2, and \u03bb > 0 is a parameter. In order for LRR to avoid the coherence parameters which have potential to\n1. Generally, coherent (resp. incoherent) data refers to the matrices whose coherence parameters are relatively large (resp. small). Yet there is no deterministic threshold to divide all matrices into coherent matrices and incoherent ones. To avoid confusion, in this paper we say that a matrix is incoherent if and only if the column vectors of the matrix are sampled from a single subspace uniformly at random. Apart from this particular case, the matrix is said to be coherent. In that sense, strictly speaking, the \u201cincoherent data\u201d stated in this paper does not exist in realistic environments. 2. Note that it is unimportant to determine the value of d. Suppose Z\u2217 is the optimal solution with respect to Z. Then LRR uses AZ\u2217 to restore L0. It is easy to see that LRR falls back to RPCA when A = I (identity matrix), and it can actually be further proved that the recovery produced by LRR is the same as RPCA whenever the dictionary A is orthogonal.\nbe large in the presence of extra structures, we prove that it is sufficient to construct in advance a dictionary matrix A which is low-rank by itself. This additional condition (i.e., the dictionary A is low-rank) gives a generic prescription to defend the possible infections raised by coherent data, providing an elementary criterion for learning the dictionary matrix A. Subsequently, we propose a simple and effective algorithm that utilizes the output of RPCA to construct the dictionary in LRR. Our extensive experiments demonstrated on randomly generated matrices and motion data show promising results. In summary, the contributions of this paper include:\n\u22c4 For the first time, this paper studies the problem of recovering low-rank, but coherent matrices from their corrupted versions. We investigate the physical regime where coherent data arises \u2014 The widely existed clustering structure is a typical example that leads to coherent data. We prove some basic theories for resolving the problem of recovering coherent data, and also establish a practical algorithm that works better than RPCA in our experiments.\n\u22c4 The studies of this paper help to understand the physical meaning of coherence, which is now standard and widely used in various literatures, e.g., (Cande\u0300s and Plan, 2010; Cande\u0300s and Recht, 2009; Cande\u0300s et al., 2011; Xu et al., 2010; Liu et al., 2012). We show that the coherence parameters are not \u201cassumptions\u201d for accomplishing a proof, but rather some excellent quantities that relate in nature to the extra structures (beyond low-rankness) intrinsically existed in L0.\n\u22c4 This paper provides insights regarding the LRR model proposed by (Liu et al., 2013). While the special case of A = X has been extensively studied, the LRR model (1.3) with general dictionaries was not fully understood. We show that LRR (1.3) equipped with proper dictionaries could well handle coherent data.\n\u22c4 The idea of replacing L with AZ is essentially related to the spirit of matrix factorization which has been explored for long, e.g., (Srebro and Jaakkola, 2005; Weimer et al., 2007). In that sense, the explorations of this paper help to understand why factorization techniques are useful.\nThe remainder of this paper is organized as follows. Section 2 summarizes mathematical notations used throughout this paper. In Section 3, we explore the problem of recovering coherent data from corrupted observations, providing some theories and an algorithm for resolving the problem. Section 4 presents the complete proof procedure of our main result. Section 5 demonstrates experimental results and Section 6 concludes this paper."}, {"heading": "2. Summary of Main Notations", "text": "Capital letters such as M are used to represent matrices, and accordingly, [M ]ij denotes its (i, j)th entry. Letters U , V , \u2126 and their variants (complements, subscripts, etc.) are reserved for left singular vectors, right singular vectors and support set, respectively. We slightly abuse the notation U (resp. V ) to denote the linear space spanned by the columns of U (resp. V ), i.e., the column space (resp. row space). The projection onto the column space U , is denoted by PU and given by PU (M) = UUTM , and similarly for the row space\nPV (M) = MV V T . We also abuse the notation \u2126 to denote the linear space of matrices supported on \u2126. Then P\u2126 and P\u2126\u22a5 respectively denote the projections onto \u2126 and \u2126c such that P\u2126 + P\u2126\u22a5 = I, where I is the identity operator. The symbol (\u00b7)+ denotes the Moore-Penrose pseudoinverse of a matrix: M+ = VM\u03a3 \u22121 M U T M for a matrix M with Singular Value Decomposition (SVD)3 UM\u03a3MV T M .\nSix different matrix norms are used in this paper. The first three norms are functions of the singular values: 1) The operator norm (i.e., the largest singular value) denoted by \u2016M\u2016, 2) the Frobenius norm (i.e., square root of the sum of squared singular values) denoted by \u2016M\u2016F , and 3) the nuclear norm (i.e., the sum of singular values) denoted by \u2016M\u2016\u2217. The other three are the \u21131, \u2113\u221e (i.e., sup-norm) and \u21132,\u221e norms of a matrix: \u2016M\u20161 = \u2211\ni,j |[M ]ij |, \u2016M\u2016\u221e = maxi,j{|[M ]ij |} and \u2016M\u20162,\u221e = maxj{ \u221a \u2211 i[M ] 2 ij}.\nThe Greek letter \u00b5 and its variants (e.g., subscripts and superscripts) are reserved to denote the coherence parameters of a matrix. We shall also reserve two lower case letters, m and n, to respectively denote the data dimension and the number of data points, and we use the following two symbols throughout this paper:\nn1 = max(m,n) and n2 = min(m,n).\nA complete list of notations can be found in Appendix A for convenience of readers."}, {"heading": "3. On the Recovery of Coherent Data", "text": "In this section, we shall firstly investigate the physical regime that raises coherent data, and then discuss the problem of recovering coherent data from corrupted observations, providing some basic principles and an algorithm for resolving the problem."}, {"heading": "3.1 Coherence Parameters and Their Properties", "text": "Notice that the rank function cannot fully capture all characteristics of L0, and thus it is indeed necessary to define some quantities for measuring the effects of various extra structures (beyond low-rankness) such as the clustering structure demonstrated in Figure 1. The coherence parameters defined in (Cande\u0300s and Recht, 2009; Cande\u0300s et al., 2011) are excellent exemplars of such quantities."}, {"heading": "3.1.1 \u00b51 and \u00b52", "text": "For an m \u00d7 n matrix L0 with rank r0 and SVD L0 = U0\u03a30V T0 , some of its important properties can be characterized by two coherence parameters, denoted as \u00b51 and \u00b52. The first coherence parameter, 1 \u2264 \u00b51 \u2264 m, which characterizes the column space identified by U0, is defined as\n\u00b51(L0) = m\nr0 max 1\u2264i\u2264m\n\u2016UT0 ei\u201622, (3.4)\n3. In this paper, SVD always refers to skinny SVD. For a rank-r matrix M \u2208 Rm\u00d7n, its SVD is of the form UM\u03a3MV T M , with UM \u2208 R m\u00d7r,\u03a3M \u2208 Rr\u00d7r and VM \u2208 Rn\u00d7r.\nwhere ei denotes the ith standard basis. The second coherence parameter, 1 \u2264 \u00b52 \u2264 n, which characterizes the row space identified by V0, is defined as\n\u00b52(L0) = n\nr0 max 1\u2264j\u2264n\n\u2016V T0 ej\u201622. (3.5)\nIn (Cande\u0300s et al., 2011), another coherence parameter, called as the third coherence parameter and denoted as 1 \u2264 \u00b53 \u2264 mn, is also introduced:\n\u00b53(L0) = mn\nr0 (\u2016U0V T0 \u2016\u221e)2 =\nmn\nr0 max i,j\n(|\u3008UT0 ei, V T0 ej\u3009|)2.\nNotice that \u00b53 is not indispensable, as it is actually a \u201cderivative\u201d of \u00b51 and \u00b52: Simple calculations give that \u00b53 \u2264 r0\u00b51\u00b52. The analysis of work does not need to access \u00b53. We include it just for the sake of consistence with (Cande\u0300s et al., 2011).\nThe analysis in (Cande\u0300s et al., 2011) merges the above three parameters into a single one: \u00b5(L0) = max{\u00b51(L0), \u00b52(L0), \u00b53(L0)}. As will be seen later, the behaviors of those three coherence parameters are different from each other, and thus it is indeed more adequate to consider them individually."}, {"heading": "3.1.2 \u00b52-phenomenon", "text": "Cande\u0300s et al. (2011) have proven that the success condition (regarding L0) of RPCA is\nrank (L0) \u2264 n2\ncr\u00b5(L0)(log n1)2 , (3.6)\nwhere \u00b5(L0) = max{\u00b51(L0), \u00b52(L0), \u00b53(L0)} and cr > 1 is some numerical constant. So, RPCA will be less successful when the coherence parameters are considerably larger: The success condition (3.6) is narrowed when \u00b5(L0) goes large. As an extreme example, consider the case where the latent matrix L0 is one in only one column and zero everywhere else. Such a matrix produces \u00b52(L0) = n \u2265 n2, and thus the success condition (3.6) is invalid. In this subsection, we shall further show that the widely existed clustering structure can enlarge the coherence parameters and, accordingly, degrades the performance of RPCA.\nGiven the situation that L0 is low-rank, i.e., rank (L0) \u2261 r0 \u226a n2, the data points (i.e., column vectors of L0) should be sampled from a r0-dimensional subspace. Yet the sampling is unnecessary to be uniform. Indeed, a more realistic interpretation is to consider the data points as samples from the union of k number of subspaces (i.e., clusters), and the sum of those multiple subspaces together has a dimension r0. That is to say, there are multiple \u201csmall\u201d subspaces inside one r0-dimensional \u201clarge\u201d subspace, as exemplified in Figure 1. It is arguable that such a structure of multiple subspaces exists widely in various domains, e.g., face, texture and motion (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Liu et al., 2010a). Whenever the low-rank matrix L0 is exhibiting such clustering behaviors, the second coherence parameter \u00b52(L0) will increase with the cluster number underlying L0, as shown in Figure 2. When the coherence is heightening, (3.6) suggests that the performance of RPCA will drop, as verified in Figure 2(d). For the ease of citation, we call the phenomena shown in Figure 2(b)\u223c(d) as the \u201c\u00b52-phenomenon\u201d.\nTo see why the second coherence parameter increases with the cluster number underlying L0, please refer to Appendix B. As can be seen from Figure 2(a), the first coherence\nparameter \u00b51 is invariant to the variation of the clustering number. This is because the behaviors of the data points (i.e., column vectors) can only affect the row space, while \u00b51 is defined on the column space. Nevertheless, if the row vectors of L0 also own some clustering structure, \u00b51 could be large as well. This kind of data exists widely in text documents and we leave it as future work."}, {"heading": "3.2 Avoiding \u00b52 by LRR", "text": "To accurately recover coherent matrices from their corrupted versions, one may establish some parametric models to capture the extra structures which produce high coherence. However, it is usually hard, if not impossible, to know in advance what kind of extra structures there are and which models are appropriate to use. Even if the modalities of the extra structure are known, e.g., the mixture of multiple subspaces shown in Figure 1, such a strategy still needs to face some difficult problems, e.g., the estimate of the cluster number. In sharp contrast, it is much simpler to devise an approach that can avoid the second coherence parameter \u00b52. Unfortunately, as explained in (Cande\u0300s and Recht, 2009; Cande\u0300s et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery. Even more, the \u00b52-phenomenon actually implies that \u00b52 is related in nature to some intrinsic structures of L0 and thus cannot be eschewed freely. Interestingly, we shall show that LRR can avoid \u00b52 by using some additional conditions, which are possible to obey in both supervised and unsupervised environments.\nMain Result: We shall show that, when the dictionary matrix A itself is low-rank, the recovery performance of LRR does not depend on \u00b52. Our main result is presented in the following theorem (The detailed proof procedure is deferred until Section 4).\nTheorem 1 (Noiseless) Let A \u2208 Rm\u00d7d with SVD A = UA\u03a3AV TA be a column-wisely unitnormed (i.e., \u2016Aei\u20162 = 1,\u2200i) dictionary matrix which satisfies PUA(U0) = U0 (i.e., U0 is a subspace of UA). For any 0 < \u01eb < 0.5 and some numerical constant ca > 1, if\nrank (L0) \u2264 rank (A) \u2264 \u01eb2n2\nca\u00b51(A) log n1 and |\u2126| \u2264 (0.5\u2212 \u01eb)mn, (3.7)\nthen with probability at least 1\u2212 n\u2212101 , the optimal solution to the LRR problem (1.3) with \u03bb = 1/ \u221a n1 is unique and exact, in a sense that\nZ\u2217 = A+L0 and S \u2217 = S0,\nwhere (Z\u2217, S\u2217) is the optimal solution to (1.3).\nBy U0 \u2282 UA, the column space of A should approximately have the same properties as L0, and thus, roughly, \u00b51(A) \u2248 \u00b51(L0). So, as aforementioned, this paper needs to assume that the first coherence parameter of L0 is small and only addresses the cases where the second coherence parameter might be large. It is worth noting that the restriction rank (L0) \u2264 O(n2/ log n1) is looser than that of PRCA\n4, which requires rank (L0) \u2264 O(n2/(log n1)2). The requirement of column-wisely unit-normed dictionary (i.e., \u2016Aei\u20162 = 1,\u2200i) is purely for complying the parameter estimate of \u03bb = 1/ \u221a n1, which is consistent with RPCA. The condition PUA(U0) = U0, i.e., U0 is a subspace of UA, is indispensable if we ask for exact recovery, because PUA(U0) = U0 is implied by the equality AZ\u2217 = L0. This necessary condition, together with the condition that A is low-rank, indeed provides an elementary criterion for learning the dictionary matrix A in LRR. Figure 3 presents an example, which further confirms our main result: LRR is able to avoid \u00b52 as long as U0 \u2282 UA and A is low-rank. Note that it is unnecessary for the dictionary A to strictly satisfy UA = U0, and LRR is actually tolerant to the \u201cerrors\u201d possibly existing in the dictionary.\nThe LRR program (1.3) is designed for the cases where the uncorrupted observations are noiseless. In reality this is often not true and all entries of X can be contaminated by a small amount of noises, i.e., X = L0 + S0 + N , where N is a matrix of dense Gaussian noises. In this case, the formula of LRR (1.3) need be modified to\nmin Z,S\n\u2016Z\u2016\u2217 + \u03bb\u2016S\u20161, s.t. \u2016X \u2212AZ \u2212 S\u2016F \u2264 \u03b5, (3.8)\nwhere \u03b5 is a parameter that measures the noise level of data. In the experiments of this paper, we consistently use \u03b5 = 10\u22126\u2016X\u2016F . In the presence of dense noises, the latent matrices, L0 and S0, cannot be exactly restored. Yet we have the following theorem to guarantee the near recovery property of the solution produced by (3.8) (please refer to Appendix C for the proof):\nTheorem 2 (Noisy) Suppose \u2016X\u2212L0\u2212S0\u2016F \u2264 \u03b5. Let A \u2208 Rm\u00d7d with SVD A = UA\u03a3AV TA be a column-wisely unit-normed dictionary matrix which satisfies PUA(U0) = U0. For any 0 < \u01eb < 0.35 and some numerical constant ca > 1, if\nrank (L0) \u2264 rank (A) \u2264 \u01eb2n2\nca\u00b51(A) log n1 and |\u2126| \u2264 (0.35 \u2212 \u01eb)mn, (3.9)\n4. In terms of exact recovery, O(n2/ log n1) is probably the \u201cfinest\u201d bound one could accomplish in theory.\nthen with probability at least 1\u2212n\u2212101 , any solution (Z\u2217, S\u2217) to the LRR program (3.8) with \u03bb = 1/ \u221a n1 gives a near recovery to (L0, S0), in a sense that \u2016AZ\u2217 \u2212 L0\u2016F \u2264 8 \u221a mn\u03b5 and\n\u2016S\u2217 \u2212 S0\u2016F \u2264 (8 \u221a mn+ 2)\u03b5."}, {"heading": "3.3 An Unsupervised Algorithm for Matrix Recovery", "text": "To well handle coherent data, Theorem 1 suggests that, ideally, the dictionary matrix A should be low-rank and satisfy U0 \u2282 UA. In certain supervised environment, this would be easy as one could use clear, well-processed training data to construct the dictionary. In unsupervised environments, however, it is challenging to purse a low-rank dictionary that can also satisfy U0 \u2282 UA, since U0 \u2282 UA is essentially some kind of \u201cweak\u201d supervision information: As long as the dictionary matrix A is low-rank, U0 \u2282 UA forms a prior that L0 is known to be contained by a low-rank subspace identified by UA. Interestingly, as will be shown later, it is possible to approximate the desired dictionary even when no prior about L0 is given.\nWe shall introduce a heuristic algorithm that works distinctly better than RPCA in our experiments. As can be seen from (3.6), RPCA is actually not brittle with respect to coherent data: Except for the extreme case where the coherence parameters reach the upper bound n (or m), RPCA could own a valid condition (although the condition is narrowed) to be exactly successful even when the coherence parameters are considerably large. Based on this, we propose a pretty simple algorithm, as summarized in Algorithm 1, to achieve a solid improvement over RPCA. Our idea is straightforward: We firstly obtain an estimate of L0 by using RPCA and then utilize the estimate to construct the dictionary matrix A. The post-processing steps (Step 2 and Step 3) that slightly modify the solution of RPCA are designed to encourage well-conditioned dictionary, which is the favorite circumstance indicated by Theorem 1.\nAlgorithm 1 Matrix Recovery\ninput: Observed data matrix X \u2208 Rm\u00d7n. adjustable parameter: \u03bb. 1. Solve for L\u03020 by optimizing the RPCA problem (1.2) with \u03bb = 1/ \u221a n1. 2. Estimate the rank of L\u03020 by\nr\u03020 = #{i : \u03c3i > 10\u22123\u03c31},\nwhere \u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c3n2 are the singular values of L\u03020. 3. Form L\u03030 by using the rank-r\u03020 approximation of L\u03020. That is,\nL\u03030 = argmin L\n\u2016L\u2212 L\u03020\u20162F , s.t. rank (L) \u2264 r\u03020,\nwhich is solved by SVD. 4. Construct a dictionary A\u0302 from L\u03030 by normalizing the column vectors of L\u03030:\n[A\u0302]:,i = [L\u03030]:,i\n\u2016[L\u03030]:,i\u20162 , i = 1, \u00b7 \u00b7 \u00b7 , n,\nwhere [\u00b7]:,i denotes the ith column of a matrix. 5. Solve for Z\u2217 by optimizing the LRR problem (1.3) with A = A\u0302 and \u03bb = 1/ \u221a n1. output: A\u0302Z\u2217.\nWhenever the recovery produced by RPCA is already exact, the claim in Theorem 1 gives that the recovery produced by our Algorithm 1 is exact as well. When RPCA fails to exactly recover L0, the produced dictionary is still possible to satisfy the success conditions required by Theorem 1, namely A is low-rank and U0 \u2282 UA. This is because those conditions are weaker than A = L0. Thus, in terms of exactly recovering L0 from a given X, the success probability of our Algorithm 1 is greater than or equal to that of RPCA. Also, in a computational sense, Algorithm 1 does not double RPCA, although there are two convex programs in our algorithm. In fact, according to our simulations, usually the computational time of Algorithm 1 is just 1.2 times as much as RPCA. The reason is that, as has been explored by (Liu et al., 2013), the complexity of solving the LRR problem (1.3) is O(n2rA) (assume m = n), which is much lower than that of RPCA (which requires O(n3)) provided that the obtained dictionary matrix A is fairly low-rank (i.e., rA is small).\nOne may have noticed that the procedure of Algorithm 1 could be made iterative, i.e., one can consider A\u0302Z\u2217 as a new estimate of L0 and use it to further update the dictionary matrix A, and so on. Nevertheless, we empirically find that such an iterative procedure often converges within two iterations. Hence, for the sake of simplicity, we do not consider the iterative strategies in this paper."}, {"heading": "4. Proof of Theorem 1", "text": ""}, {"heading": "4.1 Settings and Some Basic Lemmas", "text": "The same as in RPCA (Cande\u0300s et al., 2011), we assume that the locations of the corrupted entries are selected uniformly at random. In more details, we work with the Bernoulli model \u2126 = {(i, j) : \u03b4ij = 1}, where \u03b4ij \u2019s are i.i.d. variables taking value one with probability \u03c10 = |\u2126|/(mn) and zero with probability (1 \u2212 \u03c10), so that the expected cardinality of \u2126 is \u03c10mn. For the ease of presentation, we assume that the signs of the nonzero entries of S0 are symmetric Bernoulli \u00b11 values:\n[sign(S0)]ij =\n\n\n 1, with probability \u03c102 , 0, with probability 1\u2212 \u03c10, \u22121, with probability \u03c102 .\nFor general sign matrices, the same as in RPCA (Cande\u0300s et al., 2011), our Theorem 1 can still be proved by globally placing an elimination theorem and a derandomization scheme. Yet the success conditions in Theorem 2 have not been proven when sign(S0) has an arbitrary distribution, because the elimination theorem does not hold in the noisy case.\nThe following two lemmas are well-known and will be used multiple times in the proof.\nLemma 3 For any matrix M , the following holds:\n1. Let the SVD of M be UM\u03a3MV T M . Then we have \u2202\u2016M\u2016\u2217 = {UMV TM + W |UTMW =\n0,WVM = 0, \u2016W\u2016 \u2264 1}.\n2. Let the support set of M be \u2126M . Then we have \u2202\u2016M\u20161 = {sign(M) + F |P\u2126M (F ) = 0, \u2016F\u2016\u221e \u2264 1}.\nLemma 4 For any matrices M and N of consistent sizes,\n|\u3008M,N\u3009| \u2264 \u2016M\u2016\u221e\u2016N\u20161, |\u3008M,N\u3009| \u2264 \u2016M\u2016F \u2016N\u2016F , \u2016MN\u2016F \u2264 \u2016M\u2016\u2016N\u2016F , \u2016MN\u20162,\u221e \u2264 \u2016M\u2016|N\u20162,\u221e."}, {"heading": "4.2 Critical Lemmas", "text": "First of all, we would like to prove that the sparse matrix S0 does not locate in the column space of the dictionary A, i.e., UA \u2229 \u2126 = {0} or \u2016PUAP\u2126\u2016 < 1 as equal. Provided that A \u2208 Rm\u00d7d is fairly low-rank, the analysis in (Cande\u0300s et al., 2011) gives that\n\u2016PTAP\u2126\u2016 \u2264 \u221a |\u2126| mn + \u01eb\nholds with high probability for any \u01eb > 0, where TA denotes the linear space given by PUA +PVA \u2212PUAPVA . Since UA \u2282 TA and \u2016PUAP\u2126\u2016 \u2264 \u2016PTAP\u2126\u2016, it is natural to anticipate that \u2016PUAP\u2126\u2016 is smaller than 1 with high probability. The difference is that we only need the first coherence parameter \u00b51 to finish the proof. Following the techniques in (Cande\u0300s et al., 2011), we have the following lemma to bound the operator norm of PUAP\u2126.\nLemma 5 Suppose \u2126 \u223c Ber(\u03c10) with \u03c10 < 1. Then for any \u01eb > 0,\n\u2016PUAP\u2126\u2016 \u2264 \u221a \u03c10 + \u01eb\nholds with probability at least 1\u2212 n\u2212101 , provided that\nrank (A) \u2264 \u01eb 2n2\nca\u00b51(A) log n1 .\nProof For any matrix M , we have\nPUA(M) = \u2211\ni,j\n\u3008PUA(M), eieTj \u3009eieTj ,\nand so\nP\u2126\u22a5PUA(M) = \u2211\ni,j\n(1\u2212 \u03b4ij)\u3008PUA(M), eieTj \u3009eieTj ,\nwhich gives\nPUAP\u2126\u22a5PUA(M) = \u2211\ni,j\n(1\u2212 \u03b4ij)\u3008PUA(M), eieTj \u3009PUA(eieTj )\n= \u2211\ni,j\n(1\u2212 \u03b4ij)\u3008M,PUA(eieTj )\u3009PUA(eieTj ).\nNote that the Frobenius norm of a matrix is equivalent to the vector \u21132 norm, while considering the matrix as a long vector. In that sense, we have\nPUAP\u2126\u22a5PUA = \u2211\ni,j\n(1\u2212 \u03b4ij)PUA(eieTj )\u2297 PUA(eieTj ).\nThe definition of \u00b51(A) gives\n\u2016PUA(eieTj )\u20162F \u2264 \u00b51(A)rA\nm .\nThen by using the results in (Rudelson, 1999) and following the proof procedure of (Cande\u0300s and Recht, 2009), we have that\n\u2016(1 \u2212 \u03c10)PUA \u2212 PUAP\u2126\u22a5PUA\u2016 \u2264 (1\u2212 \u03c10)(\u03c61\n\u221a\n\u00b51(A)rA log n1 n2 + \u03c62\n\u221a\n\u00b51(A)\u03b2rA log n1 n2 )\n\u2264 \u03c61\n\u221a\n\u00b51(A)rA log n1 n2 + \u03c62\n\u221a\n\u00b51(A)\u03b2rA log n1 n2\nholds with probability at least 1 \u2212 n\u2212\u03b21 for some numerical constants \u03c61 and \u03c62. For any \u01eb > 0, setting \u03b2 = 10 and ca = (\u03c61 + \u221a 10\u03c62) 2 gives that\n\u2016(1\u2212 \u03c10)PUA \u2212PUAP\u2126\u22a5PUA\u2016 \u2264 \u01eb\nholds with probability at least 1\u2212 n\u2212101 , provided that rA \u2264 \u01eb2n2/(ca\u00b51(A) log n1). By PUAP\u2126PUA = \u2212\u03c10PUA \u2212 ((1\u2212 \u03c10)PUA \u2212 PUAP\u2126\u22a5PUA) and the triangle inequality,\n\u2016PUAP\u2126PUA\u2016 \u2264 \u2016\u03c10PUA\u2016+ \u2016(1 \u2212 \u03c10)PUA \u2212 PUAP\u2126\u22a5PUA\u2016\n\u2264 \u03c10 + \u01eb = |\u2126| mn + \u01eb.\nFinally, the fact \u2016PUAP\u2126PUA\u2016 = \u2016PUAP\u2126\u20162 completes the proof.\nWhile the above Lemma implies that \u2016PUAP\u2126(M)\u2016F \u2264 (\u03c10 + \u01eb)\u2016M\u2016F , we often need to bound the sup-norm of PUAP\u2126(M). The next lemma will show that, when the signs of the matrix entries are independent symmetric Bernoulli variables, the sup-norm could be arbitrarily small.\nLemma 6 Suppose P is a symmetric linear projection with \u2016P\u2016 \u2264 2, and \u03a8 \u2208 Rm\u00d7n is a random sign matrix with i.i.d. entries distributed as\n[\u03a8]ij =\n{\n1, with probability 12 , \u22121, with probability 12 .\nFor any \u01eb > 0,\n\u2016PUAPPUAP\u2126(\u03a8)\u2016\u221e \u2264 \u01eb\nholds with high probability as long as\nrank (A) \u2264 \u01eb 2n2\nca\u00b51(A) log n1 .\nProof Let \u03beij = [\u03a8]ij and\nQ = PUAPPUAP\u2126(\u03a8) = PUAPPUA( \u2211\ni,j\n\u03b4ij\u03beijeie T j )\n= \u2211\ni,j\n\u03b4ij\u03beijPUAPPUA(eieTj ).\nThen it can be seen that each entry of Q is a sum of independent random variables:\n[Q]i1j1 = \u2211\ni,j\nyij with\nyij = \u03b4ij\u03beij\u3008PUAPPUA(eieTj ), ei1eTj1\u3009.\nNote here that the variables \u03b4ij \u2019s are fixed and the randomness comes from \u03beij\u2019s.\nIt is easy to see that E(yij) = 0. We have\n|yij \u2212 E(yij)| = |\u03b4ij\u03beij\u3008PUAPPUA(eieTj ), ei1eTj1\u3009| = |\u03b4ij\u03beij\u3008PUA(eieTj ),PPUA(ei1eTj1)\u3009| \u2264 \u2016PUA(eieTj )\u2016F \u2016PPUA(ei1eTj1)\u2016F \u2264 \u2016PUA(eieTj )\u2016F \u2016P\u2016\u2016PUA(ei1eTj1)\u2016F\n\u2264 2u1(A)rA m .\nWe also have \u2211\ni,j\nV ar(yij) = \u2211\nij\n|\u03b4ij\u3008PUAPPUA(eieTj ), ei1eTj1\u3009| 2V ar(\u03beij)\n= \u2211\ni,j\n(\u03b4ij) 2|\u3008PUAPPUA(eieTj ), ei1eTj1\u3009| 2\n= \u2211\ni,j\n(\u03b4ij) 2|\u3008eieTj ,PUAPPUA(ei1eTj1)\u3009| 2\n\u2264 \u2211\ni,j\n|\u3008eieTj ,PUAPPUA(ei1eTj1)\u3009| 2\n= \u2016PUAPPUA(ei1eTj1)\u2016 2 F \u2264 \u2016PUAP\u20162\u2016PUA(ei1eTj1)\u2016 2 F \u2264 4\u00b51(A)rA m .\nThen the proof is finished by using Bernstein\u2019s inequality, which states that for a collection of uniformly bounded independent random variables {yi}pi=1 with |yi \u2212 E(yi)| < c,\nPr\n(\u2223\n\u2223 \u2223 \u2223 \u2223\np \u2211\ni=1\n(yi \u2212 E(yi)) \u2223 \u2223 \u2223 \u2223\n\u2223\n> t\n)\n\u2264 exp ( \u2212 0.5t 2\n\u2211p i=1 V ar(yi) + ct/3\n)\n.\nThus we have\nPr(|[Q]i1j1 | > \u01eb) \u2264 exp ( \u2212 0.5\u01eb 2\n4\u00b51(A)rA m + 2\u01eb\u00b51(A)rA3m\n)\n\u2264 exp ( \u2212 1.5\u01eb 2m\n(12 + 2\u01eb)\u00b51(A)rA\n)\n.\nBy union bound,\nPr (\u2016Q\u2016\u221e \u2264 \u01eb) \u2265 1\u2212 n21 exp ( \u2212 1.5\u01eb 2m\n(12 + 2\u01eb)\u00b51(A)rA\n)\n\u2265 1\u2212 n\u2212101 ,\nprovided that rA \u2264 \u01eb2n2/(ca\u00b51(A) log n1) with ca \u2265 104."}, {"heading": "4.3 Dual Conditions", "text": "It remains to prove Theorem 1 by two steps:\n1. Dual Conditions: Identify the sufficient conditions for (Z = A+L0, S = S0) to be the unique optimal solution to the LRR problem (1.3).\n2. Dual Certificates: Show that the dual conditions can be satisfied, that is to say, construct the dual certificates.\nThe dual conditions are presented in the following lemma.\nLemma 7 Let the SVD of A+L0 be U\u03a3V T . Suppose PUA(U0) = U0 and UA \u2229 \u2126 = {0}. Then (A+L0, S0) is the unique optimal solution to (1.3) if there exists a matrix F that obeys\n(a) UV T = \u03bbAT (sign(S0) + F ), (b) P\u2126(F ) = 0, (c) \u2016P\u2126\u22a5(F )\u2016\u221e < 1.\nProof By standard convexity arguments (Rockafellar, 1970), (A+L0, S0) is an optimal solution to (1.3) if\n0 \u2208 \u2202\u2016A+L0\u2016\u2217 \u2212 \u03bbAT\u2202\u2016S0\u20161.\nNote that UV T \u2208 \u2202\u2016A+L0\u2016\u2217. Furthermore, (b) and (c) imply that sign(S0) +F \u2208 \u2202\u2016S0\u20161. Thus, the conditions (a), (b) and (c) are sufficient to conclude that (A+L0, S0) is an optimal (but may not be unique) solution to (1.3).\nNext, we shall consider a feasible perturbation (A+L0 +\u22061, S0 \u2212\u2206) and show that the objective strictly increases whenever \u2206 6= 0. By L0 + S0 = X = A(A+L0 +\u22061) + S0 \u2212\u2206,\n\u2206 = A\u22061 and so \u2206 \u2208 PUA .\nLet H = \u2212P\u2126\u22a5(sign(\u2206)). Then by Lemma 3, sign(S0) +H is a subgradient of \u2016S0\u20161. By the convexity of the nuclear norm and \u21131 norm,\n\u2016A+L0 +\u22061\u2016\u2217 + \u03bb\u2016S0 \u2212\u2206\u20161 \u2265\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u3008UV T ,\u22061\u3009 \u2212 \u03bb\u3008sign(S0) +H,\u2206\u3009 =\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u3008UV T \u2212 \u03bbAT sign(S0),\u22061\u3009 \u2212 \u03bb\u3008H,\u2206\u3009 =\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u3008UV T \u2212 \u03bbAT sign(S0),\u22061\u3009+ \u03bb\u2016P\u2126\u22a5(\u2206)\u20161 =\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u03bb\u3008ATF,\u22061\u3009+ \u03bb\u2016P\u2126\u22a5(\u2206)\u20161 =\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u03bb\u3008F,A\u22061\u3009+ \u03bb\u2016P\u2126\u22a5(\u2206)\u20161 =\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u03bb\u3008F,\u2206\u3009+ \u03bb\u2016P\u2126\u22a5(\u2206)\u20161 =\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u03bb\u3008P\u2126\u22a5(F ),P\u2126\u22a5(\u2206)\u3009+ \u03bb\u2016P\u2126\u22a5(\u2206)\u20161 \u2265\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 \u2212 \u03bb\u2016P\u2126\u22a5(F )\u2016\u221e\u2016P\u2126\u22a5(\u2206)\u20161 + \u03bb\u2016P\u2126\u22a5(\u2206)\u20161 =\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u03bb(1\u2212 \u2016P\u2126\u22a5(F )\u2016\u221e)\u2016P\u2126\u22a5(\u2206)\u20161.\nBy \u2206 \u2208 PUA , \u2016P\u2126\u22a5(F )\u2016\u221e < 1 and the assumption UA \u2229\u2126 = {0}, we have \u2016P\u2126\u22a5(\u2206)\u20161 > 0. Thus, we have\n\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u03bb(1\u2212 \u2016P\u2126\u22a5(F )\u2016\u221e)\u2016P\u2126\u22a5(\u2206)\u20161 > \u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161\nstrictly holds unless \u2206 = 0. As long as A\u22061 = 0, Theorem 4.1 of (Liu et al., 2013) gives that \u2016A+L0 + \u22061\u2016\u2217 > \u2016A+L0\u2016\u2217 strictly holds unless \u22061 = 0. Hence, (A+L0, S0) is the unique optimal solution to the LRR problem (1.3)."}, {"heading": "4.4 Dual Certificates", "text": "To construct a matrix F which satisfies the dual conditions listed in Lemma 7, we need the inverse of PUAP\u2126\u22a5PUA . The following lemma shows that (PUAP\u2126\u22a5PUA)\n\u22121 is well defined and has a small operator norm.\nLemma 8 If \u2016PUAP\u2126\u2016 < 1, then the operator PUAP\u2126\u22a5PUA is an injection from PUA to PUA, and its inverse operator is given by\nI + \u221e \u2211\ni=1\n(PUAP\u2126PUA)i.\nProof Let \u03c8 \u2261 \u2016PUAP\u2126\u2016. By \u2016PUAP\u2126PUA\u2016 = \u2016PUAP\u2126\u20162 = \u03c82 < 1, we have that I+\u2211\u221ei=1(PUAP\u2126PUA)i is well defined and has an operator norm not larger than 1/(1\u2212\u03c82).\nNote that\nPUAP\u2126\u22a5PUA = PUA(I \u2212 P\u2126)PUA = PUA(I \u2212 PUAP\u2126PUA).\nThus for any M \u2208 PUA the following holds:\nPUAP\u2126\u22a5PUA(I + \u221e \u2211\ni=1\n(PUAP\u2126PUA)i)(M)\n= PUA(I \u2212 PUAP\u2126PUA)(I + \u221e \u2211\ni=1\n(PUAP\u2126PUA)i)(M)\n= PUA(I + \u221e \u2211\ni=1\n(PUAP\u2126PUA)i \u2212 PUAP\u2126PUA \u2212 \u221e \u2211\ni=2\n(PUAP\u2126PUA)i)(M)\n= PUA(I + \u221e \u2211\ni=1\n(PUAP\u2126PUA)i \u2212 \u221e \u2211\ni=1\n(PUAP\u2126PUA)i)(M)\n= PUA(M) = M.\nThe next lemma completes the construction of the dual certificates.\nLemma 9 Let\nF = P\u2126\u22a5PUA\n(\nI + \u221e \u2211\ni=1\n(PUAP\u2126PUA)i ) ( 1\n\u03bb (AT )+UV T \u2212 PUA(sign(S0))\n)\n,\nwhere U and V are the left and right singular vectors of A+L0, respectively. If the conditions stated in (3.7) are obeyed, then the above F using \u03bb = 1/ \u221a n1 satisfies (with high probability) the dual conditions (a), (b) and (c) in Lemma 7.\nProof (a): We have\n\u03bbAT (sign(S0) + F )\n=\u03bbAT sign(S0) + \u03bbA TPUA(F )\n=\u03bbAT sign(S0) + \u03bbA TPUAP\u2126\u22a5PUA(I +\n\u221e \u2211\ni=1\n(PUAP\u2126PUA)i)( 1\n\u03bb (AT )+UV T \u2212 PUA(sign(S0)))\n=\u03bbAT sign(S0) + \u03bbA T (\n1 \u03bb (AT )+UV T \u2212 PUA(sign(S0)))\n=\u03bbAT sign(S0) + VAV T A UV T \u2212 \u03bbATPUA(sign(S0)) =\u03bbAT sign(S0)\u2212 \u03bbATPUA(sign(S0)) + VAV TA UV T =VAV T A UV T = UV T ,\nwhere the last equality follows from Theorem 4.3 of (Liu et al., 2013).\n(b): It is easy to verify that P\u2126(F ) = 0.\n(c): Let P = I +\u2211\u221ei=1(PUAP\u2126PUA)i and F = P\u2126\u22a5(F1 \u2212 F2), where\nF1 = PUAPPUA( 1\n\u03bb (AT )+UV T ), F2 = PUAPPUA(sign(S0)).\nIn the following, we shall bound the sup-norm of each term individually. The proof for \u2016F2\u2016\u221e needs to access the distribution of sign(S0). When the signs of the nonzero entries of S0 are Bernoulli \u00b11 values, i.e., sign(S0) = P\u2126(\u03a8) with \u03a8 being a random sign matrix as in Lemma 6, we have indeed proven\n\u2016F2\u2016\u221e = \u2016PUAPPUAP\u2126(\u03a8)\u2016\u221e < \u01eb,\nprovided that \u2016P\u2016 \u2264 1/(1 \u2212 \u03c10 \u2212 \u01eb) \u2264 2, which follows from the condition of \u03c10 < 0.5\u2212 \u01eb. So it remains to prove that\n\u2016F1\u2016\u221e < 1\u2212 \u01eb.\nThis seems easy because we could set \u03bb \u2192 + \u221e. Nevertheless, to prove our main result, Theorem 1, with \u03bb = 1/ \u221a n1 (which is a good choice in general), one essentially needs to establish an accurate bound for \u2016F1\u2016\u221e. Even more, the golfing scheme widely adopted by previous literatures is indeed not easy to work with in our setting. Fortunately, we can\nmake use of the particular structure of F1 and devise a simple approach to accomplish the proof. Our idea is based on the following observation: For any matrix Q, the (i1, j1)th entry of the matrix PUAP\u2126(Q) is\n[PUAP\u2126(Q)]i1j1 = \u2211\ni,j\n\u03b4ij [Q]ij\u3008eieTj ,PUA(ei1eTj1)\u3009 = \u2211\ni\n\u03b4ij1 [Q]ij1 [UAU T A ]ii1 ,\nwhich reveals the fact that the absolute value of [F1]i1j1 closely relates to the length of the j1th column of (A\nT )+UV T . So it may not lose much accuracy to use the relaxation of \u2016 \u00b7 \u2016\u221e \u2264 \u2016 \u00b7 \u20162,\u221e. For the sake of consistency, we use the \u21132,\u221e norm to define as follows the third coherence parameter of L0, associating with a dictionary matrix A:\nDefinition 1 For L0 \u2208 Rm\u00d7n of rank r0, its third coherence parameter, associating with a non-orthonormal, column-wisely unit-normed dictionary matrix A which also satisfies PUA(U0) = U0, is defined as\n\u00b5A3 (L0) = n2(\u2016(AT )+UV T \u20162,\u221e)2\n(log n)2r0\u03b3A , (4.10)\nwhere U and V are the left and right singular vectors of A+L0, respectively, and \u03b3A is the condition number of the matrix A.\nFigure 4 demonstrates some properties about this particular coherence parameter, \u00b5A3 . It can be seen that \u00b5A3 is approximately a numerical constant equaling to 1, as long as the rank is not too high such that the dictionary matrix A is well-conditioned.\nBy Lemma 5, \u2016P\u2016 \u2264 1/(1 \u2212 \u03c10 \u2212 \u01eb). By (4.10),\n\u2016(AT )+UV T )\u20162,\u221e \u2264\n\u221a\n\u03b3A\u00b5 A 3 (L0)r0 log n\nn .\nThus we have\n\u2016F1\u2016\u221e = \u2016UAUTAPPUA( 1\n\u03bb (AT )+UV T )\u2016\u221e\n\u2264 max i\n\u2016eTi UA\u20162\u2016UTAPPUA( 1\n\u03bb (AT )+UV T )\u20162,\u221e\n\u2264 \u221a\n\u00b51(A)rA m \u2016P\u2016\u2016( 1 \u03bb (AT )+UV T )\u20162,\u221e\n\u2264\n\u221a\n\u00b51(A)\u00b5A3 (L0)\u03b3ArAr0 log n\n\u03bb \u221a mn(1\u2212 \u03c10 \u2212 \u01eb)\n.\nBy r0 \u2264 rA \u2264 \u01eb2n2/(ca\u00b51(A) log n1) and setting \u03bb = \u221a \u00b5A3 (L0)\u03b3A/(\u00b51(A)n1),\n\u2016F1\u2016\u221e \u2264 \u01eb2n2\n\u221a n1 log n\nca(1 \u2212 \u03c10 \u2212 \u01eb) \u221a mn log n1\n\u2264 \u01eb 2\nca(1\u2212 \u03c10 \u2212 \u01eb) .\nSince \u00b5A3 (L0)\u03b3A/\u00b51(A) \u2248 1 (provided that A is well-conditioned), we claim \u03bb = 1/ \u221a n1 for the sake of simplicity5. Now the dual condition \u2016P\u2126\u22a5(F )\u2016\u221e < 1 is proved by\n\u2016F\u2016\u221e < 1,\n\u2190 \u01eb 2\nca(1\u2212 \u03c10 \u2212 \u01eb) < 1\u2212 \u01eb,\n\u2190 \u03c10 < 1\u2212 2\u01eb, \u2190 \u03c10 < 0.5 \u2212 \u01eb.\nWe claim \u03c10 < 0.5 \u2212 \u01eb instead of \u03c10 < 1 \u2212 2\u01eb because Lemma 6 requires \u2016P\u2016 \u2264 2, which follows from \u03c10 < 0.5\u2212 \u01eb."}, {"heading": "5. Experiments", "text": "Our main result, Theorem 1, is useful in both supervised and unsupervised environments. For the fair of comparison, in the experiments of this paper we shall focus on demonstrating the superiorities of our unsupervised Algorithm 1 over RPCA."}, {"heading": "5.1 Results on Randomly Generated Matrices", "text": "We first verify the effectiveness of our Algorithm 1 on randomly generated matrices. We generate a collection of 200\u00d71000 data matrices according to the model of X = P\u2126\u22a5(L0)+ P\u2126(S0): \u2126 is a support set chosen at random; L0 is created by sampling 200 data points from each of 5 randomly generated subspaces, and its values are normalized such that \u2016L0\u2016\u221e = 1; S0 is consisting of random values from Bernoulli \u00b11. The dimension of each subspace varies from 1 to 20 with step size 1, and thus the rank of L0 varies from 5 to 100\n5. This detail also suggests that \u03bb = 1/ \u221a n1 may not be the \u201cbest\u201d choice.\nwith step size 5. The fraction |\u2126|/(mn) varies from 2.5% to 50% with step size 2.5%. For each pair of rank and support size (r0, |\u2126|), we run 10 trials, resulting in a total of 4000 (20\u00d7 20\u00d7 10) trials.\nFigure 5 compares our Algorithm 1 to RPCA, both using \u03bb = 1/ \u221a n1. It can be seen that the learnt dictionary matrix works distinctly better than the identity dictionary adopted by RPCA. Namely, the success area (i.e., the area of the white region) of our algorithm is 46% wider than that of RPCA! One may have noticed that RPCA owns a region to be exactly successful. This is because in these experiments the coherence parameters are not too large, namely \u00b51(L0) \u2264 3.5 and \u00b52(L0) \u2264 13.7. Whenever \u00b52 reaches the upper bound n, e.g., the example shown in Figure 3, the success region of RPCA will vanish."}, {"heading": "5.2 Results on Corrupted Motion Sequences", "text": "We now experiment with 11 additional sequences attached to the Hopkins155 (Tron and Vidal, 2007) database. In those sequences, about 10% of the entries in the data matrix of trajectories are unobserved (i.e., missed) due to visual occlusion. We replace each missing entry with a number from Bernoulli \u00b11, resulting in a collection of corrupted trajectory matrices for evaluating the effectiveness of matrix recovery algorithms. We perform subspace clustering on both the corrupted trajectory matrices and the recovered versions, and use the clustering error rates produced by existing subspace clustering methods as the evaluation metrics. We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) (Costeira and Kanade, 1998), Low-Rank Representation with A = X (Liu et al., 2010b) (which is referred to as \u201cLRRx\u201d) and Sparse Subspace Clustering (SSC) (Elhamifar and Vidal, 2009).\nTable 1 shows the error rates of various algorithms. Without the preprocessing of matrix recovery, all the subspace clustering methods fail to accurately categorize the trajectories of motion objects, producing error rates higher than 20%. This illustrates that it is important\nfor motion segmentation to correct the gross corruptions possibly existing in the data matrix of trajectories. By using RPCA (\u03bb = 1/ \u221a n1) to correct the corruptions, the clustering performances of all considered methods are improved dramatically. For example, the error rate of SSC is reduced from 22.9% to 9.5%. By choosing a better dictionary (than the identity) for LRR (\u03bb = 1/ \u221a n1), the error rates can be reduced again, namely from 9.5% to 5.7%, which is a 40% improvement. These results verify the effectiveness of our dictionary learning strategy in realistic environments."}, {"heading": "6. Conclusion and Future Work", "text": "In this paper, we studied the problem of disentangling the low-rank (L0) and sparse (S0) components in a given data matrix. Whenever the low-rank component owns some extra structures, the state-of-the-art RPCA method might fail even if L0 is strictly low-rank. As a typical example, consider the case where there is a mixture structure of multiple subspaces underlying L0. When the subspace (i.e., cluster) number goes large, the second coherence parameter will enlarge and thus the performance of RPCA degrades. To overcome the challenges arising from coherent data, theoretically, one needs to capture the extra structures that produce high coherence. Nevertheless, such a strategy suffers several practical issues and is therefore infeasible. In sharp contrast, it is much simpler to solve the problem by LRR: When the dictionary matrix A in LRR satisfies certain conditions, namely A is low-rank and U0 \u2282 UA, LRR can avoid the second coherence parameter that has potential to be large. Furthermore, we established a heuristic algorithm that utilizes RPCA to approximately pursue a qualified dictionary. Experimental results showed that our algorithm performed better than RPCA. However, there still remain several problems for future work.\n\u22c4 By AZ\u2217 = L0, the column space of the dictionary A approximately has the same properties as L0, and thus, roughly, \u00b51(A) \u2248 \u00b51(L0). So this paper still needs to assume that the first coherence parameter \u00b51 is small and only addresses the cases where the second coherence parameter \u00b52 might be large. In some domains such as the text documents, both the row space and column space can own some clustering structures, and thus both \u00b51 and \u00b52 can be large. New models are required to well handle such coherent data.\n\u22c4 It is possible to prove that Algorithm 1 is superior over RPCA in theory, because the conditions (i.e., A is low-rank and U0 \u2282 UA) required by Algorithm 1 to succeed are weaker than A = L0. It is significant to accurately identify in which conditions RPCA can produce a solution that is able to meet those conditions.\n\u22c4 While theorem 1 points out a generic direction for learning the dictionary matrix in LRR, the specific learning procedure is not unique and our Algorithm 1 is not exclusive either. For example, one may drive some kind of optimization framework to jointly compute the variables A and Z."}, {"heading": "Acknowledgement", "text": "Guangcan Liu is a Postdoctoral Researcher supported by nsf-dms0808864, nsf-ses1131848, nsf-eager1249316, AFOSR-FA9550-13-1-0137, and ONR-N00014-13-1-0764. Ping Li is also partially supported by nsf-iii1360971 and nsf-bigdata1419210."}, {"heading": "Appendix A. List of Notations", "text": "(\u00b7)+ Moore-Penrose pseudoinverse of a matrix. \u2297 Kronecker product. ei The ith standard basis. [\u00b7]ij The (i, j)th entry of a matrix. X \u2208 Rm\u00d7n The observed data matrix. A, UA\u03a3AV T A The dictionary matrix, and its SVD L0, U0\u03a30V T 0 The ground truth of the data matrix, and its SVD. S0 \u2208 Rm\u00d7n The ground truth of the corruption matrix. U\u03a3V T The SVD of A+L0. r0, rA The ranks of L0 and A. \u03b3A The condition number of A. \u00b51, \u00b52, \u00b53 The first, second and third coherence parameters of a matrix. \u00b5A3 (\u00b7) The third coherence parameter of a matrix, associating with A. n1, n2 n1 = max(m,n),n2 = min(m,n). \u2126 Locations of the nonzero entries of S0. \u2126c The complement of \u2126. PU0 , PV0 The projections onto the space spanned by U0 (resp. V0). P\u2126, P\u2126\u22a5 The projections onto the space of matrices supported on \u2126 (resp. \u2126c). I,I The identity matrix and the identity operator. |\u2126| The cardinality of \u2126, i.e., the number of nonzero entries in S0. sign(\u00b7) The signum function. \u2202 The subgradient of a function. \u2016 \u00b7 \u20162 The \u21132 norm of a vector. \u3008\u00b7\u3009 The inner product of two matrices or vectors. \u2016 \u00b7 \u2016 The operator norm or 2-norm of a matrix, i.e., largest singular value. \u2016 \u00b7 \u2016\u2217 The nuclear norm of a matrix. \u2016 \u00b7 \u2016F The Frobenius norm of a matrix. \u2016 \u00b7 \u20162,\u221e The \u21132,\u221e norm, i.e., the largest \u21132 norm of the columns of a matrix. \u2016 \u00b7 \u20161 The \u21131 norm of a matrix seen as a long vector. \u2016 \u00b7 \u2016\u221e The sup-norm of a matrix seen as a long vector. Ber(\u03c1) A Bernoulli distribution with expected value \u03c1 and variance \u03c1(1\u2212 \u03c1)."}, {"heading": "Appendix B. Why Does \u00b52 Increase with the Cluster Number?", "text": "B.1 Zipf\u2019s Law\nWhen the data points are sampled from a low-rank subspace uniformly at random, it has been proven by (Cande\u0300s and Recht, 2009) that the first and second coherence parameters are bounded. Namely, \u00b51(L0) \u2264 c and \u00b52(L0) \u2264 c for some numerical constant c independent of the characteristics of L0. Although correct, such a property is not enough to interpret the phenomenon that the coherence parameters increase with the cluster number underlying L0. Hence, it is necessary to establish a more accurate rule to characterize the coherence parameters. Through extensive experiments, we find that the first and second coherence parameters actually follow the well-known Zipf\u2019s law. More precisely, if the data\npoints (which form the column vectors of L0 \u2208 Rm\u00d7n) are uniformly sampled from a r0dimensional subspace, then, roughly, the logarithm of coherence is inversely proportional to the logarithm of 1 + r0. That is,\nlog(\u00b51(L0)) log(1 + r0) \u2248 c1 and log(\u00b52(L0)) log(1 + r0) \u2248 c2, (B.11)\nwhere c1 and c2 are two constants. The results in Figure 6 verify the above Zipf\u2019s law. Note that the Zipf\u2019s law (B.11) can also induce the boundedness property proved by (Cande\u0300s and Recht, 2009). Namely, (B.11) approximately gives that \u00b51(L0) \u2264 exp(c1/ log 2) and \u00b52(L0) \u2264 exp(c2/ log 2).\nThe above Zipf\u2019s law suggests that the coherence must be inversely proportional to the rank of data. This is intuitively interpretable. Let yj = [U0]ij and Cr0 = \u2016UT0 ei\u201622 = \u2211r0\nj=1 y 2 j . Then it can be seen that Cr0 is the squared Euclidean length of the first r0 components of a unit vector distributed on the m-dimensional unit sphere. With these notations, it can be seen that \u00b51 is the largest order statistic of Cr0 divided by the expectation of Cr0 :\n\u00b51(L0) = m\nr0 max i \u2016UT0 ei\u201622 = maxi \u2016UT0 ei\u201622 r0 m = max(Cr0) E(Cr0) .\nNow it is unfolded that the first (and second) coherence parameter of a matrix with rank r0 is actually some kind of uncertainty of the first r0 components of a unit-normed, mdimensional random vector. Thus if r0 = m (i.e., L0 is full rank), then the uncertainty vanishes and \u00b51(L0) = 1. Similarly if r0 = 1, the uncertainty measured by max(Cr0)/E(Cr0) is as high as that of a single random number.\nThe Zipf\u2019s law (B.11) is useful, because it provides us a trackable approach to estimate the coherence parameters when the data points are not uniformly sampled, as will be shown in the next section.\nB.2 An Explanation to the \u00b52-phenomenon\nIdeally, if the values in U0 and V0 are perfectly spreading out, namely [U0]ij = [U0]i1j1 and [V0]ij = [V0]i1j1 ,\u2200i, j, i1, j1, then \u00b51(L0) = \u00b52(L0) = 1. However, this is unlikely for \u00b52(L0) to happen, as it is provable that the row projector V0V T 0 , which is also known as Shape Interaction Matrix (SIM) in subspace clustering, measures the subspace membership of the data points (Costeira and Kanade, 1998; Liu et al., 2013). More precisely, if the data points in L0 are sampled from k number of independent subspaces, saying L0 = [L (1) 0 , \u00b7 \u00b7 \u00b7 , L (k) 0 ], where L (i) 0 with SVD Ui\u03a3iV T i is a matrix of data points from the ith subspace, then V0 is equivalent to a block-diagonal matrix that has nonzero entries only on k number of blocks:\nV0 \u223c\n\n   \nV1 0 0 0 0 V2 0 0\n0 0 . . . 0 0 0 0 Vk\n\n    .\nIn this case, it is demonstrable that the second coherence parameter \u00b52(L0) depends on the cluster number k. For the convenience of analysis, we assume that the dimensions of\nall subspaces are equal, i.e., rank (\nL (i) 0\n)\n= r0/k,\u2200, i = 1, \u00b7 \u00b7 \u00b7 , k, and the sampling in each subspace is uniform. Then the Zipf\u2019s law (B.11) gives\n\u00b52(L0) = max i\n\u00b52(L (i) 0 ) \u2248 exp( c2 log(1 + r0\nk ) ), (B.12)\nwhere k is the cluster number. Hence, approximately, the second coherence parameter \u00b52(L0) will increase with the cluster number underlying L0."}, {"heading": "Appendix C. Proof of Theorem 2", "text": "Proof Let (Z\u2217, S\u2217) be an optimal solution to (3.8). Denote NL = AZ \u2217\u2212L0, NS = S\u2217\u2212S0 and E = NL +NS. Then we have\n\u2016E\u2016F = \u2016(X \u2212 L0 \u2212 S0)\u2212 (X \u2212AZ\u2217 \u2212 S\u2217)\u2016F \u2264 \u2016(X \u2212 L0 \u2212 S0)\u2016F + \u2016(X \u2212AZ\u2217 \u2212 S\u2217)\u2016F \u2264 2\u03b5.\nProvided that |\u2126| < (0.35 \u2212 \u01eb)mn, the proof process of Lemma 9 shows that\n\u2016F\u2016\u221e < 0.5.\nBy the optimality of (Z\u2217, S\u2217),\n\u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 \u2265 \u2016Z\u2217\u2016\u2217 + \u03bb\u2016S\u2217\u20161 \u2265 \u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u3008UV T , Z\u2217 \u2212A+L0\u3009+ \u03bb\u3008sign(S0) +H,NS\u3009 = \u2016A+L0\u2016\u2217 + \u03bb\u2016S0\u20161 + \u03bb\u3008sign(S0) + F,NL\u3009+ \u03bb\u3008sign(S0) +H,NS\u3009,\nwhich leads to\n0 \u2265 \u3008sign(S0) + F,NL\u3009+ \u3008sign(S0) +H,NS\u3009 = \u3008sign(S0) + F,NL\u3009+ \u3008sign(S0) +H,E \u2212NL\u3009 = \u3008F \u2212H,NL\u3009+ \u3008sign(S0) +H,E\u3009 \u2265 0.5\u2016P\u2126\u22a5(NL)\u20161 \u2212 \u2016E\u20161.\nHence,\n\u2016P\u2126\u22a5(NL)\u2016F \u2264 \u2016P\u2126\u22a5(NL)\u20161 \u2264 2\u2016E\u20161 \u2264 2 \u221a mn\u2016E\u2016F \u2264 4 \u221a mn\u03b5.\nBy NL = AZ \u2217 \u2212 L0 \u2208 PUA ,\nNL = PPUAP\u2126\u22a5(NL),\nwhere P = I +\u2211\u221ei=1(PUAP\u2126PUA)i. By \u2016P\u2016 \u2264 2,\n\u2016NL\u2016F \u2264 \u2016P\u2016\u2016PUAP\u2126\u22a5(NL)\u2016F \u2264 \u2016P\u2016\u2016P\u2126\u22a5 (NL)\u2016F \u2264 8 \u221a mn\u03b5."}, {"heading": "Appendix D. Optimization Procedure", "text": "In this work, we use the exact ALM method to solve the optimization problem (1.3). We first convert (1.3) to the following equivalent problem:\nmin Z,S,J\n\u2016J\u2016\u2217 + \u03bb\u2016S\u20161, s.t. X = AZ + S,Z = J.\nThis problem can be solved by the ALM method, which minimizes the following augmented Lagrange function:\n\u2016J\u2016\u2217 + \u03bb\u2016S\u20161 + \u3008Y,X \u2212AZ \u2212 S\u3009+ \u3008W,Z \u2212 J\u3009+ \u03b8\n2 (\u2016X \u2212AZ \u2212 S\u20162F + \u2016Z \u2212 J\u2016 2 F )\nwith respect to J , Z and S, respectively, by fixing the other variables, and then updating the Lagrange multipliers Y and W . Algorithm 2 summarizes the whole procedure of the optimization procedure.\nAlgorithm 2 Solving Problem (1.3) by Exact ALM\nInput: data matrix X, dictionary matrix A, parameter \u03bb. Initialization: Z = J = 0, S = 0, Y = 0,W = 0, \u03b8 = 0.1, \u03c4 = 5. while not converged do\n1. Alternating minimization: while not converged do\n1.1. fix the others and update J by\nJ = argmin 1\n\u03b8 ||J ||\u2217 +\n1 2 ||J \u2212 (Z +W/\u03b8)||2F .\n1.2. fix the others and update Z by\nZ = (I+ATA)\u22121(AT (X \u2212 S) + J + (ATY \u2212W )/\u03b8).\n1.3. fix the others and update S by\nS = argmin \u03bb\n\u03b8 \u2016S\u20161 +\n1 2 ||S \u2212 (X \u2212AZ + Y/\u03b8)||2F .\nend while 2. update the Lagrange multipliers and the parameter \u03b8\nY = Y + \u03b8(X \u2212AZ \u2212 S), W = W + \u03b8(Z \u2212 J), \u03b8 = \u03b8\u03c4.\nend while"}], "references": [{"title": "Synthetic aperture radar imaging and motion estimation via robust principle component analysis", "author": ["Liliana Borcea", "Thomas Callaghan", "George Papanicolaou"], "venue": "Arxiv,", "citeRegEx": "Borcea et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Borcea et al\\.", "year": 2012}, {"title": "Matrix completion with noise", "author": ["Emmanuel Cand\u00e8s", "Yaniv Plan"], "venue": "In IEEE Proceeding,", "citeRegEx": "Cand\u00e8s and Plan.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Plan.", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "Journal of the ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "A multibody factorization method for independently moving objects", "author": ["Joao Costeira", "Takeo Kanade"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Costeira and Kanade.,? \\Q1998\\E", "shortCiteRegEx": "Costeira and Kanade.", "year": 1998}, {"title": "Sparse subspace clustering", "author": ["E. Elhamifar", "R. Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Elhamifar and Vidal.,? \\Q2009\\E", "shortCiteRegEx": "Elhamifar and Vidal.", "year": 2009}, {"title": "Matrix rank minimization with applications", "author": ["M. Fazel"], "venue": "PhD thesis,", "citeRegEx": "Fazel.,? \\Q2002\\E", "shortCiteRegEx": "Fazel.", "year": 2002}, {"title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["Martin Fischler", "Robert Bolles"], "venue": "Communications of the ACM,", "citeRegEx": "Fischler and Bolles.,? \\Q1981\\E", "shortCiteRegEx": "Fischler and Bolles.", "year": 1981}, {"title": "Robust estimates, residuals, and outlier detection with multiresponse data", "author": ["R. Gnanadesikan", "J.R. Kettenring"], "venue": null, "citeRegEx": "Gnanadesikan and Kettenring.,? \\Q1972\\E", "shortCiteRegEx": "Gnanadesikan and Kettenring.", "year": 1972}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["D. Gross"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Gross.,? \\Q2011\\E", "shortCiteRegEx": "Gross.", "year": 2011}, {"title": "Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming", "author": ["Qifa Ke", "Takeo Kanade"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Ke and Kanade.,? \\Q2005\\E", "shortCiteRegEx": "Ke and Kanade.", "year": 2005}, {"title": "A framework for robust subspace learning", "author": ["Fernando De la Torre", "Michael J. Black"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Torre and Black.,? \\Q2003\\E", "shortCiteRegEx": "Torre and Black.", "year": 2003}, {"title": "Unsupervised object segmentation with a hybrid graph model (hgm)", "author": ["Guangcan Liu", "Zhouchen Lin", "Xiaoou Tang", "Yong Yu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Yong Yu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Exact subspace segmentation and outlier detection by low-rank representation", "author": ["Guangcan Liu", "Huan Xu", "Shuicheng Yan"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Rahul Mazumder", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Low-rank and sparse matrix decomposition for accelerated dynamic mri with separation of background and dynamic components", "author": ["Ricardo Otazo", "Emmanuel Cand\u00e8s", "Daniel K. Sodickson"], "venue": "Arxiv,", "citeRegEx": "Otazo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Otazo et al\\.", "year": 2012}, {"title": "Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images", "author": ["YiGang Peng", "Arvind Ganesh", "John Wright", "Wenli Xu", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Peng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2012}, {"title": "Convex Analysis", "author": ["R. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1970\\E", "shortCiteRegEx": "Rockafellar.", "year": 1970}, {"title": "Random vectors in the isotropic position", "author": ["M. Rudelson"], "venue": "Journal of Functional Analysis,", "citeRegEx": "Rudelson.,? \\Q1999\\E", "shortCiteRegEx": "Rudelson.", "year": 1999}, {"title": "Generalization error bounds for collaborative prediction with low-rank matrices", "author": ["Nathan Srebro", "Tommi Jaakkola"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Srebro and Jaakkola.,? \\Q2005\\E", "shortCiteRegEx": "Srebro and Jaakkola.", "year": 2005}, {"title": "A benchmark for the comparison of 3-d motion segmentation algorithms", "author": ["Roberto Tron", "Rene Vidal"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Tron and Vidal.,? \\Q2007\\E", "shortCiteRegEx": "Tron and Vidal.", "year": 2007}, {"title": "Cofi rank maximum margin matrix factorization for collaborative ranking", "author": ["Markus Weimer", "Alexandros Karatzoglou", "Quoc V. Le", "Alex J. Smola"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Weimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weimer et al\\.", "year": 2007}, {"title": "Robust pca via outlier pursuit", "author": ["Huan Xu", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Outlier-robust pca: The highdimensional case", "author": ["Huan Xu", "Constantine Caramanis", "Shie Mannor"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Tilt: Transform invariant low-rank textures", "author": ["Zhengdong Zhang", "Arvind Ganesh", "Xiao Liang", "Yi Ma"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Abstract The recently established RPCA (Cand\u00e8s et al., 2011) method provides us a convenient way to restore low-rank matrices from grossly corrupted observations.", "startOffset": 39, "endOffset": 60}, {"referenceID": 15, "context": "We show that it is possible for Low-Rank Representation (LRR) (Liu et al., 2013) to overcome the challenges raised by coherent data, as long as the dictionary in LRR is configured appropriately.", "startOffset": 62, "endOffset": 80}, {"referenceID": 1, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 2, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 3, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 7, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 8, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 9, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 10, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 25, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 15, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 16, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 24, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010).", "startOffset": 2, "endOffset": 296}, {"referenceID": 1, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Fischler and Bolles, 1981; Gnanadesikan and Kettenring, 1972; Gross, 2011; Ke and Kanade, 2005; la Torre and Black, 2003; Xu et al., 2013; Liu et al., 2013; Mazumder et al., 2010; Soltanolkotabi et al., 2013; Xu et al., 2010). One of the most exciting methods is probably the so-called RPCA (Robust Principal Component Analysis) method by Cand\u00e8s et al. (2011), built upon the exploration of the following low-rank matrix recovery problem:", "startOffset": 3, "endOffset": 431}, {"referenceID": 6, "context": "where \u2016 \u00b7 \u2016\u2217 is the nuclear norm (Fazel, 2002) of a matrix, \u2016 \u00b7 \u20161 denotes the l1 norm of a matrix seen as a long vector, and \u03bb > 0 is a parameter.", "startOffset": 33, "endOffset": 46}, {"referenceID": 26, "context": ", image processing (Zhang et al., 2012), computer vision (Peng et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 18, "context": ", 2012), computer vision (Peng et al., 2012), radar imaging (Borcea et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 0, "context": ", 2012), radar imaging (Borcea et al., 2012), magnetic resonance imaging (Otazo et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 17, "context": ", 2012), magnetic resonance imaging (Otazo et al., 2012), etc.", "startOffset": 36, "endOffset": 56}, {"referenceID": 4, "context": "Figure 1 demonstrates a typical example of extra structures; that is, the clustering structure which is ubiquitous in modern applications (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Soltanolkotabi et al., 2013).", "startOffset": 138, "endOffset": 221}, {"referenceID": 5, "context": "Figure 1 demonstrates a typical example of extra structures; that is, the clustering structure which is ubiquitous in modern applications (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Soltanolkotabi et al., 2013).", "startOffset": 138, "endOffset": 221}, {"referenceID": 3, "context": "Nevertheless, as explained in (Cand\u00e8s et al., 2011; Cand\u00e8s and Recht, 2009), the coherence parameters are indeed necessary for matrix recovery (if there is no additional condition available).", "startOffset": 30, "endOffset": 75}, {"referenceID": 2, "context": "Nevertheless, as explained in (Cand\u00e8s et al., 2011; Cand\u00e8s and Recht, 2009), the coherence parameters are indeed necessary for matrix recovery (if there is no additional condition available).", "startOffset": 30, "endOffset": 75}, {"referenceID": 15, "context": "Our study is based on the following convex program termed Low-Rank Representation (LRR) (Liu et al., 2013): min Z,S \u2016Z\u2016\u2217 + \u03bb\u2016S\u20161, s.", "startOffset": 88, "endOffset": 106}, {"referenceID": 1, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Xu et al., 2010; Liu et al., 2012).", "startOffset": 2, "endOffset": 105}, {"referenceID": 2, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Xu et al., 2010; Liu et al., 2012).", "startOffset": 2, "endOffset": 105}, {"referenceID": 3, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Xu et al., 2010; Liu et al., 2012).", "startOffset": 2, "endOffset": 105}, {"referenceID": 24, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Xu et al., 2010; Liu et al., 2012).", "startOffset": 2, "endOffset": 105}, {"referenceID": 14, "context": ", (Cand\u00e8s and Plan, 2010; Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Xu et al., 2010; Liu et al., 2012).", "startOffset": 2, "endOffset": 105}, {"referenceID": 15, "context": "\u22c4 This paper provides insights regarding the LRR model proposed by (Liu et al., 2013).", "startOffset": 67, "endOffset": 85}, {"referenceID": 21, "context": ", (Srebro and Jaakkola, 2005; Weimer et al., 2007).", "startOffset": 2, "endOffset": 50}, {"referenceID": 23, "context": ", (Srebro and Jaakkola, 2005; Weimer et al., 2007).", "startOffset": 2, "endOffset": 50}, {"referenceID": 2, "context": "The coherence parameters defined in (Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011) are excellent exemplars of such quantities.", "startOffset": 36, "endOffset": 81}, {"referenceID": 3, "context": "The coherence parameters defined in (Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011) are excellent exemplars of such quantities.", "startOffset": 36, "endOffset": 81}, {"referenceID": 3, "context": "In (Cand\u00e8s et al., 2011), another coherence parameter, called as the third coherence parameter and denoted as 1 \u2264 \u03bc3 \u2264 mn, is also introduced: \u03bc3(L0) = mn r0 (\u2016U0V T 0 \u2016\u221e) = mn r0 max i,j (|\u3008U 0 ei, V T 0 ej\u3009|).", "startOffset": 3, "endOffset": 24}, {"referenceID": 3, "context": "We include it just for the sake of consistence with (Cand\u00e8s et al., 2011).", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "The analysis in (Cand\u00e8s et al., 2011) merges the above three parameters into a single one: \u03bc(L0) = max{\u03bc1(L0), \u03bc2(L0), \u03bc3(L0)}.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "2 \u03bc2-phenomenon Cand\u00e8s et al. (2011) have proven that the success condition (regarding L0) of RPCA is rank (L0) \u2264 n2 cr\u03bc(L0)(log n1) , (3.", "startOffset": 16, "endOffset": 37}, {"referenceID": 4, "context": ", face, texture and motion (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Liu et al., 2010a).", "startOffset": 27, "endOffset": 100}, {"referenceID": 5, "context": ", face, texture and motion (Costeira and Kanade, 1998; Elhamifar and Vidal, 2009; Liu et al., 2010a).", "startOffset": 27, "endOffset": 100}, {"referenceID": 2, "context": "Unfortunately, as explained in (Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery.", "startOffset": 31, "endOffset": 94}, {"referenceID": 3, "context": "Unfortunately, as explained in (Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery.", "startOffset": 31, "endOffset": 94}, {"referenceID": 14, "context": "Unfortunately, as explained in (Cand\u00e8s and Recht, 2009; Cand\u00e8s et al., 2011; Liu et al., 2012), the coherence parameters are necessary for identifying accurately the success conditions of matrix recovery.", "startOffset": 31, "endOffset": 94}, {"referenceID": 15, "context": "The reason is that, as has been explored by (Liu et al., 2013), the complexity of solving the LRR problem (1.", "startOffset": 44, "endOffset": 62}, {"referenceID": 3, "context": "1 Settings and Some Basic Lemmas The same as in RPCA (Cand\u00e8s et al., 2011), we assume that the locations of the corrupted entries are selected uniformly at random.", "startOffset": 53, "endOffset": 74}, {"referenceID": 3, "context": "For general sign matrices, the same as in RPCA (Cand\u00e8s et al., 2011), our Theorem 1 can still be proved by globally placing an elimination theorem and a derandomization scheme.", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "Provided that A \u2208 R is fairly low-rank, the analysis in (Cand\u00e8s et al., 2011) gives that \u2016PTAP\u03a9\u2016 \u2264 \u221a", "startOffset": 56, "endOffset": 77}, {"referenceID": 3, "context": "Following the techniques in (Cand\u00e8s et al., 2011), we have the following lemma to bound the operator norm of PUAP\u03a9.", "startOffset": 28, "endOffset": 49}, {"referenceID": 20, "context": "Then by using the results in (Rudelson, 1999) and following the proof procedure of (Cand\u00e8s and Recht, 2009), we have that", "startOffset": 29, "endOffset": 45}, {"referenceID": 2, "context": "Then by using the results in (Rudelson, 1999) and following the proof procedure of (Cand\u00e8s and Recht, 2009), we have that", "startOffset": 83, "endOffset": 107}, {"referenceID": 19, "context": "Proof By standard convexity arguments (Rockafellar, 1970), (AL0, S0) is an optimal solution to (1.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": "1 of (Liu et al., 2013) gives that \u2016AL0 + \u22061\u2016\u2217 > \u2016AL0\u2016\u2217 strictly holds unless \u22061 = 0.", "startOffset": 5, "endOffset": 23}, {"referenceID": 15, "context": "3 of (Liu et al., 2013).", "startOffset": 5, "endOffset": 23}, {"referenceID": 22, "context": "2 Results on Corrupted Motion Sequences We now experiment with 11 additional sequences attached to the Hopkins155 (Tron and Vidal, 2007) database.", "startOffset": 114, "endOffset": 136}, {"referenceID": 4, "context": "We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) (Costeira and Kanade, 1998), Low-Rank Representation with A = X (Liu et al.", "startOffset": 95, "endOffset": 122}, {"referenceID": 5, "context": ", 2010b) (which is referred to as \u201cLRRx\u201d) and Sparse Subspace Clustering (SSC) (Elhamifar and Vidal, 2009).", "startOffset": 79, "endOffset": 106}, {"referenceID": 2, "context": "1 Zipf\u2019s Law When the data points are sampled from a low-rank subspace uniformly at random, it has been proven by (Cand\u00e8s and Recht, 2009) that the first and second coherence parameters are bounded.", "startOffset": 114, "endOffset": 138}, {"referenceID": 2, "context": "11) can also induce the boundedness property proved by (Cand\u00e8s and Recht, 2009).", "startOffset": 55, "endOffset": 79}, {"referenceID": 4, "context": "However, this is unlikely for \u03bc2(L0) to happen, as it is provable that the row projector V0V T 0 , which is also known as Shape Interaction Matrix (SIM) in subspace clustering, measures the subspace membership of the data points (Costeira and Kanade, 1998; Liu et al., 2013).", "startOffset": 229, "endOffset": 274}, {"referenceID": 15, "context": "However, this is unlikely for \u03bc2(L0) to happen, as it is provable that the row projector V0V T 0 , which is also known as Shape Interaction Matrix (SIM) in subspace clustering, measures the subspace membership of the data points (Costeira and Kanade, 1998; Liu et al., 2013).", "startOffset": 229, "endOffset": 274}], "year": 2014, "abstractText": "The recently established RPCA (Cand\u00e8s et al., 2011) method provides us a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA may be not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when the data is strictly low-rank. This is because RPCA prefers incoherent data, which, however, could be inconsistent with some natural structures of data. As a typical example, consider the clustering structure which is ubiquitous in modern applications. As the number of cluster grows, the coherence parameters of data keep increasing, and accordingly, the recovery performance of RPCA degrades. We show that it is possible for Low-Rank Representation (LRR) (Liu et al., 2013) to overcome the challenges raised by coherent data, as long as the dictionary in LRR is configured appropriately. Namely, we mathematically prove that if the dictionary itself is low-rank then LRR can avoid the coherence parameters which have potential to be large. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Our extensive experiments on randomly generated matrices and real motion sequences show promising results.", "creator": "LaTeX with hyperref package"}}}