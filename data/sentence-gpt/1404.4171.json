{"id": "1404.4171", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2014", "title": "Dropout Training for Support Vector Machines", "abstract": "Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. The results suggest that the approach to modeling linear models is much better than a generalized linear model.\n\n\n\nThe SVMs with a training procedure are not fully predictive of error-free learning.\nIn a previous paper, we compared a model with a model with an unsupervised model. Our model was trained on repeated tests. We found that, whereas a model with a training procedure does not consistently tell the correct sample size, the model with a training procedure with a training procedure does not reliably tell the correct sample size. We also found that a model with a training procedure fails to accurately tell the correct sample size. Furthermore, a model with a training procedure does not consistently tell the correct sample size.\nIn a previous paper, we observed a linear model with a training procedure with a training procedure. We used two training models that were both not sufficiently sensitive to statistical significance. A training procedure that was highly sensitive to random variable size was the model with a training procedure that has a training procedure that can be manipulated to make a prediction of the training data. The training procedure, a model with a training procedure, has a training procedure that can be manipulated to make a prediction of the training data. This approach is not perfect, and the fact that it is possible to do so, as in previous work, is surprising.\nA model with a training procedure that can be manipulated to make a prediction of the training data. We observed a linear model with a training procedure that can be manipulated to make a prediction of the training data. We found that, whereas a model with a training procedure does not consistently tell the correct sample size, the model with a training procedure does not reliably tell the correct sample size. Similarly, a model with a training procedure does not reliably tell the correct sample size. Furthermore, a model with a training procedure does not reliably tell the correct sample size. Moreover, a model with a training procedure does not reliably tell the correct sample size. Additionally, a model with a training procedure does not reliably tell the correct sample size. Additionally, a model with a training procedure does not reliably tell the correct sample size. Furthermore, a model with a training procedure", "histories": [["v1", "Wed, 16 Apr 2014 08:54:01 GMT  (55kb,D)", "http://arxiv.org/abs/1404.4171v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ning chen", "jun zhu", "jianfei chen 0001", "bo zhang"], "accepted": true, "id": "1404.4171"}, "pdf": {"name": "1404.4171.pdf", "metadata": {"source": "CRF", "title": "Dropout Training for Support Vector Machines", "authors": ["Ning Chen", "Jun Zhu", "Jianfei Chen", "Bo Zhang"], "emails": ["{ningchen@mail,", "dcszj@mail,", "chenjf10@mails,", "dcszb@mail}.tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Artificial feature noising augments the finite training data with an infinite number of corrupted versions, by corrupting the given training examples with a fixed noise distribution. Among the many noising schemes, dropout training (Hinton et al. 2012) is an effective way to control over-fitting by randomly omitting subsets of features at each iteration of a training procedure. By formulating the feature noising methods as minimizing the expectation of some loss functions under the corrupting distributions, recent work has provided theoretical understandings of such schemes from the perspective of adaptive regularization (Wager, Wang, and Liang 2013); and has shown promising empirical results in various applications, including document classification (van der Maaten et al. 2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013).\nRegarding the loss functions, though much work has been done on the quadratic loss, logistic loss, or the log-loss\nCopyright c\u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ninduced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995). One technical challenge is that the non-smoothness of the hinge loss makes it hard to compute or even approximate its expectation under a given corrupting distribution. Existing methods are not directly applicable, therefore calling for new solutions. This paper attempts to address this challenge and fill up the gap by extending dropout training as well as other feature noising schemes to support vector machines.\nPrevious efforts on learning SVMs with feature noising have been devoted to either explicit corruption or an adversarial worst-case analysis. For example, virtual support vector machines (Burges and Scholkopf 1997) explicitly augment the training data, which are usually support vectors from previous learning iterations for computational efficiency, with additional examples that are corrupted through some invariant transformation models. A standard SVM is then learned on the corrupted data. Though simple and effective, such an approach lacks elegance and the computational cost of processing the additional corrupted examples could be prohibitive for many applications. The other work (Globerson and Roweis 2006; Dekel and Shamir 2008; Teo et al. 2008) adopts an adversarial worst-case analysis to improve the robustness of SVMs against feature deletion in testing data. Though rigorous in theory, a worst-case scenario is unlikely to be encountered in practice. Moreover, the worst-case analysis usually results in solving a complex and computationally demanding problem.\nIn this paper, we show that it is efficient to train linear SVM predictors on an infinite amount of corrupted copies of the training data by marginalizing out the corruption distributions, an average-case analysis. We concentrate on dropout training, but the results are directly applicable to other noising models, such as Gaussian, Poisson and Laplace (van der Maaten et al. 2013). For all these noising schemes, the resulting expected hinge loss can be upperbounded by a variational objective by introducing auxiliary variables, which follow a generalized inverse Gaussian distribution. We then develop an iteratively re-weighted least square (IRLS) algorithm to minimize the variational bounds. At each iteration, our algorithm minimizes the ex-\nar X\niv :1\n40 4.\n41 71\nv1 [\ncs .L\nG ]\n1 6\nA pr\n2 01\n4\npectation of a re-weighted quadratic loss under the given corrupting distribution, where the re-weights are computed in a simple closed form. We further apply the similar ideas to develop a new IRLS algorithm for the dropout training of logistic regression, which extends the well-known IRLS algorithm for standard logistic regression (Hastie, Tibshirani, and Friedman 2009). Our IRLS algorithms shed light on the connection and difference between the hinge loss and logistic loss in the context of dropout training, complementing to the previous analysis (Rosasco et al. 2004; Globerson et al. 2007) in the supervised learning settings. Finally, empirical results on classification and a challenging \u201cnightmare at test time\u201d scenario (Globerson and Roweis 2006) demonstrate the effectiveness of our approaches, in comparison with various strong competitors."}, {"heading": "Preliminaries", "text": "We setup the problem in question and review the learning with marginalized corrupted features."}, {"heading": "Regularized loss minimization", "text": "Consider the binary classification, where each training example is a pair (x, y) with x \u2208 RD being an input feature vector and y \u2208 {+1,\u22121} being a binary label. Given a set of training data D = {(xn, yn)}Nn=1, supervised learning aims to find a function f \u2208 F that maps each input to a label. To find the optimal candidate, it commonly solves a regularized loss minimization problem\nmin f\u2208F\n\u2126(f) + 2c \u00b7 R(D; f), (1)\nwhereR(D; f) is the risk of applying f to the training data; \u2126(f) is a regularization term to control over-fitting; and c is a non-negative regularization parameter.\nFor linear models, the function f is simply parameterized as f(x;w, b) = w>x+ b, where w is the weight vector and b is an offset. We will denote \u03b8 := {w, b} for clarity. Then, the regularization can be any Euclidean norms1, e.g., the `2-norm, \u2126(w) = \u2016w\u201622, or the `1-norm, \u2126(w) = \u2016w\u20161. For the loss functions, the most relevant measure is the training error, \u2211N n=1 \u03b4(f(xn;\u03b8) 6= yn), which however is not easy to optimize. A convex surrogate loss is used instead, which normally upper bounds the training error. Two popular examples are the hinge loss and logistic loss2:\nRh(D;\u03b8) = N\u2211\nn=1\nmax (0, `\u2212 ynf(xn;\u03b8)) ,\nRl(D;\u03b8) = N\u2211\nn=1\n(\u2212 log p(yn|xn,\u03b8)) ,\nwhere `(> 0) is the cost of making a wrong prediction, and p(yn|xn,\u03b8) := 1/(1 + exp(\u2212ynf(xn;\u03b8))) is the logistic likelihood. Other losses include the quadratic loss,\n\u2211N n=1(f(xn;\u03b8) \u2212 yn)2, and the exponential loss,\u2211N\nn=1 exp(\u2212ynf(xn;\u03b8)), whose feature noising analyses are relatively simpler (van der Maaten et al. 2013).\n1It is a common practice to not regularize the offset. 2The natural logarithm is not an upper bound of the training\nerror. We can simply change the base without affecting learning.\nLearning with marginalized corruption Let x\u0303 be the corrupted version of the input features x. Consider the commonly used independent corrupting model:\np(x\u0303|x) = D\u220f d=1 p(x\u0303d|xd; \u03b7d),\nwhere each individual distribution is a member of the exponential family, with the natural parameter \u03b7d. Another common assumption is that the corrupting distribution is unbiased, that is, Ep[x\u0303|x] = x, where we use Ep[\u00b7] := Ep(x\u0303|x)[\u00b7] to denote the expectation taken over the corrupting distribution p(x\u0303|x). Such examples include the unbiased blankout (or dropout) noise, Gaussian noise, Laplace noise, and Poisson noise (Vincent et al. 2008; van der Maaten et al. 2013).\nFor the explicit corruption in (Burges and Scholkopf 1997), each example (xn, yn) is corrupted M times from the corrupting model p(x\u0303n|xn), resulting in the corrupted examples (x\u0303nm, yn), m \u2208 [M ]. This procedure generates a new corrupted data set D\u0303 with a larger size of NM . The generated dataset can be trained by minimizing the average loss function over M corrupted data points:\nL(D\u0303;\u03b8) = N\u2211\nn=1\n1\nM M\u2211 m=1 R(x\u0303nm, yn;\u03b8), (2)\nwhere R(x, y;\u03b8) is the loss function of the model incurred on the training example (x, y). As L(D\u0303;\u03b8) scales linearly with the number of corrupted observations, this approach may suffer from high computational costs.\nDropout training adopts the strategy of implicit corruption, which learns the model with marginalized corrupted features by minimizing the expectation of a loss function under the corrupting distribution\nL(D;\u03b8) = N\u2211\nn=1\nEp[R(x\u0303n, yn;\u03b8)]. (3)\nThe objective can be seen as a limit case of (2) when M \u2192 \u221e, by the law of large numbers. Such an expectation scheme has been widely adopted in previous work (Wager, Wang, and Liang 2013; van der Maaten et al. 2013; Wang et al. 2013; Wang and Manning 2013).\nThe choice of the loss function R in (3) can make a significant difference, in terms of computation cost and prediction accuracy. Previous work on feature noising has covered the quadratic loss, exponential loss, logistic loss, and the loss induced from generalized linear models (GLM). For the quadratic loss and exponential loss, the expectation in Eq. (3) can be computed analytically, thereby leading to simple gradient descent algorithms (van der Maaten et al. 2013). However, it does not have a closed form to compute the expectation for the logistic loss or the GLM loss. Previous analysis has resorted to approximation methods, such as using the second-order Taylor expansion (Wager, Wang, and Liang 2013) or an upper bound by applying Jensen\u2019s inequality (van der Maaten et al. 2013), both of which lead to effective algorithms in practice. In contrast, little work has been done on the hinge loss, for which the expectation under corrupting distributions cannot be analytically computed either, therefore calling for new algorithms."}, {"heading": "Learning SVMs with Corrupting Noise", "text": "We now present a simple iteratively re-weighted least square (IRLS) algorithm to learn SVMs with the expected hinge loss under corrupting distributions. Our method consists of a variational upper bound of the expected loss and a simple algorithm that iteratively minimizes an expectation of a re-weighted quadratic loss. We also apply the similar ideas to develop a simple IRLS algorithm for minimizing the expected logistic loss, thereby allowing for a systematical comparison of the hinge loss with the logistic and quadratic losses in the context of feature noising."}, {"heading": "A variational bound with data augmentation", "text": "Let \u03b6n := ` \u2212 yn(w>x\u0303n).3 Then, the expected hinge loss can be written as\nRh(D;\u03b8) = N\u2211 n=1 Ep[max (0, \u03b6n)], (4)\nSince we do not have a closed-form of the expectation of the max function, minimizing the expected loss (4) is intractable. Here, we derive a variational upper bound based on a data augmentation formulation of the expected hinge loss. Let \u03c6(yn|x\u0303n,\u03b8) = exp{\u22122cmax(0, \u03b6n)} be the pseudo-likelihood of the response variable for sample n. Then we have\nRh(D;\u03b8) = \u2212 1\n2c \u2211 n Ep[log \u03c6(yn|x\u0303n,\u03b8)]. (5)\nUsing the ideas of data augmentation (Polson and Scott 2011; Zhu et al. 2014), the pseudo-likelihood can be expressed as\n\u03c6(yn|x\u0303n,\u03b8) = \u222b \u221e 0 1\u221a 2\u03c0\u03bbn exp { \u2212 (\u03bbn + c\u03b6n) 2 2\u03bbn } d\u03bbn, (6)\nwhere \u03bbn, n \u2208 [N ], is the augmented variable. Using (6) and Jensen\u2019s inequality, we can derive a variational upper bound L of the expected hinge loss as\nL(\u03b8, q(\u03bb)) = N\u2211 n=1 { \u2212H(\u03bbn) + 1 2 Eq[log \u03bbn] (7)\n+ Eq [ 1\n2\u03bbn Ep(\u03bbn + c\u03b6n)2\n]} + constant,\nwhere H(\u03bbn) is the entropy of the variational distribution q(\u03bbn); q(\u03bb) := \u220f n q(\u03bbn) is joint distribution; and we have defined Eq[\u00b7] := Eq(\u03bb)[\u00b7] to denote the expectation taken over a variational distribution q. Now, our variational optimization problem is\nmin \u03b8,q(\u03bb)\u2208P\n\u2016w\u201622 + L(\u03b8, q(\u03bb)), (8)\nwhere P is the simplex space of normalized distributions. We should note that when there is no feature noise (i.e., x\u0303 = x), the bound is tight and we are learning the standard SVM classifier. Please see Appendix A for the derivation. We will empirically compare with SVM in experiments.\n3We treat the offset b implicitly by augmenting xn and x\u0303n with one dimension of deterministic 1. More details will be given in the algorithm."}, {"heading": "Iteratively Re-weighted Least Square Algorithm", "text": "In the upper bound, we note that when the variational distribution q(\u03bb) is given, the term Ep[(\u03bbn+c\u03b6n)2] is an expectation of a quadratic loss, which can be analytically computed. We leverage such a nice property and develop a coordinate descent algorithm to solve problem (8). Our algorithm iteratively solves the following two steps, analogous to the common two-step procedure of a variational EM algorithm.\nFor q(\u03bb) (i.e., E-step): infer the variational distribution q(\u03bb). Specifically, optimize L over q(\u03bb), we get:\nq(\u03bbn)\u221d 1\u221a \u03bbn exp\n{ \u22121\n2\n( \u03bbn +\nc2Ep[\u03b62n] \u03bbn )} \u223cGIG ( \u03bbn; 1\n2 , 1, c2Ep[\u03b62n]\n) , (9)\nwhere the second-order expectation is Ep[\u03b62n] = w>(Ep[x\u0303n]Ep[x\u0303n]> + Vp[x\u0303n])w \u22122`ynw>Ep[x\u0303n] + `2; (10) and Vp[x\u0303n] is a D \u00d7D diagonal matrix with the dth diagonal element being the variance of x\u0303nd, under the corrupting distribution p(x\u0303n|xn). We have denoted GIG(x; p, a, b) \u221d xp\u22121 exp(\u2212 12 ( b x + ax)) as a generalized inverse Gaussian distribution. Thus, \u03bb\u22121n follows an inverse Gaussian distribution\nq(\u03bb\u22121n |x\u0303n,\u03b8) \u223c IG ( \u03bb\u22121n ;\n1 c \u221a E[\u03b62n] , 1\n) (11)\nFor \u03b8 := w (i.e., M-step): removing irrelevant terms, this step involves minimizing the following objective:\nL[\u03b8] = \u2016w\u201622 + N\u2211 n=1 Ep [ c\u03b6n + c2 2 \u03b3n\u03b6 2 n ] , (12)\nwhere \u03b3n := Eq[\u03bb\u22121n ]. We observe that this substep is equivalent to minimizing the expectation of a re-weighted quadratic loss, as summarized in Lemma 1, whose proof is deferred to Appendix B, for brevity. Lemma 1. Given q(\u03bb), the M-step minimizes the reweighted quadratic loss (with the `2-norm regularizer):\n\u2016w\u201622 + c2\n2 \u2211 n \u03b3nEp [ (w>x\u0303n \u2212 yhn)2 ] , (13)\nwhere yhn = (` + 1 c\u03b3n )yn is the re-weighted label, and the re-weights are computed in closed-form:\n\u03b3n := Eq[\u03bb\u22121n ] = 1 c \u221a Ep[\u03b62n] . (14)\nFor low-dimensional data, we can solve for the closed form solutions by doing matrix inversion. Specifically, optimizing L[\u03b8] over w, we get4:\nw =\n( 2\nc2 I + N\u2211 n=1 \u03b3n(Ep[x\u0303nx\u0303>n ]) )\u22121( N\u2211 n=1 \u03b3ny h nEp[x\u0303n] ) ,\n4To consider offset, we simply augment x and x\u0303 with an additional unit of 1. The variance Vp[x\u0303n] is augmented accordingly. The identity matrix I is augmented by adding one zero row and one zero column.\nwhere Ep[x\u0303nx\u0303>n ] = Ep[x\u0303n]Ep[x\u0303n]> + Vp[x\u0303n]. However, if the data are in a high-dimensional space, e.g., text documents, the above matrix inversion will be computationally expensive. In such cases, we can use numerical methods, e.g., the quasi-Newton method.\nTo summarize, our algorithm iteratively minimizes the expectation of a simple re-weighted quadratic loss under the given corrupting distribution, where the re-weights \u03b3n are computed in an analytic form. Therefore, it is an extension of the classical iteratively re-weighted least square (IRLS) algorithm (Hastie, Tibshirani, and Friedman 2009) for dropout training. We also observe that if we fix \u03b3n at 1c and set ` = 0, we are minimizing the quadratic loss under the corrupting distribution, as studied in (van der Maaten et al. 2013). We will empirically show that our iterative algorithm for the expected hinge-loss will consistently improve over the standard quadratic loss by adaptively updating \u03b3n. Finally, as we assume that the corrupting distribution is unbiased, i.e., Ep[x\u0303|x] = x, we only need to compute the variance of the corrupting distribution, which is easy for all the existing exponential family distributions. An overview of the variance of the commonly used corrupting distributions can be found in (van der Maaten et al. 2013).\nAn IRLS algorithm for the logistic Loss We now extend the above ideas to develop a new IRLS algorithm for the logistic-loss, which also minimizes the expectation of a re-weighted quadratic loss under the corrupting distribution and computes the re-weights analytically.\nLet \u03c9n := w>x\u0303n. Then the expected logistic loss under a corrupting distribution is\nRl(D;w) = \u2212 N\u2211 n=1 Ep [ log ( eyn\u03c9n 1 + eyn\u03c9n )] . (15)\nAgain since the expectation cannot be computed in closedform, we derive a variational bound as a surrogate. Specifically, let \u03c8(yn|x\u0303n,w) = pc(yn|x\u0303n,w) = e cyn\u03c9n\n(1+eyn\u03c9n )c be the pseudo-likelihood of the response variable for sample n. We have Rl(D;w) = \u2212 1c \u2211 n Ep[log\u03c8(yn|x\u0303n,w)]. Using the recent work of data augmentation (Polson, Scott, and Windle 2012; Chen et al. 2013), the pseudo-likelihood can be expressed as\n\u03c8(yn|x\u0303n,w) = 1\n2c e\u03ban\u03c9n \u222b \u221e 0 e\u2212 \u03bbn(yn\u03c9n) 2 2 p(\u03bbn)d\u03bbn, (16)\nwhere \u03ban := c2yn and \u03bbn is the augmented Polya-gamma variable, p(\u03bbn) \u223c PG(\u03bbn; c, 0). Using (16), we can derive the upper bound of the expected logistic loss: L\u2032(w, q(\u03bb)) = N\u2211 n=1 {1 2 Eq[\u03bbn]Ep[\u03c92n]\u2212H(\u03bbn) (17)\n\u2212Eq[log p(\u03bbn)]\u2212 c\n2 ynEp[\u03c9n]\n} + constant,\nand get the variational optimization problem\nmin w,q(\u03bb)\u2208P\n\u2016w\u201622 + L\u2032(w, q(\u03bb)), (18)\nwhere q(\u03bb) is the variational distribution We solve the variational problem with a coordinate descent algorithm as follows: For q(\u03bb) (i.e., E-step): optimizingL\u2032 over q(\u03bb), we have:\nq(\u03bbn)\u221d exp ( \u22121\n2 \u03bbnEp[\u03c92n]\n) p(\u03bbn|c, 0)\n\u223cPG ( \u03bbn; c, \u221a Ep[\u03c92n] ) (19)\na Polya-Gamma distribution (Polson, Scott, and Windle 2012), where Ep[\u03c92n] = w>(Ep[x\u0303n]Ep[x\u0303n]> + Vp[x\u0303n])w.\nFor w (i.e., M-step): removing irrelevant terms, this step minimizes the objective\nL\u2032[w] = \u2016w\u2016 2 2 + N\u2211 n=1 1 2 Eq[\u03bbn]Ep[\u03c92n]\u2212 c 2 ynEp[\u03c9n]. (20)\nWe then have the optimal solution5:\nw = ( I + 1\n2 N\u2211 n=1 Eq[\u03bbn]Ep[x\u0303nx\u0303>n ]\n)\u22121( c\n4 N\u2211 n=1 ynEp[x\u0303n]\n) .\nThis is actually equivalent to minimizing the expectation of a re-weighted quadratic loss, as in Lemma 2. The proof is similar to that of Lemma 1 and the expectation of a Polya-Gamma distribution follows (Polson, Scott, and Windle 2012). Lemma 2. Given q(\u03bb), the M-step minimizes the reweighted quadratic loss (with the `2-norm regularizer)\n\u2016w\u201622 + c\n2 \u2211 n \u03b3lnEp[(w>x\u0303n \u2212 yln)2], (21)\nwhere yln = c 2\u03b3n yn is the re-weighted label, and \u03b3ln = \u03b3n c with\n\u03b3n := Eq[\u03bbn] = c 2 \u221a Ep[\u03c92n] \u00d7 e \u221a Ep[\u03c92n] \u2212 1 1 + e \u221a Ep[\u03c92n] . (22) It can be observed that if we fix \u03b3n = c2 , the IRLS algorithm reduces to minimizing the expected quadratic loss under the corrupting distribution. This is similar as in the case with SVMs, where if we set ` = 0 and fix \u03b3n = 1c , the IRLS algorithm for SVMs essentially minimizes the expected quadratic loss under the corrupting distribution. Furthermore, by sharing a similar iterative structure, our IRLS algorithms shed light on the similarity and difference between the hinge loss and the logistic loss, as summarized in Table 1. Specifically, both losses can be minimized via iteratively minimizing the expectation of a re-weighted quadratic loss, while they differ in the update rules of the weights \u03b3n and the labels yn at each iteration.\n5The offset can be similarly incorporated as in the hinge loss."}, {"heading": "Experiments", "text": "We now present empirical results on both classification and the challenging \u201cnightmare at test time\u201d scenario (Globerson and Roweis 2006) to demonstrate the effectiveness of the dropout training algorithm for SVMs, denoted by DropoutSVM, and the new IRLS algorithm for the dropout training of the logistic loss, denoted by Dropout-Logistic. We consider the unbiased dropout (or blankout) noise model6, that is, p(x\u0303 = 0) = q and p(x\u0303 = 11\u2212qx) = 1 \u2212 q, where q \u2208 [0, 1) is a pre-specified corruption level. The variance of this model for each dimension d is Vp[x\u0303d] = q1\u2212qx 2 d."}, {"heading": "Binary classification", "text": "We first evaluate Dropout-SVM and Dropout-Logistic on binary classification tasks. We use the public Amazon book review and kitchen review datasets (Blitzer, Dredze, and Pereira 2007), which consist of the text reviews about books and kitchen, respectively. In both datasets, each document is represented as a 20,000 dimensional bag-of-words feature. The binary classification task is to distinguish whether a review content is positive or negative. Following the previous settings, we choose 2,000 documents for training and approximately 4,000 for testing.\nWe compare our methods with the methods presented in (van der Maaten et al. 2013) that minimize the quadratic loss with marginalized corrupted features (MCF), denoted by MCF-Quadratic, and that minimize the expected logistic loss, denoted by MCF-Logistic. MCF-Logistic was shown to be the state-of-the-art method for dropout training on these datasets, outperforming a wide range of competitors, including the dropout training of the exponential loss and the various loss functions with a Poisson noise model. As we have discussed, both Dropout-SVM and Dropout-Logistic iteratively minimize the expectation of a re-weighted quadratic loss, with the re-weights updated in closed-form. We include MCF-Quadratic as a baseline to demonstrate the effectiveness of our methods on adaptively tuning the re-weights to get improved results. We implement both Dropout-SVM and Dropout-Logistic using C++, and solve the re-weighted least square problems using L-BFGS methods (Liu and Nocedal 1989), which are very efficient by exploring the sparsity of bag-of-words features when computing gradients7.\nFigure 1 shows classification errors, where the results of MCF-Logistic and MCF-Quadratic are cited from (van der Maaten et al. 2013). We can see that on both datasets, Dropout-SVM and Dropout-Logistic generally outperform MCF-Quadratic except when the dropout level is larger than 0.9. In the meanwhile, the proposed two models give comparable results with (a bit better than on the kitchen dataset) the state-of-art MCF-Logistic which means that dropout training on SVMs is an effective strategy for binary classifica-\n6Other noising models (e.g., Poisson) were shown to perform worse than the dropout model (van der Maaten et al. 2013). We have similar observations for Dropout-SVM and the new IRLS algorithm for logistic regression.\n7We don\u2019t compare time with MCF methods, whose implementation are in Matlab (http://homepage.tudelft.nl/19j49/mcf/ Marginalized Corrupted Features.html), slower than ours.\ntion. Finally, by noting that Dropout-SVM reduces to the standard SVM when the corruption level q is zero, we can see that dropout training can significantly boost the classification performance for the simple linear SVMs."}, {"heading": "Dropout-SVM vs. Explicit corruption", "text": "Figure 2 shows the classification errors on the Amazonbooks dataset when a SVM classifier is trained using the explicit corruption strategy as in Eq. (2). We change the number of corrupted copies (i.e., M ) from 1 to 256. Following the previous setups (van der Maaten et al. 2013), for each value of M we choose the dropout model with q selected by cross-validation. The hyper-parameter of the SVM classifier is also chosen via cross-validation on the training data. We can observe a clear trend that the error decreases when the training set contains more corrupted versions of the original training data, i.e., M gets larger in Eq. (2). It also shows that the best performance is obtained when M approaches infinity, which is equivalent to our Dropout-SVM."}, {"heading": "Multi-class classification", "text": "We also evaluate our methods on multiclass classification tasks. We choose the CIFAR-10 image categorization dataset8. The CIFAR-10 dataset is the subset of the 80 million tiny images (Torralba, Fergus, and Freeman 2008). It consists of 10 classes of 32\u00d7 32 tiny images. We follow the\n8http://www.cs.toronto.edu/\u223ckriz/cifar.html\nexperimental setup of the previous work (Krizhevsky 2009; van der Maaten et al. 2013) and represent each image as a 8,192 dimensional feature descriptor. We use the same 50,000 images for training and 10,000 for testing. There are various approaches to applying the binary Dropout-SVM and Dropout-Logistic to multiclass classification, including \u201cone-vs-all\u201d and \u201cone-vs-one\u201d strategies. Here we choose \u201cone-vs-all\u201d, which has shown effectiveness in many applications (Rifkin and Klautau 2004). The hyper-parameters are selected via cross-validation on the training set.\nTable 2 presents the results, where the results of quadratic loss and logistic loss under the MCF learning setting9 are cited from (van der Maaten et al. 2013). We also report the results using Poisson noise. We can see that all the methods (except for the quadratic loss) can significantly boost the performance by adopting dropout training; meanwhile both Dropout-SVM and Dropout-Logistic are competitive, in fact achieving comparable performance as the state-ofthe-art method (i.e., MCF-Logistic) under the dropout training setting. Finally, the Poisson corruption model is slightly worse than the dropout noise, consistent with the previous observations (van der Maaten et al. 2013)."}, {"heading": "Nightmare at test time", "text": "Finally, we evaluate our methods under the \u201cnightmare at test time\u201d (Globerson and Roweis 2006) supervised learning scenario, where some input features that were present when building the classifiers may \u201cdie\u201d or be deleted at testing time. In such a scenario, it is crucial to design algorithms that do not assign too much weight to any single feature during testing, no matter how informative it may seem at training. Previous work has conducted the worst-case analysis as well as the learning with marginalized corrupted features. We take this scenario to test the robustness of our dropout training algorithms for both SVM and logistic regression.\nWe follow the setup of (van der Maaten et al. 2013). Specifically, we choose the the MNIST dataset, which consists of 60,000 training and 10,000 testing handwritten digital images from 10 categories (i.e., 0, \u00b7 \u00b7 \u00b7 , 9). The images are represented by 28\u00d728 pixels which results in the feature dimension of 784. We train the models on the full training set, and evaluate the performance on different versions of test set in which a certain level of the features are randomly dropped out, i.e., set to zero. We compare the performance of our dropout learning algorithms with the state-of-art MCFpredictors that use the logistic loss and quadratic loss. These two models also show the state-of-art performance on the same task to the best of our knowledge. We also compare\n9The exponential loss was shown to be worse; thus omitted.\nwith FDROP (Globerson and Roweis 2006), which is a stateof-the-art algorithm for the \u201cnightmare at test time\u201d setting that minimizes the hinge loss under an adversarial worstcase analysis. During training, we choose the best models over different dropout levels via cross-validation. For both Dropout-SVM and Dropout-Logistic, we adopt the \u201cone-vsall\u201d strategy as above for the multiclass classification task.\nFigure 3 shows the classification errors of different methods as a function of the random deletion percentage of features at the testing time. Following previous settings, for each deletion percentage, we use a small validation set with the same deletion level to determine the regularization parameters and the dropout level q on the whole training data. From the results, we can see that the proposed DropoutSVM is consistently more robust than all the other competitors, including the two methods to minimize the expected logistic-loss, especially when the feature deletion percentage is high (e.g., > 50%). Comparing with the standard SVM (i.e., the method Hinge-L2) and the worst-case analysis of hinge loss (i.e., Hinge-FDROP), Dropout-SVM consistently boosts the performance when the deletion ratio is greater than 10%. As expected, Dropout-SVM also significantly outperforms the MCF method with a quadratic loss (i.e., MCF-Quadratic), which is a special case of DropoutSVM as shown in our theory. Finally, we also note that our iterative algorithm for the logistic-loss works slightly better than the previous algorithm (i.e., MCF-Logistic) when the deletion ratio is larger than 50%."}, {"heading": "Conclusions", "text": "We present dropout training for SVMs, with an iteratively re-weighted least square (IRLS) algorithm by using data augmentation techniques. Similar ideas are applied to develop a new IRLS algorithm for the dropout training of logistic regression. Our IRLS algorithms provide insights on the connection and difference among various losses in dropout learning settings. Empirical results on various tasks demonstrate the effectiveness of our approaches.\nFor future work, it is remained open whether the kernel trick can be incorporated in dropout learning. We are also interested in developing more efficient algorithms, e.g., online dropout learning, to deal with even larger datasets, and investigating whether Dropout-SVM can be incorporated into\na deep learning architecture or learning with latent structures (Zhu et al. 2014)."}, {"heading": "Acknowledgments", "text": "This work is supported by National Key Project for Basic Research of China (Grant Nos: 2013CB329403, 2012CB316301), National Natural Science Foundation of China (Nos: 61305066, 61322308, 61332007), Tsinghua Self-innovation Project (Grant Nos: 20121088071) and China Postdoctoral Science Foundation Grant (Grant Nos: 2013T60117, 2012M520281)."}], "references": [{"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Dredze Blitzer", "J. Pereira 2007] Blitzer", "M. Dredze", "F. Pereira"], "venue": "In Association of Computational Linguistics", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Improving the accuracy and speed of support vector machiens", "author": ["Burges", "C. Scholkopf 1997] Burges", "B. Scholkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Burges et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Burges et al\\.", "year": 1997}, {"title": "Generalized relational topic models with data augmentation", "author": ["Chen"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Chen,? \\Q2013\\E", "shortCiteRegEx": "Chen", "year": 2013}, {"title": "Learning to classify with missing and corrpted features", "author": ["Dekel", "O. Shamir 2008] Dekel", "O. Shamir"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Dekel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2008}, {"title": "Nightmare at test time: Robust learning by feature deletion", "author": ["Globerson", "A. Roweis 2006] Globerson", "S. Roweis"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Globerson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2006}, {"title": "Exponentiated gradient algorithms for log-linear structured prediction", "author": ["Globerson"], "venue": null, "citeRegEx": "Globerson,? \\Q2007\\E", "shortCiteRegEx": "Globerson", "year": 2007}, {"title": "The elements of statistical learning: data mining", "author": ["Tibshirani Hastie", "T. Friedman 2009] Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580v1, preprint", "author": ["Hinton"], "venue": null, "citeRegEx": "Hinton,? \\Q2012\\E", "shortCiteRegEx": "Hinton", "year": 2012}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["Liu", "D.C. Nocedal 1989] Liu", "J. Nocedal"], "venue": "Mathematical Programming", "citeRegEx": "Liu et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1989}, {"title": "Data Augmentation for Support Vector Machines. Bayesian Analysis 6(1):1\u201324", "author": ["Polson", "N.G. Scott 2011] Polson", "S.L. Scott"], "venue": null, "citeRegEx": "Polson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Polson et al\\.", "year": 2011}, {"title": "Bayesian Inference for Logistic Models using Polya-Gamma Latent Variables. arXiv:1205.0310v1", "author": ["Scott Polson", "N.G. Windle 2012] Polson", "J.G. Scott", "J. Windle"], "venue": null, "citeRegEx": "Polson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Polson et al\\.", "year": 2012}, {"title": "In defense of one-vs-all classification", "author": ["Rifkin", "R. Klautau 2004] Rifkin", "A. Klautau"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Rifkin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2004}, {"title": "Are loss functions all the same? Neural Computation 16(5):1063\u20131076", "author": ["Rosasco"], "venue": null, "citeRegEx": "Rosasco,? \\Q2004\\E", "shortCiteRegEx": "Rosasco", "year": 2004}, {"title": "Convex learning with invariances", "author": ["Teo"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Teo,? \\Q2008\\E", "shortCiteRegEx": "Teo", "year": 2008}, {"title": "A large dataset for nonparametric object and scene recognition", "author": ["Fergus Torralba", "A. Freeman 2008] Torralba", "R. Fergus", "W. Freeman"], "venue": "IEEE Transaction on Pattern Analysis and Machine Intelligence", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Learning with marginalized corrupted features", "author": ["van der Maaten"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Maaten,? \\Q2013\\E", "shortCiteRegEx": "Maaten", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Vincent,? \\Q2008\\E", "shortCiteRegEx": "Vincent", "year": 2008}, {"title": "Dropout training as adaptive regularization", "author": ["Wang Wager", "S. Liang 2013] Wager", "S. Wang", "P. Liang"], "venue": "In Advances in Neural Information Processing", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Fast dropout training", "author": ["Wang", "S. Manning 2013] Wang", "C. Manning"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Feature noising for log-linear structured prediction", "author": ["Wang"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Wang,? \\Q2013\\E", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "Gibbs max-margin topic models with data augmentation", "author": ["Zhu"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Zhu,? \\Q2014\\E", "shortCiteRegEx": "Zhu", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013).", "startOffset": 61, "endOffset": 79}, {"referenceID": 18, "context": "induced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995).", "startOffset": 46, "endOffset": 121}, {"referenceID": 18, "context": "Such an expectation scheme has been widely adopted in previous work (Wager, Wang, and Liang 2013; van der Maaten et al. 2013; Wang et al. 2013; Wang and Manning 2013).", "startOffset": 68, "endOffset": 166}], "year": 2014, "abstractText": "Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closedform solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs. Introduction Artificial feature noising augments the finite training data with an infinite number of corrupted versions, by corrupting the given training examples with a fixed noise distribution. Among the many noising schemes, dropout training (Hinton et al. 2012) is an effective way to control over-fitting by randomly omitting subsets of features at each iteration of a training procedure. By formulating the feature noising methods as minimizing the expectation of some loss functions under the corrupting distributions, recent work has provided theoretical understandings of such schemes from the perspective of adaptive regularization (Wager, Wang, and Liang 2013); and has shown promising empirical results in various applications, including document classification (van der Maaten et al. 2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013). Regarding the loss functions, though much work has been done on the quadratic loss, logistic loss, or the log-loss Copyright c \u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. induced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995). One technical challenge is that the non-smoothness of the hinge loss makes it hard to compute or even approximate its expectation under a given corrupting distribution. Existing methods are not directly applicable, therefore calling for new solutions. This paper attempts to address this challenge and fill up the gap by extending dropout training as well as other feature noising schemes to support vector machines. Previous efforts on learning SVMs with feature noising have been devoted to either explicit corruption or an adversarial worst-case analysis. For example, virtual support vector machines (Burges and Scholkopf 1997) explicitly augment the training data, which are usually support vectors from previous learning iterations for computational efficiency, with additional examples that are corrupted through some invariant transformation models. A standard SVM is then learned on the corrupted data. Though simple and effective, such an approach lacks elegance and the computational cost of processing the additional corrupted examples could be prohibitive for many applications. The other work (Globerson and Roweis 2006; Dekel and Shamir 2008; Teo et al. 2008) adopts an adversarial worst-case analysis to improve the robustness of SVMs against feature deletion in testing data. Though rigorous in theory, a worst-case scenario is unlikely to be encountered in practice. Moreover, the worst-case analysis usually results in solving a complex and computationally demanding problem. In this paper, we show that it is efficient to train linear SVM predictors on an infinite amount of corrupted copies of the training data by marginalizing out the corruption distributions, an average-case analysis. We concentrate on dropout training, but the results are directly applicable to other noising models, such as Gaussian, Poisson and Laplace (van der Maaten et al. 2013). For all these noising schemes, the resulting expected hinge loss can be upperbounded by a variational objective by introducing auxiliary variables, which follow a generalized inverse Gaussian distribution. We then develop an iteratively re-weighted least square (IRLS) algorithm to minimize the variational bounds. At each iteration, our algorithm minimizes the exar X iv :1 40 4. 41 71 v1 [ cs .L G ] 1 6 A pr 2 01 4 pectation of a re-weighted quadratic loss under the given corrupting distribution, where the re-weights are computed in a simple closed form. We further apply the similar ideas to develop a new IRLS algorithm for the dropout training of logistic regression, which extends the well-known IRLS algorithm for standard logistic regression (Hastie, Tibshirani, and Friedman 2009). Our IRLS algorithms shed light on the connection and difference between the hinge loss and logistic loss in the context of dropout training, complementing to the previous analysis (Rosasco et al. 2004; Globerson et al. 2007) in the supervised learning settings. Finally, empirical results on classification and a challenging \u201cnightmare at test time\u201d scenario (Globerson and Roweis 2006) demonstrate the effectiveness of our approaches, in comparison with various strong competitors. Preliminaries We setup the problem in question and review the learning with marginalized corrupted features. Regularized loss minimization Consider the binary classification, where each training example is a pair (x, y) with x \u2208 R being an input feature vector and y \u2208 {+1,\u22121} being a binary label. Given a set of training data D = {(xn, yn)}n=1, supervised learning aims to find a function f \u2208 F that maps each input to a label. To find the optimal candidate, it commonly solves a regularized loss minimization problem min f\u2208F \u03a9(f) + 2c \u00b7 R(D; f), (1) whereR(D; f) is the risk of applying f to the training data; \u03a9(f) is a regularization term to control over-fitting; and c is a non-negative regularization parameter. For linear models, the function f is simply parameterized as f(x;w, b) = w>x+ b, where w is the weight vector and b is an offset. We will denote \u03b8 := {w, b} for clarity. Then, the regularization can be any Euclidean norms1, e.g., the `2-norm, \u03a9(w) = \u2016w\u20162, or the `1-norm, \u03a9(w) = \u2016w\u20161. For the loss functions, the most relevant measure is the training error, \u2211N n=1 \u03b4(f(xn;\u03b8) 6= yn), which however is not easy to optimize. A convex surrogate loss is used instead, which normally upper bounds the training error. Two popular examples are the hinge loss and logistic loss2:", "creator": "LaTeX with hyperref package"}}}