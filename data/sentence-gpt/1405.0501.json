{"id": "1405.0501", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2014", "title": "Exchangeable Variable Models", "abstract": "A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. We also show that two fundamental assumptions of the EVMs in this paper are consistent with the empirical evidence that the EVMs are optimal under zero-one loss for a large class of functions, and that it is possible for some groups to be completely non-recursively distributed in some instances.\n\n\n\nThe first step in the development of EVMs is to determine the correctness of a given model and to define it by defining it by a set of known parameters. These parameters are the property of the two model models. First, a given model is an immutable record of the previous model, and the second a non-recursively distributed model is composed of the data associated with each model. This structure, as illustrated in the figure below, is a linear representation of the original model. It takes two sets of parameters for the entire data set. First, the number of values for each model is represented by a function that takes the input and returns a single value for each model. We use the value of each variable to represent the whole set in the equation.\nThe second step is a constant of the previous model, and the corresponding value for each model is the number of values that were not present at the previous model. We define the constant of the values as the number of values that were not present at the previous model.\nA constant of the input and return values for each model are not considered a constant, or a constant, for the previous model. This constant is the number of values that were not present at the previous model. For example, for a number of values that were not present at the previous model, each time each time an input and return value is considered an infinite constant. We assume a constant of the input and return values for each model that was not present at the previous model. This constant is the constant of the last value of the current model. Since we can only specify the constant of the values for each model, this constant must be constant at every time an input and return value is not included in the", "histories": [["v1", "Fri, 2 May 2014 20:13:06 GMT  (248kb,D)", "http://arxiv.org/abs/1405.0501v1", "ICML 2014"]], "COMMENTS": "ICML 2014", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["mathias niepert", "pedro m domingos"], "accepted": true, "id": "1405.0501"}, "pdf": {"name": "1405.0501.pdf", "metadata": {"source": "META", "title": "Exchangeable Variable Models", "authors": ["Mathias Niepert", "Pedro Domingos"], "emails": ["MNIEPERT@CS.WASHINGTON.EDU", "PEDROD@CS.WASHINGTON.EDU"], "sections": [{"heading": "1. Introduction", "text": "Conditional independence is a crucial notion that facilitates efficient inference and parameter learning in probabilistic models. Its logical and algorithmic properties as well as its graphical representations have led to the advent of graphical models as a discipline within artificial intelligence (Koller & Friedman, 2009). The notion of finite (partial) exchangeability (Diaconis & Freedman, 1980a), on the other hand, has not yet been explored as a basic building block for tractable probabilistic models. A sequence of random variables is exchangeable if its distribution is invariant under variable permutations. Similar to conditional independence, partial exchangeability, a generalization of exchangeability, can reduce the complexity of parameter learning and is a concept that facilitates high tree-width graphical models with tractable inference. For instance, the graphical models (a)-(c) with Bernoulli variables in Figure 1 depict typical low tree-width models based on the notion of (conditional) independence. Graphical models (d)-(f) have high tree-width but are tractable\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nif we assume the variables with identical shades to be exchangeable. We will see that EVMs are especially beneficial for high-dimensional and sparse domains such as text and collaborative filtering problems. While there exists work on tractable models, with a majority focusing on low tree-width graphical models, a framework for finite partial exchangeability as a basic building block of tractable probabilistic models seems natural but does not yet exist.\nWe propose exchangeable variable models (EVMs), a novel family of probabilistic models for classification and probability estimation. While most probabilistic models are built on the notion of conditional independence and its graphical representation, EVMs have finite partially exchangeable sequences as basic components. We show that EVMs can represent complex positive and negative correlations between large sets of variables with few parameters and without sacrificing tractable inference. The parameters of EVMs are estimated under the maximum-likelihood principle and we assume the examples to be independent and identically distributed. We develop methods for efficient probabilistic inference, maximum-likelihood estimation, and structure learning.\nWe introduce the mixtures of EVMs (MEVMs) family of models which is strictly more expressive than the naive Bayes family of models but as efficient to learn. MEVMs represent classifiers that are optimal under zero-one loss for a large class of Boolean functions including parity and threshold functions. Extensive experiments show that exchangeable variable models, when combined with the notion of conditional independence, are effective both for\nar X\niv :1\n40 5.\n05 01\nv1 [\ncs .L\nG ]\n2 M\nay 2\n01 4\nclassification and probability estimation. The MEVM classifier significantly outperforms state of the art classifiers on numerous high-dimensional and sparse data sets. MEVMs also outperform several tractable graphical model classes on typical probability estimation problems while being orders of magnitudes more efficient."}, {"heading": "2. Background", "text": "We begin by reviewing the statistical concepts of finite exchangeability and finite partial exchangeability."}, {"heading": "2.1. Finite Exchangeability", "text": "Finite exchangeability is best understood in the context of a finite sequence of binary random variables such as a finite number of coin tosses. Here, finite exchangeability means that it is only the number of heads that matters and not their particular order. Since exchangeable variables are not necessarily independent, finite exchangeability can model highly correlated variables, a graphical representation of which would be the fully connected graph with high treewidth (see Figure 1(d)). However, as we will later see, the number of parameters and the complexity of inference remains linear in the number of variables.\nDefinition 2.1 (Exchangeability). Let X1, ..., Xn be a sequence of random variables with joint distribution P and let S(n) be the group of all permutations acting on {1, ..., n}. We say that X1, ..., Xn is exchangeable if P (X1, ..., Xn) = P (X\u03c0(1), ..., X\u03c0(n)) for all \u03c0 \u2208 S(n).\nIn this paper, we are concerned with exchangeable variables and iid examples. The literature has mostly focused on exchangeability of an infinite sequence of random variables. In this case, one can express the joint distribution as a mixture of iid sequences (de Finetti, 1938). However, for finite sequences of exchangeable variables this representation is inadequate \u2013 while finite exchangeable sequences can be approximated with de Finetti style mixtures of iid sequences, these approximations are not suitable for finite sequences of random variables not extendable to an infinite exchangeable sequence (Diaconis & Freedman, 1980b). Moreover, negative correlations can only be modeled in the finite case. There are interesting connections between the automorphisms of graphical models and finite exchangeability (Niepert, 2012). An alternative approach to exchangeability considers its relationship to sufficiency (Diaconis & Freedman, 1980a; Lauritzen et al., 1984) which is at the core of our work."}, {"heading": "2.2. Finite Partial Exchangeability", "text": "The assumption that all variables of a probabilistic model are exchangeable is often too strong. Fortunately, finite exchangeability can be generalized to the concept of finite\npartial exchangeability using the notion of a statistic. Definition 2.2 (Partial Exchangeability). Let X1, ..., Xn be a sequence of random variables with distribution P , let Val(Xi) be the domain ofXi, and let T be a finite set. The sequence X1, ..., Xn is partially exchangeable with respect to the statistic T : Val(X1)\u00d7 ...\u00d7Val(Xn)\u2192 T if\nT (x) = T (x\u2032) implies P (x) = P (x\u2032),\nwhere x and x\u2032 are assignments to the sequence of random variables X1, ..., Xn.\nThe following theorem states that the joint distribution of a sequence of random variables, which is partially exchangeable with respect to a statistic T , is a unique mixture of uniform distributions. Theorem 2.3. (Diaconis & Freedman, 1980a) Let X1, ..., Xn be a sequence of random variables with distribution P , let T be a finite set, and let T : Val(X1)\u00d7 ...\u00d7 Val(Xn) \u2192 T be a statistic. Moreover, let St = {x \u2208 Val(X1) \u00d7 ... \u00d7Val(Xn) | T (x) = t}, let Ut be the uniform distribution on St, and let wt = P (St). If X1, ..., Xn is partially exchangeable with respect to T , then\nP (x) = \u2211 t\u2208T wtUt(x). (1)\nThe theorem provides an implicit description of the distributions Ut. The challenge for specific families of random variables lies in finding a statistic T with respect to which a sequence of variables is partially exchangeable and an efficient algorithm to compute the probabilities Ut(x). For the case of exchangeable sequences of discrete random variables and, in particular, exchangeable sequences of binary random variables, an explicit description does exist and is well-known in the statistics literature (Diaconis & Freedman, 1980a; Stefanescu & Turnbull, 2003). Example 2.4. Let X1, X2, X3 be three exchangeable binary variables with joint distribution P . Then, the sequence X1, X2, X3 is partially exchangeable with respect to the statistic T : {0, 1}3 \u2192 T = {0, 1, 2, 3} with T (x = (x1, x2, x3)) = x1 + x2 + x3. Thus, we can write\nP (x) = \u2211 t\u2208T wtUt(x),\nwhere wt = P (T (x) = t), Ut(x) = [[T (x) = t]] ( 3 t )\u22121 , and [[\u00b7]] is the indicator function. Hence, the distribution can be parameterized as a unique mixture of four urn processes, where T \u2019s value is the number of black balls. Figure 2 illustrates the mixture model. The generative process is as follows. First, choose one of the four urns according to the mixing weightswt; then draw three consecutive balls from the chosen urn without replacement."}, {"heading": "3. Exchangeable Variable Models", "text": "We propose exchangeable variable models (EVMs) as a novel family of tractable probabilistic models for classification and probability estimation. While probabilistic graphical models are built on the notion of (conditional) independence and its graphical representation, EVMs are built on the notion of finite (partial) exchangeability. EVMs can model both negative and positive correlations in what would be high tree-width graphical models without losing tractability of probabilistic inference.\nThe basic components of EVMs are tuples (X, T ) where X is a sequence of discrete random variables partially exchangeable with respect to the statistic T with values T ."}, {"heading": "3.1. Probabilistic Inference", "text": "We can relate finite partial exchangeability to tractable probabilistic inference (see also (Niepert & Van den Broeck, 2014)). We assume that for every joint assignment x, P (x) can be computed in time poly(|X|). Proposition 3.1. Let X be partially exchangeable with respect to the statistic T with values T , let |T | = poly(|X|), and let, for any partial assignment e, St,e := {x | T (x) = t and x \u223c e} , where x \u223c e denotes that x and e agree on the variables in their intersection (Koller & Friedman, 2009). If we can in time poly(|X|),\n(1) for every e and every t \u2208 T , decide if there exists an x \u2208 St,e and, if so, construct such an x,\nthen the complexity of MAP inference, that is, computing argmaxy P (y, e) for any partial assignment e, is poly(|X|). If, in addition, we can in time poly(|X|),\n(2) for every e and every t \u2208 T , compute |St,e|,\nthen the complexity of marginal inference, that is, computing P (e) for any partial assignment e, is poly(|X|).\nProposition 3.1 generalizes to probabilistic models where P (x) can only be computed up to a constant factor Z such as undirected graphical models. Please note that computing conditional probabilities is tractable whenever conditions (1) and (2) are satisfied. We say a statistic is tractable if either of the conditions is fulfilled.\nProposition 3.1 provides a theoretical framework for developing tractable non-local potentials. For instance, for n exchangeable Bernoulli variables, the complexity of MAP and marginal inference is polynomial in n. This follows from the statistic T satisfying conditions (1) and (2) and since |T | = n + 1. Related work on cardinality-based potentials has mostly focused on MAP inference (Gupta et al., 2007; Tarlow et al., 2010). Finite exchangeability also speaks to marginal inference via the tractability of computing Ut(e) = |St,e|\u22121. EVMs can model unary potentials using singleton sets of exchangeable variables. While not all instances of finite partial exchangeability result in tractable probabilistic models there exist several examples satisfying conditions (1) and (2) which go beyond finite exchangeability. In the supplementary material, in addition to the proofs of all theorems and propositions, we present examples of tractable statistics that are different from those associated with cardinality-based potentials (Gupta et al., 2007; Tarlow et al., 2010; 2012; Bui et al., 2012)."}, {"heading": "3.2. Parameter Learning", "text": "The parameters of finite sequences of partially exchangeable variables are the mixture weights of the parameterization given in Equation 1 of Theorem 2.3. Estimating the parameters of these basic components of EVMs is a crucial task. We derive the maximum-likelihood estimates for these mixture weight vectors. Theorem 3.2. Let X1, ..., Xn be a sequence of random variables with joint distribution P , let T be a statistic with distinct values t0, ..., tk, and let X1, ..., Xn be partially exchangeable with respect to T . The ML estimates for N examples, x(1), ...,x(N), are MLE[(w0, ..., wk)] =( c0 N , ..., ck N ) , where ci = \u2211N j=1[[T ( x(j) ) = ti]].\nHence, the statistical parameters to be estimated are identical to the statistical parameters of a multinomial distribution with |T | distinct categories."}, {"heading": "3.3. Structure Learning", "text": "Let X\u0302 be a sequence of random variables and let x\u0302(1), ..., x\u0302(N) be N iid examples drawn from the datagenerating distribution. In order to learn the structure of EVMs we need to address two problems.\nProblem 1: Find subsequences X \u2286 X\u0302 that are exchangeable with respect to a given tractable statistic T . This identifies individual EVM components (X, T ) for which tractable inference and learning is possible. We may utilize different tractable statistics for different components.\nProblem 2: Construct graphical models whose potentials are the previously learned tractable EVM components. In order to preserve tractability of the global model, we have to restrict the class of possible graphical structures.\nWe now present approaches to these two problems that learn expressive EVMs while maintaining tractability.\nLet us first address Problem 1. We focus on EVMs with finitely exchangeable components. Fortunately, there exist several necessary conditions for finite exchangeability (see Definition 2.1) of a sequence of random variables.\nProposition 3.3. The following statements are necessary conditions for exchangeability of a finite sequence of random variables X1, ..., Xn. For all i, j, i\u2032, j\u2032 \u2208 {1, ..., n} with i 6= j and i\u2032 6= j\u2032\n(1) E(Xi) = E(Xj);\n(2) Var(Xi) = Var(Xj); and\n(3) Cov(Xi, Xj) = Cov(Xi\u2032 , Xj\u2032) \u2265 \u2212Var(Xi)(n\u22121) .\nThe necessary conditions can be exploited to assess whether a sequence of variables is finitely exchangeable. In order to learn EVM components (X, T ) we assume that a sequence of variables is exchangeable unless a statistical test contradicts some or all of the necessary conditions for finite exchangeability. For instance, if a statistical test deemed the expectations E(X) and E(X \u2032) for two variables X and X \u2032 identical, we could assume X and X \u2032 to be exchangeable. If we wanted the statistical test for finite exchangeability to be more specific and less sensitive, we would also require conditions (2) and/or (3) to hold. Please note the analogy to structure learning with conditional independence tests. Instead of identifying (conditional) independencies we identify finite exchangeability among random variables. For a sequence of identically distributed variables the assumption of exchangeability is weaker than that of independence. Testing whether two discrete variables have identical mean and variance is efficient algorithmically. Of course, the application of the necessary conditions for finite exchangeability is only one possible approach to learning the components of EVMs.\nLet us now turn to Problem 2. To ensure tractability, the global graphical structure has to be restricted to tractable classes such as chains and trees. Here, we focus on mixture models where, conditioned on the values of the latent variable, X\u0302 is partitioned into exchangeable blocks (see Figure 3). Hence, for each value y of the latent variable, we perform the statistical tests of Problem 1 with estimates of the conditional expectations E(X | y). We introduce this class of EVMs in the next section and leave more complex structures to future work.\nIn the context of longitudinal studies and repeatedmeasures experiments, where an observation is made at different times and under different conditions, there exist several models taking into account the correlation between these observations and assuming identical or similar\ncovariance structure for subsets of the variables (Jennrich & Schluchter, 1986). These compound symmetry models, however, do not make the assumption of exchangeability and, therefore, do not generally facilitate tractable inference. Nevertheless, finite exchangeability can be seen as a form of parameter tying, a method that has also been applied in the context of hidden Markov models, neural networks (Rumelhart et al., 1986) and, most notably, statistical relational learning (Getoor & Taskar, 2007). Collective graphical models (Sheldon & Dietterich, 2011) (CGMs) and high-order potentials (Tarlow et al., 2010; 2012) (HOPs) are models based on non-local potentials. Proposition 3.3 can be applied for learning the structure of novel tractable instances of CGMs and HOPs."}, {"heading": "4. Exchangeable Variable Models for Classification and Probability Estimation", "text": "We are now in the position to design model families that combine the notions of (partial) exchangeability with that of (conditional) independence. Instead of specifying a structure that solely models the (conditional) independence characteristics of the probabilistic model, EVMs also specify sequences of variables that are (partially) exchangeable. The previous results provide the necessary tools to learn both the structure and parameters of partially exchangeable sequences and to perform tractable probabilistic inference.\nThe possibilities for building families of exchangeable variable models (EVMs) are vast. Here, we focus on a family of mixtures of EVMs generalizing the widely used naive Bayes model. The family of probabilistic models is therefore also related to research on extending the naive Bayes classifier (Domingos & Pazzani, 1997; Rennie et al., 2003). The motivation behind this novel class of EVMs is that it facilitates both tractable maximum-likelihood learning and tractable probabilistic inference.\nIn line with existing work on mixture models, we derive the maximum-likelihood estimates for the fully observed setting, that is, when there are no examples with missing class labels. We also discuss the expectation maximization (EM)\nalgorithm for the case where the data is partially observed, that is, when examples with missing class labels exist.\nDefinition 4.1 (Mixture of EVMs). The mixture of EVMs (MEVM) model consists of a class variable Y with k possible values, a set of binary attributes X\u0302 = {X1, ..., Xn} and, for each y \u2208 {1, ..., k}, a set Xy specifying a partition of the attributes into blocks of exchangeable sequences. The structure of the model, therefore, is defined by X = {Xi}ki=1, the set of attribute partitions, one for each class value. The model has the following parameters:\n1. A parameter p(y) for every y \u2208 {1, ..., k} specifying the prior probability of seeing class value y.\n2. A parameter q(X)(` | y) for every y \u2208 {1, ..., k}, every X \u2208 Xy , and every ` \u2208 {0, 1, ..., |X|}. The value of q(X)(` | y) is the probability of the exchangeable sequence X \u2286 X\u0302 having an assignment with ` number of 1s, conditioned on the class label being y.\nLet nX(x\u0302) be the number of 1s in the joint assignment x\u0302 projected onto the variable sequence X \u2286 X\u0302. The probability for every y, x\u0302 = (x1, ..., xn) is then defined as\nP(y, x\u0302) = p(y) \u220f\nX\u2208Xy\nq(X)(nX(x\u0302) | y) ( |X|\nnX(x\u0302)\n)\u22121 .\nHence, conditioned on the class, the attributes are partitioned into mutually independent and disjoint blocks of exchangeable sequences. Figure 3 illustrates the model family with the naive Bayes model being positioned on one end of the spectrum. Here, Xy = {{X1}, ..., {Xn}} for all y \u2208 {1, ..., k}. On the other end of the spectrum is the model that assumes full exchangeability conditioned on the class. Here, Xy = {{X1, ..., Xn}} for all y \u2208 {1, ..., k}. For binary attributes, the number of free parameters is k + kn \u2212 1 for each member of the MEVM family. The following theorem provides the maximum-likelihood estimates for these parameters.\nTheorem 4.2. The maximum-likelihood estimates for a MEVM with attributes X\u0302, structure X = {Xi}ki=1, and a sequence of examples ( y(i), x\u0302(i) ) , 1 \u2264 i \u2264 N, are\np(y) =\n\u2211N i=1[[y (i) = y]]\nN\nand, for each y and each X \u2208 Xy ,\nq(X)(` | y) = \u2211N i=1[[y (i) = y and nX ( x\u0302(i) ) = `]]\u2211N\ni=1[[y (i) = y]]\n.\nWe utilize MEVMs for classification problems by learning the parameters and computing the MAP state of the\nAlgorithm 1 Expectation Maximization for MEVMs Input: The number of classes k. Training examples \u3008x\u0302(i) = (x(i)1 , ..., x (i) n )\u3009, 1 \u2264 i \u2264 N . A parameter speci-\nfying a stopping criterion. Initialization: Assign bN/kc random examples to each mixture component. For each class value y \u2208 {1, ..., k}, partition the n variables into exchangeable sequences X (0)y , and compute p(0)(y) and q(0)(X)(` | y) for each X \u2208 X (0)y and 0 \u2264 ` \u2264 |X| using Theorem 4.2. Iterate: until stopping criterion is met 1. For i = 1, ..., N and y = 1, ..., k compute\n\u03b4(y | i) = P(t\u22121)\n( y, x\u0302(i) )\u2211k j=1 P\n(t\u22121)(j, x\u0302(i)) . 2. For each y \u2208 {1, ..., k}, partition the variables\ninto blocks of exchangeable sequences X (t)y . 3. Update parameters for both X (t\u22121)y and X (t)y :\np(t)(y) = \u2211N i=1 \u03b4(y | i) N ,\nq (t) (X)(` | y) =\n\u2211N i=1[[nX ( x\u0302(i) ) = `]] \u03b4(y | i)\u2211N\ni=1 \u03b4(y | i) .\n4. Select the new block structure according to the maximum log-likelihood on training examples.\nOutput: Structure and parameter estimates.\nclass variable conditioned on assignments to the attribute variables. For probability estimation the class is latent and we can apply Algorithm 1. The expectation maximization (EM) algorithm is initialized by assigning random examples to the mixture components. In each EM iteration, the examples are fractionally assigned to the components, and the block structure and parameters are updated. Finally, either the previous or current structure is chosen based on the maximum likelihood. For the structure learning step we can, for instance, apply conditions from Proposition 3.3 where we use the conditional expectations E(Xj | y), estimated by \u2211N i=1 x (i) j \u03b4(y | i)/N , for the statistical tests to construct Xy . Since the new structure is chosen from a set containing the structure from the previous EM iteration, the convergence of Algorithm 1 follows from that of structural expectation maximization (Friedman, 1998).\nA crucial question is how expressive the novel model family is. We provide an analytic answer by showing that MEVMs are globally optimal under zero-one loss for a large class of Boolean functions, namely, conjunctions and disjunctions of attributes and symmetric Boolean functions. Symmetric Boolean functions are Boolean function whose value depends only on the number of ones in the\ninput (Canteaut & Videau, 2005). The class includes (a) Threshold functions, whose value is 1 on inputs vectors with k or more ones for a fixed k; (b) Exact-value functions, whose value is 1 on inputs vectors with k ones for a fixed k; (c) Counting functions, whose value is 1 on inputs vectors with the number of ones congruent to k mod m for fixed k,m; and (d) Parity functions, whose value is 1 if the input vector has an odd number of ones. Definition 4.3. (Domingos & Pazzani, 1997) The Bayes rate for an example is the lowest zero-one loss achievable by any classifier on that example. A classifier is locally optimal for an example iff its zero-one loss on that example is equal to the Bayes rate. A classifier is globally optimal for a sample iff it is locally optimal for every example in that sample. A classifier is globally optimal for a problem iff it is globally optimal for all possible samples of that problem.\nWe can now state the following theorem. Theorem 4.4. The mixtures of EVMs family is globally optimal under zero-one loss for\n1. Conjunctions and disjunctions of attributes;\n2. Symmetric Boolean functions such as\n\u2022 Threshold (m-of-n) functions \u2022 Parity functions \u2022 Counting functions \u2022 Exact value functions\nTheorem 4.4 is striking as the parity function and its special case, the XOR function, are instances of not linearly separable functions which are often used as examples of particularly challenging classification problems. The optimality for symmetric Boolean functions holds even for the model that assumes full exchangeability of the attributes given the value of the class variable (see Figure 3, right). It is known that the naive Bayes classifier is not globally optimal for threshold (m-of-n) functions despite them being linearly separable (Domingos & Pazzani, 1997). Hence, combining conditional independence and exchangeability leads to highly tractable probabilistic models that are globally optimal for a broader class of Boolean functions."}, {"heading": "5. Experiments", "text": "We conducted extensive experiments to assess the efficiency and effectiveness of MEVMs as tractable probabilistic models for classification and probability estimation. A major objective is the comparison of MEVMs and naive Bayes models. We also compare MEVMs with several state of the art classification algorithms. For the probability estimation experiments, we compare MEVMs to latent naive Bayes models and several widely used tractable graphical model classes such as latent tree models."}, {"heading": "5.1. Classification", "text": "We evaluated the MEVM classifier using both synthetic and real-world data sets. Each synthetic data set consists of 106 training and 10000 test examples. Let n(x) be the number of ones of the example x. The parity data was generated by sampling uniformly at random an example x from the set {0, 1}1000 and assigning it to the first class if n(x) mod 2 = 1, and to the second class otherwise. For the 10-of-1000 data set we assigned an example x to the first class if n(x) \u2265 10, and to the second class otherwise. For the counting data set we assigned an examples x to the first class if n(x) mod 5 = 3, and to the second class otherwise. For the exact data set we assigned an example x to the first class if n(x) \u2208 {0, 200, 400, 600, 800, 1000}, and to the second class otherwise.\nWe used the SCIKIT 0.141 functions to load the 20Newsgroup train and test samples. We removed headers, footers, and quotes from the training and test documents. This renders the classification problem more difficult and leads to significantly higher zero-one loss for all classifiers. For the Reuters-8 data set we considered only the Reuters21578 documents with a single topic and the top 8 classes that have at least one train and one test example. For the WebKB text data set we considered the classes project, course, faculty, and student. For all text data sets we used the binary bag-of-word representation resulting in feature spaces with up to 45000 dimensions. For the MNIST data set, a collection of hand-written digits, we set a feature value to 1 if the original feature value was greater than 50, and to 0 otherwise. The polarity data set is a well-known sentiment analysis problem based on movie reviews (Pang & Lee, 2004). The problem is to classify movie reviews as either positive or negative. We used the cross-validation splits provided by the authors. The Enron spam data set is a collection of e-mails from the Enron corpus that was divided into spam and no-spam messages (Metsis et al.,\n1http://scikit-learn.org/\n2006). Here, we applied randomized 100-fold cross validation. We did not apply feature extraction algorithms to any of the data sets. Table 1 lists the properties of the data sets and the mean and standard deviation of the number of blocks of the MEVMs. We distinguished between two-class and multi-class (more than 2 classes) problems. When the original data set had more than two classes, we created the two-class problems by considering every pair of classes as a separate cross-validation problem. We draw this distinction because we want to compare classification approaches independent of particular multi-class strategies (1-vs-n, 1-vs-1, etc.).\nWe exploited necessary condition (1) from Proposition 3.3 to learn the block structure of the MEVM classifiers. For each pair of variables X,X \u2032 and each class value y, we applied Welch\u2019s t-test to test the null hypothesis E(X | y) = E(X \u2032 | y). If, for two variables, the test\u2019s pvalue was less than 0.1, we rejected the null hypothesis and placed them in different blocks conditioned on y. We applied Laplace smoothing with a constant of 0.1. The same parameter values were applied across all data sets and experiments. For all other classifiers we used the SCIKIT 0.14 implementations naive bayes.BernoulliNB, tree.DecisionTreeClassifier, svm.LinearSVC, and neighbors.KNeighborsClassifier. We used the classifiers\u2019 standard settings except for the naive Bayes classifier where we applied a Laplace smoothing constant (alpha) of 0.1 to ensure a fair comparison (NB results deteriorated for alpha values of 1.0 and 0.01). The standard setting for the classifiers are available as part of the SCIKIT 0.14 documentation. All implementations and data sets will be published.\nTable 2 lists the results for the two-class problems. The MEVM classifier was one of the best classifiers for 8 out of the 10 data sets. With the exception of the MNIST data set, where the difference was insignificant, MEVM significantly outperformed the naive Bayes classifier (NB) on all data sets. The MEVM classifier outperformed SVMs on 4 data sets, two of which are real-world text classification problems and achieved a tie on 4. For the parity data set only the MEVM classifier was better than random. Table 3 shows the results on the multi-class problems. Here, the MEVM classifier significantly outperforms naive Bayes on all data set. The MEVM classifier outperformed all classifiers on the 20Newsgroup and was a close second on the Reuters-8 and WebKB data sets. The MEVM classifier is particularly suitable for high-dimensional and sparse data sets. We hypothesize that this has three reasons. First, MEVMs can model both negative and positive correlations between variables. Second, MEVMs perform a non-linear transformation of the feature space. Third, MEVMs cluster noisy variables into blocks of exchangeable sequences which acts as a form of regularization in sparse domains."}, {"heading": "5.2. Probability Estimation", "text": "We conducted experiments with a widely used collection of data sets (Van Haaren & Davis, 2012; Gens & Domingos, 2013; Lowd & Rooshenas, 2013). Table 4 lists the number of variables, training and test examples, and the number of blocks of the MEVM models. We set the latent variable\u2019s domain size to 20 for each problem and applied the same EM initialization for MEVMs and NB models. This way we could compare NB and MEVM independent of the tuning parameters specific to EM. We implemented EM exactly as described in Algorithm 1. For step (2), we exploited Proposition 3.3 (1) and, for each y, partitioned the variables into exchangeable blocks by performing a series of Welch\u2019s t-tests on the expectations E(Xj | y), estimated by \u2211N i=1 x (i) j \u03b4(y | i)/N , assigning two variables to different blocks if the null hypothesis of identical means could be rejected at a significance level of 0.1. For MEVM and NB we again used a Laplace smoothing constant of 0.1. We ran EM until the average log-likelihood increase between iterations was less than 0.001. We restarted EM 10 times and chose the model with the maximal log-likelihood on the training examples. We did not use the validation data. For LTM (Choi et al., 2011), we applied the four methods, CLRG, CLNJ, regCLRG, and regCLNJ, and chose the model with the highest validation log-likelihood.\nTable 5 lists the average log-likelihood of the test data for the MEVM, the latent naive Bayes (Lowd & Domingos, 2005) (NB), the latent tree (LTM), and the Chow-Liu tree model (Chow & Liu, 2006) (CL). Even without exploiting the validation data for model tuning, the MEVM models outperformed the CL models on all, and the LTMs on\nall but two of the data set. MEVMs achieve the highest log-likelihood score on 7 of the 12 data sets. With the exception of the Jester data set, MEVMs either outperformed or tied the NB model. While the results indicate that MEVMs are effective for higher-dimensional and sparse data sets, where the increase in log-likelihood was most significant, MEVMs also outperformed the NB models on 3 data sets with less than 100 variables. The MEVM and NB models have exactly the same number of free parameters. Since results on the same data sets are available for other tractable model classes we also compared MEVMs with SPNs (Gens & Domingos, 2013) and ACMNs (Lowd & Rooshenas, 2013). Here, MEVMs are outperformed by the more complex SPNs on 5 and by ACMNs on 6 data sets. However, MEVMs are competitive and outperform SPNs on 7 and ACMNs on 6 of the 12 data sets. Following previous work (Van Haaren & Davis, 2012), we applied the Wilcoxon signed-rank test. MEVM outperforms the other models at a significance level of 0.0124 (NB), 0.0188 (LTM), and 0.0022 (CL). The difference is insignificant compared to ACMNs (0.6384) and SPNs (0.7566).\nTo compute the probability of one example, MEVMs require as many steps as there are blocks of exchangeable variables. Hence, EM for MEVM is significantly more efficient than EM for NB, both for a single EM iteration and to reach the stopping criterion. While the difference was less significant for problems with fewer than 100 variables, the EM algorithm for MEVM was up to two orders of magnitude faster for data sets with 100 or more variables."}, {"heading": "6. Discussion", "text": "Exchangeable variable models (EVMs) provide a framework for probabilistic models combining the notions of conditional independence and partial exchangeability. As a result, it is possible to efficiently learn the parameters and structure of tractable high tree-width models. EVMs can model complex positive and negative correlations be-\ntween large numbers of variables. We presented the theory of EVMs and showed that a particular subfamily is optimal for several important classes of Boolean functions. Experiments with a large number of data sets verified that mixtures of EVMs are powerful and highly efficient models for classification and probability estimation.\nEVMs are potential components in deep architectures such as sum-product networks (Gens & Domingos, 2013). In light of Theorem 4.4, exchangeable variable nodes, complementing sum and product nodes, can lead to more compact representations with fewer parameters to learn. EVMs are also related to graphical modeling with perfect graphs (Jebara, 2013). In addition, EVMs provide an insightful connection to lifted probabilistic inference (Kersting, 2012), an active research area concerned with exploiting symmetries for more efficient probabilistic inference. We have developed a principled framework based on partial exchangeability as an important notion of structural symmetry. There are numerous opportunities for crossfertilization between EVMs, perfect graphical models, collective graphical models, and statistical relational models.\nDirections for future work include more sophisticated structure learning, EVMs with continuous variables, EVMs based on instances of partial exchangeability other than finite exchangeability, novel statistical relational formalisms incorporating EVMs, applications of EVMs, and a general theory of graphical models with exchangeable potentials."}, {"heading": "Acknowledgments", "text": "Many thanks to Guy Van den Broeck, Hung Bui, and Daniel Lowd for helpful discussions. This research was partly funded by ARO grant W911NF-08-1-0242, ONR grants N00014-13-1-0720 and N00014-12-1-0312, and AFRL contract FA8750-13-2-0019. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, ONR, AFRL, or the United States Government."}, {"heading": "A. Proof of Proposition 3.1", "text": "Let X be partially exchangeable with respect to the statistic T with values T , let |T | = poly(|X|), and let, for any partial assignment e, St,e := {x | T (x) = t and x \u223c e} , where x \u223c e denotes that x and e agree on the variables in their intersection (Koller & Friedman, 2009). If we can in time poly(|X|),\n(1) for every e and every t \u2208 T , decide if there exists an x \u2208 St,e and, if so, construct such an x,\nthen the complexity of MAP inference, that is, computing argmaxy P (y, e) for any partial assignment e, is poly(|X|). If, in addition, we can in time poly(|X|),\n(2) for every e and every t \u2208 T , compute |St,e|,\nthen the complexity of marginal inference, that is, computing P (e) for any partial assignment e, is poly(|X|).\nProof. We first prove statement (1). Let e be a given partial assignment and assume we want to compute argmaxy P (y, e). We construct an xt \u2208 St,e for each t \u2208 T and set x\u0302t := argmaxxt P (xt). By assumption, this is possible in time poly(|X|). Since we have that x\u0302t = y\u0302e with y\u0302 := argmaxy P (y, e) we can extract the solution in linear time.\nTo prove statement (2), let e be a partial assignment. We construct a xt \u2208 St,e for each t \u2208 T for which such an xt exists, compute |St,e|, and return \u2211 t\u2208T P (xt)|St,e|. By assumption, this is possible in time poly(|X|).\nWe can utilize Proposition 3.1 to prove that probabilistic inference for a sequence of n exchangeable binary variables is tractable.\nExample A.1 (Finite Exchangeability). Let X be an exchangeable sequence of binary random variables. Let n(e) be the number of 1s in a partial assignment e to the variables X. Clearly, we have that X is exchangeable with respect to the statistic T (x) = n(x) with values T = {0, ..., n}.\nFirst, we prove that for every partial assignment e to k of the n variables and every t \u2208 T , we can decide if there exists an x \u2208 St,e and, if so, construct such an x in time poly(|X|). If n(e) > t or n\u2212k+n(e) < t, then there does not exist such an x. Otherwise it is possible to generate a x with n(x) = t in linear time by assigning exactly t \u2212 n(e) ones to the unassigned variables and we have that x \u2208 St,e. Hence, MAP inference is tractable.\nNext, we prove that for every partial assignment e to k variables and every t \u2208 T , we can compute |St,e| in time\npoly(|X|). But this is possible since |St,e| = ( n\u2212k t\u2212n(e) ) . Hence, marginal inference is tractable.\nPlease note that Example A.1 implies tractability results for numerous important special cases of finite exchangeability such as parity and threshold functions.\nThere are forms of finite partial exchangeability (Diaconis & Freedman, 1980a) that go beyond the notion of full finite exchengeability and, therefore, cardinality-based potentials (Gupta et al., 2007; Tarlow et al., 2010) of Example A.1. We provide three examples.\nExample A.2 (Block Exchangeability). Let w be a fixed constant. For a sequence of binary random variables X let X = {X1, ...,Xw} be a partition of the variables X into w subsequences, and let nY(x) be the number of 1s in an assignment x projected onto the variables Y \u2286 X. Now, let T (x) = (nX1(x), ..., nXw(x)).\nIt is straight-forward to verify that |T | = poly(|X|). Moreover, with arguments similar to those made in Example A.1 one can show that conditions (1) and (2) of Proposition 3.1 are met. Hence, MAP and marginal inference are tractable for the statistic T .\nExample A.3. Let X be a sequence of n binary random variables and let \u03c40\u21921(x) be the number of times 01 occurs as a substring2 in x. Now, consider the statistic\nT (x) = \u03c40\u21921(x).\nFor example, for x = 11011111 we have T (x) = 1 and for x = 01010101 we have T (x) = 4. We also have that |T | = bn/2c+ 1 = poly(|X|).\nNow, let e be a partial assignment to k of the n variables and let 0 \u2264 t \u2264 bn/2c be a value of the statistic. Let b = {0, 1, \u2217}n be a string where the characters 0 and 1 encode the assignments to variables according to e and the character * encodes unassigned variables. We now partition b into four sets Gij , i, j \u2208 {0, 1}, of substrings defined as Gij := {s v b | s1 = i, s|s| = j, s` = * for 1 \u2264 i < ` < j \u2264 |s|}, where v denotes the substring relation. We can now complete the partial assignment e to a joint assignment x with T (x) = t if and only if (1) \u03c40\u21921(b)+|G01| \u2264 t and (2) \u03c40\u21921(b)+ \u2211 s\u2208G00 \u2308 |s|\u22122 2 \u2309 +\u2211\ns\u2208G01 \u230a |s| 2 \u230b + \u2211 s\u2208G10 \u230a |s|\u22122 2 \u230b + \u2211 s\u2208G11 \u2308 |s|\u22122 2 \u2309 \u2265 t. When these two conditions are met, the full assignment x can be constructed by completing the substring in the groups Gij so as to make T (x) = t and this is possible in linear time. Hence, MAP inference is tractable.\nIt is possible to construct novel tractable statistics by nesting statistics that are known to be tractable.\n2As opposed to subsequences, substrings are consecutive parts of a string.\nExample A.4 (Nested Tractable Statistics). Let X be an n \u00d7 n array of binary random variables. For instance, X could represent a binarized image with n rows and n columns. Let k be a fixed integer constant and let ` be the integer such that n = k`. We assume without loss of generality that such an integer exists. We partition the original array into `2 squares of dimension k \u00d7 k. For 1 \u2264 i \u2264 `2, let Si be the variables of square i. Now, let T1 : {0, 1}k 2 \u2192 {0, 1} be the statistic defined as\nT1(s = (s1, ..., sk2)) = [[ k2\u2211 i=1 si > \u03c4 ]],\nfor some \u03c4 with 0 \u2264 \u03c4 < k2. That is, T1(s) = 1, if the number of 1s in a given square exceeds a threshold of \u03c4 and T1(s) = 0 otherwise. Please note that for \u03c4 = 0 this corresponds to max-pooling. Now, let T : {0, 1}n2 \u2192 {0, ..., `2} be the statistic defined as follows:\nT (x) = `2\u2211 i=1 T1(si).\nBased on the tractability of the two statistics, it is straightforward to verify that both MAP and marginal inference is tractable for the statistic T .\nPlease note that the presented theoretical framework facilitates the discovery and development of novel tractable nonlocal potentials."}, {"heading": "B. Proof of Theorem 3.2", "text": "Let X1, ..., Xn be a sequence of random variables with joint distribution P , let T be a statistic with distinct values t0, ..., tk, and let X1, ..., Xn be partially exchangeable with respect to T . The ML estimates for N examples, x(1), ...,x(N), are MLE[(w0, ..., wk)] = ( c0 N , ..., ck N ) , where\nci = \u2211N j=1[[T ( x(j) ) = ti]].\nProof. Let \u03b8 = (w0, ..., wk). By Theorem 2.3, the loglikelihood for N examples x(1), ...,x(N) is\nL(\u03b8) = N\u2211 j=1 log ( k\u2211 i=0 wiUi ( x(j) )) .\nLet ci = \u2211N j=1[[T ( x(j) ) = ti]] and let x\u0302i be a joint assignment with T (x\u0302i) = ti. Then, L(\u03b8) = \u2211k i=0 ci log(wiUi(x\u0302i)) = \u2211k i=0 ci[log(wi) +\nlog(Ui(x\u0302i))] = \u2211k i=0 ci log(wi) + \u2211k i=0 ci log(Ui(x\u0302i)). The second term is free of parameters and, hence, finding the ML estimates amounts to maximizing the first sum. This is equivalent to finding the maximum likelihood estimate of a multinomial which can be solved with Lagrange multipliers. Hence, MLE(wi) = ciN , for 0 \u2264 i \u2264 k."}, {"heading": "C. Proof of Proposition 3.3", "text": "The following statements are necessary conditions for exchangeability of a finite sequence of random variables X1, ..., Xn. For all i, j, i\u2032, j\u2032 \u2208 {1, ..., n} with i 6= j and i\u2032 6= j\u2032\n(1) E(Xi) = E(Xj);\n(2) Var(Xi) = Var(Xj); and\n(3) Cov(Xi, Xj) = Cov(Xi\u2032 , Xj\u2032) \u2265 \u2212Var(Xi)(n\u22121) .\nThese conditions are well-known and are straight-forward to prove. Nevertheless, for the sake of completeness, we prove statement (3).\nProof. It is straight-forward to prove statements (1) and (2). In order to prove statement (3) we use statements (2) to write\n0 \u2264 Var(X1 + \u00b7 \u00b7 \u00b7+Xn) = Var(X1) + \u00b7 \u00b7 \u00b7+Var(Xn) + 2 \u2211 i<j Cov(Xi, Xj)\n= nVar(Xi) + n(n\u2212 1)Cov(Xi, Xj).\nHence, Cov(Xi, Xj) \u2265 \u2212Var(Xi)(n\u22121) ."}, {"heading": "D. Proof of Theorem 4.4", "text": "The mixtures of EVMs family is globally optimal under zero-one loss for\n1. Conjunctions and disjunctions of attributes;\n2. Symmetric Boolean functions such as\n\u2022 Threshold (m-of-n) functions \u2022 Parity functions \u2022 Counting functions \u2022 Exact value functions\nProof. Let X be the sequence of variables under consideration. We write y(x) for the (hidden) class value of example x. For conjunctions of attributes, let X\u0302 \u2286 X be the sequence of variables that are part of the conjunction. Conditioned on the binary class variable being either 0 or 1, we partition the variables into the two blocks X\u0302 and X \u2212 X\u0302. We set the parameters of the MEVM as follows.\nq(X\u0302)(` | 1) = 1.0 if ` = |X\u0302| and q(X\u0302)(` | 1) = 0.0 otherwise;\nq(X\u0302)(` | 0) = 0.0 if ` = |X\u0302| and q(X\u0302)(` | 0) = (|X\u0302|` ) 2|X\u0302| otherwise;\nq(X\u2212X\u0302)(` | 1) = (|X|\u2212|X\u0302|` ) 2|X|\u2212|X\u0302| ; q(X\u2212X\u0302)(` | 0) = (|X|\u2212|X\u0302|` ) 2|X|\u2212|X\u0302| ;\np(1) = 2 |X|\u2212|X\u0302| 2|X| ; and p(0) = (2 |X\u0302|\u22121)(2|X|\u2212|X\u0302|) 2|X| .\nThen, we have that P(1 | x) > 0 if y(x) = 1 and P(1 | x) = 0 otherwise. Moreover, P(0 | x) = 0 if y(x) = 1 and P(0 | x) > 0 otherwise. Hence, the MEVM classifier always returns the correct class value. A similar argument can be made to prove the optimality for disjunctions of attributes.\nTo prove the second statement, we consider an MEVM model with a binary class variable and the following block structure. For each of the class variable\u2019s values y, y \u2208 {0, 1}, we have that Xy = {X1, ..., Xn}. That is, conditioned on each class value, the attributes are assumed to be exchangeable (see Figure 3; right). It is straightforward to verify that this particular MEVM can learn arbitrary discrete distributions over any symmetric Boolean function."}], "references": [{"title": "Exact lifted inference with distinct soft evidence on every object", "author": ["Bui", "Hung B", "Huynh", "Tuyen N", "de Salvo Braz", "Rodrigo"], "venue": "In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Bui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2012}, {"title": "Symmetric boolean functions", "author": ["Canteaut", "Anne", "Videau", "Marion"], "venue": "Information Theory,", "citeRegEx": "Canteaut et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Canteaut et al\\.", "year": 2005}, {"title": "Learning latent tree graphical models", "author": ["Choi", "Myung Jin", "Tan", "Vincent Y. F", "Anandkumar", "Animashree", "Willsky", "Alan S"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Chow and Liu,? \\Q2006\\E", "shortCiteRegEx": "Chow and Liu", "year": 2006}, {"title": "Sur la condition d\u2019\u00e9quivalence partielle", "author": ["de Finetti", "Bruno"], "venue": "In Colloque consacre\u0301 a la theorie des probabilite\u0301s,", "citeRegEx": "Finetti and Bruno.,? \\Q1938\\E", "shortCiteRegEx": "Finetti and Bruno.", "year": 1938}, {"title": "De Finetti\u2019s generalizations of exchangeability", "author": ["Diaconis", "Persi", "Freedman", "David"], "venue": "In Studies in Inductive Logic and Probability,", "citeRegEx": "Diaconis et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Diaconis et al\\.", "year": 1980}, {"title": "Finite exchangeable sequences", "author": ["Diaconis", "Persi", "Freedman", "David"], "venue": "The Annals of Probability,", "citeRegEx": "Diaconis et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Diaconis et al\\.", "year": 1980}, {"title": "On the optimality of the simple bayesian classifier under zero-one loss", "author": ["Domingos", "Pedro", "Pazzani", "Michael J"], "venue": "Machine Learning,", "citeRegEx": "Domingos et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Domingos et al\\.", "year": 1997}, {"title": "The bayesian structural em algorithm", "author": ["Friedman", "Nir"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Friedman and Nir.,? \\Q1998\\E", "shortCiteRegEx": "Friedman and Nir.", "year": 1998}, {"title": "Learning the structure of sum-product networks", "author": ["Gens", "Robert", "Domingos", "Pedro"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Gens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gens et al\\.", "year": 2013}, {"title": "Introduction to Statistical Relational Learning", "author": ["Getoor", "Lise", "Taskar", "Ben"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "Efficient inference with cardinality-based clique potentials", "author": ["Gupta", "Rahul", "Diwan", "Ajit A", "Sarawagi", "Sunita"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Gupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2007}, {"title": "Perfect graphs and graphical modeling. In Tractability: Practical Approaches to Hard Problems", "author": ["Jebara", "Tony"], "venue": null, "citeRegEx": "Jebara and Tony.,? \\Q2013\\E", "shortCiteRegEx": "Jebara and Tony.", "year": 2013}, {"title": "Unbalanced repeatedmeasures models with structured covariance matrices", "author": ["Jennrich", "Robert I", "Schluchter", "Mark D"], "venue": null, "citeRegEx": "Jennrich et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Jennrich et al\\.", "year": 1986}, {"title": "Lifted probabilistic inference", "author": ["Kersting", "Kristian"], "venue": "In Proceedings of 20th European Conference on Artificial Intelligence (ECAI), pp", "citeRegEx": "Kersting and Kristian.,? \\Q2012\\E", "shortCiteRegEx": "Kersting and Kristian.", "year": 2012}, {"title": "Probabilistic Graphical Models", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Extreme point models in statistics", "author": ["Lauritzen", "Steffen L", "Barndorff-Nielsen", "Ole E", "A.P. Dawid", "Diaconis", "Persi", "Johansen", "S\u00f8ren"], "venue": "Scandinavian Journal of Statistics,", "citeRegEx": "Lauritzen et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Lauritzen et al\\.", "year": 1984}, {"title": "Naive bayes models for probability estimation", "author": ["Lowd", "Daniel", "Domingos", "Pedro"], "venue": "In Proceedings of the 22nd International Conference on Machine learning (ICML),", "citeRegEx": "Lowd et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lowd et al\\.", "year": 2005}, {"title": "Learning markov networks with arithmetic circuits", "author": ["Lowd", "Daniel", "Rooshenas", "Amirmohammad"], "venue": "In Proceedings of the 16th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Lowd et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lowd et al\\.", "year": 2013}, {"title": "Spam filtering with naive bayes - which naive bayes", "author": ["Metsis", "Vangelis", "Androutsopoulos", "Ion", "Paliouras", "Georgios"], "venue": "In Conference on Email and Anti-Spam (CEAS),", "citeRegEx": "Metsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metsis et al\\.", "year": 2006}, {"title": "Markov chains on orbits of permutation groups", "author": ["Niepert", "Mathias"], "venue": "In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Niepert and Mathias.,? \\Q2012\\E", "shortCiteRegEx": "Niepert and Mathias.", "year": 2012}, {"title": "Tractability through exchangeability: A new perspective on efficient probabilistic inference", "author": ["Niepert", "Mathias", "Van den Broeck", "Guy"], "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Niepert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Niepert et al\\.", "year": 2014}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Tackling the poor assumptions of naive bayes text classifiers", "author": ["Rennie", "Jason", "Shih", "Lawrence", "Teevan", "Jaime", "Karger", "David"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Rennie et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rennie et al\\.", "year": 2003}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Collective graphical models", "author": ["Sheldon", "Daniel", "Dietterich", "Thomas"], "venue": "In Proceedings of the 25th Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Sheldon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sheldon et al\\.", "year": 2011}, {"title": "Likelihood inference for exchangeable binary data with varying cluster", "author": ["Stefanescu", "Catalina", "Turnbull", "Bruce W"], "venue": "sizes. Biometrics,", "citeRegEx": "Stefanescu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Stefanescu et al\\.", "year": 2003}, {"title": "Hopmap: Efficient message passing with high order potentials", "author": ["Tarlow", "Daniel", "Givoni", "Inmar E", "Zemel", "Richard S"], "venue": "In Proceedings of 13th Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Tarlow et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2010}, {"title": "Fast exact inference for recursive cardinality models", "author": ["Tarlow", "Daniel", "Swersky", "Kevin", "Zemel", "Richard S", "Adams", "Ryan P", "Frey", "Brendan J"], "venue": "In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Tarlow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2012}, {"title": "Markov network structure learning: A randomized feature generation approach", "author": ["Van Haaren", "Jan", "Davis", "Jesse"], "venue": "In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Haaren et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haaren et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "An alternative approach to exchangeability considers its relationship to sufficiency (Diaconis & Freedman, 1980a; Lauritzen et al., 1984) which is at the core of our work.", "startOffset": 85, "endOffset": 137}, {"referenceID": 11, "context": "Related work on cardinality-based potentials has mostly focused on MAP inference (Gupta et al., 2007; Tarlow et al., 2010).", "startOffset": 81, "endOffset": 122}, {"referenceID": 27, "context": "Related work on cardinality-based potentials has mostly focused on MAP inference (Gupta et al., 2007; Tarlow et al., 2010).", "startOffset": 81, "endOffset": 122}, {"referenceID": 11, "context": "In the supplementary material, in addition to the proofs of all theorems and propositions, we present examples of tractable statistics that are different from those associated with cardinality-based potentials (Gupta et al., 2007; Tarlow et al., 2010; 2012; Bui et al., 2012).", "startOffset": 210, "endOffset": 275}, {"referenceID": 27, "context": "In the supplementary material, in addition to the proofs of all theorems and propositions, we present examples of tractable statistics that are different from those associated with cardinality-based potentials (Gupta et al., 2007; Tarlow et al., 2010; 2012; Bui et al., 2012).", "startOffset": 210, "endOffset": 275}, {"referenceID": 0, "context": "In the supplementary material, in addition to the proofs of all theorems and propositions, we present examples of tractable statistics that are different from those associated with cardinality-based potentials (Gupta et al., 2007; Tarlow et al., 2010; 2012; Bui et al., 2012).", "startOffset": 210, "endOffset": 275}, {"referenceID": 24, "context": "Nevertheless, finite exchangeability can be seen as a form of parameter tying, a method that has also been applied in the context of hidden Markov models, neural networks (Rumelhart et al., 1986) and, most notably, statistical relational learning (Getoor & Taskar, 2007).", "startOffset": 171, "endOffset": 195}, {"referenceID": 27, "context": "Collective graphical models (Sheldon & Dietterich, 2011) (CGMs) and high-order potentials (Tarlow et al., 2010; 2012) (HOPs) are models based on non-local potentials.", "startOffset": 90, "endOffset": 117}, {"referenceID": 23, "context": "The family of probabilistic models is therefore also related to research on extending the naive Bayes classifier (Domingos & Pazzani, 1997; Rennie et al., 2003).", "startOffset": 113, "endOffset": 160}, {"referenceID": 2, "context": "For LTM (Choi et al., 2011), we applied the four methods, CLRG, CLNJ, regCLRG, and regCLNJ, and chose the model with the highest validation log-likelihood.", "startOffset": 8, "endOffset": 27}], "year": 2014, "abstractText": "A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zeroone loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and probabilistic models which are solely based on independence assumptions.", "creator": "LaTeX with hyperref package"}}}