{"id": "1405.5147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2014", "title": "Predicting Online Video Engagement Using Clickstreams", "abstract": "In the nascent days of e-content delivery, having a superior product was enough to give companies an edge against the competition. With today's fiercely competitive market, one needs to be multiple steps ahead, especially when it comes to understanding consumers. Focusing on a large set of web portals owned and managed by a private communications company, we propose methods by which these sites' clickstream data can be used to provide a deep understanding of their visitors, as well as their interests and preferences. We further expand the use of this data to show that it can be effectively used to predict user engagement to video streams. We also propose solutions to a specific set of types of content providers, including websites that utilize such data, where we could offer direct support for an app that serves a specific purpose for each particular audience.\n\n\n\nOur plan also calls for both public-domain, public-domain, and state-of-the-art services. The idea of having such services is to leverage this information to better determine how well-known a site is to its users and users in particular. To further create an open access option, we would propose using the existing Open Source Control system, which currently allows you to create a new public-domain service, with the permission of the users. We propose these services as part of a framework called the Open Source Control System, which allows you to create a new open source service, with the permission of the users. This system will allow you to create a new open source service without having to wait for approval from the user. We propose this as part of a model of which public-domain is to be distributed using an alternative set of open source services and thus the user has an alternative choice to use the Open Source Control system.\nAs a result, we propose having different approaches to how websites can communicate with their users to improve their experience. We are aiming to include this model as part of the Open Source Control System, which we believe will address a specific and important issue for the web. In order to gain more knowledge about how the website can communicate with its users, we're looking for ways to improve the user experience by using a variety of approaches.", "histories": [["v1", "Tue, 20 May 2014 16:32:59 GMT  (866kb,D)", "http://arxiv.org/abs/1405.5147v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["everaldo aguiar", "saurabh nagrecha", "nitesh v chawla"], "accepted": false, "id": "1405.5147"}, "pdf": {"name": "1405.5147.pdf", "metadata": {"source": "CRF", "title": "Predicting Online Video Engagement Using Clickstreams", "authors": ["Everaldo Aguiar", "Saurabh Nagrecha", "Nitesh V. Chawla"], "emails": ["eaguiar@nd.edu", "snagrech@nd.edu", "nchawla@nd.edu"], "sections": [{"heading": "Author Keywords", "text": "Clickstream; predictive analysis; online video; user engagement."}, {"heading": "ACM Classification Keywords", "text": "I.5.2. Pattern Recognition: Design Methodology"}, {"heading": "INTRODUCTION", "text": "The constant growth in volume, speed, availability, and functionality of the Web brings with it not only a variety of challenges and risks, but also a number of opportunities. While there have been a series of major advances in the field over time, one that has been given a considerable amount of attention in more recent years is that of personalization.\nData about users\u2019 online activity is continuously captured and analyzed. Advanced recommendation systems are now able to tell us what products we might be interested in buying, the books we will enjoy reading, what movies we should watch next, and even which diseases we are at risk of contracting. From a business perspective, the benefits of being able to understand customers in this level of detail are unquestionable.\nMethods for capturing user data on the Web are also becoming increasingly efficient. As described in [20], the browsing behavior of individual users can be recorded at the granularity of mouse clicks with little to no work needing to be done. A number of services, both free and proprietary, offer user tracking solutions that can be implemented and deployed within minutes. However, the feedback that one usually gets from these tools is often in the form of simplistic aggregate statistics that do not offer a deeper understanding of user behavior.\nWith that in mind, we set to analyze the application of some of these ideas to a specific context, while having as our major \u2217Corresponding Author\ngoal the understanding of each user as an individual unit. For this study, we were provided a large dataset that describes user clicks generated within a two-month span and across a number of websites managed by a large communications company.\nThis paper describes the process through which we parsed, analyzed, and drew knowledge from a user-generated clickstream dataset provided by a large communications company. We begin by showing, from a more general perspective, how this type of data can be used to identify particularly interesting trends in user interest, and to further illustrate the usefulness of this information, we describe how we applied methods to predict user engagement to video streams and discuss their accuracy.\nDistributing content that entices user engagement and captures large audiences is the ultimate goal of all web media providers. Measuring and forecasting these variables, however, is not an easy task. As Figure 1 illustrates, as time goes by, the amount of users that remain tuned to video streams dramatically decreases. For certain categories, the percentage of users that actually watch videos to completion can be as low as 20%.\nTo address this undesired outcome, we propose the development of clickstream-based models that can learn the individual preferences and characteristics of each user, and utilize this information to predict how \u201cengaged\u201d they will be to a particular video stream. Being able to know, in advance, if\nar X\niv :1\n40 5.\n51 47\nv1 [\ncs .L\nG ]\n2 0\nM ay\n2 01\n4\na user is likely to exit a video prematurely allows content providers some leeway to implement personalized intervention strategies aimed at maximizing viewership retention.\nThe remaining portion of the paper is organized as follows \u2013 The next section gives an overview of the most recent related literature. That is followed by a detailed coverage of clickstream data representation and a description of our particular dataset. We then elaborate on the methods applied in this study, the results obtained, and their importance. Finally, the last section draws conclusions about this exercise and argues for the latent potential that resides in user-generated clickstream data."}, {"heading": "RELATED WORK", "text": "Interest in analyzing the online activities of users is as old as providing consumable content itself. This problem has piqued the interest of multiple fields, namely marketing, psychology and computer science.\nSince user activity provides an immense amount of measurable secondary data, various models to predict multiple aspects of their behavior have been proposed. User interaction has been studied at various levels\u2014 from gaze tracking [8] to broader patterns of path traversal within a website [1, 22]. Simple duration and dwell-time [4] can be used to predict when a user exits the site. User classification [21] can be used to identify what the user is specifically looking for and even morph the website [16] according to the custom tastes of that particular user profile. Personalized content based on click history has been implemented and widely adopted by commercial content providers [6, 19].\nWith the distribution of video content online becoming mainstream, the way we study user engagement has been greatly enriched. Studies like [7] have measured the role of video content quality in influencing user engagement, but did not utilize clickstreams to contextualize the video views. Online video engagement for Massive Open Online Courses (MOOCs) [13] has shown that the lessons learned from analyzing video views can be used to improve video authoring, editing and interface design. It also emphasizes the value of video dropout as a metric for engagement. Though the MOOC work lacks the contextual history of the users, in this paper we leverage similar and many other clickstream features to predict video engagement."}, {"heading": "CLICKSTREAM DATA REPRESENTATION", "text": "Clickstream data consists of a \u201cvirtual trail\u201d that users leave behind while they interact with a given system, website or application. More specifically, data that describes the state of a user\u2019s current session is recorded each time a click is performed, and the aggregation of that produces a clickstream, which can be used to reconstruct all actions taken by the user while he or she utilized that given product.\nWhile applicable to a variety of scenarios, the collection and analysis of clickstreams has become most notably popular in the context of Web-based tools and websites. As highlighted by Srivastava et. al. [25], the analysis of such information\nhas potential applications in a number of areas such as website personalization and modification, system improvement, business intelligence and usage characterization. Our contributions fall mainly within the first and last domains."}, {"heading": "Our Dataset", "text": "The data we utilized for this study was provided to us by a large U.S.-based communications company that operates in the radio, TV, newspaper and online media domain. They manage a few dozens of websites, all of which are embedded with clickstream capturing functionality. Next, we give a detailed description of the most important features this dataset contains.\nUser activity is continuously captured by numerous servers across the country and is then concatenated at the end of the day in the form of daily \u201cdumps\u201d. We utilized 59 of these files that covered the period ranging from December 4, 2012 to January 31, 2013. Altogether, these files contain an upwards of 65 million click instances.\nEach click instance recorded is characterized by a large number of features (161 in this case). Table 1 lists a small subset of the most relevant features and a brief description of each. With that information we are able to determine (1) how users reached the website, (2) what attracted them there, (3) what\nactions they performed while on the site and (4) how they eventually exited.\nNote that while there is no feature that captures the event of a user leaving the website, as is common practice, we work under the assumption that when a user is inactive for a period longer than 30 minutes (i.e., no click events originate from this person during that time), we simply say that the user has exited the site.\nThis assumption allows us to group these click events from the original datasets into user sessions, which illustrate the path a user takes while browsing the website and can be used to identify areas that attract more (or less) traffic.\nFigure 2 illustrates one individual session chosen at random from our dataset. We can see that the user in this case was referred to our domain through a link that he or she found on a social network website and that their visit consisted of several hops, most of which happened in the news section.\nAggregating these sessions allows us to visualize which areas of the website are more popular, as well as which links connecting different sections are traversed the most. Take for instance the example illustrated in Figure 3. To generate this particular graph, we isolated the sessions corresponding to a certain newspaper\u2019s website, its 12 most popular sections, and the traffic between them. Among other observations, we noticed that the readers of this particular newspaper were often prone to navigating to the sports section and reading multiple articles there.\nFurthermore, these sessions can be aggregated, producing a high-level view of the entire website structure by popularity of section. Figure 3 illustrates this concept.\nLastly, we note that based on information retrieved from specific features of our dataset, it is possible to determine if a user is simply browsing text articles, displaying image galleries or streaming online video. The following sections of this paper will describe how we used this fact to aid in the development of predictive models for video viewership engagement."}, {"heading": "METHODS", "text": ""}, {"heading": "Identification of Video Exit Instances", "text": "When a user watches a video, a separate log entry is made corresponding to when he or she completes watching a certain percentage of the video, while a player ID remains constant. This makes the clickstream log reflect a cumulative history of the viewer\u2019s progress within that video.\nBy filtering the data to get only clicks corresponding to video instances, and then by IP address, we obtain the entire video viewing activity of each IP. From this modified dataset, we isolate an individual \u201cvideo view\u201d table by specifying the player ID. This table is then sorted chronologically and filtered by session timeout. The last entry corresponds to the viewer\u2019s exit point. This gives us a unique session for a visit. In combination with the current session data, and data from cookies, we retrieve the user\u2019s unique historical browsing patterns. It should be noted that in the absence of cookies, we treat the user as a fresh incoming visitor. For our analysis, we isolated only the instances where the user exited the video. Due to the inherently discretized nature of the data collection, we get a coarse-grained estimate of when the user reached a certain percent of the video. If the last entry shows that the user watched 50% of a video, it can be inferred that the user exited at p%, such that p \u2208 [50, 75)."}, {"heading": "Feature Selection", "text": "Using various feature selection methods, we reduced the size of our dataset from the original 161 features to the 12 best descriptors. Among these were features like IP, location, content annotations, and referrer information. Out of the 161 features in a typical video exit instance, 40 are mutually redundant, and 32 are constant in value. This motivates the need to find a set of features that is the best descriptor of the target class (in this case, the percent of video the user watches before exiting) [14]. We investigated various feature selection methods which support mixed data types and ranked the top features. One would expect these features to encompass measurable user traits which influence their interest in the video.\nVarious feature selection methods aim to remove redundant and irrelevant features using different statistical means, which have their respective strengths. Though a popular choice in machine learning, correlation based feature selection (CFS) was not considered due to the sparse nature of the data [15]. A more detailed study of these can be found in [28, 12]. The feature selection methods employed in this problem are described below:"}, {"heading": "Chi Squared", "text": "The chi squared (\u03c72) method measures how much deviation is seen in the observed data from the case where the class value and the feature are independent of each other. It evaluates whether the feature and class occurrences are randomly related, or exhibit some relation."}, {"heading": "Information Gain", "text": "Information gain [23] measures how much entropy is lost when the feature is present vs. absent."}, {"heading": "Gain Ratio", "text": "Information gain favors attributes with many values over those with fewer values, the gain ratio [24] compensates for this by factoring in the amount of split caused by the feature."}, {"heading": "One R", "text": "One R formulates a set of simple relationships between the features and ranks the features based on how accurate these rules are."}, {"heading": "Symmetric Uncertainty", "text": "Symmetric uncertainty [26, 10] targets attributes which correlate well with the class but have little intercorrelation.\nThe results of these feature selection methods are summarized in Table 2. The attributes in the table are the ones which consistently appear in the top 10%. These are the attributes which influence video exit points the most.\nThe time of viewing influences at what point people are prone to exit the video. IP address, in conjunction with location, and ISP indicate who is watching the video and thus offer a personalized facet to the prediction. The number of pages viewed by a person and frequency of visits can be perceived to be reflective of the person\u2019s interest in the site. The referrer which brought the viewer on the site can influence the engagement of the viewer; a viewer coming from a social network link interacts differently than one who had the site bookmarked on their browser. The entry point is the first page the viewer saw in their current viewing session; this determines their interest in consuming further content. The actual title of the story includes the section which the video is under. As we had observed in Figure 1, users viewing \u201cTechnology\u201d related videos were less likely to exit than those viewing \u201cEntertainment\u201d related videos."}, {"heading": "Classification", "text": "Our aim is to predict how much of the video a user watches before exiting. In our dataset, we find that this is represented by 5 distinct markers, which correspond to the percentage of\nthe video the user watched before exiting. We formulate two classification tasks- to predict what percent of the video is watched, and whether the user exits the video \u201cearly\u201d (before reaching 50% of the video).\nThis prediction task involving 5 classes. Since it is relevant to predict users who exit early on in the video, we assume that users who exit the video at the beginning or having viewed 25% of the video to have exited \u201cearly\u201d. As described above, this would correspond to users who have viewed 0 to 49% of the video.\nWe can refine the problem as the binary prediction of these \u201cearly exits\u201d. The classes would then be a merger of the previously mentioned 5 classes, with the first two combined to form that of \u201cearly exits\u201d and the latter 3 being those who chose to not exit early. This simplification is depicted in the representative expected confusion matrices for both of the classification tasks as displayed in Figure 4. We have performed both prediction analyses on our data."}, {"heading": "Naive Bayes", "text": "Among the simplest and most primitive classification algorithms, this probabilistic method is based on the Bayes Theorem [2] and strong underlying independence assumptions. That is, each feature is assumed to contribute independently to the class outcome."}, {"heading": "C4.5 Decision trees", "text": "C4.5 Decision Trees [24] work by building a tree structure where split operations are performed on each node based on information gain values for each feature of the dataset and the respective class. At each level, the attribute with highest information gain is chosen as the basis for the split criterion.\nRepeated Incremental Pruning to Produce Error Reduction RIPPER [5] is a rule based classification tree learner. It is algorithmically faster than C4.5, having a complexity of O(n(log(n))2) as opposed to C4.5\u2019s complexity of the order O(n3). RIPPER constructs an initial set of rules and then iteratively optimizes it according to a tunable parameter. It is implemented in Weka under the \u201cJRip class\u201d."}, {"heading": "Random forests", "text": "Random forests [3] combine multiple tree predictors in an ensemble. New instances being classified are pushed down the trees, and each tree reports a classification. The \u201cforest\u201d then decides which label to assign to this new instance based on the aggregate number of votes given by the set of trees."}, {"heading": "Decision Tables", "text": "Decision Table classifiers [18] are built by concatenating a series of rules derived from the feature set to corresponding class outcomes. This method as its major advantages the fact that it is easy to interpret and notably efficient."}, {"heading": "Random Subspaces", "text": "The random subspace method [17] is an ensemble classifier whose individual classifiers operate on random subsets of the feature set. The predictions made by the individual classifiers are combined using the posterior probabilities of each class in the constituent classifiers. This method looks at the classification problem from various perspectives by randomizing the selection of features."}, {"heading": "Stacking", "text": "Stacking [27] is a meta-classification scheme which employs an ensemble of classifiers and performs the learning task on two levels. First, the classifiers in the ensemble are trained on the data, then the meta-classifier learns from their predictions and the training labels of the data."}, {"heading": "Key Performance Indices / Metrics Utilized", "text": "Our key performance index is the accuracy of prediction of when the user will drop-off in the video. To obtain these predictions, we perform 10-fold cross-validation on the available data using various classification methods. In 10-fold cross validation, the data is randomly partitioned into 10 subsets and predictions are made on each of these. These predictions are then aggregated to provide the overall performance of the classifier, which we measured by the accuracy and area under the Receiver Operating Characteristics curve (AUROC), all\nof which are described in further detail below. Each of these measures depict various aspects of the prediction results."}, {"heading": "Accuracy", "text": "The accuracy of a classifier is perhaps the simplest measurement of its performance. It represents the percentage of total instances that were correctly classified. We would like for this to be as high as possible. The baseline for accuracy is that of a perfectly random prediction. For a binary classification problem, this would be 50% and for a 5 class problem, the baseline accuracy would be 20%. Any classifier which delivers statistically greater accuracy than these respective baselines, is considered to be better than a random predictor."}, {"heading": "Receiver Operating Characteristics (ROC) curves", "text": "A system tuned to increase accuracy does not necessarily make it a good predictor. Relying on accuracy alone does not provide insights into the nature of misclassified instances. ROC curves [11] are a way to quickly compare multiple classifiers. The goal of a classifier in ROC space is to be as close to the upper-left corner as possible. In ROC space, if the curve for one classifier is closer to the upper-left corner than that for another, then it is considered to have a superior performance."}, {"heading": "EXPERIMENTAL RESULTS", "text": "We evaluated the performance of each of the classifiers used, with 10-fold cross validation for both the multiclass and the binary classification predictions. Table 3 summarizes the results of all experiments."}, {"heading": "Multiclass Prediction", "text": "We see that in terms of sheer accuracy, the stacked classifiers performed slightly better than other methods, achieving an accuracy of 56.9%. In terms of AUROC, however, it is seen that Naive Bayes performs much better, closely followed by Decision Tables. These simple classifiers might not have the best accuracy, but outperform the others."}, {"heading": "Binary Class Prediction", "text": "In this second scenario, we associate a semantic meaning to the drop-off percentage point and predict if the user will exit early or not. This refinement of the problem statement gives us a much better performance across the board. The stacked classifiers, for instance, achieve a remarkable accuracy of 84.6% when predicting which users exited their video streams prematurely. As it was the case with the multiclass problem, we again saw that Decision Tables and Naive Bayes surpassed the other classifiers in terms of AUROC values.\nThough stacked classifiers give greater accuracy, they are not as good as Decision Tables or Naive Bayes in predicting early drop-off. This is still reflective of the general trends observed in the multiclass problem as we have merely merged classes, the underlying data remains the same.\nIn both, the multiclass and binary class prediction, it is observed that simpler rule based learners outperform complicated meta-classifiers. This is documented in [9], showing that stacking does not always outperform the best classifier.\nWe see that simple classification algorithms can be used to achieve comparable, or even better performance than complicated meta-classifiers. Besides the obvious performance superiority, it is desirable to use simpler classifiers on grounds of computational complexity, as implementing these is algorithmically more scalable and thus offers faster runtime."}, {"heading": "CONCLUSIONS", "text": "We demonstrated how clickstream data can be used to predict \u201cearly exits\u201d in online videos. By constructing models to this effect, we were able to identify with high accuracy which video streaming sessions are likely to terminate prematurely. Additionally, we compared and contrasted the performance of a number of classifiers, highlighting those that we found to be\nparticularly fit to this problem. Having knowledge of such information would allow content providers to personalize how their media is distributed so as to increase user retention, and as a result, business value."}], "references": [{"title": "Clickstream clustering using weighted longest common subsequences", "author": ["A. Banerjee", "J. Ghosh"], "venue": "In Proceedings of the web mining workshop at the 1st SIAM conference on data mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "A model of web site browsing behavior estimated on clickstream data", "author": ["R.E. Bucklin", "C. Sismeiro"], "venue": "Journal of Marketing Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Fast effective rule induction", "author": ["W.W. Cohen"], "venue": "In ICML, vol", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A.S. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Understanding the impact of video quality on user engagement", "author": ["F. Dobrian", "V. Sekar", "A. Awan", "I. Stoica", "D.A. Joseph", "A. Ganjam", "J. Zhan", "H. Zhang"], "venue": "SIGCOMM-Computer Communication Review 41,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Internet advertising: Is anybody watching", "author": ["X. Dreze", "Hussherr", "F.-X"], "venue": "Journal of interactive marketing 17,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Is combining classifiers with stacking better than selecting the best one", "author": ["S. D\u017eeroski", "B. \u017denko"], "venue": "Machine learning 54,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Machine learning-based text mining for biomedical information analysis", "author": ["Eom", "J.-H", "Zhang", "B.-T"], "venue": "Genomics & Informatics", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "An introduction to roc analysis", "author": ["T. Fawcett"], "venue": "Pattern recognition letters 27,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "An Extensive Empirical Study of Feature Selection Metrics for Text Classification", "author": ["G. Forman"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Correlation-based Feature Selection for Machine Learning", "author": ["M.A. Hall"], "venue": "PhD thesis, The University of Waikato,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "The power of decision tables", "author": ["R. Kohavi"], "venue": "In Machine Learning: ECML-95. Springer,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1995}, {"title": "Personalized news recommendation based on click behavior", "author": ["J. Liu", "P. Dolan", "E.R. Pedersen"], "venue": "In Proceedings of the 15th international conference on Intelligent user interfaces,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Automatic Personalization Based on Web Usage Mining", "author": ["B. Mobasher", "R. Cooley", "J. Srivastava"], "venue": "Communications of the ACM 43,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Buying, searching, or browsing: Differentiating between online shoppers using in-store navigational clickstream", "author": ["W.W. Moe"], "venue": "Journal of Consumer Psychology", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Modeling online browsing and path analysis using clickstream data", "author": ["A.L. Montgomery", "S. Li", "K. Srinivasan", "J.C. Liechty"], "venue": "Marketing Science 23,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning 1,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1986}, {"title": "Web usage mining: Discovery and applications of usage patterns from web data", "author": ["J. Srivastava", "R. Cooley", "M. Deshpande", "Tan", "P.-N"], "venue": "ACM SIGKDD Explorations Newsletter", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks 5,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1992}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "In ICML, vol", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}], "referenceMentions": [{"referenceID": 15, "context": "As described in [20], the browsing behavior of individual users can be recorded at the granularity of mouse clicks with little to no work needing to be done.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "User interaction has been studied at various levels\u2014 from gaze tracking [8] to broader patterns of path traversal within a website [1, 22].", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "User interaction has been studied at various levels\u2014 from gaze tracking [8] to broader patterns of path traversal within a website [1, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 17, "context": "User interaction has been studied at various levels\u2014 from gaze tracking [8] to broader patterns of path traversal within a website [1, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": "Simple duration and dwell-time [4] can be used to predict when a user exits the site.", "startOffset": 31, "endOffset": 34}, {"referenceID": 16, "context": "User classification [21] can be used to identify what the user is specifically looking for and even morph the website [16] according to the custom tastes of that particular user profile.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "Personalized content based on click history has been implemented and widely adopted by commercial content providers [6, 19].", "startOffset": 116, "endOffset": 123}, {"referenceID": 14, "context": "Personalized content based on click history has been implemented and widely adopted by commercial content providers [6, 19].", "startOffset": 116, "endOffset": 123}, {"referenceID": 4, "context": "Studies like [7] have measured the role of video content quality in influencing user engagement, but did not utilize clickstreams to contextualize the video views.", "startOffset": 13, "endOffset": 16}, {"referenceID": 19, "context": "[25], the analysis of such information Figure 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "This motivates the need to find a set of features that is the best descriptor of the target class (in this case, the percent of video the user watches before exiting) [14].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "Though a popular choice in machine learning, correlation based feature selection (CFS) was not considered due to the sparse nature of the data [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 22, "context": "A more detailed study of these can be found in [28, 12].", "startOffset": 47, "endOffset": 55}, {"referenceID": 9, "context": "A more detailed study of these can be found in [28, 12].", "startOffset": 47, "endOffset": 55}, {"referenceID": 18, "context": "Information Gain Information gain [23] measures how much entropy is lost when the feature is present vs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "Symmetric uncertainty [26, 10] targets attributes which correlate well with the class but have little intercorrelation.", "startOffset": 22, "endOffset": 30}, {"referenceID": 7, "context": "Symmetric uncertainty [26, 10] targets attributes which correlate well with the class but have little intercorrelation.", "startOffset": 22, "endOffset": 30}, {"referenceID": 2, "context": "Repeated Incremental Pruning to Produce Error Reduction RIPPER [5] is a rule based classification tree learner.", "startOffset": 63, "endOffset": 66}, {"referenceID": 13, "context": "Decision Table classifiers [18] are built by concatenating a series of rules derived from the feature set to corresponding class outcomes.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "The random subspace method [17] is an ensemble classifier whose individual classifiers operate on random subsets of the feature set.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "Stacking [27] is a meta-classification scheme which employs an ensemble of classifiers and performs the learning task on two levels.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "ROC curves [11] are a way to quickly compare multiple classifiers.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "This is documented in [9], showing that stacking does not always outperform the best classifier.", "startOffset": 22, "endOffset": 25}], "year": 2014, "abstractText": "In the nascent days of e-content delivery, having a superior product was enough to give companies an edge against the competition. With today\u2019s fiercely competitive market, one needs to be multiple steps ahead, especially when it comes to understanding consumers. Focusing on a large set of web portals owned and managed by a private communications company, we propose methods by which these sites\u2019 clickstream data can be used to provide a deep understanding of their visitors, as well as their interests and preferences. We further expand the use of this data to show that it can be effectively used to predict user engagement to video streams. Author", "creator": "LaTeX with hyperref package"}}}