{"id": "1405.5869", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2014", "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)", "abstract": "We present the first provably sublinear time algorithm for approximate \\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets. In the general classification task, the LSH model is highly optimized and offers a consistent classification algorithm with a minimum internal product search. Our proposal is also based on an experimentally constructed algorithm for approximations of the same LSH model.", "histories": [["v1", "Thu, 22 May 2014 19:42:57 GMT  (379kb)", "http://arxiv.org/abs/1405.5869v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.IR cs.LG", "authors": ["anshumali shrivastava", "ping li 0001"], "accepted": true, "id": "1405.5869"}, "pdf": {"name": "1405.5869.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anshu@cs.cornell.edu", "pingli@stat.rutgers.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n58 69\nv1 [\nst at\n.M L\n] 2\n1Initially submitted in Feb. 2014."}, {"heading": "1 Introduction and Motivation", "text": "The focus of this paper is on the problem of Maximum Inner Product Search (MIPS). In this problem, we are given a giant data vector collection S of size N , where S \u2282 RD, and a given query point q \u2208 RD. We are interested in searching for p \u2208 S which maximizes (or approximately maximizes) the inner product qT p. Formally, we are interested in efficiently computing\np = argmax x\u2208S qTx (1)\nThe MIPS problem is related to the problem of near neighbor search (NNS), which instead requires computing\np = argmin x\u2208S \u2223\u2223q \u2212 x\u2223\u222322 = argmin x\u2208S (\u2223\u2223x\u2223\u222322 \u2212 2qTx) (2) These two problems are equivalent if the norm of every element x \u2208 S is constant. Note that the value of the norm \u2223\u2223q\u2223\u22232 has no effect as it is a constant throughout and does not change the identity of argmax or argmin. There are many scenarios in which MIPS arises naturally at places where the norms of the elements in S have very significant variations [17] and can not be controlled. As a consequence, existing fast algorithms for the problem of approximate NNS can not be directly used for solving MIPS.\nHere we list a number of practical scenarios where the MIPS problem is solved as a subroutine: (i) recommender system, (ii) large-scale object detection with DPM, (iii) structural SVM, and (iv) multi-class label prediction."}, {"heading": "1.1 Recommender Systems", "text": "Recommender systems are often based on collaborative filtering which relies on past behavior of users, e.g., past purchases and ratings. Latent factor modelling based on matrix factorization [19] is a popular approach for solving collaborative filtering. In a typical matrix factorization model, a user i is associated with a latent user characteristic vector ui, and similarly, an item j is associated with a latent item characteristic vector vj . The rating ri,j of item j by user i is modeled as the inner product between the corresponding characteristic vectors. A popular generalization of this framework, which combines neighborhood information with latent factor approach [18], leads to the following model:\nri,j = \u00b5 + bi + bj + uTi vj (3)\nwhere \u00b5 is the over all constant mean rating value, bi and bj are user and item biases, respectively. Note that Eq. (3) can also be written as ri,j = \u00b5 + [ui; bi; 1]T [vj ; 1; bj], where the form [x;y] is the concatenation of vectors x and y.\nRecently, [6] showed that a simple computation of ui and vj based on naive SVD of the sparse rating matrix outperforms existing models, including the neighborhood model, in recommending top-ranked items. In this setting, given a user i and the corresponding learned latent vector ui finding the right item j, to recommend to this user, involves computing\nj = argmax j\u2032 ri,j\u2032 = argmax j\u2032 uTi vj\u2032 (4)\nwhich is an instance of the standard MIPS problem. It should be noted that we do not have control over the norm of the learned characteristic vector, i.e., \u2225vj\u22252, which often has a wide range in practice [17].\nIf there are N items to recommend, solving (4) requires computing N inner products. Recommendation systems are typically deployed in on-line application over web where the number N is huge. A brute force linear scan over all items, for computing argmax, would be prohibitively expensive."}, {"heading": "1.2 Large-Scale Object Detection with DPM", "text": "Deformable Part Model (DPM) based representation of images is the state-of-the-art in object detection tasks [11]. In DPM model, first a set of part filters are learned from the train dataset. During detection, these learned filter activations over various patches of the test image are used to score the test image. The activation of a filter on an image patch is an inner product between them. Typically, the number of possible filters are large (e.g., millions) and so scoring the test image is costly. Very recently, it was shown that scoring based only on filters with high activations performs well in practice [10]. Identifying filters, from a large collection of possible filters, having high activations on a given image patch requires computing top inner products. Consequently, an efficient solution to the MIPS problem will benefit large scale object detections based on DPM."}, {"heading": "1.3 Structural SVM", "text": "Structural SVM, with cutting plane training [16], is one of the popular methods for learning over structured data. The most expensive step with cutting plane iteration is the call to the separation oracle which identifies the most violated constraint. In particular, given the current SVM estimate w, the separation oracle computes\ny\u0302i = argmax y\u0302\u2208Y \u2206(yi, y\u0302) +wT\u03a8(xi, y\u0302) (5) where \u03a8(xi, y\u0302) is the joint feature representation of data with the possible label y\u0302 and \u2206(yi, y\u0302) is the loss function. Clearly, this is again an instance of the MIPS problem. This step is expensive in that the number of possible elements, i.e., the size of Y , is possibly exponential. Many heuristics were deployed to hopefully improve the computation of argmax in (5), for instance caching [16]. An efficient MIPS routine can make structural SVM faster and more scalable."}, {"heading": "1.4 Multi-Class Label Prediction", "text": "The models for multi-class SVM (or logistic regression) learn a weight vector wi for each of the class label i. After the weights are learned, given a new test data vector xtest, predicting its class label is basically an MIPS problem:\nytest = argmax i\u2208L xTtest wi (6)\nwhere L is the set of possible class labels. Note that the norms of the weight vectors \u2225wi\u22252 are not constant. The size, \u2223L\u2223, of the set of class labels differs in applications. Classifying with large number of possible class labels is common in fine grained object classification, for instance, prediction task with 100,000 classes [10] (i.e., \u2223L\u2223 = 100,000). Computing such high-dimensional vector multiplications for predicting the class label of a single instance can be expensive in, for example, user-facing applications."}, {"heading": "1.5 The Need for Hashing Inner Products", "text": "Recall the MIPS problem is to find x \u2208 S which maximizes the inner product between x and the given query q, i.e. maxx\u2208S qTx. A brute force scan of all elements of S can be prohibitively costly in applications which deal with massive data and care about the latency (e.g., search).\nOwing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27, 17] similar to k-d trees [12]. That method did not come with provable runtime guarantees. In fact, it is also well-known that techniques based on space partitioning (such as k-d trees) suffer from the curse of dimensionality. For example, it was shown in [30] (both empirically and theoretically) that all current techniques (based on space partitioning) degrade to linear search, even for dimensions as small as 10 or 20.\nLocality Sensitive Hashing (LSH) [15] based randomized techniques are common and successful in industrial practice for efficiently solving NNS (near neighbor search). Unlike space partitioning techniques, both the running time as well as the accuracy guarantee of LSH based NNS are in a way independent of the dimensionality of the data. This makes LSH suitable for large scale processing system dealing with ultrahigh dimensional datasets which are common these days. Furthermore, LSH based schemes are massively parallelizable, which makes them ideal for modern \u201cBig\u201d datasets. The prime focus of this paper will be on efficient hashing based algorithms for MIPS, which do not suffer from the curse of dimensionality."}, {"heading": "1.6 Our Contributions", "text": "We develop Asymmetric LSH (ALSH), an extended LSH scheme for efficiently solving the approximate MIPS problem. Finding hashing based algorithms for MIPS was considered hard [27, 17]. In this paper, we formally show that this is indeed the case with the current framework of LSH, and there can not exist any LSH for solving MIPS. Despite this negative result, we show that it is possible to relax the current LSH framework to allow asymmetric hash functions which can efficiently solve MIPS. This generalization comes with no cost and the extended framework inherits all the theoretical guarantees of LSH.\nOur construction of asymmetric LSH is based on an interesting mathematical phenomenon that the original MIPS problem, after asymmetric transformations, reduces to the problem of approximate near neighbor search. Based on this key observation, we show an explicit construction of asymmetric hash function, leading to the first provably sublinear query time algorithm for approximate similarity search with (unnormalized) inner product as the similarity. The construction of asymmetric hash function and the new LSH framework could be of independent theoretical interest.\nExperimentally, we evaluate our algorithm for the task of recommending top-ranked items, under the collaborative filtering framework, with the proposed asymmetric hashing scheme on Netflix and Movielens datasets. Our evaluations support the theoretical results and clearly show that the proposed asymmetric hash function is superior for retrieving inner products, compared to the well known hash function based on pstable distribution for L2 norm [9] (which is also part of standard LSH package [2]). This is not surprising because L2 distances and inner products may have very different orderings."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Locality Sensitive Hashing (LSH)", "text": "Approximate versions of the near neighbor search problem [15] were proposed to break the linear query time bottleneck. The following formulation is commonly adopted.\nDefinition: (c-Approximate Near Neighbor or c-NN) Given a set of points in a D-dimensional space RD, and parameters S0 > 0, \u03b4 > 0, construct a data structure which, given any query point q, does the following with probability 1 \u2212 \u03b4: if there exists an S0-near neighbor of q in P , it reports some cS0-near neighbor of q in P .\nThe usual notion of S0-near neighbor is in terms of distance. Since we deal with similarities, we can equivalently define S0-near neighbor of point q as a point p with Sim(q, p) \u2265 S0, where Sim is the similarity function of interest.\nThe popular technique for c-NN uses the underlying theory of Locality Sensitive Hashing (LSH) [15]. LSH is a family of functions, with the property that more similar input objects in the domain of these functions have a higher probability of colliding in the range space than less similar ones. In formal terms,\nconsider S a family of hash functions mapping RD to some set I.\nDefinition: (Locality Sensitive Hashing (LSH)) A family H is called (S0, cS0, p1, p2)-sensitive if, for any two point x, y \u2208 RD, h chosen uniformly from H satisfies the following:\n\u2022 if Sim(x, y) \u2265 S0 then PrH(h(x) = h(y)) \u2265 p1 \u2022 if Sim(x, y) \u2264 cS0 then PrH(h(x) = h(y)) \u2264 p2\nFor efficient approximate nearest neighbor search, p1 > p2 and c < 1 is needed."}, {"heading": "2.2 Fast Similarity Search with LSH", "text": "In a typical task of similarity search, we are given a query q, and our aim is to find x \u2208 S with high value of Sim(q, x). LSH provides a clean mechanism of creating hash tables [2]. The idea is to concatenate K independent hash functions to create a meta-hash function of the form Bl(x) = [h1(x);h2(x); ...;hK (x)] (7) where hi, i = {1,2, ...,K} are K independent hash functions sampled from the LSH family. The LSH algorithm needs L independent meta hash functions Bl(x), l = 1,2, ...,L.\n\u2022 Pre-processing Step: During preprocessing, we assign xi \u2208 S to the bucket Bl(xi) in the hash table l, for l = 1,2, ...,L. \u2022 Querying Step: Given a query q, we retrieve union of all elements from buckets Bl(q), where the union is taken over all hash tables l, for l = 1,2, ...,L. The bucket Bl(q) contains elements xi \u2208 S whose K different hash values collide with that of the query. By the LSH property of the hash function, these elements have higher probability of being similar to the query q compared to a random point. This probability value can be tuned by choosing appropriate value for parameters K and L. Optimal choices lead to fast query time algorithm:\nFact 1: Given a family of (S0, cS0, p1, p2) -sensitive hash functions, one can construct a data structure for c-NN with O(n\u03c1 logn) query time and space O(n1+\u03c1), where \u03c1 = log p1\nlog p2 < 1.\nLSH trades off query time with extra (one time) preprocessing cost and space. Existence of an LSH family translates into provably sublinear query time algorithm for c-NN problems. It should be noted that the worst case query time for LSH is only dependent on \u03c1 and n. Thus, LSH based near neighbor search in a sense does not suffer from the curse of dimensionality. This makes LSH a widely popular technique in industrial practice [14, 25, 7]."}, {"heading": "2.3 LSH for L2 distance", "text": "[9] presented a novel LSH family for all Lp (p \u2208 (0,2]) distances. In particular, when p = 2, this scheme provides an LSH family for L2 distances. Formally, given a fixed number r, we choose a random vector a with each component generated from i.i.d. normal, i.e., ai \u223c N(0,1), and a scalar b generated uniformly at random from [0, r]. The hash function is defined as:\nhL2a,b(x) = \u230aaTx + br \u230b (8)\nwhere \u230a\u230b is the floor operation. The collision probability under this scheme can be shown to be Pr(hL2a,b(x) = hL2a,b(y)) = Fr(d), (9) Fr(d) = 1 \u2212 2\u03a6(\u2212r/d) \u2212 2\u221a\n2\u03c0(r/d) (1 \u2212 e\u2212(r/d)2/2) (10) where \u03a6(x) = \u222b x\u2212\u221e 1\u221a2\u03c0 e\u2212x22 dx is the cumulative density function (cdf) of standard normal distribution and d = \u2223\u2223x \u2212 y\u2223\u22232 is the Euclidean distance between the vectors x and y. This collision probability Fr(d) is a monotonically decreasing function of the distance d and hence hL2a,b is an LSH for L2 distances. This scheme is also the part of LSH package [2]. Here r is a parameter which can be tuned.\nAs argued previously, \u2223\u2223x \u2212 y\u2223\u22232 = \u221a(\u2223\u2223x\u2223\u222322 + \u2223\u2223y\u2223\u222322 \u2212 2xT y) is not monotonic in the inner product xT y unless the given data has a constant norm. Hence, hL2a,b is not suitable for MIPS. In Section 4, we will experimentally show that our proposed method compares favorably to hL2a,b hash function for MIPS. Very recently [23] reported an improvement of this well-known hashing scheme when the data can be normalized (for example, when x and y both have unit L2 norm). However, in our problem setting, since the data can not be normalized, we can not take advantage of the new results of [23], at the moment."}, {"heading": "3 Hashing for MIPS", "text": ""}, {"heading": "3.1 A Negative Result", "text": "We first show that, under the current LSH framework, it is impossible to obtain a locality sensitive hashing for MIPS. In [27, 17], the authors also argued that finding locality sensitive hashing for inner products could be hard, but to the best of our knowledge we have not seen a formal proof.\nTheorem 1 There can not exist any LSH family for MIPS.\nProof: Suppose, there exists such hash function h. For un-normalized inner products the self similarity of a point x with itself is Sim(x,x) = xTx = \u2223\u2223x\u2223\u222322 and there may exist another points y, such that Sim(x, y) = yTx > \u2223\u2223x\u2223\u222322+M , for any constant M . Under any single randomized hash function h, the collision probability of the event {h(x) = h(x)} is always 1. So if h is an LSH for inner product then the event {h(x) = h(y)} should have higher probability compared to the event {h(x) = h(x)} (which already has probability 1). This is not possible because the probability can not be greater than 1 . Note that we can always choose y with Sim(x, y) = S0 + \u03b4 > S0 and cS0 > Sim(x,x) \u2200S0 and \u2200c < 1. This completes the proof. \u25fb\nNote that in [4] it was shown that we can not have a hash function where the collision probability is equal to the inner product, because \u201c1 - inner product\u201d does not satisfy the triangle inequality. This does not totally eliminates the existence of LSH. For instance, under L2Hash, the collision probability is a monotonic function of distance and not the distance itself."}, {"heading": "3.2 Our Proposal: Asymmetric LSH (ALSH)", "text": "The basic idea of LSH is probabilistic bucketing and it is more general than the requirement of having a single hash function h. In the LSH algorithm (Section 2.2), we use the same hash function h for both the preprocessing step and the query step. We assign buckets in the hash table to all the candidates x \u2208 S using h. We use the same h on the query q to identify relevant buckets. The only requirement for the proof, of Fact 1, to work is that the collision probability of the event {h(q) = h(x)} increases with the similarity Sim(q, x). The theory [13] behind LSH still works if we use hash function h1 for preprocessing x \u2208 S and\na different hash function h2 for querying, as long as the probability of the event {h2(q) = h1(x)} increases with Sim(q, x), and there exist p1 and p2 with the required property. The traditional LSH definition does not allow this asymmetry but it is not a required condition in the proof. For this reason, we can relax the definition of c-NN without loosing runtime guarantees.\nAs the first step, we define a modified locality sensitive hashing, in a slightly different form which will be useful later.\nDefinition: (Asymmetric Locality Sensitive Hashing (ALSH)) A family H, along with the two vector functions Q \u2236 RD \u21a6 RD\u2032 (Query Transformation) and P \u2236 RD \u21a6 RD\u2032 (Preprocessing Transformation), is called (S0, cS0, p1, p2)-sensitive if for a given c-NN instance with query q, and the hash function h chosen uniformly from H satisfies the following:\n\u2022 if Sim(q, x) \u2265 S0 then PrH(h(Q(q))) = h(P (x))) \u2265 p1 \u2022 if Sim(q, x) \u2264 cS0 then PrH(h(Q(q)) = h(P (x))) \u2264 p2\nHere x is any point in the collection S .\nWhen Q(x) = P (x) = x, we recover the vanilla LSH definition with h(.) as the required hash function. Coming back to the problem of MIPS, if Q and P are different, the event {h(Q(x)) = h(P (x))} will not have probability equal to 1 in general. Thus, Q \u2260 P can counter the fact that self similarity is not highest with inner products. We just need the probability of the new collision event {h(Q(q)) = h(P (y))} to satisfy the conditions of Definition of c-NN for Sim(q, y) = qT y. Note that the query transformation Q is only applied on the query and the pre-processing transformation P is applied to x \u2208 S while creating hash tables. It is this asymmetry which will allow us to solve MIPS efficiently. In Section 3.3, we explicitly show a construction (and hence the existence) of asymmetric locality sensitive hash function for solving MIPS. The source of randomization h for both q and x \u2208 S is the same.\nFormally, it is not difficult to show a result analogous to Fact 1.\nTheorem 2 Given a family of hash function H and the associated query and preprocessing transformations P and Q, which is (S0, cS0, p1, p2) -sensitive, one can construct a data structure for c-NN with O(n\u03c1 logn) query time and space O(n1+\u03c1), where \u03c1 = log p1\nlog p2 .\nProof: Use the standard LSH procedure (Section 2.2) with a slight modification. While preprocessing, we assign xi to bucket Bl(P (xi)) in table l. While querying with query q, we retrieve elements from bucket Bl(Q(q)) in the hash table l. By definition of asymmetric LSH, the probability of retrieving an element, under this modified scheme, follows the same expression as in the original LSH. The proof can be completed by exact same arguments used for proving Fact 1 (See [13] for details). \u25fb"}, {"heading": "3.3 From MIPS to Near Neighbor Search (NNS)", "text": "Without loss of generality, we can assume that for the problem of MIPS the query q is normalized, i.e.,\u2223\u2223q\u2223\u22232 = 1, because in computing p = argmaxx\u2208S qTx the argmax is independent of \u2223\u2223q\u2223\u22232. In particular, we can choose to let U < 1 be a number such that \u2223\u2223xi\u2223\u22232 \u2264 U < 1, \u2200xi \u2208 S (11) If this is not the case then during the one time preprocessing we can always divide all xis by maxxi\u2208S \u2223\u2223xi\u2223\u22232 U\n. Note that scaling all xi\u2019s by the same constant does not change argmaxx\u2208S qTx.\nWe are now ready to describe the key step in our algorithm. First, we define two vector transformations P \u2236 RD \u21a6 RD+m and Q \u2236 RD \u21a6 RD+m as follows:\nP (x) = [x; \u2223\u2223x\u2223\u222322; \u2223\u2223x\u2223\u222342; ....; \u2223\u2223x\u2223\u22232m2 ] (12) Q(x) = [x; 1/2; 1/2; ....; 1/2], (13)\nwhere [;] is the concatenation. P (x) appends m scalers of the form \u2223\u2223x\u2223\u22232i2 at the end of the vector x, while Q(x) simply appends m \u201c1/2\u201d to the end of the vector x.\nBy observing that\n\u2223\u2223P (xi)\u2223\u222322 = \u2223\u2223xi\u2223\u222322 + \u2223\u2223xi\u2223\u222342 + ... + \u2223\u2223xi\u2223\u22232m2 + \u2223\u2223xi\u2223\u22232m+12 (14)\u2223\u2223Q(q)\u2223\u222322 = \u2223\u2223q\u2223\u222322 +m/4 = 1 +m/4 (15) Q(q)TP (xi) = qTxi + 1\n2 (\u2223\u2223xi\u2223\u222322 + \u2223\u2223xi\u2223\u222342 + ... + \u2223\u2223xi\u2223\u22232m2 ) (16)\nwe obtain the following key equality: \u2223\u2223Q(q) \u2212 P (xi)\u2223\u222322 = (1 +m/4) \u2212 2qTxi + \u2223\u2223xi\u2223\u22232m+12 (17) Since \u2223\u2223xi\u2223\u22232 \u2264 U < 1 \u2223\u2223xi\u2223\u22232m+1 \u2192 0, at the tower rate (exponential to exponential). The term (1 +m/4) is a fixed constant. As long as m is not too small (e.g., m \u2265 3 would suffice), we have\nargmax x\u2208S qTx \u2243 argmin x\u2208S \u2223\u2223Q(q) \u2212P (x)\u2223\u22232 (18) To the best of our knowledge, this is the first connection between solving un-normalized MIPS and approximate near neighbor search. Transformations P and Q, when norms are less than 1, provide correction to the L2 distance \u2223\u2223Q(q) \u2212 P (xi)\u2223\u22232 making it rank correlate with the (un-normalized) inner product. This works only after shrinking the norms, as norms greater than 1 will instead blow the term \u2223\u2223xi\u2223\u22232m+12 . This interesting mathematical phenomenon connects MIPS with approximate near neighbor search."}, {"heading": "3.4 Fast Algorithms for MIPS", "text": "Eq. (18) shows that MIPS reduces to the standard approximate near neighbor search problem which can be efficiently solved. As the error term \u2223\u2223xi\u2223\u22232m+12 < U2m+1 goes to zero at a tower rate, it quickly becomes negligible for any practical purposes. In fact, from theoretical perspective, since we are interested in guarantees for c-approximate solutions, this additional error can be absorbed in the approximation parameter c.\nFormally, we can state the following theorem.\nTheorem 3 Given a c-approximate instance of MIPS, i.e., Sim(q, x) = qTx, and a query q such that\u2223\u2223q\u2223\u22232 = 1 along with a collection S having \u2223\u2223x\u2223\u22232 \u2264 U < 1 \u2200x \u2208 S. Let P and Q be the vector transformations defined in Eq. (12) and Eq. (13), respectively. We have the following two conditions for hash function hL2a,b (defined by Eq. (8))\n\u2022 if qTx \u2265 S0 then\nPr[hL2a,b(Q(q)) = hL2a,b(P (x))] \u2265 Fr(\u221a1 +m/4 \u2212 2S0 +U2m+1)\n\u2022 if qTx \u2264 cS0 then\nPr[hL2a,b(Q(q)) = hL2a,b(P (x))] \u2264 Fr(\u221a1 +m/4 \u2212 2cS0) where the function Fr is defined by Eq. (10).\nProof: From Eq. (9), we have\nPr[hL2a,b(Q(q)) = hL2a,b(P (x))] =Fr(\u2223\u2223Q(q) \u2212 P (x)\u2223\u22232) =Fr(\u221a1 +m/4 \u2212 2qTx + \u2223\u2223x\u2223\u22232m+12 ) \u2265Fr(\u221a1 +m/4 \u2212 2S0 +U2m+1)\nThe last step follows from the monotonically decreasing nature of F combined with inequalities qTx \u2265 S0 and \u2223\u2223x\u2223\u22232 \u2264 U . We have also used the monotonicity of the square root function. The second inequality similarly follows using qTx \u2264 cS0 and \u2223\u2223x\u2223\u22232 \u2265 0.. This completes the proof. \u25fb\nThe conditions \u2223\u2223q\u2223\u22232 = 1 and \u2223\u2223x\u2223\u22232 \u2264 U < 1, \u2200x \u2208 S can be absorbed in the transformations Q and P respectively, but we show it explicitly for clarity.\nThus, we have obtained p1 = Fr(\u221a(1 +m/4) \u2212 2S0 +U2m+1) and p2 = Fr(\u221a(1 +m/4) \u2212 2cS0). Applying Theorem 2, we can construct data structures with worst case O(n\u03c1 logn) query time guarantees for c-approximate MIPS, where\n\u03c1 = logFr(\u221a1 +m/4 \u2212 2S0 +U2m+1)\nlogFr(\u221a1 +m/4 \u2212 2cS0) (19) We also need p1 > p2 in order for \u03c1 < 1. This requires us to have \u22122S0 +U2m+1 < \u22122cS0, which boils down to the condition c < 1 \u2212 U2m+1\n2S0 . Note that U\n2 m+1 2S0 can be made arbitrarily close to zero with the appropriate\nvalue of m. For any given c < 1, there always exist U < 1 and m such that \u03c1 < 1. This way, we obtain a sublinear query time algorithm for MIPS. The guarantee holds for any values of U and m satisfying m \u2208 N+ and U \u2208 (0,1). We also have one more parameter r for the hash function ha,b. Recall the definition of Fr in Eq. (10): Fr(d) = 1 \u2212 2\u03a6(\u2212r/d) \u2212 2\u221a\n2\u03c0(r/d) (1 \u2212 e\u2212(r/d)2/2). Given a c-approximate MIPS instance, \u03c1 is a function of 3 parameters: U , m, r. The algorithm with the\nbest query time chooses U , m and r, which minimizes the value of \u03c1. For convenience, we define\n\u03c1\u2217 = min U,m,r logFr(\u221a1 +m/4 \u2212 2S0 +U2m+1) logFr(\u221a1 +m/4 \u2212 2cS0) (20)\ns.t. U2\nm+1 2S0 < 1 \u2212 c, m \u2208 N+, r > 0 and 0 < U < 1.\nSee Figure 1 for the plots of \u03c1\u2217. With this best value of \u03c1, we can state our main result in Theorem 4.\nTheorem 4 (Approximate MIPS is Efficient) For the problem of c-approximate MIPS, one can construct a data structure having O(n\u03c1\u2217 logn) query time and space O(n1+\u03c1\u2217), where \u03c1\u2217 < 1 is the solution to constraint optimization (20).\nJust like in the typical LSH framework, the value of \u03c1\u2217 in Theorem 4 depends on the c-approximate instance we aim to solve, which requires knowing the similarity threshold S0 and the approximation ratio c. Since, \u2223\u2223q\u2223\u22232 = 1 and \u2223\u2223x\u2223\u22232 \u2264 U < 1, \u2200x \u2208 S , we have qtx \u2264 U . A reasonable choice of the threshold S0 is to choose a high fraction of U, for example, S0 = 0.9U or S0 = 0.8U .\nThe computation of \u03c1\u2217 and the optimal values of corresponding parameters can be conducted via a grid search over the possible values of U , m and r, as we only have 3 parameters. We compute the values of \u03c1\u2217 along with the corresponding optimal values of U , m and r for S0 \u2208 {0.9U, 0.8U, 0.7U, 0.6U, 0.5U} for different approximation ratios c ranging from 0 to 1. The plot of the optimal \u03c1\u2217 is shown in Figure 1, and the corresponding optimal values of U , m and r are shown in Figure 2."}, {"heading": "3.5 Practical Recommendation of Parameters", "text": "In practice, the actual choice of S0 and c is dependent on the data and is often unknown. Figure 2 illustrates that m \u2208 {2,3,4}, U \u2208 [0.8, 0.85], and r \u2208 [1.5, 3] are reasonable choices. For convenience, we recommend\nto use m = 3, U = 0.83, and r = 2.5. With this choice of the parameters, Figure 3 shows that the \u03c1 values using these parameters are very close to the optimal \u03c1\u2217 values."}, {"heading": "3.6 More Insight: The Trade-off between U and m", "text": "Let \u01eb = U2 m+1\nbe the error term in Eq. (17). As long as \u01eb is small the MIPS problem reduces to standard near neighbor search via the transformations P and Q. There are two ways to make \u01eb small, either we choose a small value of U or a large value of m. There is an interesting trade-off between parameters U and m. To see this, consider the following Figure 4 for Fr(d), i.e., the collision probability defined in Eq. (10).\nSuppose \u01eb = U2 m+1 is small, then p1 = Fr(\u221a1 +m/4 \u2212 2qTx) and p2 = Fr(\u221a1 +m/4 \u2212 2cqTx). Because of the bounded norms, we have \u2212\u2223\u2223q\u2223\u22232\u2223\u2223x\u2223\u22232 = \u2212U \u2264 qTx \u2264 U = \u2223\u2223q\u2223\u22232\u2223\u2223x\u2223\u22232. Consider the high similarity range\nd \u2208 [\u221a1 +m/4 \u2212 2U,\u221a1 +m/4 \u2212 2cU ], for some appropriately chosen c < 1. This range is wider, if U is close to 1, if U is close to 0 then 2U \u2243 2cU making the arguments to F close to each other, which in turn decreases the gap between p1 and p2. On the other hand, when m is larger, it adds bigger offset (term m/4 inside square root) to both the arguments. Adding offset, makes p1 and p2 shift towards right in the collision probability plot. Right shift, makes p1 and p2 closer because of the nature of the collision probability curve (see Figure 4). Thus, bigger m makes p1 and p2 closer. Smaller m pushes p1 and p2 towards left and thereby increasing the gap between them.\nSmall \u03c1 = log p1 log p2 roughly boils down to having a bigger gap between p1 and p2 [26]. Ideally, for small \u03c1, we would like to use big U and small m. The error \u01eb = U2 m+1\nexactly follows the opposite trend. For error to be small we want small U and not too small m."}, {"heading": "3.7 Parallelization", "text": "To conclude this section, we should mention that the hash table based scheme is massively parallelizable. Different nodes on cluster need to maintain their own hash tables and hash functions. The operation of retrieving from buckets and computing the maximum inner product over those retrieved candidates, given a query, is a local operation. Computing the final maximum can be conducted efficiently by simply communicating one single number per nodes. Scalability of hashing based methods is one of the reasons which account for their popularity in industrial practice."}, {"heading": "4 Evaluations", "text": ""}, {"heading": "4.1 Datasets", "text": "We evaluate our proposed hash function for the MIPS problem on two popular collaborative filtering datasets (on the task of item recommendations):\n\u2022 Movielens. We choose the largest available Movielens dataset, the Movielens 10M, which contains around 10 million movie ratings from 70,000 users over 10,000 movie titles. The ratings are between 1 to 5, with increments of 0.5 (i.e., 10 possible ratings in total).\n\u2022 Netflix. This dataset contains 100 million movie ratings from 480,000 users over 17,000 movie titles. The ratings are on a scale from 1 to 5 (integer). Each dataset forms a sparse user-item matrix R, where the value of R(i, j) indicates the rating of user i for movie j. Given the user-item ratings matrix R, we follow the PureSVD procedure described in [6] to generate user and item latent vectors. That is, we compute the SVD of the ratings matrix\nR =W\u03a3V T\nwhere W is nusers\u00d7f matrix and V is nitem\u00d7f matrix for some appropriately chosen rank f (which is also called the latent dimension).\nAfter the SVD step, the rows of matrix U =W\u03a3 are treated as the user characteristic vectors while rows of matrix V correspond to the item characteristic vectors. More specifically ui, the ith row of matrix U , denotes the characteristic vector of user i, while the jth row of V , i.e., vj , corresponds to the characteristic vector for item j. The compatibility between item i and item j is given by the inner product between the corresponding user and item characteristic vectors. Therefore, in recommending top-ranked items to users i, the PureSVD method returns top-ranked items based on the inner products uTi vj , \u2200j.\nThe PureSVD procedure, despite its simplicity, outperforms other popular recommendation algorithms for the task of top-ranking recommendations (see [6] for more details) on these two datasets. Following [6], we use the same choices for the latent dimension f , i.e., f = 150 for Movielens and f = 300 for Netflix."}, {"heading": "4.2 Baseline Hash Function", "text": "Our proposal is the first provable hashing scheme in the literature for retrieving inner products and hence there is no existing baseline. Since, our hash function uses Hashing for L2 distance after asymmetric transformation P (12) and Q (13), we would like to know if such transformations are even needed and furthermore get an estimate of the improvements obtained using these transformations. We therefore compare our\nproposal with L2LSH the hashing scheme hL2a,b described by Eq. (8). It is implemented in the LSH package for near neighbor search with Euclidean distance.\nAlthough, L2LSH are not optimal for retrieving un-normalized inner products, it does provide some indexing capability. Our experiments will show that the proposed method outperform L2LSH, often significantly so, in retrieving inner products. This is not surprising as we know that the rankings of L2 distance can be different from rankings of inner products."}, {"heading": "4.3 Evaluations", "text": "We are interested in knowing, how the two hash functions correlate with the top-T inner products. For this task, given a user i and its corresponding user vector ui, we compute the top-T gold standard items based on the actual inner products uTi vj , \u2200j. We then compute K different hash codes of the vector ui and all the item vectors vjs. For every item vj , we then compute the number of times its hash values matches (or collides) with the hash values of query which is user ui, i.e., we compute\nMatchesj = K\n\u2211 t=1 1(ht(ui) = ht(vj)), (21) where 1 is the indicator function. Based on Matchesj we rank all the items. This procedure generates a sorted list of all the items for a given user vector ui corresponding to every hash function under consideration. Here, we use h = hL2a,b for L2LSH. For our proposed asymmetric hash function we have h(ui) = hL2a,b(Q(ui)), since ui is the query, and h(vj) = hL2a,b(P (vj)) for all the items. The subscript t is used to distinguish independent draws of h. Ideally, for a better hashing scheme, Matchesj should be higher for items having higher inner products with the given user ui.\nWe compute the precision and recall of the top-T items for T \u2208 {1,5,10}, obtained from the sorted list based on Matches. To compute this precision and recall, we start at the top of the ranked item list and walk down in order. Suppose we are at the kth ranked item, we check if this item belongs to the gold standard top-T list. If it is one of the top-T gold standard item, then we increment the count of relevant seen by 1, else we move to k + 1. By kth step, we have already seen k items, so the total items seen is k. The precision and recall at that point is then computed as:\nPrecision = relevant seen\nk , Recall = relevant seen T\n(22)\nWe vary a large number of k values to obtain continuously-looking precision-recall curves. Note that it is important to balance both precision and recall. Methodology which obtains higher precision at a given recall is superior. Higher precision indicates higher ranking of the relevant items. We average this value of precision and recall over 2000 randomly chosen users.\nChoice of r. We need to specify this important parameter for our proposed algorithm as well as L2LSH. We have shown in Figure 3 that, for our proposed algorithm, it is overall good to choose m = 3, U = 0.83 and r = 2.5. This theoretical result largely frees us from the burden of choosing parameters. We still need to choose r for L2LSH. To ensure that L2LSH is not being suboptimal because of the choice of r, we report the results of L2LSH for all r \u2208 {1,1.5,2,2.5, 3, 3.5, 4,4.5, 5}, and we show that our proposed method (with r = 2.5) very significantly outperforms L2LSH at all choices of r, in Figure 5 and Figure 6, for K = 64,128,256,512 hashes.\nThe good performance of our algorithm shown in Figure 5 and Figure 6 is not surprising because we know from Theorem 3 that the collision under the new hash function is a direct indicator of high inner product. As the number of hash codes is increased, the performance of the proposed methodology shows bigger\nimprovements over L2LSH. The suboptimal performance of L2LSH clearly indicates that the norms of the item characteristic vectors do play a significant role in item recommendation task. Also, the experiments clearly establishes the need of proposed asymmetric transformation P and Q.\nTo close this section, we present Figure 7 to visualize the impact of the parameter r on the performance of our proposed method. Our theory and Figure 3 have already shown that we can achieve close to optimal performance by choosing m = 3, U = 0.83, and r = 2.5. Nevertheless, it is still interesting to see how the theory is confirmed by experiments. Figure 7 presents the precision-recall curves for our proposed method with m = 3, U = 0.83, and r \u2208 {1,1.5,2,2.5,3,3.5,4,4.5,5}. For clarify, we only differentiate r = 1,2.5,5 from the rest of the curves. The results demonstrate that r = 2.5 is indeed a good choice and the performance is not too sensitive to r unless it is much away from 2.5."}, {"heading": "5 Conclusion and Future Work", "text": "MIPS (maximum inner product search) naturally arises in numerous practical scenarios, e.g., collaborative filtering. Given a query data vector, the task of MIPS is to find data vectors from the repository which are most similar to the query in terms of (un-normalized) inner product (instead of distance). This problem is challenging and, prior to our work, there existed no provably sublinear time algorithms for MIPS. The current framework of LSH (locality sensitive hashing) is not sufficient for solving MIPS.\nIn this study, we develop ALSH (asymmetric LSH), which generalizes the existing LSH framework by applying (appropriately chosen) asymmetric transformations to the input query vector and the data vectors in the repository. We present an implementation of ALSH by proposing a novel transformation which converts the original inner products into L2 distances in the transformed space. We demonstrate, both theoretically and empirically, that this implementation of ALSH provides a provably efficient solution to MIPS.\nWe believe our work will lead to several interesting (and practically useful) lines of future work:\n\u2022 Three-way (and higher-order) maximum inner product search: In this paper, we have focused on pairwise similarity search. Three-way (or even higher-order) search can also be important in practice and might attract more attentions in the near future due to the recent pilot studies on efficient three-way similarity computation [20, 22, 28]. Extending ALSH to three-way MIPS could be very useful.\n\u2022 Other efficient similarities: Finding other similarities for which there exist fast retrieval algorithms is an important problem [5]. Exploring other similarity functions, which can be efficient solved using asymmetric hashing is an interesting future area. One good example is to find special ALSH schemes for binary data by exploring prior powerful hashing methods for binary data such as (b-bit) minwise hashing and one permutation hashing [3, 24].\n\u2022 Fast hashing for MIPS: Our proposed hash function uses random projection as the main hashing scheme. There is a rich set of literature [21, 1, 8, 24, 29] on making hashing faster. Evaluations of these new faster techniques will further improve the runtime guarantees in this paper.\n\u2022 Applications: We have evaluated the efficiency of our scheme in the collaborative filtering application. An interesting set of future work consist of applying this hashing scheme for other applications mentioned in Section 1. In particular, it will be exciting to apply efficient MIPS subroutines to improve the DPM based object detection [10] and structural SVMs [16]."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "STOC, pages 557\u2013563, Seattle, WA,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "E2lsh: Exact euclidean locality sensitive hashing", "author": ["A. Andoni", "P. Indyk"], "venue": "Technical report,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "On the resemblance and containment of documents", "author": ["A.Z. Broder"], "venue": "the Compression and Complexity of Sequences, pages 21\u201329, Positano, Italy,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC, pages 380\u2013388, Montreal, Quebec, Canada,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Lsh-preserving functions and their applications", "author": ["F. Chierichetti", "R. Kumar"], "venue": "SODA, pages 1078\u2013 1094,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "Proceedings of the fourth ACM conference on Recommender systems, pages 39\u201346. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Google news personalization: scalable online collaborative filtering", "author": ["A.S. Das", "M. Datar", "A. Garg", "S. Rajaram"], "venue": "Proceedings of the 16th international conference on World Wide Web, pages 271\u2013280. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast locality-sensitive hashing", "author": ["A. Dasgupta", "R. Kumar", "T. Sarl\u00f3s"], "venue": "KDD, pages 1073\u20131081,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokn"], "venue": "SCG, pages 253 \u2013 262, Brooklyn, NY,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["T. Dean", "M.A. Ruzon", "M. Segal", "J. Shlens", "S. Vijayanarasimhan", "J. Yagnik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages 1814\u20131821. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627\u20131645,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A projection pursuit algorithm for exploratory data analysis", "author": ["J.H. Friedman", "J.W. Tukey"], "venue": "IEEE Transactions on Computers, 23(9):881\u2013890,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1974}, {"title": "Approximate nearest neighbor: Towards removing the curse of dimensionality", "author": ["S. Har-Peled", "P. Indyk", "R. Motwani"], "venue": "Theory of Computing, 8(14):321\u2013350,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "author": ["M.R. Henzinger"], "venue": "SIGIR, pages 284\u2013291,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "STOC, pages 604\u2013613, Dallas, TX,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C.-N.J. Yu"], "venue": "Machine Learning, 77(1):27\u201359,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient retrieval of recommendations in a matrix factorization framework", "author": ["N. Koenigstein", "P. Ram", "Y. Shavitt"], "venue": "CIKM, pages 535\u2013544,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "KDD, pages 426\u2013434. ACM,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A sketch algorithm for estimating two-way and multi-way associations", "author": ["P. Li", "K.W. Church"], "venue": "Computational Linguistics (Preliminary results appeared in HLT/EMNLP 2005), 33(3):305\u2013354,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Very sparse random projections", "author": ["P. Li", "T.J. Hastie", "K.W. Church"], "venue": "KDD, pages 287\u2013296, Philadelphia, PA,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "b-bit minwise hashing for estimating three-way similarities", "author": ["P. Li", "A.C. K\u00f6nig", "W. Gui"], "venue": "Advances in Neural Information Processing Systems, Vancouver, BC,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Coding for random projections and approximate near neighbor search", "author": ["P. Li", "M. Mitzenmacher", "A. Shrivastava"], "venue": "Technical report, arXiv:1403.8144,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "One permutation hashing", "author": ["P. Li", "A.B. Owen", "C.-H. Zhang"], "venue": "NIPS, Lake Tahoe, NV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting Near-Duplicates for Web-Crawling", "author": ["G.S. Manku", "A. Jain", "A.D. Sarma"], "venue": "WWW, Banff, Alberta, Canada,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Maximum inner-product search using cone trees", "author": ["P. Ram", "A.G. Gray"], "venue": "KDD, pages 931\u2013939,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond pairwise: Provably fast algorithms for approximate k-way similarity search", "author": ["A. Shrivastava", "P. Li"], "venue": "NIPS, Lake Tahoe, NV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Densifying one permutation hashing via rotation for fast near neighbor search", "author": ["A. Shrivastava", "P. Li"], "venue": "ICML, Beijing, China,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A quantitative analysis and performance study for similaritysearch methods in high-dimensional spaces", "author": ["R. Weber", "H.-J. Schek", "S. Blott"], "venue": "VLDB, pages 194\u2013205,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 16, "context": "There are many scenarios in which MIPS arises naturally at places where the norms of the elements in S have very significant variations [17] and can not be controlled.", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "A popular generalization of this framework, which combines neighborhood information with latent factor approach [18], leads to the following model: ri,j = \u03bc + bi + bj + ui vj (3) where \u03bc is the over all constant mean rating value, bi and bj are user and item biases, respectively.", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "Recently, [6] showed that a simple computation of ui and vj based on naive SVD of the sparse rating matrix outperforms existing models, including the neighborhood model, in recommending top-ranked items.", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": ", \u2225vj\u22252, which often has a wide range in practice [17].", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "2 Large-Scale Object Detection with DPM Deformable Part Model (DPM) based representation of images is the state-of-the-art in object detection tasks [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "Very recently, it was shown that scoring based only on filters with high activations performs well in practice [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "3 Structural SVM Structural SVM, with cutting plane training [16], is one of the popular methods for learning over structured data.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Many heuristics were deployed to hopefully improve the computation of argmax in (5), for instance caching [16].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "Classifying with large number of possible class labels is common in fine grained object classification, for instance, prediction task with 100,000 classes [10] (i.", "startOffset": 155, "endOffset": 159}, {"referenceID": 24, "context": "Owing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27, 17] similar to k-d trees [12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 16, "context": "Owing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27, 17] similar to k-d trees [12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 11, "context": "Owing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27, 17] similar to k-d trees [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 27, "context": "For example, it was shown in [30] (both empirically and theoretically) that all current techniques (based on space partitioning) degrade to linear search, even for dimensions as small as 10 or 20.", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "Locality Sensitive Hashing (LSH) [15] based randomized techniques are common and successful in industrial practice for efficiently solving NNS (near neighbor search).", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Finding hashing based algorithms for MIPS was considered hard [27, 17].", "startOffset": 62, "endOffset": 70}, {"referenceID": 16, "context": "Finding hashing based algorithms for MIPS was considered hard [27, 17].", "startOffset": 62, "endOffset": 70}, {"referenceID": 8, "context": "Our evaluations support the theoretical results and clearly show that the proposed asymmetric hash function is superior for retrieving inner products, compared to the well known hash function based on pstable distribution for L2 norm [9] (which is also part of standard LSH package [2]).", "startOffset": 234, "endOffset": 237}, {"referenceID": 1, "context": "Our evaluations support the theoretical results and clearly show that the proposed asymmetric hash function is superior for retrieving inner products, compared to the well known hash function based on pstable distribution for L2 norm [9] (which is also part of standard LSH package [2]).", "startOffset": 282, "endOffset": 285}, {"referenceID": 14, "context": "1 Locality Sensitive Hashing (LSH) Approximate versions of the near neighbor search problem [15] were proposed to break the linear query time bottleneck.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "The popular technique for c-NN uses the underlying theory of Locality Sensitive Hashing (LSH) [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "LSH provides a clean mechanism of creating hash tables [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "This makes LSH a widely popular technique in industrial practice [14, 25, 7].", "startOffset": 65, "endOffset": 76}, {"referenceID": 23, "context": "This makes LSH a widely popular technique in industrial practice [14, 25, 7].", "startOffset": 65, "endOffset": 76}, {"referenceID": 6, "context": "This makes LSH a widely popular technique in industrial practice [14, 25, 7].", "startOffset": 65, "endOffset": 76}, {"referenceID": 8, "context": "3 LSH for L2 distance [9] presented a novel LSH family for all Lp (p \u2208 (0,2]) distances.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "This scheme is also the part of LSH package [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 21, "context": "Very recently [23] reported an improvement of this well-known hashing scheme when the data can be normalized (for example, when x and y both have unit L2 norm).", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "However, in our problem setting, since the data can not be normalized, we can not take advantage of the new results of [23], at the moment.", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "In [27, 17], the authors also argued that finding locality sensitive hashing for inner products could be hard, but to the best of our knowledge we have not seen a formal proof.", "startOffset": 3, "endOffset": 11}, {"referenceID": 16, "context": "In [27, 17], the authors also argued that finding locality sensitive hashing for inner products could be hard, but to the best of our knowledge we have not seen a formal proof.", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "\u25fb Note that in [4] it was shown that we can not have a hash function where the collision probability is equal to the inner product, because \u201c1 - inner product\u201d does not satisfy the triangle inequality.", "startOffset": 15, "endOffset": 18}, {"referenceID": 12, "context": "The theory [13] behind LSH still works if we use hash function h1 for preprocessing x \u2208 S and", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "The proof can be completed by exact same arguments used for proving Fact 1 (See [13] for details).", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "Given the user-item ratings matrix R, we follow the PureSVD procedure described in [6] to generate user and item latent vectors.", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "The PureSVD procedure, despite its simplicity, outperforms other popular recommendation algorithms for the task of top-ranking recommendations (see [6] for more details) on these two datasets.", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "Following [6], we use the same choices for the latent dimension f , i.", "startOffset": 10, "endOffset": 13}], "year": 2014, "abstractText": "We1 present the first provably sublinear time algorithm for approximate Maximum Inner Product Search (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets. Initially submitted in Feb. 2014.", "creator": "LaTeX with hyperref package"}}}