{"id": "1405.7624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2014", "title": "Simultaneous Feature and Expert Selection within Mixture of Experts", "abstract": "A useful strategy to deal with complex classification scenarios is the \"divide and conquer\" approach. The mixture of experts (MOE) technique makes use of this strategy by joinly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weights their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, for the case of high dimensional data. Our main intuition is that particular subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using $L1$ regularization, with a simultaneous expert selection. The experiments are still pending. The main goal of the data mining is to provide a powerful and well-designed strategy for learning the underlying principles of algorithms, especially when comparing an input space.", "histories": [["v1", "Thu, 29 May 2014 17:32:29 GMT  (87kb,D)", "http://arxiv.org/abs/1405.7624v1", "17 pages, 2 figures"]], "COMMENTS": "17 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["billy peralta"], "accepted": false, "id": "1405.7624"}, "pdf": {"name": "1405.7624.pdf", "metadata": {"source": "CRF", "title": "Simultaneous Feature and Expert Selection within Mixture of Experts", "authors": ["Billy Peraltaa"], "emails": ["bperalta@uct.cl"], "sections": [{"heading": null, "text": "A useful strategy to deal with complex classification scenarios is the \u201cdivide and conquer\u201d approach. The mixture of experts (MOE) technique makes use of this strategy by joinly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weights their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, for the case of high dimensional data. Our main intuition is that particular subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using L1 regularization, with a simultaneous expert selection. The experiments are still pending. Keywords: Mixture of experts, local feature selection, embedded feature selection, regularization."}, {"heading": "1. Mixture of Experts with embedded variable selection", "text": "Our main idea is to incorporate a local feature selection scheme inside each expert and gate function of a MoE formulation. Our main intuition is that, in the context of classification, different partitions of the input data can be best represented by specific\n\u2217Corresponding author, Telephone: (56 45) 255 3948 Email address: bperalta@uct.cl (Billy Peralta)\nPreprint submitted to ??? May 30, 2014\nar X\niv :1\n40 5.\n76 24\nv1 [\ncs .L\nG ]\n2 9\nM ay\n2 01\nsubsets of features. This is particularly relevant in the case of high dimensional spaces, where the common presence of noisy or irrelevant features might obscure the detection of particular class patterns. Specifically, our approach takes advantage of the linear nature of each local expert and gate function in the classical MoE formulation [17], meaning that L1 regularization can be directly applied. Below, we first briefly describe the classical MoE formulation for classification. Afterwards, we discuss the proposed modification to the MoE model that provides embedded feature selection."}, {"heading": "1.1. Mixture of Experts", "text": "In the context of supervised classification, there is available a set of N training examples, or instance-label pairs (xn, yn), representative of the domain data (x, y), where xn \u2208 <D and yn \u2208 C. Here C is a discrete set of Q class labels {c1, ..., cQ}. The goal is to use training data to find a function f that minimizes a loss function which scores the quality of f to predict the true underlying relation between x and y. From a probabilistic point of view [4], a useful approach to find f is using a conditional formulation:\nf(x) = arg max ci\u2208C\np(y = ci|x).\nIn the general case of complex relations between x and y, a useful strategy consists of approximating f through a mixture of local functions. This is similar to the case of modeling a mixture distribution [34] and it leads to the MoE model.\nWe decompose the conditional likelihood p(y|x) as:\np(y|x) = K\u2211 i=1 p(y,mi|x) = K\u2211 i=1 p(y|mi, x) p(mi|x), (1)\nwhere Equation (1) represents a MoE model with K experts mi. Figure (1) shows a schematic diagram of the MoE approach. The main idea is to obtain local models in such a way that they are specialized in a particular region of the data. In Figure (1), x corresponds to the input instance, p(y|mi, x) is the expert function, p(mi|x) is the\ngating function, and p(y|x) is the weighted sum of the experts. Note that the output of each expert model is weighted by the gating function. This weight can be interpreted as the relevance of expert mi for the classification of input instance x. Also note that the gate function has K outputs, one for each expert. There are K expert functions that have Q components, one for each class.\nThe traditional MoE technique uses multinomial logit models, also known as softmax functions [4], to represent the gate and expert functions. An important characteristic of this model is that it forces competition among its components. In MoE, such components are expert functions for the gates and class-conditional functions for the experts. The competition in soft-max functions enforces the especialization of experts in different areas of the input space [41].\nUsing multinomial logit models, a gate function is defined as:\np(mi|x) = exp(\u03bdTi x)\u2211K j=1 exp(\u03bd T j x)\n(2)\nwhere i \u2208 {1, . . . , K} refers to the set of experts and \u03bdi \u2208 <D is a vector of model parameters. Component \u03bdij of vector \u03bdi models the relation between the gate and dimension j of input instance x.\nSimilarly, an expert function is defined as:\np(y = cl|x,mi) = exp(\u03c9Tlix)\u2211M j=1 exp(\u03c9 T jix)\n(3)\nwhere \u03c9li depends on class label cl and expert i. In this way, there are a total of Q\u00d7K vectors \u03c9li. Component \u03c9lij of vector \u03c9li models the relation between expert function i and dimension j of input instance x.\nThere are several methods to find the value of the hidden parameters \u03bdij and \u03c9lij [26]. An attractive alternative is to use the EM algorithm. In the case of MoE, the EM formulation augments the model by introducing a set of latent variables, or responsibilities, indicating the expert that generates each instance. Accordingly, the EM iterations consider an expectation step that estimates expected values for responsibilities, and a maximization step that updates the values of parameters \u03bdij and \u03c9lij. Specifically, the posterior probability of the responsibility Rin assigned by the gate function to expert mi for an instance xn is given by [26]:\nRin = p(mi|xn, yn) (4)\n= p(yn|xn,mi) p(mi|xn)\u2211K j=1 p(yn|xn,mj) p(mj|xn)\nConsidering these responsibilities and Equation (1), the expected complete log-\nlikelihood \u3008Lc\u3009 used in the EM iterations is [26]:\n\u3008Lc\u3009 = N\u2211 n=1 K\u2211 i=1 Rin [log p(yn|xn,mi) + log p(mi|xn)] (5)"}, {"heading": "1.2. Regularized Mixture of Experts (RMoE)", "text": "To embed a feature selection process in the MoE approach, we use the fact that in Equations (2) and (3) the multinomial logit models for gate and experts functions contain linear relations for the relevant parameters. This linearity can be straightforwardly used in feature selection by considering that a parameter component \u03bdij or \u03c9lij\nwith zero value implies that dimension j is irrelevant for gate function p(mi|x) or expert model p(y|mi, x), respectively. Consequently, we propose to penalize complex models using L1 regularization. Similar consideration is used in the work of [29] but in the context of unsupervised learning. The idea is to maximize the likelihood of data while simultaneously minimizing the number of parameter components \u03bdij and \u03c9lij different from zero. Considering that there are Q classes, K experts, and D dimensions, the\nexpected L1 regularized log-likelihood \u2329 LRc \u232a is given by:\n\u2329 LRc \u232a = \u3008Lc\u3009 \u2212 \u03bb\u03bd K\u2211 i=1 D\u2211 j=1 |\u03bdij| \u2212 \u03bb\u03c9 Q\u2211 l=1 K\u2211 i=1 D\u2211 j=1 |\u03c9lij| . (6)\nTo maximize Equation (6) with respect to model parameters, we use first the standard fact that the likelihood function in Equation (5) can be decomposed in terms of independent expressions for gate and expert models [26]. In this way, the maximization step of the EM based solution can be performed independently with respect to gate and expert parameters [26]. In our problem, each of these optimizations has an extra term given by the respective regularization term in Equation (6). To handle this case, we observe that each of these optimizations is equivalent to the expression to solve a regularized logistic regression [20]. As shown in [20], this problem can be solved by using a coordinate ascent optimization strategy [37] given by a sequential two-step approach that first models the problem as an unregularized logistic regression and afterwards incorporates the regularization constraints.\nIn summary, we handle Equation (6) by using a EM based strategy that at each step solves the maximation with respect to model parameters by decomposing this problem in terms of gate and expert parameters. Each of these problems is in turn solved using the strategy proposed in [20]. Next, we provide details of this procedure.\nOptimization of the unregularized log-likelihood In this case, we solve the unconstrained log-likelihood given by Equation (5). First, we optimize the log-likelihood with respect to vector \u03c9li. The maximization of the expected log-likelihood \u3008Lc\u3009 implies deriving Equation (5) with respect to \u03c9li:\n\u2202 \u2211N\nn=1 \u2211K i=1Rin [log p(yn|xn,mi) ]\n\u2202\u03c9li = 0, (7)\nand applying the derivate, we have:\n\u2212 N\u2211 n=1 Rin (p(yn|xn,mi)\u2212 yn)xn = 0. (8)\nIn this case, the classical technique of least-squares can not be directly applied because of the soft-max function in p(yn|xn,mi). Fortunately, as described in [18] and later in [26], Equation (8) can be approximated by using a transformation that implies inverting the soft-max function. Using this transformation, Equation (8) is equivalent to an optimization problem that can be solved using a weighted least squares technique [4]:\nmin \u03c9li\n\u2211N n=1Rin ( \u03c9Tlixn \u2212 log yn )2 (9)\nA similar derivation can be performed with respect to vectors \u03bdi. Again deriving Equation (5), in this case with respect to parameters \u03bdij and applying the transformation suggested in [18], we obtain:\nmin \u03bdi\n\u2211N n=1 ( \u03bdTi xn \u2212 logRin )2 (10)\n(11)\nOptimization of the regularized likelihood Following the procedure of [20], we add the regularization term to the optimization problem given by Equation (9), obtaining an expression that can be solved using quadratic programming [35]:\nmin \u03c9li\n\u2211N n=1Rin ( log yn \u2212 \u03c9Tlixn )2 subject to: ||\u03c9li||1 \u2264 \u03bb\u03c9 (12)\nSimilarly, we can also obtain a standard quadratic optimization problem to find\nparameters \u03bdij :\nmin \u03bdi\n\u2211N n=1 ( logRin \u2212 \u03bdTi xn )2 subject: to ||\u03bdi||1 \u2264 \u03bb\u03bd (13)\nA practical advantage of using quadratic programming is that most available optimization packages can be utilized to solve it [6]. Specifically, in the case of T iterations, there are a total of T \u2217K \u2217 (Q+ 1) convex quadratic problems related to the maximization step of the EM algorithm. To further reduce this computational load, we slightly modify this maximization by applying the following two-steps scheme:\n\u2022 Step-1: Solve K quadratic problems to find gate parameters \u03bdij assuming that\neach expert uses all the available dimensions. In this case, there are T \u2212 1 iterations.\n\u2022 Step-2: Solve K \u2217 (Q + 1) quadratic problems to find expert parameters \u03c9lij applying the feature selection process. In this case, there is a single iteration.\nUsing the previous scheme we reduce from T \u2217K \u2217(Q+1) to K \u2217(T +1)+K \u2217(Q+1) the number of quadratic problems that we need to solve in the maximization step of the EM algorithm. In our experiments, we do not notice a drop in performance by using this simplification, but we are able to increase processing speed in one order of magnitude.\nIn summary, starting by assigning random values to the relevant parameters \u03bdij and\n\u03c9lij, our EM implementation consists of iterating the following two steps:\n\u2022 Expectation: estimating responsabilities for each expert using Equation (4), and\nthen estimating the outputs of gate and experts using Equations (2) and (3).\n\u2022 Maximization: updating the values of parameters \u03bdij and \u03c9lij in Equations (12)\nand (13) by solving K \u2217 (T + 1) + K \u2217 (Q + 1) quadratic problems according to the approximation described above in Step-1 and Step-2."}, {"heading": "2. Expert Selection", "text": "The MoE o RMoE assumes that all the gate functions affects to every data. But for example in object detection, we can assume that there are some group of objects i.e. group of vehicles, animals, kitchen stuff, where each group is assigned to a gate function. We think that considering all groups of objects can confuse the classifiers. Therefore we propose to select a subset of gates function according to each data. We denominate this idea as a \u201cexpert selection\u201d.\nRecalling that the likelihood in regular mixture of experts is:\nL = N\u220f n=1 K\u220f i=1 p(yn|xn,mi)p(mi|xn) (14)\nNow, in order to select a gate, we change the multinomial logit representation of\nthe gate function (Equation 2) in this way:\np(mi|xn) = exp\u00b5in(\u03bd T i x)\u2211K\nj=1 exp\u00b5jn(\u03bd T j x)\n(15)\nwhere all the components of Equation 2 remain the same, except \u00b5. The variable \u00b5in \u2208 {0, 1}K is the vector of model parameters of the expert selector. It depends on data xn and expert i, where i \u2208 {1, . . . , K} for the set of expert gates. When \u00b5in = 1/0, it indicates that the gate i is relevant/irrelevant for data n. In the case of \u00b5in = 0, the value is constant and we can say that the data n is ignorant about expert i and assign a constant value. In this way, it is done the expert selection.\nIn order to use EM algorithm, we show the expected log-likelihood by considering the responsabilities, i.e. the posteriori probability of experts and the respective regularization terms with the addition of the term corresponding to the expert selector:\n\u3008Lc\u3009 = N\u2211 n=1 K\u2211 i=1 Rin [log p(yn|xn,mi) + log p(mi|xn)]\n\u2212\u03bb\u03bd K\u2211 i=1 D\u2211 j=1 |\u03bdij| \u2212 \u03bb\u03c9 Q\u2211 l=1 K\u2211 i=1 D\u2211 j=1 |\u03c9lij| \u2212 P (\u00b5) (16)\nThe penalization P depends on the regularization norm, mainly 0-norm or 1-norm.\nNow, we define the posteriori probability of the gates mi as:\nRin = p(yn|xn,mi)p(mi|xn)\u2211K j=1 p(yn|xn,mj) p(mj|xn)\n(17)\nNext, we repeat the strategy of Lee et al. by first optimizing the unregularized expected log-likelihood and then, adding the restriction. In order to facilitate the calculations, we define some auxiliary variables. As the derivative is linear in the sum, we calculate the contribution of a single data and call it as E \u2032:\nE \u2032 = \u2212log K\u2211 k=1 p(yn|xn,mk) p(mk|xn) (18)\nWe solve this process using an EM algorithm, where in the E-step, we calculate the responsabilities in this case by using the equation 17. In the M-step, we assume the responsabilities as known and we find the optimal parameters \u03bd, \u03c9 and \u00b5.\nSince the use of the responsability values, the term p(yn|xn,mk) can be evaluated separatevely and then the parameter \u03c9 can be optimized using the equation used in RMoE. In the case of p(mk|xn), by fixing the parameter \u00b5, we can optimize the parameter \u03bd.\nWe use some notations in order to facilitate the calculus: the term p(yn|xn,mk) as gnk , p(mk|xn) as hkn and exp(\u00b5in\u03bdixn) as zi, we derive the equation respect to \u03bdin for having:\n\u2202E \u2032 \u2202\u03bdi = \u2202E \u2032 \u2202zi \u2202zi \u2202\u03bdi \u2202E \u2032 \u2202\u03bdi = [ K\u2211 k=1 \u2202E \u2032 \u2202hk \u2202hk \u2202zi ] \u2202zi \u2202\u03bdi\n(19)\nNow we have three terms and we evaluate the derivative over each one :\n\u2202E \u2032 \u2202hk =\n\u2202 \u2212 log \u2211K\nj=1 gjhj\n\u2202hk \u2202E \u2032 \u2202hk = \u2212gk\u2211K j=1 gjhj \u2202E \u2032\n\u2202hk = \u2212Rkn hk (20)\n\u2202hk \u2202zi\n= \u2202 exp(hk)\u2211K j=1 exp(hj)\n\u2202zi \u2202hk \u2202zi = \u03b4kihi \u2212 hihk (21)\n\u2202zli \u2202\u03bdi = \u2202\u00b5i\u03bdix\n\u2202\u03bdi \u2202zli \u2202\u03bdi = \u00b5ix\nWe integrate these elements for obtain:\n\u2202E \u2032 \u2202\u03bdi = ( K\u2211 k=1 Rkn hk (\u03b4kihi \u2212 hihk) ) \u00b5ix \u2202E \u2032 \u2202\u03bdi = (Rin \u2212 hi)\u00b5ix (22)\nBy considering all the data, the regularization term and applying the trick of Bishop\nby taking the logarithms of the outputs and equaling to zero, we have:\nmin \u03bdi\n\u2211N n=1 ( (log(Rin)\u2212 \u03bdTi \u00b5inxn )2 subject: to ||\u03bdi||1 \u2264 \u03bb\u03bd (23)\nIn this case it is a modified version of equation 13 and we can apply a QP package to solve it. Finally, we fix the parameters \u03bd and \u03c9 for optimizing the parameter \u00b5. The\nregularization over the parameter of expert selector has originally norm 0; on the other hand, it can be relaxed bu considering norm 1. We state both approaches:"}, {"heading": "A. Optimization of \u00b5 considering norm 0", "text": "As the parameter \u00b5 depends on data xn, we need to solve the optimization problem:\nmin \u00b5in\n\u2212log \u2211K\nk=1 p(yn|xn,mk) p(mk|xn)\nsubject: to : ||\u00b5in||0 \u2264 \u03bb\u00b5 (24)\nThe minimization of equation 24 requires an exploration of CK\u03bb\u00b5 combinations, however, by assuming a low number of gates K < 50 and a lower number of active experts \u03bb\u00b5 < 10, this numerical optimization is feasible in practice."}, {"heading": "B. Optimization of \u00b5 considering norm 1", "text": "A more applicable approach is relaxing the constraint of 0-norm by replacing by the use of a 1-norm, also known as LASSO regularization. Given that \u00b5 is in the same component of \u03bd, its solution has many equal steps. In particular, we find almost the same equations. Using the same notations of Equation 19, we have for the individual log-likelihood:\n\u2202E \u2032 \u2202\u00b5in = \u2202E \u2032 \u2202zi \u2202zi \u2202\u00b5in \u2202E \u2032 \u2202\u00b5in = [ K\u2211 k=1 \u2202E \u2032 \u2202hk \u2202hk \u2202zi ] \u2202zi \u2202\u00b5in\n(25)\nWe get the same Equations 20 and 21. In the case of the last component we have:\n\u2202zli \u2202\u00b5in = \u2202\u00b5in\u03bdix\n\u2202\u00b5in \u2202zli \u2202\u00b5in = \u03bdix (26)\nWe ensemble all components equations to have:\n\u2202E \u2032 \u2202\u00b5in = ( K\u2211 k=1 Rkn hk (\u03b4kihi \u2212 hihk) ) \u03bdix \u2202E \u2032 \u2202\u00b5in = (Rin \u2212 hi) \u03bdix\nIn order to find the optimum parameter \u00b5in, we fix n and consider from i = 1 to K.\nBy equaling each equation to zero, we have:\n(Rin \u2212 hi) \u03bdix = 0 (27)\nNext, we approximate the previous equation using the logarithms over the outputs\n(Bishop):\n(log(Rin)\u2212 \u00b5i\u03bdix) \u03bdix = 0 (28)\nNow, we fix n in order to find jointly the parameters of \u00b5 for a fixed data n. Therefore\nwhen we add the K equations, we have an equation system:\n( K\u2211 i=1 (log(Rin)\u2212 \u00b5in\u03bdixn) \u03bdixn ) = 0\n(29)\nThis equation can be represented as a minimization problem considering the sum of squares residuals between log(Rin) and \u00b5in\u03bdixn; where we add restriction of norm 1 over \u00b5\u2217n that represents all selected experts for data n. In this case, we have:\nmin \u00b5 \u2016log(Rn)\u2212 \u00b5\u2217n\u03bdxn\u201622\nsubject: to ||\u00b5\u2217n||1 \u2264 \u03bb\u00b5 (30)\nThis equation can be solved with a quadratic program optimization package where the variable is \u00b5\u2217n. In the training phase, we require to solve this optimization N times. And in the test phase, it is necessary to solve this optimization problem for each test data.\nBy using norm 0 or 1, we can find the parameters of the expert selector. All the process is summarized as an EM algorithm where in the M-step, first, we freeze \u03bd and \u03c9 and find \u00b5; then we freeze \u00b5 and iterate for finding the local optimum \u03bd and \u03c9; then in the E-step, we find the responsabilities Rin using the new parameters \u03bd, \u03c9 and \u00b5. In the beginning, we initialize all parameters randomly. In the following section, we will detail the results of our experiments."}], "references": [{"title": "Dataset repository in arff", "author": ["J. Aguilar"], "venue": "http://www.upo.es/eps/aguilar/datasets.html", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "http://www.ics.uci.edu/\u223cmlearn/MLRepository.html", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on Neural Networks 5 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1994}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C. Bishop"], "venue": "Springer, New York, USA, 2nd edition", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, Cambridge, United Kingdom", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning 45 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 39 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1977}, {"title": "Pattern Classification", "author": ["R. Duda", "P. Hart", "D. Stork"], "venue": "Wiley-Interscience, USA, second edition", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "View-independent face recognition with hierarchical mixture of experts using global eigenspaces", "author": ["R. Ebrahimpour", "F.M. Jafarlou"], "venue": "Journal of Communication and Computer 7 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "in: Proceedings of the European Conference on Computational Learning Theory, Springer-Verlag, London, UK", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research 3 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Journal of Machine Learning 46 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Correlation-based Feature Selection for Machine Learning", "author": ["M. Hall"], "venue": "Ph.D. thesis, University of Waikato", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "A", "author": ["J. Hampshire"], "venue": "Waibel, The meta-pi network: Building distributed knowledge representations for robust multisource pattern recognition., IEEE Transactions Pattern Analysis and Machine Intelligence 14 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Adaptive mixtures of local experts", "author": ["R. Jacobs", "M. Jordan", "S. Nowlan", "G. Hinton"], "venue": "Neural Computation 3 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Hierarchical mixtures of experts and the EM algorithm", "author": ["M. Jordan", "R. Jacobs"], "venue": "Neural Computation 6 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G. John"], "venue": "Artificial Intelligence 97 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Arizona state university: Feature selection datasets", "author": ["H. Liu"], "venue": "http://featureselection.asu.edu/datasets.php", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Chi2: Feature selection and discretization of numeric attributes", "author": ["H. Liu", "R. Setiono"], "venue": "in: J. Vassilopoulos (Ed.), Proceedings of the International Conference on Tools with Artificial Intelligence, IEEE Computer Society, Herndon, Virginia", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Probable networks and plausible predictions \u2013 a review of practical Bayesian methods for supervised neural networks", "author": ["D. MacKay"], "venue": "Network: Computation in Neural Systems 6 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1995}, {"title": "Some Methods for Training Mixtures of Experts", "author": ["P. Moerland"], "venue": "Technical Report, IDIAP Research Institute", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "A system for induction of oblique decision trees", "author": ["S.K. Murthy", "S. Kasif", "S. Salzberg"], "venue": "Journal of Artificial Intelligence Research 2 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "A novel mixture of experts model based on cooperative coevolution", "author": ["M. Nguyen", "H. Abbass", "R. McKay"], "venue": "Neurocomputing 70 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Penalized model-based clustering with application to variable selection", "author": ["W. Pan", "X. Shen"], "venue": "Journal of Machine Learning Research 8 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D.D. Cox", "J. DiCarlo"], "venue": "PLoS Computational Biology 4 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "C4.5: programs for machine learning", "author": ["J. Quinlan"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1993}, {"title": "Deformable model fitting with a mixture of local experts", "author": ["J. Saragih", "S. Lucey", "J. Cohn"], "venue": "International Conference on Computer Vision ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-dimensional density estimation", "author": ["D. Scott", "S. Sain"], "venue": "Multi-Dimensional Density Estimation, Elsevier, Amsterdam", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society (Series B) 58 ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1996}, {"title": "A", "author": ["M. Titsias"], "venue": "Likas, Mixture of experts classification using a hierarchical mixture model., Neural Computation 14 ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Convergence of block coordinate descent method for nondifferentiable maximization", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Applications 109 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "Information Retrieval", "author": ["C. Van-Rijsbergen"], "venue": "Butterworth-Heinemann, London, UK, 2nd edition", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1979}, {"title": "Variable selection for model-based high dimensional clustering and its application to microarray data", "author": ["S. Wang", "J. Zhu"], "venue": "Biometrics 64 ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Winner-take-all mechanisms", "author": ["A. Yuille", "D. Geiger"], "venue": "in: M.A. Arbib (Ed.), The handbook of brain theory and neural networks, MIT Press, Cambridge, MA, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 15, "context": "Specifically, our approach takes advantage of the linear nature of each local expert and gate function in the classical MoE formulation [17], meaning that L1 regularization can be directly applied.", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "From a probabilistic point of view [4], a useful approach to find f is using a conditional formulation:", "startOffset": 35, "endOffset": 38}, {"referenceID": 28, "context": "This is similar to the case of modeling a mixture distribution [34] and it leads to the MoE model.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "The traditional MoE technique uses multinomial logit models, also known as softmax functions [4], to represent the gate and expert functions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 34, "context": "The competition in soft-max functions enforces the especialization of experts in different areas of the input space [41].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "There are several methods to find the value of the hidden parameters \u03bdij and \u03c9lij [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Specifically, the posterior probability of the responsibility Rin assigned by the gate function to expert mi for an instance xn is given by [26]:", "startOffset": 140, "endOffset": 144}, {"referenceID": 21, "context": "Rin = p(mi|xn, yn) (4) = p(yn|xn,mi) p(mi|xn) \u2211K j=1 p(yn|xn,mj) p(mj|xn) Considering these responsibilities and Equation (1), the expected complete loglikelihood \u3008Lc\u3009 used in the EM iterations is [26]:", "startOffset": 197, "endOffset": 201}, {"referenceID": 24, "context": "Similar consideration is used in the work of [29] but in the context of unsupervised learning.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "To maximize Equation (6) with respect to model parameters, we use first the standard fact that the likelihood function in Equation (5) can be decomposed in terms of independent expressions for gate and expert models [26].", "startOffset": 216, "endOffset": 220}, {"referenceID": 21, "context": "In this way, the maximization step of the EM based solution can be performed independently with respect to gate and expert parameters [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 31, "context": "As shown in [20], this problem can be solved by using a coordinate ascent optimization strategy [37] given by a sequential two-step approach that first models the problem as an unregularized logistic regression and afterwards incorporates the regularization constraints.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "Fortunately, as described in [18] and later in [26], Equation (8) can be approximated by using a transformation that implies inverting the soft-max function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Fortunately, as described in [18] and later in [26], Equation (8) can be approximated by using a transformation that implies inverting the soft-max function.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "Using this transformation, Equation (8) is equivalent to an optimization problem that can be solved using a weighted least squares technique [4]:", "startOffset": 141, "endOffset": 144}, {"referenceID": 16, "context": "Again deriving Equation (5), in this case with respect to parameters \u03bdij and applying the transformation suggested in [18], we obtain:", "startOffset": 118, "endOffset": 122}, {"referenceID": 29, "context": "min \u03bdi \u2211N n=1 ( \u03bd i xn \u2212 logRin )2 (10) (11) Optimization of the regularized likelihood Following the procedure of [20], we add the regularization term to the optimization problem given by Equation (9), obtaining an expression that can be solved using quadratic programming [35]:", "startOffset": 274, "endOffset": 278}, {"referenceID": 4, "context": "subject: to ||\u03bdi||1 \u2264 \u03bb\u03bd (13) A practical advantage of using quadratic programming is that most available optimization packages can be utilized to solve it [6].", "startOffset": 156, "endOffset": 159}], "year": 2014, "abstractText": "A useful strategy to deal with complex classification scenarios is the \u201cdivide and conquer\u201d approach. The mixture of experts (MOE) technique makes use of this strategy by joinly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weights their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, for the case of high dimensional data. Our main intuition is that particular subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using L1 regularization, with a simultaneous expert selection. The experiments are still pending.", "creator": "LaTeX with hyperref package"}}}