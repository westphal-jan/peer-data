{"id": "1406.1061", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2014", "title": "A Methodology for Empirical Analysis of LOD Datasets", "abstract": "CoCoE stands for Complexity, Coherence and Entropy, and presents an extensible methodology for empirical analysis of Linked Open Data (i.e., RDF graphs). CoCoE can offer answers to questions like: Is dataset A better than B for knowledge discovery since it is more complex and informative?, Is dataset X better than Y for simple value lookups due its flatter structure?, etc. In order to address such questions, we introduce a set of well-founded measures based on complementary notions from distributional semantics, network analysis and information theory. These measures are part of a specific implementation of the CoCoE methodology that is available for download. Last but not least, we illustrate CoCoE by its application to selected biomedical RDF datasets.", "histories": [["v1", "Wed, 4 Jun 2014 14:45:43 GMT  (296kb,D)", "http://arxiv.org/abs/1406.1061v1", "A current working draft of the paper submitted to the ISWC'14 conference (track information available here:this http URL)"]], "COMMENTS": "A current working draft of the paper submitted to the ISWC'14 conference (track information available here:this http URL)", "reviews": [], "SUBJECTS": "cs.AI cs.SI", "authors": ["vit novacek"], "accepted": false, "id": "1406.1061"}, "pdf": {"name": "1406.1061.pdf", "metadata": {"source": "CRF", "title": "CoCoE: A Methodology for Empirical Analysis of LOD Datasets", "authors": ["V\u0131\u0301t Nov\u00e1\u010dek"], "emails": ["vit.novacek@deri.org"], "sections": [{"heading": "1 Introduction", "text": "As the LOD cloud is growing, people increasingly often face the problem of choosing the right dataset(s) for their purposes. Data publishers usually provide descriptions that can indicate possible uses of their datasets, however, such asserted descriptions may often be too shallow, subjective or vague. To complement the dataset descriptions authored by their creators, maintainers or users, we introduce a comprehensive set of empirical quantitative measures that are based on the actual content of the datasets. Our main goal is to provide means for comparison of RDF datasets along several well-founded criteria, and thus determine the most appropriate datasets to utilise in specific use cases.\nTo motivate and illustrate our contribution from a practical point of view, imagine a researcher Rob working on a novel method for discovering drug side effects. Rob knows that the most successful methods typically define and train a model in order to discover unknown side effects of drugs using their known features [1]. Rob also knows there are datasets in the LOD cloud that can be used for defining features that may not be captured by the state of the art approaches. Moreover, due to the common RDF format, the datasets can relatively easily be combined to generate completely new sets of features. Therefore using relevant LOD data can lead Rob to a breakthrough in adverse drug effect discovery.\n? This work has been supported by the \u2018KI2NA\u2019 project funded by Fujitsu Laboratories Limited in collaboration with Insight @ NUI Galway. We also want to thank Va\u0301clav Bela\u0301k for valuable discussions regarding applicable clustering algorithms.\nar X\niv :1\n40 6.\n10 61\nv1 [\ncs .A\nI] 4\nJ un\n2 01\nExamples of such data are DrugBank, SIDER and Diseasome (c.f., http:// datahub.io/dataset/fu-berlin-[drugbank|sider|diseasome]). They describe drugs, medical conditions, genes, etc. The question is how to use the datasets efficiently. Rob may wonder how much information can he typically gain from the datasets and which one is the best in this respect. Which of them is better for extracting flat features based on predicate-object pairs, and which is better for features based on more complex structural patterns? Last but not least, it may be useful to know what happens if one combines the datasets. Maybe it will bring more interesting features, and maybe nothing will change much, only the data will become larger and more difficult to process.\nCoCoE provides a well-founded methodology for empirical analysis of RDF data which can be used to determine applicability of the data to particular use cases (like Rob\u2019s in the motivating example above). The methodology is based on sampling the datasets with quasi-random heuristic walks that simulate various exploratory strategies of real agents. For each sample (i.e., walk), we compute a set of measures that are then averaged across all the samples to approximate the overall characteristics of the dataset. We define three types of measures: complexity, coherence and entropy. The purpose of the measures is to assess datasets along complementary perspectives that can be quantified in a well-founded and easy-to-interpret manner. The perspectives chosen and their possible combinations cover a broad area of use cases in which RDF datasets can possibly be applied, ranging from simple value look-ups through semantic annotations to complex knowledge discovery tasks.\nThe CoCoE methodology can obviously be implemented in many different ways, but here we describe only one specific realisation. For the complexity measures, we use network analysis algorithms [2]. For the coherence measures, two auxiliary structures are required. Firstly, we need a distributional representation of the RDF data [3], which describes each entity (subject or object) by a vector that represents its meaning based on the entities linked to it. Secondly, we need a taxonomy of nodes in the RDF (multi)graph, which is computed from the data itself by means of nonparametric hierarchical clustering. These two structures allow for representing coherence using various types of semantic similarities based on the vector space representation and the taxonomy structure, such as cosine or Wu-Palmer [4]. The taxonomy structure also serves as a basis for the entropies computed using cluster annotations of the nodes in the walks.\nThe rest of the paper is organised as follows. Section 2 summarises the related work. Details on the CoCoE methodology and its implementation are given in Section 3. Section 4 presents an experimental illustration of the CoCoE approach. We conclude the paper in Section 5."}, {"heading": "2 Related Work", "text": "The distributional representation of RDF data we use builds on our previous work [3]. We have recently introduced the notion of heuristic quasi-random walks and their empirical analysis in [5], which, however, deals with different\ntypes of data, manually curated taxonomies and predefined gold standards. The presented paper extends that work into a generally applicable methodology for analysing RDF datasets using only the data itself.\nThe clustering method introduced here builds on principles similar to k-hop clustering [6]. Another related approach is nonparametric hierarchical link clustering [7], which is more general and sophisticated than our simple method, yet its Python implementation we have experimented with proved to be intractable when used in our experiments. A comprehensive overview of semantic similarity measures applicable in CoCoE is provided in [4]. The similarities used in our experiments were the cosine and Wu-Palmer ones, chosen as representatives of the vector space-based and taxonomy-based similarity types.\nThe most relevant tools and approaches for RDF data analysis are [8,9,10,11]. Perhaps closest to our work is [9] that computes a set of statistics and histograms for a given RDF dataset. The statistics are, however, concerned mostly about distributions of statements, instances and explicit statement patterns. This may be useful for tasks like SPARQL query optimisation, but cannot directly answer questions that motivate our work. Graph summaries [8] propose high-level abstractions of RDF data intended to facilitate formulation of SPARQL queries, which is orthogonal to our approach aimed at quantitative characteristics of the data itself. Usage-based RDF data analysis [10] provides insights into common patterns of utilising RDF data by agents, but does not offer means for actually analysing the data. Finally, the recent approach [11] is useful for knowledge discovery in RDF data based on user-defined query patterns and analytical perspectives. Our approach complements [11] by characterising application-independent features of RDF datasets taken as a whole."}, {"heading": "3 Methods", "text": "In this section, we first introduce various RDF data representations that underlie the CoCoE methodology. Then we describe the clustering method used for computing taxonomies that are needed for certain CoCoE measures. The concept of heuristic quasi-random walks is described then, followed by details on the CoCoE measures. Finally, we explain how to interpret the measure values."}, {"heading": "3.1 Representations of RDF Datasets", "text": "Let us assume an RDF dataset consisting of triples (s, p, o) that range over a set of URIs U and literals L such that s, p \u2208 U , o \u2208 U \u222a L. A direct graph representation of the dataset is a directed labelled multigraphGd = (V,Ed, Ld) where V = U\u222aL is a set of nodes corresponding to the subjects and objects, Ed is a set of ordered pairs (u, v) \u2208 V \u00d7V , and Ld : Ed \u2192 U is a function that assigns a predicate label p to every edge (s, o) such that (s, p, o) exist in the dataset. Note that we do not distinguish between URI and literal objects in the current implementation of CoCoE as we are interested in the most generic schemaindependent features of the datasets. An example of a direct graph representation is given in Figure 1. The left hand side of the figure contains RDF statements\ncoming from DrugBank and Diseasome. For better readability, we represent the statements as simple tuples, with the DB and DS abbreviations referring to the corresponding namespaces. We also use symbolic names instead of alphanumeric IDs present in the original data. In the graph representation in the right hand side of the figure, arginine, Alzheimer disease, APOE and urokinase correspond to the A, B, C and D codes, respectively. Similarly, the predicates possibleDiseaseTarget, possibleDrug and associatedGene correspond to the r, s and t codes, respectively. Drug entities are displayed in white, while disease and gene are in dark and light grey, respectively.\nNext we define a distributional representation of an RDF dataset as a matrix M . The row indices of M correspond to the set V of nodes in the dataset\u2019s direct graph representation Gd. The column indices represent the context of the nodes by means of their connections in the Gd graph, and are defined as a union of two sets of pairs corresponding to all possible outgoing and incoming edges: {(Ld((x, y)), y)|x \u2208 V \u2227 (x, y) \u2208 Ed} \u222a {(x, Ld((x, y)))|y \u2208 V \u2227 (x, y) \u2208 Ed}. The values of the M matrix indexed by a row a and column (b, c) are 1 if there is an edge (a, c) with predicate label b or an edge (b, a) with predicate label c in Gd, and 0 otherwise. The dataset from the previous example corresponds to the following distributional representation:\n(r,B) (s,D) (t,C) (A,r) (B,s) (B,t) (D,r)\nA 1 0 0 0 0 0 0 B 0 1 1 1 0 0 1 C 0 0 0 0 0 1 0 D 1 0 0 0 1 0 0 .\nThe rows of the matrix can be used for computing similarities between particular data items using measures like cosine distance. Let us use a notation x to refer to a row vector in M corresponding to the entity x (i.e., a subject or object in the original dataset). Then the cosine similarity between two entities x, y is: simcos(x, y) = x\u00b7y |x||y| . For instance, the similarity between A, D (i.e., arginine and urokinase) is simcos(A, D) = 1\u221a 1 \u221a 2 ' 0.707.\nFor larger datasets, it is practical to use dimensionality reduction to facilitate computations utilising the distributional representation. In the experiments presented here, we used a simple method for ranking and filtering out the columns \u2013 the \u03c72 statistic [12], which can be used for computing divergence of specific observations from expected values. In our case, observations are the columns in a distributional representation, and their values are the column frequencies in the data. More formally, let us assume an m \u00d7 n distributional representation M with row index set V = {v1, v2, . . . , vm} and column index\nset C = {c1, c2, . . . , cn}. Then the expected (i.e., mean) and observed values for the \u03c72 statistic are E(M) = 1|C| \u2211 r\u2208V,c\u2208CMr,c, and O(ci,M) = \u2211 r\u2208V Mr,ci (for a column ci). Using these formulae, the \u03c7 2 statistic of a column ci is \u03c72(ci,M) = (O(ci,M)\u2212E(M))2 E(M) . The \u03c7 2 values for the columns in our example distributional representation are as follows. The expected value is 87 ' 1.14 (sum of all values in the matrix divided by the number of columns). All the columns but (r,B) have \u03c72 value of 156 ' 0.02. The (r,B) column has \u03c7\n2 value of 914 ' 0.64. Therefore one can consider (r,B) as the only significant column. The similarity between the A and D entities then increases to 1 as their corresponding vectors, reduced to the only significant dimension, are equal.\nIn addition to reducing the dimensionality, we use the \u03c72 scores to construct weighted indirect representations of RDF datasets, Gw = (V,Ew, Lw). Gw is an undirected graph with node set V and edge set Ew that consists of 2- multisets of elements from V . The Ew set is constructed from the corresponding direct graph representation Gd as {{u, v}|(u, v) \u2208 Ed\u2228(v, u) \u2208 Ed}. The labeling function Lwu : Ew \u2192 R associates the edges with a weight that is computed as maximum from the values {\u03c72((p, u),M)|p \u2208 PI} \u222a {\u03c72((p, v),M)|p \u2208 PO} \u222a {\u03c72((u, p),M)|p \u2208 PO} \u222a {\u03c72((v, p),M)|p \u2208 PI}, where PI , PO are sets of RDF predicates linking v to u and u to v, respectively. Figure 2 shows how the direct graph representation can be turned into the indirect weighted one.\nThe last structure we need for computing the CoCoE measures is a similarity representation Gs = (V,Es, Ls). Similarly to Gw, Gs is a weighted undirected graph. It captures the similarities between the entities in the corresponding RDF dataset. The edge set Es is defined as {{u, v}|simcos(u,v) > }, where u,v are the vectors in the dataset\u2019s distributional representation M and \u2208 [0, 1) is a threshold. The edge labeling function Ls : Es \u2192 (0, 1] then assigns the actual similarities to the particular edges. Our example dataset has a sparse similarity representation, as most of the similarities are 0, except of simcos(A,D) = simcos(D,A) = 1 (or \u221a 2 2 when using all dimensions)."}, {"heading": "3.2 Nonparametric Hierarchical Clustering", "text": "To compute many of the CoCoE measures, a taxonomy of the nodes in the RDF data representations is required. In some domains, standard, manually curated taxonomies exist (such as MeSH in life sciences, c.f., http://download. bio2rdf.org/current/mesh/mesh.html). Unfortunately, such authoritative resources are not available for most domains, or they do not cover many RDF\ndatasets sufficiently. Therefore we devised a simple algorithm that computes a hierarchical cluster structure (i.e., taxonomy) based on traversing the graph representations of the data. We compute two taxonomies Tw, Ts based on the Gw, Gs representations, respectively. Tw is based on the data representation directly, while Ts captures the taxonomy induced by the entity similarities.\nThe most specific (i.e., leaf-level) clusters are computed as follows (using the corresponding G? = (V,E?, L?) representation where ? is one of w, s):\n1. Compute a list L of nodes v \u2208 V ranked according to their clustering coefficients 2\u03bbG? (v)\n|a(v)|(|a(v)|\u22121) , where \u03bbG?(v) is the number of complete subgraphs of\nG? containing v, and a(v) is a set of neighbours of v in G? (we use clustering coefficient as a simple quantification of node complexity and their potential for spawning clusters). 2. Set a cluster identifier i to 0 and initialise a mapping \u03bd : N \u2192 2V between cluster identifiers and corresponding node sets. 3. While L is not empty, do: (a) Pick a node x with the highest rank from L. (b) Set cluster \u03bd(i) to a set of nodes {u|\u03a0e\u2208p(x,u)L?(e) > } where p(x, u) is a\nset of edges on a path between the nodes x, u in G?, and is a predefined threshold (in our experiments, we set the threshold dynamically to ca. 75th percentile of the actual edge weights in the given graph).\n(c) Remove all \u03bd(i) nodes from L and increment i by 1. 4. Return the cluster-to-nodes mapping \u03bd.\nClusters of level k are computed using the above algorithm from clusters at the level k\u22121, continually adding new cluster identifiers. The algorithm is, however, applied on an undirected weighted cluster graph Gc = (Vc, Ec, Lc) and generates the higher-level clusters by unions of nodes associated with the lower level ones. The Gc graph for a level k is defined as follows. Let us assume that the level k \u2212 1 consists of n clusters c1, c2, . . . , cn that correspond to sets of nodes \u03bd(c1), \u03bd(c2), . . . , \u03bd(cn). Then the node set Vc for level k equals to {c1, c2, . . . , cn} and the edge set Ec is computed as {{u, v}|\u2203x\u2203y.x, y \u2208 Vc\u2227(u \u2208 \u03bd(x)\u2229\u03bd(y)\u2228v \u2208 \u03bd(x) \u2229 \u03bd(y))}. The weight labeling Lc assigns a weight to each edge in Ec according to the following formula: Lc({x, y}) = 13 (2 \u2211 e\u2208E\u2217 L?(e)+ \u2211 e\u2208E+ L?(e)), where L? is the weight labeling function of the corresponding G? graph representation, and E\u2217, E+ are sets of edges in G? that are fully and partially covered by the nodes in the \u03bd(x) \u2229 \u03bd(y) intersection (full coverage means that both nodes of an edge are in the intersection, while the partial coverage requires exactly one edge node to be present there). It is easy to see that the Gc graphs connect clusters that have non-empty node overlap. The weights of the connections are computed as a weighted arithmetic mean of the weights of edges with nodes in the cluster intersections, where the edges with both nodes in the intersection contribute twice as much as the edges with only one node there.\nThe final product of the clustering algorithm is a mapping between cluster identifiers and corresponding sets of nodes. As the more specific (i.e., lower-level) clusters are incrementally merged into more abstract ones, each node can be assigned a set of so called tree codes that reflect its membership in the particular\nclusters. The tree codes have the form L1.L2. . . . .Ln\u22121.Ln where Li are identifiers of clusters of increasing specificity (i.e., L1, Ln are the most general and specific, respectively). For the CoCoE measures, we sometimes consider only the top-level cluster identifiers which we denote by CXT for an entity X. The notation CXS refers to the set of all specific cluster identifiers associated with an entity X.\nTo give an example of how the clustering works, let us assume the threshold is set to the minimum of the graph weights at each level of the clustering. Considering the dataset from the previous examples, the computation of the initial clusters according to the Gw representation can start from any node as their clustering coefficient is always zero. Let us start from the node A then. The corresponding hierarchical clustering process is depicted in Figure 3, together with the resulting cluster structure (i.e., dendrogram). The value of in the first step\nis 0.02 and thus the clustering puts the nodes A, B, D into a cluster C1 first, proceeding from C then and creating a cluster C2 consisting of C, B. No other traversals are possible as the multiplied edge weights fall below the threshold already. The next level uses the {B,C} edge weight 0.02 as the threshold again as it is the only edge connecting the C1, C2 clusters. The top-most cluster C3 is a union of the C1, C2 ones. The resulting sets of tree codes are: {C3.C1} for nodes A, D; {C3.C2} for node C; {C3.C1, C3.C2} for node B.\nFor some of the measures defined later on, we need a notion of the number and size of clusters. Let us assume a set of entities Z \u2286 V . The number of clusters associated with the entities from Z, cn(Z), is then cn(Z) = | \u22c3 x\u2208Z C x ? | where ? is one of T, S (depending on whether we are interested in the top or specific clusters, respectively). The size of a cluster Ci \u2208 Cx? , cs(Ci), is an absolute frequency of the mentions of Ci among the clusters associated with the entities in Z. More formally, cs(Ci) = |{x|x \u2208 Z \u2227 Ci \u2208 Cx? }|.\nThe taxonomies can be used for defining taxonomy-based similarity that reflects the closeness of entities depending on which clusters they belong to. The similarity is simtax(x, y) = max({ 2\u00b7dpt(lcs(u,v))dpt(u)+dpt(v) | u \u2208 C x S , v \u2208 C y S}), where the specific tree codes in CxS , C y S are interpreted as nodes in the taxonomy induced by the dataset\u2019s hierarchical clustering. The lcs function computes the least common subsumer of two nodes in the taxonomy and dpt is the depth of a node in the taxonomy (defined as zero if no node is supplied as an argument, i.e., if lcs has no result). The formula we use is essentially based on a popular Wu-Palmer similarity measure [4]. We only maximise it across all possible cluster annota-\ntions to find the best match (as the data are supposed to be unambiguous, such a strategy is safe). To illustrate the taxonomy-based similarity, let us assume the hierarchical clusters from the previous example: {C3.C1} for nodes A, D; {C3.C2} for node C; {C3.C1, C3.C2} for node B. The taxonomy-based similarities between the nodes are then as follows: simtax(A, C) = simtax(D, C) = 0.5 (the taxonomy root is their least common subsumer), simtax(B, C) = simtax(B, A) = simtax(B, D) = simtax(A, D) = 1 (the nodes are siblings)."}, {"heading": "3.3 Heuristic Quasi-Random Walks", "text": "The CoCoE measures of a dataset are computed using its Gw representation on which we execute multiple heuristic quasi-random walks defined as follows. Let l be a natural number and h : V \u2192 V a heuristic function that selects a node to follow for any given node in Gw. Then a heuristic quasi-random walk on Gw of length l according to heuristic h is an ordered tuple W = (v, h(v), h(h(v)), . . . , hl\u22121(v), hl(v)) where v is a random initial node in Gw. The walks simulate exploration of RDF datasets, either by a human user browsing the corresponding graph, or by an automated traversal and/or query agent. We use the indirect representation to cater for a broader range of possible traversal strategies (agents can easily explore the subject-predicate-object links in both directions, for instance by means of describe queries). By running a high number of walks, one can examine characteristic patterns of the dataset much earlier then by an exhaustive exploration of all possible connections (which is generally in the O(n!) range w.r.t. the number of entities). Formal bounds of representativeness implied by a specific number of random walks are currently an open problem. However, our experiments suggest that a number ensuring representative enough sampling can be easily determined empirically.\nTo simulate different types of exploration, we can define various heuristics. For a given input node v, all heuristics compute a ranked list of the neighbours of v. The list is then iteratively processed (starting with the highest-ranking neighbour), attempting to select the next node with a probability that is inversely proportional to its rank. If no node has been selected after processing the whole list, a random neighbour is picked. The distinguishing factor of the heuristics are the criteria for ranking the neighbour list. We employed the following selection preferences in our experiments: (1) nodes that have not been visited before (H1); (2) unvisited nodes connected by edges with higher weight (H2); (3) unvisited nodes that are more similar to the current one, using the simtax similarity introduced before (H3); (4) unvisited nodes that are less similar (H4). H1 simulates more or less random exploration that, however, prefers unvisited nodes. H2 follows more significant relations. Finally, H3 and H4 are dual heuristics, with H3 simulating exploration of topics related to the current node and H4 attempting to cover as many topics as possible.\nEach walk W can be associated with an envelope e(W, r) with a radius r, which is a sub-graph of Gw limited to a set of nodes V r W . V r W represents a neighbourhood of the walk and is defined as \u22c3 u\u2208W {v|v \u2208 V \u2227 |pGw(u, v)| \u2264 r} where pGw(u, v) is a shortest path between nodes u, v in Gw. The envelope is used\nfor computing the complexity and entropy measures later on, as it corresponds to the contextual information available to agents along a walk."}, {"heading": "3.4 CoCoE Measures", "text": "Having introduced all the preliminaries, we can finally define the measures used in our sample implementation of CoCoE. The first type of measures is based on complexity of the graph representations. We distinguish between local and global complexities. The global ones are associated with the graphs as a whole, and we compute specifically graph diameters, average shortest paths and node distributions along walks. The local measures associated with the walk envelopes are: (A) envelope size in nodes; (B) envelope size in biconnected components; (C) average component size in nodes; (D) average clustering coefficient of the walk nodes w.r.t. the envelope graph.\nThe coherences of walks are based on similarities. Let us assume a sequence of v1, v2, . . . , vn walk nodes. Then the particular coherences are: (E) taxonomybased start/end coherence simtax(v1, vn); (F) taxonomy-based product coherence \u03a0i\u2208{1,...,n\u22121} simtax(vi, vi+1); (G) average taxonomy-based coherence 1 n\u22121\u2211\ni\u2208{1,...,n\u22121} simtax(vi, vi+1); (H) distributional start/end coherence simcos(v1, vn); (I) distributional product coherence \u03a0i\u2208{1,...,n\u22121}simcos(vi, vi+1); (J) av-\nerage distributional coherence 1n\u22121 \u2211 i\u2208{1,...,n\u22121} simcos(vi, vi+1). This family of measures helps us to assess how topically convergent (or divergent) are the walks. To compute walk entropies, we use the Tw, Ts taxonomies. By definition, the higher the entropy of a variable, the more information the variable contains. In our context, a high entropy value associated with a walk means that there is a lot of information available for agents to possibly utilise when processing the graph. The entropy measures we use relate to the following sets of nodes and types of clusters representing the context of the walks: (K) walk nodes only, top clusters; (L) walk nodes only, specific clusters; (M) walk and envelope nodes, top clusters; (N) walk and envelope nodes, specific clusters. The entropies of the sets (K-N) are defined using the notion of cluster size (cs(. . . )) introduced before. Given a set Z of nodes of interest, the entropy H(Z) is computed as\nH(Z) = \u2212 \u2211 Ci\u2208C?(Z) cs(Ci)\u2211 Cj\u2208C?(Z) cs(Cj) \u00b7 log2 cs(Ci)\u2211 Cj\u2208C?(Z) cs(Cj) , where ? is one of T, S, for top or specific clusters, respectively."}, {"heading": "3.5 Interpreting the Measures", "text": "Generally speaking, high complexity means a lot of potentially useful structural information, but also more expensive search (e.g., by means of queries) due to high branching factors among the nodes, and the other way around. High coherence means that in general, any exploratory walk through the dataset tends to be focused in terms of topics covered, while low coherence indicates rather serendipitous nature of a dataset where exploration tends to lead through many different topics. Finally, high entropy means more information and also less predictable topic distributions along the nodes in the walks and envelopes, with balanced\ncluster cardinalities. Low entropy means high predictability of the node topics (in other words, strongly skewed cluster cardinalities).\nPossible combinations of measures can be enumerated as follows. Let us refer to comparatively higher and lower measures by the \u2191 and \u2193 symbols. Then the combinations of relative complexity, coherence and entropy measures, respectively, are: 1, \u2191\u2191\u2191: Complex patterns and informative topic annotations about focused subject domains. 2, \u2191\u2191\u2193: Focused around unevenly distributed sets of topics with complex structural information context. 3, \u2191\u2193\u2191: Serendipitous, a lot of equally significant complex contextual information. 4, \u2193\u2191\u2191: Focused, with balanced and simple contextual information. 5, \u2191\u2193\u2193: Serendipitous with complex contextual topics of uneven cardinality. 6, \u2193\u2191\u2193: Focused with simple uneven contexts. 7, \u2193\u2193\u2191: Serendipitous with simple balanced contexts. 8, \u2193\u2193\u2193: Serendipitous with simple uneven contexts.\nSome of the specific measure combinations may be particularly (un)suitable for certain use cases. To give few non-exhaustive examples, the combination 1, \u2191\u2191\u2191 is suitable for knowledge discovery about focused subject domains, but also challenging for querying. Combination 3, \u2191\u2193\u2191 is good for serendipitous browsing. Combination 4, \u2193\u2191\u2191 may be useful for semantic annotations of a set of core domain entities as it provides for simple lookups of focused and balanced contextual information. Similarly, combination 7, \u2193\u2193\u2191 may be more applicable for annotations of varied domain entities."}, {"heading": "4 Experiments", "text": "In this section, we first present settings of experiments with CoCoE applied to sample RDF datasets. Then we report on results of the experiments and discuss their interpretation. Note that the implementation of the CoCoE methodology used in the experiments, including the corresponding data and scripts, is available at http://goo.gl/Wxnb3B."}, {"heading": "4.1 Datasets and Settings", "text": "The datasets we used were: 1. DrugBank \u2013 information on marketed drugs, including indications, chemical and molecular features, manufacturers, protein bindings, etc.; 2. SIDER \u2013 information on drug side effects; 3. Diseasome \u2013 a network of disorders and associated genes; 4. all \u2013 an aggregate of the DrugBank, SIDER and Diseasome datasets using the DrugBank URIs as a core vocabulary to which the other datasets are mapped. The dataset selection was motivated by our recent work in adverse drug effect discovery, for which we have been compiling a knowledge base from relevant biomedical Linked Open Data [13]. One of the main purposes of the knowledge base is to extract features applicable to training adverse effect discovery models. In this context, we were interested in characteristics of the knowledge bases corresponding to the isolated and merged datasets, yet we lacked the means for measuring this. Therefore we decided to use the knowledge bases being created in [13] as a test case for CoCoE.\nFor each dataset, we generated: (1) The direct graph and distributional representations Gd,M , with M reduced to 250 most significant dimensions according to their \u03c72 scores. (2) The weighted indirect and similarity representations Gw, Gs, taking into account only similarity values above 0.5. (3) Taxonomies Tw, Ts based on the Gw, Gs graph clustering, respectively.\nThe quasi-random heuristic walks were ran using all combinations of the following parameters for each dataset: (1) Walk lengths l \u2208 {2, 10, 20}. (2) Envelope diameters r \u2208 {0, 1}. (3) Heuristics h \u2208 {H1, H2, H3, H4} (i.e., random, weight, similarity and dissimilarity preference). The number of samples (i.e., walk executions per a parameter combination) was |V |k(l+1) , where |V |, l are the number of graph nodes and the walk length in the given experimental batch, respectively, and k is a constant equal to the average shortest path length in the graphs, truncated to integer value. In our experiments, the observed relative trends were stable after reaching this number of repetitions and therefore we took it as a sufficient \u2018sampling rate.\u2019"}, {"heading": "4.2 Results", "text": "Figure 4 gives an overview of how the specific heuristics perform per each dataset regarding the node visit frequency. The x-axis reflects the ranking of nodes\naccording to the number of visits to them. The y-axis represents the visit frequencies. Both axes are log-scale, since all the distributions have very steep long tails. The prevalent trends in the plots are: 1. The heuristics H2 and H3 (edge weight and similarity preference), especially when using the Tw taxonomies, tend to have generally more long-tail distributions than the others (the pattern is most obvious in the Diseasome dataset). 2. The H4 heuristic, using the Tw taxonomy, has the most even distribution. 3. The heuristics using the Ts taxonomies\ntend to have very similar node visit frequency distributions, close to H1 that exhibits the most \u2018average\u2019 behaviour (presumably due to its highest randomness). 4. The heuristics seem to follow similar patterns in the DrugBank and all datasets. 5. In SIDER, the behaviour of the heuristics appears to be most irregular (for instance, the random heuristic H1 behaves differently for Tw and Ts taxonomies although the taxonomy used should not have any influence on that heuristic).\nTable 1 summarises global characteristics of the datasets and the corresponding Gw graph representations. |V |, |E| are numbers of nodes and edges in\nGw, respectively, D is the graph density (defined as D = 2\u00b7|E|\n|V |(|V |\u22121) ), d is the\ngraph diameter, lG is the average shortest path length and |C| is the number of connected components. All graphs have so called small world property [2], as their densities are rather small and yet there is very little separation between any two nodes in the graph in general. This typically happens in highly complex graphs with a lot of interesting patterns in them.\nFigure 5 presents plots of the complexity measures based on the walk sampling. The x-axis represents the combinations of experimental parameters,\ngrouped by the type of heuristic \u2013 the 1.,2.,3. and 4. horizontal quarters of the plot correspond to H1, H2, H3 and H4, respectively. For each heuristic, there are six different combinations of the path length and envelope diameter, progressively increasing from left to right. The y-axis represents the actual value of the measure plotted, rendered in an appropriate log-scale if there are too big\nrelative differences between the plotted values. Each plot represents one type of measure and different colours correspond to specific datasets (red for Diseasome, green for DrugBank, blue for SIDER and black for all). The full and dashed lines are for experiments using the Tw and Ts taxonomies, respectively. All the walk-sampling results reported below are plotted in this fashion.\nThe results of the complexity measures can be summarised as follows: 1. The size and number of components increase with longer walks and larger envelopes. 2. The SIDER dataset has generally lowest number of components of smallest size, while Diseasome is dominating in these measures. 3. The all dataset has relatively large components in average, but there is less of them than in case of Diseasome. 4. The all dataset has the largest complexity in terms of clustering coefficients, with Diseasome being closely second and DrugBank comparatively much smaller. SIDER has zero complexity according to the clustering coefficient.\nThe results of the coherence analysis are in Figure 6. The general observa-\ntions are: 1. The start/end coherences tend to be higher for shorter path lengths. 2. The coherences in the samples using the Tw taxonomies are generally higher than the ones using the Ts taxonomy. 3. SIDER has the lowest coherence in most cases. 4. The product and average coherences tend to be relatively lower for the H4 (dissimilarity) heuristic. 5. The Diseasome dataset is generally the second best for most coherence types. DrugBank is generally third, except of the start/end coherence where it is mostly the best. For the average and product coherences, the all dataset usually performs best. The trend is clearer for the coherences based on taxonomical similarity.\nThe entropy results using the Tw, Ts taxonomies for the topic annotations are in Figure 7. The observations can be summarised as follows: 1. The entropies computed using the Ts taxonomy are always higher than the ones based on Tw when taking into account only the most general identifiers of the cluster annotations (the left hand side plots). The trend is opposite, though not so\nclear, for the full (i.e., specific) cluster annotations. 2. The entropies tend to be higher for the H2, H3 heuristics (weight and similarity preferences). 3. Generally, the entropies increase with the length of the walks, however, the all dataset tends to exhibit such behaviour more often than the others (which do so basically only in case of H2, H3 heuristics for top clusters). 4. The isolated datasets tend to have higher entropies than the all one for specific clusters (right hand side plots), with Diseasome or SIDER being the most entropic ones and DrugBank usually being the second-highest. 5. On the other hand, the all dataset has generally highest entropy for the abstract clusters (left hand side plots) based on the Tw taxonomy. 6. The results based on the Ts taxonomies are mostly close to each other, however, the Diseasome and SIDER datasets tend to have higher entropies than the others for the H2 and H3, H4 heuristics, respectively."}, {"heading": "4.3 Interpreting the Results", "text": "The classification of the SIDER, Diseasome, DrugBank and all datasets is \u2193\u2193\u2191, \u2191\u2191\u2193, \u2193\u2212\u2212, \u2191\u2191\u2212, respectively. We assigned the \u2191 or \u2193 symbols to datasets that have the corresponding measures distinctly higher or lower than at least two other datasets in more than half of all possible settings. We used a new \u2212 symbol if there is no clearly prevalent higher-than or lower-than trend. According to the classification, SIDER is more serendipitous with simple balanced contexts and Diseasome is more focused around uneven sets of topics with complex structural information context. The general classification of the DrugBank and all datasets is trickier due to less significant trends observed. However, DrugBank is definitely simpler (even more so than much smaller Diseasome), and the all dataset is more focused and complex. Another general observation is that the parameter settings typically do not influence the relative differences between the dataset performances. The only exceptions are simcos start/end coherences and specific cluster entropies, but the differences do not seem to be too dramatic even there.\nThe conflicting trends in coherences and entropies in case of DrugBank and all datasets are related to the slightly different semantics of the particular measures within those classes. In case of coherence, the start/end one can be interpreted as an approximation of dataset\u2019s \u201cattractiveness,\u201d i.e., the likelihood of ending up in a similar topic no matter where and how one goes. The other coherences take into account consequent steps on the walks and thus are more related to the measure of average or cumulative topical \u201cdispersal\u201d across single steps. For entropies, the top-cluster and all-cluster entropies measure the information content regarding abstract and specific topics, respectively. Therefore the measures can exhibit different trends for datasets that have uneven granularity of the taxonomy levels.\nTo compare the results of the empirical analysis of the datasets with the intentions of their creators, let us start with SIDER that has been designed as simple-structured dataset where one can easily retrieve associations between drugs and their side effects. Our observations indeed confirm this \u2013 SIDER is classified as relatively simple, with balanced contexts and without any significant \u201cattractor\u201d topics. Diseasome focuses on capturing complex disease-gene relationships, which again corresponds to our analysis \u2013 the dataset is relatively focused and complex with rather low entropy in the contexts. Finally, DrugBank is supposed to link drugs with comprehensive information spanning across multiple domains like pharmacology, chemistry or genetics, with the information usually defined in external vocabularies. The high start/end and low cumulative coherences indicate a strong attractiveness despite of frequent context switching (i.e., no matter where you start, it is likely that you will be in a drug-related context and you will end up there again even if you switch between other topics on the way). The low complexity measured by CoCoE indicates relatively simple structure of the links. This is consistent with a manually assessed structure of DrugBank \u2013 it contains many relations fanning out from drug entities while the other nodes are seldom linked to anything else than other drugs.\nOne of the most interesting dataset-specific observations, though, is related to the aggregate all dataset. It is clearly most complex. It has rather low start/end coherence, but generally quite high cumulative coherences. In addition, the abstract and specific entropies are relatively high and low, respectively. This means that a traversing agent explores increasingly more distant topics, but shifting only a little at a time. The specific contextual topics are quite unpredictable, but the abstract topics tend to be more regular, meaning that one can learn a lot of details about few general domains using the dataset. These characteristics make the all dataset most suitable for tasks like knowledge discovery and/or extraction of complex features associated with drugs or diseases. This is very useful information in the scope of our original motivations for picking the experimental datasets (i.e., feature selection for adverse drug effect discovery models)."}, {"heading": "5 Conclusions and Future Work", "text": "We have presented CoCoE, a well-founded methodology for empirical analysis of LOD datasets. We have also described a publicly available implementation of\nthe methodology. The experimental results demonstrated the utility of CoCoE, as it provided a meaningful automated assessment of biomedical datasets that is consistent with the intentions of the dataset authors and maintainers.\nOur future work involves more scalable clustering and graph traversal algorithms that would make CoCoE readily applicable even to the largest LOD datasets like DBpedia or Uniprot. We also want to experiment with other implementations of the methodology, using and formally analysing especially different similarities and clusterings. Another interesting research topic is studying correlation between the performance of specific SPARQL query types and particular CoCoE measure value ranges, which could provide valuable insights for maintainers and users of SPARQL end-points. We also want to work together with dataset providers in order to establish a more systematic and thorough mapping between CoCoE assessment of datasets and their suitability to particular use cases. Last but not least, we intend to investigate other possible applications of the CoCoE measures, such as machine-aided modelling or vocabulary debugging."}], "references": [{"title": "Novel data-mining methodologies for adverse drug event discovery and analysis", "author": ["R. Harpaz", "W. DuMouchel", "N.H. Shah", "D. Madigan", "P. Ryan", "C. Friedman"], "venue": "Clinical Pharmacology & Therapeutics 91(6)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Social Network Analysis: Methods and Applications", "author": ["S. Wasserman", "K. Faust"], "venue": "Cambridge University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Getting the meaning right: A complementary distributional layer for the web semantics", "author": ["V. Nov\u00e1\u010dek", "S. Handschuh", "S. Decker"], "venue": "Proceedings of ISWC\u201911, Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Semantic similarity in biomedical ontologies", "author": ["C. Pesquita", "D. Faria", "A.O. Falco", "P. Lord", "F.M. Couto"], "venue": "PLoS Computational Biololgy 5(7)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "SKIMMR: Facilitating knowledge discovery in life sciences by machine-aided skim reading", "author": ["V. Nov\u00e1\u010dek", "G.A. Burns"], "venue": "PeerJ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Connectivity based k-hop clustering in wireless networks", "author": ["F.G. Nocetti", "J.S. Gonzalez", "I. Stojmenovic"], "venue": "Telecommunication systems 22(1-4)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Link communities reveal multiscale complexity in networks", "author": ["Y.Y. Ahn", "J.P. Bagrow", "S. Lehmann"], "venue": "Nature 466(7307)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Introducing RDF graph summary with application to assisted SPARQL formulation", "author": ["S. Campinas", "T.E. Perry", "D. Ceccarelli", "R. Delbru", "G. Tummarello"], "venue": "Proceedings of DEXA\u201912, IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "RDFStats - an extensible RDF statistics generator and library", "author": ["A. Langegger", "W. W\u00f6\u00df"], "venue": "DEXA Workshops.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning from linked open data usage: Patterns & metrics", "author": ["K. M\u00f6ller", "M. Hausenblas", "R. Cyganiak", "S. Handschuh"], "venue": "Proceedings of the WebSci\u201910, Web Science Trust", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "RDF Analytics: Lenses over Semantic Graphs", "author": ["D. Colazzo", "F. Goasdou\u00e9", "I. Manolescu", "A. Roatis"], "venue": "Proceedings of WWW\u201914, ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistics for Research", "author": ["S. Dowdy", "S. Weardon", "D. Chilko"], "venue": "Wiley", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Supporting knowledge discovery by linking diverse biomedical data", "author": ["A.T. Abdelrahman", "E. Munoz", "V. Nov\u00e1\u010dek", "P.Y. Vandenbussche"], "venue": "AMIA\u201914 Abstracts, AMIA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Rob knows that the most successful methods typically define and train a model in order to discover unknown side effects of drugs using their known features [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 1, "context": "For the complexity measures, we use network analysis algorithms [2].", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "Firstly, we need a distributional representation of the RDF data [3], which describes each entity (subject or object) by a vector that represents its meaning based on the entities linked to it.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "These two structures allow for representing coherence using various types of semantic similarities based on the vector space representation and the taxonomy structure, such as cosine or Wu-Palmer [4].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "The distributional representation of RDF data we use builds on our previous work [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "We have recently introduced the notion of heuristic quasi-random walks and their empirical analysis in [5], which, however, deals with different", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "The clustering method introduced here builds on principles similar to k-hop clustering [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "Another related approach is nonparametric hierarchical link clustering [7], which is more general and sophisticated than our simple method, yet its Python implementation we have experimented with proved to be intractable when used in our experiments.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "A comprehensive overview of semantic similarity measures applicable in CoCoE is provided in [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "The most relevant tools and approaches for RDF data analysis are [8,9,10,11].", "startOffset": 65, "endOffset": 76}, {"referenceID": 8, "context": "The most relevant tools and approaches for RDF data analysis are [8,9,10,11].", "startOffset": 65, "endOffset": 76}, {"referenceID": 9, "context": "The most relevant tools and approaches for RDF data analysis are [8,9,10,11].", "startOffset": 65, "endOffset": 76}, {"referenceID": 10, "context": "The most relevant tools and approaches for RDF data analysis are [8,9,10,11].", "startOffset": 65, "endOffset": 76}, {"referenceID": 8, "context": "Perhaps closest to our work is [9] that computes a set of statistics and histograms for a given RDF dataset.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "Graph summaries [8] propose high-level abstractions of RDF data intended to facilitate formulation of SPARQL queries, which is orthogonal to our approach aimed at quantitative characteristics of the data itself.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "Usage-based RDF data analysis [10] provides insights into common patterns of utilising RDF data by agents, but does not offer means for actually analysing the data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "Finally, the recent approach [11] is useful for knowledge discovery in RDF data based on user-defined query patterns and analytical perspectives.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "Our approach complements [11] by characterising application-independent features of RDF datasets taken as a whole.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "In the experiments presented here, we used a simple method for ranking and filtering out the columns \u2013 the \u03c7 statistic [12], which can be used for computing divergence of specific observations from expected values.", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "The formula we use is essentially based on a popular Wu-Palmer similarity measure [4].", "startOffset": 82, "endOffset": 85}, {"referenceID": 12, "context": "The dataset selection was motivated by our recent work in adverse drug effect discovery, for which we have been compiling a knowledge base from relevant biomedical Linked Open Data [13].", "startOffset": 181, "endOffset": 185}, {"referenceID": 12, "context": "Therefore we decided to use the knowledge bases being created in [13] as a test case for CoCoE.", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "All graphs have so called small world property [2], as their densities are rather small and yet there is very little separation between any two nodes in the graph in general.", "startOffset": 47, "endOffset": 50}], "year": 2014, "abstractText": "CoCoE stands for Complexity, Coherence and Entropy, and presents an extensible methodology for empirical analysis of Linked Open Data (i.e., RDF graphs). CoCoE can offer answers to questions like: Is dataset A better than B for knowledge discovery since it is more complex and informative?, Is dataset X better than Y for simple value lookups due its flatter structure?, etc. In order to address such questions, we introduce a set of well-founded measures based on complementary notions from distributional semantics, network analysis and information theory. These measures are part of a specific implementation of the CoCoE methodology that is available for download. Last but not least, we illustrate CoCoE by its application to selected biomedical RDF datasets.", "creator": "LaTeX with hyperref package"}}}