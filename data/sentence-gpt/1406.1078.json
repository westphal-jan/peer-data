{"id": "1406.1078", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2014", "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. In the context of the model, we discuss the potential applications of reinforcement learning and a particular algorithm to improve neural networks. These computational and computational models are not supported by the traditional computational model for training recurrent neural networks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 3 Jun 2014 17:47:08 GMT  (875kb,D)", "http://arxiv.org/abs/1406.1078v1", null], ["v2", "Thu, 24 Jul 2014 20:07:13 GMT  (460kb,D)", "http://arxiv.org/abs/1406.1078v2", "EMNLP 2014"], ["v3", "Wed, 3 Sep 2014 00:25:02 GMT  (551kb,D)", "http://arxiv.org/abs/1406.1078v3", "EMNLP 2014"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE stat.ML", "authors": ["kyunghyun cho", "bart van merrienboer", "\u00e7aglar g\u00fcl\u00e7ehre", "dzmitry bahdanau", "fethi bougares", "holger schwenk", "yoshua bengio"], "accepted": true, "id": "1406.1078"}, "pdf": {"name": "1406.1078.pdf", "metadata": {"source": "CRF", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "authors": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "emails": ["kyunghyun.cho@umontreal.ca", "vanmerb@iro.umontreal.ca", "gulcehrc@iro.umontreal.ca", "firstname.lastname@lium.univ-lemans.fr", "find.me@on.the.web"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks have shown great success in various applications such as objection recognition (see, e.g., (Krizhevsky et al., 2012)) and speech recognition (see, e.g., (Dahl et al., 2012)). Furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing (NLP). These include, but are not limited to, paraphrase detection (Socher et al., 2011), word embedding extraction (Mikolov et al., 2013) and language modeling (Bengio et al., 2003). In the field of statistical machine learning (SMT), deep neural networks have begun to show promising results. (Schwenk, 2012) summarizes a successful usage of feedforward neural networks in the framework of phrase-based SMT system.\nAlong this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as a part of the conventional phrase-based SMT system. The proposed neural network architecture, which we will refer to as an RNN Encoder\u2013Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector\nrepresentation back to a variable-length target sequence. The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. Additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training.\nThe proposed RNN Encoder\u2013Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French. We train the model to learn the translation probability of an English phrase to a corresponding French phrase. The model is then used as a part of a standard phrase-based SMT system by scoring each phrase pair in the phrase table. The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder\u2013Decoder improves the translation performance.\nWe qualitatively analyze the trained RNN Encoder\u2013 Decoder by comparing its phrase scores with those given by the existing translation model. The qualitative analysis shows that the RNN Encoder\u2013Decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance. The further analysis of the model reveals that the RNN Encoder\u2013Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase."}, {"heading": "2 RNN Encoder\u2013Decoder", "text": ""}, {"heading": "2.1 Preliminary: Recurrent Neural Networks", "text": "A recurrent neural network (RNN) is a neural network that consists of a hidden state h and an optional output y which operates on a variable-length sequence x = (x1, . . . , xT ). At each time step t, the hidden state h\u3008t\u3009 of the RNN is updated by\nh\u3008t\u3009 = f ( h\u3008t\u22121\u3009, xt ) , (1)\nwhere f is a non-linear activation function. f may be as simple as an element-wise logistic sigmoid function and as complex as a long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997).\nAn RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. In that case, the output at each timestep t is\nar X\niv :1\n40 6.\n10 78\nv1 [\ncs .C\nL ]\n3 J\nun 2\n01 4\nthe conditional distribution p(xt | xt\u22121, . . . , x1). For example, a multinomial distribution (1-of-K coding) can be output using a softmax activation function\np(xt,j = 1 | xt\u22121, . . . , x1) = exp\n( wjh\u3008t\u3009 )\u2211K j\u2032=1 exp ( wj\u2032h\u3008t\u3009\n) , (2)\nfor all possible symbols j = 1, . . . ,K, where wj are the rows of a weight matrix W. By combining these probabilities, we can compute the probability of the sequence x using\np(x) = T\u220f t=1 p(xt | xt\u22121, . . . , x1). (3)\nFrom this learned distribution, it is straightforward to sample a new sequence by iteratively sampling a symbol at each time step."}, {"heading": "2.2 RNN Encoder\u2013Decoder", "text": "In this paper, we propose a novel neural network architecture that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector representation back into a variable-length sequence. From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence, e.g. p(y1, . . . , yT \u2032 | x1, . . . , xT ), where one should note that the input and output sequence lengths T and T \u2032 may differ.\nThe encoder is an RNN that reads each symbol of an input sequence x sequentially. As it reads each symbol, the hidden state of the RNN changes according to Eq. (1). After reading the end of the sequence (marked by an end-of-sequence symbol), the hidden state of the RNN is a summary c of the whole input sequence.\nThe decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol yt given the hidden state h\u3008t\u3009. However, unlike the RNN described in Sec. 2.1, both yt and h\u3008t\u3009 are also conditioned on yt\u22121 and on the summary c of the input sequence. Hence, the hidden state of the decoder at time t is computed by,\nh\u3008t\u3009 = f ( h\u3008t\u22121\u3009, yt\u22121,c ) ,\nand similarly, the conditional distribution of the next symbol is\nP(yt|yt\u22121, yt\u22122, . . . , y1,c) = g ( h\u3008t\u3009, yt\u22121,c ) .\nfor given activation functions f and g (the latter must produce valid probabilities, e.g. with a softmax).\nSee Fig. 1 for a graphical depiction of the proposed model architecture.\nThe two components of the proposed RNN Encoder\u2013Decoder are jointly trained to maximize the conditional log-likelihood\nmax \u03b8\n1\nN N\u2211 n=1 log p\u03b8(yn | xn), (4)\nwhere \u03b8 is the set of the model parameters and each (xn,yn) is an (input sequence, output sequence) pair from the training set. In our case, as the output of the decoder, starting from the input, is differentiable, we can use a gradient-based algorithm to estimate the model parameters.\nOnce the RNN Encoder\u2013Decoder is trained, the model can be used in two ways. One way is to use the model to generate a target sequence given an input sequence. On the other hand, the model can be used to score a given pair of input and output sequences, where the score is simply a probability p\u03b8(y | x) from Eqs. (3) and (4)."}, {"heading": "2.3 Hidden Unit that Adaptively Remembers and Forgets", "text": "In addition to a novel model architecture, we also propose a new type of hidden unit (f in Eq. (1)) that has been motivated by the LSTM unit but is much simpler to compute and implement.1 Fig. 2 shows the graphical depiction of the proposed hidden unit.\nLet us describe how the activation of the j-th hidden unit is computed. First, the reset gate rj is computed by\nrj = \u03c3 ( [Wrx]j + [ Urh\u3008t\u22121\u3009 ] j ) , (5)\n1 The LSTM unit, which has shown impressive results in several applications such as speech recognition, has a memory cell and four gating units that adaptively control the information flow inside the unit. For details on LSTM networks, see, e.g., (Graves, 2012).\nwhere \u03c3 is the logistic sigmoid function, and [.]j denotes the j-th element of a vector. x and ht\u22121 are the input and the previous hidden state, respectively. Wr and Ur are weight matrices which are learned.\nSimilarly, the update gate zj is computed by\nzj = \u03c3 ( [Wzx]j + [ Uzh\u3008t\u22121\u3009 ] j ) . (6)\nThe actual activation of the proposed unit hj is then computed by\nh \u3008t\u3009 j = zjh \u3008t\u22121\u3009 j + (1\u2212 zj)h\u0303 \u3008t\u3009 j , (7)\nwhere\nh\u0303 \u3008t\u3009 j = f ( [Wx]j + rj [ Uh\u3008t\u22121\u3009 ]) . (8)\nIn this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the previous hidden state and reset with the current input only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation.\nOn the other hand, the update gate controls how much information from the previous hidden state will carry over to the current hidden state. This acts similarly to the memory cell in the LSTM network and helps the RNN to remember long-term information. Furthermore, this may be considered an adaptive variant of a leaky-integration unit (Bengio et al., 2013).\nAs each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales. Those units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active."}, {"heading": "3 Statistical Machine Translation", "text": "In a commonly used statistical machine translation system (SMT), the goal of the system (decoder,\nspecifically) is to find a translation f given a source sentence e, which maximizes\nlog p(f | e) \u221d log p(e | f) + log p(f),\nwhere the first term at the right hand side is called translation model and the latter is language model (see, e.g., (Koehn, 2005)). In practice, however, most SMT systems model log p(f | e) as a log-linear model with additional features and corresponding weights:\nlog p(f | e) \u221d N\u2211 n=1 wnfn(f,e), (9)\nwhere fn and wn are the n-th feature and weight, respectively. The weights are often optimized to maximize the BLEU score on a development set.\nIn the phrase-based SMT framework introduced in (Koehn et al., 2003) and (Marcu and Wong, 2002), the translation model log p(e | f) is factorized into the translation probabilities of matching phrases in the source and target sentences.2 These probabilities are once again considered additional features in the log-linear model (see Eq. (9)) and are weighted accordingly to maximize the BLEU score.\nSince the neural net language model was proposed in (Bengio et al., 2003), neural networks have been used widely in SMT systems. In many cases, neural networks have been used to rescore translation hypotheses (n-best lists) proposed by the existing SMT system or decoder using a target language model (see, e.g., (Schwenk et al., 2006)). Recently, however, there has been interest in training neural networks to score the translated sentence (or phrase pairs) using a representation of the source sentence as an additional input. See, e.g., (Schwenk, 2012), (Son et al., 2012) and (Zou et al., 2013)."}, {"heading": "3.1 Scoring Phrase Pairs with RNN Encoder\u2013Decoder", "text": "Here we propose to train the RNN Encoder\u2013Decoder (see Sec. 2.2) on a table of phrase pairs and use its scores as additional features in the log-linear model in Eq. (9) when tuning the SMT decoder.\nWhen we train the RNN Encoder\u2013Decoder, we ignore the (normalized) frequencies of each phrase pair in the original corpora. This measure was taken in order (1) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and (2) to ensure that the RNN Encoder\u2013Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences. One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies\n2 Without loss of generality, from here on, we refer to p(e | f) for each phrase pair as a translation model as well\nof the phrase pairs in the original corpus. With a fixed capacity of the RNN Encoder\u2013Decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities, i.e., distinguishing between plausible and implausible translations, or learning the \u201cmanifold\u201d (region of probability concentration) of plausible translations.\nOnce the RNN Encoder\u2013Decoder is trained, we add a new score for each phrase pair to the existing phrase table. This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation.\nAs Schwenk pointed out in (Schwenk, 2012), it is possible to completely replace the existing phrase table with the proposed RNN Encoder\u2013Decoder. In that case, for a given source phrase, the RNN Encoder\u2013Decoder will need to generate a list of (good) target phrases. This requires, however, an expensive sampling procedure to be performed repeatedly. At the moment we consider doing this efficiently enough to allow integration with the decoder an open problem, and leave this to future work."}, {"heading": "3.2 Related Approaches: Neural Networks in Machine Translation", "text": "Before presenting the empirical results, we discuss a number of recent works that have proposed to use neural networks in the context of SMT.\nSchwenk in (Schwenk, 2012) proposed a similar approach of scoring phrase pairs. Instead of the RNNbased neural network, he used a feedforward neural network that has fixed-size inputs (7 words in his case, with zero-padding for shorter phrases) and fixed-size outputs (7 words in the target language). When it is used specifically for scoring phrases for the SMT system, the maximum phrase length is often chosen to be small. However, as the length of phrases increases or as we apply neural networks to other variable-length sequence data, it is important that the neural network can handle variable-length input and output. The proposed RNN Encoder\u2013Decoder is well-suited for these applications.\nAlthough it is not exactly a neural network they train, the authors of (Zou et al., 2013) proposed to learn a bilingual embedding of words/phrases. They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system.\nIn (Chandar et al., 2014), a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase. This is closely related to both the proposed RNN Encoder\u2013Decoder and the model proposed in (Schwenk, 2012), except that their input representation of a phrase is a bag-of-words. Earlier, a similar encoder\u2013decoder model using two recursive neural networks was proposed in (Socher et al., 2011), but their model was restricted to a monolingual setting,\ni.e. the model reconstructs an input sentence. One important difference between the proposed RNN Encoder\u2013Decoder and the approaches in (Zou et al., 2013) and (Chandar et al., 2014) is that the order of the words in source and target phrases is taken into account. The RNN Encoder\u2013Decoder naturally distinguishes between sequences that have the same words but in a different order, whereas the aforementioned approaches effectively ignore order information.\nThe closest approach related to the proposed RNN Encoder\u2013Decoder is the Recurrent Continuous Translation Model (Model 2) proposed in (Kalchbrenner and Blunsom, 2013). In their paper, they proposed a similar model that consists of an encoder and decoder. The difference with our model is that they used a convolutional n-gram model (CGM) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder. They, however, evaluated their model on rescoring the n-best list proposed by the conventional SMT system."}, {"heading": "4 Experiments", "text": "We evaluate our approach on the English/French translation task of the WMT\u201914 workshop."}, {"heading": "4.1 Data and Baseline System", "text": "Large amounts of resources are available to build an English/French SMT system in the framework of the WMT\u201914 translation task. The bilingual corpora include Europarl (61M words), news commentary (5.5M), UN (421M), and two crawled corpora of 90M and 780M words respectively. The last two corpora are quite noisy. To train the French language model, about 712M words of crawled newspaper material is available in addition to the target side of the bitexts. All the word counts refer to French words after tokenization.\nIt is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance, and results in extremely large models which are difficult to handle. Instead, one should focus on the most relevant subset of the data for a given task. We have done so by applying the data selection method proposed in (Moore and Lewis, 2010), and its extension to bitexts (Axelrod et al., 2011). By these means we selected a subset of 418M words out of more than 2G words for language modeling and a subset of 348M out of 850M words for training the RNN Encoder\u2013Decoder. We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT, and newstest2014 as our test set. Each set has more than 70 thousand words and a single reference translation.\nFor training the neural networks, including the proposed RNN Encoder\u2013Decoder, we limited the source and target vocabulary to the most frequent 15,000\nwords for both English and French. This covers approximately 93% of the dataset. All the out-of-vocabulary words were mapped to a special token ([UNK]).\nThe baseline phrase-based SMT system was built using Moses with default settings. The phrase table was created using only the 2% highest scoring sentences of the full dataset, according to Axelrod\u2019s method (a total of 15M French words). This was done in order to keep decoding time with the CSLM reasonable. This system achieves a BLEU score of 27.63 and 29.33 on the test set (see Table 1)."}, {"heading": "4.1.1 RNN Encoder\u2013Decoder", "text": "The RNN Encoder\u2013Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder. The input matrix between each input symbol x\u3008t\u3009 and the hidden unit is approximated with two lower-rank matrices, and the output matrix is approximated similarly. We used rank-100 matrices, equivalent to learning an embedding of dimension 100 for each word. The activation function used for h\u0303 in Eq. (8) is a hyperbolic tangent function. The computation from the hidden state in the decoder to the output is implemented as a deep neural network (Pascanu et al., 2014) with a single intermediate layer having 500 maxout units each pooling 2 inputs (Goodfellow et al., 2013).\nAll the weight parameters in the RNN Encoder\u2013 Decoder were initialized by sampling from an isotropic zero-mean (white) Gaussian distribution with its standard deviation fixed to 0.01, except for the recurrent weight parameters. For the recurrent weight matrices, we first sampled from a white Gaussian distribution and used its left singular vectors matrix multiplied with a small constant (0.01), following (Saxe et al., 2014).\nWe used Adadelta to train the RNN Encoder\u2013 Decoder with hyperparameters = 10\u22126 and \u03c1 = 0.95 (Zeiler, 2012). At each update, we used 64 randomly selected phrase pairs from a phrase table (which was created from 348M words). The model was trained for approximately three days.\nDetails of the architecture used in the experiments are explained in more depth in the supplementary material."}, {"heading": "4.1.2 Neural Language Model", "text": "In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder\u2013Decoder, we also tried a more traditional approach of using a neural network for learning a target language model (CSLM) (Schwenk, 2007). Especially, the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder\u2013Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT system add up or are redundant.\nWe trained the CSLM model on 7-grams from the target corpus. Each input word was projected into the embedding space R512, and they were concatenated to form a 3072-dimensional vector. The concatenated vector was fed through two rectified layers (of size 1536 and 1024) (Glorot et al., 2011). The output layer was a simple softmax layer (see Eq. (2)). All the weight parameters were initialized uniformly between \u22120.01 and 0.01, and the model was trained until the validation perplexity did not improve for 10 epochs. After training, the language model achieved a perplexity of 45.80. The validation set was a random selection of 0.1% of the corpus. The model was used to score partial translations during the decoding process, which generally leads to higher gains in BLEU score than n-best list rescoring (Vaswani et al., 2013).\nTo address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stack-search performed by the decoder. Only when the buffer is full, or a stack is about to be pruned, the n-grams are scored by the CSLM. This allows us to perform fast matrix-matrix multiplication on GPU using Theano (Bergstra et al., 2010; Bastien et al., 2012). This approach results in a significant speedup compared to scoring each n-gram in isolation."}, {"heading": "4.2 Quantitative Analysis", "text": "We tried the following combinations:\n1. Baseline configuration 2. Baseline + CSLM 3. Baseline + RNN 4. Baseline + CSLM + RNN 5. Baseline + CSLM + RNN + Word penalty\nThe results are presented in Table 1. As expected, adding features computed by neural networks consistently improves the performance over the baseline performance. Noticeably, the phrase pair scores computed by the proposed RNN Encoder\u2013Decoder (RNN) were able to improve more, compared to the more traditional approach of having only a target language model (CSLM). This is a significant improvement considering that the additional computational complexity induced by having simply one additional phrase pair score is minimal when tuning the decoder.\nThe best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder\u2013Decoder. This suggests that the contributions of the CSLM and the RNN Encoder\u2013Decoder are not too correlated and that one can expect better results by improving each method independently. Furthermore, we were able to improve the translation quality further (+1.85 BLEU over baseline) by penalizing the number of words that are unknown to the neural networks (i.e. words which are not in the shortlist). We do so by simply adding the number of unknown words as an additional feature the log-linear model in Eq. (9).3\n3 To understand the effect of the penalty, consider the set of all words in the 15,000 large shortlist, SL. All words xi /\u2208 SL\nare replaced by a special token [UNK] before being scored by the neural networks. Hence, the conditional probability of any xit /\u2208 SL is actually given by the model as\np (xt = [UNK] | x<t) = p (xt /\u2208 SL | x<t) = \u2211\nx j t /\u2208SL\np ( xjt | x<t ) \u2265 p ( xit | x<t ) ,\nwhere x<t is a shorthand notation for xt\u22121, . . . , x1. As a result, the probability of words not in the shortlist is always overestimated. For CSLMs this shortcoming can be addressed by using a separate back-off n-gram language model that only contains non-shortlisted words (see (Schwenk, 2007)). However, since there is no direct equivalent of this approach for the RNN Encoder\u2013Decoder, we opt for introducing a word penalty instead, which counteracts the word probability overestimation.\nAdditionally, we performed an experiment showing that the performance improvement is not solely due to a larger set of phrase pairs used to train the RNN Encoder\u2013Decoder. In this experiment, we tuned the SMT decoder with the full bitext instead of the reduced one (348M words). With baseline + RNN, in this case, we obtained 31.20 and 33.89 BLEU scores on the development and test sets, while the baseline scores are 30.62 and 33.30 respectively. This clearly suggests that the proposed approach is applicable to and improves a large-scale SMT system as well."}, {"heading": "4.3 Qualitative Analysis", "text": "In order to understand where the performance improvement comes from, we analyze the phrase pair scores computed by the RNN Encoder\u2013Decoder against those p(f | e), the so-called inverse phrase translation probability from the translation model. Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus, we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases. Also, as we mentioned earlier in Sec. 3.1, we further expect the RNN Encoder\u2013Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus.\nWe focus on those pairs whose source phrase is long (more than 3 words per source phrase) and frequent. For each such source phrase, we look at the target phrases that have been scored high either by the translation probability p(f | e) or by the RNN Encoder\u2013Decoder. Similarly, we perform the same procedure with those pairs whose source phrase is long but rare in the corpus.\nTable 2 lists the top-3 target phrases per source phrase favored either by the translation model or by the RNN Encoder\u2013Decoder. The source phrases were randomly chosen among long ones having more than 4 or 5 words.\nIn most cases, the choices of the target phrases by the RNN Encoder\u2013Decoder are closer to actual or literal translations. We can observe that the RNN Encoder\u2013Decoder prefers shorter phrases in general.\nInterestingly, many phrase pairs were scored similarly by both the translation model and the RNN Encoder\u2013Decoder, but there were as many other phrase pairs that were scored radically different (see Fig. 3). This could arise from the proposed approach of training the RNN Encoder\u2013Decoder on a set of unique phrase pairs, discouraging the RNN Encoder\u2013Decoder from learning simply the frequencies of the phrase pairs from the corpus, as explained earlier.\nFurthermore, in Table 3, we show for each of the source phrases in Table 2, the generated samples from the RNN Encoder\u2013Decoder. For each source phrase, we generated 50 samples and show the top-five phrases accordingly to their scores. We can see that the RNN Encoder\u2013Decoder is able to propose well-formed target phrases without looking at the actual phrase table. Importantly, the generated phrases do not overlap completely with the target phrases from the phrase table. This encourages us to further investigate the possibility of replacing the whole or a part of the phrase table with the proposed RNN Encoder\u2013Decoder in the future."}, {"heading": "4.4 Word and Phrase Representations", "text": "Since the proposed RNN Encoder\u2013Decoder is not specifically designed only for the task of machine translation, here we briefly look at the properties of the trained model.\nIt has been known for some time that continuous space language models using neural networks are able to learn semantically meaningful embeddings (See, e.g., (Bengio et al., 2003; Mikolov et al., 2013)). Since the proposed RNN Encoder\u2013Decoder also projects to and maps back from a sequence of words into a continuous space vector, we expect to see a similar property with the proposed model as well.\nThe left plot in Fig. 4 shows the 2\u2013D embedding of the words using the word embedding matrix learned by the RNN Encoder\u2013Decoder. The projection was done by the recently proposed Barnes-HutSNE (van der Maaten, 2013). We can clearly see that semantically similar words are clustered with each other (see the zoomed-in plots in Fig. 4).\nThe proposed RNN Encoder\u2013Decoder naturally generates a continuous-space representation of a phrase. The representation (c in Fig. 1) in this case is a 1000-dimensional vector. Similarly to the word representations, we visualize the representations of the phrases that consists of four or more words using the Barnes-Hut-SNE in Fig. 5.\nFrom the visualization, it is clear that the RNN Encoder\u2013Decoder captures both semantic and syntactic structures of the phrases. For instance, in the top-right plot, all the phrases are about the percentage of a\nfollowing object, and importantly, the model is able to correctly map various different ways to phrase % into a similar representation (e.g., percent, per cent, %). The bottom-left plot shows phrases that are related to date or duration and at the same time are grouped by syntactic similarities among themselves. A similar trend can also be observed in the last plot."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a new neural network architecture, called an RNN Encoder\u2013Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length. The proposed RNN Encoder\u2013 Decoder is able to either score a pair of sequences (in\nterms of a conditional probability) or generate a target sequence given a source sequence. Along with the new architecture, we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence.\nWe evaluated the proposed model with the task of statistical machine translation, where we used the RNN Encoder\u2013Decoder to score each phrase pair in the phrase table. Qualitatively, we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder\u2013 Decoder is able to propose well-formed target phrases.\nThe scores by the RNN Encoder\u2013Decoder were found to improve the overall translation performance in terms of BLEU scores. Also, we found that the contribution by the RNN Encoder\u2013Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system, so that we can improve further the performance by using, for instance, the RNN Encoder\u2013 Decoder and the neural net language model together.\nOur qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level. This suggests that there may be more natural language related applications that may benefit from the proposed RNN Encoder\u2013Decoder.\nThe proposed architecture has large potential in further improvement and analysis. One approach that was not investigated here is to replace the whole, or a part of the phrase table by letting the RNN Encoder\u2013Decoder propose target phrases. Also, noting that the proposed model is not limited to being used with written language, it will be an important future research to apply the proposed architecture to other applications such as speech transcription."}, {"heading": "Acknowledgments", "text": "The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR."}, {"heading": "A RNN Encoder\u2013Decoder", "text": "In this document, we describe in detail the architecture of the RNN Encoder\u2013Decoder used in the experiments.\nLet us denote an source phrase by X = (x1,x2, . . . ,xN) and a target phrase by Y = (y1,y2, . . . ,yM). Each phrase is a sequence ofK-dimensional one-hot vectors, such that only one element of the vector is 1 and all the others are 0. The index of the active (1) element indicates the word represented by the vector.\nA.1 Encoder Each word of the source phrase is embedded in a 100-dimensional vector space: e(xi) \u2208 R100. e(x) is used in Sec. 4.4 to visualize the words.\nThe hidden state of an encoder consists of 1000 hidden units, and each one of them at time t is computed by\nh \u3008t\u3009 j = zjh \u3008t\u22121\u3009 j + (1\u2212 zj)h\u0303 \u3008t\u3009 j ,\nwhere\nh\u0303 \u3008t\u3009 j =tanh ( [We(xt)]j + rj [ Uh\u3008t\u22121\u3009 ]) ,\nzj =\u03c3 ( [Wze(xt)]j + [ Uzh\u3008t\u22121\u3009 ] j ) ,\nrj =\u03c3 ( [Wre(xt)]j + [ Urh\u3008t\u22121\u3009 ] j ) .\n\u03c3 is a logistic sigmoid function. To make the equations uncluttered, we omit biases. The initial hidden state h \u30080\u3009 j is fixed to 0. Once the hidden state at theN step (the end of the source phrase) is computed, the representation of the source phrase c is\nc = tanh ( Vh\u3008N\u3009 ) .\nAlso, we collect the average word embeddings of the source phrase such that\nmx = 1\nN N\u2211 t=1 e(xt).\nA.1.1 Decoder The decoder starts by initializing the hidden state with\nh\u2032 \u30080\u3009 = tanh ( V\u2032c ) ,\nwhere we will use \u00b7\u2032 to distinguish parameters of the decoder from those of the encoder. The hidden state at time t of the decoder is computed by\nh\u2032 \u3008t\u3009 j = z \u2032 jh \u2032\u3008t\u22121\u3009 j + (1\u2212 z \u2032 j)h\u0303\u2032 \u3008t\u3009 j ,\nwhere\nh\u0303\u2032 \u3008t\u3009 j =tanh ([ W\u2032e(yt\u22121) ] j + r\u2032j [ U\u2032h\u2032\u3008t\u22121\u3009 +Cc ]) ,\nz\u2032j =\u03c3 ([ W\u2032ze(yt\u22121) ] j + [ U\u2032zh \u2032 \u3008t\u22121\u3009 ] j + [Czc]j ) ,\nr\u2032j =\u03c3 ([ W\u2032re(yt\u22121) ] j + [ U\u2032rh \u2032 \u3008t\u22121\u3009 ] j + [Crc]j ) ,\nand e(y0) is an all-zero vector. Similarly to the case of the encoder, e(y) is an embedding of a target word. Unlike the encoder which simply encodes the source phrase, the decoder is learned to generate a target phrase. At each time t, the decoder computes the probability of generating j-th word by\np(yt,j = 1 | yt\u22121, . . . ,y1,X) = exp\n( gjs\u3008t\u3009 )\u2211K j\u2032=1 exp ( gj\u2032s\u3008t\u3009 ) ,\nwhere the i-element of s\u3008t\u3009 is\ns \u3008t\u3009 i = max { s\u2032 \u3008t\u3009 2i\u22121, s \u2032\u3008t\u3009 2i } and\ns\u2032 \u3008t\u3009 = Ohh \u2032\u3008t\u3009 +Oyyt\u22121 +Occ+Owmx.\nIn short, the s\u3008t\u3009i is a so-called maxout unit. For the computational efficiency, instead of a single-matrix output weight G, we use a product of two matrices such that\nG = GlGr,\nwhere Gl \u2208 RK\u00d7100 and Gr \u2208 R100\u00d71000."}, {"heading": "B Word and Phrase Representations", "text": "Here, we show enlarged plots of the word and phrase representations in Figs. 4\u20135.\nFigure 6: 2\u2013D embedding of the learned word representation. The top left one shows the full embedding space, while the other three figures show the zoomed-in view of specific regions (color\u2013coded).\nFigure 7: 2\u2013D embedding of the learned phrase representation. The top left one shows the full representation space (5000 randomly selected points), while the other three figures show the zoomed-in view of specific regions (color\u2013coded)."}], "references": [{"title": "Domain adaptation via pseudo in-domain data selection", "author": ["Xiaodong He", "Jianfeng Gao"], "venue": "In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Axelrod et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Axelrod et al\\.", "year": 2011}, {"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Advances in optimizing recurrent networks", "author": ["Bengio et al.2013] Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In Proceedings of the 38th International Conference on Acoustics,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha"], "venue": null, "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition", "author": ["Dahl et al.2012] George E. Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Deep sparse rectifier neural networks. In AISTATS", "author": ["Glorot et al.2011] X. Glorot", "A. Bordes", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": "Studies in Computational Intelligence. Springer", "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Two recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In Machine Translation Summit X,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A phrase-based, joint probability model for statistical machine translation", "author": ["Marcu", "Wong2002] Daniel Marcu", "William Wong"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10,", "citeRegEx": "Marcu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Marcu et al\\.", "year": 2002}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Intelligent selection of language model training data", "author": ["Moore", "Lewis2010] Robert C. Moore", "William Lewis"], "venue": "In Proceedings of the ACL 2010 Conference Short Papers,", "citeRegEx": "Moore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2010}, {"title": "How to construct deep recurrent neural networks", "author": ["Pascanu et al.2014] R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe et al.2014] Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": "In Proceedings of the Second International Conference on Learning Representations (ICLR", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Continuous space language models for the iwslt 2006", "author": ["Marta R. Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa"], "venue": null, "citeRegEx": "Schwenk et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2006}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Comput. Speech Lang.,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Continuous space translation models with neural networks", "author": ["Son et al.2012] Le Hai Son", "Alexandre Allauzen", "Fran\u00e7ois Yvon"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Son et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Son et al\\.", "year": 2012}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "Technical report,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning"], "venue": "In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": ", (Krizhevsky et al., 2012)) and speech recognition (see, e.", "startOffset": 2, "endOffset": 27}, {"referenceID": 6, "context": ", (Dahl et al., 2012)).", "startOffset": 2, "endOffset": 21}, {"referenceID": 21, "context": "These include, but are not limited to, paraphrase detection (Socher et al., 2011), word embedding extraction (Mikolov et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 15, "context": ", 2011), word embedding extraction (Mikolov et al., 2013) and language modeling (Bengio et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 2, "context": ", 2013) and language modeling (Bengio et al., 2003).", "startOffset": 30, "endOffset": 51}, {"referenceID": 8, "context": ", (Graves, 2012).", "startOffset": 2, "endOffset": 16}, {"referenceID": 3, "context": "Furthermore, this may be considered an adaptive variant of a leaky-integration unit (Bengio et al., 2013).", "startOffset": 84, "endOffset": 105}, {"referenceID": 12, "context": ", (Koehn, 2005)).", "startOffset": 2, "endOffset": 15}, {"referenceID": 11, "context": "In the phrase-based SMT framework introduced in (Koehn et al., 2003) and (Marcu and Wong, 2002), the translation model log p(e | f) is factorized into the translation probabilities of matching phrases in the source and target sentences.", "startOffset": 48, "endOffset": 68}, {"referenceID": 2, "context": "Since the neural net language model was proposed in (Bengio et al., 2003), neural networks have been used widely in SMT systems.", "startOffset": 52, "endOffset": 73}, {"referenceID": 19, "context": ", (Schwenk et al., 2006)).", "startOffset": 2, "endOffset": 24}, {"referenceID": 22, "context": ", (Schwenk, 2012), (Son et al., 2012) and (Zou et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 25, "context": ", 2012) and (Zou et al., 2013).", "startOffset": 12, "endOffset": 30}, {"referenceID": 25, "context": "Although it is not exactly a neural network they train, the authors of (Zou et al., 2013) proposed to learn a bilingual embedding of words/phrases.", "startOffset": 71, "endOffset": 89}, {"referenceID": 5, "context": "In (Chandar et al., 2014), a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase.", "startOffset": 3, "endOffset": 25}, {"referenceID": 21, "context": "Earlier, a similar encoder\u2013decoder model using two recursive neural networks was proposed in (Socher et al., 2011), but their model was restricted to a monolingual setting, i.", "startOffset": 93, "endOffset": 114}, {"referenceID": 25, "context": "One important difference between the proposed RNN Encoder\u2013Decoder and the approaches in (Zou et al., 2013) and (Chandar et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 5, "context": ", 2013) and (Chandar et al., 2014) is that the order of the words in source and target phrases is taken into account.", "startOffset": 12, "endOffset": 34}, {"referenceID": 0, "context": "We have done so by applying the data selection method proposed in (Moore and Lewis, 2010), and its extension to bitexts (Axelrod et al., 2011).", "startOffset": 120, "endOffset": 142}, {"referenceID": 17, "context": "The computation from the hidden state in the decoder to the output is implemented as a deep neural network (Pascanu et al., 2014) with a single intermediate layer having 500 maxout units each", "startOffset": 107, "endOffset": 129}, {"referenceID": 18, "context": "01), following (Saxe et al., 2014).", "startOffset": 15, "endOffset": 34}, {"referenceID": 24, "context": "95 (Zeiler, 2012).", "startOffset": 3, "endOffset": 17}, {"referenceID": 20, "context": "In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder\u2013Decoder, we also tried a more traditional approach of using a neural network for learning a target language model (CSLM) (Schwenk, 2007).", "startOffset": 212, "endOffset": 227}, {"referenceID": 7, "context": "The concatenated vector was fed through two rectified layers (of size 1536 and 1024) (Glorot et al., 2011).", "startOffset": 85, "endOffset": 106}, {"referenceID": 23, "context": "The model was used to score partial translations during the decoding process, which generally leads to higher gains in BLEU score than n-best list rescoring (Vaswani et al., 2013).", "startOffset": 157, "endOffset": 179}, {"referenceID": 4, "context": "This allows us to perform fast matrix-matrix multiplication on GPU using Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 80, "endOffset": 125}, {"referenceID": 1, "context": "This allows us to perform fast matrix-matrix multiplication on GPU using Theano (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 80, "endOffset": 125}, {"referenceID": 20, "context": "For CSLMs this shortcoming can be addressed by using a separate back-off n-gram language model that only contains non-shortlisted words (see (Schwenk, 2007)).", "startOffset": 141, "endOffset": 156}, {"referenceID": 2, "context": ", (Bengio et al., 2003; Mikolov et al., 2013)).", "startOffset": 2, "endOffset": 45}, {"referenceID": 15, "context": ", (Bengio et al., 2003; Mikolov et al., 2013)).", "startOffset": 2, "endOffset": 45}], "year": 2014, "abstractText": "In this paper, we propose a novel neural network model called RNN Encoder\u2013Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2013Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "creator": "LaTeX with hyperref package"}}}