{"id": "1406.1385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2014", "title": "Learning the Information Divergence", "abstract": "Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the beta-divergence family. Selecting the best beta then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate alpha-divergence in terms of beta-divergence, which enables automatic selection of alpha by maximum likelihood with reuse of the learning principle for beta-divergence. Furthermore, we show the connections between gamma and beta-divergences as well as R\\'enyi and alpha-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families.\n\n\n\n\nThe results indicate that in the current version of the machine learning model, the bias between alpha-divergence, and beta-divergence cannot be quantified by its use of stochastic regression. If we use Bayesian distribution for the binary distribution of beta-divergence in a particular task, we find a very small bias in alpha-divergence (e.g., a difference in beta-divergence in the alpha-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in the beta-divergence in", "histories": [["v1", "Thu, 5 Jun 2014 13:44:25 GMT  (358kb)", "http://arxiv.org/abs/1406.1385v1", "12 pages, 7 figures"]], "COMMENTS": "12 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["onur dikmen", "zhirong yang", "erkki oja"], "accepted": false, "id": "1406.1385"}, "pdf": {"name": "1406.1385.pdf", "metadata": {"source": "CRF", "title": "Learning the Information Divergence", "authors": ["Onur Dikmen", "Zhirong Yang"], "emails": ["onur.dikmen@aalto.fi;", "rong.yang@aalto.fi;", "erkki.oja@aalto.fi"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n13 85\nv1 [\ncs .L\nG ]\n5 J\nun 2\n01 4\nIndex Terms\u2014information divergence, Tweedie distribution, maximum likelihood, nonnegative matrix factorization, stochastic neighbor embedding.\nI. INTRODUCTION\nInformation divergences are an essential element in modern machine learning. They originated in estimation theory where a divergence maps the dissimilarity between two probability distributions to nonnegative values. Presently, information divergences have been extended for nonnegative tensors and used in many learning problems where the objective is to minimize the approximation error between the observed data and the model. Typical applications include Nonnegative Matrix Factorization (see e.g. [1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].\nThere exist a large variety of information divergences. In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.g. [15]). The four parametric families in turn belong to broader ones such as the Csisz\u00e1r-Morimoto f -divergences [16], [17] and Bregman divergences [18]. Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].\nThe authors are with Department of Information and Computer Science, Aalto University, 00076, Finland. e-mail: onur.dikmen@aalto.fi; zhirong.yang@aalto.fi; erkki.oja@aalto.fi\nCompared to the rich set of available information divergences, there is little research on how to select the best one for a given application. This is an important issue because the performance of a given divergence-based estimation or modeling method in a particular task very much depends on the divergence used. Formulating a learning task in a family of divergences greatly increases the flexibility to handle different types of noise in data. For example, Euclidean distance is suitable for data with Gaussian noise; Kullback-Leibler divergence has shown success for finding topics in text documents [7]; and Itakura-Saito divergence has proven to be suitable for audio signal processing [21]. A conventional workaround is to select among a finite number of candidate divergences using a validation set. This however cannot be applied to divergences that are non-separable over tensor entries. The validation approach is also problematic for tasks where all data are needed for learning, for example, cluster analysis.\nIn Section III, we propose a new method of statistical learning for selecting the best divergence among the four popular parametric families in any given data modeling task. Our starting-point is the Tweedie distribution [22], which is known to have a relationship with \u03b2-divergence [23], [24]. The Maximum Tweedie Likelihood (MTL) is in principle a disciplined and straightforward method for choosing the optimal \u03b2 value. However, in order for this to be feasible in practice, two shortcomings with the MTL method have to be overcome: 1) Tweedie distribution is not defined for all \u03b2; 2) calculation of Tweedie likelihood is complicated and prone to numerical problems for large \u03b2. To overcome these drawbacks, we propose here a novel distribution using an exponential over the \u03b2-divergence with a specific augmentation term. The new distribution has the following nice properties: 1) it is close to the Tweedie distribution, especially at four important special cases; 2) it exists for all \u03b2 \u2208 R; 3) its likelihood can be calculated by standard statistical software. We call the new density the Exponential Divergence with Augmentation (EDA). EDA is a non-normalized density, i.e., its likelihood includes a normalizing constant which is not analytically available. But, since the density is univariate the normalizing constant can be efficiently and accurately estimated by numerical integration. The method of Maximizing the Exponential Divergence with Augmentation Likelihood (MEDAL) thus gives a more robust \u03b2 selection in a wider range than MTL. \u03b2 estimation on EDA can also be carried out using parameter estimation methods, e.g., Score Matching (SM) [25], specifically proposed for nonnormalized densities. In the experiments section, we show that SM on EDA also performs as accurately as MEDAL.\nBesides \u03b2-divergence, the MEDAL method is extended to select the best divergence in other parametric families. We reformulate \u03b1-divergence in terms of \u03b2-divergence after a change of parameters so that \u03b1 can be optimized using the\n2 MEDAL method. Our method can also be applied to nonseparable cases. We show the equivalence between \u03b2 and \u03b3-divergences, and between \u03b1 and R\u00e9nyi divergences by a connecting scalar, which allows us to choose the best \u03b3- or R\u00e9nyi-divergence by reusing the MEDAL method.\nWe tested our method with extensive experiments, whose results are presented in Section IV. We have used both synthetic data with a known distribution and real-world data including music, stock prices, and social networks. The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6]. We also demonstrate that our method outperforms Score Matching on Exponential Divergence distribution (ED), a previous approach for \u03b2-divergence selection [29]. Conclusions and discussions on future work are given in Section V.\nII. INFORMATION DIVERGENCES\nMany learning objectives can be formulated as an approximation of the form x \u2248 \u00b5, where x > 0 is the observed data (input) and \u00b5 is the approximation given by the model. The formulation for \u00b5 totally depends on the task to be solved. Consider Nonnegative Matrix Factorization: then x > 0 is a data matrix and \u00b5 is a product of two lower-rank nonnegative matrices which typically give a sparse representation for the columns of x. Other concrete examples are given in Section IV.\nThe approximation error can be measured by various information divergences. Suppose \u00b5 is parameterized by \u0398. The learning problem becomes an optimization procedure that minimizes the given divergence D(x||\u00b5(\u0398)) over \u0398. Regularization may be applied for \u0398 for complexity control. For notational brevity we focus on definitions over vectorial x, \u00b5, \u0398 in this section, while they can be extended to matrices or higher order tensors in a straightforward manner.\nIn this work we consider four parametric families of divergences, which are the widely used \u03b1-, \u03b2-, \u03b3- and R\u00e9nyidivergences. This collection is rich because it covers most commonly used divergences. The definition of the four families and some of their special cases are given below.\n\u2022 \u03b1-divergence [10], [11] is defined as\nD\u03b1(x||\u00b5) = \u2211 i x \u03b1 i \u00b5 1\u2212\u03b1 i \u2212 \u03b1xi + (\u03b1 \u2212 1)\u00b5i\n\u03b1(\u03b1\u2212 1) . (1)\nThe family contains the following special cases:\nD\u03b1=2(x||\u00b5) =DP(x||\u00b5) = 1\n2\n\u2211\ni\n(xi \u2212 \u00b5i)2 \u00b5i\nD\u03b1\u21921(x||\u00b5) =DI(x||\u00b5) = \u2211\ni\n( xi ln\nxi \u00b5i\n\u2212 xi + \u00b5i )\nD\u03b1=1/2(x||\u00b5) =2DH(x||\u00b5) = 2 \u2211\ni\n( \u221a xi \u2212 \u221a \u00b5i) 2\nD\u03b1\u21920(x||\u00b5) =DI(\u00b5||x) = \u2211\ni\n( \u00b5i ln\n\u00b5i xi\n\u2212 \u00b5i + xi )\nD\u03b1=\u22121(x||\u00b5) =DIP(x||\u00b5) = 1\n2\n\u2211\ni\n(xi \u2212 \u00b5i)2 xi\nwhere DI, DP, DIP, and DH denote non-normalized Kullback-Leibler, Pearson Chi-square, inverse Pearson and Hellinger distances, respectively.\n\u2022 \u03b2-divergence [30], [31] is defined as\nD\u03b2(x||\u00b5) = \u2211 i x \u03b2+1 i + \u03b2\u00b5 \u03b2+1 i \u2212 (\u03b2 + 1)xi\u00b5\u03b2i\n\u03b2(\u03b2 + 1) . (2)\nThe family contains the following special cases:\nD\u03b2=1(x||\u00b5) =DEU(x||\u00b5) = 1\n2\n\u2211\ni\n(xi \u2212 \u00b5i)2 (3)\nD\u03b2\u21920(x||\u00b5) =DI(x||\u00b5) = \u2211\ni\n( xi ln\nxi \u00b5i\n\u2212 xi + \u00b5i )\n(4)\nD\u03b2\u2192\u22121(x||\u00b5) =DIS(x||\u00b5) = \u2211\ni\n( xi \u00b5i \u2212 ln xi \u00b5i \u2212 1 )\n(5)\nD\u03b2=\u22122(x||\u00b5) = \u2211\ni\n( xi 2\u00b52i \u2212 1 \u00b5i + 1 2xi ) , (6)\nwhere DEU and DIS denote the Euclidean distance and Itakura-Saito divergence, respectively.\n\u2022 \u03b3-divergence [13] is defined as\nD\u03b3(x||\u00b5) = 1\n\u03b3(\u03b3 + 1)\n[ ln ( \u2211\ni\nx\u03b3+1i\n) + \u03b3 ln ( \u2211\ni\n\u00b5\u03b3+1i\n)\n\u2212(\u03b3 + 1) ln ( \u2211\ni\nxi\u00b5 \u03b3 i\n)] . (7)\nThe normalized Kullback-Leibler (KL) divergence is a special case of \u03b3-divergence:\nD\u03b3\u21920(x||\u00b5) = DKL(x\u0303||\u00b5\u0303) = \u2211\ni\nx\u0303i ln x\u0303i \u00b5\u0303i , (8)\nwhere x\u0303i = xi/ \u2211 j xj and \u00b5\u0303i = \u00b5i/ \u2211\nj \u00b5j . \u2022 R\u00e9nyi divergence [32] is defined as\nD\u03c1(x||\u00b5) = 1 \u03c1\u2212 1 ln ( x\u0303\u03c1i \u00b5\u0303 1\u2212\u03c1 i ) (9)\nfor \u03c1 > 0. The R\u00e9nyi divergence also includes the normalized Kullback-Leibler divergence as its special case when \u03c1 \u2192 1."}, {"heading": "III. DIVERGENCE SELECTION BY STATISTICAL LEARNING", "text": "The above rich collection of information divergences basically allows great flexibility to the approximation framework. However, practitioners must face a choice problem: how to select the best divergence in a family? In most existing applications the selection is done empirically by the human. A conventional automatic selection method is cross-validation [33], [34], where the training only uses part of the entries of x and the remaining ones are used for validation. This method has a number of drawbacks: 1) it is only applicable to the divergences where the entries are separable (e.g. \u03b1or \u03b2-divergence). Leaving out some entries for \u03b3- and R\u00e9nyi divergences is infeasible due to the logarithm or normalization; 2) separation of entries is not applicable in some applications\n3 where all entries are needed in the learning, for example, cluster analysis.\nOur proposal here is to use the familiar and proven technique of maximum likelihood estimation for automatic divergence selection, using a suitably chosen and very flexible probability density model for the data. In the following we discuss this statistical learning approach for automatic divergence selection in the family of \u03b2 -divergences, followed by its extensions to the other divergence families."}, {"heading": "A. Selecting \u03b2-divergence", "text": "1) Maximum Tweedie Likelihood (MTL): We start from the probability density function (pdf) of an exponential dispersion model (EDM) [22]:\npEDM(x; \u03b8, \u03c6, p) = f(x, \u03c6, p) exp\n[ 1\n\u03c6 (x\u03b8 \u2212 \u03ba(\u03b8))\n] (10)\nwhere \u03c6 > 0 is the dispersion parameter, \u03b8 is the canonical parameter, and \u03ba(\u03b8) is the cumulant function (when \u03c6 = 1 its derivatives w.r.t. \u03b8 give the cumulants). Such a distribution has mean \u00b5 = \u03ba\u2032(\u03b8) and variance V (\u00b5, p) = \u03c6\u03ba\u2032\u2032(\u03b8). This density is defined for x \u2265 0, thus \u00b5 > 0.\nA Tweedie distribution is an EDM whose variance has a special form, V (\u00b5) = \u00b5p with p \u2208 R\\(0, 1). The canonical parameter and the cumulant function that satisfy this property are [22]\n\u03b8 = { \u00b51\u2212p\u22121 1\u2212p , if p 6= 1 ln\u00b5, if p = 1 , \u03ba(\u03b8) = { \u00b52\u2212p\u22121 2\u2212p , if p 6= 2 ln\u00b5, if p = 2 .\n(11)\nNote that ln\u00b5 is the limit of \u00b5 t\u22121 t as t \u2192 0. Finite analytical forms of f(x, \u03c6, p) in Tweedie distribution are generally unavailable. The function can be expanded with infinite series [35] or approximated by saddle point estimation [36].\nIt is known that the Tweedie distribution has a connection to \u03b2-divergence (see, e.g., [23], [24]): maximizing the likelihood of Tweedie distribution for certain p values is equivalent to minimizing the corresponding divergence with \u03b2 = 1\u2212 p. Especially, the gradients of the log-likelihood of Gamma, Poisson and Gaussian distributions over \u00b5i are equal to the ones of \u03b2divergence with \u03b2 = \u22121, 0, 1, respectively. This motivates a \u03b2divergence selection method by Maximum Tweedie Likelihood (MTL).\nHowever, MTL has the following two shortcomings. First, Tweedie distribution is not defined for p \u2208 (0, 1). That is, if the best \u03b2 = 1\u2212p happens to be in the range (0, 1), it cannot be found by MTL; in addition, there is little research on the Tweedie distribution with \u03b2 > 1 (p < 0). Second, f(x, \u03c6, p) in Tweedie distribution is not the probability normalizing constant (note that it depends on x), and its evaluation requires ad hoc techniques. The existing software using the infinite series expansion approach [35] (see Appendix A) is prone to numerical computation problems especially for \u22120.1 < \u03b2 < 0. There is no existing implementation that can calculate Tweedie likelihood for \u03b2 > 1.\n2) Maximum Exponential Divergence with Augmentation Likelihood (MEDAL): Our answer to the above shortcomings in MTL is to design an alternative distribution with the following properties: 1) it is close to the Tweedie distribution, especially for the four crucial points when \u03b2 \u2208 {\u22122,\u22121, 0, 1}; 2) it should be defined for all \u03b2 \u2208 R; 3) its pdf can be evaluated more robustly by standard statistical software.\nFrom (10) and (11) the pdf of the Tweedie distribution is written as\npTw(x;\u00b5, \u03c6, \u03b2) = f(x, \u03c6, \u03b2) exp\n[ 1\n\u03c6\n( x\u00b5\u03b2\n\u03b2 \u2212 \u00b5\n\u03b2+1\n\u03b2 + 1\n)] (12)\nw.r.t. \u03b2 instead of p, using the relation \u03b2 = 1\u2212 p. This holds when \u03b2 6= 0 and \u03b2 6= \u22121. The extra terms 1/(1 \u2212 p) and 1/(2\u2212 p) in (11) have been absorbed in f(x, \u03c6, \u03b2). The cases \u03b2 = 0 or \u03b2 = \u22121 have to be analyzed separately.\nTo make an explicit connection with \u03b2-divergence defined in (2), we suggest a new distribution given in the following form:\npapprox(x;\u00b5, \u03c6, \u03b2) = g(x, \u03c6, \u03b2) exp { \u2212 1 \u03c6 D\u03b2(x||\u00b5) }\n= g(x, \u03c6, \u03b2) exp\n[ 1\n\u03c6\n( \u2212 x \u03b2+1\n\u03b2(\u03b2 + 1) +\nx\u00b5\u03b2\n\u03b2 \u2212 \u00b5\n\u03b2+1\n\u03b2 + 1\n)] .\n(13)\nNow the \u03b2-divergence for scalar x appears in the exponent, and g(x, \u03c6, \u03b2) will be used to approximate this with the Tweedie distribution. Ideally, the choice\ng(x, \u03c6, \u03b2) = f(x, \u03c6, \u03b2)/ exp\n[ 1\n\u03c6\n( \u2212 x \u03b2+1\n\u03b2(\u03b2 + 1)\n)]\nwould result in full equivalence to Tweedie distribution, as seen from (12). However, because f(x, \u03c6, \u03b2) is unknown in the general case, such g is also unavailable.\nWe can, however, try to approximate g using the fact that papprox must be a proper density whose integral is equal to one. From (13) it then follows\nexp\n[ 1\n\u03c6\n\u00b5\u03b2+1\n\u03b2 + 1\n]\n= \u222b dx g(x, \u03c6, \u03b2) exp [ 1\n\u03c6\n( \u2212 x \u03b2+1\n\u03b2(\u03b2 + 1) +\nx\u00b5\u03b2\n\u03b2\n)]\n(14)\nThis integral is, of course, impossible to evaluate because we do not even know the function inside. However, the integral can be approximated nicely by Laplace\u2019s method. Laplace\u2019s approximation is\n\u222b b\na\ndx f(x)eMh(x) \u2248 \u221a\n2\u03c0\nM |h\u2032\u2032(x0)| f(x0)e\nMh(x0)\nwhere x0 = argmaxx h(x) and M is a large constant. In order to approximate (14) by Laplace\u2019s method, 1/\u03c6 takes the role of M and thus the approximation is valid for small \u03c6. We need the maximizer of the exponentiated term h(x) = \u2212 x\u03b2+1\u03b2(\u03b2+1) + x\u00b5\u03b2 \u03b2 . This term has a zero first derivative\n4 and negative second derivative, i.e., it is maximized, at x = \u00b5. Thus, Laplace\u2019s method gives us\nexp\n[ 1\n\u03c6\n\u00b5\u03b2+1\n\u03b2 + 1\n]\n\u2248 \u221a 2\u03c0\u03c6 | \u2212 \u00b5\u03b2\u22121| g(\u00b5, \u03c6, \u03b2) exp [ 1 \u03c6 ( \u2212 \u00b5 \u03b2+1 \u03b2(\u03b2 + 1) + \u00b5\u03b2+1 \u03b2 )]\n=\n\u221a 2\u03c0\u03c6\n\u00b5\u03b2\u22121 g(\u00b5, \u03c6, \u03b2) exp\n[ 1\n\u03c6\n\u00b5\u03b2+1\n\u03b2 + 1\n] .\nThe approximation gives g(\u00b5, \u03c6, \u03b2) = 1\u221a 2\u03c0\u03c6 \u00b5(\u03b2\u22121)/2 which suggests the function\ng(x, \u03c6, \u03b2) = 1\u221a 2\u03c0\u03c6 x(\u03b2\u22121)/2 = 1\u221a 2\u03c0\u03c6 exp\n[ (\u03b2 \u2212 1)\n2 lnx\n] .\nPutting this result into (13) as such does not guarantee a proper pdf however, because it is an approximation, only valid at the limit \u03c6 \u2192 0. To make it proper, we have to add a normalizing constant into the density in (13).\nThe pdf of the final distribution, for a scalar argument x, thus becomes\npapprox(x;\u00b5, \u03b2, \u03c6) = 1\nZ(\u00b5, \u03b2, \u03c6) exp\n{ R(x, \u03b2)\u2212 1\n\u03c6 D\u03b2(x||\u00b5)\n}\n(15)\nwhere Z(\u00b5, \u03b2, \u03c6) is the normalizing constant counting for the terms which are independent of x, and R(x, \u03b2) is an augmentation term given as\nR(x, \u03b2) = \u03b2 \u2212 1 2 lnx . (16)\nThis pdf is a proper density for all \u03b2 \u2208 R, which is guaranteed by the following theorem.\nTheorem 1: Let f(x) = exp {\n\u03b2\u22121 2 lnx\u2212 1\u03c6D\u03b2(x||\u00b5)\n} .\nThe improper integral \u222b\u221e 0 f(x)dx converges.\nProof: Let q = \u2223\u2223\u2223\u03b2\u221212 \u2223\u2223\u2223 + 1 + \u01eb with any \u01eb \u2208 (0,\u221e), and g(x) = x\u2212q . By these definitions, we have q > \u2223\u2223\u2223\u03b2\u221212 \u2223\u2223\u2223, and then for x \u2265 1, (\n\u03b2\u22121 2 + q ) \u03c6 lnx \u2264 0 \u2264 D\u03b2(x||\u00b5),\ni.e. 0 \u2264 f(x) \u2264 g(x). By Cauchy convergence test, we know that \u222b\u221e 1 g(x)dx is convergent because q > 1, and so\nis \u222b\u221e 1 f(x)dx. Obviously f(x) is continuous and bounded for\nx \u2208 [0, 1]. Therefore, for x \u2265 0, \u222b\u221e 0 f(x)dx = \u222b 1 0 f(x)dx +\u222b\u221e\n1 f(x)dx also converges.\nFinally, for vectorial x, the pdf is a product of the marginal densities:\npEDA(x;\u00b5, \u03b2, \u03c6) = 1\nZ(\u00b5, \u03b2, \u03c6) exp\n{ R(x, \u03b2) \u2212 1\n\u03c6 D\u03b2(x||\u00b5)\n}\n(17)\nwhere D\u03b2(x||\u00b5) is defined in (2) and\nR(x, \u03b2) = \u03b2 \u2212 1 2\n\u2211\ni\nlnxi . (18)\nWe call (17) the Exponential Divergence with Augmentation (EDA) distribution, because it applies an exponential over an information divergence plus an augmentation term.\nThe log-likelihood of the EDA density can be written as ln p(x;\u00b5, \u03b2, \u03c6) = \u2211\ni\nln p(xi;\u00b5i, \u03b2, \u03c6)\n= \u2211\ni\n[ \u03b2 \u2212 1 2 lnxi \u2212 1 \u03c6 D\u03b2(xi||\u00b5i)\u2212 lnZ(\u00b5i, \u03b2, \u03c6) ] (19)\ndue to the fact that D\u03b2(x||\u00b5) in Eq. (2) and the augmentation term in (18) are separable over xi, (i.e. xi are independent given \u00b5i). The best \u03b2 is now selected by\n\u03b2\u2217 = argmax \u03b2 [ max \u03c6 ln p(x;\u00b5, \u03b2, \u03c6) ] , (20)\nwhere \u00b5 = argmin\u03b7 D\u03b2(x||\u03b7). We call the new divergence selection method Maximum EDA Likelihood (MEDAL).\nLet us look at the four special cases of Tweedie distribution: Gaussian (N ), Poisson (PO), Gamma (G) and Inverse Gaussian (IN ). They correspond to \u03b2 = 1, 0,\u22121,\u22122. For simplicity of notation, we may drop the subscript i and write x and \u00b5 for one entry in x and \u00b5. Then, the log-likelihoods of the above four special cases are\nln pN (x;\u00b5, \u03c6) =\u2212 1 2 ln(2\u03c0\u03c6)\u2212 1 2\u03c6 (x\u2212 \u00b5)2,\nln pPO(x;\u00b5) =x ln\u00b5\u2212 \u00b5\u2212 ln \u0393(x + 1), \u2248x ln\u00b5\u2212 \u00b5\u2212 ln(2\u03c0x)/2\u2212 x lnx+ x,\nln pG(x; 1/\u03c6, \u03c6\u00b5) =(1/\u03c6\u2212 1) lnx\u2212 x\n\u03c6\u00b5\n\u2212 (1/\u03c6) ln(\u03c6\u00b5)\u2212 ln \u0393(1/\u03c6),\nln pIN (x;\u00b5, 1/\u03c6) =\u2212 1 2 ln(2\u03c0\u03c6x3)\u2212 1 \u03c6\n( 1\n2\nx \u00b52 \u2212 1 \u00b5 + 1 2x\n) ,\nwhere in the Poisson case we employ Stirling\u2019s approximation1. To see the similarity of these four special cases with the general expression for the EDA log-likelihood in Eq. (19), let us look at one term in the sum there. It is a fairly straightforward exercise to plug in the \u03b2-divergences from Eqs. (3,4,5,6) and the augmentation term from Eq. (18) and see that the log-likelihoods coincide. The normalizing term lnZ(\u00b5, \u03b2, \u03c6)] for these special cases can be determined from the corresponding density.\nIn general, the normalizing constant Z(\u00b5, \u03b2, \u03c6) is intractable except for a few special cases. Numerical evaluation of Z(\u00b5, \u03b2, \u03c6) can be implemented by standard statistical software. Here we employ the approximation with GaussLaguerre quadratures (details in Appendix B).\nFinally, let us note that in addition to the maximum likelihood estimator, Score Matching (SM) [25], [37] can be applied to estimation of \u03b2 as a density parameter (see Section IV-A). In a previous effort, Lu et al. [29] proposed a similar exponential divergence (ED) distribution\npED(x;\u00b5, \u03b2) \u221d exp [\u2212D\u03b2(x||\u00b5)] , (21)\nbut without the augmentation. It is easy to show that ED also exists for all \u03b2 by changing q = 1+\u01eb in the proof of Theorem\n1The case \u03b2 = 0 and \u03c6 6= 1 does not correspond to Poisson distribution, but the transformation pEDM(x;\u00b5, \u03c6, 1) = pPO(x/\u03c6;\u00b5/\u03c6)/\u03c6 can be used to evaluate the pdf.\n5 1. We will empirically illustrate the discrepancy between ED and EDA in Section IV-A, showing that the selection based on ED is however inaccurate, especially for \u03b2 \u2264 0."}, {"heading": "B. Selecting \u03b1-divergence", "text": "We extend the MEDAL method to \u03b1-divergence selection. This is done by relating \u03b1-divergence to \u03b2-divergence with a nonlinear transformation between \u03b1 and \u03b2. Let yi = x\u03b1i /\u03b1\n2\u03b1, mi = \u00b5 \u03b1 i /\u03b1 2\u03b1 and \u03b2 = 1/\u03b1\u2212 1 for \u03b1 6= 0. We have\nD\u03b2(yi||mi) = 1\n\u03b2(\u03b2 + 1)\n( y\u03b2+1i + \u03b2m \u03b2+1 i \u2212 (\u03b2 + 1)yim\u03b2i )\n= \u2212\u03b12 \u03b1\u2212 1 ( xi \u03b12 + 1\u2212 \u03b1 \u03b1 \u00b5i \u03b12 \u2212 1 \u03b1 x\u03b1i \u03b12\u03b1 \u00b51\u2212\u03b1i \u03b12(1\u2212\u03b1) ) =D\u03b1(xi||\u00b5i)\nThis relationship allows us to evaluate the likelihood of \u00b5 and \u03b1 using yi and \u03b2:\np(xi;\u00b5i, \u03b1, \u03c6) = p(yi;mi, \u03b2, \u03c6) \u2223\u2223\u2223\u2223 dyi dxi \u2223\u2223\u2223\u2223\n= p(yi;mi, \u03b2, \u03c6) x\u03b1\u22121i\n\u03b12(\u03b1\u22121/2)\n= p(yi;mi, \u03b2, \u03c6)y \u2212\u03b2 i |\u03b2 + 1|\nIn vectorial form, the best \u03b1 for D\u03b1(x||\u00b5) is then given by \u03b1\u2217 = 1/(\u03b2\u2217 + 1) where\n\u03b2\u2217 =argmax \u03b2 { max \u03c6 [ ln p(y;m, \u03b2)\n\u2212 \u03b2 ln yi + ln |\u03b2 + 1| ]} , (22)\nwhere m = argmin\u03b7 D\u03b2(y||\u03b7). This transformation method can handle all \u03b1 except \u03b1 \u2192 0 since it corresponds to \u03b2 \u2192 \u221e."}, {"heading": "C. Selecting \u03b3- and R\u00e9nyi divergences", "text": "Above we presented the selection methods for two families where the divergence is separable over the tensor entries. Next we consider selection among \u03b3- and R\u00e9nyi divergence families where their members are not separable. Our strategy is to reduce \u03b3-divergence to \u03b2-divergence with a connecting scalar. This is formally given by the following result.\nTheorem 2: For x \u2265 0 and \u03c4 \u2208 R,\nargmin \u00b5\u22650 D\u03b3\u2192\u03c4 (x||\u00b5) = argmin \u00b5\u22650 [ min c>0 D\u03b2\u2192\u03c4 (x||c\u00b5) ] (23)\nThe proof is done by zeroing the derivative right hand side with respect to c (details in Appendix C).\nTheorem 2 states that with a positive scalar, the learning problem formulated by a \u03b3-divergence is equivalent to the one by the corresponding \u03b2-divergence. The latter is separable and can be solved by the methods described in the Section III-A. An example is between normalized KL-divergence (in \u03b3-divergence) and the non-normalized KL-divergence (in \u03b2divergence) with the optimal connecting scalar c =\n\u2211 i xi\u2211 i \u00b5i\n. Example applications on selecting the best \u03b3-divergence are given in Section IV-D.\nSimilarly, we can also reduce a R\u00e9nyi divergence to its corresponding \u03b1-divergence with the same proof technique (see Appendix C).\nTheorem 3: For x \u2265 0 and \u03c4 > 0,\nargmin \u00b5\u22650 D\u03c1\u2192\u03c4 (x||\u00b5) = argmin \u00b5\u22650 [ min c>0 D\u03b1\u2192\u03c4 (x||c\u00b5) ] . (24)"}, {"heading": "IV. EXPERIMENTS", "text": "In this section we demonstrate the proposed method on various data types and learning tasks. First we provide the results on synthetic data, whose density is known, to compare the behavior of MTL, MEDAL and the score matching method [29]. Second, we illustrate the advantage of the EDA density over ED. Third, we apply our method on \u03b1- and \u03b2-divergence selection in Nonnegative Matrix Factorization (NMF) on realworld data including music and stock prices. Fourth, we test MEDAL in selecting non-separable cases (e.g. \u03b3-divergence) for Projective NMF and s-SNE visualization learning tasks across synthetic data, images, and a dolphin social network."}, {"heading": "A. Synthetic data", "text": "1) \u03b2-divergence selection: We use here scalar data generated from the four special cases of Tweedie distributions, namely, Inverse Gaussian, Gamma, Poisson, and Gaussian distributions. We simply fit the best Tweedie, EDA or ED density to the data using either the maximum likelihood method or score matching (SM).\nIn Fig. 1 (first row), the results of the Maximum Tweedie Likelihood (MTL) are shown. The \u03b2 value that maximizes the likelihood in Tweedie distribution is consistent with the true parameters, i.e., -2, -1, 0 and 1 respectively for the above distributions. Note that Tweedie distributions are not defined for \u03b2 \u2208 (0, 1), but \u03b2-divergence is defined in this region, which will lead to discontinuity in the log-likelihood over \u03b2.\nThe second and third rows in Fig. 1 present results of the exponential divergence density ED given in Eq. (21). The loglikelihood and negative score matching objectives [29] on the same four datasets are shown. The estimates are consistent with the ground truth Gaussian and Poisson data. However, for Gamma and Inverse Gaussian data, both \u03b2 estimates deviate from the ground truth. Thus, estimators based on ED do not give as accurate estimates as the MTL method. The ED distribution [29] has an advantage that it is defined also for \u03b2 \u2208 (0, 1). In the above, we have seen that \u03b2 selection by using ED is accurate when \u03b2 \u2192 0 or \u03b2 = 1. However, as explained in Section III-A2, in the other cases ED and Tweedie distributions are not the same because the terms containing the observed variable in these distributions are not exactly the same as those of the Tweedie distributions.\nEDA, the augmented ED density introduced in Section III-A, not only has both the advantage of continuity but also gives very accurate estimates for \u03b2 < 0. The MEDAL loglikelihood curves over \u03b2 based on EDA are given in Fig. 1 (fourth row). In the \u03b2 selection of Eq. (20), the \u03c6 value that maximizes the likelihood with \u03b2 fixed is found by a grid search. The likelihood values are the same as those of special Tweedie distributions and there are no abrupt changes\nor discontinuities in the likelihood surface. We also estimated \u03b2 for the EDA density using Score Matching, and curves of the negative SM objective are presented in the bottom row of Fig. 1. They also recover the ground truth accurately.\n2) \u03b1-divergence selection: There is only one known generative model for which the maximum likelihood estimator corresponds to the minimizer of the corresponding \u03b1 divergence. It is the Poisson distribution. We thus reused the Poisson-distributed data of the previous experiments with the \u03b2-divergence. In Fig. 2a, we present the log-likelihood objective over \u03b1 obtained with Tweedie distribution (MTL)\nand the transformation from Section III-B. The ground truth \u03b1 \u2192 1 is successfully recovered with MTL. However, there are no likelihood estimates for \u03b1 \u2208 (0.5, 1), corresponding to \u03b2 \u2208 (0, 1) for which no Tweedie distributions are defined. Moreover, to our knowledge there are no studies concerning the pdf\u2019s of Tweedie distributions with \u03b2 > 1. For that reason, the likelihood values for \u03b1 \u2208 [0, 0.5) are left blank in the plot.\nIt can be seen from Fig. 2b and 2c, that the augmentation in the MEDAL method also helps in \u03b1 selection. Again, both ED and EDA solve most of the discontinuity problem except \u03b1 = 0. Selection using ED fails to find the ground truth\nwhich equals 1, which is however successfully found by the MEDAL method. SM on EDA recovers the ground truth as well (Fig. 2d)."}, {"heading": "B. Divergence selection in NMF", "text": "The objective in nonnegative matrix factorization (NMF) is to find a low-rank approximation to the observed data by expressing it as a product of two nonnegative matrices, i.e., V \u2248 V\u0302 = WH with V \u2208 RF\u00d7N+ , W \u2208 RF\u00d7K+ and H \u2208 RK\u00d7N+ . This objective is pursued through the minimization of an information divergence between the data and the approximation, i.e., D(V||V\u0302). The divergence can be any appropriate one for the data/application such as \u03b2, \u03b1, \u03b3, R\u00e9nyi, etc. Here, we chose the \u03b2 and \u03b1 divergences to illustrate the MEDAL method for realistic data.\nThe optimization of \u03b2-NMF was implemented using the standard multiplicative update rules [23], [38]. Similar multiplicative update rules are also available for \u03b1-NMF [23]. Alternatively, the algorithm for \u03b2-NMF can be used for \u03b1divergence minimization as well, using the transformation explained in Section III-B.\n1) A Short Piano Excerpt: We consider the piano data used in [21]. It is an audio sequence recorded in real conditions, consisting of four notes played all together in the first measure and in all possible pairs in the subsequent measures. A power spectrogram with analysis window of size 46 ms was computed, leading to F = 513 frequency bins and N = 676 time frames. These make up the data matrix V, for which a matrix factorization V\u0302 = WH with low rank K = 6 is sought for.\nIn Fig. 3a and 3b, we show the log-likelihood values of the MEDAL method for \u03b2 and \u03b1, respectively. For each parameter value \u03b2 and \u03b1, the multiplicative algorithm for the respective divergence is run for 100 iterations and likelihoods are evaluated with mean values calculated from the returned matrix factorizations. For each value of \u03b2 and \u03b1, the highest likelihood w.r.t. \u03c6 (see Eq. (20)) is found by a grid search.\nThe found maximum likelihood estimate \u03b2 = \u22121 corresponds to Itakura-Saito divergence, which is in harmony with the empirical results presented in [21] and the common belief that IS divergence is most suitable for audio spectrograms. The optimal \u03b1 value value was 0.5 corresponding to Hellinger distance. We can also see that the log likelihood value associated\nwith \u03b1 = 0.5 is still much less than the one for \u03b2 = \u22121. SM also finds \u03b2 = \u22121 as can be seen from Fig. 3c."}, {"heading": "C. Stock Prices", "text": "Next, we repeat the same experiment on a stock price dataset which contains Dow Jones Industrial Average. There are 30 companies included in the data. They are major American companies from various sectors such as services (e.g., Walmart), consumer goods (e.g., General Motors) and healthcare (e.g., Pfizer). The data was collected from 3rd January 2000 to 27th July 2011, in total 2543 trading dates. We set K = 5 in NMF and masked 50% of the data by following [39]. The stock data curves are displayed in Fig. 4 (left).\nThe EDA likelihood curve with \u03b2 \u2208 [\u22122, 2] is shown in Figure 4 (bottom left). We can see that the best divergence selected by MEDAL is \u03b2 = 0.4. The corresponding best \u03c6 = 0.006. These results are in harmony with the findings of Tan and F\u00e9votte [39] using the remaining 50% of the data as validation set, where they found that \u03b2 \u2208 [0, 0.5] (mind that our \u03b2 values equal theirs minus one) performs\n8 500 1000 1500 2000 2500 0 50 100 150\ndays\nst oc\nk pr\nic es\n\u22122 \u22121 0 1 2 \u22128\n\u22126\n\u22124\n\u22122\n0x 10 4\nlo g\u2212\nlik el\nih oo\nd (E\nD A\n)\n\u03b2 (best \u03b2=0.4)\nstock prices\n\u22122 \u22121 0 1 2 0\n2\n4\n6\n8x 10 8\nne ga\ntiv e\nS M\no bj\nec tiv\ne (E\nD A\n)\n\u03b2 (best \u03b2=1)\nstock prices\nFig. 4. Top: the stock data. Bottom left: the EDA log-likelihood for \u03b2 \u2208 [\u22122, 2]. Bottom right: negative SM objective function for \u03b2 \u2208 [\u22122, 2].\nwell for a large range of \u03c6\u2019s. Differently, our method is more advantageous because we do not need additional criteria nor data for validations. In Figure 4 (bottom right), negative SM objective function is plotted for \u03b2 \u2208 [\u22122, 2]. With SM, the optimal \u03b2 is found to be 1."}, {"heading": "D. Selecting \u03b3-divergence", "text": "In this section we demonstrate that the proposed method can be applied to applications beyond NMF and to nonseparable divergence families. To our knowledge, no other existing methods can handle these two cases.\n1) Multinomial data: We first exemplify \u03b3-divergence selection for synthetic data drawn from a multinomial distribution. We generated a 1000-dimensional stochastic vector p from the uniform distribution. Next we drew x \u223c Multinomial(n,p) with n = 107. The MEDAL method is applied to find the best \u03b3-divergence for the approximation of x by p.\nFig. 6 (1st row, left) shows the MEDAL log-likelihood. The peak appears when \u03b3 = 0, which indicates that the normalized KL-divergence is the most suitable one among the \u03b3-divergence family. Selection using score matching of EDA gives the best \u03b3 also close to zero (Fig. 6 1st row, right). The result is expected, because the maximum likelihood estimator of p in multinomial distribution is equivalent to minimizing the KL-divergence over p. Our finding also justifies the usage of KL-divergence in topic models with the multinomial distribution [40], [7].\n2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19]. Given a nonnegative matrix V \u2208 RF\u00d7N+ , PNMF seeks a low-rank nonnegative matrix W \u2208 RF\u00d7K+ (K < F ) that minimizes D\u03b3 ( V||V\u0302 ) , where V\u0302 = WWTV. PNMF is able to produce a highly orthogonal W and thus finds its applications in part-based feature extraction and clustering analysis, etc. Different from conventional NMF (or linear NMF) where each factorizing\nmatrix only appears once in the approximation, the matrix W occurs twice in V\u0302. Thus it is a special case of Quadratic Nonnegative Matrix Factorization (QNMF) [41].\nWe choose PNMF for two reasons: 1) we demonstrate the MEDAL performance on QNMF besides the linear NMF already shown in Section IV-B; 2) PNMF contains only one variable matrix in learning, without the issue of how to interleave the updates of different variable matrices.\nWe first tested MEDAL on a synthetic dataset. We generated a diagonal blockwise data matrix V of size 50\u00d730, where two blocks are of sizes 30\u00d7 20 and 20\u00d7 10. The block entries are uniformly drawn from [0, 10]. We then added uniform noise from [0, 1] to the all matrix entries. For each \u03b3, we ran the multiplicative algorithm of PNMF by Yang and Oja [28], [42] to obtain W and V\u0302. The MEDAL method was then applied to select the best \u03b3. The resulting approximated log-likelihood for \u03b3 \u2208 [\u22122, 2] is shown in Fig. 6 (2nd row). We can see MEDAL and score matching of EDA give similar results, where the best \u03b3 appear at \u22120.76 and \u22120.8, respectively. Both resulting W \u2019s give perfect clustering accuracy of data rows.\nWe also tested MEDAL on the swimmer dataset [43] which is popularly used in the NMF field. Some example images from this dataset are shown in Fig. 5 (left). We vectorized each image in the dataset as a column and concatenated the columns into a 1024\u00d7 256 data matrix V. This matrix is then fed to PNMF and MEDAL as in the case for the synthetic dataset. Here we empirically set the rank to K = 17 according to Tan and F\u00e9votte [44] and Yang et al. [45]. The matrix W was initialized by PNMF based on Euclidean distance to avoid poor local minima. The resulting approximated log-likelihood for \u03b3 \u2208 [\u22121, 3] is shown in Figure 6 (3rd row, left). We can see a peak appearing around 1.7. Zooming in the region near the peak shows the best \u03b3 = 1.69. The score matching objective over \u03b3 values (Fig. 6 3rd row, right) shows a similar peak and the best \u03b3 very close to the one given by MEDAL. Both methods result in excellent and nearly identical basis matrix (W) of the data, where the swimmer body as well as four limbs at four angles are clearly identified (see Fig. 5 bottom row).\n3) Symmetric Stochastic Neighbor Embedding: Finally, we show an application beyond NMF, where MEDAL is used to find the best \u03b3-divergence for the visualization using Symmetric Stochastic Neighbor Embedding (s-SNE) [5], [6].\nSuppose there are n multivariate data samples {xi}ni=1 with xi \u2208 RD and their pairwise similarities are represented by an n \u00d7 n symmetric nonnegative matrix P where Pii = 0 and \u2211 ij Pij = 1. The s-SNE visualization seeks a lowdimensional embedding Y = [y1,y2, . . . ,yn] T \u2208 Rn\u00d7d such that pairwise similarities in the embedding approximate those in the original space. Generally d = 2 or d = 3 for easy visualization. Denote qij = q(\u2016yi \u2212 yj\u20162) with a certain kernel function q, for example qij = ( 1 + \u2016yi \u2212 yj\u20162 )\u22121 . The pairwise similarities in the embedding are then given by Qij = qij/ \u2211 kl:k 6=l qkl. The s-SNE target is that Q is as close to P as possible. To measure the dissimilarity between P and Q, the conventional s-SNE uses the Kullback-Leibler divergence DKL(P||Q). Here we generalize s-SNE to the\n9\nwhole family of \u03b3-divergences as dissimilarity measures and select the best divergence by our MEDAL method.\nWe have used a real-world dolphins dataset2. It is the adjacency matrix of the undirected social network between 62 dolphins. We smoothed the matrix by PageRank random walk in order to find its macro structures. The smoothed matrix was then fed to s-SNE based on \u03b3-divergence, with \u03b3 \u2208 [\u22122, 2]. The EDA log-likelihood is shown in Fig. 6 (4th row, left). By the MEDAL principle the best divergence is \u03b3 = \u22120.6 for s-SNE and the dolphins dataset. Score matching of EDA also indicates the best \u03b3 is smaller than 0. The resulting visualizations created by s-SNE with the respective best gamma-divergence are shown in Fig. 7, where the node layouts by both methods are very similar. In both visualizations we can clearly see two dolphin communities."}, {"heading": "V. CONCLUSIONS", "text": "We have presented a new method called MEDAL to automatically select the best information divergence in a parametric family. Our selection method is built upon a statistical learning approach, where the divergence is learned as the result of standard density parameter estimation. Maximizing the likelihood of the Tweedie distribution is a straightforward way for selecting \u03b2-divergence, which however has some shortcomings. We have proposed a novel distribution, the Exponential Divergence with Augmentation (EDA), which overcomes these shortcomings and thus can give a more robust\n2available at http://www-personal.umich.edu/~mejn/netdata/\nselection for the parameter over a wider range. The new method has been extended to \u03b1-divergence selection by a nonlinear transformation. Furthermore, we have provided new results that connect the \u03b3- and \u03b2-divergences, which enable us to extend the selection method to non-separable cases. The extension also holds for R\u00e9nyi divergence with similar relationship to \u03b1-divergence. As a result, our method can be applied to most commonly used information divergences in learning.\nWe have performed extensive experiments to show the accuracy and applicability of the new method. Comparison on synthetic data has illustrated that our method is superior to Maximum Tweedie Likelihood, i.e., it finds the ground truth as accurately as MTL, while being defined on all values of \u03b2 and being less prone to numerical problems (no abrupt changes in the likelihood). We also showed that a previous estimation\n10\napproach by Score Matching on Exponential Divergence distribution (ED, i.e., EDA before augmentation) is not accurate, especially for \u03b2 < 0. In the application to NMF, we have provided experimental results on various kinds of data including audio and stock prices. In the non-separable cases, we have demonstrated selecting \u03b3-divergence for synthetic data, Projective NMF, and visualization by s-SNE. In those cases where the correct parameter value is known in advance for the synthetic data, or there is a wide consensus in the application community on the correct parameter value for real-world data, the MEDAL method gives expected results. These results show that the presented method has not only broad applications but also accurate selection performance. In the case of new kinds of data, for which the appropriate information divergence is not known, the MEDAL method provides a disciplined and rigorous way to compute the optimal parameter values.\nIn this paper we have focused on information divergence for vectorial data. There exist other divergences for higher-order tensors, for example, LogDet divergence and von Newmann divergence (see e.g. [47]) that are defined over eigenvalues of matrices. Selection among these divergences remains an open problem.\nHere we mainly consider a positive data matrix and selecting the divergence parameter in (\u221e,+\u221e). Tweedie distribution has no support for zero entries when \u03b2 < 0 and thus gives zero likelihood of the whole matrix/tensor by independence. In future work, extension of EDA to accommodate nonnegative data matrices could be developed for \u03b2 \u2265 0.\nMEDAL is a two-phase method: the \u03b2 selection is based on the optimization result of \u00b5. Ideally, both variables should be selected by optimizing the same objective. For maximum log-likelihood estimator, this requires that the negative loglikelihood equals the \u03b2-divergence, which is however infea-\nsible for all \u03b2 due to intractability of integrals. Non-ML estimators could be used to attack this open problem.\nThe EDA distribution family includes the exact Gaussian, Gamma, and Inverse Gaussian distributions, and approximated Poisson distribution. In the approximation we used the firstorder Stirling expansion. One could apply higher-order expansions to improve the approximation accuracy. This could be implemented by further augmentation with higher-order terms around \u03b2 \u2192 0."}, {"heading": "VI. ACKNOWLEDGMENT", "text": "This work was financially supported by the Academy of Finland (Finnish Center of Excellence in Computational Inference Research COIN, grant no 251170; Zhirong Yang additionally by decision number 140398)."}, {"heading": "APPENDIX A INFINITE SERIES EXPANSION IN TWEEDIE DISTRIBUTION", "text": "In the series expansion, an EDM random variable is represented as a sum of G independent Gamma random variables x = \u2211G g yg , where G is Poisson distributed with parameter \u03bb = \u00b5 2\u2212p\n\u03c6(2\u2212p) ; and the shape and scale parameters of the Gamma\ndistribution are \u2212a and b, with a = 2\u2212p1\u2212p and b = \u03c6(p\u22121)\u00b5p\u22121. The pdf of the Tweedie distribution is obtained analytically at x = 0 as e\u2212 \u00b52\u2212p\n\u03c6(2\u2212p) . For x > 0 the function f(x, \u03c6, p) = 1 x \u2211\u221e j=1 Wj(x, \u03c6, p), where for 1 < p < 2\nWj = x\u2212ja(p\u2212 1)ja\n\u03c6j(1\u2212a)(2 \u2212 p)jj!\u0393(\u2212ja) (25)\nand for p > 2\nWj = 1\n\u03c0 \u0393(1 + ja)\u03c6j(a\u22121)(p\u2212 1)ja \u0393(1 + j)(p\u2212 1)jxja (\u22121) j sin(\u2212\u03c0ja). (26)\nThis infinite summation needs approximation in practice. Dunn and Smyth [35] described an approach to select a subset of these infinite terms to accurately approximate f(x, \u03c6, p). In their approach, Stirling\u2019s approximation of the Gamma functions are used to find the index j which gives the highest value of the function. Then, in order to find the most significant region, the indices are progressed in both directions until negligible terms are reached."}, {"heading": "APPENDIX B GAUSS-LAGUERRE QUADRATURES", "text": "This method (e.g. [48]) can evaluate definite integrals of the form\n\u222b \u221e\n0\ne\u2212zf(z)dz \u2248 n\u2211\ni\nf(zi)wi, (27)\nwhere zi is the ith root of the n-th order Laguerre polynomial Ln(z), and the weights are given by\nwi = zi\n(n+ 1)2L2n(zi) . (28)\nThe recursive definition of Ln(z) is given by\nLn+1(z) = 1\nn+ 1 [(2n+ 1\u2212 z)Ln(z)\u2212 nLn\u22121(z)] , (29)\n11\nwith L0(z) = 1 and L1(z) = 1 \u2212 z. In our experiments, we used the Matlab implementation by Winckel3 with n = 5000."}, {"heading": "APPENDIX C PROOFS OF THEOREMS 2 AND 3", "text": "Lemma 4: argminz af(z) = argminz a ln f(z) for a \u2208 R and f(z) > 0. The proof of the lemma is simply by the monotonicity of ln.\nNext we prove Theorem 2. For \u03b2 \u2208 R\\{\u22121, 0}, zeroing \u2202D\u03b2(x||c\u00b5)\n\u2202c gives\nc\u2217 =\n\u2211 i xi\u00b5\n\u03b2 i\u2211\ni \u00b5 1+\u03b2 i\n. (30)\nPutting it back to min\u00b5mincD\u03b2(x||c\u00b5), we obtain: min \u00b5 min c D\u03b2(x||c\u00b5)\n=min \u00b5\n1\n\u03b2(1 + \u03b2)\n  \u2211\ni\nx1+\u03b2i + \u03b2 \u2211\ni\n(\u2211 j xj\u00b5\n\u03b2 j\u2211\nj \u00b5 1+\u03b2 j\n\u00b5i\n)1+\u03b2\n\u2212(1 + \u03b2) \u2211\ni\nxi\n(\u2211 j xj\u00b5\n\u03b2 j\u2211\nj \u00b5 1+\u03b2 j\n\u00b5i\n)\u03b2 \n=min \u00b5\n1\n\u03b2(1 + \u03b2)\n  \u2211\ni\nx1+\u03b2i \u2212\n(\u2211 i xi\u00b5 \u03b2 i )1+\u03b2\n(\u2211 j \u00b5 1+\u03b2 j )\u03b2\n \nDropping the constant, and by Lemma 4, the above is equivalent to minimizing\n1\n\u03b2(1 + \u03b2)\n \u03b2 ln  \u2211\nj\n\u00b51+\u03b2j\n \u2212 (1 + \u03b2) ln ( \u2211\ni\nxi\u00b5 \u03b2 i\n) \nAdding a constant 1\u03b2(1+\u03b2) ln (\u2211 i x 1+\u03b2 i ) , the objective becomes minimizing \u03b3-divergence (replacing \u03b2 with \u03b3; see Eq. (7)).\nWe can apply the similar technique to prove Theorem 3. For \u03b1 \u2208 R\\{0, 1}, zeroing \u2202D\u03b1(x||c\u00b5)\u2202c gives\nc\u2217 =\n(\u2211 i x \u03b1 i \u00b5\n1\u2212\u03b1 i\u2211\ni \u00b5i\n)1/\u03b1 (31)\nPutting it back, we obtain\nD\u03b1(x||c\u2217\u00b5) (32)\n= 1 \u03b1(1\u2212 \u03b1) \u2211\ni\n  \u03b1xi + (1 \u2212 \u03b1) (\u2211 j x \u03b1 j \u00b5 1\u2212\u03b1 j\u2211\nj \u00b5j\n)1/\u03b1 \u00b5i\n(33)\n\u2212x\u03b1i\n  (\u2211 j x \u03b1 j \u00b5 1\u2212\u03b1 j\u2211\nj \u00b5j\n)1/\u03b1 \u00b5i   1\u2212\u03b1\n (34)\n= 1\n\u03b1\u2212 1\n \u2211\ni\nx\u03b1i ( \u00b5i\u2211 j \u00b5j )1\u2212\u03b1  1/\u03b1 + \u2211 i xi 1\u2212 \u03b1 . (35)\n3available at http://www.mathworks.se/matlabcentral/fileexchange/\nDropping the constant \u2211\ni xi 1\u2212\u03b1 , and by Lemma 4, minimizing\nthe above is equivalent to minimization of (for \u03b1 > 0)\n1\n\u03b1\u2212 1 ln\n \u2211\ni\nx\u03b1i ( \u00b5i\u2211 j \u00b5j )1\u2212\u03b1  (36)\nAdding a constant \u03b11\u2212\u03b1 ln \u2211\ni xi to the above, the objective becomes minimizing R\u00e9nyi-divergence (replacing \u03b1 with \u03c1; see Eq. (9)).\nThe proofs for the special cases are similar, where the main steps are given below\n\u2022 \u03b2 = \u03b3 \u2192 0 (or \u03b1 = \u03c1 \u2192 1): zeroing \u2202D\u03b2\u21920(x||c\u00b5)\u2202c gives c\u2217 =\n\u2211 i xi\u2211 i \u00b5i\n. Putting it back, we obtain D\u03b2\u21920(x||c\u2217\u00b5) = ( \u2211\ni xi)D\u03b3\u21920(x||\u00b5). \u2022 \u03b2 = \u03b3 \u2192 \u22121: zeroing \u2202D\u03b2\u2192\u22121(x||c\u00b5)\u2202c gives c\u2217 =\n1 M \u2211 i xi \u00b5i\n, where M is the length of x. Putting it back, we obtain D\u03b2\u2192\u22121(x||c\u2217\u00b5) = MD\u03b3\u2192\u22121(x||\u00b5).\n\u2022 \u03b1 = \u03c1 \u2192 0: zeroing \u2202D\u03b1\u21920(x||c\u00b5)\u2202c gives\nc\u2217 = exp ( \u2212 \u2211 i \u00b5i ln \u00b5i xi\u2211\ni \u00b5i\n) .\nPutting it back, we obtain\nD\u03b1\u21920(x||c\u2217\u00b5) = \u2212 exp ( \u2212 \u2211\ni\n\u00b5\u0303i ln \u00b5\u0303i xi\n) + \u2211\ni\nxi,\nwhere \u00b5\u0303i = \u00b5i/ \u2211 j \u00b5j . Dropping the constant \u2211\ni xi, minimizing D\u03b1\u21920(x||c\u2217\u00b5) is equivalent to minimization of \u2211 i \u00b5\u0303i ln \u00b5\u0303i xi . Adding the constant ln \u2211\nj xj to the latter, the objective becomes identical to D\u03c1\u21920(x||\u00b5), i.e. DKL(\u00b5||x)."}], "references": [{"title": "A generalized divergence measure for nonnegative matrix factorization,", "author": ["R. Kompass"], "venue": "Neural Computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Generalized nonnegative matrix approximations with Bregman divergences,", "author": ["I.S. Dhillon", "S. Sra"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Non-negative matrix factorization with \u03b1-divergence,", "author": ["A. Cichocki", "H. Lee", "Y.-D. Kim", "S. Choi"], "venue": "Pattern Recognition Letters,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Stochastic neighbor embedding,", "author": ["G. Hinton", "S. Roweis"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Visualizing data using t-SNE,", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Rethinking collapsed variational bayes inference for lda,", "author": ["I. Sato", "H. Nakagawa"], "venue": "in International Conference on Machine Learning (ICML),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Divergence measures and message passing,", "author": ["T. Minka"], "venue": "Microsoft Research, Tech. Rep.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "A measure of asymptotic efficiency for tests of a hypothesis based on a sum of observations,", "author": ["H. Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1952}, {"title": "Differential-Geometrical Methods in Statistics", "author": ["S. Amari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1985}, {"title": "Robust and efficient estimation by minimising a density power divergence,", "author": ["A. Basu", "I.R. Harris", "N. Hjort", "M. Jones"], "venue": "Biometrika, vol", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Robust paramater estimation with a small bias against heavy contamination,", "author": ["H. Fujisawa", "S. Eguchi"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Families of alpha- beta- and gammadivergences: Flexible and robust measures of similarities,", "author": ["A. Cichocki", "S.-i. Amari"], "venue": "Entropy, vol. 12,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Generalized alpha-beta divergences and their application to robust nonnegative matrix factorization,", "author": ["A. Cichocki", "S. Cruces", "S.-I. Amari"], "venue": "Entropy, vol", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Eine informationstheoretische ungleichung und ihre anwendung auf den beweis der ergodizitat von markoffschen ketten,", "author": ["I. Csisz\u00e1r"], "venue": "Publications of the Mathematical Institute of Hungarian Academy of Sciences Series A,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Markov processes and the h-theorem,", "author": ["T. Morimoto"], "venue": "Journal of the Physical Society of Japan,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1963}, {"title": "The relaxation method of finding the common points of convex sets and its application to the solution of problems in convex programming,", "author": ["L.M. Bregman"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1967}, {"title": "Kullback-leibler divergence for nonnegative for nonnegative matrix factorization,", "author": ["Z. Yang", "H. Zhang", "Z. Yuan", "E. Oja"], "venue": "Proceedings of 21st International Conference on Artificial Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Projective nonnegative matrix factorization with \u03b1-divergence,", "author": ["Z. Yang", "E. Oja"], "venue": "Proceedings of 19th International Conference on Artificial Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Nonnegative matrix factorization with the Itakura-Saito divergence. With application to music analysis,", "author": ["C. F\u00e9votte", "N. Bertin", "J.-L. Durrieu"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Exponential dispersion models,", "author": ["B. J\u00f8rgensen"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1987}, {"title": "Nonnegative Matrix and Tensor Factorization", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S. Amari"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Alpha/beta divergences and Tweedie models,", "author": ["Y.K. Yilmaz", "A.T. Cemgil"], "venue": "CoRR, vol. abs/1209.4280,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Estimation of non-normalized statistical models using score matching,", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Learning the parts of objects by nonnegative matrix factorization,", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Projective nonnegative matrix factorization for image compression and feature extraction,", "author": ["Z. Yuan", "E. Oja"], "venue": "Proceedings of 14th Scandinavian Conference on Image Analysis,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Linear and nonlinear projective nonnegative matrix factorization,", "author": ["Z. Yang", "E. Oja"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Selecting \u03b2-divergence for nonnegative matrix factorization by score matching,", "author": ["Z. Lu", "Z. Yang", "E. Oja"], "venue": "Proceedings of the 22nd International Conference on Artificial Neural Networks (ICANN", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Robust blind source separation by beta divergence,", "author": ["M. Minami", "S. Eguchi"], "venue": "Neural Computation,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "On measures of information and entropy,", "author": ["A. R\u00e9nyi"], "venue": "Procedings of 4th Berkeley Symposium on Mathematics, Statistics and Probability,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1960}, {"title": "Robust prewhitening for ica by minimizing beta-divergence and its application to fastica,", "author": ["M. Mollah", "S. Eguchi", "M. Minami"], "venue": "Neural Processing Letters,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning alpha-integration with partially-labeled data,", "author": ["H. Choi", "S. Choi", "A. Katake", "Y. Choe"], "venue": "Proc. of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Series evaluation of Tweedie exponential dispersion model densities,", "author": ["P.K. Dunn", "G.K. Smyth"], "venue": "Statistics and Computing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Some extensions of score matching,", "author": ["A. Hyv\u00e4rinen"], "venue": "Comput. Stat. Data Anal.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Algorithms for nonnegative matrix factorization with the beta-divergence,", "author": ["C. F\u00e9votte", "J. Idier"], "venue": "Neural Computation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Automatic relevance determination in nonnegative matrix factorization with the \u03b2-divergence,", "author": ["C.F.V. Tan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Probabilistic latent semantic indexing,", "author": ["T. Hofmann"], "venue": "in International Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1999}, {"title": "Quadratic nonnegative matrix factorization,", "author": ["Z. Yang", "E. Oja"], "venue": "Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts?", "author": ["D. Donoho", "V. Stodden"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2003}, {"title": "Automatic rank determination in projective nonnegative matrix factorization,", "author": ["Z. Yang", "Z. Zhu", "E. Oja"], "venue": "Proceedings of the 9th International Conference on Latent Variable Analysis and Signal Separation (LVA2010),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "On Estimation of a Probability Density Function and Mode,", "author": ["E. Parzen"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1962}, {"title": "Low-rank kernel learning with bregman matrix divergences,", "author": ["B. Kulis", "M.A. Sustik", "I.S. Dhillon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th ed", "author": ["M. Abramowitz", "I.A. Stegun", "Eds"], "venue": "New York: Dover,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1972}], "referenceMentions": [{"referenceID": 0, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "[1], [2], [3], [4]), Stochastic Neighbor Embedding [5], [6], topic models [7], [8], and Bayesian network optimization [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "In Section II, we summarize the most popularly used parametric families including \u03b1-, \u03b2-, \u03b3- and R\u00e9nyi-divergences [10], [11], [12], [13], [14] and their combinations (e.", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The four parametric families in turn belong to broader ones such as the Csisz\u00e1r-Morimoto f -divergences [16], [17] and Bregman divergences [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The four parametric families in turn belong to broader ones such as the Csisz\u00e1r-Morimoto f -divergences [16], [17] and Bregman divergences [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "The four parametric families in turn belong to broader ones such as the Csisz\u00e1r-Morimoto f -divergences [16], [17] and Bregman divergences [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 158, "endOffset": 161}, {"referenceID": 17, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 177, "endOffset": 181}, {"referenceID": 18, "context": "Data analysis techniques based on information divergences have been widely and successfully applied to various data such as text [19], electroencephalography [3], facial images [20], and audio spectrograms [21].", "startOffset": 206, "endOffset": 210}, {"referenceID": 18, "context": "For example, Euclidean distance is suitable for data with Gaussian noise; Kullback-Leibler divergence has shown success for finding topics in text documents [7]; and Itakura-Saito divergence has proven to be suitable for audio signal processing [21].", "startOffset": 245, "endOffset": 249}, {"referenceID": 19, "context": "Our starting-point is the Tweedie distribution [22], which is known to have a relationship with \u03b2-divergence [23], [24].", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "Our starting-point is the Tweedie distribution [22], which is known to have a relationship with \u03b2-divergence [23], [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Our starting-point is the Tweedie distribution [22], which is known to have a relationship with \u03b2-divergence [23], [24].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": ", Score Matching (SM) [25], specifically proposed for nonnormalized densities.", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 24, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "The MEDAL method is applied to different learning problems: Nonnegative Matrix Factorization (NMF) [26], [3], [1], Projective NMF [27], [28] and Symmetric Stochastic Neighbor Embedding for visualization [5], [6].", "startOffset": 208, "endOffset": 211}, {"referenceID": 26, "context": "We also demonstrate that our method outperforms Score Matching on Exponential Divergence distribution (ED), a previous approach for \u03b2-divergence selection [29].", "startOffset": 155, "endOffset": 159}, {"referenceID": 7, "context": "\u2022 \u03b1-divergence [10], [11] is defined as", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "\u2022 \u03b1-divergence [10], [11] is defined as", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "\u2022 \u03b2-divergence [30], [31] is defined as", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "\u2022 \u03b3-divergence [13] is defined as", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "\u2022 R\u00e9nyi divergence [32] is defined as", "startOffset": 19, "endOffset": 23}, {"referenceID": 29, "context": "A conventional automatic selection method is cross-validation [33], [34], where the training only uses part of the entries of x and the remaining ones are used for validation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 30, "context": "A conventional automatic selection method is cross-validation [33], [34], where the training only uses part of the entries of x and the remaining ones are used for validation.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "Selecting \u03b2-divergence 1) Maximum Tweedie Likelihood (MTL): We start from the probability density function (pdf) of an exponential dispersion model (EDM) [22]:", "startOffset": 154, "endOffset": 158}, {"referenceID": 19, "context": "The canonical parameter and the cumulant function that satisfy this property are [22]", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "The function can be expanded with infinite series [35] or approximated by saddle point estimation [36].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": ", [23], [24]): maximizing the likelihood of Tweedie distribution for certain p values is equivalent to minimizing the corresponding divergence with \u03b2 = 1\u2212 p.", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": ", [23], [24]): maximizing the likelihood of Tweedie distribution for certain p values is equivalent to minimizing the corresponding divergence with \u03b2 = 1\u2212 p.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "The existing software using the infinite series expansion approach [35] (see Appendix A) is prone to numerical computation problems especially for \u22120.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Obviously f(x) is continuous and bounded for x \u2208 [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 22, "context": "Finally, let us note that in addition to the maximum likelihood estimator, Score Matching (SM) [25], [37] can be applied to estimation of \u03b2 as a density parameter (see Section IV-A).", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "Finally, let us note that in addition to the maximum likelihood estimator, Score Matching (SM) [25], [37] can be applied to estimation of \u03b2 as a density parameter (see Section IV-A).", "startOffset": 101, "endOffset": 105}, {"referenceID": 26, "context": "[29] proposed a similar exponential divergence (ED) distribution pED(x;\u03bc, \u03b2) \u221d exp [\u2212D\u03b2(x||\u03bc)] , (21)", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "First we provide the results on synthetic data, whose density is known, to compare the behavior of MTL, MEDAL and the score matching method [29].", "startOffset": 140, "endOffset": 144}, {"referenceID": 26, "context": "The loglikelihood and negative score matching objectives [29] on the same four datasets are shown.", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "The ED distribution [29] has an advantage that it is defined also for \u03b2 \u2208 (0, 1).", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "The optimization of \u03b2-NMF was implemented using the standard multiplicative update rules [23], [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 33, "context": "The optimization of \u03b2-NMF was implemented using the standard multiplicative update rules [23], [38].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "Similar multiplicative update rules are also available for \u03b1-NMF [23].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "1) A Short Piano Excerpt: We consider the piano data used in [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "The found maximum likelihood estimate \u03b2 = \u22121 corresponds to Itakura-Saito divergence, which is in harmony with the empirical results presented in [21] and the common belief that IS divergence is most suitable for audio spectrograms.", "startOffset": 146, "endOffset": 150}, {"referenceID": 34, "context": "We set K = 5 in NMF and masked 50% of the data by following [39].", "startOffset": 60, "endOffset": 64}, {"referenceID": 34, "context": "These results are in harmony with the findings of Tan and F\u00e9votte [39] using the remaining 50% of the data as validation set, where they found that \u03b2 \u2208 [0, 0.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "Our finding also justifies the usage of KL-divergence in topic models with the multinomial distribution [40], [7].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "2) Projective NMF: Next we apply the MEDAL method to Projective Nonnegative Matrix Factorization (PNMF) [27], [28] based on \u03b3-divergence [13], [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 36, "context": "Thus it is a special case of Quadratic Nonnegative Matrix Factorization (QNMF) [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "The block entries are uniformly drawn from [0, 10].", "startOffset": 43, "endOffset": 50}, {"referenceID": 0, "context": "We then added uniform noise from [0, 1] to the all matrix entries.", "startOffset": 33, "endOffset": 39}, {"referenceID": 25, "context": "For each \u03b3, we ran the multiplicative algorithm of PNMF by Yang and Oja [28], [42] to obtain W and V\u0302.", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "We also tested MEDAL on the swimmer dataset [43] which is popularly used in the NMF field.", "startOffset": 44, "endOffset": 48}, {"referenceID": 38, "context": "[45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "3) Symmetric Stochastic Neighbor Embedding: Finally, we show an application beyond NMF, where MEDAL is used to find the best \u03b3-divergence for the visualization using Symmetric Stochastic Neighbor Embedding (s-SNE) [5], [6].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "3) Symmetric Stochastic Neighbor Embedding: Finally, we show an application beyond NMF, where MEDAL is used to find the best \u03b3-divergence for the visualization using Symmetric Stochastic Neighbor Embedding (s-SNE) [5], [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 39, "context": "The background illustrates the node density by the Parzen method [46].", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "[47]) that are defined over eigenvalues of matrices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Dunn and Smyth [35] described an approach to select a subset of these infinite terms to accurately approximate f(x, \u03c6, p).", "startOffset": 15, "endOffset": 19}, {"referenceID": 41, "context": "[48]) can evaluate definite integrals of the form \u222b \u221e", "startOffset": 0, "endOffset": 4}], "year": 2014, "abstractText": "Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the \u03b2-divergence family. Selecting the best \u03b2 then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate \u03b1-divergence in terms of \u03b2-divergence, which enables automatic selection of \u03b1 by maximum likelihood with reuse of the learning principle for \u03b2-divergence. Furthermore, we show the connections between \u03b3and \u03b2-divergences as well as R\u00e9nyiand \u03b1-divergences, such that our automatic selection framework is extended to nonseparable divergences. Experiments on both synthetic and realworld data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families.", "creator": "LaTeX with hyperref package"}}}