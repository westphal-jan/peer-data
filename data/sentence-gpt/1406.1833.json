{"id": "1406.1833", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2014", "title": "Unsupervised Feature Learning through Divergent Discriminative Feature Accumulation", "abstract": "Unlike unsupervised approaches such as autoencoders that learn to reconstruct their inputs, this paper introduces an alternative approach to unsupervised feature learning called divergent discriminative feature accumulation (DDFA) that instead continually accumulates features that make novel discriminations among the training set. Thus DDFA features are inherently discriminative from the start even though they are trained without knowledge of the ultimate classification problem. Interestingly, DDFA also continues to add new features indefinitely (so it does not depend on a hidden layer size), is not based on minimizing error, and is inherently divergent instead of convergent, thereby providing a unique direction of research for unsupervised feature learning. In this paper the quality of its learned features is demonstrated on the MNIST dataset, where its performance confirms that indeed DDFA is a viable technique for learning useful features. However, the general public is skeptical about its usefulness in distinguishing between these two approaches.\n\n\nThe purpose of this paper is to explore the applicability of an unsupervised feature training approach to unsupervised feature learning, where the data are spatially separated from the training set (and to find that the neural network is not the most generalised data set). To investigate whether this is a useful strategy to improve learning for the task set, we investigated whether the neural network has not yet been able to infer that a neural network is capable of conveying useful features from the dataset without using discriminant features. To demonstrate, the unsupervised feature training approach has a minimum of 80 training runs, and is able to reliably train with a gradient descent of a gradient gradient of 5.5 times more than the typical network. The inference in this paper is based on a generalized unsupervised feature training approach to unsupervised feature learning in the same order as the typical network, but for the first time, it does not use discriminative features, which requires an unbiased approach. In this paper the inference, based on a generalized unsupervised feature training approach, is only one possible predictor. Thus, the posterior probability of a gradient descent in the dataset is one of the most important predictors.\nIn summary, the unsupervised feature training approach was applied to the unsupervised feature training approach by applying the sparse classification model to training set. This approach provides information about neural networks and discriminative features that are not a true predictor of learning, which is why it was introduced by the neural network. To investigate whether there is no effect of the sparse classification model, we used a", "histories": [["v1", "Fri, 6 Jun 2014 23:45:03 GMT  (75kb,D)", "https://arxiv.org/abs/1406.1833v1", "9 pages, 2 figures"], ["v2", "Tue, 10 Jun 2014 03:37:45 GMT  (75kb,D)", "http://arxiv.org/abs/1406.1833v2", "Corrected citation formatting"]], "COMMENTS": "9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["paul a szerlip", "gregory morse", "justin k pugh", "kenneth o stanley"], "accepted": true, "id": "1406.1833"}, "pdf": {"name": "1406.1833.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Learning through Divergent Discriminative Feature Accumulation", "authors": ["Paul A. Szerlip", "Gregory Morse", "Justin K. Pugh"], "emails": ["pszerlip@eecs.ucf.edu,", "jpugh@eecs.ucf.edu,", "kstanley@eecs.ucf.edu,", "gregorymorse07@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cires\u0327an et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al., 2013). While the beginning of this renaissance focused on the sequential unsupervised training of individual layers one upon another (Bengio et al., 2007; Hinton et al., 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al., 2013; Schmidhuber et al., 2011). This explosion has in turn raised the question of what makes a good representation, and how it is best learned (Bengio et al., 2013). The main contribution of this paper is to advance our understanding of good representation learning by suggesting a new principle for obtaining useful representations that is accompanied by a practical algorithm embodying the principle.\nThe feature representation obtained through learning algorithms is often impacted by the nature of the training. For example, supervised approaches such as stochastic gradient descent (Cires\u0327an et al., 2010) that aim to minimize the error in a classification problem in effect encourage the exclusive discovery of features that help to discriminate among the target classifications. In contrast, unsupervised approaches, which include both generative representations such as restricted Boltzmann machines (RBMs) (Hinton et al., 2006) and autoencoders that are trained to reproduce their inputs (Bengio et al., 2007), yield a more general feature set that captures dimensions of variation that may or may not be essential to the classification objective. The hope of course is that such a set would nevertheless be useful for classification in any case, and the pros and cons of e.g. generative versus discriminative features have proven both subtle and complex (Jaakkola et al., 1999; Ng and Jordan, 2002). Nevertheless, one benefit of unsupervised training is that it does not require labeled data to gain traction.\nar X\niv :1\n40 6.\n18 33\nv2 [\ncs .N\nE ]\n1 0\nJu n\nAn important insight in this paper is that there is an unrecognized option outside this usual unsupervised versus supervised (or generative versus discriminative) dichotomy. In particular, there is an alternative kind of discriminative learning that is unsupervised rather than supervised. In this proposed alternative approach, called divergent discriminative feature accumulation (DDFA), instead of searching for features constrained by the objective of solving the discriminative classification problem, a learning algorithm can instead attempt to collect as many features that discriminate strongly among training examples as possible, without regard to any particular classification problem.\nThe approach in such unsupervised discriminative learning is thus to search continually for novel features that discriminate among training examples in new ways. Interestingly, unlike conventional algorithms in deep learning, such a search is explicitly divergent by design and therefore continues to accumulate new features without converging. In effect, a high-scoring feature is therefore relevant to discriminating among the examples, even though the ultimate discrimination problem is not known. A comprehensive set of such features that discriminate among the training set in fundamental ways is thereby suitable in principle for later supervised training from those collected features for any particular discrimination task. This idea is intuitive in the sense that even for humans, distinctions among experiences can be learned before we know how we will apply such distinctions. Furthermore, as the results will show, if the search gradually shifts from simple to more complex distinctions, only a small subset of all possible distinctions (many of which are obscure) needs to be discovered.\nIn fact, this perspective on feature learning has the advantage over more conventional approaches in deep learning that learning does not depend on a fixed a priori number of features. Rather, it simply continues to accumulate new features as long as the algorithm is run. Furthermore, unlike in other unsupervised approaches, the accumulated features are known explicitly to be discriminative, suiting them well to later discriminative learning. Another potentially advantageous property of such a feature accumulator is its lack of convergence (thereby avoiding the problem of local optima), which stems from the fact that it is inherently divergent because it is not based on minimizing an error. In these ways DDFA is uniquely flexible and autonomous.\nThe driving force behind the feature accumulator is the imperative of finding novel features. Thus a well-suited algorithm for implementing this idea in practice is the recent novelty search algorithm (Lehman and Stanley, 2011), which is a divergent evolutionary algorithm that is rewarded for moving away in the search space of candidate behaviors (such as discriminations) from where it has already visited to where it has not. By accumulating features that are themselves ANNs, novelty search in this paper enables DDFA. As with other unsupervised pretraining approaches such as autoencoders, once DDFA determines that sufficient features are collected, a classifier is trained above them for the classification task (through backpropagation in this paper). To demonstrate the potential of DDFA to collect useful features, it is tested in this paper by collecting single-layer features for the MNIST digital handwriting recognition benchmark (LeCun and Cortes, 1998). Even with the consequent two-layer shallow classifier network, its testing performance rivals more conventional training techniques.\nThis initial proof of concept establishes the efficacy of accumulating features as a basis for representation learning. While the simple one-layer accumulated discriminative features from DDFA perform well, DDFA can conceivably improve further through layering (e.g. accumulating multilayer features or searching for novel features that are built above already-discovered features) and convolution (LeCun and Bengio, 1995), just like other deep learning algorithms. Most importantly, based on the novel representational principle that discriminators make good features for classification problems, DDFA opens up a new class of learning approaches."}, {"heading": "2 Background", "text": "This section reviews the two algorithms, novelty search and HyperNEAT, that underpin the DDFA approach."}, {"heading": "2.1 Novelty Search", "text": "The problem of collecting novel instances of a class is different from the more familiar problem of minimizing error. While error minimization aims at converging towards minima in the search space,\ncollecting novelty requires diverging away from past discoveries and fanning out across the search space in all directions that appear to lead towards further novelty. This fanning-out process is thus well-suited to a population-based approach that accumulates and remembers novel discoveries to help push the search continually to even more novelty as it progresses. The novelty search algorithm (Lehman and Stanley, 2011) implements such a process in practice through an evolutionary approach, which naturally provides the population-driven context appropriate for finding novelty. However, it is important to note that novelty search is unlike even traditional evolutionary algorithms (EAs), which themselves are usually driven to converge to higher fitness. In fact, while EAs are often viewed as an alternative approach to optimization, their natural capacity to diversify and collect may instead better capture their practical potential to contribute to problems in learning.\nThe idea in novelty search is to reward candidates (by increasing their probability of reproduction) who are behaviorally novel. If the candidates are ANNs as in the present study, then the word \u201cbehaviorally\u201d becomes critical because it refers to what the discovered ANNs actually do (e.g. how they discriminate) as opposed to just their underlying genetic representations (i.e. genomes), which may or may not do anything interesting. Thus discovering novel behaviors requires search (as opposed to just enumerating random sets of genes), thereby instantiating a nontrivial alternative to the traditional objective gradient.\nThis point is particularly important in the context of deep learning, where researchers have commented on the potential long-term limitations of optimization gradients and the need for a broader and less convergent approaches for learning representations. For example, when discussing the future of representation learning, Bengio (2013) notes:\nThe basic idea is that humans (and current learning algorithms) are limited to \u201clocal descent\u201d optimization methods, that make small changes in the parameter values with the effect of reducing the expected loss in average. This is clearly prone to the presence of local minima, while a more global search (in the spirit of both genetic and cultural evolution) could potentially reduce this difficulty.\nNovelty search (Lehman and Stanley, 2011) can be viewed as an embodiment of just such a \u201cgenetic evolution\u201d that is suited to accumulating discoveries free from the pitfalls of \u201clocal descent.\u201d In fact, while novelty search was originally shown sometimes to find the objective of an optimization problem more effectively than objective-based optimization (Lehman and Stanley, 2011), Cully and Mouret (2013) recently raised the intriguing notion of novelty search as a repertoire collector. That is, instead of searching for a solution to a problem, novelty search can collect a set of novel skills (each one a point in the search space) intended for later aggregation by a higher-level mechanism. This repertoire-collecting idea aligns elegantly with the problem of accumulating features for deep learning, wherein each feature detector can be viewed as a \u201cskill\u201d within the repertoire of a classifier.\nIn practice, novelty search maintains an archive of previously novel discoveries as part of the algorithm. Future candidates are then compared to the archive to determine whether they too are novel. A random sampling of candidates is entered into the archive, which implies that more frequentlyvisited areas will be more densely covered. Intuitively, if the average distance to the nearest neighbors of a given behavior b is large then it is in a sparse area; it is in a dense region if the average distance is small. The sparseness \u03c1 at point b is given by\n\u03c1(x) = 1\nk k\u2211 i=0 dist(b, \u00b5i), (1)\nwhere \u00b5i is the ith-nearest neighbor of b with respect to the distance metric dist, which is a domaindependent measure of behavioral difference between two individuals in the search space. The nearest neighbors calculation must take into consideration individuals from the current population and from the permanent archive of novel individuals. Candidates from more sparse regions of this behavioral search space then receive higher novelty scores, which lead to a higher chance of reproduction. It is important to note that this novelty space cannot be explored purposefully, that is, it is not known a priori how to enter areas of low density just as it is not known a priori how to construct a solution close to the objective. Thus, moving through the space of novel behaviors requires exploration. The gradient of novelty is interesting in particular because novel discoveries lead to other novel discoveries, which means that a search algorithm following gradients of novelty is likely to make many interesting discoveries.\nNovelty search in effect runs as a regular EA wherein novelty replaces fitness as the criterion for selection, and an expanding archive of past novel discoveries is maintained. This simple idea will empower DDFA in this paper to accumulate a collection of novel features."}, {"heading": "2.2 HyperNEAT", "text": "The term for algorithms that search for ANNs through an evolutionary process is neuroevolution (Floreano et al., 2008; Stanley and Miikkulainen, 2002). It is important to note that modern neuroevolution algorithms are not like conventional EAs based on bit strings, but instead implement a variety of sophisticated heuristics and encodings that enable the discovery of large and wellorganized networks. This section is designed to introduce the particular neuroevolution algorithm (called HyperNEAT) that is combined with novelty search to search for features in this paper. Because neuroevolution is an independent field that may be unfamiliar to many in deep learning, this section is written to emphasize the main ideas that make it appealing for the purpose of feature learning, without including details that are unnecessary to understand the operation of the proposed DDFA algorithm. The complete details of HyperNEAT can be found in its primary sources (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010).\nIn a domain like visual recognition, the pattern of weights in useful features can be expected to exhibit a degree of contiguity and perhaps regularity. For example, it is unlikely that an entirely random pattern of largely unconnected pixels corresponds to a useful or interesting feature. It has accordingly long been recognized in neuroevolution that entirely random perturbations of weight patterns, which are likely to emerge for example from random mutations, are unlikely to maintain contiguity or regularity. While stochastic gradient descent (SGD) algorithms at least justify their trajectory through the search space through descent, a completely random perturbation of weights is arguably less principled and therefore perhaps less effective. Nevertheless, SGD still suffers to some extent from the same problem that even a step that reduces error may not maintain contiguity or regularity in the feature geometry. Neuroevolution algorithms have responded to this concern with a class of representations called indirect encodings (Stanley and Miikkulainen, 2003), wherein the weight pattern is generated by an evolving genetic encoding that is biased towards contiguity and regularity by design. That way, when a mutation is applied to a feature, the feature deforms in a systematic though still randomized fashion (figure 1).\nHyperNEAT, which stands for Hypercube-based NeuroEvolution of Augmenting Topologies (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010) is a contemporary neuroevolution algorithm based on such an indirect encoding. In short, HyperNEAT evolves an encoding network called a compositional pattern producing network (CPPN; Stanley 2007) that describes the pattern of connectivity within the ANN it encodes. Therefore, mutations in HyperNEAT happen to the CPPN, which then transfers their effects to the encoded ANN. In this way the CPPN is like DNA, which transfers the effects of its own mutations to the structures it encodes, such as the brain. Because the CPPN encoding is designed to describe patterns of weights across the geometry of the encoded network, the weights in HyperNEAT ANNs tend to deform in contiguity-preserving and regularity-preserving ways (as seen in figure 1), thereby providing a useful bias (Gauci and Stanley, 2010; Stanley et al., 2009). Furthermore, CPPNs in HyperNEAT grow over evolution (i.e. their structure is augmented over learning), which means that the pattern of weights in the ANN they describe (which is fixed in size) can become more intricate and complex over time. For unfamiliar readers, it is worth noting that this HyperNEAT style of representation for ANNs is well-established\nand has appeared in mainstream venues such as AAAI (Gauci and Stanley, 2008), Neural Computation (Gauci and Stanley, 2010), and JMLR (Verbancsics and Stanley, 2010).\nAn important observation is that HyperNEAT\u2019s tendency to preserve geometric properties in its weights means that it is not invariant to permutations in the input vector. In effect (in e.g. the case of MNIST) it is exploiting the known two-dimensional geometry of the problem. However, at the same time, while it does exploit geometry, its use in this paper is not convolutional either: its input field is never broken into receptive fields and is rather projected in whole directly (without intervening layers) to a single-feature output node. Thus the powerful advantage of convolution for visual problems is not available in this investigation, making the problem more challenging. As a consequence, the DDFA implementation in this paper does not fit neatly into the permutation-invariant-or-not dichotomy, and may be considered somewhere closer to typical permutation-invariant scenarios.\nThis overview of HyperNEAT is left brief because its other details (which are widely disseminated in the venues above) are not essential to the main idea in this paper, which is to accumulate feature detectors through novelty search."}, {"heading": "3 Approach: Divergent Discriminative Feature Accumulation (DDFA)", "text": "Unsupervised pretraining in deep learning has historically focused on approaches such as autoencoders and RBMs (Bengio et al., 2007; Hinton et al., 2006) that attempt to reconstruct training examples by first translating them into a basis of features different from the inputs, and then from those features regenerating the inputs. This idea is appealing because the imperative of reconstruction demands that the learned features must ultimately reflect some aspect of the underlying structure of the training set. Of course, one potential problem with this approach is that there is no assurance that the learned features actually align with any particular classification or discrimination problem for which they might be used in the future. Yet this conventional approach to learning features also raises some interesting deeper questions. For example, is there any other way to extract meaningful features and thereby learn representations from a set of examples without explicit supervision?\nThere are some well-known simple alternatives, though they are not usually characterized as featurelearning algorithms. For example clustering algorithms such as K-means or Gaussian mixture models in effect extract structure from data that can then assist in classification; in fact at least one study has shown that such clustering algorithms can yield features as effective or more so for classification than autoencoders or RBMs (Coates et al., 2011). This result highlights that reconstruction is not the only effective incentive for extracting useful structure from the world.\nThe approach introduced here goes beyond simple clustering by emphasizing the general ability to learn diverse distinctions. That is, while one can learn how to describe the world, one can also learn how different aspects of the world relate to each other. Importantly, there is no single \u201ccorrect\u201d view of such relations. Rather, a rich set of learned relationships can support drawing important distinctions later. For example, in one view palm trees and \u201cregular\u201d trees share properties that distinguish them from other plants. However, in another view, palm trees are in fact distinct from regular trees. Both such views can be useful in understanding nature, and one can hold both simultaneously with no contradiction. When an appropriate question comes up, such as which plants are tall and decorative, the feature tall becomes available because it was learned to help make such general distinctions about the world in the past.\nThe idea in DDFA is to continually accumulate such distinctions systematically through novelty search, thereby building an increasingly rich repertoire of features that help divide and relate observations of the world. Specifically, suppose there are n training examples {x(1), ..., x(n)}; whether or not they are labeled will not matter because feature learning will be unsupervised. Suppose also that any single feature detector hi (i.e. a single hidden node that detects a particular feature) outputs a real number whose intensity represents the degree to which that feature is present in the input. It follows that hi will assign a real number h (t) i to every example x\n(t) depending on the degree to which x(t) contains the feature of interest for hi. The output of hi for all features x(t) where t = 1, . . . , n thereby forms a vector {h(1)i , . . . , h (n) i } that can be interpreted as the signature of feature detector hi across the entire training set. In effect the aim is to continually discover new such signatures.\nThis problem of continually discovering novel signatures is naturally captured by novelty search, which can be set up explicitly to evolve feature detectors hi, each of which takes a training example\nas input and returns a single output. The signature {h(1)i , ..., h (n) i } of hi over all training examples is then its behavior characterization for novelty search. The novelty of the signature is then measured by comparing it to the k-nearest signatures in the novelty archive, following equation 1. Novelty search then dictates that more novel features are more likely to reproduce, which means that gradients of novel signatures will be followed in parallel by the evolving population. Those features whose sparseness \u03c1 (i.e. novelty) exceeds a minimum threshold \u03c1min are stored in the growing novel feature collection for later classifier training.\nA likely source of confusion is the question of whether DDFA is a kind of exhaustive search over signatures, which would not tend to discover useful features in a reasonable runtime. After all, the number of theoretically possible distinctions is exponential in the number of training examples. However, a critical facet of novelty-based searches that are combined with HyperNEAT-based neuroevolution is that the complexity of features (and hence distinctions) tends to increase over the run (Lehman and Stanley, 2011). As a result, the initial features discovered encompass simple principles (e.g. is the left side of the image dark?) that gradually increase in complexity. For this reason, the most arbitrary and incoherent features (e.g. are there 17 particular dots at specific non-contiguous coordinates in the image?) are possible to discover only late in the search. Furthermore, because the novelty signature is measured over the training set, features that make broad separations relevant to the training set itself are more likely to be discovered early. In effect, over the course of DDFA, the feature discoveries increasingly shift from simple principles to intricate minutia. Somewhere along this road are likely diminishing returns, well before all possible signatures are discovered. Empirical results reported here support this point.\nInterestingly, because DDFA does not depend on the minimization of error, in principle it can continue to collect features virtually indefinitely, but in practice at some point its features are fed into a classifier that is trained from the collected discriminative features."}, {"heading": "4 Experiment", "text": "The key question addressed in this paper is whether a divergent discriminative feature accumulator can learn useful features, which means they should aid in effective generalization on the test set. If that is possible, the implication is that DDFA is a viable alternative to other kinds of unsupervised pretraining. To investigate this question DDFA is trained and tested on the MNIST handwritten digit recognition dataset (LeCun and Cortes, 1998), which consists of 60,000 training images and 10,000 test images. Therefore, the signature of each candidate feature discovered by DDFA during training is a vector of 60,000 real values.\nBecause the structure of the networks that are produced by HyperNEAT can include as many hidden layers as the user chooses, the question arises how many hidden layers should be allowed in individual features hi learned by HyperNEAT. This consideration is substantive because in principle DDFA can learn arbitrarily-deep individual features all at once, which is unlike e.g. the layer-bylayer training of a deep stack of autoencoders. However, the explicit choice was made in this introductory experiment to limit DDFA to single-layer features (i.e. without hidden nodes) to disentangle the key question of whether the DDFA process represents a useful principle from other questions of representation such as the implications of greater depth. Therefore, feature quality is addressed straightforwardly in this study by observing the quality of classifier produced based only on singlelayer DDFA features. As a result, the final classifier ANN has just two layers: the layer of collected features and the ten-unit output layer for classifying MNIST digits.\nThe single-layer DDFA approach with novelty search and HyperNEAT is difficult to align directly with common deep learning approaches in part because of its lack of permutation invariance even though it is not convolutional in any sense (thereby lacking the representational power of such networks), and its lack of depth in this initial test. Thus to get a fair sense of whether DDFA learns useful features it is most illuminating to contrast it with the leading result on an equivalently shallow two-layer architecture (which are rare in recent years) that similarly avoided special preprocessing like elastic distortions or deskewing. In particular, Simard et al. (2003) obtained one of the best such results of 1.6% test error performance. Thus a significant improvement on that result would suggest that DDFA generates useful features that help to stretch the capacity of such a shallow network to generalize. DDFA\u2019s further ability to approach the performance of conventional vanilla deep networks, such as the original 1.2% result from Hinton et al. (2006) on a four-layer network pretrained by a RBM, would hint at DDFA\u2019s potential utility in the future for pretraining deeper networks.\nDuring the course of evolution, features are selected for reproduction based on their signature\u2019s novelty score (sparseness \u03c1) calculated as the sum of the distances to the k-nearest neighbors (k = 20), where neighbors include other members of the population as well as the historical novelty archive. At the end of each generation, each individual in the population (size = 100) has a 1% chance of being added to the novelty archive, resulting in an average of 1 individual added to the novelty archive on each generation. Separately, a list of individuals called the feature list is maintained. At the end of each generation, each member of the population is scored against the current feature list by finding the distance to the nearest neighbor (k = 1), where neighbors are members of the feature list. Those individuals that score above a threshold \u03c1min = 2,000 are added to the feature list. In effect, the feature list is constructed in such a way that all collected features have signatures that differ by at least \u03c1t from all others in the collection. This threshold-based collection process protects against collecting redundant features. A simple variant of HyperNEAT called HyperNEAT-LEO (Verbancsics and Stanley, 2011) (which leads to less connectivity) was the main neuroevolution engine. The HyperNEAT setup and parameters can be easily reproduced in full because they are simply the default parameters of the SharpNEAT 2.0 publicly-available package (Green, 2003\u20132014).\nTo observe the effect of collecting different numbers of features, DDFA was run separately until both 1,500 and 3,000 features were collected. After collection concludes, a set of ten classification nodes is added on top of the collected features, and simple backpropagation training commences. The training and validation procedure mirrors that followed by Hinton et al. (2006): first training is run on 50,000 examples for 50 epochs to find the network that performs best on a 10,000-example validation set. Then training shifts to the full 60,000-example set, which is trained until it reaches the same training error as in the best validation epoch. The resulting network is finally tested on the full 10,000-example test set. This whole procedure is similar to how autoencoders are trained before gradient descent in deep learning (Bengio et al., 2007)."}, {"heading": "5 Results", "text": "The main results are shown in Table 1. DDFA was able to achieve test errors of 1.42% and 1.25% from collections of 1,500 and 3,000 features, respectively, which are both well below the 1.6% error of the similar shallow network trained without preprocessing from Simard et al. (2003). In fact, the result for the 3,000-feature network even approaches the 1.2% error of the significantly deeper network of Hinton et al. (2006), showing that shallow networks can generalize surprisingly well by finding sufficiently high-quality feature sets, even despite a lack of exposure to distortions during training. It also appears that more collected features lead to better generalization, at least at these sizes. It took 338 and 676 generations of feature collection to obtain the 1,500 and 3,000 features, respectively. Collecting 3,000 features took about 36 hours of computation on 12 3.0 GHz cores.\nFigure 2 shows a typical set of features collected by DDFA. Interestingly, unlike the bottom layer of deep learning networks that typically exhibit various line-orientation detectors, DDFA also collects more complex features because newer features of increasing complexity evolve from older features.\nTo rule out the possibility that the reason for the testing performance is simply the HyperNEATbased encoding of features, a random CPPN control was also run. It follows an identical procedure for training and testing, except that novelty scores and adding to the feature list during the feature accumulation phase are decided randomly, which means the final collection in effect contained random features with a range of CPPN complexity similar to the normal run. To further investigate the value of the HyperNEAT representation, an additional random weights control was tested whose weights were assigned from a uniform random distribution, bypassing HyperNEAT entirely. As the results in Table 1 show, the CPPN encoding in HyperNEAT provides a surprisingly good basis for training even when the features are entirely randomly-generated. However, they are still inferior to the features collected by normal DDFA. As shown in the last column, without HyperNEAT, testing performance with a collection of random features is unsurprisingly poor. In sum these controls show that the pretraining in DDFA is essential to priming the later classifier for the best possible performance."}, {"heading": "6 Discussion and Future Work", "text": "The results suggest that DDFA can indeed collect useful features and thereby serve as an alternative unsupervised feature learner. While it may ultimately lead to better training performance in some cutting-edge problems, future work with more layers and on larger problems is clearly necessary to investigate its full potential for exceeding top results.\nHowever, it is important to recognize that significantly more than performance is at stake in the dissemination of alternative unsupervised training techniques based on new principles. Deep learning faces several fundamental challenges that are not only about testing performance. For example, recent surprising results from Szegedy et al. (2013) show that very small yet anomalous perturbations of training images that are imperceptible to the human eye can fool several different kinds of deep networks that nevertheless ominously score well on the test set. The implications of these anomalies are not yet understood. At the same time, as Bengio (2013) points out, local descent on its own will not ultimately be enough to tackle the most challenging problems, suggesting the need for radical new kinds of optimization that are more global. These kinds of considerations suggest that simply scoring well on a test set in the short run may not necessarily foreshadow continuing success for the field in the long run.\nTherefore, the expanded possibilities that a validated new principle can inspire are essential to the health of an evolving field, whether or not it ultimately breaks a particular benchmark record. For example, DDFA shows that unsupervised discriminative learning is possible and can be effective, bringing with it several intriguing corrollaries. Among those, it is possible to conceive training methods that act as continual feature accumulators that do not require a fixed \u201chidden layer size.\u201d Furthermore, it is possible to learn useful features without any kind of error minimization (which is even used in conventional unsupervised techniques). Relatedly, an interesting question is whether anomalous results are sometimes a side effect of the very idea that all useful knowledge ultimately must come from minimizing error. The divergent dynamics of novelty search also mean that the search is inherently more global than local descent for the very reason that it is continually diverging, thereby offering a hint of how more expansive feature sets can be collected. Thus, in addition to the many possibilities for training multilayer deep features in DDFA, another important path for future work is to investigate the long-term implications of these more subtle differences from conventional techniques, and to determine whether similar such unique properties can be introduced to deep learning through non-evolutionary techniques that follow gradients other than error."}], "references": [{"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Statistical Language and Speech Processing, 1\u201337. Springer.", "citeRegEx": "Bengio,? 2013", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 1798\u20131928.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems 19 (NIPS). Cambridge, MA: MIT Press.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["D. Cire\u015fan", "U. Meier", "L. Gambardella", "J. Schmidhuber"], "venue": "Neural computation, 22(12):3207\u20133220.", "citeRegEx": "Cire\u015fan et al\\.,? 2010", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "International Conference on Artificial Intelligence and Statistics, 215\u2013223.", "citeRegEx": "Coates et al\\.,? 2011", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Behavioral repertoire learning in robotics", "author": ["A. Cully", "Mouret", "J.-B."], "venue": "Proceeding of the fifteenth annual conference on Genetic and evolutionary computation conference, GECCO \u201913, 175\u2013182. New York, NY, USA: ACM.", "citeRegEx": "Cully et al\\.,? 2013", "shortCiteRegEx": "Cully et al\\.", "year": 2013}, {"title": "Neuroevolution: from architectures to learning", "author": ["D. Floreano", "P. D\u00fcrr", "C. Mattiussi"], "venue": "Evolutionary Intelligence, 1:47\u201362.", "citeRegEx": "Floreano et al\\.,? 2008", "shortCiteRegEx": "Floreano et al\\.", "year": 2008}, {"title": "A case study on the critical role of geometric regularity in machine learning", "author": ["J. Gauci", "K.O. Stanley"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence (AAAI-2008). Menlo Park, CA: AAAI Press.", "citeRegEx": "Gauci and Stanley,? 2008", "shortCiteRegEx": "Gauci and Stanley", "year": 2008}, {"title": "Autonomous evolution of topographic regularities in artificial neural networks", "author": ["J. Gauci", "K.O. Stanley"], "venue": "Neural Computation, 22(7):1860\u20131898.", "citeRegEx": "Gauci and Stanley,? 2010", "shortCiteRegEx": "Gauci and Stanley", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W."], "venue": "Neural Computation, 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Exploiting generative models in discriminative classifiers. Advances in neural information processing", "author": ["T. Jaakkola", "D Haussler"], "venue": null, "citeRegEx": "Jaakkola and Haussler,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola and Haussler", "year": 1999}, {"title": "Convolutional networks for images, speech, and time-series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks. MIT Press.", "citeRegEx": "LeCun and Bengio,? 1995", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": null, "citeRegEx": "LeCun and Cortes,? \\Q1998\\E", "shortCiteRegEx": "LeCun and Cortes", "year": 1998}, {"title": "Abandoning objectives: Evolution through the search for novelty alone", "author": ["J. Lehman", "K.O. Stanley"], "venue": "Evolutionary Computation, 19(2):189\u2013223.", "citeRegEx": "Lehman and Stanley,? 2011", "shortCiteRegEx": "Lehman and Stanley", "year": 2011}, {"title": "Sparse feature learning for deep belief networks", "author": ["R. Marc\u2019Aurelio", "L. Boureau", "Y. LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Marc.Aurelio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Marc.Aurelio et al\\.", "year": 2007}, {"title": "On discriminative vs", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems, 2:841\u2013 848.", "citeRegEx": "Ng and Jordan,? 2002", "shortCiteRegEx": "Ng and Jordan", "year": 2002}, {"title": "On fast deep nets for agi vision", "author": ["J. Schmidhuber", "D. Cire\u015fan", "U. Meier", "J. Masci", "A. Graves"], "venue": "The Fourth Conference on Artificial General Intelligence (AGI), 243\u2013246. New York, NY: Springer.", "citeRegEx": "Schmidhuber et al\\.,? 2011", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 2011}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "International Conference on Document Analysis and Recogntion (ICDAR), vol. 3, 958\u2013962.", "citeRegEx": "Simard et al\\.,? 2003", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Compositional pattern producing networks: A novel abstraction of development", "author": ["K.O. Stanley"], "venue": "Genetic Programming and Evolvable Machines Special Issue on Developmental Systems, 8(2):131\u2013162.", "citeRegEx": "Stanley,? 2007", "shortCiteRegEx": "Stanley", "year": 2007}, {"title": "A hypercube-based indirect encoding for evolving large-scale neural networks", "author": ["K.O. Stanley", "D.B. D\u2019Ambrosio", "J. Gauci"], "venue": null, "citeRegEx": "Stanley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stanley et al\\.", "year": 2009}, {"title": "Evolving neural networks through augmenting topologies", "author": ["K.O. Stanley", "R. Miikkulainen"], "venue": "Evolutionary Computation, 10:99\u2013127.", "citeRegEx": "Stanley and Miikkulainen,? 2002", "shortCiteRegEx": "Stanley and Miikkulainen", "year": 2002}, {"title": "A taxonomy for artificial embryogeny", "author": ["K.O. Stanley", "R. Miikkulainen"], "venue": "Artificial Life, 9(2):93\u2013130.", "citeRegEx": "Stanley and Miikkulainen,? 2003", "shortCiteRegEx": "Stanley and Miikkulainen", "year": 2003}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "venue": "CoRR, abs/1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2013", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Evolving static representations for task transfer", "author": ["P. Verbancsics", "K.O. Stanley"], "venue": "Journal of Machine Learning Research (JMLR), 11:1737\u20131769.", "citeRegEx": "Verbancsics and Stanley,? 2010", "shortCiteRegEx": "Verbancsics and Stanley", "year": 2010}, {"title": "Constraining connectivity to encourage modularity in HyperNEAT", "author": ["P. Verbancsics", "K.O. Stanley"], "venue": "GECCO \u201911: Proceedings of the 13th annual conference on Genetic and evolutionary computation, 1483\u20131490. Dublin, Ireland: ACM.", "citeRegEx": "Verbancsics and Stanley,? 2011", "shortCiteRegEx": "Verbancsics and Stanley", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 3, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 9, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 14, "context": "The increasing realization in recent years that artificial neural networks (ANNs) can learn many layers of features (Bengio et al., 2007; Cire\u015fan et al., 2010; Hinton et al., 2006; Marc\u2019Aurelio et al., 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al.", "startOffset": 116, "endOffset": 207}, {"referenceID": 1, "context": ", 2007) has reinvigorated the study of representation learning in ANNs (Bengio et al., 2013).", "startOffset": 71, "endOffset": 92}, {"referenceID": 2, "context": "While the beginning of this renaissance focused on the sequential unsupervised training of individual layers one upon another (Bengio et al., 2007; Hinton et al., 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al.", "startOffset": 126, "endOffset": 168}, {"referenceID": 9, "context": "While the beginning of this renaissance focused on the sequential unsupervised training of individual layers one upon another (Bengio et al., 2007; Hinton et al., 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al.", "startOffset": 126, "endOffset": 168}, {"referenceID": 1, "context": ", 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al., 2013; Schmidhuber et al., 2011).", "startOffset": 129, "endOffset": 176}, {"referenceID": 16, "context": ", 2006), the number of approaches and variations that have proven effective at training in such deep learning has since exploded (Bengio et al., 2013; Schmidhuber et al., 2011).", "startOffset": 129, "endOffset": 176}, {"referenceID": 1, "context": "This explosion has in turn raised the question of what makes a good representation, and how it is best learned (Bengio et al., 2013).", "startOffset": 111, "endOffset": 132}, {"referenceID": 3, "context": "For example, supervised approaches such as stochastic gradient descent (Cire\u015fan et al., 2010) that aim to minimize the error in a classification problem in effect encourage the exclusive discovery of features that help to discriminate among the target classifications.", "startOffset": 71, "endOffset": 93}, {"referenceID": 9, "context": "In contrast, unsupervised approaches, which include both generative representations such as restricted Boltzmann machines (RBMs) (Hinton et al., 2006) and autoencoders that are trained to reproduce their inputs (Bengio et al.", "startOffset": 129, "endOffset": 150}, {"referenceID": 2, "context": ", 2006) and autoencoders that are trained to reproduce their inputs (Bengio et al., 2007), yield a more general feature set that captures dimensions of variation that may or may not be essential to the classification objective.", "startOffset": 68, "endOffset": 89}, {"referenceID": 15, "context": "generative versus discriminative features have proven both subtle and complex (Jaakkola et al., 1999; Ng and Jordan, 2002).", "startOffset": 78, "endOffset": 122}, {"referenceID": 13, "context": "Thus a well-suited algorithm for implementing this idea in practice is the recent novelty search algorithm (Lehman and Stanley, 2011), which is a divergent evolutionary algorithm that is rewarded for moving away in the search space of candidate behaviors (such as discriminations) from where it has already visited to where it has not.", "startOffset": 107, "endOffset": 133}, {"referenceID": 12, "context": "To demonstrate the potential of DDFA to collect useful features, it is tested in this paper by collecting single-layer features for the MNIST digital handwriting recognition benchmark (LeCun and Cortes, 1998).", "startOffset": 184, "endOffset": 208}, {"referenceID": 11, "context": "accumulating multilayer features or searching for novel features that are built above already-discovered features) and convolution (LeCun and Bengio, 1995), just like other deep learning algorithms.", "startOffset": 131, "endOffset": 155}, {"referenceID": 13, "context": "The novelty search algorithm (Lehman and Stanley, 2011) implements such a process in practice through an evolutionary approach, which naturally provides the population-driven context appropriate for finding novelty.", "startOffset": 29, "endOffset": 55}, {"referenceID": 0, "context": "For example, when discussing the future of representation learning, Bengio (2013) notes:", "startOffset": 68, "endOffset": 82}, {"referenceID": 13, "context": "Novelty search (Lehman and Stanley, 2011) can be viewed as an embodiment of just such a \u201cgenetic evolution\u201d that is suited to accumulating discoveries free from the pitfalls of \u201clocal descent.", "startOffset": 15, "endOffset": 41}, {"referenceID": 13, "context": "\u201d In fact, while novelty search was originally shown sometimes to find the objective of an optimization problem more effectively than objective-based optimization (Lehman and Stanley, 2011), Cully and Mouret (2013) recently raised the intriguing notion of novelty search as a repertoire collector.", "startOffset": 163, "endOffset": 189}, {"referenceID": 13, "context": "Novelty search (Lehman and Stanley, 2011) can be viewed as an embodiment of just such a \u201cgenetic evolution\u201d that is suited to accumulating discoveries free from the pitfalls of \u201clocal descent.\u201d In fact, while novelty search was originally shown sometimes to find the objective of an optimization problem more effectively than objective-based optimization (Lehman and Stanley, 2011), Cully and Mouret (2013) recently raised the intriguing notion of novelty search as a repertoire collector.", "startOffset": 16, "endOffset": 407}, {"referenceID": 6, "context": "The term for algorithms that search for ANNs through an evolutionary process is neuroevolution (Floreano et al., 2008; Stanley and Miikkulainen, 2002).", "startOffset": 95, "endOffset": 150}, {"referenceID": 20, "context": "The term for algorithms that search for ANNs through an evolutionary process is neuroevolution (Floreano et al., 2008; Stanley and Miikkulainen, 2002).", "startOffset": 95, "endOffset": 150}, {"referenceID": 19, "context": "The complete details of HyperNEAT can be found in its primary sources (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010).", "startOffset": 70, "endOffset": 154}, {"referenceID": 23, "context": "The complete details of HyperNEAT can be found in its primary sources (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010).", "startOffset": 70, "endOffset": 154}, {"referenceID": 21, "context": "Neuroevolution algorithms have responded to this concern with a class of representations called indirect encodings (Stanley and Miikkulainen, 2003), wherein the weight pattern is generated by an evolving genetic encoding that is biased towards contiguity and regularity by design.", "startOffset": 115, "endOffset": 147}, {"referenceID": 19, "context": "HyperNEAT, which stands for Hypercube-based NeuroEvolution of Augmenting Topologies (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010) is a contemporary neuroevolution algorithm based on such an indirect encoding.", "startOffset": 84, "endOffset": 168}, {"referenceID": 23, "context": "HyperNEAT, which stands for Hypercube-based NeuroEvolution of Augmenting Topologies (Gauci and Stanley, 2008, 2010; Stanley et al., 2009; Verbancsics and Stanley, 2010) is a contemporary neuroevolution algorithm based on such an indirect encoding.", "startOffset": 84, "endOffset": 168}, {"referenceID": 18, "context": "In short, HyperNEAT evolves an encoding network called a compositional pattern producing network (CPPN; Stanley 2007) that describes the pattern of connectivity within the ANN it encodes.", "startOffset": 97, "endOffset": 117}, {"referenceID": 8, "context": "Because the CPPN encoding is designed to describe patterns of weights across the geometry of the encoded network, the weights in HyperNEAT ANNs tend to deform in contiguity-preserving and regularity-preserving ways (as seen in figure 1), thereby providing a useful bias (Gauci and Stanley, 2010; Stanley et al., 2009).", "startOffset": 270, "endOffset": 317}, {"referenceID": 19, "context": "Because the CPPN encoding is designed to describe patterns of weights across the geometry of the encoded network, the weights in HyperNEAT ANNs tend to deform in contiguity-preserving and regularity-preserving ways (as seen in figure 1), thereby providing a useful bias (Gauci and Stanley, 2010; Stanley et al., 2009).", "startOffset": 270, "endOffset": 317}, {"referenceID": 7, "context": "and has appeared in mainstream venues such as AAAI (Gauci and Stanley, 2008), Neural Computation (Gauci and Stanley, 2010), and JMLR (Verbancsics and Stanley, 2010).", "startOffset": 51, "endOffset": 76}, {"referenceID": 8, "context": "and has appeared in mainstream venues such as AAAI (Gauci and Stanley, 2008), Neural Computation (Gauci and Stanley, 2010), and JMLR (Verbancsics and Stanley, 2010).", "startOffset": 97, "endOffset": 122}, {"referenceID": 23, "context": "and has appeared in mainstream venues such as AAAI (Gauci and Stanley, 2008), Neural Computation (Gauci and Stanley, 2010), and JMLR (Verbancsics and Stanley, 2010).", "startOffset": 133, "endOffset": 164}, {"referenceID": 2, "context": "Unsupervised pretraining in deep learning has historically focused on approaches such as autoencoders and RBMs (Bengio et al., 2007; Hinton et al., 2006) that attempt to reconstruct training examples by first translating them into a basis of features different from the inputs, and then from those features regenerating the inputs.", "startOffset": 111, "endOffset": 153}, {"referenceID": 9, "context": "Unsupervised pretraining in deep learning has historically focused on approaches such as autoencoders and RBMs (Bengio et al., 2007; Hinton et al., 2006) that attempt to reconstruct training examples by first translating them into a basis of features different from the inputs, and then from those features regenerating the inputs.", "startOffset": 111, "endOffset": 153}, {"referenceID": 4, "context": "For example clustering algorithms such as K-means or Gaussian mixture models in effect extract structure from data that can then assist in classification; in fact at least one study has shown that such clustering algorithms can yield features as effective or more so for classification than autoencoders or RBMs (Coates et al., 2011).", "startOffset": 312, "endOffset": 333}, {"referenceID": 13, "context": "However, a critical facet of novelty-based searches that are combined with HyperNEAT-based neuroevolution is that the complexity of features (and hence distinctions) tends to increase over the run (Lehman and Stanley, 2011).", "startOffset": 197, "endOffset": 223}, {"referenceID": 12, "context": "To investigate this question DDFA is trained and tested on the MNIST handwritten digit recognition dataset (LeCun and Cortes, 1998), which consists of 60,000 training images and 10,000 test images.", "startOffset": 107, "endOffset": 131}, {"referenceID": 16, "context": "In particular, Simard et al. (2003) obtained one of the best such results of 1.", "startOffset": 15, "endOffset": 36}, {"referenceID": 9, "context": "2% result from Hinton et al. (2006) on a four-layer network pretrained by a RBM, would hint at DDFA\u2019s potential utility in the future for pretraining deeper networks.", "startOffset": 15, "endOffset": 36}, {"referenceID": 24, "context": "A simple variant of HyperNEAT called HyperNEAT-LEO (Verbancsics and Stanley, 2011) (which leads to less connectivity) was the main neuroevolution engine.", "startOffset": 51, "endOffset": 82}, {"referenceID": 2, "context": "This whole procedure is similar to how autoencoders are trained before gradient descent in deep learning (Bengio et al., 2007).", "startOffset": 105, "endOffset": 126}, {"referenceID": 6, "context": "The training and validation procedure mirrors that followed by Hinton et al. (2006): first training is run on 50,000 examples for 50 epochs to find the network that performs best on a 10,000-example validation set.", "startOffset": 63, "endOffset": 84}, {"referenceID": 16, "context": "6% error of the similar shallow network trained without preprocessing from Simard et al. (2003). In fact, the result for the 3,000-feature network even approaches the 1.", "startOffset": 75, "endOffset": 96}, {"referenceID": 9, "context": "2% error of the significantly deeper network of Hinton et al. (2006), showing that shallow networks can generalize surprisingly well by finding sufficiently high-quality feature sets, even despite a lack of exposure to distortions during training.", "startOffset": 48, "endOffset": 69}, {"referenceID": 21, "context": "For example, recent surprising results from Szegedy et al. (2013) show that very small yet anomalous perturbations of training images that are imperceptible to the human eye can fool several different kinds of deep networks that nevertheless ominously score well on the test set.", "startOffset": 44, "endOffset": 66}, {"referenceID": 0, "context": "At the same time, as Bengio (2013) points out, local descent on its own will not ultimately be enough to tackle the most challenging problems, suggesting the need for radical new kinds of optimization that are more global.", "startOffset": 21, "endOffset": 35}], "year": 2014, "abstractText": "Unlike unsupervised approaches such as autoencoders that learn to reconstruct their inputs, this paper introduces an alternative approach to unsupervised feature learning called divergent discriminative feature accumulation (DDFA) that instead continually accumulates features that make novel discriminations among the training set. Thus DDFA features are inherently discriminative from the start even though they are trained without knowledge of the ultimate classification problem. Interestingly, DDFA also continues to add new features indefinitely (so it does not depend on a hidden layer size), is not based on minimizing error, and is inherently divergent instead of convergent, thereby providing a unique direction of research for unsupervised feature learning. In this paper the quality of its learned features is demonstrated on the MNIST dataset, where its performance confirms that indeed DDFA is a viable technique for learning useful features.", "creator": "LaTeX with hyperref package"}}}