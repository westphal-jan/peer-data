{"id": "1406.1853", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2014", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "abstract": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is the Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. This represents the first unified framework for model-based reinforcement learning and provides state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm \\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.", "histories": [["v1", "Sat, 7 Jun 2014 03:02:09 GMT  (18kb)", "https://arxiv.org/abs/1406.1853v1", null], ["v2", "Fri, 31 Oct 2014 23:36:00 GMT  (22kb)", "http://arxiv.org/abs/1406.1853v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian osband", "benjamin van roy"], "accepted": true, "id": "1406.1853"}, "pdf": {"name": "1406.1853.pdf", "metadata": {"source": "CRF", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "authors": ["Ian Osband", "Benjamin Van Roy"], "emails": ["iosband@stanford.edu", "bvr@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n18 53\nv2 [\nst at\n.M L\n] 3\n1 O\nct 2\n\u221a dKdET ) where T is time elapsed, dK is the\nKolmogorov dimension and dE is the eluder dimension. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm posterior sampling for reinforcement learning (PSRL) that satisfies these bounds."}, {"heading": "1 Introduction", "text": "We consider the reinforcement learning (RL) problem of optimizing rewards in an unknown Markov decision process (MDP) [1]. In this setting an agent makes sequential decisions within its enironment to maximize its cumulative rewards through time. We model the environment as an MDP, however, unlike the standard MDP planning problem the agent is unsure of the underlying reward and transition functions. Through exploring poorlyunderstood policies, an agent may improve its understanding of its environment but it may improve its short term rewards by exploiting its existing knowledge [2, 3].\nThe focus of the literature in this area has been to develop algorithms whose performance will be close to optimal in some sense. There are numerous criteria for statistical and computational efficiency that might be considered. Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7]. We will focus our attention upon regret, or the shortfall in the agent\u2019s expected rewards compared to that of the optimal policy. We believe this is a natural criteria for performance during learning, although these concepts are closely linked. A good overview of various efficiency guarantees is given in section 3 of Li et al. [6].\nBroadly, algorithms for RL can be separated as either model-based, which build a generative model of the environment, or model-free which do not. Algorithms of both type have been developed to provide PAC-MDP bounds polynomial in the number of states S and actions A [8, 9, 10]. However, model-free approaches can struggle to plan efficient exploration. The only near-optimal regret bounds to time T of O\u0303(S \u221a AT ) have only been attained by modelbased algorithms [7, 11]. But even these bounds grow with the cardinality of the state and action spaces, which may be extremely large or even infinite. Worse still, there is a lower bound \u2126( \u221a SAT ) for the expected regret in an arbitrary MDP [7].\nIn special cases, where the reward or transition function is known to belong to a certain functional family, existing algorithms can exploit the structure to move beyond this \u201c\u2018tabula rasa\u201d (where nothing is assumed beyond S and A) lower bound. The most widely-studied\nparameterization is the degenerate MDP with no transitions, the mutli-armed bandit [12, 13, 14]. Another common assumption is that the transition function is linear in states and actions. Papers here establigh regret bounds O\u0303( \u221a T ) for linear quadratic control [16], but with constants that grow exponentially with dimension. Later works remove this exponential dependence, but only under significant sparsity assumptions [17]. The most general previous analysis considers rewards and transitions that are \u03b1-Ho\u0308lder in a d-dimensional space to establish regret bounds O\u0303(T (2d+\u03b1)/(2d+2\u03b1)) [18]. However, the proposed algorithm UCCRL is not computationally tractable and the bounds approach linearity in many settings.\nIn this paper we analyse the simple and intuitive algorithm posterior sampling for reinforcement learning (PSRL) [20, 21, 11]. PSRL was initially introduced as a heuristic method [21], but has since been shown to satisfy state of the art regret bounds in finite MDPs [11] and also exploit the structure of factored MDPs [15]. We show that this same algorithm satisfies general regret bounds that depends upon the dimensionality, rather than the cardinality, of the underlying reward and transition function classes. To characterize the complexity of this learning problem we extend the definition of the eluder dimension, previously introduced for bandits [19], to capture the complexity of the reinforcement learning problem. Our results provide a unified analysis of model-based reinforcement learning in general and provide new state of the art bounds in several important problem settings."}, {"heading": "2 Problem formulation", "text": "We consider the problem of learning to optimize a random finite horizon MDP M = (S,A, RM , PM , \u03c4, \u03c1) in repeated finite episodes of interaction. S is the state space, A is the action space, RM (s, a) is the reward distribution over R and PM (\u00b7|s, a) is the transition distribution over S when selecting action a in state s, \u03c4 is the time horizon, and \u03c1 the initial state distribution. All random variables we will consider are on a probability space (\u2126,F,P).\nA policy \u00b5 is a function mapping each state s \u2208 S and i = 1, . . . , \u03c4 to an action a \u2208 A. For each MDP M and policy \u00b5, we define a value function V :\nV M\u00b5,i(s) := EM,\u00b5 [\n\u03c4 \u2211\nj=i\nrM (sj , aj) \u2223 \u2223 \u2223si = s ]\n(1)\nwhere rM (s, a) := E[r|r \u223c RM (s, a)] and the subscripts of the expectation operator indicate that aj = \u00b5(sj , j), and sj+1 \u223c PM (\u00b7|sj , aj) for j = i, . . . , \u03c4 . A policy \u00b5 is said to be optimal for MDP M if V M\u00b5,i(s) = max\u00b5\u2032 V M \u00b5\u2032,i(s) for all s \u2208 S and i = 1, . . . , \u03c4 . We will associate with each MDP M a policy \u00b5M that is optimal for M .\nWe require that the state space S is a subset of Rd for some finite d with a \u2016 \u00b7 \u20162-norm induced by an inner product. These result actually extend to general Hilbert spaces, but we will not deal with that in this paper. This allows us to decompose the transition function as a mean value in S plus additive noise s\u2032 \u223c PM (\u00b7|s, a) =\u21d2 s\u2032 = pM (s, a) + \u01ebP . At first this may seem to exclude discrete MDPs with S states from our analysis. However, we can represent the discrete state as a probability vector st \u2208 S = [0, 1]S \u2282 RS with a single active component equal to 1 and 0 otherwise. In fact, the notational convention that S \u2286 Rd should not impose a great restriction for most practical settings. For any distribution \u03a6 over S, we define the one step future value function U to be the expected value of the optimal policy with the next state distributed according to \u03a6.\nUMi (\u03a6) := EM,\u00b5M [ VM\u00b5M ,i+1(s) \u2223 \u2223s \u223c \u03a6 ] . (2)\nOne natural regularity condition for learning is that the future values of similar distributions should be similar. We examine this idea through the Lipschitz constant on the means of these state distributions. We write E(\u03a6) := E[s|s \u223c \u03a6] \u2208 S for the mean of a distribution \u03a6 and express the Lipschitz continuity for UMi with respect to the \u2016 \u00b7 \u20162-norm of the mean:\n|UMi (\u03a6)\u2212 UMi (\u03a6\u0303)| \u2264 KMi (D)\u2016E(\u03a6)\u2212 E(\u03a6\u0303)\u20162 for all \u03a6, \u03a6\u0303 \u2208 D (3) We define KM (D) := maxiKMi (D) to be a global Lipschitz contant for the future value function with state distributions from D. Where appropriate, we will condense our notation\nto write KM := KM (D(M)) where D(M) := {PM (\u00b7|s, a)|s \u2208 S, a \u2208 A} is the set of all possible one-step state distributions under the MDP M .\nThe reinforcement learning agent interacts with the MDP over episodes that begin at times tk = (k \u2212 1)\u03c4 + 1, k = 1, 2, . . .. Let Ht = (s1, a1, r1, . . . , st\u22121, at\u22121, rt\u22121) denote the history of observations made prior to time t. A reinforcement learning algorithm is a deterministic sequence {\u03c0k|k = 1, 2, . . .} of functions, each mapping Htk to a probability distribution \u03c0k(Htk) over policies which the agent will employ during the kth episode. We define the regret incurred by a reinforcement learning algorithm \u03c0 up to time T to be\nRegret(T, \u03c0,M\u2217) :=\n\u2308T/\u03c4\u2309 \u2211\nk=1\n\u2206k,\nwhere \u2206k denotes regret over the kth episode, defined with respect to the MDP M \u2217 by\n\u2206k :=\n\u222b\ns\u2208S\n\u03c1(s) ( V M \u2217\n\u00b5\u2217,1 \u2212 V M \u2217\n\u00b5k,1\n)\n(s)\nwith \u00b5\u2217 = \u00b5M \u2217 and \u00b5k \u223c \u03c0k(Htk). Note that regret is not deterministic since it can depend on the random MDP M\u2217, the algorithm\u2019s internal random sampling and, through the history Htk , on previous random transitions and random rewards. We will assess and compare algorithm performance in terms of regret and its expectation."}, {"heading": "3 Main results", "text": "We now review the algorithm PSRL, an adaptation of Thompson sampling [20] to reinforcement learning. PSRL was first proposed by Strens [21] and later was shown to satisfy efficient regret bounds in finite MDPs [11]. The algorithm begins with a prior distribution over MDPs. At the start of episode k, PSRL samples an MDP Mk from the posterior. PSRL then follows the policy \u00b5k = \u00b5 Mk which is optimal for this sampled MDP during episode k.\nAlgorithm 1 Posterior Sampling for Reinforcement Learning (PSRL)\n1: Input: Prior distribution \u03c6 for M\u2217, t=1 2: for episodes k = 1, 2, .. do 3: sample Mk \u223c \u03c6(\u00b7|Ht) 4: compute \u00b5k = \u00b5Mk 5: for timesteps j = 1, .., \u03c4 do 6: apply at \u223c \u00b5k(st, j) 7: observe rt and st+1 8: advance t = t+ 1 9: end for 10: end for\nTo state our results we first introduce some notation. For any set X and Y \u2286 Rd for d finite let PC,\u03c3X ,Y be the family the distributions from X to Y with mean \u2016 \u00b7 \u20162-bounded in [0, C] and additive \u03c3-sub-Gaussian noise. We let N(F , \u03b1, \u2016 \u00b7 \u20162) be the \u03b1-covering number of F with respect to the \u2016 \u00b7 \u20162-norm and write nF = log(8N(F , 1/T 2, \u2016 \u00b7 \u20162)T ) for brevity. Finally we write dE(F) = dimE(F , T\u22121) for the eluder dimension of F at precision T\u22121, a notion of dimension specialized to sequential measurements described in Section 4.\nOur main result, Theorem 1, bounds the expected regret of PSRL at any time T .\nTheorem 1 (Expected regret for PSRL in parameterized MDPs). Fix a state space S, action space A, function families R \u2286 PCR,\u03c3RS\u00d7A,R and P \u2286 P CP ,\u03c3P S\u00d7A,S for any CR, CP , \u03c3R, \u03c3P > 0. Let M \u2217 be an MDP with state space S, action space A, rewards R\u2217 \u2208 R and transitions P \u2217 \u2208 P. If \u03c6 is the distribution of M\u2217 and K\u2217 = KM\u2217 is a global Lipschitz constant for the future value function as per (3) then:\nE[Regret(T, \u03c0PS ,M\u2217)] \u2264 [ CR + CP ] + D\u0303(R) + +E[K\u2217] ( 1 + 1\nT \u2212 1\n)\nD\u0303(P) (4)\nWhere for F equal to either R or P we will use the shorthand: D\u0303(F) := 1 + \u03c4CFdE(F) + 8 \u221a dE(F)(4CF + \u221a 2\u03c32F log(32T 3)) + 8 \u221a 2\u03c32FnFdE(F)T .\nTheorem 1 is a general result that applies to almost all RL settings of interest. In particular, we note that any bounded function is sub-Gaussian. To clarify the assymptotics if this bound we use another classical measure of dimensionality. Definition 1. The Kolmogorov dimension of a function class F is given by:\ndimK(F) := lim sup \u03b1\u21930 log(N(F , \u03b1, \u2016 \u00b7 \u20162)) log(1/\u03b1) .\nUsing Definition 1 in Theorem 1 we can obtain our Corollary.\nCorollary 1 (Assymptotic regret bounds for PSRL in parameterized MDPs). Under the assumptions of Theorem 1 and writing dK(F) := dimK(F):\nE[Regret(T, \u03c0PS ,M\u2217)] = O\u0303 ( \u03c3R \u221a dK(R)dE(R)T + E[K\u2217]\u03c3P \u221a dK(P)dE(P)T )\n(5)\nWhere O\u0303(\u00b7) ignores terms logarithmic in T .\nIn Section 4 we provide bounds on the eluder dimension of several function classes. These lead to explicit regret bounds in a number of important domains such as discrete MDPs, linear-quadratic control and even generalized linear systems. In all of these cases the eluder dimension scales comparably with more traditional notions of dimensionality. For clarity, we present bounds in the case of linear-quadratic control.\nCorollary 2 (Assymptotic regret bounds for PSRL in bounded linear quadratic systems). Let M\u2217 be an n-dimensional linear-quadratic system with \u03c3-sub-Gaussian noise. If the state is \u2016 \u00b7 \u20162-bounded by C and \u03c6 is the distribution of M\u2217, then:\nE[Regret(T, \u03c0PS ,M\u2217)] = O\u0303 ( \u03c3C\u03bb1n 2 \u221a T ) . (6)\nHere \u03bb1 is the largest eigenvalue of the matrix Q given as the solution of the Ricatti equations for the unconstrained optimal value function V (s) = \u2212sTQs [22].\nProof. We simply apply the results of for eluder dimension in Section 4 to Corollary 1 and upper bound the Lipschitz constant of the constrained LQR by 2C\u03bb1, see Appendix D.\nAlgorithms based upon posterior sampling are intimately linked to those based upon optimism [14]. In Appendix E we outline an optimistic variant that would attain similar regret bounds but with high probility in a frequentist sense. Unfortunately this algorithm remains computationally intractable even when presented with an approximate MDP planner. Further, we believe that PSRL will generally be more statistically efficient than an optimistic variant with similar regret bounds since the algorithm is not affected by loose analysis [11]."}, {"heading": "4 Eluder dimension", "text": "To quantify the complexity of learning in a potentially infinite MDP, we extend the existing notion of eluder dimension for real-valued functions [19] to vector-valued functions. For any G \u2286 PC,\u03c3X ,Y we define the set of mean functions F = E[G] := {f |f = E[G] for G \u2208 G}. If we consider sequential observations yi \u223c G\u2217(xi) we can equivalently write them as yi = f\u2217(xi)+ \u01ebi for some f\n\u2217(xi) = E[y|y \u223c G\u2217(xi)] and \u01ebi zero mean noise. Intuitively, the eluder dimension of F is the length d of the longest possible sequence x1, .., xd such that for all i, knowing the function values of f(x1), .., f(xi) will not reveal f(xi+1). Definition 2 ((F , \u01eb)\u2212 dependence). We will say that x \u2208 X is (F , \u01eb)-dependent on {x1, ..., xn} \u2286 X\n\u21d0\u21d2 \u2200f, f\u0303 \u2208 F , n \u2211\ni=1\n\u2016f(xi)\u2212 f\u0303(xi)\u201622 \u2264 \u01eb2 =\u21d2 \u2016f(x)\u2212 f\u0303(x)\u20162 \u2264 \u01eb.\nx \u2208 X is (\u01eb,F)-independent of {x1, .., xn} iff it does not satisfy the definition for dependence.\nDefinition 3 (Eluder Dimension). The eluder dimension dimE(F , \u01eb) is the length of the longest possible sequence of elements in X such that for some \u01eb\u2032 \u2265 \u01eb every element is (F , \u01eb\u2032)-independent of its predecessors.\nTraditional notions from supervised learning, such as the VC dimension, are not sufficient to characterize the complexity of reinforcement learning. In fact, a family learnable in constant time for supervised learning may require arbitrarily long to learn to control well [19]. The eluder dimension mirrors the linear dimension for vector spaces, which is the length of the longest sequence such that each element is linearly independent of its predecessors, but allows for nonlinear and approximate dependencies. We overload our notation for G \u2286 PC,\u03c3X ,Y and write dimE(G, \u01eb) := dimE(E[G], \u01eb), which should be clear from the context."}, {"heading": "4.1 Eluder dimension for specific function classes", "text": "Theorem 1 gives regret bounds in terms of the eluder dimension, which is well-defined for any F , \u01eb. However, for any given F , \u01eb actually calculating the eluder dimension may take some additional analysis. We now provide bounds on the eluder dimension for some common function classes in a similar approach to earlier work for real-valued functions [14]. These proofs are available in Appendix C.\nProposition 1 (Eluder dimension for finite X ). A counting argument shows that for |X | = X finite, any \u01eb > 0 and any function class F :\ndimE(F , \u01eb) \u2264 X This bound is tight in the case of independent measurements.\nProposition 2 (Eluder dimension for linear functions). Let F = {f |f(x) = \u03b8\u03c6(x) for \u03b8 \u2208 Rn\u00d7p, \u03c6 \u2208 Rp, \u2016\u03b8\u20162 \u2264 C\u03b8, \u2016\u03c6\u20162 \u2264 C\u03c6} then \u2200X :\ndimE(F , \u01eb) \u2264 p(4n\u2212 1) e e\u2212 1 log [( 1 + ( 2C\u03c6C\u03b8 \u01eb )2 ) (4n\u2212 1) ] + 1 = O\u0303(np)\nProposition 3 (Eluder dimension for quadratic functions). Let F = {f |f(x) = \u03c6(x)T \u03b8\u03c6(x) for \u03b8 \u2208 Rp\u00d7p, \u03c6 \u2208 Rp, \u2016\u03b8\u20162 \u2264 C\u03b8, \u2016\u03c6\u20162 \u2264 C\u03c6} then \u2200X :\ndimE(F , \u01eb) \u2264 p(4p\u2212 1) e\ne\u2212 1 log\n\n\n\n1 +\n(\n2pC2\u03c6C\u03b8\n\u01eb\n)2 \n (4p\u2212 1)\n\n+ 1 = O\u0303(p2).\nProposition 4 (Eluder dimension for generalized linear functions). Let g(\u00b7) be a component-wise independent function on Rn with derivative in each component bounded \u2208 [h, h] with h > 0. Define r = hh > 1 to be the condition number. If F = {f |f(x) = g(\u03b8\u03c6(x)) for \u03b8 \u2208 Rn\u00d7p, \u03c6 \u2208 Rp, \u2016\u03b8\u20162 \u2264 C\u03b8, \u2016\u03c6\u20162 \u2264 C\u03c6} then for any X :\ndimE(F , \u01eb) \u2264 p ( r2(4n\u2212 2) + 1 ) e\ne\u2212 1\n(\nlog\n[\n( r2(4n\u2212 2) + 1 )\n(\n1 + (2C\u03b8C\u03c6\n\u01eb\n)2 )])\n+1 = O\u0303(r2np)"}, {"heading": "5 Confidence sets", "text": "We now follow the standard argument that relates the regret of an optimistic or posterior sampling algorithm to the construction of confidence sets [7, 11]. We will use the eluder dimension build confidence sets for the reward and transition which contain the true functions with high probability and then bound the regret of our algorithm by the maximum deviation within the confidence sets. For observations from f\u2217 \u2208 F we will center the sets around the least squares estimate f\u0302LSt \u2208 argminf\u2208F L2,t(f) where L2,t(f) := \u2211t\u22121 i=1 \u2016f(xt) \u2212 yt\u201622 is the cumulative squared prediciton error. The confidence\nsets are defined Ft = Ft(\u03b2t) := {f \u2208 F|\u2016f \u2212 f\u0302LSt \u20162,Et \u2264 \u221a \u03b2t} where \u03b2t controls the growth of the confidence set and the empirical 2-norm is defined \u2016g\u201622,Et := \u2211t\u22121 i=1 \u2016g(xi)\u201622.\nFor F \u2286 PC,\u03c3X ,Y , we define the distinguished control parameter:\n\u03b2\u2217t (F , \u03b4, \u03b1) := 8\u03c32 log(N(F , \u03b1, \u2016 \u00b7 \u20162)/\u03b4) + 2\u03b1t ( 8C + \u221a 8\u03c32 log(4t2/\u03b4)) )\n(7)\nThis leads to confidence sets which contain the true function with high probability.\nProposition 5 (Confidence sets with high probability). For all \u03b4 > 0 and \u03b1 > 0 and the confidence sets Ft = Ft(\u03b2\u2217t (F , \u03b4, \u03b1)) for all t \u2208 N then:\nP\n(\nf\u2217 \u2208 \u221e \u22c2\nt=1\nFt ) \u2265 1\u2212 2\u03b4\nProof. We combine standard martingale concentrations with a discretization scheme. The argument is essentially the same as Proposition 6 in [14], but extends statements about R to vector-valued functions. A full derivation is available in the Appendix A."}, {"heading": "5.1 Bounding the sum of set widths", "text": "We now bound the deviation from f\u2217 by the maximum deviation within the confidence set.\nDefinition 4 (Set widths). For any set of functions F we define the width of the set at x to be the maximum L2 deviation between any two members of F evaluated at x.\nwF (x) := sup f,f\u2208F\n\u2016f(x)\u2212 f(x)\u20162\nWe can bound for the number of large widths in terms of the eluder dimension.\nLemma 1 (Bounding the number of large widths). If {\u03b2t > 0 \u2223\n\u2223t \u2208 N} is a nondecreasing sequence with Ft = Ft(\u03b2t) then m \u2211\nk=1\n\u03c4 \u2211\ni=1\n1{wFtk (xtk+i) > \u01eb} \u2264 ( 4\u03b2T \u01eb2 + \u03c4 ) dimE(F , \u01eb)\nProof. This result follows from proposition 8 in [14] but with a small adjustment to account for episodes. A full proof is given in Appendix B.\nWe now use Lemma 1 to control the cumulative deviation through time.\nProposition 6 (Bounding the sum of widths). If {\u03b2t > 0 \u2223\n\u2223t \u2208 N} is nondecreasing with Ft = Ft(\u03b2t) and \u2016f\u20162 \u2264 C for all f \u2208 F then: m \u2211\nk=1\n\u03c4 \u2211\ni=1\nwFtk (xtk+i) \u2264 1 + \u03c4CdimE(F , T \u22121) + 4\n\u221a\n\u03b2TdimE(F , T\u22121)T (8)\nProof. Once again we follow the analysis of Russo [14] and strealine notation by letting wt = wFtk (xtk+i) abd d = dimE(F , T\n\u22121). Reordering the sequence (w1, .., wT ) \u2192 (wi1 , .., wiT ) such that wi1 \u2265 .. \u2265 wiT we have that:\nm \u2211\nk=1\n\u03c4 \u2211\ni=1\nwFtk (xtk+i) =\nT \u2211\nt=1\nwit \u2264 1 + T \u2211\ni=1\nwit1{wit \u2265 T\u22121}\n.\nBy the reordering we know that wit > \u01eb means that \u2211m k=1 \u2211\u03c4 i=1 1{wFtk (xtk+i) > \u01eb} \u2265 t. From Lemma 1, \u01eb \u2264 \u221a\n4\u03b2T d t\u2212\u03c4d . So that if wit > T \u22121 then wit \u2264 min{C, \u221a 4\u03b2T d t\u2212\u03c4d}. Therefore,\nT \u2211\ni=1\nwit1{wit \u2265 T\u22121} \u2264 \u03c4Cd+ T \u2211\nt=\u03c4d+1\n\u221a\n4\u03b2Td\nt\u2212 \u03c4d \u2264 \u03c4Cd+2 \u221a \u03b2T\n\u222b T\n0\n\u221a\nd t dt \u2264 \u03c4Cd+4 \u221a \u03b2T dT"}, {"heading": "6 Analysis", "text": "We will now show reproduce the decomposition of expected regret in terms of the Bellman error [11]. From here, we will apply the confidence set results from Section 5 to obtain our regret bounds. We streamline our discussion of PM , RM , V M\u00b5,i , U M i and T M\u00b5 by simply writing \u2217 in place of M\u2217 or \u00b5\u2217 and k in place of Mk or \u00b5k where appropriate; for example V \u2217k,i := V M\u2217 \u00b5\u0303k,i .\nThe first step in our ananlysis breaks down the regret by adding and subtracting the imagined optimal reward of \u00b5k under the MDP Mk.\n\u2206k = ( V \u2217\u2217,1 \u2212 V \u2217k,1 ) (s0) = ( V \u2217\u2217,1 \u2212 V kk,1 ) (s0) + ( V kk,1 \u2212 V \u2217k,1 ) (s0) (9)\nHere s0 is a distinguished initial state, but moving to general \u03c1(s) poses no real challenge. Algorithms based upon optimism bound (V \u2217\u2217,1 \u2212 V kk,1) \u2264 0 with high probability. For PSRL we use Lemma 2 and the tower property to see that this is zero in expectation.\nLemma 2 (Posterior sampling). If \u03c6 is the distribution of M\u2217 then, for any \u03c3(Htk)-measurable function g,\nE[g(M\u2217)|Htk ] = E[g(Mk)|Htk ] (10)\nWe introduce the Bellman operator T M\u00b5 , which for any MDP M = (S,A, RM , PM , \u03c4, \u03c1), stationary policy \u00b5 : S \u2192 A and value function V : S \u2192 R, is defined by\nT M\u00b5 V (s) := rM (s, \u00b5(s)) + \u222b\ns\u2032\u2208S\nPM (s\u2032|s, \u00b5(s))V (s\u2032).\nThis returns the expected value of state s where we follow the policy \u00b5 under the laws of M , for one time step. The following lemma gives a concise form for the dynamic programming paradigm in terms of the Bellman operator.\nLemma 3 (Dynamic programming equation). For any MDP M = (S,A, RM , PM , \u03c4, \u03c1) and policy \u00b5 : S \u00d7 {1, . . . , \u03c4} \u2192 A, the value functions V M\u00b5 satisfy V M\u00b5,i = T M\u00b5(\u00b7,i)V M\u00b5,i+1 (11) for i = 1 . . . \u03c4 , with V M\u00b5,\u03c4+1 := 0.\nThrough repeated application of the dynamic programming operator and taking expectation of martingale differences we can mirror earlier analysis [11] to equate expected regret with the cumulative Bellman error:\nE[\u2206k] =\n\u03c4 \u2211\ni=1\n(T kk,i \u2212 T \u2217k,i)V kk,i+1(stk+i) (12)"}, {"heading": "6.1 Lipschitz continuity", "text": "Efficient regret bounds for MDPs with an infinite number of states and actions require some regularity assumption. One natural notion is that nearby states might have similar optimal values, or that the optimal value function function might be Lipschitz. Unfortunately, any discontinuous reward function will usually lead to discontious values functions so that this assumption is violated in many settings of interest. However, we only require that the future value is Lipschitz in the sense of equation (3). This will will be satisfied whenever the underlying value function is Lipschitz, but is a strictly weaker requirement since the system noise helps to smooth future values.\nSince P has \u03c3P -sub-Gaussian noise we write st+1 = pM (st, at) + \u01ebPt in the natural way. We now use equation (12) to reduce regret to a sum of set widths. To reduce clutter and more closely follow the notation of Section 4 we will write xk,i = (stk+i, atk+i).\nE[\u2206k] \u2264 E [ \u03c4 \u2211\ni=1\n{ rk(xk,i)\u2212 r\u2217(xk,i) + Uki (P k(xk,i))\u2212 Uki (P \u2217(xk,i)) }\n]\n\u2264 E [ \u03c4 \u2211\ni=1\n{ |rk(xk,i)\u2212 r\u2217(xk,i)|+Kk\u2016pk(xk,i)\u2212 p\u2217(xk,i)\u20162 }\n]\n(13)\nWhere Kk is a global Lipschitz constant for the future value function of Mk as per (3).\nWe now use the results from Sections 4 and 5 to form the corresponding confidence sets Rk := Rtk(\u03b2\u2217(R, \u03b4, \u03b1)) and Pk := Ptk(\u03b2\u2217(P , \u03b4, \u03b1)) for the reward and transition functions respectively. Let A = {R\u2217, Rk \u2208 Rk \u2200k} and B = {P \u2217, Pk \u2208 Pk \u2200k} and condition upon these events to give:\nE[Regret(T, \u03c0PS ,M\u2217)] \u2264 E [ m \u2211\nk=1\n\u03c4 \u2211\ni=1\n{ |rk(xk,i)\u2212 r\u2217(xk,i)|+Kk\u2016pk(xk,i)\u2212 p\u2217(xk,i)\u20162 }\n]\n\u2264 m \u2211\nk=1\n\u03c4 \u2211\ni=1\n{\nwRk(xk,i) + E[K k|A,B]wPk(xk,i) + 8\u03b4(CR + CP )\n}\n(14)\nThe posterior sampling lemma ensures that E[Kk] = E[K\u2217] so that E[Kk|A,B] \u2264 E[K \u2217]\nP(A,B) \u2264 E[K\u2217] 1\u22128\u03b4 by a union bound on {Ac \u222aBc}. We fix \u03b4 = 1/8T to see that:\nE[Regret(T, \u03c0PS,M\u2217)] \u2264 (CR + CP) + m \u2211\nk=1\n\u03c4 \u2211\ni=1\nwRk (xk,i) +E[K \u2217] ( 1 + 1 T \u2212 1 )\nm \u2211\nk=1\n\u03c4 \u2211\ni=1\nwPt(xk,i)\nWe now use equation (7) together with Proposition 6 to obtain our regret bounds. For ease of notation we will write dE(R) = dimE(R, T\u22121) and dE(P) = dimE(P , T\u22121).\nE[Regret(T, \u03c0PS ,M\u2217)] \u2264 2 + (CR + CP) + \u03c4(CRdE(R) + CPdE(P)) +\n4 \u221a \u03b2\u2217T (R, 1/8T, \u03b1)dE(R)T + 4 \u221a \u03b2\u2217T (P , 1/8T, \u03b1)dE(P)T(15)\nWe let \u03b1 = 1/T 2 and write nF = log(8N(F , 1/T 2, \u2016 \u00b7 \u20162)T ) for R and P to complete our proof of Theorem 1:\nE[Regret(T, \u03c0PS ,M\u2217)] \u2264 [ CR + CP ] + D\u0303(R) + E[K\u2217] ( 1 + 1\nT \u2212 1\n)\nD\u0303(P) (16)\nWhere D\u0303(F) is shorthand for 1 + \u03c4CFdE(F) + 8 \u221a dE(F)(4CF + \u221a 2\u03c32F log(32T 3)) + 8 \u221a\n2\u03c32FnFdE(F)T . The first term [CR + CP ] bounds the contribution from missed confidence sets. The cost of learning the reward function R\u2217 is bounded by D\u0303(R). In most problems the remaining contribution bounding transitions and lost future value will be dominant. Corollary 1 follows from the Definition 1 together with nR and nP ."}, {"heading": "7 Conclusion", "text": "We present a new analysis of posterior sampling for reinforcement learning that leads to a general regret bound in terms of the dimensionality, rather than the cardinality, of the underlying MDP. These are the first regret bounds for reinforcement learning in such a general setting and provide new state of the art guarantees when specialized to several important problem settings. That said, there are a few clear shortcomings which we do not address in the paper. First, we assume that it is possible to draw samples from the posterior distribution exactly and in some cases this may require extensive computational effort. Second, we wonder whether it is possible to extend our analysis to learning in MDPs without episodic resets. Finally, there is a fundamental hurdle to model-based reinforcement learning that planning for the optimal policy even in a known MDP may be intractable. We assume access to an approximateMDP planner, but this will generally require lengthy computations. We would like to examine whether similar bounds are attainable in model-free learning [23], which may obviate complicated MDP planning, and examine the computational and statistical efficiency tradeoffs between these methods."}, {"heading": "Acknowledgments", "text": "Osband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc. This work was supported in part by Award CMMI-0968707 from the National Science Foundation."}, {"heading": "A Confidence sets with high probability", "text": "In this appendix we will build up to a proof of Proposition 5, that the confidence sets defined by \u03b2\u2217 in equation 7 hold with high probability. We begin with some elementary results from martingale theory.\nLemma 4 (Exponential Martingale). Let Zi \u2208 L1 be real-calued random variables adapted to Hi. We define the conditional mean \u00b5i = E[Zi|Hi\u22121] and conditional cumulant generating function \u03c8i(\u03bb) = logE[exp (\u03bb(Zi \u2212 \u00b5i)) |Hi\u22121], then\nMn(\u03bb) = exp\n(\nn \u2211\n1\n\u03bb(Zi \u2212 \u00b5i)\u2212 \u03c8i(\u03bb) )\nis a martingale with E[Mn(\u03bb)] = 1.\nLemma 5 (Concentration Guarantee). For Zi adapted real L\n1 random variables adapted to Hi. We define the conditional mean \u00b5i = E[Zi|Hi\u22121] and conditional cumulant generating function \u03c8i(\u03bb) = logE[exp (\u03bb(Zi \u2212 \u00b5i)) |Hi\u22121].\nP\n(\n\u221e \u22c3\nn=1\n{ n \u2211\n1\n\u03bb(Zi \u2212 \u00b5i)\u2212 \u03c8i(\u03bb) \u2265 x} ) \u2264 e\u2212x\nBoth of these lemmas are available in earlier discussion for real-valued variables [14]. We now specialize our discussion to the vector space Y \u2286 Rd where the inner product < y, y >= \u2016y\u201622. To simplify notation we will write f\u2217t := f \u2217(xt) and ft = f(xt) for arbitrary f \u2208 F . We now define\nZt = \u2016f\u2217t \u2212 yt\u20162 \u2212 \u2016ft \u2212 yt\u20162 = < f\u2217t \u2212 yt, f\u2217t \u2212 yt > \u2212 < ft \u2212 yt, ft \u2212 yt, ft \u2212 yt > = \u2212 < ft \u2212 f\u2217t , ft \u2212 f\u2217t > +2 < ft \u2212 f\u2217t , yt \u2212 f\u2217t > = \u2212\u2016ft \u2212 f\u2217t \u20162 + 2 < ft \u2212 f\u2217t , \u01ebt >\nso that clearly \u00b5t = \u2212\u2016ft \u2212 f\u2217t \u20162. Now since we have said that the noise is \u03c3-sub-Gaussian, E[exp (< \u03c6, \u01eb >)] \u2264 exp ( \u2016\u03c6\u20162 2 \u03c32\n2\n)\n\u2200\u03c6 \u2208 Y. From here we can deduce that:\n\u03c8t(\u03bb) = logE[exp (\u03bb(Zt \u2212 \u00b5t)) |Ht\u22121] = logE[exp(2\u03bb < ft \u2212 f\u2217t , \u01ebt >)]\n\u2264 \u20162\u03bb(ft \u2212 f \u2217 t )\u201622\u03c32\n2 .\nWe now write \u2211t\u22121 i=1 Zi = L2,t(f\u2217) \u2212 L2,t(f) according to our earlier definition of L2,t. We can apply Lemma 5 with \u03bb = 1/4\u03c32, x = log(1/\u03b4) to obtain:\nP{ ( L2,t(f) \u2265 L2,t(f\u2217) + 1 2 \u2016f \u2212 f\u2217\u20162,Et \u2212 4\u03c32 log(1/\u03b4) ) \u2200t} \u2265 1\u2212 \u03b4\nsubstituting f = f\u0302 to be the least squares solution which minimizes L2,t(f) we can remove L2,t(f\u0302)\u2212 L2,t(f\u2217) \u2264 0. From here we use an \u03b1-cover discretization argument to complete the proof of Proposition 5.\nLet F\u03b1 \u2282 F be an \u03b1-2 cover of F such that \u2200f \u2208 F there is some \u2016f\u03b1 \u2212 f\u20162 \u2264 \u03b1. We can use a union bound on F\u03b1 so that \u2200f \u2208 F :\nL2,t(f)\u2212 L2,t(f\u2217) \u2265 12\u2016f \u2212 f \u2217\u20162,Et \u2212 4\u03c32 log(N(F , \u03b1, \u2016 \u00b7 \u20162)/\u03b4) +DE(\u03b1) (17)\nFor DE(\u03b1) = min f\u03b1\u2208F\u03b1 {1 2 \u2016f\u03b1 \u2212 f\u2217\u201622,Et \u2212 1 2 \u2016f \u2212 f\u2217\u201622,Et + L2,t(f)\u2212 L2,t(f \u03b1) }\nWe will now seek to bound this discretization error with high probability.\nLemma 6 (Bounding discretization error). If \u2016f\u03b1(x)\u2212 f(x)\u20162 \u2264 \u03b1 for all x \u2208 X then with probability at least 1\u2212 \u03b4:\nDE(\u03b1) \u2264 \u03b1t [ 8C + \u221a 8\u03c32 log(4t2/\u03b4) ]\nProof. For non-trivial bounds we will consider the case of \u03b1 \u2264 C and note that via Cauchy-Schwarz: \u2016f\u03b1(x)\u201622 \u2212 \u2016f(x)\u201622 \u2264 max\n\u2016y\u20162\u2264\u03b1 \u2016f(x) + y\u201622 \u2212 \u2016f\u201622 \u2264 2C\u03b1+ \u03b12.\nFrom here we can say that\n\u2016f\u03b1(x)\u2212 f\u2217(x)\u201622 \u2212 \u2016f(x)\u2212 f\u2217(x)\u201622 = \u2016f\u03b1(x)\u201622 \u2212 \u2016f(x)\u201622 + 2 < f\u2217(x), f(x)\u2212 f\u03b1(x) >\u2264 4C\u03b1 \u2016y \u2212 f(x)\u201622 \u2212 \u2016y \u2212 f\u03b1(x)\u201622 = 2 < y, f\u03b1(x)\u2212 f(x) > +\u2016f(x)\u201622 \u2212 \u2016f\u03b1(x)\u201622 \u2264 2\u03b1|y|+ 2C\u03b1+ \u03b12 Summing these expressions over time i = 1, .., t\u22121 and using sub-gaussian high probability bounds on |y| gives our desired result.\nFinally we apply Lemma 6 to equation 17 and use the fact that f\u0302LSt is the L2,t minimizer to obtain the result that with probability at least 1\u2212 2\u03b4:\n\u2016f\u0302LSt \u2212 f\u2217\u20162,Et \u2264 \u221a\n\u03b2\u2217t (F , \u03b1, \u03b4) Which is our desired result."}, {"heading": "B Bounding the number of large widths", "text": "Lemma 1 (Bounding the number of large widths). If {\u03b2t > 0 \u2223\n\u2223t \u2208 N} is a nondecreasing sequence with Ft = Ft(\u03b2t) then m \u2211\nk=1\n\u03c4 \u2211\ni=1\n1{wFtk (xtk+i) > \u01eb} \u2264 (4\u03b2T\n\u01eb2 + \u03c4\n)\ndimE(F , \u01eb)\nProof. We first imagine that wFt(xt) > \u01eb and is \u01eb-dependent on K disjoint subsequences of x1, .., xt\u22121. If xt is \u01eb-dependent on K disjoint subsequences then there exist \u2016f \u2212 f\u20162,Et > K\u01eb2. By the triangle inequality \u2016f \u2212 f\u20162,Et \u2264 2 \u221a \u03b2t \u2264 2 \u221a \u03b2T so that K < 4\u03b2T /\u01eb2.\nIn the case without episodic delay, Russo went on to show that in any sequence of length l there is some element which is \u01eb-dependent on at least l\ndimE(F,\u01eb) \u22121 disjoint subsequences [14]. Our analysis\nfollows similarly, but we may lose up to \u03c4 \u22121 proper subsequences due to the delay in updating the episode. This means that we can only say that K \u2265 l\ndimE(F,\u01eb) \u2212 \u03c4 . Considering the subsequence\nwFtk (xtk+i) > \u01eb we see that l \u2264 ( 4\u03b2T \u01eb2 + \u03c4 ) dimE(F , \u01eb) as required."}, {"heading": "C Eluder dimension for specific function classes", "text": "In this section of the appendix we will provide bounds upon the eluder dimension for some canonical function classes. Recalling Definition 3, dimE(F , \u01eb) is the length d of the longest sequence x1, .., xd such that for some \u01eb\u2032 \u2265 \u01eb:\nwk = sup\n{ \u2016(f \u2212 f)(xk)\u20162 \u2223 \u2223 \u2223\n\u2223\n\u2016f \u2212 f\u20162,Et \u2264 \u01eb\u2032 } > \u01eb\u2032 (18)\nfor each k \u2264 d.\nC.1 Finite domain X\nAny x \u2208 X is \u01eb-dependent upon itself for all \u01eb > 0. Therefore for all \u01eb > 0 the eluder dimension of F is bounded by |X |.\nC.2 Linear functions f(x) = \u03b8\u03c6(x)\nLet F = {f |f(x) = \u03b8\u03c6(x) for \u03b8 \u2208 Rn\u00d7p, \u03c6 \u2208 Rp, \u2016\u03b8\u20162 \u2264 C\u03b8, \u2016\u03c6\u20162 \u2264 C\u03c6}. To simplify our notation we will write \u03c6k = \u03c6(xk) and \u03b8 = \u03b81 \u2212 \u03b82. From here, we may manipulate the expression\n\u2016\u03b8\u03c6\u201622 = \u03c6Tk \u03b8T \u03b8\u03c6k = Tr(\u03c6Tk \u03b8T \u03b8\u03c6k) = Tr(\u03b8\u03c6k\u03c6Tk \u03b8)\n=\u21d2 wk = sup \u03b8\n{\u2016\u03b8\u03c6k\u20162 \u2223 \u2223Tr(\u03b8\u03a6k\u03b8 T ) \u2264 \u01eb2} where \u03a6k :=\nk\u22121 \u2211\ni=1\n\u03c6i\u03c6 T i\nWe next require a lemma which gives an upper bound for trace constrained optimizations.\nLemma 7 (Bounding norms under trace constraints). Let \u03b8 \u2208 Rn\u00d7p, \u03c6 \u2208 Rp and V \u2208 Rp\u00d7p++ , the set of positive definite p\u00d7 p matrices, then:\nW 2 = max \u03b8 \u2016\u03b8\u03c6\u201622 subject to Tr(\u03b8V \u03b8T ) \u2264 \u01eb2\nis bounded above by W 2 \u2264 (2n\u2212 1)\u01eb2\u2016\u03c6\u20162V \u22121 where \u2016\u03c6\u20162A := \u03c6TA\u03c6.\nProof. We first note that \u2016\u03b8\u03c6\u201622 = Tr(\u03b8\u03c6\u03c6T\u03b8T ) = \u2211n 1 (\u03b8\u03c6)2i \u2264 ( \u2211n 1 (\u03b8\u03c6)i )2 by Jensen\u2019s inequality. We define \u03a6\u0303 \u2208 Rn\u00d7p such that each row of \u03a6\u0303 = \u03c6T . Then this inequality can be expressed as: W 2 = Tr(\u03b8\u03c6\u03c6T\u03b8T ) \u2264 Sum(\u03b8 \u2297 \u03a6\u0303)2\nWhere A\u2297B = C for Cij = AijBij and Sum(C) := \u2211\ni,j Cij We can now obtain an upper bound\nfor our original problem through this convex relaxation:\nmax \u03b8\nSum(\u03b8 \u2297 \u03a6\u0303) subject to Tr(\u03b8V \u03b8T ) \u2264 \u01eb2\nWe can now form the lagrangian L(\u03b8, \u03bb) = \u2212Sum(\u03b8 \u2297 \u03a6\u0303) + \u03bb(Tr(\u03b8V \u03b8T ) \u2212 \u01eb2). Solving for first order optimality \u25bd\u03b8L = 0 =\u21d2 \u03b8\u2217 = 12\u03bb \u03a6\u0303V \u22121. From here we form the dual objective\ng(\u03bb) = \u2212Sum( 1 2\u03bb \u03a6\u0303V \u22121 \u2297 \u03a6\u0303) + Tr( 1 4\u03bb \u03a6\u0303V \u22121\u03a6\u0303T )\u2212 \u03bb\u01eb2\nHere we solve for the dual-optimal \u03bb\u2217 \u25bd\u03bbg = 0 =\u21d2 12\u03bb\u2217 2 Sum( 1 2\u03bb \u03a6\u0303V \u22121 \u2297 \u03a6\u0303) \u2212\n1 4\u03bb\u2217 2 Tr( 1 4\u03bb \u03a6\u0303V \u22121\u03a6\u0303T ) = \u01eb2. From the definition of \u03a6\u0303, Sum(\u03a6\u0303V \u22121 \u2297 \u03a6\u0303) = n\u03c6TV \u22121\u03c6 and Tr(\u03a6\u0303V \u22121\u03a6\u0303T ) = \u03c6TV \u22121\u03c6. From this we can simplify our expression to conclude:\nn 2\u03bb\u22172 \u03c6TV \u22121\u03c6\u2212 1 4\u03bb\u22172 \u03c6TV \u22121\u03c6 = \u01eb2 =\u21d2 \u03bb\u2217 =\n\u221a\n(n\u2212 1/2) 2\u01eb2 \u2016\u03c6\u2016V \u22121\n=\u21d2 g(\u03bb\u2217) = \u2212 n 2\u03bb\u2217 \u2016\u03c6\u20162V \u22121 + 1 4\u03bb\u2217 \u2016\u03c6\u20162V \u22121 \u2212 \u03bb\u2217\u01eb\nstrong duality =\u21d2 f(\u03b8\u2217) = g(\u03bb\u2217) = \u221a 2n\u2212 1\u01eb\u2016\u03c6\u2016V \u22121\nFrom here we conclude that the optimal value of W 2 \u2264 f(\u03b8\u2217)2 \u2264 (2n\u2212 1)\u01eb2\u2016\u03c6\u20162V \u22121 .\nUsing this lemma, we will be able to address the eluder dimension for linear functions. Using the definition of wk from equation 18 together with \u03a6k we may rewrite:\nwk = max \u03b8\n{ \u221a Tr(\u03b8\u03c6k\u03c6Tk \u03b8) \u2223 \u2223 Tr(\u03b8\u03a6k\u03b8 T ) \u2264 \u01eb2}.\nLet Vk := \u03a6k + (\n\u01eb 2C\u03b8\n)2\nI so that Tr(\u03b8\u03a6k\u03b8T ) \u2264 \u01eb2 =\u21d2 Tr(\u03b8Vk\u03b8T ) \u2264 2\u01eb2 through a triangle inequality. Now applying Lemma 7 we can say that wk \u2264 \u01eb \u221a 4n\u2212 2\u2016\u03c6k\u2016V \u22121\nk\n. This means that if\nwk \u2265 \u01eb then \u2016\u03c6k\u20162V \u22121 k > 1 4n\u22122 > 0.\nWe now imagine that wi \u2265 \u01eb for each i < k. Then since Vk = Vk\u22121 + \u03c6k\u03c6Tk we can use the Matrix Determinant together with the above observation to say that:\ndet(Vk) = det(Vk\u22121)(1 + \u03c6 T k V \u22121 K \u03c6k) \u2265 det(Vk\u22121)\n(\n1 + 1\n4n\u2212 2\n) \u2265 .. \u2265 \u03bbp ( 1 + 1\n4n\u2212 2\n)k\u22121\n(19)\nfor \u03bb := (\n\u01eb 2C\u03b8\n)2\n. To get an upper bound on the determinant we note that det(Vk) is maximized\nwhen all eigenvalues are equal or equivalently:\ndet(Vk) \u2264 (\nTr(Vk) p\n)p \u2264 ( C2\u03c6(k \u2212 1) p + \u03bb )p\n(20)\nNow using equations 19 and 20 together we see that k must satistfy the inequality (\n1 + 1 4n\u22122\n)(k\u22121)/p \u2264 C 2 \u03c6 (k\u22121)\n\u03bbp + 1. We now write \u03b60 = 14n\u22122 and \u03b10 =\nC2 \u03c6 \u03bb = ( 2C\u03c6C\u03b8 \u01eb )2 so that we\ncan epress this as:\n(1 + \u03b60) k\u22121 p \u2264 \u03b10 k \u2212 1\np + 1\nWe now use the result that B(x, \u03b1) = max{B \u2223\n\u2223 (1+x)B \u2264 \u03b1B+1} \u2264 1+x x e e\u22121 {log(1+\u03b1)+log( 1+x x )}. We complete our proof of Proposition 2 through computing this upper bound at (\u03b60, \u03b10),\ndimE(F , \u01eb) \u2264 p(4n\u2212 1) e e\u2212 1 log [( 1 + (2C\u03c6C\u03b8 \u01eb )2 ) (4n\u2212 1) ] + 1 = O\u0303(np).\nC.3 Quadratic functions f(x) = \u03c6T (x)\u03b8\u03c6(x)\nLet F = {f |f(x) = \u03c6(x)T \u03b8\u03c6(x) for \u03b8 \u2208 Rp\u00d7p, \u03c6 \u2208 Rp, \u2016\u03b8\u20162 \u2264 C\u03b8, \u2016\u03c6\u20162 \u2264 C\u03c6} then for any X we can say that:\ndimE(F , \u01eb) \u2264 p(4p\u2212 1) e e\u2212 1 log [( 1 + ( 2pC2\u03c6C\u03b8 \u01eb )2 ) (4p\u2212 1) ] + 1 = O\u0303(p2).\nWhere we have simply applied the linear result with \u01eb\u0303 = \u01eb pCP . This is valid since if we can identify the linear function g(x) = \u03b8\u03c6(x) to within this tolerance then we will certainly know f(x) as well.\nC.4 Generalized linear models\nLet g(\u00b7) be a component-wise independent function on Rn with derivative in each component bounded \u2208 [h, h] with h > 0. Define r = h\nh > 1 to be the condition number. If F = {f |f(x) =\ng(\u03b8\u03c6(x)) for \u03b8 \u2208 Rn\u00d7p, \u03c6 \u2208 Rp, \u2016\u03b8\u20162 \u2264 C\u03b8, \u2016\u03c6\u20162 \u2264 C\u03c6} then for any X :\ndimE(F , \u01eb) \u2264 p ( r2(4n\u2212 2) + 1 ) e\ne\u2212 1\n(\nlog\n[\n( r2(4n\u2212 2) + 1 )\n(\n1 + (2C\u03b8C\u03c6\n\u01eb\n)2 )])\n+1 = O\u0303(r2np)\nThis proof follows exactly as per the linear case, but first using a simple reduction on the form of equation (18).\nwk = sup\n{ \u2016(f \u2212 f)(xk)\u20162 \u2223 \u2223 \u2223\n\u2223\n\u2016f \u2212 f\u20162,Et \u2264 \u01eb\u2032 }\n\u2264 max \u03b81,\u03b82\n{\n\u2016g(\u03b81\u03c6k)\u2212 g(\u03b82\u03c6k)\u20162 \u2223 \u2223\nk\u22121 \u2211\ni=1\n\u2016g(\u03b81\u03c6i)\u2212 g(\u03b82\u03c6i)\u201622 \u2264 \u01eb2 }\n\u2264 max \u03b8\n{\nh\u2016\u03b8\u03c6k\u20162 \u2223 \u2223\nk\u22121 \u2211\ni=1\nh2\u2016\u03b8\u03c6i\u201622 \u2264 \u01eb2 }\nTo which we can now apply Lemma 7 with the \u01eb rescaled by r. Following the same arguments as for linear functions now completes our proof."}, {"heading": "D Bounded LQR control", "text": "We imagine a standard linear quadratic controller with rewards with x = (s, a) the state-action vector. The rewards and transitions are given by:\nR(x) = \u2212xTAx+ \u01ebR , P (x) = \u03a0C(Bx+ \u01ebP ), where A < 0 is positive semi-definite and \u03a0C projects x onto the \u2016 \u00b7 \u20162-ball at radius C. In the case of unbounded states and actions the Ricatti equations give the form of the optimal value function V (s) = \u2212sTQs for Q < 0. In this case we can see that the difference in values of two states: |V (s)\u2212 V (s\u2032)| = | \u2212 sTQs+ s\u2032TQs\u2032| = | \u2212 (s+ s\u2032)TQ(s\u2212 s\u2032)| \u2264 2C\u03bb1\u2016s \u2212 s\u2032\u20162 where \u03bb1 is the largest eigenvalue of Q and C is an upper bound on the \u2016 \u00b7 \u20162-norm of both s and s\u2032. We note that 2C\u03bb1 works as an effective Lipshcitz constant when we know what C can bound s, s\u2032.\nWe observe that for any projection \u03a0C(x) = \u03b1x for \u03b1 \u2208 (0, 1] and that for all positive semi-definite matrices A < 0, (\u03b1x)TA(\u03b1x) = \u03b12xTAx \u2264 xTAx. Using this observation together with reward and transition functions we can see that the value function of the bounded LQR system is always greater than or equal to that of the unconstrained value function. The effect of excluding the lowreward outer region, but maintaining the higher-reward inner region means that the value function becomes more flat in the bounded case, and so 2C\u03bb1 works as an effective Lipschitz constant for this problem too."}, {"heading": "E UCRL-Eluder", "text": "For completeness, we explicitly outline an optimistic algorithm which uses the confidence sets in our analysis of PSRL to guarantee similar regret bounds with high probability over all MDP M\u2217. The algorithm follows the style of UCRL2 [7] so that at the start of the kth episode the algorithm form Mk = {M |PM \u2208 Pk, RM \u2208 Rk} and then solves for the optimistic policy that attains the highest reward over any M in Mk.\nAlgorithm 2 UCRL-Eluder\n1: Input: Confidence parameter \u03b4 > 0, t=1 2: for episodes k = 1, 2, .. do 3: form confidence sets Rk(\u03b2\u2217(R, \u03b4, 1/k2)) and Pk(\u03b2\u2217(P , \u03b4, 1/k2)) 4: compute \u00b5k optimistic policy over Mk = {M |PM \u2208 Pk, RM \u2208 Rk} 5: for timesteps j = 1, .., \u03c4 do 6: apply at \u223c \u00b5k(st, j) 7: observe rt and st+1 8: advance t = t+ 1 9: end for\n10: end for\nGenerally, step 4 of this algorithm with not be computationally tractable even when solving for \u00b5M is possible for a given M ."}], "references": [{"title": "Optimal adaptive policies for Markov decision processes", "author": ["Apostolos Burnetas", "Michael Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1985}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "arXiv preprint cs/9605103,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1984}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Nick Littlestone"], "venue": "Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Lihong Li", "Michael L Littman", "Thomas J Walsh", "Alexander L Strehl"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Pac modelfree reinforcement learning", "author": ["Alexander Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael Littman"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "More) Efficient Reinforcement Learning via Posterior Sampling", "author": ["Ian Osband", "Daniel Russo", "Benjamin Van Roy"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Using confidence bounds for exploitation-exploration trade-offs", "author": ["Peter Auer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Learning to optimize via posterior sampling", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "CoRR, abs/1301.2609,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Near-optimal regret bounds for reinforcement learning in factored MDPs", "author": ["Ian Osband", "Benjamin Van Roy"], "venue": "arXiv preprint arXiv:1403.3741,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Improved algorithms for linear stochastic bandits", "author": ["Yassin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Efficient reinforcement learning for high dimensional linear quadratic systems", "author": ["Morteza Ibrahimi", "Adel Javanmard", "Benjamin Van Roy"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ronald Ortner", "Daniil Ryabko"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Eluder dimension and the sample complexity of optimistic exploration", "author": ["Daniel Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William Thompson"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1933}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcom Strens"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2000}, {"title": "Dynamic programming and optimal control, volume 1", "author": ["Dimitri Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction We consider the reinforcement learning (RL) problem of optimizing rewards in an unknown Markov decision process (MDP) [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Through exploring poorlyunderstood policies, an agent may improve its understanding of its environment but it may improve its short term rewards by exploiting its existing knowledge [2, 3].", "startOffset": 182, "endOffset": 188}, {"referenceID": 2, "context": "Through exploring poorlyunderstood policies, an agent may improve its understanding of its environment but it may improve its short term rewards by exploiting its existing knowledge [2, 3].", "startOffset": 182, "endOffset": 188}, {"referenceID": 3, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "Some of the most common include PAC (Probably Approximately Correct) [4], MB (Mistake Bound) [5], KWIK (Knows What It Knows) [6] and regret [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Algorithms of both type have been developed to provide PAC-MDP bounds polynomial in the number of states S and actions A [8, 9, 10].", "startOffset": 121, "endOffset": 131}, {"referenceID": 8, "context": "Algorithms of both type have been developed to provide PAC-MDP bounds polynomial in the number of states S and actions A [8, 9, 10].", "startOffset": 121, "endOffset": 131}, {"referenceID": 9, "context": "Algorithms of both type have been developed to provide PAC-MDP bounds polynomial in the number of states S and actions A [8, 9, 10].", "startOffset": 121, "endOffset": 131}, {"referenceID": 6, "context": "The only near-optimal regret bounds to time T of \u00d5(S \u221a AT ) have only been attained by modelbased algorithms [7, 11].", "startOffset": 109, "endOffset": 116}, {"referenceID": 10, "context": "The only near-optimal regret bounds to time T of \u00d5(S \u221a AT ) have only been attained by modelbased algorithms [7, 11].", "startOffset": 109, "endOffset": 116}, {"referenceID": 6, "context": "Worse still, there is a lower bound \u03a9( \u221a SAT ) for the expected regret in an arbitrary MDP [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 11, "context": "parameterization is the degenerate MDP with no transitions, the mutli-armed bandit [12, 13, 14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 12, "context": "parameterization is the degenerate MDP with no transitions, the mutli-armed bandit [12, 13, 14].", "startOffset": 83, "endOffset": 95}, {"referenceID": 14, "context": "Papers here establigh regret bounds \u00d5( \u221a T ) for linear quadratic control [16], but with constants that grow exponentially with dimension.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Later works remove this exponential dependence, but only under significant sparsity assumptions [17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "The most general previous analysis considers rewards and transitions that are \u03b1-H\u00f6lder in a d-dimensional space to establish regret bounds \u00d5(T ) [18].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "In this paper we analyse the simple and intuitive algorithm posterior sampling for reinforcement learning (PSRL) [20, 21, 11].", "startOffset": 113, "endOffset": 125}, {"referenceID": 19, "context": "In this paper we analyse the simple and intuitive algorithm posterior sampling for reinforcement learning (PSRL) [20, 21, 11].", "startOffset": 113, "endOffset": 125}, {"referenceID": 10, "context": "In this paper we analyse the simple and intuitive algorithm posterior sampling for reinforcement learning (PSRL) [20, 21, 11].", "startOffset": 113, "endOffset": 125}, {"referenceID": 19, "context": "PSRL was initially introduced as a heuristic method [21], but has since been shown to satisfy state of the art regret bounds in finite MDPs [11] and also exploit the structure of factored MDPs [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "PSRL was initially introduced as a heuristic method [21], but has since been shown to satisfy state of the art regret bounds in finite MDPs [11] and also exploit the structure of factored MDPs [15].", "startOffset": 140, "endOffset": 144}, {"referenceID": 13, "context": "PSRL was initially introduced as a heuristic method [21], but has since been shown to satisfy state of the art regret bounds in finite MDPs [11] and also exploit the structure of factored MDPs [15].", "startOffset": 193, "endOffset": 197}, {"referenceID": 17, "context": "To characterize the complexity of this learning problem we extend the definition of the eluder dimension, previously introduced for bandits [19], to capture the complexity of the reinforcement learning problem.", "startOffset": 140, "endOffset": 144}, {"referenceID": 0, "context": "However, we can represent the discrete state as a probability vector st \u2208 S = [0, 1] \u2282 R with a single active component equal to 1 and 0 otherwise.", "startOffset": 78, "endOffset": 84}, {"referenceID": 18, "context": "3 Main results We now review the algorithm PSRL, an adaptation of Thompson sampling [20] to reinforcement learning.", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "PSRL was first proposed by Strens [21] and later was shown to satisfy efficient regret bounds in finite MDPs [11].", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "PSRL was first proposed by Strens [21] and later was shown to satisfy efficient regret bounds in finite MDPs [11].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "(6) Here \u03bb1 is the largest eigenvalue of the matrix Q given as the solution of the Ricatti equations for the unconstrained optimal value function V (s) = \u2212sTQs [22].", "startOffset": 160, "endOffset": 164}, {"referenceID": 12, "context": "Algorithms based upon posterior sampling are intimately linked to those based upon optimism [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Further, we believe that PSRL will generally be more statistically efficient than an optimistic variant with similar regret bounds since the algorithm is not affected by loose analysis [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 17, "context": "4 Eluder dimension To quantify the complexity of learning in a potentially infinite MDP, we extend the existing notion of eluder dimension for real-valued functions [19] to vector-valued functions.", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "In fact, a family learnable in constant time for supervised learning may require arbitrarily long to learn to control well [19].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "We now provide bounds on the eluder dimension for some common function classes in a similar approach to earlier work for real-valued functions [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 6, "context": "5 Confidence sets We now follow the standard argument that relates the regret of an optimistic or posterior sampling algorithm to the construction of confidence sets [7, 11].", "startOffset": 166, "endOffset": 173}, {"referenceID": 10, "context": "5 Confidence sets We now follow the standard argument that relates the regret of an optimistic or posterior sampling algorithm to the construction of confidence sets [7, 11].", "startOffset": 166, "endOffset": 173}, {"referenceID": 12, "context": "The argument is essentially the same as Proposition 6 in [14], but extends statements about R to vector-valued functions.", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "This result follows from proposition 8 in [14] but with a small adjustment to account for episodes.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "Once again we follow the analysis of Russo [14] and strealine notation by letting wt = wFtk (xtk+i) abd d = dimE(F , T ).", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "6 Analysis We will now show reproduce the decomposition of expected regret in terms of the Bellman error [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": "Through repeated application of the dynamic programming operator and taking expectation of martingale differences we can mirror earlier analysis [11] to equate expected regret with the cumulative Bellman error: E[\u2206k] = \u03c4", "startOffset": 145, "endOffset": 149}], "year": 2014, "abstractText": "We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as \u00d5( \u221a dKdET ) where T is time elapsed, dK is the Kolmogorov dimension and dE is the eluder dimension. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm posterior sampling for reinforcement learning (PSRL) that satisfies these bounds.", "creator": "LaTeX with hyperref package"}}}