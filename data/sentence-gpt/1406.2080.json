{"id": "1406.2080", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2014", "title": "Training Convolutional Networks with Noisy Labels", "abstract": "We propose several simple approaches to training deep neural networks on data with noisy labels. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark, showing how additional noisy data can improve state-of-the-art recognition models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 9 Jun 2014 05:45:12 GMT  (6370kb,D)", "http://arxiv.org/abs/1406.2080v1", null], ["v2", "Sat, 20 Dec 2014 21:10:03 GMT  (6391kb,D)", "http://arxiv.org/abs/1406.2080v2", null], ["v3", "Tue, 3 Mar 2015 21:13:47 GMT  (859kb,D)", "http://arxiv.org/abs/1406.2080v3", null], ["v4", "Fri, 10 Apr 2015 16:44:00 GMT  (859kb,D)", "http://arxiv.org/abs/1406.2080v4", "Accepted as a workshop contribution at ICLR 2015"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["sainbayar sukhbaatar", "joan bruna", "manohar paluri", "lubomir bourdev", "rob fergus"], "accepted": true, "id": "1406.2080"}, "pdf": {"name": "1406.2080.pdf", "metadata": {"source": "CRF", "title": "Learning from Noisy Labels with Deep Neural Networks", "authors": ["Sainbayar Sukhbaatar"], "emails": ["sainbayar@cs.nyu.edu", "fergus@cs.nyu.edu"], "sections": [{"heading": null, "text": "1 Introduction In recent years, deep learning methods have shown impressive results on image classification tasks. However, this achievement is only possible because of large amount of labeled images. Labeling images by hand is a laborious task and takes a lot of time and money. An alternative approach is to generate labels automatically. This includes user tags from social web sites and keywords from image search engines. Considering the abundance of such noisy labels, it is important to find a way to utilize them in deep learning. Unfortunately, those labels are very noisy and unlikely to help training deep networks without additional tricks.\nOur goal is to study the effect label noise on deep networks, and explore simple ways of improvement. We focus on the robustness of deep networks instead of data cleaning methods, which are well studied and can be used together with robust models directly. Although many noise robust classifiers are proposed so far, there are not many works on training deep networks on noisy labeled data, especially on large scale datasets.\nOur contribution in this paper is a novel way of modifying deep learning models so they can be effectively trained on data with high level of label noise. The modification is simply done by adding a linear layer on top of the softmax layer, which makes it easy to implement. This additional layer changes the output from the network to give better match to the noisy labels. Also, it is possible to learn the noise distribution directly from the noisy data. Using real-world image classification tasks, we demonstrate that the model actually works very well in practice. We even show that random images without labels (complete noise) can improve the classification performance.\n2 Related Work In any classification model, degradation of performance is inevitable when there is noise in training labels [13, 15]. A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected [1, 3]. However, a weakness of this approach is the difficulty of distinguishing informative hard samples from harmful mislabeled ones [6]. Instead, in this paper, we focus on models robust to presence of label noise.\nar X\niv :1\n40 6.\n20 80\nv1 [\ncs .C\nV ]\n9 J\nun 2\nThe effect of label noise is well studied in common classifiers (e.g., SVMs, kNN, logistic regression), and their label noise robust variants have been proposed. See [5] for comprehensive review. A more recent work [2] proposed a generic unbiased estimator for binary classification with noisy labels. They employ a surrogate cost function that can be expressed by a weighted sum of the original cost functions, and gave theoretical bounds on the performance. In this paper, we will also consider this idea and extend it multiclass.\nA cost function similar to ours is proposed in [2] to make logistic regression robust to label noise. They also proposed a learning algorithm for noise parameters. However, we consider deep networks, a more powerful and complex classifier than logistic regression, and propose a different learning algorithm for noise parameters that is more suited for back-propagation training.\nConsidering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels. In [11, 9], noise modeling is incorporated to neural network in the same way as our proposed model. However, only binary classification is considered in [11], and [9] assumed symmetric label noise (noise is independent of the true label). Therefore, there is only a single noise parameter, which can be tuned by cross-validation. In this paper, we consider multiclass classification and assume more realistic asymmetric label noise, which makes it impossible to use cross-validation to adjust noise parameters (there can be a million parameters).\n3 Approach In this paper, we consider two approaches to make an existing classification model, which we call the base model, robust against noisy labels: bottom-up and top-down noise models. In the bottomup model, we add an additional layer to the model that changes the label probabilities output by the base model so it would better match to noisy labels. Top-down model, on other hand, changes given noisy labels before feeding them to the base model. Both models require a noise model for training, so we will give an easy way to estimate noise levels using clean data. Also, it is possible to learn noise distribution from noisy data in the bottom-up model. Although only deep neural networks are used in our experiments, the both approaches can be applied to any classification model with a cross entropy cost.\n3.1 Bottom-up Noise Model We assume that label noise is random conditioned on the true class, but independent of the input x (see [10] for more detail about this type of noise). Based on this assumption, we add an additional layer to a deep network (see Figure 1) that changes its output so it would better match to the noisy labels. The weights of this layer corresponds to the probabilities of a certain class being mislabeled to another class. Because those probabilities are often unknown, we will show how estimate them from additional clean data, or from the noisy data itself.\nLet D be the true data distribution generating correctly labeled samples (x, y\u2217), where x is an input vector and y\u2217 is the corresponding label. However, we only observe noisy labeled samples (x, y\u0303) that generated from a some noisy distribution D\u0303. We assume that the label noise is random conditioned on the true labels. Then, the noise distribution can be parameterized by a matrix Q = {qji}: qji := p(y\u0303 = j|y\u2217 = i). Q is a probability matrix because its elements are positive and each column sums to one. The probability of input x being labeled as j in D\u0303 is given by\np(y\u0303 = j|x, \u03b8) = \u2211\ni\np(y\u0303 = j|y\u2217 = i)p(y\u2217 = i|x) = \u2211\ni\nqjip(y \u2217 = i|x, \u03b8). (1)\nwhere p(y\u2217 = i|x, \u03b8) is the probabilistic output of the base model with parameters \u03b8. If the true noise distribution is known, we can modify this for noisy labeled data. During training, Q will act as an adapter that transforms the model\u2019s output to better match the noisy labels.\nDeep \u00a0network \u00a0\nLearnin from noisy labels in deep neural networks\nSainbayar Sukhbaatar Dept. of Computer Science, NYU,\n715 Broadway, New Y rk, NY 10003\nsainbar@cs.nyu.edu\nRob Fergus Courant Institute, NYU,\n715 Broadway, New York, NY 10003\nfergus@cs.nyu.edu\nAbstract\nThe abstract goes here.\n1 Introduction\nIntroduction ...\n2 Noise model\nHere, we consider a scenario where D = {(x1, y1), .., (xN , yN )} is true labeled data, but we only observed noisy labeled data D\u0303 = {(x1, y\u03031), .., (xN , y\u0303N )}, in which labels can be incorrect. The noise in labels could be a result of inaccurate measurement or human mistakes.\nWe will assume that mistakes in lab ls are consistent and i dependent of inputs given their true labels. Therefore, noisy labels can be considered as random samples from a multinomial distribution which depends on only true labels.\ny\u0303 \u21e0 Mult(q1y, ..., qKy), where qjy are outcome robabilities and K is the number f categories. Then, the probability of input x being labeled as j is\np(y\u0303 = j|x) = X\ni\np(y\u0303 = j|y = i)p(y = i|x) = X\ni\nqjip(y = i|x).\nThe matrix containing noise parameters Q = {qji} is a probability matrix because its elements are\nnon-negative and each column sums to 1. In the following sections, we describe two approaches for\ntraining learning model based on this noise model.\n2.1 Bottom-up approach\nIn this approach, we extend an existing model so that its output would better match to noisy data D\u0303. Let \u2713 be the parameters of a probabilistic classification model. Then, the extended model would have parameters \u27130 = {Q, \u2713}, and its output will be\np(y\u0303 = j|x, \u27130) = X\ni\nqjip(y = i|x, \u2713).\nWe can think Q as an adaptor that changes the output from the model to label distributions of the noisy data. Although the extended model is trained on noisy labeled data, the original model should be used for testing.\n1\nLearning from noisy labels in deep neural networks\nSainbayar Sukhbaatar Dept. of Computer Science, NYU,\n715 Broadway, New York, NY 10003\nsainbar@cs.nyu.edu\nRob Fergus Courant Institute, NYU,\n715 Broadway, New York, NY 10003\nfergus@cs.nyu.edu\nAbstract\nThe abstract goes here.\n1 Introduction\nIntroduction ...\n2 Noise model\nHere, we consider a scenario where D = {(x1, y1), .., (xN , yN )} is true labeled data, but we only observed noisy labeled data D\u0303 = {(x1, y\u03031), .., (xN , y\u0303N )}, in which labels can be incorrect. The noise in labels could be a result of inaccurate measurement or human mistakes.\nWe will assume that mistakes in labels are consistent and independent of inputs given their true labels. Therefore, noisy labels can be considered as random samples from a multinomial distribution which depends on only true labels.\ny\u0303 \u21e0 Mult(q1y, ..., qKy), where qjy are outcome probabilities and K is the number of categories. Then, the probability of input x being labeled as j is\np(y\u0303 = j|x) X\ni\np(y\u0303 = j|y = i)p(y = i|x) = X\ni\nqjip(y = i|x).\nThe matrix containing noise parameters Q = {qji} is a probability matrix because its elements are non-negative and each column sums to 1. In the following sections, we describe two approaches for tra ing learni g model based on his noise model.\n2.1 Bottom-up approach\nIn this approach, we extend an existing model so that its output would better match to noisy data D\u0303. Let \u2713 be the parameters of a probabilistic classification model. Then, the extended model would have parameters \u27130 = {Q, \u2713}, and its output will be\np(y\u0303 = j|x, \u27130) = X\ni\nqjip(y = i|x, \u2713).\nWe can think Q as an adaptor that changes the output from the model to label distributions of the noisy data. Although the extended model is trained on noisy labeled data, the original model should be used for testing.\n1\nSo,max \u00a0  \u00a0 \u00a0Linear\nIf we use vector form c(x) = [c1(x), ..., cK(x)]T and c0(x) = [c01(x), ..., c 0 K(x)] T\nQc0(x) = c(x).\nWhen Q is invertible, we can write surrogate cost functions as\nc0(x) = Q 1c(x)\nAs with bottom-up approach, when Q is unknown we will replace Q 1 with R = {rji}.\nc0(x) = Rc(x).\nThe final cost function becomes\nC(R, \u2713) = NX\nn=1\nKX\ni=1\nry\u0303ni log p(y = i|xn, \u2713).\nThis is the same as replacing noisy label y\u0303 = i with surrogate vector label ri = [ri1, ..., riK ]T . Unlike bot om-up approach, we cannot learn parame ers R by minimizing this cost f nction.\n2.4 Learning from multiple sources\nIn some cases, training data might consist from several sources with different noise levels. For example, we might have a clear dataset with 100% correct labels and a noisy dataset with many incorrect labels. It is sensible to put more importance on datasets with less noise. This can be done by putting different weights on different sources in the cost function:\n1\n|C| +Pk |Nk|\nX n2C l(y(n), f(x(n))) + X k k X n2Nk l(y(n), f(x(n)))\n!\n3 Experiments\n3.1 SVHN deliberately noise added\nStreet-View House Number (SVHN) is a dataset containing images of digits captured from house numbers. It has about 600 thousand training images and 26 thousand test images. However, we only used 100 thousand training images in our experiments for the sake of speed-up.\nAlthough all training images in SVHN are correctly labeled, we synthesized a noisy version of it by deliberately changing labels. Original label i is changed to j with fixed probability qji. An example of noise matrix Q = {qji} is shown in Figure 4. Test images are used with their original labels. The goal of this experiments is to show that bottom-up noise model can actually learn noise matrix Q from noisy labeled data. To understand the effect noise level on learning, we experimented with various amounts of true and false labels.\n3.2 CIFAR-10 deliberately noise added\nCIFAR10 is a dataset of small natural images labeled into ten categories. It has 50 thousand images for training and 10 thousand images for testing. In this experiment, we deliberately changed some labels of training images to simulate noisy labeled training data. We used the same noise matrix Q as the SVHN experiments.\nFigure 5 shows experimental results.\n3.3 CIFAR-10 + Noisy TinyImage\nCIFAR10 is originally created by cleaning up a subset of TinyImage dataset, which has about 8 million small images with very noisy labels (it is estimated that about 80% of the labels are incorrect). In the process hand picking CIFAR10 images, some images are excluded because they were not showing object clearly or labeled falsely. In this set of experiments, we used 150 thousand images\nNLL \u00a0cost \u00a0\nBase model Noise layer\nBack-propagation\nNoisy label\nFigure 1: In the bottom-up noise model, we add a noise layer betwee th softmax and cost layers. The n ise layer is a special linear layer with weights equal to the noise distributi n. It changes output probabilities from th base model i to a di tribution that better matches to noisy labels.\n2\nIf the base model is a neural network with a softmax output layer, this modification can be simply done by adding a noise layer on top of the softmax layer, as shown in Figure 1. The role of the noise layer is to perform operations in Eqn. 1. Therefore, the noise layer is a normal linear layer except there is no bias and its weights are constrained between 0 and 1 because they represent conditional probabilities qji. Also, weights coming from a single node should sum to one because \u2211 j qji = 1.\nWhen training labels are clean, we would set the weight matrix of the noise layer to identity, which is equivalent to not having the noise layer. For noisy labels, we would use noise distribution Q instead of the identity matrix. Because the Q matrix is linear, we can pass gradients through it during training and perform back-propagation to train the rest of the network. As before, the learning objective is to maximize the log likelihood over N samples, but now the objective incorporates the noise matrix Q:\nL(\u03b8) = 1 N\nN\u2211\nn=1\nlog p(y\u0303 = y\u0303n|xn, \u03b8) = 1\nN\nN\u2211\nn=1\nlog\n(\u2211\ni\nqy\u0303nip(y \u2217 = i|xn, \u03b8)\n) (2)\n3.2 Estimating Noise Distribution Using Clean Data In the bottom-up noise model, we need to know the noise distributionQ in order to train the network. Unfortunately,Q is often unknown for real-world tasks. Because the number of free parameters inQ is the square of the number of classes, it is impossible to find Q using cross-validation methods for large-scale datasets. However, we can get unbiased estimation of Q if some clean data is available in addition to the noisy data, which is often the case. The idea is to use the clean data to get the confusion matrix of a some pre-trained model. We also can measure the confusion matrix on the noisy data. Then, the difference between those two confusion matrices should be the noise distribution Q. This is because the model\u2019s mistakes on the noisy data is a combination of two types of mistakes: (1) mistakes of the model and (2) mistakes in noisy labels. The statistics of the first type mistakes can be measured by the clean data, and the second type mistakes correspond to the noise distribution.\nLet us formulate this more precisely. We have clear data D\u2217 and noisy data D\u0303. The goal is to estimate the noise distribution of D\u0303. We also need a some pre-trained model M , which could have been trained on either noisy or clear data (but it should be separate from D and D\u0303). Let C\u2217 and C\u0303 be the confusion matrices of M on the two data types\nD\u2217 M\u2212\u2192 C\u2217 : c\u2217ij = p(y = i|y\u2217 = j,M) and D\u0303 M\u2212\u2192 C\u0303 : c\u0303ij = p(y = i|y\u0303 = j,M).\nThen, the relation between those two is: \u2211\ni c \u2217 kirij = c\u0303kj for \u2200j, k, where rij denotes p(y\u2217 = i|y\u0303 =\nj). If we use a matrix form, we can write:\nC\u2217R = C\u0303 =\u21d2 R = C\u2217\u22121C\u0303, (3)\nwhen C\u2217 is an invertible matrix. If computed R has negative values, it should be projected back to the subspace of probability matrices. Finally, we can compute Q from R using Bayes\u2019 rule\np(y\u0303 = j|y\u2217 = i) = p(y \u2217 = i|y\u0303 = j)p(y\u0303 = j)\np(y\u2217 = i) =\u21d2 qji =\nrijp(y\u0303 = j)\np(y\u2217 = i) . (4)\nWe can measure p(y\u0303 = j) from the data, and p(y\u2217 = i) can be computed from the fact \u2211\nj qji = 1.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u2217 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n(0.5\u2016C\u2217R\u2212 C\u0303\u20162 + \u03bb|R|) (5)\nunder constraint that R should be a probability matrix. Here \u03bb is a hyper-parameter controlling the L1 sparsity of R, which can determined by cross-validation. We can effectively solve this optimization with simple gradient descend method. Such sparse prior over R and Q is useful because in real-world data classes are likely only to be mislabeled with a small set of other classes.\n3.3 Learning Noise Distribution From Noisy Data In practice, the noise distribution Q is often unknown and we might not have clear data from which to estimate it. If we only have noisy training data, then we have to learn Q\u0302, an approximation of Q, from the noisy data itself. Since the elements of Q\u0302 correspond the weights of the noise layer in our model, it can be learned in the same way as other weights using back-propagation. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this is that there is no guarantee Q\u0302would converge trueQ. The combined model can estimate the noise Q via the product of Q\u0302 and C, the base model confusion matrix: Q\u0302C = Q. Thus, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 will be an identity matrix. To prevent this, we add a regularizer tr(Q\u0302) to the objective, which in turn makes the base model less noisy, so encouraging Q\u0302 to converge to Q, see Theorem 1.\nTheorem 1. In the following optimization problem, the only global minimum is Q\u0302 = Q and C = I . (where Q, Q\u0302 and C are probability matrices).\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for \u2200i, j 6= i.\nProof. Let Q\u2217 be the global solution. Then\ntr(Q) = tr(Q\u2217C) = \u2211\ni\n( \u2211\nj\nq\u2217ijcji) \u2264 \u2211\ni\n( \u2211\nj\nq\u2217iicji) = \u2211\ni\nq\u2217ii( \u2211\nj\ncji) = \u2211\ni\nq\u2217ii = tr(Q \u2217)\nThe equality will only hold true only when C = I . Therefore, Q\u2217 = Q.\nBefore the model is converged, Q\u0302C may only approximately equal Q, but empirically we show that the tr(Q\u0302) regularization works well. In practice, we use weight decay on Q\u0302 since it is already implemented in most deep learning packages and has the same effect.\n3.4 Training a Bottom-up Model\nThen, the relation between those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learning noise distribution from noisy data\nIn practice, the noise distribution Q is often unknown to us and we might not have clear data to estimate it. If we only have noisy training data, then we have to learn Q\u0302 approximate of Q from the noisy data itself. Since the elements of Q\u0302 correspond the weights of the noise layer in our model, it can be learned in the same way as other weights using back-propagation. In other words, we w ll maximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient descent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this learning is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less noisy. The confusion matrix of the modified model can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss function is defined as cross-entropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can only measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning always reaches the optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider the following optimization problem for probability matrices Q, Q\u0302 and C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has an inverse relation to the noise level of Q because it represents the correct labels. Then, this theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that we presented this theorem to support our intuition. In practice, we cannot guarantee that minimizing tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will empirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302 rather than the trace cost because it has a similar effect, but it is already implemented in most of deep learning packages.\n3\ntime/epochs\nThen, the relation between those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learning noise distribution from noisy data\nIn practice, the noise distribution Q is often unknown to us and we might not have clear data to estimate it. If we only have noisy training data, then we have to learn Q\u0302 approximate of Q from the noisy data itself. Since the elements of Q\u0302 correspond the weights of the noise layer in our model, it can be learned in the same way as other weights using back-propagation. In other words, we will maximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient descent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this learning is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less noisy. The confusion matrix of the modified model can be writt n as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close t the noise distribution Q of training data because the loss function is defined as cross-entropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can only measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning always reaches the optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider the following optimization problem for probability matrices Q, Q\u0302 and C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has an inverse relation to the noise level of Q because it represents the correct labels. Then, this theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that we presented this theorem to support our intuition. In practice, we cannot guarantee that minimizing tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will empirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302\nrather than the trace cost because it has a similar effect, but it is already implemented in most of\ndeep learning packages.\n3\nidentity\nuniform\ntru\nfixed\nless noi y\nmore noisy\n2.3 Training a bottom-up model\nLet us explain the training procedure for a bottom-up mod l. If we only have noisy data for training:\n1. Initialize a bottom-up model with identity Q\u0302. 2. Train several epochs while fixing Q\u0302 to identity. We cannot allow Q\u0302 to updated until the\nconfusion matrix has large values at its diagonal. 3. Continue trai ing, but now allow Q\u0302 t be updated. Put small weight decay on Q\u0302 if neces-\nsary. 4. When testing the model, th ow away the noise layer or use an identity matrix instead of Q\u0302.\nThe training procedure is little bit complicated when we have both clean and noisy training data:\n1. Split the training data into three sets: clean data D\u21e4, Dm and noisy data D\u0303. If clean data is li ited and noise level is not high, use noisy data for Dm. 2. Train a normal deep netwo k M using data Dm in a usual way. 3. M asure M \u2019s co fusion trices using data D\u21e4 and D\u0303. Then, compute the estimate Q\u0302 of\nthe noise distribution Q using th se confusion matrices. Use sparsity constraint if the clean data is limited and Q is high-dimensional. 4. Add a noise layer to M . Initialize its weight with the estimated Q\u0302. 5. Train M on all of the training data. If the current batch is from noisy data, use the estimated\nQ\u0302 in the noise layer. If it is from the clean data, always use an identity matrix instead of Q\u0302. We can also allow Q\u0302 to be updated during training. Also, the clean data is more important than the noisy data, put smaller weight on the noisy data.\n2.4 Top-down approach\nHere we consider an approach of changing given noisy labels prior to training.\nLet ck(x) be the original cost function for label y = k. In case of maximum likelihood estimation,\nthis cost function is\nck(x) = log p(y = k|x, \u2713), where \u2713 is the parameters of the model. For noisy labeled data D\u0303, we will use surrogate cost function c0k(x) which should suffice the condition of unbiasedness:\nci(x) = X\nj\np(y\u0303 = j|y = i)c0j(x)\n= X\nj\nqjic 0 j(x) for i = 1, ..., K.\nIf we use vector form c(x) = [c1(x), ..., cK(x)]T and c0(x) = [c01(x), ..., c 0 K(x)] T\nQc0(x) = c(x).\nWhen Q is invertible, we can write surrogate cost functions as\nc0(x) = Q 1c(x)\nAs with bottom-up approach, when Q is unknown we will replace Q 1 with R = {rji}. c0(x) = Rc(x).\nThe final cost function becomes\nC(R, \u2713) = NX\nn=1\nKX\ni=1\nry\u0303ni log p(y = i|xn, \u2713).\nThis is the same as replacing noisy label y\u0303 = i with surrogate vector label ri = [ri1, ..., riK ]T . Unlike bottom-up approach, we cannot learn parameters R by minimizing this cost function.\n4\n: confusion of the base model\nStart \u00a0upda)ng \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0with \u00a0small \u00a0 trace \u00a0cost \u00a0or \u00a0weight \u00a0decay \u00a0\n2.3 Training a bottom-up model\nLet us explain th training procedure for a bottom-up model. If we only have noisy data for training:\n1. Initialize a bottom- p model with identity Q\u0302. 2. Train several epochs while fixing Q\u0302 to identity. We cannot allow Q\u0302 to updated until the\nconfusion matrix has large values at its diagonal. 3. Continue training, but now llow Q\u0302 to be updated. Put small weight decay on Q\u0302 if neces-\nsary. 4. When testing the model, throw away the noise layer or use an identity matrix instead of Q\u0302.\nThe training procedure is little bit complicated when we have both clean and noisy training data:\n1. Split the training data into three sets: clean data D\u21e4, Dm and noisy data D\u0303. If clean data is limited and noise lev l is not high, use noisy data for Dm. 2. Train a normal deep network M using data Dm in a usual way. 3. Measure M \u2019s confusion matrices using data D\u21e4 and D\u0303. The , compute the estimate Q\u0302 of\nthe noise distribution Q using those confusion matrices. Use sparsity constraint if the clean data is limited and Q is high-dimension l. 4. Add a nois layer to M . Initialize its w ight with the estimated Q\u0302. 5. Train M on all of the training data. If the current batch is from noisy data, use the estimated\nQ\u0302 in the noise layer. If it is from the clean data, always use an identity matrix instead of Q\u0302. We can also allow Q\u0302 to be updated during training. Also, the clean data is more important than the noisy data, put smaller weight on the noisy data.\n2.4 Top-down approach\nHere we consider an approach of changing given noisy labels prior to training.\nLet ck(x) be the original cost function for label y = k. In case of maximum likelihood estimation, this cost function is ck(x) = log p(y = k|x, \u2713), where \u2713 is the parameters of the model. For noisy labeled data D\u0303, we will use surrogate cost function c0k(x) which should suffice the condition of unbiasedness:\nci(x) = X\nj\np(y\u0303 = j|y = i)c0j(x)\n= X\nj\nqjic 0 j(x) for i = 1, ..., K.\nIf we use vector form c(x) = [c1(x), ..., cK(x)]T and c0(x) = [c01(x), ..., c 0 K(x)] T\nQc0(x) = c(x).\nWhen Q is invertible, we can write surrogate cost functions as\nc0(x) = Q 1c(x)\nAs with bottom-up approach, when Q is unknown we will replace Q 1 with R = {rji}. c0(x) = Rc(x).\nThe final cost function becomes\nC(R, \u2713) = NX\nn=1\nKX\ni=1\nry\u0303ni log p(y = i|xn, \u2713).\nThis is the same as replacing noisy label y\u0303 = i with surrogate vector label ri = [ri1, ..., riK ]T . Unlike bottom-up approach, we cannot learn parameters R by minimizing this cost function.\n4\nThen, the relation between those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learning noise distribution from noisy data\nIn practice, the noise distribution Q is often unknown to us and we might not have clear data to estimate it. If we only have noisy training data, then we have to learn Q\u0302 approximate of Q from the noisy data itself. Since the ele ents of Q\u0302 correspond the weights of the noise layer in our model, it can be learned in the same way as other weights using back-propagation. In other words, we will maximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient d scent. However, weight matrix Q\u0302 have to be p oject d back to the subspace of probability matrices after each update.\nA problem with this learning is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would m ke base model less noisy. The confus on matrix of the modified model can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusio matrix of the base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss fun tion is defined as cross-entropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be i ntity because we can only measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy ould be make Cb mor c ose to identity. The intuit on for this comes from an ideal cas where the learning always reaches he optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider the following optimization problem for probability matrices Q, Q\u0302 and C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has an inverse relation to the noise level of Q because it represents the correct labels. Then, this theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that we presented this theorem to upport our intuition. In practice, we cannot guarantee that minimizing tr(Q\u0302) will m ke C id ntity because Q\u0302C = Q is too an ideal assumption. However, we will empir cally show th t this actually w rks well in practice. In experiments, we used L2 cost on Q\u0302 rather tha the trace cost because it has a similar effect, but it is already implemented in most of deep learning packages.\n3\nThe \u00a0base \u00a0model \u00a0 learns \u00a0noise \u00a0in \u00a0data \u00a0\n2.3 Training a bottom-up model\nLet us explain the training procedure for a bottom-up model. If we only have noisy d ta for training:\n1. Initialize a b ttom-up model with identity Q\u0302. 2. Train several epochs while fixing Q\u0302 to identity. We cannot allow Q\u0302 to upda ed until the\nconfusion matrix has large values at its dia onal. 3. Continue training, but now allow Q\u0302 to be upd ted. Put small weight decay on Q\u0302 if neces-\nsary. 4. When testing the mod l, hrow away the noise layer or us an dentity matrix instead of Q\u0302.\nThe training procedure is little bit complicated when we have both cl an a d noisy training dat :\n1. Split the training data into three sets: clea data D\u21e4, Dm and noisy data D\u0303. If clean dat is limited and noise level is not high, us no y data for Dm. 2. Tr in a n rmal deep netw rk M using data Dm i a usual w y. 3. M asure M \u2019s confusion matrices using data D\u21e4 and D\u0303. The , compute the estimate Q\u0302 of\nth noise distribution Q u ing those c fusion matric s. Use sparsity con traint if the clean data is limited nd Q s high-dime sio al. 4. Add a noise layer to M . Initialize its weight w th the estimat Q\u0302. 5. Train M on all of the training d ta. If the current batch is from noisy data, use the estimated\nQ\u0302 in the noise layer. If it is from the clean data, always use an identity matrix instead of Q\u0302. We can also allow Q\u0302 to be upd ted during training. Also, the clean data i more import nt than the noisy data, put smaller weight on the noisy data.\n2.4 Top-down approach\nHere we consider an approach of changing given noisy labels prior to training.\nLet ck(x) be the original cost function for abel y = k. In case of maximum likelihood estimation,\nthis cost function is\nck(x) = log p(y = k|x, \u2713), where \u2713 is the parameters of the model. Fo noisy labeled data D\u0303, we will use surrogate cost function c0k(x) which should suffice the condition of unbiasedness:\nci(x) = X\nj\np(y\u0303 = j|y = i)c0j(x)\n= X\nj\nqjic 0 j(x) for i = 1, ..., K.\nIf we use vector form c(x) = [ 1(x), ..., K(x)]T and c0(x) = [c01(x), ..., c 0 K(x)] T\nQc0(x) = c(x).\nWhen Q is invertible, we can write surrogate cost functions s\nc0(x) = Q 1c(x)\nAs with bottom-up approach, when Q is unknown we will replace Q 1 with R = {rji}. c0(x) = Rc(x).\nThe final cost function becomes\nC(R, \u2713) = NX\nn=1\nKX\ni=1\nry\u0303ni log p(y = i|xn, \u2713).\nThis is the same as replacing noisy lab l y\u0303 = i w th surrogate vector label ri = [ri1, ..., riK ]T . Unlike bottom-up approach, we cannot learn parameters R by minimizing this cost function.\n4\nThen, the relation between those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckj ji = c\u0303ki f r 8i, k,\nwhere rij deno es p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using he inverse of C\u21e4 is not g od dea because th inverse operation is unstable presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a pr bability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is us ful because in rea -w rld data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learning noise distribution fr m oisy data\nIn practice, the noise distribution Q is often unknown to us and we might not have clear data to estimate it. If we only ave noisy tra ning data, then we have to learn Q\u0302 approximate of Q from the noisy data itself. Since the elements of Q\u0302 correspond the weights of the noise layer in our model, it can be learned in the same way as other weights using back-propagation. In other words, we will maximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient descent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this learning is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by f rcing Q\u0302 to be more n isy, which in turn would make the base model less noisy. The confusion atrix of the modified odel can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusi n matrix of th base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss function is defin d as cross-e tropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly f rce Cb to be id ntity becau e e can only measur it with cle n ata. H wever, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb ore close to id ntity. Th intuition for this comes from an ideal case where the learning always reaches the optimal state w re the following holds true\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider th f ll wing optimiz tion proble f probability tric s Q, Q\u0302 and C.\nmi imize Q\u0302,C r(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > q j for 8i, j 6= .\nThen, Q\u0302 = Q and C = I is the only glob l minimum.\nTrace h s an inverse relation to the noise level of Q because it represents the correct labels. Then,\nthis theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that\nwe presented this theorem to support our intuition. In practice, we cannot guarantee that minimiz-\ning tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will\nempirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302\nrather than the trace cost because it has a similar ffect, but it is already implemented in most of\ndeep learning packages.\n3\nThen, the relation between those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we can compute Q from R using B yes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in pr sence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a prob bility matrix. Here is a hyper-param ter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in r al-world data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learning noise distribution from noisy data\nIn practice, the noise distribution Q is often unknown to us and we might not have clear da a to estimate it. If we only have noisy training data, then we have to learn Q\u0302 approximate of Q from the noisy data itself. Since the elements of Q\u0302 correspond the weights of the n ise layer in our mod l, it can be learned in the same way as other weights using back-propagation. In other words, we will maximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient descent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this learning i that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful nough, it can le rn the noise distribution and Q\u0302 can b an identity matrix. H wever, it is possible to prevent this problem by fo cing Q\u0302 o be ore noisy, which n turn woul make the base model less noisy. The confusion matrix f the modifi d model can be writte as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss function is defined as cross-entropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can only measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning always reaches the optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to less noisy C, because the pr duct f Q\u0302 and C is constant. Theorom 1. Let us consider the following optimization problem for probability matrices Q, Q\u0302 and C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has an inverse relation to the noise level of Q because it represents the correct labels. Then,\nthis theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that\nwe presented this theorem to support our intuition In practice, we cannot guarantee that minimiz-\ning tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will\nempirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302\nrather than the trace cost because it has a similar effect, but it is already implemented in most of\ndeep learning packages.\n3\n: co fusion of th whole odel\nnoisy \u00a0 da a \u00a0\nmodel \u00a0\nprobability of images being incorrectly labeled as \u201cjaguar\u201d if its true class is \u201csport car\u201d because of the car manufacturer named \u201cJaguar\u201d. Based on this assumption, we add an additional layer to an deep network that changes its output so it would better match to the noisy labels. The weights of this layer corresponds to the probabilities of a certain class being mislabeled to another class. Because those probabilities are often unknown to us, we will show how estimate them using additional clean data. Also, we show how to learn them from the noisy data itself. Let D be the true data distribution generating correctly labeled samples (x, y\u21e4), where x is an input vector and y\u21e4 is the corresponding label. However, we only observe noisy labeled samples (x, y\u0303) that generated from a some noisy distribution D\u0303. We assume that the label noise is random conditioned on the true labels. Then, the noise distribution can be parameterized by a matrix Q = {qji} such that qji := p(y\u0303 = j|y\u21e4 = i). Here, matrix Q is a probability matrix because its elements are positive and each column sums to one. The probability of input x being labeled as j in D\u0303 is given by\np(y\u0303 = j|x) = X\ni\np(y\u0303 = j|y\u21e4 = i)p(y\u21e4 = i|x) = X\ni\nqjip(y \u21e4 = i|x).\nIf the true noise distr ution is known, we can modify an existing model on noisy labeled data. During training, Q will act as an adapter that transforms model\u2019s output so that would better match to noisy labels. Let \u2713 be the parameters of a some probabilistic classification model. Then, output from the modified model would become\np(y\u0303 = j|x, Q, \u2713) = X\ni\nqjip(y \u21e4 = i|x, \u2713).\nIn case the base model is a neural network with a softmax output layer, this modification can be simply done by adding a fully connected linear layer on top of the softmax layer, as shown in Figure 1. The weights of the noise layer will correspond to the noise distribution Q. Because the extended model is still a neural network, it can be trained by the back-propagation algorithm. The learning objective is to maximize the log likelihood\nL(\u2713) = 1 N\nNX\nn=1\nlog p(y\u0303 = y\u0303n|xn, \u27130) = 1\nN\nNX\nn=1\nlog\nX\ni\nqy\u0303nip(y \u21e4 = i|xn, \u2713)\n! ,\nwhere N is the number of samples in training data.\n2.1 Esti at ng the noise distribution using clean data\nIn the bottom-up noise model, we need to know the noise distribution Q in order to train the network. Unfortunately, Q is often unknown for real-world tasks. Because the number of free parameters in Q is the square of the number of classes, it is impossible to find Q using cross-validation methods for large-scal datasets. However, we can get unbiased estimation of Q if some clean data is available in addit on to the noisy data. The idea is to use the clean data to get the confusion matrix of a some pre-trained model. We also can measure the confusion matrix on the noisy data. Then, the difference between those two confusion matrix should be the noise distribution. This is because the model\u2019s mistakes on the noisy data is a combination of two types of mistakes: (1) mistakes of the model and (2) mistakes in noisy labels. The statistics of type 1 mistakes can be measured by the clean data, and type 2 mistakes correspond to the noise distribution.\nLet us formulate this more precisely. We have clear data D\u21e4 and noisy data D\u0303. The goal is to estimate the noise distribution of D\u0303. We also need a some pre-trained model M , which could have been trained on either noisy or clear data (but it should be separate from D and D\u0303). Let C\u21e4 and C\u0303 be the confusion matrices of M on the two data types\nD\u21e4 M ! C : c\u21e4ij = p(y = i|y\u21e4 = j, M)\nD\u0303 M ! C\u0303 : c\u0303ij = p(y = i|y\u0303 = j, M).\n2\nclean data 1. train\nThen, the relati n between t ose two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nun er con raint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learning noise distribution from noisy data\nIn practice, the noise distribution Q is often unknown to us and we might not have clear data to\nestimate it. If we only have noisy training data, then we have to learn Q\u0302 approximate of Q from the\nnoisy data itself. Since the elements of Q\u0302 correspond the weights of the noise layer in our model,\nit can be learned in the same way as other weights using back-propagation. In other words, we will\nmaximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient\ndescent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this learning is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less noisy. The confusion matrix of the modified model can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss function is defined as cross-entropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can only measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning always reaches the optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider the following optimization problem for probability matrices Q, Q\u0302 and C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has an inverse relation to the noise level of Q because it represents the correct labels. Then, this theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that we presented this theorem to support our intuition. In practice, we cannot guarantee that minimizing tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will empirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302 rather than the trace cost because it has a similar effect, but it is already implemented in most of deep learning packages.\n3\n2. measure confusion\nThen, the relation be ween those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of eleme ts in R, using he inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we c n pu a sparsi y prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 s a sity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class bei g mislabeled to all other classes.\n2.2 Learning noise distribution from noisy data\nIn pract ce, the noise distribution Q is often unkn wn to us and e might not have c ear data to\nesti te it. If w o ly have noisy training data, then we have to le rn Q\u0302 a proximate of Q from the\nnoisy data itself. Since the elements of Q\u0302 correspond the weights of the noise layer in our model,\nit can be l arned in th same way as other weights using back-propagation. In other words, we will maximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient descent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this learning is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less noisy. The confusion matrix of the modified model can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss function is defined as cross-entropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can only measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning always reaches the optimal state where the following holds tru\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider the following optimization problem for probability matrices Q, Q\u0302 and C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has an inverse rel tion to the noise level of Q because it represents the correct labels. Th n, this theorem show that C will become identity if we manage to make Q\u0302 the most noisy. Note that we presented this theo em to upport our intuition. In p actice, we cannot guarantee that minimizing tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will empirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302 rather than the trace cost because it has a similar effect, but it is already implemented in most of deep learning packages.\n3\nThen, the relation etween those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible ma rix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class being misl beled to all other classes.\n2.2 Lear ing noise distributio from noisy data\nI practice, the noise distributio Q is often unknown to us and we might not have clear data to estimate t. If we only have noisy training data, then we have to learn Q\u0302 approximate of Q from the noisy data itself. Since the elements of Q\u0302 correspond the weights of the noise layer in our model, it can be learned in the same way as other weights using back-propagation. In other words, we will\nmaximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient\ndescent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices\nafter each update.\nA problem with this lear ing is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less noisy. The confusion matrix of the modified odel can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss function is defined as cross-entropy between them. Ideally, we wa t Cb to be ident ty and Q\u0302 to b equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we ca only meas re it with clea data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning always reaches the optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will correspond to le s noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider the follo ing optimization problem for probability matrices Q, Q\u0302 d C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has n inve se relation to the noise level of Q because it represents the correct labels. Then, this theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that we presented this theorem to support our intuition. In practice, we cannot guaran ee that minimizng tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will empirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302 rather than the trace cost because it has a similar effect, but it is already implemented in most of deep learning packages.\n3\nthe \u00a0base \u00a0 model \u00a0\nnoise \u00a0 layer \u00a0\nThen, the relati between thos two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij enotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, w can compute Q from R using Bayes\u2019 rule.\nIn case whe the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certai class being mislabeled to all other classes.\n2.2 Learni g noise distribution from noisy data\nIn practice, th noise distrib tio Q is often unk wn to us nd we might not have clear data to\nestima e t. If w only have noisy trai ing ata, then we have to l arn Q\u0302 approximate of Q from the\nnoisy data itself. Since the elements of Q\u0302 c rrespond the weights of the noise layer in our model, it can be learned in he sam way s o her weights using back-prop gation. In other words, we will maximize the log likelihood with respect to both \u2713 and Q\u0302 at the same time using stochastic gradient descent. Howev r, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this arning is that there is no guarantee Q\u0302 would converge true Q. Actually, if the base model is powerful enough, it can learn the noise distribution and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less noisy. The confusion matrix of the modified model can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the noise distribution Q of training data because the loss function is defined as cross-entropy between them. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can only measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning always reaches the optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, m re n sy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is constant. Theorom 1. Let us consider the following opt ization problem f r probability matrices Q, Q\u0302 and C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace ha an inverse relation to the noise l vel of Q because it represents the correct labels. Then, this theorem shows that C will become identity if we manag to make Q\u0302 the most noisy. Note that\ne pr sented this theor to support our intuiti n. In practice, we ca not guarantee that minimizing tr(Q\u0302) will make C dentity because Q\u0302C = Q is too a ideal a sumption. However, we will empirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302 ra her than the trace cost b caus it has a imilar effect, but it is already implemented in most of deep learning packages.\n3\nThen, th relation between those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewri e this in a matrix form C\u21e4 \u02dc =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible matrix. Finally, we ca compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learni g noise distribution from noisy data\nIn practice, the noise distribution Q is ft n unkno n to us and we might not have clear data to\ns imate it. If we only have noisy training data, then we have to learn Q\u0302 approxi ate of Q from the\nnoisy data itself. Since the el ments f Q\u0302 corresp nd the weights of the noi e layer in our model, it can be learned in th same way as other weights using back-propagation. In other words, we will maximize the log likelihood with respect to both \u2713 and Q\u0302 at th same time using s o hastic gradient descent. However, weight matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA pr bl m w t his learning is that there s no guarantee would converge true Q. Actually, if the base model is p werful enough it can learn the noise distribution and Q\u0302 can be an identity matrix. How ver, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less noisy. The confusion matrix of the modified model can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the noise distrib tion Q of traini g data becaus the l ss function is defined as cross-entropy between them. Ide lly, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can nly measure it with clean data. However, we can put any constraint on Q\u0302 because it is a weight matrix in the model. We argue that making Q\u0302 more noisy would be make Cb more close to identity. The intuition for this comes from an ideal case where the learning lways reach s the optimal state where the following holds true\nQ\u0302C = Q.\nIn that case, more n isy Q\u0302 will correspond to less noisy C, because the product of Q\u0302 and C is const nt. Theorom 1. Let us consider the following optimization problem for probability matrices Q, Q\u0302 and\n. minimize\nQ\u0302,C\ntr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global minimum.\nTrace has an inverse relation to the noise level of Q because it represents the correct labels. Then, this theorem shows that C will become identity if we manage to make Q\u0302 the most noisy. Note that we presented this theorem to support our intuition. In practice, we cannot guarantee that minimizing tr(Q\u0302) will make C identity because Q\u0302C = Q is too an ideal assumption. However, we will empirically show that this actually works well in practice. In experiments, we used L2 cost on Q\u0302 rather than the trace cost because it has a similar effect, but it is already implemented in most of deep learning packages.\n3. compute\nThen, the relation between those two isX\nj\np(y = k|y\u21e4 = j, M)p(y\u21e4 = j|y\u0303 = i) = p(y = k|y\u0303 = j, M) =) X\nj\nckjrji = c\u0303ki for 8i, k,\nwhere rij denotes p(y\u21e4 = i|y\u0303 = j). If we rewrite this in a matrix form C\u21e4R = C\u0303 =) R = C\u21e4 1C\u0303, when C\u21e4 is an invertible atrix. Finally, we can compute Q from R using Bayes\u2019 rule.\nIn case when the number of clean samples is small relative to the number of elements in R, using the inverse of C\u21e4 is not good idea because the inverse operation is unstable in presence of noise. Instead we can put a sparsity prior on R and solve the following optimization problem\nmin R\n 1\n2 ||C\u21e4R C\u0303||2 + |R|\nunder constraint that R should be a probability matrix. Here is a hyper-parameter controlling the L1 sparsity of R. Such sparse prior over R and Q is useful because in real-world data it is unlikely that a certain class being mislabeled to all other classes.\n2.2 Learning noise dist ibution from noisy data\nIn practice, the nois distribution Q is oft n unknown to us and we might not have clear data to estim e i . If we on y have n isy training dat , t en we have t learn Q\u0302 approximate of Q from the noisy data itself. Since the elem nts of Q\u0302 correspond the weights of the noise layer n our model, it ca be l arned in the ame w y a other weights using back-pr pagation. In ot er words, we will maximize the l g likelihood with espect to both \u2713 and Q\u0302 at the same time using stochastic gradient de cent. However, weig t matrix Q\u0302 have to be projected back to the subspace of probability matrices after each update.\nA problem with this learning is th t there is no guarantee Q\u0302 would converge true Q. Actually, if the bas model is powerful enough, it can learn the noise distributi n and Q\u0302 can be an identity matrix. However, it is possible to prevent this problem by forcing Q\u0302 to be more noisy, which in turn would make the base model less i y. The confusi n matrix of the mo ified model can be written as\nCm = Q\u0302Cb,\nwhere Cb is the confusion matrix of the base model. Training of model would bring Cm close to the\nnoise distribution Q of training data because the loss function is defined as cross-entropy between\nthem. Ideally, we want Cb to be identity and Q\u0302 to be equal to true Q. Unfortunately, we cannot directly force Cb to be identity because we can only measure it with clean data. However, we can put any cons raint on Q\u0302 because i is a weight matrix in the model. We argue that making Q\u0302 more n isy would be ake Cb more close to identity. The int ition f r this comes from an ideal case where the learning always reaches the optimal state where t e following olds tru\nQ\u0302C = Q.\nIn that case, more noisy Q\u0302 will corr spond t less noisy C, because the product of Q\u0302 C is constant. Th orom 1. Let us consider the following optimizati n problem f r probability m trices Q, Q\u0302 an C.\nminimize Q\u0302,C tr(Q\u0302) subject to Q\u0302C = Q, q\u0302ii > q\u0302ij , qii > qij for 8i, j 6= i.\nThen, Q\u0302 = Q and C = I is the only global inimum.\nTrace has an inverse relation to the noise level of Q because it represents th correct l bels. Then, this theorem shows that C will become identity if we manag to mak Q\u0302 the most oisy. Note that we present d this theorem to support our intuition. I practice, we cannot guarantee that minimizing tr(Q\u0302) ll make C iden ity b caus Q\u0302C = is t o an id al assumption. Howev r, we will empirically that this actually works well n practice. In experiments, we used L2 cost on Q\u0302 rather than the trac cost because it has a si lar effect, but it is already mplemen ed in mos of deep learning pac ages.\n3\n4. train\nnoisy labels clean labels\n( ) (b)\nFigure 2: (a) The training sequence when learning from noisy data alone. The noise matrix Q\u0302 (red) is initi lly set to e identity, whil the b se model (green) is trained, inadverte tly learning the n ise i th data. Then we start updating Q\u0302 also (with r gularization) and this captures the noise properties of the data, leaving the model to ake \u201ccl an\u201d predictions. (b) Training when we have noisy and clea data available. 1: We train a model one on the clean data. 2 & 3: Comparing the confusion matrices on clean/noisy da a, we co pute . 4: We train a ew model with the noise lay r Q fixed.\nNoisy labels nly: We first consider the case where we only have n isy labeled training data. We start by initializing the base model in usual way (there are no special requir m ts). Initially, e fix Q\u0302 = I during training until the validation error stops decreasi . At this int, the base model could have learned the noise in the training data, which is not what we wan . Therefore, w start updating Q\u0302 along with the rest of the network, using weight decay to pus Q\u0302 toward Q. As Q\u0302 gets more noisy, it should absorb the noise from the base model, thus making the base model more accurate. H eve , large weight decay (or trace cost) would make Q\u0302 n isier ha the true Q, which will hurt the performan e. Therefore, we ave to find right amount of we ght decay us g cross valid ti . We c nti u th training until the validation error st ps decreasing to prevent overfitting. This procedure is illustrated in figure 2(a). If we wan to ake predicti or test th model on lear data, the noise layer should be removed (or set to identity I), but for noisy la eled validation data we us the learned Q\u0302.\n4\nNoisy and clean data: If some clean training data is available, we can use it to estimate the noise distribution Q of the noisy data, using the method described in section 3.2. We first train a model (which can be a normal deep network) on a subset of the clean data. Then, we measure the confusion matrices on the both clean (must exclude the subset used for training to avoid bias) and noisy data. From the difference between those two matrices, we compute an estimate of the noise distribution Q using Eqns. 3 & 4. This can then be used to train a new model on both clean and noisy dataset. Figure 2(b) shows the training scheme. A variant of this procedure involves one further stage where the initial estimate of Q is refined using the procedure in Section 3.3.\n3.5 Top-down Noise Model\nDeep \u00a0network \u00a0\nLearning from noisy labels in deep neural networks Sainbayar Sukhbaatar Dept. of Computer Science, NYU, 715 Broadway, New York, NY 10003 sainbar@cs.nyu.edu Rob Fergus Courant Institute, NYU, 715 Broadway, New York, NY 10003 fergus@cs.nyu.edu Abstract The abstract goes here. 1 Introduction\nIntroduction ...\n2 Noise model\nHere, we consider a scenario where D = {(x1, y1), .., (xN , yN )} is true labeled data, but we only observed noisy labeled data D\u0303 = {(x1, y\u03031), .., (xN , y\u0303N )}, in which labels can be incorrect. The noise in labels could be a result of inaccurate measurement or human mistakes.\nWe will assume that mistakes in labels are consistent and independent of inputs given their true labels. Therefore, noisy labels can be considered as random samples from a multinomial distribution which depends on only true labels.\ny\u0303 \u21e0 Mult(q1y, ..., qKy), where qjy are outcome probabilities and K is the number of categories. Then, the probability of input x being labeled as j is\np(y\u0303 = j|x) = X\ni\np(y\u0303 = j|y = i)p(y = i|x) = X\ni\nqjip(y = i|x).\nThe matrix containing noise parameters Q = {qji} is a probability matrix because its elements are\nnon-negative and each column sums to 1. In the following sections, we describe two approaches for training learning model based on this noise model.\n2.1 Bottom-up approach\nIn this approach, we extend an existing model so that its output would better match to noisy data D\u0303. Let \u2713 be the parameters of a probabilistic classification model. Then, the extended model would have parameters \u27130 = {Q, \u2713}, and its output will be\np(y\u0303 = j|x, \u27130) = X\ni\nqjip(y = i|x, \u2713).\nWe can think Q as an adaptor that changes the output from the model to label distributions of the noisy data. Although the extended model is trained on noisy labeled data, the original model should be used for testing.\n1\nLearning from noisy labels in deep neural networks Sainbayar Sukhbaatar Dept. of Computer Science, NYU, 715 Broadway, New York, NY 10003 sainbar@cs.nyu.edu Rob Fergus Courant Institute, NYU, 715 Broadway, New York, NY 10003 fergus@cs.nyu.edu Abstract\nThe abstract goes here.\n1 Introduction\nIntroduction ...\n2 Noise model\nHere, we consider scen rio where D = {(x1, y1), .., (xN , yN )} is true labeled data, but we only observed noisy labeled data D\u0303 = {(x1, y\u03031), .., (xN , y\u0303N )}, in which labels can be incorrect. The noise in labels could be a result of inaccurate measurement r human mistakes.\nWe will assume that mistakes in labels are consistent and independent of inputs given their true labels. Therefore, noisy labels can be considered as random samples from a multinomial distribution which depends on only true labels.\ny\u0303 \u21e0 Mult(q1y, ..., qKy), where qjy are outcome probabilities and K is the number of categories. Then, the probability of input x being labeled as j is\np(y\u0303 = j|x) X\ni\np(y\u0303 = j|y = i)p(y = i|x) = X\ni\nqjip(y = i|x).\nThe matrix containing noise parameters Q = {qji} is a probability matrix because its elements are\nnon-negative and each column sums to 1. In the following sections, we describe two approaches for\ntra ing learni g model based on his noise model.\n2.1 Bottom-up approach\nIn this approach, we extend an existing model so that its output would better match to noisy data D\u0303. Let \u2713 be the parameters of a probabilistic classification model. Then, the extended model would have parameters \u27130 = {Q, \u2713}, and its output will be\np(y\u0303 = j|x, \u27130) = X\ni\nqjip(y = i|x, \u2713).\nWe can think Q as an adaptor that changes the output from the model to label distributions of the noisy data. Although the extended model is trained on noisy labeled data, the original model should be used for testing.\n1\nSo,max \u00a0  \u00a0 \u00a0Linear NLL \u00a0cost \u00a0\nBase model Noise layer\nNoisy label\nand noisy data. From the difference between those two matrices, we compute an estimate of the noise distribution Q. The general training schema is shown in figure 3.\nSecond, we can use the both clean and noisy data for training the bottom-up noisy model. We will use th estimate of Q in the noise layer. However, we should bypass the noise layer for samples from clean data.\n3.5 Top-down Noise Model\nAs opposed the bottom-up model, another way to deal with noisy labels is a top-down model. Instead of modifying the model to match with noisy labels, we can change the n isy lab ls so that it should produce unbiased classification. When a noisy label is i, we would replace it with vector label si. Let S be the conversion matrix consisting from column vectors si. The learning objective is still to maximize the log likelihood, but now noisy labels are converted by matrix S.\nL(\u2713) = 1 N\nNX\nn=1\nKX\ni=1\nsy\u0303ni log p(y = i|xn, \u2713), (8)\nwhere K is the number of classes. The only difference between eq. 8 and 2 is that the sum over classes is inside or outside the log operator. Unfortunately, we cannot optimize this objective with respect to S, because there is a degenerate solution where S converts all labels to one class and the model always predicts that class. Therefore, we cannot learn S directly from the noisy data.\nAn unbiased estimator for binary classification is introduced in [1], where the cost fun tion s replaced by a surrogate cost function that combines the two costs (costs for class +1 and -1) with certain coefficients that depends on the noise level. We can view this as replacing noisy labels with different label vectors. If we generalize this surrogate function to multi classes, we can see that S should be the inverse of the noise distribution Q (or at least QS should be equal to an identity plus some constant). However, the inverse operation is unstable with respect to noise, and also it usually contain negative elements when Q is a probability matrix. Such a negative element can make eq. 8 diverge to infinity. In practice, using the Q inverse for S does not work well and often perform worse than a normal model (e.g. a normal deep network) even if know the true Q.\n4 Experiments\nWe experimented on two standard computer vision tasks where deep learning methods have achieved state-of-art results. Although, the both datasets are correctly labeled, but we simulated noisy data by flipping labels randomly. Then we trained our model on this noisy data, and compared it with a normal neural network on clean test data.\n4.1 SVHN Deliberately Noise Added\n0 2 4 6 8 10\nx 10 4\n8\n10\n12\n14\n16\n18\n20\n22\n24\ntraining data size\nte st\ne rr\no r\n(% )\nnormal training noise prob ground truth noise prob learned\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0\n10\n20\n30\n40\n50\n60\n70\nfraction of false labels in training data\nte st\ne rr\no r\n(% )\nnormal training noise prob ground truth noise prob learned\nFigure 4: Left: Test errors on SVHN dataset when 50% of the labels the training data are false. Right: Test errors on different noise levels\n6\nBack-propagation\nFigure 3: In the top-down noise model, noisy labels are converted by matrix S before being used by the base model for training.\nInstead of mo ifying the model t match with noisy labels, we can change the noisy labels so that it should produce an unbiased classification. Given a noisy label is i, we replace it with vector label si (see Figure 3). Let S be the conversion matrix consisting from column vectors si. The learning objective is still to maximize the log likelihood, but now noisy labels ar converted by matrix S.\nL(\u03b8) = 1 N\nN\u2211\nn=1\nK\u2211\ni=1\nsiy\u0303n log p(y = i|xn, \u03b8), (6)\nwhere K is the number of classes. Note the difference between Eqn. 6 and 2 is that the sum over classes being inside or outside the log operator. U fortunately, we cannot optimize this objective with respect to S, because there is a degenerate solution where S converts all labels to one class and the odel always predicts that class. Therefore, we cannot learn S directly from the noisy data.\nAn u biased estim tor for binary classificati n is introduc d in [12], whe e the cost unction is replaced by a surrogate objective that combines the two costs (costs for class +1 and -1) with coefficients that depends on the noise level. We can view this as replacing noisy labels with different label vectors. If we generalize this surrogate function to multi classes, we can see that S should be the inverse of the noise distributionQ (or at leastQS should be equal to an identity plus some constant). However, the inverse operation is unstable with r spect to noise, and also it usually contain negative elements when Q is a probability matrix, which can make Eqn. 6 diverge to infinity.\nAs there is no practical way to learn S reliably, in our experiments we fix it to \u03b1 \u00b7 I +(1\u2212\u03b1)/K \u00b71, where 1 is a K \u00d7K matrix of ones. The hyperparameter \u03b1 is selected by cross-validation.\n3.6 Reweighting of Noisy Data In some experiments in this paper, we have clean data in addition to noisy data. If we just mix them, we lose the valuable information about which labels are trustwort y. A simple way to express the relative confidence between the two sets of data is to down-weight the noisy examples, relative to the clean, in the loss function, as shown in Eqn. 7. This trick can be combined with both the bottom-up and top-down noise models described above. The objective is to maximize\nL(\u03b8) = 1 Nc +Nn\n( Nc\u2211\nn=1\nlog p(y = yn|xn, \u03b8) + \u03b3 Nn\u2211\nn=1\nlog p(y\u0303 = y\u0303n|x\u0303n, \u03b8) )\n(7)\nwhere Nc and Nn are the number of clean and noisy samples respectively. The hyper-parameter \u03b3 is the weight on noisy labels and is set by cross validation.\n4 Experiments In this section, we empirically examine the robustness of deep networks with and without noise modeling. We experiment on several different image classification datasets with label noise. As the\nbase model, we use convolutional deep networks because they produce state-of-art performance on many image classification tasks.\nThis section consists from two parts. In the first part, we perform controlled experiments by deliberately adding label noise to clean datasets. This is done by randomly changing some labels in the training data according to some known noise distribution. This allows us to check if the learned noise distribution is close the ground-truth noise distribution. First, we experiment on the Google street-view house number dataset (SVHN) [14], which consists of 32x32 images of house number digits captured from Google Streetview. It has about 600k images for training and 26k images for testing. Next, we experiment on CIFAR-10 [7], a more challenging dataset consisting from 60k small images of 10 object categories. The both datasets are hand-labeled, so all labels are clean.\nIn the second part, we show more realistic experiments using two datasets with inherent label noise. The first dataset consist from clean images from CIFAR-10 dataset and noisy images from Tiny Images dataset [18]. The second dataset consists of clean images from ImageNet [4] and noisy images downloaded from web search engines. In the both datasets, we do not know the true noise distribution of noisy labels.\n4.1 Deliberate Label Noise We synthesize noisy data from clean data by deliberately changing some of the labels. Original label i is randomly changed to j with fixed probability qji. An example of an noise distribution Q = {qji} we use is shown in Figure 4(c). By changing the probability on the diagonal, we can generate datasets with different noise levels. The labels of test images are left unperturbed.\nWe use a publicly available fast GPU code1 for training deep networks. As the base model, we use their \u201c18% model\u201d with three convolutional layers (layers-18pct.cfg) for both SVHN and CIFAR-10 experiments. No data augmentation is done. The only data preprocessing was the contrast normalization for SVHN images.\n0 20 40 60 80 100\n10\n15\n20\n25\n30\ntraining data size (k)\nte s t\ne rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned) top\u2212down (fix \u03b1=0.7)\n0 20 40 60 80 0\n20\n40\n60\n80\n% of incorrect labels in training data\nte s t\ne rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned) top\u2212down (fix \u03b1=0.7)\n0 20 40 60 80 100 5\n10\n15\n20\n25\ntraining data size (k)\nte s t\ne rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned)\n0 20 40 60 80 0\n10\n20\n30\n40\n50\n60\n70\n% of incorrect labels in training data\nte s t e rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned)\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 5: Left: Test errors on SVHN dataset when 50% of the labels the training data are false. Right: Test errors on different noise levels\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n2\n4\n6\n8\n10\n12\n14\n16\nx 10 \u22123\ntrue noise learned noise |Q Q\u0302| distribution Q distribution Q\u0302\nFigure 6: The ground truth noise distribution used for generating noisy labels is in comparison with the learned one. This shows the bottom-up noise model can successfully learn the hidden noise distribution from the noisy data itself even if 50% of the training labels are incorrect.\nIn figure 5, we also plotted error rates when the bottom-up model is trained with the true noise distribution Q, rather than learning it from data. Even with this advantage, there was no significant difference, except for an extreme noise of 80%. This shows that the proposed method for learning Q from data is effective. Figure 6 shows one such learned Q\u0302 in comparison with the corresponding true Q. We can see that the difference between them is negligible.\nnormal model bottom-up model\n# of true labels in training data\n# o\nf fa\nls e\nl a\nb e ls in t ra in in g d a ta\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n1\n1.2\n1.4\n1.6\n1.8\n0%\n6%\n25%\n40%\n63%\n# of true labels in training data\n# o\nf fa\nls e l a b e ls\nin t ra\nin in\ng d\na ta\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n1\n1.2\n1.4\n1.6\n1.8\n0%\n6%\n25%\n40%\n63%\nFigure 7: The effect of incorrect labels in the training data on test error.\nFigure 7 shows the effect of label noise on performance in more detail for SVHN data. For a normal deep network, the performance drops quickly as the number of incorrect labels increases. In contrast, the bottom-up model showed more robustness against incorrect labels. For example, in the\n8\n0 20 40 60 80 100 5\n10\n15\n20\n25\ntraining data size (k)\nte st\ne rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned)\n0 20 40 60 80 0\n10\n20\n30\n40\n50\n60\n70\n% of incorrect labels in training data\nte st\ne rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned)\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 5: Left: Test errors on SVHN dataset when 50% of the labels the training data are false. Right: Test errors on different noise levels\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n2\n4\n6\n8\n10\n12\n14\n16\nx 10 \u22123\ntrue noise learned noise |Q Q\u0302| distribution Q distribution Q\u0302\nFigure 6: The ground truth noise distribution used for generating noisy labels is in comparison with the learned one. This shows the bottom-up noise model can successfully learn the hidden noise distribution from the noisy data itself even if 50% of the training labels are incorrect.\nIn figure 5, we also plotted error rates when the botto -up model is trained with the true noise distribution Q, rather than learning it from data. Even with this advantage, there was no significant difference, except for an extreme noise of 80%. This shows that the proposed method for learning Q from data is effective. Figure 6 shows one such learned Q\u0302 in comparison with the corresponding true Q. We can see that the difference between them is negligible.\nnormal model bottom-up model\n# of true labels in training data\n# o\nf fa\nls e la\nb e ls\nin t ra\nin in\ng d\na ta\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n1\n1.2\n1.4\n1.6\n1.8\n0%\n6%\n25%\n40%\n63%\n# of true labels in training data\n# o\nf fa\nls e\nla b\ne ls\nin t\nra in\nin g\nd a\nta\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n1\n1.2\n1.4\n1.6\n1.8\n0%\n6%\n25%\n40%\n63%\nFigure 7: The effect of incorrect labels in the training data on test error.\nFigure 7 shows the effect of label noise on performance in more detail for SVHN data. For a normal deep network, the performance drops quickly as the number of incorrect labels increases. In contrast, the bottom-up model showed more robustness against incorrect labels. For example, in the\n8\ntrue\nlearned\nestimate\n0 20 40 60 80 100 5\n10\n15\n20\n25\ntraining data size (k)\nte st\ne rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned)\n0 20 40 60 80 0\n10\n20\n30\n40\n50\n60\n70\n% of incorrect labels in training data\nte st\ne rr\no r\n(% )\nnormal model bottom\u2212up (ground truth) bottom\u2212up (learned)\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 5: Left: Test e rors on SVHN dataset when 50% of the labels the training data are false. Right: Test errors on different noise levels\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n2\n4\n6\n8\n10\n12\n14\n16\nx 10 \u22123\ntrue noise learned noise |Q Q\u0302| distribution Q distribu ion Q\u0302\nFigure 6: The ground truth noise distribution used f r generating noisy labels in comparison with the learned one. This shows the bottom-up noise model can successfully learn the hidden noise distribution from the n isy data it elf even if 50% of the training labels are incorrect.\nIn figure 5, we also plott d error rates when the bottom-up model is trained with the true noise distribution Q, rather than learning it from data. Even with this advantage, there was no significant difference, except for an extreme noise of 80%. This shows that the proposed method for learning Q from data is effective. Figure 6 shows one such learned Q\u0302 in comparison with the corresponding true Q. We can see that the difference between them is negligible.\nnormal model bottom-up model\n# of true labels in training data\n# o\nf fa\nls e la\nb e ls\nin t ra\nin in\ng d\na ta\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n1\n1.2\n1.4\n1.6\n1.8\n0%\n6%\n25%\n40%\n63%\n# of true labels in training data\n# o\nf fa\nls e\nla b\ne ls\nin t\nra in\nin g\nd a\nta\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n1\n1.2\n1.4\n1.6\n1.8\n0%\n6%\n25%\n40%\n63%\nFigure 7: The effect of incorrect labels in the training data on test error.\nFigure 7 shows the effect of label noise on performance in more detail for SVHN data. For a normal deep network, the performance drops quickly as the number of incorrect labels increases. In c ntrast, the bottom-up mod l showed mor robu tness against incorrect labels. For example, in the\n8\n(a) (b) (c) Figure 4: (a) Test errors on SVHN dataset when noise level is 50% (b) Test errors when trained on 100k samples. (c) The true noise distribution Q for 50% noise compared with learned and estimated Q\u0302.\nSVHN noisy only: When training a bottom-up model with SVHN data, we fix Q\u0302 to identity for the first five epochs. Then, Q\u0302 is updated with weight decay 0.05 for 100 epochs. Figure 4(a) and (b) shows the test errors for different training data sizes and different noise levels. Compared with a normal deep network, the bottom-up model always achieves better accuracy. However, the top-down model do not work well for any value of \u03b1 (even using the true Q does not improve).\nIn Figure 4(a) and (b), we also plot error rates for a bottom-up model trained using the true noise distribution Q. We see that it performs as well as the learned Q\u0302, showing that our proposed method for learning the noise distribution from data is effective. Figure 4(c) shows one such learned Q\u0302 alongside the ground truthQ used to generate the noisy data. We can see that the difference between them is negligible.\nFigure 5(a) shows the effect of label noise on performance in more detail for SVHN data. For a normal deep network, the performance drops quickly as the number of incorrect labels inc e s s. In contrast, the bottom-up model shows more robustness against incorrect labels. For example, in the normal model, training on 50k correct + 40k incorrect labels is the same as training on 10k correct\n1https://code.google.com/p/cuda-convnet/\nnormal bottom-up normal bottom-up\n# of correct labels\n# o\nf in\nc o rr\ne c t la\nb e ls\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n# of correct labels\n1 2 3 4 5\nx 10 4\n0\n1\n2\n3\n4\n5 x 10\n4\n10\n16\n25\n40\n63\nsize of training data\n% o\nf in\nc o rr\ne c t la\nb e ls\n1 2 3 4 5\nx 10 4\n0\n10\n20\n30\n40\n50\n60\n70\nsize of training data\n1 2 3 4 5\nx 10 4\n0\n10\n20\n30\n40\n50\n60\n70\n20\n25\n32\n40\n50\n63\n(a) SVHN (b) CIFAR-10\nFigure 5: The effect of incorrect labels on the test error (%) for an unmodified network (\u201cnormal\u201d) and one with our bottom-up noise layer appended.\nlabels. However, we can achieve the same performance using 30k correct and 50k incorrect labels with the bottom-up model.\nCIFAR-10 noisy only: We perform the same experiments as for SVHN on the CIFAR-10 dataset, varying the training data size and noise level. We fix Q\u0302 to identity for the first 50 epochs of training and then run for another 70 epochs updating Q\u0302 with weight decay of 0.05 or 0.1 (selected on validation set). The results are shown in Figure 5(b). Again, the bottom-up model is more robust to label noise, compared to the unmodified model. The difference is especially large for high noise levels and large training sets, which shows the scalability of the bottom-up model.\nCIFAR-10 clean + noisy: Next, we consider the scenario where both noisy and clean data are available. We now are able to use the approach from Section 3.2 to estimate the noise distribution Q using clean/noisy data, instead of learning it. We prepare two subsets of data: 20k clean images for estimating Q, and 30k noisy images for training the final model. First, we train a normal network with 10k clean data, which gives 30% test error. Then, we measure two confusion matrices, one on the other 10k clean data and the other on 30k noisy data. From these we compute an estimated Q\u0302, which is used to train a new bottom-up model using only noisy data. An estimated Q\u0302 for 50% noise is shown in Figure 4(c), where it is very close to the ground truth. Table 1 compares the classification performance using learned and estimated Q\u0302 for two different noise levels within the 30k noisy set. The estimated Q\u0302 is as effective as the true Q, even at high noise levels.\n4.2 CIFAR-10 + Tiny Images CIFAR-10 was originally created by cleaning up a subset of Tiny Images, a dataset of loosely labeled 80M 32x32 images (more than half of labels are estimated to be incorrect). In the process of hand picking CIFAR-10 images, some images were excluded because they did not show the object clearly or were labeled falsely. Our training data consists of two subsets: 50k clean labeled images from CIFAR-10 training data and 150k noisy labeled images from the excluded set of Tiny Images. As shown in Figure 6, those extra training images have very noisy labels, and often contain objects not in the 10 categories.\nairplane cat horse Figure 6: Sample images from the extra training data\nWe use a model architecture similar to the Krizhevsky\u2019s \u201c18% model\u201d, but with more feature maps in the three layers (32, 32, 64 \u2192 64, 128, 128) to accomodate the larger training set. No data augmentation is used. During training, we alternate mini batches from two data sources (when the\ncurrent mini batch is from clear data, the noise modeling is removed from the network). The noise matrix is fixed to identity during the first 50 epochs of the bottom-up model. Then the noise matrix is updated simultaneously with no weight decay. In the top-down model, however, the noise matrix is always fixed to \u03b1 = 0.5. The network is evaluated on the original CIFAR-10 test set (which has clear labels).\nTable 2 shows test errors for several noise modeling approaches. In most cases, weighting the noisy labels with \u03b3 = 0.2 helps performance, as shown in Figure 7 and row 4 of Table 2. The bottom-up model performance is mediocre, most likely due to the large fraction of outside images (i.e., showing objects not in the 10 categories) present in the Tiny Images, which violate the noise model. By contrast, the top-down model imposes a more uniform label distribution on these outside images, and so works well in practice. We test this hypothesis by training with a new 150k image set, randomly drawn from the entire Tiny Image dataset (not just the excluded set) and having uniform labels (equivalent to top-down with \u03b1 = 0.1) and we see that these also give good test performance. Although we can treat outside images as a separate class, assigning them uniform labels works better in practice. It can be considered as novel way of regularizing deep networks by random images, which are cheap to obtain. However, it is important that random images are not constrained within training categories.\n0 0.2 0.4 0.6 0.8 1 13\n13.5\n14\n14.5\n15\n15.5\n16\n16.5\nnoise weight \u03b3\nte s t\ne rr\no r\n(% )\nFigure 7: Test error dependency on noise weight \u03b3\nModel Extra data Noisy weight \u03b3\nTest error\nConv. net - - 16.1% Conv. net\n150k noisy 1 15.4% Conv. net 0.2 13.2% Bottom-up 0.2 13.2% Top-down 0.4 12.5% Conv. net 150k random 0.2 13.8%\nTable 2: Test error on CIFAR-10 + Tiny images.\n4.3 ImageNet + Web Image Search We perform a large-scale experiment using the ImageNet 2012 dataset which has 1.3M image with clean labels over 1000 classes. We obtain a further noisy set of 1.4M images, scraped from Internet image search engines using the 1k ImageNet keywords. Overlapping images between the two sets were removed from the noisy set using cross-correlation. We trained the model of Krizhevsky et al. [8] on the clean and combined datasets, with several types of noise modeling. Table 3 shows that training with the combined dataset does not improve the performance compared to training on the clean dataset. But just weighting the noisy labels (using \u03b3 = 0.1) gives an improvement of 1.4%. This can be improved slightly with the bottom-up noise model. However, the absolute performance achieved with our techniques and the 1.4M additional noisy data equals the performance when training on an additional 15M clean images from ImageNet 2011 (row 3). This demonstrates that noisy data can be very beneficial for training. Note that no model averaging is done in those experiments.\n5 Conclusion In this paper, we proposed two models for learning from noisy labeled data with deep networks, which can be implemented with minimal effort in existing deep learning implementations. Our experiments show that this model can reliably learn the noise distribution from data, under reasonable\nconditions. In addition, a simple technique to estimate the noise distribution using clean data is proposed. We also found that, if noisy and clean training sets exist, simply down-weighting the noisy set can help significantly. Both these techniques were demonstrated on large scale experiments, showing significant gains over training on clean data alone. Another surprising finding was that random images can be used to regularize deep networks and improve the performance significantly.\nReferences\n[1] R. Barandela and E. Gasca. Decontamination of training samples for supervised pattern recognition methods. In Advances in Pattern Recognition, volume 1876 of Lecture Notes in Computer Science, pages 621\u2013630. Springer, 2000.\n[2] J. Bootkrajang and A. Kabn. Label-noise robust logistic regression and its applications. In Machine Learning and Knowledge Discovery in Databases, volume 7523 of Lecture Notes in Computer Science, pages 143\u2013158. Springer, 2012.\n[3] C. E. Brodley and M. A. Friedl. Identifying mislabeled training data. Journal of Artificial Intelligence Research, 11:131\u2013167, 1999.\n[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255, June 2009.\n[5] B. Frenay and M. Verleysen. Classification in the presence of label noise: A survey. Neural Networks and Learning Systems, IEEE Transactions on, 25(5):845\u2013869, May 2014.\n[6] I. Guyon, N. Matic, and V. Vapnik. Discovering informative patterns and data cleaning. In Advances in Knowledge Discovery and Data Mining, pages 181\u2013203. 1996.\n[7] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Computer Science Department, University of Toronto, Tech. Rep, 2009.\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1097\u20131105. 2012.\n[9] J. Larsen, L. Nonboe, M. Hintz-Madsen, and L. Hansen. Design of robust neural network classifiers. In Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on, volume 2, pages 1205\u20131208 vol.2, May 1998.\n[10] N. D. Lawrence and B. Scho\u0308lkopf. Estimating a kernel fisher discriminant in the presence of label noise. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 306\u2013313, 2001.\n[11] V. Mnih and G. Hinton. Learning to label aerial images from noisy data. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 567\u2013574, 2012.\n[12] N. Natarajan, I. Dhillon, P. Ravikumar, and A. Tewari. Learning with noisy labels. In Advances in Neural Information Processing Systems 26, pages 1196\u20131204. 2013.\n[13] D. Nettleton, A. Orriols-Puig, and A. Fornells. A study of the effect of different types of noise on the precision of supervised learning techniques. Artificial Intelligence Review, 33(4):275\u2013 306, 2010.\n[14] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\n[15] M. Pechenizkiy, A. Tsymbal, S. Puuronen, and O. Pechenizkiy. Class noise and supervised learning in medical domains: The effect of feature extraction. In Computer-Based Medical Systems, 2006. CBMS 2006. 19th IEEE International Symposium on, pages 708\u2013713, 2006.\n[16] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In International Conference on Learning Representations (ICLR 2014), April 2014.\n[17] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace: Closing the Gap to Human-Level Performance in Face Verification. Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\n[18] A. Torralba, R. Fergus, and W. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1958\u20131970, Nov 2008."}], "references": [{"title": "Decontamination of training samples for supervised pattern recognition methods", "author": ["R. Barandela", "E. Gasca"], "venue": "In Advances in Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Label-noise robust logistic regression and its applications. In Machine Learning and Knowledge Discovery in Databases, volume 7523 of Lecture", "author": ["J. Bootkrajang", "A. Kabn"], "venue": "Notes in Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Identifying mislabeled training data", "author": ["C.E. Brodley", "M.A. Friedl"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Classification in the presence of label noise: A survey", "author": ["B. Frenay", "M. Verleysen"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Discovering informative patterns and data cleaning", "author": ["I. Guyon", "N. Matic", "V. Vapnik"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Design of robust neural network classifiers", "author": ["J. Larsen", "L. Nonboe", "M. Hintz-Madsen", "L. Hansen"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["N.D. Lawrence", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Learning to label aerial images from noisy data", "author": ["V. Mnih", "G. Hinton"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Learning with noisy labels", "author": ["N. Natarajan", "I. Dhillon", "P. Ravikumar", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A study of the effect of different types of noise on the precision of supervised learning techniques", "author": ["D. Nettleton", "A. Orriols-Puig", "A. Fornells"], "venue": "Artificial Intelligence Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Class noise and supervised learning in medical domains: The effect of feature extraction", "author": ["M. Pechenizkiy", "A. Tsymbal", "S. Puuronen", "O. Pechenizkiy"], "venue": "In Computer-Based Medical Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W. Freeman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1958}], "referenceMentions": [{"referenceID": 12, "context": "2 Related Work In any classification model, degradation of performance is inevitable when there is noise in training labels [13, 15].", "startOffset": 124, "endOffset": 132}, {"referenceID": 14, "context": "2 Related Work In any classification model, degradation of performance is inevitable when there is noise in training labels [13, 15].", "startOffset": 124, "endOffset": 132}, {"referenceID": 0, "context": "A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected [1, 3].", "startOffset": 136, "endOffset": 142}, {"referenceID": 2, "context": "A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected [1, 3].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "However, a weakness of this approach is the difficulty of distinguishing informative hard samples from harmful mislabeled ones [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 4, "context": "See [5] for comprehensive review.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "A more recent work [2] proposed a generic unbiased estimator for binary classification with noisy labels.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "A cost function similar to ours is proposed in [2] to make logistic regression robust to label noise.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels.", "startOffset": 48, "endOffset": 59}, {"referenceID": 16, "context": "Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels.", "startOffset": 48, "endOffset": 59}, {"referenceID": 15, "context": "Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels.", "startOffset": 48, "endOffset": 59}, {"referenceID": 10, "context": "In [11, 9], noise modeling is incorporated to neural network in the same way as our proposed model.", "startOffset": 3, "endOffset": 10}, {"referenceID": 8, "context": "In [11, 9], noise modeling is incorporated to neural network in the same way as our proposed model.", "startOffset": 3, "endOffset": 10}, {"referenceID": 10, "context": "However, only binary classification is considered in [11], and [9] assumed symmetric label noise (noise is independent of the true label).", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "However, only binary classification is considered in [11], and [9] assumed symmetric label noise (noise is independent of the true label).", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": "1 Bottom-up Noise Model We assume that label noise is random conditioned on the true class, but independent of the input x (see [10] for more detail about this type of noise).", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "An unbiased estimator for binary classification is introduced in [1], where the cost fun tion s replaced by a surrogate cost function that combines the two costs (costs for class +1 and -1) with certain coefficients that depends on the noise level.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "An u biased estim tor for binary classificati n is introduc d in [12], whe e the cost unction is replaced by a surrogate objective that combines the two costs (costs for class +1 and -1) with coefficients that depends on the noise level.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "First, we experiment on the Google street-view house number dataset (SVHN) [14], which consists of 32x32 images of house number digits captured from Google Streetview.", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Next, we experiment on CIFAR-10 [7], a more challenging dataset consisting from 60k small images of 10 object categories.", "startOffset": 32, "endOffset": 35}, {"referenceID": 17, "context": "The first dataset consist from clean images from CIFAR-10 dataset and noisy images from Tiny Images dataset [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": "The second dataset consists of clean images from ImageNet [4] and noisy images downloaded from web search engines.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "[8] on the clean and combined datasets, with several types of noise modeling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] 18.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] 15M full ImageNet 16.", "startOffset": 0, "endOffset": 3}], "year": 2014, "abstractText": "We propose several simple approaches to training deep neural networks on data with noisy labels. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark, showing how additional noisy data can improve state-of-the-art recognition models. 1 Introduction In recent years, deep learning methods have shown impressive results on image classification tasks. However, this achievement is only possible because of large amount of labeled images. Labeling images by hand is a laborious task and takes a lot of time and money. An alternative approach is to generate labels automatically. This includes user tags from social web sites and keywords from image search engines. Considering the abundance of such noisy labels, it is important to find a way to utilize them in deep learning. Unfortunately, those labels are very noisy and unlikely to help training deep networks without additional tricks. Our goal is to study the effect label noise on deep networks, and explore simple ways of improvement. We focus on the robustness of deep networks instead of data cleaning methods, which are well studied and can be used together with robust models directly. Although many noise robust classifiers are proposed so far, there are not many works on training deep networks on noisy labeled data, especially on large scale datasets. Our contribution in this paper is a novel way of modifying deep learning models so they can be effectively trained on data with high level of label noise. The modification is simply done by adding a linear layer on top of the softmax layer, which makes it easy to implement. This additional layer changes the output from the network to give better match to the noisy labels. Also, it is possible to learn the noise distribution directly from the noisy data. Using real-world image classification tasks, we demonstrate that the model actually works very well in practice. We even show that random images without labels (complete noise) can improve the classification performance. 2 Related Work In any classification model, degradation of performance is inevitable when there is noise in training labels [13, 15]. A simple approach to handle noisy labels is a data preprocessing stage, where labels suspected to be incorrect are removed or corrected [1, 3]. However, a weakness of this approach is the difficulty of distinguishing informative hard samples from harmful mislabeled ones [6]. Instead, in this paper, we focus on models robust to presence of label noise. 1 ar X iv :1 40 6. 20 80 v1 [ cs .C V ] 9 J un 2 01 4 The effect of label noise is well studied in common classifiers (e.g., SVMs, kNN, logistic regression), and their label noise robust variants have been proposed. See [5] for comprehensive review. A more recent work [2] proposed a generic unbiased estimator for binary classification with noisy labels. They employ a surrogate cost function that can be expressed by a weighted sum of the original cost functions, and gave theoretical bounds on the performance. In this paper, we will also consider this idea and extend it multiclass. A cost function similar to ours is proposed in [2] to make logistic regression robust to label noise. They also proposed a learning algorithm for noise parameters. However, we consider deep networks, a more powerful and complex classifier than logistic regression, and propose a different learning algorithm for noise parameters that is more suited for back-propagation training. Considering the recent success of deep learning [8, 17, 16], there are very few works about deep learning from noisy labels. In [11, 9], noise modeling is incorporated to neural network in the same way as our proposed model. However, only binary classification is considered in [11], and [9] assumed symmetric label noise (noise is independent of the true label). Therefore, there is only a single noise parameter, which can be tuned by cross-validation. In this paper, we consider multiclass classification and assume more realistic asymmetric label noise, which makes it impossible to use cross-validation to adjust noise parameters (there can be a million parameters). 3 Approach In this paper, we consider two approaches to make an existing classification model, which we call the base model, robust against noisy labels: bottom-up and top-down noise models. In the bottomup model, we add an additional layer to the model that changes the label probabilities output by the base model so it would better match to noisy labels. Top-down model, on other hand, changes given noisy labels before feeding them to the base model. Both models require a noise model for training, so we will give an easy way to estimate noise levels using clean data. Also, it is possible to learn noise distribution from noisy data in the bottom-up model. Although only deep neural networks are used in our experiments, the both approaches can be applied to any classification model with a cross entropy cost. 3.1 Bottom-up Noise Model We assume that label noise is random conditioned on the true class, but independent of the input x (see [10] for more detail about this type of noise). Based on this assumption, we add an additional layer to a deep network (see Figure 1) that changes its output so it would better match to the noisy labels. The weights of this layer corresponds to the probabilities of a certain class being mislabeled to another class. Because those probabilities are often unknown, we will show how estimate them from additional clean data, or from the noisy data itself. Let D be the true data distribution generating correctly labeled samples (x, y\u2217), where x is an input vector and y\u2217 is the corresponding label. However, we only observe noisy labeled samples (x, \u1ef9) that generated from a some noisy distribution D\u0303. We assume that the label noise is random conditioned on the true labels. Then, the noise distribution can be parameterized by a matrix Q = {qji}: qji := p(\u1ef9 = j|y\u2217 = i). Q is a probability matrix because its elements are positive and each column sums to one. The probability of input x being labeled as j in D\u0303 is given by p(\u1ef9 = j|x, \u03b8) = \u2211 i p(\u1ef9 = j|y\u2217 = i)p(y\u2217 = i|x) = \u2211 i qjip(y \u2217 = i|x, \u03b8). (1) where p(y\u2217 = i|x, \u03b8) is the probabilistic output of the base model with parameters \u03b8. If the true noise distribution is known, we can modify this for noisy labeled data. During training, Q will act as an adapter that transforms the model\u2019s output to better match the noisy labels. Deep\t\r  network Learnin from noisy labels in deep neural networks Sainbayar Sukhbaatar Dept. of Computer Science, NYU, 715 Broadway, New Y rk, NY 10003 sainbar@cs.nyu.edu Rob Fergus Courant Institute, NYU, 715 Broadway, New York, NY 10003 fergus@cs.nyu.edu", "creator": "LaTeX with hyperref package"}}}