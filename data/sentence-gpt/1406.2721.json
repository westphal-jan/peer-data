{"id": "1406.2721", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Learning Latent Variable Gaussian Graphical Models", "abstract": "Gaussian graphical models (GGM) have been widely used in many high-dimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models. However, the current research provides useful information on the relationship between data quality and performance in many highly-dimensional applications, from models to simulations to data-processing and statistical computing. The recent work of the GGM paper provides important information about the extent and duration of data set-up for real-world applications including model generation and simulations, and provides a general framework for the analysis of such data. This paper, along with additional technical and theoretical advice, provides detailed examples of how to use GGM and other high-dimensional data for graphical systems that support full-scale application analysis. In this paper, we explore the generalization of GGM and its various use cases. As discussed earlier, GGM was used for a variety of applications such as computational analysis and statistical computing, such as graph analysis and graphical computational simulations.\n\n\n\nThe present paper examines a variety of GGM systems that support a wide variety of GGM, including Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian Gaussian", "histories": [["v1", "Tue, 10 Jun 2014 21:03:22 GMT  (436kb,D)", "http://arxiv.org/abs/1406.2721v1", "To appear in The 31st International Conference on Machine Learning (ICML 2014)"]], "COMMENTS": "To appear in The 31st International Conference on Machine Learning (ICML 2014)", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["zhaoshi meng", "brian eriksson", "alfred o hero iii"], "accepted": true, "id": "1406.2721"}, "pdf": {"name": "1406.2721.pdf", "metadata": {"source": "CRF", "title": "Learning Latent Variable Gaussian Graphical Models", "authors": ["Zhaoshi Meng", "Brian Eriksson", "Alfred O. Hero III"], "emails": ["mengzs@umich.edu,", "brian.eriksson@technicolor.com,", "hero@eecs.umich.edu"], "sections": [{"heading": "1 Introduction", "text": "Critical to many statistical inference tasks in complex real-world systems, such as prediction and detection, is the ability to extract and estimate distributional characteristics from the observations. Unfortunately, in the high-dimensional regime such model estimation often leads to ill-posed problems, particularly when the number of observations n (or sample size) is comparable to or fewer than the ambient dimensionality p of the model (i.e., the \u201clarge p, small n\u201d problem). This\nar X\niv :1\n40 6.\n27 21\nv1 [\nst at\n.M L\n] 1\nchallenge arises in many modern real-world applications ranging from recommender systems, gene microarray data, and financial data, to name a few. To perform accurate model parameter estimation and subsequent statistical inference, low dimensional structure is often imposed for regularization (Negahban et al., 2012).\nFor Gaussian-distributed data, the central problem is often to estimate the inverse covariance matrix (alternatively known as the precision, concentration or information matrix). Gaussian graphical models (GGM) provide an efficient representation of the precision matrix through a graph that represents non-zeros in the matrix (Lauritzen, 1996). In high-dimensional regimes, this graph can be forced to be sparse, imposing a low-dimensional structure on the GGM. For sufficiently sparse GGM, statistically consistent estimates of the model structure (i.e., sparsistency) can be achieved (e.g., Ravikumar et al. (2011)). On the computational side, sparsity also leads to reduced complexity of the estimator (Hsieh et al., 2013). However, when the true distribution can not be well-approximated by a sparse GGM, the standard learning paradigm suffers from either large estimation bias due to enforcing a overly sparse model, or degraded computation time for a dense model. Both result in suboptimal performance in the subsequent inference tasks.\nIn this paper, we consider a new class of high-dimensional GGM for extending the standard sparse GGM. The proposed model is motivated by many real-world applications, where there exist certain exogenous and often latent factors affecting a large portion of the variables. Examples are the price of oil on the airlines\u2019 stock price variables (Choi et al., 2010), and the genres on movie rating variables. Conditioning on these global effects, the variables are assumed to have highly localized interactions, which can be well-fitted by a sparse GGM. However, due to the marginalization over global effects, the observed (marginal) GGM, and its corresponding precision matrix, is not sparse. Unfortunately, in this regime, existing theoretical results and computational tools for sparse GGM are not applicable.\nTo address this problem, we propose to use latent variable Gaussian graphical models (LVGGM) for modeling and statistical inference. LVGGM introduce latent variables to capture the correlations due to the global effects, and the remaining effects are captured by a conditionally sparse graphical model. The resulting marginal precision matrix of the LVGGM has a sparse plus low-rank structure, therefore we consider a regularized maximum likelihood (ML) approach for parameter estimation (previously considered by Chandrasekaran et al. (2012)). By utilizing the almost strong convexity (Kakade et al., 2010) of the log-likelihood, we derive a non-asymptotic parameter error bound for the regularized ML estimator. Our derived bounds apply to the high-dimensional setting of p n due to restricted strong convexity (Negahban et al., 2012) and certain structural incoherence between the sparse and low-rank components of the precision matrix (Yang & Ravikumar, 2013).\nWe show that for sufficiently large n, the Frobenius norm error of the precision\nmatrix of LVGGM converges at the rateO( \u221a\n(s+reff\u00b7r) log p n\n), where s is the number of non-zeros in the conditionally sparse precision matrix, reff is the effective rank of the covariance matrix and r is the number of latent variables. This rate is in general significantly faster than the standard convergence rate of O( \u221a p2 log p n\n) for an unstructured dense GGM. This result offers a compelling argument for using LVGGM over sparse GGM for many inference problems.\nThe paper is structured as follows. In Section 2 we review the relevant prior literature. In Section 3 we formulate the LVGGM estimation problem. In Section 4 the main theoretical results are presented. Experimental results are shown in Section 5 and we conclude in Section 6. We use boldface letters to denote vectors and matrices. \u2016 \u00b7 \u20161, \u2016 \u00b7 \u20162, \u2016 \u00b7 \u2016F , \u2016 \u00b7 \u2016\u2217 denote the elementwise `1, spectral, Frobenius, and nuclear matrix norms, respectively."}, {"heading": "2 Background and Related Work", "text": "The problem of learning GGM with sparse inverse covariance matrices using `1- regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al. (2011); Rothman et al. (2008). In particular, the authors of Ravikumar et al. (2011) study the model selection consistency (i.e., \u201csparsistency\u201d) under certain incoherence condition. Beyond sparse GGM, Choi et al. (2010) propose a multiresolution extension of a GGM augmented with sparse inter-level correlations, while in Choi et al. (2011) the authors consider latent tree-structured graphical models. Both models lead to computationally efficient inference and learning algorithms but restrict the latent structure to trees. Recently, Liu & Willsky (2013) consider a computationally efficient learning algorithm for a class of conditionally tree-structured LVGGM.\nThe work that is most relevant to ours is by Chandrasekaran et al. (2012), who study the LVGGM learning problem, but focus on the simultaneous model selection consistency of both the sparse and low-rank components. In contrast, in this paper we focus on the Frobenius norm error bounds for estimating the precision matrix of LVGGM. Although structural consistency can be useful for deriving insights, parameter estimation error analysis is of equal or greater importance in practice. Since it provides additional, and usually more direct, insights into factors influencing the performance of the subsequent statistical inference tasks, such as prediction and detection. Also, compared with Chandrasekaran et al. (2012), our Frobenius norm error bounds are derived under mild condition on the Fisher information of the distribution.\nWe note that there is a fundamentally different line of work on estimating models with a similar structural composition, known as robust PCA (Cande\u0300s et al., 2011). In robust PCA, the data matrix is modeled as \u201clow-rank plus sparse\u201d. This model has been applied to extracting the salient foreground from background in videos, and detecting malicious user ratings in recommender system data (Xu et al., 2012). In contrast, the equivalent covariance model of our LVGGM can be decomposed into a low-rank plus a dense matrix whose inverse is sparse. A similar covariance model has recently been studied by Kalaitzis & Lawrence (2012), in which an EM algorithm is proposed for estimation but no theoretical error bounds are derived. In this paper, we instead focus on the precision matrix parameterization, which enables model estimation through a convex optimization. This formulation is of both theoretical and computational importance."}, {"heading": "3 Problem Setup", "text": "In this section, we review Gaussian graphical models and formulate the problem of latent variable Gaussian graphical model estimation via a regularized maximum likelihood optimization."}, {"heading": "3.1 Gaussian Graphical Models", "text": "Consider a p-dimensional random vector x associated with an undirected graph G = (VG, EG), where VG is a set of nodes corresponding to elements of x and EG is a set of edges connecting nodes (including self-edges for each node). Then x follows a graphical model distribution if it satisfies the Markov property with respect to G: for any pair of nonadjacent nodes in G, the corresponding pair of\nvariables in x are conditionally independent given the remaining variables, i.e., xi \u22a5 xj | x\\i,j , for all (i, j) /\u2208 EG.\nIf x follows a multivariate Gaussian distribution, the corresponding graphical model is called a Gaussian graphical model (GGM). We assume without loss of generality that x has zero mean. The Markov property in GGM is manifested in the sparsity pattern of the inverse covariance matrix J:\nJi,j = 0 for all i 6= j, (i, j) /\u2208 E. (1) An example of this property for sparse GGM is shown in Figure 1(a) and 1(b).\nThe precision matrix parameterization arises in many statistical inference problems for Gaussian distributions, in areas such as belief propagation (Malioutov et al., 2006), linear prediction, portfolio selection in financial data (Ledoit & Wolf, 2003), and anomaly detection (Chen et al., 2011). Estimation of the precision matrix in GGM is the first step in these inference problems."}, {"heading": "3.2 Latent Variable Gaussian Graphical Models", "text": "Unfortunately, due to the presence of global factors that destroy sparsity, realworld observations often do not conform exactly to a sparse GGM (Choi et al., 2010, 2011). By introducing latent variables (denoted as a r-dimensional random vector xL) to capture global factors, we can generalize the GGM. Specifically, we construct a model that is conditionally a GGM, i.e., one that has a sparse precision matrix given knowledge of latent variables, xL.\nDefining the p observed variables as xO, we assume the joint distribution of the (p + r)-dimensional concatenated random vector x = (xO,xL) follows a Gaussian distribution with covariance matrix \u2126 and precision matrix J = \u2126\u22121. An example of this structure can be seen in Figure 1(c) and 1(d). Marginalizing over the latent variables xL, the distribution of the observed variables xO remains Gaussian with observed covariance matrix, \u03a3 = \u2126O,O. The observed precision matrix \u0398 \u2208 Rp\u00d7p satisfies:\n\u0398 = \u03a3\u22121 = JO,O\ufe38\ufe37\ufe37\ufe38 S \u2212JO,LJ\u22121L,LJL,O\ufe38 \ufe37\ufe37 \ufe38 L , (2)\nwhere we have defined S := JO,O and L := \u2212JO,LJ\u22121L,LJL,O. Thus, the marginal precision matrix can be written as \u0398 = S + L, the sum of a sparse and a lowrank matrix. Similar to standard GGM, we parameterize the marginal distribution through the precision matrix. We refer to this model as the latent variable GGM, or LVGGM.\nThe LVGGM is a hierarchical model that generalizes the (sparse) GGM. Note that S\u22121 = J\u22121O,O = \u2126O,O \u2212 \u2126O,L\u2126\u22121L,L\u2126L,O is the covariance matrix of the conditional distribution of the observed variables. The matrix is not generally sparse,\neven though S is assumed to be sparse. We will also assume that the number of latent variables is much smaller than the number of observed variables, i.e., r p. We place no sparsity restrictions on the dependencies between the observed and latent variables \u2013 the submatrices JO,L and JL,O could be dense. As a result, the p \u00d7 p matrix L = \u2212JO,LJ\u22121L,LJL,O is low-rank and potentially dense. The sparse plus low-rank structure of the marginal precision matrix \u0398 is the key property of the precision matrix that will be exploited for model estimation.\nThe structural assumptions on the precision matrix of the LVGGM can be further motivated and validated on real-world recommender system data and stock return data. Due to the space limits, we defer these two motivating examples to Section A in the Appendix."}, {"heading": "3.3 Effective Rank of Covariance Matrix", "text": "We introduce the effective rank of a matrix, which will be useful to derived highdimensional error bounds. The effective rank of a matrix \u03a3 is defined as (Vershynin, 2010):\nreff(\u03a3) := tr(\u03a3)/\u2016\u03a3\u20162. (3) The effective rank can be considered a measure of the concentration level of the spectrum of \u03a3. As we will show in Section 5.1, in many situations the effective rank of the covariance matrix corresponding to a LVGGM is much smaller than p. Under this condition, our theoretical results in the sequel provide a tight Frobenius norm estimation error bound, which is significantly improved upon the error bound derived without the effective rank assumption."}, {"heading": "3.4 Regularized ML Estimation of LVGGM", "text": "Available are n samples x1, x2, . . . , xn from a LVGGM model xO, concatenated into a data matrix X \u2208 Rp\u00d7n. The negative log-likelihood function is\nL(\u0398; X) = \u3008\u03a3\u0302,\u0398\u3009 \u2212 log det(\u0398), (4) where \u03a3\u0302 := 1\nn XTX is the sample covariance matrix. The regularized ML estimate\nminimizes the objective function L(\u0398; X) + \u03bbR(\u0398), where the regularization parameter \u03bb > 0, and the regularization functionR(\u0398) is designed to enforce the sparse plus low-rank structure on \u0398.\nSimilar to Chandrasekaran et al. (2012), we consider the following regularized ML estimation problem:\nmin S,L L(S + L; X) + \u03bb\u2016S\u20161 + \u00b5\u2016L\u2016\u2217\ns.t. \u2212 L 0, S + L 0, (5)\nwhere the corresponding regularization function is the sum of two regularizers: R(\u0398) = \u2016S\u20161 + \u00b5\u03bb\u2016L\u2016\u2217, each of which has been shown to promote sparse (lowrank) structure in S (L, respectively) (Negahban et al., 2012). Constants \u03bb, \u00b5 > 0 are regularization parameters corresponding to the two functions, respectively. The LVGGM estimator is defined as a solution to the above convex optimization problem (5). Efficient convex solver, such as Ma et al. (2013), can be used to solve."}, {"heading": "4 Error Bounds on ML LVGGM Estimation", "text": "We analyze the regularized ML estimation problem (5) and provide Frobenius norm error bounds for estimating the precision matrix in high-dimensional setting. We adopt the decomposable regularization framework of Negahban et al. (2012); Agarwal et al. (2012); Yang & Ravikumar (2013) to derive these bounds. In contrast to this prior work, here we focus on multiple decomposable regularizers interacting with the non-quadratic log-likelihood loss function encountered in the LVGGM. Two important ingredients in the derivations are the restricted strong convexity of the loss function, and an incoherence condition between the two structured subspaces containing the sparse and low-rank components (S and L). We show that under assumptions on the Fisher information these two conditions are verified.\nIn the following subsections, first we define some necessary notation, then we introduce the assumptions and place them in the context of prior literature, and finally we state the main results in Theorem 1 and Theorem 2."}, {"heading": "4.1 Decomposable Regularizers and Subspace Notation", "text": "In this subsection we introduce the notion of decomposable regularizers and the corresponding subspace pairs. We refer the reader to Negahban et al. (2012) for more details.\nConsider a pair of subspaces (M,M\u22a5), where M \u2282 M \u2282 Rp\u00d7p. R(\u00b7) is called a decomposable regularization function with respect to the subspace pair if, for any u \u2208M, v \u2208M\u22a5, we haveR(u+ v) = R(u) +R(v).\nFor the sparse and low-rank matrix-valued parameters, the following two subspace pairs and their corresponding decomposable regularizers are considered: \u2022 Sparse matrices. Let E \u2286 {1, . . . , p} \u00d7 {1, . . . , p} be a subset of index pairs\n(edges). DefineM(E) =M(E) as the subspace of all sparse matrices in Rp\u00d7p that are supported in subsets of E, i.e., PM(E)(A) = AE . A decomposable regularizer is the `1 norm, since \u2016A\u20161 = \u2016AE\u20161 + \u2016AEC\u20161.\n\u2022 Low-rank PSD matrices. Consider a class of low-rank and positive semi-definite matrices A \u2282 Sp\u00d7p+ which have rank r \u2264 p. For any given matrix A \u2208 A, let col(A) denote its column space. Let U \u2282 Rn be a r-dimensional subspace and define the subspaceM(U) and the perturbation subspaceM\u22a5(U) as\nM(U) :={A \u2208 Rn\u00d7p | col(A) \u2286 U}, M\u22a5(U) :={A \u2208 Rn\u00d7p | col(A) \u2286 U\u22a5}.\nThen the nuclear normRL(\u00b7) = \u2016 \u00b7\u2016\u2217 is a decomposable regularization function with respect to the subspace pair (M(U),M\u22a5(U)).\nFor the true model parameter \u0398\u2217, we define its associated structural error set with respect to a subspaceM as (Negahban et al., 2012):\nC(M,M\u22a5; \u0398\u2217) := {\n\u2206 \u2208 Rn\u00d7p | R(\u2206M\u22a5) \u2264 3R(\u2206M) + 4R(\u0398 \u2217 M\u22a5\n) } .\nBy construction, if the norm of the projection of the true parameter \u0398\u2217 intoM\u22a5 is small, then elements \u2206 in this structural error set also have limited projection onto the perturbation subspaceM\u22a5.\nNow let \u0398\u2217 be the true (marginal) precision matrix of the LVGGM, and let the sparse and low-rank components be S\u2217 and L\u2217, respectively. For the defined subspace pairs (M(E),M(E)\u22a5) and (M(U),M(U)\u22a5), we use C(E) and C(U) as the shorthand notations for the corresponding structural error sets centered at S\u2217 and L\u2217, i.e., C(M(E),M(E)\u22a5; S\u2217) and C(M(U),M(U)\u22a5; L\u2217), respectively. Later, we will consider the perturbation of \u0398\u2217 along restricted directions in these two sets."}, {"heading": "4.2 Assumptions on Fisher Information", "text": "We characterize the interaction between the elements in the two subspaces through their inner products using the Hessian of the loss function, also known as the Fisher information of the distribution. Denoting the Fisher information matrix of a Gaussian distribution as F\u2217 (evaluated at \u0398\u2217), we find that F\u2217 = \u0398\u2217\u22121 \u2297\u0398\u2217\u22121, where \u2297 is the Kronecker product. We define the Fisher inner product between two matrices \u2206A and \u2206B as\n\u3008\u2206A,\u2206B\u3009F\u2217 := vec(\u2206A)TF\u2217vec(\u2206B) (6) = Tr(\u0398\u2217\u22121\u2206A\u0398\u2217 \u22121\u2206B), (7)\nwhere vec(\u00b7) denotes the vectorization of a matrix.\nSimilar to prior work of Kakade et al. (2010), we define the induced Fisher norm of a matrix \u2206 as\n\u2016\u2206\u20162F\u2217 := vec(\u2206)TF\u2217vec(\u2206) (8) = Tr(\u0398\u2217\u22121\u2206\u0398\u2217\u22121\u2206). (9)\nThe first assumption we make is the following Restricted Fisher Eigenvalue (RFE) condition on the true precision model with respect to the sparse and lowrank structural error sets.\nAssumption 1 (Restricted Fisher Eigenvalue). There exists some constant \u03ba\u2217min > 0, such that for all \u2206 \u2208 C(E) \u222a C(U), the following holds:\n\u2016\u2206\u20162F\u2217 \u2265 \u03ba\u2217min\u2016\u2206\u20162F . (10) This RFE condition generalizes the restricted eigenvalue (RE) condition for sparsity-promoting linear regression problems Bickel et al. (2009). It assumes that the minimum eigenvalue of the Fisher information is bounded away from zero along the directions C(E) and C(U). Due to the identity (8) and properties of the Kronecker product, a trivial lower bound for \u03ba\u2217min is \u03bb 2 min(\u0398\n\u2217), where \u03bbmin(\u00b7) denotes the minimum eigenvalue. In the high-dimensional setting, the RFE parameter \u03ba\u2217min, which is defined only with respect to the above restricted set of directions, can be substantially larger than \u03bb2min(\u0398\n\u2217). As a result, the derived error bounds, which depend on \u03ba\u2217min, are generally tighter than the bounds depending on \u03bb2min(\u0398\n\u2217) (cf. Theorem 1). Due to the sparse plus low-rank superpositioned structure, we impose a type of incoherence between the two structural error sets to ensure consistent estimation of the combined model. The incoherence condition will limit the interaction between elements from the two sets. For our problem, such interaction occurs through their inner products with the Fisher information, which motivates the following Structural Fisher Incoherence (SFI) assumption (which generalizes the C-Linear assumption proposed in Yang & Ravikumar (2013)).\nLet PE := PM(E) denote the projection operator corresponding to the subspace M(E). Similarly define PU := PM(U), PE\u22a5 := PM(E)\u22a5 , and PU\u22a5 := PM(U)\u22a5 . We assume the following condition on the Fisher information. Assumption 2 (Structural Fisher Incoherence). Given a constant M > 6, a set of regularization parameters (\u03bb, \u00b5), and the subspace pairs (M(E),M(E)\u22a5) and (M(U),M(U)\u22a5) as defined above, let \u039b = 2 + 3 max { \u03bb \u221a s\n\u00b5 \u221a r , \u00b5 \u221a r \u03bb \u221a s\n} , where\ns = |E| and r = rank(U). Then the Fisher information F\u2217 satisfies:\nmax {\u03c3 (PEF\u2217PU) , \u03c3 (PE\u22a5F\u2217PU) , \u03c3 (PEF\u2217PU\u22a5) , \u03c3 (PE\u22a5F\u2217PU\u22a5)} \u2264 \u03ba\u2217min c1\u039b2 ,\nwhere \u03c3(\u00b7) denotes the maximum singular value, and constant c1 is defined as c1 = 16M M\u22126 .\nThe constant M is related to a \u201cburn-in\u201d period after which the likelihood loss function has desirable properties in a small neighborhood of the true parameter. In particular, when M = 7, the constant c1 = 112 suffices for our theory to hold. See the main theorem and its proof for more discussion on this quantity.\nIt is interesting to compare our SFI assumption to other similar assumptions in the literature of GGM estimation. In Ravikumar et al. (2011), a form of irrepresentability condition is assumed, which limits the induced `1 norm of a matrix that is similar to the projected Fisher information onto the sparse matrix subspace pair. In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)). For model selection consistency, a more general form of irrepresentability has been shown to be necessary for model selection consistency, see Lee et al. (2013) for a recent discussion. In contrast to the above line of work, the SFI assumption we make only controls the maximum singular values of the projected Fisher information. This can be explained as we are interested in bounding a weaker quantity, the Frobenius norm of the parameter estimation error, instead of establishing the stronger model selection consistency of Ravikumar et al. (2011) or the algebraic consistency as in Chandrasekaran et al. (2012)."}, {"heading": "4.3 Error Bounds for LVGGM Estimation", "text": "We have the following bound on the parameter error of the estimated precision matrix of LVVGGM, \u0398\u0302 = S\u0302 + L\u0302, obtained by solving the regularized ML problem (5).\nTheorem 1. Suppose Assumption 1 and 2 hold for the true marginal precision matrix \u0398\u2217, and the regularization parameters are chosen such that\n\u03bb \u2265 2\u2016\u03a3\u2217 \u2212 \u03a3\u0302\u2016\u221e and \u00b5 \u2265 2\u2016\u03a3\u2217 \u2212 \u03a3\u0302\u20162. (11)\nGiven a constant M > 6, if an optimal solution pair (S\u0302, L\u0302) to the convex program (5) satisfies\nmax{\u2016S\u0302\u2212 S\u2217\u2016F\u2217 , \u2016L\u0302\u2212 L\u2217\u2016F\u2217} \u2264 1\n6M2 , (12)\nthen we have the following error bound for the estimated precision matrix \u0398\u0302 = S\u0302 + L\u0302:\n\u2016\u0398\u0302\u2212\u0398\u2217\u2016F \u2264 6\n\u03baL max\n{ \u03bb \u221a s, \u00b5 \u221a r } + \u221a 8r\u2217\u22a5 \u03baL , (13)\nwhere s = |E|, r = rank(U), and\n\u03baL := M \u2212 2\n2(M \u2212 1)\u03ba \u2217 min, (14)\nr\u2217\u22a5 := \u03bb \u2211\n(j,k)/\u2208E\n|S\u2217jk|+ \u00b5 p\u2211\nj=r+1\n\u03c3j(L \u2217). (15)\nProof sketch. The proof is inspired by Yang & Ravikumar (2013), in which a parameter estimation error bound is proven for estimating a class of superpositionstructured parameters, such as sparse plus low-rank, through M-estimation with decomposable regularizers. Critical to specializing this framework to our LVGGM estimation problem is to verify two conditions on the log-likelihood loss function (4): the restricted strong convexity (RSC) and structural incoherence (SI). The RSC condition (which originally proposed in Negahban et al. (2012)) specifies the loss function to be sufficiently curved (i.e. lower bounded by a quadratic function) along a restricted set of directions (defined by C(E) and C(U)). On the other hand, the SI condition effectively limits certain interaction between elements from the above two structural error sets. In Yang & Ravikumar (2013), under certain C-linear assumptions, the RSC and SI conditions are verified for several problems with quadratic loss functions. For the LVGGM estimation problem, however, the technical difficulty lies in the non-quadratic log-likelihood loss (4), for which the previously established RSC and SI conditions do not hold.\nTo deal with this difficulty, we leverage the almost strong convexity properties (Kakade et al., 2010) to characterize the convergence behavior of the sum of higher-order terms in the Taylor series of the log-likelihood loss function. We show that in the regime specified by condition (12), the loss function can be wellapproximated by the sum of a quadratic function and a residual term. Under this condition, the RFE assumption (Assumption 1) guarantees the RSC condition (cf. Lemma 2), and the SFI assumption (Assumption 2) leads to SI condition to hold (cf. Lemma 4). Theorem 1 can then be proven by the general theorem in Yang & Ravikumar (2013). A detailed proof of Theorem 1 can be found in Appendix B.\nWe make the following remarks:\n\u2022 The error bound (13) is a family of upper bounds defined by different sets of subspace pairs (M(E),M(E)\u22a5) and (M(U),M(U)\u22a5). The tightest bound can be achieved by appropriately choosing E and U . The first additive term in (13) captures effect of the estimation error, while the second term captures the approximation error. In many cases it is reasonable to assume the approximation error is zero, then the error bound reduces to the first additive term.\n\u2022 We note that similar derivations also apply to `1-regularized estimation of sparse GGM. For the sparse GGM, only Assumption 1 is required, and the derivations largely simplify. The final error bound also contains estimation and approximation errors, depending only on the sparse matrix subspace pair. However, when the true precision matrix \u0398\u2217 cannot be wellapproximated as a sparse matrix (such as the LVGGM case), the approximation error would be much worse, leading to an inefficient learning rate.\n\u2022 We finally remark that the SFI assumption can be relaxed to an even milder incoherence condition, \u2016L\u2016\u221e \u2264 \u03b1, as considered in Agarwal et al. (2012). Following similar derivations as in the proof of Theorem 1, the corresponding error bound can be obtained. However, as a result of this incoherence assumption, the error bound would contain an additional incoherence term which does not vanish to zero even with infinite samples. This disadvantage is overcome under the structural incoherence condition.\nThe statement of Theorem 1 is deterministic in nature and applies to any optimum of the convex program. However, the condition on the regularization parameters (11) and the error bound depend on the sampled data (in particular the sample covariance matrix \u03a3\u0302), which is random. Therefore the key to specifying the regularization parameters, and hence obtaining error bounds independent of data, is to derive tight deviation bounds of the sample covariance matrix in terms of the `\u221e and `2 norms, such that condition (11) holds with high probability. These bounds can be obtained by using concentration inequalities for Gaussian distributions, which leads to the following corollary.\nCorollary 1. Let the same assumptions in Theorem 1 hold. Given constants C1 > 1 andC2 \u2265 1, assume that the number of samples n satisfies n \u2265 max {4C21 log p, C22p}, and that the regularization parameters satisfy\n\u03bb = 160C1\u03c3 \u2217\n\u221a log p\nn and \u00b5 = 16C2\u03c1\u2217\n\u221a p\nn , (16)\nwhere \u03c3\u2217 = maxi \u03a3\u2217i,i and \u03c1 \u2217 = \u2016\u03a3\u2217\u20162. Then with probability at least 1 \u2212 4p\u22122(C1\u22121) \u2212 2 exp(\u2212C22p 2 ), we have\n\u2016\u0398\u0302\u2212\u0398\u2217\u2016F \u2264 c1 \u221a s log p\nn + c2\n\u221a rp\nn , (17)\nwhere c1 = 960\u03baL \u03c3 \u2217 and c2 = 96\u03baL\u03c1 \u2217.\nRemark: The estimation error (17) consists of two terms corresponding to the sparse and low-rank components, respectively. Note its resemblance to the error\nbounds of robust PCA (e.g., Agarwal et al. (2012); Yang & Ravikumar (2013)) and the derived bound in Chandrasekaran et al. (2012). In particular, the first term in (17) was on the same order as the estimation error of a sparse GGM (Ravikumar et al., 2011). However, due to the presence of latent variables, both the sample requirement (i.e., n & p) and the combined error bound are worse than those for learning the sparse conditional GGM.\nNext we consider a scenario under which this additional disadvantage is largely removed. Assume that the true marginal covariance matrix \u03a3\u2217 has an effective rank reff := reff(\u03a3\u2217) (recall reff(\u03a3\u2217) := tr(\u03a3\u2217)/\u2016\u03a3\u2217\u20162 ) that is much smaller than p. Then, by using recent advances on the asymptotic behavior of the sample covariance matrix (Lounici, 2012), we can obtain a much tighter bound which only depends on p logarithmically, as stated in the following theorem.\nTheorem 2. Let the same assumptions in Theorem 1 hold. Given a constant C1 > 1, assume that the number of observations n satisfies n \u2265 max { 4C1 log p, C3reff log 2(2p) }\n, and the regularization parameters satisfy\n\u03bb = 160C1\u03c3 \u2217\n\u221a log p\nn and \u00b5 = C4\u03c1\u2217\n\u221a reff log p\nn , (18)\nwhere \u03c3\u2217 = maxi \u03a3\u2217i,i, \u03c1 \u2217 = \u2016\u03a3\u2217\u20162, and C3, C4 > 0 are sufficiently large constants. Then with probability at least 1\u2212 2p\u22122(C1\u22121) \u2212 (2p)\u22121, we have\n\u2016\u0398\u0302\u2212\u0398\u2217\u2016F \u2264 c\u03031 \u221a s log p\nn + c\u03032\n\u221a reff \u00b7 r log(2p)\nn , (19)\nwhere c\u03031 = 960\u03baL \u03c3 \u2217, c\u03032 = 8C43\u03baL\u03c1 \u2217.\nProof sketch. Same as Corollary 1, we need to verify that the choices of regularization parameters (18) satisfy the condition (11) with high probability. Since the choice of \u03bb has been verified in Corollary 1, it only remains to verify the condition on \u00b5. To this end, we make use of the following sharp bound on the spectral norm deviation of the sample covariance matrix:\nLemma 1 (Lounici (2012)). Let \u03a3\u0302 be a sample covariance matrix constructed from n i.i.d. samples from a p-dimensional Gaussian distributionN (0,\u03a3\u2217). Then with probability at least 1\u2212 (2p)\u22121,\n\u2016\u03a3\u0302\u2212\u03a3\u2217\u20162 \u2264 C\u2016\u03a3\u2217\u20162 max {\u221a 2reff log(2p)\nn , 2reff log(2p)(3/8 + log(2pn) n\n} ,\nwhere C > 0 is an absolute constant.\nThen as commented in Lounici (2012) (Prop. 3), when the sample size n is sufficiently large such that n \u2265 C3reff log2 max{2p, n}, where C3 > 0 is a large constant, the choice of regularization parameter \u00b5 as in (18) suffices for the condition (11) to hold with high probability.\nNotice that when reff p, the error bound (19) is significantly tighter than the bound (17). Also the sample requirement n & reff log(p) is much milder. This result implies the efficiency of LVGGM learning when the true covariance model has a low effective rank."}, {"heading": "5 Experiments", "text": "We use a set of simulations on synthetic data to verify our reduced effective rank assumption on the covariance matrix of LVGGM, and the derived error bounds in Theorem 2."}, {"heading": "5.1 Effective Rank of Covariance of LVGGM", "text": "To better understand the effective rank of the covariance matrix of LVGGM, it is convenient to consider a hierarchical generating process for the observed variables: xO \u223c AxL + z, where xL \u223c N (0,\u2126L,L) are the latent variables, A := J\u22121O,OJO,L \u2208 Rp\u00d7r, and z \u223c N (0,S\u22121) captures the conditional effects. The marginal covariance matrix of the observed variables can be represented as\n\u03a3 = A\u2126L,LA T\n\ufe38 \ufe37\ufe37 \ufe38 G\n+S\u22121, (20)\nwhere G is a low-rank covariance matrix (global effects), and S\u22121 is a non-sparse covariance matrix (conditionally local effects) whose inverse is sparse. While the low-rank global effects naturally result in a concentrated spectrum, the sparseinverse local effects generally contribute to a diffuse spectrum. The effective rank, which is the sum of all eigenvalues divided by the magnitude of the largest one, depends on the relative energy ratio between G and S\u22121.\nSince an exact characterization of the effective rank in terms of A, \u2126L,L, and S tends to be difficult, we use Monte Carlo simulations to investigate synthetic LVGGM that conform to our assumptions. We generate LVGGM with independent latent variables (i.e., diagonal JL,L), dense latent-observed submatrix JL,O, and a sparse conditional GGM JO,O for observed variable with a random sparsity pattern (sparsity level \u2248 5%). We fix the number of latent variables to be 10, and vary the number of observed variables p = {80, 120, 200, 500}. By scaling the magnitudes of the elements in the latent variable submatrix, we sweep through\nthe relative energy ratio between the global and local factors, i.e., Tr(G)/Tr(S\u22121) from 0.1 to 10. After 550 realizations for each value of p, we plot the empirical effective ranks of observed covariance matrices in Figure 2.\nAs seen in the figure, when the global factor dominates (i.e., the ratio is large), the effective rank of the covariance matrix is very small, as expected. On the other hand, when the local effects become stronger (e.g., when the number of observed variables p increases) the effective rank increases, but at a very mild rate. In particular, when p increases from 80 to 500, the maximum empirical effective rank in our simulation only increases from 4 to 26. For all of our simulated LVGGM, the empirical effective ranks are observed as at least an order of magnitude smaller than p. This mild growing rate of the effective rank (compared to p) will lead to our improved error bound in Theorem 2 to hold."}, {"heading": "5.2 Frobenius Norm Error of LVGGM Estimation", "text": "We simulate LVGGM data with number of observed variables p = {160, 200, 320, 400} and number of latent variables in the set r = {0.1, 0.15, 0.2, 0.3}p. The sparse conditional GGM is a chain graph whose associated precision matrix is tridiagonal with off-diagonal elements Si,i\u22121 = Si,i+1 = 0.4Si,i for i = {2, . . . , p \u2212 1}. For each configuration of p and r, we draw n samples from the LVGGM, where n ranges from 200 to 1000. Using these samples, the precision matrix \u0398\u0302 is learned by solving the regularized ML estimation problem (5). As shown in Section 5.1, the effective rank of the covariance matrix grows mildly. Then Theorem 2 pre-\ndicts that the Frobenius error of the estimated precision matrix of LVGGM should scale as \u2016\u0398\u0302 \u2212 \u0398\u2217\u2016F \u221a (s log(p) + r log(2p))/n, when the regularization pa-\nrameters are chosen such that \u03bb \u03c3\u2217 \u221a\nlog(p) n\nand \u00b5 \u03c1\u2217 \u221a\nreff log(p) n . Guided by\nthis theoretical result, we set the regularization parameters as \u03bb = Ca\u03c3\u2217 \u221a log(p) n\nand \u00b5 = Cb\u03c1\u2217 \u221a reff log(p) n\n, where constants Ca and Cb are cross-validated and then fixed for all test data sets with different configurations. We plot the Frobenius estimation errors against the rescaled sample size n/(s log(p) + r log(2p)) in Figure 3. With a wide range of configurations, almost all the empirical error curves for models align and have the form of f(t) \u221d t\u22121/2 when the sample size is rescaled, as predicted by Theorem 2. In practice when the true model is unknown, one could set the regularization parameters according to the sample versions of the quantities \u03c3\u2217 and \u03c1\u2217, as discussed in Lounici (2012)."}, {"heading": "6 Conclusions", "text": "We consider a family of latent variable Gaussian graphical model whose precision matrix has a sparse plus low-rank structure. We derive parameter error bounds for regularized maximum likelihood estimation. Future work includes extending the framework to other distributions and the application to tasks such as prediction and detection."}, {"heading": "Acknowledgement", "text": "This work is in part supported by ARO grant W911NF-11-1-0391. The authors thank the anonymous reviewers for their valuable comments, along with Jason Lee and Yuekai Sun for helpful discussions."}, {"heading": "A Motivating Real-World Examples", "text": "In this section, we use two real-world examples, movie rating and stock return price data sets, to motivate the LVGGM. For each data set, we manually choose three groups of variables where variables in one group are related. Effectively we have injected certain global effects, i.e., group effect, in the data. According to the decomposition of covariance matrix of a LVGGM (see Eq. (20)), we examine whether these effects can be extracted using a low-rank component G in the covariance matrix, and whether the remaining residual effects have a precision matrix S that is sparser than its inverse S\u22121.\nWe emphasize that for these two examples we are using eigen-decomposition to decompose the covariance matrix into two components. However, this is not related to the regularized ML estimation algorithm proposed in Section 3.4. The low-rank and sparse components that would be learned from the regularized ML problem are different to what we are showing here.\nMovielens data. Using the Movielens1 movie rating data set, we choose the rating scores given by the most active 600 users and for the highest rated 20 movies from each of the following three genres: Horror, Children\u2019s, and Action. This results in a 600\u00d7 60 rating matrix with 56% completeness. We consider the joint distribution of 60 movie rating variables as a LVGGM with three latent variables. Each user\u2019s rating vector is treated as an i.i.d. sample from the LVGGM. Since the true covariance matrix is unknown, we use the sample covariance matrix as a proxy (as n p). Each covariance element is weighted by the actual number of observations to compensate for the missingness in the data.\nTo validate this intuition, we decompose the rating matrix into two matrices: a rank-3 matrix spanned by the top three leading singular vectors, and a residual matrix capturing the conditional effects. We denote the covariance matrix of the low-rank component as G\u0303, and the sparse precision matrix of the residual component as S\u0303. A heat map of the normalized G\u0303 is shown in Figure 4(a), and the sparsity patterns of the normalized S\u0303 and S\u0303\u22121 (i.e., the covariance of the residual) are shown in Figure 4(b), thresholded by 0.1. As expected, the low-rank G\u0303 captures the structure of the global effects (i.e., genre), and the residual can be well-modeled by a sparse GGM \u2013 as we observe that the precision matrix is much sparser than the covariance matrix. In addition, we find the effective rank of the covariance is equal to 7.4, much smaller than the number of variables, 60.\nStock return data. Next, we validate the LVGGM assumptions on a monthly 1http://movielens.org\nstock return data set2, which consists of 216 samples of 24 stocks from three sectors: Technologies, Industrials, and Financials. Similar to the Movielens data, we reconstruct the low-rank component matrix G\u0303 for the global effects (with rank = 4), and the sparse precision matrix S\u0303 for the residual. The heat map of G\u0303 and the sparsity patterns of S\u0303 and S\u0303\u22121 are shown in Figure 4(c) and 4(d), respectively. Again, the global structure (i.e., sector) is manifested in the low-rank matrix, and the conditional effects have a much sparser precision matrix than the covariance. We find the effective rank is equal to 2.9, which again is much smaller than the total number of variables, 24.\n2http://people.csail.mit.edu/myungjin/latentTree.html"}, {"heading": "B Proof of Theorem 1", "text": "In Yang & Ravikumar (2013), the authors proved a general superpositioned parameter estimate error bound using the decomposable regularized framework. Theorem 1 can be proven similarly by specializing the result in Yang & Ravikumar (2013) to the LVGGM learning problem (5). Then it suffices to verify the two critical conditions (C3) and (C4) in Yang & Ravikumar (2013) (the other two conditions are trivial to verify for our problem), which we introduce and elaborate in this section.\nRestricted strong convexity. Let \u03b4L(\u2206; \u0398\u2217) denote the remainder term in firstorder Taylor series approximation of the loss function L(\u00b7) at the true parameter \u0398\u2217 with respect to a perturbation \u2206 = \u0398\u2217 \u2212 \u0398\u0302:\n\u03b4L(\u2206; \u0398\u2217) := L(\u0398\u2217 + \u2206)\u2212 L(\u0398\u2217)\u2212 \u3008\u2207L(\u0398\u2217),\u2206\u3009. (21)\nIn Negahban et al. (2012), the authors introduce the restricted strong convexity (RSC) condition, which specifies that given some set C \u2286 Rp\u00d7p, there exists some curvature parameter \u03baL > 0 and tolerance function \u03c4L, such that the following holds:\n\u03b4L(\u2206; \u0398\u2217) \u2265 \u03baL\u2016\u2206\u20162F \u2212 \u03c4L(\u0398\u2217), \u2200\u2206 \u2208 C. (22)\nThe RSC condition guarantees sufficient curvature of the loss function at the true parameter along some directions specified by set C. This condition is critical for consistent estimation in the high-dimensional regime, since standard strong convexity usually does not hold in the p n setting.\nThe following shows that the restricted Fisher eigenvalue conditions defined in Assumption 1 implies the RSC condition.\nLemma 2 (RSC condition). Suppose Assumption 1 holds for the true marginal precision matrix \u0398\u2217 and let M > 2. Then for all \u2206 \u2208 C(E) \u222a C(U), such that \u2016\u2206\u20162F\u2217 \u2264 12M2 , the RSC condition (22) is satisfied with the curvature parameter \u03baL = M\u22122 2(M\u22121)\u03ba \u2217 min and the tolerance function \u03c4L = 0.\nThe proof of Lemma 2 is largely inspired by Kakade et al. (2010), in which it is shown that exponential family distributions exhibit almost strong convexity in a neighborhood. The RFE assumption makes connection between this property and the RSC condition. A proof of Lemma 2 is given in the Appendix C.\nNote there is an important difference between the RSC condition considered here and the condition introduced in Agarwal et al. (2012). The RSC condition considered here is satisfied with respect to the error matrices of each simple structure separately, while the RSC condition in Agarwal et al. (2012) is required for\nthe combined error matrices (defined in the product space of two sets), which could lie in a significantly larger set.\nStructural incoherence. The second ingredient for consistent estimation of the sparse plus low-rank parameter \u0398, is some type of incoherence condition between the sparse and low-rank components. In the present work, we consider the structural incoherence condition that was proposed more recently in Yang & Ravikumar (2013). This condition allows for a vanishing error bound when n goes to infinity, and is applicable to more general loss functions, such as the log-likelihood function in Eq. (4).\nDefine the following incoherence measure of the loss function L for two structural error matrices \u2206S and \u2206L:\ncL(\u2206S,\u2206L; \u0398 \u2217) := |L(\u0398\u2217 + \u2206S + \u2206L) + L(\u0398\u2217)\n\u2212 L(\u0398\u2217 + \u2206S)\u2212 L(\u0398\u2217 + \u2206L)|,\u2200\u2206S \u2208 C(E),\u2206L \u2208 C(U).\nThen the structural incoherence (SI) condition is satisfied if the following relation holds for all \u2206S \u2208 C(E) and \u2206L \u2208 C(U):\ncL(\u2206S,\u2206L; \u0398 \u2217) \u2264 \u03baL\n2 \u2016(\u2016\u2206S\u20162F + \u2016\u2206L\u20162F ), (23)\nwhere \u03baL is the curvature parameter in the RSC condition (22). The following lemma shows that, in addition to the restricted Fisher eigenvalue assumption (Assumption 1), if the true marginal model also satisfies the structural Fisher incoherence assumption (Assumption 2), then the above SI condition on the likelihood loss function is guaranteed.\nLemma 3 (SI condition). Suppose Assumption 1 and 2 hold for the true marginal precision matrix \u0398\u2217 and let M > 6. Then the SI condition (23) is satisfied for all \u2206S \u2208 C(E) and \u2206L \u2208 C(U), such that max{\u2016\u2206S\u20162F\u2217 , \u2016\u2206L\u20162F\u2217} \u2264 16M2 . The curvature parameter \u03baL is the same as in Lemma 2, i.e., \u03baL = M\u221222(M\u22121)\u03ba \u2217 min.\nThe proof of Lemma 3 is in Section D in Appendix. Finally, under Assumption 1 and Assumption 2, Lemma 2 and 3 imply the RSC and SI conditions hold for our LVGGM learning problem, respectively. Thus Theorem 1 can be proven by directly appealing to Theorem 1 in Yang & Ravikumar (2013)."}, {"heading": "C Proof of Lemma 2", "text": "Proof. The remainder term in the first-order Taylor series of the negative loglikelihood (4) of GGM takes the following form:\n\u03b4L(\u2206; \u0398\u2217) = L(\u0398\u2217 + \u2206)\u2212 L(\u0398\u2217)\u2212 \u3008\u2207L(\u0398\u2217),\u2206\u3009 = \u3008\u0398\u2217\u22121,\u2206\u3009 \u2212 log det(\u0398\u2217 + \u2206) + log det(\u0398\u2217).\nFor s \u2208 (0, 1], define the Taylor series of function g(s; \u0398\u2217) := log det(\u0398\u2217 + s\u2206) at \u0398\u2217\ng(s; \u0398\u2217) = log det(\u0398\u2217 + s\u2206) = \u221e\u2211\nk=0\nck(\u2206)s k\nk! , (24)\nwhere ck(\u2206) := g(k)(s; \u0398\u2217) is the k-th derivative of the log det function at \u0398\u2217. Define c0(\u2206) := log det(\u0398\u2217), the remainder can be expressed as:\n\u03b4L(s\u2206; \u0398\u2217) = \u221e\u2211\nk=2\nck(\u2206)s k\nk! = c2(\u2206)s\n2 2 + \u221e\u2211\nk=3\nck(\u2206)s k\nk! = c2(\u2206)s\n2\n2 + \u03b4g(s; \u2206,\u0398\u2217),\n(25)\nwhere the second term \u03b4g(s) is defined as the second-order Taylor error of the log-determinant function. Next we show that this error term, which is the sum of all the higher-order terms, can be bounded by a quadratic term in a small neighborhood around \u0398\u2217.\nFor exponential family distributions (Gaussian as an example), the log-partition function (i.e., log det function for Gaussian) coincides with the cumulant generating function. This implies that the derivatives ck(\u2206) are the corresponding cumulants of the distribution, which can be shown to converge to zero quite rapidly. Indeed, in Kakade et al. (2010) the authors show that for a univariate random variable z under an exponential family distribution, its k-th order cumulant satisfies\n\u2223\u2223\u2223\u2223 ck(z)\nc2(z)k/2\n\u2223\u2223\u2223\u2223 \u2264 1\n2 k!\u03b1k\u22122, \u2200k \u2265 3, (26)\nwhere \u03b1 is a finite constant, and the second-order cumulant coincides with the Fisher norm of the deviation c2(\u2206) = \u2016\u2206\u20162F\u2217 due to the definition of the Fisher information. For multivariate Gaussian distributions, \u03b1 = \u221a 2 suffices for the above relation to hold (see Sec. 3.2.2 in Kakade et al. (2010)).\nTherefore we bound the second-order Taylor error term in Eq. (25) as follows (similar to Kakade et al. (2010)):\n|\u03b4g(s; \u2206,\u0398\u2217)| = \u2223\u2223\u2223\u2223\u2223 \u221e\u2211\nk=3\nck(\u2206)s k\nk!\n\u2223\u2223\u2223\u2223\u2223 (27)\n\u2264 1 2\n\u221e\u2211\nk=3\n2 k 2 \u22121c2(\u2206) k/2sk (28)\n\u2264 s 2c2(\u2206)\n2\n\u221e\u2211\nk=1\n(s \u221a 2c2(\u2206)) k (29)\n(i) \u2264 s 2c2(\u2206)\n2\n\u221e\u2211\nk=1\n1\nMk (30)\n= s2c2(\u2206)\n2(M \u2212 1) (31)\n\u2264 c2(\u2206) 2(M \u2212 1)\n1\nmax{2M2c2(\u2206), 1} (32)\n(ii) =\nc2(\u2206)\n2(M \u2212 1) (33)\nwhere (i) and (ii) are due to our conditions on c2(\u2206) (i.e., \u2016\u2206\u20162F\u2217 \u2264 12M2 ) and s \u2264 1. Then we obtain a lower bound for \u03b4L(\u2206; \u0398\u2217):\n\u03b4L(\u2206; \u0398\u2217) \u2265 c2(\u2206) 2\n+ \u03b4g(s; \u2206,\u0398\u2217) \u2265 ( 1\n2 \u2212 1 2(M \u2212 1)\n) c2(\u2206) (ii)\n\u2265 M \u2212 2 2(M \u2212 1)\u03ba \u2217 min\u2016\u2206\u20162F ,\n(34)\nwhere (ii) is due to the RFE condition. Therefore the RSC condition is satisfied with the curvature parameter \u03baL := M\u221222(M\u22121)\u03ba \u2217 min and a zero tolerance parameter \u03c4L = 0."}, {"heading": "D Proof of Lemma 3", "text": "Proof. First we state the following lemma which gives a bound on the magnitude of Fisher inner product between elements from the two sets.\nLemma 4. Suppose Assumption 1 and 2 hold for the true marginal precision matrix \u0398\u2217. Then given a constant M \u2265 6, the following inequality holds for all \u2206S \u2208 C(E) and \u2206L \u2208 C(U) such that max{\u2016\u2206S\u20162F\u2217 , \u2016\u2206L\u20162F\u2217} \u2264 16M2 :\n|\u3008\u2206S,\u2206L\u3009F\u2217| \u2264 \u03c8 ( \u2016\u2206S\u20162F\u2217 + \u2016\u2206L\u20162F\u2217 ) , (35)\nwhere \u03c8 := 1 4 \u2212 3 2M .\nThe proof of Lemma 4 follows similarly as that of the Proposition 2 in Yang & Ravikumar (2013), and hence is omitted.\nNext we prove Lemma 3 using the above result. Following similar derivations as in the proof of Lemma 2, the incoherence measure in the SI condition can be simplified to\ncL(\u2206S,\u2206L; \u0398 \u2217) := |\u03b4L(\u2206S + \u2206L; \u0398\u2217)\u2212 \u03b4L(\u2206S; \u0398\u2217)\u2212 \u03b4L(\u2206L; \u0398\u2217)| .\nUsing the remainder in the Taylor series of \u03b4L (25), the incoherence measure can be expressed as:\ncL(\u2206S,\u2206L; \u0398 \u2217)\n= \u2223\u2223\u2223\u2223 c2(\u2206S + \u2206L)\n2 + \u03b4g(s; \u2206S + \u2206L)\u2212\n( c2(\u2206S)\n2 + \u03b4g(s1; \u2206S)\n) \u2212 ( c2(\u2206L)\n2 + \u03b4g(s2; \u2206L)\n)\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223 c2(\u2206S + \u2206L) 2 \u2212 c2(\u2206S) 2 \u2212 c2(\u2206L) 2 \u2223\u2223\u2223\u2223+ |\u03b4g(s; \u2206S + \u2206L)|+ |\u03b4g(s1; \u2206S)|+ |\u03b4g(s2; \u2206L)| (i) \u2264|\u3008\u2206S,\u2206L\u3009F\u2217|+ c2(\u2206S + \u2206L) + c2(\u2206S) + c2(\u2206L)\n2(M \u2212 1)\n=|\u3008\u2206S,\u2206L\u3009F\u2217|+ \u2016\u2206S\u20162F\u2217 + \u2016\u2206L\u20162F\u2217 + \u3008\u2206S,\u2206L\u3009F\u2217 M \u2212 1 \u2264 M M \u2212 1 |\u3008\u2206S,\u2206L\u3009F\u2217|+ \u2016\u2206S\u20162F\u2217 + \u2016\u2206L\u20162F\u2217 M \u2212 1\n(ii) \u2264M\u03c8 + 1 M \u2212 1 (\u2016\u2206S\u2016 2 F\u2217 + \u2016\u2206L\u20162F\u2217)\n(iii) \u2264 M \u2212 2 4(M \u2212 1)\u03ba \u2217 min(\u2016\u2206S\u20162F + \u2016\u2206L\u20162F )\n\u2264\u03baL 2 (\u2016\u2206S\u20162F + \u2016\u2206L\u20162F ),\nwhere in (i) we have apply (33) to bound the second-order Taylor error terms (note that the conditions on the error matrices also guarantees \u2016\u2206S+\u2206L\u20162F\u2217 \u2264 12M2 due to Lemma 4). Inequality (ii) is due to Lemma 4. Inequality (iii) can be verified by the definitions of \u03c8 and the RSC curvature parameter \u03baL."}, {"heading": "E Proof of Corollary 1", "text": "Proof. Theorem 1 is a deterministic statement, however, the condition on the regularization parameters (11) and the error bound depend on the sample covariance\nmatrix \u03a3\u0302 which is random. Note that the error bound directly follows from the deterministic error bound in Theorem 1 and the choices of regularization parameters as in Eq. (16). To prove Corollary 1, it only remains to verify that the condition (11) in Theorem 1 is guaranteed with high probability. More specifically, this requires bounding the deviation of the sample covariance matrix in terms of `\u221e and and spectral norms.\nFirst we make use of the following lemma to characterize the element-wise deviation of the sample covariance matrix3.\nLemma 5 (Ravikumar et al. (2011)). For a p-dimensional Gaussian random vector with covariance matrix \u03a3\u2217, the sample covariance matrix obtained from n samples \u03a3\u0302 satisfies\nP { |\u03a3\u0302i,j \u2212\u03a3\u2217i,j| > 1 } \u2264 4 exp ( \u2212 n 2 1\n3200\u03c3\u22172\n) , (36)\nfor all 1 \u2208 (0, 40\u03c3), where \u03c3\u2217 := maxi=1,...,p \u03a3\u2217i,i. If the number of samples satisfies n \u2265 4 log p, then by choosing 1\n2 \u03bb \u2265 1 =\n80C1\u03c3 \u2217 \u221a log p2 n \u2208 (0, 40\u03c3), where C1 > 1 is an arbitrary constant, and applying the union bound we have\nP { \u2016\u03a3\u0302\u2212\u03a3\u2217\u2016\u221e \u2264 1\n2 \u03bb\n} \u2265 P { \u2016\u03a3\u0302\u2212\u03a3\u2217\u2016\u221e \u2264 1 } \u2265 1\u2212 4p\u22122(C1\u22121).\nThen the condition on \u03bb is satisfied with high probability. Next we consider the condition on the other regularization parameter \u00b5, which requires bounding the deviation of the operation norm of the sample covariance matrix. The following lemma provides such a characterization.\nLemma 6 (Chandrasekaran et al. (2012), Lemma 3.9). For a p-dimension Gaussian random vector with covariance matrix \u03a3\u2217 and let \u03c1\u2217 = \u2016\u03a3\u2217\u20162. If the number of samples n be such that n \u2265 64p\u03c1\u22172\n22 , then the sample covariance matrix \u03a3\u0302 ob-\ntained from n samples satisfies\nP { \u2016\u03a3\u0302\u2212\u03a3\u2217\u20162 \u2265 2 } \u2264 2 exp ( \u2212 n 2 2\n128\u03c1\u22172\n) , (37)\nfor all 2 \u2208 (0, 8\u03c1\u2217). 3The original lemma applies to all sub-Gaussian variables, here we specialize to Gaussian random vectors.\nIf n \u2265 p, then by choosing 1 2 \u00b5 \u2265 2 = 8C2\u03c1\u2217 \u221a p n \u2208 (0, 8\u03c1\u2217), where C2 \u2265 1 is\nan arbitrary constant, we have\nP { \u2016\u03a3\u0302\u2212\u03a3\u2217\u20162 \u2264 1\n2 \u00b5\n} \u2265 P { \u2016\u03a3\u0302\u2212\u03a3\u2217\u20162 \u2264 2 } \u2265 1\u2212 2 exp ( \u2212C 2 2p\n2\n) .\nCombining the above results we have verified the condition (11) in Theorem 1 holds with high probability, which concludes the proof."}], "references": [{"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "author": ["Agarwal", "Alekh", "Negahban", "Sahand", "Wainwright", "Martin J"], "venue": "The Annals of Statistics,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Bickel", "Peter J", "Ritov", "Yaacov", "Tsybakov", "Alexandre B"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Robust principal component analysis", "author": ["Cand\u00e8s", "Emmanuel J", "Li", "Xiaodong", "Ma", "Yi", "Wright", "John"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Latent variable graphical model selection via convex optimization", "author": ["Chandrasekaran", "Venkat", "Parrilo", "Pablo A", "Willsky", "Alan S"], "venue": "Annals of Statistics,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q1935\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 1935}, {"title": "Robust shrinkage estimation of high-dimensional covariance matrices", "author": ["Chen", "Yilun", "Wiesel", "Ami", "Hero", "Alfred O"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Gaussian multiresolution models: Exploiting sparse Markov and covariance structure", "author": ["Choi", "Myung Jin", "Chandrasekaran", "Venkat", "Willsky", "Alan S"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Choi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2010}, {"title": "Learning latent tree graphical models", "author": ["Choi", "Myung Jin", "Tan", "Vincent YF", "Anandkumar", "Animashree", "Willsky", "Alan S"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "BIG & QUIC: Sparse inverse covariance estimation for a million variables", "author": ["Hsieh", "Cho-Jui", "Sustik", "M\u00e1ty\u00e1s A", "Dhillon", "Inderjit", "Ravikumar", "Pradeep", "Poldrack", "Russell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hsieh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2013}, {"title": "Learning exponential families in high-dimensions: Strong convexity and sparsity", "author": ["Kakade", "Sham", "Shamir", "Ohad", "Sindharan", "Karthik", "Tewari", "Ambuj"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kakade et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2010}, {"title": "Residual component analysis: Generalising PCA for more flexible inference in linear-Gaussian models", "author": ["Kalaitzis", "Alfredo", "Lawrence", "Neil D"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Kalaitzis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalaitzis et al\\.", "year": 2012}, {"title": "Graphical models, volume 17", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Improved estimation of the covariance matrix of stock returns with an application to portfolio selection", "author": ["Ledoit", "Olivier", "Wolf", "Michael"], "venue": "Journal of Empirical Finance,", "citeRegEx": "Ledoit et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ledoit et al\\.", "year": 2003}, {"title": "On model selection consistency of penalized M-estimators: a geometric theory", "author": ["Lee", "Jason", "Sun", "Yuekai", "Taylor", "Jonathan E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Learning Gaussian graphical models with observed or latent FVSs", "author": ["Liu", "Ying", "Willsky", "Alan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "High-dimensional covariance matrix estimation with missing observations", "author": ["Lounici", "Karim"], "venue": "arXiv preprint arXiv:1201.2577,", "citeRegEx": "Lounici and Karim.,? \\Q2012\\E", "shortCiteRegEx": "Lounici and Karim.", "year": 2012}, {"title": "Alternating direction methods for latent variable Gaussian graphical model selection", "author": ["Ma", "Shiqian", "Xue", "Lingzhou", "Zou", "Hui"], "venue": "Neural computation,", "citeRegEx": "Ma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2013}, {"title": "Walk-sums and belief propagation in Gaussian graphical models", "author": ["Malioutov", "Dmitry M", "Johnson", "Jason K", "Willsky", "Alan S"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Malioutov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Malioutov et al\\.", "year": 2006}, {"title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers", "author": ["Negahban", "Sahand N", "Ravikumar", "Pradeep", "Wainwright", "Martin J", "Yu", "Bin"], "venue": "Statistical Science,", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "High-dimensional covariance estimation by minimizing `1-penalized logdeterminant divergence", "author": ["Ravikumar", "Pradeep", "Wainwright", "Martin J", "Raskutti", "Garvesh", "Yu", "Bin"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Sparse permutation invariant covariance estimation", "author": ["Rothman", "Adam J", "Bickel", "Peter J", "Levina", "Elizaveta", "Zhu", "Ji"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Rothman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rothman et al\\.", "year": 2008}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin and Roman.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2010}, {"title": "Robust PCA via outlier pursuit", "author": ["Xu", "Huan", "Caramanis", "Constantine", "Sanghavi", "Sujay"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Dirty statistical models", "author": ["Yang", "Eunho", "Ravikumar", "Pradeep"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "2012), the authors introduce the restricted strong convexity (RSC) condition, which specifies that given some set C \u2286 Rp\u00d7p, there exists some curvature parameter \u03baL > 0 and tolerance function \u03c4L", "author": ["Negahban"], "venue": null, "citeRegEx": "Negahban,? \\Q2012\\E", "shortCiteRegEx": "Negahban", "year": 2012}, {"title": "Note there is an important difference between the RSC condition considered here and the condition introduced in Agarwal et al", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2012\\E", "shortCiteRegEx": "Agarwal", "year": 2012}, {"title": "\u221a 2 suffices for the above relation to hold (see Sec", "author": ["Kakade"], "venue": null, "citeRegEx": "Kakade,? \\Q2010\\E", "shortCiteRegEx": "Kakade", "year": 2010}, {"title": "For a p-dimensional Gaussian random vector with covariance matrix \u03a3\u2217, the sample covariance matrix obtained from n", "author": ["Ravikumar"], "venue": null, "citeRegEx": "Ravikumar,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar", "year": 2011}, {"title": "For a p-dimension Gaussian random vector with covariance matrix \u03a3\u2217 and let \u03c1\u2217", "author": ["Chandrasekaran"], "venue": null, "citeRegEx": "Chandrasekaran,? \\Q2012\\E", "shortCiteRegEx": "Chandrasekaran", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "To perform accurate model parameter estimation and subsequent statistical inference, low dimensional structure is often imposed for regularization (Negahban et al., 2012).", "startOffset": 147, "endOffset": 170}, {"referenceID": 11, "context": "Gaussian graphical models (GGM) provide an efficient representation of the precision matrix through a graph that represents non-zeros in the matrix (Lauritzen, 1996).", "startOffset": 148, "endOffset": 165}, {"referenceID": 8, "context": "On the computational side, sparsity also leads to reduced complexity of the estimator (Hsieh et al., 2013).", "startOffset": 86, "endOffset": 106}, {"referenceID": 5, "context": "Examples are the price of oil on the airlines\u2019 stock price variables (Choi et al., 2010), and the genres on movie rating variables.", "startOffset": 69, "endOffset": 88}, {"referenceID": 9, "context": "By utilizing the almost strong convexity (Kakade et al., 2010) of the log-likelihood, we derive a non-asymptotic parameter error bound for the regularized ML estimator.", "startOffset": 41, "endOffset": 62}, {"referenceID": 18, "context": "Our derived bounds apply to the high-dimensional setting of p n due to restricted strong convexity (Negahban et al., 2012) and certain structural incoherence between the sparse and low-rank components of the precision matrix (Yang & Ravikumar, 2013).", "startOffset": 99, "endOffset": 122}, {"referenceID": 6, "context": "Gaussian graphical models (GGM) provide an efficient representation of the precision matrix through a graph that represents non-zeros in the matrix (Lauritzen, 1996). In high-dimensional regimes, this graph can be forced to be sparse, imposing a low-dimensional structure on the GGM. For sufficiently sparse GGM, statistically consistent estimates of the model structure (i.e., sparsistency) can be achieved (e.g., Ravikumar et al. (2011)).", "startOffset": 149, "endOffset": 439}, {"referenceID": 3, "context": "The resulting marginal precision matrix of the LVGGM has a sparse plus low-rank structure, therefore we consider a regularized maximum likelihood (ML) approach for parameter estimation (previously considered by Chandrasekaran et al. (2012)).", "startOffset": 211, "endOffset": 240}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al.", "startOffset": 198, "endOffset": 221}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al. (2011); Rothman et al.", "startOffset": 198, "endOffset": 246}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al. (2011); Rothman et al. (2008). In particular, the authors of Ravikumar et al.", "startOffset": 198, "endOffset": 269}, {"referenceID": 4, "context": "The problem of learning GGM with sparse inverse covariance matrices using `1regularized maximum likelihood estimation, often referred to as the graphical lasso (Glasso) problem, has been studied in Friedman et al. (2008); Ravikumar et al. (2011); Rothman et al. (2008). In particular, the authors of Ravikumar et al. (2011) study the model selection consistency (i.", "startOffset": 198, "endOffset": 324}, {"referenceID": 4, "context": "Beyond sparse GGM, Choi et al. (2010) propose a multiresolution extension of a GGM augmented with sparse inter-level correlations, while in Choi et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 4, "context": "Beyond sparse GGM, Choi et al. (2010) propose a multiresolution extension of a GGM augmented with sparse inter-level correlations, while in Choi et al. (2011) the authors consider latent tree-structured graphical models.", "startOffset": 19, "endOffset": 159}, {"referenceID": 4, "context": "Beyond sparse GGM, Choi et al. (2010) propose a multiresolution extension of a GGM augmented with sparse inter-level correlations, while in Choi et al. (2011) the authors consider latent tree-structured graphical models. Both models lead to computationally efficient inference and learning algorithms but restrict the latent structure to trees. Recently, Liu & Willsky (2013) consider a computationally efficient learning algorithm for a class of conditionally tree-structured LVGGM.", "startOffset": 19, "endOffset": 376}, {"referenceID": 3, "context": "The work that is most relevant to ours is by Chandrasekaran et al. (2012), who study the LVGGM learning problem, but focus on the simultaneous model selection consistency of both the sparse and low-rank components.", "startOffset": 45, "endOffset": 74}, {"referenceID": 3, "context": "The work that is most relevant to ours is by Chandrasekaran et al. (2012), who study the LVGGM learning problem, but focus on the simultaneous model selection consistency of both the sparse and low-rank components. In contrast, in this paper we focus on the Frobenius norm error bounds for estimating the precision matrix of LVGGM. Although structural consistency can be useful for deriving insights, parameter estimation error analysis is of equal or greater importance in practice. Since it provides additional, and usually more direct, insights into factors influencing the performance of the subsequent statistical inference tasks, such as prediction and detection. Also, compared with Chandrasekaran et al. (2012), our Frobenius norm error bounds are derived under mild condition on the Fisher information of the distribution.", "startOffset": 45, "endOffset": 719}, {"referenceID": 2, "context": "We note that there is a fundamentally different line of work on estimating models with a similar structural composition, known as robust PCA (Cand\u00e8s et al., 2011).", "startOffset": 141, "endOffset": 162}, {"referenceID": 22, "context": "This model has been applied to extracting the salient foreground from background in videos, and detecting malicious user ratings in recommender system data (Xu et al., 2012).", "startOffset": 156, "endOffset": 173}, {"referenceID": 2, "context": "We note that there is a fundamentally different line of work on estimating models with a similar structural composition, known as robust PCA (Cand\u00e8s et al., 2011). In robust PCA, the data matrix is modeled as \u201clow-rank plus sparse\u201d. This model has been applied to extracting the salient foreground from background in videos, and detecting malicious user ratings in recommender system data (Xu et al., 2012). In contrast, the equivalent covariance model of our LVGGM can be decomposed into a low-rank plus a dense matrix whose inverse is sparse. A similar covariance model has recently been studied by Kalaitzis & Lawrence (2012), in which an EM algorithm is proposed for estimation but no theoretical error bounds are derived.", "startOffset": 142, "endOffset": 629}, {"referenceID": 17, "context": "The precision matrix parameterization arises in many statistical inference problems for Gaussian distributions, in areas such as belief propagation (Malioutov et al., 2006), linear prediction, portfolio selection in financial data (Ledoit & Wolf, 2003), and anomaly detection (Chen et al.", "startOffset": 148, "endOffset": 172}, {"referenceID": 4, "context": ", 2006), linear prediction, portfolio selection in financial data (Ledoit & Wolf, 2003), and anomaly detection (Chen et al., 2011).", "startOffset": 111, "endOffset": 130}, {"referenceID": 3, "context": "Similar to Chandrasekaran et al. (2012), we consider the following regularized ML estimation problem:", "startOffset": 11, "endOffset": 40}, {"referenceID": 18, "context": "where the corresponding regularization function is the sum of two regularizers: R(\u0398) = \u2016S\u20161 + \u03bc\u03bb\u2016L\u2016\u2217, each of which has been shown to promote sparse (lowrank) structure in S (L, respectively) (Negahban et al., 2012).", "startOffset": 192, "endOffset": 215}, {"referenceID": 16, "context": "Efficient convex solver, such as Ma et al. (2013), can be used to solve.", "startOffset": 33, "endOffset": 50}, {"referenceID": 17, "context": "We adopt the decomposable regularization framework of Negahban et al. (2012); Agarwal et al.", "startOffset": 54, "endOffset": 77}, {"referenceID": 0, "context": "(2012); Agarwal et al. (2012); Yang & Ravikumar (2013) to derive these bounds.", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "(2012); Agarwal et al. (2012); Yang & Ravikumar (2013) to derive these bounds.", "startOffset": 8, "endOffset": 55}, {"referenceID": 18, "context": "We refer the reader to Negahban et al. (2012) for more details.", "startOffset": 23, "endOffset": 46}, {"referenceID": 18, "context": "For the true model parameter \u0398\u2217, we define its associated structural error set with respect to a subspaceM as (Negahban et al., 2012): C(M,M; \u0398\u2217) := { \u2206 \u2208 Rn\u00d7p | R(\u2206M\u22a5) \u2264 3R(\u2206M) + 4R(\u0398 \u2217 M ) } .", "startOffset": 110, "endOffset": 133}, {"referenceID": 9, "context": "Similar to prior work of Kakade et al. (2010), we define the induced Fisher norm of a matrix \u2206 as \u2016\u2206\u2016F\u2217 := vec(\u2206)TF\u2217vec(\u2206) (8)", "startOffset": 25, "endOffset": 46}, {"referenceID": 1, "context": "This RFE condition generalizes the restricted eigenvalue (RE) condition for sparsity-promoting linear regression problems Bickel et al. (2009). It assumes that the minimum eigenvalue of the Fisher information is bounded away from zero along the directions C(E) and C(U).", "startOffset": 122, "endOffset": 143}, {"referenceID": 1, "context": "This RFE condition generalizes the restricted eigenvalue (RE) condition for sparsity-promoting linear regression problems Bickel et al. (2009). It assumes that the minimum eigenvalue of the Fisher information is bounded away from zero along the directions C(E) and C(U). Due to the identity (8) and properties of the Kronecker product, a trivial lower bound for \u03bamin is \u03bb 2 min(\u0398 \u2217), where \u03bbmin(\u00b7) denotes the minimum eigenvalue. In the high-dimensional setting, the RFE parameter \u03bamin, which is defined only with respect to the above restricted set of directions, can be substantially larger than \u03bbmin(\u0398 \u2217). As a result, the derived error bounds, which depend on \u03bamin, are generally tighter than the bounds depending on \u03bbmin(\u0398 \u2217) (cf. Theorem 1). Due to the sparse plus low-rank superpositioned structure, we impose a type of incoherence between the two structural error sets to ensure consistent estimation of the combined model. The incoherence condition will limit the interaction between elements from the two sets. For our problem, such interaction occurs through their inner products with the Fisher information, which motivates the following Structural Fisher Incoherence (SFI) assumption (which generalizes the C-Linear assumption proposed in Yang & Ravikumar (2013)).", "startOffset": 122, "endOffset": 1276}, {"referenceID": 17, "context": "In Ravikumar et al. (2011), a form of irrepresentability condition is assumed, which limits the induced `1 norm of a matrix that is similar to the projected Fisher information onto the sparse matrix subspace pair.", "startOffset": 3, "endOffset": 27}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.", "startOffset": 3, "endOffset": 32}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)).", "startOffset": 3, "endOffset": 273}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)). For model selection consistency, a more general form of irrepresentability has been shown to be necessary for model selection consistency, see Lee et al. (2013) for a recent discussion.", "startOffset": 3, "endOffset": 436}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)). For model selection consistency, a more general form of irrepresentability has been shown to be necessary for model selection consistency, see Lee et al. (2013) for a recent discussion. In contrast to the above line of work, the SFI assumption we make only controls the maximum singular values of the projected Fisher information. This can be explained as we are interested in bounding a weaker quantity, the Frobenius norm of the parameter estimation error, instead of establishing the stronger model selection consistency of Ravikumar et al. (2011) or the algebraic consistency as in Chandrasekaran et al.", "startOffset": 3, "endOffset": 826}, {"referenceID": 3, "context": "In Chandrasekaran et al. (2012), the notion of irrepresentability is extended to two subspace pairs (i.e., sparse and low-rank), but detailed behaviors of the projected Fisher information are controlled (see the main assumption on page 1949 of Chandrasekaran et al. (2012)). For model selection consistency, a more general form of irrepresentability has been shown to be necessary for model selection consistency, see Lee et al. (2013) for a recent discussion. In contrast to the above line of work, the SFI assumption we make only controls the maximum singular values of the projected Fisher information. This can be explained as we are interested in bounding a weaker quantity, the Frobenius norm of the parameter estimation error, instead of establishing the stronger model selection consistency of Ravikumar et al. (2011) or the algebraic consistency as in Chandrasekaran et al. (2012).", "startOffset": 3, "endOffset": 890}, {"referenceID": 9, "context": "To deal with this difficulty, we leverage the almost strong convexity properties (Kakade et al., 2010) to characterize the convergence behavior of the sum of higher-order terms in the Taylor series of the log-likelihood loss function.", "startOffset": 81, "endOffset": 102}, {"referenceID": 23, "context": "The proof is inspired by Yang & Ravikumar (2013), in which a parameter estimation error bound is proven for estimating a class of superpositionstructured parameters, such as sparse plus low-rank, through M-estimation with decomposable regularizers.", "startOffset": 32, "endOffset": 49}, {"referenceID": 17, "context": "The RSC condition (which originally proposed in Negahban et al. (2012)) specifies the loss function to be sufficiently curved (i.", "startOffset": 48, "endOffset": 71}, {"referenceID": 17, "context": "The RSC condition (which originally proposed in Negahban et al. (2012)) specifies the loss function to be sufficiently curved (i.e. lower bounded by a quadratic function) along a restricted set of directions (defined by C(E) and C(U)). On the other hand, the SI condition effectively limits certain interaction between elements from the above two structural error sets. In Yang & Ravikumar (2013), under certain C-linear assumptions, the RSC and SI conditions are verified for several problems with quadratic loss functions.", "startOffset": 48, "endOffset": 397}, {"referenceID": 9, "context": "To deal with this difficulty, we leverage the almost strong convexity properties (Kakade et al., 2010) to characterize the convergence behavior of the sum of higher-order terms in the Taylor series of the log-likelihood loss function. We show that in the regime specified by condition (12), the loss function can be wellapproximated by the sum of a quadratic function and a residual term. Under this condition, the RFE assumption (Assumption 1) guarantees the RSC condition (cf. Lemma 2), and the SFI assumption (Assumption 2) leads to SI condition to hold (cf. Lemma 4). Theorem 1 can then be proven by the general theorem in Yang & Ravikumar (2013). A detailed proof of Theorem 1 can be found in Appendix B.", "startOffset": 82, "endOffset": 651}, {"referenceID": 0, "context": "\u2022 We finally remark that the SFI assumption can be relaxed to an even milder incoherence condition, \u2016L\u2016\u221e \u2264 \u03b1, as considered in Agarwal et al. (2012). Following similar derivations as in the proof of Theorem 1, the corresponding error bound can be obtained.", "startOffset": 127, "endOffset": 149}, {"referenceID": 19, "context": "In particular, the first term in (17) was on the same order as the estimation error of a sparse GGM (Ravikumar et al., 2011).", "startOffset": 100, "endOffset": 124}, {"referenceID": 0, "context": ", Agarwal et al. (2012); Yang & Ravikumar (2013)) and the derived bound in Chandrasekaran et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": ", Agarwal et al. (2012); Yang & Ravikumar (2013)) and the derived bound in Chandrasekaran et al.", "startOffset": 2, "endOffset": 49}, {"referenceID": 0, "context": ", Agarwal et al. (2012); Yang & Ravikumar (2013)) and the derived bound in Chandrasekaran et al. (2012). In particular, the first term in (17) was on the same order as the estimation error of a sparse GGM (Ravikumar et al.", "startOffset": 2, "endOffset": 104}], "year": 2014, "abstractText": "Gaussian graphical models (GGM) have been widely used in many highdimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models. In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference.", "creator": "LaTeX with hyperref package"}}}