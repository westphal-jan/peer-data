{"id": "1406.3676", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2014", "title": "Question Answering with Subgraph Embeddings", "abstract": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few hand-crafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields state-of-the-art results on a competitive benchmark of the literature. Finally, we introduce a new system that learns to answer questions from an array of hand-crafted features and to assess whether their information is relevant to a specific language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 14 Jun 2014 03:00:23 GMT  (24kb)", "http://arxiv.org/abs/1406.3676v1", null], ["v2", "Wed, 3 Sep 2014 01:02:11 GMT  (152kb,D)", "http://arxiv.org/abs/1406.3676v2", null], ["v3", "Thu, 4 Sep 2014 00:25:35 GMT  (152kb,D)", "http://arxiv.org/abs/1406.3676v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antoine bordes", "sumit chopra", "jason weston"], "accepted": true, "id": "1406.3676"}, "pdf": {"name": "1406.3676.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Antoine Bordes", "Sumit Chopra", "Jason Weston"], "emails": ["abordes@fb.com", "spchopra@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n36 76\nv1 [\ncs .C\nL ]\n1 4\nJu n\n20 14"}, {"heading": "1 Introduction", "text": "Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been a long standing goal in Artificial Intelligence. With the rise of large scale structured knowledge bases (KBs), this problem, known as open-domain question answering (or open QA), boils down to being able to query efficiently such databases with natural language. These KBs, such as Freebase [3] encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format. However, the scale and the difficulty for machines to interpret natural language still makes this task a challenging problem.\nThe state-of-the-art techniques in open QA can be classified into two main classes, namely, information retrieval based and semantic parsing based. Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer [7,11,13]. On the other hand, semantic parsing methods focus on the correct interpretation of the meaning of a question by a semantic parsing system. A correct interpretation converts a question into the exact database query that returns the correct answer. Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.\nYet, even if both kinds of system have shown the ability to handle largescale KBs, they still require experts to hand-craft lexicons, grammars, and KB schema to be effective. This non-negligible human intervention might not be generic enough to conveniently scale up to new databases with other schema,\nbroader vocabularies or languages other than English. In contrast, [5] proposed a framework for open QA requiring almost no human annotation. Despite being an interesting approach, this method is outperformed by other competing methods. [4] introduced an embedding model, which learns low-dimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of [5] while being able to achieve better prediction performance. However, this approach is only compared with [5] which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods.\nIn this paper, we improve the model of [4] by providing the ability to answer more complicated questions. The main contributions of the paper are: (1) a more sophisticated inference procedure; and (2) a richer representation of the answers which encodes the question-answer path and surrounding subgraph of the KB. Our approach achieves identical performance to the current state-of-the-art on the competitive benchmark WebQuestions [1] without using any lexicon, rules or additional system for part-of-speech tagging, syntactic or dependency parsing during training as most other systems do."}, {"heading": "2 Task Definition", "text": "Our main motivation is to provide a system for open QA able to be trained as long as it has access to: (1) a training set of questions paired with answers and (2) a KB providing a structure among answers. We suppose that all potential answers are entities in the KB and that questions are sequences of words that include one identified KB entity. When this entity is not given, plain string matching is used to perform entity resolution. Smarter methods could be used but this is not our focus.\nWe evaluate our system using the WebQuestions benchmark [1] which is built using Freebase as the KB. Since it contains few training samples, it is impossible to learn on it alone, and this section describes the various data sources that were used for training. These are similar to those used in [2].\nFreebase Freebase [3] is a huge and freely available database of general facts; data is organized as triplets (subject, type1.type2.predicate, object), where two entities subject and object (identified by mids) are connected by the relation type type1.type2.predicate. We used a subset, created by only keeping triples where one of the entities was appearing in either the WebQuestions training/validation set or in ClueWeb extractions. We also removed all entities appearing less than 5 times and finally obtained a Freebase set containing 14M triples made of 2.2M entities and 7k relation types. 1 Since the format of triples does not correspond to any structure one could find in language, we decided to transform them into automatically generated questions. Hence, all triples were converted into questions \u201cWhat is the predicate of the type2 subject?\u201d (using the mid of the subject) with the answer being object.\n1 WebQuestions contains \u223c2k entities, hence restricting Freebase to 2.2M entities does not ease the task for us.\nWebQuestions This dataset, which contains 5,810 question-answer pairs, was introduced in [1] and is used as our evaluation benchmark. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. We used the original split (3,778 examples for training and 2,032 for testing), and isolated 1k questions from the training set for validation. WebQuestions is built on Freebase since all answers are defined as Freebase entities. In each question, we identified one Freebase entity using string matching between words of the question and entity names in Freebase. When multiple entities were retrieved (the same string matches multiple entities or multiple matches were found in the question), only the entity appearing in most triples, i.e. the most popular in Freebase, was kept.\nClueWeb Extractions Freebase data allow to train our model on 14M questions but these have a fixed lexicon and vocabulary, which is not realistic. Following [1], we also created questions using ClueWeb extractions provided by [9]. Using string matching, we ended up with 2M extractions structured as (subject, \u201ctext string\u201d, object) with both subject and object linked to Freebase. We also converted these triples into questions by using simple patterns and Freebase types.\nParaphrases The automatically generated questions that are useful to connect Freebase triples and natural language, do not provide a satisfactory modeling of natural language because of their semi-automatic wording and rigid syntax. To overcome this issue, we follow [5] and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website. On WikiAnswers, users can tag pairs of questions as rephrasings of each other: [5] harvested a set of 2M distinct questions from WikiAnswers, which were grouped into 350k paraphrase clusters."}, {"heading": "3 Embedding Questions and Answers", "text": "Inspired by [4], our model works by learning low-dimensional vector embeddings of words appearing in questions and of entities and relation types of Freebase,\nso that representations of questions and of their corresponding answers are close to each other in the joint embedding space. Let q denote a question and a a candidate answer. Learning embeddings is achieved by learning a scoring function S(q, a), so that S generates a high score if a is the correct answer to the question q, and a low score otherwise. Note that both q and a are represented as a combination of the embeddings of their individual words and/or symbols; hence, learning S essentially involves learning these embeddings. In our model, the form of the scoring function is:\nS(q, a) = f(q)\u22a4g(a). (1)\nLet W be a matrix of Rk\u00d7N , where k is the dimension of the embedding space which is fixed a-priori, and N is the dictionary of embeddings to be learned. Let NW denote the total number of words and NS the total number of entities and relation types. With N = NW +NS , the i-th column of W is the embedding of the i-th element (word, entity or relation type) in the dictionary. The function\nf(.), which maps the questions into the embedding space Rk is defined as f(q) = W\u03c6(q), where \u03c6(q) \u2208 NN , is a sparse vector indicating the number of times each word appears in the question q (usually 0 or 1). Likewise the function g(.) which maps the answer into the same embedding space Rk as the questions, is given by g(a) = W\u03c8(a). Here \u03c8(a) \u2208 NN is a sparse vector representation of the answer a, which we now detail. We consider three different types of representation for a candidate answer, corresponding to different subgraphs of Freebase around it:\n(i) Single Entity. The answer is represented as a single entity from Freebase: \u03c8(a) is a 1-of-N coded vector with 1 corresponding to the entity of the answer, and 0 elsewhere.\n(ii) Path Representation. The answer is represented as a path from the entity mentioned in the question to the answer entity. In our experiments, we considered paths of lengths 1 and 2. This results in a \u03c8(a) which is a 3-of-N or 4-of-N coded vector, expressing the start and end entities of the path and the relation types (but not entities) in-between.\n(iii) Subgraph Representation. We encode both the path representation from (ii), and the entire subgraph of entities connected to the candidate answer entity. That is, for each entity connected to the answer we include both the relation type and the entity itself in the representation \u03c8(a). In order to represent the answer path differently to the surrounding subgraph (so the model can differentiate them), we double the dictionary size for entities, and use one embedding representation if they are in the path and another if they are in the subgraph. Thus we now learn a parameter matrix Rk\u00d7N where N = NW + 2NS.\nOur hypothesis is that including more information about the answer in its representation will lead to improved results. While it is possible that all required information could be encoded in the k dimensional embedding of the single entity (i), it is unclear what dimension k should be to make this possible. For example the embedding of a country entity encoding all of its citizens seems unrealistic. Similarly, only having access to the path ignores all the other information we have about the answer entity, unless it is encoded in the embeddings of either the entity of the question, the answer or the relations linking them, which might be quite complicated as well. We thus adopt the subgraph approach."}, {"heading": "3.1 Training and Loss Function", "text": "As in [12], we train our model using a margin-based ranking loss function. Let D = {(qi, ai) : i = 1, . . . , |D|} be the training set of questions qi paired with their correct answer ai. The loss function we minimize is\n|D|\u2211\ni=1\n\u2211\na\u0304\u2208A\u0304(ai)\nmax{0,m\u2212 S(qi, ai) + S(qi, a\u0304)}, (2)\nwhere m is the margin (fixed to 0.1). Minimizing Eq. (2) learns the embedding matrix W so that the score of a question paired with a correct answer is greater\nthan with any incorrect answer a\u0304 by at least m. a\u0304 is sampled from a set of incorrect candidates A\u0304. This is achieved by sampling 50% of the time from the set of entities connected to the entity of the question (i.e. other candidate paths), and by replacing the answer entity by a random one otherwise. Optimization is accomplished using stochastic gradient descent, multi-threaded with Hogwild! [10], with the constraint that the columns wi of W remain within the unit-ball, i.e., \u2200i, ||wi||2 \u2264 1."}, {"heading": "3.2 Multitask Training of Embeddings", "text": "Since a large number of questions in our training datasets are synthetically generated, they do not adequately cover the range of syntax used in natural language. Hence, we also multi-task the training of our model with the task of paraphrase prediction. We do so by alternating the training of S with that of a scoring function Sprp(q1, q2) = f(q1)\n\u22a4f(q2), which uses the same embedding matrix W and makes the embeddings of a pair of questions (q1, q2) similar to each other if they are paraphrases (i.e. if they belong to the same paraphrase cluster), and make them different otherwise. Training Sprp is similar to that of S except that negative samples are obtained by sampling a question from another paraphrase cluster.\nWe also multitask the training of the embeddings with the mapping of the mids of Freebase entities to the actual words of their names, so that the model learns that the embedding of the mid of an entity should be similar to the embedding of the word(s) that compose its name(s)."}, {"heading": "3.3 Inference", "text": "Once W is trained, at test time, for a given question q the model predicts the answer with:\na\u0302 = argmaxa\u2032\u2208A(q)S(q, a \u2032) (3)\nwhere A(q) is the candidate answer set. This candidate set could be the whole KB but this has both speed and potentially precision issues. Instead, we create a candidate set A(q) for each question.\nWe recall that each question contains one identified Freebase entity. A(q) is first populated with all triples from Freebase involving this entity. This allows to answer simple factual questions whose meaning can be represented by a single triple. This simple strategy is denoted C1. Since a system able to answer only such questions would be limited, we supplement A(q) with examples situated in the KB graph at 2-hops from the entity of the question. We do not add all such examples since this would lead to very large candidate sets. Instead, as in information retrieval approaches, we select a subset of them. Our model first ranks relation types using Eq. (1), i.e. selects which relation types are the most likely to be expressed in q. We keep the top 10 types (10 was selected on the valid. set) and only add candidates to A(q) when these relations appeared in their path. Scores of 1-hop candidates are weighted by 1.5 since they have one less element than 2-hops ones. This strategy, denoted C2, is used by default."}, {"heading": "4 Experiments", "text": "We compare our system in accuracy (proportion of questions with a correct answer returned by Eq. (3)) on the WebQuestions test set with the original model proposed for this dataset in [1] and other systems recently published.2\nThe upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6]. [13] report results that outperform us but this was achieved using a very large additional training data of 1 billion triples extracted from ClueWeb (to be compared with our 2M).\nThe lower part of Table 3 compares various versions of our model. Our default approach uses the Subgraph representation for answers and C2 as candidate answers set. Replacing C2 by C1 induces a drop in performance of more than 5%, because many questions do not have answers that are simple triples (not in C1). However, using all 2-hop connections as a candidate set is also detrimental, because the larger number of candidates confuses (and slows) our ranking based inference. Our results also verify our hypothesis of Section 3, that a richer representation for answers (using the local subgraph) can store more pertinent information. Finally, we demonstrate that we greatly improve upon the model of [4], which actually corresponds to a setting with the Path representation and C1 as candidate set."}, {"heading": "5 Conclusion", "text": "This paper presented an embedding model that learns to perform open QA using training data made of questions paired with their answers and of a KB to provide a structure among answers, and can achieve state-of-the-art performance on the competitive benchmark WebQuestions.\n2 Results of baselines except [4] have been extracted from the original papers. For our experiments, all hyperparameters have been selected on the WebQuestions validation set: k was chosen among {64, 128, 256}, the learning rate on a log. scale between 10\u22124 and 10\u22121 and we used at most 100 paths in the subgraph representation."}], "references": [{"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["J. Berant", "P. Liang"], "venue": "In To appear in Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Open question answering with weakly supervised embedding models", "author": ["A. Bordes", "J. Weston", "N. Usunier"], "venue": "arXiv preprint arXiv:1404.4326,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Paraphrase-driven learning for open question answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In To appear in Proceedings of KDD,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A survey on question answering technology from an information retrieval perspective", "author": ["O. Kolomiyets", "M.-F. Moens"], "venue": "Information Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Entity linking at web scale. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 84\u201388", "author": ["T. Lin", "O. Etzioni"], "venue": "Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. R\u00e9", "S.J. Wright", "F. Niu"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Template-based question answering over rdf data", "author": ["C. Unger", "L. B\u00fchmann", "J. Lehmann", "A.-C. Ngonga Ngomo", "D. Gerber", "P. Cimiano"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["X. Yao", "B. Van Durme"], "venue": "In To appear in Proceedings of ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "These KBs, such as Freebase [3] encompass huge ever growing amounts of information and ease open QA by organizing a great variety of answers in a structured format.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer [7,11,13].", "startOffset": 245, "endOffset": 254}, {"referenceID": 10, "context": "Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer [7,11,13].", "startOffset": 245, "endOffset": 254}, {"referenceID": 12, "context": "Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine-grained detection heuristics to identify the exact answer [7,11,13].", "startOffset": 245, "endOffset": 254}, {"referenceID": 0, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 7, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 1, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 5, "context": "Interestingly, recent works [1,8,2,6] have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs.", "startOffset": 28, "endOffset": 37}, {"referenceID": 4, "context": "In contrast, [5] proposed a framework for open QA requiring almost no human annotation.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "[4] introduced an embedding model, which learns low-dimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of [5] while being able to achieve better prediction performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] introduced an embedding model, which learns low-dimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of [5] while being able to achieve better prediction performance.", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "However, this approach is only compared with [5] which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "In this paper, we improve the model of [4] by providing the ability to answer more complicated questions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "Our approach achieves identical performance to the current state-of-the-art on the competitive benchmark WebQuestions [1] without using any lexicon, rules or additional system for part-of-speech tagging, syntactic or dependency parsing during training as most other systems do.", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "We evaluate our system using the WebQuestions benchmark [1] which is built using Freebase as the KB.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "These are similar to those used in [2].", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "Freebase Freebase [3] is a huge and freely available database of general facts; data is organized as triplets (subject, type1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "WebQuestions This dataset, which contains 5,810 question-answer pairs, was introduced in [1] and is used as our evaluation benchmark.", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "Following [1], we also created questions using ClueWeb extractions provided by [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "Following [1], we also created questions using ClueWeb extractions provided by [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "To overcome this issue, we follow [5] and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "On WikiAnswers, users can tag pairs of questions as rephrasings of each other: [5] harvested a set of 2M distinct questions from WikiAnswers, which were grouped into 350k paraphrase clusters.", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "Inspired by [4], our model works by learning low-dimensional vector embeddings of words appearing in questions and of entities and relation types of Freebase,", "startOffset": 12, "endOffset": 15}, {"referenceID": 11, "context": "As in [12], we train our model using a margin-based ranking loss function.", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "Optimization is accomplished using stochastic gradient descent, multi-threaded with Hogwild! [10], with the constraint that the columns wi of W remain within the unit-ball, i.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": ", 2013 [1] 31.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": ", 2014 [4] 31.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": ", 2014 [6] 35.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "0% Bernat and Liang, 2014 [2] 39.", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "Yao and Van Durme, 2014 [13] 36.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "(3)) on the WebQuestions test set with the original model proposed for this dataset in [1] and other systems recently published.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 12, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "The upper part of Table 3 indicates that our approach performs as well as the recent state-of-the-art approach of [2] and superior to [13], [1] and [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 12, "context": "[13] report results that outperform us but this was achieved using a very large additional training data of 1 billion triples extracted from ClueWeb (to be compared with our 2M).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Finally, we demonstrate that we greatly improve upon the model of [4], which actually corresponds to a setting with the Path representation and C1 as candidate set.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "2 Results of baselines except [4] have been extracted from the original papers.", "startOffset": 30, "endOffset": 33}], "year": 2017, "abstractText": "This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features. Our model learns low-dimensional embeddings of words and knowledge base constituents; these representations are used to score natural language questions against candidate answers. Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields state-of-the-art results on a competitive benchmark of the literature.", "creator": "LaTeX with hyperref package"}}}