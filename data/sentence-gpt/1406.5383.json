{"id": "1406.5383", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2014", "title": "Noise-Adaptive Margin-Based Active Learning and Lower Bounds under Tsybakov Noise Condition", "abstract": "We present and analyze an adaptive margin-based algorithm that actively learns the optimal linear separator for multi-dimensional data. The algorithm has the capacity of adapting to unknown level of label noise in the underlying distribution, making it suitable for model selection under the active learning setting. Compared to other alternative agnostic active learning algorithms, our proposed method is much simpler and achieves the optimal convergence rate in query budget T and data dimension d, if logarithm factors are ignored. Furthermore, our algorithm can handle classification loss functions other than the 0-1 loss, such as hinge and logistic loss, and hence is computationally feasible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 20 Jun 2014 13:42:30 GMT  (20kb,D)", "http://arxiv.org/abs/1406.5383v1", null], ["v2", "Fri, 6 Mar 2015 20:14:19 GMT  (130kb,D)", "http://arxiv.org/abs/1406.5383v2", null], ["v3", "Mon, 23 Nov 2015 23:09:47 GMT  (140kb,D)", "http://arxiv.org/abs/1406.5383v3", "16 pages, 2 figures. An abridged version to appear in Thirtieth AAAI Conference on Artificial Intelligence (AAAI), which is held in Phoenix, AZ USA in 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yining wang", "aarti singh"], "accepted": true, "id": "1406.5383"}, "pdf": {"name": "1406.5383.pdf", "metadata": {"source": "CRF", "title": "Noise-adaptive Margin-based Active Learning for Multi-dimensional Data", "authors": ["Yining Wang", "Aarti Singh"], "emails": ["ynwang.yining@gmail.com", "aartisingh@cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Active learning is an increasingly popular setting in machine learning that makes use of both labeled and unlabeled data [1, 6, 8]. In this paper we consider the pool-based active learning setting, under which algorithms have access to a pool of unlabeled examples drawn i.i.d. from the underlying distribution and have the capacity to request labels of specific examples. The hope is that by directing label queries to the most informative examples in a feedback-driven way, we might be able to achieve significant improvements in terms of sample complexity over passive learning algorithms. For instance, in the problem of learning linear separators, an exponential improvement in sample complexity could be achieved under the realizable case, where the labels are consistent with the underlying linear classifier [2, 3].\nAlthough there are significant improvements under the realizable case, in practice it is often the case that data labels are corrupted by noise. A popular assumption that characterizes the noise in the underlying label distribution is the Tsybakov noise condition (TNC), which says that the label noise decreases polynomially away from the optimal classification hyperplane. Unlike the realizable case, it is known that for most cases of the TNC condition only a polynomial achievement in sample complexity could be achieved [5, 13].\nRecently there has been substantial development of active learning algorithms for the non-realizable case with the TNC condition [5, 2, 3]. However, most of these algorithms require knowledge of the noise level, which is usually unknown in practice. Although there exist some general-purpose agnostic active learning algorithms that can adapt to some level of noise [1, 12, 9], such algorithms are often quite complicated, requiring computing some estimated bounds on classification error. Furthermore, because these methods are general purpose and are based on disagreement coefficient instead of the concept of margin, their convergence rate is not optimal in data dimension d.\nIn this paper, we present and analyze an adaptive margin-based algorithm that actively learns a linear separator for multi-dimensional data. The algorithm is inspired by recent work of adaptive algorithms for stochastic convex optimization [14] and one-dimensional active threshold learning [15]. The proposed method enjoys the following advantages:\nar X\niv :1\n40 6.\n53 83\nv1 [\nst at\n.M L\n] 2\n0 Ju\nn 20\n- The algorithm adapts to unknown levels of label noise as characterized by the TNC condition. Building on previous work [14, 15], we extend the method to multi-dimensional data and relax the constraint imposed on the TNC noise level parameter \u03b1 (earlier required to be \u2265 2). Given the connection between convex optimization and active learning described in [15, 16], we conjecture that our techniques could also lead to a stochastic convex optimization algorithm that adapts to a broader class of objective functions.\n- The proposed algorithm is margin-based and is much simpler than prior attempts at agnostic active learning. In fact, it can be viewed as a modification of the non-adaptive margin-based algorithm introduced in [2] and does not require computing complex quantities such as upper and lower classification error bounds. Furthermore, our algorithm achieves optimal convergence rate in query budget T and data dimension d if logarithm factors are ignored. The algorithm also works for general loss functions other than 0/1 loss (e.g., hinge loss and logistic loss), which extends its applicability in practice.\n- Our proposed method provides insights into the model selection problem in active learning. Unlike the passive learning case, model selection under the active learning setting is much harder and there are no established ways to address this problem. Our algorithm partly solves the model selection problem because it does not require knowledge of the level of label noise of the underlying distribution, which is usually unknown in practice."}, {"heading": "2 Pool-based active learning for multi-dimensional Data", "text": "In this section, we set up the multi-dimensional pool-based active learning problem considered in this paper and introduce a non-adaptive active learning algorithm presented in [2]. We then present our proposed margin-based active learning algorithm that is tuning-free as it adapts to the label noise and analyze the convergence rate of our proposed algorithm. Throughout the paper, we will use ln(x) to denote the natural logarithm of x and log(x) to denote log2(x)."}, {"heading": "2.1 Problem setup and notations", "text": "In the pool-based active learning setting, an algorithm is given a pool of training data with their labels unrevealed. The algorithm then actively selects data points in the training pool to request their labels. Such selection is feedback-driven and could be based on previously observed labeled and/or unlabeled data points. In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].\nWe assume the data points (x, y) are drawn from an unknown underlying distribution PX\u00d7Y , where X is the instance space and Y is the label space. Furthermore, the instances x are drawn in an i.i.d. manner. In this paper we assume that X \u2286 Rd is the unit ball in Rd and Y = {+1,\u22121}. We also assume that the underlying distribution PX is a uniform distribution over the unit ball in Rd, which is a setting commonly studied in the active learning literature [7, 8, 10].\nThe goal of active learning is to find a classifier f : X \u2192 Y such that the generalization error err(f) = E(x,y)\u223cP [l(f(x), y)] is minimized, where l(f(x), y) is a loss function between the prediction f(x) and the label y. Under the binary classification setting with Y = {+1,\u22121}, the 0/1 classification loss is of interest, where l(f(x), y) = I[yf(x) > 0] with I[\u00b7] the indicator function. In this paper we consider the case where the optimal classifier f\u2217 minimizing the 0/1 generalization error is linear, that is, f\u2217(x) = sgn(w\u2217 \u00b7 x) where w\u2217 \u2208 Rd. Given the optimal classifier f\u2217, we define the excess risk of a classifier f under 0/1 loss as err0/1(f)\u2212 err0/1(f\u2217). Without loss of generality, we assume all linear classifiers f(x) = sgn(w \u00b7 x) have \u2016w\u20162 = 1. We also use B\u03b8(w, \u03b2) to denote the model class {f(x) = w\u2032 \u00b7 x|\u03b8(w\u2032, w) \u2264 \u03b2, \u2016w\u2032\u20162 = 1} consisting of all linear classifiers that are close to w with an angle at most \u03b2. (\u03b8(w\u2032, w) = arccos(w\u2032 \u00b7w) is the angle between w\u2032 and w.)\nIn this paper we focus on non-realizable (noisy) cases, where labels are not assumed to be consistent with the optimal classifier (i.e. Pr[yw\u2217 \u00b7x \u2264 0] > 0). We consider here a noisy model characterized by the Tsybakov noise condition (TNC) along the optimal hyperplane. Various forms of the TNC condition for the one-dimensional and multi-dimensional active learning are explored in [5, 15, 2] and have been increasingly popular in the active learning literature. In this paper, we use the\nfollowing version of the TNC condition: there exists constants 0 < \u00b5 \u2264 M < \u221e such that for all linear classifier 1 w \u2208 Rd, \u2016w\u20162 = 1, we have\n\u00b5 \u00b7 \u03b8(w,w\u2217)1/(1\u2212\u03b1) \u2264 err0/1(w)\u2212 err0/1(w\u2217) \u2264M \u00b7 \u03b8(w,w\u2217)1/(1\u2212\u03b1), (1) with \u03b1 \u2208 [0, 1) a parameter characterizing the noise level in the underlying distribution."}, {"heading": "2.2 A non-adaptive margin-based active learning algorithm", "text": "Before presenting our algorithm, we first present a non-adaptive margin-based algorithm that actively learns a classification hyperplane for multi-dimensional data. The algorithm is derived in [2] and its pseudocode is presented in Algorithm 1.\nAlgorithm 1 A non-adaptive margin-based active learning algorithm for the non-realizable case 1: function NONADAPTIVEACTIVELEARNING( , \u03b4, s,{mk, bk, rk, k}k\u2208Z+ ) 2: Initialize: Pick random w\u03020 with \u2016w\u03020\u20162 = 1. 3: Draw m1 labeled examples from PXY and put them into a working set W . 4: for k = 1 to s do 5: Find w\u0302k \u2208 B\u03b8(w\u0302k\u22121, rk) that minimizes the approximate 0/1 training error\u2211\n(x,y)\u2208W I(yw\u0302k \u00b7 x) +mk k. 6: Clear the working set W . 7: while W contains less than mk+1 data points do 8: Draw sample x from PX . 9: If |w\u0302k \u00b7 x| \u2265 bk, reject x; otherwise, ask for the label of x and put (x, y) into W .\n10: end while 11: end for 12: Output: the final estimate w\u0302s. 13: end function\nThe following theorem from [2] characterizes the convergence rate of Algorithm 1 and shows how the parameters (e.g., mk, bk, rk and k) are chosen given the noise level \u03b1 and \u00b5. Theorem 1 (Theorem 3, [2]). Let d \u2265 4. Assume there exists a optimal classifier f\u2217(x) = sgn(w\u2217 \u00b7 x) that satisfies the TNC condition in Eq. (1). Let \u03b2 = \u00b5 \u00b7 (\u03c0/4)1/(1\u2212\u03b1). Then there exists a constant C such that for any , \u03b4 > 0, < \u03b2/8, using Algorithm 1 with bk = 2 \u2212(1\u2212\u03b1)k\u03c0d\u22121/2 \u221a\n5 + \u03b1k ln 2\u2212 ln\u03b2 + ln(2 + k), rk = 2\u2212(1\u2212\u03b1)k\u22122\u03c0 for k > 1, r1 = \u03c0, k = 2 \u2212\u03b1(k\u22121)\u22124\u03b2/ \u221a 5 + \u03b1k ln 2\u2212 ln\u03b2 + ln(1 + k), and mk = C \u22122k ( d+ ln k\u03b4 ) , after s = dlog(\u03b2/ )e iterations, we find a separator with 0/1 excess error smaller than with probability 1\u2212 \u03b4. Furthermore, for \u03b1 \u2208 (0, 1) to achieve a 0/1 excess error rate the number of label queries required is at least (ignoring dependence on \u03b2) \u2211 kmk = O( \u22122\u03b1 ln(1/ )(d+ ln(s/\u03b4))."}, {"heading": "2.3 Surrogate loss functions", "text": "Finding a linear classifier that minimizes the empirical 0/1 classification error could be difficult because the 0/1 loss function is neither convex nor differentiable. Instead, people often try to minimize a surrogate loss function on the training dataset. Popular choices include hinge loss, logistic loss and exponential loss. In this paper, we consider surrogate loss functions that satisfy the following two properties:\n(A1) There exists a passive learning algorithm that, without assuming TNC, achieves an excess surrogate error bounded by , i.e. with probability > 1\u2212 \u03b4,\nerr(w\u0302)\u2212 err(w\u0303\u2217) \u2264 \u2032\nfor some 0 < \u2032 < 1 that depends on \u03b4, n = T/E, d. Here w\u0302 and w\u0303\u2217 denote minimizers of the empirical and real surrogate loss error, respectively. This holds true, for example, for 0/1 loss (c.f. Thm 8 in [2]) as well as hinge and logistic loss (c.f. [18]) with \u2032 =\nO\u0303\n(\u221a d+1+log(1/\u03b4)\nn\n) .\n1To simplify notations, we will interchangeably call w, f and sgn(f) as linear classifiers.\n(A2) err0/1(w\u0302) \u2212 err0/1(w\u2217) \u2264 err(w\u0302) \u2212 err(w\u0303\u2217), i.e. the excess risk under the loss we are working with upper bounds the excess risk under 0/1 loss. Here w\u2217 denote the minimizer of the real 0/1 loss function. This is true for some surrogate losses including hinge loss, logistic loss, etc [4]."}, {"heading": "2.4 An adaptive margin-based active learning algorithm", "text": "In this section we present a margin-based active learning algorithm that adapts to different noise level parameters \u03b1 in the TNC condition in Eq. (1). The pseudocode of the proposed algorithm is shown in Algorithm 2. Algorithm 2 admits 4 parameters: d is the dimension of the instance space X; T is the sampling budget (i.e., the maximum number of label requests allowed); \u03b4 is a confidence parameter; r \u2208 (0, 1/2) is the shinkage rate for every iteration in the algorithm, and smaller r allows us to adapt to smaller \u03b1 values. The basic idea of the algorithm is to split T label requests into E iterations, using the optimal passive learning procedure within each iteration and reducing the scope of search for the best classifier after each iteration.\nThe key difference between our proposed algorithm and the one presented in Algorithm 1 is that in Algorithm 2 the number of iterations E as well as other parameters (e.g., mk, rk, bk, k) are either not needed or do not depend on the noise level \u03b1, and the number of label queries is divided evenly at each iteration. Another difference is that in our algorithm the sample budget T is fixed while in Algorithm 1 the error rate is known. It remains an open problem whether there exists a tuning-free active learning algorithm when a target error rate instead of query budget T is given [15]. Finally, instead of optimizing an approximate empirical 0/1 error in Algorithm 1, our proposed algorithm works for any surrogate loss functions with properties (A1) and (A2).\nAlgorithm 2 An adaptive active learning algorithm for multi-dimensional data 1: function ADAPTIVEACTIVELEARNING(d, \u03b4, T , r) 2: Initialize: E = 12 log T , n = T/E, \u03b20 = \u03c0, random w\u03020 with \u2016w\u03020\u2016 = 1. 3: for k = 1 to E do 4: Clear the working set W . Set bk\u22121 =\n2\u03b2k\u22121\u221a d\n\u221a E(1 + log(1/r)) if k > 1 and bk\u22121 =\n+\u221e if k = 1. 5: while |W | < n do 6: Draw a sample x from PX . 7: If |w\u0302k\u22121 \u00b7 x| > bk\u22121, reject; otherwise, ask for the label of x, and put (x, y) into W . 8: end while 9: Find w\u0302k \u2208 B\u03b8(w\u0302k\u22121, \u03b2k\u22121) that minimizes the empirical error \u2211 (x,y)\u2208W loss(w \u00b7 xy).\n10: Update: \u03b2k \u2190 r \u00b7 \u03b2k\u22121, k \u2190 k + 1. 11: end for 12: Output: the final estimate w\u0302E . 13: end function\nIn Theorem 2 we show a convergence rate analysis of Algorithm 2. As we can see, the algorithm adapts to all noise level parameters in a certain range. Furthermore, the convergence rate presented in Theorem 2 matches the one in Theorem 1 and is optimal in T and d if logarithm factors are ignored, as we present a lower bound on noisy adaptive learning in Section 4.1. This means our effort to make the active learning algorithm adaptive does not sacrifice its sample complexity. Theorem 2. Fix \u03b4 \u2208 (0, 1). Suppose d \u2265 4, r \u2208 (0, 1/2) and T \u2265 4. Let w\u2217 be the optimal linear classifier and w\u0302E be the estimated classifier output by Algorithm 2. If 1/(1 + log(1/r)) \u2264 \u03b1 < 1, then under properties (A1) and (A2), with probability at least 1\u2212 \u03b4E,\nerr0/1(w\u0302E)\u2212 err0/1(w\u2217) \u2264 \u00b5 r(1\u2212 r) \u00b7 ( r\u00b5 )1/\u03b1 , (2)\nwhere = \u2032 \u221a E(1 + log(1/r)) + 2 \u221a 2 T .\nTypically \u2032 = O\u0303( \u221a\nd+log(1/\u03b4) n ) where n = T/E (for example, when using 0/1, hinge or logistic\nloss in Algorithm 2), and hence using (A2) the algorithm achieves the optimal rate of convergence for 0/1 loss (up to log factors) while being adaptive to unknown label noise parameter \u03b1."}, {"heading": "3 Proof of the main theorem", "text": "In this section we give the proof of Theorem 2. We first present a convergence rate analysis for the optimal passive learning procedure in Lemma 1 and corollary 1. Both Lemma 1 and corollary 1 have the convergence rate O\u0303( \u221a d/n), which is well-known to be the optimal convergence rate when no assumption about the label noise is made. Their proofs are partly inspired by the analysis in [2].\nLemma 1 (Optimal passive learning). Fix k. Let w\u0302k\u22121 be a linear classifier and bk\u22121 be a margin parameter. Suppose Dk = {(xi, yi)}ni=1 is a training data set of size n that satisfies |w\u0302k\u22121 \u00b7 xi| \u2264 bk\u22121 for every data point xi. Suppose w\u0302k, w\u0303\u2217k and w\u2217k are minimizers of the empirical surrogate loss error e\u0302rr on Dk, the true surrogate loss error err and the true 0/1 loss error err0/1 over all linear classifiers with w \u2208 B\u03b8(w\u0302k\u22121, \u03b2k\u22121), 0 < \u03b2k\u22121 < \u03c02 . If d \u2265 4 and\nbk\u22121 \u2265 \u03b3 := 2 sin \u03b2k\u22121\u221ad \u221a lnC + ln(1 + \u221a\nln max(1, cot\u03b2k\u22121)) where C is any constant > 0, then using (A1) and (A2), with probability at least 1\u2212 \u03b4,\nerr0/1(w\u0302k)\u2212 err0/1(w\u2217k) \u2264 \u2032 \u00b7 bk\u22121 \u221a d\n2 \u221a \u03c0 + 2 sin\u03b2k\u22121 C \u00b7 cos\u03b2k\u22121 . (3)\nProof. Define S1 := {x||w\u0302k\u22121 \u00b7 x| \u2264 bk\u22121} and S2 := {x||w\u0302k\u22121 \u00b7 x| > bk\u22121}. Clearly Dk \u2286 S1. Since \u03b8(w\u0302k\u22121, w\u0302k), \u03b8(w\u0302k\u22121, w\u2217k) \u2264 \u03b2k\u22121, by Lemma 7 in [2], we have\nPr[(w\u0302k\u22121 \u00b7 x)(w\u0302k \u00b7 x) < 0, x \u2208 S2] \u2264 sin\u03b2k\u22121 C cos\u03b2k\u22121 ,\nPr[(w\u0302k\u22121 \u00b7 x)(w\u2217k \u00b7 x) < 0, x \u2208 S2] \u2264 sin\u03b2k\u22121 C cos\u03b2k\u22121 .\nAdding the two inequalities we get\nPr[(w\u0302k \u00b7 x)(w\u2217k \u00b7 x) < 0, x \u2208 S2] \u2264 2 sin\u03b2k\u22121 C cos\u03b2k\u22121 .\nSubsequently, using (A2), we have\nerr0/1(w\u0302k)\u2212 err0/1(w\u2217k) \u2264 (err0/1(w\u0302k|S1)\u2212 err0/1(w\u2217k|S1)) Pr[x \u2208 S1] + 2 sin\u03b2k\u22121 C cos\u03b2k\u22121\n\u2264 (err(w\u0302k|S1)\u2212 err(w\u0303\u2217k|S1)) Pr[x \u2208 S1] + 2 sin\u03b2k\u22121 C cos\u03b2k\u22121\nLemma 4 in [2] tells us that Pr[x \u2208 S1] \u2264 bk\u22121 \u221a d\n2 \u221a \u03c0 . And using (A1) completes the proof.\nCorollary 1. Fix k. Let T be the number of samples obtained and E be the number of epoches. Suppose d \u2265 4, T \u2265 4, E = 12 log T , n = T/E. Assume also that \u03b2k = r\nk\u03c0 and bk\u22121 = 2\u03b2k\u22121\u221a\nd\n\u221a E(1 + log(1/r)) for some constant r \u2208 (0, 1/2). With probability at least 1\u2212 \u03b4,\nerr0/1(w\u0302k)\u2212 err0/1(w\u2217k) \u2264 \u03b2k\u22121 \u00b7\n[ \u2032 \u221a E(1 + log(1/r)) + 2 \u221a 2\nT\n] =: \u03b2k\u22121 . (4)\nProof. We first note that Eq. (4) is trivially true for k = 1 because \u03b20 = \u03c0 > 1,\u221a E(1 + log(1/r)) \u2265 1 and err0/1(w\u03021) \u2212 err0/1(w\u22171) \u2264 err(w\u03021) \u2212 err(w\u0303\u22171) \u2264 \u2032 due to properties (A1) and (A2). In the remaining part of the proof we will assume that k \u2265 2, and hence \u03b2k\u22121 \u2208 (0, \u03c0/2). Before continuing the proof, we first give a list of elementary inequalities that will be used in this proof (assuming \u03b2 < \u03c0/2):\nln(1 + x) \u2264 x; sin\u03b2 \u2264 \u03b2; cot\u03b2 \u2264 1/\u03b2; tan\u03b2 \u2264 \u221a 2\u03b2.\nPut C = 2E = \u221a T . Let\u2019s first check bk\u22121 \u2265 \u03b3:\n\u03b3 = 2 sin\u03b2k\u22121\u221a\nd\n\u221a lnC + ln(1 + \u221a ln max(1, cot\u03b2k\u22121))\n\u2264 2\u03b2k\u22121\u221a d\n\u221a lnC + \u221a ln max(1, 1/\u03b2k\u22121) \u2264\n2\u03b2k\u22121\u221a d\n\u221a E + E log(1/r) =: bk\u22121.\nApplying Lemma 1, we have\nerr0/1(w\u0302k)\u2212 err0/1(w\u2217k) \u2264 \u2032 \u03b2k\u22121\u221a \u03c0\n\u221a E(1 + log(1/r)) + 2 \u221a\n2\u03b2k\u22121\u221a T\n\u2264 \u03b2k\u22121 \u00b7 [ \u2032 \u221a E(1 + log(1/r)) + 2 \u221a 2\nT\n] = \u03b2k\u22121 .\nWith the optimal passive learning bounds in Lemma 1 and Corollary 1, we are now able to start the proof of the main theorem. The proof contains three parts: first, we do a case analysis on the TNC constant \u00b5 in Eq. (1) and show that there exists an iteration k\u2217 such that err0/1(w\u0302k\u2217)\u2212 err0/1(w\u2217k\u2217) has the desired convergence rate; second, we prove that w\u2217k is exactly the same as w\n\u2217 in all iterations prior to k\u2217; finally, we prove that after iteration k\u2217 the empirical estimate does not deviate from the optimal classifier much.\nWe first analyze the upper bound of the TNC constant \u00b5. By the TNC condition in Eq. (1) and the fact that \u03b1 < 1, we know that \u00b5 \u2264 \u03c0\u22121/(1\u2212\u03b1) \u2264 \u03c0\u2212\u03b1/(1\u2212\u03b1). Since \u03b1 \u2265 1/(1 + log(1/r)), we have the following upper bound for \u00b5:\n\u00b5 \u2264 2 (log(1/r)\u00b7 \u03b11\u2212\u03b1\u22121)E \u00b7 r\u22121\n\u03c0 \u03b1\n1\u2212\u03b1 =\n2\u2212Er\u22121\n(rE\u03c0)\u03b1/(1\u2212\u03b1) =\n2\u2212Er\u22121\n\u03b2 \u03b1/(1\u2212\u03b1) E\n\u2264 r \u22121\n\u03b2 \u03b1/(1\u2212\u03b1) E\n.\nThe last step holds since \u2265 1/ \u221a T = 2\u2212E by definition.\nGiven the upper bound above, we can do a case analysis on \u00b5. When \u00b5 is sufficiently small, that is,\n\u00b5 \u2264 r \u22121\n\u03b2 \u03b1/(1\u2212\u03b1) 1\n, (5)\nwe are done after the first iteration because by (A1), err0/1(w\u03021)\u2212 err0/1(w\u22171) \u2264 \u03b20 \u2264 r\u22121\u03b21 \u2264 r\u22121 (\nr\u00b5\n)(1\u2212\u03b1)/\u03b1 =\n1/\u03b1\nr1/\u03b1\u00b5(1\u2212\u03b1)/\u03b1\nand with probability \u2265 1 \u2212 \u03b4E we have w\u22171 = w\u2217 and using arguments akin to Lemma 4 (given below), we have\nerr0/1(w\u0302E)\u2212 err0/1(w\u03021) \u2264 r\n1\u2212 r \u03c0 .\nIf Eq. (5) does not hold, then by the upper bound of \u00b5 there must exist an iteration k\u2217 \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , E \u2212 1} such that\nr\u22121\n\u03b2 \u03b1/(1\u2212\u03b1) k\u2217\n\u2264 \u00b5 \u2264 r \u22121\n\u03b2 \u03b1/(1\u2212\u03b1) k\u2217+1\n. (6)\nWe now present Lemma 2 that completes the first step of the framework of our proof. We show that for the iteration k\u2217 defined in Eq. (6), the excess risk err0/1(w\u0302k\u2217) \u2212 err0/1(w\u2217k\u2217) has the desired convergence rate of 1/\u03b1. Lemma 2. Suppose r \u2208 (0, 1/2) and 1/(1 + log(1/r)) \u2264 \u03b1 < 1. With probability at least 1 \u2212 \u03b4, under property (A1)\nerr0/1(w\u0302k\u2217)\u2212 err0/1(w\u2217k\u2217) \u2264 \u03b2k\u2217\u22121 \u2264 1/\u03b1\nr 1+\u03b1 \u03b1 \u00b5 1\u2212\u03b1 \u03b1\n. (7)\nProof. We know that \u03b2k\u2217\u22121 = r\u22122\u03b2k\u2217+1 \u2264 r\u22122 ( r\u00b5 ) 1\u2212\u03b1 \u03b1 . Using (A1), we get\nerr0/1(w\u0302k\u2217)\u2212 err0/1(w\u2217k\u2217) \u2264 \u03b2k\u2217\u22121 \u2264 r\u22122 (\nr\u00b5\n) 1\u2212\u03b1 \u03b1\n= 1/\u03b1\nr 1+\u03b1 \u03b1 \u00b5 1\u2212\u03b1 \u03b1\nThe next observation shows that the optimal classifier w\u2217k at iteration k (which is the optimal classifier within B\u03b8(w\u0302k\u22121, \u03b2k\u22121)) is identical to w\u2217 for all iterations prior to k\u2217. This means that at least at iteration k\u2217, the left-hand side of Eq. (7) is what we want to bound. Lemma 3. With probability at least 1 \u2212 \u03b4E, under properties (A1) and (A2), for all k \u2264 k\u2217, w\u2217k = w \u2217.\nProof. We use induction to prove this lemma. When k = 0 the claim is true because \u03b20 = \u03c0. Now assume the claim is true for k, that is, \u03b8(w\u0302k\u22121, w\u2217) \u2264 \u03b2k\u22121. We want to prove \u03b8(w\u0302k, w\u2217) \u2264 \u03b2k. To see this, we apply the TNC condition in Eq. (1) to get\n\u03b8(w\u0302k, w \u2217) \u2264\n( err0/1(w\u0302k)\u2212 err0/1(w\u2217)\n\u00b5 )1\u2212\u03b1 Therefore, we only need to prove \u00b5 \u2265 (err0/1(w\u0302k)\u2212 err0/1(w\u2217))/\u03b2 1/(1\u2212\u03b1) k .\nBecause of the induction assumption \u03b8(w\u0302k\u22121, w\u2217) \u2264 \u03b2k\u22121, w\u2217 is exactly w\u2217k. Applying Corollary 1 and taking a union bound over all k \u2264 k\u2217 \u2264 E, we know that with probability at least 1 \u2212 \u03b4E, for all k \u2264 k\u2217,\nerr0/1(w\u0302k)\u2212 err0/1(w\u2217) \u03b2 1/(1\u2212\u03b1) k \u2264 \u03b2k\u22121 \u03b2 1/(1\u2212\u03b1) k = r\u22121 \u03b2 \u03b1/(1\u2212\u03b1) k \u2264 r \u22121 \u03b2 \u03b1/(1\u2212\u03b1) k\u2217 .\nThe last expression is \u2264 \u00b5 by applying Eq. (6), and hence the proof is complete.\nFinally, we complete the last step of our proof by observing that for iterations after k\u2217, w\u0302k cannot deviate too much from w\u0302k\u2217 . This assertion is formulated and proved in Lemma 4. Lemma 4. Suppose r \u2208 (0, 1/2). With probability at least 1\u2212 \u03b4E, we have\nerr0/1(w\u0302E)\u2212 err0/1(w\u0302k\u2217) \u2264 r\n1\u2212 r \u03b2k\u2217\u22121 . (8)\nProof. With probability at least 1\u2212 \u03b4E, err0/1(w\u0302E)\u2212 err0/1(w\u0302k\u2217) = E\u2211\nk=k\u2217+1\nerr0/1(w\u0302k)\u2212 err0/1(w\u0302k\u22121) \u2264 E\u2211\nk=k\u2217+1\nerr0/1(w\u0302k)\u2212 err0/1(w\u2217k)\n\u2264 E\u2211\nk=k\u2217+1\n\u03b2k\u22121 = \u03b2k\u2217\u22121 E\u2211 k=k\u2217+1 \u03b2k\u22121 \u03b2k\u2217\u22121\n= \u03b2k\u2217\u22121 E\u2211 k=k\u2217+1 rk\u2212k \u2217 \u2264 \u03b2k\u2217\u22121 (r + r2 + \u00b7 \u00b7 \u00b7 ) \u2264 r 1\u2212 r \u03b2k\u2217\u22121\nCombining Lemmas 2,3,4, we get: err0/1(w\u0302E)\u2212 err0/1(w\u2217) = err0/1(w\u0302E)\u2212 err0/1(w\u0302k\u2217) + err0/1(w\u0302k\u2217)\u2212 err0/1(w\u2217)\n= err0/1(w\u0302E)\u2212 err0/1(w\u0302k\u2217) + err0/1(w\u0302k\u2217)\u2212 err0/1(w\u2217k\u2217) \u2264 r 1\u2212 r \u03b2k\u2217\u22121 + \u03b2k\u2217\u22121\n= 1\n1\u2212 r \u03b2k\u2217\u22121 \u2264\n1/\u03b1\nr 1+\u03b1 \u03b1 (1\u2212 r)\u00b5 1\u2212\u03b1\u03b1\nThus, completing proof of the main theorem."}, {"heading": "4 Discussion and remarks", "text": ""}, {"heading": "4.1 Lower bounds on noisy adaptive learning:", "text": "We present a lower bound on the convergence rate of active learning algorithms, which is derived in [13], and matches the convergence rate in Theorem 2 in T and d, if logarithm factors are ignored. Theorem 3 (Theorem 4.3, [13]). Assume the underlying distribution PXY satisfies the TNC condition in Eq. (1) with \u03b1 \u2208 (0, 1). For any algorithm A that achieves an excess error rate with probability 1\u2212 \u03b4 for sufficiently small , \u03b4 > 0, the number of queries made should be at least\nT = \u2126 ( \u22122\u03b1(d+ log(1/\u03b4)) ) . (9)"}, {"heading": "4.2 Comparison of adaptive active learning algorithms:", "text": "In [12] another noise-robust adaptive learning algorithm is introduced. The algorithm is originally proposed in [9] and is based on the concept of disagreement coefficient introduced in [11]. The algorithm adapts to different noise level \u03b1, and achieves an excess error rate of\nO\n(( \u03b8(d log T + log(1/\u03b4))\nT\n) 1 2\u03b1 ) (10)\nwith probability 1\u2212 \u03b4, where d is the underlying dimensionality, T is the sample query budget and \u03b8 is the disagreement coefficient. Under our scenario where X is the origin-centered unit ball in Rd for d > 2, the hypothesis class C contains all linear separators whose decision surface passes through the origin and PX is the uniform distribution, the diagreement coefficient \u03b8 satisfies [11] \u03c0 4 \u221a d \u2264 \u03b8 \u2264 \u03c0 \u221a d. As a result, our proposed algorithm achieves a polynomial improvement in d in the convergence rate. Such improvements show the advantage of margin-based active learning and were also observed in [3]. Also, our algorithm is considerably much simpler and does not require computing lower and upper confidence bounds on the classification performance."}, {"heading": "4.3 Connection to adaptive convex optimization:", "text": "Our proposed algorithm is inspired by an adaptive algorithm for stochastic convex optimization presented in [14]. A function f is called uniformly convex on a closed convex set Q if there exists \u03c1 \u2265 2 and \u00b5 \u2265 0 such that for all x, y \u2208 Q and \u03b1 \u2208 [0, 1],\nf(\u03b1x+ (1\u2212 \u03b1)y) \u2264 \u03b1f(x) + (1\u2212 \u03b1)f(y)\u2212 1 2 \u00b5\u03b1(1\u2212 \u03b1)\u2016x\u2212 y\u2016\u03c1. (11)\nFurthermore, if \u00b5 > 0 we say the function f is strongly convex. In [14] an adaptive stochastic optimization algorithm for uniformly and strongly convex functions was presented. The algorithm adapts to unknown convexity parameters \u03c1 and \u00b5 in Eq. (11).\nIn [15] a connection between multi-dimensional stochastic convex optimization and onedimensional active learning was established. The TNC condition in Eq. (1) and the strongly convex condition in Eq. (11) are closely related, and the exponents \u03b1 and \u03c1 are tied together in [17]. Based on the connection, a one-dimensional active threshold learner that adapts to unknown TNC noise levels was presented.\nIn this paper, we extend the algorithms presented in [14, 15] to build an adaptive margin-based active learning for multi-dimensional data. Furthermore, note that our proposed algorithm adapts to all noisy level parameters \u03b1 \u2208 (0, 1), which corresponds to convexity parameters \u03c1 > 1. 2 Therefore, we conjecture the existence of similar stochastic optimization algorithms that can adapt to a notion of degree of convexity \u03c1 < 2."}, {"heading": "4.4 Future work:", "text": "Finally, we mention two directions of future work. First, our proposed algorithm adapts to unknown level of label noise when given the query budget T . It is an interesting problem whether there exist adaptive active learning algorithms when the target error rate instead of query budget T is given. In addition, Algorithm 2 fails to handle the case when \u03b1 = 0. Therefore, another possible direction of future work would be to design active learning algorithms that adapts to \u03b1 = 0 while still retaining the exponential improvement on convergence rate for this case, which is observed in previous active learning research [2, 3].\n2The relationship between \u03b1 and \u03c1 can be made explicitly by noting \u03b1 = 1\u2212 1/\u03c1."}], "references": [{"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A. Broder", "T. Zhang"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["M.-F. Balcan", "P. Long"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Convexity, classification, and risk bounds", "author": ["P. Bartlett", "M. Jordan", "J. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Upper and lower error bounds for active learning", "author": ["R. Castro", "R. Nowak"], "venue": "In The 44th Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A. Kalai", "C. Monteleoni"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "The Annals of Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Primal-dual subgradient methods for minimizing uniformly convex functions", "author": ["A. Juditsky", "Y. Nesterov"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Algorithmic connections between active learning and stochastic convex optimization", "author": ["A. Ramdas", "A. Singh"], "venue": "In ALT,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Optimal rates for stochastic convex optimization under tsybakov noise condition", "author": ["A. Ramdas", "A. Singh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Optimal rates for stochastic convex optimization under tsybakov noise condition", "author": ["A. Ramdas", "A. Singh"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Are loss functions all the same", "author": ["L. Rosasco", "E. De Vito", "A. Caponnetto", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Active learning is an increasingly popular setting in machine learning that makes use of both labeled and unlabeled data [1, 6, 8].", "startOffset": 121, "endOffset": 130}, {"referenceID": 5, "context": "Active learning is an increasingly popular setting in machine learning that makes use of both labeled and unlabeled data [1, 6, 8].", "startOffset": 121, "endOffset": 130}, {"referenceID": 7, "context": "Active learning is an increasingly popular setting in machine learning that makes use of both labeled and unlabeled data [1, 6, 8].", "startOffset": 121, "endOffset": 130}, {"referenceID": 1, "context": "For instance, in the problem of learning linear separators, an exponential improvement in sample complexity could be achieved under the realizable case, where the labels are consistent with the underlying linear classifier [2, 3].", "startOffset": 223, "endOffset": 229}, {"referenceID": 2, "context": "For instance, in the problem of learning linear separators, an exponential improvement in sample complexity could be achieved under the realizable case, where the labels are consistent with the underlying linear classifier [2, 3].", "startOffset": 223, "endOffset": 229}, {"referenceID": 4, "context": "Unlike the realizable case, it is known that for most cases of the TNC condition only a polynomial achievement in sample complexity could be achieved [5, 13].", "startOffset": 150, "endOffset": 157}, {"referenceID": 4, "context": "Recently there has been substantial development of active learning algorithms for the non-realizable case with the TNC condition [5, 2, 3].", "startOffset": 129, "endOffset": 138}, {"referenceID": 1, "context": "Recently there has been substantial development of active learning algorithms for the non-realizable case with the TNC condition [5, 2, 3].", "startOffset": 129, "endOffset": 138}, {"referenceID": 2, "context": "Recently there has been substantial development of active learning algorithms for the non-realizable case with the TNC condition [5, 2, 3].", "startOffset": 129, "endOffset": 138}, {"referenceID": 0, "context": "Although there exist some general-purpose agnostic active learning algorithms that can adapt to some level of noise [1, 12, 9], such algorithms are often quite complicated, requiring computing some estimated bounds on classification error.", "startOffset": 116, "endOffset": 126}, {"referenceID": 11, "context": "Although there exist some general-purpose agnostic active learning algorithms that can adapt to some level of noise [1, 12, 9], such algorithms are often quite complicated, requiring computing some estimated bounds on classification error.", "startOffset": 116, "endOffset": 126}, {"referenceID": 8, "context": "Although there exist some general-purpose agnostic active learning algorithms that can adapt to some level of noise [1, 12, 9], such algorithms are often quite complicated, requiring computing some estimated bounds on classification error.", "startOffset": 116, "endOffset": 126}, {"referenceID": 12, "context": "The algorithm is inspired by recent work of adaptive algorithms for stochastic convex optimization [14] and one-dimensional active threshold learning [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "The algorithm is inspired by recent work of adaptive algorithms for stochastic convex optimization [14] and one-dimensional active threshold learning [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Building on previous work [14, 15], we extend the method to multi-dimensional data and relax the constraint imposed on the TNC noise level parameter \u03b1 (earlier required to be \u2265 2).", "startOffset": 26, "endOffset": 34}, {"referenceID": 13, "context": "Building on previous work [14, 15], we extend the method to multi-dimensional data and relax the constraint imposed on the TNC noise level parameter \u03b1 (earlier required to be \u2265 2).", "startOffset": 26, "endOffset": 34}, {"referenceID": 13, "context": "Given the connection between convex optimization and active learning described in [15, 16], we conjecture that our techniques could also lead to a stochastic convex optimization algorithm that adapts to a broader class of objective functions.", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": "Given the connection between convex optimization and active learning described in [15, 16], we conjecture that our techniques could also lead to a stochastic convex optimization algorithm that adapts to a broader class of objective functions.", "startOffset": 82, "endOffset": 90}, {"referenceID": 1, "context": "In fact, it can be viewed as a modification of the non-adaptive margin-based algorithm introduced in [2] and does not require computing complex quantities such as upper and lower classification error bounds.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "In this section, we set up the multi-dimensional pool-based active learning problem considered in this paper and introduce a non-adaptive active learning algorithm presented in [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 0, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 5, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 7, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 4, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 11, "context": "In many problems, the active learning framework can substantially reduce the number of labels needed compared to passive learning methods [1, 6, 8, 5, 12].", "startOffset": 138, "endOffset": 154}, {"referenceID": 6, "context": "We also assume that the underlying distribution PX is a uniform distribution over the unit ball in R, which is a setting commonly studied in the active learning literature [7, 8, 10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 7, "context": "We also assume that the underlying distribution PX is a uniform distribution over the unit ball in R, which is a setting commonly studied in the active learning literature [7, 8, 10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 9, "context": "We also assume that the underlying distribution PX is a uniform distribution over the unit ball in R, which is a setting commonly studied in the active learning literature [7, 8, 10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 4, "context": "Various forms of the TNC condition for the one-dimensional and multi-dimensional active learning are explored in [5, 15, 2] and have been increasingly popular in the active learning literature.", "startOffset": 113, "endOffset": 123}, {"referenceID": 13, "context": "Various forms of the TNC condition for the one-dimensional and multi-dimensional active learning are explored in [5, 15, 2] and have been increasingly popular in the active learning literature.", "startOffset": 113, "endOffset": 123}, {"referenceID": 1, "context": "Various forms of the TNC condition for the one-dimensional and multi-dimensional active learning are explored in [5, 15, 2] and have been increasingly popular in the active learning literature.", "startOffset": 113, "endOffset": 123}, {"referenceID": 1, "context": "The algorithm is derived in [2] and its pseudocode is presented in Algorithm 1.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "The following theorem from [2] characterizes the convergence rate of Algorithm 1 and shows how the parameters (e.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Theorem 1 (Theorem 3, [2]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "Thm 8 in [2]) as well as hinge and logistic loss (c.", "startOffset": 9, "endOffset": 12}, {"referenceID": 16, "context": "[18]) with \u2032 =", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "This is true for some surrogate losses including hinge loss, logistic loss, etc [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "It remains an open problem whether there exists a tuning-free active learning algorithm when a target error rate instead of query budget T is given [15].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "Their proofs are partly inspired by the analysis in [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "Since \u03b8(\u0175k\u22121, \u0175k), \u03b8(\u0175k\u22121, w\u2217 k) \u2264 \u03b2k\u22121, by Lemma 7 in [2], we have Pr[(\u0175k\u22121 \u00b7 x)(\u0175k \u00b7 x) < 0, x \u2208 S2] \u2264 sin\u03b2k\u22121 C cos\u03b2k\u22121 ,", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Lemma 4 in [2] tells us that Pr[x \u2208 S1] \u2264 bk\u22121 \u221a d 2 \u221a \u03c0 .", "startOffset": 11, "endOffset": 14}, {"referenceID": 11, "context": "In [12] another noise-robust adaptive learning algorithm is introduced.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "The algorithm is originally proposed in [9] and is based on the concept of disagreement coefficient introduced in [11].", "startOffset": 40, "endOffset": 43}, {"referenceID": 10, "context": "The algorithm is originally proposed in [9] and is based on the concept of disagreement coefficient introduced in [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "Under our scenario where X is the origin-centered unit ball in R for d > 2, the hypothesis class C contains all linear separators whose decision surface passes through the origin and PX is the uniform distribution, the diagreement coefficient \u03b8 satisfies [11] \u03c0 4 \u221a d \u2264 \u03b8 \u2264 \u03c0 \u221a d.", "startOffset": 255, "endOffset": 259}, {"referenceID": 2, "context": "Such improvements show the advantage of margin-based active learning and were also observed in [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "Our proposed algorithm is inspired by an adaptive algorithm for stochastic convex optimization presented in [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "A function f is called uniformly convex on a closed convex set Q if there exists \u03c1 \u2265 2 and \u03bc \u2265 0 such that for all x, y \u2208 Q and \u03b1 \u2208 [0, 1], f(\u03b1x+ (1\u2212 \u03b1)y) \u2264 \u03b1f(x) + (1\u2212 \u03b1)f(y)\u2212 1 2 \u03bc\u03b1(1\u2212 \u03b1)\u2016x\u2212 y\u2016.", "startOffset": 132, "endOffset": 138}, {"referenceID": 12, "context": "In [14] an adaptive stochastic optimization algorithm for uniformly and strongly convex functions was presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [15] a connection between multi-dimensional stochastic convex optimization and onedimensional active learning was established.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "(11) are closely related, and the exponents \u03b1 and \u03c1 are tied together in [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "In this paper, we extend the algorithms presented in [14, 15] to build an adaptive margin-based active learning for multi-dimensional data.", "startOffset": 53, "endOffset": 61}, {"referenceID": 13, "context": "In this paper, we extend the algorithms presented in [14, 15] to build an adaptive margin-based active learning for multi-dimensional data.", "startOffset": 53, "endOffset": 61}, {"referenceID": 1, "context": "Therefore, another possible direction of future work would be to design active learning algorithms that adapts to \u03b1 = 0 while still retaining the exponential improvement on convergence rate for this case, which is observed in previous active learning research [2, 3].", "startOffset": 260, "endOffset": 266}, {"referenceID": 2, "context": "Therefore, another possible direction of future work would be to design active learning algorithms that adapts to \u03b1 = 0 while still retaining the exponential improvement on convergence rate for this case, which is observed in previous active learning research [2, 3].", "startOffset": 260, "endOffset": 266}], "year": 2017, "abstractText": "We present and analyze an adaptive margin-based algorithm that actively learns the optimal linear separator for multi-dimensional data. The algorithm has the capacity of adapting to unknown level of label noise in the underlying distribution, making it suitable for model selection under the active learning setting. Compared to other alternative agnostic active learning algorithms, our proposed method is much simpler and achieves the optimal convergence rate in query budget T and data dimension d, if logarithm factors are ignored. Furthermore, our algorithm can handle classification loss functions other than the 0-1 loss, such as hinge and logistic loss, and hence is computationally feasible.", "creator": "LaTeX with hyperref package"}}}