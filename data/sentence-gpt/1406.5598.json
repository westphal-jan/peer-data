{"id": "1406.5598", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2014", "title": "A survey on phrase structure learning methods for text classification", "abstract": "Text classification is a task of automatic classification of text into one of the predefined categories. The problem of text classification has been widely studied in different communities like natural language processing, data mining and information retrieval. Text classification is an important constituent in many information management tasks like topic identification, spam filtering, email routing, language identification, genre classification, readability assessment etc.\n\n\nThe basic set of rules of the classification can be found in the section below for classification, classification, classification, and classification. The first rule for classification is for a classification in the following categories:\n1. In order to use the right classification, you need to be able to choose the categories that you want to categorize in the given order.\n2. In order to use the left classification, you need to be able to categorize from the right to the correct categories. The right classification is based on the subject you want to categorize from the left to the correct categories. You must define in your language that your country has a particular type of country, in which case, the number of people to categorize based on your country.\n3. To specify the category on the right, you need to define a new category to categorize. The right classification is based on the country it was founded on which countries have a particular type of country, in which case, the number of people to categorize based on your country.\n4. To specify the category on the left, you need to specify a new category to categorize. The right classification is based on the country it was founded on which countries have a particular type of country, in which case, the number of people to categorize based on your country.\n5. To specify the category on the right, you need to specify a new category to categorize. The right classification is based on the country it was founded on which countries have a particular type of country, in which case, the number of people to categorize based on your country.\n6. To specify the category on the right, you need to specify a new category to categorize. The right classification is based on the country it was founded on which countries have a particular type of country, in which case, the number of people to categorize based on your country.\n7. To specify the category on the right, you need to specify a new category to categorize. The right classification is based on the country it was founded on which countries have a particular type of country, in which case, the number of", "histories": [["v1", "Sat, 21 Jun 2014 11:30:05 GMT  (300kb)", "http://arxiv.org/abs/1406.5598v1", "14 pages, 2 figures, 2 tables, International Journal on Natural Language Computing (IJNLC) Vol. 3, No.2, April 2014"]], "COMMENTS": "14 pages, 2 figures, 2 tables, International Journal on Natural Language Computing (IJNLC) Vol. 3, No.2, April 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["reshma prasad", "mary priya sebastian"], "accepted": false, "id": "1406.5598"}, "pdf": {"name": "1406.5598.pdf", "metadata": {"source": "CRF", "title": "A SURVEY ON PHRASE STRUCTURE LEARNING METHODS FOR TEXT CLASSIFICATION", "authors": ["Reshma Prasad", "Mary Priya Sebastian"], "emails": [], "sections": [{"heading": null, "text": "DOI : 10.5121/ijnlc.2014.3203 33\nText classification is a task of automatic classification of text into one of the predefined categories. The problem of text classification has been widely studied in different communities like natural language processing, data mining and information retrieval. Text classification is an important constituent in many information management tasks like topic identification, spam filtering, email routing, language identification, genre classification, readability assessment etc. The performance of text classification improves notably when phrase patterns are used. The use of phrase patterns helps in capturing non-local behaviours and thus helps in the improvement of text classification task. Phrase structure extraction is the first step to continue with the phrase pattern identification. In this survey, detailed study of phrase structure learning methods have been carried out. This will enable future work in several NLP tasks, which uses syntactic information from phrase structure like grammar checkers, question answering, information extraction, machine translation, text classification. The paper also provides different levels of classification and detailed comparison of the phrase structure learning methods.\nKEYWORDS\nText classification, Phrase structure, Phrase patterns, Natural Language Processing (NLP)"}, {"heading": "1. INTRODUCTION", "text": "Text classification or categorization includes automatic classification of documents or texts into predefined categories. Different application of text classification includes spam filtering, email routing, language identification, genre classification, readability assessment etc. There are different methods for text classification which includes decision trees [2], rule based classifiers [5], SVM classifiers [7], neural network classifiers [4], bayesian classifiers [3] and nearest neighbour classifiers [1]. Text classification can be improved if phrase patterns are used in the classification task and phrase pattern identification progresses with already extracted phrases.\nPhrase structure is the grammatical arrangement of words in a sentence. The words in a sentence are not arranged in just any order, but language has constraints on word order. Words are organized into phrases, groupings of words that are clumped as a unit and a sentence can be modeled as a set of phrases. Syntactic knowledge can be modeled by phrase structure. Phrase\nstructure is the backbone of many models of syntax of natural language. It can be a powerful way to express sophisticated relations among the words in a sentence. NLP activities like grammar checkers, question answering, information extraction, machine translation, text classification uses syntax information form phrase structure.\nThe advantage of using phrase patterns for text classification is that phrases helps to identify long distant dependencies, the structure can support distant relationships between words. This method provides flexibilities of modeling local word-reordering or grouping which is the main problem of free order languages. Also, phrase extraction brings in some semantic value, which is suitable when NLP activities like text classification, or machine translation is considered. Another advantage of the phrase pattern identification is that it filters words occurring frequently in isolation that does not have much weight towards classification. The different methods of phrase structure learning or extraction have to be studied, classified and compared."}, {"heading": "2. SURVEYED TECHNIQUES", "text": "The goal of the phrase structure extraction is to automatically extract phrase structures from a given corpus. The different techniques surveyed here are based on phrase structure learning. Many methods of phrase structure learning have already been developed for languages like English, Chinese, German, Japanese, Swedish etc. Phrase extraction techniques based on both bilingual and monolingual corpus are discussed in this paper. Different methods surveyed here are:"}, {"heading": "2.1. Basic N-gram based approach", "text": "N-gram based approach is a statistical approach which includes application of n-gram models to obtain phrases. William B. Cavnar and John M. Trenkle proposed an approach which extracted phrases using n-gram model and the phrases thus obtained are used for text categorization [6]. Gulila Altenbek, Ruina Sun used N-gram models for phrase structure extraction from unannotated monolingual corpus [31]. Bigram and trigram models are applied to extract phrases from the corpus. The monolingual corpus is roughly segmented and N-gram model is applied followed by a normalization process. Equation (2) represents the probability [31].\nAccuracy is measured in terms of number of phrases correct to total phrases extracted. The accuracy is measured around 51%, which is low. Among the two models, bigram model has more accuracy than trigram [31]."}, {"heading": "2.2. Rule based method", "text": "Ramshow and Marcus used transformation rule based learning for extracting the noun phrases[9]. Gulila Altenbek, Ruina Sun used rule based method [31] for noun phrase extraction from monolingual corpus. The method is a non-statistical approach which uses annotated monolingual corpus. The approach is based on the basic rules of the target monolingual language; therefore developing a rule set for the corresponding language is a necessary condition. The phrases are extracted according to the rules defined, the corpus is searched for a matched rule and the phrases thus found are extracted.\nAccuracy is measured in terms of number of phrases correct to total phrases extracted. The accuracy for rule based approach is found to be around 80% while that of N-gram based approach is found to be around 51%.\n(2)"}, {"heading": "2.3. Word alignment based method", "text": "Word alignment based method is a statistical method. Phrase alignments are learned from a corpus that has been word aligned. The basic idea is to align the parallel corpus in both directions and to take an intersection so that an alignment matrix is generated. The alignment points in the alignment matrix are expanded based on different heuristics.\nFranz Josef Och, Christoph Tillman and Hermann Ney developed an improved alignment model [11] which uses alignment templates. An alignment template is a triple (F,E,A) where A is an alignment matrix with binary values. The template describes the alignment between source class sequence F and target class sequence E. The initial step of the alignment template approach is to align the parallel corpus in the two translation directions, source to target and target to source. Expectation maximization algorithm is applied in both directions to obtain two alignment vectors. The two alignment vectors are combined to form the alignment matrix A. Iteratively checking and adding neighboring links extend the alignment. All the consistent phrase pairs of the training corpus are determined by checking if the source phrase words are aligned only to target phrase words.\nThe advantage of this approach is the fully automatic learning using bilingual training corpus [11]. The disadvantage is that it selects all templates without checking whether it is good or bad [11]. The measuring score used is word error rate (WER), position independent error rate (PER) and subjective sentence error rate (SSER). In terms of efficiency, the error rate decreased to about 6% than baseline.\nPhilip Koehn, Franz Josef Och and Daniel Marcu modified the alignment template approach later [16]. The heuristics for expansion in the alignment template approach is modified by permitting diagonal neighborhood in the expansion stage [5]. Giza++ toolkit is used for word alignment. Lexical weighting and maximum phrase length scores are applied to the model. Top performance is obtained when the phrase length is three. The method shows better performance when lexical weighting score is applied. The method has an improvement of about 0.01 BLEU score than alignment template approach."}, {"heading": "2.4. Phrase alignment based method", "text": "Phrase alignment based method is another statistical approach in which phrases are extracted from the phrase alignment using phrase-based joint probability.\nDaniel Marcu and William Wong developed a phrase based joint probability model [14]. The model captures simultaneous generation of source and target sentences in a parallel corpus rather than alignment between them. In this method, each sentence pair in our corpus is generated by the idea of generation of a bag of concepts (each concept is a phrase pair) and the bag of concepts can be arranged linearly to obtain source and target sentences [14]. The initial step of the method is to find high frequency n-grams and t-distribution table initialization. Expectation maximization learning on the Viterbi alignment is then applied iteratively which yields joint probability distribution. The performance of the phrase-based method has an average improvement of about 0.02 BLEU score than word alignment based method.\nA modification of the base model has been proposed by Philip Koehn, Franz Josef Och and Daniel Marcu [16]. The base model is modified by marginalizing the joint probabilities to conditional probability [16]. The performance of the method is high when phrases are of length three."}, {"heading": "2.5. Syntactic approach", "text": "Philip Koehn, Franz Josef Och and Daniel Marcu proposed a syntactic method [16], which involves parsing of the sentences in bilingual parallel corpus. In this method, a syntactic phrase is defined as a sequence of words which is covered in a single subtree of a syntactic parse tree [15]. The first step of the method is to word align the parallel corpus. Both side of the corpus is parsed using syntactic parsers. For consistent phrase pairs, it is checked whether both the phrases are subtrees in the parse trees generated. The measuring score used is BLEU score. A BLEU score of 0.243 is obtained when efficiency is measured [16].\n2.6.. Mutual Information based method\nYing Zhang, Stephan Vogel and Alex Waibel developed an integrated phrase segmentation and alignment algorithm [17] for statistical machine translation which uses Mutual Information (MI). The algorithm segments sentences into phrases and thus can be used as a phrase extraction technique. In this method, an initial word alignment or initial segmentation on the monolingual text is not required [17]. The phrases are identified by similarity of point wise mutual information and thus the sentences are segmented into phrases. A two dimensional matrix is constructed to represent the source and target sentence pairs where value of each cell corresponds the point wise mutual information score between source and target words. From the matrix, phrase pair with high MI value is selected and it is expanded to rectangle regions such that the expanded region has a similar MI value. The rectangular region is considered as phrase pair. Repeating this step iteratively identifies all the phrase pairs. Equation (1) is used to calculate the point wise mutual information [17]:\nAfter the segmentation of sentence pairs into phrase pairs, joint probabilities are calculated for these phrase pairs using monolingual conditional probability.\nThe advantage of this method is that it does not require to find high frequency N-grams. Precision and length penalty is used as measuring scores here and a confidence level of 99.99% over baseline HMM represents the efficiency of this approach."}, {"heading": "2.7. Bilingual N-gram based approach", "text": "Ashish Venugopal, Stephan Vogel and Alex Waibel developed an approach for phrase translation extraction using N-grams and the method builds phrase lexicons from bilingual corpus [18]. The method consists of three phases: generation, scoring, and pruning. In the generation phase, all source phrases are identified using N-gram and all possible candidate target phrases corresponding to source phrases are identified. This set is scored and pruned using various scores to remove unwanted target phrases. In the scoring phase, the phrases are scored using measures from three models, maximum approximation, word based translation lexicon and language specific measures [18]. In the pruning step, maximum likely phrase pairs are selected using maximal separation criteria [18].\nThe advantage of this method is that it is less computationally expensive and recovers well from noisy alignments, but it lacks an explanatory framework. When performance is considered, the method shows a NIST score improvement of 0.05 over baseline HMM word alignment method.\n(1)"}, {"heading": "2.8. Block based method", "text": "Ashish Venugopal, Stephan Vogel and Alex Waibel developed a phrase translation extraction from alignment models in 2005, which is based on blocks [21]. In the block based method [21], phrase pair within parallel sentence is considered as a block. The method does not use alignment. An example of a block is shown in fig.1 [21].\nIn the block, y-axis is the source sentence, x axis is the target sentence. The block is defined by source phrase and its projection, which can be represented as the left and right boundaries in the target sentence. The source phrase is bounded by the start and end positions in the source sentence.\nThree models are used in this method. The first one is Fertility model, which predicts the width of the block by computing phrase length. A dynamic programming algorithm using the source word fertilities is employed in this model and given the candidate target phrase e and a source phrase of length J, the model gives the estimation of P(J/e1). Next is the Distortion model. A simple distortion score is computed to estimate how far away the two centers are in a parallel sentence pair in a sense the block is close to the diagonal [21].Another model is Lexicon model, which is computed for translational equivalence. For each candidate block, using word level lexicon, a score within a given sentence pair is computed.\nFor each candidate block, the scores of phrase length, center based distortion and a lexicon-based score are calculated, which is followed by a local greedy search to find best scored phrase pair. The method is a general framework, in which one could plug in other scores and word alignment to get better results, but the computational expense of this method will be higher."}, {"heading": "2.9. Clustering method", "text": "Rile HU, Chengqing ZONG and Bo XU proposed an approach to automatic acquisition of translation templates which is based on phrase structure extraction and alignment [23]. The method is a statistical and data driven approach [23]. The basic idea of the method is to cluster or group words in the corpus using similarity measure. Clustering is performed in two steps, temporal clustering and spatial clustering. Temporal clustering clusters words or entities, which frequently co occurs, into groups. Frequent co occurrence of entities can be obtained by finding the mutual information score between the word pair or entity pair. Spatial clustering clusters words or entities, which have similar left and right contexts which is measured by the KullbackLeibler distance [23].\nThe initial step of the method is to find the similarity measure in terms of distance for each pair of entities and N-pairs of entities with minimum distance is selected to form semantic class. The entities are then replaced with semantic class label. The next step is to find similarity measure in terms of the Mutual Information for each entity pair and N pairs of entities with highest Mutual Information (MI) are selected to form phrasal structure groups. The entities are replaced with phrasal structure class label. This process is repeated till a stopping criterion (STC) is reached.\nThe method has higher precision, recall and f-score than base approaches Bracketing Transduction Grammar method [8] and parse to parse match[13]. Higher number of phrase groups were obtained when cosine of pointwise mutual information score was used [23]."}, {"heading": "2.10. Loose phrase extraction method", "text": "Xue Yongzeng and Li Sheng developed a loose phrase extraction method with n-best alignments [28]. The method of loose phrase extraction [28] is based on the idea of extracting phrase pairs that are not strictly consistent with word alignments. In normal phrase extraction techniques, which use bilingual corpora, exact phrase extraction is used, that is, all the phrase pairs that are consistent are extracted. But this method allows some relaxation to the rule of consistency. Loose phrase pairs can be aligned to some words outside, provided that the word is also aligned to some words inside the phrase pair [28]. After phrase extraction, constraints are applied to these loose phrase pairs to avoid ill formed phrase pairs. Main constraints applied are intersection-based constraint and heuristic based constraint. Apart from applying constraints, a union between n-best alignments from each translation direction is collected and the two unions of alignment are combined. The method achieves better performance than baseline exact match approach, also Nbest alignment results on all constraints shows better BLEU score."}, {"heading": "2.11. Word alignment and Rule based approach", "text": "Andreas Eisele, Christian Federmann, Herv\u00b4e Saint Amand, Michael Jellinghaus, Teresa Herrmann and Yu Chen developed a hybrid method integrating a rule based with a hierarchical translation system [30]. This method is a statistical and rule based hybrid approach. The hybrid system inherits the lexicons from both sub-systems as well as other merits of each system [30]. The method uses word alignment method and rule based approach to extract phrases. Phrase tables are generated from both statistical method and rule based method. These phrase tables are later combined so that the hybrid system can exploit knowledge from both methods [30]. The advantage is that the hybrid method can gain extra knowledge from rule-based system but the errors in rule-based system can affect the correct information in statistical system [30].\nAnother method with variation in combining translation models from various sources has been proposed in 2010 by Yu Chen and Andreas Eisele [32]. In this method, instead of combining phrase tables by adding one binary feature for each individual system, all features in both translation models are retained while combining.\nBLEU score is used as a measuring score. The hybrid method showed improved performance than the baseline word alignment or rule based approach. The performance difference between the hybrid system and the SMT core improved to nearly 1.5 BLEU [32]."}, {"heading": "2.12. N-gram and Rule based approach", "text": "N-gram and Rule based approach [34] is a statistical and rule based hybrid approach developed by Yoh Okuno. In this method, N-gram model is applied to preprocessed corpus using map reduce framework. The N-gram model application is followed by rule based filtering based on the part of speech patterns. Three types of errors were observed, Judgment inconsistency, Morphological analysis error, Lack of features for additional rules. Measuring scores used are\nprecision, recall and F-measure. When compared with the baseline N-gram, precision improvement of about 0.49 was observed. The method shows better performance than N-gram and rule based approaches."}, {"heading": "3. CLASSIFICATION", "text": "The level 1 classification of the different phrase structure learning methods is given in figure.2. The different methods of phrase structure learning can be broadly classified into three classes, statistical methods, rule-based methods and statistical and rule hybrid methods. The approaches, which use statistical techniques for phrase extraction are classified as statistical methods and include clustering method, mutual information based, N-gram based, syntactic based method, alignment based method, block based method. Statistical methods are based on the statistical modeling of data and depend on statistical theorems and rules. The methods do not require rule set for the language.\nThe approach, which uses basic rules for phrase structure extraction, is classified as rule based method. The rule based approach needs a developed set of rules for the language and the task is performed based on this set of rules. The approaches, which use both statistical techniques and rules for phrase structure extraction, are classified as statistical and rule hybrid methods and include N-gram and rule based method and word alignment and rule based method."}, {"heading": "4. OBSERVATIONS AND DISCUSSION", "text": "The different methods for phrase structure learning can be compared in detail based on several factors. A level 2 classification of the methods based on corpus used, initial alignment requirement of corpus, base approach, tools used, and technique used is given in Table.1. Another level of classification, level 3 based on different scores applied, evaluation metrics used and efficiency is given in Table.2.\nWhen the different methods are compared, an observation made is that the mutual information based method and probabilistic based method shows higher efficiency and performance. Mutual information based method shows a confidence value of 99.99% [17], which is promising. Hybrid methods like N-gram and rule based approach and word alignment and rule based approach shows good performance but requires set of rules for the language.\nClustering method also shows relatively better results but the concept of clustering and\nFigure.2. Level 1 classification of methods\ntranslation templates does not have much relevance when agglutinative languages like Indian languages, Japanese, Turkish etc are considered, and may need to consider words in its root form [10]. Alignment template approach, which is a word alignment based method, does not distinguish between good or bad templates though it can be learned automatically using bilingual corpus. When N-gram based approached are considered, basic N-gram shows less accuracy as internal structure of phrases are not considered. Bilingual N-grams show good performance than basic and is less computationally expensive, but it lacks an explanatory framework. Block based method is another candidate but it can be computationally expensive. Rule based approaches is not so efficient as the task of developing a rule set for a particular language is cumbersome and also large amount of rules have to be developed manually. Thus it needs time and support from trained linguistics expert. But when compared with N-grams, the accuracy is more for rule-based\napproach as it takes into consideration the internal structure of phrases. Hybrid methods like Ngram and rule based approach and word alignment and rule based approach shows good performance but requires set of rules for the language. Syntactic method is not much efficient, as syntactic models do not provide important phrase alignments. Weighting of syntactic phrases also does not improve the performance much.\nMethod Author\nand year\nCorpus\nused\nInitial\nalignment requireme\nnt of\ncorpus\nBase approach Tools\nused Technique used\nBasic N-\ngram based\nmethod\nWilliam B. Cavnar.et.\nal (1994),\nGulila\nAltenbek.e t.al (2010)\nXinjiang\ndaily\ncorpus\nNo\ninformati\non\navailable\nNo\ninformation\navailable\nNo\ninformati\non\navailable\nN-gram method for unannotated\ntext\nBasic Rule\nbased\napproach\nL.\nRamshaw.\net.al\n(1995), Gulila\nAltenbek.e t.al (2010)\nXinjiang\ndaily\ncorpus\nNo\ninformati\non\navailable\nNo\ninformation\navailable\nNo\ninformati\non\navailable\nNoun phrases\nextracted based\non set of rules\nWord\nAlignmen\nt based method\nFranz Josef\nOch.et.al\n(1999),\nPhilip\nKoehn.et.a\nl (2003)\nEuroparl\ncorpus\nSentence\naligned\nSingle word\nbased\napproach(uses\nmanual\ndictionary)\nGIZA++\ntool kit\nPhrases from\nword based alignments -\nalignment\ntemplate\napproach and\nmodified alignment\ntemplate approach\nPhrase\nalignment\nbased\nmethod\nDaniel\nMarcu.et.a\nl (2002),\nPhilip\nKoehn.et.a\nl (2003)\nEuroparl\ncorpus\nSentence\naligned\nword\nalignment\nmethod(IBM\nmodel 4)\nNo\ninformati\non\navailable\nPhrases are\nextracted from\nthe phrase\nalignment using\nphrase-based\njoint probability\nmodel\nSyntactic\nmethod\nImamura, K (2002),\nPhilip\nKoehn.et.a\nl (2003)\nEuroparl\ncorpus\nSentence\naligned\nSyntactic\ntranslation\nmodels\nSyntacti c parser\nSyntactic phrase\npairs extracted\nfrom word\naligned corpus,\nword alignment-\nmodified alignment\ntemplate approach\nMutual\nInformati on based\nmethod\nYing\nZhang\n.et.al\n(2003)\nXinhua English\nnews\ncorpora\nSentence\naligned\nWord-word translations\n(IBM model1),\nPhrase-phrase\ntranslations from HMM\nword\nalignment\nNo\ninformati\non\navailable\nIntegrated phrase segmentation and\nalignment algorithm-\nphrases identified\nby similarity of\npoint wise mutual\ninformation\nMethod Author\nand year\nCorpus\nused\nInitial\nalignment requireme\nnt of\ncorpus\nBase approach Tools\nused Technique used\nBilingual\nN-gram\nbased\nmethod\nAshish\nVenugopal\n.et.al\n(2003)\nEnglishChinese parallel language\ncorpus\nWord\naligned\nusing IBM\nalignment\nmodel\nHMM\nalignment\nmodel, word level system( IBM model1)\nGIZA\ntool\nBuilding phrasal\nlexicons by N-\ngram method\nwith generation, scoring, pruning\nsteps\nBlock\nbased\nmethod\nBing\nZhao.et.al\n(2005)\nEnglish-\nFrench corpus\nSentence\naligned\nword\nalignment\nmethod(IBM\nmodel 4)\nGIZA++,\npharaoh decoder\nFertility model-to\npredict width of\nthe block.\nDistortion model-\nto predict how\nclose centers of\nsource and target\nphrase are.\nLexicon model-\nfor translation\nequivalence.\nClustering\nmethod\nRile\nHu.et.al\n(2006)\nEnglishChinese parallel\nspoken\nlanguage\ncorpus\nSentence\naligned\nPhrase\nalignment\nmethod using\nBracketing\nTransduction\nGrammar, Syntactic\nmethod using parse-to-parse\nmatch\nNo\ninformat\nion\navailable\nPhrase\nextraction-\ntemporal and\nspatial clustering,\nAlignmentbracketing transduction\ngrammar\nLoose phrase\nbased\nmethod\nXue\nYongzeng.\net.al\n(2007)\nIWSLT-04\nChinese-\nEnglish\ntranslation\ntask\nSentence\naligned\nWord\nalignment\nmethod using\nIBM word alignment\nmodel\nPharaoh\ntrainer\nand\ndecoder\nBased on loose\nphrase extraction,\nwith extensions of word position based constraints\nand n-best alignments\nWord\nalignment and Rule\nhybrid\nAndreas\nEisele.et.al (2008), Yu Chen.et.al\n(2010)\nEuroparl\ncorpus\nSentence\naligned\nRule based\napproach and\nWord\nalignment\nbased\nmethod(IBM\nword\nalignment\nmodel)\nMoses or\nJoshua\ndecoder,\nLucy,\nSRILM\ntoolkit\nBased on word\nalignment\nmethod and rule\nbased method\nN-gram and Rule\nhybrid\nYoh\nOkuno (2011)\nJapanese\nblog\ncorpus\nNo\ninformati\non\navailable\nN-gram based\nmethod\nNo\ninformati\non\navailable\nBased on N-gram\nmodel and rule based filtering"}, {"heading": "5. CONCLUSION", "text": "Text classification is an important natural language processing task, which has got many useful applications like spam filtering, email routing, language identification, genre classification, readability assessment. The use of phrases helps in capturing non-local behaviors and thus helps in the improvement of text classification task. In this survey paper, different phrase structure learning methods for text classification have been studied. The approaches are classified in a broader aspect into three groups, statistical methods, rule-based methods and statistical and rule hybrid methods. Different techniques are further classified into two more levels based on the technique used and efficiency. The methods are classified and compared in detail based on different factors like corpus used, initial alignment requirement, base approach, tools used, techniques in the level 2 classification. The methods are again compared and classified based on different scores applied, evaluation metrics used and efficiency in level 3 classification. One observation made is that the major works in phrase structure learning are focused on statistical\nand hybrid methods as rule based approach needs time and trained linguistics personnel. The major observation made is that mutual information based approach is the most promising technique for phrase structure extraction and shows better performance and efficiency."}], "references": [{"title": "Nearest neighbor pattern classification", "author": ["T.Cover", "P. Hart"], "venue": "IEEE transactions on Information Theory, Volume 13,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1967}, {"title": "Induction of Decision Trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,1(1),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Autoclass: A Bayesian classication system", "author": ["P. Cheeseman", "J. Kelly", "M. Self", "J. Stutz", "W. Taylor", "D. Freeman"], "venue": "Proceedings of the Fifth International Conference on Machine Learning (pp. 54{64)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "Automated Learning of Decision Rules for Text Categorization", "author": ["C. Apte", "F. Damerau", "S. Weiss"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "N-grambased text categorization", "author": ["W.B. Cavnar", "Trenkle", "J. M"], "venue": "In Proceedings of SDAIR-94, 3rd Annual Symposium on Docu-ment Analysis and Information Retrieval", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "\u201cAn algorithm for simultaneously bracketing parallel texts by aligning words,", "author": ["D. Wu"], "venue": "in Proc. 33th Annu. Meeting Association for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Text Chunking using Transformation-Based Learning", "author": ["L. Ramshaw", "M. Marcus"], "venue": "In Proceedings of the Third Workshop on Very Large Corpora", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "1998 ) \u201cLearning Translation Templates from Examples", "author": ["H. Altay G \u0308uvenir", "Ilyas Cicekli"], "venue": "Information Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Improved Alignment Models for Statistical Machine Translation", "author": ["Franz Josef Och", "Christoph Tillmann", "Hermann Ney"], "venue": "Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Probabilistic techniques for phrase extraction\u201d, Information Processing and Management", "author": ["Fangfang Feng", "W. Bruce Croft"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Hierarchical phrase alignment harmonized with parsing,", "author": ["K. Imamura"], "venue": "Proc. 6th Natural Language Processing Pacific Rim Symp.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Application of translation knowledge acquired by hierarchical phrase alignment for pattern based machine translation", "author": ["K Imamura"], "venue": "Proceedings of the conference on Empirical methods in Natural Language Processing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "A Phrase-Based, Joint Probability Model for Statistical Machine Translation,\u2019", "author": ["Daniel Marcu", "William Wong"], "venue": "Proceedings of the Conference on Empiric01 Methods in Natural Language Processing", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Statistical phrase based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Integrated Phrase segmentation and alignment algorithm for statistical machine translation", "author": ["Ying Zhang", "Stephan Vogel", "Alex Waibel"], "venue": "Proceedings of International Conference on Natural Language Processing and Knowledge Engineering", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Effective Phrase Translation Extraction from Alignment Models", "author": ["Ashish Venugopal", "Stephan Vogel", "Alex Waibel"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Semiautomatic acquisition of translation templates from monolingual unannotated corpora", "author": ["R. Hu", "C. Zong", "B. Xu"], "venue": "Proc. Int. Conf. Natural Language Processing and Knowledge Engineering,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Key-phrase extraction for classification", "author": ["Nikitas N. Karanikolas", "Christos Skourlas"], "venue": "X Mediterranean Conference on Medical and Biological Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "A Generalized Alignment-Free Phrase Extraction", "author": ["Bing Zhao", "Stephan Vogel"], "venue": "Proceedings of the ACL Workshop on Building and Using Parallel Texts", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Automatic term extraction based on Perplexity of compound words", "author": ["M Yoshida", "H Nakagawa"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "An Approach to Automatic Acquisition of Translation Templates Based on Phrase Structure Extraction and Alignment", "author": ["Rile Hu", "Cheng qing Zong"], "venue": "IEEE transactions on Audio,  International Journal on Natural Language Computing (IJNLC)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Improved Statistical Machine Translation Using Paraphrases", "author": ["Chris Callison-Burch", "Philipp Koehn", "Miles Osborne"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "N-gram-based SMT System Enhanced with Reordering Patterns", "author": ["Josep M. Crego", "Adria de Gispert", "Patrik Lambert"], "venue": "Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "A Clustered Global Phrase Reordering Model for Statistical Machine Translation", "author": ["Masaaki Nagata", "Kuniko Saito", "Kazuhide Yamamoto", "Kazuteru Ohashi"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Extraction and Clustering of Term Definition, and Term Recognition\u201d, Beijing Language and Culture", "author": ["Rong Zhang"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Loose phrase extraction with n-best alignments", "author": ["Xue Yongzeng", "Li Sheng"], "venue": "Journal of Electronics, Volume", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Hierarchical phrase based translation", "author": ["D. Chiang"], "venue": "Computational Linguistics", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Using moses to integrate multiple rule based machine translation engines into a hybrid system", "author": ["Andreas Eisele", "Christian Federmann", "Herv \u0301e Saint Amand", "Michael Jellinghaus", "Teresa Herrmann", "Yu Chen"], "venue": "Third Workshop on Statistical Machine Translation", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Kazakh Noun Phrase Extraction based on N-gram and Rules", "author": ["Gulila Altenbek", "Ruina Sun"], "venue": "Internation conference on Asian Language Processing", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Integrating a Rule-based with a Hierarchical Translation System", "author": ["Yu Chen", "Andreas Eisele"], "venue": "Proceedings of 7th International Conference on Language Resources and Evaluation", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "An Unsupervised Model for Joint Phrase Alignment and Extraction\u201d, Proceedings of 49  th Annual meeting of the Association for Computational Linguistics: Human Language Technologies", "author": ["Graham Neubig", "Taro Watanabe", "Eiichiro Sumita", "Shinsuke Mori", "Tatsuya Kawahara"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Phrase Extraction for Japanese Predictive Input Method as Post-Processing", "author": ["Yoh Okuno"], "venue": "Proceedings of the Workshop on Advances in Text Input Methods", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Learning Phrase Patterns for Text Classification", "author": ["Bin Zhang", "Alex Marin", "Brian Hutchinson", "Mari Ostendorf"], "venue": "IEEE transactions on Audio, Speech and Language Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Semiautomatic Acquisition of Translation Templates from Monolingual Unannotated Chinese Patent Corpus", "author": ["Dechun Yin", "Dakui Zhang"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "There are different methods for text classification which includes decision trees [2], rule based classifiers [5], SVM classifiers [7], neural network classifiers [4], bayesian classifiers [3] and nearest neighbour classifiers [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "There are different methods for text classification which includes decision trees [2], rule based classifiers [5], SVM classifiers [7], neural network classifiers [4], bayesian classifiers [3] and nearest neighbour classifiers [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 5, "context": "There are different methods for text classification which includes decision trees [2], rule based classifiers [5], SVM classifiers [7], neural network classifiers [4], bayesian classifiers [3] and nearest neighbour classifiers [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "There are different methods for text classification which includes decision trees [2], rule based classifiers [5], SVM classifiers [7], neural network classifiers [4], bayesian classifiers [3] and nearest neighbour classifiers [1].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "There are different methods for text classification which includes decision trees [2], rule based classifiers [5], SVM classifiers [7], neural network classifiers [4], bayesian classifiers [3] and nearest neighbour classifiers [1].", "startOffset": 227, "endOffset": 230}, {"referenceID": 4, "context": "Trenkle proposed an approach which extracted phrases using n-gram model and the phrases thus obtained are used for text categorization [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 29, "context": "Gulila Altenbek, Ruina Sun used N-gram models for phrase structure extraction from unannotated monolingual corpus [31].", "startOffset": 114, "endOffset": 118}, {"referenceID": 29, "context": "Equation (2) represents the probability [31].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "Among the two models, bigram model has more accuracy than trigram [31].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "Ramshow and Marcus used transformation rule based learning for extracting the noun phrases[9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 29, "context": "Gulila Altenbek, Ruina Sun used rule based method [31] for noun phrase extraction from monolingual corpus.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "Franz Josef Och, Christoph Tillman and Hermann Ney developed an improved alignment model [11] which uses alignment templates.", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "The advantage of this approach is the fully automatic learning using bilingual training corpus [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "The disadvantage is that it selects all templates without checking whether it is good or bad [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "Philip Koehn, Franz Josef Och and Daniel Marcu modified the alignment template approach later [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "The heuristics for expansion in the alignment template approach is modified by permitting diagonal neighborhood in the expansion stage [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 12, "context": "Daniel Marcu and William Wong developed a phrase based joint probability model [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "In this method, each sentence pair in our corpus is generated by the idea of generation of a bag of concepts (each concept is a phrase pair) and the bag of concepts can be arranged linearly to obtain source and target sentences [14].", "startOffset": 228, "endOffset": 232}, {"referenceID": 14, "context": "A modification of the base model has been proposed by Philip Koehn, Franz Josef Och and Daniel Marcu [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "The base model is modified by marginalizing the joint probabilities to conditional probability [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "Philip Koehn, Franz Josef Och and Daniel Marcu proposed a syntactic method [16], which involves parsing of the sentences in bilingual parallel corpus.", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "In this method, a syntactic phrase is defined as a sequence of words which is covered in a single subtree of a syntactic parse tree [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "243 is obtained when efficiency is measured [16].", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Ying Zhang, Stephan Vogel and Alex Waibel developed an integrated phrase segmentation and alignment algorithm [17] for statistical machine translation which uses Mutual Information (MI).", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "In this method, an initial word alignment or initial segmentation on the monolingual text is not required [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "Equation (1) is used to calculate the point wise mutual information [17]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Ashish Venugopal, Stephan Vogel and Alex Waibel developed an approach for phrase translation extraction using N-grams and the method builds phrase lexicons from bilingual corpus [18].", "startOffset": 178, "endOffset": 182}, {"referenceID": 16, "context": "In the scoring phase, the phrases are scored using measures from three models, maximum approximation, word based translation lexicon and language specific measures [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "In the pruning step, maximum likely phrase pairs are selected using maximal separation criteria [18].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "Ashish Venugopal, Stephan Vogel and Alex Waibel developed a phrase translation extraction from alignment models in 2005, which is based on blocks [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "In the block based method [21], phrase pair within parallel sentence is considered as a block.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "1 [21].", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "A simple distortion score is computed to estimate how far away the two centers are in a parallel sentence pair in a sense the block is close to the diagonal [21].", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": "Rile HU, Chengqing ZONG and Bo XU proposed an approach to automatic acquisition of translation templates which is based on phrase structure extraction and alignment [23].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "The method is a statistical and data driven approach [23].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Spatial clustering clusters words or entities, which have similar left and right contexts which is measured by the KullbackLeibler distance [23].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "The method has higher precision, recall and f-score than base approaches Bracketing Transduction Grammar method [8] and parse to parse match[13].", "startOffset": 112, "endOffset": 115}, {"referenceID": 11, "context": "The method has higher precision, recall and f-score than base approaches Bracketing Transduction Grammar method [8] and parse to parse match[13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 21, "context": "Higher number of phrase groups were obtained when cosine of pointwise mutual information score was used [23].", "startOffset": 104, "endOffset": 108}, {"referenceID": 26, "context": "Xue Yongzeng and Li Sheng developed a loose phrase extraction method with n-best alignments [28].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "The method of loose phrase extraction [28] is based on the idea of extracting phrase pairs that are not strictly consistent with word alignments.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "Loose phrase pairs can be aligned to some words outside, provided that the word is also aligned to some words inside the phrase pair [28].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Andreas Eisele, Christian Federmann, Herv \u0301e Saint Amand, Michael Jellinghaus, Teresa Herrmann and Yu Chen developed a hybrid method integrating a rule based with a hierarchical translation system [30].", "startOffset": 197, "endOffset": 201}, {"referenceID": 28, "context": "The hybrid system inherits the lexicons from both sub-systems as well as other merits of each system [30].", "startOffset": 101, "endOffset": 105}, {"referenceID": 28, "context": "These phrase tables are later combined so that the hybrid system can exploit knowledge from both methods [30].", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "The advantage is that the hybrid method can gain extra knowledge from rule-based system but the errors in rule-based system can affect the correct information in statistical system [30].", "startOffset": 181, "endOffset": 185}, {"referenceID": 30, "context": "Another method with variation in combining translation models from various sources has been proposed in 2010 by Yu Chen and Andreas Eisele [32].", "startOffset": 139, "endOffset": 143}, {"referenceID": 30, "context": "5 BLEU [32].", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "N-gram and Rule based approach [34] is a statistical and rule based hybrid approach developed by Yoh Okuno.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "99% [17], which is promising.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "translation templates does not have much relevance when agglutinative languages like Indian languages, Japanese, Turkish etc are considered, and may need to consider words in its root form [10].", "startOffset": 189, "endOffset": 193}], "year": 2014, "abstractText": "Text classification is a task of automatic classification of text into one of the predefined categories. The problem of text classification has been widely studied in different communities like natural language processing, data mining and information retrieval. Text classification is an important constituent in many information management tasks like topic identification, spam filtering, email routing, language identification, genre classification, readability assessment etc. The performance of text classification improves notably when phrase patterns are used. The use of phrase patterns helps in capturing non-local behaviours and thus helps in the improvement of text classification task. Phrase structure extraction is the first step to continue with the phrase pattern identification. In this survey, detailed study of phrase structure learning methods have been carried out. This will enable future work in several NLP tasks, which uses syntactic information from phrase structure like grammar checkers, question answering, information extraction, machine translation, text classification. The paper also provides different levels of classification and detailed comparison of the phrase structure learning methods.", "creator": "PScript5.dll Version 5.2.2"}}}