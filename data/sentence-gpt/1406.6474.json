{"id": "1406.6474", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2014", "title": "On the Convergence Rate of Decomposable Submodular Function Minimization", "abstract": "Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of \"simple\" submodular functions, and thereby reduce complexity and complexity.\n\n\n\n\n\nThe computational problems that can be solved are a few of the major challenges of machine learning:\nThe number of parallelizable systems is relatively small and is not easily defined. The system can't provide enough information, or the complexity of a number of algorithms can be very large.\nAn important constraint in the optimization of learning is that the number of parallelizable systems is too small to be easily defined. However, the number of parallelizable systems does not seem to be sufficient for the complexity of a number of computations.\nWhen the number of parallelizable systems is not significantly larger than the number of non-linear, non-linear, and non-linear (PNG) systems, the system cannot provide enough information. The difficulty in computing the non-linear, non-linear, and non-linear algorithms depends on the power of a single computer to perform the many operations. Therefore, one of the more important problems with computer learning is that there are still problems in computing the non-linear, non-linear, and non-linear algorithms that require the computing power of one to perform the many operations.\nThe difficulty in solving the non-linear, non-linear, and non-linear algorithms depends on the power of a single computer to perform the many operations. Therefore, one of the more important problems with computer learning is that there are still problems in computing the non-linear, non-linear, and non-linear algorithms that require the computing power of one to perform the many operations. Therefore, one of the more important problems with computer learning is that there are still problems in computing the non-linear, non-linear, and non-linear algorithms that require the computing power of one to perform the many operations. However, in a way, it is impossible to say that the problem of computing the non-linear, non-linear, and non-linear algorithms are correct.\nIn general, it is important to understand how a certain type of machine can overcome the difficulty in processing, and to use this approach to solve the problems in computation. A machine may perform only one task, but a certain type of machine", "histories": [["v1", "Wed, 25 Jun 2014 06:52:33 GMT  (43kb)", "https://arxiv.org/abs/1406.6474v1", "19 pages, 3 figures"], ["v2", "Fri, 27 Jun 2014 18:12:03 GMT  (43kb)", "http://arxiv.org/abs/1406.6474v2", "19 pages, 3 figures"], ["v3", "Wed, 5 Nov 2014 07:19:00 GMT  (599kb)", "http://arxiv.org/abs/1406.6474v3", "17 pages, 3 figures"]], "COMMENTS": "19 pages, 3 figures", "reviews": [], "SUBJECTS": "math.OC cs.DM cs.DS cs.LG cs.NA", "authors": ["robert nishihara", "stefanie jegelka", "michael i jordan"], "accepted": true, "id": "1406.6474"}, "pdf": {"name": "1406.6474.pdf", "metadata": {"source": "CRF", "title": "On the Convergence Rate of Decomposable Submodular Function Minimization", "authors": ["Robert Nishihara", "Stefanie Jegelka", "Michael I. Jordan"], "emails": ["rkn@eecs.berkeley.edu", "stefje@eecs.berkeley.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n64 74\nv3 [\nm at\nh. O\nC ]\n5 N"}, {"heading": "1 Introduction", "text": "A large body of recent work demonstrates that many discrete problems in machine learning can be phrased as the optimization of a submodular set function [2]. A set function F : 2V \u2192 R over a ground set V of N elements is submodular if the inequality F (A)+F (B) \u2265 F (A\u222aB)+F (A\u2229B) holds for all subsets A,B \u2286 V . Problems like clustering [33], structured sparse variable selection [1], MAP inference with higher-order potentials [28], and corpus extraction problems [31] can be reduced to the problem of submodular function minimization (SFM), that is\nmin A\u2286V F (A). (P1)\nAlthough SFM is solvable in polynomial time, existing algorithms can be inefficient on large-scale problems. For this reason, the development of scalable, parallelizable algorithms has been an active area of research [24, 25, 29, 35]. Approaches to solving Problem (P1) are either based on combinatorial optimization or on convex optimization via the Lova\u0301sz extension.\nFunctions that occur in practice are usually not arbitrary and frequently possess additional exploitable structure. For example, a number of submodular functions admit specialized algorithms that solve Problem (P1) very quickly. Examples include cut functions on certain kinds of graphs, concave functions of the cardinality |A|, and functions counting joint ancestors in trees. We will use the term simple to refer to functions F for which we have a fast subroutine for minimizing F + s, where s \u2208 RN is any modular function. We treat these subroutines as black boxes. Many commonly occuring submodular functions (for example, graph cuts, hypergraph cuts, MAP inference with higher-order potentials [16, 28, 37], co-segmentation [22], certain structured-sparsity inducing functions [26], covering functions [35], and combinations thereof) can be expressed as a sum\nF (A) = \u2211R\nr=1 Fr(A) (1)\nof simple submodular functions. Recent work demonstrates that this structure offers important practical benefits [25, 29, 35]. For instance, it admits iterative algorithms that minimize each Fr separately and combine the results in a straightforward manner (for example, dual decomposition).\nIn particular, it has been shown that the minimization of decomposable functions can be rephrased as a best-approximation problem, the problem of finding the closest points in two convex sets [25]. This formulation brings together SFM and classical projection methods and yields empirically fast, parallel, and easy-to-implement algorithms. In these cases, the performance of projection methods depends heavily on the specific geometry of the problem at hand and is not well understood in general. Indeed, while Jegelka et al. [25] show good empirical results, the analysis of this alternative approach to SFM was left as an open problem.\nContributions. In this work, we study the geometry of the submodular best-approximation problem and ground the prior empirical results in theoretical guarantees. We show that SFM via alternating projections, or block coordinate descent, converges at a linear rate. We show that this rate holds for the best-approximation problem, relaxations of SFM, and the original discrete problem. More importantly, we prove upper and lower bounds on the worst-case rate of convergence. Our proof relies on analyzing angles between the polyhedra associated with submodular functions and draws on results from spectral graph theory. It offers insight into the geometry of submodular polyhedra that may be beneficial beyond the analysis of projection algorithms.\nSubmodular minimization. The first polynomial-time algorithm for minimizing arbitrary submodular functions was a consequence of the ellipsoid method [19]. Strongly and weakly polynomialtime combinatorial algorithms followed [32]. The current fastest running times are O(N5\u03c41 +N6) [34] in general and O((N4\u03c41 + N5) logFmax) for integer-valued functions [23], where Fmax = maxA |F (A)| and \u03c41 is the time required to evaluate F . Some work has addressed decomposable functions [25, 29, 35]. The running times in [29] apply to integer-valued functions and range from O((N +R)2 logFmax) for cuts to O((N +Q2R)(N +Q2R+QR\u03c42) logFmax), where Q \u2264 N is the maximal cardinality of the support of any Fr , and \u03c42 is the time required to minimize a simple function. Stobbe and Krause [35] use a convex optimization approach based on Nesterov\u2019s smoothing technique. They achieve a (sublinear) convergence rate of O(1/k) for the discrete SFM problem. Their results and our results do not rely on the function being integral.\nProjection methods. Algorithms based on alternating projections between convex sets (and related methods such as the Douglas\u2013Rachford algorithm) have been studied extensively for solving convex feasibility and best-approximation problems [4, 5, 7, 11, 12, 20, 21, 36, 38]. See Deutsch [10] for a survey of applications. In the simple case of subspaces, the convergence of alternating projections has been characterized in terms of the Friedrichs angle cF between the subspaces [5, 6]. There are often good ways to compute cF (see Lemma 6), which allow us to obtain concrete linear rates of convergence for subspaces. The general case of alternating projections between arbitrary convex sets is less well understood. Bauschke and Borwein [3] give a general condition for the linear convergence of alternating projections in terms of the value \u03ba\u2217 (defined in Section 3.1). However, except in very limited cases, it is unclear how to compute or even bound \u03ba\u2217. While it is known that \u03ba\u2217 < \u221e for polyhedra [5, Corollary 5.26], the rate may be arbitrarily slow, and the challenge is to bound the linear rate away from one. We are able to give a specific uniform linear rate for the submodular polyhedra that arise in SFM.\nAlthough both \u03ba\u2217 and cF are useful quantities for understanding the convergence of projection methods, they largely have been studied independently of one another. In this work, we relate these two quantities for polyhedra, thereby obtaining some of the generality of \u03ba\u2217 along with the computability of cF . To our knowledge, we are the first to relate \u03ba\u2217 and cF outside the case of subspaces. We feel that this connection may be useful beyond the context of submodular polyhedra."}, {"heading": "1.1 Background", "text": "Throughout this paper, we assume that F is a sum of simple submodular functions F1, . . . , FR and that F (\u2205) = 0. Points s \u2208 RN can be identified with (modular) set functions via s(A) = \u2211n\u2208A sn. The base polytope of F is defined as the set of all modular functions that are dominated by F and that sum to F (V ),\nB(F ) = {s \u2208 RN | s(A) \u2264 F (A) for all A \u2286 V and s(V ) = F (V )}. The Lova\u0301sz extension f : RN \u2192 R of F can be written as the support function of the base polytope, that is f(x) = maxs\u2208B(F ) s\u22a4x. Even though B(F ) may have exponentially many faces, the extension f can be evaluated in O(N logN) time [15]. The discrete SFM problem (P1) can be relaxed to\nthe non-smooth convex optimization problem\nmin x\u2208[0,1]N f(x) \u2261 min x\u2208[0,1]N\nR\u2211\nr=1\nfr(x), (P2)\nwhere fr is the Lova\u0301sz extension of Fr. This relaxation is exact \u2013 rounding an optimal continuous solution yields the indicator vector of an optimal discrete solution. The formulation in Problem (P2) is amenable to dual decomposition [30] and smoothing techniques [35], but suffers from the nonsmoothness of f [25]. Alternatively, we can formulate a proximal version of the problem\nmin x\u2208RN f(x) + 12\u2016x\u20162 \u2261 min x\u2208RN\nR\u2211\nr=1\n(fr(x) + 1 2R\u2016x\u20162). (P3)\nBy thresholding the optimal solution of Problem (P3) at zero, we recover the indicator vector of an optimal discrete solution [17], [2, Proposition 8.4].\nLemma 1. [25] The dual of the right-hand side of Problem (P3) is the best-approximation problem\nmin \u2016a\u2212 b\u20162 a \u2208 A, b \u2208 B, (P4)\nwhere A = {(a1, . . . , aR) \u2208 RNR | \u2211R r=1 ar = 0} and B = B(F1)\u00d7 \u00b7 \u00b7 \u00b7 \u00d7B(FR).\nLemma 1 implies that we can minimize a decomposable submodular function by solving Problem (P4), which means finding the closest points between the subspace A and the product B of base polytopes. Projecting onto A is straightforward because A is a subspace. Projecting onto B amounts to projecting onto each B(Fr) separately. The projection \u03a0B(Fr)z of a point z onto B(Fr) may be solved by minimizing Fr \u2212 z [25]. We can compute these projections easily because each Fr is simple.\nThroughout this paper, we use A and B to refer to the specific polyhedra defined in Lemma 1 (which live in RNR) and P and Q to refer to general polyhedra (sometimes arbitrary convex sets) in RD. Note that the polyhedron B depends on the submodular functions F1, . . . , FR, but we omit the dependence to simplify our notation. Our bound will be uniform over all submodular functions."}, {"heading": "2 Algorithm and Idea of Analysis", "text": "A popular class of algorithms for solving best-approximation problems are projection methods [5]. The most straightforward approach uses alternating projections (AP) or block coordinate descent. Start with any point a0 \u2208 A, and inductively generate two sequences via bk = \u03a0Bak and ak+1 = \u03a0Abk. Given the nature of A and B, this algorithm is easy to implement and use in our setting, and it solves Problem (P4) [25]. This is the algorithm that we will analyze.\nThe sequence (ak, bk) will eventually converge to an optimal pair (a\u2217, b\u2217). We say that AP converges linearly with rate \u03b1 < 1 if \u2016ak\u2212a\u2217\u2016 \u2264 C1\u03b1k and \u2016bk\u2212b\u2217\u2016 \u2264 C2\u03b1k for all k and for some constants C1 and C2. Smaller values of \u03b1 are better.\nAnalysis: Intuition. We will provide a detailed analysis of the convergence of AP for the polyhedra A and B. To motivate our approach, we first provide some intuition with the following muchsimplified setup. Let U and V be one-dimensional subspaces spanned by the unit vectors u and v respectively. In this case, it is known that AP converges linearly with rate cos2 \u03b8, where \u03b8 \u2208 [0, \u03c02 ] is the angle such that cos \u03b8 = u\u22a4v. The smaller the angle, the slower the rate of convergence. For subspaces U and V of higher dimension, the relevant generalization of the \u201cangle\u201d between the subspaces is the Friedrichs angle [11, Definition 9.4], whose cosine is given by\ncF (U, V ) = sup { u\u22a4v |u \u2208 U \u2229 (U \u2229 V )\u22a5, v \u2208 V \u2229 (U \u2229 V )\u22a5, \u2016u\u2016 \u2264 1, \u2016v\u2016 \u2264 1 } . (2)\nIn finite dimensions, cF (U, V ) < 1. In general, when U and V are subspaces of arbitrary dimension, AP will converge linearly with rate cF (U, V )2 [11, Theorem 9.8]. If U and V are affine spaces, AP still converges linearly with rate cF (U \u2212 u, V \u2212 v)2, where u \u2208 U and v \u2208 V . We are interested in rates for polyhedra P and Q, which we define as the intersection of finitely many halfspaces. We generalize the preceding results by considering all pairs (Px, Qy) of\nfaces of P and Q and showing that the convergence rate of AP between P and Q is at worst maxx,y cF (aff0(Px), aff0(Qy))\n2, where aff(C) is the affine hull of C and aff0(C) = aff(C) \u2212 c for some c \u2208 C. The faces {Px}x\u2208RD of P are defined as the nonempty maximizers of linear functions over P , that is\nPx = argmax p\u2208P\nx\u22a4p. (3)\nWhile we look at angles between pairs of faces, we remark that Deutsch and Hundal [13] consider a different generalization of the \u201cangle\u201d between arbitrary convex sets.\nRoadmap of the Analysis. Our analysis has two main parts. First, we relate the convergence rate of AP between polyhedra P and Q to the angles between the faces of P and Q. To do so, we give a general condition under which AP converges linearly (Theorem 2), which we show depends on the angles between the faces of P and Q (Corollary 5) in the polyhedral case. Second, we specialize to the polyhedra A and B, and we equate the angles with eigenvalues of certain matrices and use tools from spectral graph theory to bound the relevant eigenvalues in terms of the conductance of a specific graph. This yields a worst-case bound of 1\u2212 1N2R2 on the rate, stated in Theorem 12.\nIn Theorem 14, we show a lower bound of 1\u2212 2\u03c02N2R on the worst-case convergence rate."}, {"heading": "3 The Upper Bound", "text": "We first derive an upper bound on the rate of convergence of AP between the polyhedra A and B. The results in this section are proved in Appendix A."}, {"heading": "3.1 A Condition for Linear Convergence", "text": "We begin with a condition under which AP between two closed convex sets P and Q converges linearly. This result is similar to that of Bauschke and Borwein [3, Corollary 3.14], but the rate we achieve is twice as fast and relies on slightly weaker assumptions.\nWe will need a few definitions from Bauschke and Borwein [3]. Let d(K1,K2) = inf{\u2016k1 \u2212 k2\u2016 : k1 \u2208 K1, k2 \u2208 K2} be the distance between sets K1 and K2. Define the sets of \u201cclosest points\u201d as\nE = {p \u2208 P | d(p,Q) = d(P,Q)} H = {q \u2208 Q | d(q, P ) = d(Q,P )}, (4) and let v = \u03a0Q\u2212P 0 (see Figure 1). Note that H = E + v, and when P \u2229 Q 6= \u2205 we have v = 0 and E = H = P \u2229 Q. Therefore, we can think of the pair (E,H) as a generalization of the intersection P \u2229Q to the setting where P and Q do not intersect. Pairs of points (e, e+ v) \u2208 E\u00d7H are solutions to the best-approximation problem between P and Q. In our analysis, we will mostly study the translated version Q\u2032 = Q\u2212 v of Q that intersects P at E. For x \u2208 RD\\E, the function \u03ba relates the distance to E with the distances to P and Q\u2032,\n\u03ba(x) = d(x,E)\nmax{d(x, P ), d(x,Q\u2032)} .\nIf \u03ba is bounded, then whenever x is close to both P and Q\u2032, it must also be close to their intersection. If, for example, D \u2265 2 and P and Q are balls of radius one whose centers are separated by distance\nexactly two, then \u03ba is unbounded. The maximum \u03ba\u2217 = supx\u2208(P\u222aQ\u2032)\\E \u03ba(x) is useful for bounding the convergence rate.\nTheorem 2. Let P and Q be convex sets, and suppose that \u03ba\u2217 < \u221e. Then AP between P and Q converges linearly with rate 1\u2212 1\u03ba2\n\u2217\n. Specifically,\n\u2016pk \u2212 p\u2217\u2016 \u2264 2\u2016p0 \u2212 p\u2217\u2016(1\u2212 1\u03ba2 \u2217 )k and \u2016qk \u2212 q\u2217\u2016 \u2264 2\u2016q0 \u2212 q\u2217\u2016(1\u2212 1\u03ba2 \u2217 )k."}, {"heading": "3.2 Relating \u03ba\u2217 to the Angles Between Faces of the Polyhedra", "text": "In this section, we consider the case of polyhedra P and Q, and we bound \u03ba\u2217 in terms of the angles between pairs of their faces. In Lemma 3, we show that \u03ba is nondecreasing along the sequence of points generated by AP between P and Q\u2032. We treat points p for which \u03ba(p) = 1 separately because those are the points from which AP between P and Q\u2032 converges in one step. This lemma enables us to bound \u03ba(p) by initializing AP at p and bounding \u03ba at some later point in the resulting sequence.\nLemma 3. For any p \u2208 P\\E, either \u03ba(p) = 1 or 1 < \u03ba(p) \u2264 \u03ba(\u03a0Q\u2032p). Similarly, for any q \u2208 Q\u2032\\E, either \u03ba(q) = 1 or 1 < \u03ba(q) \u2264 \u03ba(\u03a0P q).\nWe can now bound \u03ba by angles between faces of P and Q.\nProposition 4. If P and Q are polyhedra and p \u2208 P\\E, then there exist faces Px and Qy such that\n1\u2212 1 \u03ba(p)2 \u2264 cF (aff0(Px), aff0(Qy))2.\nThe analogous statement holds when we replace p \u2208 P\\E with q \u2208 Q\u2032\\E.\nNote that aff0(Qy) = aff0(Q\u2032y). Proposition 4 immediately gives us the following corollary.\nCorollary 5. If P and Q are polyhedra, then\n1\u2212 1 \u03ba2\u2217 \u2264 max x,y\u2208RD cF (aff0(Px), aff0(Qy)) 2."}, {"heading": "3.3 Angles Between Subspaces and Singular Values", "text": "Corollary 5 leaves us with the task of bounding the Friedrichs angle. To do so, we first relate the Friedrichs angle to the singular values of certain matrices in Lemma 6. We then specialize this to base polyhedra of submodular functions. For convenience, we prove Lemma 6 in Appendix A.5, though this result is implicit in the characterization of principal angles between subspaces given in [27, Section 1]. Ideas connecting angles between subspaces and eigenvalues are also used by Diaconis et al. [14].\nLemma 6. Let S and T be matrices with orthonormal rows and with equal numbers of columns. If all of the singular values of ST\u22a4 equal one, then cF (null(S), null(T )) = 0. Otherwise, cF (null(S), null(T )) is equal to the largest singular value of ST\u22a4 that is less than one.\nFaces of relevant polyhedra. Let Ax and By be faces of the polyhedra A and B from Lemma 1. Since A is a vector space, its only nonempty face is Ax = A. Hence, Ax = null(S), where S is an N \u00d7NR matrix of N \u00d7N identity matrices IN :\nS = 1\u221a R\n(\nIN \u00b7 \u00b7 \u00b7 IN \ufe38 \ufe37\ufe37 \ufe38\nrepeated R times\n)\n. (5)\nThe matrix for aff0(By) requires a bit more elaboration. Since B is a Cartesian product, we have By = B(F1)y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 B(FR)yR , where y = (y1, . . . , yR) and B(Fr)yr is a face of B(Fr). To proceed, we use the following characterization of faces of base polytopes [2, Proposition 4.7].\nProposition 7. Let F be a submodular function, and let B(F )x be a face of B(F ). Then there exists a partition of V into disjoint sets A1, . . . , AM such that\naff(B(F )x) =\nM\u22c2\nm=1\n{s \u2208 RN | s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Am) = F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAm)}.\nThe following corollary is immediate.\nCorollary 8. Define F , B(F )x, and A1, . . . , AM as in Proposition 7. Then\naff0(B(F )x) =\nM\u22c2\nm=1\n{s \u2208 RN | s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Am) = 0}.\nBy Corollary 8, for each Fr, there exists a partition of V into disjoint sets Ar1, . . . , ArMr such that\naff0(By) = R\u22c2\nr=1\nMr\u22c2\nm=1\n{(s1, . . . , sR) \u2208 RNR | sr(Ar1 \u222a \u00b7 \u00b7 \u00b7 \u222a Arm) = 0}. (6)\nIn other words, we can write aff0(By) as the nullspace of either of the matrices\nT \u2032 =\n\n            \n1\u22a4A11 ...\n1\u22a4A11\u222a\u00b7\u00b7\u00b7\u222aA1M1 . . .\n1\u22a4AR1 ...\n1\u22a4AR1\u222a\u00b7\u00b7\u00b7\u222aARMR\n\n             or T =\n\n                \n1\u22a4A11\u221a |A11| ...\n1\u22a4A1M1\u221a |A1M1 | . . .\n1\u22a4AR1\u221a |AR1| ...\n1\u22a4ARMR\u221a |ARMR |\n\n                 ,\nwhere 1A is the indicator vector of A \u2286 V . For T \u2032, this follows directly from Equation (6). T can be obtained from T \u2032 via left multiplication by an invertible matrix, so T and T \u2032 have the same nullspace. Lemma 6 then implies that cF (aff0(Ax), aff0(By)) equals the largest singular value of\nST\u22a4 = 1\u221a R ( 1A11\u221a |A11| \u00b7 \u00b7 \u00b7 1A1M1\u221a|A1M1 | \u00b7 \u00b7 \u00b7 1AR1\u221a |AR1| \u00b7 \u00b7 \u00b7 1ARMR\u221a|ARMR | )\nthat is less than one. We rephrase this conclusion in the following remark.\nRemark 9. The largest eigenvalue of (ST\u22a4)\u22a4(ST\u22a4) less than one equals cF (aff0(Ax), aff0(By))2.\nLet Mall = M1 + \u00b7 \u00b7 \u00b7+MR. Then (ST\u22a4)\u22a4(ST\u22a4) is the Mall \u00d7Mall square matrix whose rows and columns are indexed by (r,m) with 1 \u2264 r \u2264 R and 1 \u2264 m \u2264 Mr and whose entry corresponding to row (r1,m1) and column (r2,m2) equals\n1\nR\n1\u22a4Ar1m1 1Ar2m2 \u221a\n|Ar1m1 ||Ar2m2 | =\n1\nR\n|Ar1m1 \u2229 Ar2m2 | \u221a\n|Ar1m1 ||Ar2m2 | ."}, {"heading": "3.4 Bounding the Relevant Eigenvalues", "text": "It remains to bound the largest eigenvalue of (ST\u22a4)\u22a4(ST\u22a4) that is less than one. To do so, we view the matrix in terms of the symmetric normalized Laplacian of a weighted graph. Let G be the graph whose vertices are indexed by (r,m) with 1 \u2264 r \u2264 R and 1 \u2264 m \u2264 Mr. Let the edge between vertices (r1,m1) and (r2,m2) have weight |Ar1m1 \u2229 Ar2m2 |. We may assume that G is connected (the analysis in this case subsumes the analysis in the general case). The symmetric normalized Laplacian L of this graph is closely related to our matrix of interest,\n(ST\u22a4)\u22a4(ST\u22a4) = I \u2212 R\u22121R L. (7)\nHence, the largest eigenvalue of (ST\u22a4)\u22a4(ST\u22a4) that is less than one can be determined from the smallest nonzero eigenvalue \u03bb2(L) of L. We bound \u03bb2(L) via Cheeger\u2019s inequality (stated in Appendix A.6) by bounding the Cheeger constant hG of G. Lemma 10. For R \u2265 2, we have hG \u2265 2NR and hence \u03bb2(L) \u2265 2N2R2 .\nWe prove Lemma 10 in Appendix A.7. Combining Remark 9, Equation (7), and Lemma 10, we obtain the following bound on the Friedrichs angle.\nProposition 11. Assuming that R \u2265 2, we have cF (aff0(Ax), aff0(By))2 \u2264 1\u2212 R\u22121R 2N2R2 \u2264 1\u2212 1N2R2 .\nTogether with Theorem 2 and Corollary 5, Proposition 11 implies the final bound on the rate.\nTheorem 12. The AP algorithm for Problem (P4) converges linearly with rate 1\u2212 1N2R2 , i.e.,\n\u2016ak \u2212 a\u2217\u2016 \u2264 2\u2016a0 \u2212 a\u2217\u2016(1\u2212 1N2R2 )k and \u2016bk \u2212 b\u2217\u2016 \u2264 2\u2016b0 \u2212 b\u2217\u2016(1\u2212 1N2R2 )k."}, {"heading": "4 A Lower Bound", "text": "To probe the tightness of Theorem 12, we construct a \u201cbad\u201d submodular function and decomposition that lead to a slow rate. Appendix B gives the formal details. Our example is an augmented cut function on a cycle: for each x, y \u2208 V , define Gxy to be the cut function of a single edge (x, y),\nGxy = { 1 if |A \u2229 {x, y}| = 1 0 otherwise .\nTake N to be even and R \u2265 2 and define the submodular function F lb = F lb1 + \u00b7 \u00b7 \u00b7+ F lbR , where F lb1 = G12 +G34 + \u00b7 \u00b7 \u00b7+G(N\u22121)N F lb2 = G23 +G45 + \u00b7 \u00b7 \u00b7+GN1\nand F lbr = 0 for all r \u2265 3. The optimal solution to the best-approximation problem is the all zeros vector.\nLemma 13. The cosine of the Friedrichs angle between A and aff(Blb) is cF (A, aff(Blb))2 = 1\u2212 1R ( 1\u2212 cos ( 2\u03c0 N )) .\nAround the optimal solution 0, the polyhedra A and Blb behave like subspaces, and it is possible to pick initializations a0 \u2208 A and b0 \u2208 Blb such that the Friedrichs angle exactly determines the rate of convergence. That means 1\u2212 1/\u03ba2\u2217 = cF (A, aff(Blb))2, and\n\u2016ak\u2016 = (1\u2212 1R (1\u2212 cos(2\u03c0N )))k\u2016a0\u2016 and \u2016bk\u2016 = (1\u2212 1R (1\u2212 cos(2\u03c0N )))k\u2016b0\u2016.\nBounding 1\u2212 cos(x) \u2264 12x2 leads to the following lower bound on the rate. Theorem 14. There exists a decomposed function F lb and initializations for which the convergence rate of AP is at least 1\u2212 2\u03c02N2R .\nThis theoretical bound can also be observed empirically (Figure 3 in Appendix B)."}, {"heading": "5 Convergence of the Primal Objective", "text": "We have shown that AP generates a sequence of points {ak}k\u22650 and {bk}k\u22650 in RNR such that (ak, bk) \u2192 (a\u2217, b\u2217) linearly, where (a\u2217, b\u2217) minimizes the objective in Problem (P4). In this section, we show that this result also implies the linear convergence of the objective in Problem (P3) and of the original discrete objective in Problem (P1). The proofs may be found in Appendix C.\nDefine the matrix \u0393 = \u2212R1/2S, where S is the matrix defined in Equation (5). Multiplication by \u0393 maps a vector (w1, . . . , wR) to \u2212 \u2211\nr wr, where wr \u2208 RN for each r. Set xk = \u0393bk and x\u2217 = \u0393b\u2217. As shown in Jegelka et al. [25], Problem (P3) is minimized by x\u2217. Proposition 15. We have f(xk) + 12\u2016xk\u20162 \u2192 f(x\u2217) + 12\u2016x\u2217\u20162 linearly with rate 1\u2212 1N2R2 .\nThis linear rate of convergence translates into a linear rate for the original discrete problem.\nTheorem 16. Choose A\u2217 \u2208 argminA\u2286V F (A). Let Ak be the suplevel set of xk with smallest value of F . Then F (Ak) \u2192 F (A\u2217) linearly with rate 1\u2212 12N2R2 ."}, {"heading": "6 Discussion", "text": "In this work, we analyze projection methods for parallel SFM and give upper and lower bounds on the linear rate of convergence. This means that the number of iterations required for an accuracy of \u01eb is logarithmic in 1/\u01eb, not linear as in previous work [35]. Our rate is uniform over all submodular functions. Moreover, our proof highlights how the number R of components and the facial structure of B affect the convergence rate. These insights may serve as guidelines when working with projection algorithms and aid in the analysis of special cases. For example, reducing R is often possible. Any collection of Fr that have disjoint support, such as the cut functions corresponding to the rows or columns of a grid graph, can be grouped together without making the projection harder.\nOur analysis also shows the effects of additional properties of F . For example, suppose that F is separable, that is, F (V ) = F (S) + F (V \\S) for some nonempty S ( V . Then the subsets Arm \u2286 V defining the relevant faces of B satisfy either Arm \u2286 S or Arm \u2286 Sc [2]. This makes G in Section 3.4 disconnected, and as a result, the N in Theorem 12 gets replaced by max{|S|, |Sc|} for an improved rate. This applies without the user needing to know S when running the algorithm.\nA number of future directions suggest themselves. For example, Jegelka et al. [25] also considered the related Douglas\u2013Rachford (DR) algorithm. DR between subspaces converges linearly with rate cF [6], as opposed to c2F for AP. We suspect that our approach may be modified to analyze DR between polyhedra. Further questions include the extension to cyclic updates (instead of parallel ones), multiple polyhedra, and stochastic algorithms.\nAcknowledgments. We would like to thank Ma\u0306da\u0306lina Persu for suggesting the use of Cheeger\u2019s inequality. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Apple, C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and Yahoo!. This work is supported in part by the Office of Naval Research under grant number N00014-11-1-0688, the US ARL and the US ARO under grant number W911NF-11-1-0391, and the NSF under grant number DGE-1106400."}, {"heading": "A Upper Bound Results", "text": ""}, {"heading": "A.1 Proof of Theorem 2", "text": "For the proof of this theorem, we will need the fact that projection maps are firmly nonexpansive, that is, for a closed convex nonempty subset C \u2286 RD, we have\n\u2016\u03a0Cx\u2212\u03a0Cy\u20162 + \u2016(x\u2212\u03a0Cx)\u2212 (y \u2212\u03a0Cy)\u20162 \u2264 \u2016x\u2212 y\u20162 for all x, y \u2208 RD. Now, suppose that \u03ba\u2217 < \u221e. Let e = \u03a0Epk and note that v = \u03a0Qe \u2212 e and that \u03a0Qe \u2208 H . We have\n\u03ba\u22122\u2217 d(pk, E) 2 \u2264 d(pk, Q\u2032)2\n\u2264 \u2016pk \u2212\u03a0Qpk + v\u20162\n\u2264 \u2016(pk \u2212\u03a0Qpk)\u2212 (e \u2212\u03a0Qe)\u20162\n\u2264 \u2016pk \u2212 e\u20162 \u2212 \u2016\u03a0Qpk \u2212\u03a0Qe\u20162\n\u2264 d(pk, E)2 \u2212 d(qk, H)2. It follows that d(qk, H) \u2264 (1 \u2212 \u03ba\u22122\u2217 )1/2d(pk, E). Similarly, we have d(pk+1, E) \u2264 (1 \u2212 \u03ba\u22122\u2217 ) 1/2d(qk, H). When combining these, induction shows that\nd(pk, E) \u2264 (1\u2212 \u03ba\u22122\u2217 )kd(p0, E) d(qk, H) \u2264 (1\u2212 \u03ba\u22122\u2217 )kd(q0, H).\nAs shown in [3, Theorem 3.3], the above implies that pk \u2192 p\u2217 \u2208 E and qk \u2192 q\u2217 \u2208 H and that \u2016pk \u2212 p\u2217\u2016 \u2264 2\u2016p0 \u2212 p\u2217\u2016(1\u2212 \u03ba\u22122\u2217 )k\n\u2016qk \u2212 q\u2217\u2016 \u2264 2\u2016q0 \u2212 q\u2217\u2016(1\u2212 \u03ba\u22122\u2217 )k."}, {"heading": "A.2 Connection Between \u03ba and cF in the Subspace Case", "text": "In this section, we introduce a simple lemma connecting \u03ba and cF in the case of subspaces U and V . We will use this lemma in several subsequent proofs. Lemma 17. Let U and V be subspaces and suppose u \u2208 U \u2229 (U \u2229 V )\u22a5 and that u 6= 0. Then\n(a) \u2016\u03a0V u\u2016 \u2264 cF (U, V )\u2016u\u2016 (b) \u03ba(u) \u2264 (1\u2212 cF (U, V )2)\u22121/2 (c) \u03ba(u) = (1\u2212 cF (U, V )2)\u22121/2 if and only if \u2016\u03a0V u\u2016 = cF (U, V )\u2016u\u2016.\nProof. Part (a) follows from the definition of cF . Indeed,\ncF (U, V ) \u2265 u\u22a4(\u03a0V u)\n\u2016u\u2016\u2016\u03a0V u\u2016 = \u2016\u03a0V u\u20162 \u2016u\u2016\u2016\u03a0V u\u2016 = \u2016\u03a0V u\u2016 \u2016u\u2016 .\nPart (b) follows from Part (a) and the observation that \u03ba(u) = (1 \u2212 \u2016\u03a0V u\u20162/\u2016u\u20162)\u22121/2. Part (c) follows from the same observation."}, {"heading": "A.3 Proof of Lemma 3", "text": "It suffices to prove the statement for p \u2208 P\\E. For p \u2208 P\\E, define q = \u03a0Q\u2032p, e = \u03a0Eq, and p\u2032\u2032 = \u03a0[p,e]q, where [p, e] denotes the line segment between p and e (which is contained in P by convexity). See Figure 2 for a graphical depiction. If q \u2208 E, then \u03ba(p) = 1. So we may assume that q /\u2208 E which also implies that d(p\u2032\u2032, E) > 0 and d(\u03a0P q, E) > 0. We have\n\u03ba(p) = d(p,E) d(p,Q\u2032) \u2264 \u2016p\u2212 e\u2016\u2016p\u2212 q\u2016 \u2264 \u2016q \u2212 e\u2016 \u2016q \u2212 p\u2032\u2032\u2016 \u2264 d(q, E) d(q, P ) = \u03ba(q). (8)\nThe first inequality holds because d(p,E) \u2264 \u2016p\u2212e\u2016 and d(p,Q\u2032) = \u2016p\u2212q\u2016. The middle inequality holds because the area of the triangle with vertices p, q, and e can be expressed as both 12\u2016p\u2212e\u2016\u2016q\u2212 p\u2032\u2032\u2016 and 12\u2016p\u2212 q\u2016\u2016q \u2212 e\u2016 sin \u03b8, where \u03b8 is the angle between vectors p\u2212 q and e\u2212 q, so \u2016p\u2212 e\u2016\u2016q \u2212 p\u2032\u2032\u2016 = \u2016p\u2212 q\u2016\u2016q \u2212 e\u2016 sin \u03b8 \u2264 \u2016p\u2212 q\u2016\u2016q \u2212 e\u2016. The third inequality holds because \u2016q \u2212 e\u2016 = d(q, E) and \u2016q \u2212 p\u2032\u2032\u2016 \u2265 d(q, P ). The chain of inequalities in Equation (8) prove the lemma."}, {"heading": "A.4 Proof of Proposition 4", "text": "Suppose that p \u2208 P\\E (the case q \u2208 Q\u2032\\E is the same), and let e = \u03a0Ep. If \u03ba(p) = 1, the statement is evident, so we may assume that \u03ba(p) > 1. We will construct sequences of polyhedra\nP \u2287 P1 \u2287 \u00b7 \u00b7 \u00b7 \u2287 PJ Q\u2032 \u2287 Q\u20321 \u2287 \u00b7 \u00b7 \u00b7 \u2287 Q\u2032J .\nwhere Pj+1 is a face of Pj and Q\u2032j+1 is a face of Q \u2032 j for 1 \u2264 j \u2264 J \u2212 1. Either dim(aff(Pj+1)) < dim(aff(Pj)) or dim(aff(Q\u2032j+1)) < dim(aff(Q \u2032 j)) will hold. We will further define Ej = Pj \u2229Q\u2032j , which will contain e, so that we can define\n\u03baj(x) = d(x,Ej)\nmax{d(x, Pj), d(x,Q\u2032j)} for x \u2208 RD\\Ej (this is just the function \u03ba defined for the polyhedra Pj and Q\u2032j). Our construction will yield points pj \u2208 Pj , and qj \u2208 Q\u2032j such that pj \u2208 relint(Pj)\\Ej , qj \u2208 relint(Q\u2032j)\\Ej , and qj = \u03a0Q\u2032jpj for each j. Furthermore, we will have\n\u03ba(p) \u2264 \u03ba1(p1) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03baJ(pJ ). (9)\nNow we describe the construction. For any t \u2208 [0, 1], define pt = (1 \u2212 t)p + te to be the point obtained by moving p by the appropriate amount toward e. Note that t 7\u2192 \u03ba(pt) is a nondecreasing function on the interval [0, 1). Choose \u01eb > 0 sufficiently small so that every face of either P or Q\u2032 that intersects B\u01eb(e), the ball of radius \u01eb centered on e, necessarily contains e. Now choose 0 \u2264 t0 < 1 sufficiently close to 1 so that \u2016pt0 \u2212 e\u2016 < \u01eb. It follows that e is contained in the face of P whose relative interior contains pt0 . It further follows that e is contained in the face of Q\u2032 whose relative interior contains \u03a0Q\u2032pt0 because\n\u2016\u03a0Q\u2032pt0 \u2212 e\u2016 = \u2016\u03a0Q\u2032pt0 \u2212\u03a0Q\u2032e\u2016 \u2264 \u2016pt0 \u2212 e\u2016 < \u01eb.\nTo initialize the construction, set\np1 = p t0\nq1 = \u03a0Q\u2032p t0 ,\nand let P1 and Q\u20321 be the unique faces of P and Q \u2032 respectively such that p1 \u2208 relint(P1) and q1 \u2208 relint(Q\u20321) (the relative interiors of the faces of a polyhedron partition that polyhedron [8, Theorem 2.2]). Note that q1 /\u2208 E because \u03ba(p1) \u2265 \u03ba(p) > 1. Note that e \u2208 E1 = P1 \u2229Q\u20321 so that\n\u03ba(p) \u2264 \u03ba(p1) = d(p1, E)\nd(p1, Q\u2032) = \u2016p1 \u2212 e\u2016 \u2016p1 \u2212 q1\u2016 = d(p1, E1) d(p1, Q\u20321) = \u03ba1(p1).\nNow, inductively assume that we have defined Pj , Q\u2032j , pj , and qj satisfying the stated properties. Generate the sequences {xk}k\u22650 and {yk}k\u22650 with xk \u2208 Pj and yk \u2208 Q\u2032j by running AP between the polyhedraPj andQ\u2032j initialized with x0 = pj . There are two possibilities, either xk \u2208 relint(Pj) and yk \u2208 relint(Q\u2032j) for every k, or there is some k for which either xk /\u2208 relint(Pj) or yk /\u2208 relint(Q\u2032j). Note that Pj and Q \u2032 j intersect and that AP between them will not terminate after a finite number of steps.\nSuppose that xk \u2208 relint(Pj) and yk \u2208 relint(Q\u2032j) for every k. Then set J = j and terminate the procedure. Otherwise, choose k\u2032 such that either xk\u2032 /\u2208 relint(Pj) or yk\u2032 /\u2208 relint(Q\u2032j). Now set pj+1 = xk\u2032 and qj+1 = yk\u2032 . Let Pj+1 and Q\u2032j+1 be the unique faces of Pj and Q \u2032 j respectively such that pj+1 \u2208 relint(Pj+1) and qj+1 \u2208 relint(Q\u2032j+1). Note that pj+1, qj+1 /\u2208 Ej+1 = Pj+1 \u2229 Q\u2032j+1 and e \u2208 Ej+1. We have\n\u03baj(pj) < \u03baj(pj+1) = d(pj+1, Ej)\nd(pj+1, Q\u2032j) =\nd(pj+1, Ej) \u2016pj+1 \u2212 qj+1\u2016 \u2264 d(pj+1, Ej+1) d(pj+1, Q\u2032j+1) = \u03baj+1(pj+1).\nThe preceding work shows the inductive step. Note that if Pj+1 6= Pj then dim(aff(Pj+1)) < dim(aff(Pj)) and if Q\u2032j+1 6= Q\u2032j then dim(aff(Q\u2032j+1)) < dim(aff(Q\u2032j)). One of these will hold, so the induction will terminate after a finite number of steps.\nWe have produced the sequence in Equation (9) and we have created pJ , PJ , and Q\u2032J such that AP between PJ and Q\u2032J , when initialized at pJ , generates the same sequence of points as AP between aff(PJ ) and aff(Q\u2032J). Using this fact, along with [12, Theorem 9.3], we see that \u03a0aff(PJ )\u2229aff(Q\u2032J )pJ \u2208 EJ . Using this, along with Lemma 17(b), we see that \u03baJ(pJ ) \u2264 (1\u2212 cF (aff0(PJ ), aff0(Q\u2032J))2)\u22121/2. (10) Equations (10) and (9) prove the result. Note that PJ and Q\u2032J are faces of P and Q\n\u2032 respectively. We can switch between faces of Q\u2032 and faces of Q because doing so amounts to translating by v which does not affect the angles."}, {"heading": "A.5 Proof of Lemma 6", "text": "We have\ncF (null(S), null(T )) = cF (range(S \u22a4)\u22a5, range(T\u22a4)\u22a5)\n= cF (range(S \u22a4), range(T\u22a4)),\nwhere the first equality uses the fact that null(W ) = range(W\u22a4)\u22a5 for matrices W , and the second equality uses the fact that cF (U\u22a5, V \u22a5) = cF (U, V ) for subspaces U and V [6, Fact 2.3].\nLet S\u22a4 and T\u22a4 have dimensions D\u00d7 J and D\u00d7K respectively, and let X and Y be the subspaces spanned by the columns of S\u22a4 and T\u22a4 respectively. Without loss of generality, assume that J \u2264 K . Let \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3J be the singular values of ST\u22a4 with corresponding left singular vectors u1, . . . , uJ and right singular vectors v1, . . . , vJ . Let xj = S\u22a4uj and let yj = T\u22a4vj for 1 \u2264 j \u2264 J . By definition, we can write\n\u03c3j = max u,v\n{u\u22a4ST\u22a4v |u \u22a5 span(u1, . . . , uj\u22121), v \u22a5 span(v1, . . . , vj\u22121), \u2016u\u2016 = 1, \u2016v\u2016 = 1}.\nSince the {uj}j are orthonormal, so are the {xj}j . Similarly, since the {vj}j are orthonormal, so are the {yj}j . Suppose that all of the singular values of ST\u22a4 equal one. Then we must have xj = yj for each j, which implies that X \u2286 Y , and so cF (X,Y ) = 0. Now suppose that \u03c31 = \u00b7 \u00b7 \u00b7 = \u03c3\u2113 = 1, and \u03c3\u2113+1 6= 1. It follows that\nX \u2229 Y = span(x1, . . . , x\u2113) = span(y1, . . . , y\u2113), and so\n\u03c3\u2113+1 = sup u,v\n{u\u22a4ST\u22a4v |u \u2208 span(u1, . . . , u\u2113)\u22a5, v \u2208 span(v1, . . . , v\u2113)\u22a5, \u2016u\u2016 = 1, \u2016v\u2016 = 1}\n= sup x,y\n{x\u22a4y |x \u2208 X \u2229 (X \u2229 Y )\u22a5, y \u2208 Y \u2229 (X \u2229 Y )\u22a5, \u2016x\u2016 = 1, \u2016y\u2016 = 1}\n= cF (X,Y )."}, {"heading": "A.6 Cheeger\u2019s Inequality", "text": "For an overview of spectral graph theory, see Chung [9]. We state Cheeger\u2019s inequality below.\nLet G be a weighted, connected graph with vertex set VG and edge weights (wij)i,j\u2208VG . Define the weighted degree of a vertex i to be \u03b4i = \u2211\nj 6=i wij , define the volume of a subset of vertices to be the sum of their weighted degrees, vol(U) = \u2211\ni\u2208U \u03b4i, and define the size of the cut between U and its complement U c to be the sum of the weights of the edges between U and U c,\n|E(U,U c)| = \u2211\ni\u2208U,j\u2208Uc wij .\nThe Cheeger constant is defined as\nhG = min \u22056=U(VG |E(U,U c)| min(vol(U), vol(U c)) .\nLet L be the unnormalized Laplacian of G, i.e. the |VG| \u00d7 |VG| matrix whose entries are defined by\nLij = { \u2212wij i 6= j \u03b4i otherwise .\nLet D be the |VG| \u00d7 |VG| diagonal matrix defined by Dii = \u03b4i. Then L = D\u22121/2LD\u22121/2 is the normalized Laplacian. Let \u03bb2(L) denote the second smallest eigenvalue of L (since G is connected, there will be exactly one eigenvalue equal to zero).\nTheorem 18 (Cheeger\u2019s inequality). We have \u03bb2(L) \u2265 h 2 G\n2 ."}, {"heading": "A.7 Proof of Lemma 10", "text": "Proof. We have\nmin(vol(U), vol(U c)) \u2264 1 2 vol(VG)\n= 1\n2\n\u2211\n(r,m)\n\n \u2211\n(r\u2032,m\u2032) 6=(r,m) |Arm \u2229 Ar\u2032m\u2032 |\n\n\n= 1\n2\n\u2211\n(r,m)\n(R\u2212 1)|Arm|\n= 1\n2 NR(R\u2212 1).\nSince G is connected, for any nonempty set U ( VG, there must be some element v \u2208 V (here V is the ground set of our submodular functionF , not the set of vertices VG) such that v \u2208 Ar1m1\u2229Ar2m2 for some (r1,m1) \u2208 U and (r2,m2) \u2208 U c. Suppose that v appears in k of the subsets of V indexed by elements of U and in R\u2212 k of the subsets of V indexed by elements of U c. Then\n|E(U,U c)| \u2265 k(R \u2212 k) \u2265 R \u2212 1. It follows that\nhG \u2265 R\u2212 1\n1 2NR(R\u2212 1)\n= 2\nNR .\nIt follows from Theorem 18 that \u03bb2(L) \u2265 2N2R2 ."}, {"heading": "B Results for the Lower Bound", "text": ""}, {"heading": "B.1 Some Helpful Results", "text": "In Lemma 19, we show how AP between subspaces U and V can be initialized to exactly achieve the worst-case rate of convergence. Then in Corollary 20, we show that if subsets U \u2032 and V \u2032 look like subspaces U and V near the origin, we can initialize AP between U \u2032 and V \u2032 to achieve the same worst-case rate of convergence.\nLemma 19. Let U and V be subspaces with U 6\u2286 V and V 6\u2286 U . Then there exists some nonzero point u0 \u2208 U \u2229 (U \u2229 V )\u22a5 such that when we initialize AP at u0, the resulting sequences {uk}k\u22650 and {vk}k\u22650 satisfy\n\u2016uk\u2016 = cF (U, V )2k\u2016u0\u2016 \u2016vk\u2016 = cF (U, V )2k\u2016v0\u2016.\nProof. Find u\u2217 \u2208 U \u2229 (U \u2229 V )\u22a5 and v\u2217 \u2208 V \u2229 (U \u2229 V )\u22a5 with \u2016u\u2217\u2016 = 1 and \u2016v\u2217\u2016 = 1 such that u\u22a4\u2217 v\u2217 = cF (U, V ), which we can do by compactness. By Lemma 17(a),\ncF (U, V ) = v \u22a4 \u2217 u\u2217 = v \u22a4 \u2217 \u03a0V u\u2217 \u2264 \u2016\u03a0V u\u2217\u2016 \u2264 cF (U, V ).\nSet u0 = u\u2217 and generate the sequences {uk}k\u22650 and {vk}k\u22650 via AP. Since \u2016\u03a0V u0\u2016 = cF (U, V ), Lemma 17(c) implies that \u03ba(u0) = (1 \u2212 cF (U, V )2)\u22121/2. Since \u03ba attains its maximum at u0, Lemma 3 implies that \u03ba attains the same value at every element of the sequences {uk}k\u22650 and {vk}k\u22650. Therefore, Lemma 17(c) implies that \u2016\u03a0V uk\u2016 = cF (U, V )\u2016uk\u2016 and \u2016\u03a0Uvk\u2016 = cF (U, V )\u2016vk\u2016 for all k. This proves the lemma. Corollary 20. Let U and V be subspaces with U 6\u2286 V and V 6\u2286 U . Let U \u2032 \u2286 U and V \u2032 \u2286 V be subsets such that U \u2032\u2229B\u01eb(0) = U \u2229B\u01eb(0) and V \u2032\u2229B\u01eb(0) = V \u2229B\u01eb(0) for some \u01eb > 0. Then there is a point u\u20320 \u2208 U \u2032 such that the sequences {u\u2032k}k\u22650 and {v\u2032k}k\u22650 generated by AP between U \u2032 and V \u2032 initialized at u\u20320 satisfy\n\u2016u\u2032k\u2016 = cF (U, V )2k\u2016u\u20320\u2016 \u2016v\u2032k\u2016 = cF (U, V )2k\u2016v\u20320\u2016.\nProof. Use Lemma 19 to choose some nonzero u0 \u2208 U \u2229 (U \u2229 V )\u22a5 satisfying this property. Then set u\u20320 = \u01eb \u2016u0\u2016u0."}, {"heading": "B.2 Proof of Lemma 13", "text": "Observe that we can write\naff(Blb) = {(s1,\u2212s1, . . . , sN 2 ,\u2212sN 2 ,\u2212tN 2 , t1,\u2212t1, . . . , tN 2 , 0, . . . , 0, . . . , 0, . . . , 0) | si, tj \u2208 R}.\nWe can write aff(Blb) as the nullspace of the matrix\nTlb =\n\n    \nTlb,1 Tlb,2\nIN . . .\nIN\n\n     ,\nwhere the N\u00d7N identity matrix IN is repeated R\u22122 times and where Tlb,1 and Tlb,2 are the N2 \u00d7N matrices\nTlb,1 = 1\u221a 2\n\n  \n1 1 1 1\n. . . 1 1\n\n  \nTlb,2 = 1\u221a 2\n\n  \n1 1 1 1\n. . . 1 1\n\n   .\nRecall that we can write A as the nullspace of the matrix S defined in Equation (5). It follows from Lemma 6 that cF (A, aff(Blb)) equals the largest singular value of ST\u22a4lb that is less than one. We have\nST\u22a4lb = 1\u221a R ( T\u22a4lb,1 T \u22a4 lb,2 IN \u00b7 \u00b7 \u00b7 IN ) .\nWe can permute the columns of ST\u22a4lb without changing the singular values, so cF (A, aff(Blb)) equals the largest singular value of\n1\u221a R ( T\u22a4lb,0 IN \u00b7 \u00b7 \u00b7 IN ) ,\nthat is less than one, where Tlb,0 is the N \u00d7N circulant matrix\nTlb,0 = 1\u221a 2\n\n    \n1 1 1 1\n. . . 1 1\n1 1\n\n     .\nTherefore, cF (A, aff(Blb))2 equals the largest eigenvalue of 1 R ( T\u22a4lb,0 IN \u00b7 \u00b7 \u00b7 IN ) ( T\u22a4lb,0 IN \u00b7 \u00b7 \u00b7 IN )\u22a4 = 1R ( T\u22a4lb,0Tlb,0 + (R\u2212 2)IN ) that is less than one. Therefore, it suffices to examine the N \u00d7N circulant matrix\nT\u22a4lb,0Tlb,0 = 1\n2\n\n    \n2 1 1 1 2\n. . . 2 1\n1 1 2\n\n     .\nThe eigenvalues of T\u22a4lb,0Tlb,0 are given by \u03bbj = 1 + cos ( 2\u03c0j N ) for 0 \u2264 j \u2264 N \u2212 1 (see Gray [18, Section 3.1] for a derivation). Therefore,\ncF (A, aff(Blb))2 = 1\u2212 1R (1\u2212 cos(2\u03c0N ))."}, {"heading": "B.3 Lower Bound Illustration", "text": "The proof of Theorem 14 shows that there is some a0 \u2208 A such that when we initialize AP between A and Blb at a0, we generate a sequence {ak}k\u22650 satisfying\nd(ak, E) = (1\u2212 1R (1 \u2212 cos(2\u03c0N ))kd(a0, E), where E = A \u2229 Blb is the optimal set. In Figure 3, we plot the theoretical bound in red, and in blue the successive ratios d(ak+1, E)/d(ak, E) for five runs of AP between A and Blb with random initializations. Had we initialized AP at a0, the successive ratios would exactly equal 1 \u2212 1R (1 \u2212 cos(2\u03c0N )). The plot of these ratios would coincide with the red line in Figure 3.\nFigure 3 illustrates that the empirical behavior of AP between A and Blb is often similar to the worst-case behavior, even when the initialization is random. When we initialize AP randomly, the successive ratios appear to increase to the lower bound and then remain constant. Figure 3 shows the case N = 10 and R = 10, but the plot looks similar for other N and R.\nWe also note that the graph corresponding to our lower bound example actually achieves a Cheeger constant similar to the one used in Lemma 10."}, {"heading": "C Results for Convergence of the Primal and Discrete Problems", "text": ""}, {"heading": "C.1 Proof of Proposition 15", "text": "First, suppose that s \u2208 B(F ). Let A = {n \u2208 V | sn \u2265 0} be the set of indices on which s is nonnegative. Then we have\n\u2016s\u2016 \u2264 \u2016s\u20161 = 2s(A)\u2212 s(V ) \u2264 3Fmax. (11)\nRecall that we defined Fmax = maxA |F (A)|. Now, we show that f(xk) + 12\u2016xk\u20162 converges to f(x\u2217) + 12\u2016x\u2217\u20162 linearly with rate 1 \u2212 1N2R2 . We will use Equation (11) to bound the norms of xk and x\u2217, both of which lie in \u2212B(F ). We will also use the fact that \u2016xk \u2212 x\u2217\u2016 \u2264 \u2016\u0393\u2016\u2016bk \u2212 b\u2217\u2016 \u2264\u221a R\u2016bk \u2212 b\u2217\u2016. Finally, we will use the proof of Theorem 12 to bound \u2016bk \u2212 b\u2217\u2016. First, we bound the difference between the squared norms using convexity. We have\n1 2\u2016xk\u20162 \u2212 12\u2016x\u2217\u20162 \u2264 x\u22a4k (xk \u2212 x\u2217)\n\u2264 \u2016xk\u2016\u2016xk \u2212 x\u2217\u2016 \u2264 3Fmax \u221a R\u2016bk \u2212 b\u2217\u2016 \u2264 6Fmax \u221a R\u2016b0 \u2212 b\u2217\u2016(1\u2212 1N2R2 )k. (12)\nNext, we bound the difference in Lova\u0301sz extensions. Choose s \u2208 argmaxs\u2208B(F ) s\u22a4xk . Then\nf(xk)\u2212 f(x\u2217) \u2264 s\u22a4(xk \u2212 x\u2217) \u2264 \u2016s\u2016\u2016xk \u2212 x\u2217\u2016 \u2264 3Fmax \u221a R\u2016bk \u2212 b\u2217\u2016\n\u2264 6Fmax \u221a R\u2016b0 \u2212 b\u2217\u2016(1\u2212 1N2R2 )k. (13)\nCombining the bounds (12) and (13), we find that\n(f(xk) + 1 2\u2016xk\u20162)\u2212 (f(x\u2217) + 12\u2016x\u2217\u20162) \u2264 12Fmax \u221a R\u2016b0 \u2212 b\u2217\u2016(1\u2212 1N2R2 )k. (14)"}, {"heading": "C.2 Proof of Theorem 16", "text": "We will make use of the following result, shown in [2, Proposition 10.5] and stated below for convenience.\nProposition 21. Let (w, s) \u2208 RN \u00d7B(F ) be a pair of primal-dual candidates for the minimization of 12\u2016w\u20162 + f(w), with duality gap \u01eb = 12\u2016w\u20162 + f(w) + 12\u2016s\u20162. Then if A is the suplevel set of w with smallest value of F , then\nF (A)\u2212 s\u2212(V ) \u2264 \u221a N\u01eb/2.\nUsing this result in our setting, recall that by definition Ak is the set of the form {n \u2208 V | (xk)n \u2265 c} for some constant c with smallest value of F ({n \u2208 V | (xk)n \u2265 c}). Let (w\u2217, s\u2217) \u2208 RN \u00d7B(F ) be a primal-dual optimal pair for the left-hand version of Problem (P3). The dual of this minimization problem is the projection problemmins\u2208B(F ) 1 2\u2016s\u20162. From [2, Propo-\nsition 10.5], we see that\nF (Ak)\u2212 F (A\u2217) \u2264 F (Ak)\u2212 (s\u2217)\u2212(V )\n\u2264 \u221a\nN 2 ((f(xk) + 1 2\u2016xk\u20162)\u2212 (f(x\u2217) + 12\u2016x\u2217\u20162))\n\u2264 \u221a\n6FmaxNR1/2\u2016b0 \u2212 b\u2217\u2016 (1 \u2212 1N2R2 )k/2\n\u2264 \u221a\n6FmaxNR1/2\u2016b0 \u2212 b\u2217\u2016(1\u2212 12N2R2 )k,\nwhere the third inequality uses the proof of Proposition 15. The second inequality relies on Bach [2, Proposition 10.5], which states that a duality gap of \u01eb for the left-hand version of Problem (P3) turns into a duality gap of \u221a\nN\u01eb/2 for the original discrete problem. If our algorithm converged with rate 1k , this would translate to a rate of 1\u221a k\nfor the discrete problem. But fortunately, our algorithm converges linearly, and taking a square root preserves linear convergence."}, {"heading": "C.3 Running times", "text": "Theorem 16 implies that the number of iterations required for an accuracy of \u01eb is at most\n2N2R2 log\n(\u221a\n6FmaxNR1/2\u2016b0 \u2212 b\u2217\u2016 \u01eb\n)\n. (15)\nEach iteration involves minimizing each of the Fr separately. For comparison, the number of iterations required in Stobbe and Krause [35] is\n24 \u221a NR\nFmax \u01eb .\nThe dependence of this algorithm on N and R is better, but its dependence on Fmax/\u01eb is worse. For example, to obtain the exact discrete solution, we need \u01eb < minS,T |F (S)\u2212 F (T )|. This is one for integer-valued functions (in which case the lower rate may be desirable), but can otherwise become very small. The constant Fmax can be of order O(N) in general (or even larger if the function becomes very negative). For empirical comparisons, we refer the reader to [25].\nThe running times of the combinatorial algorithm by Kolmogorov [29] apply to integer-valued functions (as opposed to the generic ones above) and range from O((N + R)2 logFmax) for cuts to O((N + Q2R)(N + Q2R + QR\u03c42) logFmax), where Q \u2264 N is the maximal cardinality of the support of any Fr , and \u03c42 is the time required to minimize a simple function. This is better than (15) if Q is a small constant, and worse as Q gets closer to N .\nFor comparison, if not exploiting decomposition, one may use combinatorial algorithms, the FrankWolfe algorithm (conditional gradient descent), or a subgradient method. The combinatorial algorithm by Orlin [34] has a running time of O(N5\u03c41 + N6), and the algorithm by Iwata [23] (for integer-valued functions) has a running time of O((N4\u03c41 +N5) logFmax), where \u03c41 is the time required to evaluate F . For an accuracy of \u01eb in the discrete objective, Frank-Wolfe will take 64N Fmax\u01eb2 iterations, each taking time O(N logN). The subgradient method behaves similarly."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Submodular functions describe a variety of discrete problems in machine learn-<lb>ing, signal processing, and computer vision. However, minimizing submodular<lb>functions poses a number of algorithmic challenges. Recent work introduced an<lb>easy-to-use, parallelizable algorithm for minimizing submodular functions that<lb>decompose as the sum of \u201csimple\u201d submodular functions. Empirically, this al-<lb>gorithm performs extremely well, but no theoretical analysis was given. In this<lb>paper, we show that the algorithm converges linearly, and we provide upper and<lb>lower bounds on the rate of convergence. Our proof relies on the geometry of<lb>submodular polyhedra and draws on results from spectral graph theory.", "creator": "LaTeX with hyperref package"}}}