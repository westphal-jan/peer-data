{"id": "1406.7444", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2014", "title": "Learning to Deblur", "abstract": "We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime.\n\n\n\n\n\nAs a visual and visualization framework, this technique may be used to better understand and integrate image deconvolution into the visual visual environment. This means that it is possible for many visual objects to be represented visually and interactively. An example of this is the example of a video game with a fully rendered image. The player controls an object by moving the hand. To illustrate this, a typical animation involves the character moving the hand. For example, a scene looks to be moving the hand with a very wide left hand. The player then controls a button which moves the hand. The user controls the button with an active key, and controls the button at a time.\n\nA simple animation involves the movement of the hands. To illustrate this, a typical animation involves the character moving the hand. For example, a scene looks to be moving the hand with a very wide left hand. The player controls a button which moves the hand. The user controls the button with an active key, and controls the button at a time.\nA simple animation involves the movement of the hands. To illustrate this, a typical animation involves the character moving the hand. For example, a scene looks to be moving the hand with a very wide left hand. The player controls a button which moves the hand. The player controls a button which moves the hand. The user controls the button with an active key, and controls the button at a time. For example, a scene looks to be moving the hand with a very wide left hand. The player controls a button which moves the hand. The player controls a button which moves the hand. The player controls a button which moves the hand. The player controls a button which moves the hand. The user controls a button which moves the hand. The user controls a button which moves the hand. The player controls a button which moves the hand. The player controls a button which moves the hand. The player controls a button which moves the hand. The user controls a button which moves the hand. The player controls a button which moves the hand. The player controls a button which moves", "histories": [["v1", "Sat, 28 Jun 2014 21:56:31 GMT  (8880kb,D)", "http://arxiv.org/abs/1406.7444v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["christian j schuler", "michael hirsch", "stefan harmeling", "bernhard sch\\\"olkopf"], "accepted": false, "id": "1406.7444"}, "pdf": {"name": "1406.7444.pdf", "metadata": {"source": "META", "title": "Learning to Deblur", "authors": ["Christian J. Schuler", "Michael Hirsch", "Stefan Harmeling", "Bernhard Sch\u00f6lkopf"], "emails": ["cschuler@tuebingen.mpg.de", "mhirsch@tuebingen.mpg.de", "harmeling@tuebingen.mpg.de", "bs@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "Learning to Deblur\nChristian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Sch\u00f6lkopf {cschuler,mhirsch,harmeling,bs}@tuebingen.mpg.de\nMax Planck Institute for Intelligent Systems\nWe describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime."}, {"heading": "1. Introduction", "text": "Blind image deconvolution is the task of recovering the underlying sharp image from a recorded image corrupted by blur and noise. Examples of such distortions are manifold: In photography, long exposure times might result in camera shake, often in combination with image noise caused by low light conditions. Lens aberrations can create blur across images in particular for wide apertures. In astronomy and remote sensing, atmospheric turbulence blurs images. All these image reconstruction tasks are examples of inverse problems, which are particularly hard to solve since not only the underlying image is unknown, but also the blur. We assume that the blurred image y is generated by linearly transforming the underlying image x (sometimes called the \u201ctrue image\u201d or \u201clatent image\u201d) by a convolution (denoted by \u2217), and additive noise n,\ny = k \u2217 x + n. (1)\nThe task of blind image deconvolution is to recover x given only the blurry image y, without knowing k. A number of approaches have recently been proposed to recover the true image from blurred photographs, e.g., [Fer+06; CL09; XJ10]. Usually these methods assume some sparsity inducing image prior for x, and follow an iterative, multi-scale estimation scheme, alternating between blur and latent image estimation. The idea of the proposed method is to \u201cunroll\u201d this reconstruction procedure and pose it as a nonlinear regression problem, where the optimal parameters are learned from artificially generated data. As a function approximator, we use a layered architecture akin to a deep neural network\nar X\niv :1\n40 6.\n74 44\nv1 [\ncs .C\nV ]\n2 8\nJu n\n20 14\nor multilayer perceptron. Some of the layers are convolutional, as popular in many recent approaches, while others are non-standard and specific to blind deconvolution. Overall, the system is inspired by [BG91] who formulated the idea of neural networks (NNs) as general processing schemes with large numbers of parameters. Using extensive training on a large image dataset in combination with simulated camera shakes, we train the blind deconvolution NN to automatically obtain an efficient procedure to approximate the true underlying image, given only the blurry measurement. A common problem with NNs, and in particular with large custom built architectures, is that the devil is in the details and it can be nontrivial to get systems to function as desired. We thus put particular emphasis on describing the implementation details, and we make the code publicly available.\nMain contributions: We show how a trainable model can be designed in order to be able to learn blind deconvolution. Results are comparable to the state of the art of hand-crafted approaches and go beyond when the model is allowed to specialize to a particular image category."}, {"heading": "2. Related work", "text": "Some recent approaches to blind deconvolution in photography have already been mentioned above. We refer the interested reader to [WZ13] for an overview of the subject, and focus only on how NNs have been previously used for deconvolution. Neural networks have been used extensively in image processing. Comprehensive reviews are [ERH02; De +03], both of which present broad overviews of applying NNs to all sorts of image processing operations, including segmentation, edge detection, pattern recognition, and nonlinear filtering. Image deconvolution, often also called image reconstruction, has been approached with NNs for a long time. However, these approaches are quite different from our proposed work:\n\u2022 NN to identify the type of blur: [Aiz+06] and similarly [KN11] apply NNs to identify the type of blur from a restricted set of parametrized blurs, e.g. linear-motion, out-of-focus and Gaussian blur, and possibly their parameters.\n\u2022 NN to model blurry images: [CD91] models the blurry image as the result of a neural network where at the different layers the blur kernel and the true image appear.\n\u2022 NN to inversely filter blurry images: [TOM96] learns an inverse filter represented by a NN to deblur text.\n\u2022 NN to optimize a regularized least squares objective: [SF94] proposes also the common two-stage procedure to first estimate the blur kernel and then to recover the image. For both tasks Hopfield networks are employed to solve the optimization problems.\n\u2022 NN to remove colored noise: The previous chapter presented a method for non-blind deblurring that starts with a straight-forward division in Fourier space and then removes the resulting artifacts (mainly colored noise) with large neural networks.\nOther learning-based approaches for blind deconvolution try to learn the deconvolution solution for a single image, in particular they attempt to learn an appropriate sparse representation for a given blurry image, e.g. [HHY10]. In our work, we follow a different strategy: instead of learning the solution or part of a solution for a single fixed image, we use a neural network to learn a general procedure that is directly applicable to other images and to different blurs. Closest to our approach is the work of [Sch+13], who train a deblurring procedure based on regression tree fields. However, even though their approach is not limited to a specific blur or a specific image, they consider only the problem of non-blind deblurring, i.e. their method assumes that the blur kernel is known. This is in contrast to our contribution, which demonstrates how to train a NN to solve the blind deconvolution problem. This is a much harder problem, since not only do we have to solve an underdetermined linear problem (originating from the least-squares formulation of non-blind deconvolution), but an underdetermined bilinear problem, which appears since the entries of the unknown blur kernel k and the pixel values of the unknown image x are multiplied by the convolution, see Eq. (1). Finally we note that deconvolutional networks, introduced in [Zei+10] and further extended in [ZTF11], are not architectures for image deconvolution. Instead, they use convolutions to link sparse features to given images. Their goals are good image representations for tasks such as object recognition and image denoising."}, {"heading": "3. Blind deconvolution as a layered network", "text": "Existing fast blind deconvolution methods work by alternating between the following three steps [XZJ13; XJ10; CL09]:\n1. Feature extraction computes image representations that are useful for kernel estimation. These representations may simply be the gradient of the blurry image and the gradient of the current estimate, possibly thresholded in flat regions [XJ10]; they may also be preliminary estimates of the sharp image, for instance computed by heuristic nonlinear filtering [CL09].\n2. Kernel estimation uses the extracted features to estimate the convolution kernel.\n3. Image estimation attempts to compute an approximation of the sharp image using the current kernel estimate.\nWe will represent these steps by a trainable deep neural network, thus adding more flexibility to them and allowing them to optimally adapt to the problem. The layers of the network alternate between (1) a local convolutional estimation to extract good features, (2) the estimation of the blur kernel, globally combining the extracted features, and finally (3) the estimation of the sharp image. Parts (2) and (3) are fixed (having only one hyper-parameter for regularization). The free\nparameters of the network appear in part (1), the feature extraction module. Thus, instead of having to learn a model on the full dimensionality of the input image, which would not be doable using realistic training set sizes, the learning problem is naturally reduced to learning filters with a limited receptive field."}, {"heading": "3.1. Architecture layout", "text": "Below, we describe the different parts of the network."}, {"heading": "3.1.1. Feature extraction module", "text": "What makes a good feature for kernel estimation can reasonably be assumed to be a translation invariant problem, i.e., independent from the position within the image. We therefore model the feature extractors using shared weights applying across all image locations, i.e., as a convolutional NN layer,1 creating several feature representations of the image. This is followed by two layers that introduce nonlinearity into the model. First, every value is transformed by a tanh-unit, then the feature representations are pixel-wise linearly combined to new hidden images, formally speaking\ny\u0303i = \u2211 j \u03b1ij tanh(fj \u2217 y) and x\u0303i = \u2211 j \u03b2ij tanh(fj \u2217 y) (2)\nwhere fj are the filters of the convolution layer (shared for x\u0303i and y\u0303i), the function tanh operates coordinate-wise, and \u03b1ij and \u03b2ij are the coefficients to linearly combine the hidden representations. Note that we usually extract several gradient-like images x\u0303i and y\u0303i. Depending on the desired nonlinearity, these two layers can be stacked multiple times, leading to the final image representations based on features useful for kernel estimation."}, {"heading": "3.1.2. Kernel estimation module", "text": "Given x\u0303i and y\u0303i which contain features tuned for optimal kernel estimation, the kernel k\u0303 can be estimated by minimizing \u2211\ni\n\u2016k\u0303 \u2217 x\u0303i \u2212 y\u0303i\u20162 + \u03b2k\u2016k\u0303\u20162 (3)\nfor k\u0303 given the results from the previous step x\u0303i and their blurry counterparts y\u0303i. Assuming no noise and a kernel without zeros in its power spectrum, the true gradients of the sharp image x and its blurred version y would return the true kernel for \u03b2k = 0. Typically, in existing methods y\u0303i are just the gradients of the blurry image, while here these can also be learned representations\n1Note that this convolution has nothing to do with the convolution appearing in our image formation model (Eq. (1)) \u2014 causally, it goes in the opposite direction, representing one step in the inverse process turning the blurry image into an estimate of the underlying image.\nB lu\nrr y\nim a\ng e\nL a te\nn t\nim a\ng e\nB lu\nrr y\nim a\ng e\nF e a\ntu re\ne x tr\na c ti o n\nm o\nd u\nle K\ne rn\ne l e\ns ti m\na ti o\nn\nm o\nd u\nle\nIm a g\ne e\ns ti m\na ti o n\nm o\nd u\nle\nF e\na tu\nre\ne x tr\na c ti o\nn\nm o d u le\nK e rn\ne l\ne s ti m\na ti o\nn\nm o d u le\nIm a g e\ne s ti m\na ti o\nn\nm o d u le\nF ir\ns t\ns ta\ng e\nS e\nc o\nn d\ns ta\ng e\nP re\nd ic\nte d\nk e\nrn e\nl\nFi gu\nre 1:\nA rc hi te ct ur e of\nou rp\nro po\nse d bl in d de bl ur rin\ng ne tw or k.\nFi rs tt he\nfe at ur e ex tra\nct io n m od\nul e tra\nns fo rm\nst he\nim ag e\nto al ea rn ed\ngr ad ie nt -li ke\nre pr es en ta tio\nn su ita\nbl ef or\nke rn el es tim\nat io n.\nN ex t, th ek\ner ne li se\nsti m at ed\nby di vi si on in Fo ur ie rs pa ce ,t he n si m ila rly th e la te nt im ag e. Th e ne xt sta ge s, ea ch co ns ist in g of th es e th re e op er at io ns ,o pe ra te on bo th th e bl ur ry im ag e an d th e la te nt im ag e.\nB lu\nrr y i m\na g\ne\nF e a\ntu re\ne x tr\na c ti o n\nm o\nd u\nle\nK e rn\ne l e s ti m\na ti o n m\no d u le\nC o\nn v -L\na y e\nr T\na n h -L\na y e\nr L\nin e a\nrL\na y e\nr T\na n h -L\na y e\nr\nL in\ne a\nrL\na y e\nr\nE s ti m\na te\nd\nK e rn\ne l\nFi gu\nre 2:\nIn te rm\ned ia ry\nou tp ut so\nfa si ng\nle -s ta ge\nN N w ith\nar ch ite\nct ur e 8\u00d7\nC on v, Ta nh\n,8 \u00d7 8,\nTa nh\n,8 \u00d7 4.\npredicted from the previous layer. The minimization problem can be solved in one step in Fourier space if we assume circular boundary conditions [CL09]:\nk\u0303 = FH \u2211\ni F x\u0303i F y\u0303i\u2211 i |F x\u0303i|2 + \u03b2k . (4)\nHere F is the discrete Fourier transform matrix, the Hadamard product, v the complex conjugate of a vector v, and the division and absolute value are performed element-wise. The one step solution is only possible because we use a simple Gaussian prior on the kernel. We call this step the quotient layer, which is an uncommon computation in NNs that usually only combine linear layers and nonlinear thresholding units. The final kernel is returned by cropping to the particular kernel size and thresholding negative values. To reduce artifacts from the incorrect assumption of circular boundary conditions, the borders of the image representations are weighted with a Barthann window such that they smoothly fade to zero. Due to varying size of the input image, we set \u03b2k = 10\u22124 for numerical stability only. This forces the network to not rely on the prior, which would lose importance for a larger input image relative to the likelihood term (since the kernel size is fixed)."}, {"heading": "3.1.3. Image estimation module", "text": "Before adding another feature extraction module, the estimated kernel is used to obtain an update of the sharp latent image: analogously to Eq. (3), we solve\n\u2016k\u0303 \u2217 x\u0303\u2212 y\u20162 + \u03b2x\u2016x\u0303\u20162 (5) for x\u0303, which can also be performed with a quotient layer. This can be done in one step (which would not be possible when using a sparse prior on x\u0303). The following convolution layer then has access to both the latent image and the blurry image, which are stacked along the third dimension (meaning that the learned filters are also three-dimensional). The hyper-parameter \u03b2x is also learned during training."}, {"heading": "3.2. Iterations as stacked networks", "text": "The feature extraction module, kernel estimation module and the image estimation module can be stacked several times, corresponding to multiple iterations in non-learned blind deconvolution methods. This leads to a single NN that can be trained end-to-end with back-propagation by taking the derivatives of all steps (see Appendix A for the derivatives of the solutions to Eq. (3) and the analogous Eq. (5)), increasing the performance as shown in Fig. 3 (but at the same time increasing runtime). Similar to other blind deconvolution approaches, we also use a multi-scale procedure. The first (and coarsest) scale is just a network as described above, trained for a particular blur kernel size. For the second scale, the previous scale network is applied to a downsampled version of the blurry image, and its estimated latent image is upsampled again to the second scale. The second scale network then takes both the blurry image and the result of the previous scale as input. We repeat these steps until the final scale can treat the desired blur kernel size on the full resolution image."}, {"heading": "3.3. Training", "text": "To train the network, we generate pairs of sharp and blurry images. The sharp image is sampled from a subset of about 1.6 million images of the ImageNet [Den+09] dataset, randomly cropped to a size of 256\u00d7256. Next, a blur trajectory is generated by sampling both its x- and y-coordinates separately from a Gaussian Process fx(t), fy(t) \u223c GP ( 0, k(t, t\u2032) ) , k(t, t\u2032) = \u03c32f ( 1 + \u221a 5|t\u2212 t\u2032| l + 5(t\u2212 t\u2032)2 3l2 ) exp ( \u2212 \u221a 5|t\u2212 t\u2032| l ) ,\n(6) which is a Mat\u00e9rn covariance function with \u03bd = 3/2 [RW06]. The length scale l is set to 0.3, the signal standard deviation \u03c3f to 1/4. The coordinates are then scaled to a fixed blur kernel size and the final kernel is shifted to have its center of mass in the middle. This simple procedure generates realistic looking blur kernels, examples for both small and large kernel sizes are illustrated in Fig. 7. For every setting, we generate 1 million noise-free training examples, and add Gaussian noise during training (by default, \u03c3 = 0.01). To avoid that the training process is disturbed by examples where the blur kernel cannot be estimated, e.g., a homogeneous area of sky without any gradient information, we reject examples where less than 6% pixels have gradients in x- and y-direction with an absolute value 0.05 or above. As described in the previous subsection, a network for a certain blur kernel, i.e., a particular scale, consists of several stages, each iterating between the feature extraction, kernel estimation and the image estimation module. We use pre-training for our network: we start by training only one stage minimizing the L2 error between the estimated and the ground truth kernel, then add a second stage after about 1 million training steps. For the next 1000 steps, the parameters of the first stage stay fixed and only the parameters of the second stage are updated. After that, the network is trained end-to-end, until a potential next stage is added. For the update of the parameters, convergence proved to be best with ADADELTA [Zei12], a heuristic weighting scheme of parameter updates using gradients and updates from previous training steps. For the influence of its parameters on the convergence speed see Fig. 6. For our experiments, we choose a learning rate of 0.01 and a decay rate of 0.95. Moreover, it makes training more robust to outliers with strong gradients since it divides by the weighted root-meansquare of the seen gradients, including the current one. Responsible for the mentioned strong gradients are typically images dominated by abrupt step-edges, which create ringing artifacts in\nthe deconvolution Eq. (5). In ImageNet these often are photos of objects with trimmed background. To make the training even more robust, we don\u2019t back-propagate examples with an error above 10 times the current average error."}, {"heading": "4. Implementation", "text": "We make the code for both training and testing our method available for download. For training, we use our own C++/CUDA-based neural network toolbox. After training once for a certain blur class (e.g., camera shake), which takes about two days per stage, applying the network is very fast and can be done in Matlab without additional dependencies. The runtimes on an Intel i5 using only Matlab are shown in Table 1. The most expensive calculation is the creation of the multiple hidden representations in the convolutional layer of the NN (in this example: 32)."}, {"heading": "5. Experiments", "text": "If not otherwise stated, all experiments were performed with a multi-scale, triple stage architecture. We use up to three scales for kernels of size 17\u00d717, 25\u00d725, 33\u00d733. On each scale each feature extraction module consists of a convolution layer with 32 filters, a tanh-layer, a linear recombination to 32 new hidden images, a further tanh-layer, and a recombination to four gradient-like images, two for x\u0303i and y\u0303i each (in the third stage: eight gradient-like images). In the case of the network with blur kernels of size 33\u00d733, we deconvolve the estimated kernels with a small Gaussian with \u03c3 = 0.5 to counter the over-smoothing effect of the L2 norm used during training. For the specific choice of the architecture, we refer to the influence of model parameters on the kernel estimation performance in Figs. 3 to 4 ."}, {"heading": "5.1. Image content specific training", "text": "A number of recent works [HY12; Sun+13; Wan+13] have pointed out the shortcoming of stateof-the-art algorithms [CL09; XJ10] to depend on the presence of strong salient edges and their diminished performance in the case of images that contain textured scenes such as natural landscape images. The reason for this is the deficiency of the so-called image prediction step, which\napplies a combination of bilateral and shock filtering to restore latent edges that are used for subsequent kernel estimation. In this context, learning the latent image prediction step offers a great advantage: by training our network with a particular class of images, it is able to focus on those features that are informative for the particular type of image. In other words, the network learns content-specific nonlinear filters, which yield improved performance. To demonstrate this, we used the same training procedure as described above, however, we reduced the training set to images from a specific image category within the ImageNet dataset. In particular, we used the image category valley2 containing a total of 1395 pictures. In a second experiment, we trained a network on the image category blackboard3 with a total of 1376 pictures. Figure 8 shows typical example images from these two classes. Fig. 9 compares deblurring results of the state-of-the-art algorithm described in [XZJ13] with our approach trained on images sampled from the entire ImageNet dataset, and trained on the aforementioned image categories only. Note that the particular images shown in Fig. 9 were not used for training. We see that content specific training outperforms both content agnostic training and the generic state-of-the-art method."}, {"heading": "5.2. Noise specific training", "text": "Typically, image noise impedes kernel estimation. To counter noise in blurry images, current state-of-the-art deblurring algorithms apply a denoising step during latent image prediction such\n2ImageNet 2011 Fall Release > Geological formation, formation > Natural depression, depression > Valley, vale (http://www.image-net.org/synset?wnid=n09468604) 3ImageNet 2011 Fall Release > Artifact, artifact > Sheet, flat solid > Blackboard, chalkboard (http://www. image-net.org/synset?wnid=n02846511)\nas bilateral filtering [CL09] or Gaussian filtering [XJ10]. However, in a recent work [Zho+13], the authors show that for increased levels of noise current methods fail to yield satisfactory results and propose a novel robust deblurring algorithm. Again, if we include image noise in our training phase, our network is able to adapt and learn filters that perform better in the presence of noise. In particular, we trained a network on images with Gaussian noise of 5% added during the training phase. Figure 9 compares the results for an image taken from [Zho+13] with 5% Gaussian noise for a network trained with 1% and 5% of added Gaussian noise during training, respectively. We also show the result of [Zho+13] and compare peak signal-to-noise ratio (PSNR) for objective evaluation. All results use the same non-blind deconvolution of [Zho+13]. The noise specific training is most successful, but even the noise agnostic NN outperforms the non-learned method on this example."}, {"heading": "5.3. Spatially-varying blur", "text": "Since the prediction step of our trained NN is independent of the convolution model, we can also use it in conjunction with the recently proposed fast forward model of [Hir+11] to restore images with spatially-varying blur. To this end, we replace the objective function Eq. (5) with Eq. (8) of [Hir+11] in our kernel estimation module in a network trained for spatially invariant deconvolution. We solve for k in a two-step procedure: first we compute local blur kernels using the efficient filter flow (EFF) model of [Hir+10]; in a second step we project the blur kernels onto a motion basis aka [Hir+11], as explained below. Figure 11 shows a comparison between recent\nstate-of-the-art algorithms for spatially-varying blur along with our deblurring result that features comparable quality. For the estimation of spatially-varying blur we solve the following objective\u2211\ni\n\u2016X\u0303i k\u0303\u2212 y\u0303i\u20162 + \u03b2k\u2016k\u0303\u20162 (7)\nin our kernel estimation module. Here X\u0303i denotes the EFF matrix of x\u0303 (cf. Eq. (9) in [Hir+10]) and k\u0303 the stacked sequence of local kernels k\u0303(r), one for each patch that are enumerated by index r. Since Eq. (7) is quadratic in k\u0303, we can solve for a local blur k\u0303(r) in a single step\nk\u0303 (r) direct \u2248 F H\n\u2211 i FCr Diag(w\n(r)) x\u0303i (FCr Diag(w(r)) y\u0303i)\u2211 i |FCr Diag(w(r)) x\u0303i|2 + \u03b2k , (8)\nwhere Cr are appropriately chosen cropping matrices, and w(r) are window functions matching the size of x\u0303i and y\u0303i. Note that Eq. (8) is only approximately true and is motivated by Eq. (8) in [Hir+11]. Subsequently, we project the estimated kernel computed by Eq. (8) to a motion blur kernel basis. In our experiments we use the same basis as [Hir+11] comprising translations within the image plane and in-plane rotations only. This additional projection step constrains the estimated blur to physically plausible ones. Formally, we compute\nk\u0303 (r) est \u2248 B(r) T\u03b7 \u2211 r (B(r)) T k\u0303 (r) direct\ufe38 \ufe37\ufe37 \ufe38\n\u00b5\n, (9)\nwhere again we make use of the notation chosen in [Hir+11], i.e. B(r) denotes the motion blur kernel basis for patch r. Then \u00b5 are the coefficients in the basis of valid motion blurs. T\u03b7 denotes a thresholding operator that sets all elements to zero below a certain threshold whereby the\nthreshold is chosen such that only \u03b7 percent of entries remain non-zero. This thresholding step is motivated by [CL09], who also apply a hard thresholding step to the estimated kernels in order to get rid of spurious artefacts. Figure 12 shows the intermediate results of our kernel estimation procedure in the case of spatially-varying blur."}, {"heading": "5.4. Comparisons", "text": ""}, {"heading": "5.4.1. Benchmark Datasets", "text": "We evaluate our method on the standard test sets from [Lev+09; Sun+13]. The four images of [Lev+09] are 255\u00d7255 pixels in size and are artificially blurred each with eight different blur kernels and contain 1% additive Gaussian noise. The performance is illustrated in Fig. 13 on the left, with blur kernels sorted according to increasing size. We compare with Levin et al. [Lev+11], Cho and Lee [CL09], and Xu et al. [XZJ13]. While our method is competitive with the state of the art for small blur kernels, our method falls short in performance for blur kernel sizes above 25\u00d725 pixels. We discuss reasons for this in Section 6.3. The second benchmark from [Sun+13] extends this dataset to 80 new images with about one megapixel in size each, using the same blur kernels as in [Lev+09]. Results are shown in Fig. 13 on the right. Here we compare with Levin et al. [Lev+11], Cho and Lee [CL09], Krishnan et al. [KTF11], Sun et al. [Sun+13], and Xu and Jia [XJ10], where however [Sun+13] has runtime in the order of hours. Again, we see competitive performance for small blur kernels."}, {"heading": "5.4.2. Real-World Images", "text": "In Figs. 14 and 15we show results of our method on real-world images. Figure 14 shows examples for invariant blur, while Fig. 15 depicts images with spatially-varying camera shake. In both examples, our approach is able to recover images comparable in quality with the state of the art."}, {"heading": "6. Discussion", "text": ""}, {"heading": "6.1. Learned filters", "text": "The task of the Feature Extraction Module is to emphasize and enhance those image features that contain information about the unknown blur kernel. Figure 16 shows the learned filters of the convolution layer for each of the three stages within a single scale of a trained NN for kernel size 17\u00d717 pixels. While the first stage takes a single (possibly down-sampled) version of a blurry image as input, the subsequent stages take both the restored latent image (obtained by nonblind deconvolution with the current estimate of the kernel) and the blurry image as input. The outputs of each stage are nonlinearly filtered versions of the input images. In Fig. 18 we visualize the effect of the first stage of a NN with two predicted output images on both the Lena image and a toy example image consisting of four disks blurred with Gaussians of varying size. Note that our feature extraction module outputs nonlinearly filtered images for both the blurry and the\nlatent sharp image, both of which serve as input to the subsequent quotient layer, which in turn computes an estimate of the blur kernel. This is in contrast to other existing approaches [CL09; XJ10], which apply a nonlinear filter to the current estimate of the latent image, but use only a linearly filtered version of the blurry input image for kernel estimation. Once these feature images have passed the subsequent tanh and recombination layer, they serve as input to the quotient layer, which computes an estimate of the unknown blur kernel. Figure 2 shows the intermediary results directly after the convolution layer and how they progressively change after passing through tanh and linear recombination layer two times, which seem to emphasize strong edges. While the learned filters of the first stage are reminiscent of gradient filters of various extent and orientations including Gabor and Laplace-like filters, the filters of the subsequent stages are much more intricate and more difficult to interpret. As discussed in Section 5, the learned filters depend on the image set that the NN was trained with, i.e., the feature extraction module learns image content specific features that are informative about the unknown blur kernel. In Fig. 17, we show the learned filters of the experiments in Section 5.1 for the valley and blackboard image category of the ImageNet dataset; they indeed differ from the ones trained on all images. For example, most of the filters learned for valley images are mirror or rotational symmetric, unlike many filters of the generic NN."}, {"heading": "6.2. Dependence on the size of the observed image", "text": "As noted by Hu and Yang [HY12], blind deblurring methods are most successful in predicting the kernel in regions of an image that exhibit strong salient edges. Other regions are less informative about the kernel, and have been shown to even hurt kernel estimation when included in the input to the blind deconvolution algorithm. Ideally, an estimation procedure should weight its input according to its information content. In the worst case, a larger input would not improve the results, but would not cause a deterioration either. We study the behavior of our method with respect to the size of the observed image. It is possible that the NN learned to ignore image content detrimental to the kernel estimation. In Fig. 19 the predicted kernels for different sized crops of a blurry image are shown. Indeed, this example suggests that for our learned algorithm the kernel converges with input images increasing in size, while a non-learned state-of-the-art algorithm [XZJ13] exhibits no such trend. The more thorough analysis in Fig. 20 confirms this behavior: when evaluating the MSE for three different kernels, averaged over the 52 largest images from [Sun+13], it monotonously decreases for larger crops of the blurry image. We also see that the NN outperforms the competing method for the two small blur kernels."}, {"heading": "6.3. Limitations", "text": "A limitation of our current approach is the performance drop in the case of larger blur kernels. Figure 21 shows an example failure case from the benchmark dataset of [Lev+09]. We believe the reason for this is the suboptimal architecture of our multi-scale approach at higher resolution scales. While a multi-scale approach exhibits better performance compared to a single scale network, the observed performance drop for larger blurs suggests that using the same architecture at all scales is not optimal."}, {"heading": "7. Conclusion", "text": "We have shown that it is possible to automatically learn blind deconvolution by reformulating the task as a single large nonlinear regression problem, mapping between blurry input and predicted kernel. The key idea is to incorporate the properties of the generative forward model into our algorithm, namely that the image is convolved with the same blur kernel everywhere. While features are extracted locally in the image, the kernel estimation module combines them globally. Next, the image estimation module propagates the information to the whole image, reducing the difficulty of the problem for the following iteration. Our approach can adapt to different settings (e.g., blurry images with strong noise, or specific image classes), and it could be further extended to combine deblurring with other steps of the imaging pipeline, including over-saturation, Bayer filtering, HDR, or super-resolution. The blur class also invites future research: instead of artificially sampling from a stochastic process, one could use recorded spatially-varying camera shakes, or a different source of unsharpness, like lens aberrations or atmospheric turbulences in astrophotography. Additionally, the insights gained from the trained system could be beneficial to existing hand-crafted methods. This includes using higher-order gradient representations and extended gradient filters. Scalability of our method to large kernel sizes is still an issue, and this may benefit from future improvements in neural net architecture and training."}, {"heading": "Appendix A Quotient Layer", "text": "As introduced in Eq. (4), the quotient layer performs the operation\nk\u0303 = FH \u2211\ni F x\u0303i F y\u0303i\u2211 i |F x\u0303i|2 + \u03b2k\n(10)\nto estimate the kernel k\u0303 from images x\u0303i and their blurry counterparts y\u0303i, both predicted by the previous layers of the NN. The quotient layer also includes a learned regularization parameter \u03b2k. To train the NN, we need the gradients of the output with respect to its parameters (in this case x\u0303i, y\u0303i and \u03b2k) in every layer. From these we obtain the gradient steps \u2206x\u0303j , \u2206y\u0303k and \u2206\u03b2k in terms of \u2206k\u0303, where \u2206k\u0303 is determined by the loss for the current training example, back-propagated through the layers subsequent to the quotient layer.\nA.1 Derivative with respect to sharp images To obtain \u2206x\u0303j , we first determine the differential form in numerator layout, which means for vectors u, v and a matrixM that\ndu = du1du2 ...  , du dv =  du1 dv1 du1 dv2 \u00b7 \u00b7 \u00b7 du2 dv1 du2 dv2 \u00b7 \u00b7 \u00b7 ... ... . . .  and dM = dm11 dm12 \u00b7 \u00b7 \u00b7dm21 dm22 \u00b7 \u00b7 \u00b7 ... ... . . .  . (11) Therefore, assuming that only x\u0303j is variable, with the rules of matrix calculus [PP12] we arrive at\ndk\u0303 = FH Fdx\u0303j F y\u0303j\u2211 i |F x\u0303i|2 + \u03b2k\n\u2212 FH ( \u2211\ni F x\u0303i F y\u0303i) d(F x\u0303j F x\u0303j) ( \u2211 i |F x\u0303i|2 + \u03b2k) 2 =\n= FH Diag ( F y\u0303j\u2211\ni |F x\u0303i|2 + \u03b2k ) F\ufe38 \ufe37\ufe37 \ufe38\nA\ndx\u0303j\n\u2212 FH Diag\n( ( \u2211\ni F x\u0303i F y\u0303i) F x\u0303j ( \u2211\ni |F x\u0303i|2 + \u03b2k) 2 ) F\ufe38 \ufe37\ufe37 \ufe38\nB\ndx\u0303j \u2212 FH Diag\n( ( \u2211\ni F x\u0303i F y\u0303i) F x\u0303j ( \u2211\ni |F x\u0303i|2 + \u03b2k) 2 ) F\ufe38 \ufe37\ufe37 \ufe38\nC\ndx\u0303j,\n(12)\nwith the Hadamard product a b = Diag(a)b and the operation \u201cDiag\u201d that creates a diagonal matrix from a vector. Additionally, x\u0303i is real, and therefore dx\u0303j = dx\u0303j . With the differential dk\u0303 = Mdx\u0303j (whereM = A+B + C) in numerator layout, we note that the derivative dk\u0303 dx\u0303j = M and the gradient step \u2206x\u0303j = MT\u2206k\u0303. Additionally, we use that BT\u2206k\u0303 is\nreal since it is the inverse Fourier transform of a point-wise product of vectors all with Hermitian symmetry (they are themselves purely real vectors that have been Fourier transformed). Hermitian symmetry for a vector v of length n means vn\u2212i+1 = vi for all its components i. Therefore,\n\u2206x\u0303j = A T\u2206k\u0303\u2212BT\u2206k\u0303\u2212 CT\u2206k\u0303 = AT\u2206k\u0303\u2212BT\u2206k\u0303\u2212 CT\u2206k\u0303 =\n= FH F\u2206k\u0303 F y\u0303j\u2211 i |F x\u0303i|2 + \u03b2k\n\u2212 FH \u2211\ni F x\u0303i F y\u0303i F x\u0303j F\u2206k\u0303 ( \u2211 i |F x\u0303i|2 + \u03b2k) 2 \u2212 F\nH ( \u2211\ni F x\u0303i F y\u0303i) F x\u0303j F\u2206k\u0303 ( \u2211\ni |F x\u0303i|2 + \u03b2k) 2\n= FH  F\u2206k\u0303 F y\u0303j\u2211 i |F x\u0303i|2 + \u03b2k \u2212 2< (\u2211 i F x\u0303i F y\u0303i F\u2206k\u0303 ) F x\u0303j ( \u2211 i |F x\u0303i|2 + \u03b2k) 2  , (13) since the transpose has no effect on a diagonal matrix. The real part of a vector v is denoted as <(v) = 1\n2 (v + v).\nA.2 Derivative with respect to blurry images The derivation for the gradient \u2206y\u0303j is similar. We again start with the differential form\ndk\u0303 = FH F x\u0303j Fdy\u0303j\u2211 i |F x\u0303i|2 + \u03b2k = FH Diag\n( F x\u0303j\u2211\ni |F x\u0303i|2 + \u03b2k ) F\ufe38 \ufe37\ufe37 \ufe38\nA\ndy\u0303j, (14)\nthis time assuming dx\u0303j and d\u03b2k to be zero. Next, we use again that a real vector is unchanged by conjugation, and we obtain\n\u2206y\u0303j = A T\u2206k\u0303 = AT\u2206k\u0303 = FHDiag\n( F x\u0303j\u2211\ni |F x\u0303i|2 + \u03b2k\n) F\u2206k\u0303 = FH ( F x\u0303j F\u2206k\u0303\u2211 i |F x\u0303i|2 + \u03b2k ) .\n(15)\nA.3 Derivative with respect to the regularization parameter Following the previous procedure for the regularization parameter \u03b2k we arrive at\ndk\u0303 = FH \u2211\ni F x\u0303i F y\u0303i ( \u2211\ni |F x\u0303i|2 + \u03b2k) 2\ufe38 \ufe37\ufe37 \ufe38\nA\nd\u03b2k, (16)\nassuming only \u03b2k to be variable. Then, multiplying \u2206k\u0303 by the transpose of dk\u0303d\u03b2\u0303k = A we get\n\u2206\u03b2k = A T\u2206k\u0303 = ( FH \u2211 i F x\u0303i F y\u0303i\n( \u2211\ni |F x\u0303i|2 + \u03b2k) 2\n)T \u2206k\u0303. (17)"}], "references": [{"title": "Blur identification using neural network for image restoration", "author": ["I. Aizenberg", "D. Paliy", "C. Moraga", "J. Astola"], "venue": "Computational Intelligence, Theory and Applications. Vol. 38. Springer,", "citeRegEx": "Aizenberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aizenberg et al\\.", "year": 2006}, {"title": "A Framework for the Cooperation of Learning Algorithms", "author": ["L. Bottou", "P. Gallinari"], "venue": "Advances Neural Information Processing Systems", "citeRegEx": "Bottou and Gallinari.,? \\Q1991\\E", "shortCiteRegEx": "Bottou and Gallinari.", "year": 1991}, {"title": "Blur identification and image restoration using a multilayer neural network", "author": ["C.M. Cho", "H.S. Don"], "venue": "IEEE Int. Joint Conf. Neural Networks", "citeRegEx": "Cho and Don.,? \\Q1991\\E", "shortCiteRegEx": "Cho and Don.", "year": 1991}, {"title": "Fast motion deblurring", "author": ["S. Cho", "S. Lee"], "venue": "ACM Trans. Graphics", "citeRegEx": "Cho and Lee.,? \\Q2009\\E", "shortCiteRegEx": "Cho and Lee.", "year": 2009}, {"title": "Nonlinear image processing using artificial neural networks", "author": ["D. De Ridder"], "venue": "Advances Imaging and Electron Physics", "citeRegEx": "Ridder,? \\Q2003\\E", "shortCiteRegEx": "Ridder", "year": 2003}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Deng,? \\Q2009\\E", "shortCiteRegEx": "Deng", "year": 2009}, {"title": "Image processing with neural networks\u2014a review", "author": ["M. Egmont-Petersen", "D. de Ridder", "H. Handels"], "venue": "Pattern recognition", "citeRegEx": "Egmont.Petersen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Egmont.Petersen et al\\.", "year": 2002}, {"title": "Removing camera shake from a single photograph", "author": ["R. Fergus"], "venue": "ACM Trans. Graphics", "citeRegEx": "Fergus,? \\Q2006\\E", "shortCiteRegEx": "Fergus", "year": 2006}, {"title": "Single Image Deblurring Using Motion Density Functions", "author": ["A. Gupta"], "venue": "IEEE Europ. Conf. Computer Vision", "citeRegEx": "Gupta,? \\Q2010\\E", "shortCiteRegEx": "Gupta", "year": 2010}, {"title": "Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake.", "author": ["S. Harmeling", "M. Hirsch", "B. Sch\u00f6lkopf"], "venue": "Advances Neural Information Processing Systems", "citeRegEx": "Harmeling et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Harmeling et al\\.", "year": 2010}, {"title": "Single image deblurring with adaptive dictionary learning", "author": ["Z. Hu", "J.-B. Huang", "M.-H. Yang"], "venue": "IEEE Int. Conf. Image Processing", "citeRegEx": "Hu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2010}, {"title": "Efficient filter flow for spacevariant multiframe blind deconvolution.", "author": ["M. Hirsch", "S. Sra", "B. Sch\u00f6lkopf", "S. Harmeling"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Hirsch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hirsch et al\\.", "year": 2010}, {"title": "Fast removal of nonuniform camera shake", "author": ["M. Hirsch", "C.J. Schuler", "S. Harmeling", "B. Sch\u00f6lkopf"], "venue": "IEEE Int. Conf. Computer Vision", "citeRegEx": "Hirsch et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hirsch et al\\.", "year": 2011}, {"title": "Good Regions to Deblur", "author": ["Z. Hu", "M.-H. Yang"], "venue": "Lecture Notes in Computer Science. Springer,", "citeRegEx": "Hu and Yang.,? \\Q2012\\E", "shortCiteRegEx": "Hu and Yang.", "year": 2012}, {"title": "Image deblurring using inertial measurement sensors", "author": ["N. Joshi", "S.B. Kang", "C.L. Zitnick", "R. Szeliski"], "venue": "ACM Trans. Graphics", "citeRegEx": "Joshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Blind deconvolution using a normalized sparsity measure", "author": ["D.Krishnan", "T. Tay", "andR. Fergus"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "D.Krishnan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "D.Krishnan et al\\.", "year": 2011}, {"title": "Understanding and evaluating blind deconvolution algorithms", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Levin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2009}, {"title": "Efficient marginal likelihood optimization in blind deconvolution", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Levin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2011}, {"title": "The matrix cookbook", "author": ["K.B. Petersen", "M.S. Pedersen"], "venue": null, "citeRegEx": "Petersen and Pedersen.,? \\Q2012\\E", "shortCiteRegEx": "Petersen and Pedersen.", "year": 2012}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Discriminative Non-blind Deblurring", "author": ["U. Schmidt"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Schmidt,? \\Q2013\\E", "shortCiteRegEx": "Schmidt", "year": 2013}, {"title": "Blind deconvolution of images by use of neural networks", "author": ["R.J. Steriti", "M.A. Fiddy"], "venue": "Optics letters", "citeRegEx": "Steriti and Fiddy.,? \\Q1994\\E", "shortCiteRegEx": "Steriti and Fiddy.", "year": 1994}, {"title": "High-quality motion deblurring from a single image", "author": ["Q. Shan", "J. Jia", "A. Agarwala"], "venue": "ACM Trans. Graphics", "citeRegEx": "Shan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shan et al\\.", "year": 2008}, {"title": "Edge-based blur kernel estimation using patch priors", "author": ["L. Sun", "S. Cho", "J. Wang", "J. Hays"], "venue": "IEEE Int. Conf. Computational Photography", "citeRegEx": "Sun et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Neural network image deconvolution", "author": ["J.E. Tansley", "M.J. Oldfield", "D.J. MacKay"], "venue": "Maximum Entropy and Bayesian Methods. Fundamental Theories of Physics. Springer,", "citeRegEx": "Tansley et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Tansley et al\\.", "year": 1996}, {"title": "Nonedge-specific adaptive scheme for highly robust blind motion deblurring of natural images", "author": ["C.Wang"], "venue": "IEEE Trans. Image Processing", "citeRegEx": "C.Wang,? \\Q2013\\E", "shortCiteRegEx": "C.Wang", "year": 2013}, {"title": "Non-uniformDeblurring for Shaken Images", "author": ["O.Whyte", "J. Sivic", "A. Zisserman", "J. Ponce"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "O.Whyte et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Whyte et al\\.", "year": 2010}, {"title": "Revisiting Bayesian Blind Deconvolution", "author": ["D. Wipf", "H. Zhang"], "venue": "eprints", "citeRegEx": "Wipf and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Wipf and Zhang.", "year": 2013}, {"title": "Two-Phase Kernel Estimation for Robust Motion Deblurring", "author": ["L. Xu", "J. Jia"], "venue": "Computer Vision \u2013 ECCV 2010. Lecture Notes in Computer Science. Springer,", "citeRegEx": "Xu and Jia.,? \\Q2010\\E", "shortCiteRegEx": "Xu and Jia.", "year": 2010}, {"title": "Unnatural l0 sparse representation for natural image deblurring", "author": ["L. Xu", "S. Zheng", "J. Jia"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "IEEEConf. Computer Vision and Pattern Recognition", "citeRegEx": "Zeiler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}, {"title": "ADADELTA:AnAdaptive Learning RateMethod", "author": ["M.D. Zeiler"], "venue": "In:ArXiv e-prints", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Handling Noise in Single Image Deblurring Using Directional Filters", "author": ["L. Zhong"], "venue": "IEEE Conf. Computer Vision and Pattern Recognition", "citeRegEx": "Zhong,? \\Q2013\\E", "shortCiteRegEx": "Zhong", "year": 2013}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "IEEE Int. Conf. Computer Vision", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}], "referenceMentions": [], "year": 2014, "abstractText": "We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime.", "creator": "LaTeX with hyperref package"}}}