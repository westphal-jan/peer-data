{"id": "1407.2657", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2014", "title": "Beyond Disagreement-Based Agnostic Active Learning", "abstract": "We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are {\\em{disagreement-based active learning}}, which has a high label requirement, and {\\em{margin-based active learning}}, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this case, the first one should be in a set of data types which have a similar level of classification complexity as all other classes. The second problem is to find an algorithm which fails to conform to the definition of a non-aggregation model in an agnostic setting, or for which it is not compatible. It may not have a general-purpose classifier for both classes. For example, in an agnostic classifier, the classifier is defined by its definition, but does not specify the type of the classifier. It will only evaluate the information that is given by a non-aggregation model.\n\n\n\n\n\n\nThe first algorithm which achieves better label complexity (1) is the classifier. It should be set up to be as simple as a group of data types, which will take into account a general classification problem as defined in an agnostic classifier. However, the algorithm is a more complex and less well known algorithm than the previous algorithm. The main problem is to find an algorithm which fails to conform to the definition of a non-aggregation model in an agnostic setting, or for which it is not compatible. It may not have a general-purpose classifier for both classes. For example, in an agnostic classifier, the classifier is defined by its definition, but does not specify the type of the classifier. It will only evaluate the information that is given by a non-aggregation model. It will only evaluate the information that is given by a non-aggregation model. For example, in an agnostic classifier, the classifier is defined by its definition, but does not specify the type of the classifier. It will only evaluate the information that is given by a non-aggregation model. It will only evaluate the information that is given by a non-aggregation model. For example, in an agnostic classifier, the classifier is defined", "histories": [["v1", "Thu, 10 Jul 2014 00:34:16 GMT  (26kb)", "https://arxiv.org/abs/1407.2657v1", null], ["v2", "Fri, 11 Jul 2014 23:35:49 GMT  (26kb)", "http://arxiv.org/abs/1407.2657v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["chicheng zhang", "kamalika chaudhuri"], "accepted": true, "id": "1407.2657"}, "pdf": {"name": "1407.2657.pdf", "metadata": {"source": "CRF", "title": "Beyond Disagreement-based Agnostic Active Learning", "authors": ["Chicheng Zhang"], "emails": ["chz038@eng.ucsd.edu", "kamalika@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 7.\n26 57\nv2 [\ncs .L\nWe study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are disagreement-based active learning, which has a high label requirement, and margin-based active learning, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems.\nIn this paper, we provide such an algorithm. Our solution is based on two novel contributions \u2013 a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor."}, {"heading": "1 Introduction", "text": "In this paper, we study active learning of classifiers in an agnostic setting, where no assumptions are made on the true function that generates the labels. The learner has access to a large pool of unlabelled examples, and can interactively request labels for a small subset of these; the goal is to learn an accurate classifier in a pre-specified class with as few label queries as possible. Specifically, we are given a hypothesis class H and a target \u01eb, and our aim is to find a binary classifier in H whose error is at most \u01eb more than that of the best classifier in H, while minimizing the number of requested labels.\nThere has been a large body of previous work on active learning; see the surveys by [Das11, Set10] for overviews. The main challenge in active learning is ensuring consistency in the agnostic setting while still maintaining low label complexity. In particular, a very natural approach to active learning is to view it as a generalization of binary search [FSST97, Das05, Now11]. While this strategy has been extended to several different noise models [Ka\u0308a\u030806, Now11, NJC13], it is generally inconsistent in the agnostic case [DH08].\nThe primary algorithm for agnostic active learning is called disagreement-based active learning. The main idea is as follows. A set Vk of possible risk minimizers is maintained with time, and the label of an example x is queried if there exist two hypotheses h1 and h2 in Vk such that h1(x) 6= h2(x). This algorithm is consistent in the agnostic setting [CAL94, BBL09, DHM07, Han07, BDL09, Han09, BHLZ10, Kol10]; however, due to the conservative label query policy, its label requirement is high. A line of work due to [BBZ07, BL13, ABL14] have provided algorithms that achieve better label complexity for linear classification on the uniform\ndistribution over the unit sphere as well as log-concave distributions; however, their algorithms are limited to these specific cases, and it is unclear how to apply them more generally.\nThus, a major challenge in the agnostic active learning literature has been to find a general active learning strategy that applies to any hypothesis class and data distribution, is consistent in the agnostic case, and has a better label requirement than disagreement based active learning. This has been mentioned as an open problem by several works, such as [BBL09, Das11, BL13].\nIn this paper, we provide such an algorithm. Our solution is based on two key contributions, which may be of independent interest. The first is a general connection between confidence-rated predictors and active learning. A confidence-rated predictor is one that is allowed to abstain from prediction on occasion, and as a result, can guarantee a target prediction error. Given a confidence-rated predictor with guaranteed error, we show how to use it to construct an active label query algorithm consistent in the agnostic setting. Our second key contribution is a novel confidence-rated predictor with guaranteed error that applies to any general classification problem. We show that our predictor is optimal in the realizable case, in the sense that it has the lowest abstention rate out of all predictors that guarantee a certain error. Moreover, we show how to extend our predictor to the agnostic setting.\nCombining the label query algorithm with our novel confidence-rated predictor, we get a general active learning algorithm consistent in the agnostic setting. We provide a characterization of the label complexity of our algorithm, and show that this is better than disagreement-based active learning in general. Finally, we show that for linear classification with respect to the uniform distribution and log-concave distributions, our bounds reduce to those of [BBZ07, BL13]."}, {"heading": "2 Algorithm", "text": ""}, {"heading": "2.1 The Setting", "text": "We study active learning for binary classification. Examples belong to an instance space X , and their labels lie in a label space Y = {\u22121, 1}; labelled examples are drawn from an underlying data distribution D on X \u00d7 Y. We use DX to denote the marginal on D on X , and DY |X to denote the conditional distribution on Y |X = x induced by D. Our algorithm has access to examples through two oracles \u2013 an example oracle U which returns an unlabelled example x \u2208 X drawn from DX and a labelling oracle O which returns the label y of an input x \u2208 X drawn from DY |X .\nGiven a hypothesis class H of VC dimension d, the error of any h \u2208 H with respect to a data distribution \u03a0 over X \u00d7 Y is defined as err\u03a0(h) = P(x,y)\u223c\u03a0(h(x) 6= y). We define: h\u2217(\u03a0) = argminh\u2208Herr\u03a0(h), \u03bd\u2217(\u03a0) = err\u03a0(h\n\u2217(\u03a0)). For a set S, we abuse notation and use S to also denote the uniform distribution over the elements of S. We define P\u03a0(\u00b7) := P(x,y)\u223c\u03a0(\u00b7), E\u03a0(\u00b7) := E(x,y)\u223c\u03a0(\u00b7).\nGiven access to examples from a data distribution D through an example oracle U and a labeling oracle O, we aim to provide a classifier h\u0302 \u2208 H such that with probability \u2265 1 \u2212 \u03b4, errD(h\u0302) \u2264 \u03bd\u2217(D) + \u01eb, for some target values of \u01eb and \u03b4; this is achieved in an adaptive manner by making as few queries to the labelling oracle O as possible. When \u03bd\u2217(D) = 0, we are said to be in the realizable case; in the more general agnostic case, we make no assumptions on the labels, and thus \u03bd\u2217(D) can be positive.\nPrevious approaches to agnostic active learning have frequently used the notion of disagreements. The disagreement between two hypotheses h1 and h2 with respect to a data distribution \u03a0 is the fraction of examples according to \u03a0 to which h1 and h2 assign different labels; formally: \u03c1\u03a0(h1, h2) = P(x,y)\u223c\u03a0(h1(x) 6= h2(x)). Observe that a data distribution \u03a0 induces a pseudo-metric \u03c1\u03a0 on the elements of H; this is called the disagreement metric. For any r and any h \u2208 H, define B\u03a0(h, r) to be the disagreement ball of radius r around h with respect to the data distribution \u03a0. Formally: B\u03a0(h, r) = {h\u2032 \u2208 H : \u03c1\u03a0(h, h\u2032) \u2264 r}.\nFor notational simplicity, we assume that the hypothesis space is \u201cdense\u201d with repsect to the data distribution D, in the sense that \u2200r > 0, suph\u2208BD(h\u2217(D),r) \u03c1D(h, h\u2217(D)) = r. Our analysis will still apply without the denseness assumption, but will be significantly more messy. Finally, given a set of hypotheses V \u2286 H, the disagreement region of V is the set of all examples x such that there exist two hypotheses h1, h2 \u2208 V for which h1(x) 6= h2(x).\nThis paper establishes a connection between active learning and confidence-rated predictors with guaranteed error. A confidence-rated predictor is a prediction algorithm that is occasionally allowed to abstain from classification. We will consider such predictors in the transductive setting. Given a set V of candidate hypotheses, an error guarantee \u03b7, and a set U of unlabelled examples, a confidence-rated predictor P either assigns a label or abstains from prediction on each unlabelled x \u2208 U . The labels are assigned with the guarantee that the expected disagreement1 between the label assigned by P and any h \u2208 V is \u2264 \u03b7. Specifically, for all h \u2208 V, Px\u223cU (h(x) 6= P (x), P (x) 6= 0) \u2264 \u03b7 (1) This ensures that if some h\u2217 \u2208 V is the true risk minimizer, then, the labels predicted by P on U do not differ very much from those predicted by h\u2217. The performance of a confidence-rated predictor which has a guarantee such as in Equation (1) is measured by its coverage, or the probability of non-abstention Px\u223cU (P (x) 6= 0); higher coverage implies better performance."}, {"heading": "2.2 Main Algorithm", "text": "Our active learning algorithm proceeds in epochs, where the goal of epoch k is to achieve excess generalization error \u01ebk = \u01eb2\nk0\u2212k+1, by querying a fresh batch of labels. The algorithm maintains a candidate set Vk that is guaranteed to contain the true risk minimizer.\nThe critical decision at each epoch is how to select a subset of unlabelled examples whose labels should be queried. We make this decision using a confidence-rated predictor P . At epoch k, we run P with candidate hypothesis set V = Vk and error guarantee \u03b7 = \u01ebk/64. Whenever P abstains, we query the label of the example. The number of labels mk queried is adjusted so that it is enough to achieve excess generalization error \u01ebk+1.\nAn outline is described in Algorithm 1; we next discuss each individual component in detail.\nAlgorithm 1 Active Learning Algorithm: Outline\n1: Inputs: Example oracle U , Labelling oracle O, hypothesis class H of VC dimension d, confidence-rated predictor P , target excess error \u01eb and target confidence \u03b4. 2: Set k0 = \u2308log 1/\u01eb\u2309. Initialize candidate set V1 = H. 3: for k = 1, 2, ..k0 do 4: Set \u01ebk = \u01eb2 k0\u2212k+1, \u03b4k = \u03b4\n2(k0\u2212k+1)2 .\n5: Call U to generate a fresh unlabelled sample Uk = {zk,1, ..., zk,nk} of size nk = 192(256\u01ebk ) 2(d ln 256\u01ebk +\nln 288\u03b4k ).\n6: Run confidence-rated predictor P with inputs V = Vk, U = Uk and error guarantee \u03b7 = \u01ebk/64 to get abstention probabilities \u03b3k,1, . . . , \u03b3k,nk on the examples in Uk. These probabilities induce a distribution \u0393k on Uk. Let \u03c6k = Px\u223cUk(P (x) = 0) = 1 nk \u2211nk i=1 \u03b3k,i. 7: if in the Realizable Case then 8: Let mk =\n768\u03c6k \u01ebk (d ln 768\u03c6k\u01ebk + ln 48 \u03b4k ). Draw mk i.i.d examples from \u0393k and query O for labels of\nthese examples to get a labelled data set Sk. Update Vk+1 using Sk: Vk+1 := {h \u2208 Vk : h(x) = y, for all (x, y) \u2208 Sk}.\n9: else\n10: In the non-realizable case, use Algorithm 2 with inputs hypothesis set Vk, distribution \u0393k, target excess error \u01ebk8\u03c6k , target confidence \u03b4k 2 , and the labeling oracle O to get a new hypothesis set Vk+1. 11: return an arbitrary h\u0302 \u2208 Vk0+1.\nCandidate Sets. At epoch k, we maintain a set Vk of candidate hypotheses guaranteed to contain the true risk minimizer h\u2217(D) (w.h.p). In the realizable case, we use a version space as our candidate set. The\n1where the expectation is with respect to the random choices made by P\nversion space with respect to a set S of labelled examples is the set of all h \u2208 H such that h(xi) = yi for all (xi, yi) \u2208 S.\nLemma 1. Suppose we run Algorithm 1 in the realizable case with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P , target excess error \u01eb and target confidence \u03b4. Then, with probability 1, h\u2217(D) \u2208 Vk, for all k = 1, 2, . . . , k0 + 1.\nIn the non-realizable case, the version space is usually empty; we use instead a (1\u2212\u03b1)-confidence set for the true risk minimizer. Given a set S of n labelled examples, let C(S) \u2286 H be a function of S; C(S) is said to be a (1\u2212 \u03b1)-confidence set for the true risk minimizer if for all data distributions \u2206 over X \u00d7 Y,\nPS\u223c\u2206n [h \u2217(\u2206) \u2208 C(S)] \u2265 1\u2212 \u03b1,\nRecall that h\u2217(\u2206) = argminh\u2208Herr\u2206(h). In the non-realizable case, our candidate sets are (1\u2212\u03b1)-confidence sets for h\u2217(D), for \u03b1 = \u03b4. The precise setting of Vk is explained in Algorithm 2.\nLemma 2. Suppose we run Algorithm 1 in the non-realizable case with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P , target excess error \u01eb and target confidence \u03b4. Then with probability 1\u2212 \u03b4, h\u2217(D) \u2208 Vk, for all k = 1, 2, . . . , k0 + 1.\nLabel Query. We next discuss our label query procedure \u2013 which examples should we query labels for, and how many labels should we query at each epoch?\nWhich Labels to Query? Our goal is to query the labels of the most informative examples. To choose these examples while still maintaining consistency, we use a confidence-rated predictor P with guaranteed error. The inputs to the predictor are our candidate hypothesis set Vk which contains (w.h.p) the true risk minimizer, a fresh set Uk of unlabelled examples, and an error guarantee \u03b7 = \u01ebk/64. For notation simplicity, assume the elements in Uk are distinct. The output is a sequence of abstention probabilities {\u03b3k,1, \u03b3k,2, . . . , \u03b3k,nk}, for each example in Uk. It induces a distribution \u0393k over Uk, from which we independently draw examples for label queries.\nHow Many Labels to Query? The goal of epoch k is to achieve excess generalization error \u01ebk. To achieve this, passive learning requires O\u0303(d/\u01ebk) labelled examples 2 in the realizable case, and O\u0303(d(\u03bd\u2217(D) + \u01ebk)/\u01eb 2 k) examples in the agnostic case. A key observation in this paper is that in order to achieve excess generalization error \u01ebk onD, it suffices to achieve a much larger excess generalization errorO(\u01ebk/\u03c6k) on the data distribution induced by \u0393k and DY |X , where \u03c6k is the fraction of examples on which the confidence-rated predictor abstains.\nIn the realizable case, we achieve this by sampling mk = 768\u03c6k \u01ebk (d ln 768\u03c6k\u01ebk + ln 48 \u03b4k ) i.i.d examples from \u0393k, and querying their labels to get a labelled dataset Sk. Observe that as \u03c6k is the abstention probability of P with guaranteed error \u2264 \u01ebk/64, it is generally smaller than the measure of the disagreement region of the version space; this key fact results in improved label complexity over disagreement-based active learning. This sampling procedure has the following property:\nLemma 3. Suppose we run Algorithm 1 in the realizable case with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P , target excess error \u01eb and target confidence \u03b4. Then with probability 1\u2212 \u03b4, for all k = 1, 2, . . . , k0 + 1, and for all h \u2208 Vk, errD(h) \u2264 \u01ebk. In particular, the h\u0302 returned at the end of the algorithm satisfies errD(h\u0302) \u2264 \u01eb.\nThe agnostic case has an added complication \u2013 in practice, the value of \u03bd\u2217 is not known ahead of time. Inspired by [Kol10], we use a doubling procedure(stated in Algorithm 2) which adaptively finds the number mk of labelled examples to be queried and queries them. The following two lemmas illustrate its properties \u2013 that it is consistent, and that it does not use too many label queries.\n2O\u0303(\u00b7) hides logarithmic factors\nLemma 4. Suppose we run Algorithm 2 with inputs hypothesis set V , example distribution \u2206, labelling oracle O, target excess error \u01eb\u0303 and target confidence \u03b4\u0303. Let \u2206\u0303 be the joint distribution on X \u00d7 Y induced by \u2206 and DY |X . Then there exists an event E\u0303, P(E\u0303) \u2265 1 \u2212 \u03b4\u0303, such that on E\u0303, (1) Algorithm 2 halts and (2) the set Vj0 has the following properties:\n(2.1) If for h \u2208 H, err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303/2, then h \u2208 Vj0 . (2.2) On the other hand, if h \u2208 Vj0 , then err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303.\nWhen event E\u0303 happens, we say Algorithm 2 succeeds.\nLemma 5. Suppose we run Algorithm 2 with inputs hypothesis set V , example distribution \u2206, labelling oracle O, target excess error \u01eb\u0303 and target confidence \u03b4\u0303. There exists some absolute constant c1 > 0, such that on the event that Algorithm 2 succeeds, nj0 \u2264 c1((d ln 1\u01eb\u0303 + ln 1\u03b4\u0303 ) \u03bd\u2217(\u2206\u0303)+\u01eb\u0303 \u01eb\u03032 ). Thus the total number of labels queried is \u2211j0\nj=1 nj \u2264 2nj0 \u2264 2c1((d ln 1\u01eb\u0303 + ln 1\u03b4\u0303 ) \u03bd\u2217(\u2206\u0303)+\u01eb\u0303 \u01eb\u03032 ).\nA naive approach (see Algorithm 4 in the Appendix) which uses an additive VC bound gives a sample complexity of O((d ln(1/\u01eb\u0303) + ln(1/\u03b4\u0303))\u01eb\u0303\u22122); Algorithm 2 gives a better sample complexity.\nThe following lemma is a consequence of our label query procedure in the non-realizable case.\nLemma 6. Suppose we run Algorithm 1 in the non-realizable case with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P , target excess error \u01eb and target confidence \u03b4. Then with probability 1\u2212 \u03b4, for all k = 1, 2, . . . , k0 + 1, and for all h \u2208 Vk, errD(h) \u2264 errD(h\u2217(D)) + \u01ebk. In particular, the h\u0302 returned at the end of the algorithm satisfies errD(h\u0302) \u2264 errD(h\u2217(D)) + \u01eb.\nAlgorithm 2 An Adaptive Algorithm for Label Query Given Target Excess Error\n1: Inputs: Hypothesis set V of VC dimension d, Example distribution \u2206, Labeling oracle O, target excess error \u01eb\u0303, target confidence \u03b4\u0303. 2: for j = 1, 2, . . . do 3: Draw nj = 2\nj i.i.d examples from \u2206; query their labels from O to get a labelled dataset Sj . Denote \u03b4\u0303j := \u03b4\u0303/(j(j + 1)).\n4: Train an ERM classifier h\u0302j \u2208 V over Sj . 5: Define the set Vj as follows:\nVj = { h \u2208 V : errSj (h) \u2264 errSj(h\u0302j) + \u01eb\u0303\n2 + \u03c3(nj , \u03b4\u0303j) +\n\u221a\n\u03c3(nj , \u03b4\u0303j)\u03c1Sj (h, h\u0302j) }\nWhere \u03c3(n, \u03b4) := 8n (2d ln 2en d + ln 24 \u03b4 ).\n6: if suph\u2208Vj (\u03c3(nj , \u03b4\u0303j) + \u221a\n\u03c3(nj , \u03b4\u0303j)\u03c1Sj (h, h\u0302j)) \u2264 \u01eb\u03036 then 7: j0 = j, break 8: return Vj0 ."}, {"heading": "2.3 Confidence-Rated Predictor", "text": "Our active learning algorithm uses a confidence-rated predictor with guaranteed error to make its label query decisions. In this section, we provide a novel confidence-rated predictor with guaranteed error. This predictor has optimal coverage in the realizable case, and may be of independent interest. The predictor P receives as input a set V \u2286 H of hypotheses (which is likely to contain the true risk minimizer), an error guarantee \u03b7, and a set of U of unlabelled examples. We consider a soft prediction algorithm; so, for each example in U , the predictor P outputs three probabilities that add up to 1 \u2013 the probability of predicting 1, \u22121 and 0. This output is subject to the constraint that the expected disagreement3 between the \u00b11 labels\n3where the expectation is taken over the random choices made by P\nassigned by P and those assigned by any h \u2208 V is at most \u03b7, and the goal is to maximize the coverage, or the expected fraction of non-abstentions.\nOur key insight is that this problem can be written as a linear program, which is described in Algorithm 3. There are three variables, \u03bei, \u03b6i and \u03b3i, for each unlabelled zi \u2208 U ; there are the probabilities with which we predict 1, \u22121 and 0 on zi respectively. Constraint (2) ensures that the expected disagreement between the label predicted and any h \u2208 V is no more than \u03b7, while the LP objective maximizes the coverage under these constraints. Observe that the LP is always feasible. Although the LP has infinitely many constraints, the number of constraints in Equation (2) distinguishable by Uk is at most (em/d)\nd, where d is the VC dimension of the hypothesis class H.\nAlgorithm 3 Confidence-rated Predictor\n1: Inputs: hypothesis set V , unlabelled data U = {z1, . . . , zm}, error bound \u03b7. 2: Solve the linear program:\nmin\nm \u2211\ni=1\n\u03b3i\nsubject to: \u2200i, \u03bei + \u03b6i + \u03b3i = 1 \u2200h \u2208 V, \u2211\ni:h(zi)=1\n\u03b6i + \u2211\ni:h(zi)=\u22121\n\u03bei \u2264 \u03b7m (2)\n\u2200i, \u03bei, \u03b6i, \u03b3i \u2265 0\n3: For each zi \u2208 U , output probabilities for predicting 1, \u22121 and 0: \u03bei, \u03b6i, and \u03b3i.\nThe performance of a confidence-rated predictor is measured by its error and coverage. The error of a confidence-rated predictor is the probability with which it predicts the wrong label on an example, while the coverage is its probability of non-abstention. We can show the following guarantee on the performance of the predictor in Algorithm 3.\nTheorem 1. In the realizable case, if the hypothesis set V is the version space with respect to a training set, then Px\u223cU (P (x) 6= h\u2217(x), P (x) 6= 0) \u2264 \u03b7. In the non-realizable case, if the hypothesis set V is an (1 \u2212 \u03b1)confidence set for the true risk minimizer h\u2217, then, w.p \u2265 1\u2212\u03b1, Px\u223cU (P (x) 6= y, P (x) 6= 0) \u2264 Px\u223cU (h\u2217(x) 6= y) + \u03b7.\nIn the realizable case, we can also show that our confidence rated predictor has optimal coverage. Observe that we cannot directly show optimality in the non-realizable case, as the performance depends on the exact choice of the (1 \u2212 \u03b1)-confidence set.\nTheorem 2. In the realizable case, suppose that the hypothesis set V is the version space with respect to a training set. If P \u2032 is any confidence rated predictor with error guarantee \u03b7, and if P is the predictor in Algorithm 3, then, the coverage of P is at least much as the coverage of P \u2032."}, {"heading": "3 Performance Guarantees", "text": "An essential property of any active learning algorithm is consistency \u2013 that it converges to the true risk minimizer given enough labelled examples. We observe that our algorithm is consistent provided we use any confidence-rated predictor P with guaranteed error as a subroutine. The consistency of our algorithm is a consequence of Lemmas 3 and 6 and is shown in Theorem 3.\nTheorem 3 (Consistency). Suppose we run Algorithm 1 with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P , target excess error \u01eb and target confidence \u03b4. Then with probability 1\u2212 \u03b4, the classifier h\u0302 returned by Algorithm 1 satisfies errD(h\u0302)\u2212 errD(h\u2217(D)) \u2264 \u01eb.\nWe now establish a label complexity bound for our algorithm; however, this label complexity bound applies only if we use the predictor described in Algorithm 3 as a subroutine.\nFor any hypothesis set V , data distribution D, and \u03b7, define \u03a6D(V, \u03b7) to be the minimum abstention probability of a confidence-rated predictor which guarantees that the disagreement between its predicted labels and any h \u2208 V under DX is at most \u03b7.\nFormally, \u03a6D(V, \u03b7) = min{ED\u03b3(x) : ED[I(h(x) = +1)\u03b6(x) + I(h(x) = \u22121)\u03be(x)] \u2264 \u03b7 for all h \u2208 V, \u03b3(x) + \u03be(x)+\u03b6(x) \u2261 1, \u03b3(x), \u03be(x), \u03b6(x) \u2265 0}. Define \u03c6(r, \u03b7) := \u03a6D(BD(h\u2217, r), \u03b7). The label complexity of our active learning algorithm can be stated as follows.\nTheorem 4 (Label Complexity). Suppose we run Algorithm 1 with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P of Algorithm 3, target excess error \u01eb and target confidence \u03b4. Then there exist constants c3, c4 > 0 such that with probability 1\u2212 \u03b4: (1) In the realizable case, the total number of labels queried by Algorithm 1 is at most:\nc3\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln \u03c6(\u01ebk, \u01ebk/256)\n\u01ebk + ln( \u2308log(1/\u01eb)\u2309 \u2212 k + 1 \u03b4 )) \u03c6(\u01ebk, \u01ebk/256) \u01ebk\n(2) In the agnostic case, the total number of labels queried by Algorithm 1 is at most:\nc4\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n\u01ebk + ln( \u2308log(1/\u01eb)\u2309 \u2212 k + 1 \u03b4 )) \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) \u01ebk (1 + \u03bd\u2217(D) \u01ebk )\nComparison. The label complexity of disagreement-based active learning is characterized in terms of the disagreement coefficient. Given a radius r, the disagreement coefficent \u03b8(r) is defined as:\n\u03b8(r) = sup r\u2032\u2265r\nP(DIS(BD(h \u2217, r\u2032)))\nr\u2032 ,\nwhere for any V \u2286 H, DIS(V ) is the disagreement region of V . As P(DIS(BD(h\u2217, r))) = \u03c6(r, 0) [EYW10], in our notation, \u03b8(r) = supr\u2032\u2265r \u03c6(r\u2032,0) r\u2032 .\nIn the realizable case, the label complexity of disagreement-based active learning is O\u0303(\u03b8(\u01eb) \u00b7 ln(1/\u01eb) \u00b7 (d ln \u03b8(\u01eb) + ln ln(1/\u01eb))) [Han13]4. Our label complexity bound may be simplified to:\nO\u0303\n(\nln 1\n\u01eb \u00b7 sup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(\u01ebk, \u01ebk/256) \u01ebk \u00b7 ( d ln ( sup k\u2264\u2308log(1/\u01eb)\u2309 \u03c6(\u01ebk, \u01ebk/256) \u01ebk ) + ln ln 1 \u01eb )) ,\nwhich is essentially the bound of [Han13] with \u03b8(\u01eb) replaced by supk\u2264\u2308log(1/\u01eb)\u2309 \u03c6(\u01ebk,\u01ebk/256)\n\u01ebk . As enforcing a\nlower error guarantee requires more abstention, \u03c6(r, \u03b7) is a decreasing function of \u03b7; as a result,\nsup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(\u01ebk, \u01ebk/256)\n\u01ebk \u2264 \u03b8(\u01eb),\nand our label complexity is better.\nIn the agnostic case, [DHM07] provides a label complexity bound of O\u0303(\u03b8(2\u03bd\u2217(D) + \u01eb) \u00b7 (d\u03bd \u2217(D)2\n\u01eb2 ln(1/\u01eb)+\nd ln2(1/\u01eb))) for disagreement-based active-learning. In contrast, by Proposition 1 our label complexity is at most:\nO\u0303\n(\nsup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) 2\u03bd\u2217(D) + \u01ebk \u00b7 ( d \u03bd\u2217(D)2 \u01eb2 ln(1/\u01eb) + d ln2(1/\u01eb) )\n)\n4Here the O\u0303() notation hides factors logarithmic in 1/\u03b4\nAgain, this is essentially the bound of [DHM07] with \u03b8(2\u03bd\u2217(D) + \u01eb) replaced by the smaller quantity\nsup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n2\u03bd\u2217(D) + \u01ebk ,\n[Han13] has provided a more refined analysis of disagreement-based active learning that gives a label\ncomplexity of O\u0303(\u03b8(\u03bd\u2217(D) + \u01eb)(\u03bd \u2217(D)2\n\u01eb2 + ln 1 \u01eb )(d ln \u03b8(\u03bd \u2217(D) + \u01eb) + ln ln 1\u01eb )); observe that their dependence is still on \u03b8(\u03bd\u2217(D) + \u01eb). We leave a more refined label complexity analysis of our algorithm for future work."}, {"heading": "3.1 Tsybakov Noise Conditions", "text": "An important sub-case of learning from noisy data is learning under the Tsybakov noise conditions [Tsy04].\nDefinition 1. (Tsybakov Noise Condition) Let \u03ba \u2265 1. A labelled data distribution D over X \u00d7 Y satisfies (C0, \u03ba)-Tsybakov Noise Condition with respect to a hypothesis class H for some constant C0 > 0, if for all h \u2208 H, \u03c1D(h, h\u2217(D)) \u2264 C0(errD(h)\u2212 errD(h\u2217(D))) 1 \u03ba .\nThe following theorem shows the performance guarantees achieved by Algorithm 1 under the Tsybakov noise conditions.\nTheorem 5. Suppose (C0, \u03ba)-Tsybakov Noise Condition holds for D with respect to H. Then Algorithm 1 with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P of Algorithm 3, target excess error \u01eb and target confidence \u03b4 satisfies the following properties. There exists a constant c5 > 0 such that with probability 1\u2212 \u03b4, the total number of labels queried by Algorithm 1 is at most:\nc5\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln(\u03c6(C0\u01eb 1 \u03ba k , \u01ebk/256)\u01eb 1 \u03ba\u22122 k ) + ln( \u2308log 1\u01eb \u2309 \u2212 k + 1\n\u03b4 ))\u03c6(C0\u01eb\n1 \u03ba k , \u01ebk/256)\u01eb 1 \u03ba\u22122 k\nComparison. [Han13] provides a label complexity bound of O\u0303(\u03b8(C0\u01eb 1 \u03ba )\u01eb 2 \u03ba\u22122 ln 1\u01eb (d ln \u03b8(C0\u01eb 1 \u03ba ) + ln ln 1\u01eb )) for disagreement-based active learning. For \u03ba > 1, by Proposition 2, our label complexity is at most:\nO\u0303\n(\nsup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(C0\u01eb 1/\u03ba k , \u01ebk/256)\n\u01eb 1/\u03ba k\n\u00b7 \u01eb2/\u03ba\u22122k \u00b7 d ln(1/\u01eb) ) ,\nFor \u03ba = 1, our label complexity is at most\nO\u0303\n(\nln 1\n\u01eb \u00b7 sup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(C0\u01ebk, \u01ebk/256) \u01ebk \u00b7 ( d ln( sup k\u2264\u2308log(1/\u01eb)\u2309 \u03c6(C0\u01ebk, \u01ebk/256) \u01ebk ) + ln ln 1 \u01eb )) .\nIn both cases, our bounds are better, as supk\u2264\u2308log(1/\u01eb)\u2309 \u00b7 \u03c6(C0\u01eb\n1/\u03ba k ,\u01ebk/256)\nC0\u01eb 1/\u03ba k \u2264 \u03b8(C0\u01eb1/\u03ba). In further work, [HY12] provides a refined analysis with a bound of O\u0303(\u03b8(C0\u01eb 1 \u03ba )\u01eb 2 \u03ba\u22122 d ln \u03b8(C0\u01eb 1\n\u03ba )); however, this work is not directly comparable to ours, as they need prior knowledge of C0 and \u03ba."}, {"heading": "3.2 Case Study: Linear Classification under the Log-concave Distribution", "text": "We now consider learning linear classifiers with respect to log-concave data distribution on Rd. In this case, for any r, the disagreement coefficient \u03b8(r) \u2264 O( \u221a d ln(1/r)) [BL13]; however, for any \u03b7 > 0, \u03c6(r,\u03b7)r \u2264 O(ln(r/\u03b7)) (see Lemma 14 in the Appendix), which is much smaller so long as \u03b7/r is not too small. This leads to the following label complexity bounds.\nCorollary 1. Suppose DX is isotropic and log-concave on R d, and H is the set of homogeneous linear classifiers on Rd. Then Algorithm 1 with inputs example oracle U , labelling oracle O, hypothesis class H, confidence-rated predictor P of Algorithm 3, target excess error \u01eb and target confidence \u03b4 satisfies the following properties. With probability 1\u2212 \u03b4: (1) In the realizable case, there exists some absolute constant c8 > 0 such that the total number of labels queried is at most c8 ln 1 \u01eb (d+ ln ln 1 \u01eb + ln 1 \u03b4 ). (2) In the agnostic case, there exists some absolute constant c9 > 0 such that the total number of labels queried is at most c9( \u03bd\u2217(D)2 \u01eb2 + ln 1 \u01eb ) ln \u01eb+\u03bd\u2217(D) \u01eb (d ln \u01eb+\u03bd\u2217(D) \u01eb + ln 1 \u03b4 ) + ln 1 \u01eb ln \u01eb+\u03bd\u2217(D) \u01eb ln ln 1 \u01eb . (3) If (C0, \u03ba)-Tsybakov Noise condition holds for D with respect to H, then there exists some constant c10 > 0 (that depends on C0, \u03ba) such that the total number of labels queried is at most c10\u01eb 2 \u03ba\u22122 ln 1\u01eb (d ln 1 \u01eb + ln 1 \u03b4 ).\nIn the realizable case, our bound matches [BL13]. For disagreement-based algorithms, the bound is\nO\u0303(d 3 2 ln2 1\u01eb (ln d + ln ln 1 \u01eb )), which is worse by a factor of O(\n\u221a d ln(1/\u01eb)). [BL13] does not address the fully\nagnostic case directly; however, if \u03bd\u2217(D) is known a-priori, then their algorithm can achieve roughly the same label complexity as ours.\nFor the Tsybakov Noise Condition with \u03ba > 1, [BBZ07, BL13] provides a label complexity bound for\nO\u0303(\u01eb 2 \u03ba\u22122 ln2 1\u01eb (d + ln ln 1 \u01eb )) with an algorithm that has a-priori knowledge of C0 and \u03ba. We get a slightly better bound. On the other hand, a disagreement based algorithm [Han13] gives a label complexity of O\u0303(d 3\n2 ln2 1\u01eb \u01eb 2 \u03ba\u22122(ln d + ln ln 1\u01eb )). Again our bound is better by factor of \u2126( \u221a d) over disagreement-based\nalgorithms. For \u03ba = 1, we can tighten our label complexity to get a O\u0303(ln 1\u01eb (d + ln ln 1 \u01eb + ln 1 \u03b4 )) bound, which again matches [BL13], and is better than the ones provided by disagreement-based algorithm \u2013 O\u0303(d 3\n2 ln2 1\u01eb (ln d+ ln ln 1 \u01eb )) [Han13]."}, {"heading": "4 Related Work", "text": "Active learning has seen a lot of progress over the past two decades, motivated by vast amounts of unlabelled data and the high cost of annotation [Set10, Das11, Han13]. According to [Das11], the two main threads of research are exploitation of cluster structure [UWBD13, DH08], and efficient search in hypothesis space, which is the setting of our work. We are given a hypothesis class H, and the goal is to find an h \u2208 H that achieves a target excess generalization error, while minimizing the number of label queries.\nThree main approaches have been studied in this setting. The first and most natural one is generalized binary search [FSST97, Das04, Das05, Now11], which was analyzed in the realizable case by [Das05] and in various limited noise settings by [Ka\u0308a\u030806, Now11, NJC13]. While this approach has the advantage of low label complexity, it is generally inconsistent in the fully agnostic setting [DH08]. The second approach, disagreement-based active learning, is consistent in the agnostic PAC model. [CAL94] provides the first disagreement-based algorithm for the realizable case. [BBL09] provides an agnostic disagreement-based algorithm, which is analyzed in [Han07] using the notion of disagreement coefficient. [DHM07] reduces disagreement-based active learning to passive learning; [BDL09] and [BHLZ10] further extend this work to provide practical and efficient implementations. [Han09, Kol10] give algorithms that are adaptive to the Tsybakov Noise condition. The third line of work [BBZ07, BL13, ABL14], achieves a better label complexity than disagreement-based active learning for linear classifiers on the uniform distribution over unit sphere and logconcave distributions. However, a limitation is that their algorithm applies only to these specific settings, and it is not apparent how to apply it generally.\nResearch on confidence-rated prediction has been mostly focused on empirical work, with relatively less theoretical development. Theoretical work on this topic includes KWIK learning [LLW08], conformal prediction [SV08] and the weighted majority algorithm of [FMS04]. The closest to our work is the recent learning-theoretic treatment by [EYW10, EYW11]. [EYW10] addresses confidence-rated prediction with guaranteed error in the realizable case, and provides a predictor that abstains in the disagreement region of the version space. This predictor achieves zero error, and coverage equal to the measure of the agreement region. [EYW11] shows how to extend this algorithm to the non-realizable case and obtain zero error with respect to the best hypothesis in H. Note that the predictors in [EYW10, EYW11] generally achieve less\ncoverage than ours for the same error guarantee; in fact, if we plug them into our Algorithm 1, then we recover the label complexity bounds of disagreement-based algorithms [DHM07, Han09, Kol10].\nA formal connection between disagreement-based active learning in realizable case and perfect confidencerated prediction (with a zero error guarantee) was established by [EYW12]. Our work can be seen as a step towards bridging these two areas, by demonstrating that active learning can be further reduced to imperfect confidence-rated prediction, with potentially higher label savings.\nAcknowledgements. We thank NSF under IIS-1162581 for research support. We thank Sanjoy Dasgupta and Yoav Freund for helpful discussions. CZ would also like to thank Liwei Wang for introducing the problem of selective classification to him."}, {"heading": "A Additional Notation and Concentration Lemmas", "text": "We begin with some additional notation that will be used in the subsequent proofs. Recall that we define:\n\u03c3(n, \u03b4) = 8\nn (2d ln\n2en\nd + ln\n24\n\u03b4 ), (3)\nwhere d is the VC dimension of the hypothesis class H. The following lemma is an immediate corollary of the multiplicative VC bound; we pick the version of the multiplicative VC bound due to [Hsu10].\nLemma 7. Pick any n \u2265 1, \u03b4 \u2208 (0, 1). Let Sn be a set of n iid copies of (X,Y ) drawn from a distribution D over labelled examples. Then, the following hold with probability at least 1\u2212 \u03b4 over the choice of Sn: (1) For all h \u2208 H,\n|errD(h)\u2212 errSn(h)| \u2264 min(\u03c3(n, \u03b4) + \u221a \u03c3(n, \u03b4)errD(h), \u03c3(n, \u03b4) + \u221a \u03c3(n, \u03b4)errSn(h)) (4)\nIn particular, all classifiers h in H consistent with Sn satisfies errD(h) \u2264 \u03c3(n, \u03b4) (5)\n(2) For all h, h\u2032 in H,\n|(errD(h)\u2212 errD(h\u2032))\u2212 (errSn(h)\u2212 errSn(h\u2032))| \u2264 \u03c3(n, \u03b4) +min( \u221a \u03c3(n, \u03b4)\u03c1D(h, h\u2032), \u221a \u03c3(n, \u03b4)\u03c1Sn(h, h \u2032)) (6)\n|\u03c1D(h, h\u2032)\u2212 \u03c1Sn(h, h\u2032)| \u2264 \u03c3(n, \u03b4) + min( \u221a \u03c3(n, \u03b4)\u03c1D(h, h\u2032), \u221a \u03c3(n, \u03b4)\u03c1Sn(h, h \u2032)) (7)\nWhere \u03c3(n, \u03b4) is defined in Equation (3).\nWe occasionally use the following (weaker) version of Lemma 7.\nLemma 8. Pick any n \u2265 1, \u03b4 \u2208 (0, 1). Let Sn be a set of n iid copies of (X,Y ). The following holds with probability at least 1\u2212 \u03b4: (1) For all h \u2208 H,\n|errD(h)\u2212 errSn(h)| \u2264 \u221a 4\u03c3(n, \u03b4) (8)\n(2) For all h, h\u2032 in H,\n|(errD(h)\u2212 errD(h\u2032))\u2212 (errSn(h)\u2212 errSn(h\u2032))| \u2264 \u221a 4\u03c3(n, \u03b4) (9)\n|\u03c1D(h, h\u2032)\u2212 \u03c1Sn(h, h\u2032)| \u2264 \u221a 4\u03c3(n, \u03b4) (10)\nWhere \u03c3(n, \u03b4) is defined in Equation (3).\nFor an unlabelled sample Uk, we use U\u0303k to denote the joint distribution over X \u00d7Y induced by uniform distribution over Uk and DY |X . We have:\nLemma 9. If the size of nk of the unlabelled dataset Uk is at least 192( 256 \u01ebk )2(d ln 256\u01ebk + ln 288 \u03b4k ), then with probability 1\u2212 \u03b4k/4, the following conditions hold for all h, h\u2032 \u2208 Vk:\n|errD(h)\u2212 errU\u0303k(h)| \u2264 \u01ebk 64\n(11)\n|(errD(h)\u2212 errD(h\u2032))\u2212 (errU\u0303k(h)\u2212 errU\u0303k(h \u2032))| \u2264 \u01ebk\n32 (12)\n|\u03c1D(h, h\u2032)\u2212 \u03c1U\u0303k(h, h \u2032)| \u2264 \u01ebk\n64 (13)\nLemma 10. If the size of nk of the unlabelled dataset Uk is at least 192( 256 \u01ebk )2(d ln 256\u01ebk + ln 288 \u03b4k ), then with probability 1\u2212 \u03b4k/4, the following hold: (1) The outputs {(\u03bek,i, \u03b6k,i, \u03b3k,i)}nki=1 of any confidence-rated predictor with inputs hypothesis set Vk, unlabelled data Uk, and error bound \u01ebk/64 satisfy:\n1\nnk\nnk \u2211\ni=1\n[I(h(xi) 6= h\u2032(xi))(1 \u2212 \u03b3k,i)] \u2264 \u01ebk 32 ; (14)\n(2) The outputs {(\u03bek,i, \u03b6k,i, \u03b3k,i)}nki=1 of the confidence-rated predictor of Algortihm 3 with inputs hypothesis set Vk, unlabelled data Uk, and error bound \u01ebk/64 satisfy:\n\u03c6k \u2264 \u03a6D(Vk, \u01ebk 128 ) + \u01ebk 256\n(15)\nWe use \u0393\u0303k to denote the joint distribution over X\u00d7Y induced by \u0393k andDY |X . Denote \u03b3k(x) : X \u2192 [0, 1], where \u03b3k(xi) = \u03b3k,i, and 0 elsewhere. Clearly, \u0393k({x}) = \u03b3k(x)nk\u03c6k and \u0393\u0303k({(x, y)}) = U\u0303k({(x,y)})\u03b3k(x) \u03c6k . Also, Equations (14) and (15) of Lemma 10 can be restated as\n\u2200h, h\u2032 \u2208 Vk,EU\u0303k [(1\u2212 \u03b3k(x))I(h(x) 6= h \u2032(x))] \u2264 \u01ebk\n32\nEU\u0303k [\u03b3k(x)] = \u03c6k \u2264 \u03a6D(Vk, \u01ebk 128 ) + \u01ebk 256\nIn the realizable case, define event\nEr = {For all k = 1, 2, . . . , k0: Equations (11), (12), (13), (14), (15) hold for U\u0303k and all classifiers consistent with Sk have error at most\n\u01ebk 8\u03c6k with respect to \u0393\u0303k }.\nFact 1. P(Er) \u2265 1\u2212 \u03b4.\nProof. By Equation (5) of Lemma 7, with probability 1\u2212 \u03b4k/2, if h \u2208 Vk is consistent with Sk, then\nerr\u0393\u0303k(h) \u2264 \u03c3(mk, \u03b4k/2)\nBecause mk = 768\u03c6k \u01ebk (d ln 768\u03c6k\u01ebk + ln 48 \u03b4k ), we have err\u0393\u0303k(h) \u2264 \u01ebk/8\u03c6k. The fact follows from combining the fact above with Lemma 9 and Lemma 10, and the union bound.\nIn the non-realizable case, define event\nEa = {For all k = 1, 2, . . . , k0: Equations (11), (12), (13), (14), (15) hold for U\u0303k, and Algorithm 2 succeeds with inputs hypothesis set V = Vk, example distribution \u2206 = \u0393k, labelling oracle O, target excess error \u01eb\u0303 = \u01ebk 8\u03c6k and target confidence \u03b4\u0303 = \u03b4k 2 }.\nFact 2. P(Ea) \u2265 1\u2212 \u03b4.\nProof. This is an immediate consequence of Lemma 9, Lemma 10, Lemma 4 and union bound.\nRecall that we assume the hypothesis space is \u201cdense\u201d, in the sense that \u2200r > 0, suph\u2208BD(h\u2217(D),r) \u03c1(h, h\u2217(D)) = r. We will call this the \u201cdenseness assumption\u201d."}, {"heading": "B Proofs related to the properties of Algorithm 2", "text": "We first establish some properties of Algorithm 2. The inputs to Algorithm 2 are a set V of hypotheses of VC dimension d, an example distribution \u2206, a labeling oracle O, a target excess error \u01eb\u0303 and a target confidence \u03b4\u0303.\nWe define the event\nE\u0303 = {For all j = 1, 2, . . . : Equations (4)-(7) hold for sample Sj with n = nj and \u03b4 = \u03b4\u0303j }\nBy union bound, P(E\u0303) \u2265 1\u2212\u2211j \u03b4\u0303j \u2265 1\u2212 \u03b4\u0303.\nProof. (of Lemma 4) Assume E\u0303 happens. For the proof of (1), define jmax as the smallest integer j such that \u03c3(nj , \u03b4\u0303j) \u2264 \u01eb\u03032/144. Since njmax is a power of 2,\nnjmax \u2264 2min{n = 1, 2, . . . : 8(2d ln 2end + ln 24 logn(log n+1) \u03b4 )\nn \u2264 \u01eb\n2\n144 }\nThus, njmax \u2264 192 144\u01eb\u03032 (d ln 144\u01eb\u0303 + ln 24\u03b4\u0303 ). Then in round jmax, the stopping criterion (6) of Algorithm 2 is satisified; thus, Algorithm 2 halts with j0 \u2264 jmax.\nTo prove (2.1), we observe that as h\u2217(\u2206\u0303) is the risk minimizer in V , if h satisfies err\u2206\u0303(h)\u2212err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303 2 , then err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302j0) \u2264 \u01eb\u03032 . By Equation (6) of Lemma 7,\n(errSj0 (h)\u2212 errSj0 (h\u0302j0)) \u2264 (err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302j0)) + \u03c3(nj0 , \u03b4\u0303j0) + \u221a \u03c3(nj0 , \u03b4\u0303j0)\u03c1Sj0 (h, h\u0302j0)\n\u2264 \u01eb\u0303 2 + \u03c3(nj0 , \u03b4\u0303j0) +\n\u221a\n\u03c3(nj0 , \u03b4\u0303j0)\u03c1Sj0 (h, h\u0302j0)\nHence h \u2208 Vj0 .\nFor the proof of (2.2), note first that by (2.1), in particular, h\u2217(\u2206\u0303) \u2208 Vj0 . Hence by Equation (6) of Lemma 7, and the stopping criterion Equation (6),\n(err\u2206\u0303(h\u0302j0)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)))\u2212 (errSj0 (h\u0302j0 )\u2212 errSj0 (h \u2217(\u2206\u0303))) \u2264 \u03c3(nj0 , \u03b4\u0303j0) +\n\u221a\n\u03c3(nj0 , \u03b4\u0303j0)\u03c1Sj0 (h\u0302j0 , h \u2217(\u2206\u0303)) \u2264 \u01eb\u0303\n6\nThus,\nerr\u2206\u0303(h\u0302j0)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303\n6 (16)\nOn the other hand, if h \u2208 Vj0 , then\n(err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302j0))\u2212 (errSj0 (h)\u2212 errSj0 (h\u0302j0)) \u2264 \u03c3(nj0 , \u03b4\u0303j0) + \u221a \u03c3(nj0 , \u03b4\u0303j0)\u03c1Sj0 (h, h\u0302j0) \u2264 \u01eb\u0303\n6\nBy definition of Vj0 ,\n(errSj0 (h)\u2212 errSj0 (h\u0302j0)) \u2264 \u03c3(nj0 , \u03b4\u0303j0) + \u221a \u03c3(nj0 , \u03b4\u0303j0)\u03c1Sj0 (h, h\u0302j0) + \u01eb\u0303 2 \u2264 2\u01eb\u0303 3\nHence,\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302j0 ) \u2264 5\u01eb\u0303\n6 (17)\nCombining Equations (16) and (17), we have\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303\nProof. (of Lemma 5) Assume E\u0303 happens. For each j, by triangle inequality, we have that \u03c1Sj (h\u0302j , h) \u2264 errSj (h\u0302j) + errSj (h). If h \u2208 Vj , then, by defintion of Vj ,\nerrSj (h)\u2212 errSj (h\u0302j) \u2264 \u01eb\u0303\n2 + \u03c3(nj , \u03b4\u0303j) +\n\u221a\n\u03c3(nj , \u03b4\u0303j)errSj (h\u0302j) + \u221a \u03c3(nj , \u03b4\u0303j)errSj (h)\nUsing the fact that A \u2264 B + C \u221a A \u21d2 A \u2264 2B + C2,\nerrSj (h) \u2264 \u01eb\u0303+ 2errSj (h\u0302j) + 2 \u221a \u03c3(nj , \u03b4\u0303j)errSj (h\u0302j) + 3\u03c3(nj , \u03b4\u0303j) \u2264 3errSj (h\u0302j) + 4\u03c3(nj , \u03b4\u0303j) + \u01eb\u0303\nSince\nerrSj (h\u0302j) \u2264 errSj (h\u2217(\u2206\u0303)) \u2264 \u03bd\u2217(\u2206\u0303) + \u221a \u03c3(nj , \u03b4\u0303j)\u03bd\u2217(\u2206\u0303) + \u03c3(nj , \u03b4\u0303j) \u2264 2\u03bd\u2217(\u2206\u0303) + 2\u03c3(nj , \u03b4\u0303j),\nby the triangle inequality, we get that for all h \u2208 Vj ,\n\u03c1Sj (h, h\u0302j) \u2264 errSj (h) + errSj (h\u0302j) \u2264 8\u03bd\u2217(\u2206\u0303) + 12\u03c3(nj, \u03b4\u0303j) + \u01eb\u0303 (18)\nNow observe that for any j,\nsup h\u2208Vj\n\u221a\n\u03c3(nj , \u03b4\u0303j)\u03c1Sj (h, h\u0302j) + \u03c3(nj , \u03b4\u0303j)\n\u2264 sup h\u2208Vj max(2\n\u221a\n\u03c3(nj , \u03b4\u0303j)\u03c1Sj (h, h\u0302j), 2\u03c3(nj , \u03b4\u0303j))\n\u2264 max(2 \u221a (8\u03bd\u2217(\u2206\u0303) + 12\u03c3(nj , \u03b4\u0303j) + \u01eb\u0303)\u03c3(nj , \u03b4\u0303j), 2\u03c3(nj , \u03b4\u0303j))\n\u2264 max(12 \u221a 2\u03bd\u2217(\u2206\u0303)\u03c3(nj , \u03b4\u0303j), \u01eb\u0303/6, 216\u03c3(nj, \u03b4\u0303j)),\nWhere the first inequality follows from A + B \u2264 2max(A,B), the second inequality follows from Equation (18), the third inequality follows from \u221a A+B \u2264 \u221a A + \u221a B, A + B + C \u2264 3max(A,B,C) and\u221a\nAB \u2264 max(A,B). It can be easily seen that there exists some constant c1 > 0, such that taking j1 = \u2308log ( c1 2 (d ln 1 \u01eb\u0303 + ln 1 \u03b4\u0303 )(\u03bd \u2217(\u2206\u0303)+\u01eb\u0303 \u01eb\u03032 ) ) \u2309\nensures that nj1 \u2265 c12 (d ln 1\u01eb\u0303 + ln 1\u03b4\u0303 )( \u03bd\u2217(\u2206\u0303)+\u01eb\u0303 \u01eb\u03032 ); this, in turn, suffices to make\nmax(12\n\u221a\n2\u03bd\u2217(\u2206\u0303)\u03c3(nj , \u03b4\u0303j), 216\u03c3(nj, \u03b4\u0303j)) \u2264 \u01eb\u0303/6\nHence the stopping criterion suph\u2208Vj\n\u221a\n\u03c3(nj , \u03b4\u0303j)\u03c1Sj (h, h\u0302j) + \u03c3(nj , \u03b4\u0303j) \u2264 \u01eb\u0303/6 is satisfied in iteration j1, and Algorithm 2 exits at iteration j0 \u2264 j1, which ensures that nj0 \u2264 nj1 \u2264 c1(d ln 1\u01eb\u0303 + ln 1\u03b4\u0303 )( \u03bd\u2217(\u2206\u0303)+\u01eb\u0303 \u01eb\u03032 ).\nThe following lemma examines the behavior of Algorithm 2 under the Tsybakov Noise Condition and is crucial in the proof of Theorem 5. We observe that even if the (C0, \u03ba)-Tsybakov Noise Conditions hold with respect to D, they do not necessarily hold with respect to \u0393k. In particular, it is not necessarily true that:\n\u03c1\u0393\u0303k(h, h \u2217(D)) \u2264 C0(err\u0393\u0303k(h)\u2212 err\u0393\u0303k(h \u2217(D))) 1 \u03ba , \u2200h \u2208 Vk\nHowever, we show that an \u201capproximate\u201d Tsybakov Noise Condition with a significantly larger \u201cC0\u201d, namely Condition (19) is met by \u0393\u0303k and Vk, with C = max(8C0, 4)\u03c6 1 \u03ba\u22121 k and h\u0303 = h \u2217(D). In the Lemma below, we carefully track the dependence of the number of our label queries on C, since C = max(8C0, 4)\u03c6 1 \u03ba\u22121\nk can be \u03c9(1) in our particular application.\nLemma 11. Suppose we run Algorithm 2 with inputs hypothesis set V , example distribution \u2206\u0303, labelling oracle O, excess generalization error \u01eb\u0303 and confidence \u03b4\u0303. Then there exists some absolute constant c2 > 0 (independent of C) such that the following holds. Suppose there exist C > 0 and a classifier h\u0303 \u2208 V , such that\n\u2200h \u2208 V, \u03c1\u2206\u0303(h, h\u0303) \u2264 Cmax(\u01eb\u0303, err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303)) 1 \u03ba , (19)\nwhere \u01eb\u0303 is the target exccess error parameter in Algorithm 2. Then, on the event that Algorithm 2 succeeds,\nnj0 \u2264 c2 max((d ln 1\n\u01eb\u0303 + ln\n1 \u03b4\u0303 )\u01eb\u0303\u22121, (d ln(C\u01eb\u0303 1 \u03ba\u22122) + ln 1 \u03b4\u0303 )C\u01eb\u0303 1 \u03ba\u22122)\nObserve that Condition (19), the approximate Tsybakov Noise Condition in the statement of Lemma 11, is with respect to h\u0303, which is not necessarily the true risk minimizer in V with respect to \u2206\u0303. We therefore prove Lemma 11 in three steps; first, in Lemma 12, we analyze the difference err\u2206\u0303(h\u0302)\u2212 err\u2206\u0303(h\u0303), where h\u0302 is the empirical risk minimizer. Then, in Lemma 13, we bound the difference err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) for any h \u2208 Vj for some j. Finally, we combine these two lemmas to provide sample complexity bounds for the Vj0 output by Algorithm 2.\nProof. (of Lemma 11) Assume the event E\u0303 happens. Then, Consider iteration j, by Lemma 13, if h \u2208 Vj , then\n\u03c1\u2206\u0303(h, h\u0302j) \u2264 \u03c1\u2206\u0303(h, h\u0303) + \u03c1\u2206\u0303(h\u0302j , h\u0303) \u2264 max(2C(36\u01eb\u0303) 1 \u03ba , 2C(52\u03c3(nj, \u03b4\u0303j)) 1 \u03ba , 2C(6400C\u03c3(nj, \u03b4\u0303j)) 1 2\u03ba\u22121 ). (20)\nWe can write:\nsup h\u2208Vj \u03c3(nj , \u03b4\u0303j) +\n\u221a\n\u03c3(nj , \u03b4\u0303j)\u03c1Sj (h, h\u0302j) \u2264 sup h\u2208Vj 3\u03c3(nj , \u03b4\u0303j) +\n\u221a\n2\u03c3(nj , \u03b4\u0303j)\u03c1\u2206\u0303(h, h\u0302j)\n\u2264 sup h\u2208Vj max(6\u03c3(nj , \u03b4\u0303j), 2\n\u221a\n2\u03c3(nj , \u03b4\u0303j)\u03c1\u2206\u0303(h, h\u0302j)),\nwhere the first inequality follows from Equation (23) and the second inequality follows A+B \u2264 2max(A,B). We can further use Equation (20) to show that this is at most:\n\u2264 max(6\u03c3(nj , \u03b4\u0303j), (16C\u03c3(nj , \u03b4\u0303j)) 1 2 (36\u01eb\u0303) 1 2\u03ba , (16C\u03c3(nj , \u03b4\u0303j)) 1 2 (52\u03c3(nj , \u03b4\u0303j)) 1 2\u03ba , (6400C\u03c3(nj, \u03b4\u0303j)) \u03ba 2\u03ba\u22121 ) \u2264 max(6\u03c3(nj , \u03b4\u0303j), \u01eb\u0303/6, (6400C\u03c3(nj, \u03b4\u0303j)) \u03ba 2\u03ba\u22121 )\nHere the last inequality follows from the fact that (16C\u03c3(nj , \u03b4\u0303j)) 1 2 (36\u01eb\u0303) 1 2\u03ba \u2264 max((3456C\u03c3(nj , \u03b4\u0303j)) \u03ba 2\u03ba\u22121 , \u01eb\u0303/6) and (16C\u03c3(nj , \u03b4\u0303j)) 1 2 (52\u03c3(nj , \u03b4\u0303j)) 1 2\u03ba \u2264 max((144C\u03c3(nj , \u03b4\u0303j)) \u03ba 2\u03ba\u22121 , 6\u03c3(nj , \u03b4\u0303j)), since A 2\u03ba\u22121 2\u03ba B 1\n2\u03ba \u2264 max(A,B). It can be easily seen that there exists c2 > 0, such that taking j1 = \u2308log c22 (d ln max(C,1) \u01eb\u0303 + ln 1 \u03b4\u0303 )(C\u01eb\u0303 1 \u03ba\u22122 +\n\u01eb\u0303\u22121)\u2309, so that nj \u2265 c22 (d ln max(C,1) \u01eb\u0303 + ln 1 \u03b4\u0303 )(C\u01eb\u0303 1 \u03ba\u22122 + \u01eb\u0303\u22121) suffices to make\nmax(6\u03c3(nj , \u03b4\u0303j), (6400C\u03c3(nj, \u03b4\u0303j)) \u03ba 2\u03ba\u22121 ) \u2264 \u01eb\u0303/6\nHence the stopping criterion suph\u2208Vj\n\u221a\n\u03c3(nj , \u03b4\u0303j)\u03c1Sj (h, h\u0302j) + \u03c3(nj , \u03b4\u0303j) \u2264 \u01eb\u0303/6 is satisfied in iteration j1. Thus the number of the exit iteration j0 satisfies j0 \u2264 j1, and nj0 \u2264 nj1 \u2264 c2 max((d ln 1\u01eb\u0303 +ln 1\u03b4\u0303 )\u01eb\u0303 \u22121, (d ln(C\u01eb\u0303 1 \u03ba\u22122)+ ln 1 \u03b4\u0303 )C\u01eb\u0303 1 \u03ba\u22122).\nLemma 12. Suppose there exist C > 0 and a classifier h\u0303 \u2208 V , such that Equation (19) holds. Suppose we draw a set S of n examples, denote the empirical risk minimizer over S as h\u0302, then with probability 1\u2212 \u03b4:\nerr\u2206\u0303(h\u0302)\u2212 err\u2206\u0303(h\u0303) \u2264 max(2\u03c3(n, \u03b4), (4C\u03c3(n, \u03b4)) \u03ba 2\u03ba\u22121 , 2\u01eb\u0303)\n\u03c1\u2206\u0303(h\u0302, h\u0303) \u2264 max(C(2\u03c3(n, \u03b4)) 1 \u03ba , C(4C\u03c3(n, \u03b4)) 1 2\u03ba\u22121 , C(2\u01eb\u0303) 1 \u03ba )\nProof. By Lemma 7, with probability 1\u2212 \u03b4, Equation (6) holds. Assume this happens.\nerr\u2206\u0303(h\u0302)\u2212 err\u2206\u0303(h\u0303)\n\u2264 \u03c3(n, \u03b4) + \u221a\n\u03c3(n, \u03b4)\u03c1\u2206\u0303(h\u0302, h\u0303)\n\u2264 2max(\u03c3(n, \u03b4), \u221a\n\u03c3(n, \u03b4)C(err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) 1 \u03ba ),\n\u221a\n\u03c3(n, \u03b4)C\u01eb\u0303 1 \u03ba )\n\u2264 max(2\u03c3(n, \u03b4), (4C\u03c3(n, \u03b4)) \u03ba2\u03ba\u22121 , 2\u01eb\u0303)\nWhere the first inequality is by Equation (6) of Lemma 7; the second inequality follow from Equation (19) and A + B \u2264 2max(A,B). The third inequality follows from 2 \u221a \u03c3(n, \u03b4)C\u01eb\u0303 1\n\u03ba \u2264 max(2(C\u03c3(n, \u03b4)) \u03ba2\u03ba\u22121 , 2\u01eb\u0303), since A 2\u03ba\u22121 2\u03ba B 1 2\u03ba \u2264 max(A,B). As a consequence, by Equation (19),\n\u03c1\u2206\u0303(h\u0302, h\u0303) \u2264 max(C(2\u03c3(n, \u03b4)) 1 \u03ba , C(4C\u03c3(n, \u03b4)) 1 2\u03ba\u22121 , C(2\u01eb\u0303) 1 \u03ba )\nLemma 13. Suppose there exist a C > 0 and a classifier h\u0303 \u2208 V such that Equation (19) holds. Suppose we draw a set S of n iid examples, and let h\u0302 denote the empirical risk minimizer over S. Moreover, we define:\nV\u0303 = { h \u2208 V : errS(h) \u2264 errS(h\u0302) + \u01eb\u0303\n2 + \u03c3(n, \u03b4) +\n\u221a\n\u03c3(n, \u03b4)\u03c1S(h, h\u0302) }\nthen with probability 1\u2212 \u03b4, for all h \u2208 V\u0303 ,\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) \u2264 max(52\u03c3(n, \u03b4), 36\u01eb\u0303, (6400C\u03c3(n, \u03b4)) \u03ba 2\u03ba\u22121 )\n\u03c1\u2206\u0303(h, h\u0303) \u2264 max(C(36\u01eb\u0303) 1 \u03ba , C(52\u03c3(n, \u03b4)) 1 \u03ba , C(6400C\u03c3(n, \u03b4)) 1 2\u03ba\u22121 )\nProof. First, by Lemma 12,\nerr\u2206\u0303(h\u0302)\u2212 err\u2206\u0303(h\u0303) \u2264 max(2\u03c3(n, \u03b4), (4C\u03c3(n, \u03b4)) \u03ba 2\u03ba\u22121 , 2\u01eb\u0303) (21)\n\u03c1\u2206\u0303(h\u0302, h\u0303) \u2264 max(C(2\u03c3(n, \u03b4)) 1 \u03ba , C(4C\u03c3(n, \u03b4)) 1 2\u03ba\u22121 , C(2\u01eb\u0303) 1 \u03ba ) (22)\nNext, if h \u2208 V\u0303 , then errS(h)\u2212 errS(h\u0302) \u2264 \u03c3(n, \u03b4) + \u221a \u03c3(n, \u03b4)\u03c1S(h, h\u0302) + \u01eb\u0303\n2\nCombining it with Equation (6) of Lemma 7: err\u2206\u0303(h) \u2212 err\u2206\u0303(h\u0302) \u2264 errS(h) \u2212 errS(h\u0302) + \u221a \u03c3(n, \u03b4)\u03c1S(h, h\u0302) + \u03c3(n, \u03b4), we get\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302) \u2264 2\u03c3(n, \u03b4) + 2 \u221a \u03c3(n, \u03b4)\u03c1S(h, h\u0302) + \u01eb\u0303\n2\nBy Equation (7) of Lemma 7,\n\u03c1S(h, h\u0302) \u2264 \u03c1\u2206\u0303(h, h\u0302) + \u221a \u03c3(n, \u03b4)\u03c1\u2206\u0303(h, h\u0302) + \u03c3(n, \u03b4) \u2264 2\u03c1\u2206\u0303(h, h\u0302) + 2\u03c3(n, \u03b4) (23)\nTherefore,\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302) \u2264 5\u03c3(n, \u03b4) + 3 \u221a \u03c3(n, \u03b4)\u03c1\u2206\u0303(h, h\u0302) + \u01eb\u0303\n2 (24)\nHence\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) = (err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302)) + (err\u2206\u0303(h\u0302)\u2212 err\u2206\u0303(h\u0303)) \u2264 (4C\u03c3(n, \u03b4)) \u03ba2\u03ba\u22121 + 7\u03c3(n, \u03b4) + 3\u01eb\u0303+ 3 \u221a\n\u03c3(n, \u03b4)\u03c1\u2206\u0303(h, h\u0302)\n\u2264 (4C\u03c3(n, \u03b4)) \u03ba2\u03ba\u22121 + 7\u03c3(n, \u03b4) + 3\u01eb\u0303+ 3 \u221a\n\u03c3(n, \u03b4)\u03c1\u2206\u0303(h, h\u0303) + 3\n\u221a\n\u03c3(n, \u03b4)\u03c1\u2206\u0303(h\u0303, h\u0302)\nHere the first inequality follows from Equations (21) and (24) and max(A,B,C) \u2264 A + B + C, and the second inequality follows from triangle inequality and \u221a A+B \u2264 \u221a A+ \u221a B.\nFrom Equation (22), \u03c3(n, \u03b4)\u03c1\u2206\u0303(h\u0302, h\u0303) is at most:\n\u2264 C\u03c3(n, \u03b4) \u00b7 ((2\u01eb\u0303)1/\u03ba + (2\u03c3(n, \u03b4))1/\u03ba + (4C\u03c3(n, \u03b4))1/(2\u03ba\u22121)) \u2264 (4C\u03c3(n, \u03b4))2\u03ba/(2\u03ba\u22121) + C\u03c3(n, \u03b4)((2\u01eb\u0303)1/\u03ba + (2\u03c3(n, \u03b4))1/\u03ba) \u2264 (4C\u03c3(n, \u03b4))2\u03ba/(2\u03ba\u22121) +max(4\u01eb\u03032, (C\u03c3(n, \u03b4))2\u03ba/(2\u03ba\u22121)) + max(4\u03c3(n, \u03b4)2, (C\u03c3(n, \u03b4))2\u03ba/(2\u03ba\u22121)),\nwhere the first step follows from Equation (22), the second step from algebra, and the third step from using the fact that A 2\u03ba\u22121 \u03ba B 1\n\u03ba \u2264 max(A2, B2). Plugging this in to the previous equation, and using max(A,B) \u2264 A+B and \u221a A+B \u2264 \u221a A+ \u221a B, we get that:\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) \u2264 10(4C\u03c3(n, \u03b4))\u03ba/(2\u03ba\u22121) + 9\u01eb\u0303+ 13\u03c3(n, \u03b4) + 3 \u221a \u03c3(n, \u03b4)\u03c1\u2206\u0303(h, h\u0303)\nCombining this with the fact that A+B + C +D \u2264 4max(A,B,C,D), we get that this is at most:\n\u2264 max(40(4C\u03c3(n, \u03b4))\u03ba/(2\u03ba\u22121), 36\u01eb\u0303, 52\u03c3(n, \u03b4), 12 \u221a\n\u03c3(n, \u03b4)\u03c1\u2206\u0303(h, h\u0303))\nCombining this with Condition (19), we get that this is at most:\nmax(40(4C\u03c3(n, \u03b4))\u03ba/(2\u03ba\u22121), 36\u01eb\u0303, 52\u03c3(n, \u03b4), 12 \u221a C\u03c3(n, \u03b4)\u01eb\u03031/\u03ba, 12\n\u221a\nC\u03c3(n, \u03b4)(err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303))1/\u03ba)\nUsing A(2\u03ba\u22121)/2\u03baB1/2\u03ba \u2264 max(A,B), we get that \u221a\nC\u03c3(n, \u03b4)\u01eb\u03031/\u03ba \u2264 max(\u01eb\u0303, (C\u03c3(n, \u03b4))\u03ba/(2\u03ba\u22121)). Also note err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) \u2264 12 \u221a\nC\u03c3(n, \u03b4)(err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303))1/\u03ba implies err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) \u2264 (144C\u03c3(n, \u03b4))\u03ba/(2\u03ba\u22121). Thus we have\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0303) \u2264 max(36\u01eb\u0303, 52\u03c3(n, \u03b4), (6400C\u03c3(n, \u03b4)) \u03ba 2\u03ba\u22121 )\nInvoking (19) again, we have that:\n\u03c1\u2206\u0303(h, h\u0303) \u2264 max(C(36\u01eb\u0303) 1 \u03ba , C(52\u03c3(n, \u03b4)) 1 \u03ba , C(6400C\u03c3(n, \u03b4)) 1 2\u03ba\u22121 )"}, {"heading": "C Remaining Proofs from Section 2", "text": "Proof. (Of Lemma 1) Assuming Er happens, we prove the lemma by induction. Base Case: For k = 1, clearly h\u2217(D) \u2208 V1 = H. Inductive Case: Assume h\u2217(D) \u2208 Vk. As we are in the realizable case, h\u2217(D) is consistent with the examples Sk drawn in Step 8 of Algorithm 1; thus h \u2217(D) \u2208 Vk+1. The lemma follows.\nProof. (Of Lemma 2) We use h\u0303k = argminh\u2208Vkerr\u0393\u0303k(h) to denote the optimal classifier in Vk with respect to the distribution \u0393\u0303k. Assuming Ea happens, we prove the lemma by induction. Base Case: For k = 1, clearly h\u2217(D) \u2208 V1 = H. Inductive Case: Assume h\u2217 \u2208 Vk. In order to show the inductive case, our goal is to show that:\nP\u0393\u0303k (h\u2217(D)(x) 6= y)\u2212 P\u0393\u0303k(h\u0303k(x) 6= y) \u2264 \u01ebk 16\u03c6k\n(25)\nIf (25) holds, then, by (2.1) of Lemma 4, we know that if Algorithm 2 succeeds when called in iteration k of Algorithm 1, then, it is guaranteed that h\u2217 \u2208 Vk+1.\nWe therefore focus on showing (25). First, from Equation (12) of Lemma 9, we have:\n(errU\u0303k(h \u2217(D)) \u2212 errU\u0303k(h\u0303k))\u2212 (errD(h \u2217(D))\u2212 errD(h\u0303k)) \u2264 \u01ebk 32\nAs errD(h \u2217(D)) \u2264 errD(h\u0303k), we get:\nerrU\u0303k(h \u2217(D)) \u2264 errU\u0303k(h\u0303k) + \u01ebk 32\n(26)\nOn the other hand, by Equation (14) of Lemma 10 and triangle inequality,\nEU\u0303k [I(h\u0303k(x) 6= y)(1\u2212 \u03b3k(x))] \u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)(1 \u2212 \u03b3k(x))] (27)\n\u2264 EU\u0303k [I(h \u2217(D)(x) 6= h\u0303k(x))(1 \u2212 \u03b3k(x))] \u2264 \u01ebk 32\n(28)\nCombining Equations (26) and (27), we get:\nEU\u0303k [I(h\u2217(D)(x) 6= y)\u03b3k(x)] = errU\u0303k(h \u2217(D)(x)) \u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)(1\u2212 \u03b3k(x))]\n\u2264 errU\u0303k(h\u0303k(x)) + \u01ebk/32\u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)(1\u2212 \u03b3k(x))] \u2264 EU\u0303k [I(h\u0303k(x) 6= y)\u03b3k(x)] + EU\u0303k [I(h\u0303(x) 6= y)(1\u2212 \u03b3k(x))] + \u01ebk/32 \u2212EU\u0303k [I(h\n\u2217(D)(x) 6= y)(1\u2212 \u03b3k(x))] \u2264 EU\u0303k [I(h\u0303k(x) 6= y)\u03b3k(x)] + \u01ebk/16\nDividing both sides by \u03c6k, we get:\nP\u0393\u0303k (h\u2217(D)(x) 6= y)\u2212 P\u0393\u0303k(h\u0303k(x) 6= y) \u2264 \u01ebk 16\u03c6k ,\nfrom which the lemma follows.\nProof. (of Lemma 3) Assuming Er happens, we prove the lemma by induction. Base Case: For k = 1, clearly errD(h) \u2264 1 \u2264 \u01eb1 = \u01eb2k0 , \u2200h \u2208 V1 = H. Inductive Case: Note that \u2200h, h\u2032 \u2208 Vk+1 \u2286 Vk, by Equation (14) of Lemma 10, we have:\nEU\u0303k [I(h(x) 6= h\u2032(x))(1 \u2212 \u03b3k(x))] \u2264 \u01ebk 8\nBy the proof of Lemma 1, h\u2217(D) \u2208 Vk+1 on event Er, thus \u2200h \u2208 Vk+1,\nEU\u0303k [I(h(x) 6= h\u2217(D)(x))(1 \u2212 \u03b3k(x))] \u2264 \u01ebk 8\n(29)\nSince any h \u2208 Vk+1, h is consistent with Sk of size mk = 768\u03c6k\u01ebk (d ln 768\u03c6k \u01ebk + ln 48\u03b4k ), we have that for all h \u2208 Vk+1,\nP\u0393\u0303k (h(x) 6= h\u2217(D)(x)) \u2264 \u01ebk\n8\u03c6k That is,\nEU\u0303k [I(h(x) 6= h\u2217(D)(x))\u03b3k(x)] \u2264 \u01ebk 8\nCombining this with Equation (29) above,\nPU\u0303k (h(x) 6= h\u2217(D)(x)) \u2264 \u01ebk\n4\nBy Equation (11) of Lemma 9,\nPD(h(x) 6= h\u2217(D)(x)) \u2264 \u01ebk 2 = \u01ebk+1\nThe lemma follows.\nProof. (of Lemma 6) Assuming Ea happens, we prove the lemma by induction. Base Case: For k = 1, clearly errD(h)\u2212 errD(h\u2217(D)) \u2264 1 \u2264 \u01eb1 = \u01eb2k0 , \u2200h \u2208 V1 = H. Inductive Case: Note that \u2200h, h\u2032 \u2208 Vk+1 \u2286 Vk, by Equation (14) of Lemma 10,\nEU\u0303k [I(h(x) 6= y)(1\u2212 \u03b3k(x))] \u2212 EU\u0303k [I(h \u2032(D)(x) 6= y)(1 \u2212 \u03b3k(x))] \u2264 EU\u0303k [I(h(x) 6= h \u2032(D)(x))(1 \u2212 \u03b3k(x))] \u2264 \u01ebk 8\nFrom Lemma 2, h\u2217(D) \u2208 Vk whenever the event Ea happens. Thus \u2200h \u2208 Vk+1,\nEU\u0303k I(h(x) 6= y)(1 \u2212 \u03b3k(x)) \u2212 EU\u0303kI(h \u2217(D)(x) 6= y)(1\u2212 \u03b3k(x)) \u2264 \u01ebk 8\n(30)\nOn the other hand, if Algorithm 2 succeeds with target excess error \u01ebk8\u03c6k , by item(2.2) of Lemma 4, for any h \u2208 Vk+1,\nP\u0393\u0303k (h(x) 6= y)\u2212 min\nh\u2208Vk P\u0393\u0303k (h(x) 6= y) \u2264 \u01ebk 8\u03c6k\nMoreover, as h\u2217(D) \u2208 Vk from Lemma 2,\nP\u0393\u0303k (h(x) 6= y)\u2212 P\u0393\u0303k(h \u2217(D)(x) 6= y) \u2264 \u01ebk 8\u03c6k\nIn other words,\nEU\u0303k [I(h(x) 6= y)\u03b3k(x)]\u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)\u03b3k(x)] \u2264 \u01ebk 8\nCombining this with Equation (30), we get that for all h \u2208 Vk+1,\nPU\u0303k (h(x) 6= y)\u2212 PU\u0303k(h \u2217(D)(x) 6= y) \u2264 \u01ebk 4\nFinally, combining this with Equation (12) of Lemma 9, we have that:\nPD(h(x) 6= y)\u2212 PD(h\u2217(D)(x) 6= y) \u2264 \u01ebk 2 = \u01ebk+1\nThe lemma follows.\nProof. (of Theorem 1) In the realizable case, We observe that for example zi, \u03b6i = P(P (zi) = \u22121), \u03bei = P(P (zi) = 1), and \u03b3i = P(P (zi) = 0). Suppose h\n\u2217 \u2208 H is the true hypothesis which has 0 error with respect to the data distribution. By the realizability assumption, h\u2217 \u2208 V . Moreover, PU (P (x) 6= h\u2217(x), P (x) 6= 0) = 1 m ( \u2211 i:h\u2217(zi)=+1 \u03b6i + \u2211 i:h\u2217(zi)=\u22121 \u03bei) \u2264 \u03b7 by Algorithm 3. In the non-realizable case, we still have Px\u223cU (h \u2217(x) 6= P (x), P (x) 6= 0) \u2264 \u03b7, hence by triangle inequality, Px\u223cU (P (x) 6= x, P (x) 6= 0)\u2212 Px\u223cU (h\u2217(x) 6= y, P (x) 6= 0) \u2264 \u03b7. Thus\nPx\u223cU(P (x) 6= y, P (x) 6= 0) \u2264 Px\u223cU (h\u2217(x) 6= y) + \u03b7\nProof. (of Theorem 2) Suppose P \u2032 assigns probabilities {[\u03be\u2032i, \u03b6\u2032i, \u03b3\u2032i], i = 1, . . . ,m} to the unlabelled examples zi, and suppose for the sake of contradiction that \u2211m i=1 \u03be \u2032 i + \u03b6 \u2032 i > \u2211m i=1 \u03bei + \u03b6i. Then, {\u03be\u2032i, \u03b6\u2032i, \u03b3\u2032i}\u2019s cannot satisfy the LP in Algorithm 3, and thus there exists some h\u2032 \u2208 V for which constraint (2) is violated. The true hypothesis that generates the data could be any h \u2208 V ; if this true hypothesis is h\u2032, then Px\u223cU (P \u2032(x) 6= h\u2032(x), P \u2032(x) 6= 0) > \u03b4."}, {"heading": "D Proofs from Section 3", "text": "Proof. (of Theorem 4) (1) In the realizable case, suppose that event Er happens. Then from Equation (15) of Lemma 10, while running Algorithm 3, we have that:\n\u03c6k \u2264 \u03a6D(Vk, \u01ebk 128 ) + \u01ebk 256 \u2264 \u03a6D(BD(h\u2217, \u01ebk), \u01ebk 128 ) + \u01ebk 256 \u2264 \u03a6D(BD(h\u2217, \u01ebk), \u01ebk 256 ) = \u03c6(\u01ebk, \u01ebk 256 )\nwhere the second inequality follows from the fact that Vk \u2286 BD(h\u2217(D), \u01ebk), and third inequality follows from Lemma 18 and denseness assuption. Thus, there exists c3 > 0 such that, in round k,\nmk = (d ln 768\u03c6k \u01ebk + ln 48 \u03b4k ) 768\u03c6k \u01ebk \u2264 c3(d ln \u03c6(\u01ebk, \u01ebk/256) \u01ebk + ln( k0 \u2212 k + 1 \u03b4 )) \u03c6(\u01ebk, \u01ebk/256) \u01ebk\nHence the total number of labels queried by Algorithm 1 is at most\n\u2308log 1\u01eb \u2309 \u2211\nk=1\nmk \u2264 c3 \u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln \u03c6(\u01ebk, \u01ebk/256)\n\u01ebk + ln( k0 \u2212 k + 1 \u03b4 )) \u03c6(\u01ebk, \u01ebk/256) \u01ebk\n(2) In the agnostic case, suppose the event Ea happens. First, given Ea, from Equation (15) of Lemma 10 when running Algorithm 3,\n\u03c6k \u2264 \u03a6D(Vk, \u01ebk 128 ) + \u01ebk 256 \u2264 \u03a6D(BD(h\u2217, 2\u03bd\u2217(D) + \u01ebk), \u01ebk 256 ) = \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk 256 ) (31)\nwhere the second inequality follows from the fact that Vk \u2286 BD(h\u2217(D), 2\u03bd\u2217(D)+ \u01ebk) and the third inequality follows from Lemma 18 and denseness assumption.\nSecond, recall that h\u0303k = argminh\u2208Vkerr\u0393\u0303k(h),\nerr\u0393\u0303k(h\u0303k) = minh\u2208Vk err\u0393\u0303k(h)\n\u2264 err\u0393\u0303k(h \u2217(D))\n= EU\u0303k [I(h\u2217(D)(x) 6= y)\u03b3k(x)] \u03c6k\n\u2264 PU\u0303k (h\u2217(D)(x) 6= y) \u03c6k \u2264 \u03bd \u2217(D) + \u01ebk/64\n\u03c6k\nHere the first inequality follows from the suboptimality of h\u2217(D) under distribution \u0393\u0303k, the second inequality follows from \u03b3k(x) \u2264 1, and the third inequality follows from Equation (11). Thus, conditioned on Ea, in iteration k, Algorithm 2 succeeds by Lemma 5, and there exists a constant c4 > 0 such that the number of labels queried is\nmk \u2264 c1 \u01ebk 8\u03c6k + err\u0393\u0303k(h\u0303k)\n( \u01ebk8\u03c6k ) 2\n(d ln 1 \u01ebk 8\u03c6k + ln 2 \u03b4k )\n\u2264 c4(d ln \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n\u01ebk + ln( k0 \u2212 k + 1 \u03b4 )) \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) \u01ebk (1 + \u03bd\u2217(D) \u01ebk )\nHere the last line follows from Equation (31). Hence the total number of examples queried is at most:\n\u2308log 1\u01eb \u2309 \u2211\nk=1\nmk \u2264 c4 \u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n\u01ebk + ln( k0 \u2212 k + 1 \u03b4 )) \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) \u01ebk (1 + \u03bd\u2217(D) \u01ebk )\nProof. (of Theorem 5) Assume Ea happens. First, from Equation (15) of Lemma 10 when running Algorithm 3,\n\u03c6k \u2264 \u03a6D(Vk, \u01ebk 128 )+ \u01ebk 256 \u2264 \u03a6D(BD(h\u2217, C0\u01eb 1 \u03ba k ), \u01ebk 128 )+ \u01ebk 256 \u2264 \u03a6D(BD(h\u2217, C0\u01eb 1 \u03ba k ), \u01ebk 256 ) = \u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 ) (32)\nwhere the second inequality follows from the fact that Vk \u2286 BD(h\u2217(D), C0\u01eb 1 \u03ba\nk ), and the third inequality follows from Lemma 18 and denseness assumption. Second, for all h \u2208 Vk,\n\u03c6k\u03c1\u0393\u0303k(h, h \u2217(D))\n= EU\u0303kI(h(x) 6= h \u2217(D)(x))\u03b3k(x) \u2264 \u03c1U\u0303k(h, h \u2217(D)) \u2264 \u03c1D(h, h\u2217(D)) + \u01ebk/32 \u2264 C0(errD(h)\u2212 errD(h\u2217(D))) 1 \u03ba + \u01ebk/32 \u2264 C0(errU\u0303k(h)\u2212 errU\u0303k(h \u2217(D)) + \u01ebk/64) 1 \u03ba + \u01ebk/32 = C0(EU\u0303k [I(h(x) 6= y)\u03b3k(x)]\u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)\u03b3k(x)]\n+EU\u0303k [I(h(x) 6= y)(1\u2212 \u03b3k(x))] \u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)(1\u2212 \u03b3k(x))] + \u01ebk/16)\n1 \u03ba + \u01ebk/32\nHere the first inequality follows from \u03b3k(x) \u2264 1, the second inequality follows from Equation (13) of Lemma 9, the third inequality follows from Definition 1 and the fourth inequality follows from Equation (12) of Lemma 9. The above can be upper bounded by:\n\u2264 C0(EU\u0303k [I(h(x) 6= y)\u03b3k(x)]\u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)\u03b3k(x)] + \u01ebk/16)\n1 \u03ba + \u01ebk/32\n\u2264 2C0(EU\u0303k [I(h(x) 6= y)\u03b3k(x)] \u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)\u03b3k(x)])\n1 \u03ba + 2C0(\u01ebk/16) 1 \u03ba + \u01ebk/32\n\u2264 max(8C0, 4)max((EU\u0303k [I(h(x) 6= y)\u03b3k(x)] \u2212 EU\u0303k [I(h \u2217(D)(x) 6= y)\u03b3k(x)]), \u01ebk 16 ) 1 \u03ba = max(8C0, 4)(\u03c6k) 1 \u03ba max(P\u0393\u0303k(h(x) 6= y)\u2212 P\u0393\u0303k(h \u2217(D)(x) 6= y), \u01ebk\n8\u03c6k )\n1 \u03ba\nHere the first inequality follows from Equation (14) of Lemma 10 and triangle inequality EU\u0303k [I(h(x) 6= y)\u03b3k(x)]\u2212EU\u0303k [I(h \u2217(D)(x) 6= y)\u03b3k(x)] \u2264 EU\u0303k [I(h(x) 6= h \u2217(D)(x))\u03b3k(x)] \u2264 \u01ebk/32, and the last two inequalities follow from simple algebra. Dividing both sides by \u03c6k, we get:\n\u03c1\u0393\u0303k(h, h \u2217(D)) \u2264 C1(\u03c6k)\n1 \u03ba\u22121 max(err\u0393\u0303k(h)\u2212 err\u0393\u0303k(h \u2217(D)), \u01ebk 8\u03c6k ) 1 \u03ba\nwhere C1 = max(8C0, 4). Thus in iteration k, Condition (19) in Lemma 11 holds with C := C1(\u03c6k) 1 \u03ba\u22121 and h\u0303 := h\u2217(D). Thus, from Lemma 11, Algorithm 2 succeeds, and there exists a constant c5 > 0, such that the number of labels queried is\nmk \u2264 c2 max((d ln(C1(\u03c6k) 1 \u03ba\u22121( \u01ebk 8\u03c6k ) 1 \u03ba\u22122) + ln 2 \u03b4k )(C1(\u03c6k) 1 \u03ba\u22121( \u01ebk 8\u03c6k ) 1 \u03ba\u22122),\n(d ln( \u01ebk 8\u03c6k )\u22121 + ln 2 \u03b4k )( \u01ebk 8\u03c6k )\u22121)\n\u2264 c5(d ln(\u03c6k\u01eb 1 \u03ba\u22122 k ) + ln( k0 \u2212 k + 1\n\u03b4 ))\u03c6k\u01eb\n1 \u03ba\u22122 k\n\u2264 c5(d ln(\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k ) + ln( k0 \u2212 k + 1 \u03b4 ))\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k\nWhere the last line follows from Equation (31). Hence the total number of examples queried is at most\n\u2308log 1\u01eb \u2309 \u2211\nk=1\nmk \u2264 c5 \u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln(\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k ) + ln( k0 \u2212 k + 1 \u03b4 ))\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k\nThe following lemma is an immediate corollary of Theorem 21, item (a) of Lemma 2 and Lemma 3 of [BL13]:\nLemma 14. Suppose D is isotropic and log-concave on Rd, and H is the set of homogeneous linear classifiers on Rd, then there exist absolute constants c6, c7 > 0 such that \u03c6(r, \u03b7) \u2264 c6r ln c7r\u03b7 .\nProof. (of Lemma 14) Denote wh as the unit vector w such that h(x) = sign(w \u00b7 x), and \u03b8(w,w\u2032) to be the angle between vectors w and w\u2032. If h \u2208 BD(h\u2217, r), then by Lemma 3 of [BL13], there exists some constant c11 > 0 such that \u03b8(wh, wh\u2217) \u2264 rc11 . Also, by Lemma 21 of [BL13], there exists some constants c12, c13 > 0, such that, if \u03b8(w,w\u2032) = \u03b1 then\nPD(sign(w \u00b7 x) 6= sign(w\u2032 \u00b7 x), |w \u00b7 x| \u2265 b) \u2264 c12\u03b1 exp(\u2212c13 b\n\u03b1 )\nWe define a special solution (\u03be, \u03b6, \u03b3) as follows:\n\u03be(x) := I(wh\u2217 \u00b7 x \u2265 r\nc11c13 ln\nc12r c11\u03b7 )\n\u03b6(x) := I(wh\u2217 \u00b7 x \u2264 \u2212 r\nc11c13 ln\nc12r c11\u03b7 )\n\u03b3(x) := I(|wh\u2217 \u00b7 x| \u2264 r\nc11c13 ln\nc12r c11\u03b7 )\nThen it can be checked that for all h \u2208 BD(h\u2217, r),\nE[I(h(x) = +1)\u03b6(x) + I(h(x) = \u22121)\u03be(x)] = PD(sign(wh\u2217 \u00b7 x) 6= sign(wh \u00b7 x), |wh\u2217 \u00b7 x| \u2265 r\nc11c13 ln\nc12r c11\u03b7 ) \u2264 \u03b7\nAnd by item (a) of Lemma 2 of [BL13], we have\nE\u03b3(x) = PD(|wh\u2217 \u00b7 x| \u2264 r\nc11c13 ln\nc12r c11\u03b7 ) \u2264 r c11c13 ln c12r c11\u03b7\nHence,\n\u03c6(r, \u03b7) \u2264 r c11c13 ln c12r c11\u03b7\nProof. (of Corollary 1) This is an immediate consequence of Lemma 14 and Theorems 4 and 5 and algebra."}, {"heading": "E A Suboptimal Alternative to Algorithm 2", "text": "Algorithm 4 An Nonadaptive Algorithm for Label Query Given Target Excess Error\n1: Inputs: Hypothesis set V of VC dimension d, Example distribution \u2206, Labeling oracle O, target excess error \u01eb\u0303, target confidence \u03b4\u0303. 2: Draw n = 6144\u01eb\u03032 (d ln 6144 \u01eb\u03032 +ln 24 \u03b4\u0303 ) i.i.d examples from \u2206; query their labels from O to get a labelled dataset\nS. 3: Train an ERM classifier h\u0302 \u2208 V over S. 4: Define the set V as follows:\nV1 = { h \u2208 V : errS(h) \u2264 errS(h\u0302) + 3\u01eb\u0303\n4\n}\n5: return V1.\nIt is immediate that we have the following lemma.\nLemma 15. Suppose we run Algorithm 4 with inputs hypothesis set V , example distribution \u2206, labelling oracle O, target excess error \u01eb\u0303 and target confidence \u03b4\u0303. Then there exists an event E\u0303, P(E\u0303) \u2265 1\u2212 \u03b4\u0303, such that on E\u0303, the set V1 has the following property. (1) If for h \u2208 H, err\u2206\u0303(h) \u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303/2, then h \u2208 V1. (2) On the other hand, if h \u2208 V1, then err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303.\nWhen E\u0303 happens, we say that Algorithm 4 succeeds.\nProof. By Equation (9) of Lemma 8 and because n = 6144\u01eb\u03032 (d ln 6144 \u01eb\u03032 + ln 24 \u03b4\u0303 ), we have for all h, h\u2032 \u2208 H,\n(err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2032))\u2212 (errS(h)\u2212 errS(h\u2032)) \u2264 \u01eb\n4\nFor the proof of (1), for any h \u2208 V , err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303/2, then\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u0302) \u2264 \u01eb\u0303/2 Thus\nerrS(h)\u2212 errS(h\u0302) \u2264 3\u01eb\u0303\n4 proving h \u2208 V1. For the proof of (2), for any h \u2208 V1,\nerrS(h)\u2212 errS(h\u2032) \u2264 3\u01eb\u0303\n4 Thus\nerrS(h)\u2212 errS(h\u2217(\u2206\u0303)) \u2264 3\u01eb\u0303\n4\nCombining with the fact that (err\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)))\u2212 (errS(h)\u2212 errS(h\u2217(\u2206\u0303))) \u2264 \u01eb4 we have\nerr\u2206\u0303(h)\u2212 err\u2206\u0303(h\u2217(\u2206\u0303)) \u2264 \u01eb\u0303\nCorollary 2. Suppose we replace the calls to Algorithm 2 with Algorithm 4 in Algorithm 1, then run it with inputs example oracle U , labelling oracle O, hypothesis class V , confidence-rated predictor P of Algorithm 3, target excess error \u01eb and target confidence \u03b4. Then the modified algorithm has a label complexity of\nO\u0303(\n\u2308log 1/\u01eb\u2309 \u2211\nk=1\n(d( \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n\u01ebk )2)\nin the agnostic case and\nO\u0303(\n\u2308log 1/\u01eb\u2309 \u2211\nk=1\nd( \u03c6(C0\u01eb\n1 \u03ba\nk , \u01ebk 256 )\n\u01eb 1 \u03ba\nk\n)2\u01eb 2 \u03ba\u22122\nk )\nunder (C0, \u03ba)-Tsybakov Noise Condition.\nUnder denseness assumption, by Lemma 17, we have \u03c6(r, \u03b7) \u2265 r\u2212 2\u03b7, the label complexity bounds given by Corollary 2 is always no better than the ones given by Theorem 4 and 5.\nProof. (Sketch) Define event\nEa = {For all k = 1, 2, . . . , k0: Equations (11), (12), (13), (14), (15) hold for U\u0303k with confidence \u03b4k/2, and Algorithm 4 succeeds with inputs hypothesis set V = Vk, example distribution \u2206 = \u0393k, labelling oracle O, target excess error \u01eb\u0303 = \u01ebk 8\u03c6k and target confidence \u03b4\u0303 = \u03b4k 2 }.\nClealy, P(Ea) \u2265 1\u2212 \u03b4. On the event Ea, there exists an absolute constant c13 > 0, such that the number of examples queried in interation k is\nmk \u2264 c13(d ln 8\u03c6k \u01ebk + ln 2 \u03b4 )( 8\u03c6k \u01ebk )2\nCombining it with Equation (15) of Lemma 10\n\u03c6k \u2264 \u03a6D(Vk, \u01ebk 128 ) + \u01ebk 256\nwe have\nmk \u2264 O((d ln \u03a6D(Vk,\n\u01ebk 128 ) + \u01ebk 256 \u01ebk + ln 2 \u03b4k )( \u03a6D(Vk, \u01ebk 128 ) + \u01ebk 256 \u01ebk )2)\nThe rest of the proof follows from Lemma 18 and denseness assumption, along with algebra."}, {"heading": "F Proofs of Concentration Lemmas", "text": "Proof. (of Lemma 9) We begin by observing that:\nerrU\u0303k(h) = 1\nnk\nnk \u2211\ni=1\n[PD(Y = +1|X = xi)I(h(xi) = \u22121) + PD(Y = \u22121|X = xi)I(h(xi) = +1)]\nMoreover, max(S({I(h(x) = 1, h \u2208 H)}, n),S({I(h(x) = \u22121, h \u2208 H)}, n)) \u2264 ( end )d. Combining this fact with Lemma 16, the following equations hold simultaneously with probability 1\u2212 \u03b4k/6:\n\u2223 \u2223 \u2223 1\nnk\nnk \u2211\ni=1\nPD(Y = +1|X = xi)I(h(xi) = \u22121)\u2212 PD(h(x) = \u22121, y = +1) \u2223 \u2223 \u2223 \u2264\n\u221a\n8(d ln enkd + ln 24 \u03b4k )\nnk \u2264 \u01ebk 128\n\u2223 \u2223 \u2223 1\nnk\nnk \u2211\ni=1\nPD(Y = \u22121|X = xi)I(h(xi) = +1)\u2212 PD(h(x) = +1, y = \u22121) \u2223 \u2223 \u2223 \u2264\n\u221a\n8(d ln enkd + ln 24 \u03b4k )\nnk \u2264 \u01ebk 128\nThus Equation (11) holds with probability 1 \u2212 \u03b4k/6. Moreover, we observe that Equation (11) implies Equation (12). To show Equation (13), we observe that by Lemma 8, with probability 1\u2212 \u03b4k/12,\n|\u03c1D(h, h\u2032)\u2212 \u03c1U\u0303k(h, h \u2032)| = |\u03c1D(h, h\u2032)\u2212 \u03c1Sk(h, h\u2032)| \u2264 2\n\u221a \u03c3(nk, \u03b4k/12) \u2264 \u01ebk 64\nThus, Equation (13) holds with probability \u2265 1 \u2212 \u03b4k/12. By union bound, with probability 1 \u2212 \u03b4k/4, Equations (11), (12), and (13) hold simultaneously.\nProof. (of Lemma 10) (1) Given a confidence-rated predictor with inputs hypothesis set Vk, unlabelled data Uk, and error bound \u01ebk/64, the outputs {(\u03bek,i, \u03b6k,i, \u03b3k,i)}nki=1 must satisfy that for all h, h\u2032 \u2208 Vk,\n1\nnk\nnk \u2211\ni=1\n[I(h(xk,i) = \u22121)\u03bek,i + I(h(xk,i) = +1)\u03b6k,i] \u2264 \u01ebk 64\n1\nnk\nnk \u2211\ni=1\n[I(h\u2032(xk,i) = \u22121)\u03bek,i + I(h\u2032(xk,i) = +1)\u03b6k,i] \u2264 \u01ebk 64\nSince I(h(x) 6= h\u2032(x)) \u2264 min(I(h(x) = \u22121) + I(h\u2032(x) = \u22121), I(h(x) = +1) + I(h\u2032(x) = +1)), adding up the two inequalities above, we get\n1\nnk\nnk \u2211\ni=1\n[I(h(xk,i) 6= h\u2032(xk,i))(\u03bek,i + \u03b6k,i)] \u2264 \u01ebk 32\nThat is,\n1\nnk\nnk \u2211\ni=1\n[I(h(xk,i) 6= h\u2032(xk,i))(1 \u2212 \u03b3k,i)] \u2264 \u01ebk 32\n(2) By definition of \u03a6D(V, \u03b7), there exist nonnegative functions \u03be, \u03b6, \u03b3 such that \u03be(x) + \u03b6(x) + \u03b3(x) \u2261 1, ED[\u03b3(x)] = \u03a6D(Vk, \u01ebk/128) and for all h \u2208 Vk,\nED[\u03be(x)I(h(x) = \u22121) + \u03b6(x)I(h(x) = +1)] \u2264 \u01ebk 128\nConsider the linear progam in Algorithm 3 with inputs hypothesis set Vk, unlabelled data Uk, and error bound \u01ebk/64. We consider the following special (but possibly non-optimal) solution for this LP: \u03bek,i = \u03be(zk,i), \u03b6k,i = \u03b6(zk,i), \u03b3k,i = \u03b3(zk,i). We will now show that this solution is feasible and has coverage\n\u03a6D(Vk, \u01ebk/128) plus O(\u01ebk) with high probability. Observe that max(S({I(h(x) = 1, h \u2208 H)}, n),S({I(h(x) = \u22121, h \u2208 H)}, n)) \u2264 ( end )d. Therefore, from Lemma 16 and the union bound, with probability 1\u2212 \u03b4k/4, the following hold simultaneously for all h \u2208 H:\n\u2223 \u2223 \u2223 1\nnk\nnk \u2211\ni=1\n\u03b3(zk,i)\u2212 ED\u03b3(x) \u2223 \u2223 \u2223 \u2264\n\u221a\nln 2\u03b4k 2nk \u2264 \u01ebk 256\n(33)\n\u2223 \u2223 \u2223 1\nnk\nnk \u2211\ni=1\n\u03be(zk,i)I(h(zk,i) = \u22121)\u2212 ED[\u03be(x)I(h(x) = \u22121)] \u2223 \u2223 \u2223 \u2264\n\u221a\n8(d ln enkd + ln 24 \u03b4k )\nnk \u2264 \u01ebk 256 (34)\n\u2223 \u2223 \u2223 1\nnk\nnk \u2211\ni=1\n\u03b6(zk,i)I(h(zk,i) = +1)\u2212 ED[\u03b6(x)I(h(x) = +1)] \u2223 \u2223 \u2223 \u2264\n\u221a\n8(d ln enkd + ln 24 \u03b4k )\nnk \u2264 \u01ebk 256 (35)\nAdding up Equations (34) and (35),\n\u2223 \u2223 \u2223 1\nnk\nnk \u2211\ni=1\n[\u03b6(xi)I(h(xi) = +1) + \u03be(xi)I(h(xi) = \u22121)]\u2212 ED[\u03be(x)I(h(x) = \u22121) + \u03b6(x)I(h(x) = +1))] \u2223 \u2223 \u2223 \u2264 \u01ebk 128\nThus {(\u03be(zk,i), \u03b6(zk,i)}nki=1 is a feasible solution of the linear program of Algorithm 3. Also, by Equation (33), 1 nk \u2211nk i=1 \u03b3(zk,i) \u2264 \u03a6D(Vk, \u01ebk128 ) + \u01ebk 64 . Thus, the outputs {(\u03bek,i, \u03b6k,i, \u03b3k,i)} nk i=1 of the linear program in Algorithm 3 satisfy\n\u03c6k = 1\nnk\nnk \u2211\ni=1\n\u03b3k,i \u2264 1\nnk\nnk \u2211\ni=1\n\u03b3(zk,i) \u2264 \u03a6D(Vk, \u01ebk 128 ) + \u01ebk 256\ndue to their optimality.\nLemma 16. Pick any n \u2265 1, \u03b4 \u2208 (0, 1), a family F of functions f : Z \u2192 {0, 1}, a fixed weighting function w : Z \u2192 [0, 1]. Let Sn be a set of n iid copies of Z. The following holds with probability at least 1\u2212 \u03b4:\n\u2223 \u2223 \u2223 1\nn\nn \u2211\ni=1\nw(zi)f(zi)\u2212 E[w(z)f(z)] \u2223 \u2223 \u2223 \u2264\n\u221a\n8(lnS(F , n) + ln 2\u03b4 ) n\nwhere S(F , n) = maxz1,...,zn\u2208Z |{(f(z1), . . . , f(zn)) : f \u2208 F}| is the growth function of F .\nProof. The proof is fairly standard, and follows immediately from the proof of additive VC bounds. With\nprobability 1\u2212 \u03b4,\nsup f\u2208F\n\u2223 \u2223 \u2223 1\nn\nn \u2211\ni=1\nw(zi)f(zi)\u2212 Ew(z)f(z) \u2223 \u2223 \u2223\n\u2264 ES\u223cDn sup f\u2208F\n\u2223 \u2223 \u2223 1\nn\nn \u2211\ni=1\nw(zi)f(zi)\u2212 Ew(z)f(z) \u2223 \u2223 \u2223+\n\u221a\n2 ln 1\u03b4 n\n\u2264 ES\u223cDn,S\u2032\u223cDn sup f\u2208F\n\u2223 \u2223 \u2223 1\nn\nn \u2211\ni=1\n(w(zi)f(zi)\u2212 w(z\u2032i)f(z\u2032i)) \u2223 \u2223 \u2223+\n\u221a\n2 ln 1\u03b4 n\n\u2264 ES\u223cDn,S\u2032\u223cDn,\u03c3\u223cU({\u22121,+1}n) sup f\u2208F\n\u2223 \u2223 \u2223 1\nn\nn \u2211\ni=1\n\u03c3i(w(zi)f(zi)\u2212 w(z\u2032i)f(z\u2032i)) \u2223 \u2223 \u2223 +\n\u221a\n2 ln 1\u03b4 n\n\u2264 2ES\u223cDn,\u03c3\u223cU({\u22121,+1}n) sup f\u2208F\n\u2223 \u2223 \u2223 1\nn\nn \u2211\ni=1\n\u03c3iw(zi)f(zi) \u2223 \u2223 \u2223+\n\u221a\n2 ln 1\u03b4 n\n\u2264 2 \u221a 2 ln(2S(F , n)) n +\n\u221a\n2 ln 1\u03b4 n \u2264\n\u221a\n8(lnS(F , n) + ln 2\u03b4 ) n\nWhere the first inequality is by McDiarmid\u2019s Lemma; the second inequality follows from Jensen\u2019s Inequality; the third inequality follows from symmetry; the fourth inequality follows from |A+B| \u2264 |A|+ |B|; the fifth inequality follows from Massart\u2019s Finite Lemma.\nLemma 17. Let 0 < 2\u03b7 \u2264 r \u2264 1. Given a hypothesis set V and data distribution D over X \u00d7 Y, if there exist h1, h2 \u2208 V such that \u03c1D(h1, h2) \u2265 r, then \u03a6D(V, \u03b7) \u2265 r \u2212 2\u03b7.\nProof. Let (\u03be, \u03b6, \u03b3) be a triple of functions from X to R3 satisfying the following conditions: \u03be, \u03b6, \u03b3 \u2265 0, \u03be + \u03b6 + \u03b3 \u2261 1, and for all h \u2208 V ,\nED[\u03be(x)I(h(x) = +1) + \u03b6(x)I(h(x) = \u22121)] \u2264 \u03b7\nThen, in particular, we have:\nED[\u03be(x)I(h1(x) = +1) + \u03b6(x)I(h1(x) = \u22121)] \u2264 \u03b7\nED[\u03be(x)I(h1(x) = +1) + \u03b6(x)I(h2(x) = \u22121)] \u2264 \u03b7 Thus, by I(h1(x) 6= h2(x)) \u2264 min(I(h1(x) = \u22121) + I(h1(x) = \u22121), I(h2(x) = +1) + I(h2(x) = +1)), adding the two inequalities up, ED[(\u03be(x) + \u03b6(x))I(h1(x) 6= h2(x))] \u2264 2\u03b7 Since \u03c1D(h1, h2) = EDI(h1(x) 6= h2(x)) \u2265 r We have ED[\u03b3(x)I(h1(x) 6= h2(x))] = ED[(1\u2212 \u03be(x) \u2212 \u03b6(x))I(h1(x) 6= h2(x))] \u2265 r \u2212 2\u03b7 Thus, ED[\u03b3(x)] \u2265 ED[\u03b3(x)I(h1(x) 6= h2(x))] \u2265 r \u2212 2\u03b7 Hence \u03a6D(V, \u03b7) \u2265 r \u2212 2\u03b7.\nLemma 18. Given hypothesis set V and data distribution D over X \u00d7 Y, 0 < \u03bb < \u03b7 < 1, if there exist h1, h2 \u2208 V such that \u03c1D(h1, h2) \u2265 2\u03b7 \u2212 \u03bb, then \u03a6D(V, \u03b7) + \u03bb \u2264 \u03a6D(V, \u03b7 \u2212 \u03bb).\nProof. Suppose (\u03be1, \u03b61, \u03b31) are nonnegative functions satisfying \u03be1 + \u03b61 + \u03b31 \u2261 1, and for all h \u2208 V , ED[\u03b61(x)I(h(x) = +1)+\u03be1(x)I(h(x) = \u22121)] \u2264 \u03b7\u2212\u03bb, and ED\u03b31(x) = \u03a6D(V, \u03b7\u2212\u03bb). Notice by Lemma 17,\u03a6D(V, \u03b7\u2212 \u03bb) \u2265 2\u03b7 \u2212 \u03bb\u2212 2(\u03b7 \u2212 \u03bb) = \u03bb.\nThen we pick nonnegative functions (\u03be2, \u03b62, \u03b32) as follows. Let \u03be2 = \u03be1, \u03b32 = (1 \u2212 \u03bb\u03a6D(V,\u03b7\u2212\u03bb) )\u03b31, and \u03b62 = 1\u2212 \u03be2 \u2212 \u03b32. It is immediate that (\u03be2, \u03b62, \u03b32) is a valid confidence rated predictor and \u03b62 \u2265 \u03b61, \u03b32 \u2264 \u03b31, ED\u03b32(x) = \u03a6D(V, \u03b7 \u2212 \u03bb) \u2212 \u03bb. It can be readily checked that the confidence rated predictor (\u03be2, \u03b62, \u03b32) has error guarantee \u03b7, specifically:\nED[\u03b62(x)I(h(x) = +1) + \u03be2(x)I(h(x) = \u22121)] \u2264 ED[(\u03b62(x)\u2212 \u03b61(x))I(h(x) = +1) + (\u03be2(x)\u2212 \u03be1(x))I(h(x) = \u22121)] + \u03b7 \u2212 \u03bb \u2264 ED[(\u03b62(x)\u2212 \u03b61(x)) + (\u03be2(x)\u2212 \u03be1(x))] + \u03b7 \u2212 \u03bb \u2264 \u03bb+ \u03b7 \u2212 \u03bb = \u03b7\nThus, \u03a6D(V, \u03b7), which is the minimum abstention probability of a confidence-rated predictor with error guarantee \u03b7 with respect to hypothesis set V and data distribution D, is at most \u03a6D(V, \u03b7 \u2212 \u03bb)\u2212 \u03bb."}, {"heading": "G Detailed Derivation of Label Complexity Bounds", "text": "G.1 Agnostic\nProposition 1. In agnostic case, the label complexity of Algorithm 1 is at most\nO\u0303( sup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n2\u03bd\u2217(D) + \u01ebk (d\n\u03bd\u2217(D)2\n\u01eb2 ln\n1 \u01eb + d ln2 1 \u01eb )),\nwhere the O\u0303 notation hides factors logarithmic in 1/\u03b4.\nProof. Applying Theorem 5, the total number of labels queried is at most:\nc4\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n\u01ebk + ln( \u2308log(1/\u01eb)\u2309 \u2212 k + 1 \u03b4 )) \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) \u01ebk (1 + \u03bd\u2217(D) \u01ebk )\nUsing the fact that \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) \u2264 1, this is\nc4\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n\u01ebk + ln( \u2308log(1/\u01eb)\u2309 \u2212 k + 1 \u03b4 )) \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) \u01ebk (1 + \u03bd\u2217(D) \u01ebk )\n= O\u0303\n\n\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n\u01ebk + ln log(1/\u01eb))\n\u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n2\u03bd + \u01ebk (1 +\n\u03bd\u2217(D)2\n\u01eb2k )\n\n\n\u2264 O\u0303\n\n sup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n2\u03bd\u2217(D) + \u01ebk\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(1 + \u03bd\u2217(D)2\n\u01eb2k )(d ln\n1 \u01eb + ln ln 1 \u01eb )\n\n\n\u2264 O\u0303 (\nsup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256)\n2\u03bd\u2217(D) + \u01ebk (d\n\u03bd\u2217(D)2\n\u01eb2 ln\n1 \u01eb + d ln2 1 \u01eb )\n)\n,\nwhere the last line follows as \u01ebk is geometrically decreasing.\nG.2 Tsybakov Noise Condition with \u03ba > 1\nProposition 2. Suppose the hypothesis class H and the data distribution D satisfies (C0, \u03ba)-Tsybakov Noise Condition with \u03ba > 1. Then the label complexity of Algorithm 1 is at most\nO\u0303( sup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\n\u01eb 1 \u03ba\nk\n\u01eb 2 \u03ba\u22122d ln 1\n\u01eb ),\nwhere the O\u0303 notation hides factors logarithmic in 1/\u03b4.\nProof. Applying Theorem 5, the total number of labels queried is at most:\nc5\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln(\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k ) + ln( k0 \u2212 k + 1 \u03b4 ))\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k\nUsing the fact that \u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 ) \u2264 1, we get\nc5\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln(\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k ) + ln( k0 \u2212 k + 1 \u03b4 ))\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k\n\u2264 O\u0303\n\n sup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\n\u01eb 1 \u03ba\nk\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n\u01eb 2 \u03ba\u22122 k d ln 1\n\u01eb\n\n\n\u2264 O\u0303 (\nsup k\u2264\u2308log(1/\u01eb)\u2309\n\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\n\u01eb 1 \u03ba\nk\n\u01eb 2 \u03ba\u22122d ln 1\n\u01eb\n)\nG.3 Fully Agnostic, Linear Classification of Log-Concave Distribution\nWe show in this subsection that in agnostic case, if H is the class of homogeneous linear classifiers in Rd, DX is isotropic log-concave in R d, then, our label complexity bound is at most\nO(ln \u01eb+ \u03bd\u2217(D)\n\u01eb (ln\n1 \u01eb +\n\u03bd\u2217(D)2\n\u01eb2 )(d ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb + ln\n1 \u03b4 ) + ln 1 \u01eb ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb ln ln\n1 \u01eb )\nRecall by Lemma 14, we have \u03c6(2\u03bd\u2217(D) + \u01ebk, \u01ebk/256) \u2264 C(\u03bd\u2217(D) + \u01ebk) ln \u03bd \u2217(D)+\u01ebk\n\u01ebk for some constant C > 0.\nApplying Theorem 4, the label complexity is\nO(\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln( 2\u03bd\u2217(D) + \u01ebk\n\u01ebk ln 2\u03bd\u2217(D) + \u01ebk \u01ebk ) + ln( log(1/\u01eb)\u2212 k + 1 \u03b4 )) ln 2\u03bd\u2217(D) + \u01ebk \u01ebk (1 + \u03bd\u2217(D)2 \u01eb2k ))\nThis can be simplified to (treating 1 and \u03bd \u2217(D)2\n\u01eb2k separately)\nO(\n\u2308log 1\u01eb \u2309 \u2211\nk=1\nln \u03bd\u2217(D) + \u01ebk\n\u01ebk (d ln \u03bd\u2217(D) + \u01ebk \u01ebk + ln k0 \u2212 k + 1 \u03b4 )\n+\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n\u03bd\u2217(D)2\n\u01eb2k ln \u03bd\u2217(D) + \u01ebk \u01ebk (d ln \u03bd\u2217(D) + \u01ebk \u01ebk + ln k0 \u2212 k + 1 \u03b4 ))\n\u2264 O(ln 1 \u01eb ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb (d ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb + ln ln\n1 \u01eb + ln 1 \u03b4 ) +\n\u03bd\u2217(D)2\n\u01eb2 ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb (d ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb + ln\n1 \u03b4 ))\n\u2264 O(ln \u01eb+ \u03bd \u2217(D)\n\u01eb (ln\n1 \u01eb +\n\u03bd\u2217(D)2\n\u01eb2 )(d ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb + ln\n1 \u03b4 ) + ln 1 \u01eb ln\n\u01eb+ \u03bd\u2217(D)\n\u01eb ln ln\n1 \u01eb )\nG.4 Tsybakov Noise Conditon with \u03ba > 1, Log-Concave Distribution\nWe show in this subsection that under (C0, \u03ba)-Tsybakov Noise Condition, if H is the class of homogeneous linear classifiers in Rd, and DX is isotropic log-concave in R d, our label complexity bound is at most\nO(\u01eb 2 \u03ba\u22122 ln 1\n\u01eb (d ln\n1 \u01eb + ln 1 \u03b4 ))\nRecall by Lemma 14, we have\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 ) \u2264 C\u01eb 1 \u03ba k ln 1 \u01ebk\nfor some constant C > 0. Applying Theorem 5, the label complexity is:\nO(\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln(\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k ) + ln( k0 \u2212 k + 1 \u03b4 ))\u03c6(C0\u01eb 1 \u03ba k , \u01ebk 256 )\u01eb 1 \u03ba\u22122 k )\nThis can be simplified to :\nO(\n\u2308log 1\u01eb \u2309 \u2211\nk=1\n(d ln(\u01eb 2 \u03ba\u22122 k ln 1\n\u01ebk ) + ln( k0 \u2212 k + 1 \u03b4 ))\u01eb 2 \u03ba\u22122 k ln 1 \u01ebk )\n\u2264 O(( \u2308log 1\u01eb \u2309 \u2211\nk=1\n\u01eb 2 \u03ba\u22122 k ) ln 1\n\u01eb (d ln\n1 \u01eb + ln 1 \u03b4 ))\n\u2264 O(\u01eb 2\u03ba\u22122 ln 1 \u01eb (d ln 1 \u01eb + ln 1 \u03b4 ))"}], "references": [{"title": "The power of localization for efficiently learning linear separators with noise", "author": ["P. Awasthi", "M-F. Balcan", "P.M. Long"], "venue": "In STOC,", "citeRegEx": "Awasthi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2014}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A.Z. Broder", "T. Zhang"], "venue": "In COLT,", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "Importance weighted active learning", "author": ["A. Beygelzimer", "S. Dasgupta", "J. Langford"], "venue": "In ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Agnostic active learning without constraints", "author": ["A. Beygelzimer", "D. Hsu", "J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2010}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["M.-F. Balcan", "P.M. Long"], "venue": "In COLT,", "citeRegEx": "Balcan and Long.,? \\Q2013\\E", "shortCiteRegEx": "Balcan and Long.", "year": 2013}, {"title": "Improving generalization with active learning", "author": ["D.A. Cohn", "L.E. Atlas", "R.E. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "Dasgupta.,? \\Q2004\\E", "shortCiteRegEx": "Dasgupta.", "year": 2004}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "Dasgupta.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta.", "year": 2005}, {"title": "Two faces of active learning", "author": ["S. Dasgupta"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Dasgupta.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta.", "year": 2011}, {"title": "Hierarchical sampling for active learning", "author": ["S. Dasgupta", "D. Hsu"], "venue": "In ICML,", "citeRegEx": "Dasgupta and Hsu.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 2008}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS,", "citeRegEx": "Dasgupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2007}, {"title": "On the foundations of noise-free selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": null, "citeRegEx": "El.Yaniv and Wiener.,? \\Q2010\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2010}, {"title": "Agnostic selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "In NIPS,", "citeRegEx": "El.Yaniv and Wiener.,? \\Q2011\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2011}, {"title": "Active learning via perfect selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": null, "citeRegEx": "El.Yaniv and Wiener.,? \\Q2012\\E", "shortCiteRegEx": "El.Yaniv and Wiener.", "year": 2012}, {"title": "Generalization bounds for averaged classifiers", "author": ["Y. Freund", "Y. Mansour", "R.E. Schapire"], "venue": "The Ann. of Stat.,", "citeRegEx": "Freund et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2004}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "In ICML,", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Adaptive rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "In COLT,", "citeRegEx": "Hanneke.,? \\Q2009\\E", "shortCiteRegEx": "Hanneke.", "year": 2009}, {"title": "A statistical theory of active learning", "author": ["S. Hanneke"], "venue": null, "citeRegEx": "Hanneke.,? \\Q2013\\E", "shortCiteRegEx": "Hanneke.", "year": 2013}, {"title": "Algorithms for Active Learning", "author": ["D. Hsu"], "venue": "PhD thesis, UC San Diego,", "citeRegEx": "Hsu.,? \\Q2010\\E", "shortCiteRegEx": "Hsu.", "year": 2010}, {"title": "Surrogate losses in passive and active learning", "author": ["S. Hanneke", "L. Yang"], "venue": "CoRR, abs/1207.3772,", "citeRegEx": "Hanneke and Yang.,? \\Q2012\\E", "shortCiteRegEx": "Hanneke and Yang.", "year": 2012}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "In ALT,", "citeRegEx": "K\u00e4\u00e4ri\u00e4inen.,? \\Q2006\\E", "shortCiteRegEx": "K\u00e4\u00e4ri\u00e4inen.", "year": 2006}, {"title": "Rademacher complexities and bounding the excess risk in active learning", "author": ["V. Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii.,? \\Q2010\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2010}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["L. Li", "M.L. Littman", "T.J. Walsh"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Noisy bayesian active learning", "author": ["M. Naghshvar", "T. Javidi", "K. Chaudhuri"], "venue": "In Allerton,", "citeRegEx": "Naghshvar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Naghshvar et al\\.", "year": 2013}, {"title": "The geometry of generalized binary search", "author": ["R.D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nowak.,? \\Q2011\\E", "shortCiteRegEx": "Nowak.", "year": 2011}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Technical report, University of Wisconsin-Madison,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "A tutorial on conformal prediction", "author": ["G. Shafer", "V. Vovk"], "venue": null, "citeRegEx": "Shafer and Vovk.,? \\Q2008\\E", "shortCiteRegEx": "Shafer and Vovk.", "year": 2008}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A.B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "Tsybakov.,? \\Q2004\\E", "shortCiteRegEx": "Tsybakov.", "year": 2004}, {"title": "Plal: Cluster-based active learning", "author": ["R. Urner", "S. Wulff", "S. Ben-David"], "venue": "In COLT,", "citeRegEx": "Urner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urner et al\\.", "year": 2013}], "referenceMentions": [], "year": 2014, "abstractText": "We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithms for this problem are disagreement-based active learning, which has a high label requirement, and margin-based active learning, which only applies to fairly restricted settings. A major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions \u2013 a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.", "creator": "LaTeX with hyperref package"}}}