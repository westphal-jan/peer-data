{"id": "1407.3068", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2014", "title": "Deep Networks with Internal Selective Attention through Feedback Connections", "abstract": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do change the outcome in the future.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 11 Jul 2014 08:56:54 GMT  (954kb)", "https://arxiv.org/abs/1407.3068v1", "13 pages, 3 figures"], ["v2", "Mon, 28 Jul 2014 08:22:50 GMT  (955kb)", "http://arxiv.org/abs/1407.3068v2", "13 pages, 3 figures"]], "COMMENTS": "13 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["marijn f stollenga", "jonathan masci", "faustino j gomez", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1407.3068"}, "pdf": {"name": "1407.3068.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["marijn@idsia.ch*,", "jonathan@idsia.ch*,", "tino@idsia.ch", "juergen@idsia.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 7.\n30 68\nv2 [\ncs .C"}, {"heading": "1 Introduction", "text": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]). These architectures consist of many stacked feedforward layers, mimicking the bottom-up path of the human visual cortex, where each layer learns progressively more abstract representations of the input data. Low-level stages tend to learn biologically plausible feature detectors, such as Gabor filters [17]. Detectors in higher layers learn to respond to concrete visual objects or their parts, e.g., [18\u201321]. Once trained, the CNN never changes its weights or filters during evaluation.\nEvolution has discovered efficient feedforward pathways for recognizing certain objects in the blink of an eye. However, an expert ornithologist, asked to classify a bird belonging to one of two very similar species, may have to think for more than a few milliseconds before answering [22, 23], implying that several feedforward evaluations are performed, where each evaluation tries to elicit different information from the image.\nSince humans benefit greatly from this strategy, we hypothesize CNNs can too. This requires: (1) the formulation of a non-stationary CNN that can adapt its own behaviour post-training, and (2) a process that decides how to adapt the CNNs behaviour.\nThis paper introduces Deep Attention Selective Networks (dasNet) which model selective attention in deep CNNs by allowing each layer to influence all other layers on successive passes over an image through special connections (both bottom-up and top-down), that modulate the activity of the convolutional filters. The weights of these special connections implement a control policy that is learned through reinforcement learning after the CNN has been trained in the usual way via supervised learning. Given an input image, the attentional policy can enhance or suppress features over multiple passes to improve the classification of difficult cases not captured by the initially supervised training. Our aim is to let the system check the usefulness of internal CNN filters automatically, omitting manual inspection [24].\nIn our current implementation, the attentional policy is evolved using Separable Natural Evolution Strategies (SNES; [25]), instead of a conventional, single agent reinforcement learning method (e.g. value iteration, temporal difference, policy gradients, etc.) due to the large number of parameters (over 1 million) required to control CNNs of the size typically used in image classification. Experiments on CIFAR-10 and CIFAR100 [26] show that on difficult classification instances, the network corrects itself by emphasizing and de-emphasizing certain filters, outperforming a previous state-ofthe-art CNN."}, {"heading": "2 Maxout Networks", "text": "In this work we use the Maxout networks [10], combined with dropout [27], as the underlying model for dasNet. Maxout networks represent the state-of-the-art for object recognition in various tasks and have only been outperformed (by a small margin) by averaging committees of several convolutional neural networks. A similar approach, which does not reduce dimensionality in favor of sparsity in the representation has also been recently presented [28]. Maxout CNNs consist of a stack of alternating convolutional and maxout layers, with a final classification layer on top:\nConvolutional Layer. The input to this layer can be an image or the output of a previous layer, consisting of c input maps of width m and height n: x \u2208 Rc\u00d7m\u00d7n. The output consists of a set of c\u2032 output maps: y \u2208 Rc\n\u2032\u00d7m\u2032\u00d7n\u2032 . The convolutional layer is parameterized by c \u00b7 c\u2032 filters of size k \u00d7 k. We denote the filters by F \u2113i,j \u2208 R\nk\u00d7k , where i and j are indexes of the input and output maps and \u2113 denotes the layer.\ny\u2113j =\ni=c\u2211\ni=0\n\u03c6(xi \u2217 F \u2113 i,j) (1)\nwhere i and j index the input and output map respectively, \u2217 is the convolutional operator, \u03c6 is an element-wise nonlinear function, and \u2113 is used to index the layer. The size of the output is determined by the kernel size and the stride used for the convolution (see [10]).\nPooling Layer. A pooling layer is used to reduced the dimensionality of the output from a convolutional layer. The usual approach is to take the maximum value among non- or partially-overlapping patches in every map, therefore reducing dimensionality\nalong the height and width [2]. Instead, a Maxout pooling layer reduces every b consecutive maps to one map, by keeping only the maximum value for every pixel-position, where b is called the block size. Thus the map reduces c input maps to c\u2032 = c/b output maps.\ny\u2113j,x,y = b\nmax i=0\ny\u2113\u22121j\u00b7b+i,x,y (2)\nwhere y\u2113 \u2208 Rc \u2032\u00d7m\u2032\u00d7n\u2032 , and \u2113 again is used to index the layer. The output of the pooling layer can either be used as input to another pair of convolutional- and pooling layers, or form input to a final classification layer.\nClassification Layer. Finally, a classification step is performed. First the output of the last pooling layer is flattened into one large vector ~x, to form the input to the following equations:\ny\u0304\u2113j = max i=0..b F \u2113j\u00b7b+i~x (3)\nv = \u03c3(F \u2113+1y\u0304\u2113) (4)\nwhere F \u2113 \u2208 RN\u00d7|~x| (N is chosen), and \u03c3(\u00b7) is the softmax activation function which produces the class probabilities v. The input is projected by F and then reduced using a maxout, similar to the pooling layer (3)."}, {"heading": "3 Reinforcement Learning", "text": "Reinforcement learning (RL) is a general framework for learning to make sequential decisions order to maximize an external reward signal [29, 30]. The learning agent can be anything that has the ability to act and perceive in a given environment.\nAt time t, the agent receives an observation ot \u2208 O of the current state of the environment st \u2208 S, and selects an action, at \u2208 A, chosen by a policy \u03c0 : O \u2192 A, where S,O and A the spaces of all possible states, observations, and action, respectively.1 The agent then enters state st+1 and receives a reward rt \u2208 R. The objective is to find the policy, \u03c0, that maximizes the expected future discounted reward, E[ \u2211\nt \u03b3 trt],\nwhere \u03b3 \u2208 [0, 1] discounts the future, modeling the \u201cfarsightedness\u201d of the agent. In dasNet, both the observation and action spaces are real valued O = Rdim(O), A = Rdim(A). Therefore, policy \u03c0\u03b8 must be represented by a function approximator, e.g. a neural network, parameterized by \u03b8. Because the policies used to control the attention of the dasNet have state and actions spaces of close to a thousand dimensions, the policy parameter vector, \u03b8, will contain close to a million weights, which is impractical for standard RL methods. Therefore, we instead evolve the policy using a variant for Natural Evolution Strategies (NES; [31, 32]), called Separable NES (SNES; [25]). The NES family of black-box optimization algorithms use parameterized probability distributions over the search space, instead of an explicit population (i.e., a conventional ES [33\u201335]). Typically, the distribution is a multivariate Gaussian parameterized by mean \u00b5 and covariance matrix \u03a3. Each epoch a generation is sampled from the distribution, which is then updated the direction of the natural gradient of the expected fitness of the distribution. SNES differs from standard NES in that\n1In this work \u03c0 : O \u2192 A is a deterministic policy; given an observation it will always output the same action. However, \u03c0 could be extended to stochastic policies.\nAlgorithm 1 TRAIN DASNET (M, \u00b5, \u03a3, p, n)\n1: while True do 2: images \u21d0 NEXTBATCH(n) 3: for i = 0 \u2192 p do 4: \u03b8i \u223c N(\u00b5,\u03a3) 5: for j = 0 \u2192 n do 6: a0 \u21d0 1 {Initialize gates a with identity activation} 7: for t = 0 \u2192 T do 8: vt = Mt(\u03b8i, xi) 9: ot \u21d0 h(Mt)\n10: at+1 \u21d0 \u03c0\u03b8i(ot) 11: end for 12: Li = \u2212\u03bbboostd log(vT ) 13: end for 14: F [i] \u21d0 f(\u03b8i) 15: \u0398[i] \u21d0 \u03b8i 16: end for 17: UPDATESNES(F , \u0398) 18: end while\ninstead of maintaining the full covariance matrix of the search distribution, uses only the diagonal entries. SNES is theoretically less powerful than standard NES, but is substantially more efficient."}, {"heading": "4 Deep Attention Selective Networks (dasNet)", "text": "The idea behind dasNet is to harness the power of sequential processing to improve classification performance by allowing the network to iteratively focus the attention of its filters. First, the standard Maxout net (see Section 2) is augmented to allow the filters to be weighted differently on different passes over the same image (compare to equation 1):\ny\u2113j = a \u2113 j\ni=c\u2211\ni=0\n\u03c6(xi \u2217 F \u2113 i,j), (5)\nwhere a\u2113j is the weight of the j-th output map in layer \u2113, changing the strength of its activation, before applying the maxout pooling operator. The vector a = [a00, a 0 1, \u00b7 \u00b7 \u00b7 , a 0 c\u2032 , a10, \u00b7 \u00b7 \u00b7 , a 1 c\u2032 , \u00b7 \u00b7 \u00b7 ] represents the action that the learned policy must select in order to sequentially focus the attention of the Maxout net on the most discriminative features in the image being processed. Changing action a will alter the behaviour of the CNN, resulting in different outputs, even when the image x does not change. We indicate this with the following notation:\nvt = Mt(\u03b8, x) (6)\nwhere \u03b8 is the parameter vector of the policy, \u03c0\u03b8 , and vt is the output of the network on pass t.\nAlgorithm 1 describes the dasNet training algorithm. Given a Maxout net, M, that has already been trained to classify images using training set, X, the policy, \u03c0,\nis evolved using SNES to focus the attention of M. Each pass through the while loop represents one generation of SNES. Each generation starts by selecting a subset of n images from X at random. Then each of the p samples drawn from the SNES search distribution (with mean \u00b5 and covariance \u03a3) representing the parameters, \u03b8i, of a candidate policy, \u03c0\u03b8i , undergoes n trials, one for each image in the batch. During a trial, the image is presented to the Maxout net T times. In the first pass, t = 0, the action, a0, is set to ai = 1, \u2200i, so that the Maxout network functions as it would normally \u2014 the action has no effect. Once the image is propagated through the net, an observation vector, o0, is constructed by concatenating the following values extracted from M, by h(\u00b7):\n1. the average activation of every output map Avg(yj) (Equation 2), of each Maxout layer.\n2. the intermediate activations y\u0304j of the classification layer.\n3. the class probability vector, vt.\nWhile averaging map activations provides only partial state information, these values should still be meaningful enough to allow for the selection of good actions. The candidate policy then maps the observation to an action:\n\u03c0\u03b8i(o) = dim(A)\u03c3(\u03b8iot) = at, (7)\nwhere \u03b8 \u2208 Rdim(A)\u00d7dim(O) is the weight matrix of the neural network, and \u03c3 is the softmax. Note that the softmax function is scaled by the dimensionality of the action space so that elements in the action vector average to 1 (instead of regular softmax which sums to 1), ensuring that all network outputs are positive, thereby keeping the filter activations stable.\nOn the next pass, the same image is processed again, but this time using the filter weighting, a1. This cycle is repeated until pass T (see figure 1 for a illustration of the process), at which time the performance of the network is scored by:\nLi = \u2212\u03bbboostd log(vT ) (8)\nvT = MT (\u03b8i, xi) (9)\n\u03bbboost =\n{\n\u03bbcorrect if d = \u2016vT \u2016\u221e \u03bbmisclassified otherwise,\n(10)\nwhere v is the output of M at the end of the pass T , d is the correct classification, and \u03bbcorrect and \u03bbmisclassified are constants. Li measures the weighted loss, where misclassified samples are weighted higher than correctly classified samples \u03bbmisclassified > \u03bbcorrect. This simple form of boosting is used to focus on the \u2018difficult\u2019 misclassified images. Once all of the input images have been processed, the policy is assigned the fitness:\nf(\u03b8i) =\ncumulative score \ufe37 \ufe38\ufe38 \ufe37 n\u2211\ni=1\nLi +\nregularization \ufe37 \ufe38\ufe38 \ufe37 \u03bbL2\u2016\u03b8i\u20162 (11)\nwhere \u03bbL2 is a regularization parameter. Once all of the candidate policies have been evaluated, SNES updates its distribution parameters (\u00b5,\u03a3) according the natural gradient calculated from the sampled fitness values, F . As SNES repeatedly updates the distribution over the course of many generations, the expected fitness of the distribution improves, until the stopping criterion is met."}, {"heading": "5 Related Work", "text": "Human vision is still the most advanced and flexible perceptual system known. Architecturally, visual cortex areas are highly connected, including direct connections over multiple levels and top-down connections. Felleman and Van Essen [36] constructed a (now famous) hierarchy diagram of 32 different visual cortical areas in macaque visual cortex. About 40% of all pairs of areas were considered connected, and most connected areas were connected bidirectionally. The top-down connections are more numerous than bottom-up connections, and generally more diffuse [37]. They are thought to play primarily a modulatory role, while feedforward connections serve as directed information carriers [38].\nAnalysis of response latencies to a newly-presented image lends credence to the theory that there are two stages of visual processing: a fast, pre-attentive phase, due to feedforward processing, followed by an attentional phase, due to the influence of recurrent processing [39]. After the feedforward pass, we can recognize and localize simple salient stimuli, which can \u201cpop-out\u201d [40], and response times do not increase regardless of the number of distractors. However, this effect has only been conclusively shown for basic features such as color or orientation; for categorical stimuli or faces, whether there is a pop-out effect remains controversial [41, 42]. Regarding the attentional phase, feedback connections are known to play important roles, such as in feature grouping [43], in differentiating a foreground from its background, (especially\nwhen the foreground is not highly salient [44, 45]), and perceptual filling in [46]. Work by Bar et al. [47] supports the idea that top-down projections from prefrontal cortex play an important role in object recognition by quickly extracting low-level spatial frequency information to provide an initial guess about potential categories, forming a top-down expectation that biases recognition. Recurrent connections seem to rely heavily on competitive inhibition and other feedback to make object recognition more robust [48, 49].\nIn the context of computer vision, RL has been shown to be able to learn saccades in visual scenes to learn selective attention [50], learn feedback to lower levels [51, 52], and improve face recognition [53\u201355]. It has been shown to be effective for object recognition [56], and has also been combined with traditional computer vision primitives [57]. Iterative processing of images using recurrency has been successfully used for image reconstruction [58] and face-localization [59]. All these approaches show that recurrency in processing and an RL perspective can lead to novel algorithms that improve performance. However, this research is often applied to simplified datasets for demonstration purposes due to computation constraints, and are not aimed at improving the state-of-the-art. In contrast, we apply this perspective directly to the known state-of-the-art neural networks to show that this approach is now feasible and actually increases performance."}, {"heading": "6 Experiments on CIFAR-10/100", "text": "The experimental evaluation of dasNet focuses on ambiguous classification cases in the CIFAR-10 and CIFAR-100 data sets where, due to a high number of common features, two classes are often mistaken for each other. These are the most interesting cases for our approach. By learning on top of an already trained model, dasNet must aim at fixing these erroneous predictions without disrupting, or forgetting, what has been learned.\nThe CIFAR-10 dataset [26] is composed of 32\u00d7 32 color images split into 5\u00d7 104 training and 104 testing samples, where each image is assigned to one of 10 classes. The CIFAR-100 is similarly composed, but contains 100 classes.\nThe number of steps, T , for the RL was experimentally determined and fixed at 5; enough steps to allow dasNet to adapt while being small enough to be practical. While it is be possible to iterate until some condition is met, this could be a serious limitation in real-time applications where predictable processing latency is critical. In all experiments we set \u03bbcorrect = 0.005, \u03bbmisclassified = 1 and \u03bbL2 = 0.005.\nThe Maxout network, M, used in the experiments was trained with data augmentation following the suggested global contrast normalization and ZCA normalization protocol. The model consists of three convolutional maxout layers followed by a fully connected maxout and softmax outputs. Dropout of 0.5 was used in all layers except the input layer, and 0.2 for the input layer. The population size for SNES was set to 50.\nTable 1 shows the performance of dasNet vs. other methods, where it achieves a relative improvement of 6% with respect to the vanilla CNN. This establishes a new state-of-the-art result for this challenging dataset.\nFigure 3 shows the classification of a cat-image from the test-set. All output map activations in the final step are shown at the top. The difference in activations compared to the first step, i.e., the (de-)emphasis of each map, is shown on the bottom. On the left are the class probabilities for each time-step. At the first step, the classification is \u2018dog\u2019, and the cat could indeed be mistaken for a puppy. Note that in the first step,\n0.44\n0.442\n0.444\n0.446\n0.448\n0.45\n0.452\n0 1 2 3 4 5 6 7 8 9\n% C\nor re\nct\nNumber of steps evaluated\n0 steps 1 step 2 steps\nFigure 2: Two dasNets were trained on CIFAR-100 for different values of T . Then they were allowed to run for [0..9] iterations for each image. The performance peeks at the number of steps that the network is trained on, after which the performance drops, but does not explode, showing the dynamics are stable.\nthe network has not yet received any feedback. In the next step, the probability for \u2018cat\u2019 goes up dramatically, beating \u2019dog\u2019, and subsequently drops a bit in the following steps. The network has successfully disambiguated a cat from a dog. If we investigate the filters, we see that already in the lower layer emphasis changes significantly. Some filters focus more on surroundings whilst others de-emphasize the eyes.\nIn the second layer, almost all output maps are emphasized. In the third and highest convolutional layer, the most complex changes to the network. At this level the positional correspondence is largely lost, and the filters are known to code for \u2018higher level\u2019 features. It is in this layer that changes are the most influential because they are closest to the final output layers. It is hard to analyze the effect of the alterations, but we can see that the differences are not simple increases or decreases of the output maps, as we then would expect the final activations and their corresponding increases to be largely similar. Instead we see complex emphasis and pattern suppression.\nDynamics To investigate the dynamics, a small 2-layer dasNet network was trained for different values of T . Then they were evaluated by allowing them to run for [0..9] steps. Figure 2 shows results of training dasNet on CIFAR-100 for T = 1 and T = 2. The performance goes up from the vanilla CNN, peaks at the step = T as expected, and reduces but stays stable after that. So even though the dasNet was trained using only a small number of steps, the dynamics stay stable when these are evaluated for as many as 10 steps.\nTo verify whether the dasNet policy is actually making good use of its gates, their information content is estimated the following way: The gate values in the last step are taking and used directly for classification. If the gates are used properly then their activation should contain information that is relevant for classification and we would expect a\ndasNet that was trained with T = 2 and are used as features for classification. Then using only the final gate-values (so without e.g. the output of the classification layer), a classification using 15-nearest neighbour and logistic regression was performed. This resulted in a performance of 40.70% and 45.74% correct respectively, similar to the\nperformance of dasNet, confirming that they contain significant information and we can conclude that they are purposefully used."}, {"heading": "7 Conclusion", "text": "DasNet is a deep neural network with feedback connections that are learned by through reinforcement learning to direct selective internal attention to certain features extracted from images. After a rapid first shot image classification through a standard stack of feedforward filters, the feedback can actively alter the importance of certain filters \u201cin hindsight\u201d, correcting the initial guess via additional internal \u201cthoughts\u201d.\nDasNet successfully learned to correct image misclassifications produced by a fully trained feedforward Maxout network. Its active, selective, internal spotlight of attention enabled state-of-the-art results.\nFuture research will also consider more complex actions that spatially focus on (or alter) parts of observed images."}, {"heading": "Acknowledgments", "text": "We acknowledge Matthew Luciw, who provided a short literature review, partially included in the Related Work section."}], "references": [{"title": "Neural network model for a mechanism of pattern recognition unaffected by shift in position - Neocognitron", "author": ["K. Fukushima"], "venue": "Trans. IECE, J62-A(10):658\u2013665", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1979}, {"title": "Cresceptron: a self-organizing neural network which grows adaptively", "author": ["Juyang Weng", "Narendra Ahuja", "Thomas S Huang"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Back-propagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation, 1(4):541\u2013551", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR \u201907. IEEE Conference on, pages 1\u20138", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. M\u00fcller", "S. Behnke"], "venue": "Proc. International Conference on Artificial Neural Networks (ICANN), pages 92\u2013101", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Flexible", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "high performance convolutional neural networks for image classification. In Intl. Joint Conference on Artificial Intelligence IJCAI, pages 1237\u20131242", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-column deep neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition CVPR 2012", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "I Sutskever", "G. E Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS 2012),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Mitosis detection in breast cancer histology images with deep neural networks", "author": ["Dan Claudiu Ciresan", "Alessandro Giusti", "Luca Maria Gambardella", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. MICCAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Deep neural networks segment neuronal membranes in electron microscopy", "author": ["Dan Claudiu Ciresan", "Alessandro Giusti", "Luca Maria Gambardella", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning hierarchical features for scene labeling", "author": ["Cl\u00e9ment Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1915}, {"title": "Pedestrian detection with unsupervised multi-stage feature learning", "author": ["P. Sermanet", "K. Kavukcuoglu", "S. Chintala", "Y. LeCun"], "venue": "Proc. International Conference on Computer Vision and Pattern Recognition (CVPR\u201913). IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Michael Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Technical Report IDSIA- 03-14 / arXiv:1404.7828v1 [cs.NE], The Swiss AI Lab IDSIA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Theory of communication. Part 1: The analysis of information", "author": ["Dennis Gabor"], "venue": "Electrical Engineers-Part III: Journal of the Institution of Radio and Communication Engineering,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1946}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "CoRR, abs/1311.2901,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D. Zeiler", "Graham W. Taylor", "Rob Fergus"], "venue": "In 2011 International Conference on Computer", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2018}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Visual recognition with humans in the loop", "author": ["Steve Branson", "Catherine Wah", "Florian Schroff", "Boris Babenko", "Peter Welinder", "Pietro Perona", "Serge Belongie"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Caltech- UCSD Birds 200", "author": ["P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie", "P. Perona"], "venue": "Technical Report CNS-TR-2010-001, California Institute of Technology", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Technical Report arXiv:1311.2901 [cs.CV], NYU", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "High dimensions and heavy tails for natural evolution strategies", "author": ["Tom Schaul", "Tobias Glasmachers", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 13th annual conference on Genetic and evolutionary computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Computer Science Department, University of Toronto", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Compete to compute", "author": ["Rupesh Kumar Srivastava", "Jonathan Masci", "Sohrob Kazerounian", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Reinforcement learning: a survey", "author": ["Leslie Pack Kaelbling", "Michael L. Littman", "Andrew W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1996}, {"title": "Natural evolution strategies", "author": ["D. Wierstra", "T. Schaul", "J. Peters", "J. Schmidhuber"], "venue": "IEEE Congress on Evolutionary Computation, pages 3381\u20133387. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Exponential natural evolution strategies", "author": ["T. Glasmachers", "T. Schaul", "S. Yi", "D. Wierstra", "J. Schmidhuber"], "venue": "12th annual conference on Genetic and Evolutionary Computation, pages 393\u2013400. ACM", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution", "author": ["I. Rechenberg"], "venue": "Dissertation", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1971}, {"title": "Numerische Optimierung von Computer-Modellen", "author": ["H.P. Schwefel"], "venue": "Dissertation", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1974}, {"title": "Adaptation in Natural and Artificial Systems", "author": ["J.H. Holland"], "venue": "University of Michigan Press, Ann Arbor", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1975}, {"title": "Distributed hierarchical processing in the primate cerebral cortex", "author": ["Daniel J Felleman", "David C Van Essen"], "venue": "Cerebral cortex,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1991}, {"title": "Recurrent excitation in neocortical circuits", "author": ["Rodney J Douglas", "Christof Koch", "Misha Mahowald", "KA Martin", "Humbert H Suarez"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "Hierarchies of cortical areas", "author": ["J. Bullier"], "venue": "J.H. Kaas and C.E. Collins, editors, The Primate Visual System, pages 181\u2013204. CRC Press, New York", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "The distinct modes of vision offered by feedforward and recurrent processing", "author": ["Victor AF Lamme", "Pieter R Roelfsema"], "venue": "Trends in neurosciences,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2000}, {"title": "Visual salience", "author": ["L. Itti"], "venue": "2(9):3327", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Perceptual selectivity is task dependent: The pop-out effect poops out", "author": ["Carl M Francolini", "Howard E Egeth"], "venue": "Perception & Psychophysics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1979}, {"title": "On second glance: Still no high-level pop-out effect for faces", "author": ["Rufin VanRullen"], "venue": "Vision research,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Brain states: top-down influences in sensory processing", "author": ["Charles D Gilbert", "Mariano Sigman"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2007}, {"title": "Cortical feedback improves discrimination between figure and background by v1", "author": ["JM Hupe", "AC James", "BR Payne", "SG Lomber", "P Girard", "J Bullier"], "venue": "v2 and v3 neurons. Nature, 394(6695):784\u2013787", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "The role of feedback connections in shaping the responses of visual cortical neurons", "author": ["Jean Bullier", "Jean-Michel Hup\u00e9", "Andrew C James", "Pascal Girard"], "venue": "Progress in brain research,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2001}, {"title": "Blindsight: the role of feedforward and feedback corticocortical connections", "author": ["Victor AF Lamme"], "venue": "Acta psychologica,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2001}, {"title": "Top-down facilitation of visual recognition", "author": ["Moshe Bar", "Karim S Kassam", "Avniel Singh Ghuman", "Jasmine Boshyan", "Annette M Schmid", "Anders M Dale", "MS H\u00e4m\u00e4l\u00e4inen", "Ksenija Marinkovic", "DL Schacter", "BR Rosen"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2006}, {"title": "The role of competitive inhibition and top-down feedback in binding during object recognition", "author": ["Dean Wyatte", "Seth Herd", "Brian Mingus", "Randall O\u2019Reilly"], "venue": "Frontiers in Psychology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded", "author": ["Dean Wyatte", "Tim Curran", "Randall O\u2019Reilly"], "venue": "Journal of Cognitive Neuroscience,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Learning to generate artificial fovea trajectories for target detection", "author": ["J. Schmidhuber", "R. Huber"], "venue": "International Journal of Neural Systems, 2(1 & 2):135\u2013141", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1991}, {"title": "Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm", "author": ["Randall C O\u2019Reilly"], "venue": "Neural Computation,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1996}, {"title": "Restoring partly occluded patterns: A neural network model with backward paths", "author": ["Kunihiko Fukushima"], "venue": "Artificial Neural Networks and Neural Information Processing ICANN/ICONIP 2003,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2003}, {"title": "Learning to combine foveal glimpses with a thirdorder boltzmann machine", "author": ["Hugo Larochelle", "Geoffrey Hinton"], "venue": "Image, 1:x2,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2010}, {"title": "Reinforcement learning based visual attention with application to face detection", "author": ["Benjamin Goodrich", "Itamar Arel"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Using guided autoencoders on face recognition", "author": ["M.F. Stollenga", "M.A. Wiering", "L.R.B. Schomaker"], "venue": "Master\u2019s thesis. University of Groningen", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2011}, {"title": "Recurrent processing during object recognition", "author": ["Randall C OReilly", "Dean Wyatte", "Seth Herd", "Brian Mingus", "David J Jilk"], "venue": "Frontiers in Psychology,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Reinforcement Learning for the adaptive control of perception and action", "author": ["S.D. Whitehead"], "venue": "PhD thesis,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1992}, {"title": "Learning iterative image reconstruction in the neural abstraction pyramid", "author": ["Sven Behnke"], "venue": "International Journal of Computational Intelligence and Applications,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2001}, {"title": "Face localization and tracking in the neural abstraction pyramid", "author": ["Sven Behnke"], "venue": "Neural Computing & Applications,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2005}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Matthew D. Zeiler", "Rob Fergus"], "venue": "CoRR, abs/1301.3557,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 94, "endOffset": 99}, {"referenceID": 3, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 94, "endOffset": 99}, {"referenceID": 4, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 94, "endOffset": 99}, {"referenceID": 5, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 167, "endOffset": 173}, {"referenceID": 7, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 167, "endOffset": 173}, {"referenceID": 8, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 167, "endOffset": 173}, {"referenceID": 9, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 198, "endOffset": 206}, {"referenceID": 10, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 198, "endOffset": 206}, {"referenceID": 11, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 226, "endOffset": 233}, {"referenceID": 12, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 226, "endOffset": 233}, {"referenceID": 13, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 226, "endOffset": 233}, {"referenceID": 14, "context": "Deep convolutional neural networks (CNNs) [1] with max-pooling layers [2] trained by backprop [3\u20135] on GPUs [6] have become the state-of-the-art in object recognition [7\u201310], segmentation/detection [11, 12], and scene parsing [13\u201315] (for an extensive review see [16]).", "startOffset": 263, "endOffset": 267}, {"referenceID": 15, "context": "Low-level stages tend to learn biologically plausible feature detectors, such as Gabor filters [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": ", [18\u201321].", "startOffset": 2, "endOffset": 9}, {"referenceID": 17, "context": ", [18\u201321].", "startOffset": 2, "endOffset": 9}, {"referenceID": 18, "context": ", [18\u201321].", "startOffset": 2, "endOffset": 9}, {"referenceID": 19, "context": ", [18\u201321].", "startOffset": 2, "endOffset": 9}, {"referenceID": 20, "context": "However, an expert ornithologist, asked to classify a bird belonging to one of two very similar species, may have to think for more than a few milliseconds before answering [22, 23], implying that several feedforward evaluations are performed, where each evaluation tries to elicit different information from the image.", "startOffset": 173, "endOffset": 181}, {"referenceID": 21, "context": "However, an expert ornithologist, asked to classify a bird belonging to one of two very similar species, may have to think for more than a few milliseconds before answering [22, 23], implying that several feedforward evaluations are performed, where each evaluation tries to elicit different information from the image.", "startOffset": 173, "endOffset": 181}, {"referenceID": 22, "context": "Our aim is to let the system check the usefulness of internal CNN filters automatically, omitting manual inspection [24].", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "In our current implementation, the attentional policy is evolved using Separable Natural Evolution Strategies (SNES; [25]), instead of a conventional, single agent reinforcement learning method (e.", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Experiments on CIFAR-10 and CIFAR100 [26] show that on difficult classification instances, the network corrects itself by emphasizing and de-emphasizing certain filters, outperforming a previous state-ofthe-art CNN.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "In this work we use the Maxout networks [10], combined with dropout [27], as the underlying model for dasNet.", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "A similar approach, which does not reduce dimensionality in favor of sparsity in the representation has also been recently presented [28].", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "along the height and width [2].", "startOffset": 27, "endOffset": 30}, {"referenceID": 27, "context": "Reinforcement learning (RL) is a general framework for learning to make sequential decisions order to maximize an external reward signal [29, 30].", "startOffset": 137, "endOffset": 145}, {"referenceID": 0, "context": "The objective is to find the policy, \u03c0, that maximizes the expected future discounted reward, E[ \u2211 t \u03b3 rt], where \u03b3 \u2208 [0, 1] discounts the future, modeling the \u201cfarsightedness\u201d of the agent.", "startOffset": 118, "endOffset": 124}, {"referenceID": 28, "context": "Therefore, we instead evolve the policy using a variant for Natural Evolution Strategies (NES; [31, 32]), called Separable NES (SNES; [25]).", "startOffset": 95, "endOffset": 103}, {"referenceID": 29, "context": "Therefore, we instead evolve the policy using a variant for Natural Evolution Strategies (NES; [31, 32]), called Separable NES (SNES; [25]).", "startOffset": 95, "endOffset": 103}, {"referenceID": 23, "context": "Therefore, we instead evolve the policy using a variant for Natural Evolution Strategies (NES; [31, 32]), called Separable NES (SNES; [25]).", "startOffset": 134, "endOffset": 138}, {"referenceID": 30, "context": ", a conventional ES [33\u201335]).", "startOffset": 20, "endOffset": 27}, {"referenceID": 31, "context": ", a conventional ES [33\u201335]).", "startOffset": 20, "endOffset": 27}, {"referenceID": 32, "context": ", a conventional ES [33\u201335]).", "startOffset": 20, "endOffset": 27}, {"referenceID": 33, "context": "Felleman and Van Essen [36] constructed a (now famous) hierarchy diagram of 32 different visual cortical areas in macaque visual cortex.", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "The top-down connections are more numerous than bottom-up connections, and generally more diffuse [37].", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "They are thought to play primarily a modulatory role, while feedforward connections serve as directed information carriers [38].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "Analysis of response latencies to a newly-presented image lends credence to the theory that there are two stages of visual processing: a fast, pre-attentive phase, due to feedforward processing, followed by an attentional phase, due to the influence of recurrent processing [39].", "startOffset": 274, "endOffset": 278}, {"referenceID": 37, "context": "After the feedforward pass, we can recognize and localize simple salient stimuli, which can \u201cpop-out\u201d [40], and response times do not increase regardless of the number of distractors.", "startOffset": 102, "endOffset": 106}, {"referenceID": 38, "context": "However, this effect has only been conclusively shown for basic features such as color or orientation; for categorical stimuli or faces, whether there is a pop-out effect remains controversial [41, 42].", "startOffset": 193, "endOffset": 201}, {"referenceID": 39, "context": "However, this effect has only been conclusively shown for basic features such as color or orientation; for categorical stimuli or faces, whether there is a pop-out effect remains controversial [41, 42].", "startOffset": 193, "endOffset": 201}, {"referenceID": 40, "context": "Regarding the attentional phase, feedback connections are known to play important roles, such as in feature grouping [43], in differentiating a foreground from its background, (especially", "startOffset": 117, "endOffset": 121}, {"referenceID": 41, "context": "when the foreground is not highly salient [44, 45]), and perceptual filling in [46].", "startOffset": 42, "endOffset": 50}, {"referenceID": 42, "context": "when the foreground is not highly salient [44, 45]), and perceptual filling in [46].", "startOffset": 42, "endOffset": 50}, {"referenceID": 43, "context": "when the foreground is not highly salient [44, 45]), and perceptual filling in [46].", "startOffset": 79, "endOffset": 83}, {"referenceID": 44, "context": "[47] supports the idea that top-down projections from prefrontal cortex play an important role in object recognition by quickly extracting low-level spatial frequency information to provide an initial guess about potential categories, forming a top-down expectation that biases recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "Recurrent connections seem to rely heavily on competitive inhibition and other feedback to make object recognition more robust [48, 49].", "startOffset": 127, "endOffset": 135}, {"referenceID": 46, "context": "Recurrent connections seem to rely heavily on competitive inhibition and other feedback to make object recognition more robust [48, 49].", "startOffset": 127, "endOffset": 135}, {"referenceID": 47, "context": "In the context of computer vision, RL has been shown to be able to learn saccades in visual scenes to learn selective attention [50], learn feedback to lower levels [51, 52], and improve face recognition [53\u201355].", "startOffset": 128, "endOffset": 132}, {"referenceID": 48, "context": "In the context of computer vision, RL has been shown to be able to learn saccades in visual scenes to learn selective attention [50], learn feedback to lower levels [51, 52], and improve face recognition [53\u201355].", "startOffset": 165, "endOffset": 173}, {"referenceID": 49, "context": "In the context of computer vision, RL has been shown to be able to learn saccades in visual scenes to learn selective attention [50], learn feedback to lower levels [51, 52], and improve face recognition [53\u201355].", "startOffset": 165, "endOffset": 173}, {"referenceID": 50, "context": "In the context of computer vision, RL has been shown to be able to learn saccades in visual scenes to learn selective attention [50], learn feedback to lower levels [51, 52], and improve face recognition [53\u201355].", "startOffset": 204, "endOffset": 211}, {"referenceID": 51, "context": "In the context of computer vision, RL has been shown to be able to learn saccades in visual scenes to learn selective attention [50], learn feedback to lower levels [51, 52], and improve face recognition [53\u201355].", "startOffset": 204, "endOffset": 211}, {"referenceID": 52, "context": "In the context of computer vision, RL has been shown to be able to learn saccades in visual scenes to learn selective attention [50], learn feedback to lower levels [51, 52], and improve face recognition [53\u201355].", "startOffset": 204, "endOffset": 211}, {"referenceID": 53, "context": "It has been shown to be effective for object recognition [56], and has also been combined with traditional computer vision primitives [57].", "startOffset": 57, "endOffset": 61}, {"referenceID": 54, "context": "It has been shown to be effective for object recognition [56], and has also been combined with traditional computer vision primitives [57].", "startOffset": 134, "endOffset": 138}, {"referenceID": 55, "context": "Iterative processing of images using recurrency has been successfully used for image reconstruction [58] and face-localization [59].", "startOffset": 100, "endOffset": 104}, {"referenceID": 56, "context": "Iterative processing of images using recurrency has been successfully used for image reconstruction [58] and face-localization [59].", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "The CIFAR-10 dataset [26] is composed of 32\u00d7 32 color images split into 5\u00d7 10 training and 10 testing samples, where each image is assigned to one of 10 classes.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "Method CIFAR-10 CIFAR-100 Dropconnect [9] 9.", "startOffset": 38, "endOffset": 41}, {"referenceID": 57, "context": "32% Stochastic Pooling [60] 15.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "13% Multi-column CNN [7] 11.", "startOffset": 21, "endOffset": 24}], "year": 2014, "abstractText": "Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.", "creator": "gnuplot 4.6 patchlevel 3"}}}