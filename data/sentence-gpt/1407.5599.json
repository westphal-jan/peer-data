{"id": "1407.5599", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2014", "title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "abstract": "The general perception is that kernel methods are not scalable, and neural nets are the methods of choice for nonlinear learning problems. Or have we simply not tried hard enough for kernel methods? Here we propose an approach that scales up kernel methods using a novel concept called \"doubly stochastic functional gradients\". Our approach relies on the fact that many kernel methods can be expressed as convex optimization problems, and we solve the problems by making two unbiased stochastic approximations to the functional gradient, one using random training points and another using random functions associated with the kernel, and then descending using this noisy functional gradient as a model. The gradient, the difference between a normal kernel and a polynomial solution, is then used to show that we can apply optimization problems to a polynomial solution using an ordinary polynomial.\n\nThe results from this approach are quite impressive. To begin with, we propose a solution that can be represented as a function of a nonlinear distribution. We introduce a number of features that can be applied to each method. We give the general impression that in addition to the general perception of a polynomial solution, this function will give us a more accurate representation of the general perceptual processing problem. The function of a polynomial is a representation of the general perceptual processing problem. Each function will use a function of a polynomial function, but does not compute the general perceptual processing problem. The function of a polynomial is a representation of the general perceptual processing problem. For a polynomial function, we give the general perceptual processing problem (for the polynomial function, the polynomial function, the polynomial function) as a function of a polynomial function. This approach is very useful when the general perceptual processing problem is a problem. Here we give the general perceptual processing problem (for the polynomial function, the polynomial function, the polynomial function) as a function of a polynomial function.\nA polynomial function uses the general perceptual processing problem (for the polynomial function, the polynomial function, the polynomial function) as a function of a polynomial function.\nIf you are interested in the general perception of the general perceptual processing problem, please do not hesitate to check out this article.\nFor more information about the general perceptual processing problem, click here.", "histories": [["v1", "Mon, 21 Jul 2014 19:05:47 GMT  (297kb,D)", "https://arxiv.org/abs/1407.5599v1", "22 pages, 11 figures"], ["v2", "Tue, 5 Aug 2014 17:58:57 GMT  (297kb,D)", "http://arxiv.org/abs/1407.5599v2", "22 pages, 11 figures"], ["v3", "Tue, 23 Sep 2014 15:39:03 GMT  (345kb,D)", "http://arxiv.org/abs/1407.5599v3", "24 pages, 14 figures"], ["v4", "Thu, 10 Sep 2015 16:40:45 GMT  (1737kb,D)", "http://arxiv.org/abs/1407.5599v4", "32 pages, 22 figures"]], "COMMENTS": "22 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["bo dai", "bo xie 0002", "niao he", "yingyu liang", "anant raj", "maria-florina balcan", "le song"], "accepted": true, "id": "1407.5599"}, "pdf": {"name": "1407.5599.pdf", "metadata": {"source": "CRF", "title": "Scalable Kernel Methods via Doubly Stochastic Gradients", "authors": ["Bo Dai", "Bo Xie", "Niao He", "Yingyu Liang", "Anant Raj", "Maria-Florina Balcan", "Le Song"], "emails": ["araj34}@gatech.edu,", "lsong@cc.gatech.edu", "nhe6@gatech.edu", "ninamf@cs.cmu.edu"], "sections": [{"heading": null, "text": "\u221a t).\nOur approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show that our method can achieve competitive performance to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features."}, {"heading": "1 Introduction", "text": "The general perception is that kernel methods are not scalable. When it comes to large-scale nonlinear learning problems, the methods of choice so far are neural nets where theoretical understanding remains incomplete. Are kernel methods really not scalable? Or is it simply because we have not tried hard enough, while neural nets have exploited sophisticated design of feature architectures, virtual example generation for dealing with invariance, stochastic gradient descent for efficient training, and GPUs for further speedup?\nA bottleneck in scaling up kernel methods is the storage and computation of the kernel matrix, K, which is usually dense. Storing the matrix requires O(n2) space, and computing it takes O(n2d) operations, where n is the number of data points and d is the dimension. There have been many great attempts to scale up kernel methods, including efforts from numerical linear algebra, functional analysis, and numerical optimization perspectives.\nA common numerical linear algebra approach is to approximate the kernel matrix using low-rank factors, K \u2248 A>A, with A \u2208 Rr\u00d7n and rank r 6 n. This low-rank approximation usually requires O(nr2 + nrd)\nar X\niv :1\n40 7.\n55 99\nv4 [\ncs .L\nG ]\noperations, and then subsequent kernel algorithms can directly operate on A. Many works, such as Greedy basis selection techniques [1], Nystro\u0308m approximation [2] and incomplete Cholesky decomposition [3], all followed this strategy. In practice, one observes that kernel methods with approximated kernel matrices often result in a few percentage of losses in performance. In fact, without further assumption on the regularity of the kernel matrix, the generalization ability after low-rank approximation is typically of the order O(1/ \u221a r + 1/ \u221a n) [4, 5], which implies that the rank needs to be nearly linear in the number of data points! Thus, in order for kernel methods to achieve the best generalization ability, the low-rank approximation based approaches quickly become impractical for big datasets due to their O(n3 + n2d) preprocessing time and O(n2) memory requirement.\nRandom feature approximation is another popular approach for scaling up kernel methods [6, 7]. Instead of approximating the kernel matrix, the method directly approximates the kernel function using explicit feature maps. The advantage of this approach is that the random feature matrix for n data points can be computed in time O(nrd) using O(nr) memory, where r is the number of random features. Subsequent algorithms then only operate on an O(nr) matrix. Similar to low-rank kernel matrix approximation approach, the generalization ability of random feature approach is of the order O(1/ \u221a r+1/ \u221a n) [8, 9], which implies that the number of random features also needs to be O(n). Another common drawback of these two approaches is that it is not easy to adapt the solution from a small r to a large r\u2032. Often one is interested in increasing the kernel matrix approximation rank or the number of random features to obtain a better generalization ability. Then special procedures need to be designed to reuse the solution obtained from a small r, which is not straightforward.\nAnother approach that addresses the scalability issue rises from optimization perspective. One general strategy is to solve the dual forms of kernel methods using coordinate or block-coordinate descent (e.g., [10, 11, 12]). By doing so, each iteration of the algorithm only incurs O(nrd) computation and O(nr) memory, where r is the size of the parameter block. A second strategy is to perform functional gradient descent by looking at a batch of data points at a time (e.g., [13, 15]). Thus, the computation and memory requirements are also O(nrd) and O(nr) respectively in each iteration, where r is the batch size. These approaches can easily change to a different r without restarting the optimization and has no loss in generalization ability since they do not approximate the kernel matrix or function. However, a serious drawback of these approaches is that, without further approximation, all support vectors need to be kept for testing, which can be as big as the entire training set! (e.g., kernel ridge regression and non-separable nonlinear classification problems.)\nIn summary, there exists a delicate trade-off between computation, memory and statistics if one wants to scale up kernel methods. Inspired by various previous efforts, we propose a simple yet general strategy to scale up many kernel methods using a novel concept called \u201cdoubly stochastic functional gradients\u201d. Our method relies on the fact that most kernel methods can be expressed as convex optimization problems over functions in reproducing kernel Hilbert spaces (RKHS) and solved via functional gradient descent. Our algorithm proceeds by making two unbiased stochastic approximations to the functional gradient, one using random training points and the other one using random features associated with the kernel, and then descending using this noisy functional gradient. The key intuitions behind our algorithm originate from\n(i) the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased, the convergence of the algorithm is guaranteed [16]; and\n(ii) the property of pseudo-random number generators that the random samples can in fact be completely determined by an initial value (a seed).\nWe exploit these properties and enable kernel methods to achieve better balances between computation, memory and statistics. Our method interestingly combines kernel methods, functional analysis, stochastic optimization and algorithmic trick, and it possesses a number of desiderata:\nGenerality and simplicity. Our approach applies to many kernel methods, such as kernel ridge regression, support vector machines, logistic regression, two-sample test, and many different types of kernels, such as shift-invariant kernels, polynomial kernels, general inner product kernels, and so on. The algorithm can be summarized in just a few lines of code (Algorithm 1 and 2). For a different problem and kernel, we just need\nto adapt the loss function and the random feature generator.\nFlexibility. Different from previous uses of random features which typically prefix the number of features and then optimize over the feature weightings, our approach allows the number of random features, and hence the flexibility of the function class, to grow with the number of data points. This allows our method to be applicable to data streaming setting, which is not possible for previous random feature approach, and achieve the full potential of nonparametric methods.\nEfficient computation. The key computation of our method is evaluating the doubly stochastic functional gradient, which involves the generation of the random features with specific random seeds and the evaluation of these random features on the small batch of data points. For iteration t, the computational complexity is O(td).\nSmall memory. The doubly stochasticity also allows us to avoid keeping the support vectors which becomes prohibitive in large-scale streaming setting. Instead, we just need to keep a small program for regenerating the random features, and sample previously used random feature according to pre-specified random seeds. For iteration t, the memory needed is O(t) independent of the dimension of the data.\nTheoretical guarantees. We provide a novel and nontrivial analysis involving Hilbert space martingale and a newly proved recurrence relation, and show that the estimator produced by our algorithm, which might be outside of the RKHS, converges to the optimal RKHS function. More specifically, both in expectation and with high probability, our algorithm can estimate the optimal function in the RKHS in the rate of O(1/t), which are indeed optimal [16], and achieve a generalization bound of O(1/ \u221a t). The variance of the random features, introduced during our second approximation to the functional gradient, only contributes additively to the constant in the final convergence rate. These results are the first of the kind in kernel method literature, which can be of independent interest.\nStrong empirical performance. Our algorithm can readily scale kernel methods up to the regimes which are previously dominated by neural nets. We show that our method compares favorably to other scalable kernel methods in medium scale datasets, and to neural nets in big datasets such as 8 million handwritten digits from MNIST, 2.3 million materials from MolecularSpace, and 1 million photos from ImageNet using convolution features. Our results suggest that kernel methods, theoretically well-grounded methods, can potentially replace neural nets in many large scale real-world problems where nonparametric estimation are needed.\nIn the remainder, we will first introduce preliminaries on kernel methods and functional gradients. We will then describe our algorithm and provide both theoretical and empirical supports."}, {"heading": "2 Duality between Kernels and Random Processes", "text": "Kernel methods owe their name to the use of kernel functions, k(x, x\u2032) : X \u00d7 X 7\u2192 R, which are symmetric positive definite (PD), meaning that for all n > 1, and x1, . . . , xn \u2208 X , and c1, . . . , cn \u2208 R, we have\u2211n i,j=1 cicjk(xi, xj) > 0. There is an intriguing duality between kernels and stochastic processes which will play a crucial role in our later algorithm design. More specifically,\nTheorem 1 (e.g.,[17]; [18]) If k(x, x\u2032) is a PD kernel, then there exists a set \u2126, a measure P on \u2126, and random feature \u03c6\u03c9(x) : X 7\u2192 R from L2(\u2126,P), such that k(x, x\u2032) = \u222b \u2126 \u03c6\u03c9(x)\u03c6\u03c9(x \u2032) dP(\u03c9).\nEssentially, the above integral representation relates the kernel function to a random process \u03c9 with measure P(\u03c9). Note that the integral representation may not be unique. For instance, the random process can be a Gaussian process on X with the sample function \u03c6\u03c9(x), and k(x, x\u2032) is simply the covariance function between two point x and x\u2032. If the kernel is also continuous and shift invariant, i.e., k(x, x\u2032) = k(x\u2212x\u2032) for x \u2208 Rd, then the integral representation specializes into a form characterized by inverse Fourier transformation (e.g., [19, Theorem 6.6]),\nTheorem 2 (Bochner) A continuous, real-valued, symmetric and shift-invariant function k(x\u2212x\u2032) on Rd is a PD kernel if and only if there is a finite non-negative measure P(\u03c9) on Rd, such that k(x \u2212 x\u2032) =\n\u222b Rd e i\u03c9>(x\u2212x\u2032) dP(\u03c9) = \u222b Rd\u00d7[0,2\u03c0] 2 cos(\u03c9\n>x+ b) cos(\u03c9>x\u2032 + b) d (P(\u03c9)\u00d7 P(b)) , where P(b) is a uniform distribution on [0, 2\u03c0], and \u03c6\u03c9(x) = \u221a 2 cos(\u03c9>x+ b).\nFor Gaussian RBF kernel, k(x \u2212 x\u2032) = exp(\u2212\u2016x \u2212 x\u2032\u20162/2\u03c32), this yields a Gaussian distribution P(\u03c9) with density proportional to exp(\u2212\u03c32\u2016\u03c9\u20162/2); for the Laplace kernel, this yields a Cauchy distribution; and for the Martern kernel, this yields the convolutions of the unit ball [20].\nSimilar representation where the explicit form of \u03c6\u03c9(x) and P(\u03c9) are known can also be derived for rotation invariant kernel, k(x, x\u2032) = k(\u3008x, x\u2032\u3009), using Fourier transformation on sphere [20]. For polynomial kernels, k(x, x\u2032) = (\u3008x, x\u2032\u3009+ c)p, a random tensor sketching approach can also be used [21]. Explicit random features have been designed for many other kernels, such as dot product kernel [33], additive/multiplicative class of homogeneous kernels [34], e.g., Hellinger\u2019s, \u03c72, Jensen-Shannon\u2019s and Intersection kernel, as well as kernels on Abelian semigroups [35]. We summarized these kernels with their explicit features and associated densities in Table 1.\nInstead of finding the random process P(\u03c9) and function \u03c6\u03c9(x) given a kernel, one can go the reverse direction, and construct kernels from random processes and functions (e.g., [19]).\nTheorem 3 If k(x, x\u2032) = \u222b\n\u2126 \u03c6\u03c9(x)\n>\u03c6\u03c9(x \u2032) dP(\u03c9) for a nonnegative measure P(\u03c9) on \u2126 and \u03c6\u03c9(x) : X 7\u2192 Rr,\neach component from L2(\u2126,P), then k(x, x\u2032) is a PD kernel.\nFor instance, \u03c6\u03c9(x) := cos(\u03c9 >\u03c8\u03b8(x)+b), where \u03c8\u03b8(x) can be a random convolution of the input x parametrized by \u03b8, or \u03c6\u03c9(x) = [\u03c6\u03c91(x), \u03c6\u03c92(x), . . . , \u03c6\u03c9r (x)], where \u03c6\u03c91(x) denote the random feature for kernel k1(x, x \u2032). The former random features define a hierachical kernel [45], and the latter random features induce a linear combination of multiple kernels. It is worth to note that the Hellinger\u2019s, \u03c72, Jensen-Shannon\u2019s and Intersection kernels in [34] are special cases of multiple kernels combination. For simplicity, we assume \u03c6w(x) \u2208 R following, and our algorithm is still applicable to \u03c6w(x) \u2208 Rr.\nAnother important concept is the reproducing kernel Hilbert space (RKHS). An RKHS H on X is a Hilbert space of functions from X to R. H is an RKHS if and only if there exists a k(x, x\u2032) : X \u00d7 X 7\u2192 R such that \u2200x \u2208 X , k(x, \u00b7) \u2208 H, and \u2200f \u2208 H, \u3008f(\u00b7), k(x, \u00b7)\u3009H = f(x). If such a k(x, x\u2032) exist, it is unique and it is a PD kernel. A function f \u2208 H if and only if \u2016f\u20162H := \u3008f, f\u3009H < \u221e, and its L2 norm is dominated by RKHS norm \u2016f\u2016L2 6 \u2016f\u2016H ."}, {"heading": "3 Doubly Stochastic Functional Gradients", "text": "Many kernel methods can be written as convex optimizations over functions in the RKHS and solved using the functional gradient methods [13, 15]. Inspired by these previous works, we will introduce a novel concept called \u201cdoubly stochastic functional gradients\u201d to address the scalability issue. Let l(u, y) be a scalar (potentially non-smooth) loss function convex of u \u2208 R. Let the subgradient of l(u, y) with respect to u be l\u2032(u, y). Given a PD kernel k(x, x\u2032) and the associated RKHS H, many kernel methods try to find a function f\u2217 \u2208 H which solves the optimization problem\nargmin f\u2208H\nR(f) := E(x,y)[l(f(x), y)] + \u03bd\n2 \u2016f\u20162H \u21d0\u21d2 argmin \u2016f\u2016H6B(\u03bd) E(x,y)[l(f(x), y)] (1)\nwhere \u03bd > 0 is a regularization parameter, B(\u03bd) is a non-increasing function of \u03bd, and the data (x, y) follow a distribution P(x, y). The functional gradient \u2207R(f) is defined as the linear term in the change of the objective after we perturb f by in the direction of g, i.e.,\nR(f + g) = R(f) + \u3008\u2207R(f), g\u3009H +O( 2). (2)\nFor instance, applying the above definition, we have \u2207f(x) = \u2207\u3008f, k(x, \u00b7)\u3009H = k(x, \u00b7), and \u2207\u2016f\u2016 2 H = \u2207\u3008f, f\u3009H = 2f . Stochastic functional gradient. Given a data point (x, y) \u223c P(x, y) and f \u2208 H, the stochastic functional gradient of E(x,y)[l(f(x), y)] with respect to f \u2208 H is \u03be(\u00b7) := l\u2032(f(x), y)k(x, \u00b7), (3)\nT ab\nle 1:\nS u\nm m\nar y\no f\nke rn\nel s\nin [6\n, 4 6 ,\n3 3 ,\n2 1 ,\n3 4 ,\n3 5 ,\n4 5 ]\na n\nd th\nei r\nex p\nli ci\nt fe\na tu\nre s\nK er\nn el\nk (x ,x \u2032 )\n\u03c6 \u03c9\n(x )\np (\u03c9\n)\nG au\nss ia\nn ex\np (\u2212 \u2016x \u2212 x \u2032 \u2016\n2 2 2 )\nex p\n(\u2212 i\u03c9 > x\n) (2 \u03c0\n)\u2212 d 2\nex p\n(\u2212 \u2016\u03c9 \u20162 2 2\n)\nL ap\nla ci\nan ex\np (\u2212 \u2016x \u2212 x \u2032 \u2016\n1 )\nex p\n(\u2212 i\u03c9 > x\n) \u220f d i=1\n1 \u03c0\n(1 + \u03c9\n2 i )\nC au\nch y\n\u220f d i=1 2 1 +\n(x i \u2212 x \u2032 i)\n2 ex\np (\u2212 i\u03c9 > x\n) ex\np (\u2212 \u2016\u03c9 \u2016 1\n)\nM at\ne\u0301r n\n2 1 \u2212 \u03bd \u0393 (\u03bd\n)\n( \u221a 2 \u03bd \u2016x \u2212 x \u2032 \u2016\n2\n`\n) \u03bd K \u03bd ( \u221a 2\n\u03bd \u2016x \u2212 x \u2032 \u2016\n2\n`\n) ex\np (\u2212 i\u03c9 > x\n) h\n(\u03bd ,d ,`\n)( 2\u03bd `2 +\n4\u03c0 2 \u2016\u03c9 \u20162 2 ) \u03bd+d\n/ 2\nD ot\nP ro\nd u\nct \u2211 \u221e n=\n0 a n \u3008x ,x \u2032 \u3009n\na n >\n0 \u221a a N\np N\n+ 1 \u220f N i=1\n\u03c9 > i x\nP[ N\n= n\n] =\n1 p n +\n1\np (\u03c9 j i |N\n= n\n) =\n1 2\n\u03c9 j i +\n1 2 1 2\n1 \u2212 \u03c9 j i 2\nP ol\ny n\nom ia\nl (\u3008 x ,x \u2032 \u3009\n+ c) p\nF F T \u2212\n1 (F F T (C\n1 x\n) .. . F F T (C\np x\n)) C j\n= S j D j D j \u2208 R d \u00d7 d S j \u2208 R D \u00d7 d\nH el\nli n\nge r\n\u2211 d i= 1\n\u221a x i x \u2032 i\n2\u03c9 > \u221a x\n1 2\n\u03c9 i +\n1 2 1 2\n1 \u2212 \u03c9 i\n2 , \u03c9 i \u2208 {\u2212\n1, + 1} \u03c7 2 2 \u2211 d i= 1 x i x \u2032 i x i + x \u2032 i [ exp (\u2212 i\u03c9 lo g x j )\u221a x j ] d j=1 se ch (\u03c0 \u03c9 ) In te rs ec ti on \u2211 d i= 1 m in (x i, x \u2032 i) [ exp (\u2212 i\u03c9 lo g x j )\u221a 2 x j ] d j=1 1 \u03c0 (1 + 4 \u03c9 2 ) J en se n -S h an n on \u2211 d i= 1 K J S (x i, x \u2032 i) [ exp (\u2212 i\u03c9 lo g x j )\u221a 2 x j ] d j=1 s e c h (\u03c0 \u03c9 ) lo g 4 (1 + 4 \u03c9 2 ) S k ew ed -\u03c7 2 2 \u220f d i=1 \u221a x i + c \u221a x \u2032 i+ c x i + x \u2032 i+ 2 c ex p (\u2212 i\u03c9 > lo g (x + c) ) \u220f d i=1 se ch (\u03c0 \u03c9 i) S k ew ed -I n te rs ec ti o n \u220f d i=1 m in ( \u221a x i + c x \u2032 i+ c ,\u221a x \u2032 i+ c x i + c ) ex p (\u2212 i\u03c9 > lo g (x + c) ) \u220f d i=1 1 \u03c0 (1 + 4 \u03c9 2 i ) E x p o n en ti a lS em ig ro u p ex p (\u2212 \u03b2 \u2211 d i= 1 \u221a x i + x \u2032 j) ex p (\u2212 \u03c9 > x ) \u220f d i=1 \u03b2 2 \u221a \u03c0 \u03c9 \u2212 3 2 i ex p (\u2212 \u03b2 4 \u03c9 i ) R ec ip ro ca lS em ig ro u p \u220f d i=1 \u03bb x i + x \u2032 i+ \u03bb ex p (\u2212 \u03c9 > x ) \u220f d i=1 \u03bb ex p (\u2212 \u03bb \u03c9 i) A rc -C os in e 1 \u03c0 \u2016x \u2016n \u2016x \u2032 \u2016 n J n (\u03b8 ) (\u03c9 > x )n m a x (0 ,\u03c9 > x ) 2\u03c0 \u2212 d 2 ex p (\u2212 \u2016\u03c9 \u20162 2 2 )\nD j\nis ra\nn d\nom {\u00b1\n1 }\nd ia\ngo n\nal m\nat ri\nx a n d\nth e\nco lu\nm n\ns o f S j\na re\nu n\nif o rm\nly se\nle ct\ned fr\no m {e\n1 ,. .. ,e D }. \u03bd\na n\nd `\na re\np o si\nti ve\np a ra\nm et\ner s.\nh (\u03bd ,d ,`\n) =\n2 d \u03c0 d / 2 \u0393\n(\u03bd + d / 2 )(\n2 \u03bd )\u03bd\n\u0393 (\u03bd\n)` 2 \u03bd\n. K \u03bd\nis a\nm o d\nifi ed\nB es\nse l\nfu n\nct io\nn . K J S\n(x ,x \u2032 )\n= x 2\nlo g\n2 x\n+ x \u2032 x +\nx \u2032 2\nlo g\n2 x\n+ x \u2032 x \u2032\n.\n\u03b8 =\nco s\u2212\n1 x > x \u2032 \u2016x \u2016\u2016 x \u2032 \u2016\n, J n (\u03b8\n) =\n(\u2212 1 )n\n(s in \u03b8)\n2 n\n+ 1\n( 1 sin \u03b8 \u2202 \u2202 \u03b8\n) n( \u03c0 \u2212 \u03b8 si n \u03b8\n)\nAlgorithm 1: {\u03b1i}ti=1 = Train(P(x, y)) Require: P(\u03c9), \u03c6\u03c9(x), l(f(x), y), \u03bd.\n1: for i = 1, . . . , t do 2: Sample (xi, yi) \u223c P(x, y). 3: Sample \u03c9i \u223c P(\u03c9) with seed i. 4: f(xi) = Predict(xi, {\u03b1j}i\u22121j=1). 5: \u03b1i = \u2212\u03b3il\u2032(f(xi), yi)\u03c6\u03c9i(xi). 6: \u03b1j = (1\u2212 \u03b3i\u03bd)\u03b1j for j = 1, . . . , i\u2212 1. 7: end for\nAlgorithm 2: f(x) = Predict(x, {\u03b1i}ti=1) Require: P(\u03c9), \u03c6\u03c9(x).\n1: Set f(x) = 0. 2: for i = 1, . . . , t do 3: Sample \u03c9i \u223c P(\u03c9) with seed i. 4: f(x) = f(x) + \u03b1i\u03c6\u03c9i(x). 5: end for\nwhich is essentially a single data point approximation to the true functional gradient. Furthermore, for any g \u2208 H, we have \u3008\u03be(\u00b7), g\u3009H = l\u2032(f(x), y)g(x). Inspired by the duality between kernel functions and random processes, we can make an additional approximation to the stochastic functional gradient using a random feature \u03c6\u03c9(x) sampled according to P(\u03c9). More specifically,\nDoubly stochastic functional gradient. Let \u03c9 \u223c P(\u03c9), then the doubly stochastic gradient of E(x,y)[l(f(x), y)] with respect to f \u2208 H is\n\u03b6(\u00b7) := l\u2032(f(x), y)\u03c6\u03c9(x)\u03c6\u03c9(\u00b7). (4)\nNote that the stochastic functional gradient \u03be(\u00b7) is in RKHSH but \u03b6(\u00b7) may be outsideH, since \u03c6\u03c9(\u00b7) may be outside the RKHS. For instance, for the Gaussian RBF kernel, the random feature \u03c6\u03c9(x) =\u221a\n2 cos(\u03c9>x+b) is outside the RKHS associated with the kernel function.\nHowever, these functional gradients are related by \u03be(\u00b7) = E\u03c9 [\u03b6(\u00b7)], which lead to unbiased estimators of the original functional gradient, i.e.,\nWe emphasize that the source of randomness associated with the random feature is not present in the data, but artificially introduced by us. This is crucial for the development of our scalable algorithm in the next section. Meanwhile, it also creates additional challenges in the analysis of the algorithm which we will deal with carefully."}, {"heading": "4 Doubly Stochastic Kernel Machines", "text": "The first key intuition behind our algorithm originates from the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased, the convergence of the algorithm is guaranteed [16]. In our algorithm, we will exploit this property and introduce two sources of randomness, one from data and another artificial, to scale up kernel methods.\nThe second key intuition behind our algorithm is that the random features used in the doubly stochastic functional gradients will be sampled according to pseudo-random number generators, where the sequences of apparently random samples can in fact be completely determined by an initial value (a seed). Although these random samples are not the \u201ctrue\u201d random sample in the purest sense of the word, however they suffice for our task in practice.\nMore specifically, our algorithm proceeds by making two unbiased stochastic approximation to the functional gradient in each iteration, and then descending using this noisy functional gradient. The overall algorithms for training and prediction is summarized in Algorithm 1 and 2. The training algorithm essentially just performs random feature sampling and doubly stochastic gradient evaluation, and maintains a collection of real number {\u03b1i}, which is computationally efficient and memory friendly. A crucial step in the\nalgorithm is to sample the random features with \u201cseed i\u201d. The seeds have to be aligned between training and prediction, and with the corresponding \u03b1i obtained from each iteration. The learning rate \u03b3t in the algorithm needs to be chosen as O(1/t), as shown by our later analysis to achieve the best rate of convergence. For now, we assume that we have access to the data generating distribution P(x, y). This can be modified to sample uniformly randomly from a fixed dataset, without affecting the algorithm and the later convergence analysis. Let the sampled data and random feature parameters be Dt := {(xi, yi)}ti=1 and \u03c9t := {\u03c9i} t i=1 respectively after t iteration, the function obtained by Algorithm 1 is a simple additive form of the doubly stochastic functional gradients\nft+1(\u00b7) = ft(\u00b7)\u2212 \u03b3t(\u03b6t(\u00b7) + \u03bdft(\u00b7)) = \u2211t\ni=1 ait\u03b6i(\u00b7), \u2200t > 1, and f1(\u00b7) = 0, (7) where ait = \u2212\u03b3i \u220ft j=i+1(1 \u2212 \u03b3j\u03bd) are deterministic values depending on the step sizes \u03b3j(i 6 j 6 t) and regularization parameter \u03bd. This simple form makes it easy for us to analyze its convergence. We note that our algorithm can also take a mini-batch of points and random features at each step, and estimate an empirical covariance for preconditioning to achieve potentially better performance. Our algorithm is general and can be applied to most of the kernel machines which are formulated in the convex optimization (1) in a RKHS H associated with given kernel k(x, x\u2032). We will instantiate the doubly stochastic gradients algorithms for a few commonly used kernel machines for different tasks and loss functions, e.g., regression, classification, quantile regression, novelty detection and estimating divergence functionals/likelihood ratio. Interestingly, the Gaussian process regression, which is a Bayesian model, can also be reformulated as the solution to particular convex optimizations in RKHS, and therefore, be approximated by the proposed algorithm.\nKernel Support Vector Machine (SVM). Hinge loss is used in kernel SVM where l(u, y) = max{0, 1\u2212uy}\nwith y \u2208 {\u22121, 1}. We have l\u2032(u, y) =\n{ 0 if yu > 1\n\u2212y if yu < 1 and the step 5 in Algorithm. 1. becomes\n\u03b1i =\n{ 0 if yif(xi) > 1\n\u03b3iyi\u03c6\u03c9i(xi) if yif(xi) < 1 .\nRemark: [14] used squared hinge loss, l(u, y) = 12 max{0, 1\u2212 uy} 2, in `2-SVM. With this loss function,\nwe have l\u2032(u, y) =\n{ 0 if yu > 1\nu\u2212 y if yu < 1 and the step 5 in Algorithm. 1. becomes\n\u03b1i =\n{ 0 if yif(xi) > 1\n\u03b3i(yi \u2212 f(xi))\u03c6\u03c9i(xi) if yif(xi) < 1 .\nKernel Logistic Regression. Log loss is used in kernel logistic regression for binary classification where l(u, y) = log(1+exp(\u2212yu)) with y \u2208 {\u22121, 1}. We have l\u2032(u, y) = \u2212 y exp(\u2212yu)1+exp(\u2212yu) and the step 5 in Algorithm. 1. becomes\n\u03b1i = \u03b3iyi exp(\u2212yif(xi))\n1 + exp(\u2212yif(xi)) \u03c6\u03c9i(xi).\nFor the multi-class kernel logistic regression, the l(u, y) = \u2212 \u2211C c=1 \u03b4c(y)uc + log (\u2211C c=1 exp(uc) ) where\nC is the number of categories, u \u2208 RC\u00d71, y \u2208 {1, . . . , C} and \u03b4c(y) = 1 only if y = c, otherwise \u03b4c(y) = 0. In such scenario, we denote f(xi) = [f 1(xi), . . . , f C(xi)], and therefore, the corresponding \u03b1 = [\u03b1\n1, . . . , \u03b1C ]. The update rule for \u03b1 in Algorithm. 1. is\n\u03b1ci = \u03b3i ( \u03b4c(yi)\u2212\nexp(f c(xi))\u2211C c=1 exp(f c(xi))\n) \u03c6\u03c9i(xi) \u2200c = 1, . . . , C,\n\u03b1cj = (1\u2212 \u03b3i\u03bd)\u03b1cj ,\u2200j < i,\u2200c = 1, . . . , C. Kernel Ridge Regression. Square loss is used in kernel ridge regression where l(u, y) = 12 (u \u2212 y) 2. We\nhave l\u2032(u, y) = (u\u2212 y) and the step 5 in Algorithm. 1. becomes \u03b1i = \u2212\u03b3i(f(xi)\u2212 yi)\u03c6\u03c9i(xi). Kernel Robust Regression. Huber\u2019s loss is used for robust regression [22] where\nl(u, y) =\n{ 1 2 (u\u2212 y)\n2 if |u\u2212 y| 6 1 |u\u2212 y| \u2212 12 if |u\u2212 y| > 1 .\nWe have l\u2032(u, y) = { (u\u2212 y) if |u\u2212 y| 6 1 sign(u\u2212 y) if |u\u2212 y| > 1 and the step 5 in Algorithm. 1. becomes\n\u03b1i = { \u2212\u03b3i(f(xi)\u2212 yi)\u03c6\u03c9i(xi) if |f(xi)\u2212 yi| 6 1 \u2212\u03b3i sign(f(xi)\u2212 yi)\u03c6\u03c9i(xi) if |f(xi)\u2212 yi| > 1\nKernel Support Vector Regression (SVR). -insensitive loss function is used in kernel SVR where\nl(u, y) = max{0, |u\u2212 y|\u2212 }. We have l\u2032(u, y) = { 0 if |u\u2212 y| 6 sign(u\u2212 y) if |u\u2212 y| > and the step 5 in Algorithm. 1.\nbecomes\n\u03b1i = { 0 if |f(xi)\u2212 yi| 6 \u2212\u03b3i sign(f(xi)\u2212 yi)\u03c6\u03c9i(xi) if |f(xi)\u2212 yi| >\nRemark: Note that if we set = 0, the -intensitive loss function will become absolute deviatin, i.e., l(u, y) = |u\u2212 y|. Therefore, we have the updates for kernel least absolute deviatin regression. Kernel Quantile Regression. The loss function for quantile regression is l(u, y) = max{\u03c4(y \u2212 u), (1 \u2212\n\u03c4)(u\u2212 y)}. We have l\u2032(u, y) = { 1\u2212 \u03c4 if u > y \u2212\u03c4 if u < y and the step 5 in Algorithm. 1. becomes\n\u03b1i = { \u03b3i(\u03c4 \u2212 1)\u03c6\u03c9i(xi) if f(xi) > yi \u03b3i\u03c4\u03c6\u03c9i(xi) if f(xi) < yi .\nKernel Novelty Detection. The loss function l(u, \u03c4) = max{0, \u03c4\u2212u} [23] is proposed for novelty detection. Since \u03c4 is also a variable which needs to be optimized, the optimization problem is formulated as\nmin \u03c4\u2208R,f\u2208H\nEx[l(f(x), \u03c4)] + \u03bd\n2 \u2016f\u20162H \u2212 \u03bd\u03c4,\nand the gradient of l(u, \u03c4) is\n\u2202l(u, \u03c4)\n\u2202u =\n{ 0 if u > \u03c4\n\u22121 if u < \u03c4 ,\n\u2202l(u, \u03c4)\n\u2202\u03c4 =\n{ 0 if u > \u03c4\n1 if u < \u03c4 .\nThe step 5 in Algorithm. 1. becomes\n\u03b1i = { 0 if f(xi) > \u03c4i\u22121 \u03b3i\u03c6\u03c9i(xi) if f(xi) < \u03c4i\u22121 , \u03c4i = { \u03c4i\u22121 + \u03b3i\u03bd if f(xi) > \u03c4i\u22121 \u03c4i\u22121 \u2212 \u03b3i(1\u2212 \u03bd) if f(xi) < \u03c4i\u22121 .\nKernel Density Ratio Estimation. Based on the variational form of Ali-Silvey divergence, i.e., Ep [ r( qp ) ] , where r : R+ \u2192 R is a convex function with r(1) = 0, [24] proposed a nonparametric estimator for the logarithm of the density ratio, log qp , which is the solution of following convex optimization,\nargmin f\u2208H\nEq[exp(f)] + Ep[r\u2217(\u2212 exp(f))] + \u03bd\n2 \u2016f\u20162H (8)\nwhere r\u2217 denotes the Fenchel-Legendre dual of r, r(\u03c4) := sup\u03c7 \u03c7\u03c4 \u2212 r\u2217(\u03c7). In Kullback-Leibler (KL) divergence, the rKL(\u03c4) = \u2212 log(\u03c4). Its Fenchel-Legendre dual is\nr\u2217KL(\u03c4) = { \u221e if \u03c4 > 0 \u22121\u2212 log(\u2212\u03c4) if \u03c4 < 0\nSpecifically, the optimization becomes\nmin f\u2208H\nR(f) = Ey\u223cq[exp(f(y))]\u2212 Ex\u223cp[f(x)] + \u03bd\n2 \u2016f\u20162H = 2Ez,x,y [ \u03b41(z) exp(f(y))\u2212 \u03b40(z)f(x) ] + \u03bd\n2 \u2016f\u20162H.\nwhere z \u223c Bernoulli(0.5). Denote l(ux, uy, z) = \u03b41(z) exp(uy)\u2212 \u03b40(z)ux, we have l\u2032(ux, uy, z) = \u03b41(z) exp(uy)\u2212 \u03b40(z)\nand the the step 5 in Algorithm. 1. becomes\n\u03b1i = \u22122\u03b3i(\u03b41(zi) exp(f(yi))\u03c6\u03c9i(yi)\u2212 \u03b40(zi)\u03c6\u03c9i(xi)), zi \u223c Bernoulli(0.5). In particular, the xi and yi are not sampled in pair, they are sampled independently from P(x) and Q(x) respectively.\n[24] proposed another convex optimization based on rKL(\u03c4) whose solution is a nonparametric estimator for the density ratio. [25] designed rnv(\u03c4) = max(0, \u03c1 \u2212 log \u03c4) for novelty detection. Similarly, the doubly stochastic gradients algorithm is also applicable to these loss functions. Gaussian Process Regression. The doubly stochastic gradients can be used for approximating the posterior of Gaussian process regression by reformulating the mean and variance of the predictive distribution as the solutions to the convex optimizations with particular loss functions. Let y = f(x) + where \u223c N (0, \u03c32) and f(x) \u223c GP(0, k(x, x\u2032)), given the dataset {xi, yi}ni=1, the posterior distribution of the function at the test point x\u2217 can be derived as\nf\u2217|X,y, x\u2217 \u223c N ( k\u2217> ( K + \u03c32I )\u22121 y, k(x\u2217, x\u2217)\u2212 k\u2217> ( K + \u03c32I )\u22121 k\u2217 )\n(9)\nwhere K \u2208 Rn\u00d7n, Kij = K(xi, xj), k\u2217 = [k(x\u2217, x1), . . . , k(x\u2217, xn)]> and I \u2208 Rn\u00d7n is the identity matrix. Obviously, the posterior mean of the Gaussian process for regression can be thought as the solution to optimization problem (1) with square loss and setting \u03bd = 2\u03c32. Therefore, the update rule for approximating the posterior mean will be the same as kernel ridge regression.\nTo compute the predictive variance, we need to evaluate the k\u2217> ( K + \u03c32I )\u22121 k\u2217. Following, we will\nintroduce two different optimizations whose solutions can be used for evaluating the quantity.\n1. Denote \u03c6 = [k(x1, \u00b7), . . . , k(xn, \u00b7)], then k\u2217> ( K + \u03c32I )\u22121 k\u2217 = k(x\u2217, \u00b7)>\u03c6 ( \u03c6>\u03c6+ \u03c32I )\u22121 \u03c6>k(x\u2217, \u00b7)\n= k(x\u2217, \u00b7)>\u03c6\u03c6> ( \u03c6\u03c6> + \u03c32I )\u22121 k(x\u2217, \u00b7)\nwhere the second equation based on identity ( \u03c6\u03c6> + \u03c32I ) \u03c6 = \u03c6 ( \u03c6>\u03c6+ \u03c32I ) . Therefore, we just need to estimate the operator:\nA = C ( C + \u03c3 2\nn I\n)\u22121 where C = 1\nn \u03c6\u03c6> =\n1\nn n\u2211 i=1 k(xi, \u00b7)\u2297 k(xi, \u00b7). (10)\nWe can express A as the solution to the following convex optimization problem\nmin A R(A) = 1 2n n\u2211 i=1 \u2016k(xi, \u00b7)\u2212Ak(xi, \u00b7)\u20162H + \u03c32 2n \u2016A\u20162HS\nwhere \u2016 \u00b7 \u2016HS is the Hilbert-Schmidt norm of the operator. We can achieve the optimum by \u2207R = 0, which is equivalent to Eq. 10.\nBased on this optimization, we approximate the At using \u2211t i6j,i=1 \u03b8ij\u03c6\u03c9i(\u00b7)\u2297\u03c6\u03c9j (\u00b7) by doubly stochas-\ntic functional gradients. The update rule for \u03b8 is\n\u03b8ij =\n( 1\u2212 \u03c3 2\nn \u03b3t\n) \u03b8ij , \u2200i 6 j < t\n\u03b8it = \u2212\u03b3t t\u22121\u2211 j>i \u03b8ij\u03c6\u03c9\u2032j (xt)\u03c6\u03c9\u2032t(xt), \u2200i < t\n\u03b8tt = \u03b3t\u03c6\u03c9t(xt)\u03c6\u03c9\u2032t(xt).\nPlease refer to Appendix D for the details of the derivation.\n2. Assume that the testing points, {x\u2217i }mi=1, are given beforehand, instead of approximating the operator A, we target on functions F \u2217 = [f\u22171 , . . . , f\u2217m]> where f\u2217i (\u00b7) = k(\u00b7)> ( K + \u03c32I )\u22121 k\u2217i , k(\u00b7) =\n[k(x1, \u00b7), . . . , k(x2, \u00b7)] and k\u2217i = [k(x\u2217i , x1), . . . , k(x\u2217i , xn)]>. Estimating f\u2217i (\u00b7) can be accomplished by solving the optimization problem (1) with square loss and setting yj = k(x \u2217 i , xj),\u2200j = 1, . . . , n, \u03bd = 2\u03c32, leading to the same update rule as kernel ridge regression.\nAfter we obtain these estimators, we can calculate the predictive variance on x\u2217i by either k(x \u2217 i , x \u2217 i ) \u2212 A(x\u2217i , x\u2217i ) or k(x\u2217i , x\u2217i )\u2212f\u2217i (x\u2217i ). We conduct experiments to justify the novel formulations for approximating both the mean and variance of posterior of Gaussian processes for regression, and the doubly stochastic update rule in Section.(7).\nNote that, to approximate the operator A, doubly stochastic gradient requires O(t2) memory. Although we do not need to save the whole training dataset, which savesO(dt) memory cost, this is still computationally expensive. When the m testing data are given, we estimate m functions and each of them requires O(t) memory cost, the total cost will be O(tm) by the second algorithm."}, {"heading": "5 Theoretical Guarantees", "text": "In this section, we will show that, both in expectation and with high probability, our algorithm can estimate the optimal function in the RKHS with rate O(1/t), and achieve a generalization bound of O(1/ \u221a t). The analysis for our algorithm has a new twist compared to previous analysis of stochastic gradient descent algorithms, since the random feature approximation results in an estimator which is outside the RKHS. Besides the analysis for stochastic functional gradient descent, we need to use martingales and the corresponding concentration inequalities to prove that the sequence of estimators, ft+1, outside the RKHS converge to the optimal function, f\u2217, in the RKHS. We make the following standard assumptions ahead for later references:\nA. There exists an optimal solution, denoted as f\u2217, to the problem of our interest (1). B. Loss function `(u, y) : R\u00d7R\u2192 R and its first-order derivative is L-Lipschitz continous in terms of the\nfirst argument. C. For any data {(xi, yi)}ti=1 and any trajectory {fi(\u00b7)}ti=1, there exists M > 0, such that |`\u2032(fi(xi), yi)| 6\nM . Note in our situation M exists and M < \u221e since we assume bounded domain and the functions ft we generate are always bounded as well. D. There exists \u03ba > 0 and \u03c6 > 0, such that k(x, x\u2032) 6 \u03ba, |\u03c6\u03c9(x)\u03c6\u03c9(x\u2032)| 6 \u03c6, \u2200x, x\u2032 \u2208 X , \u03c9 \u2208 \u2126. For example, when k(\u00b7, \u00b7) is the Gaussian RBF kernel, we have \u03ba = 1, \u03c6 = 2. We now present our main theorems as below. Due to the space restrictions, we will only provide a short sketch of proofs here. The full proofs for the these theorems are given in the Appendix A-C.\nTheorem 4 (Convergence in expectation) When \u03b3t = \u03b8 t with \u03b8 > 0 such that \u03b8\u03bd \u2208 (1, 2) \u222a Z+,\nEDt,\u03c9t [ |ft+1(x)\u2212 f\u2217(x)|2 ] 6\n2C2 + 2\u03baQ21 t , for any x \u2208 X\nwhere Q1 = max { \u2016f\u2217\u2016H , (Q0 + \u221a Q20 + (2\u03b8\u03bd \u2212 1)(1 + \u03b8\u03bd)2\u03b82\u03baM2)/(2\u03bd\u03b8 \u2212 1) } , with Q0 = 2 \u221a 2\u03ba1/2(\u03ba + \u03c6)LM\u03b82, and C2 = 4(\u03ba+ \u03c6)2M2\u03b82.\nTheorem 5 (Convergence with high probability) When \u03b3t = \u03b8 t with \u03b8 > 0 such that \u03b8\u03bd \u2208 Z+ and t > \u03b8\u03bd, for any x \u2208 X , we have with probability at least 1\u2212 3\u03b4 over (Dt,\u03c9t),\n|ft+1(x)\u2212 f\u2217(x)|2 6 C2 ln(2/\u03b4)\nt +\n2\u03baQ22 ln(2t/\u03b4) ln 2(t)\nt , where C is as above and Q2 = max { \u2016f\u2217\u2016H , Q0 + \u221a Q20 + \u03baM 2(1 + \u03b8\u03bd)2(\u03b82 + 16\u03b8/\u03bd) } , with Q0 = 4 \u221a 2\u03ba1/2M\u03b8(8+ (\u03ba+ \u03c6)\u03b8L).\nProof sketch: We focus on the convergence in expectation; the high probability bound can be established in a similar fashion. The main technical difficulty is that ft+1 may not be in the RKHS H. The key of the proof is then to construct an intermediate function ht+1, such that the difference between ft+1 and ht+1 and the difference between ht+1 and f\u2217 can be bounded. More specifically,\nht+1(\u00b7) = ht(\u00b7)\u2212 \u03b3t(\u03bet(\u00b7) + \u03bdht(\u00b7)) = \u2211t\ni=1 ait\u03bei(\u00b7), \u2200t > 1, and h1(\u00b7) = 0, (11)\nwhere \u03bet(\u00b7) = E\u03c9t [\u03b6t(\u00b7)]. Then for any x, the error can be decomposed as two terms |ft+1(x)\u2212 f\u2217(x)|2 6 2 |ft+1(x)\u2212 ht+1(x)|2\ufe38 \ufe37\ufe37 \ufe38\nerror due to random features\n+ 2\u03ba \u2016ht+1 \u2212 f\u2217\u20162H\ufe38 \ufe37\ufe37 \ufe38 error due to random data\nFor the error term due to random features, ht+1 is constructed such that ft+1 \u2212 ht+1 is a martingale, and the stepsizes are chosen such that |ait| 6 \u03b8t , which allows us to bound the martingale. In other words, the choices of the stepsizes keep ft+1 close to the RKHS. For the error term due to random data, since ht+1 \u2208 H, we can now apply the standard arguments for stochastic approximation in the RKHS. Due to the additional randomness, the recursion is slightly more complicated, et+1 6 ( 1\u2212 2\u03bd\u03b8t ) et + \u03b21 t \u221a et t + \u03b22 t2 , where et+1 = EDt,\u03c9t [\u2016ht+1 \u2212 f\u2217\u20162H], and \u03b21 and \u03b22 depends on the related parameters. Solving this recursion then leads to a bound for the second error term.\nTheorem 6 (Generalization bound) Let the true risk be Rtrue(f) = E(x,y) [l(f(x), y)]. Then with probability at least 1\u2212 3\u03b4 over (Dt,\u03c9t), and C and Q2 defined as previously\nRtrue(ft+1)\u2212Rtrue(f\u2217) 6 (C \u221a ln(8 \u221a et/\u03b4) + \u221a 2\u03baQ2 \u221a ln(2t/\u03b4) ln(t))L\u221a\nt .\nProof By the Lipschitz continuity of l(\u00b7, y) and Jensen\u2019s Inequality, we have Rtrue(ft+1)\u2212Rtrue(f\u2217) 6 LEx|ft+1(x)\u2212 f\u2217(x)| 6 L \u221a Ex|ft+1(x)\u2212 f\u2217(x)|2 = L\u2016ft+1 \u2212 f\u2217\u20162.\nAgain, \u2016ft+1 \u2212 f\u2217\u20162 can be decomposed as two terms O ( \u2016ft+1 \u2212 ht+1\u201622 ) and O(\u2016ht+1 \u2212 f\u2217\u20162H), which can be bounded similarly as in Theorem 5 (see Corollary 12 in the appendix).\nRemarks. The overall rate of convergence in expectation, which is O(1/t), is indeed optimal. Classical complexity theory (see, e.g. reference in [16]) shows that to obtain -accuracy solution, the number of iterations needed for the stochastic approximation is \u2126(1/ ) for strongly convex case and \u2126(1/ 2) for general convex case. Different from the classical setting of stochastic approximation, our case imposes not one but two\nsources of randomness/stochasticity in the gradient, which intuitively speaking, might require higher order number of iterations for general convex case. However, the variance of the random features only contributes additively to the constant in the final convergence rate. Therefore, our method is still able to achieve the same rate as in the classical setting. Notice that these bounds are achieved by adopting the classical stochastic gradient algorithm, and they may be further refined with more sophisticated techniques and analysis. For example, techniques for reducing variance of SGD proposed in [37], mini-batch and preconditioning [41, 42] can be used to reduce the constant factors in the bound significantly. Theorem 4 also reveals bounds in L\u221e and L2 sense as in Appendix B. The choices of stepsizes \u03b3t and the tuning parameters given in these bounds are only for sufficient conditions and simple analysis; other choices can also lead to bounds in the same order."}, {"heading": "6 Computation, Memory and Statistics Trade-off", "text": "To investigate computation, memory and statistics trade-off, we will fix the desired L2 error in the function estimation to , i.e., \u2016f \u2212 f\u2217\u201622 6 , and work out the dependency of other quantities on . These other quantities include the preprocessing time, the number of samples and random features (or rank), the number of iterations of each algorithm, and the computational cost and memory requirement for learning and prediction. We assume that the number of samples, n, needed to achieve the prescribed error is of the order O(1/ ), the same for all methods. Furthermore, we make no other regularity assumption about margin properties or the kernel matrix such as fast spectrum decay. Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].\nWe will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystro\u0308m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystro\u0308m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively. The comparisons are summarized below in Table. 21\nFrom Table 2, one can see that our method, r-SDCA, r-Pegasos, r-SBMD and r-RBCD achieve the best dependency on the dimension, d, of the data up to a log factor. However, often one is interested in increasing the number of random features as more data points are observed to obtain a better generalization ability, e.g., in streaming setting. Then special procedures need to be designed for updating the r-SDCA, rPegasos, r-SBMD and r-RBCD solutions, which is not clear how to do easily and efficiently with theoretical\n1We only considered general kernel algorithms in this section. For some specific loss functions, e.g., hinge-loss, there are algorithms proposed to achieve better memory saving with extra training cost, such as support vector reduction technique [40].\nguarantees. As a more refined comparison, our algorithm is also the cheapest in terms of per training iteration computation and memory requirement. We list the computational and memory requirements at a particular iteration t < n for these five algorithms to achieve error in Table 3."}, {"heading": "7 Experiments", "text": "We show that our method compares favorably to other scalable kernel methods in medium scale datasets, and neural nets in large scale datasets. Below is a summary of the datasets used. A \u201cyes\u201d for the last column means that virtual examples (random cropping and mirror imaging of the original pictures) are generated for training. K-ridge stands for kernel ridge regression; GPR stands for Gaussian processes regression; K-SVM stands for kernel SVM; K-logistic stands for kernel logistic regression.\nExperiment settings. We first justify the doubly stochastic algorithm for Gaussian processes regression on dataset (1), comparing with NORMA. The dataset is medium size, so that the closed-form for posterior is tractable. For the large-scale datasets (2) \u2014 (5), we compare with the first seven algorithms for solving kernel methods discussed in Table 2. For the algorithms based on low rank kernel matrix approximation and random features, i.e., pegasos and SDCA, we set the rank r or number of random features r to be 28. We use the same batch size for both our algorithms and the competitors. We adopted two stopping criteria for different purposes. We first stopped the algorithms when they pass through the entire dataset once (SC1). This stopping criterion is designed for justifying our motivation. By investigating the performances of these algorithms with different levels of random feature approximations but the same number of training samples, we could identify that the bottleneck of the performances of the vanilla methods with explicit feature will be their approximation ability. To further demonstrate the advantages of the proposed algorithm in computational cost, we also conduct experiments on datasets (3) \u2013 (5) running the competitors within the same time budget as the proposed algorithm (SC2). We do not count the preprocessing time of Nystro\u0308m\u2019s method for n-Pegasos and n-SDCA, though it takes substantial amount of time. The algorithms are executed on the machine with AMD 16 2.4GHz Opteron CPUs and 200G memory. It should be noticed that this gives advantage to NORMA and k-SDCA which could save all the data in the memory. For fairness, we also record\nThree models to compare\nr l t r\nThree models to compare\nas many random features as the memory allowed. For datasets (6) \u2014 (8), we compare with neural nets for images (\u201cjointly-trained\u201d). In order to directly compare the performance of nonlinear classifiers rather than feature learning abilities, we also use the convolution layers of a trained neural net to extract features, then apply our algorithm and a nonlinear neural net on top to learn classifiers (\u201cfixed\u201d). The structures of these neural nets in Figure 3. For datasets (9) and (10), we compare with the neural net described in [30] and use exactly the same input. In all the experiments, we select the batch size so that for each update, the computation resources can be utilized efficiently."}, {"heading": "7.1 Kernel Ridge Regression", "text": "In this section, we compare our approach with alternative algorithms for kernel ridge regression on 2D synthetic dataset. The data are generated by\ny = cos(0.5\u03c0\u2016x\u20162) exp(\u22120.1\u03c0\u2016x\u20162) + 0.1e where x \u2208 [\u22125, 5]2 and e \u223c N (0, 1). We use Gaussian RBF kernel with kernel bandwidth \u03c3 chosen to be 0.1 times the median of pairwise distances between data points (median trick). The regularization parameter \u03bd is set to be 10\u22126. The batch size and feature block are set to be 210.\nThe results are shown in Figure 4. In Figure 4(1), we plot the optimal functions generating the data. We justify our proof of the convergence rate in Figure 4(2). The blue dotted line is a convergence rate of 1/t as a guide. f\u0302t denotes the average solution after t-iteration, i.e., f\u0302t(x) = 1 t \u2211t i=1 fi(x). It could be seen that our algorithm indeed converges in the rate of O(1/t). In Figure 4 (3), we compare the first seven algorithms listed in the Table 2 for solving the kernel ridge regression.\nThe comparison on synthetic dataset demonstrates the advantages of our algorithm clearly. Our algorithm achieves comparable performance with NORMA, which uses full kernel, in similar time but less memory cost. The pegasos and SDCA using 28 random or Nystro\u0308m features perform worse."}, {"heading": "7.2 Gaussian Processes Regression", "text": "As we introduced in Section. (4), the mean and variance of posterior of Gaussian processes for regression problem can be formulated as solutions to some convex optimization problems. We conduct experiments on synthetic dataset for justification. Since the task is computing the posterior, we evaluate the performances by comparing the solutions to the posterior mean and variance, denoted as fgp and \u03c3 2 gp, obtained by closedform (9). We select 211 data from the same model in previous section for training and 210 data for testing, so that the closed-form of posterior is tractable. We use Gaussian RBF kernel with kernel bandwidth \u03c3 chosen by median trick. The noise level \u03c32 is set to be 0.1. The batch size is set to be 64 and feature block is set to be 512.\nWe compared the doubly stochastic algorithm with NORMA. The results are shown in Figure 5. Both the doubly stochastic algorithm and NORMA converge to the posterior, and our algorithm achieves comparable performance with NORMA in approximating both the mean and variance."}, {"heading": "7.3 Kernel Support Vector Machine", "text": "We evaluate our algorithm solving kernel SVM on three datasets (3)\u2013(5) comparing with other several algorithms listed in Table 2 using stopping criteria SC1 and SC2.\nAdult. We use Gaussian RBF kernel with kernel bandwidth obtained by median trick. The regularization parameter \u03bd is set to be 1/(100n) where n is the number of training samples. We set the batch size to be 26 and feature block to be 25. After going through the whole dataset one pass, the best error rate is achieved by NORMA and k-SDCA which is 15% while our algorithm achieves comparable result 15.3%. The performances are illustrated in Figure 6(1). Under the same time budget, all the algorithms perform similarly in Figure 6(4). The reason of flat region of r-pegasos, NORMA and the proposed method on this dataset is that Adult dataset is unbalanced. There are about 24% positive samples while 76% negative samples.\nMNIST 8M 8 vs. 6. We first reduce the dimension to 50 by PCA and use Gaussian RBF kernel with kernel bandwidth \u03c3 = 9.03 obtained by median trick. The regularization parameter \u03bd is set to be 1/n where n is the number of training samples. We set the batch size to be 210 and feature block to be 28. The results are shown in Figure 6(2) and (5) under SC1 and SC2 respectively. Under both these two stopping criteria, our algorithm achieves the best test error 0.26% using similar training time.\nForest. We use Gaussian RBF kernel with kernel bandwidth obtained by median trick. The regularization parameter \u03bd is set to be 1/n where n is the number of training samples. We set the batch size to be 210 and feature block to be 28. In Figure 6(3), we shows the performances of all algorithms using SC1. NORMA and k-SDCA achieve the best error rate, which is 10%, while our algorithm achieves around 15%, but still much better than the pegasos and SDCA with 28 features. In the same time budget, the proposed algorithm performs better than all the alternatives except NORMA in Figure 6(6).\nAs seen from the performance of pegasos and SDCA on Adult and MNIST, using fewer features does not deteriorate the classification error. This might be because there are cluster structures in these two binary classification datasets. Thus, they prefer low rank approximation rather than full kernel. Different from\nthese two datasets, in the forest dataset, algorithms with full kernel, i.e., NORMA and k-SDCA, achieve best performance. With more random features, our algorithm performs much better than pegasos and SDCA under both SC1 and SC2. Our algorithm is preferable for this scenario, i.e., huge dataset with sophisticated decision boundary. Although utilizing full kernel could achieve better performance, the computation and memory requirement for the kernel on huge dataset are costly. To learn the sophisticated boundary while still considering the computational and memory cost, we need to efficiently approximate the kernel in O( 1 ) with O(n) random features at least. Our algorithm could handle so many random features efficiently in both computation and memory cost, while for pegasos and SDCA such operation is prohibitive."}, {"heading": "7.4 Classification Comparisons to Convolution Neural Networks", "text": "We also compare our algorithm with the state-of-the-art neural network. In these experiments, the block size is set to be O(104). Compared to the number of samples, O(108), this block size is reasonable.\nMNIST 8M. In this experiment, we compare to a variant of LeNet-5 [32], where all tanh units are replaced with rectified linear units. We also use more convolution filters and a larger fully connected layer. Specifically, the first two convolutions layers have 16 and 32 filters, respectively, and the fully connected layer contains 128 neurons. We use kernel logistic regression for the task. We extract features from the last max-pooling layer with dimension 1568, and use Gaussian RBF kernel with kernel bandwidth \u03c3 equaling to four times the median pairwise distance. The regularization parameter \u03bd is set to be 0.0005.\nThe result is shown in Figure 7(1). As expected, the neural net with pre-learned features is faster to train than the jointly-trained one. However, our method is much faster compared to both methods. In addition, it achieves a lower error rate (0.5%) compared to the 0.6% error provided by the neural nets.\nMolecular property prediction From molecular structure to molecular property\n33\nCIFAR 10. In this experiment, we compare to a neural net with two convolution layers (after contrast normalization and max-pooling layers) and two local layers that achieves 11% test error2 on CIFAR 10 [28]. The features are extracted from the top max-pooling layer from a trained neural net with 2304 dimension. We use kernel logistic regression for this problem. The kernel bandwidth \u03c3 for Gaussian RBF kernel is again four times the median pairwise distance. The regularization parameter \u03bd is set to be 0.0005. We also perform a PCA (without centering) to reduce the dimension to 256 before feeding to our method.\nThe result is shown in Figure 7(2). The test error for our method drops significantly faster in the earlier phase, then gradually converges to that achieved by the neural nets. Our method is able to produce the same performance within a much restricted time budget.\nImageNet. In this experiment, we compare our algorithm with the neural nets on the ImageNet 2012 dataset, which contains 1.3 million color images from 1000 classes. Each image is of size 256 \u00d7 256, and we randomly crop a 240 \u00d7 240 region with random horizontal flipping. The jointly-trained neural net is Alex-net [29]. The 9216 dimension features for our classifier and fixed neural net are from the last pooling layer of the jointly-trained neural net. The kernel bandwidth \u03c3 for Gaussian RBF kernel is again four times the median pairwise distance. The regularization parameter \u03bd is set to be 0.0005.\nTest error comparisons are shown in Figure 7(3). Our method achieves a test error of 44.5% by further max-voting of 10 transformations of the test set while the jointly-trained neural net arrives at 42% (without variations in color and illumination). At the same time, fixed neural net can only produce an error rate of 46% with max-voting. There may be some advantages to train the network jointly such that the layers work together to achieve a better performance. Although there is still a gap to the best performance by the jointly-trained neural net, our method comes very close with much faster convergence rate. Moreover, it achieves superior performance than the neural net with pre-learned features, both in accuracy and speed."}, {"heading": "7.5 Regression Comparisons to Neural Networks", "text": "We test our algorithm for kernel ridge regression with neural network proposed in [30] on two large-scale real-world regression datasets, (9) and (10) in Table 4. To our best knowledge, this is the first comparison between kernel ridge regression and neural network on the dataset MolecularSpace.\n2The specification is at https://code.google.com/p/cuda-convnet/\nQuantumMachine. In this experiment, we use the same binary representations converted based on random Coulomb matrices as in [30]. We first generate a set of randomly sorted coulomb matrices for each molecule. And then, we break each dimension of the Coulomb matrix apart into steps and convert them to the binary predicates. Predictions are made by taking average of all prediction made on various Coulomb matrices of the same molecule. The procedure is illustrated in Figure. 8. For this experiment, 40 sets of randomly permuted matrices are generated for each training example and 20 for each test example. We use Gaussian kernel with kernel bandwidth \u03c3 = 60 obtained by median trick. The batch size is set to be 50000 and the feature block is 211. The total dimension of random features is 220.\nThe results are shown in Figure 7(4). In QuantumMachine dataset, our method achieves Mean Absolute Error (MAE) of 2.97 kcal/mole, outperforming neural nets results, 3.51 kcal/mole. Note that this result is already close to the 1 kcal/mole required for chemical accuracy.\nMolecularSpace. In this experiment, the task is to predict the power conversion efficiency (PCE) of the molecule. This dataset of 2.3 million molecular motifs is obtained from the Clean Energy Project Database. We use the same feature representation as for \u201cQuantumMachine\u201d dataset [30]. We set the kernel bandwidth of Gaussian RBF kernel to be 290 by median trick. The batch size is set to be 25000 and the feature block is 211. The total dimension of random features is 220.\nThe results are shown in Figure 7(5). It can be seen that our method is comparable with neural network on this 2.3 million dataset."}, {"heading": "8 Discussion", "text": "Our work contributes towards making kernel methods scalable for large-scale datasets. Specifically, by introducing artificial randomness associated with kernels besides the random data samples, we propose doubly stochastic functional gradient for kernel machines which makes the kernel machines efficient in both computation and memory requirement. Our algorithm successfully reduces the memory requirement of kernel machines from O(dn) to O(n). Meanwhile, we also show that our algorithm achieves the optimal rate of convergence, O(1/t), for strongly convex stochastic optimization. We compare our algorithm on both classification and regression problems with the state-of-the-art neural networks as well as some other competing algorithms for kernel methods on several large-scale datasets. With our efficient algorithm, kernel methods could perform comparable to sophisticated-designed neural network empirically.\nThe theoretical analysis, which provides the rate of convergence independent to the dimension, is also highly non-trivial. It twists martingale techniques and the vanilla analysis for stochastic gradient descent and provides a new perspective for analyzing optimization in infinite-dimensional spaces, which could be of independent interest. It should be pointed out that although we applied the algorithm to many kernel machines even with non-smooth loss functions, our current proof relies on the Lipschitz smoothness of the loss function. Extending the guarantee to non-smooth loss function will be one interesting future work.\nAnother key property of our method is its simplicity and ease of implementation which makes it versatile and easy to be extened in various aspects. It is straightforward to replace the sampling strategy for random features with Fastfood [7] which enjoys the efficient computational cost, or Quasi-Monte Carlo sampling [43], data-dependent sampling [47] which enjoys faster convergence rate with fewer generated features. Meanwhile, by back-propogation trick, we could refine the random features by adapting their weights for better performance [36]."}, {"heading": "Acknowledgement", "text": "M.B. is supported in part by NSF grant CCF-1101283, AFOSR grant FA9550-09-1-0538, a Microsoft Faculty Fellowship, and a Raytheon Faculty Fellowship. L.S. is supported in part by NSF IIS-1116886, NSF/NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, and a Raytheon Faculty Fellowship."}, {"heading": "A Convergence Rate", "text": "We first provide specific bounds and detailed proofs for the two error terms appeared in Theorem 4 and Theorem 5.\nA.1 Error due to random features\nLemma 7 We have (i) For any x \u2208 X , EDt,\u03c9t [|ft+1(x)\u2212 ht+1(x)|2] 6 B21,t+1 := 4M2(\u03ba+ \u03c6)2 \u2211t i=1 |ait|2.\n(ii) For any x \u2208 X , with probability at least 1\u2212 \u03b4 over (Dt,\u03c9t), |ft+1(x)\u2212 ht+1(x)|2 6 B22,t+1 := 2M2(\u03ba+ \u03c6)2 ln ( 2\n\u03b4 ) t\u2211 i=1 |ait|2\nProof Let Vi(x) = Vi(x;Di,\u03c9i) := ait (\u03b6i(x)\u2212 \u03bei(x)). Since Vi(x) is a function of (Di,\u03c9i) and EDi,\u03c9i [ Vi(x)|\u03c9i\u22121 ] = aitEDi,\u03c9i [ \u03b6i(x)\u2212 \u03bei(x)|\u03c9i\u22121 ] = aitEDi,\u03c9i\u22121 [ E\u03c9i [ \u03b6i(x)\u2212 \u03bei(x)|\u03c9i\u22121 ]] = 0, we have that {Vi(x)} is a martingal difference sequence. Further note that |Vi(x)| 6 ci = 2M(\u03c6+ \u03ba)|ait|.\nThen by Azuma\u2019s Inequality, for any > 0,\nPr Dt,\u03c9t { | t\u2211 i=1 Vi(x)| > } 6 2 exp { \u2212 2 2\u2211t i=1 c 2 i } which is equivalent as\nPr Dt,\u03c9t  ( t\u2211 i=1 Vi(x) )2 > ln(2/\u03b4) t\u2211 i=1 c2i /2  6 \u03b4. Moreover,\nEDt,\u03c9t ( t\u2211 i=1 Vi(x) )2 = \u222b \u221e 0 Pr Dt,\u03c9t  ( t\u2211 i=1 Vi(x) )2 >  d = \u222b \u221e 0 2 exp { \u2212 2 \u2211t i=1 c 2 i } d = t\u2211 i=1 c2i\nSince ft+1(x)\u2212 ht+1(x) = \u2211t i=1 Vi(x), we immediately obtain the two parts of the lemma.\nLemma 8 Suppose \u03b3i = \u03b8 i (1 6 i 6 t) and \u03b8\u03bd \u2208 (1, 2) \u222a Z+. Then we have\n(1) |ait| 6 \u03b8t . Consequently, \u2211t i=1(a i t) 2 6 \u03b8 2 t . (2) \u2211t i=1 \u03b3i|ait| 6 { \u03b82(ln(t)+1) t , if \u03b8\u03bd \u2208 [1, 2), \u03b82\nt , if \u03b8\u03bd \u2208 [2,+\u221e) \u2229 Z+ .\nProof (1) follows by induction on i. |att| 6 \u03b8t is trivially true. We have\n|ait| = |ai+1t \u03b3i \u03b3i+1 (1\u2212 \u03bd\u03b3i+1)| = i+ 1 i |1\u2212 \u03bd\u03b8 i+ 1 | \u00b7 |ai+1t | = | i+ 1\u2212 \u03bd\u03b8 i | \u00b7 |ai+1t |.\nWhen \u03bd\u03b8 \u2208 (1, 2), i \u2212 1 < i + 1 \u2212 \u03bd\u03b8 < i for any i > 1, so |ait| < |ai+1t | 6 \u03b8t . When \u03bd\u03b8 \u2208 Z+, if i > \u03bd\u03b8 \u2212 1, then |ait| < |ai+1t | 6 \u03b8t ; if i 6 \u03bd\u03b8 \u2212 1, then |a i t| = 0. For (2), when \u03b8\u03bd \u2208 [1, 2),\nt\u2211 i=1 \u03b3t|ait| = t\u2211 i=1 \u03b82 i2 \u00b7 i+ 1\u2212 \u03b8\u03bd i+ 1 \u00b7 \u00b7 \u00b7 t\u2212 \u03b8\u03bd t 6 t\u2211 i=1 \u03b82 i2 \u00b7 i i+ 1 \u00b7 \u00b7 \u00b7 t\u2212 1 t 6 t\u2211 i=1 \u03b82 it 6 \u03b82(ln(t) + 1) t .\nWhen \u03b8\u03bd \u2208 Z+ and 2 6 \u03b8\u03bd 6 t, t\u2211 i=1 \u03b3t|ait| = t\u2211 i=2 \u03b82 i2 \u00b7 i+ 1\u2212 \u03b8\u03bd i+ 1 \u00b7 \u00b7 \u00b7 t\u2212 \u03b8\u03bd t 6 t\u2211 i=1 \u03b82 i2 \u00b7 i\u2212 1 i+ 1 \u00b7 \u00b7 \u00b7 t\u2212 2 t 6 t\u2211 i=2 \u03b82(i\u2212 1) it(t\u2212 1) 6 \u03b82 t .\nA.2 Error due to random data\nLemma 9 Assume l\u2032(u, y) is L-Lipschitz continous in terms of u \u2208 R. Let f\u2217 be the optimal solution to our target problem. Then\n(i) If we set \u03b3t = \u03b8 t with \u03b8 such that \u03b8\u03bd \u2208 (1, 2) \u222a Z+, then EDt,\u03c9t [ \u2016ht+1 \u2212 f\u2217\u20162H ] 6 Q21 t ,\nwhere\nQ1 = max { \u2016f\u2217\u2016H , Q0 + \u221a Q20 + (2\u03b8\u03bd \u2212 1)(1 + \u03b8\u03bd)2\u03b82\u03baM2\n2\u03bd\u03b8 \u2212 1\n} , Q0 = 2 \u221a 2\u03ba1/2(\u03ba+ \u03c6)LM\u03b82.\nParticularly, if \u03b8\u03bd = 1, we have Q1 6 max { \u2016f\u2217\u2016H , 4 \u221a 2((\u03ba+ \u03c6)L+ \u03bd) \u00b7 \u03ba 1/2M \u03bd2 } .\n(ii) If we set \u03b3t = \u03b8 t with \u03b8 such that \u03b8\u03bd \u2208 Z+ and t > \u03b8\u03bd, then with probability at least 1\u22122\u03b4 over (D t,\u03c9t),\n\u2016ht+1 \u2212 f\u2217\u20162H 6 Q 2 2\nln(2t/\u03b4) ln(t)\nt .\nwhere\nQ2 = max { \u2016f\u2217\u2016H , Q0 + \u221a Q20 + \u03baM 2(1 + \u03b8\u03bd)2(\u03b82 + 16\u03b8/\u03bd) } , Q0 = 4 \u221a 2\u03ba1/2M\u03b8(8 + (\u03ba+ \u03c6)\u03b8L).\nParticularly, if \u03b8\u03bd = 1, we have Q2 6 max { \u2016f\u2217\u2016H , 8 \u221a 2((\u03ba+ \u03c6)L+ 9\u03bd) \u00b7 \u03ba 1/2M \u03bd2 } .\nProof For the sake of simple notations, let us first denote the following three different gradient terms, which are\ngt = \u03bet + \u03bdht = l \u2032(ft(xt), yt)k(xt, \u00b7) + \u03bdht, g\u0302t = \u03be\u0302t + \u03bdht = l \u2032(ht(xt), yt)k(xt, \u00b7) + \u03bdht,\ng\u0304t = EDt [g\u0302t] = EDt [l\u2032(ht(xt), yt)k(xt, \u00b7)] + \u03bdht. Note that by our previous definition, we have ht+1 = ht \u2212 \u03b3tgt,\u2200t > 1.\nDenote At = \u2016ht \u2212 f\u2217\u20162H. Then we have At+1 = \u2016ht \u2212 f\u2217 \u2212 \u03b3tgt\u20162H\n= At + \u03b3 2 t \u2016gt\u2016 2 H \u2212 2\u03b3t\u3008ht \u2212 f\u2217, gt\u3009H = At + \u03b3 2 t \u2016gt\u2016 2 H \u2212 2\u03b3t\u3008ht \u2212 f\u2217, g\u0304t\u3009H + 2\u03b3t\u3008ht \u2212 f\u2217, g\u0304t \u2212 g\u0302t\u3009H + 2\u03b3t\u3008ht \u2212 f\u2217, g\u0302t \u2212 gt\u3009H\nBecause of the strongly convexity of (1) and optimality condition, we have\n\u3008ht \u2212 f\u2217, g\u0304t\u3009H > \u03bd \u2016ht \u2212 f\u2217\u20162H\nHence, we have\nAt+1 6 (1\u2212 2\u03b3t\u03bd)At + \u03b32t \u2016gt\u2016 2 H + 2\u03b3t\u3008ht \u2212 f\u2217, g\u0304t \u2212 g\u0302t\u3009H + 2\u03b3t\u3008ht \u2212 f\u2217, g\u0302t \u2212 gt\u3009H,\u2200t > 1 (12)\nProof for (i): Let us denote Mt = \u2016gt\u20162H, Nt = \u3008ht \u2212 f\u2217, g\u0304t \u2212 g\u0302t\u3009H, Rt = \u3008ht \u2212 f\u2217, g\u0302t \u2212 gt\u3009H. We first show that Mt,Nt,Rt are bounded. Specifically, we have for t > 1,\n(1) Mt 6 \u03baM2(1 + \u03bdct)2, where ct = \u221a\u2211t\u22121 i,j=1 |ait\u22121| \u00b7 |a j t\u22121| for t > 2 and c1 = 0;\n(2) EDt,\u03c9t [Nt] = 0; (3) EDt,\u03c9t [Rt] 6 \u03ba1/2LB1,t \u221a EDt\u22121,\u03c9t\u22121 [At], where B21,t := 4M2(\u03ba+\u03c6)2 \u2211t\u22121 i=1 |ait\u22121|2 for t > 2 and B1,1 =\n0;\nWe prove these results separately in Lemma 10 below. Let us denote et = EDt\u22121,\u03c9t\u22121 [At], given the above bounds, we arrive at the following recursion,\net+1 6 (1\u2212 2\u03b3t\u03bd)et + \u03baM2\u03b32t (1 + \u03bdct)2 + 2\u03ba1/2L\u03b3tB1,t \u221a et. (13)\nWhen \u03b3t = \u03b8/t with \u03b8 such that \u03b8\u03bd \u2208 (1, 2) \u222a Z+, from Lemma 8, we have |ait| 6 \u03b8t ,\u22001 6 i 6 t. Consequently, ct 6 \u03b8 and B21,t 6 4M 2(\u03ba + \u03c6) \u03b8 2\nt\u22121 . Applying these bounds leads to the refined recursion as follows\net+1 6\n( 1\u2212 2\u03bd\u03b8\nt\n) et + \u03baM 2 \u03b8 2\nt2 (1 + \u03bd\u03b8)2 + 2\u03ba1/2L\n\u03b8\nt\n\u221a 4M2(\u03ba+ \u03c6)2 \u03b82\nt\u2212 1 \u221a et\nthat can be further written as\net+1 6\n( 1\u2212 2\u03bd\u03b8\nt\n) et +\n\u03b21 t \u221a et t + \u03b22 t2 ,\nwhere \u03b21 = 4 \u221a 2\u03ba1/2LM(k + \u03c6)\u03b82 and \u03b22 = \u03baM 2(1 + \u03bd\u03b8)2\u03b82. Invoking Lemma 14 with \u03b7 = 2\u03b8\u03bd > 1, we obtain\net 6 Q21 t ,\nwhere Q1 = max { \u2016f\u2217\u2016H , Q0+ \u221a Q20+(2\u03b8\u03bd\u22121)(1+\u03b8\u03bd)2\u03b82\u03baM2 2\u03bd\u03b8\u22121 } , and Q0 = 2 \u221a 2\u03ba1/2(\u03ba+ \u03c6)LM\u03b82.\nProof for (ii): Cumulating equations (12) with i = 1, . . . t, we end up with the following inequality At+1 6 \u220ft i=1(1\u2212 2\u03b3i\u03bd)A1 + 2 \u2211t i=1 \u03b3i \u220ft j=i+1(1\u2212 2\u03bd\u03b3j)\u3008hi \u2212 f\u2217, g\u0304i \u2212 g\u0302i\u3009H\n+2 \u2211t i=1 \u03b3i \u220ft j=i+1(1\u2212 2\u03bd\u03b3j)\u3008hi \u2212 f\u2217, g\u0302i \u2212 gi\u3009H + \u2211t i=1 \u03b3 2 i \u220ft j=i+1(1\u2212 2\u03bd\u03b3j) \u2016gi\u2016 2 H\n(14)\nLet us denote bit = \u03b3i \u220ft j=i+1(1\u2212 2\u03bd\u03b3j), 1 6 i 6 t, the above inequality is equivalent as\nAt+1 6 t\u220f i=1 (1\u2212 2\u03b3i\u03bd)A1 + t\u2211 i=1 \u03b3ib i tMi + 2 t\u2211 i=1 bitNi + 2 t\u2211 i=1 bitRi\nWe first show that\n(4) for any 0 < \u03b4 < 1/e and t > 4, with probability 1\u2212 \u03b4 over (Dt,\u03c9t),\u2211t i=1 b i tNi 6 2 max { 4\u03ba1/2M \u221a\u2211t i=1(b i t) 2Ai, maxi |bit| \u00b7 C0 \u221a ln(ln(t)/\u03b4) }\u221a ln(ln(t)/\u03b4),\nwhere C0 = 4 max16i6tMi\n\u03bd .\n(5) for any \u03b4 > 0, with probability 1\u2212 \u03b4 over (Dt,\u03c9t),\u2211t i=1 b i tRi 6 \u2211t i=1 b i t\u03ba 1/2LB\u03022,i \u221a Ai,\nwhere B\u030222,i = 2M 2(\u03ba+ \u03c6)2 ln ( 2t \u03b4 )\u2211i\u22121 j=1 |a j i\u22121|2.\nAgain, the proofs of these results are given separately in Lemma 10. Applying the above bounds leads to the refined recursion as follows,\nAt+1 6 t\u220f i=1 (1\u2212 2\u03b3i\u03bd)A1 + t\u2211 i=1 \u03b3ib i tMi + 2 t\u2211 i=1 bit\u03ba 1/2LB2,i \u221a Ai\n+4 max 4\u03ba1/2M \u221a\u221a\u221a\u221a t\u2211\ni=1\n(bit) 2Ai, max i |bit| \u00b7 C0\n\u221a ln(ln(t)/\u03b4) \u221aln(ln(t)/\u03b4) with probability 1\u22122\u03b4. When \u03b3t = \u03b8/t with \u03b8 such that \u03b8\u03bd \u2208 Z+, with similar reasons in Lemma 8, we have |bit| 6 \u03b8t , 1 6 i 6 t and also we have \u220ft i=1(1\u2212 2\u03b3i\u03bd) = \u220f\u03b8\u03bd\u22121 i=1 (1\u2212 2 \u03b8\u03bd i ) \u220ft i=\u03b8\u03bd+1(1\u2212 2 \u03b8\u03bd i )(1\u2212 2\n\u03b8\u03bd \u03b8\u03bd ) = 0, and\u2211t\ni=1 \u03b3ib i t 6\n\u03b82\nt . Therefore, we can rewrite the above recursion as\nAt+1 6 \u03b21 t\n+ \u03b22 \u221a ln(2t/\u03b4) \u00b7 t\u2211 i=1 \u221a Ai t \u221a i + \u03b23 \u221a ln(ln(t)/\u03b4)\n\u221a\u2211t i=1Ai\nt + \u03b24 ln(ln(t/\u03b4))\n1 t (15)\nwhere \u03b21 = \u03baM 2(1+\u03bd\u03b8)2\u03b82, \u03b22 = 2 \u221a 2\u03ba1/2LM(\u03ba+\u03c6)\u03b82, \u03b23 = 16\u03ba 1/2M\u03b8, \u03b24 = 16\u03baM 2(1+\u03b8\u03bd)2\u03b8/\u03bd. Invoking Lemma 15, we obtain\nAt+1 6 Q22 ln(2t/\u03b4) ln 2(t)\nt ,\nwith the specified Q2.\nLemma 10 In this lemma, we prove the inequalities (1)\u2013(5) in Lemma 9.\nProof Given the definitions of Mt,Nt,Rt in Lemma 9, we have (1) Mt 6 \u03baM2(1 + \u03bd \u221a\u2211t\u22121 i,j=1 |ait\u22121| \u00b7 |a j t\u22121|)2;\nThis is because Mt = \u2016gt\u20162H = \u2016\u03bet + \u03bdht\u20162H 6 (\u2016\u03bet\u2016H + \u03bd\u2016ht\u2016H)2. We have \u2016\u03bet\u2016H = \u2016l\u2032(ft(xt), yt)k(xt, \u00b7)\u2016H 6 \u03ba1/2M, and\n\u2016ht\u20162H = t\u22121\u2211 i=1 t\u22121\u2211 j=1 ait\u22121a j t\u22121l \u2032(fi(xi), yi)l \u2032(fj(xj), yj)k(xi, xj)\n6 \u03baM2 t\u22121\u2211 i=1 t\u22121\u2211 j=1 |ait\u22121| \u00b7 |a j t\u22121|.\n(2) EDt,\u03c9t [Nt] = 0; This is because Nt = \u3008ht \u2212 f\u2217, g\u0304t \u2212 g\u0302t\u3009H,\nEDt,\u03c9t [Nt] = EDt\u22121,\u03c9t [ EDt [ \u3008ht \u2212 f\u2217, g\u0304t \u2212 g\u0302t\u3009H|Dt\u22121,\u03c9t ]] = EDt\u22121,\u03c9t [\u3008ht \u2212 f\u2217,EDt [g\u0304t \u2212 g\u0302t]\u3009H] = 0.\n(3) EDt,\u03c9t [Rt] 6 \u03ba1/2LB1,t \u221a EDt\u22121,\u03c9t\u22121 [At], where B21,t := 4M2(\u03ba+ \u03c6)2 \u2211t\u22121 i=1 |ait\u22121|2;\nThis is because Rt = \u3008ht \u2212 f\u2217, g\u0302t \u2212 gt\u3009H, EDt,\u03c9t [Rt] = EDt,\u03c9t [\u3008ht \u2212 f\u2217, g\u0302t \u2212 gt\u3009H]\n= EDt,\u03c9t [\u3008ht \u2212 f\u2217, [l\u2032(ft(xt), yt)\u2212 l\u2032(ht(xt), yt)]k(xt, \u00b7)\u3009H] 6 EDt,\u03c9t [|l\u2032(ft(xt), yt)\u2212 l\u2032(ht(xt), yt)| \u00b7 \u2016k(xt, \u00b7)\u2016H \u00b7 \u2016ht \u2212 f\u2217\u2016H] 6 \u03ba1/2L \u00b7 EDt,\u03c9t [|ft(xt)\u2212 ht(xt)| \u2016ht \u2212 f\u2217\u2016H]\n6 \u03ba1/2L \u221a EDt,\u03c9t |ft(xt)\u2212 ht(xt)|2 \u221a EDt,\u03c9t \u2016ht \u2212 f\u2217\u20162H\n6 \u03ba1/2LB1,t \u221a EDt\u22121,\u03c9t\u22121 [At]\nwhere the first and third inequalities are due to Cauchy\u2013Schwarz Inequality and the second inequality is due to L-Lipschitz continuity of l\u2032(\u00b7, \u00b7) in the first parameter, and the last step is due to Lemma 7 and the definition of At.\n(4) for any 0 < \u03b4 < 1/e and t > 4, with probability at least 1\u2212 \u03b4 over (Dt,\u03c9t),\u2211t i=1 b i tNi 6 2 max { 4\u03ba1/2M \u221a\u2211t i=1(b i t) 2Ai, maxi |bit| \u00b7 C0 \u221a ln(ln(t)/\u03b4) }\u221a ln(ln(t)/\u03b4),\nwhere C0 = 4 max16i6tMi\n\u03bd . This result follows directly from Lemma 3 in [31]. Let us define di = di(Di,\u03c9i) := bitNi = bit\u3008hi \u2212 f\u2217, g\u0304i \u2212 g\u0302i\u3009H, 1 6 i 6 t, we have\n\u2022 {di}ti=1 is martingale difference sequence since EDi,\u03c9i [ Ni|Di\u22121,\u03c9i\u22121 ] = 0. \u2022 |di| 6 maxi |bit| \u00b7 C0, with C0 = 4 max16i6tMi\n\u03bd , \u22001 6 i 6 t. \u2022 V ar(di|Di\u22121,\u03c9i\u22121) 6 4\u03baM2|bit|2Ai,\u22001 6 i 6 t.\nPlugging in these specific bounds in Lemma 3 in [Alexander et.al., 2012], which is, Pr (\u2211t i=1 dt > 2 max{2\u03c3t, dmax \u221a ln(1/\u03b4)} \u221a ln(1/\u03b4) ) 6 ln(t)\u03b4.\nwhere \u03c32t = \u2211t i=1 V ari\u22121(di) and dmax = max16i6t |di|, we immediately obtain the above inequality as desired.\n(5) for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 over (Dt,\u03c9t),\u2211t i=1 b i tRi 6 \u2211t i=1 |bit|\u03ba1/2LB\u03022,i \u221a Ai,\nwhere B\u030222,i = 2M 2(\u03ba+ \u03c6)2 ln ( 2t \u03b4 )\u2211i\u22121 j=1 |a j i\u22121|2.\nThis is because, for any 1 6 i 6 t, recall that from analysis in (3), we have Ri 6 \u03ba1/2L|ft(xt)\u2212ht(xt)| \u00b7 \u2016ht \u2212 f\u2217\u2016H, therefore from Lemma 9,\nPr(bitRi 6 \u03ba1/2L|bit|B\u03022,i \u221a Ai) > Pr(|fi(xi)\u2212 hi(xi)|2 6 B\u030222,i) > 1\u2212 \u03b4/t.\nTaking the sum over i, we therefore get Pr( \u2211t i=1 b i tRi 6 \u2211t i=1 |bit|\u03ba1/2LB2,i \u221a Ai) > 1\u2212 \u03b4.\nApplying these lemmas immediately gives us Theorem 4 and Theorem 5, which implies pointwise distance between the solution ft+1(\u00b7) and f\u2217(\u00b7). Now we prove similar bounds in the sense of L\u221e and L2 distance."}, {"heading": "B L\u221e distance, L2 distance, and generalization bound", "text": "Corollary 11 (L\u221e distance) Theorem 4 also implies a bound in L\u221e sense, namely,\nEDt,\u03c9t \u2016ft+1 \u2212 f\u2217\u20162\u221e 6 2C2 + 2\u03baQ21\nt . Consequently, for the average solution f\u0302t+1(\u00b7) := 1t \u2211t i=1 fi(\u00b7), we also have\nEDt,\u03c9t\u2016f\u0302t+1 \u2212 f\u2217\u20162\u221e 6 (2C2 + 2\u03baQ21)(ln(t) + 1)\nt .\nThis is because \u2016ft+1 \u2212 f\u2217\u2016\u221e = maxx\u2208X |ft+1(x)\u2212 f\u2217(x)| = |ft+1(x\u2217)\u2212 f\u2217(x\u2217)|, where x\u2217 \u2208 X always exists since X is closed and bounded. Note that the result for average solution can be improved without log factor using more sophisticated analysis (see also reference in [31]).\nCorollary 12 (L2 distance) With the choices of \u03b3t in Lemma 9, we have\n(i) EDt,\u03c9t\u2016ft+1 \u2212 f\u2217\u201622 6 2C2+2\u03baQ21 t ,\n(ii) \u2016ft+1 \u2212 f\u2217\u201622 6 C2 ln(8\n\u221a et/\u03b4)+2\u03baQ22 ln(2t/\u03b4) ln\n2(t) t , with probability at least 1\u2212 3\u03b4 over (D t,\u03c9t).\nProof (i) follows directly from Theorem 4. (ii) can be proved as follows. First, we have \u2016ft+1 \u2212 f\u2217\u201622 = Ex|ft+1(x)\u2212 f\u2217(x)|2 6 2Ex|ft+1(x)\u2212 ht+1(x)|2 + 2\u03ba\u2016ht+1 \u2212 f\u2217\u2016H. From Lemma 9, with probability at least 1\u2212 2\u03b4, we have\n\u2016ht+1 \u2212 f\u2217\u20162H 6 Q22 ln(2t/\u03b4) ln 2(t)\nt . (16)\nFrom Lemma 7, for any x \u2208 X , we have\nPr Dt,\u03c9t\n{ |ft+1(x)\u2212 ht+1(x)|2 > 2(\u03ba+ \u03c6)2M2 ln( 2 )\u03b8 2\nt\n} 6 .\nSince C2 = 4(\u03ba+ \u03c6)2M2\u03b82, the above inequality can be writen as\nPr Dt,\u03c9t\n{ |ft+1(x)\u2212 ht+1(x)|2 > C2 ln( 2 )\n2t\n} 6 .\nwhich leads to\nPr x\u223cP(x) Pr Dt,\u03c9t\n{ |ft+1(x)\u2212 ht+1(x)|2 > C2 ln( 2 )\n2t\n} 6 .\nBy Fubini\u2019s theorem and Markov\u2019s inequality, we have\nPr Dt,\u03c9t\n{ Pr\nx\u223cP(x)\n{ |ft+1(x)\u2212 ht+1(x)|2 > C2 ln( 2 )\n2t\n} >\n\u03b4\n} 6 \u03b4.\nFrom the analysis in Lemma 7, we also have that |ft+1(x) \u2212 ht+1(x)| 6 C2. Therefore, with probability at least 1\u2212 \u03b4 over (Dt,\u03c9t), we have\nEx\u223cP(x)[|ft+1(x)\u2212 ht+1(x)|2] 6 C2 ln( 2 )\n2t (1\u2212 \u03b4 ) + C2 \u03b4\nLet = \u03b44t , we have\nEx\u223cP(x)[|ft+1(x)\u2212 ht+1(x)|2] 6 C2\n2t (ln(8t/\u03b4) +\n1 2 ) =\nC2 ln(8 \u221a et/\u03b4)\n2t . (17)\nSumming up equation (17) and (16), we have\n\u2016ft+1 \u2212 f\u2217\u201622 6 C2 ln(8\n\u221a et/\u03b4) + 2\u03baQ22 ln(2t/\u03b4) ln 2(t)\nt as desired.\nFrom the bound on L2 distance, we can immediately get the generalization bound. Theorem 6 (Generalization bound) Let the true risk be Rtrue(f) = E(x,y) [l(f(x), y)]. Then with probability at least 1\u2212 3\u03b4 over (Dt,\u03c9t), and C and Q2 defined as previously\nRtrue(ft+1)\u2212Rtrue(f\u2217) 6 (C \u221a ln(8 \u221a et/\u03b4) + \u221a 2\u03baQ2 \u221a ln(2t/\u03b4) ln(t))L\u221a\nt .\nProof By the Lipschitz continuity of l(\u00b7, y) and Jensen\u2019s Inequality, we have Rtrue(ft+1)\u2212Rtrue(f\u2217) 6 LEx|ft+1(x)\u2212 f\u2217(x)| 6 L \u221a Ex|ft+1(x)\u2212 f\u2217(x)|2 = L\u2016ft+1 \u2212 f\u2217\u20162.\nThen the theorem follows from Corollary 12."}, {"heading": "C Suboptimality", "text": "For comprehensive purposes, we also provide the O(1/t) bound for suboptimality.\nCorollary 13 If we set \u03b3t = \u03b8 t with \u03b8\u03bd = 1, then the average solution f\u0302t+1 := 1 t \u2211t i=1 fi satisfies\nR(EDt,\u03c9t [f\u0302t+1])\u2212R(f\u2217) 6 Q(ln(t) + 1)\nt .\nwhere Q = (4\u03baM2 + 2 \u221a 2\u03ba1/2LM(\u03ba+ \u03c6)Q1)/\u03bd, with Q1 defined as in Lemma 9.\nProof From the anallysis in Lemma 9,we have\n\u3008ht \u2212 f\u2217, g\u0304t\u3009H = 1\n2\u03b3t At \u2212\n1\n2\u03b3t At+1 + \u03b3tMt +Nt +Rt\nInvoking strongly convexity of R(f), we have \u3008ht\u2212f\u2217, g\u0304t\u3009 > R(ht)\u2212R(f\u2217)+ \u03bd2\u2016ht\u2212f\u2217\u2016 2 H. Taking expectaion on both size and use the bounds in last lemma, we have\nEDt,\u03c9t [R(ht)\u2212R(f\u2217)] 6 ( 1 2\u03b3t \u2212 \u03bd 2 )et \u2212 1 2\u03b3t et+1 + \u03b3t\u03baM 2(1 + \u03bdct) 2 + \u03ba1/2LB1,t \u221a et\nAssume \u03b3t = \u03b8 t with \u03b8 = 1 \u03bd , then cumulating the above inequalities leads to\nt\u2211 i=1 EDt,\u03c9t [R(hi)\u2212R(f\u2217)] 6 t\u2211 i=1 \u03b3i\u03baM 2(1 + \u03bdci) 2 + t\u2211 i=1 \u03ba1/2LB1,i \u221a ei\nwhich can be further bounded by t\u2211 i=1 EDt,\u03c9t [R(hi)\u2212R(f\u2217)] 6 t\u2211 i=1 \u03b3i\u03baM 2(1 + \u03bdci) 2 + t\u2211 i=1 \u03ba1/2LB1,i \u221a ei\n6 4\u03baM2\n\u03bd t\u2211 i=1 1 i + 2 \u221a 2\u03ba1/2LM(\u03ba+ \u03c6) \u03bd t\u2211 i=1 \u221a ei i\n6 4\u03baM2\n\u03bd (ln(t) + 1) +\n2 \u221a 2\u03ba1/2LM(\u03ba+ \u03c6)\n\u03bd Q1(ln(t) + 1)\n= Q(ln(t) + 1)\nt\nBy convexity, we have EDt,\u03c9t [R(h\u0302t+1) \u2212 R(f\u2217)] 6 Q(ln(t)+1)t . The corollary then follows from the fact that EDt,\u03c9t [f\u0302t+1] = EDt,\u03c9t [h\u0302t+1] and R(EDt,\u03c9t [h\u0302t+1]) 6 EDt,\u03c9t [R(h\u0302t+1)].\nC.1 Technical lemma for recursion bounds\nLemma 14 Suppose the sequence {\u0393t}\u221et=1 satisfies \u03931 > 0, and \u2200t > 1 \u0393t+1 6 (\n1\u2212 \u03b7 t\n) \u0393t + \u03b21\nt \u221a t\n\u221a \u0393t +\n\u03b22 t2 ,\nwhere \u03b7 > 1, \u03b21, \u03b22 > 0. Then \u2200t > 1,\n\u0393t 6 R\nt , where R = max\n{ \u03931, R 2 0 } , R0 = \u03b21 + \u221a \u03b221 + 4(\u03b7 \u2212 1)\u03b22 2(\u03b7 \u2212 1) .\nProof The proof follows by induction. When t = 1, it always holds true by the definition of R. Assume the conclusion holds true for t with t > 1, i.e., \u0393t 6 Rt , then we have\n\u0393t+1 6 (\n1\u2212 \u03b7 t\n) \u0393t + \u03b21\nt \u221a t\n\u221a \u0393t +\n\u03b22 t2\n= R t \u2212 \u03b7R\u2212 \u03b21\n\u221a R\u2212 \u03b22\nt2 6\nR\nt+ 1 +\nR t(t+ 1) \u2212 \u03b7R\u2212 \u03b21\n\u221a R\u2212 \u03b22\nt2\n6 R t+ 1 \u2212 1 t2\n[ \u2212R+ \u03b7R\u2212 \u03b21 \u221a R\u2212 \u03b22 ] 6 R\nt+ 1 where the last step can be verified as follows.\n(\u03b7 \u2212 1)R\u2212 \u03b21 \u221a R\u2212 \u03b22 = (\u03b7 \u2212 1) [\u221a R\u2212 \u03b21\n2(\u03b7 \u2212 1)\n]2 \u2212 \u03b2 2 1\n4(\u03b7 \u2212 1) \u2212 \u03b22\n> (\u03b7 \u2212 1) [ R0 \u2212\n\u03b21 2(\u03b7 \u2212 1)\n]2 \u2212 \u03b2 2 1\n4(\u03b7 \u2212 1) \u2212 \u03b22 > 0\nwhere the last step follows from the defintion of R0.\nLemma 15 Suppose the sequence {\u0393t}\u221et=1 satisfies\n\u0393t+1 6 \u03b21 t\n+ \u03b22 \u221a ln(2t/\u03b4) \u00b7 t\u2211 i=1 \u221a \u0393i t \u221a i + \u03b23 \u221a ln(ln(t)/\u03b4)\n\u221a\u2211t i=1 \u0393i\nt + \u03b24 ln(ln(t/\u03b4))\n1\nt\nwhere \u03b21, \u03b22, \u03b23, \u03b24 > 0 and \u03b4 \u2208 (0, 1/e). Then \u22001 6 j 6 t(t > 4),\n\u0393j 6 R ln(2t/\u03b4) ln2(t)\nj , where R = max{\u03931, R20}, R0 = 2\u03b22 + 2\n\u221a 2\u03b23 + \u221a (2\u03b22 + 2 \u221a 2\u03b23)2 + \u03b21 + \u03b24.\nProof The proof follows by induction. When j = 1 it is trivial. Let us assume it holds true for 1 6 j 6 t\u22121, therefore,\n\u0393j+1 6 \u03b21 j\n+ \u03b22 \u221a ln(2j/\u03b4) \u00b7 j\u2211 i=1 \u221a \u0393i j \u221a i + \u03b23 \u221a ln(ln(j)/\u03b4)\n\u221a\u2211j i=1 \u0393i\nj + \u03b24 ln(ln(j/\u03b4))\n1\nj\n6 \u03b21 j\n+ \u03b22 \u221a ln(2j/\u03b4)/j \u00b7 j\u2211 i=1\n\u221a R ln(2t/\u03b4) ln2(t)\ni\n+\u03b23 \u221a ln(ln(j)/\u03b4)\n\u221a\u2211j i=1R ln(2t/\u03b4) ln 2(t)/i\nj + \u03b24 ln(ln(j/\u03b4))\n1\nj\n6 \u03b21 j\n+ \u03b22 \u221a ln(2j/\u03b4)/j \u221a R ln(2t/\u03b4) ln2(t)(1 + ln(j))\n+\u03b23 \u221a ln(ln(j)/\u03b4)/j \u221a R ln(2t/\u03b4) ln2(t) \u221a ln(j) + 1 + \u03b24 ln(ln(j/\u03b4)) 1\nj\n6 \u03b21 j\n+ 2\u03b22 \u221a R ln(2t/\u03b4) ln2(t)/j + \u221a 2\u03b23 \u221a R ln(2t/\u03b4) ln2(t)/j + \u03b24 ln(2t/\u03b4) 1\nj\n6 (2\u03b22 + \u221a 2\u03b23) \u221a R ln(2t/\u03b4) ln2(t)\nj + (\u03b21 + \u03b24 ln(2t/\u03b4))\n1\nj\n6 ln(2t/\u03b4) ln2(t)\nj [(2\u03b22 +\n\u221a 2\u03b23) \u221a R+\n\u03b21 2 + \u03b24 2 )\nSince \u221a R > 2\u03b22 +2 \u221a 2\u03b23 + \u221a (2\u03b22 + 2 \u221a 2\u03b23)2 + \u03b21 + \u03b24, we have (2\u03b22 +2 \u221a 2\u03b23) \u221a R+ \u03b212 + \u03b24 2 6 R/2. Hence, \u0393j+1 6 R ln(2t/\u03b4) ln2(t)\nj+1 ."}, {"heading": "D Doubly Stochastic Gradient Algorithm for Posterior Variance", "text": "Operator in Gaussian Process Regression\nAs we show in Section 4, the estimation of the variance of the predictive distribution of Gaussian process for regression problem could be recast as estimating the operator A defined in (10). We first demonstrate that the operator A is the solution to the following optimization problem\nmin A R(A) = 1 2n n\u2211 i=1 \u2016k(xi, \u00b7)\u2212Ak(xi, \u00b7)\u20162H + \u03c32 2n \u2016A\u20162HS\nwhere \u2016 \u00b7 \u2016HS is the Hilbert-Schmidt norm of the operator. The gradient of R(A) with respect to A is\n\u2207R(A) = 1 n n\u2211 i=1 ( (Ak(x, \u00b7)\u2212 k(x, \u00b7))\u2297 k(x, \u00b7) ) + \u03c32 n A = A ( C + \u03c3 2 n I ) \u2212 C\nSet \u2207R(A) = 0, we could obtain the optimal solution, C ( C + \u03c3 2 n I )\u22121\n, exactly the same as (10). To derive the doubly stochastic gradient update for A, we start with stochastic functional gradient of\nR(A). Given xi \u223c P(x), the stochastic functional gradient of R(A) is \u03c8(\u00b7, \u00b7) = A ( C\u0302 + \u03c3 2\nn I\n) \u2212 C\u0302\nwhere C\u0302 = k(xi, \u00b7)\u2297 k(xi, \u00b7) which leads to update At+1 = At \u2212 \u03b3t\u03c8 = ( 1\u2212 \u03c3 2\nn \u03b3t\n) At \u2212 \u03b3t ( AtC\u0302t \u2212 C\u0302t ) . (18)\nWith such update rule, we could show that At+1 = \u2211t i=1,j>i \u03b2 t+1 ij k(xi, \u00b7) \u2297 k(xj , \u00b7) by induction. Let\nA1 = 0, then, A2 = \u03b31k(x1, \u00b7) \u2297 k(x1, \u00b7). Assume at t-th iteration, At = \u2211t\u22121 i=1,j>i \u03b2 t ijk(xi, \u00b7) \u2297 k(xj , \u00b7), and notice that\nAtC\u0302t = A>t (\u00b7, xt)\u2297 k(xt, \u00b7) = t\u22121\u2211 i=1 ( t\u22121\u2211 j>i \u03b2tijk(xj , xt) ) k(xi, \u00b7)\u2297 k(xt, \u00b7),\nwe have At+1 = \u2211t i=1,j>i \u03b2 t+1 ij k(xi, \u00b7)\u2297 k(xj , \u00b7) where\n\u03b2t+1ij =\n( 1\u2212 \u03c3 2\nn \u03b3t\n) \u03b2tij , \u2200i 6 j < t\n\u03b2t+1it = \u2212\u03b3t t\u2211\nj=1\n\u03b2tijk(xj , xt), \u2200i < t\n\u03b2t+1tt = \u03b3t\nRecall\nC\u0302t = E\u03c9[\u03c6\u03c9(xt)\u03c6\u03c9(\u00b7)]\u2297 E\u03c9\u2032 [\u03c6\u03c9\u2032(xt)\u03c6\u03c9\u2032(\u00b7)] = E\u03c9,\u03c9\u2032 [\u03c6\u03c9(xt)\u03c6\u03c9\u2032(xt)\u03c6\u03c9(\u00b7)\u2297 \u03c6\u03c9\u2032(\u00b7)], where \u03c9, \u03c9\u2032 are independently sampled from P(\u03c9), we could approximate the C\u0302t with random features, C\u0302\u03c9,\u03c9 \u2032\nt = \u03c6\u03c9t(xt)\u03c6\u03c9\u2032t(xt)\u03c6\u03c9t(\u00b7)\u2297 \u03c6\u03c9\u2032t(\u00b7). Plug random feature approximation into (18) leads to A\u0302t+1 = ( 1\u2212 \u03c3 2\nn \u03b3t\n) A\u0302t \u2212 \u03b3t ( A\u0302>t (\u00b7, xt)\u2297 \u03c6\u03c9\u2032t(xt)\u03c6\u03c9\u2032t(\u00b7)\u2212 C\u0302 \u03c9,\u03c9\u2032 t ) .\nTherefore, inductively, we could approximate At+1 by\nA\u0302t+1 = t\u2211 i6j \u03b8tij\u03c6\u03c9i(\u00b7)\u2297 \u03c6\u03c9\u2032j (\u00b7)\n\u03b8ij =\n( 1\u2212 \u03c3 2\nn \u03b3t\n) \u03b8ij , \u2200i 6 j < t\n\u03b8it = \u2212\u03b3t t\u22121\u2211 j>i \u03b8ij\u03c6\u03c9\u2032j (xt)\u03c6\u03c9\u2032t(xt), \u2200i < t\n\u03b8tt = \u03b3t\u03c6\u03c9t(xt)\u03c6\u03c9\u2032t(xt)."}], "references": [{"title": "Sparse greedy matrix approximation for machine learning", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Using the Nystrom method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "JMLR, 2:243\u2013264,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "On the nystr om method for approximating a gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["Corinna Cortes", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Fastfood \u2014 computing hilbert space expansions in loglinear time", "author": ["Q.V. Le", "T. Sarlos", "A.J. Smola"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "A.J. Smola", "Zoubin Ghahramani", "Bernhard Schlkopf"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["John C. Platt"], "venue": "Technical Report MSR-TR-98-14, Microsoft Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods \u2014 Support Vector Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A.J. Smola", "R.C. Williamson"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "A modified finite Newton method for fast solution of large scale linear SVMs", "author": ["S.S. Keerthi", "D. DeCoste"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Kernel conjugate gradient for fast kernel machines", "author": ["N. Ratliff", "J. Bagnell"], "venue": "In IJCAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. on Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Integral representation of pd functions", "author": ["A. Devinatz"], "venue": "Trans. AMS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1953}, {"title": "Kernels, associated structures, and generalizations", "author": ["M. Hein", "O. Bousquet"], "venue": "Technical Report 127, Max Planck Institute for Biological Cybernetics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Learning with Kernels", "author": ["Bernhard Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["N. Pham", "R. Pagh"], "venue": "In KDD. ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Predicting time series with support vector machines", "author": ["K.-R. M\u00fcller", "A.J. Smola", "G. R\u00e4tsch", "B. Sch\u00f6lkopf", "J. Kohlmorgen", "V. Vapnik"], "venue": "Artificial Neural Networks ICANN\u201997,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization", "author": ["X.L. Nguyen", "M. Wainwright", "M. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Relative novelty detection", "author": ["Alex J Smola", "Le Song", "Choon H Teo"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Training invariant support vector machines with selective sampling", "author": ["G. Loosli", "S. Canu", "L. Bottou"], "venue": "Large Scale Kernel Machines,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Learning invariant representations of molecules for atomization energy prediction", "author": ["Gr\u00e9goire Montavon", "Katja Hansen", "Siamac Fazli", "Matthias Rupp", "Franziska Biegler", "Andreas Ziehe", "Alexandre Tkatchenko", "Anatole von Lilienfeld", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Random feature maps for dot product kernels", "author": ["Purushottam Kar", "Harish Karnick"], "venue": "editors, AISTATS-12,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Random laplace feature maps for semigroup kernels on histograms", "author": ["Jiyan Yang", "Vikas Sindhwani", "Quanfu Fan", "Haim Avron", "Michael W. Mahoney"], "venue": "In CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Stochastic block mirror descent methods for nonsmooth and stochastic optimization", "author": ["Cong D. Dang", "Guanghui Lan"], "venue": "Technical report, University of Florida,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yurii Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Learning optimally sparse support vector machines", "author": ["Andrew Cotter", "Shai Shalev-Shwartz", "Nati Srebro"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Least squares revisited: Scalable approaches for multi-class prediction", "author": ["A. Agarwal", "S. Kakade", "N. Karampatziakis", "L. Song", "G. Valiant"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "On data preconditioning for regularized loss minimization", "author": ["Tianbao Yang", "Rong Jin", "Shenghuo Zhu"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Quasi-monte carlo feature maps for shift-invariant kernels", "author": ["Jiyan Yang", "Vikas Sindhwani", "Haim Avron", "Michael W. Mahoney"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Learning by stretching deep networks", "author": ["Gaurav Pandey", "Ambedkar Dukkipati"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Kernel methods for deep learning", "author": ["Youngmin Cho", "Lawrence K. Saul"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Many works, such as Greedy basis selection techniques [1], Nystr\u00f6m approximation [2] and incomplete Cholesky decomposition [3], all followed this strategy.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Many works, such as Greedy basis selection techniques [1], Nystr\u00f6m approximation [2] and incomplete Cholesky decomposition [3], all followed this strategy.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "Many works, such as Greedy basis selection techniques [1], Nystr\u00f6m approximation [2] and incomplete Cholesky decomposition [3], all followed this strategy.", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "In fact, without further assumption on the regularity of the kernel matrix, the generalization ability after low-rank approximation is typically of the order O(1/ \u221a r + 1/ \u221a n) [4, 5], which implies that the rank needs to be nearly linear in the number of data points! Thus, in order for kernel methods to achieve the best generalization ability, the low-rank approximation based approaches quickly become impractical for big datasets due to their O(n + nd) preprocessing time and O(n) memory requirement.", "startOffset": 177, "endOffset": 183}, {"referenceID": 4, "context": "In fact, without further assumption on the regularity of the kernel matrix, the generalization ability after low-rank approximation is typically of the order O(1/ \u221a r + 1/ \u221a n) [4, 5], which implies that the rank needs to be nearly linear in the number of data points! Thus, in order for kernel methods to achieve the best generalization ability, the low-rank approximation based approaches quickly become impractical for big datasets due to their O(n + nd) preprocessing time and O(n) memory requirement.", "startOffset": 177, "endOffset": 183}, {"referenceID": 5, "context": "Random feature approximation is another popular approach for scaling up kernel methods [6, 7].", "startOffset": 87, "endOffset": 93}, {"referenceID": 6, "context": "Random feature approximation is another popular approach for scaling up kernel methods [6, 7].", "startOffset": 87, "endOffset": 93}, {"referenceID": 7, "context": "Similar to low-rank kernel matrix approximation approach, the generalization ability of random feature approach is of the order O(1/ \u221a r+1/ \u221a n) [8, 9], which implies that the number of random features also needs to be O(n).", "startOffset": 145, "endOffset": 151}, {"referenceID": 8, "context": "Similar to low-rank kernel matrix approximation approach, the generalization ability of random feature approach is of the order O(1/ \u221a r+1/ \u221a n) [8, 9], which implies that the number of random features also needs to be O(n).", "startOffset": 145, "endOffset": 151}, {"referenceID": 9, "context": ", [10, 11, 12]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": ", [10, 11, 12]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": ", [10, 11, 12]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", [13, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [13, 15]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": "The key intuitions behind our algorithm originate from (i) the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased, the convergence of the algorithm is guaranteed [16]; and", "startOffset": 213, "endOffset": 217}, {"referenceID": 15, "context": "More specifically, both in expectation and with high probability, our algorithm can estimate the optimal function in the RKHS in the rate of O(1/t), which are indeed optimal [16], and achieve a generalization bound of O(1/ \u221a t).", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": ",[17]; [18]) If k(x, x\u2032) is a PD kernel, then there exists a set \u03a9, a measure P on \u03a9, and random feature \u03c6\u03c9(x) : X 7\u2192 R from L2(\u03a9,P), such that k(x, x\u2032) = \u222b \u03a9 \u03c6\u03c9(x)\u03c6\u03c9(x \u2032) dP(\u03c9).", "startOffset": 1, "endOffset": 5}, {"referenceID": 17, "context": ",[17]; [18]) If k(x, x\u2032) is a PD kernel, then there exists a set \u03a9, a measure P on \u03a9, and random feature \u03c6\u03c9(x) : X 7\u2192 R from L2(\u03a9,P), such that k(x, x\u2032) = \u222b \u03a9 \u03c6\u03c9(x)\u03c6\u03c9(x \u2032) dP(\u03c9).", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "For Gaussian RBF kernel, k(x \u2212 x\u2032) = exp(\u2212\u2016x \u2212 x\u2032\u20162/2\u03c32), this yields a Gaussian distribution P(\u03c9) with density proportional to exp(\u2212\u03c3\u2016\u03c9\u2016/2); for the Laplace kernel, this yields a Cauchy distribution; and for the Martern kernel, this yields the convolutions of the unit ball [20].", "startOffset": 275, "endOffset": 279}, {"referenceID": 19, "context": "Similar representation where the explicit form of \u03c6\u03c9(x) and P(\u03c9) are known can also be derived for rotation invariant kernel, k(x, x\u2032) = k(\u3008x, x\u2032\u3009), using Fourier transformation on sphere [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "For polynomial kernels, k(x, x\u2032) = (\u3008x, x\u2032\u3009+ c), a random tensor sketching approach can also be used [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "Explicit random features have been designed for many other kernels, such as dot product kernel [33], additive/multiplicative class of homogeneous kernels [34], e.", "startOffset": 95, "endOffset": 99}, {"referenceID": 33, "context": "Explicit random features have been designed for many other kernels, such as dot product kernel [33], additive/multiplicative class of homogeneous kernels [34], e.", "startOffset": 154, "endOffset": 158}, {"referenceID": 34, "context": ", Hellinger\u2019s, \u03c7, Jensen-Shannon\u2019s and Intersection kernel, as well as kernels on Abelian semigroups [35].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": ", [19]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "The former random features define a hierachical kernel [45], and the latter random features induce a linear combination of multiple kernels.", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "It is worth to note that the Hellinger\u2019s, \u03c7, Jensen-Shannon\u2019s and Intersection kernels in [34] are special cases of multiple kernels combination.", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "Many kernel methods can be written as convex optimizations over functions in the RKHS and solved using the functional gradient methods [13, 15].", "startOffset": 135, "endOffset": 143}, {"referenceID": 14, "context": "Many kernel methods can be written as convex optimizations over functions in the RKHS and solved using the functional gradient methods [13, 15].", "startOffset": 135, "endOffset": 143}, {"referenceID": 5, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 3, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 5, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 1, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 0, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 3, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 2, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 4, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 3, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 4, "context": "T ab le 1 S u m m a y o f ke rn el s in [6 , 4 6 , 3 3 , 2 1 , 3 4 , 3 5 , 4 5 ] a n d th ei r ex p li ci t fe a tu re s", "startOffset": 40, "endOffset": 80}, {"referenceID": 15, "context": "The first key intuition behind our algorithm originates from the property of stochastic gradient descent algorithm that as long as the stochastic gradient is unbiased, the convergence of the algorithm is guaranteed [16].", "startOffset": 215, "endOffset": 219}, {"referenceID": 13, "context": "Remark: [14] used squared hinge loss, l(u, y) = 12 max{0, 1\u2212 uy} , in `2-SVM.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Huber\u2019s loss is used for robust regression [22] where", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "The loss function l(u, \u03c4) = max{0, \u03c4\u2212u} [23] is proposed for novelty detection.", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": ", Ep [ r( q p ) ] , where r : R \u2192 R is a convex function with r(1) = 0, [24] proposed a nonparametric estimator for the logarithm of the density ratio, log q p , which is the solution of following convex optimization, argmin f\u2208H Eq[exp(f)] + Ep[r(\u2212 exp(f))] + \u03bd 2 \u2016f\u2016H (8) where r\u2217 denotes the Fenchel-Legendre dual of r, r(\u03c4) := sup\u03c7 \u03c7\u03c4 \u2212 r\u2217(\u03c7).", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "[24] proposed another convex optimization based on rKL(\u03c4) whose solution is a nonparametric estimator for the density ratio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] designed rnv(\u03c4) = max(0, \u03c1 \u2212 log \u03c4) for novelty detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "reference in [16]) shows that to obtain -accuracy solution, the number of iterations needed for the stochastic approximation is \u03a9(1/ ) for strongly convex case and \u03a9(1/ ) for general convex case.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "For example, techniques for reducing variance of SGD proposed in [37], mini-batch and preconditioning [41, 42] can be used to reduce the constant factors in the bound significantly.", "startOffset": 65, "endOffset": 69}, {"referenceID": 39, "context": "For example, techniques for reducing variance of SGD proposed in [37], mini-batch and preconditioning [41, 42] can be used to reduce the constant factors in the bound significantly.", "startOffset": 102, "endOffset": 110}, {"referenceID": 40, "context": "For example, techniques for reducing variance of SGD proposed in [37], mini-batch and preconditioning [41, 42] can be used to reduce the constant factors in the bound significantly.", "startOffset": 102, "endOffset": 110}, {"referenceID": 3, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 4, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 7, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 8, "context": "Thus the required number of random feature (or ranks), r, will be of the order O(n) = O(1/ ) [4, 5, 8, 9].", "startOffset": 93, "endOffset": 105}, {"referenceID": 12, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 508, "endOffset": 512}, {"referenceID": 36, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 553, "endOffset": 557}, {"referenceID": 37, "context": "We will pick a few representative algorithms for comparison, namely, (i) NORMA [13]: kernel methods trained with stochastic functional gradients; (ii) k-SDCA [12]: kernelized version of stochastic dual coordinate ascend; (iii) r-SDCA: first approximate the kernel function with random features, and then run stochastic dual coordinate ascend; (iv) n-SDCA: first approximate the kernel matrix using Nystr\u00f6m\u2019s method, and then run stochastic dual coordinate ascend; similarly we will combine Pegasos algorithm [26], stochastic block mirror descent (SBMD) [38], and random block coordinate descent (RBCD) [39] with random features and Nystr\u00f6m\u2019s method, and obtain (v) r-Pegasos, (vi) n-Pegasos, (vii) r-SBMD, (viii) n-SBMD, (ix) r-RBCD, and (x) n-RBCD, respectively.", "startOffset": 602, "endOffset": 606}, {"referenceID": 38, "context": ", hinge-loss, there are algorithms proposed to achieve better memory saving with extra training cost, such as support vector reduction technique [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "3M 2850 [0, 13] no guarantees.", "startOffset": 8, "endOffset": 15}, {"referenceID": 29, "context": "For datasets (9) and (10), we compare with the neural net described in [30] and use exactly the same input.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "In this experiment, we compare to a variant of LeNet-5 [32], where all tanh units are replaced with rectified linear units.", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "In this experiment, we compare to a neural net with two convolution layers (after contrast normalization and max-pooling layers) and two local layers that achieves 11% test error on CIFAR 10 [28].", "startOffset": 191, "endOffset": 195}, {"referenceID": 28, "context": "The jointly-trained neural net is Alex-net [29].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "5 Regression Comparisons to Neural Networks We test our algorithm for kernel ridge regression with neural network proposed in [30] on two large-scale real-world regression datasets, (9) and (10) in Table 4.", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "In this experiment, we use the same binary representations converted based on random Coulomb matrices as in [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "We use the same feature representation as for \u201cQuantumMachine\u201d dataset [30].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "It is straightforward to replace the sampling strategy for random features with Fastfood [7] which enjoys the efficient computational cost, or Quasi-Monte Carlo sampling [43], data-dependent sampling [47] which enjoys faster convergence rate with fewer generated features.", "startOffset": 89, "endOffset": 92}, {"referenceID": 41, "context": "It is straightforward to replace the sampling strategy for random features with Fastfood [7] which enjoys the efficient computational cost, or Quasi-Monte Carlo sampling [43], data-dependent sampling [47] which enjoys faster convergence rate with fewer generated features.", "startOffset": 170, "endOffset": 174}], "year": 2015, "abstractText": "The general perception is that kernel methods are not scalable, and neural nets are the methods of choice for large-scale nonlinear learning problems. Or have we simply not tried hard enough for kernel methods? Here we propose an approach that scales up kernel methods using a novel concept called \u201cdoubly stochastic functional gradients\u201d. Our approach relies on the fact that many kernel methods can be expressed as convex optimization problems, and we solve the problems by making two unbiased stochastic approximations to the functional gradient, one using random training points and another using random features associated with the kernel, and then descending using this noisy functional gradient. Our algorithm is simple, does not need to commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We show that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t), and achieves a generalization performance of O(1/ \u221a t). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show that our method can achieve competitive performance to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features.", "creator": "LaTeX with hyperref package"}}}