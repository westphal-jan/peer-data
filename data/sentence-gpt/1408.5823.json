{"id": "1408.5823", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2014", "title": "Improved Distributed Principal Component Analysis", "abstract": "We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as $k$-means clustering and low rank approximation, where the distribution of points is much larger than the distribution of points to the common stock of the average, or a few hundred points to the common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common stock of a common", "histories": [["v1", "Mon, 25 Aug 2014 16:24:43 GMT  (292kb,D)", "http://arxiv.org/abs/1408.5823v1", "23 pages"], ["v2", "Mon, 1 Sep 2014 01:32:30 GMT  (292kb,D)", "http://arxiv.org/abs/1408.5823v2", "23 pages"], ["v3", "Thu, 13 Nov 2014 14:51:43 GMT  (312kb,D)", "http://arxiv.org/abs/1408.5823v3", "23 pages"], ["v4", "Sun, 21 Dec 2014 01:13:00 GMT  (346kb,D)", "http://arxiv.org/abs/1408.5823v4", null], ["v5", "Tue, 23 Dec 2014 03:17:19 GMT  (346kb,D)", "http://arxiv.org/abs/1408.5823v5", null]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yingyu liang", "maria-florina balcan", "vandana kanchanapally", "david p woodruff"], "accepted": true, "id": "1408.5823"}, "pdf": {"name": "1408.5823.pdf", "metadata": {"source": "CRF", "title": "Improved Distributed Principal Component Analysis", "authors": ["Maria-Florina Balcan", "Vandana Kanchanapally", "Yingyu Liang", "David Woodruff"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Since data is often partitioned across multiple servers (Olston et al., 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model. A basic tool for distributed data analysis is Principal Component Analysis (PCA). The goal of PCA is to find an r-dimensional (affine) subspace that captures as much of the variance of the data as possible. Hence, it can reveal lowdimensional structure in very high dimensional data. Moreover, it can serve as a preprocessing step to reduce the data dimension in various machine learning tasks, such as k-means, Non-Negative Matrix Factorization (NNMF) (Lee and Seung, 2001) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003).\nIn the distributed model, approximate PCA was used by Feldman et al. (2013) for solving a number of shape fitting problems such as k-means clustering, where the approximation PCA solution is computed based on a summary of the data called coreset. The coreset has the property that local coresets can be easily combined across servers into a global coreset, which then leads to an approximate PCA solution to the union of the data sets. Designing small coresets therefore leads to communication-efficient protocols. Coresets have the nice property that their size typically does not depend on the number n of points being approximated. A beautiful property of the coresets developed in (Feldman et al., 2013) is that for approximate PCA their size also only depends linearly on the dimension d, whereas previous coresets depended quadratically on d (Feldman and Langberg, 2011). This gives the best known communication protocols for approximate PCA and k-means clustering.\n\u2217School of Computer Science, Georgia Institute of Technology. Contact: {yliang39,ninamf,vvandana}@gatech.edu \u2020IBM Research, Almaden. Contact: dpwoodru@us.ibm.com\nar X\niv :1\n40 8.\n58 23\nv1 [\ncs .L\nG ]\n2 5\nA ug\n2 01\n4\nDespite this recent exciting progress, several important questions remain. First, can we improve the communication further as a function of the number of servers, the approximation error, and other parameters of the downstream applications (such as the number k of clusters in k-means clustering)? Second, while preserving optimal or nearly-optimal communication, can we improve the computational costs of the protocols? We note that in the protocols of Feldman et al. each server has to run a singular value decomposition (SVD) on her local data set, while additional work needs to be performed to combine the outputs of each server into a global approximate PCA. Third, are these algorithms practical and do they scale well with large-scale datasets? In this paper we give answers to the above questions. To state our results more precisely, we first define the model and the problems.\nIn the distributed setting, we consider a set of s servers each of which can communicate with a central coordinator. The global data P \u2208 Rn\u00d7d, consisting of n points in d dimension, is arbitrarily partitioned on the servers, where the server i holds ni points Pi. The PCA problem is to find an r-dimensional subspace which minimizes the sum of the `2 distances of the points to their projections on the subspace.\nFor approximate distributed PCA, the following protocol is implicit in (Feldman et al., 2013): each server i computes its top O(r/ ) principal components Yi of Pi and sends them to the coordinator. The coordinator stacks the matrices Yi on top of each other, forming an O(sr/ ) \u00d7 d matrix Y, and computes the top r principal components of Y, and returns these to the servers. This provides a relative-error approximation to the PCA problem. We refer to this algorithm as Algorithm disPCA. Our Contributions. Our results are summarized as follows.\nImproved Communication: We improve the communication cost for using distributed PCA for k-means clustering and similar `2-fitting problems. The best previous approach is to use Corollary 4.5 in (Feldman et al., 2013), which shows that given a data matrix P, if we project the rows onto the space spanned by the top O(k/ 2) principal components, and solve the k-means problem in this subspace, we obtain a (1 + )- approximation. In the distributed setting, this would require first running Algorithm disPCA with parameter r = O(k/ 2), and thus communication at least O(skd/ 3) to compute the O(k/ 2) global principal components. Then one can solve a distributed k-means problem in this subspace, and an \u03b1-approximation in it translates to an overall \u03b1(1 + ) approximation.\nOur Theorem 3 shows that it suffices to run Algorithm disPCA while only incurring O(skd/ 2) communication to compute the O(k/ 2) global principal components, preserving the k-means solution cost up to a (1 + )-factor. Our communication is thus a 1/ factor better, and illustrates that for downstream applications it is sometimes important to \u201copen up the box\u201d rather than to directly use the guarantees of a generic PCA algorithm (which would give O(skd/ 3) communication). One feature of this approach is that by using the distributed k-means algorithm in (Balcan et al., 2013) on the projected data, the coordinator can sample points from the servers proportional to their local k-means cost solutions, which reduces the communication roughly by a factor of s in the k-means step, which would come from each server sending their local k-means coreset to the coordinator. Furthermore, before applying disPCA and distributed k-means algorithms, one can first run any other dimension reduction to dimension d\u2032 so that the k-means cost is preserved up to certain accuracy. For example, if we want a 1 + approximation factor, we can set d\u2032 = O(log n/ 2) by a Johnson-Lindenstrauss transform; if we want a larger 2 + approximation factor, we can set d\u2032 = O(k/ 2) using (Boutsidis et al., 2011). In this way the parameter d in the above communication cost bound can be replaced by d\u2032. Note that unlike these dimension reduction methods, our algorithm for projecting onto principal components is deterministic and does not incur error probability.\nImproved Computation: We turn to the computational cost of Algorithm disPCA, which to the best of our knowledge has not been addressed. A major bottleneck is that each server is computing a singular value decomposition (SVD) of its point set Pi, which takes min(nid2, n2i d) time. We change Algorithm disPCA\nto instead have each server first sample an oblivious subspace embedding (OSE) (Sarlo\u0301s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguye\u0302n, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi. Using known OSEs, one can choose Hi to have only a single non-zero entry per column and thus HiPi can be computed in nnz(Pi) time. Moreover, the number of rows of Hi isO(d2/ 2), which may be significantly less than the original ni number of rows. This number of rows can be further reducted to O(d logO(1) d/ 2) if one is willing to spend O(nnz(Pi) logO(1) d/ ) time (Nelson and Nguye\u0302n, 2012). We note that the number of non-zero entries of HiPi is no more than that of Pi.\nOne technical issue is that each of s servers is locally performing a subspace embedding, which succeeds with only constant probability. If we want a single non-zero entry per column of Hi, to achieve success probability 1 \u2212 O(1/s) so that we can union bound over all s servers succeeding, we naively would need to increase the number of rows of Hi by a factor linear in s. We give a general technique, which takes a subspace embedding that succeeds with constant probability as a black box, and show how to perform a procedure which applies it O(log 1/\u03b4) times independently and from these applications finds one which is guaranteed to succeed with probability 1 \u2212 \u03b4. Thus, in this setting the players can compute a subspace embedding of their data in nnz(Pi) time, for which the number of non-zero entries of HiPi is no larger than that of Pi, and without incurring this additional factor of s. This may be of independent interest.\nIt may still be expensive to perform the SVD of HiPi and for the coordinator to perform an SVD on Y in Algorithm disPCA. We therefore replace the SVD computation with a randomized approximate SVD computation with spectral norm error. Our contribution here is to analyze the error in distributed PCA and k-means after performing these speedups.\nEmpirical Results: Our speedups result in significant computational savings. The randomized techniques we use reduce the time by orders of magnitude on medium and large-scal data sets, while preserving the communication cost. Although the theory predicts a new small additive error because of our speedups, in our experiments the solution quality was only negligibly affected. Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication. Qu et al. (2002) proposed an algorithm but provided no analysis on the tradeoff between communication and approximation. Most closely related to our work is (Feldman et al., 2013), which observes that the top singular vectors of the local point set can be viewed as its summary and the union of the local summaries can be viewed as a summary of the global data, i.e., Algorithm disPCA discussed above.\nIn (Kannan et al., 2013) the authors study algorithms in the arbitrary partition model in which each server holds a matrix Pi and P = \u2211s i=1 Pi. Thus, each row of P is additively shared across the s servers, whereas in our model each row of P belongs to a single server, though duplicate rows are allowed. Our model is motivated by applications in which points are indecomposable entities. As our model is a special case of the arbitrary partition model, we can achieve more efficient algorithms. For instance, our distributed PCA algorithms provide much stronger guarantees, see, e.g., Lemma 4, which are needed for the downstream k-means application. Moreover, our k-means algorithms are more general, in the sense that they do not make a well-separability assumption, and more efficient in that the communication of (Kannan et al., 2013) is O(sd2) + s(k/ )O(1) words as opposed to our O(sdk/ 2) + sk + (k/ )O(1).\nOther related work includes the recent (Ghashami and Phillips, 2013) (see also the references therein), who give a deterministic streaming algorithm for low rank approximation in which each point of P is seen one at a time and uses O(dk/ ) words of communication. Their algorithm naturally gives an O(sdk/ ) communication algorithm for low rank approximation in the distributed model. However, their algorithm\nfor PCA doesn\u2019t satisfy the stronger guarantees of Lemma 4, and therefore it is unclear how to use it for k-means clustering. It also involves an SVD computation for each point, making the overall computation per server O(nidr2/ 2), which is slower than what we achieve, and it is not clear how their algorithm can exploit sparsity.\nSpeeding up large scale PCA using different versions of subspace embeddings was also considered in (Karampatziakis and Mineiro, 2013), though not in a distributed setting and not for `2-error shape fitting problems. Also, their error guarantees are in terms of the r-th singular value gap, and are incomparable to ours."}, {"heading": "2 Preliminaries", "text": "Communication Model. In the distributed setting, we consider a set of s nodes V = {vi, 1 \u2264 i \u2264 s}, each of which can communicate with a central coordinator v0. On each node vi, there is a local data matrix Pi \u2208 Rni\u00d7d having ni data points in d dimension (ni > d). The global data P \u2208 Rn\u00d7d is then a concatenation of the local data matrix, i.e. P> = [ P>1 ,P > 2 , . . . ,P > s ] and n = \u2211s i=1 ni. Let pi denote the\ni-th row of P. Throughout the paper, we assume that the data points are centered to have zero mean, i.e.,\u2211n i=1 pi = 0. Uncentered data requires a rank-one modification to the algorithms, whose communication and computation costs are dominated by those in the other steps. Approximate PCA and `2-Error Fitting. For a matrix A = [aij ], let \u2016A\u20162F = \u2211 i,j a 2 ij be its Frobenius norm, and let \u03c3i(A) be the i-th singular value of A. Let A(t) denote the matrix that contains the first t columns of A. Let LX denote the linear subspace spanned by the columns of X. Note that for an orthonormal matrix X, the projection of a point p to LX will be pX using the coordinates with respect to the column space of X, and will be pXX> using the original coordinates. Let \u03c0L(p) be its projection onto subspace L and let \u03c0X(p) be shorthand for \u03c0LX(p) = pXX\n>. For a point p \u2208 Rd and a subspace L \u2286 Rd, we denote the squared distance between p and L by\nd2(p, L) := min q\u2208L \u2016p\u2212 q\u201622 = \u2016p\u2212 \u03c0L(p)\u201622.\nDefinition 1. The linear (or affine) r-Subspace k-Clustering on P \u2208 Rn\u00d7d is\nmin L d2(P,L) := n\u2211 i=1 min L\u2208L d2(pi, L) (1)\nwhere P is an n\u00d7dmatrix whose rows are p1, . . . , pn, and L = {Lj}kj=1 is a set of k centers, each of which is an r-dimensional linear (or affine) subspace.\nPCA is a special case when k = 1 and the center is an r-dimensional subspace. It is well known that the optimal r-dimensional subspace is spanned by the top r eigen-vectors of the covariance matrix P>P, also known as the principal components. Equivalently, these vectors are the right singular vectors of P, and can be found using the singular value decomposition (SVD) on P.\nAnother special case of r-Subspace k-Clustering is k-means clustering when the centers are points (r = 0). Constrained versions of this problem include NNMF where the r-dimensional subspace should be spanned by positive vectors, and LDA which assumes a prior distribution defining a probability for each r-dimensional subspace. We will primarily be concerned with relative-error approximation algorithms, for which we would like to output a set L\u2032 of k centers for which d2(P,L\u2032) \u2264 (1 + ) minL d2(P,L).\nAlgorithm 1 Distributed PCA algorithm disPCA Input: local data {Pi}si=1 and parameter t1, t2 \u2208 N+.\n1: for each node vi \u2208 V do 2: Compute local SVD: Pi = Ui\u03a3iV>i . 3: Send \u03a3i(t1),Vi(t1) to the central coordinator. 4: dimension: [Pi]ni\u00d7d, [Ui]ni\u00d7ni , [\u03a3i]ni\u00d7d, [Vi]d\u00d7d, [\u03a3i (t1)]ni\u00d7t1 , [Vi (t1)]d\u00d7t1 5: end for 6: for the central coordinator do 7: Set Yi = \u03a3i(t1)(Vi(t1))>, Y> = [Y>1 , . . . ,Y > s ]. 8: Compute global SVD: Y = U\u03a3V>. 9: dimension: [Yi]ni\u00d7d, [Y]n\u00d7d, [U]n\u00d7n, [\u03a3]n\u00d7d, [V]d\u00d7d, [V\n(t2)]d\u00d7t2 10: end for Output: V(t2)."}, {"heading": "3 Tradeoff between Communication and Solution Quality", "text": "Algorithm disPCA for distributed PCA is suggested in (Qu et al., 2002; Feldman et al., 2013), which consists of a local stage and a global stage. In the local stage, each node performs SVD on its local data matrix, and communicates the first t1 singular values \u03a3i(t1) and the first t1 right singular vectors Vi(t1) to the central coordinator. Then in the global stage, the coordinator concatenates \u03a3i(t1)(Vi(t1))> to form a matrix Y, and performs SVD on it to get the first t2 right singular vectors. See Algorithm 1 for the details and see Figure 1 for an illustration.\nTo get some intuition, consider the easy case when the data points actually lie in an r-dimensional subspace. We can run Algorithm disPCA with t1 = t2 = r. Since Pi has rank r, its projection to the subspace spanned by its first t1 = r right singular vectors, P\u0302i = Ui\u03a3i(r)(Vi(r))>, is identical to Pi. Then we only need to do PCA on P\u0302, the concatenation of P\u0302i. Observing that P\u0302 = U\u0303Y where U\u0303 is orthonormal, it suffices to compute SVD on Y, and only \u03a3i(r)Vi(r) needs to be communicated. In the general case when the data may have rank higher than r, it turns out that one needs to set t1 sufficiently large, so that P\u0302i approximates Pi well enough and does not introduce too much error into the final solution. In particular, the following close projection property about SVD is the key for the analysis:\nLemma 1. Suppose A has SVD A = U\u03a3V and let A\u0302 = AV(t)(V(t))> denote its SVD truncation. If t = O(r/ ), then for any d\u00d7 r matrix X with orthonormal columns,\n0 \u2264 \u2016AX\u2212 A\u0302X\u20162F \u2264 d2(A, LX), and 0 \u2264 \u2016AX\u20162F \u2212 \u2016A\u0302X\u20162F \u2264 d2(A, LX).\nThis means that the projections of A\u0302 and A on any r-dimensional subspace are close, when the projected dimension t is sufficiently large compared to r. Now, note that the difference between \u2016P\u2212PXX>\u20162F and\n\u2016P\u0302 \u2212 P\u0302XX>\u20162F is only related to \u2016PX\u20162F \u2212 \u2016P\u0302X\u20162F = \u2211\ni[\u2016PiX\u20162F \u2212 \u2016P\u0302iX\u20162F ], each term in which is bounded by the lemma. So we can use P\u0302 as a proxy for P in the PCA task. Again, computing PCA on P\u0302 is equivalent to computing SVD on Y, as done in Algorithm disPCA. These lead to the following theorem, which is implicit in (Feldman et al., 2013), stating that the algorithm can produce a (1 + )-approximation for the distributed PCA problem.\nTheorem 2. Suppose Algorithm disPCA takes parameters t1 \u2265 r + d4r/ e \u2212 1 and t2 = r. Then\n\u2016P\u2212PV(r)(V(r))>\u20162F \u2264 (1 + ) min X \u2016P\u2212PXX>\u20162F\nwhere the minimization is over d\u00d7 r orthonormal matrices X. The communication is O( srd ) words.\n3.1 Guarantees for Distributed `2-Error Fitting\nAlgorithm disPCA can also be used as a pre-processing step for applications such as `2-error fitting. In this section, we prove the correctness of Algorithm disPCA as pre-processing for these applications. In particular, we show that by setting t1, t2 sufficiently large, the objective value of any solution merely changes when the original data P is replaced the projected data P\u0303 = PV(t2)(V(t2))>. Therefore, the projected data serves as a proxy of the original data, i.e. , any distributed algorithm can be applied on the projected data to get a solution on the original data. As the dimension is lower, the communication cost is reduced. Formally,\nTheorem 3. Let t1 = t2 = O(rk/ 2) in Algorithm disPCA for \u2208 (0, 1/3). Then there exists a constant c0 \u2265 0 such that for any set of k centers L in r-Subspace k-Clustering,\n(1\u2212 )d2(P,L) \u2264 d2(P\u0303,L) + c0 \u2264 (1 + )d2(P,L).\nThe theorem implies that any \u03b1-approximate solution L on the projected data P\u0303 is a (1 + 3 )\u03b1approximation on the original data P. To see this, let L\u2217 denote the optimal solution. Then\n(1\u2212 )d2(P,L) \u2264 d2(P\u0303,L) + c0 \u2264 \u03b1d2(P\u0303,L\u2217) + c0 \u2264 \u03b1(1 + )d2(P,L\u2217)\nwhich leads to d2(P,L) \u2264 (1 + 3 )\u03b1d2(P,L\u2217). In other words, the distributed PCA step only introduces a small multiplicative approximation factor of (1 + 3 ).\nThe key to prove the theorem is the following close projection property of Algorithm disPCA in Lemma 4. Intuitively, it means that for any low dimensional subspace spanned by X, the projections of P and P\u0303 on the subspace are close. To prove Theorem 3 by this, we choose X to be the orthonormal basis of the subspace spanning the centers. Since the problem only involves l2 error, the difference between the objective values of P and P\u0303 can be decomposed into two terms depending only on \u2016PX \u2212 P\u0303X\u20162F and \u2016PX\u20162F \u2212\u2016P\u0303X\u20162F respectively, which are small as shown by the lemma. The complete proof of Theorem 3 is provided in Appendix B.2.\nLemma 4. Let t1 = t2 = O(k/ ) in Algorithm disPCA. Then for any d \u00d7 k matrix X with orthonormal columns, 0 \u2264 \u2016PX\u2212 P\u0303X\u20162F \u2264 d2(P, LX), and 0 \u2264 \u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2264 d2(P, LX).\nProof Sketch: We first introduce some auxiliary variables for the analysis, which act as intermediate connections between P and P\u0303. Imagine we perform two kinds of projections: first project Pi to P\u0302i = PiVi (t1)(Vi (t1))>, then project P\u0302i to Pi = P\u0302iV(t2)(V(t2))>. Let P\u0302 denote the vertical concatenation of P\u0302i and let P denote the vertical concatenation of Pi. These variables are designed so that the difference between P and P\u0302 and that between P\u0302 and P are easily bounded.\nAlgorithm 2 Distributed k-means clustering Input: {Pi}si=1, k \u2208 N+ and \u2208 (0, 1/3), a non-distributed \u03b1-approximation algorithm A\u03b1\n1: Run Algorithm disPCA with t1 = t2 = O(k/ 2) to get E = V(t2), and send E to all nodes. 2: Run the distributed k-means clustering algorithm in (Balcan et al., 2013) on {PiEE>}si=1, usingA\u03b1 as\na subroutine, to get k centers L. Output: L.\nOur proof then proceeds by first bounding these differences, and then bounding that between P and P\u0303. Take the second statement as an example. We have the following decomposition:\n\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F = [ \u2016PX\u20162F \u2212 \u2016P\u0302X\u20162F ] + [ \u2016P\u0302X\u20162F \u2212 \u2016PX\u20162F ] + [ \u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F ] .\nThe first term is just \u2211s\ni=1 [ \u2016PiX\u20162F \u2212 \u2016P\u0302iX\u20162F ] , each of which can be bounded by Lemma 1, since P\u0302i\nis the SVD truncation of P. The second term can be bounded similarly. The more difficult part is the third term. Note that Pi = P\u0302iZ, P\u0303i = PiZ where Z := V(t2)(V(t2))>X, leading to\n\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F = s\u2211 i=1 [ \u2016P\u0302iZ\u20162F \u2212 \u2016PiZ\u20162F ] .\nAlthough Z is not orthonormal as required by Lemma 1, we prove a generalization (Lemma 7 in the appendix) which can be applied to show that the third term is indeed small.\nThe bound on \u2016PX\u2212 P\u0303X\u20162F can be proved by a similar argument. See Appendix B.1 for details. Application to k-Means Clustering To see the implication of Theorem 3, consider the k-means clustering problem. We can first perform any other possible dimension reduction to dimension d\u2032 so that the k-means cost is preserved up to accuracy , and then run Algorithm disPCA and finally run any distributed k-means clustering algorithm on the projected data to get a good approximate solution. For example, in the first step we can set d\u2032 = O(log n/ 2) using a Johnson-Lindenstrauss transform, or we can perform no reduction and simply use the original data.\nAs a concrete example, we can use original data (d\u2032 = d), then run Algorithm disPCA, and finally run the distributed clustering algorithm in (Balcan et al., 2013) which uses any non-distributed \u03b1-approximation algorithm as a subroutine and computes a (1 + )\u03b1-approximate solution. The resulting algorithm is presented in Algorithm 2.\nTheorem 5. With probability at least 1 \u2212 \u03b4, Algorithm 2 outputs a (1 + )2\u03b1-approximate solution for distributed k-means clustering. The total communication cost of Algorithm 2 is O( sk 2 ) vectors in Rd plus\nO ( 1 4 (k 2 2 + log 1\u03b4 ) + sk log sk \u03b4 ) vectors in RO(k/ 2)."}, {"heading": "4 Fast Distributed PCA", "text": "Subspace Embeddings One can significantly improve the time of the distributed PCA algorithms by using subspace embeddings, while keeping similar guarantees as in Lemma 4, which suffice for l2-error fitting. More precisely, a subspace embedding matrix H \u2208 R`\u00d7n for a matrix A \u2208 Rn\u00d7d has the property that for all vectors y \u2208 Rd, \u2016HAy\u20162 = (1 \u00b1 )\u2016Ay\u20162. Suppose independently, each node vi chooses a random subspace embedding matrix Hi for its local data Pi. Then, they run Algorithm disPCA on the embedded data {HiPi}si=1 instead of on the original data {Pi}si=1.\nThe work of (Sarlo\u0301s, 2006) pioneered subspace embeddings. The recent fast sparse subspace embeddings (Clarkson and Woodruff, 2013) and its optimizations (Meng and Mahoney, 2013; Nelson and Nguye\u0302n, 2012) are particularly suitable for large scale sparse data sets, since their running time is linear in the number of non-zero entries in the data matrix, and they also preserve the sparsity of the data. The algorithm takes as input an n \u00d7 d matrix A and a parameter `, and outputs an ` \u00d7 d embedded matrix A\u2032 = HA (the embedded matrix H does need to be built explicitly). The embedded matrix is constructed as follows: initialize A\u2032 = 0; for each row in A, multiply it by +1 or \u22121 with equal probability, then add it to a row in A\u2032 chosen uniformly at random.\nThe success probability is constant, while we need to set it to be 1\u2212\u03b4 where \u03b4 = \u0398(1/s). Known results which preserve the number of non-zero entries of H to be 1 per column increase the dimension of H by a factor of s. To avoid this, we propose an approach to boost the success probability by computing O(log 1\u03b4 ) independent embeddings, each with only constant success probability, and then run a cross validation style procedure to find one which succeeds with probability 1 \u2212 \u03b4. More precisely, we compute the SVD of all embedded matrices HjA = Uj\u03a3jV>j , and find a j \u2208 [r] such that for at least half of the indices j\u2032 6= j, all singular values of \u03a3jV>j Vj\u2032\u03a3 > j\u2032 are in [1 \u00b1 O( )] (see Algorithm 5 in the appendix). The reason why such an embedding HjA succeeds with high probability is as follows. Any two successful embeddings HjA and Hj\u2032A, by definition, satisfy that \u2016HjAx\u201622 = (1\u00b1O( ))\u2016Hj\u2032Ax\u201622 for all x, which we show is equivalent to passing the test on the singular values. Since with probability at least 1 \u2212 \u03b4, 9/10 fraction of the embeddings are successful, it follows that the one we choose is successful with probability 1\u2212 \u03b4. Randomized SVD The exact SVD of an n \u00d7 d matrix is impractical in the case when n or d is large. Here we show that the randomized SVD algorithm from (Halko et al., 2011) can be applied to speed up the computation without compromising the quality of the solution much. We need to use their specific form of randomized SVD since the error is with respect to the spectral norm, rather than the Frobenius norm, and so can be much smaller as needed by our applications.\nThe algorithm first probes the row space of the ` \u00d7 d input matrix A with an ` \u00d7 2t random matrix \u2126 and orthogonalizes the image of \u2126 to get a basis Q (i.e., QR-factorize A>\u2126); projects the data to this basis and computes the SVD factorization on the smaller matrix AQ. It also performs q power iterations to push the basis towards the top t singular vectors. Fast Distributed PCA for l2-Error Fitting We modify Algorithm disPCA by first having each node do a subspace embedding locally, then replace each SVD invocation with a randomized SVD invocation. We thus arrive at Algorithm 3. For `2-error fitting problems, by combining approximation guarantees of the randomized techniques with that of distributed PCA, we are able to prove:\nTheorem 6. Suppose Algorithm 3 takes \u2208 (0, 1/2], t1 = t2 = O(max { k 2 , log s\u03b4 } ), ` = O(d 2 2 ), q = O(max{log d , log sk }) as input, and sets the failure probability of each local subspace embedding to \u03b4 \u2032 = \u03b4/2s. Let P\u0303 = PVV>. Then with probability at least 1 \u2212 \u03b4, there exists a constant c0 \u2265 0, such that for any set of k points L,\n(1\u2212 )d2(P,L)\u2212 \u2016PX\u20162F \u2264 d2(P\u0303,L) + c0 \u2264 (1 + )d2(P,L) + \u2016PX\u20162F\nwhere X is an orthonormal matrix whose columns span L. The total communication is O(skd/ 2) and the total time is O ( nnz(P) + s [ d3k 4 + k 2d2 6 ] log d log sk \u03b4 ) .\nProof Sketch: It suffices to show that P\u0303 enjoys the close projection property as in Lemma 4, i.e., \u2016PX \u2212 P\u0303X\u20162F \u2248 0 and \u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2248 0 for any orthonormal matrix X whose columns span a low dimensional subspace. Note that Algorithm 3 is just running Algorithm disPCA (with randomized SVD) on TP\nAlgorithm 3 Fast Distributed PCA for l2-Error Fitting Input: {Pi}si=1; parameters t1, t2 for Algorithm disPCA; `, q for randomized techniques.\n1: for each node vi \u2208 V do 2: Compute subspace embedding P\u2032i = HiPi. 3: end for 4: Run Algorithm disPCA on {P\u2032i}si=1 to get V, where the SVD is randomized. 5: dimension: [Pi]ni\u00d7d, [P \u2032 i]`\u00d7d, [V]d\u00d72t2\nOutput: V.\nwhere T = diag(H1,H2, . . . ,Hs), so we first show that TP\u0303 enjoys this property. But now exact SVD is replaced with randomized SVD, for which we need to use the spectral error bound to argue that the error introduced is small. More precisely, for a matrix A and its SVD truncation A\u0302 computed by randomized SVD, it is guaranteed that the spectral norm of A\u2212 A\u0302 is small, then \u2016(A\u2212 A\u0302)X\u2016F is small for any X with small Frobenius norm, in particular, the orthonormal basis spanning a low dimensional subspace. This then suffices to guarantee TP\u0303 enjoys the close projection property. Given this, it suffices to show that P\u0303 enjoys this property as TP\u0303, which follows from the definition of a subspace embedding."}, {"heading": "5 Experiments", "text": "Our focus is to show the randomized techniques used in Algorithm 3 reduce the time taken significantly without compromising the quality of the solution. We perform experiments for three tasks: rank-r approximation, k-means clustering and principal component regression (PCR). Datasets We choose the following real world datasets from UCI repository (Bache and Lichman, 2013) for our experiments. For low rank approximation and k-means clustering, we choose two medium size datasets NewsGroups (18774 \u00d7 61188) and MNIST (70000 \u00d7 784), and two large-scale Bag-of-Words datasets: NYTimes news articles (BOWnytimes) (300000\u00d7 102660) and PubMed abstracts (BOWpubmed) (8200000 \u00d7 141043). We use r = 10 for rank-r approximation and k = 10 for k-means clustering. For PCR, we use MNIST and further choose YearPredictionMSD (515345\u00d7 90), CTslices (53500\u00d7 386), and a large dataset MNIST8m (800000\u00d7 784). Experimental Methodology The algorithms are evaluated on a star network. The number of nodes is s = 25 for medium-size datasets, and s = 100 for the larger ones. We distribute the data over the nodes using a weighted partition, where each point is distributed to the nodes with probability proportional to the node\u2019s weight chosen from the power law with parameter \u03b1 = 2.\nFor each projection dimension, we first construct the projected data using distributed PCA. For low rank approximation, we report the ratio between the cost of the obtained solution to that of the solution computed by SVD on the global data. For k-means, we run the algorithm in (Balcan et al., 2013) (with Lloyd\u2019s method as a subroutine) on the projected data to get a solution. Then we report the ratio between the cost of the above solution to that of a solution obtained by running Lloyd\u2019s method directly on the global data. For PCR, we perform regression on the projected data to get a solution. Then we report the ratio between the error of the above solution to that of a solution obtained by PCR directly on the global data. We stop the algorihtm if it takes more than 24 hours. For each projection dimension and each algorithm with randomness, the average ratio over 5 runs is reported. Results Figure 2 shows the results for low rank approximation. We observe that the error of the fast distributed PCA is comparable to that of the exact solution computed directly on the global data. This is also\nobserved for distributed PCA with one or none of subspace embedding and randomized SVD. Furthermore, the error of the fast PCA is comparable to that of normal PCA, which means that the speedup techniques merely affects the accuracy of the solution. The second row shows the computational time, which suggests a significant decrease in the time taken to run the fast distributed PCA. For example, on NewsGroups, the time of the fast distributed PCA improves over that of normal distributed PCA by a factor between 10 to 100. On the large dataset BOWpubmed, the normal PCA takes too long to finish and no results are presented, while the speedup versions produce good results in reasonable time. The use of the randomized techniques gives us a good performance improvement while keeping the solution quality almost the same.\nFigure 3 and Figure 4 show the results for k-means clustering and PCR respectively. Similar to that for low rank approximation, we observe that the distributed solutions are almost as good as that computed directly on the global data, and the speedup merely affects the solution quality. We again observe a huge decrease in the running time by the speedup techniques.\nAcknowledgments This work was supported in part by NSF grants CCF-0953192 and CCF-1101215, AFOSR grant FA9550-09-1-0538, ONR grant N00014-09-1-0751, a Google Research Award, and a Microsoft Research Faculty Fellowship."}, {"heading": "A Guarantees for Distributed PCA", "text": ""}, {"heading": "A.1 Proof of Lemma 1", "text": "We first prove a generalization of Lemma 1.\nLemma 7. Let A \u2208 Rn\u00d7d be an n \u00d7 d matrix with singular value decomposition A = U\u03a3V>. Let \u2208 (0, 1] and r, t \u2208 N+ with d\u2212 1 \u2265 t \u2265 r + dr/ e \u2212 1, and let A\u0302 = AV(t)(V(t))>. Then for any matrix X with d rows and \u2016X\u20162F \u2264 r, we have\n\u2016(A\u2212 A\u0302)X\u20162F = \u2016AX\u20162F \u2212 \u2016A\u0302X\u20162F \u2264 d\u2211\ni=r+1\n\u03c32i (A).\nProof. The proof follows the idea in the proof of Lemma 6.1 in (Feldman et al., 2013). For convenience, let \u03a3(t) denote the diagonal matrix that contains the first t diagonal entries in \u03a3 and is 0 otherwise. Then A\u0302 = U\u03a3(t)V> We first have\n\u2016AX\u20162F \u2212 \u2016A\u0302X\u20162F = \u2016U\u03a3V>X\u20162F \u2212 \u2016U\u03a3(t)V>X\u20162F = \u2016\u03a3V>X\u20162F \u2212 \u2016\u03a3(t)V>X\u20162F = \u2016(\u03a3\u2212\u03a3(t))V>X\u20162F = \u2016U(\u03a3\u2212\u03a3(t))V>X\u20162F = \u2016AX\u2212 A\u0302X\u20162F .\nwhere the second and fourth equalities follow since U has orthonormal columns, and the third equality follows since for M = V>X we have\n\u2016\u03a3M\u20162F \u2212 \u2016\u03a3(t)M\u20162F = d\u2211 i=1 d\u2211 j=1 \u03c32i (A)m 2 ij \u2212 t\u2211 i=1 d\u2211 j=1 \u03c32i (A)m 2 ij\n= d\u2211\ni=t+1 d\u2211 j=1 \u03c32i (A)m 2 ij = \u2016(\u03a3\u2212\u03a3(t))M\u20162F .\nNext, we bound \u2016AX\u2212 A\u0302X\u20162F . We have\n\u2016AX\u2212 A\u0302X\u20162F = \u2016(\u03a3\u2212\u03a3(t))V>X\u20162F \u2264 \u2016(\u03a3\u2212\u03a3(t))\u20162S\u2016X\u20162F = r\u03c32t+1(A)\nwhere the inequality follows because the spectral norm is consistent with the Euclidean norm. This implies the lemma since\nr\u03c32t+1(A) \u2264 (t\u2212 r + 1)\u03c32t+1(A) \u2264 t+1\u2211 i=r+1 \u03c32i (A) \u2264 d\u2211 i=r+1 \u03c32i (A). (2)\nwhere the first inequality follows for our choice of t.\nThen Lemma 1 immediately follows from Lemma 7 since any d\u00d7 r orthonormal matrix A has \u2016A\u20162F \u2264 r, and \u2211d i=r+1 \u03c3 2 i (A) \u2264 d2(A, LX) by the property of the singular value decomposition."}, {"heading": "A.2 Proof of Theorem 2", "text": "Theorem 2. Suppose Algorithm disPCA takes parameters t1 \u2265 r + d4r/ e \u2212 1 and t2 = r, and outputs V(r). Then\n\u2016P\u2212PV(r)(V(r))>\u20162F \u2264 (1 + ) min X d2(P, LX)\nwhere the minimization is over d\u00d7 r orthonormal matrices X. The communication is O( srd ) words.\nProof. Let P\u0302i := PiV (t) i (V (t) i ) >, and let P\u0302 be the concadenation of P\u0302i.\nFirst, we show that P\u0302 serves as a proxy of P for optimizing d2(P, LX). By Pythagorean Theorem, for any orthonormal matrix X of size d\u00d7 r,\nd2(P\u0302, LX)\u2212 d2(P, LX) = (\u2016P\u0302\u20162F \u2212 \u2016P\u0302X\u20162F )\u2212 (\u2016P\u20162F \u2212 \u2016PX\u20162F ) = \u2206(X)\u2212 c0 (3)\nwhere \u2206(X) := \u2016PX\u20162F \u2212 \u2016P\u0302X\u20162F and c0 := \u2016P\u20162F \u2212 \u2016P\u0302\u20162F . Since \u2206(X) is small by Lemma 1 and c0 is a constant, P\u0302 approximates P for optimizing d2(P, LX).\nNext, we note that the optimal principal components for P\u0302 are V(r). This is because P\u0302 = U\u0303Y where U\u0303 is a block-diagonal matrix with blocks U1, . . . ,Us, and thus the right singular vectors of Y are also the right singular vectors of P\u0302.\nNow, we are ready to bound \u2016P \u2212 PV(r)(V(r))>\u20162F = d2(P, LV(r)). Suppose the r optimal loadings for P are X\u2217. See Figure 5 for an illustration. Then\n\u2016P\u2212PV(r)(V(r))>\u20162F = d2(P\u0302, LV(r)) + c0 \u2212\u2206(V (r))\n\u2264 d2(P\u0302, LX\u2217) + c0 \u2212\u2206(V(r)) = d2(P, LX\u2217) + \u2206(X \u2217)\u2212\u2206(V(r)) (4)\nwhere the first and third line follow from (3) and the second follows from the fact that V(r) are the optimal principal loadings for P\u0302. By Lemma 1, \u2206(V(r)) \u2265 0 and \u2206(X\u2217) \u2264 d2(P, LX\u2217). Combining these with (4) leads to the theorem.\nNote A refinement of the proof of Lemma 1 leads to the following data dependent bound.\nLemma 8. The statement in Lemma 7 holds if t > \u03c4(A, r, ) where\n\u03c4(A, r, ) := argmin t\n{ \u03c32t (A) \u2264\nr \u2211 i>r \u03c32i (A)\n} ."}, {"heading": "Furthermore, \u03c4(A, r, ) = O( r ).", "text": "Proof. Note that the bound on t is only used in proving (2), for which t > \u03c4(A, r, ) suffices. \u03c4(A, r, ) = O( r ) follows by definition.\nTheorem 9. Suppose Algorithm disPCA takes parameters t1 \u2265 maxi \u03c4(Pi, r, ) and t2 = r,and outputs V(r). Then\n\u2016P\u2212PV(r)(V(r))>\u20162F \u2264 (1 + ) min X d2(P, LX)\nwhere the minimization is over orthonormal matrices X \u2208 Rd\u00d7r. The total communication cost isO(sdmaxi \u03c4(Pi, r, )) words.\n\u03c4(Pi, r, ) is typically much less than O(r/ ) in practice. This provides an explanation for the fact that t1 much smaller than O(r/ ) can still lead to good solution for many practical instances. Similar data dependent bounds can be derived for the other theorems in our paper.\nB Guarantees for Distributed `2-Error Fitting"}, {"heading": "B.1 Proof of Lemma 4", "text": "We first introduce some intermediate variables for our analysis. Imagine we perform two projections: first project Pi to P\u0302i = PiVi(t)(Vi(t))>, then project P\u0302i to Pi = P\u0302iV(t)(V(t))> where t = t1 = t2. Let P\u0302 denote the vertical concatenation of P\u0302i and let P denote the vertical concatenation of Pi, i.e.\nP\u0302 =  P\u03021... P\u0302s  and P =  P1... Ps  Lemma 4. Let t1 = t2 \u2265 k + d8k/ e \u2212 1 in Algorithm disPCA for k \u2208 N+ and \u2208 (0, 1). Then for any d\u00d7 k matrix X with orthonormal columns,\n0 \u2264 \u2016PX\u2212 P\u0303X\u20162F \u2264 d 2(P, LX), 0 \u2264 \u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2264 d 2(P, LX).\nProof. For the first statement, we have\n\u2016PX\u2212 P\u0303X\u20162F \u2264 2\u2016PX\u2212 P\u0302X\u20162F (5) + 2\u2016P\u0302X\u2212PX\u20162F (6) + 2\u2016PX\u2212 P\u0303X\u20162F . (7)\nFor (5), we have by Lemma 7\n\u2016PX\u2212 P\u0302X\u20162F = s\u2211 i=1 \u2016PiX\u2212 P\u0302iX\u20162F \u2264 s\u2211 i=1 4 d2(Pi, LX) = 8 d2(P, LX). (8)\nSimilarly, for (6) we have by Lemma 7\n\u2016P\u0302X\u2212PX\u20162F \u2264 8 d2(P\u0302, LX). (9)\nTo bound (7), let Y = V(t)(V(t))>X. Then by definition, PiX = P\u0302iY and P\u0303iX = PiY. By Lemma 7, we have\n\u2016PX\u2212 P\u0303X\u20162F = s\u2211 i=1 \u2016P\u0302iY \u2212PiY\u20162F (10)\n\u2264 s\u2211 i=1 8 s\u2211 i=r+1 \u03c32i (Pi) \u2264 8 s\u2211 i=1 d2(Pi, LX) = 8 d2(P, LX). (11)\nCombining (8)(9) and (11) leads to\n\u2016PX\u2212 P\u0303X\u20162F \u2264 2 d2(P, LX) + 4 d2(P\u0302, LX). (12)\nWe now only need to bound d2(P\u0302, LX) is similar to d2(P, LX), which is done in Lemma 10. The first statement then follows.\nFor the second statement, we have a similar argument.\n\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F = \u2016PX\u20162F \u2212 \u2016P\u0302X\u20162F (13) + \u2016P\u0302X\u20162F \u2212 \u2016PX\u20162F (14) + \u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F . (15)\nFor (13), we have by Lemma 7\n\u2016PX\u20162F \u2212 \u2016P\u0302X\u20162F = s\u2211 i=1 [ \u2016PiX\u20162F \u2212 \u2016P\u0302iX\u20162F ] \u2264 s\u2211 i=1 4 d2(Pi, LX) = 4 d2(P, LX). (16)\nSimilarly, for (14) we have by Lemma 7\n\u2016P\u0302X\u20162F \u2212 \u2016PX\u20162F \u2264 4 d2(P\u0302, LX). (17)\nBy Lemma 7, we have\n\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F = s\u2211 i=1 [ \u2016P\u0302iY\u20162F \u2212 \u2016PiY\u20162F ] \u2264\ns\u2211 i=1 4 s\u2211 i=r+1 \u03c32i (Pi) \u2264 4 s\u2211 i=1 d2(Pi, LX) = 4 d2(P, LX). (18)\nCombining (16)(17) and (18) leads to\n\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2264 2 d2(P, LX) + 4 d2(P\u0302, LX). (19)\nThe second statement then follows from (19) and Lemma 10.\nThe following is a technical lemma that will be used in the proof of Lemma 4.\nLemma 10. d2(P\u0302, LX) \u2264 (1 + )d2(P, LX).\nProof. We have\nd2(P\u0302, LX)\u2212 d2(P, LX) = \u2016P\u0302\u2212 P\u0302XX>\u20162F \u2212 \u2016P\u2212PXX>\u20162F = \u2016P\u0302\u20162F \u2212 \u2016P\u0302XX>\u20162F \u2212 (\u2016P\u20162F \u2212 \u2016PXX>\u20162F )\n= s\u2211 i=1 [ \u2016P\u0302i\u20162F \u2212 \u2016Pi\u20162F ] + s\u2211 i=1 [ \u2016PiXX>\u20162F \u2212 \u2016P\u0302iXX>\u20162F ] .\nBy the Pythagorean Theorem, \u2016P\u0302i\u20162F \u2264 \u2016Pi\u20162F . Also, since X is orthonormal, \u2016PiXX>\u20162F = \u2016PiX\u20162F and \u2016P\u0302iXX>\u20162F = \u2016P\u0302iX\u20162F . Then\nd2(P\u0302, LX)\u2212 d2(P, LX) \u2264 s\u2211 i=1 [ \u2016PiX\u20162F \u2212 \u2016P\u0302iX\u20162F ] \u2264 s\u2211 i=1 d2(Pi, LX) = d 2(P, LX) (20)\nwhere the second inequality follows from Lemma 1."}, {"heading": "B.2 Proof of Theorem 3", "text": "The following weak triangle inequality is useful for our analysis.\nFact 1. For any a, b \u2208 R and \u2208 (0, 1), |a2 \u2212 b2| \u2264 3(a\u2212b) 2 + 2 a 2.\nProof. Either |a| \u2264 |a\u2212b| or |a\u2212 b| \u2264 |a|, so we have |a||a\u2212 b| \u2264 (a\u2212b)2 + a 2. This leads to\n|a2 \u2212 b2| = |a\u2212 b||a+ b| \u2264 |a\u2212 b|(|2a|+ |b\u2212 a|) = 2|a||a\u2212 b|+ (a\u2212 b)2 \u2264 2(a\u2212 b) 2 + 2 a2 + (a\u2212 b)2\nwhich completes the proof.\nWe first prove the theorem for the special case of k-means clustering, and the same argument leads to the guarantee for general l2-error fitting problems.\nTheorem 11. Let t1 = t2 \u2265 k + d4k/ 2e \u2212 1 in Algorithm disPCA.Then there exists a constant c0 \u2265 0, such that for any set of k points L,\n(1\u2212 )d2(P,L) \u2264 d2(P\u0303,L) + c0 \u2264 (1 + )d2(P,L).\nProof. The proof follows that in (Feldman et al., 2013), with slight modification for the distributed setting. Let X \u2208 Rd\u00d7k has orthonormal columns that span L. Let p\u0303i be the point in P\u0303 corresponding to pi in P. Let c0 = \u2016P\u20162F \u2212 \u2016P\u0303\u20162F . Then by Pythagorean theorem we have |d2(P,L)\u2212 d2(P\u0303,L)\u2212 c0| \u2264 \u2223\u2223\u2223\u2223d2(P, L(X))\u2212 d2(P\u0303, LX)\u2212 c0\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 |P|\u2211\ni=1\n[ d(\u03c0X(pi),L)2 \u2212 d(\u03c0X(p\u0303i),L)2 ]\u2223\u2223\u2223\u2223.\nAlgorithm 4 Fast Sparse Subspace Embedding (Clarkson and Woodruff, 2013) Input: parameters n, ` \u2208 N+.\n1: Let h : [n] 7\u2192 [`] be a random map, so that for each i \u2208 [n], h(i) = j for j \u2208 [`] with probability 1/`. 2: Let \u03a6 be an `\u00d7 n binary matrix with \u03a6h(i),i = 1, and all remaining entries 0. 3: Let \u03a3 be an n \u00d7 n diagonal matrix, with each diagonal entry independently chosen as +1 or \u22121 with\nequal probability. Output: H = \u03a6\u03a3.\nFor the first part, we have by Pythagorean theorem\nd2(P, L(X))\u2212 d2(P\u0303, LX)\u2212 c0 = (\u2016P\u20162F \u2212 \u2016PX\u20162F )\u2212 (\u2016P\u0303\u20162F \u2212 \u2016P\u0303X\u20162F )\u2212 c0 = \u2016P\u0303X\u20162F \u2212 \u2016PX\u20162F . (21)\nFor the second part, by Fact 1 we have\n|P|\u2211 i=1 \u2223\u2223d(\u03c0X(pi),L)2 \u2212 d(\u03c0X(p\u0303i),L)2\u2223\u2223 \u2264 |P|\u2211 i=1 [ 12d(\u03c0X(pi), \u03c0X(p\u0303i)) 2 + 2 d(\u03c0X(pi),L)2 ]\n= 12\n\u2016(P\u2212 P\u0303)X\u20162F +\n2 |P|\u2211 i=1 d(\u03c0X(pi),L)2\n\u2264 12 \u2016(P\u2212 P\u0303)X\u20162F +\n2 |P|\u2211 i=1 d(pi,L)2. (22)\nCombining (21)(22) with Lemma 4 leads to the theorem, since d2(P, LX) \u2264 d2(P,L).\nThe general statement for `2-error geometric fitting problems follows from the same argument. Theorem 3. Let t1 = t2 = O(rk/ 2) in Algorithm disPCA for \u2208 (0, 1/3). Then there exists a constant c0 \u2265 0 such that for any set of k centers L in r-Subspace k-Clustering,\n(1\u2212 )d2(P,L) \u2264 d2(P\u0303,L) + c0 \u2264 (1 + )d2(P,L)."}, {"heading": "C Fast Distributed PCA", "text": ""}, {"heading": "C.1 Proofs for Subspace Embedding", "text": "The construction of the embedding matrix H is presented in Algorithm 4. Note that the embedding matrix H does not need to be built explicitly; we can compute the embedding HA for an given matrix A in a direct and faster way. Algorithm 4 has the following guarantee.\nTheorem 12. (Clarkson and Woodruff, 2013; Meng and Mahoney, 2013; Nelson and Nguye\u0302n, 2012) Suppose n > d and ` = O(d 2 2 ). With probability at least 99/100, \u2016HAy\u20162 = (1 \u00b1 )\u2016Ay\u20162 for all vectors y \u2208 Rd. Moreover, HA can be computed in time O(nnz(A)) where nnz(A) is the number of non-zero entries in A.\nAlgorithm 5 Boosting success probability of embedding Input: A \u2208 Rn\u00d7d, parameters , \u03b4.\n1: Construct r = O(log 1\u03b4 ) independent subspace embeddings HjA, each having accuracy /9 and success probability 99/100. 2: Compute SVD HjA = Uj\u03a3jV>j for j \u2208 [r]. 3: for j \u2208 [r] do 4: Check if for at least half j\u2032 6= j,\n\u03c3i(\u03a3j\u2032V > j\u2032Vj\u03a3 \u22121 j ) \u2208 [1\u00b1 /3], \u2200i.\n5: If so, output HjA. 6: end for\nLemma 13. Let \u2208 (0, 1/2] and k, t \u2208 N+ with d\u2212 1 \u2265 t \u2265 k + d4k/ e \u2212 1. Suppose Algorithm disPCA takes input {HiPi}si=1 and outputs V(t). Let P\u0303 = PV(t)(V(t))>. Then for any d \u00d7 k matrix X with orthonormal columns,\n\u2016PX\u2212 P\u0303X\u20162F \u2264 d2(P, LX),\u2223\u2223\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2223\u2223 \u2264 3 \u2016PX\u20162F + d2(P, LX). Proof. First note that the input to Algorithm disPCA is TP where T is a block-diagonal matrix with blocks H1, . . . ,Hs. Then the projection of the input to V(t) is TPV(t)(V(t))> = TP\u0303. By Lemma 4, for any d\u00d7k matrix X with orthonormal columns, we have\n0 \u2264 \u2016TPX\u2212TP\u0303X\u20162F \u2264 4 d2(TP, LX), (23)\n0 \u2264 \u2016TPX\u20162F \u2212 \u2016TP\u0303X\u20162F \u2264 4 d2(TP, LX). (24)\nBy properties of T, we have\n\u2016TPX\u2212TP\u0303X\u20162F = \u2016T(PX\u2212 P\u0303X)\u20162F \u2265 (1\u2212 )\u2016PX\u2212 P\u0303X\u20162F\nand\nd2(TP, LX) = \u2016TP\u2212TPXX>\u20162F \u2264 (1 + )\u2016P\u2212PXX>\u20162F = (1 + )d2(P, LX).\nCombined with (23), these lead to the first claim. Similarly, we also have \u2016TPX\u20162F = (1\u00b1 )\u2016PX\u20162F and \u2016TP\u0303X\u20162F = (1\u00b1 )\u2016P\u0303X\u20162F . Plugging these into (24), we obtain\n\u22123 \u2016PX\u20162F \u2264 \u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2264 3 \u2016PX\u20162F + d2(P, LX)\nwhich establishes the lemma.\nTheorem 14. Algorithm 5 outputs a subspace embedding with probability at least 1\u2212\u03b4. In expectation Step 3 is run only a constant number of times with expected time O(d3r2/ 2).\nAlgorithm 6 Randomized SVD (Halko et al., 2011) Input: matrix A \u2208 R`\u00d7d; parameters t, q \u2208 N+.\n1: Stage A 2: Generate an `\u00d7 2t Gaussian test matrix \u2126. 3: Set Y = (A>A)qA>\u2126, and compute QR-factorization: Y = QR. 4: dimension: [A]`\u00d7d, [\u2126]`\u00d72t, [Y]d\u00d72t, [Q]d\u00d72t 5: Stage B 6: Set B = AQ, and compute SVD: B = U\u03a3V\u0303>. 7: Set V = QV\u0303. 8: dimension: [B]`\u00d72t, [U]`\u00d7`, [\u03a3]`\u00d72t, [V\u0303]2t\u00d72t, [V]d\u00d72t\nOutput: \u03a3,V.\nProof. For each j, HjA succeeds with probability 99/100, meaning that for all x we have \u2016HjAx\u20162 = (1\u00b1 /9)\u2016Ax\u20162. Suppose for some j 6= j\u2032, HjA and Hj\u2032A are both successful. By definition we have\n\u2016HjAx\u20162 = (1\u00b1 /3)\u2016Hj\u2032Ax\u20162\nfor all x. Taking the SVD of the embeddings, this is equivalent to\n\u2016\u03a3jV>j x\u20162 = (1\u00b1 /3)\u2016\u03a3j\u2032V>j\u2032x\u20162\nfor all x. Making the change of variable y := \u03a3jV>j x, this is equivalent to\n\u2016y\u20162 = (1\u00b1 /3)\u2016\u03a3j\u2032V>j\u2032Vj\u03a3\u22121j y\u20162\nfor all y, which is true if and only if all singular values of \u03a3j\u2032V>j\u2032Vj\u03a3 \u22121 j are in [1\u2212 /3, 1 + /3].\nConversely, if all singular values of \u03a3j\u2032V>j\u2032Vj\u03a3 \u22121 j are in [1 \u2212 /3, 1 + /3], one can trace the steps backward to conclude that \u2016HjAx\u20162 = (1\u00b1 /3)\u2016Hj\u2032Ax\u20162 for all x. Since with probability at least 1\u2212\u03b4, a 9/10 fraction of the embeddings succeed with accuracy /9, there exists a j that can pass the test. It follows that any index j which passes the test in the algorithm with a majority of the j\u2032 6= j is a successful subspace embedding with accuracy .\nMoreover, if we choose a random j to compare to the remaining j\u2032, the expected number of choices of j until the test passes is only constant. Then finding the index j only takes an expected O(r) SVDs.\nThe time to do the SVD naively is O(d4/ 2). We can improve this by letting T be a fast JohnsonLindenstrauss transform matrix of dimensionO(dr/ 2)\u00d7O(d2/ 2), then we can replace HjA with THjA for all j \u2208 [d]. Then the verification procedure would only take O(d3r2/ 2) time."}, {"heading": "C.2 Proofs for Randomized SVD", "text": "The details of randomized SVD are presented in Algorithm 6, rephrased in our notations. We have the following analog of Lemma 1.\nLemma 15. Let A \u2208 R`\u00d7d be an ` \u00d7 d matrix (` > d). Let \u2208 (0, 1], k, t \u2208 N+ with d \u2212 1 \u2265 t \u2265 k+ d6k/ 2e\u2212 1. Let A\u0302 = AVV> where V is computed by Algorithm 6 with q = O(log max{`, d}). Then\nwith probability at least 1\u2212 3e\u2212t, for any matrix X with d rows and \u2016X\u20162F \u2264 k, we have\n\u2016(A\u2212 A\u0302)X\u20162F \u2264 2\n3 d\u2211 i=k+1 \u03c32i (A),\n\u2223\u2223\u2016AX\u20162F \u2212 \u2016A\u0302X\u20162F \u2223\u2223 \u2264 d\u2211 i=k+1 \u03c32i (A) + 2 \u2016AX\u20162F .\nThe algorithm runs in time O(qt`d+ t2(`+ d)).\nProof. As stated in Section 10.4 in (Halko et al., 2011), with probability at least 1\u2212 3e\u2212t, we have\n\u2016A\u2212 A\u0302\u2016S \u2264 2\u03c3t+1(A). (25)\nThen we have\n\u2016(A\u2212 A\u0302)X\u20162F \u2264 \u2016X\u20162F \u2016A\u2212 A\u0302\u20162S \u2264 2k\u03c32t+1(A)\nwhere the first inequality follows because the spectral norm is consistent with the Euclidean norm, and the second inequality follows from (25). For our choice of t, we have\nk\u03c32t+1(A) \u2264 2\n6 (t\u2212 k + 1)\u03c32t+1(A) \u2264\n2\n6 t+1\u2211 i=k+1 \u03c32i (A) \u2264 2 6 d\u2211 i=k+1 \u03c32i (A) \u2264 2 6 d2(A, LX),\nwhich leads to the first claim in the lemma. To prove the second claim, first note that \u2223\u2223\u2016AX\u2016F \u2212 \u2016A\u0302X\u2016F \u2223\u22232 \u2264 \u2016(A\u2212 A\u0302)X\u20162F \u2264 23 d2(A, LX). Then by Fact 1, we have \u2223\u2223\u2016AX\u20162F \u2212 \u2016A\u0302X\u20162F \u2223\u2223 \u2264 3 \u2223\u2223\u2016AX\u2016F \u2212 \u2016A\u0302X\u2016F \u2223\u22232 + 2 \u2016AX\u20162F \u2264 d2(A, LX) + 2 \u2016AX\u20162F which completes the proof."}, {"heading": "C.3 Proof of Theorem 6", "text": "Let T to be a diagonal block matrix with H1,H2, . . . ,Hs on the diagonal. Then Algorithm 3 is just to run Algorithm disPCA on TP to get the principal components V. Recall that the goal is to show P\u0303 = PVV> is a good proxy for the original data P with respect to `2 error fitting problems. It suffices to show that P\u0303 satisfies enjoys properties similar to those stated in Lemma 4.\nTo prove this, we begin with a lemma saying that TP\u0303 enjoys such properties, i.e. such properties are approximately preserved when replacing exact SVD with randomized SVD in Algorithm disPCA (Lemma 16). Then we can show that P\u0303 enjoys similar properties as TP\u0303, i.e. these properties are approximately preserved under subspace embedding (Lemma 18).\nLemma 16. For any d\u00d7 k matrix X with orthonormal columns,\n\u2016TPX\u2212TP\u0303X\u20162F \u2264 O( 2)d2(TP, LX) +O( 3)\u2016TPX\u20162F ,\u2223\u2223\u2223\u2016TPX\u20162F \u2212 \u2016TP\u0303X\u20162F \u2223\u2223\u2223 \u2264 O( )d2(TP, LX) +O( )\u2016TPX\u20162F . Proof. The proof follows that of Lemma 4 to TP. But now exact SVD is replaced with randomized SVD, so we need to argue that randomized SVD produces similar result as exact SVD in the sense of Lemma 7. This is already proved in Lemma 15. Also note that we need a technical lemma bounding the small error terms incurred on the intermediate result TP\u0302. This is done by Lemma 17.\nLemma 17.\n\u2016TP\u0302X\u20162F \u2264 d2(TP, LX) + (1 + 2 )\u2016TPX\u20162F , d2(TP\u0302, LX) \u2264 (1 + )d2(TP, LX) + \u2016TPX\u20162F .\nProof. For the first statement, by Lemma 15, we have\u2223\u2223\u2223\u2016TP\u0302X\u20162F \u2212 \u2016TPX\u20162F \u2223\u2223\u2223 \u2264 s\u2211 i=1 \u2223\u2223\u2223\u2016TPiX\u20162F \u2212 \u2016TP\u0302iX\u20162F \u2223\u2223\u2223 \u2264\ns\u2211 i=1 d2(TPi, LX) + 2 s\u2211 i=1 \u2016TPiX\u20162F\n\u2264 d2(TP, LX) + 2 \u2016TPX\u20162F . (26)\nFor the second statement, by Pythagorean Theorem, d2(TP\u0302, LX)\u2212 d2(TP, LX) = [ \u2016TP\u0302\u20162F \u2212 \u2016TP\u0302X\u20162F ] \u2212 [ \u2016TP\u20162F \u2212 \u2016TPX\u20162F ] = [ \u2016TP\u0302\u20162F \u2212 \u2016TP\u20162F ] + [ \u2016TPX\u20162F \u2212 \u2016TP\u0302X\u20162F\n] \u2264 \u2016TPX\u20162F \u2212 \u2016TP\u0302X\u20162F .\nThe second statement then follows from the last inequality and (26).\nLemma 18. For any d\u00d7 k matrix X with orthonormal columns,\n\u2016PX\u2212 P\u0303X\u20162F \u2264 O( 2)d2(P, LX) +O( 3)\u2016PX\u20162F ,\u2223\u2223\u2223\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2223\u2223\u2223 \u2264 O( )d2(P, LX) +O( )\u2016PX\u20162F . Proof. By the property of subspace embedding, we have \u2016TPX \u2212 TP\u0303X\u20162F = (1 \u00b1 )\u2016PX \u2212 P\u0303X\u20162F , \u2016TPX\u20162F = (1 \u00b1 )\u2016PX\u20162F and d2(TP, LX) = \u2016TP \u2212 TPXX>\u20162F = (1 \u00b1 )\u2016P \u2212 PXX>\u20162F = (1\u00b1 )d2(P, LX). Then\n(1 + )\u2016PX\u2212 P\u0303X\u20162F \u2264 \u2016TPX\u2212TP\u0303X\u20162F \u2264 O( 2)d2(TP, LX) +O( 3)\u2016TPX\u20162F \u2264 O( 2)d2(P, LX) +O( 3)\u2016PX\u20162F\nwhere the second inequality is from Lemma 16. This then leads to the first statement.\nFor the second statement, we have\n(1 + )\u2016PX\u20162F \u2212 (1\u2212 )\u2016P\u0303X\u20162F \u2264 \u2016TPX\u20162F \u2212 \u2016TP\u0303X\u20162F \u2264 O( )d2(TP, LX) +O( )\u2016TPX\u20162F \u2264 O( )d2(P, LX) +O( )\u2016PX\u20162F\nwhich leads to\n\u2016PX\u20162F \u2212 \u2016P\u0303X\u20162F \u2264 O( )d2(P, LX) +O( )\u2016PX\u20162F .\nA similar argument bounds \u2016P\u0303X\u20162F \u2212 \u2016PX\u20162F , which completes the proof.\nWe represent Theorem 6 in a general form for `2-error geometric fitting problems. Theorem 6. Suppose Algorithm 3 takes \u2208 (0, 1/2], t1 = t2 = O(max { k 2 , log s\u03b4 } ), ` = O(d 2 2 ), q = O(max{log d , log sk }) as input, and sets the failure probability of each local subspace embedding to \u03b4 \u2032 = \u03b4/2s. Let P\u0303 = PVV>. Then with probability at least 1 \u2212 \u03b4, there exists a constant c0 \u2265 0, such that for any set of k points L,\n(1\u2212 )d2(P,L)\u2212 \u2016PX\u20162F \u2264 d2(P\u0303,L) + c0 \u2264 (1 + )d2(P,L) + \u2016PX\u20162F\nwhere X is an orthonormal matrix whose columns span L. The total communication is O(skd/ 2) and the total time is O ( nnz(P) + s [ d3k 4 + k 2d2 6 ] log d log sk \u03b4 ) .\nProof. The proof of correctness follows the proof of Theorem 3, replacing the use of Lemma 4 with Lemma 18.\nOn each node vi, the subspace embedding takes time O(nnz(Pi)), and the randomized SVD takes time O(qt1`d+t 2 1(`+d)); on the central coordinator, the randomized SVD takes timeO(qt1(st1)d+t 2 1(st1+d)) since Y has O(st1) non-zero rows. The total running time then follows from the choice of the parameters. The total communication cost follows from the fact that the algorithm only sends \u03a3i(t1),Vi(t1) from each node to the central coordinator."}], "references": [{"title": "Principal component analysis for distributed data sets with updating", "author": ["Zheng-Jian Bai", "Raymond H Chan", "Franklin T Luk"], "venue": "In Proceedings of the International Conference on Advanced Parallel Processing Technologies,", "citeRegEx": "Bai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bai et al\\.", "year": 2005}, {"title": "Distributed k-means and k-median clustering on general communication topologies", "author": ["Maria-Florina Balcan", "Steven Ehrlich", "Yingyu Liang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Balcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2013}, {"title": "Stochastic dimensionality reduction for k-means clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Michael W. Mahoney", "Petros Drineas"], "venue": "CoRR, abs/1110.2897,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Spanner: Googles globallydistributed database", "author": ["James C Corbett", "Jeffrey Dean", "Michael Epstein", "Andrew Fikes", "Christopher Frost", "JJ Furman", "Sanjay Ghemawat", "Andrey Gubarev", "Christopher Heiser", "Peter Hochschild"], "venue": "In Proceedings of the USENIX Symposium on Operating Systems Design and Implementation,", "citeRegEx": "Corbett et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Corbett et al\\.", "year": 2012}, {"title": "A unified framework for approximating and clustering data", "author": ["Dan Feldman", "Michael Langberg"], "venue": "In Proceedings of the Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Feldman and Langberg.,? \\Q2011\\E", "shortCiteRegEx": "Feldman and Langberg.", "year": 2011}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "author": ["Dan Feldman", "Melanie Schmidt", "Christian Sohler"], "venue": "In Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Feldman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2013}, {"title": "Relative errors for deterministic low-rank matrix approximations", "author": ["Mina Ghashami", "Jeff M. Phillips"], "venue": "CoRR, abs/1307.7454,", "citeRegEx": "Ghashami and Phillips.,? \\Q2013\\E", "shortCiteRegEx": "Ghashami and Phillips.", "year": 2013}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A Tropp"], "venue": "SIAM review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Nimble algorithms for cloud computing", "author": ["Ravindran Kannan", "Santosh Vempala", "David Woodruff"], "venue": "arXiv preprint arXiv:1304.3162,", "citeRegEx": "Kannan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2013}, {"title": "Combining structured and unstructured randomness in large scale pca", "author": ["Nikos Karampatziakis", "Paul Mineiro"], "venue": "CoRR, abs/1310.6304,", "citeRegEx": "Karampatziakis and Mineiro.,? \\Q2013\\E", "shortCiteRegEx": "Karampatziakis and Mineiro.", "year": 2013}, {"title": "Distributed principal component analysis for wireless sensor", "author": ["Yann-A\u00ebl Le Borgne", "Sylvain Raybaud", "Gianluca Bontempi"], "venue": "networks. Sensors,", "citeRegEx": "Borgne et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Borgne et al\\.", "year": 2008}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Lee and Seung.,? \\Q2001\\E", "shortCiteRegEx": "Lee and Seung.", "year": 2001}, {"title": "Consensus-based distributed principal component analysis in wireless sensor networks", "author": ["Sergio Valcarcel Macua", "Pavle Belanovic", "Santiago Zazo"], "venue": "In Proceedings of the IEEE International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),", "citeRegEx": "Macua et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Macua et al\\.", "year": 2010}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "author": ["Xiangrui Meng", "Michael W Mahoney"], "venue": "In Proceedings of the Annual ACM symposium on Symposium on theory of computing,", "citeRegEx": "Meng and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Meng and Mahoney.", "year": 2013}, {"title": "Characterizing web-based video sharing workloads", "author": ["Siddharth Mitra", "Mayank Agrawal", "Amit Yadav", "Niklas Carlsson", "Derek Eager", "Anirban Mahanti"], "venue": "ACM Transactions on the Web,", "citeRegEx": "Mitra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2011}, {"title": "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings", "author": ["Jelani Nelson", "Huy L Nguy\u00ean"], "venue": "arXiv preprint arXiv:1211.1002,", "citeRegEx": "Nelson and Nguy\u00ean.,? \\Q2012\\E", "shortCiteRegEx": "Nelson and Nguy\u00ean.", "year": 2012}, {"title": "Adaptive filters for continuous queries over distributed data streams", "author": ["Chris Olston", "Jing Jiang", "Jennifer Widom"], "venue": "In Proceedings of the ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Olston et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Olston et al\\.", "year": 2003}, {"title": "Principal component analysis for dimension reduction in massive distributed data sets", "author": ["Yongming Qu", "George Ostrouchov", "Nagiza Samatova", "Al Geist"], "venue": "In Proceedings of IEEE International Conference on Data Mining,", "citeRegEx": "Qu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2002}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tam\u00e1s Sarl\u00f3s"], "venue": "In FOCS,", "citeRegEx": "Sarl\u00f3s.,? \\Q2006\\E", "shortCiteRegEx": "Sarl\u00f3s.", "year": 2006}, {"title": "The algorithm runs in time O(qt`d+ t2(`+ d))", "author": ["\u2016AX\u2016F"], "venue": "(Halko et al.,", "citeRegEx": ".,? \\Q2011\\E", "shortCiteRegEx": ".", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "Since data is often partitioned across multiple servers (Olston et al., 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model.", "startOffset": 56, "endOffset": 119}, {"referenceID": 4, "context": "Since data is often partitioned across multiple servers (Olston et al., 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model.", "startOffset": 56, "endOffset": 119}, {"referenceID": 15, "context": "Since data is often partitioned across multiple servers (Olston et al., 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model.", "startOffset": 56, "endOffset": 119}, {"referenceID": 12, "context": "Moreover, it can serve as a preprocessing step to reduce the data dimension in various machine learning tasks, such as k-means, Non-Negative Matrix Factorization (NNMF) (Lee and Seung, 2001) and Latent Dirichlet Allocation (LDA) (Blei et al.", "startOffset": 169, "endOffset": 190}, {"referenceID": 6, "context": "A beautiful property of the coresets developed in (Feldman et al., 2013) is that for approximate PCA their size also only depends linearly on the dimension d, whereas previous coresets depended quadratically on d (Feldman and Langberg, 2011).", "startOffset": 50, "endOffset": 72}, {"referenceID": 5, "context": ", 2013) is that for approximate PCA their size also only depends linearly on the dimension d, whereas previous coresets depended quadratically on d (Feldman and Langberg, 2011).", "startOffset": 148, "endOffset": 176}, {"referenceID": 4, "context": ", 2003; Corbett et al., 2012; Mitra et al., 2011), there is an increased interest in computing on it in the distributed model. A basic tool for distributed data analysis is Principal Component Analysis (PCA). The goal of PCA is to find an r-dimensional (affine) subspace that captures as much of the variance of the data as possible. Hence, it can reveal lowdimensional structure in very high dimensional data. Moreover, it can serve as a preprocessing step to reduce the data dimension in various machine learning tasks, such as k-means, Non-Negative Matrix Factorization (NNMF) (Lee and Seung, 2001) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). In the distributed model, approximate PCA was used by Feldman et al. (2013) for solving a number of shape fitting problems such as k-means clustering, where the approximation PCA solution is computed based on a summary of the data called coreset.", "startOffset": 8, "endOffset": 737}, {"referenceID": 6, "context": "For approximate distributed PCA, the following protocol is implicit in (Feldman et al., 2013): each server i computes its top O(r/ ) principal components Yi of Pi and sends them to the coordinator.", "startOffset": 71, "endOffset": 93}, {"referenceID": 6, "context": "5 in (Feldman et al., 2013), which shows that given a data matrix P, if we project the rows onto the space spanned by the top O(k/ 2) principal components, and solve the k-means problem in this subspace, we obtain a (1 + )approximation.", "startOffset": 5, "endOffset": 27}, {"referenceID": 1, "context": "One feature of this approach is that by using the distributed k-means algorithm in (Balcan et al., 2013) on the projected data, the coordinator can sample points from the servers proportional to their local k-means cost solutions, which reduces the communication roughly by a factor of s in the k-means step, which would come from each server sending their local k-means coreset to the coordinator.", "startOffset": 83, "endOffset": 104}, {"referenceID": 2, "context": "For example, if we want a 1 + approximation factor, we can set d\u2032 = O(log n/ 2) by a Johnson-Lindenstrauss transform; if we want a larger 2 + approximation factor, we can set d\u2032 = O(k/ 2) using (Boutsidis et al., 2011).", "startOffset": 194, "endOffset": 218}, {"referenceID": 19, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 3, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 16, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 14, "context": "to instead have each server first sample an oblivious subspace embedding (OSE) (Sarl\u00f3s, 2006; Clarkson and Woodruff, 2013; Nelson and Nguy\u00ean, 2012; Meng and Mahoney, 2013) matrix Hi, and instead run the algorithm on the point set defined by the rows of HiPi.", "startOffset": 79, "endOffset": 171}, {"referenceID": 16, "context": "This number of rows can be further reducted to O(d log d/ 2) if one is willing to spend O(nnz(Pi) log d/ ) time (Nelson and Nguy\u00ean, 2012).", "startOffset": 112, "endOffset": 137}, {"referenceID": 18, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 0, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 13, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 6, "context": "Related Work A number of algorithms for approximate distributed PCA have been proposed (Qu et al., 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication.", "startOffset": 87, "endOffset": 188}, {"referenceID": 6, "context": "Most closely related to our work is (Feldman et al., 2013), which observes that the top singular vectors of the local point set can be viewed as its summary and the union of the local summaries can be viewed as a summary of the global data, i.", "startOffset": 36, "endOffset": 58}, {"referenceID": 9, "context": "In (Kannan et al., 2013) the authors study algorithms in the arbitrary partition model in which each server holds a matrix Pi and P = \u2211s i=1 Pi.", "startOffset": 3, "endOffset": 24}, {"referenceID": 9, "context": "Moreover, our k-means algorithms are more general, in the sense that they do not make a well-separability assumption, and more efficient in that the communication of (Kannan et al., 2013) is O(sd2) + s(k/ )O(1) words as opposed to our O(sdk/ 2) + sk + (k/ )O(1).", "startOffset": 166, "endOffset": 187}, {"referenceID": 7, "context": "Other related work includes the recent (Ghashami and Phillips, 2013) (see also the references therein), who give a deterministic streaming algorithm for low rank approximation in which each point of P is seen one at a time and uses O(dk/ ) words of communication.", "startOffset": 39, "endOffset": 68}, {"referenceID": 0, "context": ", 2002; Bai et al., 2005; Le Borgne et al., 2008; Macua et al., 2010; Feldman et al., 2013), but either without theoretical guarantees, or without considering communication. Qu et al. (2002) proposed an algorithm but provided no analysis on the tradeoff between communication and approximation.", "startOffset": 8, "endOffset": 191}, {"referenceID": 10, "context": "Speeding up large scale PCA using different versions of subspace embeddings was also considered in (Karampatziakis and Mineiro, 2013), though not in a distributed setting and not for `2-error shape fitting problems.", "startOffset": 99, "endOffset": 133}, {"referenceID": 18, "context": "Algorithm disPCA for distributed PCA is suggested in (Qu et al., 2002; Feldman et al., 2013), which consists of a local stage and a global stage.", "startOffset": 53, "endOffset": 92}, {"referenceID": 6, "context": "Algorithm disPCA for distributed PCA is suggested in (Qu et al., 2002; Feldman et al., 2013), which consists of a local stage and a global stage.", "startOffset": 53, "endOffset": 92}, {"referenceID": 6, "context": "These lead to the following theorem, which is implicit in (Feldman et al., 2013), stating that the algorithm can produce a (1 + )-approximation for the distributed PCA problem.", "startOffset": 58, "endOffset": 80}, {"referenceID": 1, "context": "2: Run the distributed k-means clustering algorithm in (Balcan et al., 2013) on {PiEE}i=1, usingA\u03b1 as a subroutine, to get k centers L.", "startOffset": 55, "endOffset": 76}, {"referenceID": 1, "context": "As a concrete example, we can use original data (d\u2032 = d), then run Algorithm disPCA, and finally run the distributed clustering algorithm in (Balcan et al., 2013) which uses any non-distributed \u03b1-approximation algorithm as a subroutine and computes a (1 + )\u03b1-approximate solution.", "startOffset": 141, "endOffset": 162}, {"referenceID": 19, "context": "The work of (Sarl\u00f3s, 2006) pioneered subspace embeddings.", "startOffset": 12, "endOffset": 26}, {"referenceID": 3, "context": "The recent fast sparse subspace embeddings (Clarkson and Woodruff, 2013) and its optimizations (Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) are particularly suitable for large scale sparse data sets, since their running time is linear in the number of non-zero entries in the data matrix, and they also preserve the sparsity of the data.", "startOffset": 43, "endOffset": 72}, {"referenceID": 14, "context": "The recent fast sparse subspace embeddings (Clarkson and Woodruff, 2013) and its optimizations (Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) are particularly suitable for large scale sparse data sets, since their running time is linear in the number of non-zero entries in the data matrix, and they also preserve the sparsity of the data.", "startOffset": 95, "endOffset": 144}, {"referenceID": 16, "context": "The recent fast sparse subspace embeddings (Clarkson and Woodruff, 2013) and its optimizations (Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) are particularly suitable for large scale sparse data sets, since their running time is linear in the number of non-zero entries in the data matrix, and they also preserve the sparsity of the data.", "startOffset": 95, "endOffset": 144}, {"referenceID": 8, "context": "Here we show that the randomized SVD algorithm from (Halko et al., 2011) can be applied to speed up the computation without compromising the quality of the solution much.", "startOffset": 52, "endOffset": 72}, {"referenceID": 1, "context": "For k-means, we run the algorithm in (Balcan et al., 2013) (with Lloyd\u2019s method as a subroutine) on the projected data to get a solution.", "startOffset": 37, "endOffset": 58}, {"referenceID": 6, "context": "1 in (Feldman et al., 2013).", "startOffset": 5, "endOffset": 27}, {"referenceID": 6, "context": "The proof follows that in (Feldman et al., 2013), with slight modification for the distributed setting.", "startOffset": 26, "endOffset": 48}, {"referenceID": 3, "context": "Algorithm 4 Fast Sparse Subspace Embedding (Clarkson and Woodruff, 2013) Input: parameters n, ` \u2208 N+.", "startOffset": 43, "endOffset": 72}, {"referenceID": 3, "context": "(Clarkson and Woodruff, 2013; Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) Suppose n > d and ` = O( 2 2 ).", "startOffset": 0, "endOffset": 78}, {"referenceID": 14, "context": "(Clarkson and Woodruff, 2013; Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) Suppose n > d and ` = O( 2 2 ).", "startOffset": 0, "endOffset": 78}, {"referenceID": 16, "context": "(Clarkson and Woodruff, 2013; Meng and Mahoney, 2013; Nelson and Nguy\u00ean, 2012) Suppose n > d and ` = O( 2 2 ).", "startOffset": 0, "endOffset": 78}, {"referenceID": 8, "context": "Algorithm 6 Randomized SVD (Halko et al., 2011) Input: matrix A \u2208 R`\u00d7d; parameters t, q \u2208 N+.", "startOffset": 27, "endOffset": 47}, {"referenceID": 8, "context": "4 in (Halko et al., 2011), with probability at least 1\u2212 3e\u2212t, we have \u2016A\u2212 \u00c2\u2016S \u2264 2\u03c3t+1(A).", "startOffset": 5, "endOffset": 25}], "year": 2017, "abstractText": "We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as k-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for k-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest.", "creator": "LaTeX with hyperref package"}}}