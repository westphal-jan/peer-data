{"id": "1408.6141", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2014", "title": "Recursive Total Least-Squares Algorithm Based on Inverse Power Method and Dichotomous Coordinate-Descent Iterations", "abstract": "We develop a recursive total least-squares (RTLS) algorithm for errors-in-variables system identification utilizing the inverse power method and the dichotomous coordinate-descent (DCD) iterations. The proposed algorithm, called DCD-RTLS, outperforms the previously-proposed RTLS algorithms, which are based on the line search method, with reduced computational complexity. We perform a comprehensive analysis of the DCD-RTLS algorithm and show that it is asymptotically unbiased as well as being stable in the mean. We also find a lower bound for the forgetting factor that ensures mean-square stability of the algorithm and calculate the theoretical steady-state mean-square deviation (MSD). We verify the effectiveness of the proposed algorithm and the accuracy of the predicted steady-state MSD via simulations. Our results show that the RTLS algorithm is superior to the previous methods and, although it is still a very slow approximation of the current model, it can still be used to accurately model the distribution of errors.\n\n\n\n\n\n\nWe note that this paper has been endorsed in the peer-reviewed literature and is therefore a recommended research article.", "histories": [["v1", "Mon, 25 Aug 2014 17:40:44 GMT  (708kb)", "http://arxiv.org/abs/1408.6141v1", null], ["v2", "Tue, 11 Nov 2014 00:44:23 GMT  (726kb)", "http://arxiv.org/abs/1408.6141v2", null]], "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["reza arablouei", "kutluy{\\i}l do\\u{g}an\\c{c}ay", "stefan werner"], "accepted": false, "id": "1408.6141"}, "pdf": {"name": "1408.6141.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["reza.arablouei@unisa.edu.au;", "kutluyil.dogancay@unisa.edu.au).", "stefan.werner@aalto.fi)."], "sections": [{"heading": null, "text": "algorithm for errors-in-variables system identification utilizing the inverse power method and the dichotomous coordinatedescent (DCD) iterations. The proposed algorithm, called DCDRTLS, outperforms the previously-proposed RTLS algorithms, which are based on the line search method, with reduced computational complexity. We perform a comprehensive analysis of the DCD-RTLS algorithm and show that it is asymptotically unbiased as well as being stable in the mean. We also find a lower bound for the forgetting factor that ensures mean-square stability of the algorithm and calculate the theoretical steadystate mean-square deviation (MSD). We verify the effectiveness of the proposed algorithm and the accuracy of the predicted steady-state MSD via simulations.\nIndex Terms\u2014Adaptive filtering; dichotomous coordinatedescent algorithm; inverse power method; performance analysis; total least-squares.\nI. INTRODUCTION\nost linear systems in signal processing, control, or econometrics applications can be described using the\nerrors-in-variables (EIV) models. In the EIV models, both input and output data of a linear system are assumed to be contaminated with additive noise [1], [2]. The total leastsquares (TLS) method is well suited for identification of the EIV models [3], [4]. Unlike the least-squares (LS) method that minimizes the sum of squared estimation errors without accounting for the noise in the input data, TLS is devised to minimize the perturbation in both input and output data that makes the input-output data fit through a linear system. Therefore, TLS is expected to have superior performance to LS when the input data as well as the output data is noisy.\nIt has been shown that the TLS estimate of a linear system is in fact the eigenvector corresponding to the eigenvalue of its augmented and weighted data covariance matrix that has the smallest absolute value [5]. A popular methodology to\nThis work was supported in part by the Academy of Finland. R. Arablouei and K. Do\u011fan\u00e7ay are with the School of Engineering and the Institute for Telecommunications Research, University of South Australia, Mawson Lakes SA 5095, Australia (email: reza.arablouei@unisa.edu.au; kutluyil.dogancay@unisa.edu.au).\nS. Werner is with the Department of Signal Processing and Acoustics, School of Electrical Engineering, Aalto University, Espoo, Finland (email: stefan.werner@aalto.fi).\ncompute this eigenvector, also known as the minor component, is to minimize a Rayleigh quotient (RQ) cost function [6] using any optimization technique such as gradient descent [7], [8] or line search [9], [10]. Another common practice is to utilize the inverse power method (inverse iteration) [11]-[13].\nThe recursive TLS (RTLS) algorithms proposed in [9], [10], and [12] for adaptive finite-impulse-response (FIR) filtering have computational complexities of order \ud835\udc42(\ud835\udc3f), where \ud835\udc3f is the system order, when the input data is shift-structured. All three algorithms essentially accomplish the same task of computing the adaptive FIR filter weights by recursively estimating the minor component from the data available up to the current time. As a result, they perform very similarly at the steady state.\nIn [12], it is shown that the RTLS algorithm proposed therein, which is based on the inverse power method and a line search strategy, is asymptotically consistent, i.e., converges to the optimal solution with probability one, when the perfect statistical knowledge of the input signal and noise is available or alternatively the forgetting factor is set to unity.\nIn this paper, we propose an RTLS algorithm for estimating the parameters of an EIV model for adaptive FIR filtering by employing the inverse power method together with the dichotomous coordinate-descent (DCD) iterations. We utilize the DCD iterations to solve two systems of linear equations associated with the calculation of two auxiliary vector variables. The DCD algorithm is a shift-and-add algorithm and can solve a system of linear equation using only additions and bit shifts with no multiplication [22]. As a result, the proposed algorithm, called DCD-RTLS, has a computational complexity of \ud835\udc42(\ud835\udc3f) when there is a shift structure in the input vectors. It requires 10\ud835\udc3f + 2 multiplications as opposed to 15\ud835\udc3f + 11, 16\ud835\udc3f + 19, and 22\ud835\udc3f + 69 multiplications required by the algorithms of [9], [10], and [12], respectively. In addition, simulation results show that the DCD-RTLS algorithm outperforms these algorithms in terms of convergence speed.\nWe examine the mean and mean-square performance of the DCD-RTLS algorithm. We show that it is convergent in the mean and asymptotically unbiased. We calculate a lower bound for the forgetting factor that guarantees mean-square stability of the DCD-RTLS algorithm. We show that, at the steady state, the DCD-RTLS algorithm performs akin to a\nM\nbias-compensated recursive LS (BCRLS) algorithm. We also calculate the theoretical steady-state mean-square deviation (MSD) of the DCD-RTLS algorithm and corroborate its accuracy through numerical examples."}, {"heading": "II. SIGNAL AND SYSTEM MODEL", "text": "Consider a linear system of order \ud835\udc3f \u2208 \u2115 described by\n\ud835\udc66\ud835\udc5b = \ud835\udc31\ud835\udc5b \ud835\udc47\ud835\udc21 (1)\nwhere \ud835\udc21 \u2208 \u211d\ud835\udc3f\u00d71 is the column vector of the system parameters, superscript \ud835\udc47 denotes matrix transposition, and \ud835\udc31\ud835\udc5b \u2208 \u211d \ud835\udc3f\u00d71 and \ud835\udc66\ud835\udc5b \u2208 \u211d are the input and output of the system at time index \ud835\udc5b \u2208 \u2115, respectively. We observe noisy versions of \ud835\udc31\ud835\udc5b and \ud835\udc66\ud835\udc5b, i.e.,\n\ud835\u0303\udc31\ud835\udc5b = \ud835\udc31\ud835\udc5b + \ud835\udc2e\ud835\udc5b (2)\nand\n\ud835\u0303\udc66\ud835\udc5b = \ud835\udc66\ud835\udc5b + \ud835\udf08\ud835\udc5b (3)\nwhere \ud835\udc2e\ud835\udc5b \u2208 \u211d \ud835\udc3f\u00d71 and \ud835\udf08\ud835\udc5b \u2208 \u211d are the corresponding input and output noises. Substituting (2) and (3) into (1) gives\n\ud835\u0303\udc66\ud835\udc5b = \ud835\udc21 \ud835\udc47(\ud835\u0303\udc31\ud835\udc5b \u2212 \ud835\udc2e\ud835\udc5b) + \ud835\udf08\ud835\udc5b. (4)\nWe make the following assumptions regarding the noises: A1: The entries of the input noise vector are wide-sense\nstationary with zero mean and variance \ud835\udf02 \u2208 \u211d\u22650.\nA2: The output noise is wide-sense stationary with zero\nmean and variance \ud835\udf09 \u2208 \u211d\u22650. We also define\n\ud835\udc17\ud835\udc5b = [\ud835\udf06 (\ud835\udc5b\u22121)/2\ud835\u0303\udc311, \ud835\udf06 (\ud835\udc5b\u22122)/2\ud835\u0303\udc312, \u2026 , \ud835\udf06 1/2\ud835\u0303\udc31\ud835\udc5b\u22121, \ud835\u0303\udc31\ud835\udc5b],\n\ud835\u0303\udc32\ud835\udc5b = [\ud835\udf06 (\ud835\udc5b\u22121)/2\ud835\u0303\udc661, \ud835\udf06 (\ud835\udc5b\u22122)/2\ud835\u0303\udc662, \u2026 , \ud835\udf06 1 2\u2044 \ud835\u0303\udc66\ud835\udc5b\u22121, \ud835\u0303\udc66\ud835\udc5b],\n\ud835\udebd\ud835\udc5b = \ud835\udc17\ud835\udc5b\ud835\udc17\ud835\udc5b\n\ud835\udc47\n= \ud835\udf06\ud835\udebd\ud835\udc5b\u22121 + \ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 ,\n(5)\n\ud835\udc33\ud835\udc5b = \ud835\udc17\ud835\udc5b\ud835\u0303\udc32\ud835\udc5b \ud835\udc47 = \ud835\udf06\ud835\udc33\ud835\udc5b\u22121 + \ud835\u0303\udc66\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b\n(6)\n\ud835\udf0f\ud835\udc5b = \ud835\u0303\udc32\ud835\udc5b\ud835\u0303\udc32\ud835\udc5b \ud835\udc47\n= \ud835\udf06\ud835\udf0f\ud835\udc5b\u22121 + \ud835\u0303\udc66\ud835\udc5b 2,\n\ud835\udefe = \ud835\udf09/\ud835\udf02,\n\ud835\udc03 = [ \ud835\udc08 \ud835\udfce\n\ud835\udfce\ud835\udc47 \ud835\udefe\u22121/2 ],\nand\n\ud835\udebf\ud835\udc5b = \ud835\udc03 \ud835\udc47 [ \ud835\udc17\ud835\udc5b \ud835\u0303\udc32\ud835\udc5b ] [\ud835\udc17\ud835\udc5b \ud835\udc47 , \ud835\u0303\udc32\ud835\udc5b \ud835\udc47]\ud835\udc03\n= [ \ud835\udebd\ud835\udc5b \ud835\udefe \u22121/2\ud835\udc33\ud835\udc5b \ud835\udefe\u22121/2\ud835\udc33\ud835\udc5b \ud835\udc47 \ud835\udefe\u22121\ud835\udf0f\ud835\udc5b ]\nwhere \ud835\udf06 \u2208 \u211d>0 is a forgetting factor that satisfies\n0 \u226a \ud835\udf06 < 1,\n\ud835\udfce is the \ud835\udc3f \u00d7 1 zero vector, and \ud835\udc08 the \ud835\udc3f \u00d7 \ud835\udc3f identity matrix."}, {"heading": "III. ALGORITHM DESCRIPTION", "text": ""}, {"heading": "A. Recursive Total Least-Squares", "text": "The TLS estimate of the system parameters at time instant\n\ud835\udc5b, denoted by \ud835\udc30\ud835\udc5b \u2208 \u211d \ud835\udc3f\u00d71, is given by\n[ \ud835\udc30\ud835\udc5b \u22121 ] = \u2212 \ud835\udc2a\ud835\udc5b\n\ud835\udefe\u22121/2\ud835\udc5e\ud835\udc3f+1,\ud835\udc5b\nwhere \ud835\udc2a\ud835\udc5b \u2208 \u211d (\ud835\udc3f+1)\u00d71 is the eigenvector corresponding to the smallest (in absolute value) eigenvalue of the augmented and weighted data covariance matrix \ud835\udebf\ud835\udc5b and \ud835\udc5e\ud835\udc3f+1,\ud835\udc5b is (\ud835\udc3f + 1)th entry of \ud835\udc2a\ud835\udc5b [5]. The eigenvector \ud835\udc2a\ud835\udc5b can be estimated adaptively by executing a single iteration of the inverse power method [14] at each time instant using the following recursion:\n\ud835\udc2a\ud835\udc5b = \ud835\udebf\ud835\udc5b \u22121\ud835\udc2a\ud835\udc5b\u22121. (7)\nMultiplying both sides of (7) by \ud835\udebf\ud835\udc5b/(\ud835\udefe \u22121/2\ud835\udc5e\ud835\udc3f+1,\ud835\udc5b\u22121\ud835\udc5e\ud835\udc3f+1,\ud835\udc5b) gives\n\ud835\udebf\ud835\udc5b [ \ud835\udc30\ud835\udc5b\n\u2212\ud835\udefe1/2 ] = \ud835\udc5e\ud835\udc3f+1,\ud835\udc5b\u22121 \ud835\udc5e\ud835\udc3f+1,\ud835\udc5b [ \ud835\udc30\ud835\udc5b\u22121 \u2212\ud835\udefe1/2 ]\nor equivalently\n\ud835\udebd\ud835\udc5b\ud835\udc30\ud835\udc5b \u2212 \ud835\udc33\ud835\udc5b = \ud835\udc5e\ud835\udc3f+1,\ud835\udc5b\u22121\n\ud835\udc5e\ud835\udc3f+1,\ud835\udc5b \ud835\udc30\ud835\udc5b\u22121 (8)\nand\n\ud835\udc33\ud835\udc5b \ud835\udc47\ud835\udc30\ud835\udc5b \u2212 \ud835\udf0f\ud835\udc5b = \u2212\ud835\udefe \ud835\udc5e\ud835\udc3f+1,\ud835\udc5b\u22121 \ud835\udc5e\ud835\udc3f+1,\ud835\udc5b . (9)\nSubstituting (9) into (8) gives\n\ud835\udebd\ud835\udc5b\ud835\udc30\ud835\udc5b \u2212 \ud835\udc33\ud835\udc5b = \u2212\ud835\udefe \u22121\ud835\udc30\ud835\udc5b\u22121\ud835\udc33\ud835\udc5b \ud835\udc47\ud835\udc30\ud835\udc5b + \ud835\udefe \u22121\ud835\udf0f\ud835\udc5b\ud835\udc30\ud835\udc5b\u22121. (10)\nSolving (10) for \ud835\udc30\ud835\udc5b yields an RTLS estimate of \ud835\udc21 as\n\ud835\udc30\ud835\udc5b = (\ud835\udebd\ud835\udc5b + \ud835\udefe \u22121\ud835\udc30\ud835\udc5b\u22121\ud835\udc33\ud835\udc5b \ud835\udc47)\u22121(\ud835\udc33\ud835\udc5b + \ud835\udefe \u22121\ud835\udf0f\ud835\udc5b\ud835\udc30\ud835\udc5b\u22121). (11)\nEmploying the Sherman-Morrison formula [14], we can avoid the matrix inversion in (11) by writing it as\n\ud835\udc30\ud835\udc5b = (\ud835\udebd\ud835\udc5b \u22121 \u2212\n\ud835\udebd\ud835\udc5b \u22121\ud835\udc30\ud835\udc5b\u22121\ud835\udc33\ud835\udc5b \ud835\udc47\ud835\udebd\ud835\udc5b \u22121 \ud835\udefe + \ud835\udc33\ud835\udc5b \ud835\udc47\ud835\udebd\ud835\udc5b \u22121\ud835\udc30\ud835\udc5b\u22121 ) (\ud835\udc33\ud835\udc5b + \ud835\udefe \u22121\ud835\udf0f\ud835\udc5b\ud835\udc30\ud835\udc5b\u22121). (12)"}, {"heading": "B. Utilization of the DCD Algorithm", "text": "Defining\n\ud835\udc261,\ud835\udc5b = \ud835\udebd\ud835\udc5b \u22121\ud835\udc33\ud835\udc5b (13)\nand\n\ud835\udc262,\ud835\udc5b = \ud835\udebd\ud835\udc5b \u22121\ud835\udc30\ud835\udc5b\u22121, (14)\n(12) can be written as\n\ud835\udc30\ud835\udc5b = \ud835\udc261,\ud835\udc5b + \ud835\udefe \u22121\ud835\udf0f\ud835\udc5b\ud835\udc262,\ud835\udc5b\n\u2212 \ud835\udc33\ud835\udc5b\n\ud835\udc47(\ud835\udc261,\ud835\udc5b + \ud835\udefe \u22121\ud835\udf0f\ud835\udc5b\ud835\udc262,\ud835\udc5b)\n\ud835\udefe + \ud835\udc33\ud835\udc5b \ud835\udc47\ud835\udc262,\ud835\udc5b\n\ud835\udc262,\ud835\udc5b. (15)\nIn order to reduce the computational complexity, instead of\ncalculating \ud835\udc261,\ud835\udc5b and \ud835\udc262,\ud835\udc5b directly from (13) and (14), we can compute them by solving the following systems of linear equations utilizing the DCD iterations and exerting only additions and bit-shifts:\n\ud835\udebd\ud835\udc5b\ud835\udc261,\ud835\udc5b = \ud835\udc33\ud835\udc5b (16)\nand\n\ud835\udebd\ud835\udc5b\ud835\udc262,\ud835\udc5b = \ud835\udc30\ud835\udc5b\u22121. (17)\nFurthermore, to exploit the full potential of the DCD algorithm and to minimize the number of its required iterations, we can rewrite (16) and (17) as\n\ud835\udebd\ud835\udc5b\ud835\udc1d1,\ud835\udc5b = \ud835\udc291,\ud835\udc5b (18)\nand\n\ud835\udebd\ud835\udc5b\ud835\udc1d2,\ud835\udc5b = \ud835\udc292,\ud835\udc5b (19)\nwhere\n\ud835\udc1d1,\ud835\udc5b = \ud835\udc261,\ud835\udc5b \u2212 \ud835\udc261,\ud835\udc5b\u22121,\n\ud835\udc1d2,\ud835\udc5b = \ud835\udc262,\ud835\udc5b \u2212 \ud835\udc262,\ud835\udc5b\u22121,\n\ud835\udc291,\ud835\udc5b = \ud835\udc33\ud835\udc5b \u2212 \ud835\udebd\ud835\udc5b\ud835\udc261,\ud835\udc5b\u22121, (20)\nand\n\ud835\udc292,\ud835\udc5b = \ud835\udc30\ud835\udc5b\u22121 \u2212 \ud835\udebd\ud835\udc5b\ud835\udc262,\ud835\udc5b\u22121 (21)\nand solve (18) and (19) instead of (16) and (17).\nThe precision of the solutions to the systems of linear equations in (18) and (19) provided by the DCD algorithm can be represented by the residual vectors that are defined as\n\ud835\udc2b1,\ud835\udc5b\u22121 = \ud835\udc33\ud835\udc5b\u22121 \u2212 \ud835\udebd\ud835\udc5b\u22121\ud835\udc261,\ud835\udc5b\u22121 (22)\nand\n\ud835\udc2b2,\ud835\udc5b\u22121 = \ud835\udc30\ud835\udc5b\u22122 \u2212 \ud835\udebd\ud835\udc5b\u22121\ud835\udc262,\ud835\udc5b\u22121 (23)\nat time instant \ud835\udc5b \u2212 1. We can use these residual vectors to improve the accuracy of the solutions at time instant \ud835\udc5b. Substitution of (5) and (6) into (20) and (21) together with using (22) and (23) results in\n\ud835\udc291,\ud835\udc5b = \ud835\udf06\ud835\udc2b1,\ud835\udc5b\u22121 + (\ud835\u0303\udc66\ud835\udc5b \u2212 \ud835\u0303\udc31\ud835\udc5b \ud835\udc47\ud835\udc261,\ud835\udc5b\u22121)\ud835\u0303\udc31\ud835\udc5b\nand\n\ud835\udc292,\ud835\udc5b = \ud835\udf06\ud835\udc2b2,\ud835\udc5b\u22121 + \ud835\udc30\ud835\udc5b\u22121 \u2212 \ud835\udf06\ud835\udc30\ud835\udc5b\u22122 \u2212 (\ud835\u0303\udc31\ud835\udc5b \ud835\udc47\ud835\udc262,\ud835\udc5b\u22121)\ud835\u0303\udc31\ud835\udc5b.\nSolving (18) and (19) using the DCD algorithm yields \ud835\udc1d1,\ud835\udc5b, \ud835\udc1d2,\ud835\udc5b, \ud835\udc2b1,\ud835\udc5b, and \ud835\udc2b2,\ud835\udc5b at each time instant \ud835\udc5b [23]. Having calculated \ud835\udc1d1,\ud835\udc5b and \ud835\udc1d2,\ud835\udc5b, we obtain \ud835\udc261,\ud835\udc5b and \ud835\udc262,\ud835\udc5b using\n\ud835\udc261,\ud835\udc5b = \ud835\udc261,\ud835\udc5b\u22121 + \ud835\udc1d1,\ud835\udc5b\nand\n\ud835\udc262,\ud835\udc5b = \ud835\udc262,\ud835\udc5b\u22121 + \ud835\udc1d2,\ud835\udc5b.\nWe can then update the filter weight vector \ud835\udc30\ud835\udc5b, which is an estimate of \ud835\udc21 at time instant \ud835\udc5b, using (15).\nWe summarize the proposed DCD-RTLS algorithm in\nTable I. We also present the DCD algorithm solving (18) in Table II where \ud835\udc5f\ud835\udc59,\ud835\udc56,\ud835\udc5b and \ud835\udc51\ud835\udc59,\ud835\udc56,\ud835\udc5b denote the \ud835\udc59th entry of the vectors \ud835\udc2b\ud835\udc56,\ud835\udc5b and \ud835\udc1d\ud835\udc56,\ud835\udc5b, respectively, while \ud835\udf19\ud835\udc59,\ud835\udc59,\ud835\udc5b and \ud835\udedf\ud835\udc59,\ud835\udc5b are the (\ud835\udc59, \ud835\udc59)th entry and the \ud835\udc59th column of the matrix \ud835\udebd\ud835\udc5b, respectively. Three design parameters, \ud835\udc41 \u2208 \u2115, \ud835\udc40 \u2208 \u2115, and \ud835\udc3b \u2208 \u211d govern the accuracy and complexity of the DCD algorithm [22]. The parameter \ud835\udc40 is the number of bits used to represent the entries of the solution vectors, \ud835\udc1d1,\ud835\udc5b and \ud835\udc1d2,\ud835\udc5b, as fixed-point words within an amplitude range of [\u2212\ud835\udc3b, \ud835\udc3b]. At each run, the DCD algorithm renders maximum \ud835\udc41 iterative updates. Thus, \ud835\udc41 defines the maximum number of entries in \ud835\udc261,\ud835\udc5b and \ud835\udc262,\ud835\udc5b that are updated at each time instant. Consequently, when \ud835\udc41 < \ud835\udc3f, the DCD iterations implements a form of selective partial updates [25], [26]."}, {"heading": "IV. COMPUTATIONAL COMPLEXITY", "text": "Since \ud835\udebd\ud835\udc5b is symmetric, it is sufficient to update only its upper-triangular part. Moreover, by selecting the forgetting factor as \ud835\udf06 = 1 \u2212 2\u2212\ud835\udc43 where \ud835\udc43 is a positive integer, we can replace multiplications by \ud835\udf06 with additions and bit-shifts [23]. When there is shift structure in the input data, i.e., the input vector has the following form:\n\ud835\u0303\udc31\ud835\udc5b = [\ud835\u0303\udc65\ud835\udc5b , \ud835\u0303\udc65\ud835\udc5b\u22121, \u2026 , \ud835\u0303\udc65\ud835\udc5b\u2212\ud835\udc3f+1] \ud835\udc47\nwhere \ud835\u0303\udc65\ud835\udc5b \u2208 \u211d is the noisy input signal, updating \ud835\udebd\ud835\udc5b is significantly simplified. In this case, the upper-left (\ud835\udc3f \u2212 1) \u00d7 (\ud835\udc3f \u2212 1) block of \ud835\udebd\ud835\udc5b\u22121 can be copied to the lower-right (\ud835\udc3f \u2212 1) \u00d7 (\ud835\udc3f \u2212 1) block of \ud835\udebd\ud835\udc5b. Thus, only the first row and the first column of \ud835\udebd\ud835\udc5b are directly updated. Due to the symmetry of \ud835\udebd\ud835\udc5b, it is sufficient to update only the first column.\nIn Table III, we present the number of required multiplication, division, and square-root operations per iteration by the DCD-RTLS algorithm and the algorithms of [9], [10], and [12], which are called kRTLS, xRTLS, and AIP, respectively. We consider both cases of shift-structured and non-shift-structured input data."}, {"heading": "V. PERFORMANCE ANALYSIS", "text": "Analyzing the performance of the DCD-RTLS algorithm taking into account the impreciseness sustained by the DCD algorithm is arduous. Therefore, we assume that the solutions provided by the DCD algorithm are sufficiently accurate so that we can neglect any error due to the approximate nature of the DCD algorithm. It has been shown that usually an appropriate choice of the design parameters, especially \ud835\udc41 makes the DCD algorithm sufficiently accurate to make this assumption acceptable [23], [24]."}, {"heading": "A. Assumptions", "text": "For tractability of the analysis, we adopt the following\nadditional assumptions:\nA3: The noises are temporally uncorrelated and statistically independent of each other as well as the noiseless input\ndata.\nA4: The noiseless input vector, \ud835\udc31\ud835\udc5b, is wide-sense stationary\nand temporally uncorrelated, with a positive-definite covariance matrix \ud835\udc11 \u2208 \u211d\ud835\udc3f\u00d7\ud835\udc3f. A5: For a sufficiently large \ud835\udc5b, we can replace \ud835\udebd\ud835\udc5b, \ud835\udc33\ud835\udc5b, and\n\ud835\udf0f\ud835\udc5b with their asymptotic expected values, \ud835\u0305\udebd\u221e, \ud835\u0305\udc33\u221e, and \ud835\udf0f\u221e\u0305, respectively, which, considering A1-A4, are calculated as\n\ud835\u0305\udebd\u221e = lim \ud835\udc5b\u2192\u221e \ud835\udc38[\ud835\udebd\ud835\udc5b]\n= (1 \u2212 \ud835\udf06)\u22121(\ud835\udc11 + \ud835\udf02\ud835\udc08),\n\ud835\u0305\udc33\u221e = lim \ud835\udc5b\u2192\u221e \ud835\udc38[\ud835\udc33\ud835\udc5b]\n= (1 \u2212 \ud835\udf06)\u22121\ud835\udc11\ud835\udc21,\nand\n\ud835\udf0f\u221e\u0305 = lim \ud835\udc5b\u2192\u221e \ud835\udc38[\ud835\udf0f\ud835\udc5b]\n= (1 \u2212 \ud835\udf06)\u22121(\ud835\udc21\ud835\udc47\ud835\udc11\ud835\udc21 + \ud835\udf09)."}, {"heading": "B. Mean Convergence", "text": "Taking the expectation of both sides of (11) after a sufficiently large number of iterations while bearing in mind A5 and the Slutsky\u2019s theorem [15] results in\n\ud835\u0305\udc30\ud835\udc5b = (\ud835\u0305\udebd\u221e + \ud835\udefe \u22121\ud835\u0305\udc30\ud835\udc5b\u22121\ud835\u0305\udc33\u221e \ud835\udc47 )\u22121(\ud835\u0305\udc33\u221e + \ud835\udefe \u22121\ud835\udf0f\u221e\u0305\ud835\u0305\udc30\ud835\udc5b\u22121)\n= (\ud835\udc11 + \ud835\udf02\ud835\udc08 + \ud835\udefe\u22121\ud835\u0305\udc30\ud835\udc5b\u22121\ud835\udc21 \ud835\udc47\ud835\udc11)\u22121\n\u00d7 [\ud835\udc11\ud835\udc21 + (\ud835\udefe\u22121\ud835\udc21\ud835\udc47\ud835\udc11\ud835\udc21 + \ud835\udf02)\ud835\u0305\udc30\ud835\udc5b\u22121]\n(24)\nwhere, for convenience of the notation, we define\n\ud835\u0305\udc30\ud835\udc5b = \ud835\udc38[\ud835\udc30\ud835\udc5b].\nUsing (24), we can verify that\n\ud835\u0305\udc30\ud835\udc5b \u2212 \ud835\udc21 = \ud835\udf02(\ud835\udc11 + \ud835\udf02\ud835\udc08 + \ud835\udefe \u22121\ud835\u0305\udc30\ud835\udc5b\u22121\ud835\udc21 \ud835\udc47\ud835\udc11)\u22121(\ud835\u0305\udc30\ud835\udc5b\u22121 \u2212 \ud835\udc21) (25)\nand\n\ud835\udefe\u22121\ud835\u0305\udc30\ud835\udc5b\ud835\udc21 \ud835\udc47 + \ud835\udc08 = (\ud835\udc11 + \ud835\udf02\ud835\udc08 + \ud835\udefe\u22121\ud835\u0305\udc30\ud835\udc5b\u22121\ud835\udc21 \ud835\udc47\ud835\udc11)\u22121\n\u00d7 (\ud835\udefe\u22121\ud835\u0305\udc30\ud835\udc5b\u22121\ud835\udc21 \ud835\udc47 + \ud835\udc08)(\ud835\udefe\u22121\ud835\udc11\ud835\udc21\ud835\udc21\ud835\udc47 + \ud835\udc11 + \ud835\udf02\ud835\udc08).\n(26)\nInverting both sides of (26) then multiplying by (25) from the right gives\n\ud835\udc1a\ud835\udc5b = \ud835\udc02\ud835\udc1a\ud835\udc5b\u22121\nwhere\n\ud835\udc1a\ud835\udc5b = (\ud835\udefe \u22121\ud835\u0305\udc30\ud835\udc5b\ud835\udc21 \ud835\udc47 + \ud835\udc08)\u22121(\ud835\u0305\udc30\ud835\udc5b \u2212 \ud835\udc21)\nand\n\ud835\udc02 = \ud835\udf02(\ud835\udefe\u22121\ud835\udc11\ud835\udc21\ud835\udc21\ud835\udc47 + \ud835\udc11 + \ud835\udf02\ud835\udc08)\u22121.\nSince \ud835\udc11 and \ud835\udefe\u22121\ud835\udc21\ud835\udc21\ud835\udc47 + \ud835\udc08 are symmetric positive-definite, their multiplication, \ud835\udefe\u22121\ud835\udc11\ud835\udc21\ud835\udc21\ud835\udc47 + \ud835\udc11, is also symmetric positive-definite with all positive eigenvalues. Therefore, the spectral radius of the matrix \ud835\udc02 is equal to\n\ud835\udf0c{\ud835\udc02} = \ud835\udf02\ud835\udf01max{(\ud835\udefe\n\u22121\ud835\udc11\ud835\udc21\ud835\udc21\ud835\udc47 + \ud835\udc11 + \ud835\udf02\ud835\udc08)\u22121}\n= \ud835\udf02\n\ud835\udf01min{\ud835\udefe \u22121\ud835\udc11\ud835\udc21\ud835\udc21\ud835\udc47 + \ud835\udc11} + \ud835\udf02\n(27)\nwhere \ud835\udf01max{\u2219} and \ud835\udf01min{\u2219} return the eigenvalues of their matrix arguments that have the largest and smallest absolute values, respectively. From (27), we observe that\n\ud835\udf0c{\ud835\udc02} < 1\nand consequently\nlim \ud835\udc5b\u2192\u221e \ud835\udc1a\ud835\udc5b = \ud835\udfce\nor\nlim \ud835\udc5b\u2192\u221e \ud835\udc38[\ud835\udc30\ud835\udc5b] = \ud835\udc21.\nThis indicates that the DCD-RTLS algorithm is convergent in the mean and asymptotically unbiased."}, {"heading": "C. Bias Compensation Mechanism", "text": "Equation (10) can be rearranged as\n\ud835\udc30\ud835\udc5b = \ud835\udebd\ud835\udc5b \u22121\ud835\udc33\ud835\udc5b + \ud835\udefe \u22121(\ud835\udf0f\ud835\udc5b \u2212 \ud835\udc33\ud835\udc5b \ud835\udc47\ud835\udc30\ud835\udc5b)\ud835\udebd\ud835\udc5b \u22121\ud835\udc30\ud835\udc5b\u22121 (28)\nMaking the following approximation after a sufficiently large number of iterations:\n\ud835\udf0f\ud835\udc5b \u2212 \ud835\udc33\ud835\udc5b \ud835\udc47\ud835\udc30\ud835\udc5b \u2248 \ud835\udf0f\u221e\u0305 \u2212 \ud835\u0305\udc33\u221e \ud835\udc47 \ud835\udc21 \u2248 (1 \u2212 \ud835\udf06)\u22121\ud835\udf09\nand substituting it into (28), we obtain the following recursion:\n\ud835\udc30\ud835\udc5b = \ud835\udebd\ud835\udc5b \u22121\ud835\udc33\ud835\udc5b + (1 \u2212 \ud835\udf06) \u22121\ud835\udf02\ud835\udebd\ud835\udc5b \u22121\ud835\udc30\ud835\udc5b\u22121. (29)\nThe first term on the right-hand side of (29) is the conventional exponentially-weighted recursive LS (RLS) estimate [16]. It is known that, in the presence of input noise, the RLS estimate is biased. The asymptotic bias of the RLS estimate is calculated as\n\ud835\udc1b = lim \ud835\udc5b\u2192\u221e\n\ud835\udc38[\ud835\udebd\ud835\udc5b \u22121\ud835\udc33\ud835\udc5b \u2212 \ud835\udc21]\n= \ud835\u0305\udebd\u221e \u22121\ud835\u0305\udc33\u221e \u2212 \ud835\udc21 = (\ud835\udc11 + \ud835\udf02\ud835\udc08)\u22121\ud835\udc11\ud835\udc21 \u2212 \ud835\udc21\nand using the Woodbury matrix identity [14] as\n\ud835\udc1b = \u2212\ud835\udf02(\ud835\udc11 + \ud835\udf02\ud835\udc08)\u22121\ud835\udc21.\nApproximating (\ud835\udc11 + \ud835\udf02\ud835\udc08)\u22121 with (1 \u2212 \ud835\udf06)\u22121\ud835\udebd\ud835\udc5b \u22121 and \ud835\udc21 with \ud835\udc30\ud835\udc5b\u22121, when \ud835\udc5b is large enough, gives\n\ud835\udc1b \u2248 \u2212(1 \u2212 \ud835\udf06)\u22121\ud835\udf02\ud835\udebd\ud835\udc5b \u22121\ud835\udc30\ud835\udc5b\u22121. (30)\nSubstituting (30) into (29) results in\n\ud835\udc30\ud835\udc5b \u2248 \ud835\udebd\ud835\udc5b \u22121\ud835\udc33\ud835\udc5b \u2212 \ud835\udc1b.\nThis means, after a sufficiently large number of iterations, the update equation of the RTLS algorithm is similar to that of a BCRLS algorithm [17]-[19]."}, {"heading": "D. Weight-Error Update Equation", "text": "Multiplying both sides of (29) by \ud835\udebd\ud835\udc5b from the left then subtracting (1 \u2212 \ud835\udf06)\u22121\ud835\udf02\ud835\udc30\ud835\udc5b from it gives\n\ud835\u0301\udebd\ud835\udc5b\ud835\udc30\ud835\udc5b = \ud835\udc33\ud835\udc5b + (1 \u2212 \ud835\udf06) \u22121\ud835\udf02(\ud835\udc30\ud835\udc5b\u22121 \u2212 \ud835\udc30\ud835\udc5b) (31)\nwhere we define\n\ud835\u0301\udebd\ud835\udc5b = \ud835\udebd\ud835\udc5b \u2212 (1 \u2212 \ud835\udf06) \u22121\ud835\udf02\ud835\udc08.\nAt time instant \ud835\udc5b \u2212 1, (31) is written as\n\ud835\u0301\udebd\ud835\udc5b\u22121\ud835\udc30\ud835\udc5b\u22121 = \ud835\udc33\ud835\udc5b\u22121 + (1 \u2212 \ud835\udf06) \u22121\ud835\udf02(\ud835\udc30\ud835\udc5b\u22122 \u2212 \ud835\udc30\ud835\udc5b\u22121). (32)\nMultiplying both sides of (32) by \ud835\udf06 and using the recursive equations\n\ud835\udc33\ud835\udc5b = \ud835\udf06\ud835\udc33\ud835\udc5b\u22121 + \ud835\u0303\udc66\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b\nand\n\ud835\udebd\ud835\udc5b = \ud835\udf06\ud835\udebd\ud835\udc5b\u22121 + \ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 ,\nwe get\n\ud835\u0301\udebd\ud835\udc5b\ud835\udc30\ud835\udc5b\u22121 \u2212 \ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47\ud835\udc30\ud835\udc5b\u22121 + \ud835\udf02\ud835\udc30\ud835\udc5b\u22121 = \ud835\udc33\ud835\udc5b \u2212 \ud835\u0303\udc66\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b\n+\ud835\udf06(1 \u2212 \ud835\udf06)\u22121\ud835\udf02(\ud835\udc30\ud835\udc5b\u22122 \u2212 \ud835\udc30\ud835\udc5b\u22121). (33)\nSubtracting (33) from (31) gives\n\ud835\u0301\udebd\ud835\udc5b\ud835\udc30\ud835\udc5b = \ud835\u0301\udebd\ud835\udc5b\ud835\udc30\ud835\udc5b\u22121 \u2212 \ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47\ud835\udc30\ud835\udc5b\u22121 + \ud835\udf02\ud835\udc30\ud835\udc5b\u22121 + \ud835\u0303\udc66\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b\n+(1 \u2212 \ud835\udf06)\u22121\ud835\udf02(\ud835\udc30\ud835\udc5b\u22121 \u2212 \ud835\udc30\ud835\udc5b + \ud835\udf06\ud835\udc30\ud835\udc5b\u22121 \u2212 \ud835\udf06\ud835\udc30\ud835\udc5b\u22122). (34)\nSubtracting both sides of (34) from \ud835\u0301\udebd\ud835\udc5b\ud835\udc21 along with using (4) and assuming that, for a large \ud835\udc5b, the last term on the righthand side of (34) is negligible results in\n\ud835\u0301\udebd\ud835\udc5b\ud835\u030c\udc30\ud835\udc5b = \ud835\u0301\udebd\ud835\udc5b\ud835\u030c\udc30\ud835\udc5b\u22121 \u2212 (\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)\ud835\u030c\udc30\ud835\udc5b\u22121 + (\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)\ud835\udc21 \u2212 \ud835\udf08\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b. (35)\nwhere we define the weight-error vector as\n\ud835\u030c\udc30\ud835\udc5b = \ud835\udc21 \u2212 \ud835\udc30\ud835\udc5b .\nOwing to A5, for a sufficiently large \ud835\udc5b, we have\n\ud835\u0301\udebd\ud835\udc5b \u2248 \ud835\u0305\udebd\u221e \u2212 (1 \u2212 \ud835\udf06) \u22121\ud835\udf02\ud835\udc08 \u2248 (1 \u2212 \ud835\udf06)\u22121\ud835\udc11.\n(36)\nSubstituting (36) into (35) and multiplying both sides by (1 \u2212 \ud835\udf06)\ud835\udc11\u22121 gives the weight-error update equation of the RTLS algorithm as\n\ud835\u030c\udc30\ud835\udc5b = [\ud835\udc08 \u2212 (1 \u2212 \ud835\udf06)\ud835\udc11 \u22121(\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)]\ud835\u030c\udc30\ud835\udc5b\u22121 +(1 \u2212 \ud835\udf06)\ud835\udc11\u22121[(\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)\ud835\udc21 \u2212 \ud835\udf08\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b]. (37)"}, {"heading": "E. Mean Convergence of (29)", "text": "The assumptions A1-A4 imply the following corollary: C1: The vector \ud835\u030c\udc30\ud835\udc5b\u22121 is statistically independent of \ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 ,\n\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47 , and \ud835\udf08\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b.\nTaking the expectation of both sides of (37) while bearing in mind C1 gives\n\ud835\udc38[\ud835\u030c\udc30\ud835\udc5b] = [\ud835\udf06\ud835\udc08 + (1 \u2212 \ud835\udf06)\ud835\udf02\ud835\udc11 \u22121]\ud835\udc38[\ud835\u030c\udc30\ud835\udc5b\u22121]. (38)\nConvergence of (38) as \ud835\udc5b \u2192 \u221e, i.e., mean convergence of (29), is guaranteed if\n\ud835\udf0c{\ud835\udf06\ud835\udc08 + (1 \u2212 \ud835\udf06)\ud835\udf02\ud835\udc11\u22121} < 1\nor equivalently\n\ud835\udf01min{\ud835\udc11} > \ud835\udf02. (39)\nThe condition (39) can be interpreted as the requirement for the input data subspace to be distinct and separable from the input noise subspace. The DCD-RTLS algorithm is not bound by this condition. In Section V.B, we showed that the DCD-\nRTLS algorithm or equivalently the recursion of (11), converges in the mean provided that \ud835\udf01min{\ud835\udc11} > 0. However, its steady-state approximation, (29), which is in fact a BCRLS algorithm, requires (39) to assure its mean convergence."}, {"heading": "F. Mean-Square Convergence", "text": "Taking the expectation of the squared Euclidean norm of both sides of (37) while considering C1 gives the following variance relation:\n\ud835\udc38[\u2016\ud835\u030c\udc30\ud835\udc5b\u2016 2] = \ud835\udc38[\ud835\u030c\udc30\ud835\udc5b\u22121 \ud835\udc47 \ud835\udc12\ud835\udc5b\ud835\u030c\udc30\ud835\udc5b\u22121] + (1 \u2212 \ud835\udf06) 2\ud835\udc54 (40)\nwhere\n\ud835\udc12\ud835\udc5b = [\ud835\udc08 \u2212 (1 \u2212 \ud835\udf06)(\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)\ud835\udc11\u22121]\n\u00d7 [\ud835\udc08 \u2212 (1 \u2212 \ud835\udf06)\ud835\udc11\u22121(\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)],\n\ud835\udc54 = \ud835\udc38[\ud835\udc2d\ud835\udc5b \ud835\udc47\ud835\udc11\u22122\ud835\udc2d\ud835\udc5b] = tr{\ud835\udc11\u22122\ud835\udc38[\ud835\udc2d\ud835\udc5b\ud835\udc2d\ud835\udc5b \ud835\udc47]}, (41)\nand\n\ud835\udc2d\ud835\udc5b = (\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)\ud835\udc21 \u2212 \ud835\udf08\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b.\nSince, in view of C1, \ud835\u030c\udc30\ud835\udc5b\u22121 is independent of \ud835\udc12\ud835\udc5b, we can write\n\ud835\udc38[\ud835\u030c\udc30\ud835\udc5b\u22121 \ud835\udc47 \ud835\udc12\ud835\udc5b\ud835\u030c\udc30\ud835\udc5b\u22121] = \ud835\udc38[\ud835\u030c\udc30\ud835\udc5b\u22121 \ud835\udc47 \ud835\udc38[\ud835\udc12\ud835\udc5b]\ud835\u030c\udc30\ud835\udc5b\u22121].\nMoreover, we have\n\ud835\udc38[\ud835\udc12\ud835\udc5b] = \ud835\udc08 \u2212 2(1 \u2212 \ud835\udf06)\ud835\udc08\n+(1 \u2212 \ud835\udf06)2\ud835\udc38[(\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)\ud835\udc11\u22122(\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)] = (2\ud835\udf06 \u2212 1)\ud835\udc08 +(1 \u2212 \ud835\udf06)2(tr{\ud835\udc11\u22121}\ud835\udc11 + 2\ud835\udc08 \u2212 2\ud835\udf02\ud835\udc11\u22121 + \ud835\udf022\ud835\udc11\u22122) = (1 \u2212 2\ud835\udf06 + 2\ud835\udf062)\ud835\udc08\n+(1 \u2212 \ud835\udf06)2(tr{\ud835\udc11\u22121}\ud835\udc11 \u2212 2\ud835\udf02\ud835\udc11\u22121 + \ud835\udf022\ud835\udc11\u22122).\n(42)\nHence, (40) is stable if \ud835\udc38[\ud835\udc12\ud835\udc5b] is stable, i.e.,\n\ud835\udf0c{\ud835\udc38[\ud835\udc12\ud835\udc5b]} = 1 \u2212 2\ud835\udf06 + 2\ud835\udf06 2\n+(1 \u2212 \ud835\udf06)2\ud835\udf0c{tr{\ud835\udc11\u22121}\ud835\udc11 \u2212 2\ud835\udf02\ud835\udc11\u22121 + \ud835\udf022\ud835\udc11\u22122} < 1. (43)\nAccording to A4 and the sub-additive inequality of the spectral radius for any two multiplication-commutative matrices [21], i.e.,\n\ud835\udf0c{\ud835\udc00 + \ud835\udc01} \u2264 \ud835\udf0c{\ud835\udc00} + \ud835\udf0c{\ud835\udc01},\nwhere \ud835\udc00\ud835\udc01 = \ud835\udc01\ud835\udc00, (43) is satisfied if\n1 \u2212 2\ud835\udf06 + 2\ud835\udf062 +(1 \u2212 \ud835\udf06)2 (tr{\ud835\udc11\u22121}\ud835\udf01max{\ud835\udc11} \u2212 2\ud835\udf02\n\ud835\udf01min{\ud835\udc11} +\n\ud835\udf022\n\ud835\udf01min 2 {\ud835\udc11}\n) < 1.\nTherefore, choosing any forgetting factor that satisfies\n\ud835\udf06 > 1 \u2212\n2\ntr{\ud835\udc11\u22121}\ud835\udf01max{\ud835\udc11} + (1 \u2212 \ud835\udf02\n\ud835\udf01min{\ud835\udc11} )\n2\n+ 1\n(44)\nguarantees mean-square stability of the DCD-RTLS algorithm."}, {"heading": "G. Mean-Square Deviation", "text": "In view of A1-A4, we have\n\ud835\udc38[\ud835\udc2d\ud835\udc5b\ud835\udc2d\ud835\udc5b \ud835\udc47] = \ud835\udc38[(\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)\ud835\udc21\ud835\udc21\ud835\udc47(\ud835\udc2e\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47 \u2212 \ud835\udf02\ud835\udc08)] + \ud835\udc38[\ud835\udf08\ud835\udc5b 2\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47]\n= \ud835\udc38[\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47\ud835\udc21\ud835\udc21\ud835\udc47\ud835\udc2e\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47] + \ud835\udf022\ud835\udc21\ud835\udc21\ud835\udc47 \u2212 \ud835\udf02\ud835\udc38[\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47]\ud835\udc21\ud835\udc21\ud835\udc47\n\u2212\ud835\udf02\ud835\udc21\ud835\udc21\ud835\udc47\ud835\udc38[\ud835\udc2e\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47] + \ud835\udc38[\ud835\udf08\ud835\udc5b 2\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47]\n= \ud835\udc38[\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47\ud835\udc21\ud835\udc21\ud835\udc47\ud835\udc2e\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47] \u2212 \ud835\udf022\ud835\udc21\ud835\udc21\ud835\udc47 + \ud835\udf09(\ud835\udc11 + \ud835\udf02\ud835\udc08). (45)\nUtilizing the Isserlis\u2019 theorem [20], we also have\n\ud835\udc38[\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47\ud835\udc21\ud835\udc21\ud835\udc47\ud835\udc2e\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47] = \ud835\udc38[\ud835\udc21\ud835\udc47\ud835\udc2e\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47\ud835\udc21]\ud835\udc38[\ud835\u0303\udc31\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47] +2\ud835\udc38[\ud835\u0303\udc31\ud835\udc5b\ud835\udc2e\ud835\udc5b \ud835\udc47\ud835\udc21]\ud835\udc38[\ud835\udc21\ud835\udc47\ud835\udc2e\ud835\udc5b\ud835\u0303\udc31\ud835\udc5b \ud835\udc47]\n= \ud835\udf02\u2016\ud835\udc21\u20162(\ud835\udc11 + \ud835\udf02\ud835\udc08) + 2\ud835\udf022\ud835\udc21\ud835\udc21\ud835\udc47 ,\n(46)\nSubstituting (46) into (45) and then the resulting expression into (41) yields\n\ud835\udc54 = tr{\ud835\udc11\u22122[(\ud835\udf02\u2016\ud835\udc21\u20162 + \ud835\udf09)(\ud835\udc11 + \ud835\udf02\ud835\udc08) + \ud835\udf022\ud835\udc21\ud835\udc21\ud835\udc47]}. (47)\nThe forgetting factor, \ud835\udf06, is usually set close to unity. Thus, (1 \u2212 \ud835\udf06)2 has a small value and we may neglect the second additive term on the right-hand side of (42) to obtain\n\ud835\udc38[\ud835\udc12\ud835\udc5b] \u2248 (1 \u2212 2\ud835\udf06 + 2\ud835\udf06 2)\ud835\udc08.\nSubsequently, we can rewrite (40) as\n\ud835\udc38[\u2016\ud835\u030c\udc30\ud835\udc5b\u2016 2] = (1 \u2212 2\ud835\udf06 + 2\ud835\udf062)\ud835\udc38[\u2016\ud835\u030c\udc30\ud835\udc5b\u22121\u2016 2] + (1 \u2212 \ud835\udf06)2\ud835\udc54. (48)\nThe variance recursion (48) is stable as\n0 < 1 \u2212 2\ud835\udf06 + 2\ud835\udf062 < 1.\nTherefore, it converges to a steady state where we have\n\ud835\udc38[\u2016\ud835\u030c\udc30\u221e\u2016 2] = (1 \u2212 2\ud835\udf06 + 2\ud835\udf062)\ud835\udc38[\u2016\ud835\u030c\udc30\u221e\u2016 2] + (1 \u2212 \ud835\udf06)2\ud835\udc54. (49)\nSubstituting (47) into (49) gives the steady-state MSD of the RTLS algorithm as\n\ud835\udc38[\u2016\ud835\u030c\udc30\u221e\u2016\n2] = ( 1 \u2212 \ud835\udf06\n2\ud835\udf06 ) \ud835\udf02\n\u00d7 tr{\ud835\udc11\u22122[(\u2016\ud835\udc21\u20162 + \ud835\udefe)(\ud835\udc11 + \ud835\udf02\ud835\udc08) + \ud835\udf02\ud835\udc21\ud835\udc21\ud835\udc47]}. (50)\nNote from (50) that when \ud835\udf06 = 1, the steady-state MSD is zero, i.e., the algorithm is consistent. This in fact complies with the analytical findings of [12]."}, {"heading": "VI. SIMULATIONS", "text": "Consider an EIV system identification problem where the\nsystem parameter vector has \ud835\udc3f = 8 entries and is chosen as\n\ud835\udc21 = [\u22120.019, \u22120.213, \u22120.600, +0.235,\n+0.574, +0.377, \u22120.056, \u22120.254]\ud835\udc47 .\nThe noiseless input vector, \ud835\udc31\ud835\udc5b, is multivariate Gaussian with covariance matrix\n\ud835\udc11 = \ud835\udc10diag{\ud835\udc1f}\ud835\udc10\ud835\udc47\nwhere \ud835\udc10 \u2208 \u211d\ud835\udc3f\u00d7\ud835\udc3f is an arbitrary unitary matrix and the entries of \ud835\udc1f \u2208 \u211d\ud835\udc3f\u00d71, are drawn from a uniform distribution in the interval [0.2,1.8]. The input and output noises are also zeromean i.i.d. multivariate Gaussian, i.e., \ud835\udc2e\ud835\udc5b~\ud835\udca9(\ud835\udfce, \ud835\udf02\ud835\udc08) and \ud835\udf08\ud835\udc5b~\ud835\udca9(0, \ud835\udf09). To obtain the experimental results, we evaluate\nthe expectations by taking the ensemble-average over 103 independent simulation runs and compute the steady-state values by averaging over 103 steady-state instances. In Fig. 1, we plot the simulated time evolution of the MSD, i.e., \ud835\udc38[\u2016\ud835\u030c\udc30\ud835\udc5b\u2016 2], for the recursion of (12) and the DCD-RTLS, kRTLS, xRTLS, and AIP algorithms. The curves are for different values of \ud835\udf02 and \ud835\udefe when \ud835\udc43 = 10 (\ud835\udf06 = 0.999), \ud835\udc40 = 16, \ud835\udc3b = 1, and \ud835\udc41 = 1. Fig. 1 shows that the learning curve of the DCD-RTLS algorithm and the recursion of (12) overlay. This means that, even when exercising only a single iteration of the DCD algorithm at each time instant, the solutions provided by the DCD algorithm are sufficiently accurate and the assumption made at the beginning of Section V is realistic. We also observe from Fig. 1 that the DCD-RTLS algorithm converges faster than its contenders, the kRTLS, xRTLS, and AIP algorithms. Moreover, all the considered algorithms perform similarly at the steady state. Therefore, the calculated theoretical steady-state MSD, (50), can be used to predict the steady-state MSD of all the considered algorithms.\nIn Fig. 2, we plot the lower bound on \ud835\udf06 given in (44) versus \ud835\udf02 for the considered scenario where tr{\ud835\udc11\u22121} = 12.82, \ud835\udf01min{\ud835\udc11} = 0.2, and \ud835\udf01max{\ud835\udc11} = 1.8. Notice that, to make the DCD-RTLS algorithm mean-square-stable in the presence of high input noise, \ud835\udf06 should be chosen close to one. However, in the presence of low input noise, mean-square stability is ensured with any \ud835\udf06 larger than about 0.92. In Fig. 3, we compare the theoretical and experimental steady-state MSDs of the DCD-RTLS algorithm by plotting them against \ud835\udf02 for different values of \ud835\udf06 and \ud835\udefe when \ud835\udc40 = 16, \ud835\udc3b = 1, and \ud835\udc41 = 1. Fig. 3 shows a good match between theory and experiment."}, {"heading": "VII. CONCLUSION", "text": "We proposed a reduced-complexity recursive total leastsquares (RTLS) algorithm in the context of adaptive FIR filtering employing the inverse power method and the dichotomous coordinate-descent (DCD) iterations. We utilized the DCD algorithm to solve two systems of linear equations associated with the calculation of two auxiliary vector variables. Simulation results confirmed that the proposed DCD-RTLS algorithm outperforms the other existing RTLS algorithms while being computationally more efficient. We studied the performance of the DCD-RTLS algorithm and established its convergence and stability in both mean and mean-square senses. We also calculated the theoretical steadystate mean-square deviation (MSD) of the DCD-RTLS algorithm and verified via simulations that the theoretically predicted values of the steady-state MSD are in good agreement with the experimental results."}], "references": [{"title": "Linear dynamic errors-in-variables models", "author": ["M. Deistler"], "venue": "J. Appl. Probability, vol. 23A, pp. 23\u201339, 1986.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Errors-in-variables methods in system identification", "author": ["T. S\u00f6derstr\u00f6m"], "venue": "Automatica, vol. 43, pp. 939-958, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Total Least Squares Techniques and Errors-in-Variables Modeling: Analysis, Algorithms and  7 Applications, Dordrecht", "author": ["S. Van Huffel", "P. Lemmerling", "Eds"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Overview of total least-squares methods", "author": ["I. Markovsky", "S. Van Huffel"], "venue": "Signal Process., vol. 87, pp. 2283-2302, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "An analysis of the total least squares problem", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "SIAM J. Numerical Anal., vol. 17, no. 6, pp. 883\u2013893, Dec. 1980.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1980}, {"title": "Optimization Algorithms on Matrix Manifolds, Princeton, NJ", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Analysis of gradient algorithms for TLS-based adaptive IIR filters", "author": ["B.E. Dunne", "G.A. Williamson"], "venue": "IEEE Trans. Signal Process., vol. 52, no. 12, pp. 3345\u20133356, Dec. 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Analysis of the gradientdecent total least-squares algorithm", "author": ["R. Arablouei", "S. Werner", "K. Do\u011fan\u00e7ay"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 5, pp. 1256-1264, Mar. 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "An efficient recursive total least squares algorithm for FIR adaptive filtering", "author": ["C.E. Davila"], "venue": "IEEE Trans. Signal Process., vol. 42, no. 2, pp. 268\u2013280, Feb. 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "A fast recursive total least squares algorithm for adaptive FIR filtering", "author": ["D.-Z. Feng", "X.-D. Zhang", "D.-X. Chang", "W.X. Zheng"], "venue": "IEEE Trans. Signal Process., vol. 52, no. 10, pp. 2729\u20132737, Oct. 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Minor component analysis by incremental inverse iteration", "author": ["D.M. Sima", "S. Van Huffel"], "venue": "Proc. 16th Int. Symp. Math. Theory Networks Syst., Leuven, Belgium, Jul. 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast approximate inverse power iteration algorithm for adaptive total least-squares FIR filtering", "author": ["D.-Z. Feng", "W.X. Zheng"], "venue": "IEEE Trans. Signal Process., vol. 54, no. 10, pp. 4032\u20134039, Oct. 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Low-complexity distributed total least squares estimation in ad hoc sensor networks", "author": ["A. Bertrand", "M. Moonen"], "venue": "IEEE Trans. Signal Process., vol. 60, no. 8, pp. 4321\u20134333, Aug. 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive Filter Theory, 4 ed", "author": ["S. Haykin"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Bias correction in least-squares identification", "author": ["P. Stoica", "T. S\u00f6derstr\u00f6m"], "venue": "Int. J. Control, vol. 35, no. 3, pp. 449-457, 1982.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1982}, {"title": "Bias compensation for the bearings-only pseudolinear target track estimator", "author": ["K. Do\u011fan\u00e7ay"], "venue": "IEEE Trans. Signal Process., vol.54, no.1, pp. 59-68, Jan. 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Diffusion bias-compensated RLS estimation over adaptive networks", "author": ["A. Bertrand", "M. Moonen", "A.H. Sayed"], "venue": "IEEE Trans. Signal Process., vol. 59, pp. 5212\u20135224, Nov. 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "On a formula for the product-moment coefficient of any order of a normal frequency distribution in any number of variables", "author": ["L. Isserlis"], "venue": "Biometrika, vol. 12, no. 1/2, pp. 134\u2013139, Nov. 1918.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1918}, {"title": "A Hibert Space Problem Book, 2 ed", "author": ["P.R. Halmos"], "venue": "New York: Springer-Verlag,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1982}, {"title": "Multiplication-free iterative algorithm for LS problem", "author": ["Y.V. Zakharov", "T.C. Tozer"], "venue": "Electron. Lett., vol. 40, no. 9, pp. 567\u2013569, Apr. 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Low complexity RLS algorithms using dichotomous coordinate descent iterations", "author": ["Y. Zakharov", "G. White", "J. Liu"], "venue": "IEEE Trans. Signal Process., vol. 56, pp. 3150\u20133161, Jul. 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Architecture and FPGA design of dichotomous coordinate descent algorithms", "author": ["J. Liu", "Y.V. Zakharov", "B. Weaver"], "venue": "IEEE Trans. Circuits Syst. I, Reg. Papers, vol. 56, pp. 2425\u20132438, Nov. 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive filtering algorithms with selective partial updates", "author": ["K. Do\u011fan\u00e7ay", "O. Tanr\u0131kulu"], "venue": "IEEE Trans. Circuits Syst. II, Analog Digit. Signal Process., vol. 48, pp. 762\u2013769, Aug. 2001.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "In the EIV models, both input and output data of a linear system are assumed to be contaminated with additive noise [1], [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "In the EIV models, both input and output data of a linear system are assumed to be contaminated with additive noise [1], [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "The total leastsquares (TLS) method is well suited for identification of the EIV models [3], [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "The total leastsquares (TLS) method is well suited for identification of the EIV models [3], [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "It has been shown that the TLS estimate of a linear system is in fact the eigenvector corresponding to the eigenvalue of its augmented and weighted data covariance matrix that has the smallest absolute value [5].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "compute this eigenvector, also known as the minor component, is to minimize a Rayleigh quotient (RQ) cost function [6] using any optimization technique such as gradient descent [7], [8] or line search [9], [10].", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "compute this eigenvector, also known as the minor component, is to minimize a Rayleigh quotient (RQ) cost function [6] using any optimization technique such as gradient descent [7], [8] or line search [9], [10].", "startOffset": 177, "endOffset": 180}, {"referenceID": 7, "context": "compute this eigenvector, also known as the minor component, is to minimize a Rayleigh quotient (RQ) cost function [6] using any optimization technique such as gradient descent [7], [8] or line search [9], [10].", "startOffset": 182, "endOffset": 185}, {"referenceID": 8, "context": "compute this eigenvector, also known as the minor component, is to minimize a Rayleigh quotient (RQ) cost function [6] using any optimization technique such as gradient descent [7], [8] or line search [9], [10].", "startOffset": 201, "endOffset": 204}, {"referenceID": 9, "context": "compute this eigenvector, also known as the minor component, is to minimize a Rayleigh quotient (RQ) cost function [6] using any optimization technique such as gradient descent [7], [8] or line search [9], [10].", "startOffset": 206, "endOffset": 210}, {"referenceID": 10, "context": "Another common practice is to utilize the inverse power method (inverse iteration) [11]-[13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Another common practice is to utilize the inverse power method (inverse iteration) [11]-[13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "The recursive TLS (RTLS) algorithms proposed in [9], [10], and [12] for adaptive finite-impulse-response (FIR) filtering have computational complexities of order O(L), where L is the system order, when the input data is shift-structured.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "The recursive TLS (RTLS) algorithms proposed in [9], [10], and [12] for adaptive finite-impulse-response (FIR) filtering have computational complexities of order O(L), where L is the system order, when the input data is shift-structured.", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "The recursive TLS (RTLS) algorithms proposed in [9], [10], and [12] for adaptive finite-impulse-response (FIR) filtering have computational complexities of order O(L), where L is the system order, when the input data is shift-structured.", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "In [12], it is shown that the RTLS algorithm proposed therein, which is based on the inverse power method and a line search strategy, is asymptotically consistent, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The DCD algorithm is a shift-and-add algorithm and can solve a system of linear equation using only additions and bit shifts with no multiplication [22].", "startOffset": 148, "endOffset": 152}, {"referenceID": 8, "context": "It requires 10L + 2 multiplications as opposed to 15L + 11, 16L + 19, and 22L + 69 multiplications required by the algorithms of [9], [10], and [12], respectively.", "startOffset": 129, "endOffset": 132}, {"referenceID": 9, "context": "It requires 10L + 2 multiplications as opposed to 15L + 11, 16L + 19, and 22L + 69 multiplications required by the algorithms of [9], [10], and [12], respectively.", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "It requires 10L + 2 multiplications as opposed to 15L + 11, 16L + 19, and 22L + 69 multiplications required by the algorithms of [9], [10], and [12], respectively.", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "Recursive Total Least-Squares The TLS estimate of the system parameters at time instant n, denoted by wn \u2208 R , is given by [ wn \u22121 ] = \u2212 qn \u03b3qL+1,n where qn \u2208 R (L+1)\u00d71 is the eigenvector corresponding to the smallest (in absolute value) eigenvalue of the augmented and weighted data covariance matrix \u03a8n and qL+1,n is (L + 1)th entry of qn [5].", "startOffset": 341, "endOffset": 344}, {"referenceID": 20, "context": "Solving (18) and (19) using the DCD algorithm yields d1,n, d2,n, r1,n, and r2,n at each time instant n [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "Three design parameters, N \u2208 N, M \u2208 N, and H \u2208 R govern the accuracy and complexity of the DCD algorithm [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "Consequently, when N < L, the DCD iterations implements a form of selective partial updates [25], [26].", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "Moreover, by selecting the forgetting factor as \u03bb = 1 \u2212 2 where P is a positive integer, we can replace multiplications by \u03bb with additions and bit-shifts [23].", "startOffset": 155, "endOffset": 159}, {"referenceID": 8, "context": "In Table III, we present the number of required multiplication, division, and square-root operations per iteration by the DCD-RTLS algorithm and the algorithms of [9], [10], and [12], which are called kRTLS, xRTLS, and AIP, respectively.", "startOffset": 163, "endOffset": 166}, {"referenceID": 9, "context": "In Table III, we present the number of required multiplication, division, and square-root operations per iteration by the DCD-RTLS algorithm and the algorithms of [9], [10], and [12], which are called kRTLS, xRTLS, and AIP, respectively.", "startOffset": 168, "endOffset": 172}, {"referenceID": 11, "context": "In Table III, we present the number of required multiplication, division, and square-root operations per iteration by the DCD-RTLS algorithm and the algorithms of [9], [10], and [12], which are called kRTLS, xRTLS, and AIP, respectively.", "startOffset": 178, "endOffset": 182}, {"referenceID": 20, "context": "It has been shown that usually an appropriate choice of the design parameters, especially N makes the DCD algorithm sufficiently accurate to make this assumption acceptable [23], [24].", "startOffset": 173, "endOffset": 177}, {"referenceID": 21, "context": "It has been shown that usually an appropriate choice of the design parameters, especially N makes the DCD algorithm sufficiently accurate to make this assumption acceptable [23], [24].", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "(29) The first term on the right-hand side of (29) is the conventional exponentially-weighted recursive LS (RLS) estimate [16].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "This means, after a sufficiently large number of iterations, the update equation of the RTLS algorithm is similar to that of a BCRLS algorithm [17]-[19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "This means, after a sufficiently large number of iterations, the update equation of the RTLS algorithm is similar to that of a BCRLS algorithm [17]-[19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "According to A4 and the sub-additive inequality of the spectral radius for any two multiplication-commutative matrices [21], i.", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "Utilizing the Isserlis\u2019 theorem [20], we also have E[\ud835\u0303\udc31nun hhun\ud835\u0303\udc31n T] = E[hunun h]E[\ud835\u0303\udc31n\ud835\u0303\udc31n T] +2E[\ud835\u0303\udc31nun h]E[hun\ud835\u0303\udc31n T] = \u03b7\u2016h\u2016(R + \u03b7I) + 2\u03b7hh , (46)", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "This in fact complies with the analytical findings of [12].", "startOffset": 54, "endOffset": 58}], "year": 2014, "abstractText": "We develop a recursive total least-squares (RTLS) algorithm for errors-in-variables system identification utilizing the inverse power method and the dichotomous coordinatedescent (DCD) iterations. The proposed algorithm, called DCDRTLS, outperforms the previously-proposed RTLS algorithms, which are based on the line search method, with reduced computational complexity. We perform a comprehensive analysis of the DCD-RTLS algorithm and show that it is asymptotically unbiased as well as being stable in the mean. We also find a lower bound for the forgetting factor that ensures mean-square stability of the algorithm and calculate the theoretical steadystate mean-square deviation (MSD). We verify the effectiveness of the proposed algorithm and the accuracy of the predicted steady-state MSD via simulations.", "creator": "Microsoft\u00ae Word 2013"}}}