{"id": "1408.6788", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2014", "title": "Strongly Incremental Repair Detection", "abstract": "We present STIR (STrongly Incremental Repair detection), a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency. STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the the different stages of repairs. Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead. We evaluate its performance using incremental metrics and propose new repair processing evaluation standards. To assess the accuracy of Strongly Incremental Repair Detection by a standard threshold, the STrongly Incremental Repair Detection method, and the N-gram model with the corresponding N-gram and N-gram models, has been implemented. We evaluate STrongly Incremental Repair (NQD) in the following scenarios and examine a set of standard thresholds for each of these experiments. The N-gram model with the corresponding N-gram and N-gram models is derived by constructing a set of fixed thresholds for each of these experiments. To test the effectiveness of this step, we identify a number of standard thresholds for each of these experiments in each of these experiments. Finally, we examine the accuracy of a number of tests in the STrongly Incremental Repair Detection method and the N-gram model with the corresponding N-gram and N-gram models. The N-gram model with the corresponding N-gram and N-gram models has been implemented. Our tests for each of these experiments are summarized in Table 2. The following results, and a set of N-gram and N-gram models, are presented in Table 3. The N-gram model with the corresponding N-gram and N-gram models, have been used to predict the accuracy of the repair process. We have used STrongly Incremental Repair Detection (SRE) in both the N-gram and N-gram models for the repair process. For each of these experiments, we describe a set of standard threshold tests for each of these experiments. The N-gram model with the corresponding N-gram and N-gram models, have been used to predict the accuracy of the repair process. Finally, we use the N-gram and N-gram models for the repair process. For each of these experiments, we summarize the accuracy of the repair process. We identify a number of standard", "histories": [["v1", "Thu, 28 Aug 2014 17:29:55 GMT  (179kb)", "https://arxiv.org/abs/1408.6788v1", "12 pages, 6 figures, EMNLP conference long paper 2014"], ["v2", "Fri, 29 Aug 2014 08:44:09 GMT  (179kb)", "http://arxiv.org/abs/1408.6788v2", "12 pages, 6 figures, EMNLP conference long paper 2014"]], "COMMENTS": "12 pages, 6 figures, EMNLP conference long paper 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["julian hough", "matthew purver"], "accepted": true, "id": "1408.6788"}, "pdf": {"name": "1408.6788.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Matthew Purver"], "emails": ["julian.hough@uni-bielefeld.de", "m.purver@qmul.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 8.\n67 88\nv2 [\ncs .C\nL ]\n2 9\nA ug\n2 01\n4"}, {"heading": "1 Introduction", "text": "Self-repairs in spontaneous speech are annotated according to a well established three-phase structure from (Shriberg, 1994) onwards, and as described in Meteer et al. (1995)\u2019s Switchboard corpus annotation handbook:\nJohn [ likes \ufe38 \ufe37\ufe37 \ufe38\nreparandum\n+ {uh} \ufe38 \ufe37\ufe37 \ufe38\ninterregnum\nloves ] \ufe38 \ufe37\ufe37 \ufe38\nrepair\nMary (1)\nFrom a dialogue systems perspective, detecting repairs and assigning them the appropriate structure is vital for robust natural language understanding (NLU) in interactive systems. Downgrading the commitment of reparandum phases and assigning appropriate interregnum and repair phases permits computation of the user\u2019s intended meaning.\nFurthermore, the recent focus on incremental dialogue systems (see e.g. (Rieser and Schlangen, 2011)) means that repair detection should operate without unnecessary\nprocessing overhead, and function efficiently within an incremental framework. However, such left-to-right operability on its own is not sufficient: in line with the principle of strong incremental interpretation (Milward, 1991), a repair detector should give the best results possible as early as possible. With one exception (Zwarts et al., 2010), there has been no focus on evaluating or improving the incremental performance of repair detection.\nIn this paper we present STIR (Strongly Incremental Repair detection), a system which addresses the challenges of incremental accuracy, computational complexity and latency in selfrepair detection, by making local decisions based on relatively simple measures of fluency and similarity. Section 2 reviews state-of-the-art methods; Section 3 summarizes the challenges and explains our general approach; Section 4 explains STIR in detail; Section 5 explains our experimental set-up and novel evaluation metrics; Section 6 presents and discusses our results and Section 7 concludes."}, {"heading": "2 Previous work", "text": "Qian and Liu (2013) achieve the state of the art in Switchboard corpus self-repair detection, with an F-score for detecting reparandum words of 0.841 using a three-step weighted Max-Margin Markov network approach. Similarly, Georgila (2009) uses Integer Linear Programming post-processing of a CRF to achieve F-scores over 0.8 for reparandum start and repair start detection. However neither approach can operate incrementally.\nRecently, there has been increased interest in left-to-right repair detection: Rasooli and Tetreault (2014) and Honnibal and Johnson (2014) present dependency parsing systems with reparandum detection which perform similarly, the latter equalling Qian and Liu (2013)\u2019s F-score at 0.841. However, while operating left-to-right, these systems are not\ndesigned or evaluated for their incremental performance. The use of beam search over different repair hypotheses in (Honnibal and Johnson, 2014) is likely to lead to unstable repair label sequences, and they report repair hypothesis \u2018jitter\u2019. Both of these systems use a non-monotonic dependency parsing approach that immediately removes the reparandum from the linguistic analysis of the utterance in terms of its dependency structure and repair-reparandum correspondence, which from a downstream NLU module\u2019s perspective is undesirable. Heeman and Allen (1999) and Miller and Schuler (2008) present earlier left-to-right operational detectors which are less accurate and again give no indication of the incremental performance of their systems. While Heeman and Allen (1999) rely on repair structure template detection coupled with a multi-knowledge-source language model, the rarity of the tail of repair structures is likely to be the reason for lower performance: Hough and Purver (2013) show that only 39% of repair alignment structures appear at least twice in Switchboard, supported by the 29% reported by Heeman and Allen (1999) on the smaller TRAINS corpus. Miller and Schuler (2008)\u2019s encoding of repairs into a grammar also causes sparsity in training: repair is a general processing strategy not restricted to certain lexical items or POS tag sequences.\nThe model we consider most suitable for incremental dialogue systems so far is Zwarts et al. (2010)\u2019s incremental version of Johnson and Charniak (2004)\u2019s noisy channel repair detector, as it incrementally applies structural repair analyses (rather than just identifying reparanda) and is evaluated for its incremental properties. Following (Johnson and Charniak, 2004), their system uses an n-gram language model trained on roughly 100K utterances of reparandum-excised (\u2018cleaned\u2019) Switchboard data. Its channel model is a statistically-trained S-TAG parser whose grammar has simple reparandum-repair alignment rule categories for its non-terminals (copy, delete, insert, substitute) and words for its terminals. The parser hypothesises all possible repair structures for the string consumed so far in a chart, before pruning the unlikely ones. It performs equally well to the non-incremental model by the end of each utterance (F-score = 0.778), and can make\ndetections early via the addition of a speculative next-word repair completion category to their S-TAG non-terminals. In terms of incremental performance, they report the novel evaluation metric of time-to-detection for correctly identified repairs, achieving an average of 7.5 words from the start of the reparandum and 4.6 from the start of the repair phase. They also introduce delayed accuracy, a word-by-word evaluation against gold-standard disfluency tags up to the word before the current word being consumed (in their terms, the prefix boundary), giving a measure of the stability of the repair hypotheses. They report an F-score of 0.578 at one word back from the current prefix boundary, increasing word-by-word until 6 words back where it reaches 0.770. These results are the point-of-departure for our work."}, {"heading": "3 Challenges and Approach", "text": "In this section we summarize the challenges for incremental repair detection: computational complexity, repair hypothesis stability, latency of detection and repair structure identification. In 3.1 we explain how we address these.\nComputational complexity Approaches to detecting repair structures often use chart storage (Zwarts et al., 2010; Johnson and Charniak, 2004; Heeman and Allen, 1999), which poses a computational overhead: if considering all possible boundary points for a repair structure\u2019s 3 phases beginning on any word, for prefixes of length n the number of hypotheses can grow in the order O(n4). Exploring a subset of this space is necessary for assigning entire repair structures as in (1) above, rather than just detecting reparanda: the (Johnson and Charniak, 2004; Zwarts et al., 2010) noisy-channel detector is the only system that applies such structures but the potential runtime complexity in decoding these with their STAG repair parser is O(n5). In their approach, complexity is mitigated by imposing a maximum repair length (12 words), and also by using beam search with re-ranking (Lease et al., 2006; Zwarts and Johnson, 2011). If we wish to include full decoding of the repair\u2019s structure (as argued by Hough and Purver (2013) as necessary for full interpretation) whilst taking a strictly incremental and time-critical perspective, reducing this complexity by minimizing the size of this search space is crucial.\nStability of repair hypotheses and latency Using a beam search of n-best hypotheses on a word-by-word basis can cause \u2018jitter\u2019 in the detector\u2019s output. While utterance-final accuracy is desired, for a truly incremental system good intermediate results are equally important. Zwarts et al. (2010)\u2019s time-to-detection results show their system is only certain about a detection after processing the entire repair. This may be due to the string alignment-inspired STAG that matches repair and reparanda: a \u2018rough copy\u2019 dependency only becomes likely once the entire repair has been consumed. The latency of 4.6 words to detection and a relatively slow rise to utterance-final accuracy up to 6 words back is undesirable given repairs have a mean reparandum length of \u22481.5 words (Hough and Purver, 2013; Shriberg and Stolcke, 1998).\nStructural identification Classifying repairs has been ignored in repair processing, despite the presence of distinct categories (e.g. repeats, substitutions, deletes) with different pragmatic effects (Hough and Purver, 2013).1 This is perhaps due to lack of clarity in definition: even for human annotators, verbatim repeats withstanding, agreement is often poor (Hough and Purver, 2013; Shriberg, 1994). Assigning and evaluating repair (not just reparandum) structures will allow repair interpretation in future; however, work to date evaluates only reparandum detection."}, {"heading": "3.1 Our approach", "text": "To address the above, we propose an alternative to (Johnson and Charniak, 2004; Zwarts et al., 2010)\u2019s noisy channel model. While the model elegantly captures intuitions about parallelism in repairs and modelling fluency, it relies on string-matching, motivated in a similar way to automatic spelling correction (Brill and Moore, 2000): it assumes a speaker chooses to utter fluent utterance X according to some prior distribution P (X), but a noisy channel causes them instead to utter a noisy Y according to channel model P (Y |X). Estimating P (Y |X) directly from observed data is difficult due to sparsity of repair instances, so a transducer is trained on the rough copy alignments between reparandum and repair. This approach succeeds because repetition and simple substitution repairs\n1Though see (Germesin et al., 2008) for one approach, albeit using idiosyncratic repair categories.\nare very common; but repair as a psychological process is not driven by string alignment, and deletes, restarts and rarer substitution forms are not captured. Furthermore, the noisy channel model assumes an inherently utterance-global process for generating (and therefore finding) an underlying \u2018clean\u2019 string \u2014 much as similar spelling correction models are word-global \u2014 we instead take a very local perspective here.\nIn accordance with psycholinguistic evidence (Brennan and Schober, 2001), we assume characteristics of the repair onset allow hearers to detect it very quickly and solve the continuation problem (Levelt, 1983) of integrating the repair into their linguistic context immediately, before processing or even hearing the end of the repair phase. While repair onsets may take the form of interregna, this is not a reliable signal, occurring in only \u224815% of repairs (Hough and Purver, 2013; Heeman and Allen, 1999). Our repair onset detection is therefore driven by departures from fluency, via information-theoretic features derived incrementally from a language model in line with recent psycholinguistic accounts of incremental parsing \u2013 see (Keller, 2004; Jaeger and Tily, 2011).\nConsidering the time-linear way a repair is processed and the fact speakers are exponentially less likely to trace one word further back in repair as utterance length increases (Shriberg and Stolcke, 1998), backwards search seems to be the most efficient reparandum extent detection method.2 Features determining the detection of the reparandum extent in the backwards search can also be information-theoretic: entropy measures of distributional parallelism can characterize not only rough copy dependencies, but distributionally similar or dissimilar correspondences between sequences. Finally, when detecting the repair end and structure, distributional information allows computation of the similarity between reparandum and repair. We argue a local-detection-with-backtracking approach is more cognitively plausible than string-based left-to-right repair labelling, and using this insight should allow an improvement in incremental ac-\n2We acknowledge a purely position-based model for reparandum extent detection under-estimates prepositions, which speakers favour as the retrace start and over-estimates verbs, which speakers tend to avoid retracing back to, preferring to begin the utterance again, as (Healey et al., 2011)\u2019s experiments also demonstrate.\ncuracy, stability and time-to-detection over stringalignment driven approaches in repair detection."}, {"heading": "4 STIR: Strongly Incremental Repair detection", "text": "Our system, STIR (Strongly Incremental Repair detection), therefore takes a local incremental approach to detecting repairs and isolated edit terms, assigning words the structures in (2). We include interregnum recognition in the process, due to the inclusion of interregnum vocabulary within edit term vocabulary (Ginzburg, 2012; Hough and Purver, 2013), a useful feature for repair detection (Lease et al., 2006; Qian and Liu, 2013).\n{\n...[rmstart ...rmend + {ed}rpstart ...rpend ]... ...{ed}... (2)\nRather than detecting the repair structure in its left-to-right string order as above, STIR functions as in Figure 1: first detecting edit terms (possibly interregna) at step T1; then detecting repair onsets rpstart at T2; if one is found, backwards searching to find rmstart at T3; then finally finding the repair end rpend at T4. Step T1 relies mainly on lexical probabilities from an edit term language model; T2 exploits features of divergence from a fluent language model; T3 uses fluency of hypothesised repairs; and T4 the similarity between distributions after reparandum and repair. However, each stage integrates these basic insights via multiple related features in a statistical classifier."}, {"heading": "4.1 Enriched incremental language models", "text": "We derive the basic information-theoretic features required using n-gram language models, as they have a long history of information theoretic analysis (Shannon, 1948) and provide reproducible results without forcing commitment to one particular grammar formalism. Following recent work on modelling grammaticality judgements (Clark et al., 2013), we implement several modifications to standard language models to develop our basic measures of fluency and uncertainty.\nFor our main fluent language models we train a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995) on the words and POS tags of the standard Switchboard training data (all files with conversation numbers beginning\n\u201cJohn\u201d \u201clikes\u201d\nS0 S1 S2\nT0\n\u201cJohn\u201d \u201clikes\u201d \u201cuh\u201d\ned\nS0 S1 S2 S3\ned\nT1\n\u201cJohn\u201d \u201clikes\u201d \u201cuh\u201d\ned\n\u201cloves\u201d\nrpstart\nS0 S1 S2 S3\ned\n?\nS4\nrpstart\nT2\n\u201cJohn\u201d \u201clikes\u201d\nrmstart rmend\n\u201cuh\u201d\ned\n\u201cloves\u201d\nrpstart\nsw2*,sw3* in the Penn Treebank III release), consisting of \u2248100K utterances, \u2248600K words. We follow (Johnson and Charniak, 2004) by cleaning the data of disfluencies (i.e. edit terms and reparanda), to approximate a \u2018fluent\u2019 language model. We call these probabilities plexkn , p pos kn below.3\nWe then derive surprisal as our principal default lexical uncertainty measurement s (equation 3) in both models; and, following (Clark et al., 2013), the (unigram) Weighted Mean Log trigram probability (WML, eq. 4)\u2013 the trigram logprob of the sequence divided by the inverse summed logprob of the component unigrams (apart from the first two words in the sequence, which serve as the first trigram history). As here we use a local approach we restrict the WML measures to single trigrams (weighted by the inverse logprob of the final word). While use of standard n-gram probability conflates syntactic with lexical probability, WML gives us an approximation to incremental syntactic probability by factoring out lexical frequency.\ns(wi\u22122 . . . wi) = \u2212 log2 pkn(wi | wi\u22122, wi\u22121) (3)\nWML(w0 . . . wn) =\n\u2211i=n\ni=2 log 2 pkn(wi | wi\u22122, wi\u22121)\n\u2212 \u2211n\nj=2 log 2 pkn(wj)\n(4)\nDistributional measures To approximate uncertainty, we also derive the entropy H(w | c) of the possible word continuations w given a context c, from p(wi | c) for all words wi in the vocabulary \u2013 see (5). Calculating distributions over the entire lexicon incrementally is costly, so we approximate this by constraining the calculation to words which are observed at least once in context c in training, wc = {w|count(c, w) \u2265 1} , assuming a uniform distribution over the unseen suffixes by using the appropriate smoothing constant, and subtracting the latter from the former \u2013 see eq. (6).\nManual inspection showed this approximation to be very close, and the trie structure of our ngram models allows efficient calculation. We also make use of the Zipfian distribution of n-grams in corpora by storing entropy values for the 20% most common trigram contexts observed in training, leaving entropy values of rare or unseen contexts to be computed at decoding time with little search cost due to their small or empty wc sets.\n3We suppress the pos and lex superscripts below where we refer to measures from either model.\nH(w | c) = \u2212 \u2211\nw\u2208V ocab\npkn(w | c) log2 pkn(w | c) (5)\nH(w | c) \u2248\n[\n\u2212 \u2211\nw\u2208wc\npkn(w | c) log2 pkn(w | c)\n]\n\u2212 [n\u00d7 \u03bb log 2 \u03bb]\nwhere n = |V ocab| \u2212 |wc|\nand \u03bb = 1\u2212\n\u2211\nw\u2208wc pkn(w | c)\nn\n(6)\nGiven entropy estimates, we can also similarly approximate the Kullback-Leibler (KL) divergence (relative entropy) between distributions in two different contexts c1 and c2, i.e. \u03b8(w|c1) and \u03b8(w|c2), by pair-wise computing p(w|c1) log2( p(w|c1) p(w|c2)\n) only for words w \u2208 wc1 \u2229 wc2 , then approximating unseen values by assuming uniform distributions. Using pkn smoothed estimates rather than raw maximum likelihood estimations avoids infinite KL divergence values. Again, we found this approximation sufficiently close to the real values for our purposes. All such probability and distribution values are stored in incrementally constructed directed acyclic graph (DAG) structures (see Figure 1), exploiting the Markov assumption of n-gram models to allow efficient calculation by avoiding re-computation."}, {"heading": "4.2 Individual classifiers", "text": "This section details the features used by the 4 individual classifiers. To investigate the utility of the features used in each classifier we obtain values on the standard Switchboard heldout data (PTB III files sw4[5-9]*: 6.4K utterances, 49K words)."}, {"heading": "4.2.1 Edit term detection", "text": "In the first component, we utilise the well-known observation that edit terms have a distinctive vocabulary (Ginzburg, 2012), training a bigram model on a corpus of all edit words annotated in Switchboard\u2019s training data. The classifier simply uses the surprisal slex from this edit word model, and the trigram surprisal slex from the standard fluent model of Section 4.1. At the current position wn, one, both or none of words wn and wn\u22121 are classified as edits. We found this simple approach effective and stable, although some delayed decisions occur in cases where slex and WMLlex are high in both models before the end of the edit, e.g. \u201cI like\u201d \u2192 \u201cI {like} want...\u201d. Words classified as ed are removed from the incremental processing\ngraph (indicated by the dotted line transition in Figure 1) and the stack updated if repair hypotheses are cancelled due to a delayed edit hypothesis of wn\u22121."}, {"heading": "4.2.2 Repair start detection", "text": "Repair onset detection is arguably the most crucial component: the greater its accuracy, the better the input for downstream components and the lesser the overhead of filtering false positives required. We use Section 4.1\u2019s information-theoretic features s,WML,H for words and POS, and introduce 5 additional information-theoretic features: \u2206WML is the difference between the WML values at wn\u22121 and wn; \u2206H is the difference in entropy between wn\u22121 and wn; InformationGain is the difference between expected entropy at wn\u22121 and observed s at wn, a measure that factors out the effect of naturally high entropy contexts; BestEntropyReduce is the best reduction in entropy possible by an early rough hypothesis of reparandum onsets within 3 words; and BestWMLBoost similarly speculates on the best improvement of WML possible by positing rmstart positions up to 3 words back. We also include simple alignment features: binary features which indicate if the word wi\u2212x is identical to the current word wi for x \u2208 {1, 2, 3}. With 6 alignment features, 16 N-gram features and a single logical feature edit which indicates the presence of an edit word at position wi\u22121, rpstart detection uses 23 features\u2013 see Table 1.\nWe hypothesised repair onsets rpstart would have significantly lower plex (lower lexicalsyntactic probability) and WMLlex (lower syntac-\ntic probability) than other fluent trigrams. This was the case in the Switchboard heldout data for both measures, with the biggest difference obtained for WMLlex (non-repair-onsets: -0.736 (sd=0.359); repair onsets: -1.457 (sd=0.359)). In the POS model, entropy of continuation Hpos was the strongest feature (non-repair-onsets: 3.141 (sd=0.769); repair onsets: 3.444 (sd=0.899)). The trigram WMLlex measure for the repaired utterance \u201cI haven\u2019t had any [ good + really very good ] experience with child care\u201d can be seen in Figure 2. The steep drop at the repair onset shows the usefulness of WML features for fluency measures.\nTo compare n-gram measures against other local features, we ranked the features by Information Gain using 10-fold cross validation over the Switchboard heldout data\u2013 see Table 1. The language model features are far more discriminative than the alignment features, showing the potential of a general information-theoretic approach."}, {"heading": "4.2.3 Reparandum start detection", "text": "In detecting rmstart positions given a hypothesised rpstart (stage T3 in Figure 1), we use the noisy channel intuition that removing the reparandum (from rmstart to rpstart ) increases fluency of the utterance, expressed here as WMLboost as described above. When using gold standard input we found this was the case on the heldout data, with a mean WMLboost of 0.223 (sd=0.267) for reparandum onsets and -0.058 (sd=0.224) for other words in the 6-word history- the negative boost for non-reparandum words captures the intuition that backtracking from those points would make the utterance less grammatical, and con-\nversely the boost afforded by the correct rmstart detection helps solve the continuation problem for the listener (and our detector).\nParallelism in the onsets of rpstart and rmstart can also help solve the continuation problem, and in fact the KL divergence between \u03b8pos(w | rmstart, rmstart\u22121) and \u03b8pos(w | rpstart, rpstart\u22121) is the second most useful feature with average merit 0.429 (+- 0.010) in crossvalidation. The highest ranked feature is \u2206WML (0.437 (+- 0.003)) which here encodes the drop in the WMLboost from one backtracked position to the next. In ranking the 32 features we use, again information-theoretic ones are higher ranked than the logical features."}, {"heading": "4.2.4 Repair end detection and structure classification", "text": "For rpend detection, using the notion of parallelism, we hypothesise an effect of divergence between \u03b8lex at the reparandum-final word rmend and the repair-final word rpend : for repetition repairs, KL divergence will trivially be 0; for substitutions, it will be higher; for deletes, even higher. Upon inspection of our feature ranking this KL measure ranked 5th out of 23 features (merit= 0.258 (+- 0.002)).\nWe introduce another feature encoding paral-\nlelism ReparandumRepairDifference : the difference in probability between an utterance cleaned of the reparandum and the utterance with its repair phase substituting its reparandum. In both the POS (merit=0.366 (+- 0.003)) and word (merit=0.352 (+- 0.002)) LMs, this was the most discriminative feature."}, {"heading": "4.3 Classifier pipeline", "text": "STIR effects a pipeline of classifiers as in Figure 3, where the ed classifier only permits non ed words to be passed on to rpstart classification and for rpend classification of the active repair hypotheses, maintained in a stack. The rpstart classifier passes positive repair hypotheses to the rmstart classifier, which backwards searches up to 7 words back in the utterance. If a rmstart is classified, the output is passed on for rpend classification at the end of the pipeline, and if not rejected this is pushed onto the repair stack. Repair hypotheses are are popped off when the string is 7 words beyond its rpstart position. Putting limits on the stack\u2019s storage space is a way of controlling for processing overhead and complexity. Embedded repairs whose rmstart coincide with another\u2019s rpstart are easily dealt with as they are added to the stack as separate hypotheses.4\nClassifiers Classifiers are implemented using Random Forests (Breiman, 2001) and we use different error functions for each stage using MetaCost (Domingos, 1999). The flexibility afforded by implementing adjustable error functions in a pipelined incremental processor allows control of the trade-off of immediate accuracy against runtime and stability of the sequence classification.\nProcessing complexity This pipeline avoids an exhaustive search all repair hypotheses. If we limit the search to within the \u3008rmstart, rpstart\u3009 possibilities, this number of repairs grows approximately in the triangular number series\u2013 i.e. n(n+1)2 , a nested loop over previous words as n gets incremented \u2013 which in terms of a complexity class is a quadratic O(n2). If we allow more than one \u3008rmstart, rpstart\u3009 hypothesis per word, the complexity goes up to O(n3), however in the tests that we describe below, we are able to achieve good detection results without permitting this extra search\n4We constrain the problem not to include embedded deletes which may share their rpstart word with another repair \u2013 these are in practice very rare.\nspace. Under our assumption that reparandum onset detection is only triggered after repair onset detection, and repair extent detection is dependent on positive reparandum onset detection, a pipeline with accurate components will allow us to limit processing to a small subset of this search space."}, {"heading": "5 Experimental set-up", "text": "We train STIR on the Switchboard data described above, and test it on the standard Switchboard test data (PTB III files 4[0-1]*). In order to avoid overfitting of classifiers to the basic language models, we use a cross-fold training approach: we divide the corpus into 10 folds and use language models trained on 9 folds to obtain feature values for the 10th fold, repeating for all 10. Classifiers are then trained as standard on the resulting featureannotated corpus. This resulted in better feature utility for n-grams and better F-score results for detection in all components in the order of 5-6%.5\nTraining the classifiers Each Random Forest classifier was limited to 20 trees of maximum depth 4 nodes, putting a ceiling on decoding time. In making the classifiers cost-sensitive, MetaCost resamples the data in accordance with the cost functions: we found using 10 iterations over a resample of 25% of the training data gave the most effective trade-off between training time and accuracy.6 We use 8 different cost functions in rpstart with differing costs for false negatives and positives of the form below, where R is a repair element word and F is a fluent onset:\n(\nRhyp F hyp\nRgold 0 2 F gold 1 0\n)\n5Zwarts and Johnson (2011) take a similar approach on Switchboard data to train a re-ranker of repair analyses.\n6As (Domingos, 1999) demonstrated, there are only relatively small accuracy gains when using more than this, with training time increasing in the order of the re-sample size.\nWe adopt a similar technique in rmstart using 5 different cost functions and in rpend using 8 different settings, which when combined gives a total of 320 different cost function configurations. We hypothesise that higher recall permitted in the pipeline\u2019s first components would result in better overall accuracy as these hypotheses become refined, though at the cost of the stability of the hypotheses of the sequence and extra downstream processing in pruning false positives.\nWe also experiment with the number of repair hypotheses permitted per word, using limits of 1- best and 2-best hypotheses. We expect that allowing 2 hypotheses to be explored per rpstart should allow greater final accuracy, but with the trade-off of greater decoding and training complexity, and possible incremental instability.\nAs we wish to explore the incrementality versus final accuracy trade-off that STIR can achieve we now describe the evaluation metrics we employ."}, {"heading": "5.1 Incremental evaluation metrics", "text": "Following (Baumann et al., 2011) we divide our evaluation metrics into similarity metrics (measures of equality with or similarity to a gold standard), timing metrics (measures of the timing of relevant phenomena detected from the gold standard) and diachronic metrics (evolution of incremental hypotheses over time).\nSimilarity metrics For direct comparison to previous approaches we use the standard measure of overall accuracy, the F-score over reparandum words, which we abbreviate Frm (see 7):\nprecision = rmcorrect\nrmhyp\nrecall = rmcorrect\nrmgold\nFrm = 2\u00d7 precision \u00d7 recall precision + recall\n(7)\nWe are also interested in repair structural classification, we also measure F-score over all repair components (rm words, ed words as inter-\nregna and rp words), a metric we abbreviate Fs. This is not measured in standard repair detection on Switchboard. To investigate incremental accuracy we evaluate the delayed accuracy (DA) introduced by (Zwarts et al., 2010), as described in section 2 against the utterance-final gold standard disfluency annotations, and use the mean of the 6 word F-scores.\nTiming and resource metrics Again for comparative purposes we use Zwarts et al\u2019s time-todetection metrics, that is the two average distances (in numbers of words) consumed before first detection of gold standard repairs, one from rmstart , TDrm and one from rpstart , TDrp. In our 1-best detection system, before evaluation we know a priori TDrp will be 1 token, and TDrm will be 1 more than the average length of rmstart\u2212 rpstart repair spans correctly detected. However when we introduce a beam where multiple rmstarts are possible per rpstart with the most likely hypothesis committed as the current output, the latency may begin to increase: the initially most probable hypothesis may not be the correct one. In addition to output timing metrics, we account for intrinsic processing complexity with the metric processing overhead (PO), which is the number of classifications made by all components per word of input.\nDiachronic metrics To measure stability of repair hypotheses over time we use (Baumann et al., 2011)\u2019s edit overhead (EO) metric. EO measures the proportion of edits (add, revoke, substitute) applied to a processor\u2019s output structure that are unnecessary. STIR\u2019s output is the repair label sequence shown in Figure 1, however rather than evaluating its EO against the current gold standard labels, we use a new mark-up we term the incremental repair gold standard: this does not penalise lack of detection of a reparandum word rm as a bad edit until the\ncorresponding rpstart of that rm has been consumed. While Frm, Fs and DA evaluate against what Baumann et al. (2011) call the current gold standard, the incremental gold standard reflects the repair processing approach we set out in 3. An example of a repaired utterance with an EO of 44% (49 ) can be seen in Figure 4: of the 9 edits (7 repair annotations and 2 correct fluent words), 4 are unnecessary (bracketed). Note the final \u2295rm is not counted as a bad edit for the reasons just given."}, {"heading": "6 Results and Discussion", "text": "We evaluate on the Switchboard test data; Table 2 shows results of the best performing settings for each of the metrics described above, together with the setting achieving the highest total score (TS)\u2013 the average % achieved of the best performing system\u2019s result in each metric.7 The settings found to achieve the highest Frm (the metric standardly used in disfluency detection), and that found to achieve the highest TS for each stage in the pipeline are shown in Figure 5.\nOur experiments showed that different system settings perform better in different metrics, and no individual setting achieved the best result in all of them. Our best utterance-final Frm reaches 0.779, marginally though not significantly exceeding (Zwarts et al., 2010)\u2019s measure and STIR achieves 0.736 on the previously unevaluated Fs. The setting with the best DA improves on (Zwarts et al., 2010)\u2019s result significantly in terms of mean values (0.718 vs. 0.694), and also in terms of the steepness of the curves\n7We do not include time-to-detection scores in TS as it did not vary enough between settings to be significant, however there was a difference in this measure between the 1-best stack condition and the 2-best stack condition \u2013 see below.\n(Figure 6). The fastest average time to detection is 1 word for TDrp and 2.6 words for TDrm (Table 3), improving dramatically on the noisy channel model\u2019s 4.6 and 7.5 words.\nIncrementality versus accuracy trade-off We aimed to investigate how well a system could do in terms of achieving both good final accuracy and incremental performance, and while the best Frm setting had a large PO and relatively slow DA increase, we find STIR can find a good trade-off setting: the highest TS scoring setting achieves an Frm of 0.754 whilst also exhibiting a very good DA (0.711) \u2013 over 98% of the best recorded score \u2013 and low PO and EO rates \u2013 over 96% of the best recorded scores. See the bottom row of Table 2. As can be seen in Figure 5, the cost functions for these winning settings are different in nature. The best non-incremental Frm measure setting requires high recall for the rest of the pipeline to work on, using the highest cost, 64, for false negative rpstart words and the highest stack depth of 2 (similar to a wider beam); but the best overall TS scoring system uses a less permissive setting to increase incremental performance.\nWe make a preliminary investigation into the effect of increasing the stack capacity by comparing stacks with 1-best rmstart hypotheses per rpstart and 2-best stacks. The average differences between the two conditions is shown in Table 3. Moving to the 2-stack condition results in gain in overall accuracy in Frm and Fs, but at the cost of EO and also time-to-detection scores TDrm and TDrp. The extent to which the stack can be increased without increasing jitter, latency and complexity will be investigated in future work."}, {"heading": "7 Conclusion", "text": "We have presented STIR, an incremental repair detector that can be used to experiment with incremental performance and accuracy trade-offs. In future work we plan to include probabilistic and distributional features from a top-down incremental parser e.g. Roark et al. (2009), and use STIR\u2019s distributional features to classify repair type."}, {"heading": "Acknowledgements", "text": "We thank the three anonymous EMNLP reviewers for their helpful comments. Hough is supported by the DUEL project, financially supported\nby the Agence Nationale de la Research (grant number ANR-13-FRAL-0001) and the Deutsche Forschungsgemainschaft. Much of the work was carried out with support from an EPSRC DTA scholarship at Queen Mary University of London. Purver is partly supported by ConCreTe: the project ConCreTe acknowledges the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET grant number 611733."}], "references": [{"title": "Evaluation and optimisation of incremental processors", "author": ["Baumann et al.2011] T. Baumann", "O. Bu\u00df", "D. Schlangen"], "venue": "Dialogue & Discourse,", "citeRegEx": "Baumann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baumann et al\\.", "year": 2011}, {"title": "How listeners compensate for disfluencies in spontaneous speech", "author": ["Brennan", "Schober2001] S.E. Brennan", "M.F. Schober"], "venue": "Journal of Memory and Language,", "citeRegEx": "Brennan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Brennan et al\\.", "year": 2001}, {"title": "An improved error model for noisy channel spelling correction", "author": ["Brill", "Moore2000] Eric Brill", "Robert C Moore"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Brill et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Brill et al\\.", "year": 2000}, {"title": "Statistical representation of grammaticality judgements: the limits of ngram models", "author": ["Gianluca Giorgolo", "Shalom Lappin"], "venue": "In Proceedings of the Fourth Annual Workshop on Cognitive Modeling and Computa-", "citeRegEx": "Clark et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2013}, {"title": "Metacost: A general method for making classifiers cost-sensitive", "author": ["Pedro Domingos"], "venue": "In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Domingos.,? \\Q1999\\E", "shortCiteRegEx": "Domingos.", "year": 1999}, {"title": "Using integer linear programming for detecting speech disfluencies", "author": ["Kallirroi Georgila"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Georgila.,? \\Q2009\\E", "shortCiteRegEx": "Georgila.", "year": 2009}, {"title": "Hybrid multi-step disfluency detection", "author": ["Tilman Becker", "Peter Poller"], "venue": "In Machine Learning for Multimodal Interaction,", "citeRegEx": "Germesin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Germesin et al\\.", "year": 2008}, {"title": "The Interactive Stance: Meaning for Conversation", "author": ["Jonathan Ginzburg"], "venue": null, "citeRegEx": "Ginzburg.,? \\Q2012\\E", "shortCiteRegEx": "Ginzburg.", "year": 2012}, {"title": "Making a contribution: Processing clarification requests in dialogue", "author": ["Arash Eshghi", "Christine Howes", "Matthew Purver"], "venue": "In Proceedings of the 21st Annual Meeting of the Society for Text and Discourse,", "citeRegEx": "Healey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Healey et al\\.", "year": 2011}, {"title": "Speech repairs, intonational phrases, and discourse markers: modeling speakers\u2019 utterances in spoken dialogue", "author": ["Heeman", "Allen1999] Peter Heeman", "James Allen"], "venue": null, "citeRegEx": "Heeman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Heeman et al\\.", "year": 1999}, {"title": "Joint incremental disfluency detection and dependency parsing. Transactions of the Association of Computational Linugistics (TACL), 2:131\u2013142", "author": ["Honnibal", "Johnson2014] Matthew Honnibal", "Mark Johnson"], "venue": null, "citeRegEx": "Honnibal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Honnibal et al\\.", "year": 2014}, {"title": "Modelling expectation in the selfrepair processing of annotat-, um, listeners", "author": ["Hough", "Purver2013] Julian Hough", "Matthew Purver"], "venue": "In Proceedings of the 17th SemDial Workshop on the Semantics and Pragmatics of Dialogue (DialDam),", "citeRegEx": "Hough et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hough et al\\.", "year": 2013}, {"title": "On language utility: Processing complexity and communicative efficiency", "author": ["Jaeger", "Tily2011] T Florian Jaeger", "Harry Tily"], "venue": "Wiley Interdisciplinary Reviews: Cognitive Science,", "citeRegEx": "Jaeger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jaeger et al\\.", "year": 2011}, {"title": "A TAG-based noisy channel model of speech repairs", "author": ["Johnson", "Charniak2004] Mark Johnson", "Eugene Charniak"], "venue": "In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Johnson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2004}, {"title": "The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data", "author": ["Frank Keller"], "venue": "In EMNLP,", "citeRegEx": "Keller.,? \\Q2004\\E", "shortCiteRegEx": "Keller.", "year": 2004}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Ney1995] Reinhard Kneser", "Hermann Ney"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Recognizing disfluencies in conversational speech. Audio, Speech, and Language Processing", "author": ["Lease et al.2006] Matthew Lease", "Mark Johnson", "Eugene Charniak"], "venue": "IEEE Transactions on,", "citeRegEx": "Lease et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lease et al\\.", "year": 2006}, {"title": "Monitoring and self-repair in speech. Cognition, 14(1):41\u2013104", "author": ["W.J.M. Levelt"], "venue": null, "citeRegEx": "Levelt.,? \\Q1983\\E", "shortCiteRegEx": "Levelt.", "year": 1983}, {"title": "Disfluency annotation stylebook for the switchboard corpus", "author": ["Meteer et al.1995] M. Meteer", "A. Taylor", "R. MacIntyre", "R. Iyer"], "venue": null, "citeRegEx": "Meteer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Meteer et al\\.", "year": 1995}, {"title": "A syntactic time-series model for parsing fluent and disfluent speech", "author": ["Miller", "Schuler2008] Tim Miller", "William Schuler"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Miller et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2008}, {"title": "Axiomatic Grammar, Non-Constituent Coordination and Incremental Interpretation", "author": ["David Milward"], "venue": "Ph.D. thesis,", "citeRegEx": "Milward.,? \\Q1991\\E", "shortCiteRegEx": "Milward.", "year": 1991}, {"title": "Disfluency detection using multi-step stacked learning", "author": ["Qian", "Liu2013] Xian Qian", "Yang Liu"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Qian et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2013}, {"title": "Non-monotonic parsing of fluent umm I mean disfluent sentences", "author": ["Rasooli", "Joel Tetreault"], "venue": "EACL", "citeRegEx": "Rasooli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rasooli et al\\.", "year": 2014}, {"title": "Introduction to the special issue on incremental processing in dialogue", "author": ["Rieser", "Schlangen2011] Hannes Rieser", "David Schlangen"], "venue": "Dialogue & Discourse,", "citeRegEx": "Rieser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rieser et al\\.", "year": 2011}, {"title": "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing", "author": ["Roark et al.2009] Brian Roark", "Asaf Bachrach", "Carlos Cardenas", "Christophe Pallier"], "venue": null, "citeRegEx": "Roark et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2009}, {"title": "A mathematical theory of communication", "author": ["Claude E. Shannon"], "venue": null, "citeRegEx": "Shannon.,? \\Q1948\\E", "shortCiteRegEx": "Shannon.", "year": 1948}, {"title": "How far do speakers back up in repairs? A quantitative model", "author": ["Shriberg", "Stolcke1998] Elizabeth Shriberg", "Andreas Stolcke"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "Shriberg et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Shriberg et al\\.", "year": 1998}, {"title": "Preliminaries to a Theory of Speech Disfluencies", "author": ["Elizabeth Shriberg"], "venue": "Ph.D. thesis,", "citeRegEx": "Shriberg.,? \\Q1994\\E", "shortCiteRegEx": "Shriberg.", "year": 1994}, {"title": "The impact of language models and loss functions on repair disfluency detection", "author": ["Zwarts", "Johnson2011] Simon Zwarts", "Mark Johnson"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Zwarts et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zwarts et al\\.", "year": 2011}, {"title": "Detecting speech repairs incrementally using a noisy channel approach", "author": ["Zwarts et al.2010] Simon Zwarts", "Mark Johnson", "Robert Dale"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics,", "citeRegEx": "Zwarts et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zwarts et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 27, "context": "Self-repairs in spontaneous speech are annotated according to a well established three-phase structure from (Shriberg, 1994) onwards, and as described in Meteer et al.", "startOffset": 108, "endOffset": 124}, {"referenceID": 18, "context": "Self-repairs in spontaneous speech are annotated according to a well established three-phase structure from (Shriberg, 1994) onwards, and as described in Meteer et al. (1995)\u2019s Switchboard corpus annotation handbook:", "startOffset": 154, "endOffset": 175}, {"referenceID": 20, "context": "However, such left-to-right operability on its own is not sufficient: in line with the principle of strong incremental interpretation (Milward, 1991), a repair detector should give the best results possible as early as possible.", "startOffset": 134, "endOffset": 149}, {"referenceID": 29, "context": "With one exception (Zwarts et al., 2010), there has been no focus on evaluating or improving the incremental performance of repair detection.", "startOffset": 19, "endOffset": 40}, {"referenceID": 5, "context": "Similarly, Georgila (2009) uses Integer Linear Programming post-processing of a CRF to achieve F-scores over 0.", "startOffset": 11, "endOffset": 27}, {"referenceID": 28, "context": "The model we consider most suitable for incremental dialogue systems so far is Zwarts et al. (2010)\u2019s incremental version of Johnson and Charniak (2004)\u2019s noisy channel repair detector, as it incrementally applies structural repair analyses (rather than just identifying reparanda) and is evaluated for its incremental properties.", "startOffset": 79, "endOffset": 100}, {"referenceID": 28, "context": "The model we consider most suitable for incremental dialogue systems so far is Zwarts et al. (2010)\u2019s incremental version of Johnson and Charniak (2004)\u2019s noisy channel repair detector, as it incrementally applies structural repair analyses (rather than just identifying reparanda) and is evaluated for its incremental properties.", "startOffset": 79, "endOffset": 153}, {"referenceID": 29, "context": "Computational complexity Approaches to detecting repair structures often use chart storage (Zwarts et al., 2010; Johnson and Charniak, 2004; Heeman and Allen, 1999), which poses a computational overhead: if considering all possible boundary points for a repair structure\u2019s 3 phases beginning on any word, for prefixes of length n the number of hypotheses can grow in the order O(n4).", "startOffset": 91, "endOffset": 164}, {"referenceID": 29, "context": "Exploring a subset of this space is necessary for assigning entire repair structures as in (1) above, rather than just detecting reparanda: the (Johnson and Charniak, 2004; Zwarts et al., 2010) noisy-channel detector is the only system that applies such structures but the potential runtime complexity in decoding these with their STAG repair parser is O(n5).", "startOffset": 144, "endOffset": 193}, {"referenceID": 16, "context": "In their approach, complexity is mitigated by imposing a maximum repair length (12 words), and also by using beam search with re-ranking (Lease et al., 2006; Zwarts and Johnson, 2011).", "startOffset": 137, "endOffset": 183}, {"referenceID": 16, "context": "In their approach, complexity is mitigated by imposing a maximum repair length (12 words), and also by using beam search with re-ranking (Lease et al., 2006; Zwarts and Johnson, 2011). If we wish to include full decoding of the repair\u2019s structure (as argued by Hough and Purver (2013) as necessary for full interpretation) whilst taking a strictly incremental and time-critical perspective, reducing this complexity by minimizing the size of this search space is crucial.", "startOffset": 138, "endOffset": 285}, {"referenceID": 27, "context": "Zwarts et al. (2010)\u2019s time-to-detection results show their system is only certain about a detection after processing the entire repair.", "startOffset": 0, "endOffset": 21}, {"referenceID": 27, "context": "1 This is perhaps due to lack of clarity in definition: even for human annotators, verbatim repeats withstanding, agreement is often poor (Hough and Purver, 2013; Shriberg, 1994).", "startOffset": 138, "endOffset": 178}, {"referenceID": 29, "context": "To address the above, we propose an alternative to (Johnson and Charniak, 2004; Zwarts et al., 2010)\u2019s noisy channel model.", "startOffset": 51, "endOffset": 100}, {"referenceID": 6, "context": "Though see (Germesin et al., 2008) for one approach, albeit using idiosyncratic repair categories.", "startOffset": 11, "endOffset": 34}, {"referenceID": 17, "context": "In accordance with psycholinguistic evidence (Brennan and Schober, 2001), we assume characteristics of the repair onset allow hearers to detect it very quickly and solve the continuation problem (Levelt, 1983) of integrating the repair into their linguistic context immediately, before processing or even hearing the end of the repair phase.", "startOffset": 195, "endOffset": 209}, {"referenceID": 14, "context": "Our repair onset detection is therefore driven by departures from fluency, via information-theoretic features derived incrementally from a language model in line with recent psycholinguistic accounts of incremental parsing \u2013 see (Keller, 2004; Jaeger and Tily, 2011).", "startOffset": 229, "endOffset": 266}, {"referenceID": 8, "context": "We acknowledge a purely position-based model for reparandum extent detection under-estimates prepositions, which speakers favour as the retrace start and over-estimates verbs, which speakers tend to avoid retracing back to, preferring to begin the utterance again, as (Healey et al., 2011)\u2019s experiments also demonstrate.", "startOffset": 268, "endOffset": 289}, {"referenceID": 7, "context": "We include interregnum recognition in the process, due to the inclusion of interregnum vocabulary within edit term vocabulary (Ginzburg, 2012; Hough and Purver, 2013), a useful feature for repair detection (Lease et al.", "startOffset": 126, "endOffset": 166}, {"referenceID": 16, "context": "We include interregnum recognition in the process, due to the inclusion of interregnum vocabulary within edit term vocabulary (Ginzburg, 2012; Hough and Purver, 2013), a useful feature for repair detection (Lease et al., 2006; Qian and Liu, 2013).", "startOffset": 206, "endOffset": 246}, {"referenceID": 25, "context": "We derive the basic information-theoretic features required using n-gram language models, as they have a long history of information theoretic analysis (Shannon, 1948) and provide reproducible results without forcing commitment to one particular grammar formalism.", "startOffset": 152, "endOffset": 167}, {"referenceID": 3, "context": "Following recent work on modelling grammaticality judgements (Clark et al., 2013), we implement several modifications to standard language models to develop our basic measures of fluency and uncertainty.", "startOffset": 61, "endOffset": 81}, {"referenceID": 3, "context": "3 We then derive surprisal as our principal default lexical uncertainty measurement s (equation 3) in both models; and, following (Clark et al., 2013), the (unigram) Weighted Mean Log trigram probability (WML, eq.", "startOffset": 130, "endOffset": 150}, {"referenceID": 7, "context": "In the first component, we utilise the well-known observation that edit terms have a distinctive vocabulary (Ginzburg, 2012), training a bigram model on a corpus of all edit words annotated in Switchboard\u2019s training data.", "startOffset": 108, "endOffset": 124}, {"referenceID": 4, "context": "Classifiers Classifiers are implemented using Random Forests (Breiman, 2001) and we use different error functions for each stage using MetaCost (Domingos, 1999).", "startOffset": 144, "endOffset": 160}, {"referenceID": 4, "context": "As (Domingos, 1999) demonstrated, there are only relatively small accuracy gains when using more than this, with training time increasing in the order of the re-sample size.", "startOffset": 3, "endOffset": 19}, {"referenceID": 0, "context": "Following (Baumann et al., 2011) we divide our evaluation metrics into similarity metrics (measures of equality with or similarity to a gold standard), timing metrics (measures of the timing of relevant phenomena detected from the gold standard) and diachronic metrics (evolution of incremental hypotheses over time).", "startOffset": 10, "endOffset": 32}, {"referenceID": 29, "context": "To investigate incremental accuracy we evaluate the delayed accuracy (DA) introduced by (Zwarts et al., 2010), as described in section 2 against the utterance-final gold standard disfluency annotations, and use the mean of the 6 word F-scores.", "startOffset": 88, "endOffset": 109}, {"referenceID": 0, "context": "Diachronic metrics To measure stability of repair hypotheses over time we use (Baumann et al., 2011)\u2019s edit overhead (EO) metric.", "startOffset": 78, "endOffset": 100}, {"referenceID": 0, "context": "While Frm, Fs and DA evaluate against what Baumann et al. (2011) call the current gold standard, the incremental gold standard reflects the repair processing approach we set out in 3.", "startOffset": 43, "endOffset": 65}, {"referenceID": 29, "context": "779, marginally though not significantly exceeding (Zwarts et al., 2010)\u2019s measure and STIR achieves 0.", "startOffset": 51, "endOffset": 72}, {"referenceID": 29, "context": "The setting with the best DA improves on (Zwarts et al., 2010)\u2019s result significantly in terms of mean values (0.", "startOffset": 41, "endOffset": 62}, {"referenceID": 24, "context": "Roark et al. (2009), and use STIR\u2019s distributional features to classify repair type.", "startOffset": 0, "endOffset": 20}], "year": 2014, "abstractText": "We present STIR (STrongly Incremental Repair detection), a system that detects speech repairs and edit terms on transcripts incrementally with minimal latency. STIR uses information-theoretic measures from n-gram models as its principal decision features in a pipeline of classifiers detecting the different stages of repairs. Results on the Switchboard disfluency tagged corpus show utterance-final accuracy on a par with state-of-the-art incremental repair detection methods, but with better incremental accuracy, faster time-to-detection and less computational overhead. We evaluate its performance using incremental metrics and propose new repair processing evaluation standards.", "creator": "LaTeX with hyperref package"}}}