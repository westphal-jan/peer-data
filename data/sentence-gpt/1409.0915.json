{"id": "1409.0915", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2014", "title": "An Approach for Text Steganography Based on Markov Chains", "abstract": "A text steganography method based on Markov chains is introduced, together with a reference implementation. This method allows for information hiding in texts that are automatically generated following a given Markov model. Other Markov - based systems of this kind rely on big simplifications of the language model to work, which produces less natural looking and more easily detectable texts. The method described here is designed to generate texts within a good approximation of the original language model provided. The implementation has also been tested with a set of tests using Python. We will now be able to use this data as well as the output to build a fully-functional model. In addition to providing support for the text in the same codebase, we also include a command line executable that provides a user interface, with a few optional features and functions.\n\n\n\n\nThe output for this program is a script generator, which can be downloaded from Github. Note that this will require a Python interpreter and it can be used in any system. This is to facilitate an interpreter. The command line is compiled, run, and run to generate text and documents. The program also contains the following command line commands. The script can be installed from command line on a Raspberry Pi, but can run from command line from command line on a Raspberry Pi. These commands run either a .sh command or a .sh command. These commands will not take long, but can be downloaded at all costs.\n\nThe script can be run from command line at any time. The output is also generated and run to generate text and documents.\nThe script can be run from command line at any time. The output is also generated and run to generate text and documents. This command does not allow the user to edit files or read text that a single command line can contain. All this code is based on Python.\nThe scripts can be downloaded from command line on a Raspberry Pi, but can run from command line on a Raspberry Pi. These commands will not take long, but can be downloaded at all costs.\nIt is important to note that these scripts are also written with a Python script. If you require more advanced features, there is no requirement for them. These scripts are not written in Python, and cannot be downloaded from command line. This is also to ensure that the script's output is fully functioning on the Raspberry Pi, so that the program has all the necessary dependencies.\nThere is also a tutorial to download the script from Python, which can be found here.\nThe script can be downloaded", "histories": [["v1", "Tue, 2 Sep 2014 22:59:52 GMT  (21kb)", "http://arxiv.org/abs/1409.0915v1", "Presented at 41 JAIIO - WSegI 2012"]], "COMMENTS": "Presented at 41 JAIIO - WSegI 2012", "reviews": [], "SUBJECTS": "cs.MM cs.CL", "authors": ["h hernan moraldo"], "accepted": false, "id": "1409.0915"}, "pdf": {"name": "1409.0915.pdf", "metadata": {"source": "CRF", "title": "An Approach for Text Steganography Based on Markov Chains", "authors": ["H. Hernan Moraldo"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n09 15\nv1 [\ncs .M\nM ]\n2 S\nep 2\n01 4\nKeywords: steganography, Markov chain, Markov model, text, linguistics"}, {"heading": "1 Introduction", "text": "Steganography is the field that deals with the problem of sending a message from a sender A to a recipient B through a channel that can be read by a so-called Warden, in a way that the Warden doesn\u2019t suspect that the message is there.\nSteganographic techniques exist for hiding messages in images, audio, videos, and other media. In particular, text steganography studies information hiding on texts. There are many techniques for this, as summarized on [1] [2]. One of the simplest steganographic methods for texts works by encoding a fixed amount of bits per word, using a table that maps words to codes, and vice versa. A disadvantage of this trivial technique is that the text will be obviously random at a syntactic level, as words are generated in a way that is independent of context.\nThere are other simple methods that store data in the text format, using spacing, capitalization, font or HTML tags. For example SNOW [3] hides information in tabs and spaces at the end of each line, that are usually not visible on text viewers. Also, some techniques start from a base text (the covertext), and modify it in some way: for example by switching words to near synonymous, or by changing sentences from their original grammatical structure to another one that preserves the meaning. The technique shown in [4] hides information by modifying words in a way that resembles ortographical or typographical errors. There are other techniques that rely on translation [5].\nIn other cases, texts are generated using a grammar model; this kind of system has the advantage of producing texts that make sense at a grammatical level, although not at a semantical level.\nAnd there are techniques, like the one described on this paper, that are based on using Markov models to generate texts that encode some hidden message on\nthem. Weihui Dai et al. [6] [7] explore a method for encoding data on this way; [8] shows a simple implementation of a similar concept.\nThis article explores a specific method for using Markov chains for text steganography. How this method compares to other similar methods and how it works is explored further in the next sections. A reference implementation of the method described here is also included in the open source program MarkovTextStego [9]."}, {"heading": "2 Related Work", "text": "Many methods for text steganography that are not based on Markov chains are known. An example is NiceText [10], which shows a way to encode ciphertext to text, that uses custom styles, Context Free Grammars and dictionaries.\nThe approach used in [6] [7] is based on Markov chains. When encoding, some data is provided as input, and the system generates a text as output using a given Markov chain. The stegotexts are generated in a way that simulates that they were generated by the Markov chain.\nHowever, to avoid complex calculations, the Markov model is simplified by assuming that all probabilities from a given state to any other state are equal. This can change the quality of the texts generated by the Markov chain significantly. For example words like \u201dthe\u201d and \u201dnaturally\u201d are both potential starts of a phrase, but the former should be much more frequent than the latter; and this difference is not preserved by the simplification.\nOther Markov - based models or similar models require of similar simplifications of the Markov chain, typically by making all outbound probabilities of each state equal (as in the previous example), or by replacing them by other ones, either explicitly or implicitly through the operation of the encoding algorithm [8] [11] [12].\nThe method described here aims to be an answer to the question of whether it is possible to preserve the probabilities in the Markov models to higher levels of accuracy. The method is not optimally precise, but it generates texts that use a language model that is a good approximation of the provided Markov model."}, {"heading": "3 Markov Chain Models", "text": "A Markov chain is a model for a stochastic process. A sequence of random variables X = (X1, ..., XT ) with values from a finite set S is a Markov chain, if it has the Markov properties [13] [14]:\nLimited Horizon property:\nP (Xt+1 = sk|X1, ..., Xt) = P (Xt+1 = sk|Xt) (1)\nTime Invariant property:\nP (Xt+1 = sk|Xt) = P (X2 = sk|X1) (2)\nThe first property means that the Markov chain doesn\u2019t have memory of any states, beyond the last one. The second property means that the conditional probabilities for all states do not depend on the position (time) on the sequence.\nDiagrams like the one shown in Fig. 1 are frequently used to represent the transitions in Markov chains. All nodes in the graph represent states (elements of S), and any arrow from sj to sk with a value of p means that P (sk|sj) = p. We call any state sk an outbound state of sj , if there is an arrow from sj to sk. For every sj that doesn\u2019t have an arrow to another sk state, P (sk|sj) = 0.\nMarkov chains and models are frequently used to model language [13]; when that\u2019s the case, states in the chain are used, for example, to represent words, characters, or n-grams. Also, Markov models are used in steganography (as described above), and in steganalysis [15] [16].\nA Markov language model may be useful to compute probabilities for phrases, from the n-gram probabilities. For example given the Markov chain shown in Fig. 1, if the process were to start from \u201dstart\u201d (symbol that we use both for start and end of a sentence), the probability of generating the text composed by the sequence of words or states [s1, s4, s7] would be 0.28.\nThese models can also be used to generate random texts. For this, a random source is used that can pick a next state sk with probability P (sk|sj), given the current state sj . The algorithm for generating the random text starts by setting \u201dstart\u201d to be the current state; then, in every iteration it uses the random source to pick the next word, which also becomes the new current state in the next iteration. To generate a single sentence, the process can be made to stop when the state \u201dstart\u201d is reached.\nAlthough Markov chains only have memory of a single previous state, every state can be a bigram, or an n-gram. This way, a Markov language model can have memory for more than a single word. Although this article only describes the steganographic method based on states that are single words, the reference\nimplementation of the system [9] allows using both unigrams and bigrams as states, and it is possible to extend it to support n-grams with n > 2. Table 3 compares the results of the encoding procedure when using unigrams and bigrams.\nA Markov language model with states as single words can be computed from the frequencies of all bigrams, and all unigrams in a text:\nP (wn|wn\u22121) = count(wn\u22121, wn)\ncount(wn\u22121) (3)\nwhere count(a, b) is the number of occurrences of word a followed immediately by word b in the text, and count(a) is the number of occurrences of word a.\nAs discussed above, some steganographic methods are based on these Markov language models. The language model is usually simplified in some way; for example [6] sets all P (x|w) with w fixed, to a fixed k.\nIn these models, once the simplification is done, the Markov chain is used to encode data into text; every word stores some fixed or variable amount of bytes, and every bigram in the generated text is required to have conditional probability P (wn|wn\u22121) > 0 in the Markov chain. A decoding algorithm that reverses the process, transforming the text into data, is also defined.\nThe approach shown in this article avoids much of the simplification in the probabilities of the Markov chain. Although there is still some precision loss in the model, for the most part, the proportions between the frequencies of different n-grams are preserved, specially for long texts."}, {"heading": "4 Fixed-size Steganography", "text": "A main objective in this article is to describe two functions, encode, and decode, that are used to create a text out of a data input, and to get the original data out of an encoded text. In steganography literature, it would be said that encode generates a stegotext out of the input plaintext, while decode does the reverse process. The encode function is not cryptographically secure; it assumes that its input is a plaintext, or some data that has already been encrypted using an independent system. In the latter case, encode\u2019s input can be called ciphertext.\nWe require the encoding function to be invertible; that is, for every input data d1 and d2, encode(d1) = encode(d2) only if d1 = d2. Also decode is the inverse of encode, so for every input d, decode(encode(d)) = d. The encoding function encode is required to work on all the domain of data d; the required domain of decode however needs only be the image of encode.\nThe encode and decode functions will be built out of simpler functions, for fixed-size encoding and decoding. These functions are encodefixed(data, datasize), and decodefixed(text, datasize). Both the Markov chain and the starting symbol are actually required for these functions too, but they are left out for simplicity. Only when it is required for the purposes of the explanation, a third argument\nis added to both functions, for the start symbol: encodefixed(data, datasize, startsymbol), and decodefixed(text, datasize, startsymbol).\nIn this system, the size of d in bits is known beforehand both for encodefixed and for decodefixed. The requirements for both functions are weaken compared to those for their non-fixed counterparts; it is required that for every input data d1 and d2 such that length(d1) = length(d2), encodefixed(d1, length(d1)) = encodefixed(d2, length(d2)) only if d1 = d2 (where length(d) is the size of d in bits). This weaker restriction means that the encoder may produce the same text for two different data inputs, only if they have different lengths, as can be seen in the examples in Table 1.\nAlso, decodefixed(z, length(z)) = d with z = encodefixed(d, length(d))."}, {"heading": "4.1 Mapping of Probabilities to Ranges", "text": "A basic component for encoding and decoding is the function named subranges, that maps all outbound states from a given state, to subranges of a given range. These subranges are a partition of the original range.\nsubranges(mc, s, r) = [(s1, r1), ..., (sn, rn)] (4)\nwhere mc is a Markov chain, s is a state in S, and r is a range of natural numbers [a, b]. The result is a list that must have some properties that are described below.\nThe behavior of this function is that it maps outbound states of a Markov chain to subranges of a given range, in a way that approximately matches the proportion between the sizes of the different subranges, to the proportion between the probabilities of the respective states. For example, ifmc is the chain in Fig. 1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])]. That is, because each outbound state has a 0.5 probability, it has to get half of the full range. If s = s2, the expected result would be [(s4, [0, 0]), (s5, [1, 3])], where again the length of the subranges matches the proportion of their respective probabilities.\nThis partitioning method will be used in an iterative way, both for encoding and for decoding. Fig. 2 (in Section 4.2) shows how this is done, although the details of the operation are described in the next sections.\nThe returned value for subranges in Equation 4 is a list of pairs (sk, rk), where sk is a state such that P (sk|s) > 0, and rk is a subrange of r. The subranges of r returned by subranges are a partition of r.\nA good implementation of the function generates a mapping between subranges rk and states sk, such that the fraction of the total range length for each rk is approximately equal to the probability of the respective state sk. That is:\nlength(rk)\nlength(r) \u2248 P (sk|s) (5)\nWhere the length of a range is defined to be length([a, b]) = b\u2212 a+ 1.\nThe property in Equation 5 is not a strict requirement, as even without this condition the encoding function will still generate texts that are decoded correctly. However, only when this condition is held the texts that are generated will be approximately described by the original Markov language model.\n[Condition of Minimal Length] The following condition is actually required for the steganographic system to work, however. Any time that there are at least two different states s1 and s2 such that P (s1|s) > 0 and P (s2|s) > 0 (that is, every time s has at least two outbound states), it is required that subranges returns a list with at least two elements.\nThis restriction is necessary for ensuring that both the encoder and decoder methods halt for all inputs. It might produce precision loss in many cases however, as in the following example: an s state has two outbound states s1 and s2, with conditional probabilities P (s1|s) = 0.99 and P (s2|s) = 0.01, and the input range to process is r = [0, 1].\nIn this case, it would seem that the best output would map s1 to the full range: the returned value for this would be [(s1, [0, 1])]. However this value doesn\u2019t hold the Condition of Minimal Length, as the list has a single element, despite s having more than one outbound state.\nBecause of this, the only valid results for this example would be [(s1, [0, 0]), (s2, [1, 1])] and a symmetrical one (same ranges but switching states). As can be seen, these valid options are worse approximations to the input conditional probabilities, than just mapping s1 to the full state; however the condition described disallows this better approximation.\nThe following three functions are used by the encoding and decoding methods.\nAs described, subranges returns a list that maps ranges to states. The function subrangeForState uses the list to return the subrange that is assigned to a given state:\nsubrangeForState(mc, sk, r, sl) = rk\nfrom [..., (sk, rk), ...] = subranges(mc, sk, r) such that sk = sl (6)\nThe function subrangeForNumber returns the subrange in the list that contains a given number:\nsubrangeForNumber(mc, sk, r, number) = subrange rk\nfrom [..., (sk, rk), ...] = subranges(mc, sk, r) such that number \u2208 rk (7)\nThe function stateForNumber returns the state that is assigned to the subrange returned by subrangeForNumber:\nstateForNumber(mc, sk, r, number) = state sk\nfrom [..., (sk, rk), ...] = subranges(mc, sk, r) such that number \u2208 rk (8)\nThese functions will be used in the next sections."}, {"heading": "4.2 Encoding Fixed-size Data Using Markov Chains", "text": "The function stateForNumber, can also be seen as a function that encodes data to a single word. Given a Markov chain, a state, a range and a number (the input data), it finds the corresponding state or word in the chain for that number. Related to that, subrangeForNumber also defined above, returns the subrange that corresponds to the word returned by stateForNumber.\nBased on these two functions, a sequence of states st and a sequence of ranges rt can be generated as described in the following two equations. These sequences are computed given a Markov chain mc, an initial state s0 (typically \u201dstart\u201d), an input data (number), and an initial range r0 (typically [0, 2\nn\u2212 1], where n is the length of the data to store):\nst = stateForNumber(mc, st\u22121, rt\u22121, number) (9)\nrt = subrangeForNumber(mc, st\u22121, rt\u22121, number) (10)\nBoth sequences are defined to be finite (as we want to encode data to a finite sequence of words); the final element for both is T such that length(rT ) = 1. This means that we stop encoding when the sequence of words describes a single number.\nFinally, encodefixed(data, length(data)) = [s1, ..., sT ]. This encoding process works by partitioning an input range in a way that matches the outbound states of a given state, and then selecting the outbound state whose subrange contains the number to encode. After this is done, the selected subrange and state are used as the input for the next iteration of the algorithm. When the process finishes, the encoded text is the sequence of states that the algorithm went through.\nThe subranges function is restricted by the Condition of Minimal Length in Section 4.1 to always split a range in more than one subrange, whenever possible; therefore the iteration of this process will produce ranges that are smaller and smaller. (Even though it is possible that a Markov chain that is computed from a text contains states with only one outbound state, those will eventually lead to start, which will have more than one outbound state.) Also all subranges must contain at least one element, so the iterative generation of subranges converges to a subrange of length 1.\nBecause the selected subrange length converges to 1, the process has to finish, and when it finishes there is a subrange around a single number (the original input) and a list of states (words). For every input data, there is a final result.\nFor every number d of size n, this final result can be seen as a path that points to d, as every state in the word sequence tells which subrange to choose from the partitions generated by subranges. Using this path intuition, it can be seen that if d1 and d2 are two different numbers of the same size n, their encoded texts are necessarily different, as they lead to different numbers. In the same way, the decoding system can find d using the text as a path to the length 1 subrange.\nFig. 2 shows this partitioning process. The example in the figure uses the range [0, 8], with the numbers encoded in binary. If we use the Markov chain shown in Fig. 1 and we start from start, in a first step the range has to be split in half, because the probabilities for the two states s1 and s2 are both 0.5. The subrange assigned to s2 can then be split in two other parts, now for the states s4 and s5, but the proportions are 0.25 and 0.75 in this case. This shows that if we were to encode the binary number 100, with a fixed size n = 3 bits, we would get the text [s2, s4]. If we were trying to encode the binary number 111, we would need to continue partitioning the range for s5, until there is only a single number in the last subrange.\nTable 1 shows the output of encodefixed for a number of inputs. The reference implementation [9] was used, and the results may vary in other implementations, depending on specific details of the range partitioning algorithm. All examples use the Markov chain shown in Fig. 1, with \u201dstart\u201d as the starting state. In particular, it is possible to see that 100 indeed encodes to [s2, s4], as described above."}, {"heading": "4.3 Decoding of Fixed-size Data Using Markov Chains", "text": "Decoding of fixed-size data is based on subrangeForState, which was described on Section 4.1. It was previously described as a function that returns the subrange that is assigned to a given state; but it can also be seen as a decoder from states to numbers. In this way, the function subrangeForState(mc,wk, r, wl) decodes a single word state wl, given that the previous state was wk. The decoded value is not a number, but a range of numbers: [a, b] where both a and b are natural numbers.\nGiven an input sequence of states or words wt (where w0 is taken to be the initial state used for encoding) and an initial range r0 (typically [0, 2\nn \u2212 1]) we define the sequence of ranges rt as:\nrt = subrangeForState(mc,wt, rt\u22121, wt\u22121) (11)\nThe output of the decodefixed is the value of the range rT , where T is the first t such that length(rt) = 1. Since when that happens the range covers a single number, the decoding process can just return that number.\nA valid output isn\u2019t guaranteed for all texts (sequences of words), only for words that have been generated by using the encodefixed process described above.\nThe decoding process works because it follows the same path that the encoder process followed when generating the text, and this path leads to the original input data. The encoder writes a sequence of words while refining subranges until finding a range that has length 1. The decoding process follows the states written by the encoder, which lead to exactly the same sequence of subranges. This means that decodefixed will reach the input of encodefixed, when feed with the output of encodefixed. This makes decodefixed acts as the inverse for encodefixed, for fixed n.\nAn additional property of decodefixed as it is defined here is that if data = decodefixed(text), then also data = decodefixed(text + text2), where text2 is any text and \u201d+\u201d is the list concatenation operation. This is because the fixed decoding algorithm finishes computing the value for data when the last subranges converge to a single number, and that happens at the same place in the text sequence for text and for text+ text2.\nThis property is useful because it allows us to concatenate encoded texts, and they can be decoded directly as the decoder can tell where every text starts and ends. This is applied to the variable encoding algorithm discussed in Section 5."}, {"heading": "4.4 Implementation Details", "text": "A direct implementation of the algorithms described above would require that many operations are applied to the n bit ranges in every iteration of encoding and decoding. For example, in every iteration of the fixed-size decoding algorithm, a call to subranges needs to be done with a range of numbers with n bits of size, until the length of the selected range is 1 (so that the range matches the original input). This is very inefficient both regarding memory usage and processing time.\nIt is possible to avoid processing on the full n bits on every iteration, by making some changes to the underlying algorithms. Some data with length n can be processed more efficiently if only a short, moving window of a few bits is processed in every iteration. We define subrangesfast:\nsubrangesfast(mc, s, rmbits, n) = subranges(mc, s, expand(rshort, n)) (12)\nwhere rshort = [a, b] is defined to be a range where a and b are two numbers that can be expressed in up tom bits, and expand(range,m, n) computes [a2, b2], with a2 identical to a in all its leftmost m bits, and 0 in the remaining bits, and with b2 identical to b in all its leftmost m bits, and 1 in the remaining bits. This means that we can use expand to convert short ranges like [01, 10] (in binary) to the longer 4 bit range [0100, 1011], if n = 4.\nAn efficient implementation of subrangesfast returns all subranges in short form, for any input. When the ranges have to be split in a way that requires infinite or long precision (for example if there are two states, with P (s1|s) =\n0.3 and P (s2|s) = 0.7), this is only possible if a precision limit is set in the implementation. This precision limit can be set to mean that regardless of the input of subrangesfast, there is a maximum number of bits that can be used for the partitioning process.\nFor example, with n = 100 and the probabilities described above, the ranges returned could be: [00000000, 01001101] for s1, and [01001110, 11111111] for s2. In this case, s1 really has about 0.305 of the numbers of the total range, so using 8 of the 100 bits is a good approximation. If we were to use only 4 bits in subrangesfast for this case, it would return: [0000, 0100] for s1, and [0101, 1111] for s2. In this case s1 maps to about 0.312 numbers of the total range; this is a slightly worse approximation, but it might be better as it requires using only half the amount of bits.\nBoth for encoding and decoding, a bit stream data structure will be needed. For encoding, this stream of bits will be read; when decoding, it will be used to write the data output, in a bitwise fashion.\nWhen encoding, in every iteration subrangesfast will require a small number of bits to be read from the bit stream. As soon as those bits are read, they can be discarded from the bit stream. Also, subrangesfast will generate new ranges in every call, and in every iteration these ranges will be more precise, that is, ranges that cover a smaller amount of numbers. This means that the subranges will require more bits to be stored.\nHowever, if the precision for subrangesfast is set to a finite value (as described above), the number of bits at the right of the range that differ from each other will be at most k, for some k. This means that with every iteration, the ranges will grow in size n, but the leftmost bits will at the same time converge bitwise to the same values (for range [a, b], leftmost bits of a and b will be identical). The leftmost bits can then be discarded, as they are already known to match the leftmost bits in the input data.\nThis process ensures that in every iteration of encoding, subrangesfast only has to deal with a moving window that has a limited number of bits, related to the precision set to the system in the implementation.\nSimilarly for decoding; in very iteration, the range that subrangesfast returns will grow in size (as measured in bits). However, while the range grows in size, the leftmost bits converge, so they can be removed, and added to an output bit stream. When the process finishes, the output bit stream will contain the full output of the decoding algorithm: all the bits of the converged range."}, {"heading": "5 Variable Size Encoding and Decoding", "text": "The encoding and decoding process described above only allows to decode data from a text, given that the size of the data is known beforehand. However, requiring the recipient of a steganographic system to know the size of the hidden data before it is decoded is not optimal. An extension of the encoding and decoding methods for variable-size data solves this problem.\nFor variable size encoding and decoding it is required that an integer m is shared beforehand. This number is not the data size, but the size used for a header; it is typically a small value like 16 or 32. Texts c1 and c2 are encoded as shown below, using the three arguments version of encodefixed.\nThe header is encoded first, into c1. This is done using the fixed-data encoding algorithm, with the fixed size m that is known both for encoder and decoder:\nn = length(data) (13)\nc1 = encodefixed(n,m, start) (14)\nOnce the header was encoded, the actual data is encoded into c2. We use w as starting symbol, to ensure that there isn\u2019t an interruption in the flow of the generated text between the last symbol in c1 and the first one in c2:\nw = last word in c1 text sequence (15)\nc2 = encodefixed(data, n, w) (16)\nFinally, encode(data) is defined simply as:\nencode(data) = c1 + c2 (17)\nThat is, the encoded data is just the header text followed by the data text. As w was used as starting symbol for generating c2, there will be no interruption in the flow between both texts.\nFor decoding an input text, we define:\nn\u2032 = decodefixed(text,m, start) (18)\nThat is, decodefixed is used to extract the length information from the header, using the shared value m.\ntext1 = list of words used in decoding n\u2032 (19)\ntext2 = list of words not used in decoding n\u2032 (20)\nw\u2032 = last of text1 (21)\nFinally, decode can be defined:\ndecode(text) = decodefixed(text2, n\u2032, w\u2032) (22)\nIt can be seen that when data\u2032 = decode(encode(data)), it follows that: n\u2032 = n, text1 = c1, text2 = c2, and w\u2032 = w. For this reason, data\u2032 = data, which means that decode is the right decoding function.\nIt is also possible to extend encode, without changing this last property, in this way:\nencode(data) = c1 + c2 + randomText(z) (23)\nwhere z is the last word in c2, and randomText(symbol) generates a random text that ends in period, using the Markov chain and starting from the given state. This can be used to ensure that all texts generated by encode have a final sentence that is complete, and finishes with period. Adding any text won\u2019t affect the decoding at all, as explained in Section 4.3.\nDepending on the kind of data that is being transmitted, it might be useful to encode into c1 the length of the data in bytes, instead of encoding it in bits. Also, the way the length is actually represented into bits matters; if big endian is used to represent a multi-byte length into bytes, short encoded lengths will start with a sequence of 0 bits; this could produce the encoded texts to always start with the same words, or with a small variety of different words (because all leftmost bits are zero). For this reason, either little endian or a representation that reverses the bits of big endian would be preferable."}, {"heading": "6 Conclusions and Future Research", "text": "This article presented a steganographic method based on Markov chains that differs from other similar models in the way precision loss in the language model is avoided. A reference implementation for this method was also presented.\nThe examples shown in Table 1 could seem to show that the system has very low capacity. However this is only because of the Markov chain used; if the system uses a small Markov chain, it will have low capacity, but if it uses a bigger Markov chain it will typically have a higher capacity.\nPreliminary results of empirical tests using a big Markov chain computed from an actual literary text show that the encoded data takes the size of about 6 - 7 times the size of the original data, with an n value that is big enough (for very small n, yet bigger than a few bytes, this factor can be higher, e.g. around 9). Because the produced output is a text, it can be compressed with a high ratio; the compressed size of the texts is about 2 times the size of the original data. However, these results require a more complete and thorough analysis.\nOther possibilities for further research are: to combine this method to other known language based steganographic systems, for producing an overall better steganographic text generation method; to analyze what is the actual, measured performance for this new algorithm, and how this new algorithm compares to other existing algorithms, in terms of stegoanalysis."}], "references": [{"title": "Linguistic Steganography: Survey, Analysis, and Robustness Concerns for Hiding Information in Text", "author": ["K. Bennett"], "venue": "CERIAS Tech Report 2004-13, Purdue University.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Applying Statistical Methods to Text Steganography", "author": ["I. Nechta", "A. Fionov"], "venue": "CoRR.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "SNOW", "author": ["M. Kwan"], "venue": "http://www.darkside.com.au/snow/manual.html", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Information Hiding Through Errors, A Confusing Approach", "author": ["M. Topkara", "U. Topkara", "M.J. Atallah"], "venue": "Proceedings of the SPIE International Conference on Security, Steganography, and Watermarking of Multimedia Contents.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "TranslationBased Steganography", "author": ["C. Grothoff", "K. Grothoff", "L. Alkhutova", "R. Stutsman", "M. Atallah"], "venue": "Proceedings of the 2005 Information Hiding Workshop (IH 2005). Paper 1624.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Text Steganography System Using Markov Chain Source Model and DES Algorithm", "author": ["W. Dai", "Y. Yu", "Y. Dai", "B. Deng"], "venue": "Journal of Software, vol. 5, issue 7, pp. 785-792.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "BinText Steganography Based on Markov State Transferring Probability", "author": ["W. Dai", "Y. Yu", "B. Deng"], "venue": "2nd International Conference on Interaction Sciences: Information Technology, Culture and Human (ICIS \u201909).", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "MarkovTextStego", "author": ["H.H. Moraldo"], "venue": "https://github.com/hmoraldo/markovTextStego", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Hiding the Hidden: a Software System for Concealing Ciphertext as Innocuous Text", "author": ["M. Chapman"], "venue": "Masters thesis, University of Wisconsin-Milwaukee", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "NL Stego", "author": ["C. Siefkes"], "venue": "http://www.siefkes.net/software/nlstego/", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Natural Language Steganography Based on Statistical Text Generation", "author": ["C. Siefkes"], "venue": "http://www.siefkes.net/software/nlstego/slides/slides-nlstego.sxi", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "The MIT Press.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Artificial Intelligence, a Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": "Prentice Hall, 2nd Edition.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Hidden Markov Models and Steganalysis", "author": ["M. Sidorov"], "venue": "2004 workshop on Multimedia and security (MM&Sec \u201904).", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Attacks on Lexical Natural Language Steganography Systems", "author": ["C.M. Taskiran", "U. Topkara", "M Topkara", "E.J. Delp"], "venue": "SPIE International Conference on Security, Steganography, and Water-marking of Multimedia Contents.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "There are many techniques for this, as summarized on [1] [2].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "There are many techniques for this, as summarized on [1] [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "For example SNOW [3] hides information in tabs and spaces at the end of each line, that are usually not visible on text viewers.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "The technique shown in [4] hides information by modifying words in a way that resembles ortographical or typographical errors.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "There are other techniques that rely on translation [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "[6] [7] explore a method for encoding data on this way; [8] shows a simple implementation of a similar concept.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] [7] explore a method for encoding data on this way; [8] shows a simple implementation of a similar concept.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "A reference implementation of the method described here is also included in the open source program MarkovTextStego [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "An example is NiceText [10], which shows a way to encode ciphertext to text, that uses custom styles, Context Free Grammars and dictionaries.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "The approach used in [6] [7] is based on Markov chains.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "The approach used in [6] [7] is based on Markov chains.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "Other Markov - based models or similar models require of similar simplifications of the Markov chain, typically by making all outbound probabilities of each state equal (as in the previous example), or by replacing them by other ones, either explicitly or implicitly through the operation of the encoding algorithm [8] [11] [12].", "startOffset": 319, "endOffset": 323}, {"referenceID": 10, "context": "Other Markov - based models or similar models require of similar simplifications of the Markov chain, typically by making all outbound probabilities of each state equal (as in the previous example), or by replacing them by other ones, either explicitly or implicitly through the operation of the encoding algorithm [8] [11] [12].", "startOffset": 324, "endOffset": 328}, {"referenceID": 11, "context": ", XT ) with values from a finite set S is a Markov chain, if it has the Markov properties [13] [14]: Limited Horizon property:", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": ", XT ) with values from a finite set S is a Markov chain, if it has the Markov properties [13] [14]: Limited Horizon property:", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Markov chains and models are frequently used to model language [13]; when that\u2019s the case, states in the chain are used, for example, to represent words, characters, or n-grams.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "Also, Markov models are used in steganography (as described above), and in steganalysis [15] [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "Also, Markov models are used in steganography (as described above), and in steganalysis [15] [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "implementation of the system [9] allows using both unigrams and bigrams as states, and it is possible to extend it to support n-grams with n > 2.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "The language model is usually simplified in some way; for example [6] sets all P (x|w) with w fixed, to a fixed k.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 22, "endOffset": 28}, {"referenceID": 0, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 65, "endOffset": 71}, {"referenceID": 1, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 79, "endOffset": 85}, {"referenceID": 2, "context": "1, s = start, and r = [0, 3], the expected result would be [(s1, [0, 1]), (s2, [2, 3])].", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "If s = s2, the expected result would be [(s4, [0, 0]), (s5, [1, 3])], where again the length of the subranges matches the proportion of their respective probabilities.", "startOffset": 60, "endOffset": 66}, {"referenceID": 2, "context": "If s = s2, the expected result would be [(s4, [0, 0]), (s5, [1, 3])], where again the length of the subranges matches the proportion of their respective probabilities.", "startOffset": 60, "endOffset": 66}, {"referenceID": 0, "context": "01, and the input range to process is r = [0, 1].", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "In this case, it would seem that the best output would map s1 to the full range: the returned value for this would be [(s1, [0, 1])].", "startOffset": 124, "endOffset": 130}, {"referenceID": 0, "context": "Because of this, the only valid results for this example would be [(s1, [0, 0]), (s2, [1, 1])] and a symmetrical one (same ranges but switching states).", "startOffset": 86, "endOffset": 92}, {"referenceID": 0, "context": "Because of this, the only valid results for this example would be [(s1, [0, 0]), (s2, [1, 1])] and a symmetrical one (same ranges but switching states).", "startOffset": 86, "endOffset": 92}, {"referenceID": 7, "context": "The reference implementation [9] was used, and the results may vary in other implementations, depending on specific details of the range partitioning algorithm.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "This means that we can use expand to convert short ranges like [01, 10] (in binary) to the longer 4 bit range [0100, 1011], if n = 4.", "startOffset": 63, "endOffset": 71}, {"referenceID": 7, "context": "Example benchmarks and results when running MarkovTextStego [9] with Markov chains generated from War and Peace by Tolstoy.", "startOffset": 60, "endOffset": 63}], "year": 2014, "abstractText": "A text steganography method based on Markov chains is introduced, together with a reference implementation. This method allows for information hiding in texts that are automatically generated following a given Markov model. Other Markov based systems of this kind rely on big simplifications of the language model to work, which produces less natural looking and more easily detectable texts. The method described here is designed to generate texts within a good approximation of the original language model provided.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}