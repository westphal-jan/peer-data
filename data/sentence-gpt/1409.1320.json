{"id": "1409.1320", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2014", "title": "Marginal Structured SVM with Hidden Variables", "abstract": "In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu &amp; Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice.", "histories": [["v1", "Thu, 4 Sep 2014 05:06:34 GMT  (381kb,D)", "https://arxiv.org/abs/1409.1320v1", "Accepted by the 31st International Conference on Machine Learning (ICML 2014). 12 pages version with supplement"], ["v2", "Fri, 5 Sep 2014 21:13:36 GMT  (379kb,D)", "http://arxiv.org/abs/1409.1320v2", "Accepted by the 31st International Conference on Machine Learning (ICML 2014). 12 pages version with supplement"]], "COMMENTS": "Accepted by the 31st International Conference on Machine Learning (ICML 2014). 12 pages version with supplement", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["wei ping", "qiang liu", "alexander t ihler"], "accepted": true, "id": "1409.1320"}, "pdf": {"name": "1409.1320.pdf", "metadata": {"source": "META", "title": "Marginal Structured SVM with Hidden Variables", "authors": ["Wei Ping", "Qiang Liu", "Alexander Ihler"], "emails": ["WPING@ICS.UCI.EDU", "QLIU1@ICS.UCI.EDU", "IHLER@ICS.UCI.EDU"], "sections": [{"heading": "1. Introduction", "text": "Conditional random fields (CRFs) (Lafferty et al., 2001) and structured SVMs (SSVMs) (Taskar et al., 2003; Tsochantaridis et al., 2005) are standard tools for structured prediction in many important domains, such as computer vision (Nowozin & Lampert, 2011), natural language processing (Getoor & Taskar, 2007) and computational biology (e.g., Li et al., 2007; Sato & Sakakibara, 2005). However, many practical cases are not well handled by these tools, due to the presence of latent variables or partially labeled datasets. For example, one approach to image segmentation classifies each pixel into a predefined semantic category. While it is expensive to collect labels for every single pixel (perhaps even impossible for ambiguous regions), partially labeled data are relatively easy to ob-\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\ntain (e.g., Verbeek & Triggs, 2007). Examples also arise in natural language processing, such as semantic role labeling, where the semantic predictions are inherently coupled with latent syntactic relations (Naradowsky et al., 2012). However, accurate syntactic annotations are unavailable in many language resources.\nIn past few years, several solutions have been proposed to address hidden variable problems in structured prediction. Perhaps the most notable of these are hidden conditional random fields (HCRFs) (Quattoni et al., 2007) and latent structured SVMs (LSSVMs) (Yu & Joachims, 2009), which are derived from conditional random fields and structured SVMs, respectively. However, both approaches have several shortcomings. CRF-based models often perform worse than SSVM-based methods in practical datasets, especially when the number of training instances is small or the model assumptions are heavily violated (e.g., Taskar et al., 2003). On the other hand, LSSVM relies on a joint maximum a posteriori (MAP) procedure that assigns the hidden variables to deterministic values, and does not take into account their uncertainty. Unfortunately, this can produce poor predictions of the output variables even for exact models (Liu & Ihler, 2013). A better approach is to average over possible states, corresponding to a marginal MAP inference task (Koller & Friedman, 2009; Liu & Ihler, 2013) that marginalizes the hidden variables before optimizing over the output variables.\nContributions. We propose a novel structured SVM algorithm that takes into account the uncertainty of the hidden variables, by incorporating marginal MAP inference that \u201caverages\u201d over the possible hidden states. We show that our method performs significantly better than LSSVM and other state of art methods, especially when the uncertainty of the hidden variables is high. Our method also inherits the general advantages of structured SVMs and consistently outperforms HCRFs, especially when the training sample size is small. We also study the effect of different training algorithms under various models. In particular we show that gradient-based algorithms for our framework are much more efficient than for LSSVM, because\nar X\niv :1\n40 9.\n13 20\nv2 [\nst at\n.M L\n] 5\nS ep\n2 01\n4\nour objective function is smoother than that of LSSVM as it marginalizes, instead of maximizes, over the hidden variables. Finally, we propose a unified framework that includes both our and existing methods as special cases, and provide general insights on the choice of models and optimization algorithms for practitioners.\nWe organize the rest of the paper as follows. In Section 2, we introduce related work. We present background and notation in Section 3, and derive our marginal structured SVM in Section 4. The unified framework is proposed in Section 5. Learning and inference algorithms for the model are presented in Section 6. We report experimental results in Section 7 and conclude the paper in Section 8."}, {"heading": "2. Related Work", "text": "HCRFs naturally extend CRFs to include hidden variables, and have found numerous applications in areas such as object recognition (Quattoni et al., 2004) and gesture recognition (Wang et al., 2006). HCRFs have the same pros and cons as general CRFs; in particular, they perform well when the model assumptions hold and when there are enough training instances, but may otherwise perform badly. Alternatively, the LSSVM (Yu & Joachims, 2009) is an extension of structured SVM that handles hidden variables, with wide application in areas like object detection (Zhu et al., 2010), human action recognition (Wang & Mori, 2009), document-level sentiment classification (Yessenalina et al., 2010) and link prediction (Xu et al., 2013). However, LSSVM relies on a joint MAP procedure, and may not perform well when a non-trivial uncertainty exists in the hidden variables. Recently, Schwing et al. (2012) proposed an -extension framework for discriminative graphical models with hidden variables that includes both HCRFs and LSSVM as special cases.\nA few recent works also incorporate uncertainty over hidden variables explicitly into their optimization frameworks. For example, Miller et al. (2012) proposed a max margin min-entropy (M3E) model that minimizes an uncertainty measure on hidden variables while performing max-margin learning. They assume that minimizing hidden uncertainty will improve the output accuracy. This is valid in some applications, such as object detection, where reducing the uncertainty of object location can improve the category prediction. However, in cases like image segmentation, the missing labels may come from ambiguous regions, and maintaining that ambiguity can be important. In another work, Kumar et al. (2012) proposes a learning procedure that encourages agreement between two separate models \u2013 one for predicting outputs and another for representing the uncertainty over the hidden variables. They model the uncertainty of hidden variable during training, and rely on a joint MAP procedure during prediction.\nOur proposed method builds on recent work for marginal MAP inference (Koller & Friedman, 2009; Liu & Ihler, 2013), which averages over the hidden variables (or variables that are not of direct interest), and then optimizes over the output variables (or variables of direct interest). In many domains, marginal MAP can provide significant improvement over joint MAP estimation, which jointly optimizes hidden and output variables; recent examples include blind deconvolution in computer vision (Fergus et al., 2006; Levin et al., 2011) and relation extraction and semantic role labeling in natural language processing (Naradowsky et al., 2012). Unfortunately, marginal MAP tasks on graphical models are notoriously difficult; marginal MAP can be NP-hard even when the underlying graphical model is tree-structured (Koller & Friedman, 2009). Recently, Liu & Ihler (2013) proposed efficient variational algorithms that approximately solve marginal MAP. In our work, we use their mixed-product belief propagation algorithm as our inference component.\nSub-gradient decent (SGD) (Ratliff et al., 2007) and the concave-convex procedure (CCCP)(Yuille & Rangarajan, 2003) are two popular training algorithms for structured prediction problems. Generally, SGD is straightforward to implement and effective in practice, but may be slow to converge, especially on non-convex and non-smooth objective functions as arise in LSSVMs. CCCP is a general framework for minimizing non-convex functions by transforming the non-convex optimization into a sequence of convex optimizations by iteratively linearizing the nonconvex component of the objective. It has been applied widely in many areas of machine learning, particularly when hidden variables or missing data are involved. We explore both these training methods and compare them across the various models we consider."}, {"heading": "3. Structured Prediction with Hidden Variables", "text": "In this section we review the background on structured prediction with hidden variables. Assume we have structured input-output pairs (x, y) \u2208 X \u00d7 Y , where X , Y are the spaces of the input and output variables. In many applications, this input-output relationship is not only characterized by (x, y), but also depends on some unobserved hidden or latent variables h \u2208 H. Suppose (x, y, h) follows a conditional model,\np(y, h|x;w) = 1 Z(x;w) exp [wT\u03c6(x, y, h)], (1)\nwhere \u03c6(x, y, h) : X \u00d7 Y \u00d7 H \u2192 RD is a set of features which describe the relationships among the (x, y, h), and w \u2208 RD are the corresponding weights, or model parameters. The function Z(x;w) is the normalization constant,\nor partition function, Z(x;w) = \u2211 y,h exp [wT\u03c6(x, y, h)].\nAssuming the weights w are known, the LSSVM of Yu & Joachims (2009) decodes the output variables y given input variables x by performing a joint maximum a posteriori (MAP) inference,\n[y\u0303(w), h\u0303(w)] = argmax (y,h)\u2208Y\u00d7H\np(y, h|x)\n= argmax (y,h)\u2208Y\u00d7H\nwT\u03c6(x, y, h).\nThis gives the optimal prediction of the (y, h)-pair, and one obtains a prediction on y by simply discarding the h component. Unfortunately, the optimal prediction for (y, h) jointly does not necessarily give an optimal prediction on y; instead, it may introduce strong biases even for simple cases (e.g., see Example 1 in Liu & Ihler (2013)). Intuitively, the joint MAP prediction is \u201coverly optimistic\u201d, since it deterministically assign the hidden variables to their most likely states; this approach is not robust to the inherent uncertainty in h, which may cause problems if that uncertainty is significant.\nTo address this issue, we use marginal MAP predictor,\ny\u0302(w) = argmax y\u2208Y \u2211 h p(y, h|x;w)\n= argmax y\u2208Y log \u2211 h exp [wT\u03c6(x, y, h)], (2)\nwhich explicitly takes into account the uncertainty of the hidden variables. It should be noted that y\u0302(w) is in fact the Bayes optimal prediction of y, measured by zero-one loss. The main contribution of this work is to introduce a novel structured SVM-based method for training the marginal MAP predictor, which significantly improves over previous methods."}, {"heading": "4. Marginal Structured SVM", "text": "In this section we derive our main method, the marginal structured SVM (MSSVM), which minimizes an upper bound of the empirical risk function. Assume we have a set of training instances S = {(x1, y1), \u00b7 \u00b7 \u00b7 , (xn, yn)} \u2208 (X \u00d7 Y)n. The risk is measured by an user-specified empirical loss function \u2206(yi, y\u0302i), which quantifies the difference between an estimator y\u0302i and the correct output yi. It is usually difficult to exactly minimize the loss function because it is typically non-convex and discontinuous with w (e.g., Hamming loss). Instead, one adopts surrogate upper bounds to overcome this difficulty.\nAssume y\u0302i(w) is the marginal MAP prediction on instance xi as defined in (2). We upper bound the empirical loss\nfunction \u2206(yi, y\u0302i(w)) as follows,\n\u2206(yi, y\u0302i(w)) \u2264 \u2206(yi, y\u0302i(w)) + log \u2211 h exp[wT\u03c6(xi, y\u0302i(w)), h)]\n\u2212 log \u2211 h exp[wT\u03c6(xi, yi, h)]\n\u2264 max y\n{ \u2206(yi, y) + log \u2211 h exp [wT\u03c6(xi, y, h)] }\n\u2212 log \u2211 h exp [wT\u03c6(xi, yi, h)],\nwhere the first inequality holds because y\u0302i(w) is the marginal MAP prediction (2), and the second because it jointly maxmizes two terms.\nMinimizing this upper bound over the training set with aL2 regularization, we obtain the following objective function for our marginal structured SVM,\n1 2 \u2016w\u20162 + C n\u2211 i=1 max y { \u2206(yi, y) + log \u2211 h exp[wT\u03c6(xi, y, h)] }\n\u2212 C n\u2211\ni=1 log \u2211 h exp [ wT\u03c6(xi, yi, h) ] . (3)\nThe constraint form of (3) can be found in the supplement. Note that the first part of the objective requires a lossaugmented marginal MAP inference, which marginalizes the hidden variables h and then optimizes over the output variables y, while the second part only requires a marginalization over the hidden variables. Both these terms and their gradients are intractable to compute on loopy graphical models, but can be efficiently approximated by mixedproduct belief propagation (Liu & Ihler, 2013) and sumproduct belief propagation (Wainwright & Jordan, 2008), respectively. We will discuss training algorithms for optimizing this objective in Section 6."}, {"heading": "5. A Unified Framework", "text": "In this section, we compare our framework with a spectrum of existing methods, and introduce a more general framework that includes all these methods as special cases. To start, note that the objective function of the LSSVM (Yu & Joachims, 2009) is\n1 2 \u2016w\u20162 + C n\u2211 i=1 max y max h { \u2206(yi, y) + w T\u03c6(xi, y, h) }\n\u2212 C n\u2211 i=1 max h [ wT\u03c6(xi, yi, h) ] . (4)\nOur objective in (3) is similar to (4), except replacing the max operator of h with the log-sum-exp function, the so\nTable 1. Model comparisons within our unified framework.\nModel h \u2192 0+(maxh) h = 1 (\n\u2211\nh)\ny \u2192 0+ (maxy) LSSVM MSSVM y = 1 ( \u2211 y) N/A HCRF y = h \u2208 (0, 1) -extension model\ncalled soft-max operator. One may introduce a \u201ctemperature\u201d parameter that smooths between max and soft-max, which motivates a more general objective function that includes MSSVM, LSSVM and other previous methods as special cases,\n1 2 \u2016w\u20162 + C n\u2211 i=1 y log \u2211 y exp { 1 y [ \u2206(yi, y)\n+ h log \u2211 h exp (wT\u03c6(xi, y, h) h )]} \u2212 C\nn\u2211 i=1 h log \u2211 h exp (wT\u03c6(xi, yi, h) h ) , (5)\nwhere y and h are temperature parameters that control how much uncertainty we want account for in y and h, respectively. Similar temperature-based approaches have been used both in structured prediction (Hazan & Urtasun, 2010; Schwing et al., 2012) and in other problems, such as semi-supervised learning (Samdani et al., 2012; Dhillon et al., 2012).\nOne can show (Lemma 1 in supplement) that objective (5) is an upper bound of the empirical loss function \u2206(yi, y\u0302i\nh(w)) over the training set, where the prediction y\u0302i h(w) is decoded by \u201cannealed\u201d marginal MAP,\ny\u0302i h(w) = arg max y log \u2211 h exp [wT\u03c6(xi, y, h) h ] .\nThis framework includes a number of existing methods as special cases. It reduces to MSSVM in (3) if y \u2192 0+ and h = 1, and LSSVM in (4) if y \u2192 0+ and h \u2192 0+. If we set y = h = 1, we obtain the loss-augmented likelihood objective in Volkovs et al. (2011), and further reduces to the standard likelihood objective of HCRFs if we assume \u2206(yi, y) \u2261 0. Our framework also generalizes the -extension model by Schwing et al. (2012), which corresponds to the restriction that y = h. See Table 1 for a summarization of these model comparisons. In the sequel, we provide some general insights on selecting among these different models through our empirical evaluations."}, {"heading": "6. Training Algorithms", "text": "In this section, we introduce two optimization algorithms for minimizing the objective function in (3), including\na sub-gradient descent (SGD) algorithm and a concaveconvex procedure (CCCP). An empirical comparison of these two algorithms is given in the experiments of Section 7."}, {"heading": "6.1. Sub-gradient Descent (SGD)", "text": "According to Danskin\u2019s theorem, the sub-gradient of the MSSVM objective (3) is:\n\u2207wM = w + C n\u2211 i=1 Ep(h|xi,y\u0302i)[\u03c6(xi, y\u0302i, h)]\n\u2212 C n\u2211 i=1 Ep(h|xi,yi)[\u03c6(xi, yi, h)], (6)\nwhere,\ny\u0302i = arg max y\u2208Y\n{ \u2206(yi, y) + log\n\u2211 h exp[wT\u03c6(xi, y, h)] } (7)\nis the loss-augmented marginal MAP prediction, which can be approximated via mixed-product belief propagation as described in Liu & Ihler (2013). The Ep(h|xi,y\u0302i) and Ep(h|xi,yi) denote the expectation over the distributions p(h|xi, y\u0302i) and p(h|xi, yi), respectively. Both expectations can similarly be approximated using the marginal probabilities obtained from belief propagation. See Algorithm 1 for details of the sub-gradient descent (SGD) algorithm for MSSVM.\nFurthermore, one can show (Lemma 2 in supplement) that the (sub-)gradient of the unified framework (5) is\n\u2207wU = w + C n\u2211 i=1 Ep( y, h)(y,h|xi)[\u03c6(xi, y, h)]\n\u2212 C n\u2211 i=1 Ep h (h|xi,yi)[\u03c6(xi, yi, h)]. (8)\nwhere the corresponding temperature controlled distributions are defined as,\np h(h|xi, y) \u221d exp [wT\u03c6(xi, y, h)\nh\n] ,\np( y, h)(y|xi) \u221d exp { 1 y [ \u2206(y, yi)\n+ h log \u2211 h exp (wT\u03c6(xi, y, h) h )]} ,\np( y, h)(y, h|xi) = p h(h|xi, y) \u00b7 p( y, h)(y|xi).\nExactly as in Table 1, this reduces to the sub-gradient of MSSVM (6) if y \u2192 0+ and h = 1, the sub-gradient of LSSVM if y \u2192 0+ and h \u2192 0+, and the gradient of\nAlgorithm 1 Sub-gradient Descent for MSSVM Input: number of iterations T , learning rate \u03b7 Output: the learned weight vector w\u2217 w = 0 for t = 1 to T do \u2207w = 0 for i = 1 to n do\n1. Calculate \u03c6m = Ep(h|xi,y\u0302i)[\u03c6(xi, y\u0302i, h)] by mixed-product BP (y\u0302i is defined in (7)) 2. Calculate \u03c6s = Ep(h|xi,yi)[\u03c6(xi, yi, h)] by sumproduct BP 3. \u2207w \u2190 \u2207w + C(\u03c6m \u2212 \u03c6s)\nend for w \u2190 (1\u2212 \u03b7)w \u2212 \u03b7\u2207w\nend for w\u2217 \u2190 w\nHCRF if y = 1, h = 1 and \u2206(y, yi) \u2261 0. One can simply substitute these (sub-)gradients into Algorithm 1 to obtain the corresponding training algorithms for LSSVM and HCRF. In those cases, max-product BP and sum-product BP can be used to approximate the inference operations instead."}, {"heading": "6.2. CCCP Training Algorithm", "text": "The concave-convex procedure (CCCP) (Yuille & Rangarajan, 2003) is a general non-convex optimization algorithm with wide application in machine learning. It is based on the idea of rewriting the non-convex objective function into a sum of a convex function and a concave function (or equivalently a difference of two convex functions), and transforming the non-convex optimization problem into a sequence of convex sub-problems by linearizing the concave part. CCCP provides a straightforward solution for our problem, since the objective functions of all the methods we have discussed \u2013 in (3), (4) and (5) \u2013 are naturally differences of two convex functions. For example, the MSSVM objective in (3) can be naturally written as,\nf(w) = f+(w)\u2212 f\u2212(w),\nwhere,\nf+(w) = 1\n2 \u2016w\u20162 + C n\u2211 i=1 max y { \u2206(yi, y)\n+ log \u2211 h exp[wT\u03c6(xi, y, h)] } ,\nf\u2212(w) = C n\u2211 i=1 log \u2211 h exp[wT\u03c6(xi, yi, h)].\nDenoting the parameter vector at iteration t by wt, the CCCP algorithm updates to new parameters wt+1 by min-\nAlgorithm 2 CCCP Training of MSSVM Input: number of outer iterations T , learning rate \u03b7, tolerance for inner loops Output: the learned weight vector w\u2217 w = 0 for t = 1 to T do u = 0 for i = 1 to n do\n1. Calculate \u03c6s = Ep(h|xi,yi)[\u03c6(xi, yi, h)] by sum-product BP\n2. u = u+ \u03c6s end for repeat \u2207w = 0 for i = 1 to n do\n1. Calculate \u03c6m = Ep(h|xi,y\u0302i)[\u03c6(xi, y\u0302i, h)] by mixed-product BP (y\u0302i is defined in (7))\n2. \u2207w \u2190 \u2207w + C\u03c6m end for \u2207w = \u2207w \u2212 Cu w \u2190 (1\u2212 \u03b7)w \u2212 \u03b7\u2207w\nuntil ||\u2207w|| \u2264 end for w\u2217 \u2190 w\nimizing a convex surrogate function where f\u2212(w) is linearized:\nwt+1 \u2190 arg min w {f+(w)\u2212 wT\u2207f\u2212(wt)}, where \u2207f\u2212(wt) = C \u2211 i Ep(h|xi,yi)[\u03c8(xi, yi, h)]\nis the gradient of f\u2212(w) at wt and its expectation can be evaluated (approximately) by belief propagation. See Algorithm 2 for more details of CCCP for the MSSVM."}, {"heading": "7. Experiments", "text": "In this section, we compare our MSSVM with other stateof-art methods on both simulated and real-world datasets. We demonstrate that the MSSVM significantly outperforms LSSVM, max-margin min-entropy (M3E) model (Miller et al., 2012) and loss-based learning by modeling latent variable(ModLat) (Kumar et al., 2012), especially when the uncertainty over hidden variables is high. Our method also largely outperforms HCRFs in all experiments, especially with a small training sample size."}, {"heading": "7.1. Simulated Data", "text": "We simulate both training and testing data from a pairwise Markov random field (MRF) over graph G = (V,E) with discrete random variables taking values in {0, 1, 2, 3}n,\ngiven by,\np(x, y, h|w) \u221d exp [ \u2211 xi\u2208V wTxi\u03c6(xi) + \u2211 yj\u2208V wTyj\u03c6(yj) + \u2211 hk\u2208V wThk\u03c6(hk)\n+ \u2211\n(xi,yj)\u2208E\nwT(xi,yj)\u03c6(xi, yj) + \u2211\n(xi,hk)\u2208E\nwT(xi,hk)\u03c6(xi, hk)\n+ \u2211\n(yj ,hk)\u2208E\nwT(yj ,hk)\u03c6(yj , hk) ] ,\nwhere the graph structure G is either a \u201chidden chain\u201d (40 nodes) or a 2D grid (size 6 \u00d7 6 \u00d7 2 = 72 nodes), as illustrated in Figure 1. The log-linear weights w are randomly generated from normal distributions. The singleton parameters wxi , wyj and whk are drawn from N(0, \u03c3 2 x \u00b7 I), N(0, \u03c32y \u00b7 I) and N(0, \u03c32h \u00b7 I), respectively, corresponding to indicator vectors \u03c6(xi), \u03c6(yj) and \u03c6(hk). The pairwise parameters w(yj ,hk)=(s,t), w(xi,yj)=(r,s) and w(xi,hk)=(r,t) are drawn from N(0, \u03c32yh), N(0, \u03c3 2 xy) and N(0, \u03c3 2 xh), respectively, corresponding to indicators \u03c6(yj = s, hk = t), \u03c6(xi = r, yj = s) and \u03c6(xi = r, hk = t). Note that the variance parameters \u03c3h and \u03c3yh control the degree of uncertainty in the hidden variables and their importance for estimating the output variables y: the uncertainty of h is high for small values of \u03c3h, and the correlation between h and y is high when \u03c3yh is large.\nWe sample 20 training instances and 100 test instances from both the hidden chain MRF and 2D grid MRF. We set \u03c3x = \u03c3y = \u03c3h = 0.1, \u03c3yh = \u03c3yx = \u03c3hx = 2. Then, we train our MSSVM, LSSVM and HCRF models using both SGD and CCCP. Hamming loss is used in both training and evaluation. In our experiments, we always set the regularization weight C = 1. See Table 2 for the results across different algorithms. We can see that our MSSVM always achieves the highest accuracy when using either training algorithm. It is worth noting that LSSVM obtains a significantly better result using CCCP than SGD; this is mainly due to SGD\u2019s difficulty converging on the piecewise linear objective of LSSVM.\nEmpirical Convergence of SGD and CCCP. Using subgradient descent with learning rate \u03b7M = 0.02, we found that for our MSSVM, training error converged quickly (within 50 iterations). However, sub-gradient descent on the LSSVM would only converge using a much smaller learning rate (\u03b7L = 0.001), and converged more slowly (usually after 250 iterations). This effect is mainly because the LSSVM hard-max makes the objective function nonsmooth, causing sub-gradient descent to be slow to converge. On the other hand, gradient descent on HCRFs converges more easily and quickly than either MSSVM or LSSVM, because its objective function is smoother. Figure 2 shows the oscillation during the iteration of (sub)gradient descent for each model, and empirically illustrates the convergence process.\nWe also observe CCCP converging faster than SGD (using smaller number of inference steps), especially for LSSVM, since CCCP transforms the complex piecewise linear objective into a sequence of easier convex sub-problems. In our empirical study, CCCP always converged well even using approximate inference and non-convex objectives.1 To provide a fair comparison, all methods are trained using the\n1However, it is challenging to provide rigorous convergence guarantees for the non-convex & intractable setting, and not really the focus of this paper.\nCCCP algorithm in the sequel.\nTraining Sample Size. We compared the influence of sample size for each method by ranging the training size from 22 to 210 (with a testing size of 500). The data are all simulated from a MRF on the 20-node hidden chain shown in Figure 1(a). We set \u03c3x = \u03c3y = \u03c3h = 0.1 and \u03c3yh = \u03c3yx = \u03c3hx = 2 as before.\nResults are shown in Figure 3. We found that our MSSVM always considerably outperforms LSSVM, and largely outperforms HCRFs when the training sample sizes are small. HCRFs perform worse than LSSVM for few training data, but outperform LSSVM as the training sample increases.\nOur experiment shows that MSSVM consistently outperforms HCRFs even with reasonably large training sets on a relatively simple toy model. Although the maximum likelihood estimator (as used in HCRFs) is generally considered asymptotically optimal if the model assumptions are correct, this assumes a sufficiently large training size, which may be difficult to acheive in practice. Given enough data (and the correct model), the HCRF should thus eventually improve, but this seems unrealistic in practice since most applications are likely to exhibit high dimensional parameters and relatively few training instances. Additional analysis of the test likelihood and prediction accuracy can be found in the supplement.\nUncertainty of Hidden Variables. We investigate the influence of uncertainty in the hidden variables for each method by adjusting the noise level \u03c3h, which controls the uncertainty of the hidden variables. We draw 20 training samples and 100 test samples from a MRF on a 40-node hidden chain shown in Figure 1(a), with fixed \u03c3x = \u03c3y = 0.1 and \u03c3yh = \u03c3yx = \u03c3hx = 2. For comparison, we also evaluate the performance of M3E (Miller et al., 2012) and\nModLat (Kumar et al., 2012). In accordance with our default setting C = 1, we use the default hyper-parameters in their package. We encourage people to carefully tune these hyper-parameters by cross-validation in future study.\nResults are shown in Table 3. We find that our MSSVM is competitive with LSSVM and M3E when the uncertainty in the hidden variables is low, and becomes significantly better than them as the uncertainty increases. Because LSSVM uses the joint MAP, it does not take into account this uncertainty. On the other hand, M3E explicitly tries to minimize this uncertainty, which can also mislead the prediction. Our MSSVM consistently outperforms HCRFs for moderate training sample sizes. Due to the limitations in current implementations of M3E and ModLat, we only provide their results on chain models."}, {"heading": "7.2. Image Segmentation", "text": "In this section, we evaluate our MSSVM method on the task of segmenting weakly labeled images. Our settings are motivated by the experiments in (Schwing et al., 2012). We assume a ground truth image of 20\u00d740 pixels as shown in Figure 4 (a), where each pixel i has a label yi taking values in {1, \u00b7 \u00b7 \u00b7 , 5}. The observed image x is obtained by adding Gaussian noise, N(0, 5), on the ground truth image as Figure 4 (b).\nWe use the 2D-grid model as in Figure 1 (b), with local features \u03c6(yi, xi) = eyi \u2297 xi and pairwise features \u03c6(yi, yj) = eyi \u2297 eyj \u2208 R5\u00d75 as defined in Nowozin & Lampert (2011), where eyi is the unit normal vector with entry one on dimension yi and \u2297 is the outer product. The set of missing labels (hidden variables) are determined at random, in proportions ranging from 10% to 95%. The performance of MSSVM, LSSVM, and HCRFs are evaluated using the CCCP algorithm.\nFigure 4 (c) lists the performance of each method as the percentage of missing labels is increased. We can see that\nthe performance of LSSVM degrades significantly as the number of hidden variables grows. Most notably, MSSVM is consistently the best method across all settings. This can be explained by the fact that the MSSVM combines both the max-margin property and the improved robustness given by properly accounting for uncertainty in the hidden labels."}, {"heading": "7.3. Object Categorization", "text": "Finally, we evaluate our MSSVM method on the task of object categorization using partially labeled images. We use the Microsoft Research Cambridge data set (Winn et al., 2005), consisting of 240 images with 213\u00d7320 pixels and their partial pixel-level labelings. The missing labels may correspond to ambiguous regions, undefined categories or object boundaries, etc.\nModeled on the approach outlined in Verbeek & Triggs (2007), we use 20 \u00d7 20 pixel patches with centers at 10 pixel intervals and treat each patch as a node in our model. This results in a 20 \u00d7 31 grid model as in Figure 1 (b). The local features of each patch are encoded using texture and color descriptors. For texture, we compute the 128- dimensional SIFT descriptor of the patch and vector quantize it into a 500-word codebook, learned by k-means clustering of all patches in the entire dataset. For color, we take 48-dimensional RGB color histogram for each patch. In our experiment, we select the 5 most frequent categories in the dataset and use 2-fold cross validation for testing.\nTable 4 shows the accuracies of each method across the various categories. Again, we find that MSSVM consistently outperforms other methods across all categories, which can be explained by both the superiority of SSVM-based methods for moderate sample size and the improved robustness by maintaining the uncertainty over the missing labels in the learning procedure."}, {"heading": "8. Conclusion", "text": "We proposed a novel structured SVM method for structured prediction with hidden variables. We demonstrate that our MSSVM consistently outperforms state-of-the-art methods in both simulated and real-world datasets, especially when the uncertainty of hidden variables is large. Compared to the popular LSSVM, the objective function of our MSSVM is easier to optimize due to the smoothness of its objective function. We also provide a unified framework which includes our method as well as a spectrum of previous methods as special cases.\nAcknowlegements. This work was sponsored in part by NSF grants IIS-1065618 and IIS-1254071, and in part by by the United States Air Force under Contract No. FA875014-C-0011 under the DARPA PPAML program."}], "references": [{"title": "Deterministic annealing for semi-supervised structured output learning", "author": ["P. Dhillon", "Keerthi", "S. Sathiya", "K. Bellare", "O. Chapelle", "S. Sundararajan"], "venue": "In Proceedings of AISTAT, pp", "citeRegEx": "Dhillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2012}, {"title": "Introduction to statistical relational learning", "author": ["L. Getoor", "B. Taskar"], "venue": "The MIT press,", "citeRegEx": "Getoor and Taskar,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Taskar", "year": 2007}, {"title": "A primal-dual message-passing algorithm for approximated large scale structured prediction", "author": ["T. Hazan", "R. Urtasun"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Hazan and Urtasun,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Urtasun", "year": 2010}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "The MIT press,", "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Modeling latent variable uncertainty for loss-based learning", "author": ["P. Kumar", "B. Packer", "D. Koller"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proceedings of ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Efficient marginal likelihood optimization in blind deconvolution", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Levin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levin et al\\.", "year": 2011}, {"title": "Protein\u2013protein interaction site prediction based on conditional random", "author": ["M.H. Li", "L. Lin", "X.L. Wang", "T. Liu"], "venue": "field. Bioinformatics,", "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "Variational algorithms for marginal map", "author": ["Q. Liu", "A. Ihler"], "venue": "JMLR, 14:3165\u20133200,", "citeRegEx": "Liu and Ihler,? \\Q2013\\E", "shortCiteRegEx": "Liu and Ihler", "year": 2013}, {"title": "Max-margin min-entropy models", "author": ["K. Miller", "P. Kumar", "B. Packer", "D. Goodman", "D. Koller"], "venue": "In Proceedings of AISTATS, pp", "citeRegEx": "Miller et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2012}, {"title": "Improving NLP through marginalization of hidden syntactic structure", "author": ["J. Naradowsky", "S. Riedel", "D. Smith"], "venue": "In Proceeding of EMNLP,", "citeRegEx": "Naradowsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Naradowsky et al\\.", "year": 2012}, {"title": "Structured prediction and learning in computer vision", "author": ["S. Nowozin", "C. Lampert"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Nowozin and Lampert,? \\Q2011\\E", "shortCiteRegEx": "Nowozin and Lampert", "year": 2011}, {"title": "Conditional random fields for object recognition", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Quattoni et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2004}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Transactions on PAMI,", "citeRegEx": "Quattoni et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2007}, {"title": "Online) Subgradient methods for structured prediction", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "In Proceedings of AISTATS, pp", "citeRegEx": "Ratliff et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Unified expectation maximization", "author": ["R. Samdani", "M.W. Chang", "D. Roth"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Samdani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Samdani et al\\.", "year": 2012}, {"title": "RNA secondary structural alignment with conditional random fields", "author": ["K. Sato", "Y. Sakakibara"], "venue": null, "citeRegEx": "Sato and Sakakibara,? \\Q2005\\E", "shortCiteRegEx": "Sato and Sakakibara", "year": 2005}, {"title": "Efficient structured prediction with latent variables for general graphical models", "author": ["A. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun"], "venue": "In Proceedings of ICML,", "citeRegEx": "Schwing et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwing et al\\.", "year": 2012}, {"title": "Max-margin Markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, 6:1453\u20131484,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Scene segmentation with CRFs learned from partially labeled images", "author": ["J. Verbeek", "B. Triggs"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Verbeek and Triggs,? \\Q2007\\E", "shortCiteRegEx": "Verbeek and Triggs", "year": 2007}, {"title": "Losssensitive training of probabilistic conditional random fields", "author": ["M. Volkovs", "H. Larochelle", "R.S. Zemel"], "venue": "Technical report,", "citeRegEx": "Volkovs et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Volkovs et al\\.", "year": 2011}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Hidden conditional random fields for gesture recognition", "author": ["Wang", "S.B", "A. Quattoni", "L. Morency", "D. Demirdjian"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Max-margin hidden conditional random fields for human action recognition", "author": ["Y. Wang", "G. Mori"], "venue": "In Proceedings of CVPR, pp", "citeRegEx": "Wang and Mori,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mori", "year": 2009}, {"title": "Object categorization by learned universal visual dictionary", "author": ["J. Winn", "A. Criminisi", "T. Minka"], "venue": "In Proceedings of ICCV, pp", "citeRegEx": "Winn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Winn et al\\.", "year": 2005}, {"title": "Hyperlink prediction in hypernetworks using latent social features", "author": ["Y. Xu", "D. Rockmore", "A. Kleinbaum"], "venue": "In Discovery Science,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Multi-level structured models for document-level sentiment classification", "author": ["A. Yessenalina", "Y. Yue", "C. Cardie"], "venue": "In Proceedings of EMNLP, pp", "citeRegEx": "Yessenalina et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yessenalina et al\\.", "year": 2010}, {"title": "Learning structural SVMs with latent variables", "author": ["C. Yu", "T. Joachims"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Yu and Joachims,? \\Q2009\\E", "shortCiteRegEx": "Yu and Joachims", "year": 2009}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation,", "citeRegEx": "Yuille and Rangarajan,? \\Q2003\\E", "shortCiteRegEx": "Yuille and Rangarajan", "year": 2003}, {"title": "Latent hierarchical structural learning for object detection", "author": ["L. Zhu", "Y. Chen", "A. Yuille", "W. Freeman"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Zhu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets.", "startOffset": 95, "endOffset": 118}, {"referenceID": 5, "context": "Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) and structured SVMs (SSVMs) (Taskar et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 18, "context": ", 2001) and structured SVMs (SSVMs) (Taskar et al., 2003; Tsochantaridis et al., 2005) are standard tools for structured prediction in many important domains, such as computer vision (Nowozin & Lampert, 2011), natural language processing (Getoor & Taskar, 2007) and computational biology (e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 19, "context": ", 2001) and structured SVMs (SSVMs) (Taskar et al., 2003; Tsochantaridis et al., 2005) are standard tools for structured prediction in many important domains, such as computer vision (Nowozin & Lampert, 2011), natural language processing (Getoor & Taskar, 2007) and computational biology (e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 10, "context": "Examples also arise in natural language processing, such as semantic role labeling, where the semantic predictions are inherently coupled with latent syntactic relations (Naradowsky et al., 2012).", "startOffset": 170, "endOffset": 195}, {"referenceID": 13, "context": "Perhaps the most notable of these are hidden conditional random fields (HCRFs) (Quattoni et al., 2007) and latent structured SVMs (LSSVMs) (Yu & Joachims, 2009), which are derived from conditional random fields and structured SVMs, respectively.", "startOffset": 79, "endOffset": 102}, {"referenceID": 12, "context": "Related Work HCRFs naturally extend CRFs to include hidden variables, and have found numerous applications in areas such as object recognition (Quattoni et al., 2004) and gesture recognition (Wang et al.", "startOffset": 143, "endOffset": 166}, {"referenceID": 23, "context": ", 2004) and gesture recognition (Wang et al., 2006).", "startOffset": 32, "endOffset": 51}, {"referenceID": 30, "context": "Alternatively, the LSSVM (Yu & Joachims, 2009) is an extension of structured SVM that handles hidden variables, with wide application in areas like object detection (Zhu et al., 2010), human action recognition (Wang & Mori, 2009), document-level sentiment classification (Yessenalina et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 27, "context": ", 2010), human action recognition (Wang & Mori, 2009), document-level sentiment classification (Yessenalina et al., 2010) and link prediction (Xu et al.", "startOffset": 95, "endOffset": 121}, {"referenceID": 26, "context": ", 2010) and link prediction (Xu et al., 2013).", "startOffset": 28, "endOffset": 45}, {"referenceID": 6, "context": "In many domains, marginal MAP can provide significant improvement over joint MAP estimation, which jointly optimizes hidden and output variables; recent examples include blind deconvolution in computer vision (Fergus et al., 2006; Levin et al., 2011) and relation extraction and semantic role labeling in natural language processing (Naradowsky et al.", "startOffset": 209, "endOffset": 250}, {"referenceID": 10, "context": ", 2011) and relation extraction and semantic role labeling in natural language processing (Naradowsky et al., 2012).", "startOffset": 90, "endOffset": 115}, {"referenceID": 14, "context": "Sub-gradient decent (SGD) (Ratliff et al., 2007) and the concave-convex procedure (CCCP)(Yuille & Rangarajan, 2003) are two popular training algorithms for structured prediction problems.", "startOffset": 26, "endOffset": 48}, {"referenceID": 8, "context": "Related Work HCRFs naturally extend CRFs to include hidden variables, and have found numerous applications in areas such as object recognition (Quattoni et al., 2004) and gesture recognition (Wang et al., 2006). HCRFs have the same pros and cons as general CRFs; in particular, they perform well when the model assumptions hold and when there are enough training instances, but may otherwise perform badly. Alternatively, the LSSVM (Yu & Joachims, 2009) is an extension of structured SVM that handles hidden variables, with wide application in areas like object detection (Zhu et al., 2010), human action recognition (Wang & Mori, 2009), document-level sentiment classification (Yessenalina et al., 2010) and link prediction (Xu et al., 2013). However, LSSVM relies on a joint MAP procedure, and may not perform well when a non-trivial uncertainty exists in the hidden variables. Recently, Schwing et al. (2012) proposed an -extension framework for discriminative graphical models with hidden variables that includes both HCRFs and LSSVM as special cases.", "startOffset": 144, "endOffset": 912}, {"referenceID": 7, "context": "For example, Miller et al. (2012) proposed a max margin min-entropy (M3E) model that minimizes an uncertainty measure on hidden variables while performing max-margin learning.", "startOffset": 13, "endOffset": 34}, {"referenceID": 4, "context": "In another work, Kumar et al. (2012) proposes a learning procedure that encourages agreement between two separate models \u2013 one for predicting outputs and another for representing the uncertainty over the hidden variables.", "startOffset": 17, "endOffset": 37}, {"referenceID": 4, "context": "In another work, Kumar et al. (2012) proposes a learning procedure that encourages agreement between two separate models \u2013 one for predicting outputs and another for representing the uncertainty over the hidden variables. They model the uncertainty of hidden variable during training, and rely on a joint MAP procedure during prediction. Our proposed method builds on recent work for marginal MAP inference (Koller & Friedman, 2009; Liu & Ihler, 2013), which averages over the hidden variables (or variables that are not of direct interest), and then optimizes over the output variables (or variables of direct interest). In many domains, marginal MAP can provide significant improvement over joint MAP estimation, which jointly optimizes hidden and output variables; recent examples include blind deconvolution in computer vision (Fergus et al., 2006; Levin et al., 2011) and relation extraction and semantic role labeling in natural language processing (Naradowsky et al., 2012). Unfortunately, marginal MAP tasks on graphical models are notoriously difficult; marginal MAP can be NP-hard even when the underlying graphical model is tree-structured (Koller & Friedman, 2009). Recently, Liu & Ihler (2013) proposed efficient variational algorithms that approximately solve marginal MAP.", "startOffset": 17, "endOffset": 1207}, {"referenceID": 17, "context": "Similar temperature-based approaches have been used both in structured prediction (Hazan & Urtasun, 2010; Schwing et al., 2012) and in other problems, such as semi-supervised learning (Samdani et al.", "startOffset": 82, "endOffset": 127}, {"referenceID": 15, "context": ", 2012) and in other problems, such as semi-supervised learning (Samdani et al., 2012; Dhillon et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 0, "context": ", 2012) and in other problems, such as semi-supervised learning (Samdani et al., 2012; Dhillon et al., 2012).", "startOffset": 64, "endOffset": 108}, {"referenceID": 20, "context": "If we set y = h = 1, we obtain the loss-augmented likelihood objective in Volkovs et al. (2011), and further reduces to the standard likelihood objective of HCRFs if we assume \u2206(yi, y) \u2261 0.", "startOffset": 74, "endOffset": 96}, {"referenceID": 17, "context": "Our framework also generalizes the -extension model by Schwing et al. (2012), which corresponds to the restriction that y = h.", "startOffset": 55, "endOffset": 77}, {"referenceID": 9, "context": "We demonstrate that the MSSVM significantly outperforms LSSVM, max-margin min-entropy (M3E) model (Miller et al., 2012) and loss-based learning by modeling latent variable(ModLat) (Kumar et al.", "startOffset": 98, "endOffset": 119}, {"referenceID": 4, "context": ", 2012) and loss-based learning by modeling latent variable(ModLat) (Kumar et al., 2012), especially when the uncertainty over hidden variables is high.", "startOffset": 68, "endOffset": 88}, {"referenceID": 9, "context": "For comparison, we also evaluate the performance of M3E (Miller et al., 2012) and Table 3.", "startOffset": 56, "endOffset": 77}, {"referenceID": 4, "context": "ModLat (Kumar et al., 2012).", "startOffset": 7, "endOffset": 27}, {"referenceID": 17, "context": "Our settings are motivated by the experiments in (Schwing et al., 2012).", "startOffset": 49, "endOffset": 71}, {"referenceID": 17, "context": "Our settings are motivated by the experiments in (Schwing et al., 2012). We assume a ground truth image of 20\u00d740 pixels as shown in Figure 4 (a), where each pixel i has a label yi taking values in {1, \u00b7 \u00b7 \u00b7 , 5}. The observed image x is obtained by adding Gaussian noise, N(0, 5), on the ground truth image as Figure 4 (b). We use the 2D-grid model as in Figure 1 (b), with local features \u03c6(yi, xi) = eyi \u2297 xi and pairwise features \u03c6(yi, yj) = eyi \u2297 eyj \u2208 R5\u00d75 as defined in Nowozin & Lampert (2011), where eyi is the unit normal vector with entry one on dimension yi and \u2297 is the outer product.", "startOffset": 50, "endOffset": 500}, {"referenceID": 25, "context": "We use the Microsoft Research Cambridge data set (Winn et al., 2005), consisting of 240 images with 213\u00d7320 pixels and their partial pixel-level labelings.", "startOffset": 49, "endOffset": 68}, {"referenceID": 25, "context": "We use the Microsoft Research Cambridge data set (Winn et al., 2005), consisting of 240 images with 213\u00d7320 pixels and their partial pixel-level labelings. The missing labels may correspond to ambiguous regions, undefined categories or object boundaries, etc. Modeled on the approach outlined in Verbeek & Triggs (2007), we use 20 \u00d7 20 pixel patches with centers at 10 pixel intervals and treat each patch as a node in our model.", "startOffset": 50, "endOffset": 320}], "year": 2014, "abstractText": "In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice.", "creator": "LaTeX with hyperref package"}}}