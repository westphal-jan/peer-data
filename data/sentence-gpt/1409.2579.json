{"id": "1409.2579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2014", "title": "A theoretical contribution to the fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices", "abstract": "The null linear discriminant analysis method is a competitive approach for dimensionality reduction. The implementation of this method, however, is computationally expensive. Recently, a fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices was proposed by Rambler and his collaborators. Thus, our results demonstrate that the robustness and generality of the algorithm for dimensionality reduction are the primary benefits of the data-based statistical analysis. In our data-based methods, we are able to evaluate the performance and safety of the data-based method. The data-based technique is available in a full paper.\n\n\n\n\n\nWe believe that the general-purpose algorithm can be applied in some circumstances to reduce the variance in the distributions of the distributions. The general-purpose algorithm is also feasible for a subset of the distribution of the distributions. For example, for example, a function in the form of a function with the sum of its 2,000 and so on, is given as (f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f))))))))))))\n\n\nThe main advantage of the implementation of the general-purpose algorithm is that it can be applied in some circumstances to reduce the variance in the distributions. For example, a function in the form of a function with the sum of its 2,000 and so on, is given as (f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f))))))))))))))\n\n\nThe general-purpose algorithm, which generates random matrix multiplication and compacting the distributions in a small group, is used to estimate the distribution of the distribution of the distributions. In the paper, we describe the generalized and general-purpose algorithm and describe its application.\n\n\n\n\n\nThe general-purpose algorithm can be applied in some situations to reduce the variance in the distributions. For example, a function in the form of a function with the sum of its 2,000 and so on, is given as (f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f(f)))))))))))))))\n\nTo understand the general-purpose algorithm, it", "histories": [["v1", "Tue, 9 Sep 2014 11:46:40 GMT  (7kb)", "http://arxiv.org/abs/1409.2579v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.NA cs.CV cs.LG", "authors": ["ting-ting feng", "gang wu"], "accepted": false, "id": "1409.2579"}, "pdf": {"name": "1409.2579.pdf", "metadata": {"source": "CRF", "title": "A theoretical contribution to the fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices", "authors": ["Ting-ting Feng", "Gang Wu"], "emails": ["tofengtingting@163.com.", "gangwu76@126.com", "wugangzy@gmail.com."], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n25 79\nv1 [\ncs .N\nA ]\n9 S\nep 2\n01 4\nThe null linear discriminant analysis method is a competitive approach for dimensionality reduction. The implementation of this method, however, is computationally expensive. Recently, a fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices was proposed. However, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient, and some useful discriminant information will be lost. In this paper, we investigate how to choose the random matrix properly, such that the two criteria of the null LDA method are satisfied theoretically. We give a necessary and sufficient condition to guarantee full column rank of the orientation matrix. Moreover, the geometric characterization of the condition is also described. Keywords:Dimensionality reduction, Linear discriminant analysis (LDA), Null linear discriminant analysis (Null LDA), Small sample size problem."}, {"heading": "1 Introduction", "text": "Dimensionality reduction has become an ubiquitous preprocessing step in many applications. In general, its\nobjectives are to remove irrelevant and redundant data to reduce the computational cost and to improve the\nquality of data for efficient data-intensive processing tasks such as face recognition and data mining. Linear\ndiscriminant analysis (LDA) is one of the most popular and powerful dimensionality reduction techniques\nfor classification (Fukunaga, 1990). However, a main disadvantage of LDA is that the so-called total scatter\nmatrix must be nonsingular. Indeed, in many applications, the scatter matrices can be singular since the\ndata points are from a very high-dimensional space, and thus usually the number of the data samples is\nmuch smaller than the data dimension. This is the well-known small sample size (SSS) problem or the\nundersampled problem (Fukunaga, 1990).\nLet X = [x1,x2, . . . ,xn] be a set of training samples in a d-dimensional feature space, and \u2126 = {\u03c9j : j = 1, 2, . . . , c} be the class labels, with \u03c9j being the j-th class. We denote by nj the number of samples in the j-th class, which satisfies \u2211c\nj=1 nj = n. Let \u00b5j be the centroid of the j-th class, and \u00b5 be the global centroid\nof the training data set. Then we define the within-class scatter matrix\nSW =\nc \u2211\nj=1\n\u2211\nxi\u2208\u03c9j\n(xi \u2212 \u00b5j)(xi \u2212 \u00b5j)T ,\n1School of Mathematics and statistics, Jiangsu Normal University, Xuzhou, 221116, Jiangsu, P.R. China. Email: tofengtingting@163.com. This author is supported by the Postgraduate Innovation Project of Jiangsu Province under grant CXLX13 968. 2Corresponding author (G. Wu). Department of Mathematics, China University of Mining and Technology & School of Mathematics and statistics, Jiangsu Normal University, Xuzhou, 221116, Jiangsu, P.R. China. Email: gangwu76@126.com and wugangzy@gmail.com. This author is supported by the National Science Foundation of China under grant 11371176, the Natural Science Foundation of Jiangsu Province under grant BK20131126, the 333 Project of Jiangsu Province, and the Talent Introduction Program of China University of Mining and Technology.\nand the between-class scatter matrix\nSB =\nc \u2211\nj=1\nnj(\u00b5j \u2212 \u00b5)(\u00b5j \u2212 \u00b5)T \u2261 BBT ,\nwhere B = [ \u221a n1(\u00b51 \u2212 \u00b5), \u221a n2(\u00b52 \u2212 \u00b5), . . . , \u221a nc(\u00b5c \u2212 \u00b5)] \u2208 Rd\u00d7c. The total scatter matrix is defined as\nST =\nn \u2211\nj=1\n(xj \u2212 \u00b5)(xj \u2212 \u00b5)T ,\nmoreover, it is known that (Fukunaga, 1990)\nST = SW + SB.\nWithout loss of generality, we assume that the n training vectors are linear independent. Consequently, the ranks of the matrices ST , SB and SW are n\u2212 1, c\u2212 1 and n\u2212 c, respectively. The LDA method is realized by maximizing the between-class scatter distance while minimizing the total\nscatter (or the within-class scatter) distance (Fukunaga 1990). However, when the dimension of data is much\nlarger than the number of training samples, the total scatter matrix ST (or the within scatter matrix SW ) will be singular, and we suffer from the small sample size problem (Fukunaga, 1990).\nThe null linear discriminant analysis (null LDA) method (Chen et al., 2000) is a competitive approach\nto overcome this difficulty. It first computes the null space of the within-class scatter matrix SW , and then computes the principal components of the between-class scatter matrix SB within the null space of SW . In essence, the null LDA method is to find the orientation (or the transformation) matrix W = [w1,w2, . . . ,wh] \u2208 Rd\u00d7h (of rank h with 1 \u2264 h \u2264 c\u2212 1) that satisfies the following two conditions (Sharma et al., 2012)\nSWW = 0, (1.1)\nand\nSBW 6= 0. (1.2)\nWhen ST is singular, the null LDA method solves\nW = S\u2020TSBW, (1.3)\nfor the orientation matrix W , where S\u2020T stands for the pseudo inverse (or the Moore-Penrose inverse) of ST . In (Sharma et al., 2012), it was shown that the equation (1.3) is a sufficient condition for the null LDA method. However, the null LDA method requires eigenvalue decomposition of S\u2020TSB, and the computational cost will be prohibitive when d is large. In order to release the overhead, Sharma and Paliwal (Sharma et al., 2012) propose to replace W on the right-hand side of (1.3) by any random matrix Y \u2208 Rd\u00d7(c\u22121) of rank c\u2212 1, and make use of\nW = S\u2020TSBY (1.4)\nas the orientation matrix, moreover, they present a fast implementation of null LDA method in (Sharma et\nal., 2012). In recent years, this method has gained wide attentions in the area of dimensionality reduction\nand data mining (Alvarez-Ginarte et al., 2013; Lu et al., 2013; Lyons et al., 2014; Sharma et al., 2014).\nThe following theorem is the main theorem of (Sharma et al., 2012). It shows that (1.4) is a sufficient\ncondition for null LDA. Meanwhile, it is also the basis of the fast implementation of null LDA method\n(Sharma et al., 2012); for more details, we refer to (Sharma et al., 2012).\nTheorem 1. [Theorem 3 of (Sharma et al., 2012)] If the orientation matrix W \u2208 Rd\u00d7(c\u22121) is obtained by using the relation W = S\u2020TSBY (where Y \u2208 Rd\u00d7(c\u22121) is any random matrix of rank c\u22121), then it satisfies the two criteria on null LDA method ( Eqs. (1.1) and (1.2) ) .\nRemark 1. However, we find that this theorem is incomplete. For example, let X = [x1,x2;x3,x4], where {x1,x2} \u2208 \u03c91 and {x3,x4} \u2208 \u03c92. Suppose that \u00b51 = x1+x22 = e\u0302 and \u00b52 = x3+x42 = 2e\u0302, where\ne\u0302 = [1, 0, 1, 1, . . . , 1]T \u2208 Rd,\nwith d \u226b n = 4. Therefore, \u00b5 = (x1 +x2 +x3 +x4)/4 = 32 e\u0302, B = [ \u221a 2(\u00b51 \u2212\u00b5), \u221a 2(\u00b52 \u2212\u00b5)] = [\u2212 \u221a 2 2 e\u0302, \u221a 2 2 e\u0302], and\nSB = BB T = e\u0302e\u0302T .\nNote that rank(SB) = c\u2212 1 = 1. In terms of Theorem 1, as Y can be chosen as any random vector, we pick\nY = [0, \u03b1, 0, . . . , 0]T \u2208 Rd,\nwhere \u03b1 is any positive number that satisfies 0 < \u03b1 < 1. Then, SBY = 0, W = S \u2020 TSBY = 0, and\nSBW = 0,\nwhich does not satisfy the criterion (1.2).\nAs a result, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient,\nand some discriminant information is lost. In this paper, we revisit the fast implementation of null linear\ndiscriminant analysis method and consider how to choose the random matrix properly, such that the two\ncriteria (1.1) and (1.2) of the null LDA method are satisfied theoretically. We give a necessary and sufficient\ncondition to guarantee that the orientation matrix W from (1.4) is of full column rank. Moreover, the\ngeometric characterization of this condition is also investigated."}, {"heading": "2 The main result", "text": "Since the n training vectors {x}ni=1 are linear independent, and the orientation matrix W is required to be of full column rank in the null LDA method, in this paper, we focus on how to choose Y \u2208 Rd\u00d7(c\u22121) (of rank c\u2212 1) in (1.4), such that rank(W ) = c\u2212 1. We follow the notations used in (Sharma et al., 2012).\nLet\nST = U\u03a3 2UT = [U1, U2]\n[\n\u03a321 0\n0 0\n][\nUT1 UT2\n]\nbe the eigenvalue decomposition of ST , where U1 \u2208 Rd\u00d7(n\u22121) corresponds to the range of ST , U2 \u2208 R\nd\u00d7(d\u2212n+1) corresponds to the null space of ST , and \u03a31 \u2208 R(n\u22121)\u00d7(n\u22121) is a diagonal matrix with positive diagonal elements. From now on, we denote G = S\u2020TSB for notation simplicity. By Lemma A3 of (Sharma et al., 2012), we have that\nG = S\u2020TSB = U\n[\n\u03a3\u221221 0\n0 0\n]\nUTSBUU T = U\n[\n\u03a3\u221221 U T 1 SBU1 0\n0 0\n]\nUT ,\nand\nGU = U\n[\n\u03a3\u221221 U T 1 SBU1 0\n0 0\n]\n.\nRecall that SB = BB T , thus\nGU = U\n[\n\u03a3\u221211 \u03a3 \u22121 1 U T 1 BB TU1\u03a3 \u22121 1 \u03a31 0\n0 0\n]\n.\nLet Q = \u03a3\u221211 U T 1 B, and QQ T = R\u039bRT be the eigenvalue decomposition, where R \u2208 R(n\u22121)\u00d7(n\u22121) is orthonormal and \u039b \u2208 R(n\u22121)\u00d7(n\u22121) is diagonal. So we arrive at\nG[U1, U2] = [U1, U2]\n[\n\u03a3\u221211 R\u039bR T\u03a31 0\n0 0\n]\n. (2.5)\nThat is,\nGU1 = U1\u03a3 \u22121 1 R\u039bR T\u03a31, (2.6)\nand\nGU2 = 0. (2.7)\nMoreover, it was proven in Lemma A2 of (Sharma et al., 2012) that \u039b =\n[\nIc\u22121 0\n0 0\n]\n, where Ic\u22121 is the\n(c\u2212 1)\u00d7 (c\u2212 1) identity matrix. It follows from (2.6) that\nGU1\u03a3 \u22121 1 R = U1\u03a3 \u22121 1 R\u039b = U1\u03a3 \u22121 1 R\n[\nIc\u22121 0\n0 0\n]\n.\nNotice that span{U1\u03a3\u221211 R} = span{U1}. Decompose U1\u03a3\u221211 R = [U\u03021, U\u03022], where U\u03021 \u2208 Rd\u00d7(c\u22121) is the matrix composed of the first c\u2212 1 columns of U1\u03a3\u221211 R, and U\u03022 \u2208 Rd\u00d7(n\u2212c), then\nG[U\u03021, U\u03022] = [U\u03021, U\u03022]\n[\nIc\u22121 0\n0 0\n]\n= [U\u03021, 0],\ni.e.,\nGU\u03021 = U\u03021 and GU\u03022 = 0. (2.8)\nRemark 2. Denote U = [U1\u03a3\u221211 R, U2] = [U\u03021, U\u03022, U2] \u2208 Rd\u00d7d, it is seen that the columns of U construct a basis in Rd, moreover, we have that\nrank ( [U\u03022, U2] ) = d\u2212 c+ 1 \u226b 1.\nTherefore, if the d\u00d7 (c\u2212 1) matrix Y \u2208 span{U\u03022, U2}, then it follows from (2.7) and (2.8) that W = GY = S\u2020TSBY = 0, SBW = 0, and Theorem 1 fails to hold.\nNext, we aim to give a necessary and sufficient condition for rank(W ) = c \u2212 1. As the columns of U = [U\u03021, U\u03022, U2] construct a basis of Rd, for any matrix Y \u2208 Rd\u00d7(c\u22121), there exists a matrix [Z\u0302T1 , Z\u0302T2 , ZT2 ]T \u2208 R d\u00d7(c\u22121), such that\nY = [U\u03021, U\u03022, U2]\n\n \nZ\u03021 Z\u03022 Z2\n\n  = U\u03021Z\u03021 + U\u03022Z\u03022 + U2Z2. (2.9)\nThus,\n[Z\u0302T1 , Z\u0302 T 2 , Z T 2 ] T = U\u22121Y,\nand Z\u03021 = (U\u22121Y )(1 : c\u2212 1, :) \u2208 R(c\u22121)\u00d7(c\u22121) is the first c\u2212 1 rows of U\u22121Y . Here (U\u22121Y )(1 : c\u2212 1, :) stands for the first c\u2212 1 rows of the matrix U\u22121Y .\nFrom (1.4), (2.7), (2.8) and (2.9), we obtain\nW = S\u2020TSBY = S \u2020 TSB[U\u03021, U\u03022, U2]\n\n \nZ\u03021 Z\u03022 Z2\n\n  = S\u2020TSBU\u03021Z\u03021\n= GU\u03021Z\u03021 = U\u03021Z\u03021. (2.10)\nSince U\u03021 is of full column rank, we have from (2.10) that rank(W ) = c\u2212 1 if and only if rank(Z\u03021) = c\u2212 1, i.e., Z\u03021 is nonsingular.\nWe are in a position to consider how to evaluate Z\u03021 in practice. Recall that U = [U\u03021, U\u03022, U2] = [U1(\u03a3 \u22121 1 R), U2]. Let \u03a3 \u22121 1 R = Q\u0302R\u0302 be the QR decomposition, where Q\u0302 \u2208 R(n\u22121)\u00d7(n\u22121) is an orthogonal matrix and R\u0302 \u2208 R(n\u22121)\u00d7(n\u22121) is an upper triangular matrix, then\nU = [U1\u03a3\u221211 R, U2] = [U1Q\u0302R\u0302, U2] = [U1Q\u0302, U2] [ R\u0302 0\n0 Id\u2212n+1\n]\n,\nis the QR decomposition of U , where [U1Q\u0302, U2] is orthonormal and In\u2212d+1 is the (n\u2212 d+ 1)\u00d7 (n\u2212 d+ 1) identity matrix. Thus,\nU\u22121Y = [ R\u0302\u22121 0\n0 In\u2212d+1\n][\nQ\u0302TUT1 UT2\n]\nY\n=\n[\nR\u0302\u22121 0\n0 In\u2212d+1\n][\nQ\u0302TUT1 Y\nUT2 Y\n]\n=\n[\nR\u0302\u22121Q\u0302TUT1 Y\nUT2 Y\n]\n. (2.11)\nLet R\u0302\u22121 =\n[\nR\u0302T1 R\u0302T2\n]\n, where R\u0302T1 \u2208 R(c\u22121)\u00d7(n\u22121) is composed of the first c \u2212 1 rows of R\u0302\u22121, and R\u0302T2 \u2208\nR (n\u2212c)\u00d7(n\u22121) is composed of the last n\u2212 c rows of R\u0302\u22121. So we obtain from (2.11) that\nZ\u03021 = (U\u22121Y )(1 : c\u2212 1, :) = (R\u0302\u22121Q\u0302TUT1 Y )(1 : c\u2212 1, :) = ( U1Q\u0302R\u03021 )T Y. (2.12)\nFurthermore, if rank(Z\u03021) = c\u22121, then we have from (2.10) that W = S\u2020TSBY is of rank c\u22121. According to Lemma A3 of (Sharma et al., 2012), we have\nS\u2020TSBW = (S \u2020 TSB)(S \u2020 TSB)Y = S \u2020 TSBY = W,\nand it follows from Theorem 1 and Theorem 2 of (Sharma et al., 2012) that W satisfies the null LDA criteria\n(1.1) and (1.2).\nIn summary, we have the main theorem that is a modification to Theorem 1 [Theorem 3 in (Sharma et\nal., 2012)].\nTheorem 2. Let Y \u2208 Rd\u00d7(c\u22121) be a random matrix of rank c\u2212 1, and let\nZ\u03021 = ( U1Q\u0302R\u03021 )T Y (13)\nbe the (c\u2212 1)\u00d7 (c\u2212 1) matrix composed of the first c\u2212 1 rows of U\u22121Y . Then W = S\u2020TSBY is of rank c\u2212 1 if and only if Z\u03021 is nonsingular. Moreover, if Z\u03021 is nonsigular, then W = S \u2020 TSBY satisfies the criteria of the null LDA method ( Eqs. (1.1) and (1.2) ) .\nNotice that U1Q\u0302R\u03021 is of full rank. Given a random matrix Y \u2208 Rd\u00d7(c\u22121), the following theorem describes the geometric characterization of the condition for Z\u03021 being nonsingular.\nTheorem 3. Suppose that Y \u2208 Rd\u00d7(c\u22121) is of full column rank, and denote by span{Y } the subspace spanned by the columns of Y . Let K = span{Y } and L = span{U1Q\u0302R\u03021}, then Z\u03021 is nonsingular if and only if any nonzero vector x \u2208 K (or y \u2208 L), it is not orthogonal to L (or K).\nProof. The proof is by contradiction. On one hand, suppose that there is a nonzero vector x \u2208 K and x \u22a5 L. Then there exists a nonzero vector z \u2208 Rc\u22121, such that x = Y z. Since x \u22a5 L, we obtain\n0 = (U1Q\u0302R\u03021) Tx =\n( U1Q\u0302R\u03021 )T Y z = Z\u03021z,\nand Z\u03021 is singular. This shows that, if Z\u03021 is nonsingular, then for any nonzero vector x \u2208 K, it is not orthogonal to L. On the other hand, we assume that Z\u03021 is singular. Then there is a nonzero vector z \u2208 Rc\u22121, such that Z\u03021z = ( U1Q\u0302R\u03021 )T\nY z = 0. Let x \u2261 Y z \u2208 K, then x 6= 0, and it is orthogonal to L. This implies that, if for any nonzero vector x \u2208 K, it is not orthogonal to L, then Z\u03021 is nonsingular.\nRemark 3. Given a random matrix Y , Theorem 2 can be utilized to check whether W is of full rank a prior\nin the fast implementation of the null LDA method (Sharma et al., 2012). Indeed, it indicates that W is of rank c \u2212 1 if and only if Z\u03021 = ( U1Q\u0302R\u03021 )T Y is nonsingular. Equivalently, Theorem 3 shows that this only happens if and only if for any nonzero vector x in K = span{Y }, it is not orthogonal to L = span{U1Q\u0302R\u03021}, moreover, for any nonzero vector y in L = span{U1Q\u0302R\u03021}, it is not orthogonal to K = span{Y }.\nIn practice, however, the case of a \u201cnear singular\u201d (i.e., the smallest eigenvalue is not zero but is close\nto zero) Z\u03021 can occur if Y is chosen arbitrarily (Golub et al., 2013). Consequently, W will be near rank deficient and the two criteria of the null LDA method can not be satisfied any more. In this situation, we\nsuggest using another random matrix Y instead."}, {"heading": "Acknowledgments", "text": "The first author is supported by the Postgraduate Innovation Project of Jiangsu Province under grant\nCXLX13 968. The second author is supported by the National Science Foundation of China under grant\n11371176, the Natural Science Foundation of Jiangsu Province under grant BK20131126, the 333 Project\nof Jiangsu Province, and the Talent Introduction Program of China University of Mining and Technology.\nMeanwhile, the authors would like to thank Ting-ting Xu for helpful discussions."}], "references": [{"title": "Integration of ligand and structure-based virtual screening for identification of leading anabolic steroids, The Journal of steroid biochemistry and molecular biology", "author": ["Y. Alvarez-Ginarte", "L Montero-Cabrera"], "venue": "Pattern Recognition,", "citeRegEx": "Alvarez.Ginarte and Montero.Cabrera,? \\Q2013\\E", "shortCiteRegEx": "Alvarez.Ginarte and Montero.Cabrera", "year": 2013}, {"title": "Complexity-reduced implementations of complete and null-space-based linear discriminant analysis", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Matrix Computations,", "citeRegEx": "Golub and Loan,? \\Q2013\\E", "shortCiteRegEx": "Golub and Loan", "year": 2013}, {"title": "Protein fold recognition by alignment of amino acid residues using kernelized dynamic time warping", "author": ["J. Lyons", "N. Biswas", "A Sharma"], "venue": "Journal of theoretical biology,", "citeRegEx": "Lyons et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lyons et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "In recent years, this method has gained wide attentions in the area of dimensionality reduction and data mining (Alvarez-Ginarte et al., 2013; Lu et al., 2013; Lyons et al., 2014; Sharma et al., 2014).", "startOffset": 112, "endOffset": 200}], "year": 2014, "abstractText": "The null linear discriminant analysis method is a competitive approach for dimensionality reduction. The implementation of this method, however, is computationally expensive. Recently, a fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices was proposed. However, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient, and some useful discriminant information will be lost. In this paper, we investigate how to choose the random matrix properly, such that the two criteria of the null LDA method are satisfied theoretically. We give a necessary and sufficient condition to guarantee full column rank of the orientation matrix. Moreover, the geometric characterization of the condition is also described.", "creator": "LaTeX with hyperref package"}}}