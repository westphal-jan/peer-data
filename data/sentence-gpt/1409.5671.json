{"id": "1409.5671", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion Systems", "abstract": "We propose a technique to detect and generate patterns in a network of locally interacting dynamical systems. Central to our approach is a novel spatial superposition logic, whose semantics is defined over the quad-tree of a partitioned image. We show that formulas in this logic can be efficiently learned from positive and negative examples of several types of patterns. We also demonstrate that pattern detection, which is implemented as a model checking algorithm, performs very well for test data sets different from the learning sets. We define a quantitative semantics for the logic and integrate the model checking algorithm with particle swarm optimization in a computational framework for synthesis of parameters leading to desired patterns in reaction-diffusion systems.\n\n\n\n\n\nIn this work, the centrality and the importance of the concept of \"synthesis\" is revealed.", "histories": [["v1", "Fri, 12 Sep 2014 05:21:06 GMT  (666kb,D)", "http://arxiv.org/abs/1409.5671v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CE cs.LG cs.LO cs.SY", "authors": ["ebru aydin gol", "ezio bartocci", "calin belta"], "accepted": false, "id": "1409.5671"}, "pdf": {"name": "1409.5671.pdf", "metadata": {"source": "CRF", "title": "A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion Systems", "authors": ["Ebru Aydin Gol", "Ezio Bartocci", "Calin Belta"], "emails": ["(ebru@bu.edu)", "(cbelta@bu.edu)", "(ezio.bartocci@tuwien.ac.at)"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nFrom the stripes of a zebra and the spots on a leopard to the filaments (Anabaena) [1], spirals, squares (Thiopedia rosea), and vortex (Paenibacillus) [2] formed by singlecell organisms, patterns can be found everywhere in nature. Pattern formation is at the very origin of morphogenesis and developmental biology, and it is at the core of technologies such as self-assembly, tissue engineering, and amorphous computing. Even though it received a lot of attention from diverse communities such as biology, computer science, and physics, the problem of pattern formation is still not well understood.\nPattern recognition is usually formulated as a machine learning problem [3], in which patterns are characterized either statistically [4] or through a structural relationship among their features [5]. Despite its success in several application areas [6], pattern recognition still lacks a formal foundation. Can patterns be specified in a formal language with well-defined syntax and semantics? Can we develop algorithms for pattern detection from specification given in such a language? Given a large collection of locally interacting agents, can we design parameter synthesis rules, control and interaction strategies guaranteeing the emergence of global patterns? In this paper, by drawing inspiration from model checking [7], [8], we provide partial answers to these questions.\nWe address the following problem: Given a network of locally interacting dynamical systems, and given sets of positive and negative examples of a desired pattern, find parameter values that guarantee the occurrence of the pattern in the network at steady state. Our approach is based on a novel spatial superposition logic, called Tree Spatial Superposition Logic (TSSL), whose semantics is defined over quad-trees of\nEbru Aydin Gol (ebru@bu.edu) and Calin Belta (cbelta@bu.edu) are with Boston University. Ezio Bartocci (ezio.bartocci@tuwien.ac.at) is with Vienna University of Technology.\npartitioned images. The decision of whether a pattern exists in an image becomes a model checking problem. A pattern descriptor is a TSSL formula, and we employ machinelearning techniques to infer such a formula from the given positive and negative examples of the pattern. To synthesize parameters of the original networked system leading to a desired pattern, we use a particle swarm optimization (PSO) algorithm. The optimization fitness function is given by a measure of satisfaction induced by the quantitative semantics that we introduce for the logic. We present examples showing that formulas in the proposed logic are good classifiers for some commonly encountered patterns. While the overall algorithm can, in principle, be applied to any network of locally interacting systems, in this paper we focus on the Turing reaction-diffusion system [9], and show that patternproducing parameters can be automatically generated with our method.\nThe rest of the paper is organized as follows. In Section II we discuss the work. In Section III we formulate the problem and outline our approach. We define the syntax and semantics of TSSL in Section IV. A machine learning technique to learn TSSL formulas from positive and negative examples of desired patterns is developed in Section V. The solution to the pattern generation problem is presented in Section VI as a supervised, iterative procedure that integrates quantitative model checking and optimization. We conclude with final remarks and directions for future work in Section VII."}, {"heading": "II. RELATED WORK", "text": "Pattern recognition is a well-established technique in machine learning. Given a data set and a set of classes, the goal is to assign each data to one class, or to provide a \u201cmost likely\u201d matching of the data to the classes. The two main steps in pattern recognition are: (a) to extract distinctive features [10], [11], [12], [13] with relevant information from a set of input data representing the pattern of interest and (b) to build, using one of the several available machine learning techniques (see [14] for a detailed survey), an accurate classifier trained with the extracted features. The descriptor chosen in feature extraction phase depends on the application domain and the specific problem.\nThis work is related to pattern recognition in computer vision, where these descriptors may assume different forms. Feature descriptors such as Textons [10] and Histograms of Oriented Gradients (HoG) [11] are concerned with statistical information of color distribution or of intensity gradients and edge directions. The scale-invariant feature transform (SIFT), proposed by Lowe in [13], is based on the appearance\nar X\niv :1\n40 9.\n56 71\nv1 [\ncs .A\nI] 1\n2 Se\np 20\n14\nof an object at particular interest points, and is invariant to image scale and rotation. The shape context [12] is another feature descriptor intended to describe the shape of an object by the points of its contours and the surrounding context.\nIn this paper we establish an interesting connection between verification and pattern recognition. Both classical verification [15], [16], [17], [18], [19] and pattern recognition techniques aim to verify (and possibly quantify) the emergence of a behavioral pattern. We propose logic formulas as pattern descriptors and verification techniques as pattern classifiers. The logical nature of such pattern descriptors allows to reason about the patterns and to infer interesting properties. For example, in [20], the spatial modalities are used to characterize self-similar (fractal) texture. Furthermore, combining different pattern descriptors using both modal and logical operators is quite intuitive.\nThis paper is inspired by the original work on morphogenesis by Alan Turing [9], and is closely related to [20]. In the latter, the authors introduced a Linear Spatial Superposition Logic (LSSL), whose formulas were interpreted, as in this paper, over quad-tree partitions. The existence of a pattern in an image corresponded to the existence of a path in the corresponding tree from the root to the leaf corresponding to a representative point in the image. As a consequence, the method was shown to work for spirals, for which the center was chosen as the representative point. The tree logic proposed here is more general as it does not depend on the choice of such a point and captures the pattern \u201cglobally\u201d. For example, the patterns considered in this paper cannot be expressed in LSSL, because they rely on a tree representation rather than a path representation.\nAs opposed to [20], we also define a quantitative semantics for the logic, and use the distance to satisfaction as a fitness function while searching for pattern-producing parameters. This quantitative semantics and the discounted model checking on a computational tree are inspired from [21], with the notable difference that we do not need a metric distance, but rather a measure of satisfiability. Such measures have also been used in [15], [16], [17], [18], [19]. The main novelty of this paper, compared to the other pattern recognition approaches, is that we can quantify \u201chow far\u201d a system is from producing a desired pattern. This, which is possible due to the quantitative semantics of our logic, enables the use of optimization algorithms to search for pattern-producing parameters."}, {"heading": "III. PROBLEM FORMULATION", "text": "Notation. We use R, R+, N and N+ to denote the set of real numbers, non-negative reals, integer numbers, and nonnegative integers, respectively. For any c \u2208R and set S \u2286R, S>c := {x \u2208 S | x > c}, and for any a,b \u2208R, S[a,b] := {x \u2208 S | a\u2264 x\u2264 b}.\nA reaction-diffusion system S is modeled as a spatially distributed and locally interacting K\u00d7K rectangular grid of identical systems, where each location (i, j) \u2208N[1,K]\u00d7N[1,K]\ncorresponds to a system:\nSi, j : dx(n)i, j\ndt = Dn(u\n(n) i, j \u2212 x (n) i, j )+ fn(xi, j,R), n = 1, . . . ,N,\n(III.1) where xi, j = [x (1) i, j , . . . ,x (N) i, j ] is the state vector of system Si, j, which captures the concentrations of all species of interest. D and R are the parameters of system S. D = [D1, . . . ,DN ]\u2208 RN+ is the vector of diffusion coefficients. R \u2208 RP\u2212N is the vector of parameters that defines the local dynamics fn :RN+\u00d7 RP\u2212N \u2192 R for each of the species n = 1, . . . ,N. Note that the parameters and dynamics are the same for all systems Si, j,(i, j)\u2208N[1,K]\u00d7N[1,K]. The diffusion coefficient is strictly positive for diffusible species and it is 0 for non-diffusible species. Finally, ui, j = [u (1) i, j , . . . ,u (N) i, j ] is the input of system Si, j from the neighboring systems:\nu(n)i, j = 1 |\u03bdi, j| \u2211v\u2208\u03bdi, j x(n)v ,\nwhere \u03bdi, j denotes the set of indices of systems adjacent to Si, j.\nGiven a parameter vector p = [D,R] \u2208 RP, we use S(p) to denote an instantiation of a reaction-diffusion system. We use x(t)\u2208RK\u00d7K\u00d7N+ to denote the state of system S(p) at time t, and xi, j(t)\u2208RN+ to denote the state of system S (p) i, j at time t. While the model captures the dynamics of concentrations of all species of interest, we assume that a subset {n1, . . . ,no}\u2286 {1, . . . ,N} of the species is observable through:\nH : RK\u00d7K\u00d7N+ \u2192 RK\u00d7K\u00d7o[0,b] : y = H(x),\nfor some b \u2208 R+. For example, a subset of the genes in a gene network are tagged with fluorescent reporters. The relative concentrations of the corresponding proteins can be inferred by using fluorescence microscopy.\nWe are interested in analyzing the observations generated by system (III.1) in steady state. Therefore, we focus on parameters that generate steady state behavior, which can be easily checked through a running average:\nK\n\u2211 i=1\nK\n\u2211 j=1\nN\n\u2211 n=1 | x(n)i, j (t)\u2212 x (n) i, j |< \u03b5, (III.2)\nwhere x(n)i, j = \u222b T t\u2212T x (n) i, j (\u03c4)d\u03c4/T for some T \u2264 t. The system is said to be in steady state at time t\u0304, if (III.2) holds for all t \u2265 t\u0304. In the rest of the paper, we will simply call the observation of a trajectory at steady state as the observation of the trajectory, and denote it as H(x(t\u0304)).\nExample 3.1: We consider a 32\u00d7 32 reaction-diffusion system with two species (i.e. K = 32, N = 2):\ndx(1)i, j dt\n= D1 ( u(1)i, j \u2212 x (1) i, j ) +R1x (1) i, j x (2) i, j \u2212 x (1) i, j +R2,\ndx(2)i, j dt\n= D2 ( u(2)i, j \u2212 x (2) i, j ) +R3x (1) i, j x (2) i, j +R4. (III.3)\nThe system is inspired from Turing\u2019s reaction-diffusion system and is presented in [22] as a model of the skin pigments\nof an animal. At a cell (location (i, j)), the concentration of species 1, x(1)i, j , depends on the concentration of species 1 in this cell and in its neighbors (if D1 > 0), and the concentration of species 2 in this cell only, i.e. x(2)i, j . Similarly, x(2)i, j depends on the concentration of species 2 in this cell and in its neighbors (if D2 > 0), and x (1) i, j (if R3 6= 0). We assume that species 1 is observable through mapping H : R32\u00d732\u00d72+ \u2192 R32\u00d732[0,1] given by:\ny = H(x), where yi, j = x(1)i, j\nmaxm,n x (1) m,n\n.\nWe simulate the system from random initial conditions with parameters R = [1,\u221212,\u22121,16], and different diffusion parameters D1 = [5.6,24.5], D2 = [0.2,20], and D3 = [1.4,5.3]. The observed concentrations of species 1 at different time points are shown in Figure 1. At time t = 50, all trajectories are in steady state. Note that, in all three cases, the spatial distribution of the steady state concentrations of species 1 has some regularity, i.e. it forms a \u201cpattern\u201d. We will use large spots (LS), fine patches (FP), and small spots (SS) to refer to the patterns corresponding to D1, D2, and D3, respectively.\nProblem 3.1: Given a reaction-diffusion system S as defined in (III.1), a finite set of initial conditions X0\u2282RK\u00d7K\u00d7N , ranges of the design parameters P =P1\u00d7 . . .\u00d7PP, Pi\u2282R, i= 1, . . . ,P, a set of observations Y+ = {yi}i=1,...,N+ that contain a desired pattern, a set of observations Y\u2212 = {yi}i=1,...,N\u2212 that do not contain the pattern, find parameters p\u2217 \u2208 P such that the trajectories of system S(p\u2217) originating from X0 are guaranteed to produce observations similar to the ones from the set Y+. To solve Problem 3.1, we need to perform two steps:\n\u2022 Design a mechanism that decides whether an observation contains a pattern. \u2022 Develop a search algorithm over the state space of the design parameters to find p\u2217.\nThe first step requires to define a pattern descriptor. To this goal, we develop a new spatial logic over spatialsuperposition trees obtained from the observations, and treat the decision problem as a model checking problem. The new logic and the superposition trees are explained in Section IV. Then, finding a pattern descriptor reduces to finding a\nformula of the new logic that specifies the desired pattern. We employ machine-learning techniques to learn such a formula from the given sets of observations Y+ and Y\u2212.\nThe second step is the synthesis of parameters p\u2217 such that the observations produced by the corresponding reactiondiffusion system S(p\u2217) satisfy the formula learned in the first step. To this end, we introduce quantitative semantics for the new logic, which assigns a positive valuation only to the superposition-trees that satisfy the formula. This quantitative valuation is treated as a measure of satisfaction, and is used as the fitness function in a particle swarm optimization (PSO) algorithm. The choice of PSO is motivated by its inherent distributed nature, and its ability to operate on irregular search spaces, i.e. it does not require a differentiable fitness function. Finally, we propose a supervised, iterative procedure to find p\u2217 that solves Problem 3.1. The procedure involves iterative applications of steps one and two, and an update of the set Y\u2212 until a parameter set that solves Problem 3.1 is found, which is decided by the user."}, {"heading": "IV. TREE SPATIAL SUPERPOSITION LOGIC", "text": ""}, {"heading": "A. Quad-tree spatial representation", "text": "We represent the observations of a reaction-diffusion system as a matrix Ak,k of 2k\u00d72k elements ai, j with k \u2208 N>0. Each element corresponds to a small region in the space and is defined as a tuple ai, j = \u3008a (1) i, j , \u00b7 \u00b7 \u00b7 ,a (o) i, j \u3009 of values representing the concentration of the observable species within an interval a(c)i, j \u2208 [0,b], with b \u2208 R+. Given a matrix Ak,k, we use Ak,k[is, ie; js, je] to denote the sub-matrix formed by selecting the rows with indices from is to ie and the columns with indices from js to je.\nDefinition 4.1: A quad-tree Q = (V,R) is a quaternary tree [23] representation of Ak,k where each vertex v \u2208 V represents a sub-matrix of Ak,k and the relation R \u2282 V \u00d7V defines the four children of each node v that is not a leaf. A vertex v is a leaf when all the elements of the sub-matrix that it represents have the same values.\nFigure 2 shows an example of a quadtree, where node v0 represents the entire matrix; child v1 represents the submatrix {1, \u00b7 \u00b7 \u00b7 ,2k\u22121}\u00d7{1, \u00b7 \u00b7 \u00b7 ,2k\u22121}; child v7 represents the sub-matrix {2k\u22122 + 1, \u00b7 \u00b7 \u00b7 ,2k\u22121}\u00d7{2k\u22122 + 1, \u00b7 \u00b7 \u00b7 ,2k\u22121}; etc. In Figure 2, we also label each edge in the quad-tree with the direction of the sub-matrix represented by the child: north west (NW), north east (NE), south west (SW), south east (SE).\nDefinition 4.2: We define the mean function \u00b5c : V \u2192 [0,b] for sub-matrix Ak,k[is, ie; js, je] represented by the vertex v \u2208V of the quad-tree Q = (V,R) as follows:\n\u00b5c(v) = 1\n(ie\u2212 is +1)( je\u2212 js +1) \u2211i, j\u2208{is,\u00b7\u00b7\u00b7 ,ie}\u00d7{ js,\u00b7\u00b7\u00b7 , je} a(c)i, j\nThe function \u00b5c provides the expected value for an observable variable with index c,1 \u2264 c \u2264 o in a particular region of the space represented by the vertex v.\nDefinition 4.3: Two vertices va,vb \u2208 V are said to be equivalent when the mean function applied to the elements of the sub-matrices that they represent produce the same values:\nva \u2261 vb\u21d0\u21d2 \u00b5c(va) = \u00b5c(vb),\u2200c,1\u2264 c\u2264 o We use the mean of the concentration of the observable species as a spatial abstraction (superposition) of the observations in a particular region of the system, avoiding in this way to enumerate the observations of all locations. This approach is inspired by previous papers [20], [24], where the authors aim to combat the state-explosion problem that would stem otherwise.\nProposition 4.1: Given a vertex v\u2208V of a quad-tree Q = (V,R) and its four children vNE ,vNW ,vSE ,vSW the following property holds:\n\u00b5c(v) = \u00b5c(vNE)+\u00b5c(vNW )+\u00b5c(vSE)+\u00b5c(vSW )\n4 Proof: The proof can be easily derived by expanding the terms of Definition 4.2. Proposition 4.2: The number of vertices needed for the quad-tree representation Q = (V,R) of a matrix Ak,k is upper bounded by \u2211ki=0 22i.\nProof: The proof follows from the fact that the worst case scenario is when all the elements have different values. In this case the cardinality of the set V is equal to the cardinality of a full and complete quaternary tree. For example, to represent the matrix A3,3, it would require a max number of vertices |V | \u2264 1+4+16+64 = 85."}, {"heading": "B. Quad Transition System", "text": "We now introduce the notion of quad transition system that extends the classical quad-tree structure, allowing for a more compact exploration for model checking.\nDefinition 4.4: A Quad Transition System (QTS) is a tuple QT S = (S,s\u03b9,\u03c4,\u03a3, [.],L), where:\n1) S is a finite set of states with s\u03b9 \u2208 S the initial state;\nAlgorithm BUILDINGQUADTRANSITIONSYSTEM\nInput: Matrix Ak,k of 2k\u00d72k of elements ai, j = \u3008a(1)i, j , \u00b7 \u00b7 \u00b7 ,a (o) i, j \u3009,\nits quad-tree Q=(V,R), the root v0 \u2208V , and a labeling function LQ : R\u2192D = {NW,NE,SE,SW}\nOutput: Quad Transition System QT S = (S,s\u03b9,\u03c4,\u03a3, [.],L)\n1: \u03a3 := {m1, \u00b7 \u00b7 \u00b7 ,mo} . Initialize the set of variables \u03a3 of QT S . 2: \u03c4 = /0 . Initialize the set \u03c4 of the transition relation \u03c4 of QT S . 3: S := {s\u03b9} . Initialize the set of states S of QT S . 4: T S := {\u3008s\u03b9,{v0}\u3009} . Each tuple in TS contains a state in S and a set of vertices in V. 5: LF := {v \u2208V | 6 \u2203t \u2208V : (v, t) \u2208 R} . LF is the set of leaves of Q 6: PLF := {Pi \u2286 LF,1\u2264 i\u2264 n | Pi 6= /0\u2227\u2200va,vb \u2208 Pi,\n\u2200vc \u2208 Pj 6=i,va \u2261 vb \u2227 va 6\u2261 vc} . PLF is a partition of LF with equivalent leaves.\n7: for each P\u0302 \u2208 PLF do . For each partition element, create a state s\u2032 with a self-loop and . a transition to the state s\u03b9 if P\u0302 contains a child of v0. 8: add new state s\u2032 to S and a tuple \u3008s\u2032, P\u0302\u3009 to T S 9: \u03c4 := \u03c4\u222a{(s\u2032,s\u2032)}\u222a{(s,s\u2032) : \u3008s,V S\u3009 \u2208 T S,\n\u2203v \u2208V S,\u2203v\u2032 \u2208 P\u0302 : (v,v\u2032) \u2208 R} 10: end for 11: FS := {v \u2208V |(v0,v) \u2208 R}\\LF . explore the children of v0 that are not leaves. 12: while FS 6= /0 do . FS contains the frontier vertices to be explored. 13: LFS := {v \u2208 FS | \u2200v\u2032 \u2208V : (v,v\u2032) \u2208 R : \u2203\u3008s,V S\u3009 \u2208 T S\u2227 v\u2032 \u2208V S} 14: PLFS := {Pi\u2208I \u2286 LFS | I 6= /0,Pi 6= /0,\u2200va,vb \u2208 Pi, \u2200vc \u2208 Pj 6=i,va \u2261 vb \u2227 va 6\u2261 vc} 15: for each P\u0302 \u2208 PLFS do 16: add new state s\u2032 to S and a tuple \u3008s\u2032, P\u0302\u3009 to T S 17: \u03c4 := (\u22c3s:\u3008s,V S\u3009\u2208T S:\u2203v\u2208P\u0302,\u2203v\u2032\u2208V S,(v,v\u2032)\u2208R(s\u2032,s))\u222a \u03c4 18: if \u2203v \u2208 P\u0302\u2227\u2203\u3008s,V S\u3009 : \u2203v\u2032 \u2208V S\u2227 (v\u2032,v) \u2208 R then 19: \u03c4 := \u03c4\u222a{(s,s\u2032)} 20: end if 21: end for 22: for each v\u0302 \u2208 FS\\LFS do 23: add new state s\u2032 to S and a tuple \u3008s\u2032,{v\u0302}\u3009 to TS 24: \u03c4 := (\u22c3s:\u3008s,V S\u3009\u2208T S:\u2203v\u2032\u2208V S,(v\u0302,v\u2032)\u2208R(s\u2032,s))\u222a \u03c4 25: if \u2203\u3008s,V S\u3009 : \u2203v\u2032 \u2208V S\u2227 (v\u2032, v\u0302) \u2208 R then 26: \u03c4 := \u03c4\u222a{(s,s\u2032)} 27: end if 28: end for 29: FS := {v \u2208V | \u2203v\u0304 \u2208 FS,(v\u0304,v) \u2208 R}\\LF 30: end while 31: define func [.] as [c\u0304](s\u0304) := \u00b5c\u0304(vs\u0304), c\u0304 \u2208 {1, \u00b7 \u00b7 \u00b7 ,o}, vs\u0304 \u2208V S : \u3008s\u0304,V S\u3009 \u2208 T S 32: define func L as L(s, t) := (t = s)?D :\u22c3\nv\u0303\u2208V\u0303 S,v\u0304\u2208 \u00afV T :\u3008s,V\u0303 S\u3009,\u3008t, \u00afV T \u3009\u2208T S,(v\u0303,v\u0304)\u2208R LQ(v\u0303, v\u0304)\n33: return S,s\u03b9,\u03c4,\u03a3, [.],L\n2) \u03c4\u2286 S\u00d7S is the transition relation. We require \u03c4 to be non-blocking and bounded-branching: \u2200s \u2208 S,\u2203t \u2208 S : (s, t) \u2208 \u03c4 and \u2200s \u2208 S, if T (s) = {t : (s, t) \u2208 \u03c4} is the set of all successors of s, the cardinality of |T (s)| \u2264 4;\n3) \u03a3 is a finite set of variables; 4) [.] is a function [.] : S\u2192 (\u03a3\u2192 [0,b]) that assigns to\neach state s \u2208 S and a variable m \u2208 \u03a3 a rational value [s](m) in [0,b] with b \u2208 R+; 5) L is a labeling function for the transition L : \u03c4\u2192 2D with D = {NW,NE,SE,SW} and with the property that \u2200(s, t),(s, t \u2032) \u2208 \u03c4, with t 6= t \u2032 it holds that L(s, t)\u2229 L(s, t \u2032) = /0, \u22c3 \u2200t\u2208S:(s,t)\u2208\u03c4 L(s, t) = D .\nThe BUILDINGQUADTRANSITIONSYSTEM algorithm shows how to generate a QTS starting from a quad-tree representation Q=(V,R) of a a matrix Ak,k and a labeling function LQ : R\u2192D .\nProposition 4.3: A quad transition system (QTS) QT S = (S,s\u03b9,\u03c4,\u03a3, [.],L) generated by the BUILDINGQUADTRANSITIONSYSTEM algorithm has always a least fixed point, that is \u2203s \u2208 S : (s,s) \u2208 \u03c4.\nProof: This property holds because the algorithm generates a state with a self-loop transition for each partition of equivalent leaves in the quad-tree.\nDefinition 4.5 (Labeled paths): Given a set B of labels representing the spatial directions, a labeled path (lpath) of a QTS Q is an infinite sequence \u03c0B = s0s1s2 \u00b7 \u00b7 \u00b7 of states such that (si,si+1) \u2208 \u03c4 \u2227 L(si,si+1)\u2229B 6= /0, \u2200i \u2208 N. Given a state s, we denote LPathsB(s) the set of all labeled paths starting in s, and with \u03c0Bi the i-th element of a path \u03c0B \u2208 LPathsB(s). For example, in Figure 3, LPathsB(s\u03b9) = {s\u03b9s1s2s2 \u00b7 \u00b7 \u00b7} if B = {NW,SE}."}, {"heading": "C. TSSL Syntax and Semantics", "text": "Definition 4.6 (TSSL syntax): The syntax of TSSL is defined as follows:\n\u03d5 ::=>|\u22a5|m\u223c d |\u00ac\u03d5 |\u03d51 \u2227\u03d52 | \u2203B\u00a9 \u03d5 |\u2200B\u00a9 \u03d5 |\u2203B \u03d51 Uk \u03d52 |\u2200B \u03d51 Uk \u03d52\nwith \u223c\u2208 {\u2264 ,\u2265}, d \u2208 [0,b], b\u2208R+, k \u2208N>0, B\u2286D : B 6= /0, and m \u2208 \u03a3, with \u03a3 the set of variables.\nFrom this basic syntax one can derive other two temporal operators: the exist eventually operator \u2203BFk, the forall eventually operator \u2200BFk, the exist globally operator \u2203BGk, and the forall globally operator \u2200BGk defined such that:\n\u2203BFk\u03d5 := \u2203B>Uk \u03d5 \u2203BGk \u03d5 := \u00ac\u2200BFk\u00ac\u03d5. \u2200BFk\u03d5 := \u2200B>Uk \u03d5 \u2200BGk \u03d5 := \u00ac\u2203BFk\u00ac\u03d5.\nThe TSSL logic resembles the classic CTL logic [25], with the main difference that the next and until are not temporal, but spatial operators meaning a change of resolution (or zoom in). The set B selects the spatial directions in which the operator is allowed to work and the parameter k limits the until to operate on a finite sequence of states. In the following we provide the TSSL qualitative semantics that, given a spatial model and a formula representing the pattern to detect, provides a yes/no answer.\nDefinition 4.7 (TSSL Qualitative Semantics): Let Q = (S,s\u03b9,\u03c4,\u03a3, [.],L) be a QTS, Then, Q satisfies a TSSL formula \u03d5, written Q |= \u03d5, if and only if Q ,s\u03b9 |= \u03d5, where:\nQ ,s |=> Q ,s |= m\u223c d Q ,s |= \u00ac\u03d5 Q ,s |= \u03d51 \u2227\u03d52 Q ,s |= \u2203B\u00a9 \u03d5 Q ,s |= \u2200B\u00a9 \u03d5 Q ,s |= \u2203B\u03d51 Uk \u03d52\nQ ,s |= \u2200B\u03d51 Uk \u03d52\nand \u21d4 \u21d4 \u21d4 \u21d4 \u21d4 \u21d4\n\u21d4\nQ,s 6|=\u22a5 [s](m)\u223c d Q ,s 6|= \u03d5 Q ,s |= \u03d51 \u2227Q ,s |= \u03d52 \u2203s\u2032 : (s,s\u2032) \u2208 \u03c4\u2227L(s,s\u2032)\u2229B 6= /0\u2227Q ,s\u2032 |= \u03d5 \u2200s\u2032 : (s,s\u2032) \u2208 \u03c4\u2227L(s,s\u2032)\u2229B 6= /0\u2227Q ,s\u2032 |= \u03d5 \u2203\u03c0B \u2208 LPathsB(s) : \u2203i,0 < i\u2264 k :\n(Q,\u03c0Bi |= \u03d52)\u2227 (\u2200 j < i,(Q,\u03c0 j |= \u03d51)) \u2200\u03c0B \u2208 LPathsB(s) : \u2203i,0 < i\u2264 k :\n(Q,\u03c0Bi |= \u03d52)\u2227 (\u2200 j < i,(Q,\u03c0 j |= \u03d51)) Example 4.1: Checkerboard pattern. The checkerboard pattern from Fig 3 a) can be characterized with the following TSSL formula (B\u2217 = {SW,NE,NW,SE}):\n\u2200B\u2217\u00a9 (\u2200B\u2217\u00a9 ((\u2200{SW,NE}\u00a9 (m\u2265 1))\u2227 (\u2200{NW,SE}\u00a9 (m\u2264 0)))).\nThe \u201ceventually\u201d operator can be used to define all the possible checkerboards of different sizes less or equal than 42 as follows:\n\u2200B\u2217F2((\u2200{SW,NE}\u00a9 (m\u2265 1))\u2227 (\u2200{NW,SE}\u00a9 (m\u2264 0))) The qualitative semantics is useful to check if a given spatial model violates or satisfies a pattern expressed in TSSL. However, it does not provide any information about how much the property is violated or satisfied. This information may be useful to guide a simulation-based parameter exploration for pattern generation. For this reason we equip our logic also with a quantitative valuation that provides a measure of satisfiability in the same spirit of [17]. Since the valuation of a TSSL formula with spatial operators requires to traverse and to compare regions of space at different resolution, we apply a discount factor of 14 on the result each time a transition is taken in QTS.\nDefinition 4.8 ( TSSL Quantitative Semantics): Let Q = (S,s\u03b9,\u03c4,\u03a3, [.],L) be a QTS. The quantitative valuation J\u03d5K : S\u2192 [\u2212b,b] of a TSSL formula \u03d5 is defined as follows:\nJ>K(s) = b J\u22a5K(s) =\u2212b\nJm\u223c dK(s) = (\u223c is\u2265) ? ([m](s)\u2212d) : (d\u2212 [m](s)) J\u00ac\u03d5K(s) =\u2212J\u03d5K(s)\nJ\u03d51 \u2227\u03d52K(s) = min(J\u03d51K(s),J\u03d52K(s)) J\u2203B\u00a9 \u03d5K(s) = 1 4 max \u03c0B\u2208LPathsB(s) J\u03d5K(\u03c0B1 ) J\u2200B\u00a9 \u03d5K(s) = 1 4 min \u03c0B\u2208LPathsB(s) J\u03d5K(\u03c0B1 )\nJ\u2203B\u03d51 Uk \u03d52K(s) = sup \u03c0B\u2208LPathsB(s)\n{min( 1 4i J\u03d52K(\u03c0Bi ), inf{ 1 4 j J\u03d51K(\u03c0Bj ) | j < i}) | 0 < i\u2264 k}}\nJ\u2200B\u03d51 Uk \u03d52K(s) = inf \u03c0B\u2208LPathsB(s) {min( 1 4i J\u03d52K(\u03c0Bi ), inf{ 1 4 j J\u03d51K(\u03c0Bj ) | j < i}) | 0 < i\u2264 k}\nTheorem 4.1 (Soundness): Let Q = (S,s\u03b9,\u03c4,\u03a3, [.],L) be a QTS, s \u2208 S a state of Q , and \u03d5 a TSSL formula. Then, the following properties hold for the two semantics:\nJ\u03d5K(s)> 0 =\u21d2 Q ,s |= \u03d5\nJ\u03d5K(s)< 0 =\u21d2 Q ,s 6|= \u03d5 Proof: The proof can be derived by structural induction on the operational semantics. Remark 4.1: Theorem 4.1 provides the basis of the techniques for pattern generation discussed in the following sections. It is worth to note that, in the case J\u03d5K(s) = 0, it is not possible to infer whether Q violates or satisfies a TSSL formula \u03d5 and only in this particular case we need to resort to the qualitative semantics for determining it."}, {"heading": "V. TSSL PATTERN CLASSIFIERS", "text": "A QTS can be seen in the context of multi-resolution representation, since the nodes that appear at deeper levels provide information for higher resolutions. Therefore, a TSSL formula can effectively capture properties of an image. However, it is difficult to write a formula that describes a desired property, such as a pattern. Here, we propose to use machine-learning techniques to find such a formula from given sets of positive (Y+) and negative (Y\u2212) examples.\nWe first define a labeled data set from the given data sets Y+ and Y\u2212 as\nL = {(Qy,+) | y \u2208 Y+}\u222a{(Qy,\u2212) | y \u2208 Y\u2212},\nwhere Qy is the QTS generated from y. Then, we separate the data set L into disjoint training and testing sets LL,LT . In machine-learning, the training set is used to learn a classifier for a target class, e.g. +, and the testing set is used to measure the accuracy of the classifier. We employ RIPPER [26], a rule based learner, to learn a classifier from LL, and then translate the classifier into a TSSL formula characterizing +. Each rule obtained from the learning algorithm is described as\nri : Ci\u21d2\u223ci,\nwhere Ci is a boolean formula over linear predicates over the variables of the states of a QTS, e.g. [s](m)> d, and \u223ci takes values from the label set {+,\u2212}. A linear predicate for a state s \u2208 S can be written as a TSSL formula via the QTS path from the root s\u03b9 to s. Therefore, each Ci can be translated into an equivalent TSSL formula \u03a6i. The classification rules are interpreted as nested if-else statements. Hence, a logically equivalent TSSL formula for the desired property is defined as follows:\n\u03a6+ := \u2228\nj\u2208R+\n( \u03a6 j \u2227\n\u2227 i=1,..., j\u22121 \u00ac\u03a6i\n) , (V.4)\nwhere R+ is the set of indices of rules ri with \u223ci= +, and \u03a6i is the TSSL formula obtained from Ci.\nExample 5.1: LS pattern. For the LS pattern from Example 3.1, we generate a data set Y(1)+ containing 8000 positive examples by simulating the reaction-diffusion system (III.3) from random initial conditions with parameters R and D1. Similarly, to generate the data set Y (1) \u2212 containing 8000 negative examples, we simulate system (III.3) from random initial conditions. However, in this case we use R and randomly choose the diffusion coefficients from R2[0,30]. As stated before, we only consider the observation of a system in steady-state, for this reason, simulated trajectories that\ndo not reach steady state-in 60 time units are discarded. A sample set of images from the sets Y(1)+ and Y (1) \u2212 is shown in Figure 4. We generate a labeled set L(1) of QTS from these sets, and separate L(1) into L(1)L ,L (1) T . We use RIPPER algorithm implemented in Weka [27] to learn a classifier from L(1)L . The learning step took 228.5sec on an iMac with a Intel Core i5 processor at 2.8GHz with 8GB of memory. The classifier consists of 24 rules. The first rule is\nr1 :(R\u2265 0.59)\u2227 (R\u2264 0.70)\u2227 (R.NW.NW.NW.SE \u2264 0.75)\u2227 (R.NW.NW.NW.NW \u2265 0.45)\u21d2+,\nwhere R denotes the root of a QTS, and the labels of the children are explained in Figure 2. Rule r1 translates to the following TSSL formula:\n\u03a61 :(m\u2265 0.59)\u2227 (m\u2264 0.70)\u2227 (\u2203NW\u00a9\u2203NW\u00a9\u2203NW\u00a9\u2203SE\u00a9m\u2265 0.75)\u2227 (\u2203NW\u00a9\u2203NW\u00a9\u2203NW\u00a9\u2203NW\u00a9m\u2265 0.45).\nWe define the TSSL formula \u03a6(1)+ characterizing the pattern as in (V.4), and model check QTSs from L(1)T (|L (1) T |= 8000) against \u03a6(1)+ , which yields a high prediction accuracy (96.11%) with 311 miss-classified QTSs.\nFP and SS patterns. We follow the above explained steps to generate data sets Y(i)+ ,Y (i) \u2212 , generate labeled data sets L(i)L ,L (i) T , and finally learn formulas \u03a6 (i) + for the FP and SS patterns corresponding to diffusion coefficient vectors Di, i = 2,3 from Example 3.1. Due to the space limitations, we only present the results on the test sets. The model checking of the QTSs from the corresponding test sets yields high prediction accuracies 98.01%, and 93.13% for \u03a6(2)+ , and \u03a6(3)+ , respectively."}, {"heading": "VI. PARAMETER SYNTHESIS FOR PATTERN GENERATION", "text": "In this section we present the solution to Problem 3.1, i.e. a framework to synthesize parameters p \u2208 P of a reactiondiffusion system S (III.1) such that the observations of system S(p) satisfy a given TSSL formula \u03a6. First, we show that the parameters of a reaction-diffusion system that produce trajectories satisfying the TSSL formula can be found by optimizing quantitative model checking results. Second, we include the optimization in a supervised iterative procedure for parameter synthesis.\nWe slightly abuse the terminology and say that a trajectory x(t), t \u2265 0 of system S(p) satisfies \u03a6 if the QTS Q = (S,s\u03b9,\u03c4,\u03a3, [.],L) of the corresponding observation, H(x(t\u0304)), satisfies \u03a6, i.e Q |= \u03a6, or J\u03a6K(s\u03b9)> 0.\nWe first define an induced quantitative valuation of a system S(p) and a set of initial conditions X0 from a TSSL formula \u03a6 as:\nJ\u03a6K(S(p)) = min x0\u2208X0 {J\u03a6K(s\u03b9) | Q = (S,s\u03b9,\u03c4,\u03a3, [.],L) is QTS of H(x(t\u0304)),x(0) = x0} (VI.5) The definition of the induced valuation of a system S(p) implies that all trajectories of S(p) originating from X0 satisfy \u03a6 if J\u03a6K(S(p)) > 0. Therefore, it is sufficient to find p that maximizes (VI.5). It is assumed that the ranges P = P1\u00d7 . . .\u00d7PP of the design parameters are known. Therefore,\nthe parameters maximizing (VI.5) can be found with a greedy search on a quantization of P . However, the computation of J\u03a6K(S(p)) for a given p\u2208 P is expensive, since it requires to perform the following steps for each x0 \u2208 X0: simulating the system S(p) from x0, generating QTS Q of the corresponding observation, and quantitative model checking of Q against \u03a6. Here, we use the particle swarm optimization (PSO) algorithm [28] over P with (VI.5) as the fitness function. The choice of PSO is motivated by its inherent distributed nature, and its ability to operate on irregular search spaces. In particular, PSO does not require a differentiable fitness function.\nExample 6.1: LS pattern. We consider the reactiondiffusion system from Example 3.1 and the TSSL formula \u03a6(1)+ corresponding to the LS pattern from Example 5.1. We assume that the parameters of the local dynamics are known, R = [1,\u221212,\u22121,16], and the diffusion coefficients D1 and D2 are set as the design parameters with P =R2[0,30]. We implement PSO to find p \u2208 P maximizing the induced valuation (VI.5). The PSO computation was distributed on 16 processors at 2.1GHz on a cluster, and the running time was around 18 minutes. The optimized parameters are D1 = 2.25 and D2 = 29.42, and the valuation of the system is 0.0023. A set of observations obtained by simulating S([2.25,29.42]) is shown in Figure 6-(a). Note that, while all the observations have some spatial periodicity indicating the presence of a pattern, they are still different from the desired LS pattern.\nFP and SS patterns. We also apply the PSO algorithm on the same setting explained above to maximize the induced valuation (VI.5) for the TSSL formulas \u03a6(2)+ (FP pattern) and \u03a6(3)+ (SS pattern) from Example 5.1. The optimized parameters are [0.083,11.58] and [1.75,7.75] for \u03a6(2)+ and \u03a6(3)+ , respectively. Sets of observations obtained by simulating systems S([0.083,11.58]) and S([1.75,7.75]) are shown in Figure 5. In contrast with the LS pattern, the observations are similar to the ones from the corresponding data sets i.e. Y(2)+ and Y (3) + .\nRemark 6.1: In this paper, we consider the observations generated from a given set of initial conditions X0. However,\nthe initial condition can be set as a design parameter and optimized in PSO over a given domain RK\u00d7K\u00d7N[a,b] .\nAs seen in Example 6.1, it is possible that simulations of the system corresponding to optimized parameters do not necessarily lead to desired patterns. This should not be unexpected, as the formula reflects the original training set of positive and negative examples, and was not \u201caware\u201d that these new simulations are not good patterns. A natural extension of our method should allow to add the newly obtained simulations to the negative training set, and to reiterate the whole procedure. This approach is summarized in the INTERACTIVEDESIGN algorithm.\nAlgorithm INTERACTIVEDESIGN\nInput: Parametric reaction-diffusion system S, ranges of parameters P , a set of initial states X0, sets of observations Y+ and Y\u2212 Output: Optimized parameters p, the corresponding valuation \u03b3 (no solution if \u03b3 < 0)\n1: while True do 2: \u03a6 = Learning(Y+,Y\u2212) 3: {p,\u03b3}= Optimization(S,X0,\u03a6)\n. \u03b3 is the induced valuation of S(p)\n4: if \u03b3 < 0 then return p,\u03b3 5: end if 6: UserQuery: Show observations of trajectories of S(p) originating from X0. 7: if User approves then return p,\u03b3 8: else 9: Y\u2212 = Y\u2212 \u222a{H(x(t\u0304)) | x(t), t \u2265 0,\nis generated by S(p),x(0) \u2208 X0}. 10: end if 11: end while\nWe start with the user defined sets of observations Y+ and Y\u2212, and learn a TSSL formula \u03a6 from the QTS representations of the observations (Section V). Then, in the optimization step, we find a set of parameters p that maximizes \u03b3 = J\u03a6K(S(p)). If \u03b3 < 0, then we terminate the algorithm as parameters producing observations similar to the ones from the set Y+ with respect to the TSSL formula \u03a6 could not be found. If \u03b3 \u2265 0, then the observations of system S(p) satisfy \u03a6. Finally, the user inspects the observations generated from the reaction-diffusion system with the optimized set of parameters S(p). If the observations are similar to the ones from the set Y+, then we find a solution. If, however, the user decides that the observations do not contain the pattern, then we add observations obtained from system S(p) to Y\u2212, and repeat the process, i.e learn a new formula, run the optimization until the user terminates the process or the optimization step fails (\u03b3 < 0).\nExample 6.2: LS pattern. We apply INTERACTIVEDESIGN algorithm to the system from Example 6.1. A sample set of observations obtained in the first iteration is shown in Figure 6-(a). We decide that these observations are not similar to the ones from the set Y(1)+ shown in Figure 4-\n(a), and add these 250 observations generated with the optimized parameters to Y(1)\u2212 (line 9). In the second iteration, the optimized parameters are D1 = 3.75 and D2 = 28.75, and the observations obtained by simulating S([3.75,28.75]) are shown in Figure 6-(b). We continue by adding these to Y(1)\u2212 . The parameters computed in the third iteration are D1 = 6.25 and D2 = 29.42. The observations obtained by simulating S([6.25,29.42]) are shown in Figure 6-(c). Although the optimized parameters are different from D1, which was used to generate Y(1)+ , the observations of S([6.25,29.42]) are similar to the ones from the set Y(1)+ and we terminate the algorithm."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "We defined a tree spatial superposition logic (TSSL) whose semantics is naturally interpreted over quad trees of partitioned images. We showed that formulas in this logic can be efficiently learned from positive and negative examples. We defined a quantitative semantics for TSSL and combined with an optimization algorithm to develop a supervised, iterative procedure for synthesis of patternproducing parameters.\nWhile the experiments show that the current version of the logic works quite well and can accommodate translational and rotational symmetries commonly found in biology patterns, there are several directions of future work. First, we expect that even better results could be obtained if more statistical moments were used, rather than just the mean as in the current version of this work. Second, we do not exploit the full semantics of the logic in this paper. In future work, we plan to investigate reasoning about multiple branches and using the \u201cuntil\u201d operator. Third, we plan to apply this method to more realistic networks, such as populations of locally interacting engineered cells. We expect that experimental techniques from synthetic biology can be used to \u201ctune\u201d existing synthetic gene circuits to produce global desired patterns."}], "references": [{"title": "Heterocyst formation in anabaena", "author": ["J. Golden", "H. Yoon"], "venue": "Curr Opin Microbiol., vol. 1, no. 6, pp. 623\u2013629, 1998.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Structure, partial elemental composition, and size of thiopedia rosea cells and platelets", "author": ["R. Scherrer", "V. Shull"], "venue": "Can J Microbiol., vol. 32, no. 7, pp. 607\u2013610, 1986.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "Statistical pattern recognition: A review", "author": ["A. Jain", "R. Duin", "J. Mao"], "venue": "IEEE Transactions on pattern analysis and machine learning, vol. 22, pp. 4\u201337, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "State-of-the-art in shape matching", "author": ["R.C. Veltkamp", "M. Hagedoorn"], "venue": "Principles of Visual Information Retrieval, Tech. Rep., 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Temporal and modal logic", "author": ["E.A. Emerson"], "venue": "Handbook of Theoretical Computer Science: Formal Models and Semantics, J. van Leeuwen, Ed. North-Holland Pub. Co./MIT Press, 1990, vol. B, pp. 995\u20131072.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "The chemical basis of morphogenesis", "author": ["A.M. Turing"], "venue": "Philosophical Transactions of the Royal Society of London, vol. 327, pp. 37\u201372, 1952.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1952}, {"title": "Textons, the elements of texture perception, and their interactions", "author": ["B. Julesz"], "venue": "Nature, vol. 290, pp. 91\u201397, 1981.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1981}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. of CVPR 2005: the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 1, June 2005, pp. 886\u2013893.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Shape Matching and Object Recognition Using Shape Contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, pp. 509\u2013521, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "Proc. of the International Conference on Computer Vision, vol. 2, 1999, pp. 1150\u20131157.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "From Model-Checking to Temporal Logic Constraint Solving", "author": ["A. Rizk", "F. Fages"], "venue": "Proc. of CP 2009: the 15th International Conference on Principles and Practice of Constraint Programming, Lisbon, Portugal, 20-24 September, ser. Lecture Notes in Computer Science, vol. 5732. Springer, 2009, pp. 319\u2013334.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust satisfaction of temporal logic over real-valued signals", "author": ["A. Donz\u00e9", "O. Maler"], "venue": "Proc. of FORMATS 2010, the 8th International Conference on Formal Modeling and Analysis of Timed Systems, Klosterneuburg, Austria, September 8\u201310, vol. 6246, 2010, pp. 92\u2013 106.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Robustness analysis and behavior discrimination in enzymatic reaction networks", "author": ["A. Donz\u00e9", "E. Fanchon", "L.M. Gattepaille", "O. Maler", "P. Tracqui"], "venue": "PLoS One, vol. 6, no. 9, p. e24246, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust Sampling for MITL specifications", "author": ["G. Fainekos", "G. Pappas"], "venue": "Proc. of FORMATS 2007, the 5th International Conference on Formal Modeling and Analysis of Timed Systems, ser. Lecture Notes in Computer Science, 2007, vol. 8044, pp. 264\u2013279.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Robustness of temporal logic specifications for continuous-time signals", "author": ["G.E. Fainekos", "G.J. Pappas"], "venue": "Theor. Comput. Sci., vol. 410, no. 42, pp. 4262\u20134291, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning and detecting emergent behavior in networks of cardiac myocytes", "author": ["R. Grosu", "S. Smolka", "F. Corradini", "A. Wasilewska", "E. Entcheva", "E. Bartocci"], "venue": "Communications of the ACM, vol. 52, no. 3, pp. 97\u2013105, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Linear and branching system metrics", "author": ["L. de Alfaro", "M. Faella", "M. Stoeling"], "venue": "IEEE Transactions on Software Engineering, vol. 35, no. 2, pp. 258\u2013273, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithm alley. Dr. Dobb\u2019s journal", "author": ["R. Collantes"], "venue": "December 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Quad trees a data structure for retrieval on composite keys", "author": ["R. Finkel", "J. Bentley"], "venue": "Acta Informatica, vol. 4, no. 1, pp. 1\u20139, 1974.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1974}, {"title": "Scalable modeling and performance evaluation of wireless sensor networks", "author": ["Y. Kwon", "G. Agha"], "venue": "Real-Time and Embedded Technology and Applications Symposium, 2006. Proceedings of the 12th IEEE, 2006, pp. 49\u201358.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Design and synthesis of synchronization skeletons using branching-time temporal logic", "author": ["E.M. Clarke", "E.A. Emerson"], "venue": "Proc. of Logic of Programs Workshop, ser. Lecture Notes in Computer Science, vol. 131, 1982, pp. 52\u201371.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1982}, {"title": "Fast effective rule induction", "author": ["W.W. Cohen"], "venue": "Proceedings of the Twelfth International Conference on Machine Learning. Morgan Kaufmann, 1995, pp. 115\u2013123.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "The weka data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explor. Newsl., vol. 11, no. 1, pp. 10\u201318, Nov. 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Particle swarm optimization", "author": ["J. Kennedy", "R. Eberhart"], "venue": "Proceedings of the IEEE International Conference on Neural Networks, vol. 4, 1995, pp. 1942\u20131948.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "From the stripes of a zebra and the spots on a leopard to the filaments (Anabaena) [1], spirals, squares (Thiopedia rosea), and vortex (Paenibacillus) [2] formed by singlecell organisms, patterns can be found everywhere in nature.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "From the stripes of a zebra and the spots on a leopard to the filaments (Anabaena) [1], spirals, squares (Thiopedia rosea), and vortex (Paenibacillus) [2] formed by singlecell organisms, patterns can be found everywhere in nature.", "startOffset": 151, "endOffset": 154}, {"referenceID": 2, "context": "Pattern recognition is usually formulated as a machine learning problem [3], in which patterns are characterized either statistically [4] or through a structural relationship among their features [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "Despite its success in several application areas [6], pattern recognition still lacks a formal foundation.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "model checking [7], [8], we provide partial answers to these questions.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "While the overall algorithm can, in principle, be applied to any network of locally interacting systems, in this paper we focus on the Turing reaction-diffusion system [9], and show that patternproducing parameters can be automatically generated with our method.", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "The two main steps in pattern recognition are: (a) to extract distinctive features [10], [11], [12], [13] with relevant information from a set of input data representing the pattern of interest and (b) to build, using one of the several available machine learning", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "The two main steps in pattern recognition are: (a) to extract distinctive features [10], [11], [12], [13] with relevant information from a set of input data representing the pattern of interest and (b) to build, using one of the several available machine learning", "startOffset": 89, "endOffset": 93}, {"referenceID": 8, "context": "The two main steps in pattern recognition are: (a) to extract distinctive features [10], [11], [12], [13] with relevant information from a set of input data representing the pattern of interest and (b) to build, using one of the several available machine learning", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "The two main steps in pattern recognition are: (a) to extract distinctive features [10], [11], [12], [13] with relevant information from a set of input data representing the pattern of interest and (b) to build, using one of the several available machine learning", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "Feature descriptors such as Textons [10] and Histograms of", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "Oriented Gradients (HoG) [11] are concerned with statistical information of color distribution or of intensity gradients and edge directions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "The scale-invariant feature transform (SIFT), proposed by Lowe in [13], is based on the appearance ar X iv :1 40 9.", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "The shape context [12] is another feature descriptor intended to describe the shape of an object by the points of its contours and the surrounding context.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Both classical verification [15], [16], [17], [18], [19] and pattern recognition techniques aim to verify (and possibly quantify) the emergence of a behavioral pattern.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "Both classical verification [15], [16], [17], [18], [19] and pattern recognition techniques aim to verify (and possibly quantify) the emergence of a behavioral pattern.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Both classical verification [15], [16], [17], [18], [19] and pattern recognition techniques aim to verify (and possibly quantify) the emergence of a behavioral pattern.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Both classical verification [15], [16], [17], [18], [19] and pattern recognition techniques aim to verify (and possibly quantify) the emergence of a behavioral pattern.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "Both classical verification [15], [16], [17], [18], [19] and pattern recognition techniques aim to verify (and possibly quantify) the emergence of a behavioral pattern.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "For example, in [20], the spatial modalities are used to characterize self-similar (fractal) texture.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "This paper is inspired by the original work on morphogenesis by Alan Turing [9], and is closely related to [20].", "startOffset": 76, "endOffset": 79}, {"referenceID": 15, "context": "This paper is inspired by the original work on morphogenesis by Alan Turing [9], and is closely related to [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "As opposed to [20], we also define a quantitative semantics for the logic, and use the distance to satisfaction as a fitness function while searching for pattern-producing parameters.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "This quantitative semantics and the discounted model checking on a computational tree are inspired from [21], with the notable difference that we do not need a metric distance, but rather a measure of satisfiability.", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "been used in [15], [16], [17], [18], [19].", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "been used in [15], [16], [17], [18], [19].", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "been used in [15], [16], [17], [18], [19].", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "been used in [15], [16], [17], [18], [19].", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "been used in [15], [16], [17], [18], [19].", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "The system is inspired from Turing\u2019s reaction-diffusion system and is presented in [22] as a model of the skin pigments", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "We assume that species 1 is observable through mapping H : R32\u00d732\u00d72 + \u2192 R32\u00d732 [0,1] given by:", "startOffset": 79, "endOffset": 84}, {"referenceID": 18, "context": "1: A quad-tree Q = (V,R) is a quaternary tree [23] representation of Ak,k where each vertex v \u2208 V represents a sub-matrix of Ak,k and the relation R \u2282 V \u00d7V defines the four children of each node v that is not a leaf.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "This approach is inspired by previous papers [20], [24], where the authors aim to combat the state-explosion problem that would stem otherwise.", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "This approach is inspired by previous papers [20], [24], where the authors aim to combat the state-explosion problem that would stem otherwise.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "The TSSL logic resembles the classic CTL logic [25], with the main difference that the next and until are not temporal, but spatial operators meaning a change of resolution (or zoom in).", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "For this reason we equip our logic also with a quantitative valuation that provides a measure of satisfiability in the same spirit of [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 21, "context": "We employ RIPPER [26], a rule based learner, to learn a classifier from LL, and then translate the classifier into a TSSL formula characterizing +.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "We use RIPPER algorithm implemented in Weka [27] to learn a classifier from L L .", "startOffset": 44, "endOffset": 48}, {"referenceID": 23, "context": "Here, we use the particle swarm optimization (PSO) algorithm [28] over P with (VI.", "startOffset": 61, "endOffset": 65}], "year": 2014, "abstractText": "We propose a technique to detect and generate patterns in a network of locally interacting dynamical systems. Central to our approach is a novel spatial superposition logic, whose semantics is defined over the quad-tree of a partitioned image. We show that formulas in this logic can be efficiently learned from positive and negative examples of several types of patterns. We also demonstrate that pattern detection, which is implemented as a model checking algorithm, performs very well for test data sets different from the learning sets. We define a quantitative semantics for the logic and integrate the model checking algorithm with particle swarm optimization in a computational framework for synthesis of parameters leading to desired patterns in reaction-diffusion systems.", "creator": "LaTeX with hyperref package"}}}