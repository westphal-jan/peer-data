{"id": "1409.7165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2014", "title": "Heterogeneous Metric Learning with Content-based Regularization for Software Artifact Retrieval", "abstract": "The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs.\n\n\n\nIn some cases, the most important part of the problem of software artifact retrieval is that it is difficult to provide the correct and valid information about programs that contain certain kinds of information. The best tools available to perform this type of retrieval include a set of semantic features, and the ability to analyze the code (the semantic features were used in the original implementation of the standard system).\nMany of the current problems with semantic features include many of the most important problems with semantic features:\nThere are currently several problems with semantic features (see Section 2.2 and Section 3.3). The following is a list of most commonly used semantic features. The most commonly used semantic features are:\nThe following is a list of commonly used semantic features.\nFor example, in the example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the following example of the example of the following example of the following example of the following example of the following example of the following example of the", "histories": [["v1", "Thu, 25 Sep 2014 06:33:57 GMT  (576kb)", "http://arxiv.org/abs/1409.7165v1", "to appear in IEEE International Conference on Data Mining (ICDM), Shen Zhen, China, December 2014"]], "COMMENTS": "to appear in IEEE International Conference on Data Mining (ICDM), Shen Zhen, China, December 2014", "reviews": [], "SUBJECTS": "cs.LG cs.IR cs.SE", "authors": ["liang wu", "hui xiong", "liang du", "bo liu", "guandong xu", "yong ge", "yanjie fu", "yuanchun zhou", "jianhui li"], "accepted": false, "id": "1409.7165"}, "pdf": {"name": "1409.7165.pdf", "metadata": {"source": "CRF", "title": "Heterogeneous Metric Learning with Content-based Regularization for Software Artifact Retrieval", "authors": ["Liang Wu", "Hui Xiong", "Liang Du", "Bo Liu", "Guandong Xu", "Yong Ge", "Yanjie Fu", "Yuanchun Zhou", "Jianhui Li"], "emails": ["lijh}@cnic.cn", "yanjie.fu}@rutgers.edu", "duliang@ios.ac.cn", "liubo@research.nec.com.cn", "guandong.xu@uts.edu.au", "yong.ge@uncc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n71 65\nv1 [\ncs .L\nG ]\n2 5\nSe p\nI. INTRODUCTION\nSoftware artifact retrieval, which is also frequently mentioned as software traceability, is of considerable usefulness for developers, since traceability provides insights into system development and evolution assisting in locating software features, analyzing requirements, managing and reusing legacy systems and etc. It gives essential support in understanding the relationships within and across software requirements, design and implementation [31]. It is a fundamental task throughout the software development life-cycle, and is especially important for large-scale and complex software systems.\nThe task has been viewed as a special case of information retrieval in the area of software engineering. The documents to be retrieved are code programs, and queries are often formulated by developers or automatically taken from requirements and bug reports, e.g., titles of bug reports are used to localize bugs in code programs.\nIn order to retrieve code artifacts, traditional methods try to generate a textual representation for source code by picking up words from programs and then compare them with queries,\ni.e., transforming the problem to a text retrieval task. Existing methods only leverage textual similarity between queries and the extracted textual content of code, because the similarity between code and text cannot be computed directly.\nAlong this stream of research, vector space model and stochastic language model are first adopted [1], [5]. The query and codes are both represented as bag-of-words feature vectors. Subsequently, several other information retrieval models, as well as their variants and ensembles, are also experimented to improve the accuracy of the software artifact retrieval task[7], [19], [4]. These text-based methods are reasonable for the task, since some words may be used in programs by developers. The problem is that these methods have not fully exploited the semantics and operational information embedded in the program. Although the code is composed of text, simply breaking them into a bag of words loses important information and descriptive features. The adoption of proper semantic representation to measure the similarity between code itself and query motivates the work conducted in this paper.\nTo this end, in this paper we first extract the relationships between functions and classes, including reference, implementation, inheritance, as features because they indicate the associations between different entities. In addition, as most source code implements a certain target by assembling several functional code fragments, the code patterns which frequently appear in programs bear more semantic than the rest. We attempt to discover and exploit them to serve as the additional features of the corresponding source code. More specifically, we first extract code relationship features of code programs, and then organize the code as a tree structure, in which each node depicts the adjacent program statements separated by the block delimiters. The tree nodes are then combined iteratively in an agglomerative manner, and frequent code patterns are extracted as code features. Thus the extracted code features contain both the textual content of the code and also preserve the functional information by clustering the logically related statements together. The details are presented in Section II-C.\nAlthough homogeneous distance metric learning [28], [29] has been proposed to learn a good metric to compare two objects and has played a significant role in statistical classification and information retrieval, it requires the objects to share identical features and are comparable. The extracted\ncode features and text features in our task, however, are heterogeneous, so they cannot be compared directly. Thus we propose a novel heterogeneous distance metric learning method to discover the shared semantic representation of text and code features. The proposed approach in our work enables the distance calculation between instances from different feature spaces. More specifically, two transformation matrices are produced to map the code-feature representation of code and the text-feature representation of queries into a shared semantic space, where codes and queries own a homogeneous low-rank representation. Then queries and programs can be compared directly. Similar ideas have been proposed to solve the problem of cross modal multimedia retrieval, such as using text as queries to retrieve music and using text to retrieve pictures [21], [26], [3], [9]. Researchers proposed to build distance metric between different media and help to get better results. However, since the code features may contain words that overlap with the words in queries, in our task the heterogeneous features are partially comparable. The overlapped words, which may be useful for retrieving software artifacts, will be lost, if we simply use the multimedia retrieval techniques. Thus, we further propose a data matrix to preserve the content-based similarity between text features and code features, which is further adopted to regularize the objective function by constraining the parallel features to be similar in the new space. The content-based regularization exploits the similarity between text and code, and is useful for improving other methods from the domain of heterogeneous multimedia information retrieval according to the experimental results.\nBy integrating the heterogeneous distance metric learning approach and the content-based regularization into a unified framework, we propose a model named Heterogeneous Metric Learning with Content-based Regularization (HMLCR) to build a distance metric between the feature spaces of code and text. Extensive experiments are conducted to verify the effectiveness of the proposed method. The experiments are based on real-world data sets, which are obtained from two open-source applications with distinct functions and different programming languages.\nThe main contributions of the paper lie in four aspects:\n1) Two kinds of code features, namely code relationship features and frequent code snippet features are proposed to better represent programs, which contain the operational and functional information of codes and help to overcome the bottlenecks of traditional methods in the area of software engineering. 2) A novel heterogeneous distance metric learning model is proposed to allow similarity computation between codes and text. The similarity is then used to enhance the retrieval results. 3) We evaluate our method using real world open source software data. The experimental results demonstrate that the proposed model can improve the prediction accuracy significantly and outperform several baselines. 4) We demonstrate a case study using HMLCR to retrieve corresponding word features for code features, which are used to further explain the changes brought by incorporating the proposed model.\nThe remaining of the paper is organized as follows. The\nproblem definition of software traceability and the proposed approach are presented in Section II. Experiments are conducted in Section III, where the experimental results and a case study are provided. Then we discuss close related topics in Section IV and Section V concludes the work and provides possible future directions."}, {"heading": "II. PROPOSED APPROACH", "text": "As the code contains useful clues for linking text, code and between them, our target is to directly compute the similarity between code and queries, and further use the similarity to enhance the retrieval results. To allow computation in different feature spaces for code and text, two transformation matrices are constructed to map the text queries and the code files into a same semantic space. Thus, in this section, we first introduce the definition of the problem and then describe the framework we proposed. Two kinds of code features are discussed subsequently. Finally, the HMLCR model and its optimization are presented."}, {"heading": "A. Problem Definition", "text": "The task has been viewed as a general information retrieval task, where the queries are written in natural languages, and the code has two parts, containing both source code and textual information consisting of comments, identifier names and etc. Here we denote the code as D = {(xc1, x d 1, l1), \u00b7 \u00b7 \u00b7 , (x c m, x d m, lm)}, which contains m code programs, each consisting of the source code xc and textual information xd. Every code has a label li, which represents the specific function of the code. Labels are generated either manually or according to certain disciplines of a specific domain. For most projects, every code file has a unique identifier. But in some real applications, the label is used to denote the function of a code file. Thus different programs may share an identical label. The query set is denoted as Q = {(xd1, l1), \u00b7 \u00b7 \u00b7 , (x d n, ln)}, where each query contains a short paragraph describing itself and a label which is taken from the same vocabulary as the labels of code. The source of queries includes bug reports, specifications and requirements, which normally contain references to code files that a query describes. The reference is used to produce labels of queries.\nDefinition 1: (Software Artifact Retrieval) Given a program data set D = {(xc1, x d 1, l1), \u00b7 \u00b7 \u00b7 , (x c m, x d m, lm)} and an unlabeled query, the basic objective of the software artifact retrieval task is to retrieve relevant code files in the unlabeled data set T = {(xc1, x d 1), \u00b7 \u00b7 \u00b7 , (x c n, x d n)}.\nTraditional methods focus on computing the similarity between queries and the textual representation xd of code files. The contribution of our work is to incorporate the similarity between queries and xc. In order to enable the similarity computation, we extract code features from xc, and build heterogeneous distance metrics between code features and word features."}, {"heading": "B. Overview", "text": "Figure 1 presents the framework of our proposed approach. The word features are first extracted from textual content of codes. Since words cannot fully reveal functional information of program codes, we further propose code features to\novercome the bottleneck. The code features are obtained by extracting the code relationships and iteratively combining the adjacent program expressions based on their structure. Since code and word features are heterogeneous, in the training process, as depicted in Figure 1(a), two transformation matrices U and V are built by the HMLCR model. The two transformation matrices can be viewed as the heterogeneous distance metric between the text features and the code features, and are used to enable the direct comparison of similarity between heterogeneous feature spaces. Although it is hard to characterize the intrinsic structure of the homogeneous and heterogenous data in the new space, fortunately, various contextual information is available in real applications of software artifact retrieval, which can be leveraged to assist the learning process. Here, we put forward four constraints on the generation of the distance metrics: 1)pull loss is used to force the textual representation and code representation of programs with the same label to be near in the new space. 2)graph regularization is employed to gather both homogeneous and heterogeneous entities with a same label together in the new space. 3)As some code features contain words, and these features should be similar to the word features which also contain the words in the new space. Thus, the content based regularization aims to reserve this similarity by constraining similar features to be related in the new space after the projection. 4)A scale regularization item is adopted to avoid over-fitting. The HMLCR model focuses more on the relationships between textual representation and code representation of programs, as in real world applications, they are born together and thus the links are sufficient for training; while the query-code relationship is not taken into consideration, since the links between queries and code are much harder to collect and far less than enough.\nAs depicted in Figure 1(b), the final similarity between a query and a code file is two-fold: text-text similarity can be directly computed between the textual representation of code programs and queries, as they are homogeneous; textcode similarity is acquired by transforming the queries and code into a common feature space and representing them by hidden topics. The linear ensemble of them is taken as the final similarity."}, {"heading": "C. Feature Extraction for Code Programs.", "text": "Most existing work on source code retrieval focuses on extracting the words in programs, that is, collecting words to build documents from comments, identifier types, function names and etc., which ignores the structures of the programs. This line of methods may encounter difficulties when the textual content is not sufficient. For example, As the queries and the code are often written by different organizations, a variety of vocabularies may be employed to characterize a similar or even same entity/behavior. This results in a gap between the queries and the code descriptions.\nWe observe that, though different words are used to describe a same thing, the specific lines of programs, which implement the corresponding behavior or entity, almost keep still from time to time. Based on the observation, a natural intuition is that the specific lines of code may be extracted and leveraged to bridge the gap between different queries and programs during retrieval.\nThe code features are extracted as the bridges. They are used to gain more information by linking them to the frequently co-occurred words. Since the content of code is also useful for representing programs, we also adopt the content of code features during optimization.\n1) Code Relationship Feature Extraction: The first kind of code features are the code relationship features. Between different code files, some linking information is explicitly announced. The related information between different code files indicate the associations between different entities, and thus are useful for inferring the semantic of code files. So we extract the relationships between functions and classes, including reference, implementation, inheritance, as code relationship features.\n2) Code Snippet Feature Extraction: A key issue of extracting frequent code patterns is how to split a whole program file into pieces. As most source code can be logically formalized like a tree structure, i.e., the whole program is the root node and its sub modules are first layer children. We collect these nodes to build the candidate set of our code features. Each node represents the expressions which are separated and combined\nby the block delimiters, since expressions of source code are often separated to different domains based on the ending characters, like the curly brackets \u201c{\u201d and \u201c}\u201d in C and Java. The adjacent code expressions are then merged together as a tree node and then added to the code tree. They are further merged into their higher level parents. Therefore, we extract the nodes hierarchically, from leaf nodes to the root node.\nAlgorithm 1 displays the algorithm of the extraction and vectorization of code snippet features. The generated tree nodes of programs are sequentially checked by the algorithm. The mapping between code and features are saved to produce the data matrix, and the candidate set C reserves all code nodes. We name the extracted code nodes as code snippets. Here, a code snippet consists of at least one expression of source code. As stated earlier, we first merge the program statements between the delimiters as the lowest level snippets; and we then merge the snippets together according to the positions of the delimiters, as the snippets are normally organized and nested to perform more complex behaviors. These snippets are then combined in a agglomerative manner according to the hierarchical organization of the program, thus a set of code snippet features is generated. It should be noted that, as we\nAlgorithm 1 Construction of Code Snippet Feature Candidate Set Input: The dataset of code programs: C. Output: The candidate set of code features: F ; The map\nbetween program files and code features: M . 1: for each program c in C do 2: Extract All Nodes N from c 3: for each node n in N do 4: if n /\u2208 F then 5: Add n to F 6: end if 7: Add the Program Feature Map to M 8: end for 9: end for\n10: return F and M\ncall the set of code snippets as code feature candidates, not all code snippets are finally used afterwards. A lower and a higher bound of number of occurrences are set to filter out those rare and common snippets. The rare snippets, which seldom appear in programs, make the relationship between features and codes sparser; the code feature candidates which appear too frequently in the dataset are less informative for the retrieval task. In this paper, we set the bounds empirically.\nOur basic intuition to extract code snippets is they can be used to accurately represent a behavior or an entity, and are useful to bridge the different text and enrich the sparse textual information, since code is function-dependent, i.e., code snippets with same functions should share a same structure. In order to avoid taking in too much noise, a set of preprocessing operations, like transforming the identifer names to identifer types, are performed on the programs before features being extracted. A similar line of methods that are also designed for this goal of removing the useless expressions has been proposed and studied well in the area of software engineering, which is formally named as program slicing method [33].\nAfter extracting code features, including code relationships\nand code snippets, our objective is to infer its semantic by mining the relationships between code and text, thus to allow direct comparison between code and text.\n3) Content Information Extraction: As discussed in Section II-B, the content of code features is useful for the distance metric learning. Thus, we propose to use a data matrix R to save this content-based similarity between heterogeneous features. As introduced in Table I, R is a dx \u00d7 dy matrix, in which each entry ri,j represents the similarity between text feature i and code snippet j.\nFor simplicity, the similarity is defined to be 1 if the text feature i and the code feature j contain a same word. A problem is that, in code snippet features, as the high level tree nodes contain many low level nodes, they may be similar to too many text features erroneously. In order to avoid this, the words are encoded in the first layer and no longer available when they are wrapped more than once. That is, in a code feature, the word is only available in its lowest code snippet. When the code snippet is iteratively combined in the higher code snippet, the word is wrapped in its lowest feature and will not be included as code content more than one time."}, {"heading": "D. Heterogeneous Metric Learning with Content-based Regularization", "text": "Distance metric learning [27] has attracted much attention in the last decade. The objective of distance metric learning is to learn a linear transformation matrix to map the data into a new space. Then the distance between objects in the new space is defined by the distance metric. The learning process forces the new space to preserve the similarity and dissimilarity between any two objects in the training data. Distance metric learning proves to be useful in many machine learning tasks, as it helps to capture the optimal distance between two objects.\nIn the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9]. Unlike the homogeneous distance metric learning, heterogeneous distance metric learning aims to learn multiple transformation matrices. Each matrix is used to transform the corresponding kind of media to the semantic space.\nIn Section II-C, we have described the features extracted from code. In order to calculate the similarity between code programs and textual descriptions, we regard code semantic representation as a new media and propose the HMLCR model, to learn two transformation matrices U and V for text and code via optimization.\nAfter that, each textual representation and code feature representation are projected to a new space with dimensionality of k. k can be viewed as the number of latent topics. For simplicity, k is set empirically in this work and is normally smaller than dx and dy. Then based on the new space, the heterogeneous similarity can be measured. Later in this section, the details of HMLCR are introduced.\nWe first describe our loss function \u01ebpull that helps to build up the connections between code and text feature spaces. Then we introduce a joint graph regularization item g which constrains the new space to reserve the links between the textual and code feature-based representation of programs. Then we show how to leverage the content information of code features to regularize the training objective. The optimization objective and the optimization method are presented in the end. Some of the notations are summarized in Table I.\n1) Loss Function: In order to reserve the relationships between code feature representation and textual representation, we propose to use a loss function to build the new space. As depicted in Equation 1, the item \u01ebpull(U, V ) penalizes the large distance between code and its corresponding textual representation, i.e., pulling them together.\n\u01ebpull(U, V ) = 1\n2 ||XTU \u2212 Y TV ||2F (1)\n2) Joint Graph Regularization: The similarity between code feature and code feature based representation, the similarity between textual and textual representation are also useful for heterogeneous metric learning. Thus, in order to force the low dimensional representations to reserve the relationships, we introduce a joint graph regularization term\ng(U, V ) = 1\n2 tr(OL\u0304TOT ) (2)\nwhere O is a c \u00d7 (m +m) data matrix which represents the coordinates of the original data in the new semantic space:\nO = ( UTX,V TY )\n(3)\nand L is the normalized graph Laplacian of W , which is produced as follows:\nL\u0304 = I \u2212D\u2212 1 2WD\u2212 1 2 (4)\nin which W = wij (m+m)\u00d7(m+m) means the relationship between the i-th object and j-th object. The symmetric similarity of labels is encoded into the matrix as follows:\nwij =\n{\n1, li = lj \u2227 i 6= j; 0, otherwise\n(5)\nHere, I is an identity matrix and D is a diagonal matrix with each entry dii equals the sum of the corresponding row of W , namely\n\u2211m+m j wij .\nAs denoted in Equation 6, the matrix L\u0304 can be decomposed into four parts:\nL\u0304 =\n( L\u0304xxL\u0304xy\nL\u0304yxL\u0304yy\n)\n. (6)\nthus Equation 2 can be rewritten as:\ng(U, V ) = 1\n2 tr(OL\u0304OT )\n= 1\n2 tr(UTXL\u0304xxXTU) +\n1 2 tr(UTXL\u0304xyY TV )\n+ 1\n2 tr(V TY L\u0304yxXTU) +\n1 2 tr(V TY L\u0304yyY TV )\nwhere tr(UTXL\u0304xxXTU) is the trace of the matrix UTXL\u0304xxXTU . g(U, V ) formulates pairwise similarity between the code feature representation and the textual representation of programs using graph. It improves the smoothness of the mappings by penalizing the functions that change abruptly on the joint data graph [23].\n3) Content-based Regularization: As mentioned in Section II-C, R is the content-based similarity between code features and text features. To force the transformation matrices U and V to reserve it, we incorporate another term to regularize the objective as follows,\nc(U, V ) = 1\n2 ||UV T \u2212R||2F (7)\nwhere UV Tdx\u00d7dy denotes the associations between features in the new space, so c(U, V ) aims to penalize the large distance between code features and text features which share the same words.\n4) Optimization Objective: Finally, we introduce a regularization term r(U, V ) to control the scale of the transformation matrices U and V . In this paper, we define the regularization function as follows:\nr(U, V ) = 1\n2 ||U ||2F +\n1 2 ||V ||2F (8)\nThus, the optimization objective is the combination of the aforementioned items:\nargmin U,V \u03bb1\u01ebpull + \u03bb2g + \u03bb3c+ r (9)\nwhere \u03bb1, \u03bb2 and \u03bb3 control the impact of each constraint.\n5) Optimization: We will introduce how to minimize the optimization problem in Equation 9. The optimization is an unconstrained optimization problem with two matrices U and V , which is not jointly convex to them. Since we cannot get the closed-form solutions, we turn to solve the problem by fixing one matrix and optimize the other iteratively. Therefore, gradient descent is adopted to approach the optimal results. By taking the derivatives over the objective function, we have the gradient of U as:\n\u2202Loss\n\u2202U = \u03bb1X(X\nTU \u2212 Y TV )\n+ \u03bb2(XL\u0304 xXTU +XL\u0304xyY TV + \u03bb3(UV T \u2212R)V + U\nand the gradient of V as:\n\u2202Loss\n\u2202V = \u03bb1Y (Y\nTV \u2212XTU)\n+ \u03bb2(Y L\u0304 yxXTU + Y L\u0304yY TV ) + \u03bb3(UV T \u2212R)TU + V\nThe optimization process can be found in Algorithm 2. Note that the matrices U and V are initialized (line 1) based on the Cross-modal Factor Analysis algorithm[15].\nAlgorithm 2 Iterative Optimization for HMLCR Input: The data matrices: X , Y ; The similarity matrices:\nW , R; The parameters, \u03bb1...3; The learning rate \u03b7; The maximal number of iterations MaxIter.\nOutput: The transformation matrices U and V . 1: Generate U and V 2: for i = 1 to MaxIter do 3: U \u2190 U+\u03b7 \u2202Loss\n\u2202U\n4: V \u2190 V +\u03b7 \u2202Loss \u2202V 5: if convergence then 6: break 7: end if 8: end for 9: return U and V"}, {"heading": "III. EXPERIMENT", "text": "In this section, we will first introduce the experimental settings as well as the dataset. Then we will describe the evaluation metrics we adopted. Next, we discuss some major results of the experiments. Finally, some case studies are presented and discussed."}, {"heading": "A. Dataset", "text": "The model has been incorporated and test in a commercial software to index programs by textual descriptions, but the result cannot be disclosed due to intelligence property issues. So we obtain datasets from two real world open source software, the platform of Eclipse1 and Filezilla2 and thus make the experimental results reproducible.\nEclipse is a popular open-source IDE for many programming languages written mainly in Java. The project contains approximately 7,000 classes with about 89,000 methods in about 2.4 million lines of code (MLOC).\nFilezilla is an open-source FTP client for Windows, Mac OS X and GNU/Linux. The project is written in C and is much smaller than Eclipse, with about 8,012 methods in 410 KLOC.\nIn order to test our model, we extract titles of bug reports of Eclipse and change logs of Filezilla as queries to retrieve code. The approach of using bug reports and change logs is frequently adopted in the area of software engineering, which is based on change reenactment [32]. For example, Eclipse bug 51383, as depicted in Figure 2, reports an error on Double Click function of user interfaces. The title \u201cDouble-click-drag to select multiple words doesn\u2019t work\u201d is collected as a query and the corresponding code files can be found in the fix patch. We crawled over 1700 bug reports for Eclipse and over 1320 change logs for Filezilla, which own a clear connection to code programs. They are further used for evaluation in our experiments.\n1http://www.eclipse.org 2https://filezilla-project.org 3https://bugs.eclipse.org/bugs/show bug.cgi?id=5138"}, {"heading": "B. Evaluation Metrics", "text": "In order to measure the accuracy of the proposed approach, we use three methods to evaluate the retrieval results. The first one is Precision at n(P@n). Here, the position of the true positives is not taken into consideration, as another measure will be adopted to model the usefulness of the returned results in terms of ranking orders. The calculation of P@n is illustrated in Equation 10.\nP@n = |{relevant code files} \u2229 {retrieved code files}|\nn (10)\nAs in the area of software engineering, traceability module is often designed for professional users who are more patient to click the lower ranked results. Therefore, it is more important to retrieve all the code files given a query, So Recall at n (R@n) is adopted to be the second evaluation metric. Similar with the P@n, the positions of results are not taken to penalize the low ranked right answers. The details of the metric is illustrated in Equation 11;\nR@n = |{relevant code files} \u2229 {retrieved code files}|\n|{relevant code files}| (11)\nAnother measure method we employ is the normalized Discounted Cumulative Gain(nDCG)[18]. This measure is useful for computing the quality of ranking results as it considers both the returned contents and the order of the results. To evaluate the quality of a ranking list, we rank the retrieved code files for each of the queries based on their computed similarity.\nIn particular, nDCG[p] measures the top p results as follows:\nDCG[p] = rel1 +\np \u2211\ni=2\nreli log2i\nnDCG[p] = DCG[p]\nIDCG[p] (12)\nwhere IDCG[p] is the DCG[p] value given the ranking list of the ground truth. reli is the relevance value. Thus, if the\nhigher ranked results are more relevant, a bigger nDCG will be produced, i.e. a higher nDCG represents a better ranking result."}, {"heading": "C. Settings", "text": "In order to investigate the effectiveness of the proposed approach, we implemented several methods from the domain of software engineering and multimedia information retrieval as our baselines.\n\u2022 COS: The cosine similarity based method [1], [19] which calculates the cosine similarity between the textual representation of code and queries.\n\u2022 LM: This method adopts language modeling to calculate the similarity between the textual representation of code and queries [1], [5].\n\u2022 LSI: The latent semantic indexing method that first compresses the textual representation of code and queries and then calculate their similarity [7], [25].\n\u2022 CFA: Cross-modal Factor Analysis model, which is first proposed in [15] and is designed to discover the associations between the feature space of different media. We adopt this method to calculate the correlations between word features and code features. This method ranks the code files based on two parts, the text-text similarity and the text-code similarity.\n\u2022 CFA+CR: In this method, we adopt the content-based constraint to regularize the training process of CFA. Thus, we test the impact of the content based regularization on the retrieval results.\n\u2022 HMLCR: The proposed Heterogeneous Metric Learning with Content based Regularization. This method ranks the code files based on two parts, the text-text similarity and the text-code similarity.\nThe first three baselines, COS, LM and LSI, are taken from the area of software engineering, which treat code files as natural languages and extract words from code to form a textual representation. The other baselines are more similar with the methods in cross media information retrieval, which model the code as heterogeneous media and try to bridge the gaps between different feature spaces.\nAll these methods can be improved by exploiting application-specific rules, manual labeling and interactive user guidance. In this work, however, we focus on the automatic part of the problem, that is, the similarity computation between code and text. So the baselines we choose, COS, LM and LSI, concentrate more on text comparisons. The rules, labeling and user guidance are not used in our model, either. The proposed method and the baselines can be improved when used in real projects by leveraging these domain knowledge. So the proposed method can be easily equipped to the online systems by replacing the existing calculation modules.\nIn all experiments, five-fold cross validation is employed. We split the queries into five folds randomly and in each round we select one fold, the queries and the corresponding code files, as the training data, and use the rest for testing. The reported result is the average of all rounds."}, {"heading": "D. Experimental Results", "text": "In this section, we describe the retrieval performance of the proposed model and baselines. Table II illustrates the precision at top 1, 2, 4 and 5 for Eclipse dataset. The best result is achieved by the proposed approach and the precision at the first and the second position is obviously improved. An interesting observation is that the CFA+CR method, which incorporates Content-based Regularization (CR) into Crossmodal Factor Analysis model, achieves the second best result. The observation, on the one hand, proves the usefulness of regarding code as multimedia and adopting heterogeneous metric learning to model it; On the other hand, it demonstrates the effectiveness of content-based regularization, as the method significantly outperforms the original CFA method.\nSimilar findings are observed in Filezilla\u2019s P@n results, as depicted in Table III. A difference is that the result of CFA+CR baseline draws nearer to that of HMLCR. In the setting of P@4, it even outperforms the proposed approach. The result reveals that the content based regularization has a larger impact on the Filezilla dataset. The phenomena can be explained by the difference of ways of obtaining queries. The queries of Eclipse are extracted from bug reports, which may be filed by ordinary users; while the queries of Filezilla are taken from the change logs, which are issued officially. Thus, the words used in change logs are more refined and accurate, and more closely tending to overlap with the vocabulary of source code.\nThe experimental results in terms of recall at top 1, 3, 5 and 20 of Eclipse and Filezilla datasets are displayed in Table IV and V respectively. Similar findings can be observed by checking the results. The recall of Filezilla experiment is higher than that of Eclipse on average, since the Filezilla project has much fewer code files and thus the search space is far smaller.\n5 10 15 20 25 30 35 40 45 0.167\n0.168\n0.169\n0.17\n0.171\n0.172\n0.173\n0.174\n0.175\nN D\nC G\n@ 5\nHMLCR CFA+CR\n(b) The experimental results based on Filezilla Dataset.\nThe nDCG results of Eclipse and Filezilla project are shown in Table VI and VII. As presented in the results, the multimedia methods outperform the baselines from software engineering domain on average, which shows that the functional information of code is more descriptive than the code content. The best result is achieved by considering both the functional and content information: by incorporating the content-based regularization, considerable improvement is encountered on our method and the CFA+CR.\nIn order to investigate the impact of the content-based regularization in detail, we present the performance of HMLCR and the CFA+CR baseline with a varying \u03bb3 in Figure 3. In\nthis experiment, Eclipse and Filezilla datasets are used and nDCG is adopted as the evaluation metric, since it can better reveal the quality of the ranking results.\nFigure 3 presents the results of Eclipse and Filezilla dataset respectively. For Eclipse dataset, as denoted in Figure 3(a) the best performance is achieved when \u03bb3 is around 0.15 to 0.30. When the value of \u03bb3 draws near to 0.0, the performance decreases rapidly to that of the baselines without contentbased regularization. When the parameter becomes too large, a reduction is found of the nDCG@5, as it may overwhelm other useful information like the links between text and code files.\nSimilar observations can be found in the experimental results of Filezilla dataset, as illustrated in Figure 3(b). It should be noted that the scale of \u03bb3 of Filezilla is larger than that of Eclipse. The source of queries of the Filezilla experiment seems to be the causes. As mentioned earlier, the code content regularization is more effective for Filezilla dataset."}, {"heading": "E. Case Study", "text": "To further explain the changes brought by the proposed method, HMLCR, several automatically generated cases are displayed in Table VIII. There lies a manually selected case in each row of the table. In each case, a code feature, and the corresponding textual features are given for comparison. The textual features are the top scored words generated by CFA and HMLCR.\nThere mainly exist two kinds of features, code relationship features and code snippet features in this work. As frequent code patterns are more difficult to understand, we select some class name features from code relationships for simplicity.\nThe score of each word is computed based on the transformation matrices U and V . Since U describes the similarity between words and topics, and V describes the similarity between code features and topics, UV T can be viewed as the\nsimilarity between textual and code features in the semantic space.\nR = UV T = {r1 \u00b7 \u00b7 \u00b7 rdy} (13)\nAs denoted in Equation 13, the relatedness matrix Rdx\u00d7dy consists of dy column vector. Each of the vector represents the relatedness between the code feature and every word feature. The relatedness is adopted as the score in our case study. The top scored words extracted by transformation matrices of CFA are also displayed here for comparison.\nThough some bad cases are still existing in the results of HMLCR, our results look better than the results of CFA obviously. For example, in the first case, \u201cformat\u201d is more related to the class \u201corg.eclipse.swt.swt\u201d than the word \u201cabstract\u201d, since the class is a user interface component and SWT is short for Standard Widget Toolkit. As shown in the table, our model ranks it higher in the list than the baseline.\nIn the second case, the proposed algorithm assigns the word \u201cmax\u201d a higher score than the baseline. The class \u201cjava.io.inputstream\u201d is the superclass of all classes in Java representing an input stream of bytes, and the word \u201cmax\u201d is used to describe the limitation of data being transferred.\nAlso in the third case, \u201chandler\u201d is more related to the Cascading Style Sheet class (\u201corg.w3c.dom.css.cssvalue\u201d). While in this case and the first case, CFA both ranks the textual feature \u201cabstract\u201d higher, it is not reasonable. Though the word may frequently appear in codes, but it is obviously not informative in our application,\nAnother interesting case is the last one. HMLCR puts a word \u201cexception\u201d on the top of our word list. As the word is very descriptive to the code feature \u201cjava.io.ioexception\u201d and they both share the textual content \u201cexception\u201d, the content based regularization clearly contributes to the performance."}, {"heading": "IV. RELATED WORK", "text": "To the best of our knowledge, there is no prior work that leverages codes themselves to enhance the software artifact retrieval task in the area of software engineering. The most similar work may be the task of automatic software traceability. Their methods focus on calculating the similarity between textual queries and the text contents of code. Another source of related work may come from cross-modal information retrieval, i.e., using queries in one media format (e.g., images) to retrieve objects in heterogeneous formats (e.g., video). In this section, we introduce some of the representative work in each domain.\nRetrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20]. Antoniol et al. [1] first proposed to adopt Information Retrieval (IR) techniques to solve the problem. Naive Bayes and TF-IDF [2] are used in their systems. More advanced approaches like Latent Semantic Indexing [8] are incorporated to increase the accuracy [19], [4]. The researchers focus more on exploiting the domain specific rules to further improve\ntheir systems [11], [14] and better visualize the results [4]. Some successful applications are summarized in [5]. These methods are effective at solving the problem of source code understanding, retracing of requirements and maintaining IT systems. But besides the rule-based systems, of which the methods can hardly be generalized to similar problems, most existing work regards the code as general text. The functional information of code structures are ignored. In this work, the functional information is encoded in the code features.\nCross Modal information retrieval aims to build up a bridge between heterogeneous media by linking their features, which is very similar to the task of the proposed HMLCR model in this work, linking the code features and text features in a shared new space. In [22], they predefined a concept dictionary and proposed a model to map the visual features to the concepts. A similar work is proposed in [21], while in this work the concepts are replaced with the automatically produced word clusters. In [32], [13], they seek to find the relatedness between the images and keywords. The aforementioned methods are designed for linking text and images, and the clustering of text and the clustering of image features are often separated: the clusters (latent topics) are first discovered and then the image features are mapped to the latent topics. Distance metric learning has also been used to transfer object recognition models to new domains, linking between visual features [30]. A more general research topic is to find a heterogeneous distance metric between objects in different spaces [25]. In [26], the joint graph regularization is incorporated. The heterogenous features are completely different in their task, while in our task, the code features and text features share useful information. The existing work on multimedia retrieval is not proper to be directly applied to the application, as the content information may be ignored."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this paper, based on the requirements of retrieval of software artifacts, we put forward a novel approach to calculate the similarity between code and text. We first formulated the problem and then proposed a novel feature extraction method for code programs. Two kinds of code features were proposed to exploit the functional and operational information of source codes, which are different from the textual features adopted by traditional methods. To calculate the similarity between codes and words, we introduced a novel heterogeneous distance metric learning approach to map the heterogeneous media into a unified space. Since the code features contain useful textual content, content-based regularization was further proposed to capture the content of source code. Content-based regularization distinguishes our work from the existing cross modal multimedia retrieval techniques. Data sets obtained from two open source projects were used in our experiments. Based on the real world data, such as bug reports, change logs and corresponding programs, experimental results validate the effectiveness of our proposed approach: the proposed code features and heterogeneous distance metric learning algorithms are helpful to improve the retrieval results, because both the proposed model and multimedia information retrieval methods outperform the traditional methods from the area of software engineering, which focuses on the textual part of programs; Content-based regularization proves to be useful, because it\nmakes the cross modal information retrieval method compatible with our problem. A case study was also presented to further explain how the proposed framework worked.\nIn this work, HMLCR is used to bridge between code and text in the same project. But in real applications, identical code features, such as frequently occurred code patterns and class references, may exist in more than one projects. Thus, we also explore the relationship between different projects written in same language, and even projects written in different programming languages by leveraging the same code features and the co-occurrence of code and textual features. As a fundamental part of software development life cycle, software artifact retrieval is especially vital to large scale software project. So we will extend our algorithm to larger and more complex data sets in the future."}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank the support of the National Natural Science Foundation of China 91224006, the Strategic Priority Research Program of Chinese Academy of Sciences XDA06010202 and XDA05050601), \u201c12th Five Year\u201d Plan for Science & Technology Support 2012BAK17B01 and 2013BAD15B02, the joint project by the Foshan and the Chinese Academy of Science under Grant No. 2012YS23, China National 973 program 2014CB340301."}], "references": [{"title": "Recovering traceability links between code and documentation", "author": ["G. Antoniol", "G. Canfora", "G. Casazza", "A. De Lucia", "E. Merlo"], "venue": "Software Engineering, IEEE Transactions on, 28(10):970\u2013983", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "et al", "author": ["R. Baeza-Yates", "B. Ribeiro-Neto"], "venue": "Modern information retrieval, volume 463. ACM press New York", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. De Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research, 3:1107\u20131135", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Extraction and visualization of traceability relationships between documents and source code", "author": ["X. Chen"], "venue": "Proceedings of the IEEE/ACM international conference on Automated software engineering, pages 505\u2013510. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Best practices for automated traceability", "author": ["J. Cleland-Huang", "R. Settimi", "E. Romanova", "B. Berenbach", "S. Clark"], "venue": "Computer, 40(6):27\u201335", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Hipikat: Recommending pertinent software development artifacts", "author": ["D. Cubranic", "G.C. Murphy"], "venue": "Software Engineering, 2003. Proceedings. 25th International Conference on, pages 408\u2013418. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Adams re-trace: traceability link recovery via latent semantic indexing", "author": ["A. De Lucia", "R. Oliveto", "G. Tortora"], "venue": "Proceedings of the 30th international conference on Software engineering, pages 839\u2013842. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "Journal of The American Society for Information Science and Technology, 41(6):391\u2013 407", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Heterogeneous metric learning for cross-modal multimedia retrieval", "author": ["J. Deng", "L. Du", "Y.-D. Shen"], "venue": "Web Information Systems Engineering\u2013WISE 2013, pages 43\u201356. Springer Berlin Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Cerberus: Tracing requirements to source code using information retrieval", "author": ["M. Eaddy", "A.V. Aho", "G. Antoniol", "Y.-G. Gu\u00e9h\u00e9neuc"], "venue": "dynamic analysis, and program analysis. In Program Comprehension, 2008. ICPC 2008. The 16th IEEE International Conference on, pages 53\u201362. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A scenario-driven approach to trace dependency analysis", "author": ["A. Egyed"], "venue": "Software Engineering, IEEE Transactions on, 29(2):116\u2013132", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "On the use of relevance feedback in ir-based concept location", "author": ["G. Gay", "S. Haiduc", "A. Marcus", "T. Menzies"], "venue": "Software Maintenance, 2009. ICSM 2009. IEEE International Conference on, pages 351\u2013360. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic image annotation and retrieval using cross-media relevance models", "author": ["J. Jeon", "V. Lavrenko", "R. Manmatha"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 119\u2013126. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Xtraque: traceability for product line systems", "author": ["W. Jirapanthong", "A. Zisman"], "venue": "Software & Systems Modeling, 8(1):117\u2013144", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimedia content processing through cross-modal association", "author": ["D. Li", "N. Dimitrova", "M. Li", "I.K. Sethi"], "venue": "Proceedings of the eleventh ACM international conference on Multimedia, pages 604\u2013611. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature location via information retrieval based filtering of a single scenario execution trace", "author": ["D. Liu", "A. Marcus", "D. Poshyvanyk", "V. Rajlich"], "venue": "Proceedings of the 22nd IEEE/ACM international conference on Automated software engineering, pages 234\u2013243. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Bug localization using latent dirichlet allocation", "author": ["S.K. Lukins", "N.A. Kraft", "L.H. Etzkorn"], "venue": "Information and Software Technology, 52(9):972\u2013990", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "volume 1. Cambridge University Press, Cambridge", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Recovering documentation-to-sourcecode traceability links using latent semantic indexing", "author": ["A. Marcus", "J.I. Maletic"], "venue": "Software Engineering, 2003. Proceedings. 25th International Conference on, pages 125\u2013135. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Augmented bug localization using past bug information", "author": ["B.D. Nichols"], "venue": "Proceedings of the 48th Annual Southeast Regional Conference, page 61. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "Proceedings of the international conference on Multimedia, pages 251\u2013260. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Bridging the gap: Query by semantic example", "author": ["N. Rasiwasia", "P.J. Moreno", "N. Vasconcelos"], "venue": "Multimedia, IEEE Transactions on, 9(5):923\u2013938", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernels and regularization on graphs", "author": ["A.J. Smola", "R. Kondor"], "venue": "Learning theory and kernel machines, pages 144\u2013158. Springer Berlin Heidelberg", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Locating user functionality in old code", "author": ["N. Wilde", "J.A. Gomez", "T. Gust", "D. Strasburg"], "venue": "Software Maintenance, 1992. Proceerdings., Conference on, pages 200\u2013205. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning similarity function between objects in heterogeneous spaces", "author": ["W. Wu", "J. Xu", "H. Li"], "venue": "Microsoft Research Technique Report", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Heterogeneous metric learning with joint graph regularization for cross-media retrieval", "author": ["X. Zhai", "Y. Peng", "J. Xiao"], "venue": "Twenty-Seventh AAAI Conference on Artificial Intelligence", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell", "A. Ng"], "venue": "Advances in neural information processing systems, pages 505\u2013512", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["Kilian Weinberger", "John Blitzer", "Lawrence Saul"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Information-theoretic metric learning", "author": ["Jason V. Davis", "Brian Kulis", "Prateek Jain", "Suvrit Sra", "Inderjit S. Dhillon"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Adapting Visual Category Models to New Domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Traceability", "author": ["J.D. Palmer"], "venue": "Software Requirements Engineering, Second Edition, IEEE Computer Society Press, pages 412\u2013422", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Experiences in discovering", "author": ["C. Jensen", "W. Scacchi"], "venue": "modeling, and reenacting open source software development processes. In Unifying the Software Process Spectrum, pages 449\u2013462. Springer Berlin Heidelberg", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Program slicing", "author": ["M. Weiser"], "venue": "Proceedings of the 5th international conference on Software engineering, pages 439\u2013449. IEEE Press", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1981}], "referenceMentions": [{"referenceID": 30, "context": "It gives essential support in understanding the relationships within and across software requirements, design and implementation [31].", "startOffset": 129, "endOffset": 133}, {"referenceID": 0, "context": "Along this stream of research, vector space model and stochastic language model are first adopted [1], [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Along this stream of research, vector space model and stochastic language model are first adopted [1], [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "Subsequently, several other information retrieval models, as well as their variants and ensembles, are also experimented to improve the accuracy of the software artifact retrieval task[7], [19], [4].", "startOffset": 184, "endOffset": 187}, {"referenceID": 18, "context": "Subsequently, several other information retrieval models, as well as their variants and ensembles, are also experimented to improve the accuracy of the software artifact retrieval task[7], [19], [4].", "startOffset": 189, "endOffset": 193}, {"referenceID": 3, "context": "Subsequently, several other information retrieval models, as well as their variants and ensembles, are also experimented to improve the accuracy of the software artifact retrieval task[7], [19], [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 27, "context": "Although homogeneous distance metric learning [28], [29] has been proposed to learn a good metric to compare two objects and has played a significant role in statistical classification and information retrieval, it requires the objects to share identical features and are comparable.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "Although homogeneous distance metric learning [28], [29] has been proposed to learn a good metric to compare two objects and has played a significant role in statistical classification and information retrieval, it requires the objects to share identical features and are comparable.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "Similar ideas have been proposed to solve the problem of cross modal multimedia retrieval, such as using text as queries to retrieve music and using text to retrieve pictures [21], [26], [3], [9].", "startOffset": 175, "endOffset": 179}, {"referenceID": 25, "context": "Similar ideas have been proposed to solve the problem of cross modal multimedia retrieval, such as using text as queries to retrieve music and using text to retrieve pictures [21], [26], [3], [9].", "startOffset": 181, "endOffset": 185}, {"referenceID": 2, "context": "Similar ideas have been proposed to solve the problem of cross modal multimedia retrieval, such as using text as queries to retrieve music and using text to retrieve pictures [21], [26], [3], [9].", "startOffset": 187, "endOffset": 190}, {"referenceID": 8, "context": "Similar ideas have been proposed to solve the problem of cross modal multimedia retrieval, such as using text as queries to retrieve music and using text to retrieve pictures [21], [26], [3], [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 32, "context": "A similar line of methods that are also designed for this goal of removing the useless expressions has been proposed and studied well in the area of software engineering, which is formally named as program slicing method [33].", "startOffset": 221, "endOffset": 225}, {"referenceID": 26, "context": "Distance metric learning [27] has attracted much attention in the last decade.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 162, "endOffset": 166}, {"referenceID": 12, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 168, "endOffset": 172}, {"referenceID": 24, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "In the area of cross modal multimedia retrieval, researchers proposed heterogeneous distance metric learning methods to compare different media [22], [21], [26], [15], [13], [25], [3], [9].", "startOffset": 185, "endOffset": 188}, {"referenceID": 22, "context": "It improves the smoothness of the mappings by penalizing the functions that change abruptly on the joint data graph [23].", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": "Note that the matrices U and V are initialized (line 1) based on the Cross-modal Factor Analysis algorithm[15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 31, "context": "The approach of using bug reports and change logs is frequently adopted in the area of software engineering, which is based on change reenactment [32].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Another measure method we employ is the normalized Discounted Cumulative Gain(nDCG)[18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "\u2022 COS: The cosine similarity based method [1], [19] which calculates the cosine similarity between the textual representation of code and queries.", "startOffset": 42, "endOffset": 45}, {"referenceID": 18, "context": "\u2022 COS: The cosine similarity based method [1], [19] which calculates the cosine similarity between the textual representation of code and queries.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "\u2022 LM: This method adopts language modeling to calculate the similarity between the textual representation of code and queries [1], [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "\u2022 LM: This method adopts language modeling to calculate the similarity between the textual representation of code and queries [1], [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "\u2022 LSI: The latent semantic indexing method that first compresses the textual representation of code and queries and then calculate their similarity [7], [25].", "startOffset": 148, "endOffset": 151}, {"referenceID": 24, "context": "\u2022 LSI: The latent semantic indexing method that first compresses the textual representation of code and queries and then calculate their similarity [7], [25].", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "\u2022 CFA: Cross-modal Factor Analysis model, which is first proposed in [15] and is designed to discover the associations between the feature space of different media.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Retrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20].", "startOffset": 186, "endOffset": 189}, {"referenceID": 23, "context": "Retrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20].", "startOffset": 206, "endOffset": 210}, {"referenceID": 11, "context": "Retrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20].", "startOffset": 249, "endOffset": 253}, {"referenceID": 15, "context": "Retrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20].", "startOffset": 267, "endOffset": 271}, {"referenceID": 9, "context": "Retrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20].", "startOffset": 314, "endOffset": 318}, {"referenceID": 16, "context": "Retrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20].", "startOffset": 363, "endOffset": 367}, {"referenceID": 19, "context": "Retrieving the code files by a query written in natural languages is first introduced in the area of software engineering and has been applied in a variety of tasks, such as development [6] and maintenance [24] of software, localization of concepts [12] and features [16], tracing requirements back to source code [10] and identify the corresponding code of a bug[17], [20].", "startOffset": 369, "endOffset": 373}, {"referenceID": 0, "context": "[1] first proposed to adopt Information Retrieval (IR) techniques to solve the problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Naive Bayes and TF-IDF [2] are used in their systems.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "More advanced approaches like Latent Semantic Indexing [8] are incorporated to increase the accuracy [19], [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "More advanced approaches like Latent Semantic Indexing [8] are incorporated to increase the accuracy [19], [4].", "startOffset": 101, "endOffset": 105}, {"referenceID": 3, "context": "More advanced approaches like Latent Semantic Indexing [8] are incorporated to increase the accuracy [19], [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "The researchers focus more on exploiting the domain specific rules to further improve their systems [11], [14] and better visualize the results [4].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "The researchers focus more on exploiting the domain specific rules to further improve their systems [11], [14] and better visualize the results [4].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "The researchers focus more on exploiting the domain specific rules to further improve their systems [11], [14] and better visualize the results [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 4, "context": "Some successful applications are summarized in [5].", "startOffset": 47, "endOffset": 50}, {"referenceID": 21, "context": "In [22], they predefined a concept dictionary and proposed a model to map the visual features to the concepts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "A similar work is proposed in [21], while in this work the concepts are replaced with the automatically produced word clusters.", "startOffset": 30, "endOffset": 34}, {"referenceID": 31, "context": "In [32], [13], they seek to find the relatedness between the images and keywords.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [32], [13], they seek to find the relatedness between the images and keywords.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "Distance metric learning has also been used to transfer object recognition models to new domains, linking between visual features [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "A more general research topic is to find a heterogeneous distance metric between objects in different spaces [25].", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "In [26], the joint graph regularization is incorporated.", "startOffset": 3, "endOffset": 7}], "year": 2014, "abstractText": "The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.", "creator": "LaTeX with hyperref package"}}}