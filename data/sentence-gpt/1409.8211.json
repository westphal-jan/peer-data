{"id": "1409.8211", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2014", "title": "Efficient multivariate sequence classification", "abstract": "Kernel-based approaches for sequence classification have been successfully applied to a variety of domains, including the text categorization, image classification, speech analysis, biological sequence analysis, time series and music classification, where they show some of the most accurate results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 29 Sep 2014 18:03:22 GMT  (193kb,D)", "https://arxiv.org/abs/1409.8211v1", "multivariate sequence classification, string kernels, vector quantization, direct feature quantization, music classification, protein classification"], ["v2", "Tue, 30 Sep 2014 14:46:42 GMT  (197kb,D)", "http://arxiv.org/abs/1409.8211v2", "multivariate sequence classification, string kernels, vector quantization, direct feature quantization, music classification, protein classification"]], "COMMENTS": "multivariate sequence classification, string kernels, vector quantization, direct feature quantization, music classification, protein classification", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pavel p kuksa"], "accepted": false, "id": "1409.8211"}, "pdf": {"name": "1409.8211.pdf", "metadata": {"source": "CRF", "title": "Efficient multivariate kernels for sequence classification", "authors": ["Pavel P. Kuksa"], "emails": ["pavel@pkuksa.org"], "sections": [{"heading": null, "text": "Typical kernel functions for sequences in these domains (e.g., bag-of-words, mismatch, or subsequence kernels) are restricted to discrete univariate (i.e. one-dimensional) string data, such as sequences of words in the text analysis, codeword sequences in the image analysis, or nucleotide or amino acid sequences in the DNA and protein sequence analysis. However, original sequence data are often of real-valued multivariate nature, i.e. are not univariate and discrete as required by typical k-mer based sequence kernel functions.\nIn this work, we consider the problem of the multivariate sequence classification (e.g., classification of multivariate music sequences, or multidimensional protein sequence representations). To this end, we extend univariate kernel functions typically used in sequence domains and propose efficient multivariate similarity kernel method (MVDFQ-SK) based on (1) a direct feature quantization (DFQ) of each sequence dimension in the original real-valued multivariate sequences and (2) applying novel multivariate discrete kernel measures on these multivariate discrete DFQ sequence representations to more accurately capture similarity relationships among sequences and improve classification performance.\nExperiments using the proposed MVDFQ-SK kernel method show excellent classification performance on three challenging music classification tasks as well as protein sequence classification with significant 25-40% improvements over univariate kernel methods and existing state-of-the-art sequence classification methods. Keywords: multivariate sequence classification, string kernels, vector quantization, direct feature quantization, music classification, protein classification"}, {"heading": "1. INTRODUCTION", "text": "Large-scale sequence analysis has become an important task in data mining inspired by numerous applications such as the document and text classification or the analysis of time series, music data, or biological sequences. Classification of string data, i.e. univariate sequences of discrete symbols (such as words, amino acids, codewords), has at-\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\ntracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].\nIn particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].\nIn a kernel-based framework, the classification of the sequenceX = x1, . . . , xnX is based on a kernel functionK(X,Y ) which is computed to measure the similarity between pairs of sequences X and Y . For instance, given a set of positive training instances C+ and a set of negative training instances C\u2212, the SVM [32] learns a classification function of the following form\nf(X) = \u2211\nY +\u2208C+\n\u03b1 +\nY +K(X,Y\n+ )\u2212 \u2211\nY \u2212\u2208C\u2212\n\u03b1 \u2212\nY \u2212K(X,Y\n\u2212 )\n(1) Typical k-mer-based kernel functions (e.g., mismatch or spectrum kernels, gapped and wildcard kernel functions [17, 22]) essentially rely on symbolic Hamming-distance based matching of one-dimensional (1D) k-mers (k-long substrings) in the input sequences. For example, given one-dimensional (1D) sequences X and Y over alphabet \u03a3 (e.g., amino acid sequences with |\u03a3|=20), the spectrum-k kernel [18] and the mismatch-(k,m) kernel [19] measure similarity between sequences as\nK(X,Y |k,m) = \u3008\u03a6k,m(X),\u03a6k,m(Y )\u3009 = \u2211 \u03b3\u2208\u03a3k \u03a6k,m(\u03b3|X)\u03a6k,m(\u03b3|Y )\n= \u2211 \u03b3\u2208\u03a3k (\u2211 \u03b1\u2208X Im(\u03b1, \u03b3) )\u2211 \u03b2\u2208Y Im(\u03b2, \u03b3)  = \u2211 \u03b1\u2208X \u2211 \u03b2\u2208Y \u2211 \u03b3\u2208\u03a3k Im(\u03b1, \u03b3)Im(\u03b2, \u03b3) (2)\nwhere\n\u03a6k,m(\u03b3|X) = (\u2211 \u03b1\u2208X Im(\u03b1, \u03b3) ) (3)\nis the number of occurrences (possibly with up to m mismatches) of the k-mer \u03b3 in X, and the matching/indicator function Im(\u03b1, \u03b3) = 1 if \u03b1 is in the mutational neighborhood Nk,m(\u03b3) of \u03b3, i.e. \u03b1 and \u03b3 are at the Hamming distance of at most m. This kernel (Eq. 2) essentially amounts to a cumulative Hamming-distance based pairwise comparison of\nar X\niv :1\n40 9.\n82 11\nv2 [\ncs .L\nG ]\n3 0\nSe p\n20 14\nall k-mers \u03b1 and \u03b2 contained in sequences X and Y , respectively, with maximum number of mismatches m. The level of similarity of each pair of substrings (\u03b1, \u03b2) here is indicated by the number of identical substrings in the mutational neighborhoods Nk,m(\u03b1) and Nk,m(\u03b2) of \u03b1 and \u03b2,\u2211 \u03b3\u2208\u03a3k Im(\u03b1, \u03b3)Im(\u03b2, \u03b3) (Eq. 2). For the spectrum kernel (m=0), this similarity level is simply the exact matching of \u03b1 and \u03b2.\nOn the other hand, in many practical applications input sequences are multivariate, i.e. input data is in the form of sequences of R-dimensional real-valued feature vectors, as opposed to one-dimensional discrete strings. This is the case, for instance, in commonly used MFCC representations for music data as series of 13-dimensional MFCC feature vectors (e.g., [31, 20, 16]) extracted from short time segments, or 20-dimensional profile representations of protein sequences as series of probabilistic amino acid substitution vectors in biological sequence analysis [11, 8].\nSuch original, multivariate real-valued feature sequences are typically transformed into univariate sequences in order to apply a univariate string kernel method such as spectrum or mismatch, e.g., [23, 37, 14, 12]. For example, this transformation is frequently accomplished by applying a vector quantization (VQ) algorithm to feature vectors thus transforming a multivariate R-dim real-valued sequence into a discrete univariate codeword sequence.\nIn contrast, in this work we consider an alternative approach to real-valued multivariate sequence classification which directly exploits these richer multivariate (R-dimensional) sequences (e.g., MFCC feature sequences for music, or sequences of physico-chemical amino acid descriptors for proteins). In this approach, the R-dimensional multivariate sequences are considered as R\u00d7|X| feature - spatio/temporal matrices with rows corresponding to feature dimensions and columns corresponding to temporal or spatial coordinates. Using these representations, we propose an efficient discrete multivariate kernel method (MVDFQ-SK) based on (1) a direct feature quantization (DFQ) (Sec. 3.1) of the original multivariate sequence, and (2) novel manifold-based discrete multivariate kernel functions applied to these discrete DFQ representations (Sec. 3.2, 3.3). The developed approach is applicable to a wide range of sequence domains, both discrete- and real- valued, such as music, images, or biological sequences.\nExperiments using the new multivariate direct feature quantization kernels (MVDFQ-SK) kernels on music genre and artist recognition, as well as protein sequence classification tasks show excellent predictive performance (Sec. 4) with significant 25%-40% improvements in predictive accuracy over univariate kernel functions and a number of other stateof-the-art sequence classification methods."}, {"heading": "2. RELATED WORK", "text": "Recently, a large variety of methods have been proposed to solve the sequence classification problem, including generative, such as HMMs, or discriminative approaches. Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.\nIn the kernel-based approaches, the similarity between sequences X and Y is frequently computed based on the cooccurrence of string features (e.g., k-mers), as in spectrum kernels [18] or substring kernels [34]. Inexact comparison\nof the sequences in this framework is typically achieved using different families of mismatch [19] or profile [11] kernels. Both spectrum-k and mismatch-(k,m) kernels directly extract string features (k-mers) from the observed sequence, X. On the other hand, the profile kernel, proposed by Kuang et al. in [11], first builds a 20 \u00d7 |X|-dim profile [8] PX and then derives a similar |\u03a3|k-dimensional representation from PX . Such profile representations have been shown to perform well in protein sequence analysis [11, 24]. Constructing the profile for each sequence may not be practical in some application domains, since the size of the profile is dependent on the size of the alphabet set, as well as the difficulty of defining a general sequence similarity search algorithm (e.g., as PSI-BLAST) for non-biological sequence domains. While for bio-sequences |\u03a3| = 4 or 20, for music or text classification |\u03a3| can potentially be very large, on the order of tens of thousands of symbols.\nThe existing string kernel methods essentially amount to the analysis of univariate (i.e. one-dimensional) sequences over finite alphabets \u03a3 with one-dimensional k-mers as basic sequence features. However, original input sequences are often in the form of sequences of feature vectors, i.e. each input sequence X is a sequence of identically sized (R-dim) feature vectors which could be considered as a R\u00d7|X| feature matrix.\nExamples of these multivariate feature sequences include\n\u2022 Music data. A music sequence X in the commonly used MFCC feature representation [16, 20] is a sequence of 13-dimensional MFCC feature vectors, i.e. a multivariate sequence of size 13\u00d7 |X|. \u2022 Image data. An image can be considered as a multi-\nvariate sequence of feature vectors extracted from image patches (e.g., as in [23]);\n\u2022 Biological data. Protein sequences can be viewed as profiles [11], or as multivariate sequences of R-dim feature vectors describing physical/chemical properties of individual amino acids [30].\nWhile typical string kernel methods essentially use symbolic Hamming distance-based matching (as in Eq. 2), recent work in [13] introduced the so-called generalized similarity (non-Hamming) kernels that allow to incorporate general similarity metrics S(\u00b7, \u00b7) into similarity evaluation and improve performance compared to symbolic Hamming distancebased matching [13]. In particular, most related to the current work, are the distance-preserving symbolic embedding kernels [13] which use similarity hashing [35] to obtain binary representations for sequences such that Hamming distance h(\u00b7, \u00b7) between these binary representations is proportional to the original similarity score S(\u00b7, \u00b7) [13]. In contrast, the direct feature quantization (DFQ) method proposed in this work results in more accurate non-binary representations that are simpler as they do not require Hamming embedding learning step as in [13], and display higher accuracy (see Experiments, Sec. 4) compared to the binary Hamming-based distance-preserving embedding.\nRelated methods for the time series classification have also been introduced and include a large variety of methods, e.g., kernels on dynamical systems [33], or alignment-based methods [6, 28] with a quadratic time complexity. We will compare in the experiments with a number of these methods for time series.\nIn this work, in contrast to kernel methods on univariate string representations, we aim at methods that directly exploit multivariate sequence representations to improve accuracy and propose a family of efficient, linear-time discrete multivariate similarity kernels (MVDFQ-SK, MVDFQM-SK) using direct feature quantization (DFQ) and manifold kernel embedding (Sec. 3.1, 3.3). We show empirically (Sec. 4) that proposed MVDFQ/MVDFQM kernels and manifold embedding (Sec. 3.3) provide effective improvements in practice over traditional univariate (1D) VQ-based sequence kernels, binary similarity-preserving embedding kernels [13], as well as other state-of-the-art sequence classification methods for a number of challenging classification problems."}, {"heading": "3. MULTIVARIATE DIRECT FEATURE QUANTIZATION METHOD", "text": "In a typical sequence classification setting, string kernels are restricted to the univariate string data, e.g., word sequences in text analysis, amino acid sequences in the biological sequence analysis, or codeword sequences in the time series analysis [23, 22, 14, 19].\nIn order to apply these univariate kernel functions to multivariate (R-dimensional) sequences, individual feature vectors at each position in the sequence in the widely used codebook learning framework are first encoded using codebook IDs (Figure 1), then standard univariate string kernel methods can be applied on these discrete codeword sequence representations (see e.g., [23, 14]).\nXWednesday, January 25, 2012\nAs illustrated in the Fig. 1, the R-dim features vectors from input sequences are first quantized (clustered) to obtain a codebook C, a set of codebook (prototype) vectors, C = {C1, C2, ..., CN}, for instance, by applying a Vector Quantization (VQ) algorithm. Then a multivariate input sequence X, a sequence of n = |X| identically sized (realvalued) R-dimensional feature vectors,\nX = (x1, x2, . . . , xn), xi \u2208 RR \u2200i\nis encoded as a univariate (1D) discrete sequence c(X) of codebook IDs\nc(x) = (c1, c2, . . . , cn), ci \u2208 {1 . . . D} \u2200i\nby mapping each of the vectors xi to the nearest codeword vector ci in the codebook C = {C1, C2, . . . , CD}. The resulting codeword sequence c(x) is essentially a discrete sequence\nover finite alphabet \u03a3 = {1, . . . , D}. Univariate (1D) string kernels can then be used for classification with SVM.\nIn contrast to these commonly employed codebook-based univariate representations, in this work we consider an alternative multivariate direct quantization which preserves feature information for each dimension with the manifold embedding representations (MVDFQ/MVDFQM) of the original multivariate (continuous-valued) sequences.\nIn the following, we first describe the direct feature quantization (DFQ) representations (Sec. 3.1) and contrast them with vector quantization / codebook based representations. We then define a novel family of kernels on these multivariate DFQ representations (Sec. 3.2)."}, {"heading": "3.1 Direct feature quantization", "text": "A multivariate direct feature quantization (MVDFQ) representation of the original continuous-valued multivariate sequence\nX = (x1, x2, . . . , xn), xi \u2208 RR\nis obtained by the direct quantization of each of the R feature dimensions. In this approach, each jth feature f j , j = 1 . . . R is quantized by dividing its range (f jmin, f j max) into the finite number of intervals, B. In the simplest case, the intervals can be defined, for instance, using a uniform quantization with a pre-specified number of bins B, where the entire feature data range is divided into B equal intervals of length \u03b4 = (fmax \u2212 fmin)/B and the index of the quantized feature value Q(f) = b(f \u2212 fmin)/\u03b4c is used to represent the feature value f .\nPartitioning of the feature data range could also be obtained by using 1D clustering, e.g., k-means, to adaptively choose dicretization levels and the number of bins for each dimension. Discretization levels also can be chosen using, for example, Gaussian distribution assumption (see, e.g., [21]) as breakpoints under Gaussian curve producing equal-sized areas.\nVarying the number of quantization levels B will result in more accurate (larger B) or more coarse (smaller B) representation of the original real-valued data. Here we choose appropriate number of quantization levels B using a small scale cross-validation experiments on the subset of the training data.\nFigure 2 shows an example of a DFQ representation for the 3-dimensional time series X (R=3) where the 3-dimensional DFQ representation has been obtained using a uniform binning (B=64) along each of the three data dimensions. As can be seen from the figure, compared to the vector quantization approach, the DFQ retains feature values along each dimension, thus providing a more accurate description of the original real-valued sequence.\nWe will show in the experiments that using DFQ multivariate representations and MVDFQ kernels described below can significantly (by 25-40%) improve predictive accuracy compared to traditional 1D (univariate) kernel representations as well as other state-of-the-art approaches (Sec. 4)."}, {"heading": "3.2 Multivariate Direct Feature Quantization Similarity Kernels", "text": "In the following, we first define an efficient multivariate DFQ similarity kernel (MVDFQ-SK)K(DFQ(X), DFQ(X)) for the direct feature quantization (DFQ) representation defined in Sec. 3.1. We then present MVDFQ with the man-\nWednesday, September 12, 12\nifold embedding (MVDFQM) in Sec. 3.3 that as we show experimentally further improve predictive ability of the classifiers on a number of challenging tasks and datasets.\nTo compute similarity between two multivariate sequences X and Y , we propose a kernel function defined as\nKMVDFQ(DFQ(X), DFQ(Y )) =\u2211 \u03b1R\u00d7k\u2208DFQ(X) \u2211 \u03b2R\u00d7k\u2208DFQ(Y ) K(\u03b1R\u00d7k, \u03b2R\u00d7k) (4)\nwhere \u03b1R\u00d7k and \u03b2R\u00d7k are R \u00d7 k submatrices contained in DFQ(X) and DFQ(Y ) and K(\u03b1R\u00d7k, \u03b2R\u00d7k) is a kernel function defined for measuring similarity between two R\u00d7k submatrices. Similarly to k-mer based kernel functions (e.g., Eq. 2), this kernel function essentially computes the similarity between sequences by a cumulative comparison of all pairs of R \u00d7 k submatrices contained in DFQ(X) and DFQ(Y ) using a submarix kernel function K(\u00b7, \u00b7).\nOne natural definition for the submatrix kernel K(\u00b7, \u00b7) is cumulative row-based comparison\nK(\u03b1R\u00d7k, \u03b2R\u00d7k) = R\u2211 r=1 I1\u00d7k(\u03b1 r R\u00d7k, \u03b2 r R\u00d7k) (5)\nwhere I1\u00d7k(\u00b7, \u00b7) is a similarity/indicator function for matching 1D rows \u03b1rR\u00d7k and \u03b2 r R\u00d7k. The matching function I1\u00d7k(\u00b7, \u00b7) could be defined as I1\u00d7k(\u03b1, \u03b2) = 1 if d(\u03b1, \u03b2) \u2264 m, and 0 otherwise (similar to the mismatch kernel).\nIn the experiments, we use the state-of-the-art spectrum, mismatch [19], and spatial sample (SSSK) [14] kernel functions as our one-dimensional row matching function\nI1\u00d7k(\u03b1 r R\u00d7k, \u03b2 r R\u00d7k)\nin Eq. 5, which results in corresponding multivariate DFQ spectrum, mismatch, and spatial sample kernels (referred as MVDFQ-Spectrum, MVDFQ-Mismatch, and MVDFQSSSK, respectively).\nIntuitively, according to the kernel definition (Eq. 5), similar R \u00d7 k submatrices (i.e. submatrices with many similar rows) will result in a large kernel value K(\u00b7, \u00b7).\nUsing Eq. 5, the multivariate DFQ kernel in Eq. 4 can be\nwritten as\nKMVDFQ(X,Y ) =\nR\u2211 r=1 \u2211 \u03b1R\u00d7k\u2208DFQ(X) \u2211 \u03b2R\u00d7k\u2208DFQ(Y ) I1\u00d7k(\u03b1 r R\u00d7k, \u03b2 r R\u00d7k) (6)\nwhich can be efficiently computed by running the corresponding kernel with a 1D k-mer matching function I1\u00d7k(\u00b7, \u00b7) B times, i.e. for each row b = 1 . . . R. The overall complexity of evaluating multivariate kernelKMVDFQ(DFQ(X), DFQ(Y )) for two R-dim DFQ sequences DFQ(X) and DFQ(Y ) is then O(R \u00b7k \u00b7n), i.e. is linear in the sequence length n = |X| and the number of dimensions R."}, {"heading": "3.3 Manifold embedding", "text": "While typical string kernel methods assume Euclidean feature space and use Euclidean distance, a probabilistic manifold assumption on the geometry of the data space could be more natural and effective (see e.g. [38, 10]). Given d-dim feature representation of a sequence, \u03a6(X) = (\u03c61(X), . . . , \u03c6d(X)), the sequence X can be considered as a point on the multinomial manifold using L1 embedding of \u03a6(X):\n\u03a6\u0302(X) = ( \u03c61(X)\u2211 i \u03c6i(X) , . . . , \u03c6d(X)\u2211 i \u03c6i(X) ) (7)\nwhere, e.g., in the simple k-mer frequency representation, \u03c6i(X) = f(ki, X), the frequency of k-mer ki in sequence X.\nThen, a natural measure of affinity between the distributions \u03a6\u0302(X) and \u03a6\u0302(Y ) on the multinomial manifold is a Bhattacharyya affinity [2], i.e.\nKmanifold(X,Y ) =< \u221a \u03a6\u0302(X), \u221a \u03a6\u0302(Y ) >\n= \u2211 i \u221a \u03c6\u0302i(X) \u221a \u03c6\u0302i(Y )\nUsing the equation above and equations for MVDFQ Eq. 6, we obtain the MVDFQ kernel with the manifold embedding (MVDFQM):\nKMVDFQM (X,Y ) = R\u2211 r=1 \u2211 \u03b3 \u221a \u03c6r\u03b3(X) \u221a \u03c6r\u03b3(Y ) (8)\nwhere \u03b3 \u2208 {1, . . . , B}K is a k-mer over the discretization\nalphabet \u03a3 = {1, . . . , B} and \u03c6r\u03b3(X) = \u2211\n\u03b1r R\u00d7k\u2208DFQ(X)\nI(\u03b1rR\u00d7k, \u03b3) (9)\nis the number of occurrences of \u03b3 in the r-th dimension/row of X.\nIn the experiments, we test the manifold embedding with MVDFQ kernel as well as other standard string kernels (we will refer to the MVDFQ with the manifold embedding as MVDFQM, and to the standard VQ kernels with the manifold embedding as VQ-M)."}, {"heading": "3.4 Advantages of multivariate DFQ", "text": "The proposed multivariate DFQ kernel method has the following merits:\n\u2022 It improves the predictive ability of typical discrete univariate kernel methods with VQ by applying them jointly to multiple discrete sequences obtained from direct discretization of each data dimension of the original real-valued multidimensional sequence.\n\u2022 Unlike the state-of-the-art approach of quantizing highdimensional data samples into codewords, it allows for classifier to learn importance of each feature for classification, as the significance of each data dimension for classification can be different.\n\u2022 It does not rely on clustering or binary similarity-preserving hashing techniques (e.g., as in [13]) as it directly discretizes the feature space using, e.g., uniform binning or adaptive clustering algorithm (k-means).\n\u2022 It has a low computational cost as it runs in linear time and is scalable to large sequence data sets.\n\u2022 It can be used with any of the existing univariate sequence kernels (mismatch/spectrum [19], kernels [14], gapped/subsequence kernels [22, 17], etc) to improve performance."}, {"heading": "4. EXPERIMENTAL EVALUATION", "text": "We study the performance of our methods in terms of the predictive accuracy and the running time on a number of challenging sequence classification problems using standard benchmark datasets for the music genre classification and artist recognition, as well as protein sequence analysis."}, {"heading": "4.1 Datasets and experimental setup", "text": "We test proposed methods on a number of multi-class sequence classification tasks:\n1. 10-class music genre classification1. This dataset is a reference music genre recognition dataset introduced in [31, 20]. It contains 1000 30-sec song fragments grouped into 10 genres (blues, rock, classical, etc.), with each genre represented by 100 songs. The task here is to correctly predict the genre of the musical sample.\n2. 6-class music genre recognition (ISMIR contest2). This is a benchmark music genre recognition task with samples classified into 6 genres.\n1http://opihi.cs.uvic.ca/sound/genres 2http://ismir2004.ismir.net/genre contest/index.htm\n3. 20-class music artist identification (artist20 dataset3). This benchmark dataset contains songs from 120 albums (6 albums per artist) with the task of correctly identifying artist for songs from previously unseen albums.\n4. protein remote homology detection (7329 sequences, 54 experiments) [36, 24]. The task here is to correctly infer membership of a given protein in protein superfamilies.\nTable 1 provides details of the datasets used in the experiments.\nFor all music classification tasks input sequences are multivariate sequences of 13-dimensional MFCC feature vectors."}, {"heading": "4.2 Baseline methods", "text": "We compare the proposed multivariate direct feature quantization (MVDFQ) kernel approach for the multivariate sequence classification to three related baselines:\n1. Traditional vector quantization (VQ) approaches with univariate string kernels. This approach has been used in a number of previous studies for the image categorization, text analysis, music classification (see e.g., [23, 15, 14]).\n2. A similarity hashing-based kernel approach described recently in [13] (a Euclidean similarity-preserving binary Hamming embedding of original real-valued MFCC feature vectors).\n3. A multivariate VQ approach using multiple codebooks of different sizes. In this approach, VQ representations are stacked to obtain essentially a multivariate VQ codebook sequence representation.\nWe also compare with a number of other state-of-the-art methods specifically developed for the music sequence classification, namely multivariate autoregressive models [26], multilinear models [25], as well as methods with more problemspecific and sophisticated features (aggregate Adaboost [1], classifier fusion with rich spectral and cepstral features [16], non-negative matrix factorization-based approaches [9]).\nWe test our methods using the state-of-the-art spectrum/mismatch [17] and spatial (SSSK) [14] kernels as our basic univariate kernels, i.e. to implement row matching functions I1\u00d7k(\u00b7, \u00b7) in Eq. 4).\nWe use Support Vector Machines (SVMs) classifiers with all kernels.\nWe also explore the performance impact of varying the number of codewords, discretization bins, a discretization algorithm (uniform, k-means)."}, {"heading": "4.3 Evaluation measures", "text": "For music genre classification experiments a standard 5- fold cross-validation procedure is used as in previous studies [20, 1] to evaluate classification performance. For music artist recognition, we follow a 6-fold leave-one-album-out validation procedure proposed in the previous work [7].\nWe report average multi-class classification errors as well as F1 scores for all tasks.\n3http://labrosa.ee.columbia.edu/projects/artistid/"}, {"heading": "4.4 Parameters and settings", "text": "For the vector quantization (VQ) models, we construct codebooks with 2048 codewords from input MFCC vectors. For the multiple-codebook VQ method, we use codebooks with 1024, 2048, an 4096 codewords.\nWe test our direct feature quantization approach using (1) a uniform quantization of each feature dimension into a fixed number of bins (B=32) and (2) using k-means clustering along each dimension to adaptively select quantization levels and the number of bins per dimension. The number of bins have been found from an initial cross-validation on the subset of training data.\nDuring the testing (classification), for input values outside of the (fmin, fmax) range, we use special values of 0 and B+1 for values smaller than fmin or larger than fmax.\nFor the discrete embedding with similarity hashing [13], we set the number of bits E = 32 which has been found to perform well in [13].\nThe length of k-mers used in the MVDFQ-spectrum/mismatch kernels has been set to k=6 and and the number of mismatches is set as m=1 for the mismatch kernels (these values have been selected using cross-validation on the subset of the training data).\nFor the univariate kernels on codeword (VQ) settings, the best settings of the k-mer length and the number of mismatches are similar, k = 5\u2212 6 and m=1,2.\nFor the spatial sample kernels (SSSK) [14] we use the spatial kernel with three (t=3) k=1-mer features and the maximum distance parameter d=5.\nAll experiments are performed on a single 2.8GHz CPU. The datasets used in our experiments and the supplementary data/code are available at http://pkuksa.org/~pkuksa/mvdfq.html."}, {"heading": "4.5 Music genre recognition", "text": "We first compare the proposed multivariate DFQ kernel approaches (MVDFQ) (Sec. 3.2, 3.3) with the vector quantization-based univariate kernels, and the recently proposed binary Hamming similarity hashing kernels [13].\nAll of the methods are compared with and without the proposed manifold embedding (Sec. 3.3).\nAs shown in Table 2, on a widely used benchmark dataset for music genre recognition [20, 31] (10 genres, each with\n100 sequences), proposed multivariate DFQ kernels improve over traditional univariate VQ kernels, as well as recently proposed binary Hamming similarity hashing kernels [13]. Using direct feature quantization (DFQ) kernels effectively improves accuracy compared to the VQ, and the similarity hashing approach for all basic kernels (spectrum, mismatch, and sparse spatial sample kernels (SSSK)). For instance, MVDFQ-SSSK achieves a significantly lower error rate of 22.6% compared to 31.1% using VQ or 25.9% using similarity hashing (27% and 13% improvements, respectively).\nUsing the k-means clustering for the discretization with DFQ results in the performance similar to the uniform quantization (e.g., MVDFQ-Spectrum and MVDFQ-SSSK with k-means achieve slightly lower errors of 22.8% and 16.9% compared to 23.0% and 17.2% with the uniform quantization).\nWe also note that using the manifold embedding further reduces error for all of the methods, including the VQ-based and similarity hashing kernels. These consistent improvements in accuracy across all of the methods, could be attributed to the manifold embedding effectively exploiting intrinsic geometric structure of music data. Overall, MVDFQ with manifold embedding (MVDFQM-SSSK) achieves the best error rate of 17% compared to the the best 23% error rate of similarity hashing kernel or 25.0% using VQ. (24% and 30% relative improvements in the error rates, respectively).\nWe also compare with previous best results on this music genre recognition dataset and baselines in Table 3, including multivariate autoregressive models [26], wavelet-based DWCH [20] method, aggregate AdaBoost [1], approaches specifically developed for the music classification that also use many other features in addition to MFCC. As can be seen from the table, using multivariate DFQ kernels (MVDFQSK) compares well with the state-of-the-art results (e.g., AdaBoost method [1] with much richer feature set).\nThe proposed MVDFQM method is also more effective than using the multiple codebook VQ method. We also note that expanding feature set by adding a set of 64 FFT features to the 13 MFCC feature set (i.e. 77\u00d7|X| multivariate representation), could further increase accuracy to 86.4% compared to that of 82.7% with MFCC features alone (Table 3).\nWe also note the utility of the multivariate direct feature quantization representation and kernel (MVDFQ) as opposed to the univariate representations: the univariate kernel on the one-dimensional (1D) sequence obtained from the DFQ multivariate sequence by encoding each R=13-dim feature vector as one codeword c = \u2211R i Q(fi)B i\u22121, i.e. using alphabet size |\u03a3| = BR, gives a higher error rate of 22% compared to that of 17.3% when using the multivariate DFQ kernel with the manifold embedding (MVDFQM).\nWe observe similar overall improvements for multivariate MVDFQ-SK kernels on another benchmark dataset (ISMIR2004 genre contest), Table 4. For instance, using the MVDFQ-SK string kernel with uniform direct feature quantization (MFCC only) reduces the error rate to 16.7% compared to that of 19.6% when using VQ with the univariate kernel.\nAs also can be seen from the Table 5, obtained error rates (16-17%) for ISMIR genre recognition compare well with a number of previous best results, including the recent nonnegative matrix factorization method [9](16.5% error), or complex auditory model and cortical representation of [25] (19% error)."}, {"heading": "4.6 Artist recognition", "text": "We also illustrate the utility of our multivariate MVDFQSK kernels and representations on the multi-class artist identification problem using the standard artist20 dataset with 20 artists, 6 albums each (1413 tracks total). Table 6 lists results for the 6-fold album-wise cross-validation with one album per artist held out for testing. Using multivariate MVDFQ-SK kernels with the direct uniform quantization of MFCC features yields a much lower 25.7% error compared to the best error rate for 42.9% for univariate kernels (a 40% relative improvement in error)."}, {"heading": "4.7 Protein remote homology detection", "text": "In Table 7, we compare our proposed multivariate DFQ string kernel method (using 20-dim BLOSUM rows as feature vectors for individual amino acids, i.e. 20\u00d7 |X| multivariate sequences) with a number of state-of-the-art kernel methods for the remote homology detection including spectrum/mismatch kernels [19, 18], spatial sample kernels [14], similarity hashing kernels [13], as well as recently proposed spectrum-RBF and mismatch-RBF methods [30] which also incorporate physico-chemical descriptors.\nAs can be seen from results in Table 7, multivariate DFQ string kernel (MVDFQ) provides effective improvements over other methods. For instance, using MVDFQ spectrum and mismatch kernels with BLOSUM substitution profiles sig-\nnificantly improves average ROC50 scores from 27.91 and 41.92 to 43.29 and 49.17, respectively (relative improvements of 50% and 17%), compared to traditional univariate spectrum/mismatch approaches."}, {"heading": "4.8 Running time", "text": "In Table 8, we compare the classification performance and the running time of our method to the recent binary Hamming embedding (similarity hashing) [13], and vector quantization-based univariate kernels. We vary the dimensionality of the embedding space E, the codebook size, and the number of discretization bins B, respectively. We note that for mismatch-(k,m) kernel computation we use the linear time sufficient-statistic based algorithm from [12]) and for all other methods we use their existing state-of-the-art implementations.\nAs can be seen from Table 8, multivariate kernels with the direct feature quantization (MVDFQ) display a better performance compared to the similarity hashing [13] and traditional univariate kernels."}, {"heading": "5. CONCLUSIONS", "text": "We presented novel discrete multivariate direct feature quantization kernel methods (MVDFQ-SK and MVDFQSK with the manifold embedding) for data in the form of sequences of feature vectors (as in music MFCC sequences, biological sequence profiles, or image sequences). The proposed approach directly exploits original multivariate feature sequences to improve sequence classification as opposed to using univariate codeword sequences. On three music classification tasks as well as protein sequence classification this shows significant 25-40% improvements compared to the traditional codebook learning and state-of-the-art sequence classification methods."}, {"heading": "6. REFERENCES", "text": "[1] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and\nB. Ke\u0301gl. Aggregate features and adaboost for music classification. Mach. Learn., 65:473\u2013484, December 2006. [2] A. Bhattacharyya. On a measure of divergence between two statistical populations defined by their probability distributions. Bulletin of the Calcutta Mathematical Society, 35:99\u2013109, 1943. [3] W. M. Campbell. Generalized linear discriminant sequence kernels for speaker recognition. In Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International Conference on, volume 1, pages I\u2013161 \u2013I\u2013164, may 2002.\nand chroma features. In S. Dixon, D. Bainbridge, and R. Typke, editors, ISMIR, pages 339\u2013340. Austrian Computer Society, 2007.\n[8] M. Gribskov, A. McLachlan, and D. Eisenberg. Profile analysis: detection of distantly related proteins. Proceedings of the National Academy of Sciences, 84:4355\u20134358, 1987. [9] A. Holzapfel and Y. Stylianou. Musical genre classification using nonnegative matrix factorization-based features. Audio, Speech, and Language Processing, IEEE Transactions on, 16(2):424 \u2013434, feb. 2008.\n[10] T. Jebara, R. Kondor, and A. Howard. Probability product kernels. J. Mach. Learn. Res., 5:819\u2013844, December 2004. [11] R. Kuang, E. Ie, K. Wang, K. Wang, M. Siddiqi, Y. Freund, and C. S. Leslie. Profile-based string kernels for remote homology detection and motif extraction. In CSB, pages 152\u2013160, 2004. [12] P. Kuksa, P.-H. Huang, and V. Pavlovic. Scalable algorithms for string kernels with inexact matching. In NIPS, 2008. [13] P. P. Kuksa, I. Khan, and V. Pavlovic. Generalized similarity kernels for efficient sequence classification. In SDM, 2012. [14] P. P. Kuksa and V. Pavlovic. Spatial representation for efficient sequence classification. In ICPR, 2010. [15] P. P. Kuksa and Y. Qi. Semi-supervised bio-named entity recognition with word-codebook learning. In SDM, 2010. [16] C.-H. Lee, J.-L. Shih, K.-M. Yu, and H.-S. Lin. Automatic music genre classification based on modulation spectral analysis of spectral and cepstral features. Multimedia, IEEE Transactions on, 11(4):670 \u2013682, june 2009. [17] C. Leslie and R. Kuang. Fast string kernels using inexact matching for protein sequences. J. Mach. Learn. Res., 5:1435\u20131455, 2004. [18] C. S. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: A string kernel for SVM protein classification. In Pacific Symposium on Biocomputing, pages 566\u2013575, 2002. [19] C. S. Leslie, E. Eskin, J. Weston, and W. S. Noble. Mismatch string kernels for SVM protein classification. In NIPS, pages 1417\u20131424, 2002. [20] T. Li, M. Ogihara, and Q. Li. A comparative study on content-based music genre classification. In SIGIR \u201903, pages 282\u2013289, New York, NY, USA, 2003. ACM. [21] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A symbolic representation of time series, with implications for streaming algorithms. In Proceedings of the 8th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, DMKD \u201903, pages 2\u201311, New York, NY, USA, 2003. ACM. [22] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text classification using string kernels. J. Mach. Learn. Res., 2:419\u2013444, 2002. [23] Z. Lu and H. Ip. Image categorization with spatial mismatch kernels. Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, 0:397\u2013404, 2009.\n[24] I. Melvin, E. Ie, J. Weston, W. S. Noble, and C. Leslie. Multi-class protein classification using adaptive codes. J. Mach. Learn. Res., 8:1557\u20131581, 2007. [25] I. Panagakis, E. Benetos, and C. Kotropoulos. Music genre classification: A multilinear approach. In ISMIR\u201908, pages 583\u2013588, 2008.\n[26] H. Rump, S. Miyabe, E. Tsunoo, N. Ono, and S. Sagayama. Autoregressive mfcc models for genre classification improved by harmonic-percussion separation. In J. S. Downie and R. C. Veltkamp, editors, Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010, pages 87\u201392. International Society for Music Information Retrieval, 2010. [27] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA, 2004. [28] H. Shimodaira, K.-I. Noma, M. Nakai, and S. Sagayama. Dynamic time-alignment kernel in support vector machine. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press. [29] S. Sonnenburg, G. Ra\u0308tsch, and B. Scho\u0308lkopf. Large scale genomic sequence SVM classifiers. In ICML \u201905, pages 848\u2013855, New York, NY, USA, 2005. [30] N. Toussaint, C. Widmer, O. Kohlbacher, and G. Ratsch. Exploiting physico-chemical properties in string kernels. BMC Bioinformatics, 11(Suppl 8):S7, 2010. [31] G. Tzanetakis and P. Cook. Musical genre classification of audio signals. Speech and Audio Processing, IEEE Transactions on, 10(5):293 \u2013 302, jul 2002. [32] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, September 1998. [33] S. V. Vishwanathan, A. J. Smola, and R. Vidal. Binet-cauchy kernels on dynamical systems and its application to the analysis of dynamic scenes. Int. J. Comput. Vision, 73(1):95\u2013119, June 2007. [34] S. V. N. Vishwanathan and A. Smola. Fast kernels for string and tree matching. In NIPS, 2002. [35] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1753\u20131760. 2009. [36] J. Weston, C. Leslie, E. Ie, D. Zhou, A. Elisseeff, and W. S. Noble. Semi-supervised protein classification using cluster kernels. Bioinformatics, 21(15):3241\u20133247, 2005. [37] Z. Zeng, S. Zhang, H. Li, W. Liang, and H. Zheng. A novel approach to musical genre classification using probabilistic latent semantic analysis model. In Multimedia and Expo, 2009. ICME 2009. IEEE International Conference on, pages 486 \u2013489, 28 2009-july 3 2009. [38] D. Zhang, X. Chen, and W. S. Lee. Text classification with kernels on the multinomial manifold. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR \u201905, pages 266\u2013273, New York, NY, USA, 2005. ACM."}], "references": [{"title": "Aggregate features and adaboost for music classification", "author": ["J. Bergstra", "N. Casagrande", "D. Erhan", "D. Eck", "B. K\u00e9gl"], "venue": "Mach. Learn., 65:473\u2013484, December", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "On a measure of divergence between two statistical populations defined by their probability distributions", "author": ["A. Bhattacharyya"], "venue": "Bulletin of the Calcutta Mathematical Society, 35:99\u2013109,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1943}, {"title": "Generalized linear discriminant sequence kernels for speaker recognition", "author": ["W.M. Campbell"], "venue": "Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International Conference on, volume 1, pages I\u2013161 \u2013I\u2013164, may", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "A machine learning information retrieval approach to protein fold recognition", "author": ["J. Cheng", "P. Baldi"], "venue": "Bioinformatics, 22(12):1456\u20131463, June", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Rational kernels: Theory and algorithms", "author": ["C. Cortes", "P. Haffner", "M. Mohri"], "venue": "Journal of Machine Learning Research, 5:1035\u20131062,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "A kernel for time series based on global alignments", "author": ["M. Cuturi", "J.-P. Vert", "\u00d8.. Birkenes", "T. Matsui"], "venue": "In Proceedings of the Intern. Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Classifying music audio with timbral  and chroma features", "author": ["D.P.W. Ellis"], "venue": "S. Dixon, D. Bainbridge, and R. Typke, editors, ISMIR, pages 339\u2013340. Austrian Computer Society,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Profile analysis: detection of distantly related proteins", "author": ["M. Gribskov", "A. McLachlan", "D. Eisenberg"], "venue": "Proceedings of the National Academy of Sciences, 84:4355\u20134358,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1987}, {"title": "Musical genre classification using nonnegative matrix factorization-based features", "author": ["A. Holzapfel", "Y. Stylianou"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, 16(2):424 \u2013434, feb.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "J. Mach. Learn. Res., 5:819\u2013844, December", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Profile-based string kernels for remote homology detection and motif extraction", "author": ["R. Kuang", "E. Ie", "K. Wang", "K. Wang", "M. Siddiqi", "Y. Freund", "C.S. Leslie"], "venue": "CSB, pages 152\u2013160,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Scalable algorithms for string kernels with inexact matching", "author": ["P. Kuksa", "P.-H. Huang", "V. Pavlovic"], "venue": "NIPS,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalized similarity kernels for efficient sequence classification", "author": ["P.P. Kuksa", "I. Khan", "V. Pavlovic"], "venue": "SDM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Spatial representation for efficient sequence classification", "author": ["P.P. Kuksa", "V. Pavlovic"], "venue": "ICPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised bio-named entity recognition with word-codebook learning", "author": ["P.P. Kuksa", "Y. Qi"], "venue": "SDM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic music genre classification based on modulation spectral analysis of spectral and cepstral features", "author": ["C.-H. Lee", "J.-L. Shih", "K.-M. Yu", "H.-S. Lin"], "venue": "Multimedia, IEEE Transactions on, 11(4):670 \u2013682, june", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast string kernels using inexact matching for protein sequences", "author": ["C. Leslie", "R. Kuang"], "venue": "J. Mach. Learn. Res., 5:1435\u20131455,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "The spectrum kernel: A string kernel for SVM protein classification", "author": ["C.S. Leslie", "E. Eskin", "W.S. Noble"], "venue": "Pacific Symposium on Biocomputing, pages 566\u2013575,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Mismatch string kernels for SVM protein classification", "author": ["C.S. Leslie", "E. Eskin", "J. Weston", "W.S. Noble"], "venue": "NIPS, pages 1417\u20131424,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "A comparative study on content-based music genre classification", "author": ["T. Li", "M. Ogihara", "Q. Li"], "venue": "SIGIR \u201903, pages 282\u2013289, New York, NY, USA,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "A symbolic representation of time series, with implications for streaming algorithms", "author": ["J. Lin", "E. Keogh", "S. Lonardi", "B. Chiu"], "venue": "Proceedings of the 8th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, DMKD \u201903, pages 2\u201311, New York, NY, USA,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "J. Mach. Learn. Res., 2:419\u2013444,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Image categorization with spatial mismatch kernels", "author": ["Z. Lu", "H. Ip"], "venue": "Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, 0:397\u2013404,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-class protein classification using adaptive codes", "author": ["I. Melvin", "E. Ie", "J. Weston", "W.S. Noble", "C. Leslie"], "venue": "J. Mach. Learn. Res., 8:1557\u20131581,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Music genre classification: A multilinear approach", "author": ["I. Panagakis", "E. Benetos", "C. Kotropoulos"], "venue": "ISMIR\u201908, pages 583\u2013588,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Autoregressive mfcc models for genre classification improved by harmonic-percussion separation", "author": ["H. Rump", "S. Miyabe", "E. Tsunoo", "N. Ono", "S. Sagayama"], "venue": "J. S. Downie and R. C. Veltkamp, editors, Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010, pages 87\u201392. International Society for Music Information Retrieval,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, New York, NY, USA,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Dynamic time-alignment kernel in support vector machine", "author": ["H. Shimodaira", "K.-I. Noma", "M. Nakai", "S. Sagayama"], "venue": "T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Large scale genomic sequence SVM classifiers", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "B. Sch\u00f6lkopf"], "venue": "ICML \u201905, pages 848\u2013855, New York, NY, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Exploiting physico-chemical properties in string kernels", "author": ["N. Toussaint", "C. Widmer", "O. Kohlbacher", "G. Ratsch"], "venue": "BMC Bioinformatics, 11(Suppl 8):S7,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "Speech and Audio Processing, IEEE Transactions on, 10(5):293 \u2013 302, jul", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley-Interscience, September", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Binet-cauchy kernels on dynamical systems and its application to the analysis of dynamic scenes", "author": ["S.V. Vishwanathan", "A.J. Smola", "R. Vidal"], "venue": "Int. J. Comput. Vision, 73(1):95\u2013119, June", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast kernels for string and tree matching", "author": ["S.V.N. Vishwanathan", "A. Smola"], "venue": "NIPS,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1753\u20131760.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised protein classification using cluster kernels", "author": ["J. Weston", "C. Leslie", "E. Ie", "D. Zhou", "A. Elisseeff", "W.S. Noble"], "venue": "Bioinformatics, 21(15):3241\u20133247,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "A novel approach to musical genre classification using probabilistic latent semantic analysis model", "author": ["Z. Zeng", "S. Zhang", "H. Li", "W. Liang", "H. Zheng"], "venue": "Multimedia and Expo, 2009. ICME 2009. IEEE International Conference on, pages 486 \u2013489, 28 2009-july 3", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Text classification with kernels on the multinomial manifold", "author": ["D. Zhang", "X. Chen", "W.S. Lee"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR \u201905, pages 266\u2013273, New York, NY, USA,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 10, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 18, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 13, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 28, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 22, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 32, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 33, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 2, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 36, "context": "tracted particular attention and has led to a number of new algorithms [4, 11, 19, 14, 29, 23, 33, 34, 3, 37].", "startOffset": 71, "endOffset": 109}, {"referenceID": 13, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 161, "endOffset": 169}, {"referenceID": 37, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 161, "endOffset": 169}, {"referenceID": 22, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 214, "endOffset": 217}, {"referenceID": 28, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 248, "endOffset": 260}, {"referenceID": 13, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 248, "endOffset": 260}, {"referenceID": 11, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 248, "endOffset": 260}, {"referenceID": 32, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 299, "endOffset": 307}, {"referenceID": 13, "context": "In particular, kernel-based approaches for sequence classification show some of the most accurate results for a variety of problems, such as text categorization [14, 38], image classification [23], speech analysis [3], biological sequence analysis [29, 14, 12], time series and music classification [33, 14].", "startOffset": 299, "endOffset": 307}, {"referenceID": 31, "context": "For instance, given a set of positive training instances C and a set of negative training instances C\u2212, the SVM [32] learns a classification function of the following form", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": ", mismatch or spectrum kernels, gapped and wildcard kernel functions [17, 22]) essentially rely on symbolic Hamming-distance based matching of one-dimensional (1D) k-mers (k-long substrings) in the input sequences.", "startOffset": 69, "endOffset": 77}, {"referenceID": 21, "context": ", mismatch or spectrum kernels, gapped and wildcard kernel functions [17, 22]) essentially rely on symbolic Hamming-distance based matching of one-dimensional (1D) k-mers (k-long substrings) in the input sequences.", "startOffset": 69, "endOffset": 77}, {"referenceID": 17, "context": ", amino acid sequences with |\u03a3|=20), the spectrum-k kernel [18] and the mismatch-(k,m) kernel [19] measure similarity between sequences as", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": ", amino acid sequences with |\u03a3|=20), the spectrum-k kernel [18] and the mismatch-(k,m) kernel [19] measure similarity between sequences as", "startOffset": 94, "endOffset": 98}, {"referenceID": 30, "context": ", [31, 20, 16]) extracted from short time segments, or 20-dimensional profile representations of protein sequences as series of probabilistic amino acid substitution vectors in biological sequence analysis [11, 8].", "startOffset": 2, "endOffset": 14}, {"referenceID": 19, "context": ", [31, 20, 16]) extracted from short time segments, or 20-dimensional profile representations of protein sequences as series of probabilistic amino acid substitution vectors in biological sequence analysis [11, 8].", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": ", [31, 20, 16]) extracted from short time segments, or 20-dimensional profile representations of protein sequences as series of probabilistic amino acid substitution vectors in biological sequence analysis [11, 8].", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": ", [31, 20, 16]) extracted from short time segments, or 20-dimensional profile representations of protein sequences as series of probabilistic amino acid substitution vectors in biological sequence analysis [11, 8].", "startOffset": 206, "endOffset": 213}, {"referenceID": 7, "context": ", [31, 20, 16]) extracted from short time segments, or 20-dimensional profile representations of protein sequences as series of probabilistic amino acid substitution vectors in biological sequence analysis [11, 8].", "startOffset": 206, "endOffset": 213}, {"referenceID": 22, "context": ", [23, 37, 14, 12].", "startOffset": 2, "endOffset": 18}, {"referenceID": 36, "context": ", [23, 37, 14, 12].", "startOffset": 2, "endOffset": 18}, {"referenceID": 13, "context": ", [23, 37, 14, 12].", "startOffset": 2, "endOffset": 18}, {"referenceID": 11, "context": ", [23, 37, 14, 12].", "startOffset": 2, "endOffset": 18}, {"referenceID": 31, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 57, "endOffset": 65}, {"referenceID": 26, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 57, "endOffset": 65}, {"referenceID": 10, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 18, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 28, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 16, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 13, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 4, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 22, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 37, "context": "Among the discriminative approaches, string kernel-based [32, 27] methods provide some of the most accurate results [11, 19, 29, 17, 14, 5, 23, 38] in many sequence analysis tasks.", "startOffset": 116, "endOffset": 147}, {"referenceID": 17, "context": ", k-mers), as in spectrum kernels [18] or substring kernels [34].", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": ", k-mers), as in spectrum kernels [18] or substring kernels [34].", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Inexact comparison of the sequences in this framework is typically achieved using different families of mismatch [19] or profile [11] kernels.", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "Inexact comparison of the sequences in this framework is typically achieved using different families of mismatch [19] or profile [11] kernels.", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "in [11], first builds a 20 \u00d7 |X|-dim profile [8] PX and then derives a similar |\u03a3|-dimensional representation from PX .", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "in [11], first builds a 20 \u00d7 |X|-dim profile [8] PX and then derives a similar |\u03a3|-dimensional representation from PX .", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "Such profile representations have been shown to perform well in protein sequence analysis [11, 24].", "startOffset": 90, "endOffset": 98}, {"referenceID": 23, "context": "Such profile representations have been shown to perform well in protein sequence analysis [11, 24].", "startOffset": 90, "endOffset": 98}, {"referenceID": 15, "context": "A music sequence X in the commonly used MFCC feature representation [16, 20] is a sequence of 13-dimensional MFCC feature vectors, i.", "startOffset": 68, "endOffset": 76}, {"referenceID": 19, "context": "A music sequence X in the commonly used MFCC feature representation [16, 20] is a sequence of 13-dimensional MFCC feature vectors, i.", "startOffset": 68, "endOffset": 76}, {"referenceID": 22, "context": ", as in [23]);", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "Protein sequences can be viewed as profiles [11], or as multivariate sequences of R-dim feature vectors describing physical/chemical properties of individual amino acids [30].", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "Protein sequences can be viewed as profiles [11], or as multivariate sequences of R-dim feature vectors describing physical/chemical properties of individual amino acids [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "2), recent work in [13] introduced the so-called generalized similarity (non-Hamming) kernels that allow to incorporate general similarity metrics S(\u00b7, \u00b7) into similarity evaluation and improve performance compared to symbolic Hamming distancebased matching [13].", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "2), recent work in [13] introduced the so-called generalized similarity (non-Hamming) kernels that allow to incorporate general similarity metrics S(\u00b7, \u00b7) into similarity evaluation and improve performance compared to symbolic Hamming distancebased matching [13].", "startOffset": 258, "endOffset": 262}, {"referenceID": 12, "context": "In particular, most related to the current work, are the distance-preserving symbolic embedding kernels [13] which use similarity hashing [35] to obtain binary representations for sequences such that Hamming distance h(\u00b7, \u00b7) between these binary representations is proportional to the original similarity score S(\u00b7, \u00b7) [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 34, "context": "In particular, most related to the current work, are the distance-preserving symbolic embedding kernels [13] which use similarity hashing [35] to obtain binary representations for sequences such that Hamming distance h(\u00b7, \u00b7) between these binary representations is proportional to the original similarity score S(\u00b7, \u00b7) [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "In particular, most related to the current work, are the distance-preserving symbolic embedding kernels [13] which use similarity hashing [35] to obtain binary representations for sequences such that Hamming distance h(\u00b7, \u00b7) between these binary representations is proportional to the original similarity score S(\u00b7, \u00b7) [13].", "startOffset": 319, "endOffset": 323}, {"referenceID": 12, "context": "In contrast, the direct feature quantization (DFQ) method proposed in this work results in more accurate non-binary representations that are simpler as they do not require Hamming embedding learning step as in [13], and display higher accuracy (see Experiments, Sec.", "startOffset": 210, "endOffset": 214}, {"referenceID": 32, "context": ", kernels on dynamical systems [33], or alignment-based methods [6, 28] with a quadratic time complexity.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": ", kernels on dynamical systems [33], or alignment-based methods [6, 28] with a quadratic time complexity.", "startOffset": 64, "endOffset": 71}, {"referenceID": 27, "context": ", kernels on dynamical systems [33], or alignment-based methods [6, 28] with a quadratic time complexity.", "startOffset": 64, "endOffset": 71}, {"referenceID": 12, "context": "3) provide effective improvements in practice over traditional univariate (1D) VQ-based sequence kernels, binary similarity-preserving embedding kernels [13], as well as other state-of-the-art sequence classification methods for a number of challenging classification problems.", "startOffset": 153, "endOffset": 157}, {"referenceID": 22, "context": ", word sequences in text analysis, amino acid sequences in the biological sequence analysis, or codeword sequences in the time series analysis [23, 22, 14, 19].", "startOffset": 143, "endOffset": 159}, {"referenceID": 21, "context": ", word sequences in text analysis, amino acid sequences in the biological sequence analysis, or codeword sequences in the time series analysis [23, 22, 14, 19].", "startOffset": 143, "endOffset": 159}, {"referenceID": 13, "context": ", word sequences in text analysis, amino acid sequences in the biological sequence analysis, or codeword sequences in the time series analysis [23, 22, 14, 19].", "startOffset": 143, "endOffset": 159}, {"referenceID": 18, "context": ", word sequences in text analysis, amino acid sequences in the biological sequence analysis, or codeword sequences in the time series analysis [23, 22, 14, 19].", "startOffset": 143, "endOffset": 159}, {"referenceID": 22, "context": ", [23, 14]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": ", [23, 14]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 22, "context": ", [23, 14])", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": ", [23, 14])", "startOffset": 2, "endOffset": 10}, {"referenceID": 20, "context": ", [21]) as breakpoints under Gaussian curve producing equal-sized areas.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "In the experiments, we use the state-of-the-art spectrum, mismatch [19], and spatial sample (SSSK) [14] kernel functions as our one-dimensional row matching function", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "In the experiments, we use the state-of-the-art spectrum, mismatch [19], and spatial sample (SSSK) [14] kernel functions as our one-dimensional row matching function", "startOffset": 99, "endOffset": 103}, {"referenceID": 37, "context": "[38, 10]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[38, 10]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 1, "context": "Then, a natural measure of affinity between the distributions \u03a6\u0302(X) and \u03a6\u0302(Y ) on the multinomial manifold is a Bhattacharyya affinity [2], i.", "startOffset": 135, "endOffset": 138}, {"referenceID": 12, "context": ", as in [13]) as it directly discretizes the feature space using, e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "\u2022 It can be used with any of the existing univariate sequence kernels (mismatch/spectrum [19], kernels [14], gapped/subsequence kernels [22, 17], etc) to improve performance.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "\u2022 It can be used with any of the existing univariate sequence kernels (mismatch/spectrum [19], kernels [14], gapped/subsequence kernels [22, 17], etc) to improve performance.", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "\u2022 It can be used with any of the existing univariate sequence kernels (mismatch/spectrum [19], kernels [14], gapped/subsequence kernels [22, 17], etc) to improve performance.", "startOffset": 136, "endOffset": 144}, {"referenceID": 16, "context": "\u2022 It can be used with any of the existing univariate sequence kernels (mismatch/spectrum [19], kernels [14], gapped/subsequence kernels [22, 17], etc) to improve performance.", "startOffset": 136, "endOffset": 144}, {"referenceID": 30, "context": "This dataset is a reference music genre recognition dataset introduced in [31, 20].", "startOffset": 74, "endOffset": 82}, {"referenceID": 19, "context": "This dataset is a reference music genre recognition dataset introduced in [31, 20].", "startOffset": 74, "endOffset": 82}, {"referenceID": 35, "context": "protein remote homology detection (7329 sequences, 54 experiments) [36, 24].", "startOffset": 67, "endOffset": 75}, {"referenceID": 23, "context": "protein remote homology detection (7329 sequences, 54 experiments) [36, 24].", "startOffset": 67, "endOffset": 75}, {"referenceID": 22, "context": ", [23, 15, 14]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 14, "context": ", [23, 15, 14]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 13, "context": ", [23, 15, 14]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": "A similarity hashing-based kernel approach described recently in [13] (a Euclidean similarity-preserving binary Hamming embedding of original real-valued MFCC feature vectors).", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "We also compare with a number of other state-of-the-art methods specifically developed for the music sequence classification, namely multivariate autoregressive models [26], multilinear models [25], as well as methods with more problemspecific and sophisticated features (aggregate Adaboost [1], classifier fusion with rich spectral and cepstral features [16], non-negative matrix factorization-based approaches [9]).", "startOffset": 168, "endOffset": 172}, {"referenceID": 24, "context": "We also compare with a number of other state-of-the-art methods specifically developed for the music sequence classification, namely multivariate autoregressive models [26], multilinear models [25], as well as methods with more problemspecific and sophisticated features (aggregate Adaboost [1], classifier fusion with rich spectral and cepstral features [16], non-negative matrix factorization-based approaches [9]).", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "We also compare with a number of other state-of-the-art methods specifically developed for the music sequence classification, namely multivariate autoregressive models [26], multilinear models [25], as well as methods with more problemspecific and sophisticated features (aggregate Adaboost [1], classifier fusion with rich spectral and cepstral features [16], non-negative matrix factorization-based approaches [9]).", "startOffset": 291, "endOffset": 294}, {"referenceID": 15, "context": "We also compare with a number of other state-of-the-art methods specifically developed for the music sequence classification, namely multivariate autoregressive models [26], multilinear models [25], as well as methods with more problemspecific and sophisticated features (aggregate Adaboost [1], classifier fusion with rich spectral and cepstral features [16], non-negative matrix factorization-based approaches [9]).", "startOffset": 355, "endOffset": 359}, {"referenceID": 8, "context": "We also compare with a number of other state-of-the-art methods specifically developed for the music sequence classification, namely multivariate autoregressive models [26], multilinear models [25], as well as methods with more problemspecific and sophisticated features (aggregate Adaboost [1], classifier fusion with rich spectral and cepstral features [16], non-negative matrix factorization-based approaches [9]).", "startOffset": 412, "endOffset": 415}, {"referenceID": 16, "context": "We test our methods using the state-of-the-art spectrum/mismatch [17] and spatial (SSSK) [14] kernels as our basic univariate kernels, i.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "We test our methods using the state-of-the-art spectrum/mismatch [17] and spatial (SSSK) [14] kernels as our basic univariate kernels, i.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "For music genre classification experiments a standard 5fold cross-validation procedure is used as in previous studies [20, 1] to evaluate classification performance.", "startOffset": 118, "endOffset": 125}, {"referenceID": 0, "context": "For music genre classification experiments a standard 5fold cross-validation procedure is used as in previous studies [20, 1] to evaluate classification performance.", "startOffset": 118, "endOffset": 125}, {"referenceID": 6, "context": "For music artist recognition, we follow a 6-fold leave-one-album-out validation procedure proposed in the previous work [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 30, "context": "Music genre [31, 20] 1000 10 5-fold cross-validation error ISMIR2004 contest 1458 6 5-fold cross-validation error Artist [7] 1413 20 6-fold hold-one-album-per-artist validation error SCOP protein homology [36] 7329 54 54 binary remote homology detection tasks", "startOffset": 12, "endOffset": 20}, {"referenceID": 19, "context": "Music genre [31, 20] 1000 10 5-fold cross-validation error ISMIR2004 contest 1458 6 5-fold cross-validation error Artist [7] 1413 20 6-fold hold-one-album-per-artist validation error SCOP protein homology [36] 7329 54 54 binary remote homology detection tasks", "startOffset": 12, "endOffset": 20}, {"referenceID": 6, "context": "Music genre [31, 20] 1000 10 5-fold cross-validation error ISMIR2004 contest 1458 6 5-fold cross-validation error Artist [7] 1413 20 6-fold hold-one-album-per-artist validation error SCOP protein homology [36] 7329 54 54 binary remote homology detection tasks", "startOffset": 121, "endOffset": 124}, {"referenceID": 35, "context": "Music genre [31, 20] 1000 10 5-fold cross-validation error ISMIR2004 contest 1458 6 5-fold cross-validation error Artist [7] 1413 20 6-fold hold-one-album-per-artist validation error SCOP protein homology [36] 7329 54 54 binary remote homology detection tasks", "startOffset": 205, "endOffset": 209}, {"referenceID": 12, "context": "Comparison of the error rates (%) for the vector quantization (VQ), similarity hashing [13], and the proposed multivariate direct feature quantization (MVDFQ).", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "hashing kernels [13]", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "hashing [13] MVDFQ VQ-M Sim.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "For the discrete embedding with similarity hashing [13], we set the number of bits E = 32 which has been found to perform well in [13].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "For the discrete embedding with similarity hashing [13], we set the number of bits E = 32 which has been found to perform well in [13].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "For the spatial sample kernels (SSSK) [14] we use the spatial kernel with three (t=3) k=1-mer features and the maximum distance parameter d=5.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "3) with the vector quantization-based univariate kernels, and the recently proposed binary Hamming similarity hashing kernels [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "As shown in Table 2, on a widely used benchmark dataset for music genre recognition [20, 31] (10 genres, each with 100 sequences), proposed multivariate DFQ kernels improve over traditional univariate VQ kernels, as well as recently proposed binary Hamming similarity hashing kernels [13].", "startOffset": 84, "endOffset": 92}, {"referenceID": 30, "context": "As shown in Table 2, on a widely used benchmark dataset for music genre recognition [20, 31] (10 genres, each with 100 sequences), proposed multivariate DFQ kernels improve over traditional univariate VQ kernels, as well as recently proposed binary Hamming similarity hashing kernels [13].", "startOffset": 84, "endOffset": 92}, {"referenceID": 12, "context": "As shown in Table 2, on a widely used benchmark dataset for music genre recognition [20, 31] (10 genres, each with 100 sequences), proposed multivariate DFQ kernels improve over traditional univariate VQ kernels, as well as recently proposed binary Hamming similarity hashing kernels [13].", "startOffset": 284, "endOffset": 288}, {"referenceID": 25, "context": "We also compare with previous best results on this music genre recognition dataset and baselines in Table 3, including multivariate autoregressive models [26], wavelet-based DWCH [20] method, aggregate AdaBoost [1], approaches specifically developed for the music classification that also use many other features in addition to MFCC.", "startOffset": 154, "endOffset": 158}, {"referenceID": 19, "context": "We also compare with previous best results on this music genre recognition dataset and baselines in Table 3, including multivariate autoregressive models [26], wavelet-based DWCH [20] method, aggregate AdaBoost [1], approaches specifically developed for the music classification that also use many other features in addition to MFCC.", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "We also compare with previous best results on this music genre recognition dataset and baselines in Table 3, including multivariate autoregressive models [26], wavelet-based DWCH [20] method, aggregate AdaBoost [1], approaches specifically developed for the music classification that also use many other features in addition to MFCC.", "startOffset": 211, "endOffset": 214}, {"referenceID": 0, "context": ", AdaBoost method [1] with much richer feature set).", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "55 Baseline 2: NMF [9] 26.", "startOffset": 19, "endOffset": 22}, {"referenceID": 19, "context": "51 Baseline 3: (non-MFCC): DWCH [20] 21.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "5 Baseline 4: MAR (multivariate autoregressive model) [26] 21.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "7 Baseline 5: AdaBoost (MFCC,FFT,LPC,etc) [1] 17.", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "As also can be seen from the Table 5, obtained error rates (16-17%) for ISMIR genre recognition compare well with a number of previous best results, including the recent nonnegative matrix factorization method [9](16.", "startOffset": 210, "endOffset": 213}, {"referenceID": 24, "context": "5% error), or complex auditory model and cortical representation of [25] (19% error).", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "20\u00d7 |X| multivariate sequences) with a number of state-of-the-art kernel methods for the remote homology detection including spectrum/mismatch kernels [19, 18], spatial sample kernels [14], similarity hashing kernels [13], as well as recently proposed spectrum-RBF and mismatch-RBF methods [30] which also incorporate physico-chemical descriptors.", "startOffset": 151, "endOffset": 159}, {"referenceID": 17, "context": "20\u00d7 |X| multivariate sequences) with a number of state-of-the-art kernel methods for the remote homology detection including spectrum/mismatch kernels [19, 18], spatial sample kernels [14], similarity hashing kernels [13], as well as recently proposed spectrum-RBF and mismatch-RBF methods [30] which also incorporate physico-chemical descriptors.", "startOffset": 151, "endOffset": 159}, {"referenceID": 13, "context": "20\u00d7 |X| multivariate sequences) with a number of state-of-the-art kernel methods for the remote homology detection including spectrum/mismatch kernels [19, 18], spatial sample kernels [14], similarity hashing kernels [13], as well as recently proposed spectrum-RBF and mismatch-RBF methods [30] which also incorporate physico-chemical descriptors.", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "20\u00d7 |X| multivariate sequences) with a number of state-of-the-art kernel methods for the remote homology detection including spectrum/mismatch kernels [19, 18], spatial sample kernels [14], similarity hashing kernels [13], as well as recently proposed spectrum-RBF and mismatch-RBF methods [30] which also incorporate physico-chemical descriptors.", "startOffset": 217, "endOffset": 221}, {"referenceID": 29, "context": "20\u00d7 |X| multivariate sequences) with a number of state-of-the-art kernel methods for the remote homology detection including spectrum/mismatch kernels [19, 18], spatial sample kernels [14], similarity hashing kernels [13], as well as recently proposed spectrum-RBF and mismatch-RBF methods [30] which also incorporate physico-chemical descriptors.", "startOffset": 290, "endOffset": 294}, {"referenceID": 12, "context": "In Table 8, we compare the classification performance and the running time of our method to the recent binary Hamming embedding (similarity hashing) [13], and vector quantization-based univariate kernels.", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "We note that for mismatch-(k,m) kernel computation we use the linear time sufficient-statistic based algorithm from [12]) and for all other methods we use their existing state-of-the-art implementations.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "As can be seen from Table 8, multivariate kernels with the direct feature quantization (MVDFQ) display a better performance compared to the similarity hashing [13] and traditional univariate kernels.", "startOffset": 159, "endOffset": 163}], "year": 2014, "abstractText": "Kernel-based approaches for sequence classification have been successfully applied to a variety of domains, including the text categorization, image classification, speech analysis, biological sequence analysis, time series and music classification, where they show some of the most accurate results. Typical kernel functions for sequences in these domains (e.g., bag-of-words, mismatch, or subsequence kernels) are restricted to discrete univariate (i.e. one-dimensional) string data, such as sequences of words in the text analysis, codeword sequences in the image analysis, or nucleotide or amino acid sequences in the DNA and protein sequence analysis. However, original sequence data are often of real-valued multivariate nature, i.e. are not univariate and discrete as required by typical k-mer based sequence kernel functions. In this work, we consider the problem of the multivariate sequence classification (e.g., classification of multivariate music sequences, or multidimensional protein sequence representations). To this end, we extend univariate kernel functions typically used in sequence domains and propose efficient multivariate similarity kernel method (MVDFQ-SK) based on (1) a direct feature quantization (DFQ) of each sequence dimension in the original real-valued multivariate sequences and (2) applying novel multivariate discrete kernel measures on these multivariate discrete DFQ sequence representations to more accurately capture similarity relationships among sequences and improve classification performance. Experiments using the proposed MVDFQ-SK kernel method show excellent classification performance on three challenging music classification tasks as well as protein sequence classification with significant 25-40% improvements over univariate kernel methods and existing state-of-the-art sequence classification methods.", "creator": "LaTeX with hyperref package"}}}