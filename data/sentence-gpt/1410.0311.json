{"id": "1410.0311", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2014", "title": "$\\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous Update", "abstract": "In many image processing applications, we encounter the problem of suppressing noise that obeys a non-Gaussian statistics. It is well known in the signal processing community that, in case of heavy-tailed non-Gaussian noise corruption, $\\ell_1$ distortion is a more suitable metric for data fidelity. Dictionary based image denoising algorithms existing in the literature are typically aimed at minimizing the $\\ell_2$ distortion metric, and hence not suitable for suppressing non-Gaussian or impulsive noise, as it may be used to achieve lower quality or resolution of the noise in noisy applications. A general comparison of a non-Gaussian noise-detection algorithm with a non-Gaussian noise detector might provide a good comparison. We therefore chose to compare a non-Gaussian noise detector with an impulsive noise detector that does not produce accurate results.\n\n\n\nIn general, the non-Gaussian noise detector does not have the capacity to detect noise, so, we prefer to use non-Gaussian noise detector in our algorithm, so, we use a non-Gaussian noise detector with the capacity to detect noise. The results can be obtained using a non-Gaussian noise detector, which is defined as \\frac{2}{0}{1}}(n^2) \\text{P{f\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\phi} \\text{P{F\\", "histories": [["v1", "Tue, 26 Aug 2014 07:23:04 GMT  (370kb,D)", "http://arxiv.org/abs/1410.0311v1", null], ["v2", "Tue, 3 Mar 2015 04:38:37 GMT  (237kb,D)", "http://arxiv.org/abs/1410.0311v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["subhadip mukherjee", "rupam basu", "chandra sekhar seelamantula"], "accepted": false, "id": "1410.0311"}, "pdf": {"name": "1410.0311.pdf", "metadata": {"source": "CRF", "title": "A Robust Dictionary Learning Algorithm for Image Denoising", "authors": ["Subhadip Mukherjee", "Rupam Basu", "Chandra Sekhar Seelamantula"], "emails": ["subhadip@ee.iisc.ernet.in,", "rbasu28@ece.iisc.ernet.in,", "chandra.sekhar@ieee.org."], "sections": [{"heading": null, "text": "Index Terms\u2014Noise robustness, heavy-tailed noise, dictionary learning, iteratively re-weighted least squares, K-SVD algorithm.\nI. INTRODUCTION\nIMAGE denoising has been an area of research for sev-eral decades and continues to be a challenging task with advances in imaging modalities. Recently, dictionary based algorithms have been attempted to solve the problem of denoising effectively. The principal idea behind such approaches is to train a dictionary that is capable of representing image patches parsimoniously, to be more precise, using a sparse representation. In many applications, the assumption of Gaussianness of noise may not hold in reality, thus rendering the `2 minimization based algorithms inapplicable. Moreover, these algorithms can lead to over-smoothing of the image, causing serious loss of details in the images. One way to alleviate this problem is to devise dictionary training algorithms tailored to minimizing the `1 data error, which is the main contribution of this work. We develop an algorithm that approximates the solution to the `1 minimization problem by iteratively solving weighted `2 problems. This idea is popularly known as iteratively re-weighted least squares (IRLS) [1] in the compressed sensing [2]\u2013[4] literature. We refer to the algorithm as the RDL algorithm. Before elaborating on the proposed technique, we give a brief overview of recent works on dictionary based approaches for image processing applications.\nThe authors are with the Department of Electrical Engineering, Indian Institute of Science, Bangalore 560012, India. Emails: subhadip@ee.iisc.ernet.in, rbasu28@ece.iisc.ernet.in, chandra.sekhar@ieee.org. The authors would like to thank Dr. K. N. Chaudhury for his insightful comments and suggestions in preparing the manuscript."}, {"heading": "A. Review of recent literature", "text": "Some of the initial contributions to the problem of dataadaptive dictionary learning were made by Aharon et al. [5], [6]. They proposed the K-SVD algorithm, which is by far the most popular and successful algorithm for dictionary design. They have successfully deployed the K-SVD algorithm to solve the problem of image denoising [6], specifically to eliminate zero-mean additive white Gaussian noise. They proposed two training options: (i) training on a corpus of clean image patches and (ii) training on the noisy input image itself. Since the K-SVD algorithm is not suitable for handling large image patches owing to large computational requirements, addition of a global prior was proposed to enforce sparsity. The KSVD algorithm uses the `2 distortion metric as a measure of data fidelity.\nDictionary-based approaches find applications in many image processing tasks other than denoising, for example, superresolution, inpainting, etc. In the context of image superresolution, Yang. et al. [7] proposed an algorithm, which exploits the idea of representing an image patch as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. They proposed a coupled learning strategy, where one tries to learn the common sparse representation of the low-resolution and high-resolution patches jointly and updates the corresponding dictionaries accordingly. Sahoo et al. [8] addressed the problem of image inpainting using sparse representation of local image patches, where an adaptive window is selected for local sparse representation, which affects the global recovery of the image. The problem of image restoration such as color image denoising, demosaicing and inpainting, has been addressed by Mairal et al. [9], who applied the K-SVD algorithm for handling non homogeneous noise and missing information.\nTypically, the problem of dictionary learning is formulated as an optimization problem, where the cost function trades off the data consistency term with the sparsity promoting regularization. Standard metrics used for the data term and the regularization are the `2 and `1 metrics, respectively. Nevertheless, algorithms for minimizing `1 based data terms have been proposed in the context of non-negative matrix factorization (NMF). An online `1 based NMF algorithm was developed by Kasiviswanathan et al. [10] for document detection. They imposed additional constraint of non-negativity on the dictionary and the corresponding coefficients, to maintain the interpretability of the result.\nar X\niv :1\n41 0.\n03 11\nv1 [\ncs .C\nV ]\n2 6\nA ug\n2 01\n4\n2"}, {"heading": "B. Contributions of this paper", "text": "In this paper, we propose an algorithm for dictionary learning aimed at minimizing the `1 distortion metric on the data term. The proposed RDL algorithm minimizes the `1 error using the idea of IRLS, which essentially approximates the solution of the `1 minimization problem by iteratively solving a series of `2 minimization problems. The use of `1 metric for data fidelity results in noise robustness in terms of training performance and proves to be efficient for removal of impulsive noise from images while preserving the edges and texture, as demonstrated by the experimental results."}, {"heading": "II. ROBUST DICTIONARY LEARNING (RDL) ALGORITHM", "text": "Let us denote the training dataset by T , containing N training examples {yn}Nn=1 in Rm corrupted by additive noise. The objective is to learn a dictionary D containing K atoms, tailored to the dataset T , such that D represents all the examples in T using sparse coefficient vectors {xn}Nn=1. Typically, D is over-complete, meaning that m < K. The symbols Y and X denote the matrices constructed by stacking yns and xns in the columns. In order to achieve robustness to noise, we propose to solve the following optimization problem:\nmin D,xn N\u2211 n=1 \u2016yn \u2212Dxn\u20161 + N\u2211 n=1 \u03bbn \u2016xn\u20161 . (1)\nThe use of `1 metric on the data term is the key to noise robustness. The cost function in (1) can be interpreted from a Bayesian perspective, where a Laplacian model is assumed for the prior as well as for the additive noise. The parameters \u03bbn in the regularization term adjusts the sparsity of the resulting representation against data fidelity. To solve (1), we adopt the alternating minimization strategy, where one starts with an initial guess for D, and updates xns and D, alternately. In the following two subsections, we explain the proposed algorithms for updating xns for a fixed D and vice-versa, which are commonly referred to as sparse coding and dictionary update, respectively."}, {"heading": "A. Sparse coding", "text": "To update the coefficients vectors xn when D is fixed, one needs to solve N independent problems of the form\nmin xn \u2225\u2225\u2225yn \u2212 D\u0302xn\u2225\u2225\u2225 1 + \u03bbn \u2016xn\u20161 , (2)\nwhere D\u0302 denotes the estimate of the dictionary in the current iteration. We propose to solve (2) using an iterative algorithm, which uses the idea of IRLS. Starting with an initial guess x (0) n , one updates xns in the (k + 1)st iteration as\nx(k+1)n = ( D\u0302TW (k) 1n D\u0302 + \u03bbnW (k) 2n )\u22121 D\u0302TW (k) 1n yn,\nwhere W1ns and W2ns are diagonal weight matrices of appropriate dimensions. Typically, one begins the iterations by setting W (0)1n s and W (0) 2n s to identity matrices, and they are\nupdated in the (k + 1)st iteration as\nW (k+1) 1n (j) = 1( yn \u2212 D\u0302x(k)n ) j + and\nW (k+1) 2n (j) = 1( x (k) n ) j + ,\nwhere W (j) denotes the jth diagonal entry of a diagonal matrix W , and (x)j denotes the jth entry of a vector x. A small positive constant is added to the denominator to ensure stability of the solution.\nOne can also choose to work with an equivalent constrained version of (2), which is of the form\nmin xn \u2225\u2225\u2225yn \u2212 D\u0302xn\u2225\u2225\u2225 1 subject to \u2016xn\u20161 \u2264 \u03c4n. (3)\nThe values of \u03c4n control the sparsity of the resulting solution, smaller values of \u03c4n resulting in higher sparsity. Efficient convex optimization packages, for example CVX, are available to solve (3). To further reduce the sensitivity of the solution to noise, we propose to apply an appropriate threshold on the small-magnitude entries of X , following sparse coding. Thus the proposed approach can be interpreted as `1 minimization followed by pruning."}, {"heading": "B. Updating the dictionary", "text": "Similar to the K-SVD algorithm, we adopt a sequential updating strategy for the dictionary atoms {di}Ki=1. To simultaneously update the jth atom in the dictionary and the corresponding row in X , we focus on the error matrix Ej = Y \u2212\n\u2211 i 6=j dix T i , resulting from the absence of dj , where\nxTi denotes the i th row of X . In order to obtain a refined estimate of dj and xTj , one is required to solve\nmin dj ,xTj \u2225\u2225Ej \u2212 djxTj \u2225\u22251 subject to S (xTj ) = \u2126j and \u2016dj\u20162 = 1, (4) where \u2016A\u20161 denotes the sum of the absolute values of the entries of matrix A. We seek to minimize the cost function subject to the constraint that S ( xTj )\n= \u2126j , where S(\u00b7) denotes the support operator. Therefore, it suffices to consider only those columns of Ej whose indices are in the support set \u2126j , which is denoted by Ej \u2223\u2223 \u2126j\n. Furthermore, to get rid of scale ambiguity, we restrict D to have unit-length atoms, that is, \u2016di\u20162 = 1 for all i. As a consequence, solving (4) is equivalent to solving an optimization problem of the form\nmin u,v \u2225\u2225E \u2212 uvT\u2225\u2225 1 = min u,v M\u2211 n=1 \u2016en \u2212 vnu\u20161\nsubject to \u2016u\u20162 = 1, (5)\nwhere en is the nth column of E and vn denotes the nth entry of the vector v. The key idea behind the RDL approach is to approximate the `1-norm using a suitably re-weighted `2 metric in the following manner:\nM\u2211 n=1 \u2016en \u2212 vnu\u20161 \u2248 M\u2211 n=1 (en \u2212 vnu)T Wn (en \u2212 vnu) .\n3 Differentiating with respect to u and setting to zero, we get\nu = ( M\u2211 n=1 v2nWn )\u22121( M\u2211 n=1 vnWnen ) .\nSimilarly, for a fixed u, we have vn = u TWnen uTWnu\n. Once the update of u and v is over, the jth diagonal element of W should be updated as\nwn(j)\u2190 1\n(en \u2212 vnu)j .\nDictionary update is performed by setting dj = u0 and xTj \u2223\u2223 \u2126j\n= vT0 , where (u0,v0) solves (5). The step-by-step description to find the solution of (5) is given in Algorithm 1. Algorithm 1 (RDL): To find u and v such that \u2225\u2225E \u2212 uvT\u2225\u2225 1 is minimized, subject to \u2016u\u20162 = 1.\n1. Input: Error matrix E, number of iterations J . 2. Initialization: \u2022 Set the iteration counter p to zero. \u2022 Set u(p) \u2190 a, and v(p) \u2190 \u03c3b, where a and b\nare the left and right singular vectors, respectively, corresponding to the largest singular value \u03c3 of E. \u2022 Initialize the weight matrices W (p)n \u2190 I , for n = 1, 2, \u00b7 \u00b7 \u00b7 ,M , where I denotes the identity matrix and M is the number of columns in E.\n3. Repeat the following steps J times: \u2022 w (p+1) n (j)\u2190 1(\nen\u2212v(p)n u(p) ) j ,\n\u2022 u(p+1) \u2190 [ M\u2211 n=1 ( v(p)n )2 W (p+1)n ]\u22121 [ M\u2211 n=1 v(p)n W (p+1) n en ] , \u2022 v (p+1) n \u2190 ( u(p+1)) T Wnen\n(u(p+1)) T Wn(u(p+1)) , for n = 1, 2, \u00b7 \u00b7 \u00b7 ,M , \u2022 p\u2190 p+ 1.\n4. Scaling: Set u\u2190 u (p)\n\u2016u(p)\u2016 2\nand v\u2190 \u2225\u2225u(p)\u2225\u2225\n2 v(p)\n5. Output: Solution (u,v) to (5).\nThe RDL algorithm is computationally more expensive than the K-SVD algorithm, which requires computing the singular value decomposition (SVD) of the error matrix to update each atom of D. The RDL algorithm starts with the K-SVD update and keeps refining the estimate by solving a series of weighted `2 minimization problems. However, the quality of the solution obtained using RDL is better than that obtained using K-SVD, in the case where the training set is noise contaminated, as indicated by the experimental results in the following section."}, {"heading": "III. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Synthesized signal experiment", "text": "To validate the RDL algorithm, we conduct experiments on the synthesized training sets, for Gaussian as well as Laplacian noise. Since the ground-truth dictionary D is known in the experiments, we use two performance metrics, namely the atom detection rate (ADR) and distance \u03ba ( D\u0302,D ) of the\nestimated dictionary D\u0302 from the ground-truth D. To compute\n0 5 10 15 200\n0.05\n0.1\n0.15\n0.2\nIterations\nD is\nta nc\ne fro\nm tr\nue d\nic t.\nIRLS\u2212DL KSVD\n(a)\n0 5 10 15 200\n0.2\n0.4\n0.6\n0.8\n1\nIterations\nAt om\nd et\nec tio\nn ra\nte\nIRLS\u2212DL KSVD\n(b)\n0 5 10 15 200\n0.05\n0.1\n0.15\n0.2\nIterations\nD is\nta nc\ne fro\nm tr\nue d\nic t.\nIRLS\u2212DL KSVD\n(c)\n0 5 10 15 200\n0.2\n0.4\n0.6\n0.8\n1\nIterations\nAt om\nd et\nec tio\nn ra\nte\nIRLS\u2212DL KSVD\n(d)\nFig. 1. (Color online) Comparison of the RDL and the KSVD algorithms in terms of atom detection rate (ADR) and \u03ba. The first and second rows correspond to cases where the additive noise follows Gaussian and Laplacian statistics, respectively. (a) and (c) show the variation of the distance metric \u03ba, whereas, (b) and (d) show the variation of ADR with iterations, for an input SNR of 20 dB. The plots are averaged over 25 independent realizations.\nADR, we count the number of recovered atoms in D. An atom di in D is considered to be recovered if \u2223\u2223\u2223dTi d\u0302j\u2223\u2223\u2223 exceeds 0.99 for some j. Since D and D\u0302 contain unit length atoms, this criterion ensures a near-accurate atom recovery. Finally, to compute ADR, we take the ratio of the number of recovered atoms to the total number of atoms. The metric \u03ba measures the closeness of D\u0302 to D, and is defined as\n\u03ba = 1\nK K\u2211 k=1 min j ( 1\u2212 \u2223\u2223\u2223dTk d\u0302j\u2223\u2223\u2223) . The value of \u03ba satisfies 0 < \u03ba < 1. If \u03ba is close to 0, it indicates that the recovered dictionary closely matches the ground-truth. The ground-truth dictionary is created by randomly generating K = 20 vectors as the atoms, with unit length, in m = 16 dimensional space. Each column of the coefficient matrix X has s = 3 non-zeros, with their locations chosen uniformly at random and values drawn from the Gaussian distribution with zero mean and unity variance. Subsequently, D is combined with X and contaminated by additive noise, to generate N = 800 examples. The variation of \u03ba and ADR with iterations, averaged over 25 independent trials, is demonstrated in Figure 1. To facilitate fair comparison of the RDL with the K-SVD algorithm, true values of the `1 and `0 norms of the ground-truth coefficient vactors, that is, true values of \u03c4ns in (3) and the parameter s are supplied to the algorithms, respectively. The RDL algorithm, as can be seen from Figure 1, results in a conspicuously superior performance over its `2-based counterpart K-SVD, both in terms of \u03ba and ADR. One can observe similar robustness of the RDL algorithm to both Gaussian and Laplacian noise from the experimental results. The ensemble-averaged (over\n25 realizations) values of ADR and the distance \u03ba from the ground-truth, corresponding to Laplacian noise contamination, for different values of input SNR are tabulated in Table I. Resulting values show clear improvement over the K-SVD, especially for low values of input SNR. Improvement in ADR, as we can observe from Table I, over the K-SVD algorithm is approximately 20%. Similar trend is observed for Gaussian noise corruption as well, and the corresponding values are given in Table II.\nB. Image denoising\nIn image denoising experiments, patches of size 4 \u00d7 4 are extracted from the noisy input image, with an overlap of one pixel in both horizontal and vertical directions. Subsequently, the noisy patches are vectorized, and stacked in columns of the data matrix Y . We conduct two image denoising experiments: one with the Tower image corrupted by heavy-tailed Laplacian noise and the other one with Gaussian noise contaminated Barbara image, which contains some textures and patterns. Details of the algorithms are described below separately for\nGaussian and non-Gaussian noise.\n1) Gaussian noise: Since `2 metric is more suitable for suppressing additive Gaussian noise over smooth regions, we first classify different parts of the image into smooth and non-smooth regions. The Canny edge detection algorithm is used on a smoothed version of the raw input image, obtained using a simple wavelet thresholding algorithm. Subsequently, at each pixel location, we extract a patch of size 4 \u00d7 4 and classify them in one of two categories, namely \u2018smooth patch\u2019 and \u2018edge patch\u2019. Finally we construct two training matrices, one for each kind. To denoise the smooth patches, we run the usual `2 based K-SVD algorithm, whereas on the edge patches, the RDL algorithm is deployed. The output image is obtained by computing the average of the resulting denoised patches with overlap. The outcomes of the experiment on the \u2018Barbara\u2019 image are shown in the second row of Figure 2. The corresponding PSNR values, averaged over 5 independent noise realizations, are indicated below the respective images. The average output PSNR values over 5 realizations for different images, corresponding to an input PSNR of 20.16\ndB are reported in Table III. We observe a consistent increase in the output PSNR value by approximately 0.30 dB in case of the RDL, over its `2 minimization based counterpart.\n2) Non-Gaussian noise: In the case where the input image is corrupted by non-Gaussian noise, it is not required to perform the patch classification step, since the `1 metric is the optimum choice for the overall image. Rest of the procedure is identical to the Gaussian noise case. The RDL algorithm is used to train the dictionary on the corpus of noisy patches extracted from the image, and the resulting overlapping denoised patches are averaged to obtain the final denoised output image. Experimental results on the \u2018Tower\u2019 image are shown the first row of Figure 2. The RDL algorithm results in an improvement of 2.30 dB in the PSNR value over the K-SVD algorithm. PSNR of the denoised output for different images corresponding to an input PSNR of 14.18 dB is given in Table IV. Similar to the case of Gaussian noise, we observe that the RDL algorithm results in an improvement of output PSNR over K-SVD, by a bigger margin of approximately 3\u2212 4 dB."}, {"heading": "IV. CONCLUSIONS", "text": "We have developed a dictionary learning algorithm, referred to as the RDL, aimed at minimizing the `1 error on the data term and applied the algorithm for image denoising. The motivation behind using the `1 metric as a measure of data fidelity is to achieve robustness to noise and alleviate the problem of over-smoothing the regions of texture and edges in images. Experiments on synthesized dataset indicate the superiority of the proposed algorithm over its `2 based counterpart, namely the K-SVD algorithm, in terms of ADR\nand the distance of the recovered dictionary to the groundtruth. Denoising experiments on real images show that the RDL algorithm results in output images with better PSNR values, compared with the K-SVD algorithm. Textures and edges in the images are also better preserved."}], "references": [{"title": "Iteratively re-weighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R.D. Vore", "M. Fornasier", "C.S. Gunturk"], "venue": "Communications on Pure and Applied Maths., vol. LXIII, pp. 1\u201338, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Info. Theory, vol. 52, no. 4, pp. 1289\u20131306, Apr. 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E.J. Candes", "J. Romberg", "T. Tao"], "venue": "arXiv:math/0503066v2, Dec. 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Near-Optimal signal recovery from random projections: Universal encoding strategies", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Trans. Info. Theory, vol. 52, no. 12, pp. 5406\u20135425, Dec. 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. on Sig. Proc., vol. 54, no. 11, pp. 4311\u20134322, Nov. 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. on Image Proc., vol. 15, no. 12, pp. 3736\u20133745, Dec. 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Trans. on Image Proc., vol. 19, no. 11, pp. 2861\u20132873, Nov. 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Image Inpainting using Sparse Approximation with Adaptive Window Selection", "author": ["S. Sahoo", "W. Lu"], "venue": "WISP, Oct. 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Sapiro,\u201cSparse representation for color imagerestoration", "author": ["J. Mairal", "M. Elad"], "venue": "IEEE Trans. on Image Proc.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Online `1 dictionary learning with application to novel document detection", "author": ["S.P. Kasiviswanathan", "H. Wang", "A. Banerjee", "P. Melville"], "venue": "Advances in Neural Information Processing Systems, vol. 25, pp. 2267\u2013 2275, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "This idea is popularly known as iteratively re-weighted least squares (IRLS) [1] in the compressed sensing [2]\u2013[4] literature.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "This idea is popularly known as iteratively re-weighted least squares (IRLS) [1] in the compressed sensing [2]\u2013[4] literature.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "This idea is popularly known as iteratively re-weighted least squares (IRLS) [1] in the compressed sensing [2]\u2013[4] literature.", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "[5], [6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], [6].", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "They have successfully deployed the K-SVD algorithm to solve the problem of image denoising [6], specifically to eliminate zero-mean additive white Gaussian noise.", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "[7] proposed an algorithm, which exploits the idea of representing an image patch as a sparse linear combination of elements from an appropriately chosen over-complete dictionary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] addressed the problem of image inpainting using sparse representation of local image patches, where an adaptive window is selected for local sparse representation, which affects the global recovery of the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9], who applied the K-SVD algorithm for handling non homogeneous noise and missing information.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] for document detection.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "In many image processing applications, we encounter the problem of suppressing noise that obeys a nonGaussian statistics. It is well known in the signal processing community that, in case of heavy-tailed non-Gaussian noise corruption, `1 distortion is a more suitable metric for data fidelity. Dictionary based image denoising algorithms existing in the literature are typically aimed at minimizing the `2 distortion metric, and hence not suitable for suppressing non-Gaussian or impulsive noise. In this paper, we develop a dictionary learning algorithm by minimizing the `1 error. The proposed algorithm exploits the idea of iterative minimization of suitably weighted `2 error. We refer to this algorithm as robust dictionary learning (RDL). We demonstrate that the proposed algorithm results in higher atom detection rate compared to the state-of-the-art KSVD algorithm, both in case of Gaussian and non-Gaussian noise contamination. For real images, we observe clear superiority of the proposed RDL algorithm over its `2 based counterpart, namely the K-SVD algorithm, in terms of the quality of denoised output.", "creator": "LaTeX with hyperref package"}}}