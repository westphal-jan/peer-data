{"id": "1410.2149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2014", "title": "Language-based Examples in the Statistics Classroom", "abstract": "Statistics pedagogy values using a variety of examples. Thanks to text resources on the Web, and since statistical packages have the ability to analyze string data, it is now easy to use language-based examples in a statistics class. Three such examples are discussed here. The first is from Myspace , a project running a Go application in Scala. (Note: The second, a blog post by Robert R. Toms, describes the implementation of the second code.)\n\nSo the first example I wrote about in Myspace is an example of a very simple example in a statistics class called an object representing the average of a sample. In Myspace you can see the result on the right. This is the test-bench results, which the Java-like Java compiler (not that different from the above), creates and runs a string object representing the average of a sample. The second example I wrote about in Myspace, comes from Scala, a JIT project that uses Scala's JIT framework and comes with its own language.\nThe third example I mentioned above is from a JIT project called C. I have written a lot of code here.\nThe second example I mentioned in Myspace is a simple example of an example in a statistics class called an object representing the average of a sample. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned here is from a JIT project called C. I have written a lot of code here. The third example I mentioned", "histories": [["v1", "Sun, 5 Oct 2014 01:36:30 GMT  (275kb)", "http://arxiv.org/abs/1410.2149v1", "Proceedings based on a Joint Statistical Meetings talk in 2009"]], "COMMENTS": "Proceedings based on a Joint Statistical Meetings talk in 2009", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roger bilisoly"], "accepted": false, "id": "1410.2149"}, "pdf": {"name": "1410.2149.pdf", "metadata": {"source": "CRF", "title": "Language-based Examples in the Statistics Classroom", "authors": ["Roger Bilisoly"], "emails": [], "sections": [{"heading": null, "text": "Web, and since statistical packages have the ability to analyze string data, it is now easy to use language-based examples in a statistics class. Three such examples are discussed here. First, many types of wordplay (e.g., crosswords and hangman) involve finding words with letters that satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs of words often appear together more frequently than chance. For example, in the Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E10.) Third, a pangram contains all the letters of the alphabet at least once. These are searched for in Charles Dickens' A Christmas Carol, and their lengths are compared to the expected value given by the unequal probability coupon collector's problem as well as simulations.\nKey Words: Linguistics, Wordplay, Fisher\u2019s exact test, coupon collector\u2019s problem"}, {"heading": "1. Analyzing Language", "text": "Although analyzing language data is uncommon among statisticians, there are many linguists that apply statistical techniques. For example, corpus linguists collect language samples that are representative of a particular aspect of a language, which are then analyzed using tools from both statistics and information theory. The Brown Corpus, for instance, was created to be representative of American English in 1961 and consists of 500 samples each containing about 2000 words. Computational linguists are also sophisticated users of statistics. In fact, there are statistically orientated books written by linguists such as Manning and Sch\u00fctze (1999) and Oakes (1998). Hence, if a statistics teacher wishes to use text data in class, many examples have already been worked out.\nThis paper gives three examples of applying statistics to language. The first applies a string pattern matching methodology called regular expressions to wordplay. The second discusses collocations, which is a concept from linguistics. Finally, the third applies more sophisticated mathematical techniques to pangrams, which is a type of wordplay. These examples should give the reader a taste of what can be done with language. But beware, once one starts it can be hard to stop."}, {"heading": "2. Wordplay and Regular Expressions", "text": "Several types of wordplay and word games require finding a word with letters satisfying a pre-specified string pattern. These can be formed by using a programming methodology called regular expressions, or regexes for short. It is useful because it is implemented in a variety of packages. In this paper we will use SAS\u2019s implementation of Perl regular\nexpressions, which first appeared in version 9. Only some simpler patterns are shown here: for a more thorough introduction to regexes and their use in text mining, see chapter 2 of Bilisoly (2008)."}, {"heading": "2.1 Crossword Puzzle Example", "text": "We first consider crossword puzzles. Here the length of an answer is known, and if the puzzle is partially finished, then some of the letters in the answer may be known. For example, consider a seven letter word with a b in the fourth position and a u in the last position. How informative is this, which is to say, how many words satisfy these constraints? This can be easily done in two steps. First, read a wordlist into SAS as shown in Figure 1. The file crosswd.txt is from Ward (2002), which is freely available from Project Gutenberg. This wordlist is used for all the code samples in this section.\nUsing the dataset wordlist created by the SAS code in Figure 1, Figure 2 uses a\nregular expression called regex to select the seven letter words that satisfy the crossword constraints given above. The forward slashes are delimiters of the regex, and\nthe period stands for any one character. The symbol ^ stands for the beginning of the\nline, which is the beginning of the word since crosswd.txt contains exactly one word per line. Note that SAS likes fixed length string variables, which causes blanks to be appended to the words, hence the need for a blank after the letter u. The + stands for one\nor more of the character preceding it (in this case one or more blanks), and the $ stands for the end of the line.\nFigure 3 gives the results of running the SAS code in Figures 1 and 2. It turns out that exactly one word satisfies the conditions imposed. Hence in this case, the letter pattern is quite informative.\nOf course, this is not the only way to solve this problem. The same result can be obtained\nby using the string functions substr()and length(). However, using regular expressions has the advantage of portability to many other software packages and programming languages. See Friedl (2006) for more on comparing implementations of regexes."}, {"heading": "2.2 Hangman Example", "text": "Hangman is somewhat like a single word in a crossword puzzle except that there can be letters known not to be in the word. Also, if a guessed letter appears in a word, all instances of it are revealed. These new constraints are easily implemented using regular expressions. For example, suppose a seven letter word has e for the second letter and ends in s (so none of the unknown letters are either e or s), plus the letters t, a, o, i, n, have been guessed but do not appear in this word. Figure 4 gives SAS code that can find all such words.\nNote that Figure 4 is almost exactly the same as Figure 2: the only differences are the\nregular expression used. Note that square brackets starting with a ^ means not to match\nany of the bracketed letters. Hence the ^ has a different meaning inside square brackets\nthan outside them. Finally, the {4} means four characters satisfying the letter restriction immediately preceding it. Running the SAS code in Figures 1 and 4 produces Figure 5, which shows twelve possible words.\nMany other word games or types of wordplay involve finding words that satisfy some pattern. Once one learns regular expressions, however, many of these are easy to find by just placing the appropriate regex into the Figure 2 code. For example, what is the longest word in English? Answer: the word smiles because it is a mile between the first and last letter. Are there longer words that contain the substring mile? Yes, there are, and it is left as a SAS programming exercise to find all of these."}, {"heading": "3. Word Collocations", "text": "The preceding section provides two examples that involve wordplay, and these interest students who enjoy such recreational activities. However, word analyses can also serve a more serious purpose in linguistics. The example of word collocations is given in this section, and it is both important to corpus linguists and illustrates categorical data analysis."}, {"heading": "3.1 Examples of Word Collocations", "text": "In linguistics, word collocations are two (or more) words that appear as a unit, which has a meaning that is not obvious from the meanings of the constituent words. For example, \u201cwhite house\u201d could mean just an arbitrary house that is white, but it appears frequently in print because that is where the president of the United States lives. This latter definition, however, cannot be deduced by knowing the definitions of the words \u201cwhite\u201d and \u201chouse.\u201d Hence, \u201cwhite house\u201d is an example of a word collocation.\nOne way to find word collocations is to compare the frequency of the constituent words to the frequency of the words together. Positive correlation suggests a collocation, while independence does not. Looking at English texts, one can also find negatively correlated words, e.g., \u201cultraviolet house,\u201d which only appears 110 times when searched for on Google (on 9/16/2009). Since no house is ultraviolet in color, this is not surprising.\nWord collocations have been traditionally found by concordancing, which is a sorted list of word matches. Not surprisingly, regular expressions are useful here. Figure 6 shows partial results of searching the Brown Corpus for the word up. Here the words to the left of up have been sorted, which is useful for finding collocations. This output was produced by Program 6.1 from Bilisoly (2008).\nBefore moving to the next section, note that concordancing has many uses in linguistics, two of which are explained here. First, word collocations are useful in finding idioms, and these are essential for language teachers because, by definition, students who know the individual words may not know what the collocation means. For example, in Figure 6, one sees that the preposition up of the phrasal verb to add up has no relation to the common meaning of up (referring to a higher position). In fact, saying \u201cadd together this list of numbers\u201d makes more sense, but that is not how it is said in English. However, German, in fact, does say it that way with the separable verb zusammenz\u00e4hlen.\nSecond, how does a dictionary discover the various meanings of words? One way is to analyze a corpus with concordancing, which gives the lexicographer exactly what is needed: many examples of a word in context. For example, Figure 6 shows that the phrasal verb to add up has both literal (lines 4 and 7) and figurative (lines 3 and 10) meanings."}, {"heading": "3.2 Frequency Table Analyses", "text": "In the last section it was noted that word collocations are often words that appear together more often than chance. Clearly statistics can be helpful in quantifying this, which linguists have long known. Chapter 5 of Foundations of Statistical Language Processing (Manning and Sch\u00fctze (1999)) mentions several techniques, including t-tests, chi-square tests, likelihood ratios and mutual information. The last method suggests that linguists have also delved into information theory, which is true. In this section, Fisher\u2019s exact test is applied to word pairs to check whether independence is a tenable hypothesis.\nFigure 7 shows a two-by-two contingency table to test whether or not two words are independent. If dependence is found, then it may be the case that the two words form a collocation. Using Program 6.1 (noted in the last section) applied to the Brown Corpus, two phrasal verbs are considered: to throw up vs. to throw about.\nFor the native English speaker, the two phrases \u201cthrow up one\u2019s ball\u201d and \u201cthrow up one\u2019s dinner\u201d are easily understood. The first is not an example of a collocation since the meaning of the phrasal verb to throw up does follow from the individual meanings of the verb to throw and the preposition up. However, the second phrase is a collocation since here the phrasal verb means to vomit, which has nothing to do with to throw, and little to do with up.\nTo test the independence of the words throw and up, Fisher\u2019s exact test is performed. The p-value was computed using SAS\u2019s PROC FREQ with the result 7.92E-10, so these two words are dependent in the Brown Corpus. The expected value of c11 is 0.29, and the pvalue reveals that the observed value of 8 is significantly higher than that. With this example in mind, we consider Figure 9.\nFor this second contingency table, the expected value of c11 is 0.27, but now the observed value is 1, which is much lower than before. Not surprisingly, the p-value now is much larger, 0.2343, which suggests that the words throw and about are independent in the Brown Corpus. Unlike the phrasal verb to throw up, there is not an idiomatic use of to throw about, though it can be used both figuratively and literally."}, {"heading": "4. Coupon Collecting and Pangrammatic Windows", "text": "This section has one last problem where language and statistics cross paths. A pangram is an English text that contains all 26 letters (ignoring case), and a pangrammatic window is a contiguous sample of text from a source that is also a pangram. The goal is to compare pangrammatic windows from Dickens\u2019 A Christmas Carol to what one would expect if the letters of the alphabet were independent of each other. Of course, letters are\ndependent, but it is interesting to see how independence may fail. We consider only one analysis here: comparing the length of the pangrams found in his novel compared to the lengths of pangrams via generating random letters using the empirical frequencies of the letters in his novel. Before starting this analysis, below is an example of a shorter-thanaverage pangram found in A Christmas Carol. Note that the letter j was the last to appear.\nThe Spirit dropped beneath it, so that the extinguisher covered its whole form; but though Scrooge pressed it down with all his force, he could not hide the light: which streamed from under it, in an unbroken flood upon the ground.\nHe was conscious of being exhausted, and overcome by an irresistible drowsiness; and, further, of being in his own bedroom. He gave the cap a parting squeeze, in which his hand relaxed; and had barely time to reel to bed, before he sank into a heavy sleep.\nAWAKING in the middle of a prodigiously tough snore, and sitting up in bed to get his thoughts together, Scrooge had no occasion to be told that the bell was again upon the stroke of One. He felt that he was restored to consciousness in the right nick of time, for the especial purpose of holding a conference with the second messenger dispatched to him through Jacob Marley's intervention."}, {"heading": "4.1 The Coupon Collector\u2019s Problem and Comparing Pangram Lengths", "text": "The coupon collector\u2019s problem analyzes how long it takes to collect a full set of coupons, which are the 26 letters of the alphabet in this case. Of course, the letters of the alphabet do not appear equally often, but the coupon collector\u2019s problem has a closedform solution when the coupons have unequal probabilities.\nDeriving this solution is not easy, and the key result is merely quoted here. Let pi be the probability of the ith coupon, and let Njk be the number of coupons needed so that j distinct coupons each appear at least k times. Finally, let ek(t) be the kth order Taylor approximation of the exponential function, e t . Then Theorem 2 of Flajolet et al. (1992) states the following.\nHere n equals 26, the number of letters in the Roman alphabet. Although this integral is hard to do by hand, it is easily done with a symbolic mathematics package such as Mathematica. All that is needed are the estimates of the letter frequencies, pi. This is easy to do with SAS: just read in the entire text one character at a time, then do a PROC FREQ for the letters a through z. The resulting counts and empirical frequencies are given in Figure 10.\n0 1 1, ))exp(1(1)( dttpNE\nn\ni in\nUsing the Mathematica code given in Figure 11, the expected value of N26,1 is 2473.82. Since the length of the quote at the beginning of Section 4 is 680 letters (this ignores spaces and punctuation), it is much shorter than average.\nTo finish this analysis, a thousand pangrammatic windows were found in A Christmas Carol by picking paragraphs at random then collecting text until all letters were found. Then a thousand additional pangrams were created by generating random letters using the proportions given in Figure 10. Before looking at the histograms of the lengths of these pangrams on the next page (Figures 12 and 13), how similar should these histograms be? Since writers do not use letters at random, one expects some differences, perhaps great dissimilarities.\nIt turns out that the two histograms are similar. Both have modes just above 1500 letters. In fact, their shapes are alike up to about 7,500. However, Figure 12 has a long right tail, while Figure 13 stops abruptly at 7,500. That is, A Christmas Carol has much longer stretches where there is at least one letter missing.\nThe longer tail in Figure 12 is mostly due to the following. It is caused by a rare letter not appearing as often as it would given independence. It turns out that of the 84 zs in A Christmas Carol, the name Fezziwig appears 20 times accounting for 40 of these, all of which appear on just three pages in Stave 2. Hence there are only 44 zs to be distributed through out the rest of the novel, which is roughly half the rate of the zs for Figure 13."}, {"heading": "4.2 A Birthday Problem Aside", "text": "Flajolet et al. (1992) give a general formula for E(Njk) and point out how the birthday problem is related to N12. If one looks at actual birthday data for a specific year (Chance (2009) gives U.S. data for 1978), it turns out that birthdays are not distributed uniformly because there a more than 10% drop on weekends and holidays along with a smaller seasonal affect as shown in Figure 14. It turns out that this barely changes E(N12): assuming uniformity it is 24.62, but using the 1978 data it drops to only 24.53. However, Figure 14 makes a great example for an introductory statistics class because it generates discussion.\nChance (2009) for details.)"}, {"heading": "5. Conclusions", "text": "There are two lessons to be learned from this paper. First, the linguists already have created many statistical examples, which is a great place to start for a statistics teacher wanting to include examples using language. Of especial interest are the corpus linguists who believe in creating samples of text that are representative of some aspect of language, and then employ computers to do numerous analyses. Second, statistical examples using language can be used in a variety of statistics courses including statistical programming, categorical data analysis, multivariate data analysis, and applied statistics.\nI have just starting incorporating language examples in my classes with generally positive results. However, note that some non-native speakers of English can find such examples difficult to understand."}, {"heading": "Acknowledgements", "text": "Thanks to my STAT 456 class (Introduction to SAS Programming for spring semester, 2009) for letting me try out some language examples for the first time. I owe a giant debt to all the people like Grady Ward who have released language data to the public domain and to all the sites like Project Gutenberg that provide public domain texts."}], "references": [{"title": "Practical Text Mining with Perl", "author": ["Bilisoly", "Roger"], "venue": null, "citeRegEx": "Bilisoly and Roger.,? \\Q2008\\E", "shortCiteRegEx": "Bilisoly and Roger.", "year": 2008}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Manning", "Christopher", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Statistics for Corpus Linguistics", "author": ["Oakes", "Michael"], "venue": null, "citeRegEx": "Oakes and Michael.,? \\Q1998\\E", "shortCiteRegEx": "Oakes and Michael.", "year": 1998}, {"title": "Moby Word Lists. Number 3201 in Project Gutenberg Releases", "author": ["Ward", "Grady"], "venue": "Project Gutenberg,", "citeRegEx": "Ward and Grady.,? \\Q2002\\E", "shortCiteRegEx": "Ward and Grady.", "year": 2002}], "referenceMentions": [], "year": 2009, "abstractText": "Statistics pedagogy values using a variety of examples. Thanks to text resources on the Web, and since statistical packages have the ability to analyze string data, it is now easy to use language-based examples in a statistics class. Three such examples are discussed here. First, many types of wordplay (e.g., crosswords and hangman) involve finding words with letters that satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs of words often appear together more frequently than chance. For example, in the Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E10.) Third, a pangram contains all the letters of the alphabet at least once. These are searched for in Charles Dickens' A Christmas Carol, and their lengths are compared to the expected value given by the unequal probability coupon collector's problem as well as simulations.", "creator": "Microsoft\u00ae Office Word 2007"}}}