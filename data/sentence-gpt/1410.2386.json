{"id": "1410.2386", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2014", "title": "Bayesian Robust Tensor Factorization for Incomplete Multiway Data", "abstract": "We propose a generative model for robust tensor factorization in the presence of both missing data and outliers. The objective is to explicitly infer the underlying low-CP-rank tensor capturing the global information and a sparse tensor capturing the local information (also considered as outliers), thus providing the robust predictive distribution over missing entries. The low-CP-rank tensor is modeled by multilinear interactions between multiple latent factors on which the column sparsity is enforced by a hierarchical prior, while the sparse tensor is modeled by a hierarchical view of Student-$t$ distribution that associates an individual hyperparameter with each element independently. For model learning, we develop an efficient closed-form variational inference under a fully Bayesian treatment, which can effectively prevent the overfitting problem and scales linearly with data size. In contrast to existing related works, our method can perform model selection automatically and implicitly without need of tuning parameters. More specifically, it can discover the groundtruth of CP rank and automatically adapt the sparsity inducing priors to various types of outliers. In addition, the tradeoff between the low-rank approximation and the sparse representation can be optimized in the sense of maximum model evidence. The extensive experiments and comparisons with many state-of-the-art algorithms on both synthetic and real-world datasets demonstrate the superiorities of our method from several perspectives.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 9 Oct 2014 08:50:31 GMT  (6742kb,D)", "http://arxiv.org/abs/1410.2386v1", null], ["v2", "Thu, 16 Apr 2015 05:36:23 GMT  (5041kb,D)", "http://arxiv.org/abs/1410.2386v2", "in IEEE Transactions on Neural Networks and Learning Systems, 2015"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["qibin zhao", "guoxu zhou", "liqing zhang", "rzej cichocki", "shun-ichi amari"], "accepted": false, "id": "1410.2386"}, "pdf": {"name": "1410.2386.pdf", "metadata": {"source": "CRF", "title": "Robust Bayesian Tensor Factorization for Incomplete Multiway Data", "authors": ["Qibin Zhao", "Guoxu Zhou", "Liqing Zhang", "Andrzej Cichocki", "Shun-ichi Amari"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014Tensor factorization, tensor completion, robust factorization, rank determination, variational Bayesian inference, video background modeling\nF"}, {"heading": "1 INTRODUCTION", "text": "T ENSORS (i.e., multiway arrays) can provide anefficient and faithful representation of structural properties for multidimensional data. For instance, a facial image ensemble affected by multiple conditions can be represented as a higher order tensor with dimensionality of pixel\u00d7person\u00d7pose\u00d7illumination. To model such data, tensor factorization has shown significant advantages in terms of capturing multiple interactions among a set of latent factors. Therefore its theory and algorithms have been an active area of study within the past decade, see e.g. [1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6]. The two most popular tensor factorization frameworks are Tucker [7] and CANDECOMP/PARAFAC (CP) [1], also known as canonical polyadic decomposition (CPD) [8], which naturally results in two different definitions of tensor\n\u2022 Q. Zhao is with Laboratory for Advanced Brain Signal Processing, RIKEN Brain Science Institute, Japan and Department of Computer Science and Engineering, Shanghai Jiao Tong University, China. \u2022 G. Zhou is with Laboratory for Advanced Brain Signal Processing, RIKEN Brain Science Institute, Japan. \u2022 L. Zhang is with MOE-Microsoft Laboratory for Intelligent Computing and Intelligent Systems and Department of Computer Science and Engineering, Shanghai Jiao Tong University, China. \u2022 A. Cichocki is with Laboratory for Advanced Brain Signal Processing, RIKEN Brain Science Institute, Japan and Systems Research Institute in Polish Academy of Science, Warsaw, Poland. \u2022 S. Amari is with Mathematical Neuroscience Laboratory, RIKEN Brain Science Institute, Japan.\nrank, i.e., multilinear rank and CP rank. When original data is only partially observed, tensor factorization can be applied for imputing the missing entries, known as tensor completion. CP factorization with missing values has been developed by employing CP weighted optimization (CPWOPT) [9] and nonlinear least squares (CPNLS) [10]. To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13]. The main limitation of the existing tensor factorization scheme is that the tensor rank has to be specified manually, which tends to under- or over-fit the observations, resulting in severe deterioration of predictive performance. It is important to emphasize that our knowledge about the properties of tensor rank, especially CP rank, is surprisingly limited [14]. There is no straightforward algorithm to compute CP rank of an explicitly given tensor, and the problem has been shown to be NP-hard [15], [16]. In fact, determining or even bounding the tensor rank is quite difficult in contrast to matrix rank [17], [18]. In [19], ARD framework was applied to estimate the multilinear rank. However, the solution is based on MAP point estimation and is not applicable to incomplete tensor data.\nThe convex optimization of nuclear norm has gained considerable attention in matrix completion, which essentially seeks the minimum rank under the condition of limited observations. Since multilinear rank of a tensor is defined as the rank of its mode-n matricizations, it can be optimized by simply applying nuclear norm based framework, yielding an extension\nar X\niv :1\n41 0.\n23 86\nv1 [\ncs .C\nV ]\n9 O\nct 2\n01 4\n2 to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23]. In addition, the auxiliary information can be exploited to improve completion accuracy [6], [24], which, however, is only suitable to some specific applications. It is worth noting that the convex optimization of nuclear norm requires several tuning parameters, which is prone to over- or under-estimate the tensor rank. In addition, since CP rank, the standard definition of tensor rank, cannot be optimized by applying matrix techniques straightforwardly, its determination still remains challenging so far.\nOn the other hand, non-Gaussian noises or outliers may occur frequently in image and video data. To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33]. For robust tensor factorization, 2DSVD using R1-norm as the objective function was proposed by Huang et al. [34] and robust Tucker decompositions were studied in [35], [36]. To handle both missing data and outliers, the nuclear norm regularization has been combined with L1-norm loss function, which leads to a robust tensor completion [37]. However, the main limitation of above mentioned approaches is that the performance is quite sensitive to tuning parameters whose optimal selection is unrealistic or prohibitively expensive. For example, the parameter balancing model capacity between a low-rank term and a sparse term is generally tuned by performance evaluated on the groundtruth of missing data that is unknown in practice, implying that most existing approaches are impractical to obtain the optimal results. Therefore, it is great appeal for an automatic model and parameter selection only based on observed data, which can achieve an optimal predictive performance. Another limitation of existing robust tensor factorizations is that optimizations of latent factors are mainly based on point estimation, which is prone to overfitting especially when a large amount of missing data is present and not able to provide uncertainty information of predictions.\nTo address all these issues under a unified framework, we propose a probabilistic model with aim to recover the underlying low-rank tensor, modeled by multiplicative interactions among multiple groups of latent factors, and an additive sparse tensor modeling outliers, from partially observed data represented by a tensor of any order. More specifically, for the low-rank term, we specify a hierarchical sparsity-inducing prior shared by multiple groups of latent factors, which gains a column sparsity along the latent components, resulting in an automatic determination of CP rank. For the sparse term, a hierarchical view of Student-t prior is placed independently on each element associated with an individual hyperparameter. The toplevel hyperparameters can be learned by maximizing a lower-bound of the model evidence, resulting\nin that the sparsity constraint can be automatically adapted to varying fractions of outliers. To learn the model under a fully Bayesian framework, we derive a variational Bayesian algorithm for closedform posterior inference, which is of high efficiency. Our method can be used for robust tensor factorization, robust tensor completion and anomaly detection with a significant advantage of automatic model and parameter selections without requiring any tuning parameters. Empirical results on both synthetic and real-world datasets demonstrate that the proposed method outperforms state-of-the-art methods in terms of predictive performance and robustness to outliers, even though the groundtruth is allowed to be used to tune the parameters in competing methods.\nThe rest of this paper is organized as follows. Section 2 discusses the related work. Section 3 introduces preliminary multilinear operations and notations. Section 4 presents model specification and approximate Bayesian inference for robust tensor factorization, whose advantages are summarized in Section 5. Section 6 shows extensive experimental results, followed by the conclusion in Section 7."}, {"heading": "2 RELATED WORK", "text": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42]. In [38], Beta-Bernoulli distribution is exploited to model outliers and the low-rank matrix exclusively, which, however, results in high model complexity and slow inference. Missing values are considered in [39], where the number of latent components needs to be specified in advance. In [42], Jeffreys prior is adopted to model both noise and outliers. However, it cannot handle missing values. PRMF [40] uses Laplace distribution to model the residuals, while the rank of the underlying model should be given in advance. A fully Bayesian treatment of PRMF [41] employs a hierarchical view of Laplace distribution as the noise model, and applies MCMC sampling for model inference. However, the rank also needs to be tuned manually and missing values are not considered. Finally, all these matrix based approaches cannot handle interactions of multiple factors, which is crucial for higher-order tensors.\nHigher-order robust PCA (HORPCA), proposed very recently in [37], is the only existing tensor method that can handle both missing data and outliers. It formulates the problem by a convex optimization framework in which nuclear norm and L1-norm are exploited as regularization terms on the low-rank tensor and residual errors, respectively. However, it essentially optimizes the multilinear rank and the predictive performance is sensitive to tuning parameters. To our best knowledge, our paper is the first to present a fully Bayesian model for robust tensor factorization dealing with both missing data and outliers within one framework.\n3"}, {"heading": "3 PRELIMINARIES AND NOTATIONS", "text": "The order of a tensor is the number of dimensions, also known as ways or modes. Vectors are denoted by boldface lowercase letters, e.g., a. Matrices are denoted by boldface capital letters, e.g., A. Higherorder tensors (order \u2265 3) are denoted by boldface calligraphic letters, e.g., A. Given an N th order tensor X \u2208 RI1\u00d7I2\u00d7\u00b7\u00b7\u00b7\u00d7IN , its (i1, i2, . . . , iN )th entry is denoted by Xi1i2...iN where the indices typically range from 1 to their capital version, e.g., in = 1, 2, . . . , In, n = 1, . . . , N .\nThe inner product of two tensors is defined by \u3008A,B\u3009 = \u2211i1i2...iN Ai1i2...iNBi1i2...iN , and the squared Frobenius norm by \u2016A\u20162F = \u3008A,A\u3009. Definition 3.1. The generalized inner product of N \u2265 3 vectors, matrices, or tensors is defined as a sum of element-wise products. For example,\u2329\nA(1), \u00b7 \u00b7 \u00b7 ,A(N) \u232a = \u2211 i,j \u220f n A (n) ij . (1)\nThe Hadamard product is an entrywise product of two vectors, matrices or tensors of the same sizes. For instance, A \u2208 RI\u00d7J and B \u2208 RI\u00d7J , their Hadamard product, denoted by A ~ B, is a matrix of size I \u00d7 J . Without loss of generality, the Hadamard product of a set of matrices {A(n)}Nn=1 is simply denoted by\n~ n A(n) = A(1) ~ A(2) ~ \u00b7 \u00b7 \u00b7~ A(N). (2)\nThe Kronecker product [1] of matrices A \u2208 RI\u00d7J and B \u2208 RK\u00d7L is a matrix of size IK \u00d7 JL, denoted by A \u2297B. The Khatri-Rao product of matrices A \u2208 RI\u00d7K and B \u2208 RJ\u00d7K is a matrix of size IJ \u00d7 K defined by a columnwise Kronecker product, and denoted by A B. In particular, the Khatri-Rao product of a set of matrices in a reverse order is denoted by\u2299\nn\nA(n) = A(N) A(N\u22121) \u00b7 \u00b7 \u00b7 A(1), (3)\nwhile the Khatri-Rao product of {A(n)}Nn=1 except the nth matrix, denoted by A(\\n), is defined by\u2299 k 6=n A(k) = A(N) \u00b7 \u00b7 \u00b7 A(n+1) A(n\u22121) \u00b7 \u00b7 \u00b7 A(1). (4)"}, {"heading": "4 BAYESIAN ROBUST CP FACTORIZATION", "text": ""}, {"heading": "4.1 Model Specification", "text": "Let Y be an incomplete N th-order tensor of size I1\u00d7I2\u00d7\u00b7 \u00b7 \u00b7\u00d7IN with missing entries. Y\u2126 denotes the observed entries {Yi1i2...iN |(i1, i2, \u00b7 \u00b7 \u00b7 , iN ) \u2208 \u2126} where\n\u2126 denotes a set of indices. We also define an indicator tensor O, whose entry Oi1i2\u00b7\u00b7\u00b7iN is equal to 1 if (i1, i2, \u00b7 \u00b7 \u00b7 , iN ) \u2208 \u2126, otherwise is equal to 0. We assume Y is a noisy measurement of the true latent tensor X , and is corrupted by outliers S, i.e., Y = X + S + \u03b5, where X is generated by tensor factorization with a low-CP-rank, representing the global information, S is enforced to be sparse, representing the local information, and \u03b5 is isotropic Gaussian noise (see Fig. 1).\nThe standard CP factorization [1] is expressed by\nX = R\u2211 r=1 a (1) \u00b7r \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 a(N)\u00b7r = [[A(1), . . . ,A(N)]], (5)\nwhere \u25e6 denotes the outer product of vectors and [[\u00b7 \u00b7 \u00b7 ]] is a shorthand notation of CP factorization. {A(n)|n = 1, . . . , N} are latent factor matrices corresponding to each of N modes, respectively. CP model can be interpreted as a sum of R rank-one tensors, which is related to the definition of CP rank that is the smallest integer R for which the above representation holds. The mode-n factor matrix of size In\u00d7R can be denoted by row-wise or column-wise vectors, that is,\nA(n) = [ a\n(n) 1 , . . . ,a (n) In\n]T = [ a\n(n) \u00b71 , . . . ,a (n) \u00b7R\n] .\nTo formulate robust CP factorization under the probabilistic framework, a generative model is introduced based on model assumptions. Specifically, the observation model is expressed by\np ( Y\u2126 \u2223\u2223\u2223{A(n)}Nn=1,S\u2126, \u03c4) = I1\u220f\ni1=1 \u00b7 \u00b7 \u00b7 IN\u220f iN=1\nN ( Yi1...iN \u2223\u2223\u2223\u2329a(1)i1 , \u00b7 \u00b7 \u00b7 ,a(N)iN \u232a+ Si1...iN , \u03c4\u22121)Oi1\u00b7\u00b7\u00b7iN , (6)\nwhere \u03c4 denotes the noise precision, a(n)in denotes the inth row vector of A(n), and S only has values corresponding to observed locations. The likelihood model in (6) indicates that Yi1\u00b7\u00b7\u00b7iN is generated by multiple R-dimensional latent vectors { a\n(n) in \u2223\u2223\u2223n = 1, . . . , N}, whereas each a(n)in affects a set of observations, i.e., a subtensor whose mode-n index is in. The essential difference between matrix factorization and tensor factorization is that the generalized inner product of N(\u2265 3) latent vectors allows us to capture multilinear interactions reflecting the intrinsic structural property of data, which however leads to much more difficulties in model learning.\nIn practice, CP rank, i.e., the dimensionality of latent space denoted by R, is unknown and considered as a tuning parameter whose optimal selection is quite challenging especially in the presence of missing data. Since R controls the model complexity, we actually seek an automatic model selection strategy that can infer the true CP rank from partially observed data. To achieve this, in contrast to rank minimization on X ,\n4 we attempt to minimize the dimensionality of latent space, which corresponds to column-wise sparsity of factor matrices. Hence, we employ a sparsity inducing prior over factor matrices by associating an individual hyperparameter to each latent dimension. More specifically, a hierarchical prior is equally specified over N factor matrices, which is expressed by\np ( A(n) \u2223\u2223\u03bb) = In\u220f in=1 N ( a (n) in \u2223\u22230,\u039b\u22121), \u2200n \u2208 [1, N ] p(\u03bb) =\nR\u220f r=1 Ga(\u03bbr|c0, d0), (7)\nwhere \u039b = diag(\u03bb) denotes an inverse covariance matrix and is shared by latent factor matrices in all modes. The hyperprior over \u03bb is an i.i.d. Gamma distribution Ga(x|a, b) = baxa\u22121e\u2212bx\u0393(a) where \u0393(a) is the Gamma function.\nDue to the sparsity property, the initialization of R is usually set to its maximum possible value, while the effective dimensionality can be inferred automatically under Bayesian inference framework. For instance, if a particular \u03bbr has a posterior distribution concentrated at large values, { a (n) r |\u2200n \u2208 [1, N ]} will tend to be zero. Since the priors are shared by N factor matrices, our framework can learn the same sparsity pattern for all factor matrices, yielding the minimum number of rank-one tensors.\nThe sparse term S is modeled also by a hierarchical sparsity inducing prior. More specifically, Gaussian priors are placed on each data entry associated with an individual precision hyperparameter on which an i.i.d. Gamma hyperprior is placed, that is\np(S\u2126|\u03b3) = \u220f\ni1,...,iN\nN (Si1...iN |0, \u03b3\u22121i1...iN ) Oi1...iN ,\np(\u03b3) = \u220f\ni1,...,iN\nGa(\u03b3i1...iN |a\u03b30 , b\u03b30). (8)\nNote that when an individual parameter \u03b3i1...iN goes to infinity, the corresponding element in S is enforced to be exact zero.\nThe priors in (7) and (8) are related to the framework of sparse Bayesian learning (SBL) [43] which is usually employed for variable selections. Since Laplacian and Student-t distributions are commonly applied to enforcing sparsity, we may question why the choice of a Gaussian prior should express any preference for sparsity. In fact, (8) can be interpreted as an infinite zero-mean Gaussian mixture with mixture coefficients drawn from a Gamma distribution, which is thus a hierarchical view of Student-t distribution. In other words, the marginal prior of S\u2126 is an i.i.d. Student-t distribution with the sparsity controlled by (a\u03b30 , b \u03b3 0) to some extent. For the case of noninformative hyperprior with a\u03b30 = b \u03b3 0 = 0, we obtain the improper marginal prior p(Si1...iN ) \u221d 1/|Si1...iN |. Note that if a\u03b30 = 1, the hyperprior becomes an exponential\nY\u2126\nA(1) A(n) A(N)S\u2126 \u03c4\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u03bb\u03b3\nc0 d0a0 \u03b3 b0\n\u03b3\na\u03c40 b \u03c4 0\nFig. 2. The probabilistic graphical model of Bayesian robust CP factorization of an incomplete tensor.\ndistribution, such that the marginal prior over S\u2126 is a Laplacian distribution. The elegance of this strategy therefore lies in the use of hierarchical modeling to obtain a prior which encourages sparsity while keeping fully conjugate exponential-family distributions throughout, which leads to the possibility of the fully Bayesian treatment. Although our setting is related to SBL, the crucial difference lies in that our model specification can achieve column-wise sparsity, and the statistical property is shared by a set of factor matrices {A(n)}Nn=1.\nTo complete the model, we also place a hyperprior over the noise precision \u03c4 , that is\np(\u03c4) = Ga(\u03c4 |a\u03c40 , b\u03c40). (9) Finally, the probabilistic graphical model of robust tensor factorization is illustrated in Fig. 2. For simplicity of notations, all unknowns including both latent factor matrices and hyperparameters are collected and denoted together by \u0398 = {A(1), . . . ,A(N),\u03bb,S\u2126,\u03b3, \u03c4}. Therefore, the joint distribution of the model, i.e., p(Y\u2126,\u0398), can be expressed by p ( Y\u2126 \u2223\u2223\u2223{A(n)}Nn=1,S\u2126, \u03c4) N\u220f\nn=1\np ( A(n) \u2223\u2223\u03bb)p(S\u2126|\u03b3)p(\u03bb)p(\u03b3)p(\u03c4). In general, we can simply perform MAP estima-\ntion of \u0398 from the log-joint distribution (see Sec. 1 of Appendix) and most existing tensor factorization based on optimization approaches can be interpreted as point estimation by either maximum likelihood or MAP principles. However, in this study, we aim to provide a fully Bayesian treatment of the model by inferring the posterior distribution of \u0398, expressed by p(\u0398|Y\u2126) = p(\u0398,Y\u2126)\u222b p(\u0398,Y\u2126) d\u0398 . Thus the predictive distribution over missing entries Y\\\u2126 can be also inferred by p(Y\\\u2126|Y\u2126) = \u222b p(Y\\\u2126|\u0398)p(\u0398|Y\u2126) d\u0398."}, {"heading": "4.2 Model Learning via Bayesian Inference", "text": "Since exact Bayesian inference of our model is analytically intractable, we must resort to the approximate inference. Although variational Bayesian (VB) inference [44] is difficult for derivations, especially when multiple interactions of latent factors are involved, it\n5 has advantages of closed-form posteriors and high efficiency as compared to sampling based inference methods. Hence, we employ VB inference to learn our model and present only the main results, while the detailed derivations and proofs are provided in Appendix.\nWe therefore seek a distribution q(\u0398) to approximate the true posterior distribution p(\u0398|Y\u2126) in the sense of minimizing the KL divergence, that is\nKL ( q(\u0398) \u2223\u2223\u2223\u2223p(\u0398|Y\u2126)) = ln p(Y\u2126)\u2212 L(q), where L(q) = \u222b q(\u0398) ln { p(Y\u2126,\u0398) q(\u0398) } d\u0398. (10)\nln p(Y\u2126) denotes the model evidence that is a constant, and its lower bound is denoted by L(q). Thus, minimum of KL divergence implies the maximum of L(q). By applying mean field approximation, we assume that the posteriors can be factorized as\nq (\u0398) = N\u220f n=1 q ( A(n) ) q(S\u2126)q(\u03bb)q(\u03b3)q(\u03c4). (11)\nNote that this is the only assumption about the distribution, while the particular functional forms of the individual factors can be explicitly derived in turn by virtue of conjugate exponential family in our hierarchical model."}, {"heading": "4.2.1 Posterior distribution of factor matrices", "text": "From the graphical model shown in Fig. 2, the inference of mode-n factor matrix A(n) can be performed by receiving the messages from observed data, which are expressed by the likelihood term (6), and incorporating the messages from their parents, which are expressed by the prior term (7). The posteriors are shown to be factorized as independent distributions of their rows which are also Gaussian (see Sec. 2 of Appendix for details), i.e.,\nq(A(n)) = In\u220f in=1 N ( a (n) in \u2223\u2223\u2223a\u0303(n)in ,V(n)in ) , \u2200n \u2208 [1, N ], (12) where the posterior parameters can be updated by\na\u0303 (n) in = Eq[\u03c4 ]V(n)in Eq [ A (\\n)T in ] vec (Y \u2212 Eq[S])I(Oin=1)\nV (n) in\n= ( Eq[\u03c4 ]Eq [ A (\\n)T in A (\\n) in ] + Eq[\u039b] )\u22121 .\n(13)\nEq[\u00b7] denotes the posterior expectation w.r.t. all variables involved. I(Oin = 1) is a sample function denoting a subset of the observed entries, whose mode-n index is in. The most complex term in (13) is A(\\n)Tin = (\u2299 k 6=n A (k) )T I(Oin=1)\n, where (\u00b7)I(Oin=1) denotes a subset of columns sampled according to the subtensor Oin = 1. Note that the update of V(n)in involves expectation of the Khatri-Rao product, which can not be evaluated straightforwardly. Hence, we introduce the following results:\nLemma 4.1. Given a set of independent random matrices {A(n)|n = 1, . . . , N}, we assume that \u2200n, \u2200in, the row vectors {a(n)in } are independent, then\nE [(\u2299 n A(n) )T(\u2299 n A(n) )] = \u2211\ni1,...,iN\n~ n\n( E [ a\n(n) in a (n)T in\n]) .\nProof: See Sec. 3 of Appendix for details.\nAccording to Lemma 4.1, we can obtain that\nEq [ A (\\n)T in A (\\n) in ] = \u2211\n(i1,...,iN )\u2208\u2126\n~ k 6=n\n( E [ a\n(k) ik a (k)T ik\n]) . (14)\nFor simplicity, let B(n) of size In \u00d7 R2 denote an expectation of a quadratic form related to A(n) by defining inth-row vector b (n) in = vec ( Eq [ a (n) in a (n)T in ]) =\nvec ( a\u0303\n(n) in a\u0303 (n)T in + V (n) in\n) , then (14) can be written as\nvec ( Eq [ A (\\n)T in A (\\n) in ]) = (\u2299 k 6=n B(k) )T vec(O\u00b7\u00b7\u00b7in\u00b7\u00b7\u00b7).\n(15) Note that the Khatri-Rao product in (15) is computed by all mode factors except nth mode, while the sum is performed according to the indices of observations, implying that only factors that interact with a(n)in are taken into account.\nAn intuitive interpretation of (13) is given as follows. V(n)in is updated by combining Eq[\u039b], denoting the factor prior, and covariance of other factor matrices computed by (15), while the tradeoff between these two terms is controlled by Eq[\u03c4 ] that is related to model fitness. In other words, the better fitness leads to more information from the current model than from the factor prior. a\u0303(n)in is updated firstly by a linear combination of all other factors, while the combination coefficients are observed values, which implies that the larger observation leads to more similarity of its corresponding latent factors. Subsequently, a\u0303(n)in is rotated by V(n)in and is scaled according to the model fitness Eq[\u03c4 ]."}, {"heading": "4.2.2 Posterior distribution of hyperparameters \u03bb", "text": "From Fig. 2, the inference of \u03bb can be performed by receiving messages from N factor matrices and incorporating the messages from its hyperprior. We can show the posteriors of \u03bbr,\u2200r \u2208 [1, R] are independent Gamma distribution, q(\u03bb) = \u220fR r=1 Ga(\u03bbr|crM , drM ), where crM , d r M denote the posterior parameters learned from M observations and can be updated by (see Sec. 4 of Appendix for details)\ncrM = c0 + 1\n2 N\u2211 n=1 In, d r M = d0 + 1 2 N\u2211 n=1 Eq [ a (n)T \u00b7r a (n) \u00b7r ] .\n(16) The posterior expectation term in (16) can be evaluated using the posterior parameters in (13), thus\n6 we have Eq [ a (n)T \u00b7r a (n) \u00b7r ] = a\u0303 (n)T \u00b7r a\u0303 (n) \u00b7r + \u2211 in ( V (n) in ) rr\n. Therefore, we can further simplify the computation of dM = [d1M , . . . d R M ] T by\ndM = N\u2211 n=1\n{ diag ( A\u0303(n)T A\u0303(n) +\n\u2211 in V (n) in\n)} . (17)\nBased on the updated posterior of \u03bb, we can obtain Eq[\u039b] = diag([c1M/d1M , . . . , cRM/dRM ]). An intuitive interpretation is that the smaller\u2211 n \u2016a (n) \u00b7r \u201622 leads to larger Eq[\u03bbr], which thus updates the prior over {a(n)\u00b7r }Nn=1, resulting in that the rth component is enforced more strongly to be zero. Therefore, the smaller components can be diminished eventually to exact zero and effectively pruned out after several iterations, while the larger components are enhanced to explain the data. This sparsity technique plays an key role to obtain the minimum number of components and automatic rank determination."}, {"heading": "4.2.3 Posterior distribution of sparse tensor S", "text": "By combining the priors in (8) and the likelihood in (6), we can derive the posterior approximation of S as (see Sec. 5 of Appendix for details)\nq(S) = \u220f\n(i1,...,iN )\u2208\u2126\nN ( Si1...iN \u2223\u2223\u2223S\u0303i1...iN , \u03c32i1...iN) , (18) where the posterior parameters can be updated by\nS\u0303i1...iN = \u03c32i1...iNEq[\u03c4 ] ( Yi1...iN \u2212 Eq [\u2329 a (1) i1 , . . . ,a (N) iN \u232a]) , \u03c32i1...iN = (Eq[\u03b3i1...iN ] + Eq[\u03c4 ]) \u22121.\n(19)\nObserve that S captures the information which is not explained by the low-rank CP approximation, while the magnitude is controlled by \u03c3i1...iN that is affected by the prior parameter Eq[\u03b3i1...iN ] and the precision of Gaussian noise Eq[\u03c4 ]. The intuitive interpretation is that S can model individual noises from total residuals, which are non-Gaussian. An alternative interpretation is that [[A(1), . . . ,A(N)]] explains the global information by using minimum number of rank-one tensors, while S explains the local information that is too expensive to be represented by increasing the model complexity."}, {"heading": "4.2.4 Posterior distribution of hyperparameters \u03b3", "text": "By incorporating the prior and hyperprior of S\u2126 in (8), we show that the posterior of \u03b3 is also factorized as independent distributions of each entries (see Sec. 6 of Appendix), given by\nq(\u03b3) = \u220f\n(i1,...,iN )\u2208\u2126\nGa(\u03b3i1...iN |a \u03b3i1...iN M , b \u03b3i1...iN M ), (20)\nwhose posterior parameters can be updated by\na \u03b3i1...iN M = a \u03b3 0 +\n1 2 , b \u03b3i1...iN M = b \u03b3 0 + 1 2 (S\u03032i1...iN + \u03c32i1...iN ).\n(21)\nThis indicates that the smaller Eq[S2i1...iN ] leads to larger Eq[\u03b3i1...iN ] which enforces S\u0303i1...iN to be zero more strongly by (19), and vice versa. In other words, the elements with small magnitude are forced to be zero, while the elements with large magnitude are further enhanced. It should be noted that the sparsity on S is essentially important due to the fact that Gaussian with individual hyperparameters can easily capture the whole information of data."}, {"heading": "4.2.5 Posterior distribution of hyperparameter \u03c4", "text": "The inference of noise precision \u03c4 can be performed by receiving the messages from observed data, and incorporating the messages from its hyperprior. We can show that the variational posterior is a Gamma distribution (see Sec. 7 of Appendix), i.e., q(\u03c4) = Ga(\u03c4 |a\u03c4M , b\u03c4M ) where the posterior parameters can be updated by\na\u03c4M = a \u03c4 0 +\n1\n2 \u2211 i1,...,iN Oi1...iN ,\nb\u03c4M = b \u03c4 0 +\n1 2 Eq [\u2225\u2225\u2225O ~ (Y \u2212 [[A(1), . . . ,A(N)]]\u2212 S)\u2225\u2225\u22252 F ] .\n(22)\nHowever, the posterior expectation of model residuals in the above expression can not be computed straightforward, we need to introduce the following results.\nLemma 4.2. Given a set of independent random matrices {A(n)|n = 1, . . . , N}, we assume that \u2200n, \u2200in, the row vectors {a(n)in } are independent, then\nE [\u2225\u2225\u2225[[A(1), . . . ,A(N)]]\u2225\u2225\u22252\nF ] =\n\u2211 i1,...,iN \u2329 E [ a (1) i1 a (1)T i1 ] , . . . ,E [ a (N) iN a (N)T iN ]\u232a . (23)\nProof: See Sec. 8 of Appendix for details.\nBy using Lemma 4.2, the posterior expectation in (22) can be evaluated explicitly (see Sec. 9 of Appendix for details), that is\nEq [\u2225\u2225\u2225O ~ (Y \u2212 [[A(1), . . . ,A(N)]]\u2212 S)\u2225\u2225\u22252\nF ] =\u2016Y\u2126\u20162F \u2212 2vecT (Y\u2126)vec ( [[A\u0303(1), . . . , A\u0303(N)]]\u2126\n) + vecT (O)\n(\u2299 n B(n) ) 1R2 \u2212 2vecT (Y\u2126)vec(S\u0303\u2126)\n+ 2vecT ([[A\u0303(1), . . . , A\u0303(N)]]\u2126)vec(S\u0303\u2126) + Eq[\u2016S\u2126\u20162F ]. (24)\nHence, the posterior expectation of \u03c4 can be updated by Eq[\u03c4 ] = a\u03c4M/b\u03c4M , where a\u03c4M is related to the number of observations and b\u03c4M is related to the posterior expectation of model residuals measured by squared Frobenius norm.\n7"}, {"heading": "4.2.6 Lower bound of model evidence", "text": "We can also evaluate the variational lower bound in (10) for our model. Since at each step of the iterative re-estimation procedure the value of this bound should not decrease, we can monitor the bound in order to test for convergence. The lower bound on the log marginal likelihood can be also written as\nL(q) = Eq(\u0398)[ln p(Y\u2126,\u0398)] +H(q(\u0398)), (25) where the first term denotes the posterior expectation of joint probability density, and the second term denotes the entropy of q distribution. Taking the parametric form of q distributions derived in the previous section, it can then be evaluated by an explicit form (See Sec. 10 of Appendix for details).\nThe top level hyperparameters a\u03b30 , b \u03b3 0 , a \u03c4 0 , b \u03c4 0 , c0, d0 are usually fixed to be very small values leading to a noninformative prior or set to zero leading to a Jeffrey\u2019s prior. Note that a\u03b30 , b \u03b3 0 are related to sparsity degree, we seek a strategy to automatically adopt these hyperparameters to various types of outliers. This can be easily achieved by maximizing the lower bound w.r.t. a\u03b30 , b \u03b3 0 , expressed by\nL(a\u03b30 , b\u03b30) = \u2212M ln \u0393(a\u03b30) +Ma\u03b30 ln b\u03b30 + (a\u03b30 \u2212 1) \u2211 (i1...iN )\u2208\u2126 { (\u03c8(a \u03b3i1...iN M )\u2212 ln b \u03b3i1...iN M ) } \u2212 b\u03b30\n\u2211 (i1...iN )\u2208\u2126 a \u03b3i1...iN M b \u03b3i1...iN M . (26)"}, {"heading": "4.2.7 Initialization of model inference", "text": "The variational Bayesian inference is only guaranteed to converge to a local minimum. To alleviate getting stuck in poor local solutions, it is important to choose an initialization point. In our model, the top level hyperparameters including c0, d0, a\u03c40 , b\u03c40 , a \u03b3 0 , b \u03b3 0 are set to 10\u22126, resulting in a noninformative prior. Thus the expectation of hyperparameters can be initialized by E[\u039b] = I, E[\u03c4 ] = 1 and \u2200n, \u2200in,E[\u03b3i1...iN ] = 1. For the factor matrices, E[A(n)],\u2200n \u2208 [1, N ] can be initialized by two different schemes. One is randomly drawn from N (0, I) for each row vector {a(n)in }. The other is set to A(n) = U(n)\u03a3(n) 1 2 , where U(n) denotes the left singular vectors and \u03a3(n) denotes the diagonal singular values matrix, obtained by SVD of mode-n matricization of Y . V(n) is simply set to E[\u039b\u22121]. For sparse tensor S, E[Si1...iN ] is drawn from N (0, 1), while \u03c32i1...iN is set to E[\u03b3 \u22121 i1...iN\n]. The tensor rank R is usually initialized by the maximum rank, i.e. R \u2264 minn Pn, where Pn = \u220f i 6=n Ii. For efficiency, we can also manually set the initialization value of R. The whole procedure of model inference is summarized in Algorithm 1, where the posterior factors in (11) are updated in an order that from bottom to top (see Fig. 2), which indicates that the message passing is started from observed data.\nAlgorithm 1 Bayesian Robust Tensor Factorization Input: An N th-order incomplete tensor Y and an indicator tensor O. Initialization: A\u0303(n),V(n)in ,\u2200in \u2208 [1, In],\u2200n \u2208 [1, N ], S\u0303, \u03c32, hyperparameters \u03bb,\u03b3, \u03c4 , top level hyperparameters c0, d0, a \u03b3 0 , b \u03b3 0 , a \u03c4 0 , b \u03c4 0 .\nrepeat for n = 1 to N do\nUpdate the posterior q(A(n)) by (13); end for Update the posterior q(\u03bb) by (16); Update the posterior q(\u03c4) by (22); Update the posterior q(S) by (19); Update the posterior q(\u03b3) by (21); Evaluate the lower bound by (25); Update a\u03b30 , b \u03b3 0 by maximizing (26); Model reduction by eliminating zero-components in {A(n)};\nuntil convergence."}, {"heading": "4.3 Predictive Distribution", "text": "The predictive distribution over missing entries, given observed entries, is also analytically intractable. Hence, we can approximate it by using the variational posteriors of all parameters in \u0398, yielding a Student-t distribution (see Sec. 11 of Appendix for details)\np(Y |Y\u2126) = \u220f\ni1,...,iN\nT (Yi1...iN |Y\u0303i1...iN ,\u03a8i1...iN , \u03bdy) (27)\nwith its parameters given by Y\u0303i1...iN = \u2329 a\u0303 (1) i1 , \u00b7 \u00b7 \u00b7 , a\u0303(n)iN \u232a ,\n\u03a8i1...iN = { b\u03c4M a\u03c4M + \u2211 n {( ~ k 6=n a\u0303 (k) ik )T V (n) in ( ~ k 6=n a\u0303 (k) ik )}}\u22121 ,\nand \u03bdy = 2a\u03c4M . Thus, the uncertainty of predictions can be obtained by var(Yi1...iN ) = \u03bdy\u03bdy\u22122\u03a8 \u22121 i1...iN ."}, {"heading": "4.4 Computational Complexity", "text": "The computation cost of N factor matrices in (13) is O(R2M \u2211 n In + R 3 \u2211 n In), where M denotes the number of observations, R denotes model complexity and is generally much smaller than the data size, i.e., R M . The computational costs are O(R2\u2211n In) for \u03bb, O(R2M) for \u03c4 , and O(MNR) for S. Therefore, the overall complexity of our algorithm is O((R2M + R3) \u2211 n In), which scales linearly with the data size but polynomially with the tensor rank. Note that due to the automatic model reduction, the excessive latent components are pruned out in the first few iterations such that R reduces rapidly in practice. The solution presented in this paper mainly focuses on a general tensor factorization problem. However, when data is extremely sparse or large scale, an alternative strategy for approximate inference updated by each observed entry can be developed correspondingly.\n8"}, {"heading": "4.5 Case of Complete Tensor", "text": "For fully observed tensor data, we can simply define O with all elements being 1 and apply the model inference as described previously. However, several essentially different properties arise during inference, which leads to the possibility of more efficient computation for approximate posteriors. For the inference of factor matrices shown in (13), since A(\\n)in are same for any in \u2208 [1, In], such that {V(n)in } In in=1\nare all equivalent. Hence, only one V(n) needs to be computed for each mode-n, and {a\u0303(n)in } In in=1\ncan be updated simultaneously by\nA\u0303(n) = Eq[\u03c4 ] ( Y(n) \u2212 Eq[S(n)] ) Eq [ A(\\n) ] V(n),\nV(n) = ( Eq[\u03c4 ]Eq [ A(\\n)TA(\\n) ] + Eq[\u039b] )\u22121 ,\n(28)\nwhere Y(n) denotes mode-n matricization of tensor Y . For computational efficiency, we introduce another solution related to Lemma 4.1.\nLemma 4.3. Given a set of matrices {A(n)|n = 1, . . . , N}, if the row vectors {a(n)in } In in=1\nare independent and cov[a(n)in ] = V (n),\u2200in \u2208 [1, In], then\nE [(\u2299 n A(n) )T(\u2299 n A(n) )] =~ n ( E [ A(n)TA(n) ]) ,\nwhere E[A(n)TA(n)] = { E [ A(n)T ] E [ A(n) ] + InV (n) } .\nProof: See Sec. 12 of Appendix for details.\nAccording to Lemma 4.3, the term in (28) can be computed efficiently by\nEq [ A(\\n)TA(\\n) ] = ~ k 6=n { A\u0303(k)T A\u0303(k) + IkV (k) } . (29) Hence the computational cost for factor matrices are reduced to O(R2\u2211n In+NR3). For hyperparameters \u03bb, the update rules in (17) can be simplified by\ndM = N\u2211 n=1 { diag ( A\u0303(n)T A\u0303(n) + InV (n) )} . (30)\nThe inference for S and \u03b3 are similar to the case of incomplete data. For hyperparameter \u03c4 , the posterior expectation of squared Frobenius norm of CP approximation can be computed more efficiently by introducing the following result.\nLemma 4.4. Given a set of independent random matrices {A(n)|n = 1, . . . , N}, we assume that \u2200in \u2208 [1, In], the row vectors {a(n)in } are independent, then\nE [\u2225\u2225\u2225[[A(1), . . . ,A(N)]]\u2225\u2225\u22252\nF ] =\u2329\nE [ A(1)TA(1) ] , . . . ,E [ A(N)TA(N) ]\u232a .\nProof: See Sec. 13 of Appendix for details.\nHence, the computational cost for \u03c4 is reduced to O(R2\u2211n In). In addition, the computation of lower\nbound and predictive distributions can be also simplified easily, which would not been presented in details."}, {"heading": "5 ADVANTAGES", "text": "Since our model is based on a hierarchical probabilistic framework and fully Bayesian treatment, several advantages are gained and discussed as follows: \u2022 Our method is characterized as a tuning parameter\nfree approach and all model parameters can be learned automatically from observed data. By contrast, the existing tensor factorization methods require either predefined rank or penalty parameter and tensor completion methods using nuclear norm need to tune regularization parameters. \u2022 The automatic rank determination enables us to discover the ground-truth of CP rank, while the automatic sparsity model can adapt the model to various types of outliers or non-Gaussian noises. Furthermore, the most elegant characteristic is that the tradeoff between the low-rank approximation and the sparse representation can be learned automatically in the sense of maximizing the model evidence. \u2022 In contrast to point estimations by most existing tensor methods, the uncertainty information over all model parameters are taken into account, which can effectively prevent overfitting problem. The full posteriors of factor matrices and predicted missing entries can provide confidence information regarding the solutions. \u2022 An efficient and deterministic algorithm for Bayesian inference is developed, which empirically shows a fast convergence and its computational complexity scales linearly with the data size."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "6.1 Validation on Synthetic Data", "text": "We firstly assess the performance quantitatively on synthetic data. The true low-rank tensor X of size 30 \u00d7 30 \u00d7 30 was generated by rank-3 factor matrices, i.e., A(n) \u2208 R30\u00d73, n = 1, 2, 3. Three components of the nth factor matrix are [sin(2\u03c0 n30 in), cos(2\u03c0 n 30 in), sgn(sin(0.5\u03c0in))], indicating that the first two components possess different frequencies related to n, and the third components are common in all modes. A random fraction of tensor entries were corrupted by outliers drawn from an uniform distribution U(\u2212|H|, |H|). To mimic more realistic settings, a small noise drawn from N (0, 0.01) was also considered. Subsequently, a fraction of entries were randomly selected to be observed tensor Y\u2126, when missing data was considered. We utilized the root relative square error (RRSE), defined by \u2016X\u0302\u2212X\u20162\u2016X\u20162 , to evaluate the performance of tensor recovery. As for recovering the underlying factors, factor match error\n9\n(FME) [10] between the estimated factors and groundtruth was also evaluated. We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]). It should be emphasized that the tuning parameters are necessary for most methods and have been carefully tuned based on ground-truth data, which is generally impractical for real applications. Specifically, CP-ALS, HOSVD, CPWOPT, CPNLS need to tune the parameter of tensor rank, and HORPCA needs to tune the penalty parameter. By contrast, BRTF and CP-ARD can automatically estimate tensor rank and do not require any tuning parameters. Two scenarios were considered: 1) tensor recovery from fully observed data, and 2) tensor completion from partially observed data.\nThe results for fully observed tensor are shown in Fig. 3. Rank R was initialized as maxn In for BRTF and CP-ARD. Observe that CPALS, CP-ARD and\nHOSVD are non-robust due to the sensitivity of L2norm loss function to outliers. HORPCA is shown to be robust when outliers are much larger than true data, while it performs poorly when the magnitude of outliers is within the range of true data. It should be noted that BRTF significantly outperforms competing methods under all conditions in terms of recovering the low-rank tensor and latent factors, indicating that it is more robust to outliers and non-Gaussian noises. In addition, the performance of BRTF is unaffected by the percentage and magnitude of outliers, which confirms the capability of automatically adapting the model to various types of outliers.\nA special case when the true CP Rank is larger than data dimensions, i.e., R > maxn In, was investigated under the condition of 1% outliers and the detailed results are shown in Table 1. The initial rank was set to 100 for both BRTF and CP-ARD. We observe that BRTF can correctly estimate rank R while CP-ARD overestimates it due to the corruptions by outliers. For CP-ALS and HOSVD, the optimal rank was selected from all possible values within [1, R] by multiple runs. The sensitivity of tuning parameters is also reported by standard deviation of RRSE under varying selections. Although ground-truth data was used to tune parameters by other methods, BRTF still\n10\nsignificantly outperforms all other methods in terms of RRSE and FME, while the runtime also shows its high efficiency. It should be noted that this experiment shows an essentially different property of tensor in contrast to matrix where R \u2264 minn In is always satisfied. Therefore, the straightforward extension of many matrix based techniques is not applicable to this situation, which has been demonstrated by the low performance of HORPCA that employs the robust matrix technique to each mode-n matricization of the tensor alternately.\nThe results for partially observed tensor are shown in Fig. 4, and Fig. 1 is one example of that. Several tensor factorization based completion methods are compared under varying missing ratios. For computation efficiency, the rank R was initialized as 10 in BRTF. Observe that HORPCA is more robust than CWOPT and CPNLS, which cannot handle outliers explicitly, only when outliers are much larger than true data. By contrast, BRTF achieves the best performance among competing methods and its performances are quite stable under varying missing ratios, which demonstrates its robustness to both outliers and missing data. In addition, the results confirm that BRTF can accurately estimate the ground-truth of tensor rank in all cases. It should be emphasized that all competing methods had multiple runs to tune the parameters\nand the best possible predictive performance on missing data was reported, while BRTF only needs to run once. These results demonstrate that the superiorities of BRTF is not only in automatic model selection, but also in the accuracy of tensor factorization and completion. In addition, Table 2 shows the mean and standard deviation of runtime under different missing ratios, indicating the high efficiency of BRTF."}, {"heading": "6.2 Video Background Modeling", "text": "Anomaly detection is another important ability of BRTF which can model the local information explicitly. Hence, we now consider a real-world application in surveillance video sequences with aim to separate the foreground objects from the background. Since the background is highly correlated along the frames, thus can be modeled by a low-rank tensor, while the foreground objects are moving along frames, thus can be modeled by a sparse tensor. We conducted experiments on the popular video sequences1 by extracting 100 frames from Shopping mall sequence and 300 frames from the other two sequences. The stateof-the-art methods for robust matrix/tensor factorization and background modeling were employed for comparisons. The matrix-based methods, including\n1. http://perception.i2r.a-star.edu.sg/bk model/bk index.html\n11\nPCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode. For competing methods, if necessary, the tuning parameters were selected close to optimal by visual quality due to the lack of groundtruth.\nAs shown in Fig. 5, BRTF successfully separates the background and foreground on all the sequences. From one frame of Hall, we observe that BRTF can completely separate the person, who stands for a while and then moves away, from the background, while all other methods capture this person by both background and foreground. In another frame of Hall, the ghosting effects are observed by all methods except DECOLOR and BRTF. For the sequence of Shopping Mall, VBRPCA, BRMF, DECOLOR and BRTF obtain clearer background than those by other methods. For the sequence of Water Surface, DECOLOR and BRTF are clearly superior to other methods. Note that DECOLOR obtains comparable results with BRTF, since it incorporates the auxiliary information that is specially designed for this application. By contrast, BRTF, as a general tool for robust factorization, has shown the superiorities not only in handling tensor data, but also in more robustness than existing robust matrix factorizations.\nTo illustrate the property of simultaneous robust completion and anomaly detection, we conducted additional experiments on the same sequences by randomly dropping 90% pixels and compared BRTF with RegL1ALM, PRMF and HORPCA, which can handle both missing data and outliers. As illustrated in Fig. 6, BRTF is significantly superior to matrix based RegL1ALM and PRMF and tensor based HORPCA in terms of recovering the background. We observe that in the presence of missing pixels, the ghost effects are more severe by other methods, while BRTF is unaffected, indicating its better robustness to missing data and outliers. Although HORPCA is also a robust tensor factorization method, it shows comparable results with matrix based methods and cannot recover well the color of background. It should be noted that all competing methods require tuning parameters whose selection is quite time consuming, while BRTF works in a fully automatic fashion."}, {"heading": "6.3 Facial Image Denoising", "text": "In this section, we further investigate the model property in terms of robustness to non-Gaussion noises. We use the CMU-PIE face database [46] for multilinear model analysis and evaluation. All the facial images are first aligned by their eye position and then cropped to size 32 \u00d7 32. Next, we use 1500 facial images selected from the first 30 subjects with\n5 poses and 10 illumination changes to construct a forth-order tensor X \u2208 R1024\u00d730\u00d75\u00d710. Two common types of noise that arises in the images are saltand-pepper noise (impulse noise) and poisson noise (shot photon noise), which are both non-Gaussian. We consider noise removal for the facial images corrupted by both poisson and salt-and-pepper noise with ratio of 10% (see Fig. 7) and compare BRTF with tensor based methods including HORPCA, CP-ALS, TuckerARD [19], and HOSVD. Note that the tuning parameters of HORPCA were selected carefully by using the ground-truth of images while BRTF and Tucker-ARD can automatically learn the model parameters without requiring fine-tuning. CP-ALS was performed using the CP rank obtained from BRTF and HOSVD was performed using the multilinear rank obtained from Tucker-ARD.\nFig. 7 shows the qualitative results of some example images including 2 people under 5 poses and 2 lighting variations. We can observe that BRTF and HORPCA are robust to non-Gaussian noises and obtain the satisfactory visual quality, while CP-ALS, HOSVD and Tucker-ARD cannot perform well for non-Gaussian noise removal. The detailed quantitative evaluation results are shown in Table 3 which\n12\ncontains recovery performance evaluated by RRSE and PSNR as well as computational efficiency measured by runtime. BRTF and HORPCA outperform the other methods significantly because of robust property of their model assumptions. It should be noted that although HORPCA tuned parameters by the ground-truth, resulting in the best possible performance, BRTF still outperforms HORPCA in terms of recovery performance. Due to multiple runs for tuning parameter selections, the computation of HORPCA is much more expensive than BRTF. Tucker-ARD is another method that can achieve automatic model selection, however, it cannot handle non-Gaussian noise and its computational efficiency is quite low. These results demonstrate the superiority of BRTF in terms of denoising performance, robustness to nonGaussian noise and computational efficiency. In summary, BRTF is a robust method which is similar to HORPCA; BRTF can achieve automatic rank determination which is similar to Tucker-ARD; BRTF is based on CP factorization which is similar to CPALS. The most significant characteristic of BRTF is the hierarchical probabilistic tensor model and full posterior inference of all unknown variables."}, {"heading": "7 CONCLUSION", "text": "In this paper, we have proposed a fully Bayesian generative model for robust tensor factorization, which can naturally handle missing data and outliers, together with the corresponding algorithm for efficient\nmodel inference. Our method has several significant characteristics: 1) a general framework for an arbitrary order tensor; 2) robustness to outliers, non-Gaussian noises, and overfitting; 3) tuning parameters free due to an automatic rank determination and automatic parameter selection; 4) closed-form VB algorithm for efficient and deterministic inference. Comprehensive experiments and comparisons on synthetic and realworld datasets have confirmed the superiorities of BRTF over state-of-the-art robust methods and tensor factorization methods. Therefore, BRTF has proven to be promising for robust tensor factorization, robust tensor completion and outlier detection.\nThe Appendix for detailed proofs and derivations and several videos for demonstration are provided in supplementary materials. The Matlab codes for demonstrations on synthetic data and video background modeling are also provided at http://www.bsp.brain.riken.jp/\u223cqibin/homepage/Software files/ BayesRobustTensorFactorizationP.rar."}], "references": [{"title": "Tensor Decompositions and Applications", "author": ["T. Kolda", "B. Bader"], "venue": "SIAM Review, vol. 51, no. 3, pp. 455\u2013500, 2009.  13", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonnegative Matrix and Tensor Factorizations", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.I. Amari"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Temporal collaborative filtering with Bayesian probabilistic tensor factorization", "author": ["L. Xiong", "X. Chen", "T.-K. Huang", "J. Schneider", "J.G. Carbonell"], "venue": "Proceedings of SIAM Data Mining, vol. 2010, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic latent tensor factorization model for link pattern prediction in multi-relational networks", "author": ["S. Gao", "L. Denoyer", "P. Gallinari", "J. GUO"], "venue": "The Journal of China Universities of Posts and Telecommunications, vol. 19, pp. 172\u2013181, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Higher order partial least squares (hopls): A generalized multilinear regression method", "author": ["Q. Zhao", "C.F. Caiafa", "D.P. Mandic", "Z.C. Chao", "Y. Nagasaka", "N. Fujii", "L. Zhang", "A. Cichocki"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 7, pp. 1660\u20131673, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Simultaneous tensor decomposition and completion using factor priors", "author": ["Y. Chen", "C. Hsu", "H. Liao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 3, pp. 577\u2013591, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A multilinear singular value decomposition", "author": ["L. De Lathauwer", "B. De Moor", "J. Vandewalle"], "venue": "SIAM journal on Matrix Analysis and Applications, vol. 21, no. 4, pp. 1253\u20131278, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Canonical polyadic decomposition with a columnwise orthonormal factor matrix", "author": ["M. S\u00f8rensen", "L.D. Lathauwer", "P. Comon", "S. Icart", "L. Deneire"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 33, no. 4, pp. 1190\u20131213, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable tensor factorizations for incomplete data", "author": ["E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. M\u00f8rup"], "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 106, no. 1, pp. 41\u201356, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimizationbased algorithms for tensor decompositions: Canonical polyadic decomposition, decomposition in rank-(Lr, Lr, 1) terms, and a new generalization", "author": ["L. Sorber", "M. Van Barel", "L. De Lathauwer"], "venue": "SIAM Journal on Optimization, vol. 23, no. 2, pp. 695\u2013720, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic models for incomplete multi-dimensional arrays", "author": ["W. Chu", "Z. Ghahramani"], "venue": "JMLR Workshop and Conference Proceedings, vol. 5, 2009, pp. 89\u201396.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Exponential family tensor factorization for missing-values prediction and anomaly detection", "author": ["K. Hayashi", "T. Takenouchi", "T. Shibata", "Y. Kamiya", "D. Kato", "K. Kunieda", "K. Yamada", "K. Ikeda"], "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on. IEEE, 2010, pp. 216\u2013225.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Infinite Tucker decomposition: Nonparametric Bayesian models for multiway data analysis", "author": ["Z. Xu", "F. Yan", "A. Qi"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012, pp. 1023\u20131030.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Tensor rank, invariants, inequalities, and applications", "author": ["E.S. Allman", "P.D. Jarvis", "J.A. Rhodes", "J.G. Sumner"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 34, no. 3, pp. 1014\u20131045, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Most tensor problems are NPhard", "author": ["C.J. Hillar", "L.-H. Lim"], "venue": "Journal of the ACM (JACM), vol. 60, no. 6, p. 45, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["V. De Silva", "L.-H. Lim"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 3, pp. 1084\u20131127, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Tensor rank: Some lower and upper bounds", "author": ["B. Alexeev", "M.A. Forbes", "J. Tsimerman"], "venue": "Computational Complexity (CCC), 2011 IEEE 26th Annual Conference on. IEEE, 2011, pp. 283\u2013291.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Geometric complexity theory and tensor rank", "author": ["P. B\u00fcrgisser", "C. Ikenmeyer"], "venue": "Proceedings of the 43rd annual ACM symposium on Theory of computing. ACM, 2011, pp. 509\u2013518.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic relevance determination for multi-way models", "author": ["M. M\u00f8rup", "L.K. Hansen"], "venue": "Journal of Chemometrics, vol. 23, no. 7-8, pp. 352\u2013363, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 208\u2013220, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Tensor completion via a multi-linear low-n-rank factorization model", "author": ["H. Tan", "B. Cheng", "W. Wang", "Y.-J. Zhang", "B. Ran"], "venue": "Neurocomputing, vol. 133, pp. 161\u2013169, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with tensors: a framework based on convex opti-  mization and spectral regularization", "author": ["M. Signoretto", "Q.T. Dinh", "L. De Lathauwer", "J.A. Suykens"], "venue": "Machine Learning, pp. 1\u201349, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Composite splitting algorithms for convex optimization", "author": ["J. Huang", "S. Zhang", "H. Li", "D. Metaxas"], "venue": "Computer Vision and Image Understanding, vol. 115, no. 12, pp. 1610\u20131622, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Tensor factorization using auxiliary information", "author": ["A. Narita", "K. Hayashi", "R. Tomioka", "H. Kashima"], "venue": "Data Mining and Knowledge Discovery, vol. 25, no. 2, pp. 298\u2013324, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust principal component analysis?", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Robust PCA via outlier pursuit.", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Transactions on Information Theory, vol. 58,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Online robust PCA via stochastic optimization", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 404\u2013412.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust principal component analysis with non-greedy L1-norm maximization", "author": ["F. Nie", "H. Huang", "C. Ding", "D. Luo", "H. Wang"], "venue": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence. AAAI Press, 2011, pp. 1433\u2013 1438.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Direct robust matrix factorizatoin for anomaly detection", "author": ["L. Xiong", "X. Chen", "J. Schneider"], "venue": "IEEE 11th International Conference on Data Mining (ICDM). IEEE, 2011, pp. 844\u2013853.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical low-rank matrix approximation under robust L1norm", "author": ["Y. Zheng", "G. Liu", "S. Sugimoto", "S. Yan", "M. Okutomi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 1410\u20131417.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint schatten pnorm and lp-norm robust matrix completion for missing value recovery", "author": ["F. Nie", "H. Wang", "H. Huang", "C. Ding"], "venue": "Knowledge and Information Systems, pp. 1\u201320, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient computation of robust weighted low-rank matrix approximations using the l1 norm", "author": ["A. Eriksson", "A. Van den Hengel"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1681\u20131690, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "GoDec: randomized low-rank & sparse matrix decomposition in noisy case", "author": ["T. Zhou", "D. Tao"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML), 2011, pp. 33\u201340.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust tensor factorization using R1 norm", "author": ["H. Huang", "C. Ding"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2008, pp. 1\u20138.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust Tucker tensor decomposition for effective image representation", "author": ["M. Zhang", "C. Ding"], "venue": "The IEEE International Conference on Computer Vision (ICCV), December 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust tensor subspace learning for anomaly detection", "author": ["J. Li", "G. Han", "J. Wen", "X. Gao"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 2, no. 2, pp. 89\u201398, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust low-rank tensor recovery: Models and algorithms", "author": ["D. Goldfarb", "Z. Qin"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 35, no. 1, pp. 225\u2013253, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian robust principal component analysis", "author": ["X. Ding", "L. He", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 20, no. 12, pp. 3419\u20133430, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Bayesian robust PCA of incomplete data", "author": ["J. Luttinen", "A. Ilin", "J. Karhunen"], "venue": "Neural processing letters, vol. 36, no. 2, pp. 189\u2013202, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic approach to robust matrix factorization", "author": ["N. Wang", "T. Yao", "J. Wang", "D.-Y. Yeung"], "venue": "Computer Vision\u2013 ECCV. Springer, 2012, pp. 126\u2013139.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian robust matrix factorization for image and video processing", "author": ["N. Wang", "D.-Y. Yeung"], "venue": "Proceedings of International Conference on Computer Vision, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse Bayesian methods for low-rank matrix estimation", "author": ["S.D. Babacan", "M. Luessi", "R. Molina", "A.K. Katsaggelos"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 3964\u2013 3977, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M.E. Tipping"], "venue": "The Journal of Machine Learning Research, vol. 1, pp. 211\u2013244, 2001.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "Variational message passing", "author": ["J.M. Winn", "C.M. Bishop"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 661\u2013694, 2005.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Moving object detection by detecting contiguous outliers in the low-rank representation", "author": ["X. Zhou", "C. Yang", "W. Yu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 3, pp. 597\u2013610, 2013. 14", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "The CMU pose, illumination, and expression database", "author": ["T. Sim", "S. Baker", "M. Bsat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 12, pp. 1615\u20131618, 2003.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 3, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 196, "endOffset": 199}, {"referenceID": 5, "context": "[1], [2], and have been successfully applied to various fields of applications such as face recognition, social network analysis, image and video completion, and brain signal processing [3], [4], [5], [6].", "startOffset": 201, "endOffset": 204}, {"referenceID": 6, "context": "The two most popular tensor factorization frameworks are Tucker [7] and CANDECOMP/PARAFAC (CP) [1], also known as canonical polyadic decomposition (CPD) [8], which naturally results in two different definitions of tensor", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "The two most popular tensor factorization frameworks are Tucker [7] and CANDECOMP/PARAFAC (CP) [1], also known as canonical polyadic decomposition (CPD) [8], which naturally results in two different definitions of tensor", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "The two most popular tensor factorization frameworks are Tucker [7] and CANDECOMP/PARAFAC (CP) [1], also known as canonical polyadic decomposition (CPD) [8], which naturally results in two different definitions of tensor", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "CP factorization with missing values has been developed by employing CP weighted optimization (CPWOPT) [9] and nonlinear least squares (CPNLS) [10].", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "CP factorization with missing values has been developed by employing CP weighted optimization (CPWOPT) [9] and nonlinear least squares (CPNLS) [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 104, "endOffset": 107}, {"referenceID": 10, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 11, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "To naturally deal with missing data, the probabilistic framework for tensor factorization was exploited [3], [11], which has been extended to exponential family model [12] and nonparametric Bayesian model [13].", "startOffset": 205, "endOffset": 209}, {"referenceID": 13, "context": "It is important to emphasize that our knowledge about the properties of tensor rank, especially CP rank, is surprisingly limited [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "There is no straightforward algorithm to compute CP rank of an explicitly given tensor, and the problem has been shown to be NP-hard [15], [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "There is no straightforward algorithm to compute CP rank of an explicitly given tensor, and the problem has been shown to be NP-hard [15], [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "In fact, determining or even bounding the tensor rank is quite difficult in contrast to matrix rank [17], [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "In fact, determining or even bounding the tensor rank is quite difficult in contrast to matrix rank [17], [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "In [19], ARD framework was applied to estimate the multilinear rank.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "to tensor completion [20], which thus attracted many studies on low multilinear rank approximations [21], [22], [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "In addition, the auxiliary information can be exploited to improve completion accuracy [6], [24], which, however, is only suitable to some specific applications.", "startOffset": 87, "endOffset": 90}, {"referenceID": 23, "context": "In addition, the auxiliary information can be exploited to improve completion accuracy [6], [24], which, however, is only suitable to some specific applications.", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 141, "endOffset": 145}, {"referenceID": 29, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 147, "endOffset": 151}, {"referenceID": 30, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 153, "endOffset": 157}, {"referenceID": 31, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 159, "endOffset": 163}, {"referenceID": 32, "context": "To handle this problem, many robust techniques have been developed such as robust PCA [25], [26], [27], [28] and robust matrix factorization [29], [30], [31], [32], [33].", "startOffset": 165, "endOffset": 169}, {"referenceID": 33, "context": "[34] and robust Tucker decompositions were studied in [35], [36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[34] and robust Tucker decompositions were studied in [35], [36].", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "[34] and robust Tucker decompositions were studied in [35], [36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "To handle both missing data and outliers, the nuclear norm regularization has been combined with L1-norm loss function, which leads to a robust tensor completion [37].", "startOffset": 162, "endOffset": 166}, {"referenceID": 37, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 72, "endOffset": 76}, {"referenceID": 38, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 119, "endOffset": 123}, {"referenceID": 40, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 125, "endOffset": 129}, {"referenceID": 41, "context": "Our work is somewhat related to probabilistic approaches for robust PCA [38], [39] and for robust matrix factorization [40], [41], [42].", "startOffset": 131, "endOffset": 135}, {"referenceID": 37, "context": "In [38], Beta-Bernoulli distribution is exploited to model outliers and the low-rank matrix exclusively, which, however, results in high model complexity and slow inference.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "Missing values are considered in [39], where the number of latent components needs to be specified in advance.", "startOffset": 33, "endOffset": 37}, {"referenceID": 41, "context": "In [42], Jeffreys prior is adopted to model both noise and outliers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "PRMF [40] uses Laplace distribution to model the residuals, while the rank of the underlying model should be given in advance.", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "A fully Bayesian treatment of PRMF [41] employs a hierarchical view of Laplace distribution as the noise model, and applies MCMC sampling for model inference.", "startOffset": 35, "endOffset": 39}, {"referenceID": 36, "context": "Higher-order robust PCA (HORPCA), proposed very recently in [37], is the only existing tensor method that can handle both missing data and outliers.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "The Kronecker product [1] of matrices A \u2208 RI\u00d7J and B \u2208 RK\u00d7L is a matrix of size IK \u00d7 JL, denoted by A \u2297B.", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "The standard CP factorization [1] is expressed by", "startOffset": 30, "endOffset": 33}, {"referenceID": 42, "context": "The priors in (7) and (8) are related to the framework of sparse Bayesian learning (SBL) [43] which is usually employed for variable selections.", "startOffset": 89, "endOffset": 93}, {"referenceID": 43, "context": "Although variational Bayesian (VB) inference [44] is difficult for derivations, especially when multiple interactions of latent factors are involved, it", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "(FME) [10] between the estimated factors and groundtruth was also evaluated.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 140, "endOffset": 143}, {"referenceID": 18, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 206, "endOffset": 209}, {"referenceID": 9, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 36, "context": "We compared our Bayesian robust tensor factorization (BRTF) with state-of-the-art methods including tensor factorization (CP-ALS [1], HOSVD [7], CP-ARD [19]), tensor factorization with missing data (CPWOPT [9] and CPNLS [10]), and robust tensor factorization with missing data (HORPCA [37]).", "startOffset": 285, "endOffset": 289}, {"referenceID": 24, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 66, "endOffset": 70}, {"referenceID": 40, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 77, "endOffset": 81}, {"referenceID": 44, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 95, "endOffset": 99}, {"referenceID": 36, "context": "PCP [25], GoDec [33], DRMF [29], RegL1ALM [30], VBRPCA [42], PRMF [40], BRMF [41], and DECOLOR [45], were performed on grayscale videos, while HORPCA [37] and BRTF were performed on original color videos represented by tensors with an additional RGB mode.", "startOffset": 150, "endOffset": 154}, {"referenceID": 45, "context": "We use the CMU-PIE face database [46] for multilinear model analysis and evaluation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "7) and compare BRTF with tensor based methods including HORPCA, CP-ALS, TuckerARD [19], and HOSVD.", "startOffset": 82, "endOffset": 86}], "year": 2017, "abstractText": "We propose a generative model for robust tensor factorization in the presence of both missing data and outliers. The objective is to explicitly infer the underlying low-CP-rank tensor capturing the global information and a sparse tensor capturing the local information (also considered as outliers), thus providing the robust predictive distribution over missing entries. The lowCP-rank tensor is modeled by multilinear interactions between multiple latent factors on which the column sparsity is enforced by a hierarchical prior, while the sparse tensor is modeled by a hierarchical view of Student-t distribution that associates an individual hyperparameter with each element independently. For model learning, we develop an efficient closed-form variational inference under a fully Bayesian treatment, which can effectively prevent the overfitting problem and scales linearly with data size. In contrast to existing related works, our method can perform model selection automatically and implicitly without need of tuning parameters. More specifically, it can discover the groundtruth of CP rank and automatically adapt the sparsity inducing priors to various types of outliers. In addition, the tradeoff between the low-rank approximation and the sparse representation can be optimized in the sense of maximum model evidence. The extensive experiments and comparisons with many state-of-the-art algorithms on both synthetic and real-world datasets demonstrate the superiorities of our method from several perspectives.", "creator": "LaTeX with hyperref package"}}}