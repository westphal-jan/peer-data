{"id": "1410.3463", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2014", "title": "Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson", "abstract": "Existing caching strategies, in the storage domain, though well suited to exploit short range spatio-temporal patterns, are unable to leverage long-range motifs for improving hitrates. Motivated by this, we investigate novel Bayesian non-parametric modeling(BNP) techniques for count vectors, to capture long range correlations for cache preloading, by mining Block I/O traces. Such traces comprise of a sequence of memory accesses that can be aggregated into high-dimensional sparse correlated count vector sequences, which can be scaled to a specific order of magnitude.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 13 Oct 2014 14:26:28 GMT  (109kb)", "http://arxiv.org/abs/1410.3463v1", null]], "reviews": [], "SUBJECTS": "cs.OS cs.LG cs.SY", "authors": ["lavanya sita tekumalla", "chiranjib bhattacharyya"], "accepted": false, "id": "1410.3463"}, "pdf": {"name": "1410.3463.pdf", "metadata": {"source": "CRF", "title": "Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson", "authors": ["Lavanya Sita Tekumalla", "Chiranjib Bhattacharyya"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n34 63\nv1 [\ncs .O\nS] 1\n3 O\nct 2\n01 4\nExisting caching strategies, in the storage domain, though well suited to exploit short range spatiotemporal patterns, are unable to leverage long-range motifs for improving hitrates. Motivated by this, we investigate novel Bayesian non-parametric modeling(BNP) techniques for count vectors, to capture long range correlations for cache preloading, by mining Block I/O traces. Such traces comprise of a sequence of memory accesses that can be aggregated into highdimensional sparse correlated count vector sequences.\nWhile there are several state of the art BNP algorithms for clustering and their temporal extensions for prediction, there has been no work on exploring these for correlated count vectors. Our first contribution addresses this gap by proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and its temporal extension(HMM-DP-MMVP) that captures the full covariance structure of multivariate count data. However, modeling full covariance structure for count vectors is computationally expensive, particularly for high dimensional data. Hence, we exploit sparsity in our count vectors, and as our main contribution, introduce the Sparse DP mixture of multivariate Poisson(Sparse-DPMMVP), generalizing our DP-MMVP mixture model, also leading to more efficient inference. We then discuss a temporal extension to our model for cache preloading.\nWe take the first step towards mining historical data, to capture long range patterns in storage traces for cache preloading. Experimentally, we show a dramatic improvement in hitrates on benchmark traces and lay the groundwork for further research in storage domain to reduce latencies using data mining techniques to capture long range motifs.\n1 Introduction\nBayesian non-parametric modeling, while well explored for mixture modeling of categorical and real valued data, has not been explored for multivariate count data. We explore BNP models for sparse correlated count vectors to mine block I/O traces from enterprise storage servers for Cache Preloading.\n\u2217This work was done in collaboration with NetApp, Inc. \u2020Indian Institute of Science\nExisting caching policies in systems domain, are either based on eviction strategies of removing the least relevant data from cache (Ex: Least Recently Used a.k.a LRU) or read ahead strategies for sequential access patterns. These strategies are well suited for certain types of workloads where nearby memory accesses are correlated in extremely short intervals of time, typically in milli-secs. However, often in real workloads, we find correlated memory accesses spanning long intervals of time (See fig 1), exhibiting no discernible correlations over short intervals of time (see fig 2).\nThere has been no prior work on analyzing trace data to learn long range access patterns for predicting future accesses. We explore caching alternatives to automatically learn long range spatio-temporal correlation structure by analyzing the trace using novel BNP techniques for count data, and exploit it to pro-actively preload data into cache and improve hitrates.\nCapturing long range access patterns in Trace Data: Block I/O traces comprise of a sequence of memory block access requests (often spanning millions per day). We are interested in mining such traces to capture spatio-temporal correlations arising from repetitive long range access patterns (see fig 1). For instance, every time a certain file is read, a similar sequence of accesses might be requested.\nWe approach this problem of capturing the longrange patterns by taking a more aggregated view of the data to understand longer range dependencies. We partition both the memory and time into discrete chunks and constructing histograms over memory bins for each time slice (spanning several seconds) to get a coarser view of the data. Hence, we aggregate the data into a sequence of count vectors, one for each time slice, where each component of the count vector records the count of memory access in a specific bin (a large region of memory blocks) in that time interval. Thus, a trace is transformed into a sequence of count vectors.\nThus, the components of count vector instances after aggregation are correlated within each instance since memory access requests are often characterized by spatial correlation, where adjacent regions of memory are likely to be accessed together. This leads to a rich covariance structure. Further, due to the long range temporal dependencies in access patterns (fig 1), the sequence of aggregated count vectors are also temporally correlated. Count vectors thus obtained by aggregating over time and space are also sparse, where only a small portion of memory is accessed in any time interval (fig 2). Hence a small subset of count vector dimensions have significant non zero values. Modeling such sparse correlated count vector sequences to understand their spatio-temporal structure remains unaddressed.\nModeling Sparse Correlated Count Vector Sequences: A common technique for modeling temporal correlations are Hidden Markov Models(HMMs) which are mixture model extensions for temporal data. Owing to high variability in access patterns inherent in storage traces, finite mixture models do not suffice for our application since the number of mixture components varies often based on the type of workload being modeled and the kind of access patterns. BNP techniques address this issue by automatically adjusting the number of mixture components based on complexity of data. Non-parametric clustering with Dirichlet Process(DP) mixtures and their temporal variants have been extensively studied over the past decade [15], [16]. However, to the best of our knowledge we are not aware of such models in the context of count data, particularly temporally correlated sparse count vectors.\nPoisson distribution is a natural prior for counts and the Multivariate Poisson(MVP) for correlated count vectors. However, owing to the structure of multivariate Poisson and its computational intractability [17] non parametric mixture modeling for multivariate count vectors has received less attention. Hence, we first bridge this gap by paralleling the development of DP based non-parametric mixture models and their temporal extensions for multivariate count data along the lines of those for Gaussians and multinomials .\nModeling the full covariance structure using the MVP is often computationally expensive. Hence, we further exploit the sparsity in data and introduce sparse mixture models for count vectors and their temporal extensions. We propose a sparse MVP Mixture modeling the covariance structure over a select subset of dimensions for each cluster. We are not aware of any prior work that models sparsity in count vectors.\nThe proposed predictive models showed dramatic hitrate improvement on several real world traces. At the same time, these count modeling techniques are of independent interest outside the caching problem as they can apply to a wide variety of settings such as text mining, where often counts are used.\nContributions: Our first contribution, is the DP based non-parametric mixture of Multivariate Poisson (DP-MMVP) and its temporal extensions (HMM-DPMMVP) which capture the full covariance structure of correlated count vectors. Our next contribution, is to exploit the sparsity in data, and proposing a novel technique for non parametric clustering of sparse high dimensional count vectors with the sparse DPmixture of Multivariate Poisson (Sparse-DP-MMVP). This methodology not only leads to a better fit for sparse multidimensional count data but is also computationally more tractable than modeling full covariance. We then discuss a temporal extension, Sparse-HMMDP-MMVP, for cache preloading. We are not aware of any prior work that addresses non-parametric modeling of sparse correlated count vectors.\nAs our final contribution, we take the first steps in outlining a framework for cache preloading to capture long range spatio-temporal dependencies in memory accesses. We perform experiments on real-world benchmark traces showing dramatic hitrate improvements. In particular, for the trace in Fig 1, our preloading yielded a 0.498 hitrate over 0.001 of baseline (without preloading), a 498X improvement (trace MT2: Tab 2).\n2 Related Work\nBNP for sparse correlated count vectors: Poisson distribution is a natural prior for count data. But the multivariate Poisson(MVP) [9] has seen limited use due to its computational intractability due to the\ncomplicated form of the joint probability function[17]. There has been relatively little work on MVP mixtures [8, 2, 13, 14]. On the important problem of designing MVP mixtures with an unknown number of components, [13] is the only reference we are aware of. The authors explore MVP mixture with an unknown number of components using a truncated Poisson prior for the number of mixture components, and perform uncollapsed RJMCMC inference. DP based models are a natural truncation free alternative that are well studied and amenable to hierarchical extensions [16, 4] for temporal modeling which are of immediate interest to the caching problem. To the best of our knowledge, there has been no work that examines a truncation free non-parametric approach with DP-based mixture modeling for MVP. Another modeling aspect we address is the sparsity of data. A full multivariate emission density over all components for each cluster may result in over-fitting due to excessive number of parameters introduced from unused components. We are not aware of any work on sparse MVP mixtures. There has been some work on sparse mixture models for Gaussian and multinomial densities [11] [18]. However they are specialized to the individual distributions and do not apply here. Finally, we investigate sparse MVP models for temporally correlated count vectors. We are not aware of any prior work that investigates mixture models for temporally correlated count vectors.\nCache Preloading: Preloading has been studied before [20] in the context of improving cache performance on enterprise storage servers for the problem of cache warm-up, of a cold cache by preloading the most recently accessed data, by analyzing block I/O traces. Our goal is however different, and more general, in that we are seeking to improve the cumulative hit rate by exploiting long ranging temporal dependencies even in the case of an already warmed up cache. They also serve as an excellent reference for state of the art caching related studies and present a detailed study of the properties of MSR traces. We have used the same MSR traces as our benchmark. Fine-grained prefetching tehniques to exploit short range correlations [6, 12], some specialized for sequential workload types [5] (SARC) have been investigated in the past. Our focus, however is to work with general non-sequential workloads, to capture long range access patterns, exploring prediction at larger timescales. Improving cache performance by predicting future accesses based on modeling file-system events was studied in [10]. They operate over NFS traces containing details of file system level events. This technique is not amenable for our setting, where the only data source is block I/O traces, with no file system level data."}, {"heading": "3 A framework for Cache Preloading based on mining Block I/O traces", "text": "In this section we briefly describe the caching problem and describe our framework for cache preloading.\n3.1 The Caching Problem:Application data is usually stored on a slower persistent storage medium like hard disk. A subset of this data is usually stored on cache, a faster storage medium. When an application makes an I/O request for a specific block, if the requested block is in cache, it is serviced from cache. This constitutes a cache hit with a low application latency (in microseconds). Else, in the event of a Cache miss, the requested block is first retrieved from hard disk into cache and then serviced from cache leading to much higher application latency (in milliseconds).\nThus, the application\u2019s performance improvement is measured by hitrate = #cachehits#cachehits+#cachemisses .\n3.2 The Cache Preloading Strategy:Our strategy involves observing a part of the trace Dlr for some period of time and deriving a model, which we term as Learning Phase. We then use this model to keep predicting appropriate data to place in cache to improve hitrates in Operating Phase at the end of each time slice (\u03bd secs) for the rest of the trace Dop.\nIn terms of the execution time of our algorithms, while the learning phase can take a few hours, the operational phase, is designed to run in time much less than the slice length of \u03bd secs. In this paper we restrict ourselves to learning from a fixed initial portion of the trace. In practice, the learning phase can be repeated periodically, or even done on an online fashion.\nData Aggregation: As the goal is to improve hitrates by preloading data exploiting long range dependencies, we capture this by aggregating trace data into count vector sequences. We consider a partitioning of addressable memory (LBA Range) into M equal bins. In the learning phase, we divide the trace Dlr into Tlr fixed length time interval slices of length \u03bd seconds each. Let A1, . . . , ATlr be the set of actual access requests in each interval of \u03bd seconds. We now aggregate the trace into a sequence of Tlr count vectors X1, . . . XTlr \u2208 Z\nM , each of M dimensions. Each count vector Xt is a histogram of accesses in At over M memory bins in the tth time slice of the trace spanning \u03bd seconds.\n3.3 Learning Phase (Learning a latent variable model):The input to the learning phase is a set of sparse count vectors X1, . . . , XTlr \u2208 Z\nM , correlated within and across instances obtained from a block I/O trace as described earlier. These count vectors can often be intrinsically grouped into cohesive clusters which arise as a result of long range access patterns (see Figure 1) that repeat over time albeit with some randomness.\nHence we would like to explore unsupervised learning techniques based on clustering for these count vectors, that capture temporal dependencies between count vector instances and the correlation within instances.\nHidden Markov Models(HMM), are a natural choice of predictive models for such temporal data. In a HMM, latent variables Zt \u2208 {1, . . . ,K} are introduced that follow a markov chain. Each Xt is generated based on the choice of Zt, inducing a clustering of count vectors. In the learning phase, we learn the HMM parameters, denoted by \u03b8.\nOwing to the variability of access patterns in trace data, a fixed value of K is not suitable for use in realistic scenarios motivating the use of non-parametric techniques of clustering. In section 4 we propose the HMMDP-MMVP, a temporal model for non-parametric clustering of correlated count vector sequences capturing their full covariance structure, followed by the SparseHMM-DP-MMVP in section 5, that exploits the sparsity in count vectors to better model the data, also leading to more efficient inference.\nAs an outcome of the learning phase, we have a HMM based model with appropriate parameters, that provides predictive ability to infer the next hidden state on observing a sequence of count vectors. However, since the final prediction required is that of memory accesses, we maintain a map from every value of hidden state k to the set of all raw access requests from various time slices during training that were assigned latent state k. H(k) = \u222a{t|Zt=k}At, for \u2200k."}, {"heading": "3.4 The Operating Phase (Prediction for", "text": "Preloading):Having observed {X1, . . . XTlr} aggregated from Dlr, the learning phase learns a latent variable model. In the Operating Phase, as we keep observing Dop, after the time interval t\u2032, the data is incrementally aggregated into a sequence {X \u20321, . . .X \u2032 t\u2032}. At this point, we would like the model to predict the best possible choice of blocks to load into cache for interval t\u2032 + 1 with knowledge of aggregated data {X \u20321, . . . X \u2032 t\u2032}.\nThis prediction happens in two steps. In the first step, our HMM based model Sparse-HMM-DPMMVP infers hidden state Z \u2032t\u2032+1 from observations {X \u20321, . . . , X \u2032 t\u2032}, using a Viterbi style algorithm as follows. (3.1)\n(X \u2032t\u2032+1, {Z \u2032 r} t\u2032+1 r=1 ) = argmax\n(X\u2032 t\u2032+1 ,{Z\u2032r} t\u2032+1 r=1 )\np({X \u2032r} t\u2032+1 r=1 , {Z \u2032 r} t\u2032+1 r=1 |\u03b8)\nNote the slight deviation from usual Viterbi method as X \u2032t\u2032+1 is not yet observed. We also note that alternate strategies based on MCMC might be possible based on Bayesian techniques to infer the hidden state Z \u2032t\u2032+1. However, in the operating phase, the execution time becomes important and is required to be much smaller than \u03bd, the slice length. Hence we explore such a Viterbi\nbased technique, that is quite efficient and runs in a very small fraction of \u03bd in practice for each prediction. The algorithm is detailed in the supplementary material.\nIn the second step, having predicted the hidden state Z \u2032t\u2032+1, we would now like to load the appropriate accesses. Our prediction scheme consists of loading all accesses defined by H(Z \u2032t\u2032+1) into the cache (with H as defined previously)."}, {"heading": "4 Mixture Models with Multivariate Poisson for correlated count vector sequences", "text": "We now describe non-parametric temporal models for correlated count vectors based on the MVP [9] mixtures. MVP [9] distributions are natural models for understanding multi-dimensional count data. There has been no work on exploring DP-based mixture models for count data or for modeling their temporal dependencies.\nHence, we first parallel the development of nonparametric MVP mixtures along the lines of DP based multinomial mixtures [16]. To this end we propose DP-MMVP, a DP based MVP mixture and propose a temporal extension HMM-DP-MMVP along the lines of HDP-HMM[16] for multinomial mixtures.\nHowever a more interesting challenge lies in designing algorithms of scalable complexity for high dimensional correlated count vectors. We address this in our next section( 5) by introducing the Sparse-MVP that exploits sparsity in data. DP mixtures of MVP and their sparse counterparts lead to different inference challenges addressed in section 6.\n4.1 Preliminaries:We first recall some definitions. A probability distribution G \u223c DP (\u03b1,H), when G = \u2211\u221e\nk=1 \u03b2k\u03b4\u03b8k , \u03b2 \u223c GEM(\u03b1), \u03b8k \u223c H, k = 1 . . . where H is a diffused measure. Probability measures G1 , . . . , GJ follow Hierarchical Dirichlet process(HDP)[16] if"}, {"heading": "Gj \u223c DP (\u03b1,G0), j = 1 . . . J where G0 \u223c DP (\u03b1,H)", "text": "HMMs are popular models for temporal data. However, for most applications there are no clear guidelines for fixing the number of HMM states. A DP based HMM model, HDP-HMM, [16] alleviats this need. Let X1, . . . , XT be observed data instances. Further, for any L \u2208 Z, we introduce notation [L] = {1, . . . , L}. The HDP-HMM is defined as follows. \u03b2 \u223c GEM(\u03b3)\n\u03c0k|\u03b2, \u03b1k \u223c DP (\u03b1k, \u03b2), and \u03b8k|H \u223c H, k = 1, 2, . . .\nZt|Zt \u2212 1, \u03c0 \u223c \u03c0Zt\u22121 , and Xt|Zt \u223c fZt(\u03b8k), t \u2208 [T ]\nCommonly used base distributions for H are the multivariate Gaussian and multinomial distributions. There has been no work in exploring correlated count vector emissions. In our setting, we explore MVP emissions with H being an appropriate prior for parameter \u03b8k of the MVP distribution.\nThe Multivariate Poisson(MVP): Let a\u0304, b\u0304 > 0. A random vector, X \u2208 ZM is Multivariate Poisson(MVP) distributed, denoted by X \u223c MV P (\u039b), if\nX = Y 1M Alternately, Xj =\nM\u2211\nl=1\nYjl, \u2200j \u2208 [M ]\nwhere \u2200j \u2264 l \u2208 [M ], \u03bbl,j = \u03bbj,l \u223c Gamma(a\u0304, b\u0304)\nYj,l = Yl,j \u223c Poisson(\u03bbj,l)(4.2)\nand 1M is a M dimensional vector of all 1s. It is useful to note that E(X) = \u039b1M , where \u039b is an M \u00d7M symmetric matrix with entries \u03bbj,l and Cov(Xj , Xl) = \u03bbj,l. Setting \u03bbj,l = 0, j 6= l yields Xi = Yi,i which we refer to as the Independent Poisson (IP) model as Yi,i for each dimension i are independently Poisson distributed."}, {"heading": "4.2 DP Mixture of Multivariate Poisson", "text": "(DP-MMVP): In this section we define DP-MMVP, a DP based non-parametric mixture model for clustering correlated count vectors. We propose to use a DP based prior, G \u223c DP (\u03b1,H), where H is a suitably chosen Gamma conjugate prior for the parameters of MVP, \u039b = {\u039bk : k = 1, . . .}, k being cluster identifier. We define DP-MMVP as follows.\n\u03bbkjl \u223c Gamma(a\u0304, b\u0304),\u2200j \u2264 l \u2208 [M ], k = 1, . . .\n\u03b2 \u223c GEM(\u03b1) and G =\n\u221e\u2211\nk=1\n\u03b2k\u03b4\u039bk\nZt|\u03b2 \u223c Mult(\u03b2)\u2200t \u2208 [T ]\nXt|Zt \u223c MV P (\u039bZt),\u2200t \u2208 [T ](4.3)\nwhere T is the number of observations and (\u039bk)jl = (\u039bk)lj = \u03bbkjl. We also note that the DP Mixture of Independent Poisson (DP-MIP) can be similarly defined by restricting \u03bbk,j,l = 0, \u2200j 6= l, k = 1, . . ..\n4.3 Temporal DP Mixture of MVP (HMM-DP-MMVP):DP-MMVP model does not capture temporal correlations that are useful for prediction problem of cache preloading. To this end we propose HMM-DPMMVP, a temporal extension of the previous model, as follows. Let Xt \u2208 Z\nM , t \u2208 [T ] be a temporal sequence of correlated count vectors.\n\u03bbkjl \u223c Gamma(a\u0304, b\u0304) \u2200j \u2264 l \u2208 [M ], k = 1, . . .\n\u03b2 \u223c GEM(\u03b3) \u03c0k|\u03b2, \u03b1k \u223c DP (\u03b1k, \u03b2)\u2200k = 1, . . .\nZt|Zt \u2212 1, \u03c0 \u223c \u03c0Zt\u22121 ,\u2200t \u2208 [T ]\nXt|Zt \u223c MV P (\u039bZt),\u2200t \u2208 [T ](4.4)\nThe HMM-DP-MMVP incorporates the HDP-HMM structure into DP-MMVP in equation (4.3). This model can again be restricted to the special case of diagonal covariance MVP giving rise to the HMM-DP-MIP by extending the DP-MIP model. The HMM-DP-MIP models the temporal dependence, but not the spatial correlation coming from trace data."}, {"heading": "5 Modeling with Sparse Multivariate Poisson:", "text": "We now introduce the Sparse Multivariate Poisson (SMVP). Full covariance MVP, defined with (\nM 2\n)\nlatent variables (in Y) is computationally expensive during inference for higher dimensions. However, vectors Xt emanating from traces are often very sparse with only a few significant components and most components close to 0. While there has been work on sparse multinomial[19] and sparse Gaussian[7] mixtures, there has been no work on sparse MVP Mixtures. We propose the SMVP by extending the MVP to model sparse count vectors. We then extend this to Sparse-DP-MMVP for a non-parametric mixture setting and finally propose the temporal extension Sparse-HMM-DP-MMVP."}, {"heading": "5.1 Sparse Multivariate Poisson distribution", "text": "(SMVP):We introduce the SMVP as follows. Consider an indicator vector b \u2208 {0, 1}M , that denotes whether\na dimension is active or not. Let \u03bb\u0302j \u2265 0, \u2200j \u2208 [M ] and b \u2208 {0, 1}M . We define X \u223c SMV P (\u039b, \u03bb\u0302, b) as: X = Y 1M where \u2200j \u2264 l \u2208 [M ]\n(5.5) Yj,l \u223c Poisson(\u03bbj,l)bjbl + Poisson(\u03bb\u0302j)(1\u2212 bj)\u03b4(j, l))\nwhere \u039b is a symmetric positive matrix with (\u039b)jl = \u03bbjl. If bj = 1, bl = 1 then Yj,l is distributed as Poisson(\u03bbj,l). However if bj = 0, variables Yj,j are distributed as Poisson(\u03bb\u0302j). The selection variables bj decide if the jth dimension is active. Otherwise we consider any emission at the jth dimension noise, modulated by Poisson(\u03bb\u0302j), independent of other dimensions. Parameter \u03bb\u0302j is close to zero for the extraneous noise dimensions and is common across clusters.\nWith Sparse-MVP, we are defining a full covariance MVP for a subset of dimensions while the rest of the dimensions are inactive and hence modeled independantly (with a small mean to account for noise). The full covariance MVP is a special case of Sparse-MVP where all dimensions are active.\n5.2 DP Mixture of Sparse Multivariate Poisson: In this section, we propose the Sparse-DPMMVP, extending our DP-MMVP model. For every mixture component k we introduce an indicator vector bk \u2208 {0, 1}\nM . Hence, bk,j denotes whether a dimension j is active for mixture component k.\nA natural prior for selection variables, bkj , is Bernoulli Distribution, while a Beta distribution is a natural conjugate prior for the parameter \u03b7j of the Bernaulli. \u03b7j \u223c Beta(a \u2032, b\u2032), bkj \u223c Bernoulli(\u03b7j), j \u2208 [M ], k = 1, . . . . The priors for parameters \u039b and \u03bb\u0302 are again decided based on conjugacy, where \u2200j \u2264 l \u2208\n[M ] \u03bbj,l have a gamma prior, Let a\u0302, b\u0302 > 0. We model \u03bb\u0302 to have a common Gamma prior for inactive dimen-\nsions over all clusters. \u03bb\u0302j \u223c Gamma(a\u0302, b\u0302), \u2200j \u2208 [M ] . The Sparse-DP-MMVP is defined as:\n\u03b7j \u223c Beta(a \u2032 , b \u2032), bk,j \u223c Bernoulli(\u03b7j), j \u2208 [M ], k = 1 . . .\n\u03bb\u0302j \u223c Gamma(a\u0302, b\u0302), j \u2208 [M ]\n\u03bbkjl \u223c Gamma(a\u0304, b\u0304), {j \u2264 l \u2208 [M ] : bk,j = bk,l = 1}, k = 1, . . .\nG = \u221e\u2211\nk=1\n\u03b2k\u03b4\u039bk , \u03b2 \u223c GEM(\u03b1)\nCluster selection variables Zt|\u03b2 \u223c Mult(\u03b2) and\nXt|Zt,\u039b, \u03bb\u0302, bZt \u223c SMV P (\u039bZt , \u03bb\u0302, bZt), t \u2208 [T ](5.6)\nDP-MMVP is a special case of Sparse-DP-MMVP where all dimensions of all clusters are active.\n5.3 Temporal Sparse Multivariate Poisson Mixture:We now define Sparse-HMM-DPMMVP, by extending Sparse-DP-MMVP to also capture Temporal correlation between instances by incorporating HDP-HMM into the Sparse-DP-MMVP:\n\u03b7j \u223c Beta(a \u2032 , b \u2032), bk,j \u223c Bernoulli(\u03b7j), j \u2208 [M ], k = 1 . . .\n\u03bb\u0302j \u223c Gamma(a\u0302, b\u0302), j \u2208 [M ]\n\u03bbkjl \u223c Gamma(a\u0304, b\u0304), {j \u2264 l \u2208 [M ] : bk,j = bk,l = 1}, k = 1, . . .\n\u03b2 \u223c GEM(\u03b3) and \u03c0k|\u03b2, \u03b1k \u223c DP (\u03b1k, \u03b2), k = 1, . . .\nZt|Zt \u2212 1, \u03c0 \u223c \u03c0Zt\u22121 , t \u2208 [T ]\nXt|Zt,\u039bZt , \u03bb\u0302, bZt \u223c SMV P (\u039bZt , \u03bb\u0302, bZt), t \u2208 [T ](5.7)\nThe plate diagram for the Sparse-HMM-DP-MMVP model is shown in figure 3. We again note that HMM-\nDP-MMVP model described in section 4.3 is a restricted form of Sparse-HMM-DP-MMVP where bk,j is fixed to 1. The Sparse-HMM-DP-MMVP captures the spatial correlation inherent in the trace data and the long range temporal dependencies, at the same time exploiting sparseness, reducing the number of latent variables.\n6 Inference\nWhile inference for non-parametric HMMs is well explored [16][4], MVP and Sparse-MVP emissions introduce additional challenges due to the latent variables involved in the definition of the MVP and the introduction of sparsity in a DP mixture setting for the MVP.\nWe discuss the inference of DP-MMVP in detail in the supplementary material. In this section, we give a brief overview of collapsed Gibbs sampling inference for HMM-DP-MMVP and Sparse-HMM-DP-MMVP. More details are again in the supplementary material.\nThroughout this section, we use the following notation: Y = {Yt : t \u2208 [T ]}, Z = {Zt : t \u2208 [T ] and X = {Xt : t \u2208 [T ]. A set with a subscript starting with a hyphen(\u2212) indicates the set of all elements except the index following the hyphen. The latent variables to be sampled are Zt, t \u2208 [T ], Yt,j,l, j \u2264 l \u2208 [M ], t \u2208 [T ] and bk,j , j \u2208 [M ], k = 1, . . . (For the sparse model). Further we have \u03b2 = {\u03b21, . . . , \u03b2K , \u03b2K+1 = \u2211\u221e r=K+1 \u03b2r}. and an auxiliary variable mk is introduced as a latent variable to aid the sampling of \u03b2 based on the direct sampling procedure from HDP[16]. Updates for m and \u03b2 are similar to [4], as detailed in algorithm 1. The latent variables \u039b and \u03c0 are collapsed.\nSampling Zt, t \u2208 [T ] While the update for this variable is similar to that in [16][4], the likelihood term p(Y |Zt = k, Z\u2212t, Y\u2212t,b\nold; a\u0304, b\u0304) differs due to the MVP based emissions. For the HMM-DP-MMVP, this term can be evaluated by integrating out the \u03bbs. Similarly for Sparse-HMM-DP-MMVP when k is an existing componant (for which bk is known).\nHowever, for the Sparse-HMM-DP-MMVP, an additional complication arises for the case of a new componant, since we do not know the bK+1 value, requiring summing over all possibilities of bK+1, leading to exponential complexity. Hence, we evaluate this numerically (see supplementary material for details). This process is summarized in algorithm 1.\nSampling Yt,j,l, j \u2264 l \u2208 [M ], t \u2208 [T ] : The Yt,j,l latent variables in the MVP definition, differentiate the inference procedure of an MVP mixture from standard inference for DP mixtures. Further, the large number of Yt,j,l variables ( n 2 )\nalso leads to computationally expensive inference for higher dimensions motivating sparse modeling. In, Sparse-HMM-DP-MMVP, only those Yt,j,l values are updated for which bZt,j = bZt,l = 1.\nWe have, for each dimension j, Xt,j = \u2211M\nl=1 Yt,j,l. To preserve this constraint, suppose for row j, we sample Yt,j,l, j 6= l, Yt,j,j becomes a derived quantity as Yt,j,j = Xt,j \u2212 \u2211M\np=1,p6=j Yt,p,j . We also note that, updating the value of Yt,j,l impacts the value of only two other random variables i.e Yt,j,j and Yt,l,l. The final update for Yt,j,l, j 6= l can be obtained by integrating out \u039b.\n(full expression in alg 1, more details: Appendix B).\nAlgorithm 1: Inference: Sparse-HMM-DP-MMVP Inference steps(The steps for HMM-DP-MMVP are similar and\nare shown as alternate updates in brackets)\nRepeat until convergence for t = 1, . . . , T do\n// Sample Zt from\np(Zt = k|Z\u2212t,\u2212(t+1), zt+1 = l,b old, X, \u03b2, Y ;\u03b1, a\u0304, b\u0304)\n\u221d p(Zt = k, Z\u2212t,\u2212(t+1),t+1 = l|\u03b2;\u03b1, a\u0304, b\u0304)\np(Y |Zt = k, Z\u2212t, Y\u2212t,b old; a\u0304, b\u0304)\n//Case 1: For HMM-DP-MMVP (with bk,j = 1 \u2200j,\u2200k) //and Sparse-HMM-DP-MMVP For existing k\np(Y |Zt = k, Z\u2212t, Y\u2212t,b old; a\u0304, b\u0304) \u221d \u03a0\nj\u2264l\u2208[M] F\nbk,jbk,l k,j,l\n\u03a0 j\u2208[M] F\u0302j\nWhere Fk,j,l = \u0393(a\u0304 + Sk,j,l)\n(b\u0304+ nk) (a\u0304+Sk,j,l) \u03a0\nt\u0304:Zt\u0304=k Yt\u0304,j,l!\nand F\u0302j = \u0393(a\u0302 + S\u0302j)\n(b\u0302 + n\u0302j) (a\u0302+S\u0302j) \u03a0\nt,j:bZt,j=0 Yt,j,j !\n, With S\u0302j = \u2211 t Yt,j,j(1 \u2212 bZt,j) , n\u0302j = \u2211\nt(1\u2212 bZt,j) and Sk,j,l = \u2211 t\u0304 Yt\u0304,j,l\u03b4(Zt\u0304, k), for j \u2264 l \u2208 [M ] //Case 2: For Sparse-HMM-DP-MMVP for new k, //compute following numerically,where bold = {b1, . . . , bK}\np(Y |bold, Zt = K + 1, Z\u2212t, Y\u2212t; a\u0304, b\u0304) = \u2211\nbK+1\np(bK+1|b old, \u03b7)\np(Y |bold, bK+1, Zt = K + 1, Z\u2212t, Y\u2212t; a\u0304, b\u0304) for j \u2264 l \u2208 [M ] do\nif bZt,j = bZt,k = 1 then // Sample Yt,j,l from\np(Yt,j,l|Y\u2212t,j,l, Z, a\u0304, b\u0304) \u221d Fk,j,lFk,j,jFk,l,l\nSet Yt,j,j = Xt,j \u2212 \u2211M\nj\u0304=1 Yt,j,j\u0304\nSet Yt,l,l = Xt,l \u2212 \u2211M\nl\u0304=1 Yt,l,l\u0304\nfor j = 1, . . . ,M, k = 1, . . . , K do // Sample bk,j (for Sparse-HMM-DP-MMVP) from\np(bk,j |b\u2212k,j , Y, Z) \u223c p(bk,j |b\u2212k,j)p(Y |bk,j , b\u2212k,j , Z; a\u0304b\u0304)\np(bk,j |b\u2212k,j) \u221d c\u2212kj + bk,j + a \u2032 \u2212 1\nK + a\u2032 + b\u2032 \u2212 1\nwhere c\u2212kj = \u2211K k\u0304 6=k,k\u0304=1 bk,j is the number of clusters\n(excluding k) with dimension j active for k = K, . . . ,M, k = 1, . . . do\nmk = 0 for i = 1, . . . , nk do\nu \u223c Ber( \u03b1\u03b2k i+\u03b1\u03b2k ), if (u == 1)mk ++\n[\u03b21\u03b22 . . . \u03b2K\u03b2K+1]|m,\u03b3 \u223c Dir(m1, . . . ,mk , \u03b3)\nUpdate for bk,j: For Sparse-HMM-DP-MMVP, the update for bk,j is obtained by integrating out \u03b7 to evaluate p(bk,j |b\u2212k,j) and computing the likelihood term by integrating out \u03bb, \u03bb\u0302 as before. This is shown in algorithm 1 (see supplementary material for more details).\nTrain time Complexity Comparison: SparseHMM-DP-MMVP vs HMM-DP-MMVP: The\ninference procedure for both models is similar, with different updates shown in Algorithm 1. For the HMMDP-MMVP all dimensions are active for all clusters. We sample (\nM 2\n)\nrandom variables for the symmetric matrix Yt in this step for each t \u2208 [T ]. On the other hand, for Sparse-HMM-DP-MMVP with m\u0304k active components in cluster k, we sample only (\nm\u0304 2\n)\nrandom variables which is a significant improvement when m\u0304 << M .\n7 Experimental Evaluation\nWe perform experiments on benchmark traces, to evaluate our models, in terms of likelihood and also evaluate their effectiveness for the caching problem, by measuring hitrates using our predictive models.\n7.1 Datasets:We perform experiments on diverse enterprise workloads : 10 publicly available real world Block I/O traces (MT 1-10), commonly used benchmark in storage domain, collected at Microsoft Research Cambridge[3] and 1 NetApp internal workload (NT1). See dataset details and choice of traces in Appendix C.\nWe divide the available trace into two parts Dlr that is aggregated into Tlr count vectors and D op that is aggregated into Top count vectors. In our initial experimentation, for aggregation, we fix the number of memory bins M=10 (leading to 10 dim count vectors), length of time slice \u03bd=30 seconds. Further, we use a test train split of 50% for both experiments such that Tlr = Top. (Later, we also perform some experiments to study the impact of some of these parameters with M = 100 dimensions on some of the traces.)\n7.2 Experiments:We perform two types of experiments, to understand how well our model fits data in terms of likelihood, the next to show how our model and our framework can be used to improve cache hitrates.\n7.2.1 Experiment Set 1: Likelihood Comparison:We show a likelihood comparison between HMMDP-MMVP, Sparse-DP-MMVP and baseline model HMM-DP-MIP. We train the three models using the inference procedure detailed in section 6 on Tlr and compute Log-likelihood on the held out test trace Top. The results are tabulated in Table 1.\nResults: We observe that the Sparse-HMM-DPMMVP model performs the best in terms of likelihood, while the HMM-DP-MMVP outperforms HMM-DPMIP by a large margin. Poor performance of HMM-DPMIP clearly shows that spatial correlation present across the M dimensions is an important aspect and validates the necessity for the use of a Multivariate Poisson model over an independence assumption between the dimensions. Superior performance of Sparse-HMM-DPMMVP over HMM-DP-MMVP is again testimony to the fact that there exists inherent sparsity in the data and this is better modeled by the sparse model.\n7.2.2 Experiment Set 2: Hitrates:We compute hitrate, on each of the 11 traces, with a baseline simulator without preloading and an augmented simulator with the ability to preload predicted blocks every \u03bd = 30s. Both the simulators use LRU for eviction. Off the shelf simulators for preloading are not available for our purpose and construction of the baseline simulator and that with preloading are described in detail in supplementary material- Appendix C.\nResults: We see in the barchart in figure 4 prediction improves hitrates over LRU baseline without preloading. We see that our augmented simulator gives order of magnitude better hitrates on certain traces (0.52 with preloading against 0.0002 with plain LRU).\nEffect of Training Data Size: We expect to capture long range dependencies in access patterns when we observe a sufficient portion of the trace for training where such dependencies manifest. Ideally we would like to run our algorithm in an online setting, where periodically, all the data available is used to update the model. Our model can be easily adapted to such a situation. In this paper, however, we experiment in a setting where we always train the model on 50% (see supplementary material for an explanation of the figure 50%) of available data for each trace and use this model to make predictions for the rest of the trace.\nEffect of M (aggregation granularity): To understand the impact M (count vector dimensionality),\nwe pick some of the best performing traces from the previous experiment (barchart in figure 4) and repeat our experiment with M=100 features, a finer 100 bin memory aggregation leading to 100 dimensional count vectors. We find that we beat baseline by an even higher margin with M=100. We infer this could be attributed to the higher sensitivity of our algorithm to detail in traces leading to superior clustering. This experiment also brings to focus the training time of our algorithm. We observed that the Sparse-HMM-DP-MMVP outperforms HMM-DP-MMVP not only in terms of likelihood and hitrates but also in terms of training time. We fixed the training time to at most 4 hours to run our algorithms and report hitrates in table 2. We find that Sparse-HMM-DP-MMVP ran to convergence while HMM-DP-MMVP did not finish even a single iteration for most traces in this experiment. This corroborates our understanding that handling sparsity reduces the number of latent variables in HMM-DP-MMVP, improving inference efficiency translating to faster training time, particularly for higher dimensional count vectors.\n7.3 Discussion of Results:We observe both from table 1 and the barchart (fig 4) that HMM-DPMMVP outperforms HMM-DP-MIP, and Sparse-HMMDP-MMVP performs the best, outperforming HMMDP-MMVP in terms of likelihood and hitrates, showing that traces indeed exhibit spatial correlation that is effectively modeled by the full covariance MVP and that handling sparsity leads to a better fit of the data.\nThe best results are tabulated in Table 2 where we observe that when using 100 bins Sparse-HMM-DP-\nMVP model achieves an average hitrate of h = 0.565, 30 times improvement over LRU without preloading, h = 0.0186. On all the other traces, LRU without preloading is outperformed by the sparse-HMM-DPMMVP, the improvement being dramatic for 4 of the traces. On computing average hitrate for the 11 traces in figure 4, we see 58% hitrate improvement.\nChoice of Baselines: We did not consider a parametric baseline as it is clearly not suitable for our caching application. Traces have different access patterns with varying detail (leading to varying number of clusters: fig 2). A parametric model is clearly infeasible in a realistic scenario. Further, due to lack of existing predictive models for count vector sequences, we use a HMM-DP-MIP baseline for our models.\nExtensions and Limitations: While we focus on capturing long range correlations, our framework can be augmented with other algorithms geared towards specific workload types for capturing short range correlations, like sequential read ahead and Sarc [5] to get even higher hitrates. We hope to investigate this in future.\nWe have shown that our models lead to dramatic improvement for a subset of traces and work well for the rest of our diverse set of traces. We note that there may not be discernable long range correlations present in all traces. However, we have shown, that when we can predict, the scale of its impact is huge. There are occasions, when the prediction set is larger than cache where we would have to understand the temporal order of predicted reads, to help efficiently schedule preloads. Cache size, prediction size and preload frequency, all play an important role to evaluate the full impact of our method. Incorporating our models within a full-fledged storage system involves further challenges, such as real time trace capture, smart disk scheduling algorithms for preloading, etc. These issues are beyond the scope of the paper, but form the basis for future work.\n8 Conclusions\nWe have proposed DP-based mixture models (DPMMVP, HMM-DP-MMVP) for correlated count vectors that capture the full covariance structure of multivariate count data. We have further explored the sparsity in our data and proposed models (Sparse-DP-MMVP and Sparse-HMM-DP-MMVP) that capture the correlation within a subset of dimensions for each cluster, also leading to more efficient inference algorithms. We have taken the first steps in outlining a preloading framework for leveraging long range dependencies in block I/O Traces to improve cache hitrates. Our algorithms achieve a 30X hitrate improvement on 4 real world traces, and outperform baselines on all traces. References\n[1] D. J. Aldous. In E\u0301cole d\u2019e\u0301te\u0301 de probabilite\u0301s de Saint-\nFlour, XIII\u20141983, Lecture Notes in Math., pages 1\u2013 198. Springer, Berlin, 1985. [2] L. M. Dimitris Karlis. Journal of Statistical Planning and Inference, 2007. [3] A. R. Dushyanth Narayanan, A Donnelly. Write offloading: Practical power management for enterprise storage. USENIX, FAST, 2008. [4] E. Fox, E. Sudderth, M. Jordan, and A. Willsky. A Sticky HDP-HMMwith Application to Speaker Diarization. Annals of Applied Statistics, 2011. [5] B. S. Gill and D. S. Modha. Sarc: Sequential prefetching in adaptive replacement cache. In UATC, 2005. [6] M. M. Gokul Soundararajan and C. Amza. Contextaware prefetching at the storage server. In USENIX ATC. USENIX, 2008. [7] A. K. Jain, M. Law, and M. Figueiredo. Feature Selection in Mixture-Based Clustering. In NIPS, 2002. [8] D. Karlis and L. Meligkotsidou. Finite mixtures of multivariate poisson distributions with application. Journal of Statistical Planning and Inference, 137(6):1942\u20131960, June 2007. [9] K. Kawamura. The structure of multivariate poisson distribution. Kodai Mathematical Journal, 1979. [10] T. M. Kroeger and D. D. E. Long. Predicting file system actions from prior events. In UATC, 1996. [11] M. H. C. Law, A. K. Jain, and M. A. T. Figueiredo. Feature selection in mixture-based clustering. In NIPS, pages 625\u2013632, 2002. [12] C. Z. S. S. M. Li, Z. and Y. Zhou. Mining block correlations in storage systems. In FAST. USENIX, 2004. [13] L. Meligkotsidou. Bayesian multivariate poisson mixtures with an unknown number of components. Statistics and Computing, 17, Iss 2, pp 93-107, 2007. [14] A. M. Schmidt and M. A. Rodriguez. Modelling multivariate counts varying continuously in space. In Bayesian Statistics Vol 9. 2010. [15] Y. W. Teh. Dirichlet processes. In Encyclopedia of Machine Learning. Springer, 2010. [16] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical dirichlet processes. Journal of American Statistical Association, 2004. [17] P. Tsiamyrtzis and D. Karlis. Strategies for efficient computation of multivariate poisson probabilities. Communications in Statistics (Simulation and Computation), Vol. 33, No. 2, pp. 271292, 2004. [18] B. D. M. Wang, Chong. Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process. In NIPS. [19] C. Wang and D. Blei. Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process. In NIPS. 2009. [20] Y. Zhang, G. Soundararajan, M. W. Storer, L. N. Bairavasundaram, S. Subbiah, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau. Warming up storage-level caches with bonfire. In USENIX Conference on File and Storage Technologies. USENIX, 2013.\nSupplementary Material: Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson\nAppendix A: Prediction Method The prediction problem in the operational phase involves finding the best Z \u2032t+1 using \u03b8 to solve equation 3.1. We describe a Viterbi like dynamic programming algorithm to solve this problem for Sparse-MVP emissions. (A similar procedure can be followed for MVP emissions).\nWe note that alternate strategies based on MCMC might be possible based on Bayesian inference for the variable under question. However, in the operation phase, the execution time becomes important and is required to be much smaller than \u03bd, the slice length. Hence we explore the following dynamic programming based procedure that is efficient and runs in a small fraction of slice length \u03bd.\nAt the end of the learning phase, we estimate the values of \u03b8 = {\u039b, \u03c0, b, \u03bb\u0302}, by obtaining \u039b, \u03bb\u0302, \u03c0 as the mean of their posterior, and b by thresholding the mean of its posterior and use these as parameters during prediction. A standard approach to obtain the most likely decoding of the hidden state sequence is the Viterbi algorithm, a commonly used dynamic programming technique that finds\n{Z \u2032\u2217s } t\ns=1 = argmax {Z\u2032s} t s=1\np(X \u20321, ...X \u2032 t\u2032 , {Z \u2032 s}\nt s=1)|\u03b8)\nLet \u03c9(t, k) be the highest probability along a single path ending with Z \u2032t = k. Further, let \u03c9(t, k) = max {Z\u2032s} t s=1 p({X \u2032s} t s=1, {Z \u2032 s} t s=1)|\u03b8). We have\n\u03c9(t+ 1, k) = argmax k\u2032=1,...,K\n\u03c9(t, k\u2032)\u03c0k\u2032,kSMV P (X \u2032 t+1; \u03b8)\nHence, in the standard setting of viterbi algorithm, having observed X \u2032t+1, the highest probability estimate of the latent variables is found as Z \u2032\n\u2217 t+1 = argmax\n1\u2264k\u2264K \u03c9(t+\n1, k). However, the evaluation of MVP and hence the evaluation of the SMVP pmf involves exponential complexity due to integrating out the Y variables. While there are dynamic programming based approaches explored for MVP evaluation [17], we resort to a simple approximation. Let \u00b5k,i = \u2211M\nj=1 \u03bbk,i,jbk,ibk,j+(1\u2212bk,i)\u03bb\u0302j , i \u2208 [M ], k \u2208 [K]. We consider Xt,i|Zt = k \u223c Poisson(\u00b5k,i) when Xt \u223c SMV P (\u039bk, \u03bb\u0302) (since the sum of independent Poisson random variables is again a Poisson random variable). Hence we compute p(Xt|Zt = k, \u00b5k) = \u03a0 M i=1Poisson(Xt,i;\u00b5k,i).\nIn our setting, we require finding the most likely Z \u2032\u2217t+1 without having observed X \u2032 t+1 to address our\nprediction problem from section 3. Hence we define the following optimization problem that tries to maximize the objective function over the value of X \u2032t+1 along with the latent variables {Z \u2032s} t+1 s=1.\n\u03c9\u2032(t+ 1, k) = max {Z\u2032s} t+1 s=1,X \u2032 t+1 p({X \u2032s} t+1 s=1, {Z \u2032 s} t+1 s=1)|\u03b8)\nHowever, since mode of Poisson is also its mean,\n(9.8) \u03c9\u2032(t+ 1, k) = Poisson(\u00b5k|\u00b5k) max k=1,...,K \u03c9\u2032(t, k)\u03c0k,l\nFrom equation 9.8, we have a dynamic programming algorithm similar to Viterbi algorithm (detailed in algorithm 2).\nAlgorithm 2: Prediction Algorithm\nInitial Iteration: Before X1 is observed \u03c9\u2032(1, k) = \u03c00kPoisson(\u00b5k;\u00b5k)\u2200k Z\u22171 = Argmaxk \u03c9\n\u2032(1, k) Initial Iteration: After X1 is observed \u03c9(1, k) = \u03c00kPoisson(X1;\u00b5k)\u2200k for t = 2, . . . T do\nBefore Xt is observed \u03c9\u2032(t, l) = maxk(\u03c9(t\u2212 1, k)\u03c0kl)Poisson(\u00b5l, \u00b5l)\u2200k Z\u2217t = Argmaxk \u03c9\n\u2032(t, l) After Xt is observed \u03c9(t, l) = maxk(\u03c9(t\u2212 1, k)\u03c0kl)Poisson(Xt, \u00b5l)\u2200k \u03a8(t, l) = Argmaxk (\u03c9(t\u2212 1, k)\u03c0kl)\nFinding the Path ZT = Argmaxk\u03c9(t,K) for Data points t = T \u2212 1, T \u2212 2 . . . 1 do\nZT = \u03a8(t+ 1, Z(t+ 1))"}, {"heading": "Appendix B: Inference Elaborated", "text": "In this section of supplementary material we discuss the inference procedure for DP-MMVP, HMM-DPMMVP and Sparse-HMM-DP-MMVP more elaborately adding some details that could not be accomodated in the original paper. Our Collapsed Gibbs Sampling inference procedure is described in the rest of this section.\nWe first outline the inference for DP-MMVP model in section 10.1 , followed by the HMM-DP-MMVP, its temporal extension in section 10.2. Then, in section 10.3, we describe the inference for the Sparse-HMMDP-MMVP model extending the previous procedure.\n10.1 Inference : DP-MMVP:The existance of Yt,j,l latent variables in the MVP definition differentiates the inference procedure of an MVP mixture from\nstandard inference for DP mixtures. (The large number of Yt,j,l variables also leads to computationally expensive inference for higher dimensions motivating sparse modeling).\nWe collapse \u039b variables exploiting the PoissonGamma conjugacy for faster mixing. The latent variables Zt, t \u2208 [T ], and Yt,j,l, j \u2264 l \u2208 [M ], t \u2208 [T ] require to be sampled. Throughout this section, we use the following notation: Y = {Yt : t \u2208 [T ]}, Z = {Zt : t \u2208 [T ] and X = {Xt : t \u2208 [T ]. A set with a subscript starting with a hyphen(\u2212) indicates the set of all elements except the index following the hyphen. Update for Zt: The update for cluster assignments Zt are based on the conditional obtained on integrating out G, based on the CRP[1] process leading to the following product.\np(Zt = k|Z\u2212t, X, \u03b2, Y ;\u03b1, a\u0304, b\u0304) \u221d p(Zt = k|Z\u2212t;\u03b1)fk(Yt)\n\u221d\n{\nn\u2212tk fk(Yt) k \u2208 [K]\n\u03b1fk(Yt) k=K+1 (10.9)\nWhere n\u2212tk, = \u2211 t\u03046=t \u03b4(Zt\u0304, k). The second term fk(Yt) = p(Yt, Y\u2212t|Zt = k, Z\u2212t; a\u0304, b\u0304) can be simplified by integrating out the \u039b variables based on their conjugacy. Let Sk,j,l = \u2211 t\u0304 Yt\u0304,j,l\u03b4(Zt\u0304, k), for j \u2264 l \u2208 [M ] and\nFk,j,l = \u0393(a\u0304+ Sk,j,l)\n(b\u0304+ nk)(a\u0304+Sk,j,l) \u03a0 t\u0304:Zt\u0304=k\nYt\u0304,j,l! (10.10)\nBy collapsing \u039b, fk(Yt) \u221d \u03a0 1<=j<=l<=M Fk,j,l(10.11)\nUpdate for Yt,j,l: This is the most expensive\nstep since we have to update ( M 2 )\nvariables for each observation t. The \u039b variables are collapsed, owing to the Poisson-Gamma conjugacy due to the choice of a gamma prior for the MVP.\nIn each row j of Yt, Xt,j = \u2211M\nl=1 Yt,j,l. To preserve this constraint, suppose for row j, we sample Yt,j,l, j 6= l, Yt,j,j becomes a derived quantity as Yt,j,j = Xt,j \u2212 \u2211M\np=1,p6=j Yt,p,j . The update for Yt,j,l, j 6= l can be obtained by integrating out \u039b to get an expression similar to that in equation 10.10. We however note that, updating the value of Yt,j,l impacts the value of only two other random variables i.e Yt,j,j and Yt,l,l. Hence we get the following update for Yt,j,l\np(Yt,j,l|Y\u2212t,j,l, Z, a\u0304, b\u0304) \u221d Fk,j,lFk,j,jFk,l,l(10.12)\nThe support of Yt,j,l, a positive, integer valued random variable, can be restricted as follows for efficient computation. We have Yt,j,j = Xt,j\u2212 \u2211M p=1,p6=j Yp,j \u2265 0\nSimilarly, Yt,l,l = Xt,l \u2212 \u2211M\np=1,p6=l Yp,l \u2265 0. Hence, we can reduce the support of Yt,j,l to the following: (10.13)\n0 \u2264 Yt,j,l \u2264 min\n\n(Xt,j \u2212\nM \u2211\np=1,p6=l\nYp,j), (Xt,l \u2212\nM \u2211\np=1,p6=j\nYp,l)\n\n\n10.2 Inference : HMM-DP-MMVP:The latent variables from the HMM-DP-MMVP model that require to be sampled include Zt, t \u2208 [T ], Yt,j,l, j, l \u2208 [M ], t \u2208 [T ] , and \u03b2 = {\u03b21, . . . , \u03b2K , \u03b2K+1 = \u2211\u221e r=K+1 \u03b2r}. Additionally an auxiliary variable mk (denoting the cardinality of the partitions generated by the base DP) is introduced as a latent variable to aid the sampling of \u03b2 based on the direct sampling procedure from HDP[16]. The latent variables \u039b and \u03c0 are collapsed to facilitate faster mixing. The procedure for sampling of Yt,j,l, j, l \u2208 [M ], t \u2208 [T ] is the same as that for DP-MMVP (eq: 10.12). Updates for m and \u03b2 are similar to [4], detailed in algorithm 1. We now discuss the remaining updates. Update for Zt: The update for cluster assignment for the HMM-DP-MMVP while similar to that that of DP-MMVP also considers the temporal dependency between the hidden states. Similar to the procedure outlined in [16][4] we have:\np(Zt = k|Z\u2212t,\u2212(t+1), zt+1 = l, X, \u03b2, Y ;\u03b1, a\u0304, b\u0304)\n\u221d p(Zt = k, z\u2212t,\u2212(t+1), zt+1 = l|\u03b2;\u03b1, a\u0304, b\u0304)fk(Yt) (10.14)\nWhere fk(Yt) = p(Yt, Y\u2212t|Zt = k, Z\u2212t; a\u0304, b\u0304). The first term can be evaluated to the following by integrating out \u03c0 as\np(Zt = k|Z\u2212t,\u2212(t+1), Zt+1 = l, \u03b2;\u03b1) =\n\n\n\n(n\u2212tzt\u22121,k + \u03b1\u03b2k) \u03b1\u03b2l+(n\n\u2212(t) k,l +\u03b4(Zt\u22121,k)\u03b4(k,l))\n\u03b1+n \u2212(t) k,. +\u03b4(Zt\u22121,k) k \u2208 [K]\n(\u03b1\u03b2K+1) \u03b1\u03b2l) (\u03b1) k=K+1\n(10.15)\nWhere n\u2212tk,l = \u2211\nt\u03046=t,t\u03046=t+1 \u03b4(Zt\u0304, k)\u03b4(Zt\u0304+1, l). The second term fk(Yt) is obtained from the equation 10.11."}, {"heading": "10.3 Inference : Sparse-HMM-DP-MMVP:", "text": "Sparse-HMM-DP-MMVP Inference is computationally less expensive due to the selective modeling of covariance structure. However, inference for Sparse-HMMDP-MVPM requires sampling of bk,j , j \u2208 [M ], k = 1, . . . in addition to latent variables in section 10.2 introducing challenges in the non-parametric setting that we discuss in this section. Note: Variables, \u03b7,\u039b and \u03bb\u0302 are collapsed for faster mixing. Update for bk,j : The update can be written as a product: (10.16) p(bk,j |b\u2212k,j , Y, Z) \u223c p(bk,j |b\u2212k,j)p(Y |bk,j , b\u2212k,j , Z; a\u0304b\u0304)\nBy integrating out \u03b7, we simplify the first term as follows where c\u2212kj = \u2211K\nk\u0304 6=k,k\u0304=1 bk,j is the number of clusters (excluding k) with dimension j active.\np(bk,j |b\u2212k,j) \u221d c\u2212kj + bk,j + a \u2032 \u2212 1\nK + a\u2032 + b\u2032 \u2212 1\nThe second term can be simplified as follows in terms of Fk,j,l as defined in equation 10.10 by collapsing the \u039b variables and F\u0302j obtained from integrating out the \u03bb\u0302 variables.\np(Y |bk,j , b\u2212k,j , Z; a\u0304b\u0304) \u221d \u03a0 j\u2264l\u2208[M ]\nF bk,jbk,l k,j,l \u03a0\nj\u2208[M ] F\u0302j\n(10.17)\nWhere F\u0302j = \u0393(a\u0302+ S\u0302j)\n(b\u0302 + n\u0302j)(a\u0302+S\u0302j) \u03a0 t,j:bZt,j=0\nYt,j,j ! (10.18)\n. And S\u0302j = \u2211 t Yt,j,j(1\u2212 bZt,j) and n\u0302j = \u2211\nt(1 \u2212 bZt,j) Update for Zt : Let b\no = {bk : k \u2208 [K]} be the variables selecting active dimensions for the existing clusters. The update for cluster assignments Zt, t \u2208 [T ] while similar to the direct assignment sampling algorithm of HDP[16], has to handle the case of evaluating the probability of creating a new cluster with an unknown bk+1 .\nThe conditional for Zt can be written as a product of two terms as that in equation 10.2\np(Zt = k|Z\u2212t,\u2212(t+1), zt+1 = l,b old, X, \u03b2, Y ;\u03b1, a\u0304, b\u0304)\n\u221d p(Zt = k, Z\u2212t,\u2212(t+1),t+1 = l|\u03b2;\u03b1, a\u0304, b\u0304)\np(Yt|Zt = k, Z\u2212t, Y\u2212t,b old; a\u0304, b\u0304)(10.19)\nThe first term can be simplified in a way similar to [4]. To evaluate the second term, two cases need to be considered.\nExisting topic (k \u2208 [K]) : In this case, the second term p(Yt|b\noZt = k, Z\u2212t, Y\u2212t; a\u0304, b\u0304) can be simplified by integrating out the \u039b variables as in equation (10.17).\nNew topic (k = K + 1) : In this case, we wish to compute p(Yt|b\no, Zt = K + 1, Z\u2212t, Y\u2212t; a\u0304, b\u0304). Since this expression is not conditioned on bK+1, evaluation of this term requires summing out bK+1 as follows.\np(Yt|b o, Zt = K + 1, Z\u2212t, Y\u2212t; a\u0304, b\u0304) =\n\u2211\nbK+1\np(bK+1|b o, \u03b7)p(Yt|b o, bK+1, Zt = K + 1, Z\u2212t, Y\u2212t; a\u0304, b\u0304)\nEvaluating this summation involves exponential complexity. Hence we resort to a simple numerical approximation as follows. Let us denote p(Yt|b o, bK+1, Zt = K + 1, Z\u2212t, Y\u2212t; a\u0304, b\u0304) as h(bK+1)\nThe above expression can be viewed as an expectation EbK+1 [h(bK+1)|b\no]. and can be approximated numerically by drawing samples of bK+1 with probability p(bK+1|b\no). We use Metropolis Hastings algorithm to get a fixed number S of samples using the proposal distribution that flips each element of b independently with a small probability p\u0302. The intuition here is that we expect the feature selection vector for new cluster, bK+1 to be reasonably close to bZoldt , the selection vector corresponding to the previous cluster assignment for this data point. In our experiments we set S=20 and p\u0302=0.2 to give reasonable results.\nWe note that in [18], the authors address a similar problem of feature selection, however in a multinomial DP-mixture setting, by collapsing the b selection variable. However, their technique is specific to sparse Multinomial DP-mixtures. Update for Yt,j,l: The update for Yt,j,l is similar to that in section 10.1 with the following difference. We sample only {Yt,j,l : bj = 1, bl = 1} and the rest of the elements of Y are set to 0 with the exception of diagonal elements for the inactive dimensions. We note that for the inactive dimensions {j : j \u2208 [M ], bZt,j = 0}, the value of Xt,j = Yt,j,j and hence can be set directly from the observed data without sampling.\nFor the active dimensions, {Yt,j,l : bj = 1, bl = 1, j \u2264 l \u2208 [M ]} we sample using a procedure similar to that in section 10.1 by sampling Yt,j,l, j 6= l to preserve the constraintXt,j = \u2211M\nl=1 Yt,j,l, restricting the support of the random variable in a procedure similar to section 10.1.\np(Yt,j,l|Y\u2212t,j,l, Z, a\u0304, b\u0304, a\u0302, b\u0302) \u221d Fk,j,lFk,j,jFk,l,l \u03a0 1<=j<=M F\u0302j\n(10.20)\nAppendix C: Experiment Details 11.4 Dataset Details:We perform experiments on publicly available real world block I/O traces from enterprise servers at Microsoft Research Cambridge [3]. They represent diverse enterprise workloads. These are about 36 traces comprising about a week worth of data, thus allowing us to study long ranging temporal dependencies. We eliminated 26 traces that are write heavy (write percentage > 25%) as we are focused on read cache. See Table 3 for the datasets and their read percentages. We present our results on the remaining 10 traces. We also validated our results on one of our internal workloads, NT1 comprising data collected over 24 hours.\nWe divide the available trace into two parts Dlr\nAlgorithm 3: Inference: Sparse-HMM-DP-MMVP Inference steps(The steps for HMM-DP-MMVP are similar and are shown as alternate updates in brackets)\nrepeat for t = 1, . . . , T do\nSample Zt from Eqn 10.3 (Alt: Eqn 10.2) for j \u2264 l \u2208 [M ] do\nif bZt,j = bZt,k = 1 then Sample Yt,j,l from Eqn 10.20 (Alt: Eqn 10.12)\nSet Yt,j,j = Xt,j \u2212 \u2211M\nj\u0304=1 Yt,j,j\u0304\nSet Yt,l,l = Xt,l \u2212 \u2211M\nl\u0304=1 Yt,l,l\u0304 for j = 1, . . . ,M, k = 1, . . . ,K do\nSample bk,j from Eqn 10.16 (Alt: Set bk,j = 1M )\nfor k = K, . . . ,M, k = 1, . . . do mk = 0 for i = 1, . . . , nk do\nu \u223c Ber( \u03b1\u03b2k i+\u03b1\u03b2k ), if (u == 1)mk ++\n[\u03b21\u03b22 . . . \u03b2K\u03b2K+1]|m, \u03b3 \u223c Dir(m1, . . . ,mk, \u03b3) until convergence;\nthat is aggregated into Tlr count vectors and D op that is aggregated into Top count vectors. We use a split of 50% data for learning phase and 50% for operation phase for our experiments such that Tlr = Top.\n11.5 Design of Simulator:The design of our baseline simulator and that with preloading is described below.\nBaseline: LRU Cache Simulator: We build a cache simulator that services access requests from the trace maintaining a cache. When a request for a new block comes in, the simulator checks the cache first. If the block is already in the cache it records a hit, else it records a miss and adds this block to the cache. The cache has a limited size (fixed to 5% the total trace size). When the cache is full, and a new block is to be added to the cache, the LRU replacement policy is used to\nselect an existing block to remove. We use the hitrates obtained by running the traces on this simulator as our baseline.\nLRU Cache Simulator with Preloading: In this augmented simulator, at the end of every \u03bd = 30s, predictions are made using the framework described in Section 3 and loaded into the cache (evicting existing blocks based on the LRU policy as necessary). While running the trace, hits and misses are kept track of, similar to the previous setup. The cache size used is the same as that in the previous setting.\n11.6 Hitrate Values: Figure 4 in our paper shows a barchart of hitrates for comparison. In table 4 of this section, the actual hitrate values are provided comparing preloading with Sparse-HMM-DP-MMVP and that with baseline LRU simulator without preloading. We note that we show a dramatic improvement in hitrate in 4 of the traces while we beat the baseline without preloading in most of the other traces.\n11.7 Effect of Training Data Size:We expect to capture long range dependencies in access patterns when we observe a sufficient portion of the trace for training where such dependencies manifest. We show this by running our algorithm for different splits of train and test data (corresponding to the learning phase and the operational phase) for NT1 trace.\nWe observe that when we see at least 50% of the trace, there is a marked improvement in hitrate for the NT1 trace. Hence we use 50% as our data for training for our experimentation.\nIn a real world setting, we expect the amount of data required for training to vary across workloads. To adapt our methodology in such a setting periodic retraining to update the model with more and more data for learning as it is available is required. Exploring an\nonline version of our models might also prove useful in such settings."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Existing caching strategies, in the storage do-<lb>main, though well suited to exploit short range spatio-<lb>temporal patterns, are unable to leverage long-range<lb>motifs for improving hitrates. Motivated by this,<lb>we investigate novel Bayesian non-parametric model-<lb>ing(BNP) techniques for count vectors, to capture long<lb>range correlations for cache preloading, by mining Block<lb>I/O traces. Such traces comprise of a sequence of<lb>memory accesses that can be aggregated into high-<lb>dimensional sparse correlated count vector sequences.<lb>While there are several state of the art BNP algo-<lb>rithms for clustering and their temporal extensions for<lb>prediction, there has been no work on exploring these<lb>for correlated count vectors. Our first contribution ad-<lb>dresses this gap by proposing a DP based mixture model<lb>of Multivariate Poisson (DP-MMVP) and its temporal<lb>extension(HMM-DP-MMVP) that captures the full co-<lb>variance structure of multivariate count data. However,<lb>modeling full covariance structure for count vectors is<lb>computationally expensive, particularly for high dimen-<lb>sional data. Hence, we exploit sparsity in our count<lb>vectors, and as our main contribution, introduce the<lb>Sparse DP mixture of multivariate Poisson(Sparse-DP-<lb>MMVP), generalizing our DP-MMVP mixture model,<lb>also leading to more efficient inference. We then discuss<lb>a temporal extension to our model for cache preloading.<lb>We take the first step towards mining historical<lb>data, to capture long range patterns in storage traces for<lb>cache preloading. Experimentally, we show a dramatic<lb>improvement in hitrates on benchmark traces and lay<lb>the groundwork for further research in storage domain<lb>to reduce latencies using data mining techniques to<lb>capture long range motifs.", "creator": "LaTeX with hyperref package"}}}