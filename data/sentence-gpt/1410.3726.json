{"id": "1410.3726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2014", "title": "Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding", "abstract": "Ambiguity or uncertainty is a pervasive element of many real world decision making processes. Variation in decisions is a norm in this situation when the same problem is posed to different subjects. Psychological and metaphysical research had proven that decision making by human is subjective. It is influenced by many factors such as experience, age, background, etc. Scene understanding is one of the computer vision problems that fall into this category. Conventional methods relax this problem by assuming scene images are mutually exclusive; and therefore, focus on developing different approaches to perform the binary classification tasks. In this paper, we show that scene images are non-mutually exclusive, and propose the Fuzzy Qualitative Rank Classifier (FQRC) to tackle the aforementioned problems. The proposed FQRC provides a ranking interpretation instead of binary decision. Evaluations in term of qualitative and quantitative using large numbers and challenging public scene datasets have shown the effectiveness of our proposed method in modeling the non-mutually exclusive scene images. The FQRC also is useful in evaluating whether the FQRC is correct for any situation, as such we can obtain a high accuracy rating, with no overrepresentation of the accuracy of this score, resulting in low error. The FQRC uses a large number of different factors to classify each picture as a subjective reality. This allows us to investigate whether there are any specific qualities for the picture, or when we need to adjust the score. The FQRC also includes a classification system for classification.\n\n\nThe FQRC also includes a classification system for classification.\nThe FQRC also includes a classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nAn classification system for classification.\nA classification system for classification.\nA classification system for classification.\nAn classification system for classification.\nA classification system for classification.\nAn classification system for classification.\nAn classification system for classification.\nAn classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nA classification system for classification.\nAn classification system for classification.\n", "histories": [["v1", "Tue, 14 Oct 2014 15:19:43 GMT  (2896kb,D)", "http://arxiv.org/abs/1410.3726v1", "Accepted in IEEE Transactions on Fuzzy Systems"]], "COMMENTS": "Accepted in IEEE Transactions on Fuzzy Systems", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.IR", "authors": ["chern hong lim", "anhar risnumawan", "chee seng chan"], "accepted": false, "id": "1410.3726"}, "pdf": {"name": "1410.3726.pdf", "metadata": {"source": "CRF", "title": "Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding", "authors": ["Chern Hong Lim", "Anhar Risnumawan", "Chee Seng Chan"], "emails": ["(cs.chan@um.edu.my)."], "sections": [{"heading": null, "text": "Index Terms\u2014Scene understanding, fuzzy qualitative reasoning, multi-label classification, computer vision, pattern recognition\nI. INTRODUCTION\nOne of the biggest challenges in real world decision making process is to cope with uncertainty, complexity, volatility and ambiguity. How do we deal with this growing confusion in our world? In scene understanding, an important and yet difficult image understanding problem due to their variability, ambiguity, wide range of illumination and scale conditions falls into this category. The conventional goal of the works is to assign an unknown scene image to one of the several possible classes. For example, Fig. 1(a) is a Coast class scene while Fig. 1(c) is a Mountain class scene.\nIntentionally, most state-of-the-art approaches in scene understanding domain [1]\u2013[4] are exemplar-based and assume that scene images are mutually exclusive, P (A \u2229 B) = 0. This simplifies the complex problem of scene understanding (uncertainty, complexity, volatility, and ambiguity) to a simple binary classification task. Such approaches learn patterns from a training set and subsequently, search for the images similar to it. As a result of this, classification errors often occur when the scene classes overlap in the selected feature space. For\nManuscript received May 09, 2013, revised September 19, 2013. Accepted for publication November 28, 2013. This work is supported by the University of Malaya HIR under grant No UM.C/625/1/HIR/037, J0000073579\nAuthors are with the Centre of Image and Signal Processing, Faculty of Computer Science and Information Technology, University of Malaya, 50603 Kuala Lumpur, MALAYSIA. Corresponding e-mail: (cs.chan@um.edu.my).\nexample, it is unclear that in Fig. 1(b) is a Coast class scene or a Mountain class scene.\nInspired by the fuzzy set theory proposed by Lotfi Zadeh [5], we argue that scene images are non-mutually exclusive where different people are likely to respond inconsistently. Here, we define inconsistent as the scenario where there is no definite answer (in computational term, a binary or linear answer) to a problem. This notion became popular among researchers and technologists due to wide spectrum of applications [6], [7]. In scene understanding, however, only a few numbers of the research works are aware of and had tackled this problem. The notable ones are [8]\u2013[10], where a multilabel scene classification framework is proposed. However, these approaches are not practical due to: firstly, the work requires human intervention to manually annotate the multilabel training data. This is a tedious job that leads to a large number of classes with the sparse number of sample [11]. Secondly, the annotated image\u2019s classes are potentially bias as different people tend to respond inconsistently [12] and finally, it does not able to handle multi-dimension data.\nIn this paper, our aim is to study a novel approach to remedy the aforementioned problems. We propose the Fuzzy Qualitative Rank-Classifier (FQRC) to relax the assumption that scene images are mutually exclusive. Therefore, a scene can be somewhat arbitrary and possibly sub-optimal. We compare the results from FQRC with an online survey to show that there is an influence of human factors (background, experience, age, etc.) in decision making and hence conclude that assuming scene images are mutually exclusive is impractical. Qualitative and quantitative comparisons to the state-of-the-art solutions have shown the strength and effectiveness of our proposed method.\nIn summary, our main contribution is to show that scene images are non-mutually exclusive. This is supported by an online survey that participated by more than 150 candidates\nCopyright (c) 2014 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nar X\niv :1\n41 0.\n37 26\nv1 [\ncs .C\nV ]\n1 4\nO ct\n2 01\n4\nfrom different ethnics and age using the OSR dataset [1]. The reason is to raise the awareness of computer vision community regarding this very important, but largely neglected issue. With this in mind, we propose the FQRC to model scene images in a non-mutually exclusive manner, where we develop an inference that outputs ranking result. In advance, FQRC provides a resolution toward conventional solutions which either perform binary classification [1]\u2013[4] or require human intervention [9], [10].\nThe rest of the paper is organized as follows. Section II covers the related works in scene understanding. Section III presents our proposed framework which consists of two stages, the learning and inference stages. The intuition and stability analysis of our proposed approach are discussed in Section IV. Section V demonstrates the ranking interpretation. Section VI shows the experiment results, and finally, we conclude in Section VII."}, {"heading": "II. RELATED WORK", "text": "Scene understanding has been one of the mainstream tasks in computer vision. It differs from the conventional object detection or classification tasks, to the extent that a scene is composed of several entities that are often organized in an unpredictable layout [13]. Surprisingly from our findings, there is very minimal or almost none that had tackled this problem using the fuzzy approach. The early efforts in this area were dominated by computer vision researchers who focus on using machine learning techniques. These prior works denoted the scene understanding problem were to assign one of the several possible classes to a scene image of unknown class.\nOliva and Torralba [1] proposed a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represents the dominant spatial structure of a scene - the spatial envelope as scene representation. Then, a support vector machine (SVM) classifier with Gaussian kernel is employed to classify the scene classes. Fei-Fei and Perona [2] proposed the Bayesian hierarchical model extended from latent dirichlet allocation (LDA) to learn natural scene categories. In their learning model, they represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning, finally they choose the best model as their classification result. Bosch et al. [3] inspired from the previous work and proposed probabilistic latent semantic analysis (pLSA) incorporate with KNN for scene classification. Vogel and Schiele [4] used the occurring frequency of different concepts (water, rock, etc.) in an image as the intermediate features for scene image classification. The twostage system makes use of an intermediary semantic level of block classification (concept level) to do retrieval based on the occurrence of such concepts in an image.\nHowever, in scene classification task, it is very likely that a scene image can belongs to multiple classes. As a result of this, all the aforementioned solutions that assumed scene classes are mutually exclusive are not practical and often lead to classification errors. We believe that scene images are somewhat arbitrary and possibly sub-optimal as depicted in Fig. 1. To the best of our knowledge, there are numerous\nmulti-label classification research [11], [14]; however, only a few were focused in the domain of scene understanding. Boutell et al. [9] proposed an approach using SVM with cross-training to build the classifier for every base class. Then maximum a posteriori (MAP) principle is applied with the aid of prior probability calculation and gamma fit operation toward the single and multi-label training data. This is to obtain the desired threshold to determine whether a testing sample is fall into single label event or multiple label events.\nInspired by [9], Zhang and Zhou [10] introduced multi-label lazy learning K-nearest neighbor (ML-KNN) as their classification algorithm. This is to resolve the inefficiency of using multiple independent binary classifier for each class by using SVM. Statistical information from the training set and MAP principle is utilized to determine the best label for the test instance. Unfortunately, both these methods required manual human annotation of multi-label class training data to compute the prior probability based on frequency counting of training set. This is an impractical solution since a human decision is bias and inconsistent. It also leads to large number of classes with sparse sample [11]. Besides that, human reasoning does not annotate an image as multi-class. For instance, referring to Fig. 1(b), it is very rare for one to say that \u201cthis is a Coast + Mountain class scene image\u201d. In general, one would rather comment \u201cthis is a Coast\u201d or \u201cthis is a Mountain\u201d scene.\nIn what constitutes the closer work to ours in the fuzzy domain, Lim and Chan [8] proposed a fuzzy qualitative framework and Cho and Chang [15] employed a simple fuzzy logic with two monocular images to understand the scene images. However, their work suffered from 1) finding the appropriate resolution to build their 4-tuple membership function. Currently, the model parameters are chosen manually based on prior information and in a trial-and-error manner. This is a very tedious and time consuming approach; 2) only able to accommodate two feature vectors as input data; 3) the ranking is undefined and finally 4) tested on a very limited and easy dataset (a dataset that contains only 2 scene images).\nIn this paper, we extend the work of [8] by learning the 4- tuple membership function from the training data. In order to achieve this, we used the histogram representation. It relaxes the difficulty of obtaining multi-label training data as to [9], [10] where the training steps require human intervention in manually annotate the multi-label training data. This is a daunting task as human decisions are subjective and huge amount of participants are needed. Besides that, a ranking method to describe the relationship of image to each scene class is introduced. In scene understanding, in particular where we model the scene images as non-mutually exclusive, the idea of inference engine with ranking interpretation is somehow new and unexplored."}, {"heading": "III. FUZZY QUALITATIVE RANK CLASSIFIER", "text": ""}, {"heading": "A. Basic Notation", "text": "The general framework of the proposed FQRC consists of four stages: 1) Pre-processing; 2) Learning model; 3) Inference and 4) Ranking interpretation as illustrated in Fig. 2. Let I = {I1, I2, . . . , IN} denotes the N scene images. During\nthe pre-processing stage, any existing feature representation such as texture component, color spectrum and interest point can be employed as an input to our learning model. In this paper, we have employed the attributes [16] as our image features. Let T denotes a feature extraction function, T : I \u2192 xk, where xk is a set of feature values belong to the k-th class, k \u2208 {1, 2, . . . ,K}, of input space X , K is the number of classes label. Input data, xk \u2208 X , is defined as xk = {x1, x2, . . . , xJ}k, where xj is a feature value, j \u2208 {1, 2, . . . , J}, and J is the number of features. We denote sample (x, y) as z \u2208 Z of sample space Z ."}, {"heading": "B. Motivation", "text": "In general, the task of a classifier (we denote it as a function f ) is to find a way, which, based on the observations, assigns a sample to a specified class label, y \u2208 (Y \u2286 {1, 2, . . . ,K}), where Y is the output space. The task is to estimate a function (f \u2208 F) : x\u2192 y, where F is the function space. A function f is i.i.d., generated using the input-output pairs according to an unknown distribution P (x, y) so that f can classify unseen samples (x, y),\n(x1, y1), . . . , (xN , yN ) \u2208 (X \u00d7 Y)N (1)\nThe best function f , which one can obtain is the one that minimizes the bound of error represented by a risk function (2). However, one must note that, we could not directly compute the risk R(f) since the probability of P (x, y) is unknown.\nR(f) = \u222b loss(f(x), y)P (x, y) (2)\nIn scene understanding, (2) is much difficult to achieve since scene images are non-mutually exclusive due to the inconsistent of human decision, where different people tend to provide different answers. Theoretically, the importance of the\nnon-mutually exclusive data can be derived from the inequality Chernoff bound [17]:\nP {\u2223\u2223\u2223\u2223\u2223 1N N\u2211 i=1 xi \u2212 E[x] \u2223\u2223\u2223\u2223\u2223 \u2265 } \u2264 2 exp(\u22122N 2) (3)\nThis theorem states that the probability of sample mean differ by more than from the expected mean is bounded by the exponential that depends on the number of samples N . Note that if we have more data, the probability of deviation error will converge to zero. However, this is not true because of uniform convergence of function space F [18]. Using the risk function (2) we can represent the inequality (3) as follows,\nP {|Remp(f)\u2212R(f)| \u2265 } \u2264 2 exp(\u22122N 2) (4)\nwhere Remp(f) and R(f) are the empirical and actual risk, respectively. Inequality (4) shows that for a certain function f it is highly probable that the empirical error provides good estimates of the actual risk. Luxburg and Scholkopf [18] stated that the empirical risk Remp(f) can be inaccurate when N \u2192 \u221e since Chernoff bound only holds for a fixed function f which does not depend on the training data. But in contrary, f does depend on training data. Therefore, they came up with the uniform convergence and obtained the following inequality:\nP { sup f\u2208F |Remp(f)\u2212R(f)| \u2265 } \u2264 2 exp(\u22122N 2) (5)\nSuppose we have finitely g functions, F = {f1, f2, . . . , fg} and Ci = |Remp(fi)\u2212R(fi)| \u2265 , then using the union bound we can represent (5) as:\nP { sup f\u2208F |Remp(f)\u2212R(f)| \u2265 } = P (C1 \u2228 C2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Cg)\n= g\u2211 i=1 P (Ci)\u2212 { D2 +D3 + \u00b7 \u00b7 \u00b7+Dg } \u2264 2g exp(\u22122N 2)\ufe38 \ufe37\ufe37 \ufe38\n1st term\n\u2212 bound(D2 +D3 + \u00b7 \u00b7 \u00b7+Dg)\ufe38 \ufe37\ufe37 \ufe38 2nd term\n(6)\nwhere Di is the sum of the probabilities of every combination of i event, e.g, Dg = P (C1 \u2227 C2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 Cg). This leads to a bound which states that the probability that empirical risk is close to the actual risk is upper bounded by two terms. The first term is the error bound because of the mutually exclusive data and the second term is due to the non-mutually exclusive data. Most of the conventional classification methods, however, only utilize the mutually exclusive part. In contrast, our proposed method - the FQRC models both the mutually and nonmutually exclusive parts."}, {"heading": "C. Learning the FQRC", "text": "In our learning model, we learn the non-mutually exclusive scene data with parametric approximation of the membership function where the membership distribution of a normal\nconvex fuzzy number is approximated by the 4-tuple. This fuzzy representation of qualitative values is more general than ordinary (crisp) interval representations, since it can represents not only the information stated by a well-determined real interval but also the knowledge embedded in the soft boundaries of the interval [19]. Thus, the fuzzy representation removes, or largely weakens (if not completely resolving), the boundary interpretation problem, achieved through the description of a gradual rather than an abrupt change in the degree of membership of which a physical quantity is mapped onto a particular qualitative value. It is, therefore, closer to the common sense intuition of the non-mutually exclusive problem.\nAccording to [19]\u2013[25], such representation of the 4-tuple fuzzy number is a better qualitative representation as the representation has high resolution and good compositionality. The dominant region of 4-tuple indicates the mutually exclusive part, while the intersection between 4-tuple indicates the nonmutually exclusive, as shown in Fig. 3. The 4-tuple fuzzy number is represented as m = {a, b, \u03b1, \u03b2} with the condition a < b and ab > 0. There will be J \u00d7 K matrix containing 4-tuple for each feature number and class, as in (7). Those 4-tuples are represented in the form as mjk = {a, b, \u03b1, \u03b2}jk.\nM =  m11 m12 \u00b7 \u00b7 \u00b7 m1K m21 m22 \u00b7 \u00b7 \u00b7 m2K ... ... . . . ...\nmJ1 mJ2 \u00b7 \u00b7 \u00b7 mJK  (7) The representation in (7) is to conserve the appropriate membership function, m, of each respective feature (row) for each scene class (column). This is opposed to [9], [10] which require human intervention in manually annotate the training data as prior information.\nIn order to learn the 4-tuple fuzzy number, we have chosen to use the histogram representation. As illustrated in Fig. 3, it consists of tabular frequencies, shown as adjacent rectangles, erected over discrete intervals (bins), with an area equal to the frequency of the observations in the interval. The height\nof a rectangle is also equal to the frequency density over the interval, i.e., the frequency divided by the width of the interval. The total area of the histogram is equal to the number of data.\nMore specifically, a histogram is a function that counts the number of observations, n, that fall into each of the disjoint categories (known as bins), whereas the graph of a histogram is merely one way to represent a histogram. Thus, if we let N be the total number of observations and B be the number\nof bins, then N = B\u2211 i=1 ni. In the proposed method, for every feature and class label, xjk = {xjki }Ni=1, we create a histogram in order to obtain the mjk.\nWe utilize the histogram in representing the occurrence of the training data to the corresponding feature values with an empirical bin width. There is no \u201dbest\u201d number of bins, and different bin sizes can reveal different features of the data. Some theoreticians have attempted to determine an optimal number of bins [26]\u2013[28], but these methods generally make strong assumptions about the shape of distribution. Depending on the actual data distribution and the goals of analysis, different bin number may be appropriate. So an experiment is usually needed for this purpose. Similarly, we utilize (8) to find the bin width, v. \u2308\nv = \u2227x\u2212 \u2228x\nB\n\u2309 (8)\nwhere d \u2022 e indicates the ceiling function and B = 50 is the total number of bins chosen empirically in our framework. We calculate the occurrence of the training data in each bin and yield a feature vector of N = {n1, n2, \u00b7 \u00b7 \u00b7 , nB}. From here, we locate the dominant region, \u00b5\u0304.\n\u00b5\u0304 = \u2211B i=1 ni b\n(9)\nwhere b denoted the total number of bin which satisfy n > 0. The dominant region (mutually exclusive) is defined as the region where the distribution of training data is higher than \u00b5\u0304. We mark this region with membership value equals to 1. By referring to Fig. 3, the parameters of a and b of m can be determined as the lower and upper bound of the area that possess membership value equals to 1. The intersection region (non-mutually exclusive) a\u2212\u03b1 and b+\u03b2 can be determined as the lower and upper bound of the area that possess membership value equals to 0 respectively. Algorithm 1 summarizes the learning process with a set of training image with K classes.\nD. Inference\nOur goal here is to relax the mutually-exclusive assumption on the scene data and classify an unknown scene class into their possibility scene classes and therefore, one scene image can belongs to multiple scene classes. This is unlike the conventional fuzzy inference engine that the de-fuzzification step eventually derives a crisp decision.\nGiven a testing scene image and its respective feature values x, the membership value \u00b5 of feature j belong to class k can be approximated by (10).\nAlgorithm 1 LEARNING FRAMEWORK Require: A training dataset\nStep 1: Grouping images Group every image to its respective class label, I\u2192 {Ik}Kk=1. Step 2: Acquiring the feature values for all Ik, perform preprocessing to obtain xk where J attributes are acquired. Then compute xjk = {xjki }Ni=1. Step 3: Learning Model for all j such that 1 \u2264 j \u2264 J do\nfor all k such that 1 \u2264 k \u2264 K do Build a histogram of xjk Compute \u00b5\u0304 with (9) Obtain mjk = {a, b, \u03b1, \u03b2}jk based on \u00b5\u0304\nend for end for return M\n\u00b5jk(xj) =  0, xj < a\u2212 \u03b1 \u03b1\u22121 (xj \u2212 a+ \u03b1), a\u2212 \u03b1 6 xj < a 1, a 6 xj 6 b\n\u03b2\u22121 (b+ \u03b2 \u2212 xj), b < xj 6 b+ \u03b2 0, xj > b+ \u03b2\n(10)\nwhere the parameter a, b, \u03b1, and \u03b2 are retrieved from mjk of the learnt FQRC model. We then calculate the product, Pk of membership values of all the attributes for each class, k using (11). Finally, we normalize the Pk and denote as rk (12),\nPk = J\u220f j=1 \u00b5jk(xj) (11)\nrk = Pk\u2211 P =\n\u220fJ j=1 \u00b5jk(xj)\nZ (12)\nwhere Z is the normalizer. The intuition to use the product of membership values of all the attributes for each scene class, Pk is to calculate the confident value of each class. This is the core to relate the inference mechanism closer to the principle of human reasoning and relax that scene images are mutually exclusive. If the attribute of a testing data is dominantly belonged to a certain class, k (which means the membership value of that particular attribute, \u00b5jk = 1), and the same for other attributes, at the end of the Pk calculation, testing data belongs to that particular class will be very definite because the product between values 1 is still equal to 1. On the other hand, if the uncertainties for the attributes (membership value of the attribute \u00b5jk < 1) are cumulated, the confident value decreases. In mathematical view, the products between values of less than 1 will eventually produce smaller value.\n1) Summary: Fig. 4 and 9(g) show an example walkthrough with a testing image, s and a learnt FQRC model. Let us denote the attributes of the testing image (Fig. 9(g)) as x1 = \u22120.1545 and x2 = \u22121.7597, respectively. For simplicity, we used only 2 attributes in this example but not limited to. By employing the learnt FQRC model (Fig. 4), we compute Pk as to (11) and rk as (12).\nTo the end, we obtain r1 = 0.5561, r2 = 0.0264, r3 = 0.0000 and r4 = 0.4175, respectively. Each of these values represent that the scene s has the confident value r1 belongs to \u201cInsidecity\u201d, r2 belongs to Coast, r3 belongs to \u201cOpencountry\u201d, and r4 belongs to Forest where \u2211 r = 1. From human perspective, this result is reasonable as in the picture, there are characteristics of \u201cIncidecity\u201d and \u201cForest\u201d. For examples, there are buildings, vehicles, as well as trees. Therefore, in the inference process, we observed high degree of memberships of the attributes from both classes and thus infer a high value for r1 and r4. While, on the other hand, it possesses almost zero for r2 and zero for r3 because of low or zero value determined from the respective attributes.\nAs discussed, most state-of-the-art approaches assumed that scene images are mutually-exclusive. Therefore, different strategies to a built a sophisticated binary classifier (inference engine) were proposed in those state-of-the-art approaches. As opposed to these solutions, our work argued that scene images are non-mutually exclusive. Hence, our inference engine contributes in such a way that where ranking interpretation\nreplaces the binary decision. Nevertheless, we provide comprehensive study and stability analysis of our proposed framework in the following section."}, {"heading": "IV. FUZZY MEMBERSHIP FUNCTION AND STABILITY ANALYSIS", "text": "Before we proceed to the final stage of the FQRC, we provide the intuition of using the 4-tuple membership function in our proposed framework to solve the non-mutually exclusive problem. In addition, the stability analysis of our overall framework is discussed."}, {"heading": "A. Fuzzy membership function", "text": "In this section, we discuss the intuitive idea of using 4-tuple fuzzy membership function in our framework. If we define our loss function as,\n`(fi(x), y) = 0 if y = maxk\u2208{1,...,K}r i(x)\n1 otherwise (13)\nwhere ri(x) = {ri1, . . . , riK |rik \u2208 [0, 1]} is the output of the inference of function i, the scalar output rik is defined in (12) and \u2211K k=1 r i k = 1. Suppose we have finitely g functions, then, our objective is to find a function f\u2217(x) that minimize the loss function,\nf\u2217(x) = arg min y\u2208{1,...,K} g\u2211 i=1 `(fi(x), y) (14)\nIn order to get the interpretation of (14) we will use the concept of maximum entropy. In information theory, the principle of maximum entropy is to minimize the amount of prior information built into the distribution. More specifically, the structure of maximum entropy problem is to find a probability assignment (or membership function \u00b5jk \u2208 [0, 1]) which avoid bias agreeing with any given information. In this case, while looking at (14), the membership function \u00b5jk captures such prior information. Inspired by Miyamoto and Umayahara [29], we utilize the maximum entropy to get the interpretation of 4- tuple. For simplicity we omit i, and the objective of maximum entropy,\nmax\u2212 \u2211 j \u2211 k \u00b5jk log\u00b5jk (15)\nSubject to the constraint \u2211 k \u220f j \u00b5jk Z = 1 and f\n\u2217(x) = c, where c is a constant. Then using Lagrange multipliers,\nJ =\u2212 \u2211 j \u2211 k \u00b5jk log\u00b5jk + \u03bb1\n( 1\u2212\n\u2211 k\n\u220f j \u00b5jk\nZ ) + \u03bb2(c\u2212 f\u2217(x)) (16)\nFor simplicity, we treat \u00b5jk as a fixed length vector since we assume x is discrete, then we have,\n\u2202J \u2202\u00b5jk = \u22121\u2212 log\u00b5jk \u2212 \u03bb1 Z \u2212 \u03bb2 \u2202f\u2217(x) \u2202\u00b5jk\n(17)\nSetting \u2202J\u2202\u00b5jk = 0 and get \u00b5jk yields,\n\u00b5jk = exp\n( \u2212 (\n1 + \u03bb1 Z + \u03bb2 \u2202f\u2217(x) \u2202\u00b5jk\n)) (18)\nActually this result is similar when we minimize or maximize the objective function of,\nmin/max\u2212 \u2211 j \u2211 k \u00b5jk log\u00b5jk \u2212 \u03bb2f\u2217(x) (19)\nWith subject to the constraint \u2211 k \u220f j \u00b5jk Z = 1. After taking min-max sign change and make the constant \u03bb = 1/\u03bb2 for brevity, we get the following objective,\nmin f\u2217(x)\u2212 \u03bb \u2211 j \u2211 k \u00b5jk log\u00b5jk\nsubject to \u2211 k\n\u220f j \u00b5jk\nZ = 1\n(20)\nIf we compare (20) with the formula of a classifier with regularization, f + \u03bbR, the 4-tuple membership function implicitly models the regularization. In details, the 4-tuple membership function with \u00b5jk = 1 (mutually exclusive part) models the classifier while the transition of membership function [0, 1] (non-mutually exclusive part) implicitly models the regularization."}, {"heading": "B. Stability Analysis", "text": "In this section, we discuss the robustness of the proposed framework in terms of stability analysis. In particular, the concept of stability brought by Bousquet and Elisseeff [30] is employed as it gives guarantee of a \u201cgood\u201d learning machine by deriving generalization error bounds. As a matter of fact, the error bounds are derived from the stability. More specifically, the stability measures how much the output will change for small changes in the training data. One said an algorithm that is stable, whose output will not depend on a single sample, tends to have generalization error that is close to the empirical error bounded by a constant. We define stability as follows,\nDefinition 4.1 (Stability): Let (xi, yi) = zi(\u2208 Z) be a sample from a set of samples Z and (Z\\zi) \u222a z\u2032i be a set of samples after replacing a sample zi with a new sample z\u2032i which is independent from Z . A function f : ZN \u2192 RN has stability with respect to the loss function ` by a constant \u03b2, such that\n\u2200Z \u2208 (X \u00d7 Y)N ,\u2200i \u2208 {1, . . . , N} : EZ |`(fZ , \u00b7)\u2212 `(f(Z\\zi)\u222az\u2032i , \u00b7)| \u2264 \u03b2\n(21)\nWe call f stable. Similarly, for large classes of functions if for all f \u2208 F satisfy condition (21), then F is stable as well. The constant \u03b2 should be on the order of O( 1N ) [30].\nIf we define the empirical risk as Remp = 1 N \u2211 zi\u2208Z `(f(x), y), for simplicity we will denote\nR = 1N \u2211 zi\u2208Z `zi , and let RZ\u222az\u2032 be the empirical risk after adding a sample z\u2032.\nRZ\u222az\u2032 = 1\nN + 1 \u2211 zi\u2208(Z\u222az\u2032) `zi\n= 1\nN + 1 (\u2211 zi\u2208Z `zi + `z\u2032 ) = N N + 1 ( R+ 1 N `z\u2032 ) (22)\nSimilarly, we get the risk after removing a sample z\u2032\u2032, that is RZ\\z\u2032\u2032 ,\nRZ\\z\u2032\u2032 = N\nN \u2212 1\n( R+ 1\nN `z\u2032\u2032\n) (23)\nThen the risk after we replace a sample, R(Z\\z\u2032\u2032)\u222az\u2032 , can be denoted as follows,\nR(Z\\z\u2032\u2032)\u222az\u2032 = N \u2212 1\nN + 1\u2212 1\n[ RZ\\z\u2032\u2032 + 1\nN \u2212 1 `z\u2032 ] = N \u2212 1 N [ N N \u2212 1 ( R\u2212 1 N `z\u2032\u2032 ) + 1 N \u2212 1 `z\u2032\n] = R\u2212 1\nN `z\u2032\u2032 +\n1\nN `z\u2032\n(24) Using the triangle inequality and by noting (13) that `z\u2032 , `z\u2032\u2032 \u2264 1,\n|R(Z\\z\u2032\u2032)\u222az\u2032 \u2212R| \u2264 \u03b2b \u2264 2\nN (25)\nwhere \u03b2b is the stability of the underlying binary classifier. In order to get the stability of our method, the loss function (13) must be \u03c3-admissible for any \u03c3 (it is also need to be convex) [29]\u2013[31]. Let\u2019s define the total loss as `tot(x, y) =\u2211g i=1 `(fi(x, y)), then our new loss function (parameterized over \u03b3 > 0) can be defined as,\n`\u03b3(f(x), y) = 0 if `tot(x, y) < \u2227k 6=y`tot(x, k) `tot(x,y)\u2212\u2227k 6=y`tot(x,k) \u03b3 if 0 6 `tot(x, y)\u2212 \u2227k 6=y`tot(x, k) 6 \u03b3\n1 if `tot(x, y)\u2212 \u2227k 6=y`tot(x, k) > \u03b3 (26) where k, y \u2208 {1, . . . ,K} are class labels and \u2227 = min. It is clear that for any \u03b3 > 0 then `\u03b3(f(x), y) \u2265 `(f(x, y)) and it is \u03c3-admissible (in fact, 1/\u03b3-Lipschitz with respect to its first argument). In addition, [30] has shown that for an algorithm with regularization, f + \u03bbR, it contributes to the bounded constant by 1\u03bb\u03b2b. Combining \u03c3\u2212admissible and regularization, 1 \u03bb\u03b2b, (25) becomes,\n|R(Z\\z\u2032\u2032)\u222az\u2032 \u2212R| \u2264 2\n\u03b3\u03bbN (27)\nIndeed, this result satisfies the definition of stability as stated in Definition 4.1. More specifically, when we replace a single sample, the loss function `\u03b3 will change by at most 2\u03b3\u03bbN , meaning that `tot(x, y) might increase while minr 6=y `tot(x, r) might decrease by \u03b2b. Thus, a naive bound on the stability of the multiclass system is 2K\u03b3\u03bbN [31].\nIn order to get the generalization error bound, we use the Bousquet and Elisseeff [30] theorem.\nTheorem 4.1 (Bousquet and Elisseeff [30]): A \u03b2-stable function f satisfying 0 \u2264 `(fS , z) \u2264 M for all training sets S and for all z \u2208 Z . For all > 0 and all N \u2265 0,\nP{R\u2212Remp > + 2\u03b2} \u2264 exp ( \u2212 2N 2\n(4N\u03b2 +M)2\n) (28)\nIt gives the following bounds with probability at least 1\u2212 \u03b4, R \u2264 Remp + 2\u03b2 + (4N\u03b2 +M) \u221a ln 1/\u03b4\n2N (29)\nBy substituting \u03b2 = 2K\u03b3\u03bbN and note that the loss function has maximum value M = 1, we get the following bound on multiclass classification for our proposed method,\nR \u2264 Remp + 4K\n\u03b3\u03bbN + (\n8K \u03b3\u03bb + 1)\n\u221a ln 1/\u03b4\n2N (30)\nWe summarize here, our proposed method has shown to possess stability since the error is bounded by a constant \u03b2 = 2K\u03b3\u03bbN . Moreover, our framework implicitly models regularization thereby it improves the stability (indicated by \u03b2\u03bb ) and provides generalization error bound."}, {"heading": "V. RANKING INTERPRETATION", "text": "Ranking system is a very common yet important information representation technique in many applications, and recently it has received more attention on applying it in inferring the output of the computer vision algorithm [16], [32]. From the general definition, a ranking is a relationship between a set of items such that, for any two items, the first is either \u201cranked higher than\u201d, \u201cranked lower than\u201d or \u201cranked equal to\u201d the second.\nIn the previous section, we obtained the normalized product, rk as our final output. However, these values do not provide us any meaningful information to understand the scene images. In order to interpret the results into a more useful manner, we introduce a ranking framework as shown in Table I which acts similar to a decoder to decode our results into a ranking manner. With step by step explanation for our ranking interpretation,\n1) Obtain the maximum r value, \u2228rk. 2) Discard the scene class with rk = 0 by marking the\nclass as \u00d7, which mean definitely not. 3) Compute the difference value, rdiff between \u2228rk and rk\n(all involved r values) and apply symbolic representation with the predefined threshold as in Table I.\nNote that, the parameter design in our ranking scheme is depend on the design that fits to the task on hand. In our work, we apply four levels of ranking interpretation with equally divided interval value to fit the different levels of ranking with exception to \u201cEqual to\u201d and \u201cDefinitely not\u201d. The four levels of ranking are \u201cEqual to\u201d, \u201cHigher than\u201d, \u201cMuch higher than\u201d, and \u201cDefinitely not\u201d. The value of rdiff = [0 1] which is the difference between \u2228rk with rk is used to determine the level of particular image compare to the other scene images. Refer to Table I, we define \u201cEqual to\u201d as the case when rdiff = 0, \u201cDefinitely not\u201d when the value of r for the class involved\nis equal to 0 (rk = 0), and the left out is \u201cHigher than\u201d and \u201cMuch higher than\u201d. For these two levels, we apply equally divided interval value of the maximum boundary of rdiff which is 0.5 to partition each level. However, as mentioned above, we are not limiting to this setting as it may varies across the different system designs.\nBy applying such method, we can represent a ranking in a symbolic representation. Using the same example from Section III-D1, we obtain the result as in Table II. The confident level of image s belongs to \u201cInsidecity\u201d is higher than \u201cForest\u201d and \u201cCoast\u201d, the confident level of image s belongs to \u201cForest\u201d is higher than \u201cCoast\u201d, but image s is definitely not belonged to \u201cOpencountry\u201d. So at the end of this interpretation method, we are able to obtain the ranking position of every possible classes."}, {"heading": "VI. EXPERIMENTS", "text": "We tested our approach with two public scene image datasets - the Outdoor Scene Recognition (OSR) dataset [1] and the Multi-Label Scene (MLS) dataset [9], [10]. The OSR dataset contains 2688 colour scene images, 256x256 pixels from a total of 8 outdoor scene classes (\u2018Tallbuilding, T\u2019, \u2018Insidecity, I\u2019, \u2018Street, S\u2019, \u2018Highway, H\u2019, \u2018Coast, C\u2019, \u2018Opencounty, O\u2019, \u2018Mountain, M\u2019 and \u2018Forest, F\u2019). Fig. 5 illustrates example of the OSR dataset and is publicly available1. In the meantime, MLS dataset contains a total of 2407 scene images with 15 (6 base + 9 multi-label) classes. According to [9], [10], the multi-label data in the MLS dataset were manually annotated by three human observers as part of the pre-requirement during the training stage.\nIn the feature extraction stage for the OSR dataset, we have employed 6 different attributes [16] to represent the scene images. The 6 attributes are natural, open, perspective, large objects, diagonal plane and close-depth. Note that, this work is not constrained to these representations. An alternative representation such as other feature extraction methods can be employed as the front-end. Since our focus in this study is the introduction of a fuzzy qualitative approach to perform scene classification, any existing feature representation for images can be employed as the input to our model. In the meantime, for MLS dataset, we employed the feature vector, R294 as to [9], [10]. Finally, for learning the model, in OSR dataset, we practiced the \u2018leave-one-out\u2019 method and performed classification of each testing image by using the trained model obtained from the rest of the training data. While for the MLS dataset, we followed the distribution of training and testing data in the classification pipeline. All implementations and experiments were conducted in the MATLAB environment.\nOverall, our experiment is divided into five sections (Section VI-A to VI-E) where each of them is testing on different perspectives. We set the bin number, B of the histogram as 50 and the threshold for the level of ranking interpretation as to Table I.\n1http://people.csail.mit.edu/torralba/code/spatialenvelope"}, {"heading": "A. Scene Images are non-Mutually Exclusive", "text": "Psychological and metaphysical [12] proved that there is an influence of human factors (background, experience, age, etc.) in decision making. In this experiment, we would like to show that the research in scene understanding falls within this category and scene images are indeed non-mutually exclusive. For this purpose, an online survey was created with a fair number of scene images, randomly chosen from the OSR dataset. The online survey was run for a month and participated by a group of people in the range of 12 to 60 years old from different backgrounds and countries. Their task is to select a class that best reflects the given scene accordingly without prior knowledge of what the ground truth is.\nWe show some examples of the results from the online survey in Fig. 6. For a complete result, interested reader is encouraged to look at this website2. From here, we can clearly notice that there is a variation of an answer (scene class) for each scene image. For instance, in Fig. 6(a), although the favorite selection is \u2018Highway\u2019 class, the second choice which is \u2018Insidecity\u2019 class still occupies noticeable distribution as well. From a qualitative point of view, this observation is valid as the scene image comprises of a lot of buildings that form the city view. Similar to Fig. 6(h) where the dominant choice is \u2018Forest\u2019 class while the second choice of \u201cMountain\u201d class is still valid.\nNevertheless, we should not overpass the minority choices. For example, in Fig. 6g, the dominant selection is a \u2018Mountain\u2019 class. However, there are minority who selected \u2018Coast\u2019, \u2018Opencountry\u2019 and \u2018Forest\u2019, respectively. Even though these choices are minority, the selections are still valid as we could notice similar appearance between those choices. Unfortunately, there are some outliers as depicted in Fig. 6(a) and 6(f) which could be easily eliminated with the \u03b1-cut that will be explained later.\nBesides that, one could observe that the best result from the histogram of Fig. 6(a,b,c,e,f,g,h,i) agrees with the ground truth except for the case in 6(c) where the best result is different from the ground truth. In particular, the image seems to be \u2018Opencountry\u2019 more than \u2018Insidecity\u2019. This is very interesting results to show that human are bias in identifying a scene image. As a summary, we had shown that assuming scene images are mutually exclusive and simplify the classification problem (uncertainty, complexity, volatility and ambiguity) to a binary classification task is impractical as it does not reflect how human reasoning is performed in reality."}, {"heading": "B. Effectiveness of FQRC", "text": "This is to show the correctness of our classifier in handling non-mutually exclusive data and the inconsistency of human decision making. We denote Yd as the set of result value for scenery image d from the survey and Wd be the set of predicted label from the FQRC. The results are compared in the following aspects:\n2http://web.fsktm.um.edu.my/\u223ccschan/project2.htm\n1) Qualitative Observation: We show the corresponding results from the online survey and our proposed FQRC in Fig. 7. Here, we can clearly see that the outcomes from both solutions are almost similar in terms of the ranking and the voting distributions. For instance, in Fig 7(d), majority choose \u201dTallbuilding\u201d (84.2%) and follow by \u201dInsidecity\u201d (15.4%). This is nearly close to the reading computed from FQRC where \u201dTallbuilding\u201d is 76% and \u201dInsidecity\u201d hold 22.7%.\nHowever, one should understand that, this is almost impossible to obtain exactly the same values to the survey result due to the subjective nature of human decision making. What surprised us from the observation is the ranking of the distribution are very close to the results from FQRC compared to the survey. For example, in Fig. 7(d), by considering only the \u2019hit\u2019 labels for both results (\u2018Tallbuilding, C\u2019 and \u2018Insidecity, I\u2019), the order of the distribution for FQRC computed result is T > I which is similar to the survey results, where T > I although the values are not exactly the same.\nFrom this observation, we can draw a preliminary conclusion that our proposed approach can emulate human reasoning in classifying scene images. To further validate this, quantitative evaluation is done in the following context.\n2) Quantitative Evaluation: In order to show that our proposed method is able to model the inconsistency of human decision making, we perform a quantitative evaluation using several evaluation criteria as explained in following contexts.\n\u03b1-Evaluation. Evaluation of multi-label classification results is more complicated compared to binary classification because a result can be fully correct, partly correct, or fully incorrect. By using the example given by [9], let\u2019s assume we have classes c1, c2, c3 and c4. Take an example belongs to classes c1 and c2, we may get one of the results below: \u2022 c1, c2 (fully correct), \u2022 c1 (partly correct), \u2022 c1, c3 (partly correct), \u2022 c1, c3, c4 (partly correct), \u2022 c3, c4, (fully incorrect) Herein, we wish to measure the degree of correctness of those possible results with their proposed \u03b1-Evaluation. The score is predicted by the following formula:\nscore(W bd ) =\n( 1\u2212 |\u03b2Md + \u03b3Qd|\n|Y bd \u222aW bd |\n)\u03b1 (31)\nwhere Y bd is the set of ground truth labels for the image sample d in binary form (Yd > 0) and W bd is the set of prediction labels from the FQRC in binary form (Wd > 0). Also, Md = Y bd \u2212W bd (missed labels) and Qd = W bd \u2212 Y bd (false positive labels). \u03b1, \u03b2 and \u03b3 are constraint parameters as explained in [9]. In our evaluation, we select \u03b1 = 0.5, \u03b2 = 1 and \u03b3 = 1 and we calculate the accuracy rate of D.\naccuracyD = 1 |D| \u2211 d\u2208D score(W bd ) (32)\nwhere higher accuracy reflects better reliability of the FQRC because the \u2018hit\u2019 label is almost similar to the survey results.\nCosine similarity measure. Here, we would like to investigate the similarity of the histogram obtained from the survey and the FQRC, respectively by matching the pattern of the distributions. Cosine similarity measure has been employed for this purpose. First, we calculate the cosine distance (33) of the histogram distributions of each scenery image. Then we compute the average value of the similarity (34) to get the overall performance.\ndistance(Wd) = cos\u0398 = Yd \u00b7Wd \u2016Yd\u2016 \u2016Wd\u2016\n(33)\nThe average similarity value for D;\nsimilarityD = 1 |D| \u2211 d\u2208D (1\u2212 distance(Wd)) (34)\nwhere larger value of similarityD indicates higher similarity.\nError rate calculation. In this section, we investigate how much the computed result from the FQRC is deviated from the survey results. In order to achieve this, we obtain the error vector by subtracting both of the histogram distributions (35). Then we calculate the mean and standard deviation of the error vector to observe the range of error as shown in Fig. 8.\nerr(Wd) = |Wd \u2212 Yd| (35)\nFor the overall judgment in error rate, we compute the average standard deviation of the error values obtained from the scene images. Smaller value indicates less deviation of our results from the survey results.\nAll the three evaluation criteria are tested by a comparison between the survey results (with and without \u03b1-cut) and the proposed FQRC. The results are shown in Table III.\nFrom the results in Table III, we could observe reasonable output from these three evaluation criteria. The accuracy is\nabove 70%, which indicates that the computational results using the FQRC is almost mimicking human reasoning in decision making where the \u2018hit\u2019 label is highly matched with the answer from the survey. The high similarity here shows that our approach is able to provide an outcome that is similar to a human decision in terms of voting distribution and ranking.\nBased on the qualitative and quantitative results, we clarify that scene images are non-mutually exclusive and the stateof-the-art approach that uses binary classifier to deduce an unknown image to a specific class is impractical. Besides that, our proposed FQRC has proven its effectiveness as a remedy for this situation based on the comparison with the online survey results."}, {"heading": "C. Feasibility of FQRC", "text": "In this experiment, we test the feasibility of our proposed FQRC in terms of multi-label, multi-class, multi-dimension and ranking. The explanation for each of the abilities is as below:\nTable IV shows how the FQRC distinguishes itself from the other classifiers and each of the capabilities has been clarified with the succeeding experiments in following sections.\n1) FQRC with 2 attributes and 4 scene classes (Multi-label & Multi-class): From the comparison results show in Table V, it can be observed that one drawback of [8] is it provides similar results on certain images, which is very absurd as all the corresponding images are so different from each other and imply that each of the images has its own value of attributes, which should be different from other images. Our proposed approach, in contrast, is able to model this behavior and provides an output that is closer to human perspective. Apart from that, the confident values inferred from our approach are more reasonable compared to [8], for example, in Fig. 9(e), even for subjective judgment, we will consider that the confident level of this image belonged to \u2018Insidecity\u2019 is higher than the \u2019Forest\u2019. This improvement is mainly from the proposed 4-tuple fuzzy membership learning algorithm.\n2) FQRC with 6 attributes and 4 scene classes (Multidimension): In this testing, our proposed framework shows the strength of performing multi-dimensional classification compare to [8] where we employ 6 attributes instead of 2 to perform the classification tasks. The 6 attributes are the score values of \u2018Nature\u2019, \u2018Open\u2019, \u2018Perspective\u2019, \u2018Size-Large\u2019, \u2018Diagonal-Plane\u2019, and \u2018Depth-Close\u2019, respectively. Using the similar testing images as in Fig. 9, the classification results from the FQRC are shown in Table VI.\nAs we compare the result between Table V and VI, one can observe that the result using six attributes are more reasonable than two attributes, especially in Fig. 9(a),9(e),9(f), and 9(g),\nrespectively. Here in the case of Fig. 9(e), with the used of six attributes as to two attributes, the result improved in term of eliminated the noise which is the \u2018Coast\u2019 class that should never been an option for this particular image. However, we observe that the values of confident of Fig. 9(e) in \u2018Insidecity\u2019 and \u2018Forest\u2019 have change significantly. But still, they are in the manner where the confident level of \u2018Insidecity\u2019 is more than \u2018Forest\u2019 which is matched to the subjective judgment.\nSlight changes of these results were incurred as a resultant from the additional of the number of attributes into the classification framework. In fact, more attributes tend to increase the uniqueness of one class from another and this indirectly has increased the discriminative strength of the classifier. However, it is almost impossible to find the optimum attributes (or features) that are best to distinguish one class from another classes especially in the scene understanding task (non-mutually exclusive case). Furthermore, using excessive attributes in the algorithm will increase the computational cost. Therefore, our proposed framework considers a more generative way that provides a good tradeoff between the multidimensional classification capability and the performance of the classification task.\n3) FQRC in ranking (Ranking ability): The goal of this experiment is to show the effectiveness of the proposed FQRC in higher interpretation such as ranking by classifying the possibility of an unknown image into the eight learned scene classes with the correct ordering. To provide more information, we output it with some symbolic representation explained in Section V rather than classify it as one of the eight learned scene classes. The reason is we do not assume the scene classes are mutually exclusive and we understand that scene classes are arbitrary and possibly sub-optimal.\nTable VII shows the sub-sample results using randomly selected scene images from the \u2018Insidecity\u2019 class. The visual appearances of these images are illustrated in Fig. 9. Herein, we can notice that (1) The FQRC is able to correctly classify each image which has the possibility (confident value, rk) in \u2018Insidecity\u2019 class. This is true as the benchmarking for these sub-sample images is selected from the Class = \u2018Insidecity\u2019. Nonetheless, our approach also discovered that each of these images can have possibility belongs to other classes. For instance, our approach discovered that Fig. 9(a) has the possibility as \u2018Tallbuilding\u2019 and \u2018Street\u2019 class. Table VIII shows the symbolic representation of the ranking based on the results in Table VII where q refers to scene image 11(a)\nto 11(g) in particular interpretation.\nAs mentioned in the context, our approach are capable of generate purely linguistic descriptions where an image is described relative to other categories. Fig. 10 shows the examples. Echoing our quantitative results, we can qualitatively observe that the relative descriptions are more precise and informative than the binary ones."}, {"heading": "D. Comparison to state-of-the-art binary classifiers in single label classification task", "text": "One of the strengths of the FQRC is it provides the feasibility to perform single-label classification task like other binary classifiers as well as ranking as shown in the aforementioned subsection. To verify this, here, we compare the FQRC\nagainst the state-of-the-art binary classifiers such as K-nearest neighbor (KNN), Directed Acyclic Graph SVM (DAGSVM) [33], and Fuzzy least squares SVM (LSSVM) [34]. In the FQRC, we have employed max aggregation, z = max(r) to obtain the maximum confident value as binary classification results.\nFor a fair comparison, we perform the classification task with 2 attributes and 4 classes for all classifiers as KNN could not handle the multi-dimensional classification. In the configuration of each classifier in the comparison, we use conventional KNN with empirical chosen parameter K = 5. As for DAGSVM [33] and LSSVM [34], DAGSVM runs with RBF as kernel and margin parameter, C = 100 using SMO training while LSSVM is implemented based on linear SVM with C = 2000 and incorporates with the least square solution.\nThe F\u2212score (Fig. 12) is calculated to show the accuracy of the classification task by comparing our FQRC and three other classifiers. In information retrieval literatures, the F \u2212 score is often used for evaluating this quantity:\nF = 2\u03bd\u03c1\n\u03bd + \u03c1 . (36)\nThe recall, \u03c1 and the precision, \u03bd measure the configuration errors between the ground truth and the classification result. For a good inference quality, both the recall and precision should have high values. The ROC graphs show in Fig. 11 is to evaluate the sensitivity of the classifiers while Fig. 12 illustrates the F-score for each classification task. From both figures, we can notice that our proposed method is comparable with the KNN, DAGSVM, and LSSVM. Most of the time,\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFalse Positive Rate\nT r u e P\no s it iv\ne R\na te\nROC\nFQRC KNN DAGSVM LSSVM\n(c) Opencountry\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFalse Positive Rate\nT r u e P\no s it iv\ne R\na te\nROC\nFQRC KNN DAGSVM LSSVM\n(d) Forest\nFQRC outperforms other binary classifiers but is slightly inefficient as compared to DAGSVM.\nOne of the main reasons is DAGSVM used an efficient data structure to express the decision node in the graph, and an improved decision algorithm is used to find the class of each test sample and thus makes the decision more accurate compared to other binary classifiers. In short, DAGSVM is a discriminative classifier that was implemented and trained to distinguish distinctly amongst the data where there is no crossover tolerance in the data distribution. This is in contrary to the FQRC as a generative classifier to relief the ignorance of non-mutually exclusive data. In conclusion, DAGSVM should be better in most of the case compared to FQRC as a binary classifier. However, here, in this context, the objective is to show that one of the strengths of FQRC is the capability to perform a single-label classification task while playing the role of ranking classifier, which yields comparable results with the\nFig. 12. Comparison of F-score between the classifiers. Class 1 (Insidecity), Class 2 (Coast), Class 3 (Opencountry), and Class 4 (Forest).\nother state-of-the-art binary classifiers."}, {"heading": "E. Comparison to state-of-the-art multi-label scene classification approaches", "text": "In order to show the effectiveness and efficiency of our proposed method, in this experiment, we compare the FQRC with the state-of-the-art multi-label scene classification approaches [9], [10]. This comparison is performed with MLS dataset. The comparison is done on two aspects: computational complexity and accuracy.\n1) Computational Complexity: First, we show the complexity of our method compare to both approaches with the results presented in Table IX. In this context, N denotes the number of classes, M is the number of features, and T is the number of data. The training complexity of [10] consists of three parts; prior, conditional probability, and the main function of training, while [9] requires to train a classifier for every base class. These greatly increase the computational cost compare to the FQRC.\nIn order to verify the complexity of these methods, the computational time comparison is done with the results show in Table X. From the result, we notice that, our method use the shortest time to train the model which is almost 6x faster than [10] and 227x faster than [9]. However, our inference takes a longer time compared to both methods. This is because we retrieve the fuzzy membership values by considering all the classes of 4-tuples membership functions that corresponds to all the features. This also means that with a reduction in terms of the number of features, we can obtain faster computational speed. The computational time result for testing is done using all the testing data, so it is still acceptable if we apply only one data per cycle with an average of 3 milliseconds of computational time. Nonetheless, [10] suffered from finding the optimal number of nearest neighbor involved in the classification step. This had directly affects the performance of the classification.\n2) Accuracy: For fair comparison, instead of employing all the scene data from the MLS scene dataset, we only selected the multi-label class scene data. It means we eliminate those testing data that are categorized as base class in [9] according to the ground truth and use only the test data in multi-label class. This explains why the results are different from the original paper. Again, we should point out that the intention of this work is focused on the multi-label scene classification.\nBased on [9], \u03b1 is the forgiveness rate because it reflects how much to forgive the errors made in predicting labels. Small value of \u03b1 is more aggressive (tend to forgive error) while a high value is conservative (penalizing error more harshly). In relation to the multi-label classification, \u03b1 = \u221e with a score = 1 occurs only when the prediction is fully correct (all hit and no missed) or 0 otherwise. On the other hand, when \u03b1 = 0, we get the score = 1 except when the answer is fully incorrect (all missed). From Table XI, we could observe that the FQRC outperforms the two other methods with better accuracy in the \u03b1-evaluation.\nIn summary, we have tested the performances of FQRC compared to [9], [10] using MLS scene dataset and obtained superior results. The key factors which distinguish our work from them include: Firstly, we do not require the human intervention in manually annotate the multi-label class images to serve as prior information. This is impractical because it may lead to a large number of classes with sparse sample in the dataset [11]. For instance, the class name \u201cField + Fall foliage + Mountain\u201d has only one image in [9]. Secondly, human annotation is bias, that is different people from different background tend to provide different answers for a scene image. We showed this scenario in our real-world online survey results as well as psychological and metaphysical studies [12]. Thirdly, [9], [10] only output binary results in multi-label classification task while our proposed approach provides ranking information."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we raised an important issue that scene images are non-mutually exclusive. Unfortunately, almost all the existing works focused on scene image understanding assumed that images are mutually exclusive. Related works that do not perform this such as in [9], [10] employed human expert to re-label the image manually in order to obtain multi-label class training data for further processing, which is impractical and bias. Our aim is to raise the awareness in the community regarding this very important, but largely neglected issue. In order to achieve this, we conducted an online survey among people from the different background and subsequently\nproposed the ranking classifier (FQRC) which adopting fuzzy qualitative principle as a resolution. The results from extensive experiments have shown the effectiveness, feasibility, and efficiency of our proposed approach as compared to the other state-of-the-art approaches. Our future work will focus on extending the work with the use a fuzzy loss function [35] and normalized sum of memberships, as well as the investigate the effects of different membership function as the learning model."}], "references": [{"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. of Comput. Vis., vol. 42, no. 3, pp. 145\u2013175, 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "IEEE Int. Conf. Comput. Vis. Pattern Recognit.,, vol. 2, no. 15, pp. 524\u2013531, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Scene classification via plsa", "author": ["A. Bosch", "A. Zisserman", "X. Munoz"], "venue": "Eur. Conf. Comput. Vis., pp. 517\u2013530, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Semantic modeling of natural scenes for content-based image retrieval", "author": ["J. Vogel", "B. Schiele"], "venue": "Int. J. of Comput. Vis., vol. 72, no. 2, pp. 133\u2013157, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Fuzzy sets", "author": ["L. Zadeh"], "venue": "Information and Control, vol. 8, no. 3, pp. 338 \u2013 353, 1965.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1965}, {"title": "Theory of extended fuzzy discreteevent systems for handling ranges of knowledge uncertainties and subjectivity", "author": ["X. Du", "H. Ying", "F. Lin"], "venue": "IEEE Trans. Fuzzy Syst., vol. 17, no. 2, pp. 316\u2013328, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Fuzzy discrete-event systems under fuzzy observability and a test algorithm", "author": ["D. Qiu", "F. Liu"], "venue": "IEEE Trans. Fuzzy Syst., vol. 17, no. 3, pp. 578\u2013589, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "A fuzzy qualitative approach for scene classification", "author": ["C.H. Lim", "C.S. Chan"], "venue": "IEEE Int. Conf. Fuzzy Syst., 2012, pp. 1\u20138.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multi-label scene classification", "author": ["M. Boutell", "J. Luo", "X. Shen", "C. Brown"], "venue": "Pattern Recognit., vol. 37, no. 9, pp. 1757\u20131771, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Pattern Recognit., vol. 40, no. 7, pp. 2038 \u2013 2048, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "Int. J. of Data Warehousing and Mining, vol. 3, no. 3, pp. 1\u201313, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "The ontogeny of common sense", "author": ["L. Forguson", "A. Gopnik"], "venue": "Developing Theories of Mind, pp. 226\u2013243, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Modeling scenes with local descriptors and latent aspects", "author": ["P. Quelhas", "F. Monay", "J.-M. Odobez", "D. Gatica-Perez", "T. Tuytelaars", "L. Van Gool"], "venue": "Int. Conf. Comput. Vis., 2005, pp. 883\u2013890.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Mining multi-label data", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "Data mining and knowledge discovery handbook. Springer, 2010, pp. 667\u2013685.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Scene analysis system using a combined fuzzy logic-based technique", "author": ["J.-Y. Chang", "C.-W. Cho"], "venue": "Journal of the Chinese Institute of Engineers, vol. 25, no. 3, pp. 297\u2013307, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Int. Conf. Comput. Vis., 2011, pp. 503\u2013510.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations", "author": ["H. Chernoff"], "venue": "The Annals of Mathematical Statistics, vol. 23, no. 4, pp. 493\u2013507, 1952.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1952}, {"title": "Statistical learning theory: models, concepts, and results", "author": ["U. von Luxburg", "B. Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:0810.4752, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Fuzzy qualitative trigonometry", "author": ["H. Liu", "G.M. Coghill", "D.P. Barnes"], "venue": "Int. J. of Approximate Reasoning, vol. 51, no. 1, pp. 71\u201388, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Fuzzy qualitative simulation", "author": ["Q. Shen", "R. Leitch"], "venue": "IEEE Transactions on Systems, Man And Cybernetics, vol. 23, no. 4, pp. 1038\u20131061, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Fuzzy qualitative robot kinematics", "author": ["H. Liu", "D.J. Brown", "G.M. Coghill"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 3, pp. 808\u2013822, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "A fuzzy qualitative framework for connecting robot qualitative and quantitative representations", "author": ["H. Liu"], "venue": "IEEE Trans. Fuzzy Syst., vol. 16, no. 6, pp. 1522\u20131530, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Fuzzy qualitative human motion analysis", "author": ["C.S. Chan", "H. Liu"], "venue": "IEEE Trans. Fuzzy Syst., vol. 17, no. 4, pp. 851\u2013862, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Recognition of human motion from qualitative normalised templates", "author": ["C.S. Chan", "H. Liu", "D.J. Brown"], "venue": "Journal of Intelligent and Robotic Systems, vol. 48, no. 1, pp. 79\u201395, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "A fuzzy qualitative approach to human motion recognition", "author": ["C.S. Chan", "H. Liu", "D.J. Brown", "N. Kubota"], "venue": "IEEE Int. Conf. Fuzzy Syst., 2008, pp. 1242\u20131249.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Data-based choice of histogram bin width", "author": ["M. Wand"], "venue": "The American Statistician, vol. 51, no. 1, pp. 59\u201364, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "IEEE Int. Conf. Comput. Vis. Pattern Recognit.,, vol. 1. IEEE, 2005, pp. 886\u2013893.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "A method for selecting the bin size of a time histogram", "author": ["H. Shimazaki", "S. Shinomoto"], "venue": "Neural Computation, vol. 19, no. 6, pp. 1503\u20131527, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Fuzzy clustering by quadratic regularization", "author": ["S. Miyamoto", "K. Umayahara"], "venue": "IEEE Int. Conf. Fuzzy Syst., vol. 2. IEEE, 1998, pp. 1394\u20131399.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "The Journal of Machine Learning Research, vol. 2, pp. 499\u2013526, 2002.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "Everything old is new again: a fresh look at historical approaches in machine learning", "author": ["R.M. Rifkin"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 2002.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Whittlesearch : Image search with relative attribute feedback", "author": ["A. Kovashka", "D. Parikh", "K. Grauman"], "venue": "IEEE Int. Conf. Comput. Vis. Pattern Recognit.,, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Large margin dags for multiclass classification", "author": ["J. Platt", "N. Cristianini", "J. Shawe-Taylor"], "venue": "Advances in neural information processing syst., vol. 12, no. 3, pp. 547\u2013553, 2000.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2000}, {"title": "Fuzzy least squares support vector machines for multiclass problems", "author": ["D. Tsujinishi", "S. Abe"], "venue": "Neural Networks, vol. 16, no. 56, pp. 785 \u2013 792, 2003.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Interval type-2 fuzzy sets constructed from several membership functions: Application to the fuzzy thresholding algorithm", "author": ["M. Pagola", "C. Lopez-Molina", "J. Fernandez", "E. Barrenechea", "H. Bustince"], "venue": "IEEE Trans. Fuzzy Syst., vol. 21, no. 2, pp. 230\u2013244, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Intentionally, most state-of-the-art approaches in scene understanding domain [1]\u2013[4] are exemplar-based and assume that scene images are mutually exclusive, P (A \u2229 B) = 0.", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "Intentionally, most state-of-the-art approaches in scene understanding domain [1]\u2013[4] are exemplar-based and assume that scene images are mutually exclusive, P (A \u2229 B) = 0.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Inspired by the fuzzy set theory proposed by Lotfi Zadeh [5], we argue that scene images are non-mutually exclusive where different people are likely to respond inconsistently.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "This notion became popular among researchers and technologists due to wide spectrum of applications [6], [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "This notion became popular among researchers and technologists due to wide spectrum of applications [6], [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "The notable ones are [8]\u2013[10], where a multilabel scene classification framework is proposed.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "The notable ones are [8]\u2013[10], where a multilabel scene classification framework is proposed.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "This is a tedious job that leads to a large number of classes with the sparse number of sample [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Secondly, the annotated image\u2019s classes are potentially bias as different people tend to respond inconsistently [12] and finally, it does not able to handle multi-dimension data.", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "from different ethnics and age using the OSR dataset [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "In advance, FQRC provides a resolution toward conventional solutions which either perform binary classification [1]\u2013[4] or require human intervention [9], [10].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "In advance, FQRC provides a resolution toward conventional solutions which either perform binary classification [1]\u2013[4] or require human intervention [9], [10].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "In advance, FQRC provides a resolution toward conventional solutions which either perform binary classification [1]\u2013[4] or require human intervention [9], [10].", "startOffset": 150, "endOffset": 153}, {"referenceID": 9, "context": "In advance, FQRC provides a resolution toward conventional solutions which either perform binary classification [1]\u2013[4] or require human intervention [9], [10].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "It differs from the conventional object detection or classification tasks, to the extent that a scene is composed of several entities that are often organized in an unpredictable layout [13].", "startOffset": 186, "endOffset": 190}, {"referenceID": 0, "context": "Oliva and Torralba [1] proposed a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represents the dominant spatial structure of a scene - the spatial envelope as scene representation.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Fei-Fei and Perona [2] proposed the Bayesian hierarchical model extended from latent dirichlet allocation (LDA) to learn natural scene categories.", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "[3] inspired from the previous work and proposed probabilistic latent semantic analysis (pLSA) incorporate with KNN for scene classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Vogel and Schiele [4] used the occurring frequency of different concepts (water, rock, etc.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "To the best of our knowledge, there are numerous multi-label classification research [11], [14]; however, only a few were focused in the domain of scene understanding.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "To the best of our knowledge, there are numerous multi-label classification research [11], [14]; however, only a few were focused in the domain of scene understanding.", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "[9] proposed an approach using SVM with cross-training to build the classifier for every base class.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Inspired by [9], Zhang and Zhou [10] introduced multi-label lazy learning K-nearest neighbor (ML-KNN) as their classification algorithm.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "Inspired by [9], Zhang and Zhou [10] introduced multi-label lazy learning K-nearest neighbor (ML-KNN) as their classification algorithm.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "It also leads to large number of classes with sparse sample [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "In what constitutes the closer work to ours in the fuzzy domain, Lim and Chan [8] proposed a fuzzy qualitative framework and Cho and Chang [15] employed a simple fuzzy logic with two monocular images to understand the scene images.", "startOffset": 78, "endOffset": 81}, {"referenceID": 14, "context": "In what constitutes the closer work to ours in the fuzzy domain, Lim and Chan [8] proposed a fuzzy qualitative framework and Cho and Chang [15] employed a simple fuzzy logic with two monocular images to understand the scene images.", "startOffset": 139, "endOffset": 143}, {"referenceID": 7, "context": "In this paper, we extend the work of [8] by learning the 4tuple membership function from the training data.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "It relaxes the difficulty of obtaining multi-label training data as to [9], [10] where the training steps require human intervention in manually annotate the multi-label training data.", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "It relaxes the difficulty of obtaining multi-label training data as to [9], [10] where the training steps require human intervention in manually annotate the multi-label training data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "In this paper, we have employed the attributes [16] as our image features.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "Theoretically, the importance of the non-mutually exclusive data can be derived from the inequality Chernoff bound [17]:", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "However, this is not true because of uniform convergence of function space F [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "Luxburg and Scholkopf [18] stated that the empirical risk Remp(f) can be inaccurate when N \u2192 \u221e since Chernoff bound only holds for a fixed function f which does not depend on the training data.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "This fuzzy representation of qualitative values is more general than ordinary (crisp) interval representations, since it can represents not only the information stated by a well-determined real interval but also the knowledge embedded in the soft boundaries of the interval [19].", "startOffset": 274, "endOffset": 278}, {"referenceID": 18, "context": "According to [19]\u2013[25], such representation of the 4-tuple fuzzy number is a better qualitative representation as the representation has high resolution and good compositionality.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "According to [19]\u2013[25], such representation of the 4-tuple fuzzy number is a better qualitative representation as the representation has high resolution and good compositionality.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "This is opposed to [9], [10] which require human intervention in manually annotate the training data as prior information.", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "This is opposed to [9], [10] which require human intervention in manually annotate the training data as prior information.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Some theoreticians have attempted to determine an optimal number of bins [26]\u2013[28], but these methods generally make strong assumptions about the shape of distribution.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "Some theoreticians have attempted to determine an optimal number of bins [26]\u2013[28], but these methods generally make strong assumptions about the shape of distribution.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": ", r K |r k \u2208 [0, 1]} is the output of the inference of function i, the scalar output r k is defined in (12) and \u2211K k=1 r i k = 1.", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "More specifically, the structure of maximum entropy problem is to find a probability assignment (or membership function \u03bcjk \u2208 [0, 1]) which avoid bias agreeing with any given information.", "startOffset": 126, "endOffset": 132}, {"referenceID": 28, "context": "Inspired by Miyamoto and Umayahara [29], we utilize the maximum entropy to get the interpretation of 4tuple.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "In details, the 4-tuple membership function with \u03bcjk = 1 (mutually exclusive part) models the classifier while the transition of membership function [0, 1] (non-mutually exclusive part) implicitly models the regularization.", "startOffset": 149, "endOffset": 155}, {"referenceID": 29, "context": "In particular, the concept of stability brought by Bousquet and Elisseeff [30] is employed as it gives guarantee of a \u201cgood\u201d learning machine by deriving generalization error bounds.", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "The constant \u03b2 should be on the order of O( 1 N ) [30].", "startOffset": 50, "endOffset": 54}, {"referenceID": 28, "context": "In order to get the stability of our method, the loss function (13) must be \u03c3-admissible for any \u03c3 (it is also need to be convex) [29]\u2013[31].", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "In order to get the stability of our method, the loss function (13) must be \u03c3-admissible for any \u03c3 (it is also need to be convex) [29]\u2013[31].", "startOffset": 135, "endOffset": 139}, {"referenceID": 29, "context": "In addition, [30] has shown that for an algorithm with regularization, f + \u03bbR, it contributes to the bounded constant by 1 \u03bb\u03b2b.", "startOffset": 13, "endOffset": 17}, {"referenceID": 30, "context": "Thus, a naive bound on the stability of the multiclass system is 2K \u03b3\u03bbN [31].", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "In order to get the generalization error bound, we use the Bousquet and Elisseeff [30] theorem.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "1 (Bousquet and Elisseeff [30]): A \u03b2-stable function f satisfying 0 \u2264 `(fS , z) \u2264 M for all training sets S and for all z \u2208 Z .", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "RANKING INTERPRETATION Ranking system is a very common yet important information representation technique in many applications, and recently it has received more attention on applying it in inferring the output of the computer vision algorithm [16], [32].", "startOffset": 244, "endOffset": 248}, {"referenceID": 31, "context": "RANKING INTERPRETATION Ranking system is a very common yet important information representation technique in many applications, and recently it has received more attention on applying it in inferring the output of the computer vision algorithm [16], [32].", "startOffset": 250, "endOffset": 254}, {"referenceID": 0, "context": "The value of rdiff = [0 1] which is the difference between \u2228rk with rk is used to determine the level of particular image compare to the other scene images.", "startOffset": 21, "endOffset": 26}, {"referenceID": 0, "context": "We tested our approach with two public scene image datasets - the Outdoor Scene Recognition (OSR) dataset [1] and the Multi-Label Scene (MLS) dataset [9], [10].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "We tested our approach with two public scene image datasets - the Outdoor Scene Recognition (OSR) dataset [1] and the Multi-Label Scene (MLS) dataset [9], [10].", "startOffset": 150, "endOffset": 153}, {"referenceID": 9, "context": "We tested our approach with two public scene image datasets - the Outdoor Scene Recognition (OSR) dataset [1] and the Multi-Label Scene (MLS) dataset [9], [10].", "startOffset": 155, "endOffset": 159}, {"referenceID": 8, "context": "According to [9], [10], the multi-label data in the MLS dataset were manually annotated by three human observers as part of the pre-requirement during the training stage.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "According to [9], [10], the multi-label data in the MLS dataset were manually annotated by three human observers as part of the pre-requirement during the training stage.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "In the feature extraction stage for the OSR dataset, we have employed 6 different attributes [16] to represent the scene images.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "In the meantime, for MLS dataset, we employed the feature vector, R as to [9], [10].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "In the meantime, for MLS dataset, we employed the feature vector, R as to [9], [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "Psychological and metaphysical [12] proved that there is an influence of human factors (background, experience, age, etc.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "By using the example given by [9], let\u2019s assume we have classes c1, c2, c3 and c4.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "\u03b1, \u03b2 and \u03b3 are constraint parameters as explained in [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 32, "context": "KNN X SVM X [33] X X [9] X X X FQRC X X X X", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "KNN X SVM X [33] X X [9] X X X FQRC X X X X", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "1) FQRC with 2 attributes and 4 scene classes (Multi-label & Multi-class): From the comparison results show in Table V, it can be observed that one drawback of [8] is it provides similar results on certain images, which is very absurd as all the corresponding images are so different from each other and imply that each of the images has its own value of attributes, which should be different from other images.", "startOffset": 160, "endOffset": 163}, {"referenceID": 7, "context": "Apart from that, the confident values inferred from our approach are more reasonable compared to [8], for example, in Fig.", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "2) FQRC with 6 attributes and 4 scene classes (Multidimension): In this testing, our proposed framework shows the strength of performing multi-dimensional classification compare to [8] where we employ 6 attributes instead of 2 to perform the classification tasks.", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "Scene FQRC [8]", "startOffset": 11, "endOffset": 14}, {"referenceID": 32, "context": "To verify this, here, we compare the FQRC against the state-of-the-art binary classifiers such as K-nearest neighbor (KNN), Directed Acyclic Graph SVM (DAGSVM) [33], and Fuzzy least squares SVM (LSSVM) [34].", "startOffset": 160, "endOffset": 164}, {"referenceID": 33, "context": "To verify this, here, we compare the FQRC against the state-of-the-art binary classifiers such as K-nearest neighbor (KNN), Directed Acyclic Graph SVM (DAGSVM) [33], and Fuzzy least squares SVM (LSSVM) [34].", "startOffset": 202, "endOffset": 206}, {"referenceID": 32, "context": "As for DAGSVM [33] and LSSVM [34], DAGSVM runs with RBF as kernel and margin parameter, C = 100 using SMO training while LSSVM is implemented based on linear SVM with C = 2000 and incorporates with the least square solution.", "startOffset": 14, "endOffset": 18}, {"referenceID": 33, "context": "As for DAGSVM [33] and LSSVM [34], DAGSVM runs with RBF as kernel and margin parameter, C = 100 using SMO training while LSSVM is implemented based on linear SVM with C = 2000 and incorporates with the least square solution.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Comparison to state-of-the-art multi-label scene classification approaches In order to show the effectiveness and efficiency of our proposed method, in this experiment, we compare the FQRC with the state-of-the-art multi-label scene classification approaches [9], [10].", "startOffset": 259, "endOffset": 262}, {"referenceID": 9, "context": "Comparison to state-of-the-art multi-label scene classification approaches In order to show the effectiveness and efficiency of our proposed method, in this experiment, we compare the FQRC with the state-of-the-art multi-label scene classification approaches [9], [10].", "startOffset": 264, "endOffset": 268}, {"referenceID": 9, "context": "The training complexity of [10] consists of three parts; prior, conditional probability, and the main function of training, while [9] requires to train a classifier for every base class.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "The training complexity of [10] consists of three parts; prior, conditional probability, and the main function of training, while [9] requires to train a classifier for every base class.", "startOffset": 130, "endOffset": 133}, {"referenceID": 8, "context": "TABLE IX COMPLEXITY OF FQRC COMPARED TO [9] AND [10]", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "TABLE IX COMPLEXITY OF FQRC COMPARED TO [9] AND [10]", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "[10] O(N) +O(T ) + (O(3TN) +O(N)) O(2N)", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] O(NT 3) O(N)", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "From the result, we notice that, our method use the shortest time to train the model which is almost 6x faster than [10] and 227x faster than [9].", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "From the result, we notice that, our method use the shortest time to train the model which is almost 6x faster than [10] and 227x faster than [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 9, "context": "Nonetheless, [10] suffered from finding the optimal number of nearest neighbor involved in the classification step.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "TABLE X COMPUTATIONAL TIME OF FQRC COMPARED TO [9] AND [10] ON MLS DATASET", "startOffset": 47, "endOffset": 50}, {"referenceID": 9, "context": "TABLE X COMPUTATIONAL TIME OF FQRC COMPARED TO [9] AND [10] ON MLS DATASET", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "[10] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "5025 [9] 37.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "It means we eliminate those testing data that are categorized as base class in [9] according to the ground truth and use only the test data in multi-label class.", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "TABLE XI \u03b1-EVALUATION OF FQRC COMPARED TO [10] AND [9]", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "TABLE XI \u03b1-EVALUATION OF FQRC COMPARED TO [10] AND [9]", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "5 \u03b1 = 1 \u03b1 = 2 [10] 1 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "20 [9] 1 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Based on [9], \u03b1 is the forgiveness rate because it reflects how much to forgive the errors made in predicting labels.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "In summary, we have tested the performances of FQRC compared to [9], [10] using MLS scene dataset and obtained superior results.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "In summary, we have tested the performances of FQRC compared to [9], [10] using MLS scene dataset and obtained superior results.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "This is impractical because it may lead to a large number of classes with sparse sample in the dataset [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "For instance, the class name \u201cField + Fall foliage + Mountain\u201d has only one image in [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "We showed this scenario in our real-world online survey results as well as psychological and metaphysical studies [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "Thirdly, [9], [10] only output binary results in multi-label classification task while our proposed approach provides ranking information.", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": "Thirdly, [9], [10] only output binary results in multi-label classification task while our proposed approach provides ranking information.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "Related works that do not perform this such as in [9], [10] employed human expert to re-label the image manually in order to obtain multi-label class training data for further processing, which is impractical and bias.", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "Related works that do not perform this such as in [9], [10] employed human expert to re-label the image manually in order to obtain multi-label class training data for further processing, which is impractical and bias.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "Our future work will focus on extending the work with the use a fuzzy loss function [35] and normalized sum of memberships, as well as the investigate the effects of different membership function as the learning model.", "startOffset": 84, "endOffset": 88}], "year": 2014, "abstractText": "Ambiguity or uncertainty is a pervasive element of many real world decision making processes. Variation in decisions is a norm in this situation when the same problem is posed to different subjects. Psychological and metaphysical research had proven that decision making by human is subjective. It is influenced by many factors such as experience, age, background, etc. Scene understanding is one of the computer vision problems that fall into this category. Conventional methods relax this problem by assuming scene images are mutually exclusive; and therefore, focus on developing different approaches to perform the binary classification tasks. In this paper, we show that scene images are non-mutually exclusive, and propose the Fuzzy Qualitative Rank Classifier (FQRC) to tackle the aforementioned problems. The proposed FQRC provides a ranking interpretation instead of binary decision. Evaluations in term of qualitative and quantitative using large numbers and challenging public scene datasets have shown the effectiveness of our proposed method in modeling the non-mutually exclusive scene images.", "creator": "LaTeX with hyperref package"}}}