{"id": "1410.6991", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2014", "title": "A provable SVD-based algorithm for learning topics in dominant admixture corpus", "abstract": "Making a strong assumption called \\emph{separability}, is a set of properties in the string string, that can be used for many different things. So it's often called \\emph{extensions} that is really not a straight one.\n\n\\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions}\n\\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph{extensions} \\emph", "histories": [["v1", "Sun, 26 Oct 2014 06:00:36 GMT  (48kb,D)", "https://arxiv.org/abs/1410.6991v1", null], ["v2", "Mon, 3 Nov 2014 17:27:07 GMT  (48kb,D)", "http://arxiv.org/abs/1410.6991v2", null], ["v3", "Tue, 4 Nov 2014 05:14:25 GMT  (58kb,D)", "http://arxiv.org/abs/1410.6991v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["trapit bansal", "chiranjib bhattacharyya", "ravindran kannan"], "accepted": true, "id": "1410.6991"}, "pdf": {"name": "1410.6991.pdf", "metadata": {"source": "CRF", "title": "A provable SVD-based algorithm for learning topics in dominant admixture corpus", "authors": ["Trapit Bansal"], "emails": ["trapitbansal@gmail.com", "chiru@csa.iisc.ernet.in", "kannan@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Topic models [1] assume that each document in a text corpus is generated from an ad-mixture of topics, where, each topic is a distribution over words in a Vocabulary. An admixture is a convex combination of distributions. Words in the document are then picked in i.i.d. trials, each trial has a multinomial distribution over words given by the weighted combination of topic distributions. The problem of inference, recovering the topic distributions from such a collection of documents, is provably NP-hard. Existing literature pursues techniques such as variational methods [2] or MCMC procedures [3] for approximating the maximum likelihood estimates.\nGiven the intractability of the problem one needs further assumptions on topics to derive polynomial time algorithms which can provably recover topics. A possible (strong) assumption is that each\nar X\niv :1\n41 0.\n69 91\nv3 [\nst at\n.M L\n] 4\nN ov\ndocument has only one topic but the collection can have many topics. A document with only one topic is sometimes referred as a pure topic document. [7] proved that a natural algorithm, based on SVD, recovers topics when each document is pure and in addition, for each topic, there is a set of words, called primary words, whose total frequency in that topic is close to 1. More recently, [6] show using tensor methods that if the topic weights have Dirichlet distribution, we can learn the topic matrix. Note that while this allows non-pure documents, the Dirichlet distribution gives essentially uncorrelated topic weights.\nIn an interesting recent development [4, 5] gave the first provable algorithm which can recover topics from a corpus of documents drawn from admixtures, assuming separability. Topics are said to be separable if in every topic there exists at least one Anchor word. A word in a topic is said to be an an Anchor word for that topic if it has a high probability in that topic and zero probability in remaining topics. The requirement of high probability in a topic for a single word is unrealistic.\nOur Contributions: Topic distributions, such as those learnt in LDA, try to model the cooccurrence of a group of words which describes a theme. Keeping this in mind we introduce the notion of Catchwords. A group of words are called Catchwords of a topic, if each word occurs strictly more frequently in the topic than other topics and together they have high frequency. This is a much weaker assumption than separability. Furthermore we observe, empirically, that posterior topic weights assigned by LDA to a document often have the property that one of the weights is significantly higher than the rest. Motivated by this observation, which has not been exploited by topic modeling literature, we suggest a new assumption. It is natural to assume that in a text corpus, a document, even if it has multiple themes, will have an overarching dominant theme. In this paper we focus on document collections drawn from dominant admixtures. A document collection is said to be drawn from a dominant admixture if for every document, there is one topic whose weight is significantly higher than other topics and in addition, for every topic, there is a small fraction of documents which are nearly purely on that topic. The main contribution of the paper is to show that under these assumptions, our algorithm, which we call TSVD , indeed provably finds a good approximation in total l1 error to the topic matrix. We prove a bound on the error of our approximation which does not grow with dictionary size d, unlike [5] where the error grows linearly with d.\nEmpirical evidence shows that on semi-synthetic corpora constructed from several real world datasets, as suggested by [5], TSVD substantially outperforms the state of the art [5]. In particular it is seen that compared to [5] TSVD gives 27% lower error in terms of L1 recovery on 90% of the topics.\nProblem Definition: d, k, s will denote respectively, the number of words in the dictionary, number of topics and number of documents. d, s are large, whereas, k is to be thought of as much smaller. Let Sk = {x = (x1, x2, . . . , xk) : xl \u2265 0; \u2211 l xl = 1}. For each topic, there is a fixed vector in Sk giving the probability of each word in that topic. Let M be the d\u00d7 k matrix with these vectors as its columns.\nDocuments are picked in i.i.d. trials. To pick document j, one first picks a k\u2212 vector Wij ,W2j , . . . ,Wkj of topic weights according to a fixed distribution on Sk. Let P\u00b7,j = MW\u00b7,j be the weighted combination of the topic vectors. Then the m words of the document are picked in i.i.d. trials; each trial picks a word according to the multinomial distribution with P\u00b7,j as the probabilities. All that is given as data is the frequency of words in each document, namely, we are given the d \u00d7 s matrix A, where Aij = Number of occurrences of word i in Document jm . Note that E(A|W) = P, where, the expectation is taken entry-wise.\nIn this paper we consider the problem of finding M given A."}, {"heading": "2 Previous Results", "text": "In this section we review literature related to designing provable algorithms for topic models. For an overview of topic models we refer the reader to the excellent survey[1]. Provable algorithms for recovering topic models was started by [7]. Latent Semantic Indexing(LSI) [8] remains a successful method for retrieving similar documents by using SVD. [7] showed that one can recover M from a collection of documents, with pure topics, by using SVD based procedure under the additional Primary Words Assumption. [6] showed that in the admixture case, if one assumes Dirichlet\ndistribution for the topic weights, then, indeed, using tensor methods, one can learn M to l2 error provided some added assumptions on numerical parameters like condition number are satisfied.\nThe first provably polynomial time algorithm for admixture corpus was given in [4, 5]. For a topic l, a word i is an anchor word if Mi,l \u2265 p0 ; Mi,l\u2032 = 0 \u2200l\u2032 6= l.\nTheorem 2.1 [4] If every topic has an anchor word, there is a polynomial time algorithm that returns an M\u0302 such that with high probability,\nk\u2211 l=1 d\u2211 i=1 |M\u0302il \u2212Mil| \u2264 d\u03b5 provided s \u2265 Max { O ( k6 log d a4\u03b52p60\u03b3 2m ) , O ( k4 \u03b32a2 )} ,\nwhere, \u03b3 is the condition number of E(WWT ), a is the minimum expected weight of a topic and m is the number of words in each document.\nNote that the error grows linearly in the dictionary size d, which is often large. Note also the dependence of s on parameters p0, which is, 1/p60 and on a, which is 1/a\n4. If, say, the word \u201crun\u201d is an anchor word for the topic \u201cbaseball\u201d and p0 = 0.1, then the requirement is that every 10 th word in a document on this topic is \u201crun\u201d. This seems too strong to be realistic. It would be more realistic to ask that a set of words like - \u201crun\u201d, \u201chit\u201d, \u201cscore\u201d, etc. together have frequency at least 0.1 which is what our catchwords assumption does."}, {"heading": "3 Learning Topics from Dominant Admixtures", "text": "Informally, a document is said to be drawn from a Dominant Admixture if the document has one dominant topic. Besides its simplicity, we show empirical evidence from real corpora to demonstrate that topic dominance is a reasonable assumption. The Dominant Topic assumption is weaker than the Pure Topic assumption. More importantly SVD based procedures proposed by [7] will not apply. Inspired by the Primary words assumption we introduce the assumption that each topic has a set of Catchwords which individually occur more frequently in that topic than others. This is again a much weaker assumption than both Primary Words and Anchor Words assumptions and can be verified experimentally. In this section we establish that by applying SVD on a matrix, obtained by thresholding the word-document matrix, and subsequent k means clustering can learn topics having Catchwords from a Dominant Admixture corpus."}, {"heading": "3.1 Assumptions: Catchwords and Dominant admixtures", "text": "Let \u03b1, \u03b2, \u03c1, \u03b4, \u03b50 be non-negative reals satisfying:\n\u03b2 + \u03c1 \u2264 (1\u2212 \u03b4)\u03b1. (1) \u03b1+ 2\u03b4 \u2264 0.5 ; \u03b4 \u2264 0.08. (2)\nDominant topic Assumption (a) For j = 1, 2, . . . , s, document j has a dominant topic l(j) such that Wl(j),j \u2265 \u03b1 and Wl\u2032j \u2264 \u03b2, \u2200l\u2032 6= l(j). (b)For each topic l, there are at least \u03b50w0s documents in each of which topic l has weight at least 1\u2212 \u03b4. Catchwords Assumption: There are k disjoint sets of words - S1, S2, . . . , Sk such that with \u03b5 defined in (9)\n\u2200i \u2208 Sl, \u2200l\u2032 6= l, Mil\u2032 \u2264 \u03c1Mil (3)\u2211 i\u2208Sl Mil \u2265 p0 (4)\n\u2200i \u2208 Sl,m\u03b42\u03b1Mil \u2265 8 ln ( 20\n\u03b5w0\n) . (5)\nPart (b.) of the Dominant Topic Assumption is in a sense necessary for \u201cidentifiability\u201d - namely for the model to have a set of k document vectors so that every document vector is in the convex\nhull of these vectors. The Catchwords assumption is natural to describe a theme as it tries to model a unique group of words which is likely to co-occur when a theme is expressed. This assumption is close to topics discovered by LDA like models, which try to model of co-occurence of words. If \u03b1, \u03b4 \u2208 \u2126(1), then, the assumption (5) says Mil \u2208 \u2126\u2217(1/m). In fact if Mil \u2208 o(1/m), we do not expect to see word i (in topic l), so it cannot be called a catchword at all.\nA slightly different (but equivalent) description of the model will be useful to keep in mind. What is fixed (not stochastic) are the matrices M and the distribution of the weight matrix W. To pick document j, we can first pick the dominant topic l in document j and condition the distribution of W\u00b7,j on this being the dominant topic. One could instead also think of W\u00b7,j being picked from a mixture of k distributions. Then, we let Pij = \u2211k l=1MilWlj and pick them words of the document in i.i.d multinomial trials as before. We will assume that\nTl = {j : l is the dominant topic in document j} satisfies |Tl| = wls,\nwhere, wl is the probability of topic l being dominant. This is only approximately valid, but the error is small enough that we can disregard it.\nFor \u03b6 \u2208 {0, 1, 2, . . . ,m}, let pi(\u03b6, l) be the probability that j \u2208 Tl and Aij = \u03b6/m and qi(\u03b6, l) the corresponding \u201cempirical probability\u201d:\npi(\u03b6, l) = \u222b W\u00b7,j ( m \u03b6 ) P \u03b6ij(1\u2212 Pij) m\u2212\u03b6Prob(W\u00b7,j | j \u2208 Tl) Prob(j \u2208 Tl), where, P\u00b7,j = MW\u00b7,j .\n(6)\nqi(\u03b6, l) = 1\ns |{j \u2208 Tl : Aij = \u03b6/m}| . (7)\nNote that pi(\u03b6, l) is a real number, whereas, qi(\u03b6, l) is a random variable with E(qi(\u03b6, l)) = pi(\u03b6, l). We need a technical assumption on the pi(\u03b6, l) (which is weaker than unimodality).\nNo-Local-Min Assumption: We assume that pi(\u03b6, l) does not have a local minimum, in the sense:\npi(\u03b6, l) > Min(pi(\u03b6 \u2212 1, l), pi(\u03b6 + 1, l)) \u2200 \u03b6 \u2208 {1, 2, . . . ,m\u2212 1}. (8)\nThe justification for the this assumption is two-fold. First, generally, Zipf\u2019s law kind of behaviour where the number of words plotted against relative frequencies declines as a power function has often been observed. Such a plot is monotonically decreasing and indeed satisfies our assumption. But for Catchwords, we do not expect this behaviour - indeed, we expect the curve to go up initially as the relative frequency increases, then reach a maximum and then decline. This is a unimodal function and also satisfies our assumption. Indeed, we have empirically observed, see EXPTS, that these are essentially the only two behaviours.\nRelative sizes of parameters Before we close the section we discuss the values of the parameters are in order. Here, s is large. For asymptotic analysis, we can think of it as going to infinity. 1/w0 is also large and can be thought of as going to infinity. [In fact, if 1/w0 \u2208 O(1), then, intuitively, we see that there is no use of a corpus of more than constant size - since our model has i.i.d. documents, intuitively, the number of samples we need should depend mainly on 1/w0]. m is (much) smaller, but need not be constant.\nc refers to a generic constant independent of m, s, 1/w0, \u03b5, \u03b4; its value may be different in different contexts."}, {"heading": "3.2 The TSVD Algorithm", "text": "Existing SVD based procedures for clustering on raw word-document matrices fail because the spread of frequencies of a word within a topic is often more (at least not significantly less) than the gap between the word\u2019s frequencies in two different topics. Hypothetically the frequency for the word run, in the topic Sports, may range from say 0.01 on up. But in other topics, it may range from 0 to 0.005 say. The success of the algorithm will lie on correctly identifying the dominant topics such as sports by identifying that the word run has occurred with high frequency. In this example, the gap (0.01-0.005) between Sports and other topics is less than the spread within Sports (1.0-0.01), so a 2-clustering approach (based on SVD) will split the topic Sports into two. While this is a toy\nexample, note that if we threshold the frequencies at say 0.01, ideally, sports will be all above and the rest all below the threshold, making the succeeding job of clustering easy.\nThere are several issues in extending beyond the toy case. Data is not one-dimensional. We will use different thresholds for each word; word i will have a threshold \u03b6i/m. Also, we have to compute \u03b6i/m. Ideally we would not like to split any Tl, namely, we would like that for each l and and each i, either most j \u2208 Tl have Aij > \u03b6i/m or most j \u2208 Tl have Aij \u2264 \u03b6i/m. We will show that our threshold procedure indeed achieves this. One other nuance: to avoid conditioning, we split the data A into two parts A(1) and A(2), compute the thresholds using A(1) and actually do the thresholding on A(2). We will assume that the intial A had 2s columns, so each part now has s columns. Also, T1, T2, . . . , Tk partitions the columns of A(1) as well as those of A(2). The columns of thresholded matrix B are then clustered, by a technique we call Project and Cluster, namely, we project the columns of B to its k\u2212dimensional SVD subspace and cluster in the projection. The projection before clustering has recently been proven [9] (see also [10]) to yield good starting cluster centers. The clustering so found is not yet satisfactory. We use the classic Lloyd\u2019s k-means algorithm proposed by [12]. As we will show, the partition produced after clustering, {R1, . . . , Rk} of A(2) is close to the partition induced by the Dominant Topics, {T1, . . . , Tk}. Catchwords of topic l are now (approximately) identified as the most frequently occurring words in documents in Rl. Finally, we identify nearly pure documents in Tl (approximately) as the documents in which the catchwords occur the most. Then we get an approximation to M\u00b7,l by averaging these nearly pure documents. We now describe the precise algorithm."}, {"heading": "3.3 Topic recovery using Thresholded SVD", "text": "Threshold SVD based K-means (TSVD) \u03b5 = Min ( 1\n900c20\n\u03b1p0 k3m\n, \u03b50 \u221a \u03b1p0\u03b4\n640m \u221a k ,\n) . (9)\n1. Randomly partition the columns of A into two matrices A(1) and A(2) of s columns each.\n2. Thresholding\n(a) Compute Thresholds on A(1) For each i, let \u03b6i be the highest value of \u03b6 \u2208 {0, 1, 2, . . . ,m} such that |{j : A(1)ij > \u03b6 m}| \u2265 w0 2 s; |{j : A (1) ij = \u03b6 m}| \u2264 3\u03b5w0s.\n(b) Do the thresholding on A(2): Bij =\n{\u221a \u03b6i if A (2) ij > \u03b6i/m and \u03b6i \u2265 8 ln(20/\u03b5w0)\n0 otherwise .\n3. SVD Find the best rank k approximation B(k) to B.\n4. Identify Dominant Topics\n(a) Project and Cluster Find (approximately) optimal k\u2212 means clustering of the columns of B(k).\n(b) Lloyd\u2019s Algorithm Using the clustering found in Step 4(a) as the starting clustering, apply Lloyd\u2019s algorithm k means algorithm to the columns of B (B, not B(k)).\n(c) Let R1, R2, . . . , Rk be the k\u2212partition of [s] corresponding to the clustering after Lloyd\u2019s. //*Will prove that Rl \u2248 Tl*//\n5. Identify Catchwords\n(a) For each i, l, compute g(i, l) = the b\u03b50w0s/2c) th highest element of {A(2)ij : j \u2208 Rl}. (b) Let Jl = { i : g(i, l) > Max ( 4 m\u03b42 ln(20/\u03b5w0),Maxl\u2032 6=l\u03b3 g(i, l \u2032) )} , where, \u03b3 =\n1\u22122\u03b4 (1+\u03b4)(\u03b2+\u03c1) .\n6. Find Topic Vectors Find the b\u03b50w0s/2c highest \u2211 i\u2208Jl A (2) ij among all j \u2208 [s] and return\nthe average of these A\u00b7,j as our approximation M\u0302\u00b7,l to M\u00b7,l.\nTheorem 3.1 Main Theorem Under the Dominant Topic, Catchwords and No-Local-Min assumptions, the algorithm succeeds with high probability in finding an M\u0302 so that\u2211\ni,l\n|Mil \u2212 M\u0302il| \u2208 O(k\u03b4), provided 1s \u2208 \u2126\u2217 ( 1\nw0\n( k6m2\n\u03b12p20 +\nm2k\n\u03b520\u03b4 2\u03b1p0\n+ d\n\u03b50\u03b42\n)) .\nA note on the sample complexity is in order. Notably, the dependence of s on w0 is best possible (namely s \u2208 \u2126\u2217(1/w0)) within logarithmic factors, since, if we had fewer than 1/w0 documents, a topic which is dominant with probability only w0 may have none of the documents in the collection. The dependence of s on d needs to be at least d/\u03b50w0\u03b42: to see this, note that we only assume that there are r = O(\u03b50w0s) nearly pure documents on each topic. Assuming we can find this set (the algorithm approximately does), their average has standard deviation of about \u221a Mil/ \u221a r in coordinate i. If topic vector M\u00b7,l has O(d) entries, each of size O(1/d), to get an approximation of M\u00b7,l to l1 error \u03b4, we need the per coordinate error 1/ \u221a dr to be at most \u03b4/d which implies s \u2265 d/\u03b50w0\u03b42. Note that to get comparable error in [4], we need a quadratic dependence on d. There is a long sequence of Lemmas to prove the theorem. The Lemmas and the proofs are given in Appendix. The essence of the proof lies in proving that the clustering step correctly identifies the partition induced by the dominant topics. For this, we take advantage of a recent development on the k\u2212means algorithm from [9] [see also [10]], where, it is shown that under a condition called the Proximity Condition, Lloyd\u2019s k means algorithm starting with the centers provided by the SVDbased algorithm, correctly identifies almost all the documents\u2019 dominant topics. We prove that indeed the Proximity Condition holds. This calls for machinery from Random Matrix theory (in particular bounds on singular values). We prove that the singular values of the thresholded worddocument matrix are nicely bounded. Once the dominant topic of each document is identified, we are able to find the Catchwords for each topic. Now, we rely upon part (b.) of the Dominant Topic assumption : that is there is a small fraction of nearly Pure Topic-documents for each topic. The Catchwords help isolate the nearly pure-topic documents and hence find the topic vectors. The proofs are complicated by the fact that each step of the algorithm induces conditioning on the datafor example, after clustering, the document vectors in one cluster are not anymore independent."}, {"heading": "4 Experimental Results", "text": "We compare the thresholded SVD based k-means (TSVD2) algorithm 3.2 with the algorithms of [5], Recover-KL and Recover-L2, using the code made available by the authors3. We first provide empirical support for the algorithm assumptions in Section 3.1, namely the dominant topic and the catchwords assumption. Then we show on 4 different semi-synthetic data that TSVD provides as good or better recovery of topics than the Recover algorithms. Finally on real-life datasets, we show that the algorithm performs as well as [5] in terms of perplexity and topic coherence.\nImplementation Details: TSVD parameters (w0, \u03b5, \u03b50, \u03b3) are not known in advance for real corpus. We tested empirically for multiple settings and the following values gave the best performance. Thresholding parameters used were: w0 = 1k , \u03b5 = 1 6 . For finding the catchwords, \u03b3 = 1.1, \u03b50 = 1 3 in step 5. For finding the topic vectors (step 6), taking the top 50% (\u03b50w0 = 1k ) gave empirically better results. The same values were used on all the datasets tested. The new algorithm is sensitive to the initialization of the first k-means step in the projected SVD space. To remedy this, we run 10 independent random initializations of the algorithm with K-Means++ [13] and report the best result.\nDatasets: We use four real word datasets in the experiments. As pre-processing steps we removed standard stop-words, selected the vocabulary size by term-frequency and removed documents with less than 20 words. Datasets used are: (1) NIPS4: Consists of 1,500 NIPS full papers, vocabulary of 2,000 words and mean document length 1023. (2) NYT4: Consists of a random subset of 30,000\n1The superscript \u2217 hides a logarithmic factor in dsk/\u03b4fail, where, \u03b4fail > 0 is the desired upper bound on the probability of failure.\n2Resources available at: http://mllab.csa.iisc.ernet.in/tsvd 3http://www.cs.nyu.edu/\u02dchalpern/files/anchor-word-recovery.zip 4http://archive.ics.uci.edu/ml/datasets/Bag+of+Words\ndocuments from the New York Times dataset, vocabulary of 5,000 words and mean document length 238. (3) Pubmed4: Consists of a random subset of 30,000 documents from the Pubmed abstracts dataset, vocabulary of 5,030 words and mean document length 58. (4) 20NewsGroup5 (20NG): Consist of 13,389 documents, vocabulary of 7,118 words and mean document length 160."}, {"heading": "4.1 Algorithm Assumptions", "text": "To check the dominant topic and catchwords assumptions, we first run 1000 iterations of Gibbs sampling on the real corpus and learn the posterior document-topic distribution ({W.,j}) for each document in the corpus (by averaging over 10 saved-states separated by 50 iterations after the 500 burn-in iterations). We will use this posterior document-topic distribution as the document generating distribution to check the two assumptions.\nDominant topic assumption: Table 1 shows the fraction of the documents in each corpus which satisfy this assumption with \u03b1 = 0.4 (minimum probability of dominant topic) and \u03b2 = 0.3 (maximum probability of non-dominant topics). The fraction of documents which have almost pure topics with highest topic weight at least 0.95 (\u03b4 = 0.05) is also shown. The results indicate that the dominant topic assumption is well justified (on average 64% documents satisfy the assumption) and there is also a substantial fraction of documents satisfying almost pure topic assumption.\nCatchwords assumption: We first find a k-clustering of the documents {T1, . . . , Tk} by assigning all documents which have highest posterior probability for the same topic into one cluster. Then we use step 5 of TSVD (Algorithm 3.2) to find the set of catchwords for each topic-cluster, i.e. {S1, . . . , Sk}, with the parameters: 0w0 = 13k , \u03b3 = 2.3 (taking into account constraints in Section 3.1, \u03b1 = 0.4, \u03b2 = 0.3, \u03b4 = 0.05, \u03c1 = 0.07). Table 1 reports the fraction of topics with non-empty set of catchwords and the average per topic frequency of the catchwords6. Results indicate that most topics on real data contain catchwords (Table 1, second-last column). Moreover, the average per-topic frequency of the group of catchwords for that topic is also quite high (Table 1, last column).\nNo-Local-Min Assumption: To provide support and intuition for the local-min assumption we consider the quantity qi(\u03b6, l), in (7). Recall that E[qi(\u03b6, l)] = pi(\u03b6, l), we will analyze the behavior of qi(\u03b6, l) as a function of \u03b6 for some topics l and words i. As defined, we need a fixed m to check this assumption and so we generate semi-synthetic data with a fixed m from LDA model trained on the real NYT corpus (as explained in Section 4.2.1). We find catchwords and the sets {Tl} as in the catchwords assumption above and plot qi(\u03b6, l) separately for some random catchwords and non-catchwords by fixing some random l \u2208 [k]. Figure 1 shows the plots. As explained in 3.1, the plots are monotonically decreasing for non-catchwords and satisfy the assumption. On the other hand, the plots for catchwords are almost unimodal and also satisfy the assumption."}, {"heading": "4.2 Empirical Results", "text": ""}, {"heading": "4.2.1 Topic Recovery on Semi-Synthetic Data", "text": "Semi-synthetic Data: Following [5], we generate semi-synthetic corpora from LDA model trained by MCMC, to ensure that the synthetic corpora retain the characteristics of real data. Gibbs sampling\n5http://qwone.com/\u02dcjason/20Newsgroups 6 (\n1 k \u2211k l=1 1 |Tl| \u2211 i\u2208Sl \u2211 j\u2208Tl Aij )"}, {"heading": "Corpus Documents Recover-L2 Recover-KL TSVD % Improvement", "text": "is run for 1000 iterations on all the four datasets and the final word-topic distribution is used to generate varying number of synthetic documents with document-topic distribution drawn from a symmetric Dirichlet with hyper-parameter 0.01. For NIPS, NYT and Pubmed we use k = 50 topics, for 20NewsGroup k = 20, and mean document lengths of 1000, 300, 100 and 200 respectively. Note that the synthetic data is not guaranteed to satisfy dominant topic assumption for every document (on average about 80% documents satisfy the assumption for value of (\u03b1, \u03b2) tested in Section 4.1)\nTopic Recovery: We learn the word-topic distributions (M\u0302 ) for the semi-synthetic corpora using TSVD and the Recover algorithms of [5]. Given these learned topic distributions and the original data-generating distributions (M ), we align the topics of M and M\u0302 by bipartite matching and rearrange the columns of M\u0302 in accordance to the matching with M . Topic recovery is measured by the average of the l1 error across topics (called reconstruction error [5]), \u2206(M,M\u0302), defined as: \u2206(M, M\u0302) = 1k \u2211k l=1 \u2211d i=1 |Mil \u2212 M\u0302il|.\nWe report reconstruction error in Table 2 for TSVD and the Recover algorithms, Recover-L2 and Recover-KL. TSVD has smaller error on most datasets than the Recover-KL algorithm. We observed performance of TSVD to be always better than Recover-L2. Best performance is observed on NIPS which has largest mean document length, indicating that larger m leads to better recovery. Results on 20NG are slightly worse than Recover-KL for small sample size (though better than RecoverL2), but the difference is small. While the values in Table 2 are averaged values, Figure 2 shows that TSVD algorithm achieves much better topic recovery (27% improvement in l1 error over RecoverKL) for majority of the topics (>90%) on most datasets."}, {"heading": "4.2.2 Topic Recovery on Real Data", "text": "Perplexity: A standard quantitative measure used to compare topic models and inference algorithms is perplexity [2]. Perplexity of a set of D test documents, where each document j consists of mj words, denoted by wj , is defined as: perp(Dtest) = exp { \u2212 \u2211D j=1 log p(wj)\u2211D\nj=1mj\n} . To evaluate\nperplexity on real data, the held-out sets consist of 350 documents for NIPS, 10000 documents for NYT and Pubmed, and 6780 documents for 20NewsGroup. Table 3 shows the results of perplexity on the 4 datasets. TSVD gives comparable perplexity with Recover-KL, results being slightly better on NYT and 20NewsGroup which are larger datasets with moderately high mean document lengths.\nTopic Coherence: [11] proposed Topic Coherence as a measure of semantic quality of the learned topics by approximating user experience of topic quality on top d0 words of a topic. Topic coherence is defined as TC(d0) = \u2211 i\u2264d0 \u2211 j<i log D(wi,wj)+e D(wj)\n, where D(w) is the document frequency of a word w, D(wi, wj) is the document frequency of wi and wj together, and e is a small constant. We evaluate TC for the top 5 words of the recovered topic distributions and report the average and standard deviation across topics. TSVD gives comparable results on Topic Coherence (see Table 3).\nTopics on Real Data: Table 4 shows the top 5 words of all 50 matched pair of topics on NYT dataset for TSVD, Recover-KL and Gibbs sampling. Most of the topics recovered by TSVD are more closer to Gibbs sampling topics. Indeed, the total average l1 error with topics from Gibbs sampling for topics from TSVD is 0.034, whereas for Recover-KL it is 0.047 (on the NYT dataset).\nSummary: We evaluated the proposed algorithm, TSVD, rigorously on multiple datasets with respect to the state of the art (Recover), following the evaluation methodology of [5]. In Table 2 we\nshow that the L1 reconstruction error for the new algorithm is small and on average 19.6% better than the best results of the Recover algorithms [5]. We also demonstrate that on real datasets the algorithm achieves comparable perplexity and topic coherence to Recover (Table 3. Moreover, we show on multiple real datasets that the algorithm assumptions are well justified in practice."}, {"heading": "Conclusion", "text": "Real world corpora often exhibits the property that in every document there is one topic dominantly present. A standard SVD based procedure will not be able to detect these topics, however TSVD, a thresholded SVD based procedure, as suggested in this paper, discovers these topics. While SVD is time-consuming, there have been a host of recent sampling-based approaches which make SVD easier to apply to massive corpora which may be distributed among many servers. We believe that apart from topic recovery, thresholded SVD can be applied even more broadly to similar problems, such as matrix factorization, and will be the basis for future research."}, {"heading": "TSVD Recover-KL Gibbs", "text": ""}, {"heading": "TSVD Recover-KL Gibbs", "text": ""}, {"heading": "A Line of Proof", "text": "We describe the Lemmas we prove to establish the result. The detailed proofs are in the Section B."}, {"heading": "A.1 General Facts", "text": "We start with a consequence of the no-local-minimum assumption. We use that assumption solely through this Lemma.\nLemma A.1 Let pi(\u03b6, l) be as in (6). If for some \u03b60 \u2208 {0, 1, . . . ,m} and \u03bd \u2265 0, \u2211 \u03b6\u2265\u03b60 pi(\u03b6, l) \u2265 \u03bd\nand also \u2211 \u03b6\u2264\u03b60 pi(\u03b6, l) \u2265 \u03bd then, pi(\u03b60, l) \u2265 \u03bd m .\nNext, we state a technical Lemma which is used repeatedly. It states that for every i, \u03b6, l, the empirical probability that Aij = \u03b6/m is close to the true probability. Unsurprisingly, we prove it using H-C. But we will state a consequence in the form we need in the sequel.\nLemma A.2 Let pi(\u03b6, l) and qi(\u03b6, l) be as in (6) and (7). We have \u2200i, l, \u03b6 : Prob ( |pi(\u03b6, l)\u2212 qi(\u03b6, l)| \u2265 \u03b5\n2\n\u221a w0 \u221a pi(\u03b6, l) +\n\u03b52w0 2\n) \u2264 2 exp(\u2212\u03b52sw0/8).\nFrom this it follows that with probability at least 1\u2212 2 exp(\u2212\u03b52w0s/8), 1\n2 qi(\u03b6, l)\u2212 \u03b52w0 \u2264 pi(\u03b6, l) \u2264 2qi(\u03b6, l) + 2\u03b52w0."}, {"heading": "A.1.1 Properties of Thresholding", "text": "Say that a threshold \u03b6i \u201csplits\u201d T (2) l if T (2) l has a significant number of j withAij > \u03b6i/m and also a significant number of j with Aij \u2264 \u03b6i/m. Intuitively, it would be desirable if no threshold splits any Tl, so that, in B, for each i, l, either most j \u2208 T (2)l have Bij = 0 or most j \u2208 T (2) l have Bij = \u221a \u03b6i. We now prove that this is indeed the case with proper bounds. We henceforth refer to the conclusion of the Lemma below by the mnemonic \u201cno threshold splits any Tl\u201d.\nLemma A.3 (No Threshold Splits any Tl) For a fixed i, l, with probability at least 1 \u2212 2 exp(\u2212\u03b52w0s/8), the following holds:\nMin (\nProb(A(2)ij \u2264 \u03b6i m ; j \u2208 T (2)l ), Prob(A (2) ij > \u03b6i m\n; j \u2208 T (2)l ) ) \u2264 4m\u03b5w0.\nLet \u00b5 be a d\u00d7 s matrix whose columns are given by \u2200j \u2208 T (2)l , \u00b5.,j = E(B.,j | j \u2208 Tl). \u00b5 \u2019s columns corresponding to all j \u2208 Tl are the same. The entries of the matrix \u00b5 are fixed (real numbers) once we have A(1) (and the thresholds \u03b6i are determined). Note: We have \u201cintegrated out W \u201d, i.e.,\n\u00b5ij = \u222b W\u00b7,j Prob(W.,j |j \u2208 Tl)E(Bij |W.,j).\n(So, think of W\u00b7,j for A(1) \u2019s columns being picked first from which \u03b6i is calculated. W\u00b7,j for columns of A(2) are not yet picked until the \u03b6i are determined.) But \u00b5ij are random variables before we fix A(1). The following Lemma is a direct consequence of \u201cno threshold splits any Tl\u201d.\nLemma A.4 Let \u03b6 \u2032i = Max(\u03b6i, 8 ln(20/\u03b5w0)). With probability at least 1 \u2212 4kd exp(\u2212\u03b52sw0/8) (over the choice of A(1)):\n\u2200l,\u2200j \u2208 Tl,\u2200i :\u00b5ij \u2264 \u03b5l \u221a \u03b6 \u2032i OR \u00b5ij \u2265 \u221a \u03b6 \u2032i(1\u2212 \u03b5l)\n\u2200l,\u2200i,Var(Bij) \u2264 2\u03b5l\u03b6 \u2032i, (10)\nwhere, \u03b5l = 4m\u03b5w0/wl.\nSo far, we have proved that for every i, the threshold does not split any Tl. But this is not sufficient in itself to be able to cluster (and hence identify the Tl), since, for example, this alone does not rule out the extreme cases that for most j in every Tl,A (2) ij is above the threshold (whence \u00b5ij \u2265 (1\u2212\u03b5l) \u221a \u03b6 \u2032l for almost all j) or for most j in no Tl is A (2) ij above the threshold, whence, \u00b5ij \u2264 \u03b5l \u221a \u03b6 \u2032i for almost all j. Both these extreme cases would make us loose all the information about Tl due to thresholding; this scenario and milder versions of it have to be proven not to occur. We do this by considering how thresholds handle catchwords. Indeed we will show that for a catchword i \u2208 Sl, a j \u2208 Tl has A\n(2) ij above the threshold and a j /\u2208 Tl has A (2) ij below the threshold. Both statements will only hold with high probability, of course and using this, we prove that \u00b5.,j and \u00b5.,j\u2032 are not too close for j, j\u2032 in different Tl \u2019s. For this, we need the following Lemmas.\nLemma A.5 For i \u2208 Sl, and l\u2032 6= l, we have with \u03b7i = \u230a Mil(\u03b1+ \u03b2 + \u03c1)m/2 \u230b ,\nProb(Aij \u2264 \u03b7i/m | j \u2208 Tl) \u2264 \u03b5w0/20, Prob(Aij \u2265 \u03b7i/m | j \u2208 Tl\u2032) \u2264 \u03b5w0/20.\nLemma A.6 With probability at least 1\u2212 8mdk exp(\u2212\u03b52w0s/8), we have\nfor j \u2208 Tl, j\u2032 /\u2208 Tl, |\u00b5\u00b7,j \u2212 \u00b5\u00b7,j\u2032 |2 \u2265 2\n9 \u03b1p0m."}, {"heading": "A.1.2 Proximity", "text": "Next, we wish to show that clustering as in TSVD identifies the dominant topics correctly for most documents, i.e., that Rl \u2248 Tl for all l. For this, we will use a theorem from [9] [see also [10]] which in this context says:\nTheorem A.7 If all but a f fraction of the the B\u00b7,j satisfy the \u201cproximity condition\u201d, then TSVD identifies the dominant topic in all but c1f fraction of the documents correctly after polynomial number of iterations.\nTo describe the proximity condition, first let \u03c3 be the maximum over all directions v of the square root of the mean-squared distance of B.,j to \u00b5.,j , i.e.,\n\u03c32 = Max\u2016v\u2016=1 1 s |vT (B\u2212 \u00b5)|2 = 1 s \u2016B\u2212 \u00b5\u20162.\nThe parameter \u03c3 should remind the reader of standard deviation, which is indeed what this is, since E(B|T1, T2, . . . , Tl) = \u00b5. Our random variables B.,j being d\u2212 dimensional vectors, we take the maximum standard deviation in any direction.\nDefinition: B is said to satisfy the proximity condition with respect to \u00b5, if for each l and each j \u2208 Tl and and each l\u2032 6= l and j\u2032 \u2208 Tl\u2032 , the projection of B.,j onto the line joining \u00b5.,j and \u00b5.,j\u2032 is closer to \u00b5.,j by at least\n\u2206 = c0k\u221a w0 \u03c3,\nthan it is to \u00b5.,j\u2032 . [Here, c0 is a constant.]\nTo prove proximity, we need to bound \u03c3. This will be the task of the subsection B.1 which relies heavily on Random Matrix Theory."}, {"heading": "B Proofs of Correctness", "text": "We start by recalling the Ho\u0308ffding-Chernoff (H-C) inequality in the form we use it.\nLemma B.1 Ho\u0308ffding-Chernoff IfX is the average of r independent random variables with values in [0, 1] and E(X) = \u00b5, then, for an t > 0,\nProb(X \u2265 \u00b5+ t) \u2264 exp ( \u2212 t 2r\n2(\u00b5+ t)\n) ; Prob(X \u2264 \u00b5\u2212 t) \u2264 exp ( \u2212 t 2r\n2\u00b5\n) .\nProof: (of Lemma A.1) Abbreviate pi(\u00b7, l) by f(\u00b7). We claim that either (i) f(\u03b6) \u2265 f(\u03b6 \u2212 1)\u22001 \u2264 \u03b6 \u2264 \u03b60 or (ii) f(\u03b6 + 1) \u2264 f(\u03b6)\u2200m \u2212 1 \u2265 \u03b6 \u2265 \u03b60. To see this, note that if both (i) and (ii) fail, we have \u03b61 \u2264 \u03b60 and \u03b62 \u2265 \u03b60 with f(\u03b61)\u2212 f(\u03b61 \u2212 1) < 0 < f(\u03b62 + 1)\u2212 f(\u03b62). But then there has to be a local minimum of f between \u03b61 and \u03b62. If (i) holds, clearly, f(\u03b60) \u2265 f(\u03b6)\u2200\u03b6 < \u03b60 and so the lemma follows. So, also if (ii) holds.\nProof: (of Lemma A.2) Note that qi(\u03b6, l) = 1s |{j \u2208 Tl : Aij = \u03b6/m}| = 1 s \u2211s j=1Xj , where, Xj\nis the indicator variable of Aij = \u03b6/m \u2227 j \u2208 Tl. 1s \u2211 j E(Xj) = pi(\u03b6, l) and we apply H-C with\nt = 12\u03b5 \u221a w0 \u221a pi(\u03b6, l) + 1 2\u03b5 2w0 and \u00b5 = pi(\u03b6, l). We have t 2 \u00b5+t \u2265 \u03b5 2w0/4, as is easily seen by calculating the roots of the quadratic t2 \u2212 14 t\u03b5 2w0 \u2212 14\u03b5\n2w0\u00b5 = 0. Thus we get the claimed for Tl. Note that the same proof applies for T (1)l as well as T (2) l .\nTo prove the second assertion, let a = qi(\u03b6, l) and b = \u221a pi(\u03b6, l), then, b satisfies the quadratic inequalities:\nb2 \u2212 1 2 \u03b5 \u221a w0b\u2212 (a+ 1 2 \u03b52w0) \u2264 0 ; b2 + 1 2 \u03b5 \u221a w0b\u2212 (a\u2212 1 2 \u03b52w0) \u2265 0.\nBy bounding the roots of these quadratics, it is easy to see the second assertion after some calculation.\nProof: (of Lemma A.3) Note that \u03b6i is a random variable which depends only on A(1). So, for j \u2208 T (2)l , Aij are independent of \u03b6i. Now, if\nProb(Aij \u2264 \u03b6i m ; j \u2208 T (2)l ) > 4m\u03b5w0 and Prob(Aij > \u03b6i m ; j \u2208 T (2)l ) > 4m\u03b5w0,\nby Lemma (A.1), we have\nProb(Aij = \u03b6i m ; j \u2208 T (2)l ) > 4\u03b5w0.\nSince Prob(Aij = \u03b6/m; j \u2208 T (1)l ) = Prob(Aij = \u03b6/m; j \u2208 T (2) l ) for all \u03b6, we also have\nProb(Aij = \u03b6i m ; j \u2208 T (1)l ) = pi(\u03b6i, l) > 4\u03b5w0. (11)\nPay a failure probability of 2 exp(\u2212\u03b52sw0/8) and assume the conclusion of Lemma (A.2) and we have:\n1\ns \u2223\u2223\u2223\u2223{j \u2208 T (1)l : Aij = \u03b6im} \u2223\u2223\u2223\u2223 = qi(\u03b6i, l) \u2265 pi(\u03b6i, l)\u2212 \u03b52\u221aw0pi(\u03b6i, l)\u2212 \u03b522 w0.\nNow, it is easy to see that pi(\u03b6, l)\u2212 \u03b52 \u221a w0pi(\u03b6, l) increases as pi(\u03b6, l) increases subject to (11). So,\npi(\u03b6, l)\u2212 \u03b5\n2\n\u221a w0pi(\u03b6, l)\u2212 \u03b52\n2 w0 > (4\u03b5\u2212 \u03b53/2 \u2212\n1 2 \u03b52)w0 \u2265 3\u03b5w0,\ncontradicting the definition of \u03b6i in the algorithm. This completes the proof of the Lemma.\nProof: (of Lemma A.4): After paying a failure probability of 4kd exp(\u2212\u03b52sw0/8), assume no threshold splits any Tl. [The factors of k and d come in because we are taking the union bound over\nall words and all topics.] Then,\nProb(A(2)ij \u2264 \u03b6i m | j \u2208 T (2)l ) = \u03b6i\u2211 \u03b6=0 pi(\u03b6, l)/Prob(j \u2208 Tl) \u2264 4m\u03b5 w0 wl\nor Prob(A(2)ij > \u03b6i m | j \u2208 T (2)l ) = m\u2211 \u03b6=\u03b6i+1 pi(\u03b6, l)/Prob(j \u2208 Tl) \u2264 4m\u03b5 w0 wl .\nWlg, assume that Prob(Aij \u2264 \u03b6i/m | j \u2208 Tl) \u2264 \u03b5l. Then, with probability, at least 1 \u2212 \u03b5l, A\n(2) ij > \u03b6i/m. Now, either \u03b6i < 8 ln(20/\u03b5w0) and all Bij , j \u2208 Tl are zero and then \u00b5ij = 0, or \u03b6i \u2265 8 ln(20/\u03b5w0), whence, E(Bij |j \u2208 Tl) \u2208 [(1 \u2212 \u03b5l) \u221a \u03b6 \u2032i, \u221a \u03b6 \u2032i]. So, \u00b5ij \u2265 (1 \u2212 \u03b5l) \u221a \u03b6 \u2032i and Prob(Bij = 0) \u2264 \u03b5l. So,\nVar(B2ij |j \u2208 Tl) \u2264 ( \u221a \u03b6 \u2032i\u2212(1\u2212\u03b5l) \u221a \u03b6 \u2032i) 2Prob(Bij = \u221a \u03b6 \u2032i|j \u2208 Tl)+( \u221a \u03b6 \u2032i\u22120) 2Prob(Bij = 0|j \u2208 Tl) \u2264 2\u03b5l\u03b6 \u2032i.\nThis proves the lemma in this case. The other case is symmetric.\nProof: (of Lemma A.5) Recall that Pij = \u2211 lMilWlj is the probability of word i in document j conditioned on W. Fix an i \u2208 Sl. From the dominant topic assumption,\n\u2200j \u2208 Tl, Pij = \u2211 l1 Mil1Wl1,j \u2265MilWlj \u2265Mil\u03b1. (12)\nThe Pij are themselves random variables. Note that (12) holds with probability 1. From Catchword assumption and (1), we get that\nMil\u03b1\u2212 (\u03b7i/m) \u2265Mil\u03b1\u2212Mil((\u03b1+ \u03b2 + \u03c1)/2) \u2265Mil\u03b1\u03b4/2.\nNow, we will apply H-C with \u00b5 \u2212 t = \u03b7i/m and \u00b5 \u2265 Mil\u03b1 for the m independent words in a document. By Calculus, the probability bound from H-C of exp(\u2212t2wls/2\u00b5) = exp(\u2212(\u00b5 \u2212 (\u03b7i/m))/2\u00b5) is highest subject to the constraints \u00b5 \u2265 Mil\u03b1; \u03b7i \u2264 mMil(\u03b1 + \u03b2 + \u03c1)/2, when \u00b5 = Mil\u03b1 and \u03b7i = mMil(\u03b1+ \u03b2 + \u03c1)/2, whence, we get\nProb(Aij \u2264 \u03b7i/m | j \u2208 Tl) \u2264 exp(\u2212Mil\u03b1\u03b42m/8) \u2264 \u03b5w0/20,\nusing (5). Now, we prove the second assertion of the Lemma. \u2200j \u2208 Tl\u2032 , l\u2032 6= l, \u2211 l1 Mil1Wl1,j = MilWlj + \u2211 l1 6=l Mil1Wl1,j\n\u2264MilWlj + (Maxl1 6=lMil1) (1\u2212Wlj) \u2264Mil(\u03b2 + \u03c1). (13)\n\u03b7i m \u2212Mil(\u03b2 + \u03c1) \u2265 Mil(\u03b1+ \u03b2 + \u03c1) 2 \u2212Mil(\u03b2 + \u03c1)\u2212 1 m \u2265 3Mil\u03b1\u03b4 8 ,\nusing (5) and (1). Applying the first inequality of Lemma (B.1) with \u00b5 + t = \u03b7i/m and \u00b5 \u2264 Mil(\u03b2 + \u03c1) and again using (5),\nProb(Aij \u2265 \u03b7i/m | j \u2208 Tl\u2032) \u2264 exp ( \u22129Mil\u03b1\u03b42m/64 ) \u2264 \u03b5w0/20.\nLemma B.2 For i \u2208 Sl, Prob(\u03b6i < \u03b7i) \u2264 3mke\u2212\u03b5 2sw0/8, with \u03b7i as defined in Lemma A.5.\nProof: Fix attention on i \u2208 Sl. After paying the failure probability of 3mke\u2212\u03b5 2sw0/8, assume the conclusions of Lemma (A.2) hold for all \u03b6, l. It suffices to show that\u2223\u2223\u2223{j : A(1)ij > \u03b7i/m}\u2223\u2223\u2223 \u2265 w0s2 , \u2223\u2223\u2223{j : A(1)ij = \u03b7im}\u2223\u2223\u2223 < 3w0\u03b5s, since, \u03b7i is an integer and \u03b6i is the largest integer satisfying the inequalities. The first statement follows from first assertion of Lemma A.5. The second statement is slightly more complicated. Using both the first and second assertions of Lemma A.5, we get that for all l\u2032 (including l\u2032 = l), we have\nProb(Aij = \u03b7i/m|j \u2208 T (1)l\u2032 ) \u2264 \u03b5w0/20.\n\u2223\u2223\u2223{j \u2208 T (1)l\u2032 : Aij = \u03b7i/m}\u2223\u2223\u2223 \u2264 \u03b5w0wl\u2032s/20 + \u03b52\u221aw0/wl\u2032\u221a\u03b5w0/20wl\u2032s+ \u03b52w0wl\u20322 \u2264 \u03b5w0s\n8 (wl\u2032 +\n\u221a \u03b5wl\u2032) + \u03b52w0s\n2 . Now, adding over all l\u2032 and using \u2211 l\u2032 \u221a wl\u2032 \u2264 \u221a k \u221a\u2211\nl\u2032 wl\u2032 = \u221a k, we get\u2223\u2223\u2223{j : A(1)ij = \u03b7i/m}\u2223\u2223\u2223 \u2264 \u03b5wos,\nsince \u03b5 \u2264 1/k.\nLemma B.3 Define Il = {i \u2208 Sl : \u03b6i \u2265 \u03b7i}. With probability at least 1 \u2212 8mdk exp(\u2212\u03b52w0s/8), we have for all l, \u2211\ni\u2208Il\n\u03b6 \u2032i \u2265 m\u03b1p0/2.\nProof: After paying the failure probability, we assume the conclusion of Lemma A.2 holds for all i, \u03b6, l. Now, by Lemma B.2, we have (with 1 denoting the indicator function)\nE (\u2211 i\u2208Sl Mil1(\u03b6i < \u03b7i) ) \u2264 3mk exp(\u2212\u03b52sw0/8) \u2211 i\u2208Sl Mil,\nwhich using Markov inequality implies that with probability at least 1\u2212 6mk exp(\u2212\u03b52sw0/8),\u2211 i\u2208Il Mil \u2265 1 2 \u2211 i\u2208Sl Mil \u2265 p0/2, (14)\nusing (4). Note that by (5), no catchword has \u03b6 \u2032i set to zero. So,\u2211 i\u2208Il \u03b6 \u2032i = \u2211 i\u2208Il \u03b6i \u2265 \u2211 i\u2208Il \u03b7i \u2265 \u2211 Il mMil\u03b1/2 \u2265 \u03b1p0m/2.\nProof: (of Lemma A.6) For this proof, i will denote an element of Il. By Lemma A.5,\n\u2200i \u2208 Il, l\u2032 6= l,Prob(Aij > \u03b6i m |j \u2208 T (1)l\u2032 ) \u2264 Prob(Aij > \u03b7i/m|j \u2208 T (1) l\u2032 ) \u2264 \u03b5w0 20 . (15)\nThis implies by Lemma A.2, for l\u2032 6= l,\u2223\u2223\u2223\u2223{j \u2208 T (1)l\u2032 : Aij > \u03b6im} \u2223\u2223\u2223\u2223 \u2264 wl\u2032s(\u03b5w020 + \u03b5\u221aw0/wl\u2032\u221a\u03b5w0/4)+ w0\u03b52s/2. (16)\nSumming over all l\u2032 6= l, we get (using \u2211 l\u2032 \u221a wl\u2032 \u2264 \u221a\u2211 wl\u2032 \u221a k \u2264 1/\n\u221a \u03b5 by (9))\u2211\nl\u2032 6=l\n\u2223\u2223\u2223\u2223{j \u2208 T (1)l\u2032 : Aij > \u03b6im} \u2223\u2223\u2223\u2223 \u2264 \u03b5w0s.\nNow the definition of \u03b6i in the algorithm implies that:\u2211 \u03b6>\u03b6i qi(\u03b6, l) = \u2223\u2223\u2223\u2223{j \u2208 T (1)l : Aij > \u03b6im} \u2223\u2223\u2223\u2223 \u2265 (w02 \u2212 \u03b5w0) s \u2265 w0s/4.\nSo, by Lemma A.2, Prob(j \u2208 Tl;Aij > \u03b6i/m) = \u2211 \u03b6>\u03b6i pi(\u03b6, l) \u2265 1 2 \u2211 \u03b6>\u03b6i qi(\u03b6, l)\u2212 \u03b52w0m\n\u2265 w0 8 \u2212 \u03b52w0m \u2265 w0/9,\nusing (9). Next let p\u0303 = Prob(Aij = \u03b6i/m; j \u2208 Tl). Since |{j \u2208 T (1)l : Aij = \u03b6i/m}| \u2264 3\u03b5w0s, by the definition of \u03b6i in the algorithm, we get by a similar argument\np\u0303 \u2264 2qi(\u03b6i, l) + 2\u03b52w0 \u2264 7\u03b5w0. (17)\nNow, by Lemma A.1, we have p\u0303 \u2265 Min ( w0 9m , 1 m Prob(Aij \u2264 \u03b6i/m; j \u2208 T (2)l ) ) .\nBy (9), 7\u03b5w0 < w0/9m and so we get:\nProb(Aij \u2264 \u03b6i/m; j \u2208 T (2)l ) \u2264 7\u03b5mw0.\nNoting that by (5), no catchword has \u03b6 \u2032i set to zero, Prob(Bij = 0|j \u2208 T (2) l ) \u2264 7\u03b5mw0/wl \u2264 1/6, by (9). This implies\n\u00b5ij \u2265 5\n6\n\u221a \u03b6 \u2032i.\nNow, by (15), we have for j\u2032 /\u2208 Tl, \u00b5ij\u2032 \u2264 \u221a \u03b6 \u2032i/6.\nSo, we have |\u00b5\u00b7,j \u2212 \u00b5\u00b7,j\u2032 |2 \u2265 \u2211 i\u2208Il (\u00b5ij \u2212 \u00b5ij\u2032)2 \u2265 (4/9) \u2211 i\u2208Il \u03b6 \u2032i.\nNow Lemma (B.3) implies the current Lemma."}, {"heading": "B.1 Bounding the Spectral norm", "text": "Theorem B.4 Fix an l. For j \u2208 Tl, let R.,j = B.,j \u2212 \u00b5.,j . [The R.,j , j \u2208 Tl are vector-valued random variables which are independent, even conditioned on the partition T1, T2, . . . , Tk.] With probability at least 1\u2212 10mdk exp(\u2212\u03b52w0s/8), we have ||R||2 \u2264 ckw0\u03b5sm2. Thus,\n||B\u2212 \u00b5||2 \u2264 c\u03b5w0sm2k2.\nWe will apply Random Matrix Theory, in particular the following theorem, to prove Theorem B.4.\nTheorem B.5 [15, Theorem 5.44] Suppose R is a d \u00d7 r matrix with columns R\u00b7,j which are independent identical vector-valued random variables. Let U = E(R\u00b7,jRT\u00b7,j) be the inertial matrix of R\u00b7,j . Suppose |R\u00b7,j | \u2264 \u03bd always. Then, for any t > 0, with probability at least 1\u2212de\u2212ct 2 , we have4\n||R|| \u2264 ||U ||1/2 \u221a r + t\u03bd.\nWe need the following Lemma first.\nLemma B.6 With probability at least 1\u2212 exp(\u2212s\u03b5w0/3), we have \u03b60 \u2264 4m\u03bb ; \u2211 i \u03b6 \u2032i \u2264 4km (18)\nProof: The probability of word i in document j, is given by: Pij = \u2211 lMilWlj \u2264 \u03bbi (where, \u03bbi = maxlMil). If \u03bbi < 1m ln(20/\u03b5w0), then, Prob(Aij > (8/m) ln(20/\u03b5w0)) \u2264 \u03b5w0 by H-C (since Aij is the average of m i.i.d. trials). Let Xj be the indicator function of Aij > (8/m) ln(20/\u03b5w0). Xj are independent and so using H-C, we see that with probability at least 1\u2212 exp(\u2212\u03b5w0s/3), less than w0s/2 of the Aij are greater (8/m) ln(20/\u03b5w0), whence, \u03b6 \u2032i = 0. So we have (using the union bound over all words):\nProb  \u2211 i:\u03bbi<(1/m) ln(20/\u03b5w0) \u03b6 \u2032i > 0  \u2264 d exp(\u2212\u03b5w0s/3). If \u03bbi \u2265 (1/m) ln(20/\u03b5w0), then\nProb(Aij > 4\u03bbi) \u2264 e\u2212\u03bbim \u2264 \u03b5w0/2, 4||R|| denotes the spectral norm of R.\nwhich implies by the same Xj kind of argument that with probability at least 1 \u2212 exp(\u2212\u03b5w0s/4), for a fixed i, \u03b6i \u2264 4\u03bbim. Using the union bound over all words and adding all i, we get that with probability at least 1\u2212 2d exp(\u2212\u03b5w0s/4),\u2211\ni \u03b6 \u2032i \u2264 4m \u2211 i \u03bbi \u2264 4m \u2211 i,l Mil \u2264 4km.\nNow we prove the bound on \u03b60. For each fixed i, j, we have Prob(Aij \u2265 4\u03bb) \u2264 e\u2212m\u03bb \u2264 \u03b5w0. Now, let Yj be the indicator variable of Aij \u2265 4\u03bb. The Yj , j = 1, 2, . . . , s are independent (for each fixed i). So, Prob(\u03b6i \u2265 4m\u03bb) \u2264 Prob( \u2211 j Yj \u2265 w0s/2) \u2264 e\u2212\u03b5wos/3. Using an union bound over all words, we get that Prob(\u03b60 > 4m\u03bb) \u2264 de\u2212\u03b5w0/3 by H-C.\nProof: (of Theorem B.4) First, ||U || = Max|v|=1E(vTR\u00b7,j)2 \u2264 E(|R\u00b7,j |2) \u2264 2\u03b5l \u2211 i \u03b6 \u2032i \u2264 8\u03b5lkm, by Lemma (B.6) and Lemma (A.4). We can also take \u03bd = 2 \u221a km in Theorem B.5 and with t =\u221a\n\u03b5mw0s, the first statement of the current theorem follows (noting r = wls). The second statement follows by just paying a factor of k for the k topics."}, {"heading": "B.2 Proving Proximity", "text": "From Theorem (B.4), the \u03c3 in definition A.1.2 is \u221a c\u03b5w0m2k2. So, the \u2206 in definition A.1.2 is\ncc0 \u221a \u03b5k2m. So it suffices to prove:\nLemma B.7 For j \u2208 Tl and j\u2032 \u2208 Tl\u2032 , l\u2032 6= l, let B\u0302.,j be the projection of B.,j onto the line joining \u00b5.,j and \u00b5.,j\u2032 . The probability that |B\u0302.,j \u2212 \u00b5.,j\u2032 | \u2264 |B\u0302.,j \u2212 \u00b5.,j | + cc0k2 \u221a \u03b5m is at most c\u03b5mw0 \u221a k/ \u221a \u03b1p0. Hence, with probability at least 1 \u2212 cmdk exp(\u2212cw0\u03b52s), the number of j for which B.,j does not satisfy the proximity condition is at most c\u03b50w0\u03b4s/10c1.\nProof: After paying the failure probability of cmdk exp(\u2212w0s\u03b52/8), of Lemmas (B.6) and (A.6), assume that \u03b60 \u2264 4m\u03bb , |\u00b5.,j \u2212 \u00b5.,j\u2032 |2 \u2265 2\u03b1mp0/9 and \u2211 i \u03b6 \u2032 i \u2264 4km.\nLet X = (B\u00b7,j \u2212 \u00b5\u00b7,j) \u00b7 (\u00b5\u00b7,j\u2032 \u2212 \u00b5\u00b7,j). X is a random variable, whose expectation is 0 conditioned on j \u2208 T (2)l .\nSince Prob(Bij = \u221a \u03b6 \u2032i|j \u2208 Tl) = \u00b5ij/ \u221a \u03b6 \u2032i, we have:\nE|X| \u2264 E \u2211 i |Bij \u2212 \u00b5ij | |\u00b5ij\u2032 \u2212 \u00b5ij |\n= \u2211 i [ ( \u221a \u03b6 \u2032i \u2212 \u00b5ij) \u00b5ij\u221a \u03b6 \u2032i + (1\u2212 \u00b5ij\u221a \u03b6 \u2032i )\u00b5ij ] |\u00b5ij \u2212 \u00b5ij\u2032 |\n\u2264 2\u03b5l \u2211 i \u221a \u03b6 \u2032i|\u00b5ij \u2212 \u00b5ij\u2032 | by Lemma A.4\n\u2264 2\u03b5l (\u2211 i \u03b6 \u2032i )1/2 |\u00b5.,j \u2212 \u00b5.,j\u2032 | \u2264 4\u03b5l \u221a km|\u00b5.,j \u2212 \u00b5.,j\u2032 |.\nNow apply Markov inequality to get\nProb(|X| \u2265 1 8 |\u00b5.,j \u2212 \u00b5.,j\u2032 |2) \u2264 32\u03b5l\n\u221a km/|\u00b5.,j \u2212 \u00b5.,j\u2032 | \u2264 80\u03b5l \u221a k/\u03b1p0.\nIf |X| \u2264 |\u00b5.,j \u2212 \u00b5.,j\u2032 |2/8, then, |B\u0302.,j \u2212 \u00b5.,j\u2032 | \u2265 |B\u0302.,j \u2212 \u00b5.,j |+ 3|\u00b5.,j \u2212 \u00b5.,j\u2032 |/4 \u2265 |B\u0302.,j \u2212 \u00b5.,j |+ cc0k 2 \u221a \u03b5m, by (9). This proves the first assertion of the Lemma.\nThe second statement of Lemma follows by applying H-C to the random variable \u2211 j Zj/s, where, Zj is the indicator random variable of B.,j not satisfying the proximity condition (and using (9).)\nThe last Lemma implies that the algorithm TSVD correctly identifies the dominant topic in all but at most \u03b50w0/10 fraction of the documents by Theorem (A.7).\nLemma B.8 With probability at least 1\u2212 exp(\u2212w0s\u03b52/8), TSVD correctly identifies the dominant topic in all but at most \u03b50w0\u03b4/10 fraction of documents in each Tl.\nB.3 Identifying Catchwords\nRecall the definition of Jl from Step 5a of the algorithm. The two lemmas below are roughly converses of each other which prove roughly that Jl consists of those i for which Mil is strictly higher than Mil\u2032 . Using them, Lemma B.11 says that almost all the \u03b50w0s/2 documents found in Step 6 of the algorithm are 1\u2212 \u03b4 pure for topic l.\nLemma B.9 Let \u03bd = \u03b3(1 \u2212 2\u03b4)/(1 + \u03b4). If i \u2208 Jl, then for all l\u2032 6= l, Mil \u2265 \u03bdMil\u2032 and Mil \u2265 3 m\u03b42 ln(20/\u03b5w0).\nProof: It is easy to check that the assumptions (2) and (1)imply \u03bd \u2265 2. Let i \u2208 Jl. By the definition of Jl in the algorithm, g(i, l) \u2265 (4/m\u03b42) ln(20/\u03b5w0). Note that Pij \u2264 Maxl1Mil1 for all j. So,\nmax l1\nMil1 \u2265 3\nm\u03b42 ln(20/\u03b5w0). (19)\nIf the Lemma is false, then, for l\u2032 attaining Maxl1 6=lMil1 , we have Mil < \u03bdMil\u2032 . Recall Rl\u2032 defined in Step 4c of the algorithm. Let\nT\u0302l\u2032 = Rl\u2032 \u2229 ( the set of 1\u2212 \u03b4 pure documents in Tl\u2032).\nSince all but \u03b50w0s/10 documents in Tl\u2032 belong to Rl\u2032 , we have |T\u0302l\u2032 | \u2265 0.9\u03b50w0s. For j \u2208 T\u0302l\u2032 , Pij \u2265 Mil\u2032Wl\u2032j \u2265 (1 \u2212 \u03b4)Mil\u2032 . So, Prob(Aij < Mil\u2032(1 \u2212 2\u03b4)) \u2264 exp(\u2212m\u03b42Mil\u2032/3) \u2264 \u03b5w0/4 using (19). Thus the number of documents inRl\u2032 for whichAij \u2265Mil\u2032(1\u22122\u03b4) is at least 0.9\u03b50w0s\u2212 3\u03b5w0s \u2265 .6\u03b50w0s. This implies that with probability at least 1\u2212exp(\u2212c\u03b52sw0), g(i, l\u2032) \u2265Mil\u2032(1\u2212 2\u03b4).\nNow, for j \u2208 Tl, Pij \u2264 Max(Mil,Mil\u2032) \u2264 \u03bdMil\u2032 . So, Prob(Aij > Mil\u2032\u03bd(1 + \u03b4)) \u2264 \u03b5w0/4, again using (19). At most \u03b50w0s/10 documents of other Tl1 , l1 6= l are in Rl (by Lemma B.8). So, whp, g(i, l) \u2264Mil\u2032\u03bd(1 + \u03b4) and so we have\ng(i, l) \u2264 \u03bd(1 + \u03b4) 1\u2212 2\u03b4 g(i, l\u2032),\ncontradicting the definition of Jl. So, we must have that Mil \u2265 \u03bdMil\u2032 for all l\u2032 6= l. The second assertion of the Lemma now follows from (19).\nLemma B.10 If Mil \u2265 Max ( 5 m\u03b42 ln(20/\u03b5w0),Maxl\u2032 6=l 1 \u03c1 Mil\u2032 ) , then, with probability at least 1\u2212 exp(\u2212c\u03b52w0s), we have that i \u2208 Jl. So, Sl \u2286 Jl.\nProof: Let T\u0302l = Rl\u2229 (set of 1 \u2212 \u03b4 pure documents in Tl). For j \u2208 T\u0302l, Pij \u2265 Mil(1 \u2212 \u03b4) which implies that whp, (since |T\u0302l| \u2265 0.9\u03b50s, again by Lemma B.8)\ng(i, l) \u2265Mil(1\u2212 2\u03b4) (20)\nOn the other hand, for j \u2208 Tl\u2032 and for l\u2032 6= l, i : Mil\u2032 \u2264 \u03c1Mil (hypothesis of the Lemma), Pij \u2264MilWlj + \u03c1Mil(1\u2212Wlj) \u2264Mil(\u03b2 + \u03c1). So whp,\ng(i, l\u2032) \u2264Mil(\u03b2 + \u03c1)(1 + \u03b4). (21)\nFrom (20) and (21) and hypothesis of the Lemma, it follows that g(i, l) \u2265 Max ( 4\nm\u03b42 ln(1/\u03b5w0), (1\u2212 2\u03b4) (1 + \u03b4)(\u03b2 + \u03c1) g(i, l\u2032)\n) .\nSo, i \u2208 Jl as claimed. It only remains to check that i in Sl satisfies the hypothesis of the Lemma which is obvious.\nLemma B.11 Let \u03bdl = \u2211 i\u2208Jl Mil and let L be the set of b(s\u03b50w0/2)c A.,j \u2019s whose average is returned in Step 6 of the TSVD Algorithm as M\u0302.,l. With probability at least 1\u2212 c exp(\u2212c\u03b52w0s), we have: \u2223\u2223\u2223\u2223\u2223\u2223 1|L| \u2211 j\u2208L (A.,j \u2212M.,l) \u2223\u2223\u2223\u2223\u2223\u2223 1 \u2264 O(\u03b4). (22)\nProof: The proof needs care since Jl is itself a random set dependent on A(2). To understand the proof intuitively, if we pretend that there is no conditioning of Jl on A(2), then, basically, our arguments in Lemma B.9 would yield this Lemma. However, we have to work harder to avoid conditioning effects. Define\nKl = {i : Mil \u2265 \u03bdMil\u2032\u2200l\u2032 6= l;Mil \u2265 (3/m\u03b42) ln(20/\u03b5w0)}.\nNote that Kl is not a random set; it does not depend on A, just on M which is fixed. Lemma B.9 proved that Jl \u2286 Kl. Since \u2211 iMil = 1, we have |Kl| \u2264 m\u03b42/3. The probability bounds given here will be after conditioning on W. [In other words, we prove statements of the form Prob(E|W) \u2264 a which is (the usual) shorthand for: for each possible value w of the matrix W , Prob(E |W = w) \u2264 a.] This will be possible, since, even after fixing W , the A.,j are independent, though certainly not identically distributed now, since the W.,j may differ.\nFor i \u2208 Kl, we have for all j, Pij = \u2211 l\u2032Mil\u2032Wl\u2032j \u2264Mil, since, Mil\u2032 \u2264Mil/\u03bd \u2264Mil/2 for l\u2032 6= l. For any x \u2264Mil,\nProb(|A(2)ij \u2212 Pij | \u2265 \u03b4Mil |W,Pij = x) \u2264 2 exp ( \u2212 \u03b4 2M2ilm\n2(1 + \u03b4)x\n) \u2264 2 exp ( \u2212m\u03b4\n2Mil 3\n) .\nNoting that m\u03b42Mil \u2265 3 ln(20/\u03b5w0) for i \u2208 Kl, we get\nProb(|A(2)ij \u2212 Pij | \u2265 \u03b4Mil |W ) \u2264 \u03b5w0/20.\nUsing the union bound over all i \u2208 Kl yields (for each j \u2208 [s]),\nProb(\u2203i \u2208 Kl : |A(2)ij \u2212 Pij | \u2265 \u03b4Mil |W ) \u2264 m\u03b42\u03b5w0 20 \u2264 \u03b50w0\u03b4 2 20 ,\nby (9). Let BAD = {j : \u2203i \u2208 Kl : |A(2)ij \u2212 Pij | \u2265 \u03b4Mil}.\nUsing the independence of A.,j , (even conditioned on W ), apply H-C to get that for the event\nE : |BAD| \u2265 s\u03b50w0\u03b4 10\nProb(E |W ) \u2264 2 exp(\u2212c\u03b5w0s). (23)\nAfter paying the failure probability, for the rest of the proof, assume that \u00acE holds. Let Ul = {j : Wlj \u2265 1\u2212 \u03b4}. By the dominant topic assumption, we know that |Ul| \u2265 \u03b50w0s. So, |Ul \\BAD| \u2265 4\u03b50w0s/5 and we get (using (9)):\n\u2200Nl \u2286 Kl, \u2223\u2223\u2223\u2223\u2223{j : Wlj \u2265 1\u2212 \u03b4 ; \u2211 i\u2208Nl A (2) ij \u2265 (1\u2212 2\u03b4) \u2211 i\u2208Nl Mil} \u2223\u2223\u2223\u2223\u2223 \u2265 4\u03b50w0s/5. (24) Now consider j : Wlj \u2264 (1\u2212 6\u03b4) and i \u2208 Kl.\nPij \u2264MilWlj + \u2211 l\u2032 6=l Mil\u2032Wl\u2032j \u2264Mil(1\u2212 6\u03b4) + Mil \u03bd 6\u03b4 \u2264Mil(1\u2212 3\u03b4),\nsince by (2) and (1), we have that \u03bd \u2265 2. So, for a j with Wlj \u2264 1 \u2212 6\u03b4 to have \u2211 i\u2208Jl A (2) ij \u2265 (1\u2212 2\u03b4)\u03bdl, j must be in BAD. This gives us\n\u2200Nl \u2286 Kl, \u2223\u2223\u2223\u2223\u2223{j : Wlj \u2264 (1\u2212 6\u03b4) ; \u2211 i\u2208Nl A (2) ij \u2265 (1\u2212 2\u03b4) \u2211 i\u2208Nl Mil} \u2223\u2223\u2223\u2223\u2223 \u2264 \u03b50w0\u03b4s/10. (25)\nLet L be the set of b\u03b50w0s/2c j achieving the highest \u2211 i\u2208Jl A (2) ij . By the above, L contains at most \u03b50\u03b4s/5 j\u2019s with Wlj < 1 \u2212 6\u03b4, the rest being j with Wlj \u2265 1 \u2212 6\u03b4. So are we finished with the proof - i.e., does this prove (22)? The answer is unfortunately, no. We can show from the above that \u2211 i\u2208Jl |Aij \u2212Mil| \u2264 O(\u03b4) for most j \u2208 L and so the average of A.,j , j \u2208 L is close to M.,l when we restrict only to i \u2208 Jl. But, on words not in Jl, we have not proved that the average of A\n(2) ij , j \u2208 L is close to M.,l. We will do so presently, but first note that this is not a trivial task. For example, if say, Mil = \u2126(1/d) for all i /\u2208 Kl (or for a fraction of them) so that \u2211 i/\u2208Kl Mil \u2208 \u2126(1), then an individual A.,j could have O(m) of the Aij , i /\u2208 Kl set to 1/m. [One copy of each of O(m) words picked to be in the document.] But then we would have |A.,j \u2212M.,l|1 \u2208 \u2126(1) which is too much error. We will show that since we are taking the average over L and not just a single document, this will not happen. But the proof is again tricky because of conditioning: both Jl and L depend on the data. So, to argue that the average over L behaves well, we have to prove it for each possible L. There are at most ( s b(\u03b50w0s/2)c ) \u2264 (2/\u03b50w0s)\u03b50w0s/2 possible L \u2019s and we will be able to take the union bound over all of them.\nClaim B.1 With probability at least 1 \u2212 cmdk exp(\u2212c\u03b52w0s), we have for each L \u2286 [s] with |L| = b(\u03b50w0s/2)c: \u2223\u2223\u2223\u2223\u2223\u2223 1|L| \u2211 j\u2208L (A\u00b7,j \u2212 P\u00b7,j) \u2223\u2223\u2223\u2223\u2223\u2223 1 \u2264 O(\u03b4).\nProof: LetX = \u2223\u2223\u2223 1|L|\u2211j\u2208L(A\u00b7,j \u2212 P\u00b7,j)\u2223\u2223\u2223\n1 . EachA\u00b7,j is itself the average ofm independent choices\nof words. So\nX = \u2223\u2223\u2223\u2223\u2223\u2223 1m|L| \u2211 j\u2208L m\u2211 r=1 (A (r) \u00b7,j \u2212 P\u00b7,j) \u2223\u2223\u2223\u2223\u2223\u2223 1 .\nSo, X is a function of m|L| independent random variables. Changing any one of these arbitrarily changes X by at most 1/m|L|. Recall the Bounded Difference inequality [14]:\nLemma B.12 Let z1, . . . , zn, z\u2032i are (n + 1) independent random variables each taking values in Z and h be a measurable function from Zn to R with constants ri \u2265 0, i \u2208 [n] such that\nmaxz1,...,zn,z\u2032i\u2208Z |h(z1, . . . , zn)\u2212 h(z1, . . . , z \u2032 i, . . . , zn)| \u2264 ri If E(h) is the expectation of h then Prob (|h\u2212 E(h)| \u2265 t) \u2264 2 exp ( \u2212 t\n2\u2211n i=1 r 2 i\n) .\nUsing this we get Prob(|X \u2212 EX| \u2265 c\u03b4) \u2264 exp(\u2212c\u03b42\u03b50w0sm). The \u201cextra\u201d m in the exponent helps kill the upper bound of (2/\u03b50w0s)\u03b50w0s/2 on the number of L \u2019s and gives us |X \u2212 EX| \u2264 O(\u03b4)\u2200L. We still have to bound EX . By Jenson\u2019s inequality,\nEX \u2264 1 |L| \u2211 i\nE (\u2211\nj\u2208L (Aij \u2212 Pij)) 2 1/2 \u2264 1 |L| \u2211 i \u221a\u2211 j\u2208Ll Pij \u2264 \u221a d/ \u221a |L|,\nwhere, we have used the independence of A\u00b7,j and the fact that E(Aij \u2212 Pij)2 = Var(Aij). This proves the claim. We now bound \u2223\u2223\u2223 1|L|\u2211j\u2208L(P.,j \u2212M.,l)\u2223\u2223\u2223 1 . Note that by (24) and (25), all but at most \u03b50w0\u03b4s/10 of the j \u2019s in L have Wlj \u2265 1 \u2212 6\u03b4, whence, we get |P.,j \u2212M,l|1 \u2264 6\u03b4 for these j. For the j with Wlj < 1\u2212 6\u03b4, we just use |P.,j \u2212M.,l|1 \u2264 2. So\u2223\u2223\u2223\u2223\u2223\u2223 1|L| \u2211 j\u2208L (P.,j \u2212M.,l) \u2223\u2223\u2223\u2223\u2223\u2223 1 \u2264 6\u03b4 + 0.2\u03b50w0\u03b4s 10|Ll| \u2208 O(\u03b4).\nThis finishes the proof of (22)."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents<lb>are drawn from admixtures of distributions over words, known as topics. The<lb>inference problem of recovering topics from such a collection of documents drawn<lb>from admixtures, is NP-hard. Making a strong assumption called separability, [4]<lb>gave the first provable algorithm for inference. For the widely used LDA model,<lb>[6] gave a provable algorithm using clever tensor-methods. But [4, 6] do not learn<lb>topic vectors with bounded l1 error (a natural measure for probability vectors).<lb>Our aim is to develop a model which makes intuitive and empirically supported<lb>assumptions and to design an algorithm with natural, simple components such as<lb>SVD, which provably solves the inference problem for the model with bounded l1<lb>error. A topic in LDA and other models is essentially characterized by a group of<lb>co-occurring words. Motivated by this, we introduce topic specific Catchwords,<lb>a group of words which occur with strictly greater frequency in a topic than any<lb>other topic individually and are required to have high frequency together rather<lb>than individually. A major contribution of the paper is to show that under this<lb>more realistic assumption, which is empirically verified on real corpora, a singu-<lb>lar value decomposition (SVD) based algorithm with a crucial pre-processing step<lb>of thresholding, can provably recover the topics from a collection of documents<lb>drawn from Dominant admixtures. Dominant admixtures are convex combination<lb>of distributions in which one distribution has a significantly higher contribution<lb>than the others. Apart from the simplicity of the algorithm, the sample complexity<lb>has near optimal dependence on w0, the lowest probability that a topic is domi-<lb>nant, and is better than [4]. Empirical evidence shows that on several real world<lb>corpora, both Catchwords and Dominant admixture assumptions hold and the pro-<lb>posed algorithm substantially outperforms the state of the art [5].", "creator": "LaTeX with hyperref package"}}}