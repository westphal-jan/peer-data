{"id": "1411.0895", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "Tied Probabilistic Linear Discriminant Analysis for Speech Recognition", "abstract": "Acoustic models using probabilistic linear discriminant analysis (PLDA) capture the correlations within feature vectors using subspaces which do not vastly expand the model. This allows high dimensional and correlated feature spaces to be used, without requiring the estimation of multiple high dimension covariance matrices. In this letter we extend the recently presented PLDA mixture model for speech recognition through a tied PLDA approach, which is better able to control the model size to avoid overfitting. We carried out experiments using the Switchboard corpus, with both mel frequency cepstral coefficient features and bottleneck feature derived from a deep neural network. Reductions in word error rate were obtained by using tied PLDA, compared with the PLDA mixture model, subspace Gaussian mixture models, and deep neural networks. The PLDA approach is still possible, but this approach should be more successful at predicting the predicted effect sizes across multiple levels of information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 4 Nov 2014 13:11:06 GMT  (12kb)", "http://arxiv.org/abs/1411.0895v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["liang lu", "steve renals"], "accepted": false, "id": "1411.0895"}, "pdf": {"name": "1411.0895.pdf", "metadata": {"source": "CRF", "title": "Tied Probabilistic Linear Discriminant Analysis for Speech Recognition", "authors": ["Liang Lu", "Steve Renals"], "emails": ["s.renals}@ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n08 95\nv1 [\ncs .C\nL ]\n4 N\nov 2\n01 4\nIndex Terms\u2014acoustic modelling, probabilistic linear discriminant analysis, parameters tying\nI. INTRODUCTION\nACOUSTIC models for speech recognition have advancedsubstantially over the past 25 years, but the front-end feature processing has been largely unchanged, based on mel frequency cepstral coefficients (MFCCs) [1] and perceptual linear prediction (PLP) features [2]. To a large degree this has been due to the use of acoustic models based on hidden Markov models (HMMs) with Gaussian mixture models (GMMs) [3]\u2013[5], which are well matched to feature representations which have decorrelated components and are relatively low-dimensional.\nDeep neeural network (DNN) acoustic models [6] address these limitations and have achieved significant reductions in word error rate (WER) across many speech recogniiton datasets [7]. Compared to the hybrid neural network / hidden Markov model (HMM) architecture studied in the early 1990s [8], [9], DNNs typically use more hidden layers and a wider output layer. Moreover, DNNs can be also used as a good feature extractor, for instance through the inference of bottleneck features which may append the features used in GMM-based speech recognition systems [10], [11]. However, in order to be compatible with GMMs using diagonal covariances, such augmented feature vectors must typically be relatively lowdimensional and decorrelated.\nWe have addressed the limitations of GMMs through an acoustic model based on probabilistic linear discriminant analysis (PLDA) [12], which can employ higher dimensional, correlated feature vectors. PLDA is a probabilistic extension\nLiang Lu, and Steve Renals are with University of Edinburgh, UK; email: {liang.lu, s.renals}@ed.ac.uk\nThe research was supported by EPSRC Programme Grant EP/I031022/1 (Natural Speech Technology).\nof linear discriminant analysis (LDA) [13], which has been very well studied for speaker recognition in the joint factor analysis (JFA) [14] and i-vector [15]\u2013[17] frameworks. A PLDA acoustic model factorizes the acoustic variability using HMM state dependent variables which are expected to be consistent across different acoustic conditions, and observation dependent variables which characterise per frame level acoustic changes [12]. Similarly to a subspace GMM (SGMM) [18], the factorisation is based on the inference of subspaces. However, while the SGMM uses a set of full covariance matrices to directly model the per frame acoustic variability, the PLDA model introduces another set of projections to model this variability in lower-dimension subspaces.\nWe have previously investigated using a PLDA mixture model for acoustic modelling [12], [19]. Though good results have been obtained, this model has a large number of HMM state dependent variables, and is thus prone to overfitting. In this letter we mitigate the problem by tying the PLDA state variables in PLDA, an approach analogous to the use of tied state vectors in SGMMs [18]."}, {"heading": "II. PLDA-BASED ACOUSTIC MODEL", "text": "The PLDA-based acoustic model is a generative model in which the distribution over acoustic feature vectors yt \u2208 Rd from the j-th HMM state at time t is expressed as:\nyt|j = Uxjt +Gzj + b+ \u01ebjt, \u01ebjt \u223c N (0,\u039b) . (1)\nzj \u2208 Rq is the state variable (equivalent to the betweenclass identity variable in JFA) shared by the whole set of acoustic frames generated by the j-th state and xjt \u2208 Rp is the frame variable (equivalent to the within-class channel variable in JFA) which explains the per-frame variability. Usually, the dimensionality of these two latent variables is smaller than that of the feature vector yt, i.e. p, q \u2264 d. U \u2208 Rd\u00d7p and G \u2208 Rd\u00d7q are two low rank matrices which span the subspaces to capture the major variations for xjt and zj respectively. They are analogous to the within-class and between-class subspaces in the standard LDA formulation, but are estimated probabilistically. b \u2208 Rd denotes the bias and \u01ebjt \u2208 Rd is the residual noise which is assumed to be Gaussian with zero mean and diagonal covariance. By marginalising out the residual noise variable \u01ebjt, we obtain the following likelihood function:\np(yt|xjt, zj , j) = N (yt;Uxjt +Gzj + b,\u039b) (2)\n2"}, {"heading": "A. PLDA Mixture Model", "text": "A single PLDA has a limited modelling capacity since it only approximates a single Gaussian distribution. An M - component PLDA mixture model [12] results in the following component distribution:\nyt|j,m = Umxjmt +Gmzjm + bm + \u01ebjmt, (3)\n\u01ebjmt \u223c N (0,\u039bm) (4)\nIf c to be the component indicator variable, then the prior (weight) of each component is P (c = m|j) = \u03c0jm. Given the latent variables xjmt and zjm, the state-level distribution over features is:\np(yt|j) = \u2211\nm\n\u03c0jmN (yt;Umx\u0304jmt +Gmz\u0304jm + bm,\u039bm) .\nx\u0304jmt and z\u0304jm are point estimates of the latent variables. Since the projection matrices Um and Gm are globally shared, a large number of components can be used to improve the model capacity, e.g. M = 400 [12]."}, {"heading": "B. Tied PLDA", "text": "To avoid overfitting in the PLDA mixture model, those components which are responsible for a small number of feature vectors may be deactivated. Alternatively, the state variables zjm may be tied across components, resulting in the following component distribution:\nyt|j,m = Umxjmt +Gmzj + bm + \u01ebjmt, (5)\n\u01ebjmt \u223c N (0,\u039bm) . (6)\nTying the state variables may over-simplify the model. In this case, a \u201cmixing-up\u201d strategy can be used, analogous to SGMM sub-state splitting [18]:\nyt|j, k,m = Umxjkmt +Gmzjk + bm + \u01ebjkmt, (7)\n\u01ebjkmt \u223c N (0,\u039bm) , (8)\nwhere k denotes the sub-state index, and zjk is the substate variable. This makes Tied PLDA model more scalable compared to PLDA mixture model as we can balance the number of the sub-state variables according to the amount of available training data. Tied PLDA is equivalent to SGMM if we remove the per-frame latent variable xjkmt and use full covariance \u039bm to model the residual noise. Given the latent variables, the state-level likelihood function can be written as\np(yt|j) = \u2211\nmk\ncjk \u00d7 \u03c0jmN (yt;Umx\u0304jmkt +Gmz\u0304jk + bm,\u039bm)\n= \u2211\nmk\nwjkmN (yt;Umx\u0304jmkt +Gmz\u0304jk + bm,\u039bm) (9)\nwhere cjk is the sub-state weight, \u03c0jm is the component weight which is shared for all the sub-state models, and wjkm = cjk \u00d7 \u03c0jm. This is different to an SGMM in which a weight projection matrix is used to derive the componentdependent weights:\npSGMM(yt|j) = \u2211\nk\ncjk \u2211\nm\n\u03c0jkmN (yt;Gmz\u0304jk,\u03a3m) (10)\n\u03c0SGMMjkm = expwTmz\u0304jk \u2211\nm\u2032 expw T m\u2032 z\u0304jk\n(11)\nwhere w denotes the weight projection matrix, and wm denotes its m-th column. We do not use softmax weight normalisation in order to simplify the model training; empirical findings (Section IV) indicates that linear normalisation works well. Tied PLDA also differs from the SGMM by using another subspace projection (matrix Um) to model feature correlations. It is more scalable to high dimensional feature inputs than the direct feature covariance modelling used in SGMMs."}, {"heading": "III. MAXIMUM LIKELIHOOD TRAINING", "text": ""}, {"heading": "A. Likelihoods", "text": "For tied PLDA, the likelihood may be computed according to equation (9) by make use of the MAP estimates of the latent variables xjkmt and zjk , referred to as the point estimate in [12]. However, this approach does not work well in practice because of the large uncertainty of the estimation of xjkmt, i.e. the large variance of its posterior distribution.\nAnother approach is to marginalise out the observation variable xjkmt, which is referred as the uncertainty estimate in [12]. Using N (0, I) as a prior, which is the same prior used in model training for consistency (cf. equation(15)), this likelihood function can be obtained as\np(yt|j) = \u2211\nmk\nwjkm\n\u222b\np(yt|xjkmt, j, k,m)P (xjkmt)dxjkmt\n= \u2211\nmk\nwjkmN ( yt;Gmz\u0304jk + bm,UmU T m + \u039bm )\nThis method is similar to the channel integration evaluation method used for JFA based speaker recognition [20], [21]. Note that the likelihood can be efficiently computed without inverting matrices UmUTm + \u039bm directly, but by using the Woodbury matrix inversion lemma as in [20], [22]:\n(UmU T m + \u039bm) \u22121\n= \u039b\u22121m \u2212 \u039b \u22121 m Um(I+U T m\u039b \u22121 m Um) \u22121UTm\u039b \u22121 m (12) = \u039b\u22121m \u2212 LL T (13)\nwhere L = \u039b\u22121m Um(I + U T m\u039b \u22121 m Um) \u22121/2. This makes it computationally feasible when yt is high dimensional.\nIt is also possible to marginalise out the state variable zjk alone or jointly with xjkmt similar to the methods used in [21]. However, we did not obtain a consistent improvement using this approach in our preliminary experiments. This may be the case because the variance of the posterior distribution of zjk is small owing to increased training data used for the posterior estimation. This model-based uncertainty approach is similar to Bayesian predictive classification (BPC) for GMM-based acoustic models [23], in contrast to feature space uncertainty approaches used for noise robust speech recognition [24]\u2013[26]."}, {"heading": "B. Model update", "text": "We used the Variational Bayesian inference to train the model where xjkmt and zjk are assumed to be conditionally independent. A joint model training algorithm could be obtained without making use of this assumption: however, it may be computationally infeasible in practice [19]. Similar to\n3 the PLDA mixture model [12], the EM auxiliary function to update Um in tied PLDA is\nQ(Um) = \u2211\njkt\n\u222b\nP (j, k,m|yt)P (xjkmt |yt, z\u0304jk, j, k,m)\n\u00d7 log p(yt|xjkmt , z\u0304jk, j, k,m)dxt\n= \u2211\njkt\n\u03b3jkmtE\n[\n\u2212 1\n2 xTjkmtU T m\u039b \u22121 m Umxjkmt\n+ xTjkmtU T m\u039b \u22121 m (yt \u2212Gmz\u0304jk \u2212 bm)\n]\n+ const\n= \u2211\njkt\n\u03b3jkmtTr\n(\n\u039b\u22121m\n( \u2212 1\n2 UmE[xjkmtx\nT jkmt]U T m\n+ (yt \u2212Gmz\u0304jm \u2212 bm)E T [xjkmt]U T m\n)\n)\n+ const\nwhere \u03b3jkmt denotes the component posterior probability as\n\u03b3jkmt = P (j, k,m|yt)\n= P (j|yt) wjkmp(yt|z\u0304jk, j, k,m) \u2211\nkm wjkmp(yt|z\u0304jk, j, k,m) . (14)\nP (j|yt) is the HMM state posterior which can be obtained using the forward-backward algorithm. E[\u00b7] is the expectation operation over the posterior distribution of xjkmt:\nP (xjkmt|yt, z\u0304jk, j, k,m)\n= p(yt|xjkmt, z\u0304jk, j, k,m)P (xjkmt) \u222b\np(yt|xjkmt, z\u0304jk, j, k,m)P (xjkmt)dxjkmt . (15)\nUsing N (0, I) as the prior distribution for xjkmt we can obtain\nP (xjkmt|yt, z\u0304jk, j, k,m) = N (xjkmt ;V \u22121 m pjkmt,V \u22121 m )\n(16)\nVm = I+U T m\u039b \u22121 m Um (17) pjkmt = U T m\u039b \u22121 m (yt \u2212Gmz\u0304jk \u2212 bm) (18)\nNote that using N (0, I) as a prior is reasonable since, after convergence, a nonzero mean can be accounted for by bm, and the variance can be modified by rotating and scaling the matrix Um. A similar form of posterior distribution can be obtained for zjk .\nBy setting \u2202Q(Um)/\u2202Um = 0 we obtain\nUm =\n\n\n\u2211\njkt\n\u03b3jkmt(yt \u2212Gmz\u0304jk \u2212 bm)E T [xjkmt]\n\n\n\u00d7\n\n\n\u2211\njkt\n\u03b3jkmtE [ xjkmtx T jkmt ]\n\n\n\u22121\n(19)\nSimilarly, the update for other parameters are as follows.\nGm =\n\n\n\u2211\njkt\n\u03b3jkmt(yt \u2212Umx\u0304jkmt \u2212 bm)E T [zjk]\n\n\n\u00d7\n\n\n\u2211\njkt\n\u03b3jkmtE [ zjkz T jk ]\n\n\n\u22121\n(20)\nbm =\n\u2211\njkt \u03b3jkmt(yt \u2212Umx\u0304jkmt \u2212Gmz\u0304jk) \u2211\njkt \u03b3jkmt (21)\n\u039bm = diag\n\n\n\u2211\njkt \u03b3jkmt\n(\nyjkmty T jkmt +UmV \u22121 m U T m\n)\n\u2211\njkt \u03b3jkmt\n\n\n(22)\nwhere we have defined\nyjkmt = yt \u2212Umx\u0304jkmt \u2212Gmz\u0304jk \u2212 bm (23)\nThe sub-state and component weights can be updated as\ncjk =\n\u2211\nmt \u03b3jkmt \u2211\nkmt \u03b3jkmt , \u03c0jm =\n\u2211\nkt \u03b3jkmt \u2211\nkmt \u03b3jkmt (24)\nWhen using a large number of components, e.g. M = 400 in this work, the weight should be floored by a small value for numerical stability. For computational efficiency, a background model based on a mixtures of factor analysers is used to select a small subset of the components for each frame for training and decoding, which is described in more detail in [12]."}, {"heading": "IV. EXPERIMENTS", "text": "We performed experiments using the Switchboard corpus1 [27]. The Hub-5 Eval 2000 data [28] is used as the test set, which contains the Switchboard (SWB) and CallHome (CHM) evaluation subsets. The experiments were performed using the Kaldi speech recognition toolkit2 [29], which we extended with an implementation of the PLDA-based acoustic model. In the following experiments, we have used maximum likelihood estimation without speaker adaptation or adaptive training. We used the pronunciation lexicon that was supplied by the Mississippi State transcriptions [30] and a trigram language model was used for decoding."}, {"heading": "A. MFCC features", "text": "The first set of experiments used mel frequency cepstral coefficients (MFCCs) as features. We used the standard 39-dimensional MFCCs with first and second derivatives (MFCC 0 \u2206 \u2206\u2206). To take advantage of longer context information, for the GMM and SGMM systems we have also performed experiments using spliced MFCC 0 of differing context window size, followed by a global LDA transformation to reduce the feature dimensionality to be 40, and a global semi-tied covariance (STC) matrix transform [31] to decorrelate the features. The PLDA systems directly used the concatenated MFCCs with various size of context window, without de-correlation and dimensionality reduction.\nTable I shows the results of using a 33 hour subset of the training data, and the number of active model parameters3. In this case, there are about 2,400 clustered triphone states in the GMM systems, corresponding to about 30,000 Gaussians. The PLDA and SGMM systems have a similar number of clustered\n1https://catalog.ldc.upenn.edu 2http://kaldi.sourceforge.net 3For PLDA systems, a component is considered active if its weight is above\na threshold (0.01 in this work).\n4\ntriphone states, and a 400-component background model is used for each. The state vector of SGMMs and latent variables of PLDA are all 40-dimensional. We used 20,000 sub-state vectors and state variables in the SGMM and tied PLDA systems, respectively. These results demonstrate the flexibility of PLDA systems in using different dimensional acoustic features, i.e. the spliced MFCC 0 without any frontend feature transformations. Tied PLDA systems also offer consistently lower WERs than their counterparts based on the PLDA mixture model. Using the same low dimensional features as MFCC 0(\u00b13)+LDA STC, the tied PLDA system achieved comparable recognition accuracy to SGMMs. This system is better than tied PLDA systems using spliced MFCC 0 of various context windows, which means that removing the nondiscriminative dimensions in feature space is still beneficial to tied PLDAs."}, {"heading": "B. Bottleneck features", "text": "Table II shows the WERs of DNN and bottleneck systems using 33 hours and 109 hours of training data, respectively. The DNN system has six hidden layers, each with 1024 hidden units when using 33 hours of training data. The number of hidden units is increased to be 1200 when the amount of training data is 109 hours.The bottleneck DNN system (BN hybrid) used the same training data and the same kind of\nfeature input \u2014 while reducing the size of the fifth hidden layer to be 26. Using a larger bottleneck layer was not found to be helpful [19]. We concatenated the bottleneck and MFCC 0+\u2206+\u2206\u2206 coefficients (referred as BN MFCC), and then used them to retrain our GMM and PLDA systems. We used LDA to reduce the dimensionality of the concatenated features from 65 to be 40 followed by STC to de-correlate the features for GMM and SGMM systems. Without the front-end feature transforms, the PLDA systems were able to achieve comparable or higher recognition accuracy by directly capturing the correlations between MFCCs and bottleneck features in subspaces. Again, the results demonstrate the flexibility of PLDA acoustic models in terms of using input feature vectors of varying dimension."}, {"heading": "V. CONCLUSIONS", "text": "Building upon our previous work on acoustic modelling using the PLDA mixture model, we have presented a tied PLDA based acoustic model, which is more scalable to the amount of training data. Experiments show that this model can achieve higher recognition accuracy while still enjoying the flexibility of using acoustic features of various dimension as the PLDA mixture model. Other types of acoustic feature representations can be more freely explored using this acoustic model. Along this line, we have demonstrated that the bottleneck feature from a DNN can used without any front-end feature transformation for dimensionality reduction and de-correlation. Future works include speaker adaptation and discriminative training for this model, and moreover, we are also interested in learning speech representations in an unsupervised fashion using a deep auto-encoder for this model. The source code and recipe used in this work are available from http://homepages.inf.ed.ac.uk/llu/code/plda-v1.tgz."}], "references": [{"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["Steven Davis", "Paul Mermelstein"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 28, no. 4, pp. 357\u2013366, 1980.  5", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1980}, {"title": "Perceptual linear predictive (plp) analysis of speech", "author": ["Hynek Hermansky"], "venue": "the Journal of the Acoustical Society of America, vol. 87, no. 4, pp. 1738\u20131752, 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "A review of large-vocabulary continuous-speech recognition", "author": ["Steve Young"], "venue": "Signal Processing Magazine, IEEE, vol. 13, no. 5, pp. 45, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Large-vocabulary continuous speech recognition: advances and applications", "author": ["J-L Gauvain", "Lori Lamel"], "venue": "Proceedings of the IEEE, vol. 88, no. 8, pp. 1181\u20131200, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "The application of hidden markov models in speech recognition", "author": ["M. Gales", "S. Young"], "venue": "Foundations and Trends in Signal Processing, vol. 1, no. 3, pp. 195\u2013304, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["GE Dahl", "D Yu", "L Deng", "A Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath", "Brain Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist speech recognition: a hybrid", "author": ["Herve A Bourlard", "Nelson Morgan"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["Steve Renals", "Nelson Morgan", "Herv\u00e9 Bourlard", "Michael Cohen", "Horacio Franco"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 2, no. 1, pp. 161\u2013174, 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Probabilistic and bottle-neck features for LVCSR of meetings", "author": ["Franti\u0161ek Gr\u00e9zl", "Martin Karafi\u00e1t", "Stanislav Kont\u00e1r", "J Cernocky"], "venue": "Proc. ICASSP. IEEE, 2007, vol. 4.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Improved bottleneck features using pretrained deep neural networks", "author": ["Dong Yu", "Michael L Seltzer"], "venue": "INTERSPEECH, 2011, pp. 237\u2013 240.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic linear discriminant analysis for acoustic modelling", "author": ["L Lu", "S Renals"], "venue": "IEEE Signal Processing Letters, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["Simon JD Prince", "James H Elder"], "venue": "Proc. ICCV. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 4, pp. 1435\u20131447, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian speaker verification with heavy tailed priors", "author": ["Patrick Kenny"], "venue": "Speaker and Language Recognition Workshop (IEEE Odyssey), 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Front-end factor analysis for speaker verification", "author": ["Najim Dehak", "Patrick J Kenny", "R\u00e9da Dehak", "Pierre Dumouchel", "Pierre Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Fullcovariance UBM and heavy-tailed PLDA in i-vector speaker verification", "author": ["Pavel Matejka", "Ondrej Glembek", "Fabio Castaldo", "Md Jahangir Alam", "Oldrich Plchot", "Patrick Kenny", "Lukas Burget", "Jan Cernocky"], "venue": "Proc. ICASSP. IEEE, 2011, pp. 4828\u20134831.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "The subspace Gaussian mixture model\u2014A structured model for speech recognition", "author": ["D Povey", "L Burget", "M Agarwal", "P Akyazi", "F Kai", "A Ghoshal", "O Glembek", "N Goel", "M Karafi\u00e1t", "A Rastrow", "RC Rose", "P Schwarz", "S Thomas"], "venue": "Computer Speech & Language, vol. 25, no. 2, pp. 404\u2013439, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic linear discriminant analysis with bottleneck features for speech recognition", "author": ["L Lu", "S Renals"], "venue": "Proc. INTERSPEECH, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison of scoring methods used in speaker recognition with joint factor analysis", "author": ["Ondrej Glembek", "Lukas Burget", "Najim Dehak", "Niko Brummer", "Patrick Kenny"], "venue": "Proc. ICASSP. IEEE, 2009, pp. 4057\u20134060.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Variational bayesian joint factor analysis models for speaker verification", "author": ["Xianyu Zhao", "Yuan Dong"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,, vol. 20, no. 3, pp. 1032\u20131042, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic models for inference about identity", "author": ["Peng Li", "Yun Fu", "Umar Mohammed", "James H Elder", "Simon JD Prince"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 1, pp. 144\u2013157, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A bayesian predictive classification approach to robust speech recognition", "author": ["Qiang Huo", "Chin-Hui Lee"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 8, no. 2, pp. 200\u2013204, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Uncertainty decoding with SPLICE for noise robust speech recognition", "author": ["J Droppo", "A Acero", "L Deng"], "venue": "Proc. ICASSP. IEEE, 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Joint uncertainty decoding for noise robust speech recognition", "author": ["H Liao", "MJF Gales"], "venue": "Proc. INTERSPEECH. Citeseer, 2005.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Joint uncertainty decoding for noise robust subspace Gaussian mixture models", "author": ["L Lu", "KK Chin", "A Ghoshal", "S Renals"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "SWITCH- BOARD: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane McDaniel"], "venue": "Proc. ICASSP. IEEE, 1992, pp. 517\u2013520.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "Research methodologies, observations and outcomes in (conversational) speech data collection", "author": ["Christopher Cieri", "David Miller", "Kevin Walker"], "venue": "Proceedings of the second international conference on Human Language Technology Research. Morgan Kaufmann Publishers Inc., 2002, pp. 206\u2013211.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "The Kaldi speech recognition toolkit", "author": ["D Povey", "A Ghoshal", "G Boulianne", "L Burget", "O Glembek", "N Goel", "M Hannemann", "P Motl\u0131cek", "Y Qian", "P Schwarz", "J Silovsk\u00fd", "G Semmer", "K Vesel\u00fd"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Resegmentation of SWITCHBOARD", "author": ["Neeraj Deshmukh", "Aravind Ganapathiraju", "Andi Gleeson", "Jonathan Hamaker", "Joseph Picone"], "venue": "Proc. ICSLP, 1998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Semi-tied covariance matrices for hidden Markov models", "author": ["MJF Gales"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 7, no. 3, pp. 272\u2013281, 1999.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION ACOUSTIC models for speech recognition have advanced substantially over the past 25 years, but the front-end feature processing has been largely unchanged, based on mel frequency cepstral coefficients (MFCCs) [1] and perceptual linear prediction (PLP) features [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 1, "context": "INTRODUCTION ACOUSTIC models for speech recognition have advanced substantially over the past 25 years, but the front-end feature processing has been largely unchanged, based on mel frequency cepstral coefficients (MFCCs) [1] and perceptual linear prediction (PLP) features [2].", "startOffset": 274, "endOffset": 277}, {"referenceID": 2, "context": "To a large degree this has been due to the use of acoustic models based on hidden Markov models (HMMs) with Gaussian mixture models (GMMs) [3]\u2013[5], which are well matched to feature representations which have decorrelated components and are relatively low-dimensional.", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "To a large degree this has been due to the use of acoustic models based on hidden Markov models (HMMs) with Gaussian mixture models (GMMs) [3]\u2013[5], which are well matched to feature representations which have decorrelated components and are relatively low-dimensional.", "startOffset": 143, "endOffset": 146}, {"referenceID": 5, "context": "Deep neeural network (DNN) acoustic models [6] address these limitations and have achieved significant reductions in word error rate (WER) across many speech recogniiton datasets [7].", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "Deep neeural network (DNN) acoustic models [6] address these limitations and have achieved significant reductions in word error rate (WER) across many speech recogniiton datasets [7].", "startOffset": 179, "endOffset": 182}, {"referenceID": 7, "context": "Compared to the hybrid neural network / hidden Markov model (HMM) architecture studied in the early 1990s [8], [9], DNNs typically use more hidden layers and a wider output layer.", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "Compared to the hybrid neural network / hidden Markov model (HMM) architecture studied in the early 1990s [8], [9], DNNs typically use more hidden layers and a wider output layer.", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "Moreover, DNNs can be also used as a good feature extractor, for instance through the inference of bottleneck features which may append the features used in GMM-based speech recognition systems [10], [11].", "startOffset": 194, "endOffset": 198}, {"referenceID": 10, "context": "Moreover, DNNs can be also used as a good feature extractor, for instance through the inference of bottleneck features which may append the features used in GMM-based speech recognition systems [10], [11].", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "We have addressed the limitations of GMMs through an acoustic model based on probabilistic linear discriminant analysis (PLDA) [12], which can employ higher dimensional, correlated feature vectors.", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "of linear discriminant analysis (LDA) [13], which has been very well studied for speaker recognition in the joint factor analysis (JFA) [14] and i-vector [15]\u2013[17] frameworks.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "of linear discriminant analysis (LDA) [13], which has been very well studied for speaker recognition in the joint factor analysis (JFA) [14] and i-vector [15]\u2013[17] frameworks.", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "of linear discriminant analysis (LDA) [13], which has been very well studied for speaker recognition in the joint factor analysis (JFA) [14] and i-vector [15]\u2013[17] frameworks.", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "of linear discriminant analysis (LDA) [13], which has been very well studied for speaker recognition in the joint factor analysis (JFA) [14] and i-vector [15]\u2013[17] frameworks.", "startOffset": 159, "endOffset": 163}, {"referenceID": 11, "context": "A PLDA acoustic model factorizes the acoustic variability using HMM state dependent variables which are expected to be consistent across different acoustic conditions, and observation dependent variables which characterise per frame level acoustic changes [12].", "startOffset": 256, "endOffset": 260}, {"referenceID": 17, "context": "Similarly to a subspace GMM (SGMM) [18], the factorisation is based on the inference of subspaces.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "We have previously investigated using a PLDA mixture model for acoustic modelling [12], [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "We have previously investigated using a PLDA mixture model for acoustic modelling [12], [19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "In this letter we mitigate the problem by tying the PLDA state variables in PLDA, an approach analogous to the use of tied state vectors in SGMMs [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "An M component PLDA mixture model [12] results in the following component distribution: yt|j,m = Umxjmt +Gmzjm + bm + \u01ebjmt, (3) \u01ebjmt \u223c N (0,\u039bm) (4) If c to be the component indicator variable, then the prior (weight) of each component is P (c = m|j) = \u03c0jm.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "M = 400 [12].", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "In this case, a \u201cmixing-up\u201d strategy can be used, analogous to SGMM sub-state splitting [18]: yt|j, k,m = Umxjkmt +Gmzjk + bm + \u01ebjkmt, (7) \u01ebjkmt \u223c N (0,\u039bm) , (8) where k denotes the sub-state index, and zjk is the substate variable.", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "Likelihoods For tied PLDA, the likelihood may be computed according to equation (9) by make use of the MAP estimates of the latent variables xjkmt and zjk , referred to as the point estimate in [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 11, "context": "Another approach is to marginalise out the observation variable xjkmt, which is referred as the uncertainty estimate in [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "This method is similar to the channel integration evaluation method used for JFA based speaker recognition [20], [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "This method is similar to the channel integration evaluation method used for JFA based speaker recognition [20], [21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "Note that the likelihood can be efficiently computed without inverting matrices UmUm + \u039bm directly, but by using the Woodbury matrix inversion lemma as in [20], [22]:", "startOffset": 155, "endOffset": 159}, {"referenceID": 21, "context": "Note that the likelihood can be efficiently computed without inverting matrices UmUm + \u039bm directly, but by using the Woodbury matrix inversion lemma as in [20], [22]:", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "It is also possible to marginalise out the state variable zjk alone or jointly with xjkmt similar to the methods used in [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 22, "context": "This model-based uncertainty approach is similar to Bayesian predictive classification (BPC) for GMM-based acoustic models [23], in contrast to feature space uncertainty approaches used for noise robust speech recognition [24]\u2013[26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 23, "context": "This model-based uncertainty approach is similar to Bayesian predictive classification (BPC) for GMM-based acoustic models [23], in contrast to feature space uncertainty approaches used for noise robust speech recognition [24]\u2013[26].", "startOffset": 222, "endOffset": 226}, {"referenceID": 25, "context": "This model-based uncertainty approach is similar to Bayesian predictive classification (BPC) for GMM-based acoustic models [23], in contrast to feature space uncertainty approaches used for noise robust speech recognition [24]\u2013[26].", "startOffset": 227, "endOffset": 231}, {"referenceID": 18, "context": "A joint model training algorithm could be obtained without making use of this assumption: however, it may be computationally infeasible in practice [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 11, "context": "the PLDA mixture model [12], the EM auxiliary function to update Um in tied PLDA is", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "For computational efficiency, a background model based on a mixtures of factor analysers is used to select a small subset of the components for each frame for training and decoding, which is described in more detail in [12].", "startOffset": 219, "endOffset": 223}, {"referenceID": 26, "context": "EXPERIMENTS We performed experiments using the Switchboard corpus1 [27].", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "The Hub-5 Eval 2000 data [28] is used as the test set, which contains the Switchboard (SWB) and CallHome (CHM) evaluation subsets.", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "The experiments were performed using the Kaldi speech recognition toolkit2 [29], which we extended with an implementation of the PLDA-based acoustic model.", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "We used the pronunciation lexicon that was supplied by the Mississippi State transcriptions [30] and a trigram language model was used for decoding.", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "To take advantage of longer context information, for the GMM and SGMM systems we have also performed experiments using spliced MFCC 0 of differing context window size, followed by a global LDA transformation to reduce the feature dimensionality to be 40, and a global semi-tied covariance (STC) matrix transform [31] to decorrelate the features.", "startOffset": 312, "endOffset": 316}, {"referenceID": 18, "context": "Using a larger bottleneck layer was not found to be helpful [19].", "startOffset": 60, "endOffset": 64}], "year": 2014, "abstractText": "Acoustic models using probabilistic linear discriminant analysis (PLDA) capture the correlations within feature vectors using subspaces which do not vastly expand the model. This allows high dimensional and correlated feature spaces to be used, without requiring the estimation of multiple high dimension covariance matrices. In this letter we extend the recently presented PLDA mixture model for speech recognition through a tied PLDA approach, which is better able to control the model size to avoid overfitting. We carried out experiments uisng the Switchboard corpus, with both mel frequency cepstral coefficient features and bottleneck feature derived from a deep neural network. Reductions in word error rate were obtained by using tied PLDA, compared with the PLDA mixture model, subspace Gaussian mixture models, and deep neural networks.", "creator": "LaTeX with hyperref package"}}}