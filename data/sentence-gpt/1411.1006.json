{"id": "1411.1006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2014", "title": "A Probabilistic Translation Method for Dictionary-based Cross-lingual Information Retrieval in Agglutinative Languages", "abstract": "Translation ambiguity, out of vocabulary words and missing some translations in bilingual dictionaries make dictionary-based Cross-language Information Retrieval (CLIR) a challenging task. Moreover, in agglutinative languages which do not have reliable stemmers, missing various lexical formations in bilingual dictionaries degrades CLIR performance. This paper aims to introduce a probabilistic translation model to solve the ambiguity problem, and also to provide most likely formations of a dictionary candidate. We propose Minimum Edit Support Candidates (MESC) method that exploits a monolingual corpus and a bilingual dictionary to translate users' native language queries to documents' language. Our experiments show that the proposed method outperforms state-of-the-art dictionary-based English-Persian CLIR. To perform this task, we present the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 4 Nov 2014 19:15:59 GMT  (32kb,D)", "https://arxiv.org/abs/1411.1006v1", null], ["v2", "Wed, 5 Nov 2014 06:55:11 GMT  (33kb,D)", "http://arxiv.org/abs/1411.1006v2", "The 3rd conference of Computational Linguistic, Sharif University of Technology, November 2014"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["javid dadashkarimi", "azadeh shakery", "heshaam faili"], "accepted": false, "id": "1411.1006"}, "pdf": {"name": "1411.1006.pdf", "metadata": {"source": "CRF", "title": "A Probabilistic Translation Method for Dictionary-based Cross-lingual Information Retrieval in Agglutinative Languages", "authors": ["Javid Dadashkarimi", "Azadeh Shakery", "Heshaam Faili"], "emails": ["dadashkarimi@ut.ac.ir", "shakery@ut.ac.ir", "hfaili@ut.ac.ir"], "sections": [{"heading": null, "text": "Keywords: dictionary-based CLIR, out of vocabulary, support candidate, minimum edit distance, ambiguity"}, {"heading": "1 Introduction", "text": "Languages are shared progressively in the World Wide Web. As a result, there is relatively remarkable research on Cross-language Information Retrieval (CLIR) to extract the information in such a large multilingual data. CLIR tasks mostly focus on retrieving documents in a language different from users\u2019 native language and presenting the documents in a ranked list based on their relevance to users\u2019 queries. Retrieving documents in this way can be done by employing the following approaches. 1- Translating queries to the target language, 2- Translating documents to the source language, or 3- Mapping documents and queries to a third language [11,19,17,14,7,23]. Since document translation is a time consuming and costly task, query translation is preferred. Query translation can be done by one of the following methods: 1- Using bilingual corpora and extracting a probabilistic dictionary, 2- Translation by using a Machine Translators (MT), or\n1 In Proceedings of Third Computational Linguistic Conference in Sharif University of Technology, Nov 2014\nar X\niv :1\n41 1.\n10 06\nv2 [\ncs .I\nR ]\n5 N\nov 2\n01 4\n3- Exploiting the bilingual machine readable dictionaries. Independence assumption of query terms in retrieval methods, difficulty of creating reliable MTs, and scarcity of the aligned corpora in many language pairs, make dictionary-based translation an available and straightforward solution in CLIR tasks. Dictionaries provide a list of translations for each query term. According to [7], ambiguity in translation and swamping effect2 are the most important challenges of dictionary-based methods. This paper aims to introduce Minimum Edit Support Candidates (MESC) method, a probabilistic model to overcome these challenges specifically in morphologically rich languages. In morphologically rich languages, candidates have different formations according to their parts of speech while dictionaries cannot provide all of these translations. Indeed, adding most similar words to dictionary candidates and employing a probabilistic candidate selection model, could substantially improve the CLIR task. Indeed, the proposed MESC exploits a monolingual corpora to extract different formations of the dictionary candidates. Furthermore the proposed algorithm considers other query terms to generate most probable formations. In final step, MESC builds its translation model based on candidates\u2019 bigram probabilities. Additionally it uses a simple rule-based transliterator to overcome the Out Of Vocabulary (OOV) problem. Persian is an example of highly inflected languages and there is not an effective stemmer in Persian. Experiments are specifically centered on English-Persian CLIR task. Queries are in English and documents\u2019 language is Persian. We use INQUERY retrieval system [19], rank-based methods and monolingual runs as our baselines. Experimental results show that MESC outperform the previous dictionary-based CLIR approaches. The rest of the paper is organized as follows: Section 2 reviews previous works on dictionary-based CLIR. In Section 3 we explain the MESC algorithm in details. Comparing MESC with the Pirkola\u2019s structured query system and other rankbased methods is discussed in Section 4. We bring future works and conclude the paper in Section 5."}, {"heading": "2 Previous Works", "text": "Bilingual dictionaries are truthful resources for compound word detection [5] and query expansion [4]. Ambiguity resolution is also done by Maximum Coherence Model (MCM) [15] or graph-based models [24]. MCM concerns with translation consistency within context. Mutual Information (MI) between dictionary translations be used as a measure of similarity in MCM. Phrase translation, specifically in morphologically rich languages is a main difference between MCM and MESC. As a similar analysis, graph-based models focus on correlations between translations. Authority score and hub score are translation scores in the graph-based approach. Azarbonyad et al. employ ranks of candidates to query translation [2,3]. Top N candidates after parameter tuning are selected as final translations. Swamping\n2 retrieving irrelevant documents by selecting irrelevant candidates.\neffect is the most important side effect of the rank-based methods. On the other hand Hashemi et al. utilize a bilingual dictionary and a comparable corpus to build their Term Association Network (TAN) [8]. Translations and relevant terms are extracted after creating TAN. Subject dependency of the exploited bilingual corpora is the main drawback of TAN. Nic et al. compute degree of ambiguity for each candidate and eliminate ambiguous translations [7]. Tuning the ambiguity threshold is aim of the Nic et al.\u2019s approach. Since ambiguity is a context dependent problem and Nic et al.\u2019 approach is a dictionary characteristic-based solution, the degree of ambiguity is not an effective solution. Sense disambiguation upon context terms proposed by Kishida [12]. Despite Kishida\u2019s attempts to disambiguate between multiple translations, he uses a sentence aligned corpora. Pirkola introduces INQUERY, a probabilistic retrieval system [19,20] and Oard [18] presents an overlapping character bigram-based approach, which is suitable for Chinese character re-segmentation problem. Regardless of these methods\u2019 aims to solve the ambiguity problem, crucial equivalent problem[7], and phrase translation problem, they lack accuracy: missing different formations of a candidate in a morphologically rich language, and missing powerful stemmers in such a language, make these methods fail to match accurate formations. Finding most similar formations in MESC is strongly related to error correction approaches specifically in previous Persian language studies [6,16]."}, {"heading": "3 Dictionary-based CLIR and MESC", "text": "Previous dictionary-based CLIR studies employ bilingual dictionaries to translate queries to retrieve documents in a different language from queries\u2019 language. Dictionary-based algorithms suffer from ambiguity in candidate selection. Moreover, dictionaries cannot provide all formations of a candidate. For instance, in \u2018World Cup\u2019 query, we expect CLIR algorithm to generate \u2018ja\u0302m jha\u0302ni\u2019 as the correct translation. But 1:\u2018jha\u0302n\u2019, 2:\u2018giti\u2019, 3:\u2018dnia\u0302\u2019, and 4:\u2018a\u0302lm\u2019 are only the provided translations in a bilingual English-Persian dictionary for \u2018World\u2019 and 1:\u2018fnja\u0302n\u2019, 2:\u2018ja\u0302m\u2019, and 3:\u2018pia\u0302lh\u2019 for \u2018Cup\u2019. Stemming problem in highly inflected languages as well as ambiguity problem make the previous dictionary-based CLIR methods to have degraded performances. This paper aims to overcome the mentioned challenges in CLIR task. The view presented in the proposed Minimum Edit Support Candidates (MESC) is to add similar terms based on their forms to dictionary candidate lists. Similar terms which have co-occurrences with candidates belonging to other query terms are only selected. For example \u2018jha\u0302ni\u2019 is not only similar to \u2018jha\u0302n\u2019 but also it is co-occurred with \u2018ja\u0302m\u2019, the second translation of \u2018Cup\u2019 in bilingual dictionaries. Minimum Edit Distance (MED) is an algorithm to find minimum number of steps transforming one string into another, in terms of insertion, deletion and substitution. There are also other versions of the algorithm, but the Levenshtein algorithm is a simple version that assumes the weights of all operations to be equal [13,10]. We apply the Levenshtein algorithm to find similar terms to dictionary candidates.\nDespite the support lists\u2019 attempts to solve the crucial equivalent selection problem, selecting most relevant translation, it has a drawback: Adding such derivational forms may cause selecting wrong translations due to generating irrelevant lexicons by MESC. Moreover it is important to decide which candidates in ci and slevi is the best translation according to the context. In fact, ignoring noisy support candidates and solving the ambiguity problem are other challenges of MESC. We present some notations in Section 3.1. The OOV word problem is discussed in Section 3.2. Extracting the support candidates and details of MESC\u2019s translation model are presented in Section 3.3 and Section 3.4 respectively."}, {"heading": "3.1 Notations", "text": "In our CLIR task, there is a set of documents in target language which is called Dt = {d1,d2, ..,dm} and a set of source language query terms Qs = {qs1, qs2, .., qsm} where qsi is the i-th query term. Each query term has a list of dictionary candidates in Ct = {c1, c2, .., cm}. Furthermore Vt = {v1, v2, .., vn} is the vocabulary set in target language and A = [ax,y]|Vt|\u00d7|Vt| indicates the adjacency matrix. ax,y equals to one if two terms co-occurred within a specified window w and equals to zero otherwise. Similar terms to the dictionary candidates are extracted from a monolingual corpus and make secondary lists or the support lists for the query terms. We have a support set Stlev = {slev1 , slev2 , .., slevm }, where slevi stands for a list of some vj \u2208 Vt which its minimum edit distance from a candidate in ci is equals to one or two. Whose every query term qi has two lists of translation candidates ci and s lev i and other forms of a lexicon in ci can appear in the latter list. Finally Qt = {qt1, qt2, .., qtm} is a set of translated query terms."}, {"heading": "3.2 Out Of Vocabulary", "text": "Plural nouns, different formations of a verb, proper nouns, and phrases form main parts of dictionary-based CLIR challenges. Regardless of possibility of determining stems of plural query terms by applying simple algorithms, plural form of a candidate may be produced by MESC if its minimum edit distance from its singular form is equals to one or two. For an example, in query \u2018Iran Football Coaches\u2019, the MESC algorithm generates \u2018mrbia\u0302n futba\u0302l ira\u0302n\u2019 which \u2018mrbia\u0302n\u2019 is generated due to the reason of having two edit distances with its singular form \u2018mrbi\u2019. Proper nouns are transliterated using a probabilistic rule-based transliterator. Replacing all consonant letters, generating all possible replacements of vowels and pushing all of them in query term\u2019s support candidate list is the first step in proper noun translation. Secondly, the MESC algorithm aims to select the most probable formation based on its bigram probability with other query terms. Fig. 1 presents statistics of the sources of errors in query translations in topics of CLEF 2008 and 2009. Topics in CLEF 2009 consist of more ambiguous queries.\nExisting a great number of phrases and multiple phrases within a query in topics of CLEF 2009 make CLIR a more challenging task in such topics."}, {"heading": "3.3 Extracting Support Candidates", "text": "The forgoing discussions imply that support candidates are arbitrary options to be chosen as a translation of a query term. In order to extract these words, the adjacency matrix A is firstly filled with binary numbers indicating closeness of terms in a target language collection. Accordingly, for a dictionary candidate ci,j , the support candidate s lev i,j\u2032 is some v \u2208 Vt that not only has one or two minimum edit distances from ci,j but also it has a non-zero co-occurrence value with at least a term in other query terms\u2019 candidates. In brief, support candidate list for the i-th query term is:\nslevi = { v \u2208 Vt| \u2203ci,j \u2208 ci ( \u2203ci\u2032,j\u2032 \u2208 ci\u2032 ( i 6= i\u2032 \u2227\nav,ci\u2032,j\u2032 = 1 \u2227 1 \u2264 MED(v, ci,j) \u2264 2 ))} . (1)"}, {"heading": "3.4 Minimum Edit Support Candidates Model", "text": "On balance, each query term has two lists of candidates and selecting the best candidate is aim of the current discussion. MESC defines two sets of probability lists:\n1. Pc = {pc1,pc2, ..,pcm} denotes a set of lists like pci that contains probabilities of all candidates in ci conditioned on observing the qi (to be exact, p c i,j =\nP (ci,j ; q s i ) which ci,j indicates the j-th candidate of ci).\n2. Similarly Pslev = {pslev1 ,p slev 2 , ..,p slev m } consists of lists of probabilities cor-\nresponding each query term\u2019s support candidates conditioned on observing its query term (more precisely pslevi,j = P (s lev i,j ; q s i ) that s lev i,j points to the j-th candidate of the support list slevi ).\nIn short: pci = [p c i,j ]1\u00d7|ci| p slev i = [p slev i,j ]1\u00d7|slevi | .\nThe probability of a candidate after observing its query term can be shown as:\nP (ci,j ; q s i ) = \u2211 i\u2032 6=i ( |ci\u2032 |\u2211 j\u2032=1 P (ci,j |ci\u2032,j\u2032 ; qsi )P (ci\u2032,j\u2032 ; qsi )+\n|slev i\u2032 |\u2211\nj\u2032=1\nP (ci,j |slevi\u2032,j\u2032 ; qsi )P (slevi\u2032,j\u2032 ; qsi ) ) .\n(2)\nAs a simplification assumption, translation candidates are assumed independent from other source query terms (i.e. , P (ci\u2032,j\u2032 ; qi) \u2248 P (ci\u2032,j\u2032)). Furthermore we can estimate P (ci,j |ci\u2032,j\u2032 ; qi) using bigram probability of the candidates. In more detail we have:\nP (ci,j |ci\u2032,j\u2032 ; qsi )P (ci\u2032,j\u2032 ; qsi ) \u2248 P (ci,j |ci\u2032,j\u2032)P (ci\u2032,j\u2032) = P (ci,j , ci\u2032,j\u2032).\n(3)\nSimilarly: P (ci,j |slevi\u2032,j\u2032 ; qsi )P (slevi\u2032,j\u2032 ; qsi ) \u2248 P (ci,j , slevi\u2032,j\u2032) (4)\nIf we define pci,j = P (ci,j ; q s i ) the Equation 2 can be represented as follow:\npci,j = \u2211 i\u2032 6=i ( |ci\u2032 |\u2211 j\u2032=1 P (ci,j , ci\u2032,j\u2032) + |slev i\u2032 |\u2211 j\u2032=1 P (ci,j , s lev i\u2032,j\u2032) ) . (5)\nAs be stated, every dictionary candidate\u2019s probability depends on terms in the other query terms\u2019 candidate lists. But in this case the probability of a support candidate is obtained just by comparing with dictionary candidates to prevent adding noisy probabilities. Indeed the Levenshtein algorithm may produce an irrelevant support candidate which have a high bigram probability with another irrelevant support candidate. As a result a support candidate probability equals:\npslevi,j = \u2211 i\u20326=i |ci\u2032|\u2211 j\u2032=1 P (slevi,j , ci\u2032,j\u2032). (6)\nFinal normalization consideration can be stated as follows:\n|ci|\u2211 j=1 pci,j + |slevi |\u2211 j=1 pslevi,j = 1. (7)\nThe Equation 7 points to probability distribution condition for translations of the qsi . It seems evident that Equation 7 forces p to distribute probability over the candidates in dictionary and the support list. Any other translation that is not presented in such lists gets zero probability. Indeed, coverage of the dictionary plays an important role in the proposed model."}, {"heading": "3.5 Candidate Selection", "text": "Finally, if we define T a translation candidate list, and PT a corresponding probability vector, qti , the final translation of q s i , is a candidate with highest probability:\nT = ( ci slevi ) PT = ( Pci Pslevi ) qti = arg max\nTi,j\npTi,j . (8)"}, {"heading": "4 Experiments", "text": "This section attempts to investigate the validity of the proposed algorithm. Indeed implementation the of MESC algorithm in topics of CLEF 2008 and CLEF 2009 and Hamshahri[1] collection is discussed in the current section."}, {"heading": "4.1 Experiment Setup", "text": "Data Collection Hamshahri is a Persian document collection with 166,774 documents whose average document length equals to 225 terms. This collection has been used as a resource for our retrieval task and computing unigram and bigram probabilities or joint probabilities to be exact. We can use any other large monolingual collection in target language to compute such probabilities.\nTools and Toolkits Lemur toolkit 3 is our retrieval tool and Okapi is selected as a retrieval model [21]. Pseudo relevance feedback has been done as a query expansion phase for all monolingual retrieval runs, the proposed algorithm, and previous works. Probabilities are extracted using SRILM toolkit[22].\nBilingual Dictionary Regarding comparing the CLIR performance of MESC with the previous methods, proposed algorithm is applied on title parts of the topics in CLEF 2008 and CLEF 2009. Each of them consists of 50 English queries. In addition to Hamshahri we exploit three machine readable bilingual dictionaries:\n1. Aryanpour4 bilingual dictionary which has been used in most of the previous dictionary-based English-Persian CLIR research [2,9].\n3 http://www.lemurproject.org 4 http://www.aryanpour.com\n2. Dictionary of Google5 and not using its MT. 3. Faraazin6 and using its available bilingual dictionary.\nTable 1 shows average number of candidates for each entry of a dictionary or dictionary scales [7]. Aryanpour has almost largest candidate lists. Coverage of dictionaries over topics in CLEF 2008 and CLEF 2009 are equal and the dictionaries\u2019 characteristics differ according to their scales and rankings."}, {"heading": "4.2 Persian Monolingual Retrieval", "text": "Monolingual Persian retrieval is our CLIR task\u2019s baseline. Table 2 shows results of the monolingual retrieval runs on the topics in CLEF 2008 and CLEF 2009."}, {"heading": "4.3 Top Ranked Candidates vs Pirkola\u2019s Structured Query", "text": "Most studies such as [7,9,2,3] emphasize on selecting top ranked candidates as translations of a query term. According to [7] selection in such a way could miss some important lower ranked candidates. That is to say adding more candidates to final translation causes the swamping effect [7] or retrieving irrelevant documents to be exact. Consequently it is tradeoff selecting high ranked translations or adding all candidates. Table 3 represents Mean Average Precision (MAP) of\n5 http://translate.google.com 6 http://www.faraazin.ir\nthe runs using different dictionaries. Proper nouns for all resources has been handled by Google\u2019s MT7. Google\u2019s descending results prove almost reliable rankings for its translations. As a result, adding more candidates may turn retrieval to different subjects. On the other hand there are some improvements after adding more candidates in Aryanpour and Faraazin in some points. In brief, according to the results, important translations are not necessarily provided in high-ranked translations. As [19,18] state, defining a set of translations for a query term and treating them as instances of a term, can reach reliable weightings. Table 4 shows the results of the INQUERY retrieval runs based on the Aryanpour dictionary and topics in CLEF 2008 and CLEF 2009. The results are relatively better than top ranked selection approach due to resolving the crucial equivalent problem by incorporating all translations. As an important step prior to the INQUERY retrieval run, the proper nouns has been handled by employing Google\u2019s MT."}, {"heading": "4.4 MESC and Probabilistic Selection Model", "text": "As represented in Table 1 Aryanpour has longest candidate lists and the results of top ranked selection approach support the view that it is more difficult to select the best candidate in a high coverage dictionary compared to the lower coverage ones. Nevertheless, having high coverage could benefit the MESC algorithm\u2019s performance for the following reasons: firstly, neither the number of candidates nor the ranks of them effect the performance of MESC and secondly containing more crucial equivalents benefit MESC. Table 5 shows the results of applying MESC on the CLEF dataset. Improvements of MESC compared to the INQUERY retrieval system are also presented. A closer look at the results of\n7 http://translate.google.com\nTable 5 indicates substantial improvements in terms of MAP for all mentioned dictionaries. However in topics of CLEF 2009 there are less improvements comparing to INQUERY. The statistics of the topics in CLEF 2009 in Fig. 1 show more phrases in CLEF 2008 compared to CLEF 2009. Extracting different formations of a lexicon in phrases is the main superiority of MESC and the Pirkola\u2019s structured query approach. Nonetheless we can state missing important translations of a term in a dictionary as another reason to fail accurate translation in MESC. For example in query \u2018Stress and Health\u2019, transliterating \u2018Stress\u2019 is better translation against choosing \u2018fSa\u0302r\u2019 that means pressure. In such situations, MESC is limited to the provided candidates which may differ in subject with the original query. Fig. 2 presents precision at different levels of recall. As a reason for lower precision in some points for CLEF 2009, we can point to synonym equivalent effect. For instance in query \u2018Tourist Attractions\u2019, for the query term \u2018Tourist\u2019, its transliteration is a correct translation which Aryanpour does not provide. Despite missing such a meaningful candidate, either \u2018grdSgr\u2019 or \u2018jha\u0302ngrd\u2019 is a trustful translation and both of them have significant usages in similar contexts. The structured query approach considers both of them and the INQUERY retrieval system scores documents based on containing either the former translation or the latter one. Under these circumstances, INQERY has better performance. We can state multi part translations as another reason for degraded precision. In query \u2018Freight Transport by Rail\u2019, the term \u2018Rail\u2019 means railroad. \u2018ra\u0302h-a\u0302hn\u2019 is a most common translation for the query term. In Hamshahri \u2018ra\u0302h\u2019 and \u2018a\u0302hn\u2019 almost always are separated by a space. Missing effective tokenizers in highly inflected languages like Persian, causes MESC to treat such parts as distinct terms. Finding these parts and merging them by a half space benefit the MESC algorithm to compute reliable probabilities."}, {"heading": "5 Conclusions and Future Works", "text": "In this research we proposed MESC algorithm, a probabilistic dictionary-based CLIR approach for agglutinative languages. MESC initially provides omitted\nTable 5: Results of applying MESC algorithm on CLEF.\nTopic Measure Aryanpour %M Impr. Google%M Impr. Farazin %M Impr. Mono\n2008 Map 0.3215 72.2 +18.8 0.319 71.7 +15.9 0.293 65.8 +25.7 0.4449 Prec@5 0.50 71.0 +1.6 0.492 69.9 +0.0 0.44 62.5 +0.9 0.704 Prec@10 0.502 74.7 +5.5 0.49 72.9 +6.5 0.436 64.9 +1.9 0.672\n2009 Map 0.2682 65.9 +3.4 0.278 68.2 +0.9 0.247 60.7 +6.1 0.407 Prec@5 0.428 71.3 -11.7 0.456 76.0 -4.2 0.364 60.7 -8.0 0.60 Prec@10 0.408 68.2 -1.9 0.424 70.9 -5.8 0.354 59.2 -5.8 0.598\nformations of dictionary candidates using the Levenshtein algorithm and a large monolingual collection. Secondly MESC uses a probabilistic candidate selection model to select most accurate translations. Experimental results of EnglishPersian CLIR runs provide confirmatory evidences in favor of the MESC algorithm against the Pirkola\u2019s INQUERY retrieval system. They also support the view that using a bilingual machine readable dictionary with a reliable coverage improves the CLIR performance regardless of considering any rankings. As an important future work we can state considering phrase detection algorithms and applying the proposed MESC locally to extract truthful support candidates."}, {"heading": "6 Acknowledgments", "text": "We would like to thank Behzad Mirzababaei from Natural Language Processing Lab for his helpful comments and Iman D. Behbahani from Intelligent Information Systems Lab for his advantageous advice."}], "references": [{"title": "Hamshahri: A standard persian text collection", "author": ["A. AleAhmad", "H. Amiri", "E. Darrudi", "M. Rahgozar", "F. Oroumchian"], "venue": "Know.-Based Syst. 22(5), 382\u2013387", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Using learning to rank approach for parallel corpora based cross language information retrieval", "author": ["H. Azarbonyad", "A. Shakery", "H. Faili"], "venue": "ECAI\u201912. pp. 79\u201384", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting multiple translation resources for english-persian cross language information retrieval", "author": ["H. Azarbonyad", "A. Shakery", "H. Faili"], "venue": "Information Access Evaluation. Multilinguality, Multimodality, and Visualization, pp. 93\u201399. Springer Berlin Heidelberg", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Extending query translation to cross-language query expansion with markov chain models", "author": ["G. Cao", "J. Gao", "J.Y. Nie", "J. Bai"], "venue": "Proceedings of the sixteenth ACM conference on Conference on information and knowledge management. pp. 351\u2013360. CIKM \u201907, ACM, New York, NY, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "English-chinese cross-language ir using bilingual dictionaries", "author": ["A. Chen", "H. Jiang", "F. Gey"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Grammatical and context-sensitive error correction using a statistical machine translation framework", "author": ["N. Ehsan", "H. Faili"], "venue": "Softw., Pract. Exper. 43(2), 187\u2013206", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Dictionary characteristics in crosslanguage information retrieval", "author": ["D.N. Gearailt", "C.D.N. Gearailt", "C. College"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Using Comparable Corpora for English-Persian Cross-Language Information Retrieval", "author": ["H. Hashemi"], "venue": "Master\u2019s thesis, University of Tehran, Tehran", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Mining a persian-english comparable corpus for crosslanguage information retrieval", "author": ["H.B. Hashemi", "A. Shakery"], "venue": "Inf. Process. Manage. 50(2), 384\u2013398", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition", "author": ["D. Jurafsky", "J. Martin"], "venue": "Prentice Hall series in artificial intelligence, Pearson Prentice Hall", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Technical issues of cross-language information retrieval: a review", "author": ["K. Kishida"], "venue": "Inf. Process. Manage. 41(3), 433\u2013455", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Translation disambiguation for cross-language information retrieval using context-based translation probability", "author": ["K. Kishida", "E. Ishita"], "venue": "Journal of Information Science 35(4), 481\u2013495", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Binary Codes Capable of Correcting Deletions, Insertions and Reversals", "author": ["V. Levenshtein"], "venue": "Soviet Physics Doklady 10, 707", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1966}, {"title": "Dictionary-based techniques for crosslanguage information retrieval", "author": ["G.A. Levow", "D.W. Oard", "P. Resnik"], "venue": "Inf. Process. Manage. 41(3), 523\u2013547", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "A maximum coherence model for dictionary-based crosslanguage information retrieval", "author": ["Y. Liu", "R. Jin", "J.Y. Chai"], "venue": "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. pp. 536\u2013543. ACM Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Farsispell: A spell-checking system for persian using a large monolingual corpus", "author": ["T.M. Miangah"], "venue": "LLC 29(1), 56\u201373", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-language information retrieval", "author": ["J.Y. Nie"], "venue": "Synthesis Lectures on Human Language Technologies 3(1), 1\u2013125", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Ntcir-2 ecir experiments at maryland: Comparing pirkola\u2019s structured queries and balanced translation", "author": ["D.W. Oard", "J. Wang"], "venue": "In Second National Institute of Informatics (NII) Test Collection Information Retrieval (NTCIR) workshop. forthcoming", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "The effects of query structure and dictionary setups in dictionarybased cross-language information retrieval", "author": ["A. Pirkola"], "venue": "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. pp. 55\u201363. SIGIR \u201998, ACM, New York, NY, USA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Dictionary-based crosslanguage information retrieval: Problems, methods, and research findings", "author": ["A. Pirkola", "T. Hedlund", "H. Keskustalo", "K. Jrvelin"], "venue": "Information Retrieval 4(3-4), 209\u2013230", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["S.E. Robertson", "S. Walker"], "venue": "pp. 232\u2013241. SIGIR \u201994", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Srilm - an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "Hansen, J.H.L., Pellom, B.L. (eds.) INTERSPEECH. ISCA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Cross-Language Information Retrieval Using Latent Semantic Indexing", "author": ["P.G. Young"], "venue": "Master-thesis, University of Tennessee, Knoxville, Knoxville", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}, {"title": "A hybrid technique for englishchinese cross language information retrieval", "author": ["D. Zhou", "M. Truran", "T. Brailsford", "H. Ashman"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "1- Translating queries to the target language, 2- Translating documents to the source language, or 3- Mapping documents and queries to a third language [11,19,17,14,7,23].", "startOffset": 152, "endOffset": 170}, {"referenceID": 18, "context": "1- Translating queries to the target language, 2- Translating documents to the source language, or 3- Mapping documents and queries to a third language [11,19,17,14,7,23].", "startOffset": 152, "endOffset": 170}, {"referenceID": 16, "context": "1- Translating queries to the target language, 2- Translating documents to the source language, or 3- Mapping documents and queries to a third language [11,19,17,14,7,23].", "startOffset": 152, "endOffset": 170}, {"referenceID": 13, "context": "1- Translating queries to the target language, 2- Translating documents to the source language, or 3- Mapping documents and queries to a third language [11,19,17,14,7,23].", "startOffset": 152, "endOffset": 170}, {"referenceID": 6, "context": "1- Translating queries to the target language, 2- Translating documents to the source language, or 3- Mapping documents and queries to a third language [11,19,17,14,7,23].", "startOffset": 152, "endOffset": 170}, {"referenceID": 22, "context": "1- Translating queries to the target language, 2- Translating documents to the source language, or 3- Mapping documents and queries to a third language [11,19,17,14,7,23].", "startOffset": 152, "endOffset": 170}, {"referenceID": 6, "context": "According to [7], ambiguity in translation and swamping effect are the most important challenges of dictionary-based methods.", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "We use INQUERY retrieval system [19], rank-based methods and monolingual runs as our baselines.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Bilingual dictionaries are truthful resources for compound word detection [5] and query expansion [4].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "Bilingual dictionaries are truthful resources for compound word detection [5] and query expansion [4].", "startOffset": 98, "endOffset": 101}, {"referenceID": 14, "context": "Ambiguity resolution is also done by Maximum Coherence Model (MCM) [15] or graph-based models [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "Ambiguity resolution is also done by Maximum Coherence Model (MCM) [15] or graph-based models [24].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "employ ranks of candidates to query translation [2,3].", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "employ ranks of candidates to query translation [2,3].", "startOffset": 48, "endOffset": 53}, {"referenceID": 7, "context": "utilize a bilingual dictionary and a comparable corpus to build their Term Association Network (TAN) [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "compute degree of ambiguity for each candidate and eliminate ambiguous translations [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 11, "context": "Sense disambiguation upon context terms proposed by Kishida [12].", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Pirkola introduces INQUERY, a probabilistic retrieval system [19,20] and Oard [18] presents an overlapping character bigram-based approach, which is suitable for Chinese character re-segmentation problem.", "startOffset": 61, "endOffset": 68}, {"referenceID": 19, "context": "Pirkola introduces INQUERY, a probabilistic retrieval system [19,20] and Oard [18] presents an overlapping character bigram-based approach, which is suitable for Chinese character re-segmentation problem.", "startOffset": 61, "endOffset": 68}, {"referenceID": 17, "context": "Pirkola introduces INQUERY, a probabilistic retrieval system [19,20] and Oard [18] presents an overlapping character bigram-based approach, which is suitable for Chinese character re-segmentation problem.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "Regardless of these methods\u2019 aims to solve the ambiguity problem, crucial equivalent problem[7], and phrase translation problem, they lack accuracy: missing different formations of a candidate in a morphologically rich language, and missing powerful stemmers in such a language, make these methods fail to match accurate formations.", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "Finding most similar formations in MESC is strongly related to error correction approaches specifically in previous Persian language studies [6,16].", "startOffset": 141, "endOffset": 147}, {"referenceID": 15, "context": "Finding most similar formations in MESC is strongly related to error correction approaches specifically in previous Persian language studies [6,16].", "startOffset": 141, "endOffset": 147}, {"referenceID": 12, "context": "There are also other versions of the algorithm, but the Levenshtein algorithm is a simple version that assumes the weights of all operations to be equal [13,10].", "startOffset": 153, "endOffset": 160}, {"referenceID": 9, "context": "There are also other versions of the algorithm, but the Levenshtein algorithm is a simple version that assumes the weights of all operations to be equal [13,10].", "startOffset": 153, "endOffset": 160}, {"referenceID": 0, "context": "Indeed implementation the of MESC algorithm in topics of CLEF 2008 and CLEF 2009 and Hamshahri[1] collection is discussed in the current section.", "startOffset": 94, "endOffset": 97}, {"referenceID": 20, "context": "Tools and Toolkits Lemur toolkit 3 is our retrieval tool and Okapi is selected as a retrieval model [21].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Probabilities are extracted using SRILM toolkit[22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "Aryanpour bilingual dictionary which has been used in most of the previous dictionary-based English-Persian CLIR research [2,9].", "startOffset": 122, "endOffset": 127}, {"referenceID": 8, "context": "Aryanpour bilingual dictionary which has been used in most of the previous dictionary-based English-Persian CLIR research [2,9].", "startOffset": 122, "endOffset": 127}, {"referenceID": 6, "context": "Table 1 shows average number of candidates for each entry of a dictionary or dictionary scales [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "Most studies such as [7,9,2,3] emphasize on selecting top ranked candidates as translations of a query term.", "startOffset": 21, "endOffset": 30}, {"referenceID": 8, "context": "Most studies such as [7,9,2,3] emphasize on selecting top ranked candidates as translations of a query term.", "startOffset": 21, "endOffset": 30}, {"referenceID": 1, "context": "Most studies such as [7,9,2,3] emphasize on selecting top ranked candidates as translations of a query term.", "startOffset": 21, "endOffset": 30}, {"referenceID": 2, "context": "Most studies such as [7,9,2,3] emphasize on selecting top ranked candidates as translations of a query term.", "startOffset": 21, "endOffset": 30}, {"referenceID": 6, "context": "According to [7] selection in such a way could miss some important lower ranked candidates.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "That is to say adding more candidates to final translation causes the swamping effect [7] or retrieving irrelevant documents to be exact.", "startOffset": 86, "endOffset": 89}, {"referenceID": 18, "context": "As [19,18] state, defining a set of translations for a query term and treating them as instances of a term, can reach reliable weightings.", "startOffset": 3, "endOffset": 10}, {"referenceID": 17, "context": "As [19,18] state, defining a set of translations for a query term and treating them as instances of a term, can reach reliable weightings.", "startOffset": 3, "endOffset": 10}], "year": 2014, "abstractText": "Translation ambiguity, out of vocabulary words and missing some translations in bilingual dictionaries make dictionary-based Crosslanguage Information Retrieval (CLIR) a challenging task. Moreover, in agglutinative languages which do not have reliable stemmers, missing various lexical formations in bilingual dictionaries degrades CLIR performance. This paper aims to introduce a probabilistic translation model to solve the ambiguity problem, and also to provide most likely formations of a dictionary candidate. We propose Minimum Edit Support Candidates (MESC) method that exploits a monolingual corpus and a bilingual dictionary to translate users\u2019 native language queries to documents\u2019 language. Our experiments show that the proposed method outperforms state-of-the-art dictionary-based English-Persian CLIR. 1", "creator": "LaTeX with hyperref package"}}}