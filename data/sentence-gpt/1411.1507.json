{"id": "1411.1507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "Scalable Parallel Numerical CSP Solver", "abstract": "We present a parallel solver for numerical constraint satisfaction problems (NCSPs) that can scale on a number of cores. Our proposed method runs worker solvers on the available cores and simultaneously the workers cooperate for the search space distribution and balancing. In the experiments, we attained up to 119-fold speedup using 256 cores of a parallel computer (32 cores). This speedup provided the first step in identifying an optimal task with the greatest accuracy. This method was implemented in two parallel machines using the same parallel machines. The next task was to define a total problem solver for the same task (2 cores). The next step in this step was to define a total problem solver for the same task. We created a parallel machine for these tasks. The next task was to compute the complete task. The first task was to verify the correctness of the solver (3 cores). The final task was to compute the number of threads of this task. This task was to compute the total number of threads. This task was to compute the total number of threads of this task. We also calculated the number of threads (1 thread) that all cores need for the task. These results showed that the problem solver can be run on both cores simultaneously (1 thread), but the solution was to create a number of threads using a different machine. We also calculated the total number of threads (2 threads) that all cores need for the task. We also computed the total number of threads (1 thread) that all cores need for the task. We also calculated the number of threads (1 thread) that all cores need for the task. We also calculated the total number of threads (1 thread) that all cores need for the task. We also calculated the number of threads (1 thread) that all cores need for the task. We also calculated the number of threads (1 thread) that all cores need for the task.", "histories": [["v1", "Thu, 6 Nov 2014 06:33:17 GMT  (2104kb)", "http://arxiv.org/abs/1411.1507v1", "The final publication is available at Springer"]], "COMMENTS": "The final publication is available at Springer", "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["daisuke ishii", "kazuki yoshizoe", "toyotaro suzumura"], "accepted": false, "id": "1411.1507"}, "pdf": {"name": "1411.1507.pdf", "metadata": {"source": "CRF", "title": "Scalable Parallel Numerical CSP Solver", "authors": ["Daisuke Ishii", "Kazuki Yoshizoe", "Toyotaro Suzumura"], "emails": ["dsksh@acm.org", "yoshizoe@acm.org", "suzumura@acm.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n15 07\nv1 [\ncs .A\nI] 6\nN ov"}, {"heading": "1 Introduction", "text": "Numerical constraint satisfaction problems (NCSPs, Section 2) have been successfully applied to problems described in the domain of reals[3,6]. Given a NCSP with search space represented as a box (i.e., interval vector), the branch and prune algorithm efficiently computes a paving, a set of boxes that encloses the solution set, yet its exponential computational complexity limits the tractable instances. Although the solving process exhibits a parallelism, no parallel NCSP solver has been made available to date because of the difficulty in partitioning the search space equally.\nIn this research, we parallelize a NCSP solver to scale its solving process on both shared memory and distributed memory parallel computers (Section 3). Our parallel method consists of parallel worker solvers that solve a portion of search space on CPU cores and interact with neighbor workers via message passing for dynamic load balancing. We also propose a preprocess that accelerates the initial search space distribution by sending sets of boxes via static routing between the workers. We have implemented the method by extending the Realpaver solver using the X10 language to realize a process-level parallelization over a number of cores. Section 4 reports experimental results when our method was deployed on two hardware environments.\nRelated work. There have been several work regarding parallel solving of CSPs with either discrete or continuous domains. Parallel solving of generic CSPs on massive computer clusters and supercomputers has been explored in [12,2,18]. This work focuses on a massive parallel solver for NCSPs that has a different characteristics compared to generic CSPs. In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf. portfolios)[2], and (iii) parallelization of constraint propagation. Our method belongs to the first category. A few\nworks have used approaches (ii)[11] and (iii)[8] for parallelization of NCSP solving. However, to the best of our knowledge, a massive parallelization method that uses the typical approach (i) has not yet been proposed.\nSubstantial work regarding the parallelization of the branch and bound algorithm with search-space splitting exists [9,13,14]. This approach has also been applied to CSP solvers [16,5] and SAT solvers [15]. This work explores an efficient parallel method for solving NCSPs with similar approach to [9,13]."}, {"heading": "2 Numerical Constraint Satisfaction Problems", "text": "A numerical constraint satisfaction problem (NCSP) is defined as a triple (v,v0, c) that consists of a vector of variables v = (v1, . . . , vn), an initial domain in the form of a box v0 \u2208 IR\nn (IR denotes the set of closed real intervals), and a constraint c(v) \u2261 f(v) = 0 \u2227 g(v) \u2265 0, where f : Rn \u2192 Re and g : Rn \u2192 Ri, i.e., a conjunction of e equations and i inequalities. A solution of a NCSP is an assignment of its variables v \u2208 v0 that satisfies its constraints. The solution set \u03a3 of a NCSP is the region within its initial domain that satisfies its constraints, i.e., \u03a3(v0) := {v\u0303 \u2208 v0 | c(v\u0303)}. The target of this paper is under-constrained NCSPs such that n > e. In general, an under-constrained problem may have a continuous set of infinitely many solutions.\nBranch and Prune Algorithm. The branch and prune algorithm [17] is the standard solving method for NCSPs. It takes a NCSP and a precision \u01eb as an input and outputs a set of boxes (or paving) S that approximates the solution set with precision \u01eb. Examples of S are illustrated in Figure 1.\nAn intermediate state of the algorithm is represented as a pair of sets of boxes (L, S). The solver receives an initial state ({v0}, \u2205) and iteratively applies the step computation (illustrated in Figure 2) until it reaches a final state (\u2205, S ). In the step computation, first, it takes the first element of the queue L of boxes and applies the Prune procedure, which is a filtering procedure that shaves boundary portions of the considered box. In this work, we use an implementation proposed in [6] which provides a verification process based on an interval Newton method combined with a relatively simple filtering process based on the Hull consistency[1]. As a result, a box becomes either empty, precise enough (its width is smaller than \u01eb), verified as an inner box of the solution set \u03a3, or undecided. Precise and inner boxes are appended to S and undecided boxes are passed to Branch. Second, the Branch procedure bisects the box at the midpoint along a component corresponding to one of the variables and the sub-boxes will be put back in the queue. In this work, we assume Branch selects variables in an order which makes the search to behave in a breadth-first manner and thus the solving process gradually improves the precision of the overall pavings (Figure 1).\nThe computation of Prune is expensive and is the bottleneck of the solving process. Under certain conditions, application of Prune contracts a large portion of the search space into a tight box (cf. quadratic convergence of the interval Newton methods). Prune can also filter out the whole box if the considered box is\nverified as an inner or totally inconsistent region. These characteristics of Prune result in the unbalanced nature of search trees. Therefore, a straightforward parallel method does not work efficiently. It is crucial for efficient NCSP solving to execute Prune on each step of traversing the search tree which makes it more difficult to distribute a search path among processors. These properties will be discussed in Section 4.1."}, {"heading": "3 Parallel Branch and Prune", "text": "We propose a parallel method that runs several workers on the available CPU cores p0, . . . , p#p\u22121 (we assume a single worker runs on a core and identify both a worker and a core with pi). Workers homogeneously interleave the following three procedures and cooperate in a decentralized fashion: (i) breadth-first branch and prune search, (ii) distribution and workload balancing of search space in a sender-initiated manner, and (iii) termination detection. Sub-trees in the search space of the branch and prune becomes unbalanced but can be searched independently: there are no confluence of multiple branches, and we have no shared information between branches. Distribution of the search space among workers is done in preprocess and postprocess as described in the following subsections. The preprocess distributes the portions of the search space to the other workers, and the postprocess balances the load of each worker during search. Termination is detected by circulating a termination token via idling workers based on Dijkstra\u2019s method (see Section 11.4.4 of [9]).\nPreprocess: Search-Space Splitting and Distribution. A solving process is started by a worker p0 that possesses the initial domain (i.e., a box which contains the whole search space) in the queue L. To distribute the subsequent search space (i.e., a queue of boxes) equally to each core, the preprocess invokes a partition of the queue in two (or more) and then sends a portion to another worker. Figure 3 illustrates in a downward direction some initial transitions of the box queue L distributed among three workers. In our implementation, the\ndistribution routing is formed as a binary tree whose height is \u2308log2 #p\u2309. In each node of the tree, the branch and prune process runs until the number of boxes in the queue reaches nb. Next, the queue is sorted by the volume of the boxes and the half of the content (i.e., nb/2 boxes) is sent to the other core via the right branch.\nPostprocess: Dynamic Load Balancing. During search, each worker normalizes the loads within a predefined neighborhood which consists of a small number of neighbor workers. Because there are sufficiently large number of boxes, we simply regard the number of boxes in the queue L as the amount of load. Assume #p workers are running and each worker pi possesses li boxes in its queue. We also assume for each worker pi that Ni is a set of |N | neighbor workers, N\u22121i is a set of workers where pj \u2208 N \u22121 i \u21d4 pi \u2208 Nj , Li is a set of loads of the neighbor workers, and \u2206 is a predefined load margin. The load balancing procedure of a worker pi performs the following steps once every ns branch and prune steps.\n1. For each worker pj in N \u22121 i , inform the load li and put in the list Lj. 2. Calculate the mean \u00b5 of the loads in Li. 3. If \u00b5 < \u2206, for each worker pj in Ni, send at most \u00b5 \u2212 lj boxes to pj (to be\nefficient, a certain number of boxes should be kept locally).\nNeighbor workers can be identified e.g. as adjacent nodes in the |N |-dimensional mesh of workers. The routing between neighbor workers is fixed during a solving process and thus it may happen that a worker possesses an excessive load than others. However, this load imbalance will be resolved by the subsequent load balancing processes."}, {"heading": "4 Experimental Results", "text": "We have implemented the proposed method and measured the speedup of the solving process of under-constrained NCSPs. The experiments were performed with an exhaustive set of parameter combinations to explore the optimal settings.\nImplementation. We have implemented the proposed method with C++ (gcc ver. 4.4.7 and 4.3.4) and X10 (ver. 2.3.1)[4], a high productivity language for parallel computing. In the following, we use the term place, which is a notion of X10 that in our setting represents a CPU core. Libraries Realpaver (ver. 1.1)[10] for sequential NCSP solving and Gaol (ver. 4.0.1)4 for basic interval computation were used to facilitate the implementation. The Prune procedure was realized by calling the sequential implementation in Realpaver. Each run of Prune took around 0.2\u20131ms and the overall execution became the bottleneck of the branch and prune algorithm (occupied greater than 95% of running time in sequential solving). The procedures for search space distribution and load balancing were implemented with X10. Communication of boxes and loads between places were\n4 http://sourceforge.net/projects/gaol/\nimplemented as async tasks and performed in parallel to the search process so that the overhead will be hidden. In the experiments, timings t1 for sequential runs on single core were measured using the C++ implementation described in [6], which worked identically and faster than our X10 version.\nExperiment Environments. Two sets of experiments were operated using (1) a shared-memory machine equipped with 40 cores (four of 10-core Intel Xeon E7-4860 2.26GHz) and 256GB of local memory and (2) up to 256 cores of SGI UV1000, a pseudo-shared memory machine equipped with 2,048 cores (8-core Xeon E7-8837 2.67GHz) and 16TB of memory. UV1000 works as a single shared memory machine by emulating memory accesses using communication based on a high speed NUMAlink5 network which has a bandwidth of 120Gbps. We used the MPI backend of X10 with options X10 NTHREADS = 6 and GC NPROCS = 2.\nExperiments on a Shared Memory Machine. We solved the problems shown in [6,3] using 40 cores of the machine (1). We report the results for two representative problems. Parameters in the load balancing method were set as either combination of the following values: nb = 32, |N | \u2208 {2, 4}, ns \u2208 {10, 100, 1000}, and \u2206 = 10. We also computed with and without the preprocess (when the preprocess is not used, the postprocess is executed from the beginning). For each problem, we solved two instances with two multiplicative precisions. The specification of each instance and the computational results are presented in Table 1. In the table, the columns \u201cproblem\u201d, \u201csize\u201d, \u201c\u01eb\u201d, \u201cpp\u201d, and \u201c|N |\u201d represent the name of the problem, size (i.e., the number of projection/parameter variables), the precision, usage of the preprocess, and the number of neighbor workers, respectively. The rest of the columns represents the results. t1 and #br1 represent the running time and the number of branches on single core. tns,i and #brns,i represent the running time and the largest number of branches performed by a worker when computed with the interval ns and i X10 places (best timings are underlined). Figure 4 illustrates the speedup of the solving process.\nExperiments on a Cluster with High-speed Interconnection. We solved the problem \u201c3rpr\u201d using up to 256 cores of the machine (2), UV1000. Parameters in the load balancing method were set as either combination of the following values: nb = 8, |N | \u2208 {2, 4}, ns = 1000, and \u2206 = 10. The results are presented in Table 2. Each column of the table represents the same information as presented in Table 1 except that the column \u201c#sends1000,256\u201d represents the number of loads sent by the load balancer in the solving process with ns = 1000 and 256 X10 places. Figure 5 illustrates the speedup of the solving process."}, {"heading": "4.1 Discussions", "text": "In the experiments, our method scaled up to 256 cores with the optimal configurations. We achieved speedups up to 32.3 fold using 40 cores of the shared memory machine and up to 119 fold using 256 cores of the cluster machine.\nThe best speedup of 119 fold was obtained with the preprocess. The preprocess facilitates and accelerates the workload distribution in the early stage\nof the search process. In some of the experiments without using the preprocess, the speedup ratio became saturated when using many cores (e.g., sp2-4 with ns = 1000 and the experiments on the cluster). This was because the load balancing process was too infrequent for the given number of workers and the work load diffusion became too slow. When comparing the right-hand graphs for the instance sp2-4, ns = 1000, in Figure 4, we can notice that the point of saturation shifts according to the search space size. On the other hand, in some other experiments, the results got worse with the preprocess (e.g., results with ns = 10 on the machine (1)). It occasionally happens that the preprocess mostly solves the problem. However, the preprocess can result in highly unbalanced search trees because of the Prune process, and in such cases the postprocess will not have enough time for load balancing.\nRegarding the neighborhood sizes |N | = 2, 4, there was a trade off between the workloads balance and the amount of communications required. For the\nshared memory machine, it was unclear which size had the advantage. However, for UV1000, the solver was notably slower for |N | = 4 than |N | = 2. It is understandable because larger number of neighbors significantly increased the number of communications (see \u201c#sends1000,256\u201d in Table 2) and communications between places were much more costly compared to normal shared memory machines despite the high speed network of UV1000.\nThree intervals ns = 10, 100, 1000 were used for load balancing which determined the speed of workload distribution. When the distribution was too slow, the speedup ratio did not scale well (e.g., 3rpr, \u01eb = 0.2, with ns = 1000 on the machine (1)). Conversely, small intervals required greater amount of communications and therefore we used ns = 1000 to draw better performance on the cluster where communications were more costly.\nThere was a large overhead caused by the workers sending a large number of boxes for load balancing when the number of workers was not sufficient against the problem size. Speedups for 3rpr, \u01eb = 0.1, using 40 workers or less shows an example of such overheads (Figure 4(d)). Resolving this overhead by suppressing redundant box sends is a part of the future work."}, {"heading": "5 Conclusions", "text": "In this paper, we proposed a parallel branch and prune algorithm, based on, non-portfolio, search-space splitting approach. In the experiments, using 256 X10 places (i.e., cores), we achieved speedup factors of as much as 119. We expect that our parallelized solver will be applied to large practical problems, e.g., the robotics problems in [3].\nAcknowledgments. This work was partially funded by JSPS (KAKENHI 25880008 and 25700038). Computing resources for the experiments in this paper were provided by Prof. Kazunori Ueda (Waseda University, Tokyo) and a Compute Canada RAC award, for environments (1) and (2), respectively."}], "references": [{"title": "Revising Hull and Box Consistency", "author": ["F. Benhamou", "F. Goualard", "L. Granvilliers", "J.F. Puget"], "venue": "Proc. of ICLP. pp. 230\u2013244", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Experiments with Massively Parallel Constraint Solving", "author": ["L. Bordeaux", "Y. Hamadi", "H. Samulowitz"], "venue": "Proc. of IJCAI. pp. 443\u2013448", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A branch and prune algorithm for the computation of generalized aspects of parallel robots", "author": ["S. Caro", "D. Chablat", "A. Goldsztejn", "D. Ishii", "C. Jermann"], "venue": "Artificial Intelligence 211, 34\u201350", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "X10: an object-oriented approach to nonuniform cluster computing", "author": ["P. Charles", "C. Grothoff", "V. Saraswat"], "venue": "Proc. of OOPSLA. pp. 519\u2013538", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Confidence-based work stealing in parallel constraint programming", "author": ["G. Chu", "C. Schulte", "P. Stuckey"], "venue": "Proc. of CP. pp. 226\u2013241. LNCS 5732", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Interval-based projection method for underconstrained numerical systems", "author": ["D. Ishii", "A. Goldsztejn", "C. Jermann"], "venue": "Constraints Journal 17(4), 432\u2013460", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A Preliminary Review of Literature on Parallel Constraint Solving", "author": ["I.P. Gent", "C. Jefferson", "I. Miguel", "N.C.A. Moore", "P. Nightingale", "P. Prosser", "C. Unsworth"], "venue": "Proc. of Workshop on Parallel Methods for Constraint Solving. 13 pp.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Box consistency through adaptive shaving", "author": ["A. Goldsztejn", "F. Goualard"], "venue": "Proc. of SAC. pp. 2049\u20132054", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Introduction to Parallel Computing", "author": ["A. Grama", "A. Gupta", "G. Karypis", "V. Kumar"], "venue": "Addison Wesley", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Algorithm 852: RealPaver: an interval solver using constraint satisfaction techniques", "author": ["L. Granvilliers", "F. Benhamou"], "venue": "ACM Transactions on Mathematical Software 32(1), 138\u2013156", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "A conservative scheme for parallel interval narrowing", "author": ["L. Granvilliers", "G. Hains"], "venue": "Information Processing Letters 74(3-4), 141\u2013146", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Scalable distributed depth-first search with greedy work stealing", "author": ["J. Jaffar", "A. Santosa", "R. Yap", "K. Zhu"], "venue": "Proc. of ICTAI. pp. 98\u2013103", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Mapping Tree-Structured Combinatorial Optimization Problems onto Parallel Computers", "author": ["R. L\u00fcling", "B. Monien", "A. Reinefeld", "S. Tsch\u00f6ke"], "venue": "Proc. of Solving Combinatorial Optimization Problems in Parallel. pp. 115\u2013144. LNCS 7141", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Towards Parallel Search for Optimization in Graphical Models", "author": ["L. Otten", "R. Dechter"], "venue": "Proc. of ISAIM. 8 pp.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "PaMiraXT: Parallel SAT Solving with Threads and Message Passing", "author": ["T. Schubert", "M. Lewis", "B. Becker"], "venue": "JSAT 6, 203\u2013222", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Parallel search made simple", "author": ["C. Schulte"], "venue": "Proc. of TRICS. pp. 41\u201357", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Solving Polynomial Systems Using a Branch and Prune Approach", "author": ["P. Van Hentenryck", "D. McAllester", "D. Kapur"], "venue": "SIAM Journal on Numerical Analysis 34(2), 797\u2013 827", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Massively Parallel Constraint Programming for Supercomputers : Challenges and Initial Results", "author": ["F. Xie", "A. Davenport"], "venue": "Proc. of CPAIOR. pp. 334\u2013338. LNCS 6140", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "1 Introduction Numerical constraint satisfaction problems (NCSPs, Section 2) have been successfully applied to problems described in the domain of reals[3,6].", "startOffset": 152, "endOffset": 157}, {"referenceID": 5, "context": "1 Introduction Numerical constraint satisfaction problems (NCSPs, Section 2) have been successfully applied to problems described in the domain of reals[3,6].", "startOffset": 152, "endOffset": 157}, {"referenceID": 11, "context": "Parallel solving of generic CSPs on massive computer clusters and supercomputers has been explored in [12,2,18].", "startOffset": 102, "endOffset": 111}, {"referenceID": 1, "context": "Parallel solving of generic CSPs on massive computer clusters and supercomputers has been explored in [12,2,18].", "startOffset": 102, "endOffset": 111}, {"referenceID": 17, "context": "Parallel solving of generic CSPs on massive computer clusters and supercomputers has been explored in [12,2,18].", "startOffset": 102, "endOffset": 111}, {"referenceID": 6, "context": "In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf.", "startOffset": 14, "endOffset": 17}, {"referenceID": 15, "context": "In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf.", "startOffset": 86, "endOffset": 103}, {"referenceID": 11, "context": "In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf.", "startOffset": 86, "endOffset": 103}, {"referenceID": 1, "context": "In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf.", "startOffset": 86, "endOffset": 103}, {"referenceID": 4, "context": "In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf.", "startOffset": 86, "endOffset": 103}, {"referenceID": 14, "context": "In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf.", "startOffset": 86, "endOffset": 103}, {"referenceID": 17, "context": "In the survey [7], existing work is classified into (i) search-space splitting methods[16,12,2,5,15,18], (ii) cooperative methods for heterogeneous solver agents (cf.", "startOffset": 86, "endOffset": 103}, {"referenceID": 1, "context": "portfolios)[2], and (iii) parallelization of constraint propagation.", "startOffset": 11, "endOffset": 14}, {"referenceID": 10, "context": "works have used approaches (ii)[11] and (iii)[8] for parallelization of NCSP solving.", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "works have used approaches (ii)[11] and (iii)[8] for parallelization of NCSP solving.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "Substantial work regarding the parallelization of the branch and bound algorithm with search-space splitting exists [9,13,14].", "startOffset": 116, "endOffset": 125}, {"referenceID": 12, "context": "Substantial work regarding the parallelization of the branch and bound algorithm with search-space splitting exists [9,13,14].", "startOffset": 116, "endOffset": 125}, {"referenceID": 13, "context": "Substantial work regarding the parallelization of the branch and bound algorithm with search-space splitting exists [9,13,14].", "startOffset": 116, "endOffset": 125}, {"referenceID": 15, "context": "This approach has also been applied to CSP solvers [16,5] and SAT solvers [15].", "startOffset": 51, "endOffset": 57}, {"referenceID": 4, "context": "This approach has also been applied to CSP solvers [16,5] and SAT solvers [15].", "startOffset": 51, "endOffset": 57}, {"referenceID": 14, "context": "This approach has also been applied to CSP solvers [16,5] and SAT solvers [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "This work explores an efficient parallel method for solving NCSPs with similar approach to [9,13].", "startOffset": 91, "endOffset": 97}, {"referenceID": 12, "context": "This work explores an efficient parallel method for solving NCSPs with similar approach to [9,13].", "startOffset": 91, "endOffset": 97}, {"referenceID": 16, "context": "The branch and prune algorithm [17] is the standard solving method for NCSPs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "In this work, we use an implementation proposed in [6] which provides a verification process based on an interval Newton method combined with a relatively simple filtering process based on the Hull consistency[1].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "In this work, we use an implementation proposed in [6] which provides a verification process based on an interval Newton method combined with a relatively simple filtering process based on the Hull consistency[1].", "startOffset": 209, "endOffset": 212}, {"referenceID": 8, "context": "4 of [9]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "1)[4], a high productivity language for parallel computing.", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": "1)[10] for sequential NCSP solving and Gaol (ver.", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": "In the experiments, timings t1 for sequential runs on single core were measured using the C++ implementation described in [6], which worked identically and faster than our X10 version.", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "We solved the problems shown in [6,3] using 40 cores of the machine (1).", "startOffset": 32, "endOffset": 37}, {"referenceID": 2, "context": "We solved the problems shown in [6,3] using 40 cores of the machine (1).", "startOffset": 32, "endOffset": 37}], "year": 2014, "abstractText": "We present a parallel solver for numerical constraint satisfaction problems (NCSPs) that can scale on a number of cores. Our proposed method runs worker solvers on the available cores and simultaneously the workers cooperate for the search space distribution and balancing. In the experiments, we attained up to 119-fold speedup using 256 cores of a parallel computer.", "creator": "LaTeX with hyperref package"}}}