{"id": "1411.4491", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Joint cross-domain classification and subspace learning for unsupervised adaptation", "abstract": "Domain adaptation aims at adapting a prediction function trained on a source domain, for a new different but related target domain. Recently several subspace learning methods have proposed adaptive solutions in the unsupervised case, where no labeled data are available for the target. Most of the attention has been dedicated to searching a new low-dimensional domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks. We also propose to test a simple differential learning algorithm with the target domain in order to test how it is optimal for generating a real data representation for all types of subsets. In this study we propose to use the same technique for non-supervised representations such as the predicted and sub-gradient gradient descent for the target domain. We provide a differential-learning algorithm with the target domain in order to test the model performance on the non-supervised domain by performing a cross-domain search. Finally, we discuss how we can test the same non-supervised algorithm with the target domain in order to compare the target domain with the predicted model.", "histories": [["v1", "Mon, 17 Nov 2014 14:29:35 GMT  (1963kb,D)", "http://arxiv.org/abs/1411.4491v1", null], ["v2", "Tue, 18 Nov 2014 15:55:50 GMT  (1963kb,D)", "http://arxiv.org/abs/1411.4491v2", "Paper is under consideration at Pattern Recognition Letters"], ["v3", "Wed, 29 Apr 2015 02:51:00 GMT  (1980kb,D)", "http://arxiv.org/abs/1411.4491v3", "Paper is under consideration at Pattern Recognition Letters"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["basura fernando", "tatiana tommasi", "tinne tuytelaars"], "accepted": false, "id": "1411.4491"}, "pdf": {"name": "1411.4491.pdf", "metadata": {"source": "CRF", "title": "Joint cross-domain classification and subspace learning", "authors": ["Basura Fernando", "Tatiana Tommasi", "Tinne Tuytelaars"], "emails": [], "sections": [{"heading": null, "text": "Domain adaptation aims at adapting a prediction function trained on a source domain, for a new different but related target domain. Recently several subspace learning methods have proposed adaptive solutions in the unsupervised case, where no labeled data are available for the target. Most of the attention has been dedicated to searching a new lowdimensional domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks."}, {"heading": "1 Introduction", "text": "In real world applications having a probability distribution mismatch between the training and the test data is more often the rule than an exception. Think about part of speech tagging across different text corpora [5], localization over time with wifi signal distributions that get easily outdated [33], or biological models to be used across different subjects [31]. Computer vision methods are also particularly challenged in this respect: real world conditions may alter the image statistics in many complex ways (lighting, pose, background, motion blur etc.), to not even mention the difference in quality of the acquisition device\n(e.g. resolution), or the high number of possible artificial modifications obtained by post-processing (e.g. filtering). Due to this large variability, any learning algorithm trained on a source set regardless of the final target data will most likely produce poor, unsatisfactory results.\nDomain Adaptation (DA) techniques propose to overcome these issues and make use of information coming from both source and target domains during the learning process to adapt the classifier automatically. In the unsupervised case, where no labeled samples are provided for the target, one of the most extensively studied paradigms consists in assuming the existence of a domain-invariant low dimensional feature space and searching for it. In general all the techniques based on this idea focus on transforming the representation of the source and target samples to maximize some notion of similarity between them [12, 13, 10]. However in this way the classification task is left aside and the prediction model is learned only in a second stage. As thoroughly discussed in [2, 24], the choice of the feature representation able to reduce the domain divergence is indeed a crucial factor for the adaptation. Nevertheless it is not the only one. What would happen if we project the source and target data into a low dimensional space that makes the two domains very much alike but loses the discriminative information? As an extreme case, both the domains can be projected to the exact same uninformative point. A common latent subspace where the difference between the marginal distributions of the two domains is minimized can be identified disregarding the conditional distribution of the data, which however, is essential for training a discriminative classifier that generalizes from source\nar X\niv :1\n41 1.\n44 91\nv1 [\ncs .C\nV ]\n1 7\nN ov\nto target. If it happens that in several subspaces the domains have similar marginals, would a classifier perform equally well in all of them? Here we answer these questions by presenting an algorithm that learns jointly both a low dimensional representation and a reliable classifier by optimizing a trade-off between the source-target similarity and the source training error."}, {"heading": "2 Related Work", "text": "DA has received a lot of attention in the last years and there are two main scenarios studied in the literature. In the semi-supervised setting a few labeled samples are provided for the target domain besides a large amount of annotated source data. In the unsupervised setting, the available target samples are unlabeled.\nIn the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17]. The unlabeled part of the target has also been used for co-regularization [21] with the aim of better integrate the source and the target classifiers.\nIn the more challenging unlabeled setup most of the approaches resort to the estimating the data distributions and minimizing a distance measure between them while re-weighting/selecting the samples [31, 11] or while looking for a new representation [1]. The Maximum Mean Discrepancy (MMD) [14] which maps two sets of data to a reproducing Kernel Hilbert Space has been largely used as distance measure between two distributions. Although endowed with nice properties, the choice of the kernel and kernel parameters is critical and if non-optimal can lead to very poor estimate of the distribution distance [15]. Dictionary learning methods have also been used with the goal of defining new representations that overcome the domain shift [25, 29]. Another promising direction is that of subspace modeling. As for dictionary learning, the approaches presented in this framework are mostly linear, but can be easily extended to\nnon-linear spaces through explicit feature mappings [32].\nIn [4] Canonical Correlation Analysis (CCA) has been applied to find a coupled domain-invariant subspace. Principal Component Analysis (PCA) is also widely used for subspace generation and several methods have proposed to exploit intermediate subspaces to link the source and the target data. This idea was introduced in [13] where the path across the domains is defined as a geodesic curve over a Grassmann manifold. This approach has been further extended in [12] where all the intermediate subspaces are integrated to define a cross-domain similarity measure. Despite the intuitive characterization of the problem, it is not clear why all the subspaces along this path should yield meaningful representations. Recently the Subspace Alignment (SA) method [10] demonstrated that it is possible to map directly from the source to the target subspace without necessarily passing through intermediate steps. Overall, the main focus of all the unsupervised subspace methods is on the domain invariance of the final data representation and less attention has been dedicated to its discriminative power. A first attempt in this direction has been done in [12] by substituting the use of PCA over the source subspace with Partial Least Squares (PLS).\nOur work fits in this context and go over coding discriminativeness in the subspace representation. We propose here to integrate subspace and max-margin learning, jointly looking for the optimal low-dimensional projection and for the optimal classifier in an unsupervised DA setting. We underline here that, although solutions similar in spirit have been exploited in the semi-supervised setup [18, 7], we consider the specific case of subspace modeling and the unsupervised case where no access to the target labels is available, not even for hyperparameter cross validation.\nIn the following sections we define the notation that will be used in the rest of the paper (section 3) and we briefly revise the theory of learning from different domains together with the SA method on which we build. We then introduce our approach (section 4) followed by an extensive experimental analysis that shows its effectiveness on several do-\nmain adaptation tasks (section 5). We conclude with a final discussion and sketching possible directions for future research (section 6)."}, {"heading": "3 Problem Setup and Back-", "text": "ground\nLet us consider a classification problem where the data instances are in the form (xi, yi). Here xi \u2208 RD is the feature vector for the i-th sample and yi \u2208 {1, . . . , C} is the corresponding label. We assume that ns labeled training samples are drawn from a source distribution, while a set of nt unlabeled test samples come from a different target distribution such that for the joint probabilities Ds = P (xs, ys), Dt = P (xt, yt) it holds Ds 6= Dt.\nA bound on target domain error Theoretical studies on DA allowed to establish the conditions in which a classifier trained on the source data can be expected to perform well on the target data. Specifically, the following generalization bound on the target error t has been demonstrated in [2]:\nt(h) \u2264 s(h) + dH(Ds,Dt) + \u03bb . (1)\nHere h indicates the predictor function, while H is the hypothesis class from which the predictor has been chosen. In words, the bound states that a low target error can be guaranteed if the source error s(h), a measure of the domain distribution divergence dH(Ds,Dt), and the error \u03bb of the ideal joint hypothesis on the two domains are small. Specifically with h\u2217 = argminh\u2208H( t(h) + s(h)) the joint error is \u03bb = t(h \u2217)+ s(h \u2217) and its value is supposed to be low if adaptation is possible. All the three terms on the right hand side of (1) contribute to the adaptability of the classifier.\nSubspace Alignment (SA) Given the source and the target domains, we can specify their lowdimensional intrinsic structure by the corresponding orthonormal basis set. Specifically we use S \u2208 RD\u00d7d for the source subspace and T \u2208 RD\u00d7d for the target subspace where d is the subspace dimensionality.\nWhen using PCA, the basis correspond to the top d eigenvectors of the data\u2019s covariance matrix. Alternatively, discriminative subspaces can be determined by using the source labels with PLS or Linear Discriminant Analysis (LDA). In this last case the resulting subspace dimensionality depends on the number of classes by d = C \u2212 1.\nThe domain shift can by reduced by aligning the associated basis sets and minimizing the following Bregman matrix divergence [10]\nF (M) = ||SM \u2212 T ||2F . (2)\nSince the Frobenius norm ||\u00b7 ||F is invariant to orthonormal operations, we can apply S> to both the terms of the difference and demonstrate that the optimal transformation matrix has a closed form solution M = S>T \u2208 Rd\u00d7d. Thus, it is possible to define a new target aligned coordinate system U = SM = SS>T where U \u2208 RD\u00d7d . This approach has shown promising results for visual crossdomain classification tasks outperforming other subspace adaptive methods [10]. However, on par with its competitors [13, 12], it keeps the domain adaptive process (learning M) and the classification process (e.g. learning the SVM model) separated, focusing only on the distribution divergence term of the bound in (1)."}, {"heading": "4 Proposed Approach", "text": "Under the covariate shift assumption we can consider the error \u03bb in (1) as negligible. Thus, to optimize the target performance, we can concentrate on the two remaining terms on the right hand side of the bound and search jointly for a subspace representation that minimizes the domain divergence and for the best source classification model in the same space. We name our method Joint cross-domain Classification and Subspace Learning (JCSL) and we presents its details below.\nGiven a fixed target subspace basis T \u2208 RD\u00d7d we\nminimize the following regularized risk functional F (U,w) = ||w||22+\u03b1||U\u2212T ||2F +\u03b2 ns\u2211 i L(xsi , ysi ,w, U) (3) Here w \u2208 Rd is a linear source classifier, while U \u2208 RD\u00d7d is a matrix that projects the source data to the target aligned subspace. In the defined objective function, the regularization terms aim at separately optimizing over the two variables w and U , while the loss function L leverages over their combination. In particular we use a domain adaptive regularization for U following the idea of the SA method. However, in our case it is not necessary to specify a priori the source subspace S which is now optimized together with the alignment transformation matrix M in a single step. Note that if the source and target data can be considered as belonging to the same domain (no domain shift) our method will automatically provide U = T boiling down to standard learning in the shared subspace. We followed previous literature in using PCA to define the target subspace T [10, 12]. Besides having demonstrated empirical good results, this choice has a theoretical motivation. We can write the mutual information between the target and the source as MI(source; target) = H(target) \u2212KL(source||target) , where KL(\u00b7 ||\u00b7 ) is the Kullback-Leibler divergence and H(\u00b7 ) is the entropy. Projecting the target data to the subspace T maximizes the entropy H(target), while our objective function (3) minimizes the domain divergence. Hence, we expect to get a high mutual information between source and target.\nFor our analysis we focus on classification tasks and we choose the hinge loss: L(xsi , ysi ,w, U) = max{0, 1 \u2212 xsi\n>Uwysi }, but other loss functions can be used for different cross-domain applications. Minimizing (3) jointly over (U,w) is a non-convex problem and finding a global optimum is generally intractable. However we can apply alternated minimization for U and w resulting in two interconnected convex problems that can be efficiently solved by stochastic subgradient descent. The iterative procedure terminates when the algorithm converges, showing a negligible change of either U or w between two consecutive iterations. The formulation\nholds for a binary classifier but can easily be used in its one-vs-all multiclass extension that highly benefits from the choice of the stochastic variant of the optimization process. If we indicate the classification score of class y for the target sample xti with s(xti ,wy) = x t i > Twy , the final prediction can be written as y\u2217i = argmaxy(s(x t i ,wy)). The learning strategy is summarized in Algorithm 1.\nAlgorithm 1 JCSL\nInput: step size \u03b7 and batch size \u03b3 for stochastic sub-gradient descent Output: U\u2217,w\u2217\n1: Initialize U \u2190 S,w\u2190 0, k \u2190 0 2: while not converged do 3: k \u2190 k + 1 4: calculate the partial derivatives:\n5: \u2202F (U,w) \u2202U = 2\u03b1(U \u2212 T )\u2212 \u03b2\u03a3\u03b3i=1\u0393i\n6: \u0393i =\n{ xsi >wysi if (xi\n>Uwyi) < 1 0 otherwise\n7: \u2202F (U,w) \u2202w = 2w \u2212 \u03b2\u03a3\u03b3i=1\u0398i\n8: \u0398i =\n{ xsi >Uysi if (xi\n>Uwyi) < 1 0 otherwise\n9: Fix U , identify the optimal w: 10: wk \u2190 wk\u22121 \u2212 \u03b7 ( \u2202F (U,w) \u2202w ) wk\u22121\n11: Fix w, identify the optimal U : 12: Uk \u2190 Uk\u22121 \u2212 \u03b7 ( \u2202F (U,w) \u2202U ) Uk\u22121 13: end while"}, {"heading": "5 Experiments", "text": "We validate our approach over several domain adaptation tasks. In the following we first describe our experimental setting and then we report on the obtained results with a detailed analysis on how JCSL effectively optimizes over the source classification error and the cross-domain similarity."}, {"heading": "5.1 Datasets, baselines and implementation details", "text": "We choose three image datasets (see Figure 1) and a wifi signal dataset.\nOffice + Caltech [12]. This dataset was created by combining the Office dataset [28] with Caltech256 [16] and it contains images of 10 object classes over four domains: Amazon, Dslr, Webcam and Caltech. Amazon consists of images from online merchants\u2019 catalogues, while Dslr and Webcam domains are composed by respectively high and low resolution images. Finally, Caltech corresponds to a subset of the original Caltech256. We use the features provided by Gong et al. [12] already used in several previous publications: SURF descriptors quantized into histograms of 800 bag-of-visual words and standardized by z-score normalization. All the 12 possible sourcetarget domain pairs are considered. We use the data splits provided by Hoffman et al. [18].\nMNIST [22] + USPS [19]. This dataset combines two existing image collections of digits present-\ning different gray scale data distributions. Specifically they share 10 classes of digits. We randomly selected 1800 images from USPS and 2000 images from MNIST. By following [23] we uniformly re-scale all images to size 16 \u00d7 16 and we use the gray-scale pixel values as feature vectors. Both domains are alternatively used as source and target.\nBing+Caltech [3]. In this dataset, weakly annotated images from the Bing search engine define the source domain while images of Caltech256 are used as target. We run experiments varying the number of categories (5, 10, 15, 20, 25 and 30) and the number of source examples per category (5 and 10) using the same train/test split adopted in [3]. As typically done for this dataset, Classemes features are used as image representation [3].\nWiFi [33]. This dataset was used in the 2007 IEEE ICDM contest for domain adaptation. The goal is to estimate the location of mobile devices based on the received signal strength (RSS) values from different access points. The domains correspond to two different time periods during which the collected RSS\nvalues may have different distributions. The dataset contains 621 labeled examples collected during time period A (source) and 3128 unlabeled examples collected during time period B (target). There are 248 locations in total, each considered as a class. All source samples are used during the training and the classification accuracy is measured using all test samples.\nWe benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10]. As a preliminary evaluation we compared the results obtained by GFK and SA when the basis of the source and target subspaces were obtained with PLS and LDA. Although performing similarly on average, PLS shows less stability than LDA with large changing in the outcome for small variations of the subspace dimensionality d. This can be explained considering the difficulty of finding the best d that jointly maximizes the source data/label coherence and minimizes the source/target shift. Thus, for our experiments we rely on the more stable LDA for the source. On the\nother hand, the target subspace is always obtained by applying PCA. For all the methods the final classifier is a linear SVM with the C parameter cross-validated on the source over the range {0.001, 0.01, 0.1, 1.0, 10}. As a further baseline we also consider the source classifier learned with no adaptation in the original feature space (NA) and in the target subspace (PCAT ).\nOur JCSL has three main parameters (\u03b1, \u03b2, d) that are chosen through cross validation on the source. We searched for \u03b1, \u03b2 in the same range indicated before for C. The parameter d was tuned in {10, 20, . . . , 100} both for JCSL and for the baselines PCAT and TSL. We implemented the stochastic sub-gradient descent using a step size of \u03b7 = 0.1 and a batch size of \u03b3 = 10. The alternating optimization converges for less than 100 iterations and we can obtain the results for any of the source-target pairs of the Office+Caltech in 2 minutes using a modern desktop computer (2.8GHz cpu, 4Gb of ram, 1 core)."}, {"heading": "5.2 Results - Office+Caltech and MNIST+USPS", "text": "The obtained results over the Office+Caltech and MNIST+USPS datasets are presented in Table 1.\nOverall JCSL outperforms the considered baselines in 8 source-target pairs out of 14 and shows the best average results over the two datasets. Thus, we can state that by minimizing a trade-off between source-target similarity and the source classification error pays off compared to only reducing the crossdomain representation divergence. Still SA shows an advantage with respect to JCSL in a few of the considered cases most probably because it can exploit the discriminative LDA subspace. With respect to JCSL , TCA seems to work particularly well when the domain shift is small (i.e. Amazon \u2192 Caltech, Dslr \u2192 Webcam). Interestingly JCSL is the only method that consistently outperforms NA over MNIST+USPS.\nParameter analysis To better understand the performance of JCSL we analyze how the target ac-\ncuracy varies with respect to the source accuracy while changing the learning parameters \u03b1, \u03b2 and d. The plots in Figure 2 consider four domain adaptation problems, namely (Amazon \u2192 Caltech), (Amazon \u2192 Webcam), (MNIST \u2192 USPS) and (USPS \u2192 MNIST)1. All of them present two main clusters. On the left, when the source accuracy is low, the target accuracy is uniformly distributed. This behavior mostly appears when \u03b2 is very small and \u03b1 has a high value: this indicates that minimizing only ||U \u2212 T ||2F does not guarantee stable results on the target task. On the other hand, in the second cluster the source accuracy is highly correlated with the target accuracy. On average for the points in this region both the domain divergence term and the misclassification\n1Analogous results are obtained for all the remaining source-target pairs.\nTable 2: H\u2206H analysis. Lower values indicate lower cross-domain distribution discrepancy.\nSpace A\u2192C A\u2192W MNIST\u2192USPS USPS\u2192MNIST Original features 74.82 90.18 100.00 100.00\nSA 65.96 56.56 55.78 55.74 JCSL 65.76 54.97 57.03 53.28\nloss obtain low values. The final JCSL result with the optimal (U\u2217,w\u2217) always appear in this area. Overall the dimensionality of the subspace d seems to have only a moderate influence on the final results indicating that the choice of this parameter is less relevant for JCSL than what is for the other subspace adaptive methods. The red line reported on the plots is obtained by least-square fitting over the source and target accuracies and presents an analogous trend for all the considered source-target pairs. This is an indication that when domains are adaptable (negligible \u03bb in (1)) our method is able to find a good source classifier as well as a source representation that generalizes to the target domain.\nMeasuring the domain shift For the same domain pairs considered above we also evaluate empirically the H\u2206H divergence measure defined in [2]. This is obtained by learning a linear SVM that discriminates between the source and target instances, respectively pseudo-labeled with +1 and \u22121. We separated each domain into two halves and use them for training and test when learning a linear SVM model. A high final accuracy indicates high domain divergence. Due to the considered one-vs-all multipleclass setting, our JCSL learns class-specific source subspaces Uy, so we calculate the average H\u2206H divergence and compare it with the divergence obtained when using the original features and the SA target aligned subspace representation. The results in Table 2 indicate that SA and JCSL produce comparable results in terms of domain-shift reduction suggesting that the main advantage of JCSL comes from the learned classifier.\nTable 3: Classification accuracy obtained over WiFi localization dataset [33].\nNA SA GFK TCA JCSL 16.6 17.3 17.3 19.0 20.2"}, {"heading": "5.3 Results - Bing+Caltech", "text": "Due to the way in which it was defined, Bing+Caltech can be considered as a much more challenging testbed for unsupervised domain adaptation compared to the other used datasets (see also Figure 1). At the same time it also corresponds to one of the most realistic scenarios where domain adaptation is needed: we have access to only a limited number of noisy labeled source images obtained from the web and we want to use them to classify over a curated collection of object images. For this problem exploiting at the best all the available information is crucial. Specifically, since the source is not fully reliable, coding its discriminative information in the representation (e.g. through LDA or PLS) may be misleading. On the other hand, using the subspace of the non-noisy target data to guide the learning process can be much more beneficial.\nAs shown in Figure 3, JCSL is the only method that consistently improves over the non-adaptive approach independently from the number of considered classes. TSL is always equivalent to NA, while the other subspace methods, although initially helpful for problems with few classes, lose their advantage over NA when the number of classes increases. This behavior is almost equivalent when using both 5 and 10 source samples per class."}, {"heading": "5.4 Results - WiFi Localization", "text": "To demonstrate the generality of the proposed algorithm, we evaluate JCSL also on non-visual data. Specifically, the results on the WiFi-localization task are reported in table 3. The obtained classification accuracy confirm the value of our method over the other subspace-based techniques."}, {"heading": "6 Conclusions", "text": "Motivated by the theoretical results of Ben-David et al. [2], in this paper we proposed to integrate the learning process of the source prediction function with the optimization of the invariant subspace for unsupervised domain adaptation. Specifically, JCSL learns a representation that minimizes the Bregman matrix divergence between the source subspace and the target subspace while optimizing the classification model. Extensive experimental results have shown that, by taking advantage of the described principled combination and without the need of passing through the evaluation of the data distributions, JCSL outperform several other subspace domain adaptation methods that focus only on the representation part. Recently several works have demonstrated that Convolutional Neural Network performance are robust to domain shift [6, 26]. Reasoning at high level we can identify the cause of such a robustness on the same idea at the basis of JCSL : deep architectures learn jointly a discriminative representation and the prediction function. The highly nonlinear transformation of the original data coded into the CNN activation values can also be used as input data descriptors for JCSL with the aim of obtaining a combined effect. As future work we plan to evaluate principled ways to find automatically the best subspace dimensionality d using low-rank optimization methods."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of the FP7 EC project AXES."}], "references": [{"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["Mahsa Baktashmotlagh", "Mehrtash Harandi", "Brian Lovell", "Mathieu Salzmann"], "venue": "In ICCV 2013,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Analysis of representations for domain adaptation", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": "In NIPS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach", "author": ["Alessandro Bergamo", "Lorenzo Torresani"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Domain adaptation with coupled subspaces", "author": ["John Blitzer", "Dean Foster", "Sham Kakade"], "venue": "In AISTATS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "CoRR, abs/1310.1531:1\u20138,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Domain transfer multiple kernel learning", "author": ["Lixin Duan", "Ivor W. Tsang", "Dong Xu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Domain transfer svm for video concept detection", "author": ["Lixin Duan", "Ivor W. Tsang", "Dong Xu", "Stephen J. Maybank"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["Basura Fernando", "Amaury Habrard", "Marc Sebban", "Tinne Tuytelaars"], "venue": "In ICCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["Boqing Gong", "Yuan Shi", "Fei Sha", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "Ruonan Li", "R. Chellappa"], "venue": "In ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A kernel method for the two sample problem", "author": ["Arthur Gretton", "Karsten Borgwardt", "Malte Rasch", "Bernhard Schlkopf", "Alexander Smola"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "A kernel two-sample test", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte J. Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Caltech-256 object category dataset", "author": ["Gregory Griffin", "Alex Holub", "Pietro Perona"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Discovering latent domains for multisource domain adaptation", "author": ["Judy Hoffman", "Brian Kulis", "Trevor Darrell", "Kate Saenko"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Efficient learning of domain-invariant image representations", "author": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Kate Saenko", "Trevor Darrell"], "venue": "In ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Co-regularization based semisupervised domain adaptation", "author": ["Abhishek Kumar", "Avishek Saha", "Hal Daume"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["Mingsheng Long", "Jianmin Wang", "Guiguang Ding", "Jiaguang Sun", "P.S. Yu"], "venue": "In ICCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In COLT,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Subspace interpolation via dictionary learning for unsupervised domain adaptation", "author": ["Jie Ni", "Qiang Qiu", "Rama Chellappa"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Domain adaptation via transfer component analysis", "author": ["Sinno Jialin Pan", "Ivor W. Tsang", "James T. Kwok", "Qiang Yang"], "venue": "In IJCAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Adapting visual category models to new domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "In ECCV,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Generalized domain-adaptive dictionaries", "author": ["Sumit Shekhar", "Vishal M. Patel", "Hien V. Nguyen", "Rama Chellappa"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Bregman divergence-based regularization for transfer subspace learning", "author": ["Si Si", "Dacheng Tao", "Bo Geng"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A two-stage weighting framework for multi-source domain adaptation", "author": ["Qian Sun", "Rita Chattopadhyay", "Sethuraman Panchanathan", "Jieping Ye"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "Pattern Analysis and Machine Intellingence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Estimating location using wi-fi", "author": ["Qiang Yang", "Sinno Jialin Pan", "Vincent Wenchen Zheng"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Think about part of speech tagging across different text corpora [5], localization over time with wifi signal distributions that get easily outdated [33], or biological models to be used across different subjects [31].", "startOffset": 65, "endOffset": 68}, {"referenceID": 31, "context": "Think about part of speech tagging across different text corpora [5], localization over time with wifi signal distributions that get easily outdated [33], or biological models to be used across different subjects [31].", "startOffset": 149, "endOffset": 153}, {"referenceID": 29, "context": "Think about part of speech tagging across different text corpora [5], localization over time with wifi signal distributions that get easily outdated [33], or biological models to be used across different subjects [31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "In general all the techniques based on this idea focus on transforming the representation of the source and target samples to maximize some notion of similarity between them [12, 13, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 11, "context": "In general all the techniques based on this idea focus on transforming the representation of the source and target samples to maximize some notion of similarity between them [12, 13, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 8, "context": "In general all the techniques based on this idea focus on transforming the representation of the source and target samples to maximize some notion of similarity between them [12, 13, 10].", "startOffset": 174, "endOffset": 186}, {"referenceID": 1, "context": "As thoroughly discussed in [2, 24], the choice of the feature representation able to reduce the domain divergence is indeed a crucial factor for the adaptation.", "startOffset": 27, "endOffset": 34}, {"referenceID": 22, "context": "As thoroughly discussed in [2, 24], the choice of the feature representation able to reduce the domain divergence is indeed a crucial factor for the adaptation.", "startOffset": 27, "endOffset": 34}, {"referenceID": 18, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 179, "endOffset": 187}, {"referenceID": 26, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 179, "endOffset": 187}, {"referenceID": 7, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 232, "endOffset": 242}, {"referenceID": 16, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 232, "endOffset": 242}, {"referenceID": 6, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 232, "endOffset": 242}, {"referenceID": 15, "context": "In the first case, by knowing the class to which some target data belong, it is possible to impose similarity and dissimilarity constraints across samples through metric learning [20, 28], or optimize a classifier over both domains [9, 18, 7] even with extensions to cases where more than two domains are available [8, 17].", "startOffset": 315, "endOffset": 322}, {"referenceID": 19, "context": "The unlabeled part of the target has also been used for co-regularization [21] with the aim of better integrate the source and the target classifiers.", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "In the more challenging unlabeled setup most of the approaches resort to the estimating the data distributions and minimizing a distance measure between them while re-weighting/selecting the samples [31, 11] or while looking for a new representation [1].", "startOffset": 199, "endOffset": 207}, {"referenceID": 9, "context": "In the more challenging unlabeled setup most of the approaches resort to the estimating the data distributions and minimizing a distance measure between them while re-weighting/selecting the samples [31, 11] or while looking for a new representation [1].", "startOffset": 199, "endOffset": 207}, {"referenceID": 0, "context": "In the more challenging unlabeled setup most of the approaches resort to the estimating the data distributions and minimizing a distance measure between them while re-weighting/selecting the samples [31, 11] or while looking for a new representation [1].", "startOffset": 250, "endOffset": 253}, {"referenceID": 12, "context": "The Maximum Mean Discrepancy (MMD) [14] which maps two sets of data to a reproducing Kernel Hilbert Space has been largely used as distance measure between two distributions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "Although endowed with nice properties, the choice of the kernel and kernel parameters is critical and if non-optimal can lead to very poor estimate of the distribution distance [15].", "startOffset": 177, "endOffset": 181}, {"referenceID": 23, "context": "Dictionary learning methods have also been used with the goal of defining new representations that overcome the domain shift [25, 29].", "startOffset": 125, "endOffset": 133}, {"referenceID": 27, "context": "Dictionary learning methods have also been used with the goal of defining new representations that overcome the domain shift [25, 29].", "startOffset": 125, "endOffset": 133}, {"referenceID": 30, "context": "As for dictionary learning, the approaches presented in this framework are mostly linear, but can be easily extended to non-linear spaces through explicit feature mappings [32].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "In [4] Canonical Correlation Analysis (CCA) has been applied to find a coupled domain-invariant subspace.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "This idea was introduced in [13] where the path across the domains is defined as a geodesic curve over a Grassmann manifold.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "This approach has been further extended in [12] where all the intermediate subspaces are integrated to define a cross-domain similarity measure.", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "Recently the Subspace Alignment (SA) method [10] demonstrated that it is possible to map directly from the source to the target subspace without necessarily passing through intermediate steps.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "A first attempt in this direction has been done in [12] by substituting the use of PCA over the source subspace with Partial Least Squares (PLS).", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "We underline here that, although solutions similar in spirit have been exploited in the semi-supervised setup [18, 7], we consider the specific case of subspace modeling and the unsupervised case where no access to the target labels is available, not even for hyperparameter cross validation.", "startOffset": 110, "endOffset": 117}, {"referenceID": 6, "context": "We underline here that, although solutions similar in spirit have been exploited in the semi-supervised setup [18, 7], we consider the specific case of subspace modeling and the unsupervised case where no access to the target labels is available, not even for hyperparameter cross validation.", "startOffset": 110, "endOffset": 117}, {"referenceID": 1, "context": "Specifically, the following generalization bound on the target error t has been demonstrated in [2]:", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "The domain shift can by reduced by aligning the associated basis sets and minimizing the following Bregman matrix divergence [10]", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "This approach has shown promising results for visual crossdomain classification tasks outperforming other subspace adaptive methods [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "However, on par with its competitors [13, 12], it keeps the domain adaptive process (learning M) and the classification process (e.", "startOffset": 37, "endOffset": 45}, {"referenceID": 10, "context": "However, on par with its competitors [13, 12], it keeps the domain adaptive process (learning M) and the classification process (e.", "startOffset": 37, "endOffset": 45}, {"referenceID": 8, "context": "We followed previous literature in using PCA to define the target subspace T [10, 12].", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "We followed previous literature in using PCA to define the target subspace T [10, 12].", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "Office + Caltech [12].", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "This dataset was created by combining the Office dataset [28] with Caltech256 [16] and it contains images of 10 object classes over four domains: Amazon, Dslr, Webcam and Caltech.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "This dataset was created by combining the Office dataset [28] with Caltech256 [16] and it contains images of 10 object classes over four domains: Amazon, Dslr, Webcam and Caltech.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "[12] already used in several previous publications: SURF descriptors quantized into histograms of 800 bag-of-visual words and standardized by z-score normalization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "MNIST [22] + USPS [19].", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "MNIST [22] + USPS [19].", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "By following [23] we uniformly re-scale all images to size 16 \u00d7 16 and we use the gray-scale pixel values as feature vectors.", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "Bing+Caltech [3].", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "We run experiments varying the number of categories (5, 10, 15, 20, 25 and 30) and the number of source examples per category (5 and 10) using the same train/test split adopted in [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "As typically done for this dataset, Classemes features are used as image representation [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 31, "context": "WiFi [33].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 188, "endOffset": 192}, {"referenceID": 8, "context": "We benchmark JCSL against four state of the art subspace-based DA methods, namely Geodesic Flow Kernel (GFK) [12], Transfer Subspace Learning (TSL) [30], Transfer Component Analysis (TCA) [27] and the Subspace Aligment method (SA) [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 1, "context": "Measuring the domain shift For the same domain pairs considered above we also evaluate empirically the H\u2206H divergence measure defined in [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 31, "context": "Table 3: Classification accuracy obtained over WiFi localization dataset [33].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "[2], in this paper we proposed to integrate the learning process of the source prediction function with the optimization of the invariant subspace for unsupervised domain adaptation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Recently several works have demonstrated that Convolutional Neural Network performance are robust to domain shift [6, 26].", "startOffset": 114, "endOffset": 121}, {"referenceID": 24, "context": "Recently several works have demonstrated that Convolutional Neural Network performance are robust to domain shift [6, 26].", "startOffset": 114, "endOffset": 121}], "year": 2017, "abstractText": "Domain adaptation aims at adapting a prediction function trained on a source domain, for a new different but related target domain. Recently several subspace learning methods have proposed adaptive solutions in the unsupervised case, where no labeled data are available for the target. Most of the attention has been dedicated to searching a new lowdimensional domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks.", "creator": "LaTeX with hyperref package"}}}