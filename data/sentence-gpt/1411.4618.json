{"id": "1411.4618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Relations World: A Possibilistic Graphical Model", "abstract": "We explore the idea of using a \"possibilistic graphical model\" as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user's family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model.\n\n\nThe concept of a world model is not new. We have explored how information is expressed and distributed over time. In the 1960s, we developed a new form of \"inter-linear\" model (i.e. the system is a world), based on the model being a world. In the 1970s, we applied the concept of the world model to the concept of the current world model, in which we derive the world model. Since the advent of the Internet, however, there have been few attempts at modeling and interpretation of the system.\nWe are developing a new model that uses the concept of a world model that uses text-based text-based dialog, and has shown us that the system can infer, in turn, a relational system that learns about, and learns about, a relational system that does not necessarily follow the rules of the world model. In a new article, we show how the world model uses text-based dialog to extract data from and derive a system of the user's family relations.\nWe will focus on one specific example that we explore on a new concept: a dynamic world model that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about, and learns about, a relational system that learns about", "histories": [["v1", "Mon, 17 Nov 2014 20:15:00 GMT  (850kb)", "http://arxiv.org/abs/1411.4618v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["christopher j c burges", "erin renshaw", "andrzej pastusiak"], "accepted": false, "id": "1411.4618"}, "pdf": {"name": "1411.4618.pdf", "metadata": {"source": "CRF", "title": "Relations World: A Possibilistic Graphical Model1", "authors": ["Christopher J.C. Burges", "Erin Renshaw"], "emails": [], "sections": [{"heading": null, "text": "system. As a first step we have developed a system that uses text-based dialog to derive a model of the user\u2019s family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model."}, {"heading": "Introduction", "text": "Recently we listed some desiderata that appear to us to be sensible design principles in the quest to develop\nthe machine comprehension of text [1]. We summarize them here:\n1. Inference should leverage world models which are kept\n2. as simple as possible such that\n3. they can be combined in a modular fashion. This should help us keep the design\n4. scalable, in three senses: learning time, inference time, and portability to arbitrary domains. Learning\nshould be\n5. correctable, so that mistakes the system makes can be corrected without introducing new mistakes\nelsewhere,\n6. interpretable, so that for example developers can easily understand what inferences the system has\nmade, and all the reasoning it has done,\n7. generative, so that the system can ask the user for more information about the data it would benefit\nmost from knowing, and\n8. interrogable, so that the user can ask if the system believes something and get an answer in real time.\n(1) and (2) are uncontroversial: a world model is often needed to resolve ambiguities in language. For\nexample, in the sentence Sam is my father and I have a brother named Sam, a family relations model is needed\nto infer that the two Sams must refer to different entities. Regarding (3), modularity, and composability of\nthose modules, is a fundamental principle of software design but is also expected to be a key design principle\nfor large semantic models [2]. (4), especially portability, is an open research question. (5) is a key property that\ndistinguishes human learning from machine learning (ML). Most ML approaches do not address this issue at\nall. Errors are corrected by adding more training data, or by designing better features, or by finding a better\nmodel, but all of these approaches will in general introduce new errors on the original data; a human analog\nwould be that one consequence of learning to ride a bicycle is that a child forgets how to brush his teeth. In\nthis sense (3) is not addressed by ML either: a human analog would be a teacher correcting a student\u2019s\nmisunderstanding by locking him in a library with terabytes of data for a week and asking him to update his\nparameters. Instead, the teacher can correct the student\u2019s misunderstanding by exchanging a small number of\n1 This work will appear in the 2014 NIPS Workshop on Learning Semantics.\nbits of information, correcting a specific component (module) of the student\u2019s world model. It seems unlikely\nthat the brain is moving global decision surfaces around in some high dimensional space. This also argues\nstrongly for (1), i.e. a rich shared world model, which even allows entities to model each others\u2019 models. ML\nmodels (neural nets, trees, ensembles of these, and even moderately sized probabilistic graphical models) are\nnot easily interpretable (see for example [3]): in general we do not understand why our statistical systems make\nthe errors they do. ML achieves (7) to some extent through active learning, but not in the sense of maximally\nreducing the uncertainty in a semantic world model. Similarly while ML approaches (8) by simply applying the\nmodel to unseen data, it does not do so in a modular fashion, so that for example there are no semantic\ncomponents in the model that we can interrogate individually.\nThe approach described here, although only a preliminary step, meets all these desiderata except for (4), which we hope to investigate as the models grow in complexity. We call our simple world models meaning projections, where a given meaning projection extracts meaning about one, or a very few, aspects of a piece of text (for example, spatial information about the entities involved), and can interact with other meaning projections to both correct upstream errors (for example, coreference resolution) and to make its own inferences. The idea is roughly analogous to the \u201cagents\u201d in Minsky\u2019s Society of Mind [4]. Our first step, as outlined in this paper, is a dialog system2 that maps English text to model family relations. The work can be viewed as a semantic modeling analog of Blocks World [4,5], in that it takes a simple task and uses it to explore the ideas. Although the machine is learning, there is currently no statistical machine learning involved;\nrather than using probabilistic graphical models, uncertainties are modeled using graphs and sets, and\nrobustness is achieved by carefully tracking the sources of the uncertainty and polling the user directly when\nnecessary. Clearly statistical methods will need to be added at some point, and one goal of this research is to\nunderstand how to incorporate them while meeting the above desiderata. Here, we concentrate on an approach called possibilistic graphical models3 (pGMs) that are a simple instantiation of classical relational logic\n[6], but with data structures chosen to meet the above desiderata. The uncertainty in the model is exposed\nexplicitly and actionably. While logic problems involving family relations have been given as homework\nexercises since the beginning of AI, we\u2019ve found it a useful test bed to focus on, in particular to generate dialog\ndriven by the uncertainty in the model, and to explore growing and correcting the model through dialog."}, {"heading": "A Possibilistic Graphical Model", "text": "We explain pGMs by describing how we use them to model family relations. Assume that we are given a set of \ud835\udc5b animate entities (\u201cAEs\u201d) (usually, but not necessarily, people4), denoted below by \ud835\udc38\ud835\udc56 , \ud835\udc56 = 1, \u2026 , \ud835\udc5b. Each AE has an associated name and gender which may or may not be known. The AEs are not necessarily unique: for example, upstream natural language processing may have in error separated entities which are in fact the same. It is also possible that what is presented to the system as a single AE may in fact refer to more than one, due to upstream coreference resolution errors: the system can also (often) detect and correct these kinds of error. The pGM is a directed multigraph with a node for each AE and an edge set for the possible relations between those two AEs. Thus an edge set represents our current state of knowledge about the possible relations that could hold for the two endpoints: it is assumed that exactly one of the relations is correct, but we do not know which. The pGM grows as more entities are encountered in the text, and its\n2 A demo of the system will be presented at the workshop. 3 We use pGM to emphasize that possibilistic graphical models are precursors to probabilistic GMs (PGMs). 4 For example when modeling fiction, one might have to model the fact that objects which in the real world are inanimate are treated in the text as though they are animate, with gender, name, mood, etc.\nedge sets shrink (to a minimum cardinality of one) as more information concerning its entities is discovered. Our pGM is populated from binary predicates (we use the term \u201crelations\u201d interchangeably), for example Mother(Anne,Bill), derived from natural text using a few simple pattern matching rules; from unary predicates (e.g. IsFemale(Anne)) derived using Census data and user input; and consistency rules, such as gender propagation (where for example spouseOf would be resolved to wifeOf if its first argument is discovered to be female5; the second argument would be set to male; the outgoing relations of the latter might then be resolved from parentOf to fatherOf; and so on). While this graph constitutes our evolving knowledge base, we also use a (hand crafted) fixed knowledge base matrix \ud835\udc40 that captures relations between relations. Both rows and columns of \ud835\udc40 are indexed by a relation, which is a member of the fixed set of relations \u211d that we wish to model. Our current system has \u211d = {Grandparent, Parent, Parent-in-Law, Spouse, Sibling, Sibling-in-Law, Child, Child-in-Law, Grandchild, Aunt or Uncle, Niece or Nephew, Cousin, Self, OutOfGraph} together with the gender specific versions of these. We will denote any subset of \u211d by \u211b and an individual element by \ud835\udc45. Self is used to model the fact that some entities that are passed to the system as different may in reality be the same (thus enabling the system to correct coreference errors). OutOfGraph, which we denote below by \ud835\udc450, is necessary to model the fact that we know that an induced relation may not be in the fixed knowledge base.\nThe graph encodes relations as follows. Let \u211b\ud835\udc56\ud835\udc57 be a set of relations such that for exactly one element \ud835\udc45 \u2208 \u211b\ud835\udc56\ud835\udc57, we have that \ud835\udc45(\ud835\udc38\ud835\udc56 , \ud835\udc38\ud835\udc57) = \ud835\udc47; we do not know which element this is, unless |\u211b\ud835\udc56\ud835\udc57| = 1. These sets of relations are captured in a matrix \ud835\udc40 such that \ud835\udc40\ud835\udc451\ud835\udc452 = \u211b 12, which we also write, for any entities \ud835\udc381, \ud835\udc382, \ud835\udc383, as\n\ud835\udc45(1)(\ud835\udc381, \ud835\udc382) \u2227 \ud835\udc45 (2)(\ud835\udc382, \ud835\udc383) \u2192 \u211b (12)(\ud835\udc381, \ud835\udc383) (1)\nSometimes the set \u211b(12) will be a singleton (e.g. Parent(\ud835\udc34, \ud835\udc35) \u2227 Sibling(\ud835\udc35, \ud835\udc36) \u2192 Parent(\ud835\udc34, \ud835\udc36)), but it need not be (e.g. Cousin(\ud835\udc34, \ud835\udc35) \u2227 Cousin(\ud835\udc35, \ud835\udc36) \u2192 \u211b(\ud835\udc34, \ud835\udc36) where \u211b = {Cousin, Self, Sibling, \ud835\udc450}). Every relation is assumed to have a unique inverse: that is, for any \ud835\udc381 and \ud835\udc382, and for every predicate \ud835\udc45, there is a unique predicate \ud835\udc45\u22121 : \ud835\udc45(\ud835\udc381, \ud835\udc382) \u21d4 \ud835\udc45 \u22121(\ud835\udc382, \ud835\udc381). Thus (1) also implies the set \u211b (21)(\ud835\udc383, \ud835\udc381) which is in 1-1 correspondence with the set \u211b(12)(\ud835\udc381, \ud835\udc383) and where each element of \u211b (21) is the inverse of an element in \u211b(12). We use \u211b\ud835\udc56\ud835\udc57 to denote the set of edges from node \ud835\udc38\ud835\udc56 to node \ud835\udc38\ud835\udc57 and \u211b\ud835\udc57\ud835\udc56 to denote the corresponding set of inverse edges. Equation (1) may also be written with sets on the left, in which case the\nright hand side is the union of all such implications: that is, we use \ud835\udc40(\u211b\ud835\udc56\ud835\udc57, \u211b\ud835\udc57\ud835\udc58) as shorthand for \u22c3 \ud835\udc40(\ud835\udc451, \ud835\udc452)\ud835\udc451\u2208\u211b\ud835\udc56\ud835\udc57,\ud835\udc452\u2208\u211b\ud835\udc57\ud835\udc58 .\nGraphs are always either fully connected, or are composed of fully connected subgraphs. There are two kinds of updates. The first takes an existing graph, updates one of the edges by reducing its multiplicity (to at least one), and propagates the inferred edge set reductions, recursively. Thus suppose that we have a 3- clique6 \u211b\ud835\udc56\ud835\udc57, \u211b\ud835\udc57\ud835\udc58, \u211b\ud835\udc56\ud835\udc58, and that new information leads us to replace \u211b\ud835\udc56\ud835\udc57 by \u211b\ud835\udc56\ud835\udc57 \u2032 \u2282 \u211b\ud835\udc56\ud835\udc57. For example, if \u211b12 = {Cousin, Sibling, Self, \ud835\udc450}, then one initial update might be that we have learned that \ud835\udc381 is not the self of \ud835\udc382. The first step of the recursion is then either to replace \u211b\ud835\udc56\ud835\udc58 by \u211b\ud835\udc56\ud835\udc58 \u2032 \u2254 \ud835\udc40(\u211b\ud835\udc56\ud835\udc57 \u2032 , \u211b\ud835\udc57\ud835\udc58) \u2229 \u211b\ud835\udc56\ud835\udc58, or to replace \u211b\ud835\udc57\ud835\udc58 by \u211b\ud835\udc57\ud835\udc58 \u2032 \u2254 \ud835\udc40(\u211b\ud835\udc57\ud835\udc56, \u211b\ud835\udc56\ud835\udc58) \u2229 \u211b\ud835\udc57\ud835\udc58 (we prove below that it doesn\u2019t matter which we do first). The second kind\n5 The system currently models \u201ctraditional\u201d family structures to keep the analysis simple. Relations resulting from, for example, same sex marriages, or step-relations, are easily added. 6 We use the term \u201c3-clique\u201d rather than \u201ctriangle\u201d because the structure is more general than a geometric construct.\nof update is when an edge is added to join previously disconnected graphs, which after applying inference results in a single fully connected graph. One can think of any relation as always being added or removed simultaneously with its inverse relation.\nWe emphasize that inference in this system is simply using relational logic [6]. Our intent here is to examine data structures that support the above desiderata, in particular correctability and interpretability, through the ability to track uncertainty explicitly, efficiently and actionably. Thus, for example, a pGM yields all possible relations between any two entities by simply examining the edge set connecting those two entities. To this end we provide some analysis of pGMs here.\nDefinition 1: We overload the \u2192 symbol as follows: for any two edges \ud835\udc451, \ud835\udc452 from different edge sets and which join at node \ud835\udc38 in a 3-clique, we write \ud835\udc451 \u2192 \ud835\udc452 to mean that there exists an \ud835\udc45 from the third edge set (or its inverse) such that the following holds: for \ud835\udc451, \ud835\udc452 both terminating on \ud835\udc38, \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc45, \ud835\udc451); for both \ud835\udc451 and \ud835\udc452 starting at \ud835\udc38, \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc451, \ud835\udc45); for \ud835\udc451 ending at \ud835\udc38 and \ud835\udc452 starting there, \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc451 \u22121, \ud835\udc45); and for \ud835\udc451 starting at \ud835\udc38 and \ud835\udc452 ending there, \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc45, \ud835\udc451 \u22121). We read \ud835\udc451 \u2192 \ud835\udc452 as \u201c\ud835\udc451 supports \ud835\udc452\u201d and we define \ud835\udc45 \u2192 \u211b to mean that every edge in \u211b is supported by \ud835\udc45, and \u211b1 \u2192 \u211b2 to mean that every edge in \u211b2 is supported by at least one edge in \u211b1.\nNote that the empty set is supported by any pair of relations. Note also that if \ud835\udc45 is supported by another edge in a 3-clique, it does not mean that \ud835\udc45 must occur, just that its occurrence is not inconsistent with the other edges in the clique.\nDefinition 2: A 3-clique with (different) vertex indices \ud835\udc4e, \ud835\udc4f, \ud835\udc50 \u2208 \u2115 is stable if \ud835\udc45\ud835\udc56\ud835\udc57 \u2286 \ud835\udc40(\ud835\udc45\ud835\udc56\ud835\udc58 , \ud835\udc45\ud835\udc58\ud835\udc57) \u2200 {\ud835\udc56, \ud835\udc57, \ud835\udc58} \u2208 \ud835\udc43\ud835\udc45\ud835\udc40{\ud835\udc4e, \ud835\udc4f, \ud835\udc50}, that is, if every edge set is supported by the other two. A fully-connected graph is stable if all of its three-cliques are stable.\nNote that, given the definition of the inverse of an edge, and since \ud835\udc40 is just the encapsulation of the logical assertion (1), we have that \ud835\udc40(\ud835\udc451, \ud835\udc452) = \u211b3 \u27fa \ud835\udc40(\ud835\udc452 \u22121, \ud835\udc451 \u22121) = \u211b3 \u22121. That is, for predicates \ud835\udc5d\ud835\udc56, if \ud835\udc5d1 \u2227 \ud835\udc5d2 \u21d2 \ud835\udc5d3 \u2228 \ud835\udc5d4 \u2228 \u2026 \u2228 \ud835\udc5d\ud835\udc5b and if \ud835\udc5d\ud835\udc56 \u21d4 \ud835\udc5e\ud835\udc56 then \ud835\udc5e1 \u2227 \ud835\udc5e2 \u21d2 \ud835\udc5e3 \u2228 \ud835\udc5e4 \u2228 \u2026 \u2228 \ud835\udc5e\ud835\udc5b, where here the predicates \ud835\udc5d are the relations and the predicates \ud835\udc5e, their inverses. Note also that since an edge set \u211b\ud835\udc56\ud835\udc57 is supported by \u211b\ud835\udc56\ud835\udc58 and \u211b\ud835\udc58\ud835\udc57 iff \u211b\ud835\udc57\ud835\udc56 is supported by \u211b\ud835\udc57\ud835\udc58 and \u211b\ud835\udc58\ud835\udc56, we can compute support by going either way around the clique.\nAxioms:\nA1. No element of \ud835\udc40 is the empty set (every pair of edges has implications). A2. Every individual relation, including \ud835\udc450, has a unique inverse. A3. Consistency conditions: let us assume that \ud835\udc40(\ud835\udc451, \ud835\udc452) = \u211b3. From A1, \u211b3 is non-empty. Then:\na. \u2200 \ud835\udc45 \u2208 \u211b3, \ud835\udc451 \u2208 \ud835\udc40(\ud835\udc45, \ud835\udc452 \u22121) and \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc451 \u22121, \ud835\udc45). b. \u2200 \ud835\udc45\u22121 \u2208 \u211b3 \u22121, \ud835\udc452 \u22121 \u2208 \ud835\udc40(\ud835\udc45\u22121, \ud835\udc451) and \ud835\udc451 \u22121 \u2208 \ud835\udc40(\ud835\udc452, \ud835\udc45 \u22121).\nA3 (a) is a consistency condition because if it were not so, then given \ud835\udc451 and \ud835\udc452, the resulting set \ud835\udc40(\ud835\udc451, \ud835\udc452) could be reduced to a single edge (for example, by the addition of new information) that implies the absence of \ud835\udc451 or of \ud835\udc452. Similarly, if A3 (b) were not so, then the edge set \ud835\udc40(\ud835\udc452 \u22121, \ud835\udc451 \u22121) could be reduced to a single edge that implied the removal of \ud835\udc451 \u22121 or \ud835\udc452 \u22121.\nLemma 1: Joining nodes 1 and 3 by adding an edge set \u211b13, computed as \u211b12 \u2227 \u211b23, does not have the consequence of reducing the existing edge sets \u211b12 and \u211b23. Proof: This follows directly from axiom A3. \u25a1\nLemma 2: For a given 3-clique, suppose that \ud835\udc451 belongs to one edge set and \ud835\udc452 to another. Then \ud835\udc451 \u2192 \ud835\udc452 if and only if \ud835\udc452 \u2192 \ud835\udc451. Proof: Clearly \ud835\udc451 and \ud835\udc452 belong to adjacent edge sets. Let their mutual node by \ud835\udc34. There are three possible situations: \ud835\udc451 and \ud835\udc452 both end on \ud835\udc34, \ud835\udc451 and \ud835\udc452 both start at \ud835\udc34, or one starts and one ends on \ud835\udc34 (these two are made equivalent by relabeling). If both end on \ud835\udc34, then \ud835\udc451 \u2192 \ud835\udc452 means that there exists an \ud835\udc453 on the remaining edge such that \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc453, \ud835\udc451). By A3 we have that \ud835\udc451 \u2208 \ud835\udc40(\ud835\udc453 \u22121, \ud835\udc452) so \ud835\udc452 \u2192 \ud835\udc451. Symmetry and swapping labels 1 \u27f7 2 shows that similarly, \ud835\udc452 \u2192 \ud835\udc451 implies \ud835\udc451 \u2192 \ud835\udc452. If both start on A, then (\ud835\udc451 \u2192 \ud835\udc452) \u21d2 \u2203\ud835\udc453 \u2236 \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc451, \ud835\udc453) and by A3, \ud835\udc451 \u2208 \ud835\udc40(\ud835\udc452, \ud835\udc453 \u22121) or \ud835\udc452 \u2192 \ud835\udc451. Symmetry again gives the converse result. Finally suppose that \ud835\udc451 ends on \ud835\udc34 and \ud835\udc452 begins on \ud835\udc34. Then (\ud835\udc451 \u2192 \ud835\udc452) \u21d2 \u2203\ud835\udc453 \u2236 \ud835\udc452 \u2208 \ud835\udc40(\ud835\udc451 \u22121, \ud835\udc453) and by"}, {"heading": "A3, \ud835\udc451", "text": "\u22121 \u2208 \ud835\udc40(\ud835\udc452, \ud835\udc453 \u22121) or \ud835\udc452 \u2192 \ud835\udc451 since \ud835\udc451 has unique inverse that also holds. Similarly (\ud835\udc452 \u2192 \ud835\udc451) \u21d2\n\u2203\ud835\udc453 \u2236 \ud835\udc451 \u2208 \ud835\udc40(\ud835\udc453, \ud835\udc452 \u22121) and by A3, \ud835\udc452 \u22121 \u2208 \ud835\udc40(\ud835\udc453 \u22121, \ud835\udc451) so \ud835\udc451 \u2192 \ud835\udc452 . \u25a1\nNote that Lemma 2 does not assume that the clique is stable.\nThe \u2192 relation is thus symmetric at the single edge level: support is mutual. A weaker (non-symmetric) condition holds for the mapping from an edge to an edge set:\nLemma 3: For a 3-clique with edge set \u211b\ud835\udc56\ud835\udc57, let \ud835\udc45 \u2208 \u211b\ud835\udc56\ud835\udc57. Then \ud835\udc45 \u2192 \u211b implies \u211b \u2192 \ud835\udc45. Proof: \ud835\udc451 \u2192 \u211b means that \u2200\ud835\udc452 \u2208 \u211b, \ud835\udc451 \u2192 \ud835\udc452. By lemma 2 we have that \ud835\udc452 \u2192 \ud835\udc451 \u2200\ud835\udc452 \u2208 \u211b which is also written as \u211b \u2192 \ud835\udc45. \u25a1\nThe assertions in Lemmas 2 and 3 do not extend to general subsets. For example, for edge \ud835\udc45 and edge set \u211b, it is not necessarily true that \u211b \u2192 \ud835\udc45 implies that \ud835\udc45 \u2192 \u211b since \u211b may be a superset of the edges supported by \ud835\udc45. A fortiori, for two edge sets \u211b and \ud835\udcae, \u211b \u2192 \ud835\udcae does not necessarily imply that \ud835\udcae \u2192 \u211b.\nThe lemmas and theorems below assume that we start with a stable 3-clique.\nLemma 4: Suppose that a 3-clique has some edge set \u211b with subset \u211b\u2032, and another edge set \ud835\udcae with subset \ud835\udcae\u2019. Then removal of \u211b\u2019 implies removal of \ud835\udcae\u2019 if and only if \ud835\udcae\u2019 only supports edges in \u211b\u2019. Proof: If \ud835\udcae\u2019 supports only edges in \u211b\u2019, then it does not support any edges in \u211b \u2216 \u211b\u2032 and hence, since support is mutual, \ud835\udcae\u2019 is not supported by any edges in \u211b \u2216 \u211b\u2032; hence removal of \u211b\u2019 implies removal of \ud835\udcae\u2019. If on the other hand the removal of \u211b\u2019 implies the removal of \ud835\udcae\u2019, then there are no edges in \u211b \u2216 \u211b\u2032 that support \ud835\udcae\u2019, and since support is mutual there are no edges in \ud835\udcae\u2019 that support \u211b \u2216 \u211b\u2032, that is, \ud835\udcae\u2019 only supports edges in \u211b\u2019. \u25a1\nDenoting the removal of some set \ud835\udcae by \ud835\udc5f\ud835\udc52\ud835\udc5a(\ud835\udcae), again the generalization to arbitrary sets does not hold for removal: that is, given \ud835\udc5f\ud835\udc52\ud835\udc5a(\ud835\udcae) \u2192 \ud835\udc5f\ud835\udc52\ud835\udc5a(\ud835\udc451), it may not be the case that \ud835\udc5f\ud835\udc52\ud835\udc5a(\ud835\udc451) \u2192 \ud835\udc5f\ud835\udc52\ud835\udc5a(\ud835\udcae) since \ud835\udcae may contain elements not implied by \ud835\udc451.\nLemma 5: Suppose that removal of \ud835\udc45 \u2208 \u211b\ud835\udc56\ud835\udc57 implies the removal of \ud835\udcae: that is, \u2200 \ud835\udc46 \u2208 \ud835\udcae, \ud835\udc46 \u2208 \ud835\udc40(\u211b\ud835\udc56\ud835\udc57 , \u211b\ud835\udc57\ud835\udc58) and \ud835\udc46 \u2209 \ud835\udc40(\u211b\ud835\udc56\ud835\udc57 \u2216 \ud835\udc45, \u211b\ud835\udc57\ud835\udc58). This alone cannot result in the removal of another edge \ud835\udc45\u2032 \u2208 \u211b\ud835\udc56\ud835\udc57. Proof: By Lemma 4, removal of \ud835\udcae can result in the removal of \ud835\udc45\u2032 only if \ud835\udc45\u2032 supports some edges in \ud835\udcae. But if so\nthen \u2203\ud835\udc46 \u2208 \ud835\udcae \u2236 \ud835\udc46 \u2208 \ud835\udc40(\u211b\ud835\udc56\ud835\udc57 \u2216 \ud835\udc45, \u211b\ud835\udc57\ud835\udc58). \u25a1\nLemma 6: Suppose that, for a given 3-clique \ud835\udc36 with nodes {\ud835\udc56, \ud835\udc57, \ud835\udc58}, and given arbitrary edge sets \u211b\ud835\udc56\ud835\udc57 and \u211b\ud835\udc57\ud835\udc58, we set \u211b\ud835\udc56\ud835\udc58 = \ud835\udc40(\u211b\ud835\udc56\ud835\udc57, \u211b\ud835\udc57\ud835\udc58). Then all resulting implications in \ud835\udc36 result in no changes to any edge in \ud835\udc36. Proof: Consider the set \ud835\udcae\ud835\udc56\ud835\udc57 \u2254 \ud835\udc40(\ud835\udc45\ud835\udc56\ud835\udc58 , \ud835\udc45\ud835\udc58\ud835\udc57). There can be no edge \ud835\udc45 \u2208 \u211b\ud835\udc56\ud835\udc57 such that \ud835\udc45 \u2209 \ud835\udcae\ud835\udc56\ud835\udc57 since by Axiom 1, such an \ud835\udc45 would support at least one relation in \u211b\ud835\udc56\ud835\udc58, and by Lemma 2, that relation supports \ud835\udc45; hence \u211b\ud835\udc56\ud835\udc57 \u2286 \ud835\udc40(\ud835\udc45\ud835\udc56\ud835\udc58 , \ud835\udc45\ud835\udc58\ud835\udc57). Similarly consider \ud835\udcae\ud835\udc57\ud835\udc58 \u2254 \ud835\udc40(\u211b\ud835\udc57\ud835\udc56 , \u211b\ud835\udc56\ud835\udc58). There can be no edge \ud835\udc45 \u2208 \u211b\ud835\udc57\ud835\udc58 such that \ud835\udc45 \u2209 \ud835\udcae\ud835\udc57\ud835\udc58 because by Axioms 1 and 2, \ud835\udc45\u22121 would support at least one relation in \u211b\ud835\udc58\ud835\udc56, and by Lemma 2, that relation would accept \ud835\udc45\u22121; hence \u211b\ud835\udc57\ud835\udc58 \u2286 \ud835\udc40(\u211b\ud835\udc57\ud835\udc56, \u211b\ud835\udc56\ud835\udc58). \u25a1\nWe now address the question of efficiently computing the full set of resultant edges in a clique when some edges are removed (due, for example, to the availability of new information). The figure below shows that reductions in one set of edges can induce reductions in another, which can then induce reductions in the third: can this process continue? (In other words, must we recurse to completion?) The following theorem shows that the answer is no.\nTheorem 1: Given a stable 3-clique, represent its edge sets by \u211b\ud835\udc56\ud835\udc57, \u211b\ud835\udc57\ud835\udc58, \u211b\ud835\udc56\ud835\udc58. Suppose that one or more edges are removed from \u211b\ud835\udc56\ud835\udc57 to give a new edge set \u211b\ud835\udc56\ud835\udc57 \u2032 . Suppose that \u211b\ud835\udc56\ud835\udc58 is updated via \u211b\ud835\udc56\ud835\udc58 \u2032 = \ud835\udc40(\u211b\ud835\udc56\ud835\udc57 \u2032 , \u211b\ud835\udc57\ud835\udc58) \u2229 \u211b\ud835\udc56\ud835\udc58, and that \u211b\ud835\udc57\ud835\udc58 is then updated via \u211b\ud835\udc57\ud835\udc58 \u2032 = \ud835\udc40(\u211b\ud835\udc57\ud835\udc56 \u2032 , \u211b\ud835\udc56\ud835\udc58 \u2032 ) \u2229 \u211b\ud835\udc57\ud835\udc58. Then these changes in \u211b\ud835\udc57\ud835\udc58 and \u211b\ud835\udc56\ud835\udc58 do not induce any further changes in the clique, the resulting clique is stable, and the result does not depend on the order of operations (that is, the result is the same if \u211b\ud835\udc57\ud835\udc58 is updated first and \u211b\ud835\udc56\ud835\udc58 second).\nProof: Let \ud835\udcae\ud835\udc56\ud835\udc57 = \u211b\ud835\udc56\ud835\udc57 \u2216 \u211b\ud835\udc56\ud835\udc57 \u2032 .denote the set of edges removed from \u211b\ud835\udc56\ud835\udc57. Let \ud835\udcae\ud835\udc56\ud835\udc58 denote the set of edges that are not supported by \u211b\ud835\udc56\ud835\udc57 \u2032 (that is, \ud835\udcae\ud835\udc56\ud835\udc58 is the maximal set such that \ud835\udcae\ud835\udc56\ud835\udc58 \u2282 \u211b\ud835\udc56\ud835\udc58 and \ud835\udcae\ud835\udc56\ud835\udc58 \u2229 \ud835\udc40(\u211b\ud835\udc56\ud835\udc57 \u2032 , \u211b\ud835\udc57\ud835\udc58) = \u2205) so that the removal of \ud835\udcae\ud835\udc56\ud835\udc57 implies the removal of \ud835\udcae\ud835\udc56\ud835\udc58. Similarly let \ud835\udcae\ud835\udc57\ud835\udc58 denote the set of edges that are simultaneously not supported by \u211b\ud835\udc56\ud835\udc57 \u2032 (that is, \ud835\udcae\ud835\udc57\ud835\udc58 is the maximal set such that \ud835\udcae\ud835\udc57\ud835\udc58 \u2282 \u211b\ud835\udc57\ud835\udc58 and \ud835\udcae\ud835\udc57\ud835\udc58 \u2229 \ud835\udc40(\u211b\ud835\udc57\ud835\udc56 \u2032 , \u211b\ud835\udc56\ud835\udc58) = \u2205) so that the removal of \ud835\udcae\ud835\udc56\ud835\udc57 implies the removal of \ud835\udcae\ud835\udc57\ud835\udc58. Also define \u211b\ud835\udc57\ud835\udc58 \u2032 \u2254 \u211b\ud835\udc57\ud835\udc58 \u2216 \ud835\udcae\ud835\udc57\ud835\udc58 and \u211b\ud835\udc56\ud835\udc58 \u2032 \u2254 \u211b\ud835\udc56\ud835\udc58 \u2216 \ud835\udcae\ud835\udc56\ud835\udc58. Now suppose that there were an edge \ud835\udc45 \u2208 \u211b\ud835\udc57\ud835\udc58 \u2032 whose removal is implied by the reduced set \u211b\ud835\udc56\ud835\udc58 \u2032 , that is, \ud835\udc45 \u2209 \ud835\udc40(\u211b\ud835\udc57\ud835\udc56 \u2032 , \u211b\ud835\udc56\ud835\udc58 \u2032 ). Then since \ud835\udc45 \u2208 \ud835\udc40(\u211b\ud835\udc57\ud835\udc56 \u2032 , \u211b\ud835\udc56\ud835\udc58) we must have that \ud835\udc45 \u2208 \ud835\udc40(\u211b\ud835\udc57\ud835\udc56 \u2032 , \ud835\udcae\ud835\udc56\ud835\udc58), and by lemma 2, this means that \u2203\ud835\udc46 \u2208 \ud835\udcae\ud835\udc56\ud835\udc58 such that \ud835\udc45 supports \ud835\udc46, that is, \ud835\udc46 \u2208 \ud835\udc40(\u211b\ud835\udc56\ud835\udc57 \u2032 , \ud835\udc45), which contradicts the assumption that \u211b\ud835\udc56\ud835\udc57 \u2032 does not support \ud835\udcae\ud835\udc56\ud835\udc58. A similar argument shows that the reduced set \u211b\ud835\udc57\ud835\udc58 \u2032 does not result in further removals from \u211b\ud835\udc56\ud835\udc58 \u2032 . Similarly there can be no unsupported edge \ud835\udc45 in \u211b\ud835\udc56\ud835\udc57 \u2032 since if there were, one of the removed edges must have supported \ud835\udc45, since the original clique was stable, and by lemma 4, that edge must have been supported by \ud835\udc45 and would not have been removed. Hence the order of removals does not matter, every remaining edge is supported, and so the resulting clique is stable. \u25a1"}, {"heading": "Natural Language Processing", "text": "We turn now to briefly describe the NLP pipeline we use for our prototype, which currently forms a model of the user\u2019s family relations through dialog, and uses pGMs to track uncertainty and to generate questions. We use three systems: SPLAT, a publically available NLP toolkit [7]; NLPLib, an internal NLP toolkit that uses the averaged perceptron algorithm [8] trained on the part of speech and constituency tree tags and data in OntoNotes Release 4.0 [9], which we used for entity detection [10]; and the Stanford natural language system for labeled dependency trees and coreference resolution [11]. Text is broken into sentences and tokenized. Part of speech tags are added, and the tokens lemmatized. The (labeled) dependency tree is computed. A parse into \u201cchunks\u201d is performed using SPLAT, and named entities are found using NLPLib. Finally the Stanford system is used to generate coreference chains based on the full passage. We then identify nouns, names, pronouns, mentions (which we define as names, pronouns or noun phrases that could be referred to elsewhere in the text), family relations (triggered by tokens such as father, daughter etc.), entities (whose name and gender may or may not be defined at this point), and relation triples, again using simple patterns such as \u201cMy father is named Sam\u201d. The names are identified through combining the named entities identified by NLPLib, 1990 census data, and text patterns, such as [animate possessive] [is] [named/called] NAME. Gender identification is done using census data: if a name appears exclusively on the male or female list, it is identified as such; if it is ten times more frequent on one list, it is marked \u201dprobably\u201d male/female (this information is used solely to tune the dialog generation); and all other names are left unidentified.\nRelation mentions are used to find the relations, taking into account common synonyms. Hypothesized entities are formed from the mentions, and we use the Stanford coreference resolver to group mentions. A special entity is reserved for the narrator of the story, if there is one; gender, name and narrator state are assigned from the mentions, if available. Finally relation triples are formed from pairs of entities together with adjoining relations.\nZ3 The problem of modeling family relations is well suited for first-order logic solvers. We used Z3 to compare this process with using pGMs. Z3 is an open source, SMT-solving theorem prover created at Microsoft7. We wrote a set of assertions that governs family relations in general, for example:\n(and (CousinOf a b) (CousinOf b c)) => (or (SiblingOf a c) (SelfOf a c) (CousinOf a c) (OutOfGraph a c))\nWe then wrote a second set of assertions containing the relations that are asserted in the text. Z3 was then used to check that the union of the two sets of assertions is satisfiable. When a solution exists, Z3 returns a satisfying variable assignment; otherwise it declares that none exists.\nAs it stands, the \u201cout of the box\u201d solution returned by Z3 is not easily interpretable. Many assignments are returned which combine relations with logical operators. Only the first order positive relations [e.g. (SonOf Jack John)] are easily interpretable. Furthermore, often the extracted facts lead to multiple possible solutions, of which Z3 returns only one. Additional solutions can be obtained by explicitly excluding solutions found so far, and iteratively ask for another one; but Z3 does not give direct information as to where the uncertainties lie, and so the knowledge returned by the solver could not be used to drive dialog with the user, at least directly. However, Z3 is a powerful solver and we will continue to seek to incorporate it as part of the solution, especially if the logic involved becomes more complex than discussed here."}, {"heading": "Error Correction and Language Learning", "text": "Our prototype can correct upstream NLP errors and ambiguities both automatically and with the help of the user. For example, in the sentence My brother is named Bill and my father is named Bill, the Stanford systems incorrectly identifies the two Bills as referring to the same entity; our system automatically detects that this cannot be the case and corrects the error (note that this is a simple example of a world model being required for coreference resolution; in the sentence My father is named Bill and my mother\u2019s husband is named Bill, the two Bills are coreferent, despite the similar sentence structure). Similar reasoning can be used to resolve ambiguities such as His in John's dad was named Carl. His wife was named Theresa. In terms of error correction through dialog, an example is as follows: when presented with the sentence I have a daughter. My daughter's name is Susan., the coreference fails to identify daughter with Susan; the system asks for the name of the daughter, and if told Susan, will ask if the two Susans are the same person, and if given the answer Yes (or equivalent, see below), it will correct the coreference error.\nSimilar ideas can be used to learn common paraphrases. For example, if the system asks a yes/no question and receives the answer Indeed!, then it tells the user that it does not understand Indeed!, and asks the same question again. If the user then answers Yes, the system has learnt both the answer to the question, and the fact that in the context of a yes/no question, Indeed! means the same thing as Yes. We have used this idea to gather paraphrase data for the answers to several different question types. It is important however that the context be tracked: for example, the user may write Susan is indeed my daughter when asked Is Susan your daughter?, which only is equivalent to Yes in the specific context of that question. Thus the system would log that if the question is Is X your daughter, and the answer is X is indeed my daughter, then the latter means Yes, for any X. Clearly this idea will enable powerful paraphrase learning when crowdsourced in a personal\n7 Available at: http://z3.codeplex.com/\nassistant. Some care will need to be taken to weed out spam, but this can likely be done by relying on the fact that most spammers do not collude.\nHowever, the system\u2019s main goal is to reduce uncertainty through dialog. It accomplished this by finding which edge sets in the pGM have more than one element, and then asking the user which relation holds. Clearly this could be made more natural by asking about edge sets that are close in the graph to the narrator\u2019s node, and could be made more efficient by finding that edge set whose reduction to a single edge would maximally reduce the collective cardinality of the edge sets throughout the graph."}, {"heading": "Discussion", "text": "The main goals of this work were twofold: first, to explore the issues of correctability and interpretability of learned models, using dialog as the main source of new information; and second, to embark on our exploration of using meaning projections in general, to model the semantics of text. The requirement of the explicit, efficient and actionable modeling of uncertainty led us to introduce possibilistic graphical models, which provide a simple way to track and update uncertainty to the finest granularity; and to adhere to these desiderata, we have so far avoided using statistical techniques. One very interesting research direction therefore is how to include statistical techniques as needed, in such a way as to maintain these desiderata. The meaning projections we used were low level entity detection, coreference resolution and name detection, and a high level family relations projection. We chose the latter as a first step because it is so well studied and because the relational structure is logical and clear. Of course we are not claiming that modeling family relations from text is itself of research interest (although it may be of practical interest, to automated personal assistants); but this task provided us with a clean test bed for the ideas, and we showed for example that indeed, the family relations projection could be used to correct NLP errors, either automatically or with the help of the user.\nMeaning projections can also be largely data driven. As an example, we have built a noun number projection, using Wiktionary. This is a surprisingly nontrivial task. For example, in the sentence My family is very large, and tonight they are all coming to dinner, the noun family is treated as singular in the first part of the sentence and plural in the second, yet the sentence is grammatically correct. Committing to family being either singular or plural can thus lead to coreference errors. We parsed Wiktionary to determine which nouns are definitely singular, which are definitely plural, and which are indefinite. For example, water, although non-countable, is singular and has the definite plural waters, but sheep and cannon are both indefinite; and there are plural nouns with no singular form (for example, People of Spain), and singular nouns with no plural (for example, Computer Graphics), which are also detected. This is a very simple projection, and there are many interesting ways in which large datasets can help with more complex projections. For a particularly elegant example of using the statistics of large datasets to solve what is otherwise a thorny coreference problem, see [12]. We plan to extend these ideas, using for example ClueWeb [13], to help meaning projections to find the patterns they need, automatically.\nThe main question facing us now is: how to extend the work to include more meaning projections, in such a way that the process is scalable? In fact family relations is one of the more complex examples; one might, for example, model spatial relations with \u211d = {\ud835\udc65 < 10, 10 \u2264 \ud835\udc65 < 1\ud835\udc522, 1\ud835\udc522 \u2264 \ud835\udc65 < 1\ud835\udc523, 1\ud835\udc523 \u2264 \ud835\udc65 < 1\ud835\udc524, \ud835\udc65 \u2265 1\ud835\udc524} where \ud835\udc65 is the distance in feet between two entities and bounds are chosen to approximately model \ud835\udc381 being within the {same room, same building, same block, same town, same universe} as \ud835\udc382. Even simple\nprojections could benefit from these ideas: at first blush, the task that a Possession projection solves seems clear, but in fact there are many kinds of possession and the inferences one can make will depend on which type one is considering. If a man possesses a car, and that car possesses a wheel, then it is fair to claim that the man possesses the wheel. If a man possesses an aunt and the aunt possesses a son, then a similar chain of reasoning does not hold. It is our hope that the framework presented in this paper will provide a useful tool to help guide these developments. Eventually, to achieve scalability, the meaning projections themselves will have to be learned from data; perhaps higher level meaning projections could be built automatically from a set of fundamental projections. The use of meaning projections to automatically build higher level semantic constructs such as scripts and plans [14], which encapsulate typically observed sequences of events, is also an interesting direction for research."}, {"heading": "Acknowledgements", "text": "We wish to thank Aitao Chen for generously helping us with his NLPLib package. We also wish to thank Nicolaj Bj\u00f8rner for his generous help with Z3."}], "references": [{"title": "Towards the Machine Comprehension of Text: An Essay", "author": ["C.J.C. Burges"], "venue": "Microsoft Research Technical Report MSR-TR-2013-125", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "From machine learning to machine reasoning", "author": ["L. Bottou"], "venue": "arXiv abs/1102.1808", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge engineering for large belief networks", "author": ["M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion"], "venue": "UAI 1994, as mentioned in D. Koller and N. Friedman, Probabilistic graphical models: principles and techniques, MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Society of Mind", "author": ["M. Minsky"], "venue": "Simon and Schuster", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1988}, {"title": "Machine Perception Of Three-Dimensional Solids", "author": ["L.G. Roberts"], "venue": "PhD thesis, MIT", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1963}, {"title": "Introduction to Logic", "author": ["M. Genesereth", "E. Kao"], "venue": "2 edition, Synthesis Lectures on Compute Science", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "MSR SPLAT: a language analysis toolkit", "author": ["C. Quirk", "P. Choudhury", "J. Gao", "H. Suzuki", "K. Toutanova", "M. Gamon", "W. Yih", "L. Vanderwende", "C. Cherry"], "venue": "Proceedings of the NAACL-HLT 2012: Demonstration Session", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "Proceedings of the ACL conference on Empirical methods in natural language processing, 10:1 -8. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "A", "author": ["R. Weischedel", "S. Pradhan", "L. Ramshaw", "J. Kaufman", "M. Franchini", "M. ElBachouti", "N. Xue", "M. Palmer", "M. Marcus"], "venue": "Taylor, OntoNotes Release 4.0, Technical Report, BBN Technologies, December 24", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Bootstrapping path-based pronoun resolution", "author": ["S. Bergsma", "D. Lin"], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pp. 33\u201440", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Scripts", "author": ["R.C. Schank", "R.P Abelson"], "venue": "plans, goals, and understanding: An inquiry into human knowledge structures, Psychology Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "the machine comprehension of text [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "those modules, is a fundamental principle of software design but is also expected to be a key design principle for large semantic models [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "models (neural nets, trees, ensembles of these, and even moderately sized probabilistic graphical models) are not easily interpretable (see for example [3]): in general we do not understand why our statistical systems make", "startOffset": 152, "endOffset": 155}, {"referenceID": 3, "context": "The idea is roughly analogous to the \u201cagents\u201d in Minsky\u2019s Society of Mind [4].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "The work can be viewed as a semantic modeling analog of Blocks World [4,5], in that it takes a simple task and uses it to explore the ideas.", "startOffset": 69, "endOffset": 74}, {"referenceID": 4, "context": "The work can be viewed as a semantic modeling analog of Blocks World [4,5], in that it takes a simple task and uses it to explore the ideas.", "startOffset": 69, "endOffset": 74}, {"referenceID": 5, "context": "[6], but with data structures chosen to meet the above desiderata.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We emphasize that inference in this system is simply using relational logic [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "We use three systems: SPLAT, a publically available NLP toolkit [7]; NLPLib, an internal NLP toolkit that uses the averaged perceptron algorithm [8] trained on the part of speech and constituency tree tags and data in OntoNotes Release 4.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "We use three systems: SPLAT, a publically available NLP toolkit [7]; NLPLib, an internal NLP toolkit that uses the averaged perceptron algorithm [8] trained on the part of speech and constituency tree tags and data in OntoNotes Release 4.", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "0 [9], which we used for entity detection [10]; and the Stanford natural language system for labeled dependency trees and coreference resolution [11].", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": "For a particularly elegant example of using the statistics of large datasets to solve what is otherwise a thorny coreference problem, see [12].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "The use of meaning projections to automatically build higher level semantic constructs such as scripts and plans [14], which encapsulate typically observed sequences of events, is also an interesting direction for research.", "startOffset": 113, "endOffset": 117}], "year": 2014, "abstractText": "We explore the idea of using a possibilistic graphical model as the basis for a world model that drives a dialog system. As a first step we have developed a system that uses text-based dialog to derive a model of the user\u2019s family relations. The system leverages its world model to infer relational triples, to learn to recover from upstream coreference resolution errors and ambiguities, and to learn context-dependent paraphrase models. We also explore some theoretical aspects of the underlying graphical model.", "creator": "Microsoft\u00ae Word 2013"}}}