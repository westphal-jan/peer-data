{"id": "1411.6232", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks", "abstract": "In this paper, we propose a novel semi-supervised feature selection framework by mining correlations among multiple tasks and apply it to different multimedia applications. Instead of independently computing the importance of features for each task, our algorithm leverages shared knowledge from multiple related tasks, thus, improving the performance of feature selection. Note that we build our algorithm on assumption that different tasks share common structures with each other. Therefore, the task design and algorithm design are similar, but the general approach of this paper is based on direct computation.", "histories": [["v1", "Sun, 23 Nov 2014 13:00:18 GMT  (2519kb)", "http://arxiv.org/abs/1411.6232v1", "11 pages, submitted to TNNLS"], ["v2", "Sun, 11 Jan 2015 18:47:26 GMT  (2504kb)", "http://arxiv.org/abs/1411.6232v2", "11 pages, submitted to TNNLS"]], "COMMENTS": "11 pages, submitted to TNNLS", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiaojun chang", "yi yang"], "accepted": false, "id": "1411.6232"}, "pdf": {"name": "1411.6232.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks", "authors": ["Xiaojun Chang"], "emails": ["cxj273@gmail.com,", "yi.yang@uq.edu.au)."], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n62 32\nv1 [\ncs .L\nG ]\n2 3\nN ov\nIndex Terms\u2014Multi-task feature selection, semi-supervised learning, image annotation, 3D motion data annotation"}, {"heading": "1 INTRODUCTION", "text": "IN many computer vision and pattern recognitionapplications, dimension of data representation is normally very high. Recent studies have claimed that not all features in the high-dimensional feature space are discriminative and informative, since many features are often noisy or correlated to each other, which will deteriorate the performances of subsequent data analysing tasks [1] [2] [3]. Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8]. It has twofold functions in enhancing performances of learning tasks. First, feature selection eliminates noisy and redundant information to get a better representation, thus facilitating classification and clustering tasks. Second, dimension of selected feature space becomes much lower, which makes the subsequent computation more efficient. Inspired by the motivations, much progress has been made to feature selection during last few years. According to availability of class labels of training data, feature selection algorithms fall into two\nXiaojun Chang and Yi Yang are with School of Information Technology and Electric Engineering, The University of Queensland, Australia.(email: cxj273@gmail.com, yi.yang@uq.edu.au).\ngroups, i.e. supervised feature selection and unsupervised feature selection. Supervised feature selection algorithms, for example, Fisher Score [9], only use labeled training data for feature selection. With sufficient labeled training samples, supervised feature selection is reliable to train appropriate feature selection functions because of utilization of class labels. However, labeling a large amount of training samples manually is unrealistic in real-world applications. Recent works on semi-supervised learning have indicated that it is beneficial to leverage both labeled and unlabeled training data for data analysis. Motivated by the progress of semi-supervised learning, much research attention has been paid to semi-supervised feature selection. For example, Zhao et al. propose a semi-supervised feature selection algorithm based on spectral analysis. A common limitation of the existing supervised and semi-supervised feature selection algorithms is that they evaluate the importance of each feature individually, ignoring correlations between different features. To address this problem, some stateof-the-art algorithms are proposed to take feature correlations into consideration for feature selection. For example, [10] and [3] implement their methods in a supervised way and Ma et al. design their approach in a semi-supervise way in [5]. Another limitation of current feature selection algorithms is that they select features for each task individually, which fails to mine correlations among multiple related tasks. Recent researches have indicated that it is beneficial to learn multiple related tasks jointly [11] [12] [13]. Motivated by this fact, multi-task learning has been introduced to the field of multimedia. For instance, Yang et al. present a novel feature selection algorithm which leverages shared information from related tasks in [6]. Nevertheless, they design their algorithm in a supervised way. The semi-supervised algorithm proposed in this paper combines the strengths of semi-supervised feature selection and multi-task learning. Both labeled and unlabeled training data are utilized for feature selection. Meanwhile, correlations between different features are taken into consideration to improve the performance of feature selection.\nWe illustrate how the proposed algorithm works for video classification in Figure 1. First, we represent all the training and testing videos as feature vectors. Then, sparse coefficients are learnt by exploiting relationships among different features and levearging knowledge frommultiple related tasks. After selecting the most representative features, we can apply the sparse coefficients to the feature vectors of the testing videos for classification.\nWe name our proposed algorithm Semi-supervised Feature selection by Mining Correlations among multiple tasks (SFMC). The main contributions of our work can be summarized as follows:\n1) We combine semi-supervised feature selection and multi-task learning into a single framework, which can select the most representative features with an insufficient amount of labeled training data per task. 2) To explore correlations among multimedia data, we leverage the benefit of manifold learning into our framework. 3) Since the objective function is non-smooth and difficult to solve, a fast iterative algorithm to obtain the optimal solution is proposed. Experimental results on convergence demonstrate that the proposed algorithm converges within very few iterations.\nThe rest of this paper is organized as follows: Section 2 summarizes the overview of the related work. A novel Semi-supervised Feature Selection by Mining Correlations among multiple tasks is proposed in section 3. We present our experimental results in section 4. The conclusion of our work is discussed in section 5."}, {"heading": "2 RELATED WORK", "text": "In this section, we briefly review the related research on feature selection, semi-supervised learning and multi-task learning."}, {"heading": "2.1 Feature selection", "text": "Previous works have claimed that feature selection is capable of selecting the most representative features, thus facilitating subsequent data analysing tasks [14] [15] [16]. Existing feature selection algorithms are designed in various ways. Classical feature selection algorithms, such as Fisher Score [9], evaluate the weights of all features, rank them accordingly and select the most discriminating features one by one [17]. Although these classical feature selection algorithms gain good performances in different applications, they have three main limitations. First, they only use labeled\n3 training data to exploit the correlations between features and labels for feature selection. Labeling a large amount of training data consumes a lot of human labor in real-world applications. Second, the most representative features are selected one by one, thus ignoring the correlations among different features. Third, they select features for each task independently, which fails to leverage the knowledge shared by multiple related tasks. To overcome the aforementioned limitations, researchers have proposed multiple feature selection algorithms. l2,1-norm regularization has been widely used in feature selection algorithms for its capability of selecting features across all data points with joint sparsity. For example, Zhao et al. propose an algorithm which selects features jointly based on spectral regression with l2,1-norm constraint in [18]. Nie et al. adopt l2,1-norm on both regularization term and loss function in [10]. Yang et al. propose to select features by leveraging shared knowledge from multiple related tasks in [6]. However, their algorithms are all designed in a supervised way."}, {"heading": "2.2 Semi-supervised learning", "text": "Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24]. With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28]. Hence, semi-supervised learning is beneficial in terms of both the human laboring cost and data analysis performance. Graph Laplacian based semi-supervised learning has gained increasing interest for its simplicity and efficiency. Nie et al. propose a manifold learning framework based on graph Laplacian and compared its performance with other state-of-the-art semi-supervised algorithms in [29]. Ma et al. propose a semi-supervised feature selection algorithm built upon manifold learning in [5]. In [30], Yang et al. propose a new semisupervised algorithm based on a robust Laplacian matrix for relevance feedback. Their algorithm has demonstrated its prominent performance. Therefore, we propose to leverage it in our feature selection framework. These previous works, however, independently select features for each task, which fails to consider correlations among multiple related tasks."}, {"heading": "2.3 Multi-task learning", "text": "Multi-task learning has been widely used in many applications with the appealing advantage that it learns multiple related tasks with a shared representation [11] [12] [31]. Recent researches have indicated that learning multiple related tasks jointly always outperforms learning them independently. Inspired by the progress of multi-task learning, researchers\nhave introduced it to the field of multimedia and demonstrated its promising performance on multimedia analysis. For example, Yang et al. propose a novel multi-task feature selection algorithm which improves feature selection performance by leveraging shared information among multiple related tasks [6]. In [6], Ma et al. apply knowledge adaptation to multimedia event detection and compare its performance with several state-of-the-art algorithms. Despite of their good performances, these classical algorithms are all implemented only with labeled training data."}, {"heading": "3 METHODOLOGY", "text": "In this section, we describe the approach of our proposed algorithm in detail."}, {"heading": "3.1 Problem Formulation", "text": "Suppose we are going to select features for t tasks. The l-th task contains nl training data with ml data labeled. We can formulate the regularized framework for feature selection as follows:\nmin Wl\nt\u2211\nl=1\n(loss(Wl) + \u03b1g(Wl)) + \u03b3\u2126(W ), (1)\nwhere Wl is feature selection matrix for the l-th task, W = [W1, \u00b7 \u00b7 \u00b7 ,Wt], loss(Wl) is the loss function which evaluates consistency between features and labels, g(Wl) is a regularization function, \u2126(W ) is a regularization term which is used to encode the common components of different feature selection functions, \u03b1 and \u03b3 are regularization parameters. To step further, we first give the definitions of Frobenius norm and trace norm. Given an arbitrary matrix M \u2208 Ra\u00d7b where a and b are arbitrary numbers, its Frobenius norm is defined as \u2016M\u2016F . The definition of its l2,1-norm is:\n\u2016M\u20162,1 = a\u2211\ni=1\n\u221a\u221a\u221a\u221a b\u2211\nj=1\nM2ij , (2)\nand the definition of its trace norm is:\n\u2016M\u2016\u2217 = Tr(MM T )\n1 2 , (3)\nwhere Tr(\u00b7) denotes the trace operator. In the literature, there are many approaches to define the loss function. Following the works in [5] [6], we adopt the least square loss function for its simplicity and efficiency. Recent works [10] [17] claim that minimizing the regularization term \u2016Wl\u20162,1 makes Wl sparse, which demonstrates that Wl is especially suitable for feature selection. Motivated by the works in [32] [6], we propose to leverage shared knowledge among multiple related tasks by minimizing the trace norm of W . The objective function is given by:\n4 min Wl t\u2211\nl=1\n(loss(Wl) + \u03b1\u2016Wl\u20162,1) + \u03b3\u2016W\u2016\u2217 (4)\nState-of-the-art feature selection algorithms are implemented through supervised learning and select features for each task independently. In our work, we want to incorporate multi-task learning and semisupervised learning into (1). We propose to leverage semi-supervised learning by adopting the Laplacian proposed in [30]. We adopt this Laplacian because it exploits both manifold structure and local discriminant information of multimedia data, thus resulting in better performance. To begin with, let us define Xl = [x 1 l , \u00b7 \u00b7 \u00b7 , x nl l ] as the training data matrix of the l-th task where ml data are labeled and nl is the total number of the training data of the l-th task. xil \u2208 R\nd is the i-th datum of the l-th task. Yl = [y 1 l , \u00b7 \u00b7 \u00b7 , y ml l , y ml+1 l , \u00b7 \u00b7 \u00b7 , y nl l ] T \u2208 {0, 1}nl\u00d7cl is the label matrix and cl denotes class number of the l-th task. yil | nl i=1 \u2208 R\ncl is the label vector with cl classes. Yli,j = 1 if x i l is in the j-th class of the l-th task while Yli,j = 0 otherwise. For unlabeled datum x i l , y i l is set to a zero vector. For any d, we define 1d \u2208 Rd as a column vector with all the elements equal to 1, Hd = I \u2212 1d1d1 T d \u2208 R\nd\u00d7d as a matrix for centering the data by subtracting the mean of the data. Note that Hd = HTd = HdHd. For each data point x i l of the l-th task, we construct a local clique Nlk containing xil and its k\u2212 1 nearest neighbors. Euclidean distance is used to determine whether two given data points are within k nearest neighbors in the original feature space. Gil = {i0l , i 1 l , \u00b7 \u00b7 \u00b7 , i k\u22121 l } is index set of samples in Nlk. Sli denotes selection matrix with its elements (Sli)pq = 1 if p = Gil{q} and (Sli)pq = 0 otherwise. Inspired by [30], we construct the Laplacian matrix by exploiting both manifold structure and local discriminant information. Denoting Lli = Hk(X T l Xl + \u03bbI)\u22121Hk, we compute the Laplacian matrix L as follows:\nLl =\nnl\u2211\ni=1\nSliLliS T li\n= [Sl1, \u00b7 \u00b7 \u00b7 , Slnl ]\n  Ll1\n\u00b7 \u00b7 \u00b7 Llnl\n  [Sl1, \u00b7 \u00b7 \u00b7 , Slnl ]T .\n(5) Note that Manifold Regularization is able to explore the manifold structure possessed by multimedia data [29] [33] [34]. By applying Manifold Regularization to the loss function in (1), we have\nargmin W,b\nt\u2211\nl=1\nTr(WTXlLlX T l W ) + \u03b1(\u2016Wl\u20162,1\n+\u03b2\u2016XTlLWl + 1nlb T l \u2212 YlL\u2016 2 F )) + \u03b3\u2016W\u2016\u2217,\n(6)\nwhere Tr(\u00b7) denotes trace operator, XlL and YlL are\nlabeled training data and corresponding ground truth labels of the l-th task. To make all labels of training data contribute to the optimization of Wl, we introduce a predicted label matrix Fl = [fl1, \u00b7 \u00b7 \u00b7 , fln l ] \u2208 Rnl\u00d7cl for the training data of the l-th task. fli \u2208 Rcl is the predicted label vector of xli. According to [19] [5], Fl can be obtained as follows:\nargmin Fl\nTr(FTl LlFl) + Tr((Fl \u2212 Yl) TUl(Fl \u2212 Yl)), (7)\nwhere Ul is the selection diagonal matrix of the l-th task. The diagonal element Ulii = \u221e if xli is labeled and Ulii = 1 otherwise. In the experiments, 10\n6 is used to approximate \u221e. Following the work in [5], we incorporate (7) into (6). At the same time, all the training data and corresponding labels are taken into consideration. Therefore, the objective function finally arrives at:\nmin Fl,Wl,bl\nt\u2211\nl=1\n(Tr[(Fl \u2212 Yl) TUl(Fl \u2212 Yl)] + Tr(F T l LlFl)\n+\u03b1(\u2016Wl\u20162,1 + \u03b2\u2016X T l Wl + 1nlb T l \u2212 Fl\u2016 2 F )) + \u03b3\u2016W\u2016\u2217 (8) From (8) we can see that the proposed algorithm is capable of evaluating the informativeness of all features jointly for each task with the l2,1-norm and the information from different tasks can be transferred from one to another with the trace norm."}, {"heading": "3.2 Optimization", "text": "The proposed function involves the l2,1-norm and trace norm, which are difficult to solve in a closed form. We propose to solve this problem in the following steps. By setting the derivative of (8) w.r.t bl to 0, we get\nbl = 1\nnl (Fl \u2212X\nT l Wl) T 1nl (9)\nSubstituting bl in (8) with (9), we obtain\nmin Fl,Wl,bl\nt\u2211\nl=1\n(Tr[(Fl \u2212 Yl) T Ul(Fl \u2212 Yl)] + Tr(F T l LlFl)+\n\u03b1(\u2016Wl\u20162,1 + \u03b2\u2016X T l Wl +\n1 nl 1nl1 T nl (Fl \u2212X T l Wl)\u2212 Fl\u2016 2 F ))\n+ \u03b3\u2016W \u2016\u2217\n\u21d2 min Fl,Wl\nt\u2211\nl=1\n(Tr[(Fl \u2212 Yl) T Ul(Fl \u2212 Yl)] + Tr(F T l LlFl)\n+ \u03b1(\u2016Wl\u20162,1 + \u03b2\u2016HnlX T l Wl \u2212HnlFl\u2016 2 F )) + \u03b3\u2016W \u2016\u2217\n(10)\nwhere Hnl = Inl \u2212 1\nnl 1nl1 T nl is a centering matrix. By\nsetting the derivative of (10) w.r.t Fl to 0, we have\n2UlFl \u2212 2UlYl + 2LlFl + \u03b1\u03b2(2HnlFl \u2212 2HnlX T l Wl) = 0\n5 Algorithm 1: Optimization Algorithm for SFMC\nData: Training data Xl|tl=1 \u2208 R d\u00d7nl\nTraining data labels Yl|tl=1 \u2208 R n\u00d7c\nParameters \u03b3, \u03b1 and \u03b2 Result:\nFeature Selection Matrix Wl|tl=1 \u2208 R d\u00d7cl\n1 l = 1 ; 2 while l \u2264 t do 3 Initialise Wl|tl=1 \u2208 R\nd\u00d7cl ; 4 Compute the Laplacian matrix Ll|tl=1 ; 5 Compute the Selection matrix Ul| t l=1 ; 6 Hnl = Inl \u2212 1\nnl 1nl1 T nl ;\n7 Pl = (\u03b1\u03b2Hnl + Ul + Ll) \u22121 ; 8 Rl = XlHnl(Inl \u2212 \u03b1\u03b2Pl)HnlX T l ;\n9 Tl = XlHnlPlUlYl ; 10 end 11 Set r = 0 ; 12 Set W0 = [W1, \u00b7 \u00b7 \u00b7 ,Wt] ; 13 repeat 14 l = 1 ; 15 Compute the diagonal matrix as:\nD\u0303r = (1/2)(WrW T r ) \u22121/2 ; 16 while l \u2264 t do 17 Compute the diagonal matrix Drl according to Eq. (16) ; 18 Update W rl by\nW rl = (Rl + \u03b1 \u03b2D r l + \u03b3 \u03b1\u03b2 D\u0303 r)\u22121Tl ;\n19 Update F rl by F r l =\n(\u03b1\u03b2Hnl + Ul + Ll) \u22121(\u03b1\u03b2HnlX T l Wl + UlYl) ;\n20 Update brl by b r l = 1 nl (Fl \u2212X T l Wl) T 1nl ; 21 l = l + 1 ; 22 end 23 Wr+1 = [W1, \u00b7 \u00b7 \u00b7 ,Wt] ; 24 r = r + 1 ; 25 until Convergence; 26 Return the optimal Wl|tl=1 and bl| t l=1.\nTherefore, we have\nFl = (\u03b1\u03b2Hnl + Ul + Ll) \u22121(\u03b1\u03b2HnlX T l Wl + UlYl) (11)\nDenoting Pl = (\u03b1\u03b2Hnl + Ul + Ll) \u22121 and Ql =\n\u03b1\u03b2HnlX T l Wl + UlYl, we have\nFl = PlQl (12)\nBy substituting Fl into (10) with (12), we can rewrite the objective function as follows:\nmin Ql,Wl\nt\u2211\nl=1\n(Tr[(PlQl \u2212 Yl) TUl(PlQl \u2212 Yl)]\n+ Tr(QTl P T l LlPlQl) + \u03b1(\u2016Wl\u20162,1 + \u03b2\u2016HnlX T l Wl \u2212HnlPlQl\u2016 2 F )) + \u03b3\u2016W\u2016\u2217\n(13)\nAs Tr(QTl P T l UlYl) = Tr(Y T l U T l PlQl) and\nTr(\u03b1\u03b2WTl XlHlPlQl) = Tr(\u03b1\u03b2Q T l P T l HlX T l Wl),\nthe objective function can be rewritten as follows:\nmin Wl\nt\u2211\nl=1\n(\u03b1\u03b2Tr(WTl XlHnl(Inl \u2212 \u03b1\u03b2Pl)HnlX T l Wl)\n\u22122\u03b1\u03b2Tr(WTl XlHnlPlUlYl) + \u03b1\u2016Wl\u20162,1) + \u03b3\u2016W\u2016\u2217 (14) Denoting Rl = XlHnl(Inl \u2212 \u03b1\u03b2Pl)HnlX T l , Tl =\nXlHnlPlUlYl and Wl = [w 1 l , \u00b7 \u00b7 \u00b7 , w d l ], the objection function becomes:\nmin Wl\nt\u2211\nl=1\n(\u03b1\u03b2Tr(WTl RlWl)\u2212 2\u03b1\u03b2Tr(W T l Tl)\n+ \u03b1Tr(WTl DlWl)) + \u03b3\u2016W T D\u0303W\u2016\u2217,\n(15)\nwhere D\u0303 = (1/2)(WWT )\u22121/2 and Dl is a diagonal matrix which is defined as:\nDl =   1 2\u2016w1 l \u20162\n. . . 1\n2\u2016wd l \u20162\n  . (16)\nBy setting the derivative w.r.t Wl to 0, we have\nWl = (Rl + \u03b1\n\u03b2 Dl +\n\u03b3\n\u03b1\u03b2 D\u0303)\u22121Tl (17)\nAs shown in Algorithm 1, an iterative algorithm is proposed to optimize the objective function (8) based on the above mathematical deduction."}, {"heading": "3.3 Convergence Analysis", "text": "In this section, we prove that Algorithm 1 converges by the following theorem.\nTheorem 1. The objective function value shown in (8) monotonically decreases in each iteration until convergence by applying Algorithm 1.\nProof: Suppose after the r-th iteration, we have obtained F rl , b r l and W r l . According the definition of Dl and D\u0303, the convergence of Algorithm 1 corresponds to the following inequality:\nt\u2211\nl=1\nTr[(F r+1l \u2212 Yl) T Ul(F r+1 l \u2212 Yl)] + Tr((F r+1 l ) T LlF r+1 l )\n+ \u03b1( d\u2211\nj=1\n\u2016(wr+1l ) j\u201622\n\u2016(wrl ) j\u20162\n+ \u03b2\u2016XTl W r+1 l + 1nlb r+1 l T \u2212 F r+1l \u2016 2 F )\n+ Tr((W r+1)T \u03b3\n2 (W r(W r)T )\u2212\n1 2W r+1)\n\u2264\nt\u2211\nl=1\ntr[(F rl \u2212 Yl) T Ul(F r l \u2212 Yl)] + Tr((F r l ) T LlF r l )\n+ \u03b1( d\u2211\nj=1\n\u2016(wrl ) j\u201622 \u2016(wrl ) j\u20162 + \u03b2\u2016XTl W r l + 1nlb r l T \u2212 F rl \u2016 2 F )\n+ Tr((W r)T \u03b3\n2 (W r(W r)T )\u2212\n1 2W r)\n(18)\n6 Following the works in [10] [6] [5], we have:\nt\u2211\nl=1\n(Tr[(F r+1l \u2212 Yl) T Ul(F r+1 l \u2212 Yl)] + Tr((F r+1 l ) T LlF r+1 l )\n+ \u03b1( d\u2211\nj=1\n\u2016(wr+1l ) j\u2016+ \u03b2\u2016XTl W r+1 l + 1nl (b r+1 l ) T \u2212 F r+1l \u2016 2 F ))\n+ \u03b3\n2 Tr(W r+1(W r+1)T (WW T )\u2212\n1 2 )\n\u2264\nt\u2211\nl=1\n(Tr[(F rl \u2212 Yl) T Ul(F r l \u2212 Yl)] + Tr((F r l ) T LlF r l )\n+ \u03b1( d\u2211\nj=1\n\u2016(wrl ) j\u2016+ \u03b2\u2016XTl W r l + 1nl (b r l ) T \u2212 F rl \u2016 2 F ))\n+ \u03b3\n2 Tr(W r(W r)T (W r(W r)T )\u2212\n1 2 ).\n(19)\nWe can rewrite (19) as follows:\nt\u2211\nl=1\n(Tr[(F r+1l \u2212 Yl) T Ul(F r+1 l \u2212 Yl)] + Tr((F r+1 l ) T LlF r+1 l )\n+ \u03b1( d\u2211\nj=1\n\u2016(wr+1l ) j\u2016+ \u03b2\u2016XTl W r+1 l + 1nl (b r+1 l ) T \u2212 F r+1l \u2016 2 F ))\n+ \u03b3\n2 Tr((W r+1(W r+1)T )\u2212\n1 2 ) + \u03b3\n2 Tr(W r+1(W r+1)T (WW T )\u2212\n1 2 )\n\u2212 \u03b3\n2 Tr((W r+1(W r+1)T )\u2212\n1 2 )\n\u2264 t\u2211\nl=1\n(Tr[(F rl \u2212 Yl) T Ul(F r l \u2212 Yl)] + Tr((F r l ) T LlF r l )\n+ \u03b1( d\u2211\nj=1\n\u2016(wrl ) j\u2016+ \u03b2\u2016XTl W r l + 1nl (b r l ) T \u2212 F rl \u2016 2 F ))\n+ \u03b3\n2 Tr((W r(W r)T )\u2212\n1 2 ) + \u03b3\n2 Tr(W r(W r)T (W r(W r)T )\u2212\n1 2 )\n\u2212 \u03b3\n2 Tr((W r(W r)T )\u2212\n1 2 ).\n(20)\nAccording to Lemma 1 in [6], we have:\n\u03b3 2 Tr(W r+1(W r+1)T (W r(W r)T )\u2212 1 2 )\u2212 \u03b3Tr((W r+1(W r+1)T \u2265 \u03b3\n2 Tr(W r(W r)T (W r(W r)T )\u2212\n1 2 )\u2212 \u03b3Tr(W r(W r)T )\n(21)\nBy deducting (21) from (20), we arrive at:\nt\u2211\nl=1\n(Tr[(F r+1l \u2212 Yl) T Ul(F r+1 l \u2212 Yl)] + Tr((F r+1 l ) T LlF r+1 l )\n+ \u03b1(\u2016W r+1l \u20162,1 + \u03b2\u2016X T l W r+1 l + 1nl (b r+1 l ) T \u2212 F r+1l \u2016 2 F )) + \u03b3\u2016W r+1\u2016\u2217\n\u2264\nt\u2211\nl=1\n(Tr[(F rl \u2212 Yl) T Ul(F r l \u2212 Yl)] + Tr((F r l ) T LlF r l )\n+ \u03b1(\u2016W rl \u20162,1 + \u03b2\u2016X T l W r l + 1nl (b r l ) T \u2212 F rl \u2016 2 F )) + \u03b3\u2016W r\u2016\u2217)). (22)\nEq. (22) indicates that the objective function value decreases after each iteration. Thus, we have proved Theorem 1.\nHaving Theorem 1, we can easily see that the algorithm converges."}, {"heading": "4 EXPERIMENTS", "text": "In this section, experiments are conducted to evaluate the performance of our algorithm on video classification, image annotation, human motion recognition and 3D motion data analysis, respectively. Additional experiments are conducted to study the performance w.r.t. influence of number of selected features and parameter sensitivity."}, {"heading": "4.1 Experiment Setup", "text": "We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38]. In order to demonstrate advantages of our algorithm, we compare its performance with the following approaches.\n1) All Features: We directly use the original features without feature selection as a baseline. 2) Fisher Score: This is a classical feature selection method, which evaluates importances of features and selects the most discriminating features one by one [9]. 3) Feature Selection via Joint l2,1-Norms Minimization (FSNM): Joint l2,1-norm minimization is utilized on both loss function and regularization for joint feature selection [10]. 4) SPEC: It uses spectral graph theory to conduct feature selection [18]. 5) Feature Selection with Shared Information among multiple tasks (FSSI): It simultaneously learns multiple feature selection functions of different tasks in a joint framework [6]. Hence, it is capable to utilize shared knowledge between multiple tasks to facilitate decision making. 6) Locality Sensitive Semi-supervised Feature Selection (LSDF): This is a semi-supervised feature selection based on two graph constructions, i.e. within-class graph and between-class graph [39]. 7) Structural Feature Selection with Sparsity (SFSS): It combines strengths of joint feature selection and semi-supervised learning into a single framework [5]. Labeled and unlabeled training data are both utilized for feature selection. Meanwhile, correlations between different features are taken into consideration.\nIn the experiments, a training set for each dataset is randomly generated consisting of n samples, among which m% samples are labeled. The detailed settings are shown in Table 1. The remaining data are used as testing data. We independently repeat the experiment 5 times and report the average results.\n7\nWe have to tune two types of parameters in the experiments. One is the parameter k that specifies k nearest neighbors used to compute graph Laplacian. Following [5], we fix it at 15. The other parameter is the regularization parameters, \u03b1, \u03b2 and \u03b3, which are shown in the objective function (8). These parameters are tuned in the range of {10\u22126, 10\u22124, 10\u22122, 100, 10+2, 10+4, 10+6} and the best results are reported. Linear SVM is used as classifier. Mean average precision (MAP) is used to evaluate the performance."}, {"heading": "4.2 Video Classification", "text": "First, we compare the performances of different algorithms in terms of video classification task using Columbia Consumer Video dataset (CCV) [35]. It consists of 9, 317 web videos over 20 semantic categories, in which 4, 659 videos are used as training data and 4, 658 videos are used as testing data. The semantic categories include events like \u201dbasketball\u201d and \u201dparade\u201d, scenes like \u201dbeach\u201d and \u201dplayground\u201d, and objects like \u201dcat\u201d and \u201ddog\u201d, based on which we generate three different classification tasks. Since the original videos of this dataset have not been available on the internet, we directly use the STIP features with 5, 000 dimensional BoWs representation provided by [35]. We set the number of selected features as {2500, 3000, \u00b7 \u00b7 \u00b7 , 4500, 5000} for all the algorithms, and report the best results. We show the video classification results when different percentages of labeled training data are used in Table 2. From the experimental results, we can get the following observations: 1) The performances of all the compared algorithms increase when we increase the number of labeled training data. 2) The proposed algorithm consistently gains the best performance. 3) With 5% labeled training data, our algorithm significantly outperforms other algorithms. For example, for subject 2, our algorithm is better than the second best algorithm by 6.6%. Yet the proposed algorithm gains smaller advantage with more labeled training data."}, {"heading": "4.3 Image Annotation", "text": "We use NUS-WIDE dataset [36] to test the performance of our algorithm. This dataset includes 269648 images of 81 concepts. A 500 dimension Bag-of-Words feature based on SIFT descriptor is used in this experiment. We take each concept as a separate annotation\ntask, thus resulting in 81 tasks. It is difficult to report all the results of these 81 tasks, so the average result is reported. In this experiment, we set the number of selected features as {250, 275, \u00b7 \u00b7 \u00b7 , 475, 500} and report the best results. We illustrate the experimental results in Table 3. From the experimental results, we can observe that the proposed method gains better performance than the other compared algorithms. We give the detailed results with 1%, 5% and 10% labeled training data. It can be seen that the proposed algorithm is more competitive with less labeled training data."}, {"heading": "4.4 Human Motion Recognition", "text": "We use HMDB video dataset [37] to compare the algorithms in terms of human motion recognition. HMDB dataset consists of 6,766 videos which are associated with 51 distinct action categories. These categories can be categorized into five groups: 1) General facial actions, 2) Facial actions with object manipulation, 3) General body movements, 4) Body movements with object interaction, 5) Body movements for human interaction. Therefore, in this experiment, the five groups are considered as five different tasks. Heng et al. claim that motion boundary histograms (MBH) is an efficient way to suppress camera motion in [40] and thus it is used to process the videos. A 2000 dimension Bag-of-Words feature is generated to represent the original data. We set the number of selected features as {1000, 1200, \u00b7 \u00b7 \u00b7 , 1800, 2000} for all the algorithms and report the best results. Table 4 shows the experiment results of human motion recognition. From Table 4, we observe that our method outperforms other compared algorithms. This experiment can further provide evidence that our algorithm is more advantageous with insufficient number of labeled training data."}, {"heading": "4.5 3D Motion Data Analysis", "text": "We evaluate the performance of our algorithm in terms of 3D motion data analysis using HumanEva 3D motion database. There are five different types of actions in this database, including boxing, gesturing, walking, throw-catch and jogging. Following the work in [41] [42], we randomly select 10, 000 samples of two subjects (5, 000 per subject). We encode each action as a collection of 16 joint coordinates in 3D space and obtain a 48-dimensional feature vector. Joint Relative Features between different joints are computed on top of that, resulting a feature vector with 120 dimensions. We combine the two kinds of feature vectors and get a 168-dimensional feature. In this experiment, we consider the two subjects as two different tasks. The number of selected features are tuned from {100, 110, \u00b7 \u00b7 \u00b7 , 160}. The experiment results are shown in Table 5. Table 5 gives detailed results when 1%, 5% and 10% training\n8\ndata are labeled. From the experiment results, we can observe that our algorithm consistently outperform the other compared algorithms and obtains more performance gain when small number of training data are labeled."}, {"heading": "4.6 Comparison with Other Semi-Supervised Feature Selection Methods", "text": "In this section, experiments are conducted on CCV to compare the proposed algorithm with two state-ofthe-art semi-supervised feature selection algorithms. Following the above experiments, 1%, 5%, 10%, 25%, 50% and 100% training data are labeled in this experiment. We show the experiment results in Figure 2. We can observe that our method consistently outperforms\nboth LSDF and SFSS. Visible advantages are gained when only few training data are labeled, such as 1% or 5% labeled training data. From this result, we can conclude that it is beneficial to leverage shared information from other related tasks when insufficient number of training data are labeled."}, {"heading": "4.7 Parameter Sensitivity", "text": "We study the influences of the four parameters \u03b1, \u03b2, \u03b3 and the number of selected features using CCV database with 1% labeled training data. First, we fix \u03b3 and the number of selected features at 1 and 3500 respectively, which are the median values of the tuned range of the parameters. The experimental results are shown in Figure 3. It can be seen that the performance\n9\nof our algorithm varies when the parameters (\u03b1 and \u03b2) change. More specifically, MAP is higher when \u03b1 and \u03b2 are comparable. Then, \u03b1 and \u03b2 are fixed. Figure 4 shows the parameter sensitivity results. Note that the shared information among multiple feature selection functions {W1, \u00b7 \u00b7 \u00b7 ,Wt} by the parameter \u03b3. From this figure, we can see that mining correlations between multiple related tasks is beneficial to improve the performance. We can also notice that better performances are gained when the number of features is around 3500 and 4000."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have proposed a new semisupervised feature analysis method. This method is able to mine correlations between different features and leverage shared information between multiple related tasks. Since the proposed objective function is non-smooth and difficult to solve, we propose an\n10\niterative and effective algorithm. To evaluate performances of the proposed method, we apply it to different applications, including video classification, image annotation, human motion recognition and 3D motion data analysis. The experimental results indicate that the proposed method outperforms the other compared algorithms for different applications."}], "references": [{"title": "Feature selection for high-dimensional data: a fast correlation-based filter solution", "author": ["L. Yu", "H. Liu"], "venue": "Proc. ICML, 2003, pp. 856\u2013863.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "An evaluation of filter and wrapper methods for feature selection in categorical clustering", "author": ["L. Talavera"], "venue": "Proc. IDA, 2005, pp. 440\u2013451.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Web image annotation via subspace-sparsity collaborated feature selection", "author": ["Z. Ma", "F. Nie", "Y. Yang", "J.R. Uijlings", "N. Sebe"], "venue": "IEEE Trans. Multimedia, vol. 14, no. 4, pp. 1021\u2013 1030, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminating joint feature analysis for multimedia data understanding", "author": ["M. Zhigang", "F. Nie", "Y. Yang", "J. Uijlings", "N. Sebe", "A.G. Hauptmann"], "venue": "IEEE Trans. Multimedia, vol. 14, no. 6, pp. 1662 \u2013 1672, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature selection for multimedia analysis by sharing information among multiple tasks", "author": ["Y. Yang", "Z. Ma", "A. Hauptmann", "N. Sebe"], "venue": "IEEE Trans. Multimedia, vol. 15, no. 3, pp. 661 \u2013 669, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["H. Liu", "L. Yu"], "venue": "IEEE Trans. Knowl. Data Engin., vol. 17, no. 4, pp. 491\u2013502, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "An effective feature selection method via mutual information estimation", "author": ["J. Yang", "C.J. Ong"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 42, no. 6, pp. 1550\u20131559, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient and robust feature selection via joint 2, 1-norms minimization", "author": ["N. Feiping", "H. Huang", "X. Cai", "C.H. Ding"], "venue": "Proc. NIPS, 2010, pp. 1813\u20131821.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, no. 1, pp. 41\u201375, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, vol. 73, no. 3, pp. 243\u2013272, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou"], "venue": "Proc. NIPS, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient spectral feature selection with minimum redundancy", "author": ["Z. Zhao", "L. Wang", "H. Liu"], "venue": "Proc. AAAI, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminative feature selection by nonparametric bayes error minimization", "author": ["S.H. Yang", "B.-G. Hu"], "venue": "IEEE Trans. Knowl. Data Engin., vol. 24, no. 8, pp. 1422\u20131434, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative least squares regression for multiclass classification and feature selection", "author": ["S. Xiang", "F. Nie", "G. Meng", "C. Pan", "C. Zhang"], "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 23, no. 11, pp. 1738\u20131754, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "L21-norm regularization discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "proc. IJCAI, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zheng", "H. Liu"], "venue": "Proc. ICML, 2007, pp. 1151\u2013 1157.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Computer Science, University of Wisconsin-Madison, Tech. Rep., 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Semisupervised classification with cluster regularization", "author": ["R.G.F. Soares", "H. Chen", "X. Yao"], "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 23, no. 11, pp. 1779\u20131792, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Semisupervised metric learning by maximizing constraint margin", "author": ["F. Wang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, vol. 41, no. 4, pp. 931\u2013939, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Semisupervised learning of classifiers: Theory, algorithms, and their application to human-computer interaction", "author": ["I. Cohen", "F.G. Cozman", "N. Sebe", "M.C. Cirelo", "T.S. Huang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, no. 12, pp. 1553\u20131567, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Multimodal semisupervised learning for image classification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Proc. CVPR, 2010, pp. 902\u2013909.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A convex formulation for semi-supervised multi-label feature selection", "author": ["X. Chang", "F. Nie", "Y. Yang", "H. Huang"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada., 2014, pp. 1171\u20131177.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Ranking with local regression and global alignment for cross media retrieval", "author": ["Y. Yang", "D. Xu", "F. Nie", "J. Luo", "Y. Zhuang"], "venue": "Proc. ACM Multimedia, 2009, pp. 175\u2013184.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Efcient semi-supervised feature selection with noise insensitive trace ratio criterion", "author": ["Y. Liu", "F. Nie", "J. Wu", "L. Chen"], "venue": "Neurocomputing, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised feature analysis for multimedia annotation by mining label correlation", "author": ["X. Chang", "H. Shen", "S. Wang", "J. Liu", "X. Li"], "venue": "Advances in Knowledge Discovery and Data Mining - 18th Pacific-Asia Conference, PAKDD 2014, Tainan, Taiwan, May 13-16, 2014. Proceedings, Part II, 2014, pp. 74\u201385.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Graph construction and b-matching for semi-supervised learning", "author": ["T. Jebara", "J. Wang", "S.-F. Chang"], "venue": "Proc. ICML, 2009, pp. 441\u2013448.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Flexible manifold embedding: A framework for semi-supervised and unsupervised dimension reduction", "author": ["F. Nie", "D. Xu", "I.-H. Tsang", "C. Zhang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 7, pp. 1921\u20131932, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1921}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Y. Yang", "D. Xu", "F. Nie", "S. Yan", "Y. Zhuang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761\u20132773, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Integrating low-rank and groupsparse structures for robust multi-task learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "Proc. ACM SIGKDD, 2011, pp. 42\u201350.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Statistics and Computing, vol. 20, no. 2, pp. 231\u2013252, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval", "author": ["Y. Yang", "Y.-T. Zhuang", "F. Wu", "Y.-H. Pan"], "venue": "IEEE Trans. Multimedia, vol. 10, no. 3, pp. 437\u2013446, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic manifold learning for image retrieval", "author": ["Y.-Y. Lin", "T.-L. Liu", "H.-T. Chen"], "venue": "Proc. ACM Multimedia, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Consumer video understanding: A benchmark database and an evaluation of human and machine performance", "author": ["Y.-G. Jiang", "G. Ye", "S.-F. Chang", "D. Ellis", "A.C. Loui"], "venue": "Proc. ICMR, 2011, p. 29.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Nus-wide: A real-world web image database from national university of singapore", "author": ["T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y.-T. Zheng"], "venue": "Proc. of ACM Conf. on Image and Video Retrieval (CIVR\u201909), Santorini, Greece., July 8-10, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Hmdb: a large video database for human motion recognition", "author": ["H. Kuehne", "H. Jhuang", "E. Garrote", "T. Poggio", "T. Serre"], "venue": "Proc. ICCV, 2011, pp. 2556\u20132563.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Humaneva: Synchronized video and motion capture dataset for evaluation of articulated human motion", "author": ["S. Leonid", "M.J. Black"], "venue": "Brown Univertsity, Tech. Rep. CS-06-08, 2006.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality sensitive semi-supervised feature selection", "author": ["J. Zhao", "K. Lu", "X. He"], "venue": "Neurocomputing, vol. 71, no. 10, pp. 1842\u2013 1849, 2008.  12", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1842}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "Proc. ICCV, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative learning of visual words for 3d human pose estimation", "author": ["N. Huazhong", "W. Xu", "Y. Gong", "T. Huang"], "venue": "Proc. CVPR, 2008, pp. 1\u20138.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Image clustering using local discriminant models and global integration", "author": ["Y. Yang", "D. Xu", "F. Nie", "S. Yan", "Y. Zhuang"], "venue": "IEEE Trans. Image Process., vol. 19, no. 10, pp. 2761\u20132773, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Recent studies have claimed that not all features in the high-dimensional feature space are discriminative and informative, since many features are often noisy or correlated to each other, which will deteriorate the performances of subsequent data analysing tasks [1] [2] [3].", "startOffset": 264, "endOffset": 267}, {"referenceID": 1, "context": "Recent studies have claimed that not all features in the high-dimensional feature space are discriminative and informative, since many features are often noisy or correlated to each other, which will deteriorate the performances of subsequent data analysing tasks [1] [2] [3].", "startOffset": 268, "endOffset": 271}, {"referenceID": 2, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "Consequently, feature selection is utilized to select a subset of features from the original high dimensional feature space [4] [5] [6] [7] [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "For example, [10] and [3] implement their methods in a supervised way and Ma et al.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "design their approach in a semi-supervise way in [5].", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "Recent researches have indicated that it is beneficial to learn multiple related tasks jointly [11] [12] [13].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Recent researches have indicated that it is beneficial to learn multiple related tasks jointly [11] [12] [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "Recent researches have indicated that it is beneficial to learn multiple related tasks jointly [11] [12] [13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "present a novel feature selection algorithm which leverages shared information from related tasks in [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 11, "context": "1 Feature selection Previous works have claimed that feature selection is capable of selecting the most representative features, thus facilitating subsequent data analysing tasks [14] [15] [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 12, "context": "1 Feature selection Previous works have claimed that feature selection is capable of selecting the most representative features, thus facilitating subsequent data analysing tasks [14] [15] [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "1 Feature selection Previous works have claimed that feature selection is capable of selecting the most representative features, thus facilitating subsequent data analysing tasks [14] [15] [16].", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "Classical feature selection algorithms, such as Fisher Score [9], evaluate the weights of all features, rank them accordingly and select the most discriminating features one by one [17].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "propose an algorithm which selects features jointly based on spectral regression with l2,1-norm constraint in [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "adopt l2,1-norm on both regularization term and loss function in [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "propose to select features by leveraging shared knowledge from multiple related tasks in [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 16, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "2 Semi-supervised learning Semi-supervised learning has shown its promising performance in different applications [19], [20], [21], [22], [23], [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 180, "endOffset": 184}, {"referenceID": 24, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 186, "endOffset": 190}, {"referenceID": 25, "context": "With semi-supervised learning, unlabeled training data can be exploited to learn data structure, which can save human labor cost for labeling a large amount of training data [25], [26], [27], [28].", "startOffset": 192, "endOffset": 196}, {"referenceID": 26, "context": "propose a manifold learning framework based on graph Laplacian and compared its performance with other state-of-the-art semi-supervised algorithms in [29].", "startOffset": 150, "endOffset": 154}, {"referenceID": 3, "context": "propose a semi-supervised feature selection algorithm built upon manifold learning in [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 27, "context": "In [30], Yang et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "3 Multi-task learning Multi-task learning has been widely used in many applications with the appealing advantage that it learns multiple related tasks with a shared representation [11] [12] [31].", "startOffset": 180, "endOffset": 184}, {"referenceID": 9, "context": "3 Multi-task learning Multi-task learning has been widely used in many applications with the appealing advantage that it learns multiple related tasks with a shared representation [11] [12] [31].", "startOffset": 185, "endOffset": 189}, {"referenceID": 28, "context": "3 Multi-task learning Multi-task learning has been widely used in many applications with the appealing advantage that it learns multiple related tasks with a shared representation [11] [12] [31].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "propose a novel multi-task feature selection algorithm which improves feature selection performance by leveraging shared information among multiple related tasks [6].", "startOffset": 162, "endOffset": 165}, {"referenceID": 4, "context": "In [6], Ma et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Following the works in [5] [6], we adopt the least square loss function for its simplicity and efficiency.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Following the works in [5] [6], we adopt the least square loss function for its simplicity and efficiency.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "Recent works [10] [17] claim that minimizing the regularization term \u2016Wl\u20162,1 makes Wl sparse, which demonstrates that Wl is especially suitable for feature selection.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "Recent works [10] [17] claim that minimizing the regularization term \u2016Wl\u20162,1 makes Wl sparse, which demonstrates that Wl is especially suitable for feature selection.", "startOffset": 18, "endOffset": 22}, {"referenceID": 29, "context": "Motivated by the works in [32] [6], we propose to leverage shared knowledge among multiple related tasks by minimizing the trace norm of W .", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "Motivated by the works in [32] [6], we propose to leverage shared knowledge among multiple related tasks by minimizing the trace norm of W .", "startOffset": 31, "endOffset": 34}, {"referenceID": 27, "context": "We propose to leverage semi-supervised learning by adopting the Laplacian proposed in [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 27, "context": "Inspired by [30], we construct the Laplacian matrix by exploiting both manifold structure and local discriminant information.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "(5) Note that Manifold Regularization is able to explore the manifold structure possessed by multimedia data [29] [33] [34].", "startOffset": 109, "endOffset": 113}, {"referenceID": 30, "context": "(5) Note that Manifold Regularization is able to explore the manifold structure possessed by multimedia data [29] [33] [34].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "(5) Note that Manifold Regularization is able to explore the manifold structure possessed by multimedia data [29] [33] [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "According to [19] [5], Fl can be obtained as follows:", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "According to [19] [5], Fl can be obtained as follows:", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "Following the work in [5], we incorporate (7) into (6).", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "Following the works in [10] [6] [5], we have:", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Following the works in [10] [6] [5], we have:", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "Following the works in [10] [6] [5], we have:", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "(20) According to Lemma 1 in [6], we have:", "startOffset": 29, "endOffset": 32}, {"referenceID": 32, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 135, "endOffset": 139}, {"referenceID": 34, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 171, "endOffset": 175}, {"referenceID": 35, "context": "1 Experiment Setup We use four different datasets in the experiment, including one video datasets CCV [35], one image datasets NUSWIDE [36], one human motion dataset HMDB [37] and one 3D motion skeleton dataset HumanEva [38].", "startOffset": 220, "endOffset": 224}, {"referenceID": 7, "context": "3) Feature Selection via Joint l2,1-Norms Minimization (FSNM): Joint l2,1-norm minimization is utilized on both loss function and regularization for joint feature selection [10].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "4) SPEC: It uses spectral graph theory to conduct feature selection [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "5) Feature Selection with Shared Information among multiple tasks (FSSI): It simultaneously learns multiple feature selection functions of different tasks in a joint framework [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 36, "context": "within-class graph and between-class graph [39].", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "7) Structural Feature Selection with Sparsity (SFSS): It combines strengths of joint feature selection and semi-supervised learning into a single framework [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "Following [5], we fix it at 15.", "startOffset": 10, "endOffset": 13}, {"referenceID": 32, "context": "2 Video Classification First, we compare the performances of different algorithms in terms of video classification task using Columbia Consumer Video dataset (CCV) [35].", "startOffset": 164, "endOffset": 168}, {"referenceID": 32, "context": "Since the original videos of this dataset have not been available on the internet, we directly use the STIP features with 5, 000 dimensional BoWs representation provided by [35].", "startOffset": 173, "endOffset": 177}, {"referenceID": 33, "context": "3 Image Annotation We use NUS-WIDE dataset [36] to test the performance of our algorithm.", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "4 Human Motion Recognition We use HMDB video dataset [37] to compare the algorithms in terms of human motion recognition.", "startOffset": 53, "endOffset": 57}, {"referenceID": 37, "context": "claim that motion boundary histograms (MBH) is an efficient way to suppress camera motion in [40] and thus it is used to process the videos.", "startOffset": 93, "endOffset": 97}, {"referenceID": 38, "context": "Following the work in [41] [42], we randomly select 10, 000 samples of two subjects (5, 000 per subject).", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "Following the work in [41] [42], we randomly select 10, 000 samples of two subjects (5, 000 per subject).", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "In this paper, we propose a novel semi-supervised feature selection framework by mining correlations among multiple tasks and apply it to different multimedia applications. Instead of independently computing the importance of features for each task, our algorithm leverages shared knowledge from multiple related tasks, thus, improving the performance of feature selection. Note that we build our algorithm on assumption that different tasks share common structures. The proposed algorithm selects features in a batch mode, by which the correlations between different features are taken into consideration. Besides, considering the fact that labeling a large amount of training data in real world is both time-consuming and tedious, we adopt manifold learning which exploits both labeled and unlabeled training data for feature space analysis. Since the objective function is non-smooth and difficult to solve, we propose an iterative algorithm with fast convergence. Extensive experiments on different applications demonstrate that our algorithm outperforms other state-of-the-art feature selection algorithms.", "creator": "LaTeX with hyperref package"}}}