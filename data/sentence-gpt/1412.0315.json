{"id": "1412.0315", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2014", "title": "Lifted Probabilistic Inference for Asymmetric Graphical Models", "abstract": "Lifted probabilistic inference algorithms have been successfully applied to a large number of symmetric graphical models. Unfortunately, the majority of real-world graphical models is asymmetric. This is even the case for relational representations when evidence is given. Therefore, more recent work in the community moved to making the models symmetric and then applying existing lifted inference algorithms. However, this approach has two shortcomings. First, all existing over-symmetric approximations require a relational representation such as Markov logic networks. Second, the induced symmetries often change the distribution significantly, making the computed probabilities highly biased. We present a framework for probabilistic sampling-based inference that only uses the induced approximate symmetries to propose steps in a Metropolis-Hastings style Markov chain. The framework, therefore, leads to improved probability estimates while remaining unbiased. Experiments demonstrate that the approach outperforms existing MCMC algorithms. Additionally, we also present an approach that offers a much more flexible way of understanding and performing probabilistic inference algorithms.\n\n\n\n\nM. K. A. [19:22\u201325, 1993, 2007]\nIntroduction to Machine Learning (MIS)\nTo understand machine learning, consider the following main question: How did the problem fit in the model? Specifically, how did the model fit in a context of machine learning? A recent study of computer models suggested that the model fit in a context of the model, as well as other computer models, were not properly trained in that context. The problem posed by the model could be explained in terms of the underlying inference models: how do you choose a way of representing models that fit in the model? The problem posed by the model could be explained in terms of the underlying inference models: how do you choose a way of representing models that fit in the model? The problem posed by the model could be explained in terms of the underlying inference models: how do you choose a way of representing models that fit in the model? The problem posed by the model could be explained in terms of the underlying inference models: how do you choose a way of representing models that fit in the model? The problem posed by the model could be explained in terms of the underlying inference models: how do you choose a way of representing models that fit in the model? The problem posed by the model could be explained in terms of the underlying inference models: how do you choose a way of representing models that fit in the model? The problem posed by the model could be explained in terms", "histories": [["v1", "Mon, 1 Dec 2014 00:40:33 GMT  (997kb,D)", "http://arxiv.org/abs/1412.0315v1", "To appear in Proceedings of AAAI-2015"]], "COMMENTS": "To appear in Proceedings of AAAI-2015", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["guy van den broeck", "mathias niepert"], "accepted": true, "id": "1412.0315"}, "pdf": {"name": "1412.0315.pdf", "metadata": {"source": "META", "title": "Lifted Probabilistic Inference for Asymmetric Graphical Models", "authors": ["Guy Van den Broeck", "Mathias Niepert"], "emails": ["guy.vandenbroeck@cs.kuleuven.be", "mniepert@cs.washington.edu"], "sections": [{"heading": "Introduction", "text": "Probabilistic graphical models are successfully used in a wide range of applications. Inference in these models is intractable in general and, therefore, approximate algorithms are mostly applied. However, there are several probabilistic graphical models for which inference is tractable due to (conditional) independences and the resulting low treewidth (Darwiche 2009; Koller and Friedman 2009). Examples of the former class of models are chains and tree models. More recently, the AI community has uncovered additional statistical properties based on symmetries of the graphical model that render inference tractable (Niepert and Van den Broeck 2014). In the literature, approaches exploiting these symmetries are often referred to as lifted or symmetry-aware inference algorithms (Poole 2003; Kersting 2012).\nWhile lifted inference algorithms perform well for highly symmetric graphical models, they depend heavily on the presence of symmetries and perform worse for asymmetric models due to their computational overhead. This is especially unfortunate as numerous real-world graphical models are not symmetric. To bring the achievements of the lifted\ninference community to the mainstream of machine learning and uncertain reasoning it is crucial to explore ways to apply ideas from the lifted inference literature to inference problems in asymmetric graphical models.\nRecent work has introduced methods to generate symmetric approximations of probabilistic models (Van den Broeck and Darwiche 2013; Venugopal and Gogate 2014; Singla, Nath, and Domingos 2014). All of these approaches turn approximate symmetries, that is, symmetries that \u201calmost\u201d hold in the probabilistic models, into perfect symmetries, and proceed to apply lifted inference algorithms to the symmetrized model. These approaches were shown to perform well but are also limited in a fundamental way. The introduction of artificial symmetries results in marginal probabilities that are different from the ones of the original model. The per variable Kullback-Leibler divergence, a measure often used to assess the performance of approximate inference algorithms, might improve when these symmetries are induced but it is possible that the marginals the user actually cares about are highly biased. Of course, this is a potential problem in applications. For instance, consider a medical application where one queries the probability of diseases given symptoms. A symmetric approximation may perform well in terms of the KL divergence but might skew the probabilities of the most probable diseases to become equal. A major argument for graphical models is the need to detect subtle differences in the posterior, which becomes impossible when approximate symmetries skew the distribution.\nTo apply lifted inference to asymmetric graphical models we propose a completely novel approach. As in previous approaches, we compute a symmetric approximation of the original model but leverage the symmetrized model to compute a proposal distribution for a Metropolis-Hastings chain. The approach combines a base MCMC algorithm such as the Gibbs sampler with the Metropolis chain that performs jumps in the symmetric model. The novel framework allows us to utilize work on approximate symmetries such as color passing algorithms (Kersting et al. 2014) and lowrank Boolean matrix approximations (Van den Broeck and Darwiche 2013) while producing unbiased probability estimates. We identify properties of an approximate symmetry group that make it suitable for the novel lifted MetropolisHastings approach.\nWe conduct experiments where, for the first time, lifted\nar X\niv :1\n41 2.\n03 15\nv1 [\ncs .A\nI] 1\nD ec\n2 01\n4\ninference is applied to graphical models with no exact symmetries and no color-passing symmetries, and where every random variable has distinct soft evidence. The framework, therefore, leads to improved probability estimates while remaining unbiased. Experiments demonstrate that the approach outperforms existing MCMC algorithms."}, {"heading": "Background", "text": "We review some background on concepts and methods used throughout the remainder of the paper."}, {"heading": "Group Theory", "text": "A group is an algebraic structure (G, \u25e6), where G is a set closed under a binary associative operation \u25e6 with an identity element and a unique inverse for each element. We often write G rather than (G, \u25e6). A permutation group acting on a set \u2126 is a set of bijections g : \u2126 \u2192 \u2126 that form a group. Let \u2126 be a finite set and let G be a permutation group acting on \u2126. If \u03b1 \u2208 \u2126 and g \u2208 G we write \u03b1g to denote the image of \u03b1 under g. A cycle (\u03b11 \u03b12 ... \u03b1n) represents the permutation that maps \u03b11 to \u03b12, \u03b12 to \u03b13,..., and \u03b1n to \u03b11. Every permutation can be written as a product of disjoint cycles. A generating set R of a group is a subset of the group\u2019s elements such that every element of the group can be written as a product of finitely many elements of R and their inverses.\nWe define a relation \u223c on \u2126 with \u03b1 \u223c \u03b2 if and only if there is a permutation g \u2208 G such that \u03b1g = \u03b2. The relation partitions \u2126 into equivalence classes which we call orbits. We call this partition of \u2126 the orbit partition induced by G. We use the notation \u03b1G to denote the orbit {\u03b1g | g \u2208 G} containing \u03b1."}, {"heading": "Symmetries of Graphical Models", "text": "Symmetries of a set of random variables and graphical models have been formally defined in the lifted and symmetryaware probabilistic inference literature with concepts from group theory (Niepert 2012b; Bui, Huynh, and Riedel 2013). Definition 1. Let X be a set of discrete random variables with distribution \u03c0 and let \u2126 be the set of states (configurations) of X. We say that a permutation group G acting on \u2126 is an automorphism group for X if and only if for all x \u2208 \u2126 and all g \u2208 G we have that \u03c0(x) = \u03c0(xg).\nNote that we do not require the automorphism group to be maximal, that is, it can be a subgroup of a different automorphism group for the same set of random variables. Moreover, note that the definition of an automorphism group is independent of the particular representation of the probabilistic model. For particular representations, there are efficient algorithms for computing the automorphism groups exploiting the structure of relational and propositional graphical models (Niepert 2012b; 2012a; Bui, Huynh, and Riedel 2013).\nMost probabilistic models are asymmetric. For instance, the Ising model which is used in numerous applications, is asymmetric if we assume an external field as it leads to different unary potentials. However, we can make the model symmetric simply by assuming a constant external field. Figure 1 depicts this situation. The framework we propose in\nthis paper will take advantage of such an over-symmetric model without biasing the probability estimates."}, {"heading": "Exploiting Symmetries for Lifted Inference", "text": "The advent of high-level representations of probabilistic graphical models, such as plate models and relational representations (Getoor and Taskar 2007; De Raedt et al. 2008), have motivated a new class of lifted inference algorithms (Poole 2003). These algorithms exploit the high-level structure and symmetries to speed up inference (Kersting 2012). Surprisingly, they perform tractable inference even in the absence of conditional independencies.\nOur current understanding of exact lifted inference is that syntactic properties of relational representations permit efficient lifted inference (Van den Broeck 2011; Jaeger and Van den Broeck 2012; Van den Broeck 2013; Gribkoff, Van den Broeck, and Suciu 2014). The Appendix will review such representations, and Markov logic in particular. More recently, it has been shown that (partial) exchangeability as a statistical property can explain numerous results in this literature (Niepert and Van den Broeck 2014). Indeed, there are deep connections between automorphisms and exchangeability (Niepert 2012b; 2013; Bui, Huynh, and Riedel 2013; Bui, Huynh, and de Salvo Braz 2012). Moreover, the (fractional) automorphisms of the graphical model representation have been related to lifted inference and exploited for more efficient inference (Niepert 2012b; Bui, Huynh, and Riedel 2013; Noessner, Niepert, and Stuckenschmidt 2013; Mladenov and Kersting 2013). In particular, there are a number of sampling algorithms that take advantage of symmetries (Venugopal and Gogate 2012; Gogate, Jha, and Venugopal 2012). However, these approaches expect a relational representation and require the model to be symmetric."}, {"heading": "Finite Markov Chains", "text": "Given a finite set \u2126 a Markov chain defines a random walk (x0,x1, ...) on elements of \u2126 with the property that the conditional distribution of xn+1 given (x0,x1, ...,xn) depends only on xn. For all x,y \u2208 \u2126, P (x\u2192 y) is the chain\u2019s probability to transition from x to y, and P t(x \u2192 y) = P tx(y) the probability of being in state y after t steps if the chain\nstarts at state x. We often refer to the conditional probability matrix P as the kernel of the Markov chain. A Markov chain is irreducible if for all x,y \u2208 \u2126 there exists a t such that P t(x \u2192 y) > 0 and aperiodic if for all x \u2208 \u2126, gcd{t \u2265 1 | P t(x\u2192 x) > 0} = 1. Theorem 1. Any irreducible and aperiodic Markov chain has exactly one stationary distribution.\nA distribution \u03c0 on \u2126 is reversible for a Markov chain with state space \u2126 and transition probabilities P , if for every x,y \u2208 \u2126\n\u03c0(x)P (x\u2192 y) = \u03c0(y)P (y\u2192 x).\nWe say that a Markov chain is reversible if there exists a reversible distribution for it. The AI literature often refers to reversible Markov chains as Markov chains satisfying the detailed balance property.\nTheorem 2. Every reversible distribution for a Markov chain is also a stationary distribution for the chain.\nMarkov Chains for Probability Estimation Numerous approximate inference algorithms for probabilistic graphical models draw sample points from a Markov chain whose stationary distribution is that of the probabilistic model, and use the sample points to estimate marginal probabilities. Sampling approaches of this kind are referred to as Markov chain Monte Carlo methods. We discuss the Gibbs sampler, a sampling algorithm often used in practice.\nLet X be a finite set of random variables with probability distribution \u03c0. The Markov chain for the Gibbs sampler is a Markov chainM = (x0,x1, ...) which, being in state xt, performs the following steps at time t+ 1:\n1. Select a variable X \u2208 X uniformly at random; 2. Sample x\u2032t+1(X), the value of X in the state x \u2032 t+1, ac-\ncording to the conditional \u03c0-distribution of X given that all other variables take their values according to xt; and 3. Let x\u2032t+1(Y ) = xt(Y ) for all variables Y \u2208 X \\ {X}. The Gibbs chain is aperiodic and has \u03c0 as a stationary\ndistribution. If the chain is irreducible, then the marginal estimates based on sample points drawn from the chain are unbiased once the chain reaches the stationary distribution.\nTwo or more Markov chains can be combined by constructing mixtures and compositions of the kernels (Tierney 1994). Let P1 and P2 be the kernels for two Markov chains M1 and M2 both with stationary distribution \u03c0. Given a positive probability 0 < \u03b1 < 1, a mixture of the Markov chains is a Markov chain where, in each iteration, kernel P1 is applied with probability \u03b1 and kernel P2 with probability 1 \u2212 \u03b1. The resulting Markov chain has \u03c0 as a stationary distribution. The following result relates properties of the individual chains to properties of their mixture.\nTheorem 3 (Tierney 1994). A mixture of two Markov chains M1 andM2 is irreducible and aperiodic if at least one of the chains is irreducible and aperiodic.\nFor a more in-depth discussion of combining Markov chains and the application to machine learning, we refer the interested reader to an overview paper (Andrieu et al. 2003)."}, {"heading": "Mixing Symmetric and Asymmetric Markov Chains", "text": "We propose a novel MCMC framework that constructs mixtures of Markov chains where one of the chains operates on the approximate symmetries of the probabilistic model. The framework assumes a base Markov chain MB such as the Gibbs chain, the MC-SAT chain (Poon and Domingos 2006), or any other MCMC algorithm. We then construct a mixture of the base chain and an Orbital Metropolis chain which exploits approximate symmetries for its proposal distribution. Before we describe the approach in more detail, let us first review Metropolis samplers."}, {"heading": "Metropolis-Hastings Chains", "text": "The construction of a Metropolis-Hastings Markov chain is a popular general procedure for designing reversible Markov chains for MCMC-based estimation of marginal probabilities. Metropolis-Hastings chains are associated with a proposal distribution Q(\u00b7|x) that is utilized to propose a move to the next state given the current state x. The closer the proposal distribution to the distribution \u03c0 to be estimated, that is, the closer Q(x | xt) to \u03c0(x) for large t, the better the convergence properties of the Metropolis-Hastings chain.\nWe first describe the Metropolis algorithm, a special case of the Metropolis-Hastings algorithm (Ha\u0308ggstro\u0308m 2002). Let X be a finite set of random variables with probability distribution \u03c0 and let \u2126 be the set of states of the random variables. The Metropolis chain is governed by a transition graph G = (\u2126,E) whose nodes correspond to states of the random variables. Let n(x) be the set of neighbors of state x in G, that is, all states reachable from x with a single transition. The Metropolis chain with graph G and distribution \u03c0 has transition probabilities\nP (x\u2192 y) = 1 |n(x)| min { \u03c0(y)|n(x)| \u03c0(x)|n(y)| , 1 } , if x and y are neighbors 0, if x 6= y are not neighbors 1\u2212 \u2211 y\u2032\u2208n(x) 1 |n(x)| min { \u03c0(y\u2032)|n(x)| \u03c0(x)|n(y\u2032)| , 1 } , if x = y.\nBeing in state xt of the Markov chainM = (x0,x1, ...), the Metropolis sampler therefore performs the following steps at time t+ 1:\n1. Select a state y from n(xt), the neighbors of xt, uniformly at random;\n2. Let xt+1 = y with probability min { \u03c0(y)|n(x)| \u03c0(x)|n(y)| , 1 } ;\n3. Otherwise, let xt+1 = xt.\nNote that the proposal distribution Q(\u00b7|x) is simply the uniform distribution on the set of x\u2019s neighbors. It is straight-forward to show that \u03c0 is a stationary distribution for the Metropolis chain by showing that \u03c0 is a reversible distribution for it (Ha\u0308ggstro\u0308m 2002).\nNow, the performance of the Metropolis chain hinges on the structure of the graph G. We would like the graph structure to facilitate global moves between high probability\nmodes, as opposed to the local moves typically performed by MCMC chains. To design such a graph structure, we take advantage of approximate symmetries in the model."}, {"heading": "Orbital Metropolis Chains", "text": "We propose a novel class of orbital Metropolis chains that move between approximate symmetries of a distribution. The approximate symmetries form an automorphism group G. We will discuss approaches to obtain such an automorphism group in Section . Here, we introduce a novel Markov chain that takes advantage of the approximate symmetries.\nGiven a distribution \u03c0 over random variables X with state space \u2126, and a permutation group G acting on \u2126, the orbital Metropolis chainMS for G performs the following steps:\n1. Select a state y from xGt , the orbit of xt, uniformly at random;\n2. Let xt+1 = y with probability min { \u03c0(y) \u03c0(x) , 1 } ;\n3. Otherwise, let xt+1 = xt.\nNote that a permutation group acting on \u2126 partitions the states into disjoint orbits. The orbital Metropolis chain simply moves between states in the same orbit. Hence, two states in the same orbit have the same number of neighbors and, thus, the expressions cancel out in line 2 above. It is straight-forward to show that the chainMS is reversible and, hence, that it has \u03c0 as a stationary distribution. However, the chain is not irreducible as it never moves between states that are not symmetric with respect to the permutation group G. In the binary case, for example, it cannot reach states with a different Hamming weight from the initial state."}, {"heading": "Lifted Metropolis-Hastings", "text": "To obtain an irreducible Markov chain that exploits approximate symmetries, we construct a mixture of (a) some base chainMB with stationary distribution \u03c0 for which we know that it is irreducible and aperiodic; and (b) an orbital Metropolis chainMS. We can prove the following theorem. Theorem 4. Let X be a set of random variables with distribution \u03c0 and approximate automorphisms G. Moreover, let MB be an aperiodic and irreducible Markov chain with stationary distribution \u03c0, and letMS be the orbital Metropolis chain for X and G. The mixture ofMB andMS is aperiodic, irreducible, and has \u03c0 as its unique stationary distribution.\nThe mixture of the base chain and the orbital Metropolis chain has several advantages. First, it exploits the approximate symmetries of the model which was shown to be advantageous for marginal probability estimation (Van den Broeck and Darwiche 2013). Second, the mixture of Markov chains performs wide ranging moves via the orbital Metropolis chain, exploring the state space more efficiently and, therefore, improving the quality of the probability estimates. Figure 2 depicts the state space and the transition graph of (a) the Gibbs chain and (b) the mixture of the Gibbs chain and an orbital Metropolis chain. It illustrates that the mixture is able to more freely move about the state space by jumping between orbit states. For instance, moving from state 0110 to 1001 would require 4 steps of the Gibbs chain\nbut is possible in one step with the mixture of chains. The larger the size of the automorphism groups, the more densely connected is the transition graph. Since the moves of the orbital Metropolis chain are between approximately symmetric states of the random variables, it does not suffer from the problem of most proposals being rejected. We will be able to verify this hypothesis empirically.\nThe general Lifted Metropolis-Hastings framework can be summarized as follows.\n1. Obtain an approximate automorphism group G;\n2. Run the following mixture of Markov chains:\n(a) With probability 0 < \u03b1 < 1, apply the kernel of the base chainMB;\n(b) Otherwise, apply the kernel of the orbital Metropolis chainMS for G.\nNote that the proposed approach is a generalization of lifted MCMC (Niepert 2013; 2012b), essentially using it as a subroutine, and that all MH proposals are accepted if G is an automorphism group of the original model. Moreover, note that the framework allows one to combine multiple orbital Metropolis chains with a base chain."}, {"heading": "Approximate Symmetries", "text": "The Lifted Metropolis-Hastings algorithm assumes that a permutation group G is given, representing the approximate symmetries. We now discuss several approaches to the computation of such an automorphism group. While it is not possible to go into technical detail here, we will provide pointers to the relevant literature.\nThere exist several techniques to compute the exact symmetries of a graphical model and construct G; see (Niepert 2012b; Bui, Huynh, and Riedel 2013). The color refinement algorithm is also well-studied in lifted inference (Kersting et al. 2014). It can find (exact) orbits of random variables for a slightly weaker notion of symmetry, called fractional automorphism. These techniques all require some form of exact symmetry to be present in the model.\nDetecting approximate symmetries is a problem that is largely open. One key idea is that of an over-symmetric approximations (OSAs) (Van den Broeck and Darwiche 2013). Such approximations are derived from the original model by rendering the model more symmetric. After the computation of an over-symmetric model, we can apply existing tools for exact symmetry detection. Indeed, the exact symmetries of an approximate model are approximate symmetries of the exact model. These symmetrization techniques are indispensable to our algorithm.\nRelational Symmetrization Existing symmetrization techniques operate on relational representations, such as Markov logic networks (MLNs). The full paper reviews MLNs and shows a web page classification model. Relational models have numerous symmetries. For example, swapping the web pages A and B does not change the MLN. This permutation of constants induces a permutations of random variables (e.g., between Page(A,Faculty) and Page(B,Faculty)). Unfortunately, hard and soft evidence breaks symmetries, even in highly symmetric relational models (Van den Broeck and Darwiche 2013). When the variables Page(A,Faculty) and Page(B,Faculty) get assigned distinct soft evidence, the symmetry between A and B is removed, and lifted inference breaks down.1 Similarly, when the Link relation is given, its graph is unlikely to be symmetric (Erdo\u030bs and Re\u0301nyi 1963), which in turn breaks the symmetries in the MLN. These observations motivated research on OSAs. Van den Broeck and Darwiche (2013) propose to approximate binary relations, such as Link, by a low-rank Boolean matrix factorization. Venugopal and Gogate (2014) cluster the constants in the domain of the MLN. Singla, Nath, and Domingos (2014) present a message-passing approach to clustering similar constants.\nPropositional Symmetrization A key property of our LMH algorithm is that it operates at the propositional level, regardless of how the graphical model was generated. It also means that the relational symmetrization approaches outlined above are inadequate in the general case. Unfortunately, we are not aware of any work on OSAs of propositional graphical models. However, some existing tech-\n1Solutions to this problem exist if the soft evidence is on a single unary relation (Bui, Huynh, and de Salvo Braz 2012)\nniques provide a promising direction. First, basic clustering can group together similar potentials. Second, the low-rank Boolean matrix factorization used for relational approximations can be applied to any graph structure, including graphical models. Third, color passing techniques for exact symmetries operate on propositional models (Kersting, Ahmadi, and Natarajan 2009; Kersting et al. 2014). Combined with early stopping, they can output approximate variable orbits.\nFrom OSAs to Automorphisms Given an OSA of our model, we need to compute an automorphism group G from it. The obvious choice is to compute the exact automorphisms from the OSA. While this works in principle, it may not be optimal. Let us first consider the following two concepts. When a group G operates on a set \u2126, only a subset of the elements in \u2126 can actually be mapped to an element other than itself. When \u2126 is the set of random variables, we call these elements the moved variables. When \u2126 is the set of potentials in a probabilistic graphical model, we call these the moved potentials. It is clear that we want G to move many random variables, as this will create the largest jumps and improve the mixing behavior. However, each LMH step comes at a cost: in the second step of the algorithm, the probability of the proposed approximately-symmetric state \u03c0(y) is estimated. This requires the re-evaluation of all potentials that are moved by G. Thus, the time complexity of an orbital Metropolis step is linear in the number of moved potentials. It will therefore be beneficial to construct subgroups of the automorphism group of the OSA and, in particular, ones that move many variables and few potentials. The full paper discusses a heuristic to construct such subgroups."}, {"heading": "Experiments", "text": "The LMH algorithm is implemented in the GAP algebra system which provides basic algorithms for automorphism groups such as the product replacement algorithm that allows one to sample uniformly from orbits of states (Niepert 2012b).\nFor our first experiments, we use the standard WebKB data set, consisting of web pages from four computer science departments (Craven and Slattery 2001). The data has information about approximately 800 words that appear on 1000 pages, 7 page labels and links between web pages. There are 4 folds, one for each university. We use the standard MLN structure for the WebKB domain, which has MLN formulas of the form shown above, but for all combinations of labels and words, adding up to around 5500 first-order MLN formulas. We learn the MLN parameters using Alchemy.\nWe consider a collective classification setting, where we are given the link structure and the word content of each web page, and want to predict the page labels. We run Gibbs sampling and the Lifted MCMC algorithm (Niepert 2012b), and show the average KL divergence between the estimated and true marginals in Figures 3 and 4. When true marginals are not computable, we used a very long run of a Gibbs sampler for the gold standard marginals. Since every web page contains a unique set of words, the evidence on the word content creates distinct soft evidence on the page labels. Moreover, the link structure is largely asymmetric and, therefore, there\nare no exploitable exact symmetries and Lifted MCMC coincides with Gibbs sampling. Next we construct an OSA using a rank-5 approximation of the link structure (Van den Broeck and Darwiche 2013) and group the potential weights into 6 clusters. From this OSA we construct a set of automorphisms that is efficient for LMH (see Appendix ). Figures 3 and 4 show that the LMH chain, with mixing parameter \u03b1 = 4/5, has a lower KL divergence than Gibbs and Lifted MCMC vs. the number of iterations. Note that there is a slight overhead to LMH because the orbital Metropolis chain is run between base chain steps. Despite this overhead, LMH outperforms the baselines as a function of time. The orbital Metropolis chain accepts approximately 70% of its proposals.\nFigure 5 illustrates the effect of running Lifted MCMC on OSA, which is the current state-of-the-art approach for asymmetric models. As expected, the drawn sample points produce biased estimates. As the quality of the approximation increases, the bias reduces, but so do the speedups. LMH does not suffer from a bias. Moreover, we observe that its performance is stable across different OSAs (not depicted).\nWe also ran experiments for two propositional models\nthat are frequently used in real world applications. The first model is a 100x100 ferromagnetic Ising model with constant interaction strength and external field (see Figure 1(a) for a 4x4 version). Due to the different potentials induced by the external field, the model has no symmetries. We use the model without external field to compute the approximate symmetries. The automorphism group representing these symmetries is generated by the rotational and reflectional symmetries of the grid model (see Figure 1(b)). As in the experiments with the relational models, we used the mixing parameter \u03b1 = 4/5 for the LMH algorithm. Figure 6(c) and (d) depicts the plots of the experimental results. The LMH algorithm performs better with respect to the number of iterations and, to a lesser extent, with respect to time.\nWe also ran experiments on the Chimera model which has recently received some attention as it was used to assess the performance of quantum annealing (Boixo et al. 2013). We used exactly the model as described in Boixo et al. (2013). This model is also asymmetric but can be made symmetric by assuming that all pairwise interactions are identical. The KL divergence vs. number of iterations and vs. time in seconds is plotted in Figure 6(a) and (b), respectively. Similar to the results for the Ising model, LMH outperforms Gibbs and LMCMC both with respect to the number of iterations and wall clock time. In summary, the LMH algorithm outperforms standard sampling algorithms on these propo-\nsitional models in the absence of any symmetries. We used very simple symmetrization strategies for the experiments. This demonstrates that the LMH framework is powerful and allows one to design state-of-the-art sampling algorithms."}, {"heading": "Conclusions", "text": "We have presented a Lifted Metropolis-Hastings algorithms capable of mixing two types of Markov chains. The first is a non-lifted base chain, and the second is an orbital Metropolis chain that moves between approximately symmetric states. This allows lifted inference techniques to be applied to asymmetric graphical models.\nAcknowledgments This work was supported by the Research Foundation-Flanders (FWO-Vlaanderen)."}, {"heading": "Markov Logic Networks", "text": "We first introduce some standard concepts from first-order logic. An atom P (t1, . . . , tn) consists of a predicate P/n of arity n followed by n argument terms ti, which are either constants, {A,B, . . . } or logical variables {x, y, . . . }. A formula combines atoms with connectives (e.g., \u2227, \u21d4). A formula is ground if it contains no logical variables. The groundings of a formula are obtained by instantiating the variables with particular constants.\nMany statistical relational languages have been proposed in recent years (Getoor and Taskar 2007; De Raedt et al. 2008). One such language is Markov logic networks (MLN) (Richardson and Domingos 2006). An MLN is a set of tuples (w, f), where w is a real number representing a weight and f is a formula in first-order logic. Consider for example the MLN\n1.3 Page(x,Faculty) \u21d2 HasWord(x,Hours) 1.5 Page(x,Faculty) \u2227 Link(x, y) \u21d2 Page(y,Course)\nwhich states that faculty web pages are more likely to contain the word \u201chours\u201d, and that faculty pages are more likely to link to course pages.\nGiven a domain of constants D, a first-order MLN \u2206 induces a grounding, which is the MLN obtained by replacing each formula in \u2206 with all its groundings. For the domain D = {A,B} (i.e., two pages), the MLN represents the following grounding.\n1.3 Page(A,Faculty) \u21d2 HasWord(A,Hours) 1.3 Page(B,Faculty) \u21d2 HasWord(B,Hours) 1.5 Page(A,Faculty) \u2227 Link(A,B) \u21d2 Page(B,Course) 1.5 Page(B,Faculty) \u2227 Link(B,A) \u21d2 Page(A,Course) 1.5 Page(A,Faculty) \u2227 Link(A,A) \u21d2 Page(A,Course) 1.5 Page(B,Faculty) \u2227 Link(B,B) \u21d2 Page(B,Course)\nThis grounding has ten random variables, yielding a distribution over 210 possible worlds. The weight of each world is the product of the expressions exp(w), where (w, f) is a ground MLN formula and f is satisfied by the world."}, {"heading": "Approximate Automorphism Heuristic", "text": "Given an OSA, we construct a set of approximate automorphisms as follows. First, we compute the exact automorphisms G1 of the OSA. Second, we compute the variable orbits of G1, grouping together all variables that can be mapped into each other. Then, for every orbit O, we construct a set of automorphisms as follows. We greedily search\nfor aO\u2032 \u2286 O such that the symmetric group GO\u2032 onO\u2032 maximizes the ratio between the number of moved variables (i.e., |O\u2032|) and the number of moved potentials, while keeping the number of moved potentials bounded by a constant K. This guarantees that GO\u2032 yields an efficient orbital Metropolis chain. Finally, we remove O\u2032 from O and recurse until O is empty. From this set of symmetric groups GO\u2032 , we construct a set of orbital Metropolis chains, each with it own set of moved potentials."}], "references": [{"title": "An introduction to MCMC for machine learning. Machine Learning 50(1-2):5\u201343", "author": ["C. Andrieu", "N. de Freitas", "A. Doucet", "M.I. Jordan"], "venue": null, "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "Quantum annealing with more than one hundred qubits", "author": ["S. Boixo", "T.F. R\u00f8nnow", "S.V. Isakov", "Z. Wang", "D. Wecker", "D.A. Lidar", "J.M. Martinis", "M. Troyer"], "venue": "Nature Physics 10(3):218\u2013224.", "citeRegEx": "Boixo et al\\.,? 2013", "shortCiteRegEx": "Boixo et al\\.", "year": 2013}, {"title": "Exact lifted inference with distinct soft evidence on every object", "author": ["H. Bui", "T. Huynh", "R. de Salvo Braz"], "venue": "In Proceedings of the 26th Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Bui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2012}, {"title": "Automorphism groups of graphical models and lifted variational inference", "author": ["H. Bui", "T. Huynh", "S. Riedel"], "venue": "Proceedings of the 29th Annual Conference on Uncertainty in Artificial Intelligence (UAI).", "citeRegEx": "Bui et al\\.,? 2013", "shortCiteRegEx": "Bui et al\\.", "year": 2013}, {"title": "Relational learning with statistical predicate invention: Better models for hypertext", "author": ["M. Craven", "S. Slattery"], "venue": "Machine Learning Journal 43(1/2):97\u2013119.", "citeRegEx": "Craven and Slattery,? 2001", "shortCiteRegEx": "Craven and Slattery", "year": 2001}, {"title": "Modeling and reasoning with Bayesian networks", "author": ["A. Darwiche"], "venue": "Cambridge University Press.", "citeRegEx": "Darwiche,? 2009", "shortCiteRegEx": "Darwiche", "year": 2009}, {"title": "Probabilistic inductive logic programming: theory and applications", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "Muggleton", "eds"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2008}, {"title": "Asymmetric graphs", "author": ["P. Erd\u0151s", "A. R\u00e9nyi"], "venue": "Acta Mathematica Hungarica 14(3):295\u2013315.", "citeRegEx": "Erd\u0151s and R\u00e9nyi,? 1963", "shortCiteRegEx": "Erd\u0151s and R\u00e9nyi", "year": 1963}, {"title": "Introduction to Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar"], "venue": "The MIT Press.", "citeRegEx": "Getoor and Taskar,? 2007", "shortCiteRegEx": "Getoor and Taskar", "year": 2007}, {"title": "Advances in lifted importance sampling", "author": ["V. Gogate", "A.K. Jha", "D. Venugopal"], "venue": "Proceedings of the 26th Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Gogate et al\\.,? 2012", "shortCiteRegEx": "Gogate et al\\.", "year": 2012}, {"title": "Understanding the complexity of lifted inference and asymmetric weighted model counting", "author": ["E. Gribkoff", "G. Van den Broeck", "D. Suciu"], "venue": "Proceedings of UAI.", "citeRegEx": "Gribkoff et al\\.,? 2014", "shortCiteRegEx": "Gribkoff et al\\.", "year": 2014}, {"title": "Finite Markov chains and algorithmic applications", "author": ["O. H\u00e4ggstr\u00f6m"], "venue": "London Mathematical Society student texts. Cambridge University Press.", "citeRegEx": "H\u00e4ggstr\u00f6m,? 2002", "shortCiteRegEx": "H\u00e4ggstr\u00f6m", "year": 2002}, {"title": "Liftability of probabilistic inference: Upper and lower bounds", "author": ["M. Jaeger", "G. Van den Broeck"], "venue": "Pro-", "citeRegEx": "Jaeger and Broeck,? 2012", "shortCiteRegEx": "Jaeger and Broeck", "year": 2012}, {"title": "Counting belief propagation", "author": ["K. Kersting", "B. Ahmadi", "S. Natarajan"], "venue": "Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence, 277\u2013284.", "citeRegEx": "Kersting et al\\.,? 2009", "shortCiteRegEx": "Kersting et al\\.", "year": 2009}, {"title": "Power iterated color refinement", "author": ["K. Kersting", "M. Mladenov", "R. Garnett", "M. Grohe"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1904\u20131910.", "citeRegEx": "Kersting et al\\.,? 2014", "shortCiteRegEx": "Kersting et al\\.", "year": 2014}, {"title": "Lifted probabilistic inference", "author": ["K. Kersting"], "venue": "Proceedings of European Conference on Artificial Intelligence (ECAI).", "citeRegEx": "Kersting,? 2012", "shortCiteRegEx": "Kersting", "year": 2012}, {"title": "Probabilistic Graphical Models", "author": ["D. Koller", "N. Friedman"], "venue": "The MIT Press.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Lifted inference via k-locality", "author": ["M. Mladenov", "K. Kersting"], "venue": "Proceedings of the 3rd International Workshop on Statistical Relational AI.", "citeRegEx": "Mladenov and Kersting,? 2013", "shortCiteRegEx": "Mladenov and Kersting", "year": 2013}, {"title": "Tractability through exchangeability: A new perspective on efficient probabilistic inference", "author": ["M. Niepert", "G. Van den Broeck"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2467\u20132475.", "citeRegEx": "Niepert and Broeck,? 2014", "shortCiteRegEx": "Niepert and Broeck", "year": 2014}, {"title": "Lifted probabilistic inference: An mcmc perspective", "author": ["M. Niepert"], "venue": "Proceedings of the 2nd International Workshop on Statistical Relational AI (StaRAI).", "citeRegEx": "Niepert,? 2012a", "shortCiteRegEx": "Niepert", "year": 2012}, {"title": "Markov chains on orbits of permutation groups", "author": ["M. Niepert"], "venue": "Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI).", "citeRegEx": "Niepert,? 2012b", "shortCiteRegEx": "Niepert", "year": 2012}, {"title": "Symmetry-aware marginal density estimation", "author": ["M. Niepert"], "venue": "Proceedings of the 27th Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Niepert,? 2013", "shortCiteRegEx": "Niepert", "year": 2013}, {"title": "RockIt: Exploiting Parallelism and Symmetry for MAP Inference in Statistical Relational Models", "author": ["J. Noessner", "M. Niepert", "H. Stuckenschmidt"], "venue": "Proceedings of the 27th Conference on Artificial Intelligence (AAAI).", "citeRegEx": "Noessner et al\\.,? 2013", "shortCiteRegEx": "Noessner et al\\.", "year": 2013}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 985\u2013991.", "citeRegEx": "Poole,? 2003", "shortCiteRegEx": "Poole", "year": 2003}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["H. Poon", "P. Domingos"], "venue": "Proceedings of the 21st Conference on Artificial Intelligence (AAAI), 458\u2013463.", "citeRegEx": "Poon and Domingos,? 2006", "shortCiteRegEx": "Poon and Domingos", "year": 2006}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine learning 62(1-2):107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Approximate lifting techniques for belief propagation", "author": ["P. Singla", "A. Nath", "P. Domingos"], "venue": "Proceedings of AAAI.", "citeRegEx": "Singla et al\\.,? 2014", "shortCiteRegEx": "Singla et al\\.", "year": 2014}, {"title": "Markov chains for exploring posterior distributions", "author": ["L. Tierney"], "venue": "The Annals of Statistics 22(4):1701\u20131728.", "citeRegEx": "Tierney,? 1994", "shortCiteRegEx": "Tierney", "year": 1994}, {"title": "On the complexity and approximation of binary evidence in lifted inference", "author": ["G. Van den Broeck", "A. Darwiche"], "venue": "NIPS, 2868\u20132876.", "citeRegEx": "Broeck and Darwiche,? 2013", "shortCiteRegEx": "Broeck and Darwiche", "year": 2013}, {"title": "On the completeness of firstorder knowledge compilation for lifted probabilistic inference", "author": ["G. Van den Broeck"], "venue": "Advances in Neural Information Processing Systems 24 (NIPS),, 1386\u20131394.", "citeRegEx": "Broeck,? 2011", "shortCiteRegEx": "Broeck", "year": 2011}, {"title": "Lifted Inference and Learning", "author": ["G. Van den Broeck"], "venue": null, "citeRegEx": "Broeck,? \\Q2013\\E", "shortCiteRegEx": "Broeck", "year": 2013}, {"title": "On lifting the gibbs", "author": ["D. Venugopal", "V. Gogate"], "venue": null, "citeRegEx": "Venugopal and Gogate,? \\Q2012\\E", "shortCiteRegEx": "Venugopal and Gogate", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "However, there are several probabilistic graphical models for which inference is tractable due to (conditional) independences and the resulting low treewidth (Darwiche 2009; Koller and Friedman 2009).", "startOffset": 158, "endOffset": 199}, {"referenceID": 16, "context": "However, there are several probabilistic graphical models for which inference is tractable due to (conditional) independences and the resulting low treewidth (Darwiche 2009; Koller and Friedman 2009).", "startOffset": 158, "endOffset": 199}, {"referenceID": 23, "context": "In the literature, approaches exploiting these symmetries are often referred to as lifted or symmetry-aware inference algorithms (Poole 2003; Kersting 2012).", "startOffset": 129, "endOffset": 156}, {"referenceID": 15, "context": "In the literature, approaches exploiting these symmetries are often referred to as lifted or symmetry-aware inference algorithms (Poole 2003; Kersting 2012).", "startOffset": 129, "endOffset": 156}, {"referenceID": 14, "context": "The novel framework allows us to utilize work on approximate symmetries such as color passing algorithms (Kersting et al. 2014) and lowrank Boolean matrix approximations (Van den Broeck and Darwiche 2013) while producing unbiased probability estimates.", "startOffset": 105, "endOffset": 127}, {"referenceID": 20, "context": "Symmetries of a set of random variables and graphical models have been formally defined in the lifted and symmetryaware probabilistic inference literature with concepts from group theory (Niepert 2012b; Bui, Huynh, and Riedel 2013).", "startOffset": 187, "endOffset": 231}, {"referenceID": 20, "context": "For particular representations, there are efficient algorithms for computing the automorphism groups exploiting the structure of relational and propositional graphical models (Niepert 2012b; 2012a; Bui, Huynh, and Riedel 2013).", "startOffset": 175, "endOffset": 226}, {"referenceID": 8, "context": "The advent of high-level representations of probabilistic graphical models, such as plate models and relational representations (Getoor and Taskar 2007; De Raedt et al. 2008), have motivated a new class of lifted inference algorithms (Poole 2003).", "startOffset": 128, "endOffset": 174}, {"referenceID": 23, "context": "2008), have motivated a new class of lifted inference algorithms (Poole 2003).", "startOffset": 65, "endOffset": 77}, {"referenceID": 15, "context": "These algorithms exploit the high-level structure and symmetries to speed up inference (Kersting 2012).", "startOffset": 87, "endOffset": 102}, {"referenceID": 20, "context": "Indeed, there are deep connections between automorphisms and exchangeability (Niepert 2012b; 2013; Bui, Huynh, and Riedel 2013; Bui, Huynh, and de Salvo Braz 2012).", "startOffset": 77, "endOffset": 163}, {"referenceID": 20, "context": "Moreover, the (fractional) automorphisms of the graphical model representation have been related to lifted inference and exploited for more efficient inference (Niepert 2012b; Bui, Huynh, and Riedel 2013; Noessner, Niepert, and Stuckenschmidt 2013; Mladenov and Kersting 2013).", "startOffset": 160, "endOffset": 276}, {"referenceID": 17, "context": "Moreover, the (fractional) automorphisms of the graphical model representation have been related to lifted inference and exploited for more efficient inference (Niepert 2012b; Bui, Huynh, and Riedel 2013; Noessner, Niepert, and Stuckenschmidt 2013; Mladenov and Kersting 2013).", "startOffset": 160, "endOffset": 276}, {"referenceID": 31, "context": "In particular, there are a number of sampling algorithms that take advantage of symmetries (Venugopal and Gogate 2012; Gogate, Jha, and Venugopal 2012).", "startOffset": 91, "endOffset": 151}, {"referenceID": 27, "context": "Two or more Markov chains can be combined by constructing mixtures and compositions of the kernels (Tierney 1994).", "startOffset": 99, "endOffset": 113}, {"referenceID": 27, "context": "Theorem 3 (Tierney 1994).", "startOffset": 10, "endOffset": 24}, {"referenceID": 0, "context": "For a more in-depth discussion of combining Markov chains and the application to machine learning, we refer the interested reader to an overview paper (Andrieu et al. 2003).", "startOffset": 151, "endOffset": 172}, {"referenceID": 24, "context": "The framework assumes a base Markov chain MB such as the Gibbs chain, the MC-SAT chain (Poon and Domingos 2006), or any other MCMC algorithm.", "startOffset": 87, "endOffset": 111}, {"referenceID": 11, "context": "We first describe the Metropolis algorithm, a special case of the Metropolis-Hastings algorithm (H\u00e4ggstr\u00f6m 2002).", "startOffset": 96, "endOffset": 112}, {"referenceID": 11, "context": "It is straight-forward to show that \u03c0 is a stationary distribution for the Metropolis chain by showing that \u03c0 is a reversible distribution for it (H\u00e4ggstr\u00f6m 2002).", "startOffset": 146, "endOffset": 162}, {"referenceID": 21, "context": "Note that the proposed approach is a generalization of lifted MCMC (Niepert 2013; 2012b), essentially using it as a subroutine, and that all MH proposals are accepted if G is an automorphism group of the original model.", "startOffset": 67, "endOffset": 88}, {"referenceID": 20, "context": "There exist several techniques to compute the exact symmetries of a graphical model and construct G; see (Niepert 2012b; Bui, Huynh, and Riedel 2013).", "startOffset": 105, "endOffset": 149}, {"referenceID": 14, "context": "The color refinement algorithm is also well-studied in lifted inference (Kersting et al. 2014).", "startOffset": 72, "endOffset": 94}, {"referenceID": 7, "context": "1 Similarly, when the Link relation is given, its graph is unlikely to be symmetric (Erd\u0151s and R\u00e9nyi 1963), which in turn breaks the symmetries in the MLN.", "startOffset": 84, "endOffset": 106}, {"referenceID": 5, "context": "Unfortunately, hard and soft evidence breaks symmetries, even in highly symmetric relational models (Van den Broeck and Darwiche 2013). When the variables Page(A,Faculty) and Page(B,Faculty) get assigned distinct soft evidence, the symmetry between A and B is removed, and lifted inference breaks down.1 Similarly, when the Link relation is given, its graph is unlikely to be symmetric (Erd\u0151s and R\u00e9nyi 1963), which in turn breaks the symmetries in the MLN. These observations motivated research on OSAs. Van den Broeck and Darwiche (2013) propose to approximate binary relations, such as Link, by a low-rank Boolean matrix factorization.", "startOffset": 120, "endOffset": 540}, {"referenceID": 5, "context": "Unfortunately, hard and soft evidence breaks symmetries, even in highly symmetric relational models (Van den Broeck and Darwiche 2013). When the variables Page(A,Faculty) and Page(B,Faculty) get assigned distinct soft evidence, the symmetry between A and B is removed, and lifted inference breaks down.1 Similarly, when the Link relation is given, its graph is unlikely to be symmetric (Erd\u0151s and R\u00e9nyi 1963), which in turn breaks the symmetries in the MLN. These observations motivated research on OSAs. Van den Broeck and Darwiche (2013) propose to approximate binary relations, such as Link, by a low-rank Boolean matrix factorization. Venugopal and Gogate (2014) cluster the constants in the domain of the MLN.", "startOffset": 120, "endOffset": 667}, {"referenceID": 5, "context": "Unfortunately, hard and soft evidence breaks symmetries, even in highly symmetric relational models (Van den Broeck and Darwiche 2013). When the variables Page(A,Faculty) and Page(B,Faculty) get assigned distinct soft evidence, the symmetry between A and B is removed, and lifted inference breaks down.1 Similarly, when the Link relation is given, its graph is unlikely to be symmetric (Erd\u0151s and R\u00e9nyi 1963), which in turn breaks the symmetries in the MLN. These observations motivated research on OSAs. Van den Broeck and Darwiche (2013) propose to approximate binary relations, such as Link, by a low-rank Boolean matrix factorization. Venugopal and Gogate (2014) cluster the constants in the domain of the MLN. Singla, Nath, and Domingos (2014) present a message-passing approach to clustering similar constants.", "startOffset": 120, "endOffset": 749}, {"referenceID": 14, "context": "Third, color passing techniques for exact symmetries operate on propositional models (Kersting, Ahmadi, and Natarajan 2009; Kersting et al. 2014).", "startOffset": 85, "endOffset": 145}, {"referenceID": 20, "context": "The LMH algorithm is implemented in the GAP algebra system which provides basic algorithms for automorphism groups such as the product replacement algorithm that allows one to sample uniformly from orbits of states (Niepert 2012b).", "startOffset": 215, "endOffset": 230}, {"referenceID": 4, "context": "For our first experiments, we use the standard WebKB data set, consisting of web pages from four computer science departments (Craven and Slattery 2001).", "startOffset": 126, "endOffset": 152}, {"referenceID": 20, "context": "We run Gibbs sampling and the Lifted MCMC algorithm (Niepert 2012b), and show the average KL divergence between the estimated and true marginals in Figures 3 and 4.", "startOffset": 52, "endOffset": 67}, {"referenceID": 1, "context": "We also ran experiments on the Chimera model which has recently received some attention as it was used to assess the performance of quantum annealing (Boixo et al. 2013).", "startOffset": 150, "endOffset": 169}, {"referenceID": 1, "context": "We also ran experiments on the Chimera model which has recently received some attention as it was used to assess the performance of quantum annealing (Boixo et al. 2013). We used exactly the model as described in Boixo et al. (2013). This model is also asymmetric but can be made symmetric by assuming that all pairwise interactions are identical.", "startOffset": 151, "endOffset": 233}], "year": 2014, "abstractText": "Lifted probabilistic inference algorithms have been successfully applied to a large number of symmetric graphical models. Unfortunately, the majority of realworld graphical models is asymmetric. This is even the case for relational representations when evidence is given. Therefore, more recent work in the community moved to making the models symmetric and then applying existing lifted inference algorithms. However, this approach has two shortcomings. First, all existing over-symmetric approximations require a relational representation such as Markov logic networks. Second, the induced symmetries often change the distribution significantly, making the computed probabilities highly biased. We present a framework for probabilistic sampling-based inference that only uses the induced approximate symmetries to propose steps in a MetropolisHastings style Markov chain. The framework, therefore, leads to improved probability estimates while remaining unbiased. Experiments demonstrate that the approach outperforms existing MCMC algorithms.", "creator": "TeX"}}}