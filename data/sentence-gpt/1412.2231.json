{"id": "1412.2231", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2014", "title": "Generalized Singular Value Thresholding", "abstract": "This work studies the Generalized Singular Value Thresholding (GSVT) operator ${\\Prox}_{g}^{\\bm{\\sigma}}(\\cdot)$, \\begin{equation*} \\cdot \\left{right} \\cdot \\right{left} \\cdot \\right{right} \\cdot \\right{left} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{left} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\right{right} \\cdot \\", "histories": [["v1", "Sat, 6 Dec 2014 13:08:29 GMT  (1073kb,D)", "http://arxiv.org/abs/1412.2231v1", "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2015"]], "COMMENTS": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NA math.NA", "authors": ["canyi lu", "changbo zhu", "chunyan xu", "shuicheng yan", "zhouchen lin"], "accepted": true, "id": "1412.2231"}, "pdf": {"name": "1412.2231.pdf", "metadata": {"source": "CRF", "title": "Generalized Singular Value Thresholding", "authors": ["Canyi Lu", "Changbo Zhu", "Chunyan Xu", "Shuicheng Yan", "Zhouchen Lin"], "emails": ["canyilu@nus.edu.sg,", "zhuchangbo@gmail.com,", "xuchunyan01@gmail.com,", "eleyans@nus.edu.sg,", "zlin@pku.edu.cn"], "sections": [{"heading": null, "text": "Prox\u03c3g (B) = argmin X m\u2211 i=1 g(\u03c3i(X))+ 1 2 ||X\u2212B||2F , associated with a nonconvex function g defined on the singular values of X. We prove that GSVT can be obtained by performing the proximal operator of g (denoted as Proxg(\u00b7)) on the singular values since Proxg(\u00b7) is monotone when g is lower bounded. If the nonconvex g satisfies some conditions (many popular nonconvex surrogate functions, e.g., `p-norm, 0 < p < 1, of `0-norm are special cases), a general solver to find Proxg(b) is proposed for any b \u2265 0. GSVT greatly generalizes the known Singular Value Thresholding (SVT) which is a basic subroutine in many convex low rank minimization methods. We are able to solve the nonconvex low rank minimization problem by using GSVT in place of SVT.\nIntroduction The sparse and low rank structures have received much attention in recent years. There have been many applications which exploit these two structures, such as face recognition (Wright et al. 2009), subspace clustering (Cheng et al. 2010; Liu et al. 2013b) and background modeling (Cande\u0300s et al. 2011). To achieve sparsity, a principled approach is to use the convex `1-norm. However, the `1-minimization may be suboptimal, since the `1-norm is a loose approximation of the `0-norm and often leads to an over-penalized problem. This brings the attention back to the nonconvex surrogate by interpolating the `0-norm and `1-norm. Many nonconvex penalities have been proposed, including `p-norm (0 < p < 1) (Frank and Friedman 1993), Smoothly Clipped Absolute Deviation (SCAD) (Fan and Li 2001), Logarithm (Friedman 2012), Minimax Concave Penalty (MCP) (Zhang and others 2010), Geman (Geman and Yang 1995) and Laplace (Trzasko and Manduca 2009). Their definitions are shown in Table 1. Numerical studies (Cande\u0300s, Wakin, and Boyd 2008) have shown that the nonconvex optimization usually outperforms convex models.\n\u2217Corresponding author. Copyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nTable 1: Popular nonconvex surrogate functions of `0-norm (||\u03b8||0).\nPenalty Formula g(\u03b8), \u03b8 \u2265 0, \u03bb > 0 `p-norm \u03bb\u03b8p, 0 < p < 1.\nSCAD  \u03bb\u03b8, if \u03b8 \u2264 \u03bb, \u2212\u03b82+2\u03b3\u03bb\u03b8\u2212\u03bb2 2(\u03b3\u22121) , if \u03bb < \u03b8 \u2264 \u03b3\u03bb, \u03bb2(\u03b3+1)\n2 , if \u03b8 > \u03b3\u03bb.\nLogarithm \u03bb log(\u03b3+1) log(\u03b3\u03b8 + 1)\nMCP\n{ \u03bb\u03b8 \u2212 \u03b8 2\n2\u03b3 , if \u03b8 < \u03b3\u03bb,\n1 2 \u03b3\u03bb2, if \u03b8 \u2265 \u03b3\u03bb.\nGeman \u03bb\u03b8 \u03b8+\u03b3 . Laplace \u03bb(1\u2212 exp(\u2212 \u03b8\n\u03b3 )).\nThe low rank structure is an extension of sparsity defined on the singular values of a matrix. A principled way is to use the nuclear norm which is a convex surrogate of the rank function (Recht, Fazel, and Parrilo 2010). However, it suffers from the same suboptimal issue as the `1-norm in many cases. Very recently, many popular nonconvex surrogate functions in Table 1 are extended on the singular values to better approximate the rank function (Lu et al. 2014). However, different from the convex optimization, the nonconvex low rank minimization is much more challenging than the nonconvex sparse minimization.\nThe Iteratively Reweighted Nuclear Norm (IRNN) method is proposed to solve the following nonconvex low rank minimization problem (Lu et al. 2014)\nmin X F (X) = m\u2211 i=1 g(\u03c3i(X)) + h(X), (1)\nwhere \u03c3i(X) denotes the i-th singular value of X \u2208 Rm\u00d7n (we assume m \u2264 n in this work). g : R+ \u2192 R+ is continuous, concave and nonincreasing on [0,+\u221e). Popular nonconvex surrogate functions in Table 1 are some examples. h : Rm\u00d7n \u2192 R+ is the loss function which has Lipschitz continuous gradient. IRNN updates Xk+1 by minimizing a surrogate function which upper bounds the objective function in (9). The surrogate function is constructed by linearizing g and h at Xk, simultaneously. In theory, IRNN guarantees to decrease the objective function value of (9) in each iteration. However, it may decrease slowly since the upper\nar X\niv :1\n41 2.\n22 31\nv1 [\ncs .C\nV ]\n6 D\nec 2\n01 4\nbound surrogate may be quite loose. It is expected that minimizing a tighter surrogate will lead to a faster convergence.\nA possible tighter surrogate function of the objective function in (9) is to keep g and relax h only. This leads to the following updating rule which is named as Generalized Proximal Gradient (GPG) method in this work\nXk+1 = arg min X m\u2211 i=1 g(\u03c3i(X)) + h(X k)\n+ \u3008\u2207h(Xk),X\u2212Xk\u3009+ \u00b5 2 ||X\u2212Xk||2F\n= arg min X m\u2211 i=1 g(\u03c3i(X)) + \u00b5 2 ||X\u2212Xk + 1 \u00b5 \u2207h(Xk)||2F ,\n(2) where \u00b5 > L(h), L(h) is the Lipschitz constant of h, guarantees the convergence of GPG as shown later. It can be seen that solving (10) requires solving the following problem\nProx\u03c3g (B) = arg min X m\u2211 i=1 g(\u03c3i(X)) + 1 2 ||X\u2212B||2F . (3) In this work, the mapping Prox\u03c3g (\u00b7) is called the Generalized Singular Value Thresholding (GSVT) operator associated with the function \u2211m i=1 g(\u00b7) defined on the singular\nvalues. If g(x) = \u03bbx, \u2211m i=1 g(\u03c3i(X)) is degraded to the convex nuclear norm \u03bb||X||\u2217. Then (3) has a closed form solution Prox\u03c3g (B) = UDiag(D\u03bb(\u03c3(B)))VT , where D\u03bb(\u03c3(B)) = {(\u03c3i(B) \u2212 \u03bb)+}mi=1, and U and V are from the SVD of B, i.e., B = UDiag(\u03c3(B))VT . This is the known Singular Value Thresholding (SVT) operator associated with the convex nuclear norm (when g(x) = \u03bbx) (Cai, Cande\u0300s, and Shen 2010). More generally, for a convex g, the solution to (3) is\nProx\u03c3g (B) = UDiag(Proxg(\u03c3(B)))V T , (4)\nwhere Proxg(\u00b7) is defined element-wise as follows,\nProxg(b) = arg min x\u22650\nfb(x) = g(x) + 1\n2 (x\u2212 b)2, 1 (5)\n1For x < 0, g(x) = g(\u2212x). If b \u2265 0, Proxg(b) \u2265 0. If b < 0, Proxg(b) = \u2212Proxg(\u2212b). So we only need to discuss the case b, x \u2265 0 in this work.\nwhere Proxg(\u00b7) is the known proximal operator associated with a convex g (Combettes and Pesquet 2011). That is to say, solving (3) is equivalent to performing Proxg(\u00b7) on each singular value of B. In this case, the mapping Proxg(\u00b7) is unique, i.e., (5) has a unique solution. More importantly, Proxg(\u00b7) is monotone, i.e., Proxg(x1) \u2265 Proxg(x2) for any x1 \u2265 x2. This guarantees to preserve the nonincreasing order of the singular values after shrinkage and thresholding by the mapping Proxg(\u00b7). For a nonconvex g, we still call Proxg(\u00b7) as the proximal operator, but note that such a mapping may not be unique. It is still an open problem whether Proxg(\u00b7) is monotone or not for a nonconvex g. Without proving the monotonity of Proxg(\u00b7), one cannot simply perform it on the singular values of B to obtain the solution to (3) as SVT. Even if Proxg(\u00b7) is monotone, since it is not unique, one also needs to carefully choose the solution pi \u2208 Proxg(\u03c3i(B)) such that p1 \u2265 p2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 pm. Another challenging problem is that there does not exist a general solver to (5) for a general nonconvex g.\nIt is worth mentioning that some previous works studied the solution to (3) for some special choices of nonconvex g (Nie, Huang, and Ding 2012; Chartrand 2012; Liu et al. 2013a). However, none of their proofs was rigorous since they ignored proving the monotone property of Proxg(\u00b7). See the detailed discussions in the next section. Another recent work (Gu et al. 2014) considered the following problem related to the weighted nuclear norm:\nmin X fw,B(X) = m\u2211 i=1 wi\u03c3i(X) + 1 2 ||X\u2212B||2F , (6)\nwhere wi \u2265 0, i = 1, \u00b7 \u00b7 \u00b7 ,m. Problem (6) is a little more general than (3) by taking different gi(x) = wix. It is claimed in (Gu et al. 2014) that the solution to (6) is X\u2217 = UDiag ({Proxgi(\u03c3i(B)), i = 1, \u00b7 \u00b7 \u00b7 ,m})VT , (7) where B = UDiag(\u03c3(B))VT is the SVD of B, and Proxgi(\u03c3i(B)) = max{\u03c3i(B) \u2212 wi, 0}. However, such a result and their proof are not correct. A counterexample is as follows:\nB = [ 0.0941 0.4201 0.5096 0.0089 ] , w = [ 0.5 0.25 ] ,\nX\u2217 = [ \u22120.0345 0.1287 0.0542 \u22120.0512 ] , X\u0302 = [ 0.0130 0.1938 0.1861 \u22120.0218 ] ,\nwhere X\u2217 is obtained by (7). The solution X\u2217 is not optimal to (6) since there exists X\u0302 shown above such that fw,B(X\u0302) = 0.2262 < fw,B(X\n\u2217) = 0.2393. The reason behind is that (Prox gi(\u03c3i(B))\u2212Prox gj (\u03c3j(B)))(\u03c3i(B)\u2212\u03c3j(B)) \u2265 0, (8) does not guarantee to hold for any i 6= j. Note that (8) holds when 0 \u2264 w1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 wm, and thus (7) is optimal to (6) in this case.\nIn this work, we give the first rigorous proof that Proxg(\u00b7) is monotone for any lower bounded function (regardless of the convexity of g). Then solving (3) is degenerated to solving (5) for each b = \u03c3i(B). The Generalized Singular Value Thresholding (GSVT) operator Prox\u03c3g (\u00b7) associated with any lower bounded function in (3) is much more\ngeneral than the known SVT associated with the convex nuclear norm (Cai, Cande\u0300s, and Shen 2010). In order to compute GSVT, we analyze the solution to (5) for certain types of g (some special cases are shown in Table 1) in theory, and propose a general solver to (5). At last, with GSVT, we can solve (9) by the Generalized Proximal Gradient (GPG) algorithm shown in (10). We test both Iteratively Reweighted Nuclear Norm (IRNN) and GPG on the matrix completion problem. Both synthesis and real data experiments show that GPG outperforms IRNN in terms of the recovery error and the objective function value.\nGeneralized Singular Value Thresholding"}, {"heading": "Problem Reformulation", "text": "A main goal of this work is to compute GSVT (3), and uses it to solve (9). We will show that, if Proxg(\u00b7) is monotone, problem (3) can be reformulated into an equivalent problem which is much easier to solve. Lemma 1. (von Neumann\u2019s trace inequality (Rhea 2011)) For any matrices A, B \u2208 Rm\u00d7n (m \u2264 n), Tr(ATB) \u2264\u2211m i=1 \u03c3i(A)\u03c3i(B), where \u03c31(A) \u2265 \u03c32(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 and \u03c31(B) \u2265 \u03c32(B) \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 are the singular values of A and B, respectively. The equality holds if and only if there exist unitaries U and V such that A = UDiag(\u03c3(A))VT and B = UDiag(\u03c3(B))VT are the SVDs of A and B, simultaneously. Theorem 1. Let g : R+ \u2192 R+ be a function such that Proxg(\u00b7) is monotone. Let B = UDiag(\u03c3(B))VT be the SVD of B \u2208 Rm\u00d7n. Then an optimal solution to (3) is\nX\u2217 = UDiag(%\u2217)VT , (9)\nwhere %\u2217 satisfies %\u22171 \u2265 %\u22172 \u2265 \u00b7 \u00b7 \u00b7 \u2265 %\u2217m, i = 1, \u00b7 \u00b7 \u00b7 ,m, and\n%\u2217i \u2208 Proxg(\u03c3i(B)) = argmin %i\u22650\ng(%i) + 1\n2 (%i \u2212 \u03c3i(B))2.\n(10)\nProof. Denote \u03c31(X) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3m(X) \u2265 0 as the singular values of X. Problem (3) can be rewritten as\nmin %:%1\u2265\u00b7\u00b7\u00b7\u2265%m\u22650\n{ min\n\u03c3(X)=% m\u2211 i=1 g(%i) + 1 2 ||X\u2212B||2F\n} .\n(11) By using the von Neumann\u2019s trace inequality in Lemma 1, we have\n||X\u2212B||2F = Tr (XTX)\u2212 2 Tr(XTB) + Tr(BTB)\n= m\u2211 i=1 \u03c32i (X)\u2212 2 Tr(XTB) + m\u2211 i=1 \u03c32i (B)\n\u2265 m\u2211 i=1 \u03c32i (X)\u2212 2 m\u2211 i=1 \u03c3i(X)\u03c3i(B) + m\u2211 i=1 \u03c32i (B)\n= m\u2211 i=1 (\u03c3i(X)\u2212 \u03c3i(B))2.\nNote that the above equality holds when X admits the singular value decomposition X = UDiag(\u03c3(X))VT , where\nU and V are the left and right orthonormal matrices in the SVD of B. In this case, problem (11) is reduced to\nmin %:%1\u2265\u00b7\u00b7\u00b7\u2265%m\u22650 m\u2211 i=1 ( g(%i) + 1 2 (%i \u2212 \u03c3i(B))2 ) . (12)\nSince Proxg(\u00b7) is monotone and \u03c31(B) \u2265 \u03c32(B) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3m(B), there exists %\u2217i \u2208 Proxg(\u03c3i(B)), such that %\u22171 \u2265 %\u22172 \u2265 \u00b7 \u00b7 \u00b7 \u2265 %\u2217m. Such a choice of %\u2217 is optimal to (12), and thus (9) is optimal to (3).\nFrom the above proof, it can be seen that the monotone property of Proxg(\u00b7) is a key condition which makes problem (12) separable conditionally. Thus the solution (9) to (3) shares a similar formulation as the known Singular Value Thresholding (SVT) operator associated with the convex nuclear norm (Cai, Cande\u0300s, and Shen 2010). Note that for a convex g, Proxg(\u00b7) is always monotone. Indeed,\n(Prox g(b1)\u2212Prox g(b2)) (b1 \u2212 b2) \u2265 (Proxg(b1)\u2212Proxg(b2))2 \u2265 0, \u2200 b1, b2 \u2208 R+.\nThe above inequality can be obtained by the optimality of Proxg(\u00b7) and the convexity of g.\nThe monotonicity of Proxg(\u00b7) for a nonconvex g is still unknown. There were some previous works (Nie, Huang, and Ding 2012; Chartrand 2012; Liu et al. 2013a) claiming that the solution (9) is optimal to (3) for some special choices of nonconvex g. However, their results are not rigorous since the monotone property of Proxg(\u00b7) is not proved. Surprisingly, we find that the monotone property of Proxg(\u00b7) holds for any lower bounded function g. Theorem 2. For any lower bounded function g, its proximal operator Proxg(\u00b7) is monotone, i.e., for any p\u2217i \u2208 Proxg(xi), i = 1, 2, p\u22171 \u2265 p\u22172, when x1 > x2.\nNote that it is possible that \u03c3i(B) = \u03c3j(B) for some i < j in (10). Since Proxg(\u00b7) may not be unique, we need to choose %\u2217i \u2208 Proxg(\u03c3i(B)) and %\u2217j \u2208 Proxg(\u03c3j(B)) such that %\u2217i \u2264 %\u2217j . This is the only difference between GSVT and SVT."}, {"heading": "Proximal Operator of Nonconvex Function", "text": "So far, we have proved that solving (3) is equivalent to solving (5) for each b = \u03c3i(B), i = 1, \u00b7 \u00b7 \u00b7 ,m, for any lower bounded function g. For a nonconvex g, only for some special cases, the candidate solutions to (5) have a closed form (Gong et al. 2013). There does not exist a general solver for a more general nonconvex g. In this section, we analyze the solution to (5) for a broad choice of the nonconvex g. Then a general solver will be proposed in the next section. Assumption 1. g : R+ \u2192 R+, g(0) = 0. g is concave, nondecreasing and differentiable. The gradient\u2207g is convex.\nIn this work, we are interested in the nonconvex surrogate of `0-norm. Except the differentiablity of g and the convexity of \u2207g, all the other assumptions in Assumption 2 are necessary to construct a surrogate of `0-norm. As shown later, these two additional assumptions make our analysis much easier. Note that the assumptions for the nonconvex\nfunction considered in Assumption 2 are quite general. It is easy to verify that many popular surrogates of `0-norm in Table 1 satisfy Assumption 2, including `p-norm, Logarithm, MCP, Geman and Laplace penalties. Only the SCAD penalty violates the convex\u2207g assumption, as shown in Figure 1.\nProposition 1. Given g satisfying Assumption 2, the optimal solution to (5) lies in [0, b].\nThe above fact is obvious since both g(x) and 12 (x \u2212 b) 2 are nondecreasing on [b,+\u221e). Such a result limits the solution space, and thus is very useful for our analysis. Our general solver to (5) is also based on Proposition 1.\nNote that the solutions to (5) lie in 0 or the local points {x|\u2207fb(x) = \u2207g(x) + x \u2212 b = 0}. Our analysis is mainly based on the number of intersection points of D(x) = \u2207g(x) and the line Cb(x) = b \u2212 x. Let b\u0304 = sup{b | Cb(x) and D(x) have no intersection}. We have the solution to (5) in different cases. Please refer to the supplementary material for the detailed proofs.\nProposition 2. Given g satisfying Assumption 2 and \u2207g(0) = +\u221e. Restricted on [0,+\u221e), when b > b\u0304, Cb(x) and D(x) have two intersection points, denoted as P b1 = (xb1, y b 1), P b 2 = (x b 2, y b 2), and x b 1 < x b 2. If there does not exist b > b\u0304 such that fb(0) = fb(xb2), then Proxg(b) = 0 for all b \u2265 0. If there exists b > b\u0304 such that fb(0) = fb(xb2), let b\u2217 = inf{b | fb(0) = fb(xb2) }. Then we have\nProxg(b) = argmin x\u22650 fb(x)\n{ = xb2, if b > b\n\u2217, 3 0, if b \u2264 b\u2217.\nProposition 3. Given g satisfying Assumption 2 and \u2207g(0) < +\u221e. Restricted on [0,+\u221e), if we have C\u2207g(0)(x) = \u2207g(0) \u2212 x \u2264 \u2207g(x) for all x \u2208 (0,\u2207g(0)), then Cb(x) and D(x) have only one intersection point\n0 0.5 1 1.5 2 2.5 3 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2 l1\nb\nP ro\nx g (b\n)\n(a) `1-norm\n0 0.5 1 1.5 2 2.5 3 0\n0.5\n1\n1.5\n2\n2.5\n3 lp\nb\nP ro\nx g (b\n)\n(b) `p-norm\n0 0.5 1 1.5 2 2.5 3 0\n0.5\n1\n1.5\n2\n2.5\n3 mcp\nb\nP ro\nx g (b\n)\n(c) MCP\n0 0.5 1 1.5 2 2.5 3 0\n0.5\n1\n1.5\n2\n2.5\n3 logarithm\nb\nP ro\nx g (b\n)\n(d) Logarithm\n0 0.5 1 1.5 2 2.5 3 0\n0.5\n1\n1.5\n2\n2.5\n3 laplace\nb\nP ro\nx g (b\n)\n(e) Laplace\n0 0.5 1 1.5 2 2.5 3 0\n0.5\n1\n1.5\n2\n2.5\n3 geman\nb\nP ro\nx g (b\n)\n(f) Geman\n(xb, yb) when b > \u2207g(0). Furthermore,\nProxg(b) = argmin x\u22650 fb(x)\n{ = xb, if b > \u2207g(0), 3 0, if b \u2264 \u2207g(0).\nSuppose there exists 0 < x\u0302 < \u2207g(0) such that C\u2207g(0)(x\u0302) = \u2207g(0) \u2212 x\u0302 > \u2207g(x\u0302). Then, when \u2207g(0) \u2265 b > b\u0304, Cb(x) andD(x) have two intersection points, which are denoted as P b1 = (x b 1, y b 1) and P b 2 = (x b 2, y b 2) such that x b 1 < x b 2. When \u2207g(0) < b, Cb(x) and D(x) have only one intersection point (xb, yb). Also, there exists b\u0303 such that \u2207g(0) > b\u0303 > b\u0304 and fb\u0303(0) = fb\u0303(x b 2). Let b\n\u2217 = inf{b | fb(0) = fb(xb2) }. We have\nProxg(b) = argmin x\u22650 fb(x)  = x b, if b > \u2207g(0), = xb2, if \u2207g(0) \u2265 b > b\u2217, 3 0, if b \u2264 b\u2217.\nCorollary 1. Given g satisfying Assumption 2. Denote x\u0302b = max{x|\u2207fb(x) = 0, 0 \u2264 x \u2264 b} and x\u2217 = arg minx\u2208{0,x\u0302b} fb(x). Then x\u2217 is optimal to (5).\nThe results in Proposition 2 and 3 give the solution to (5) in different cases, while Corollary 2 summarizes these results. It can be seen that one only needs to compute x\u0302b which is the largest local minimum. Then comparing the objective function value at 0 and x\u0302b leads to an optimal solution to (5).\nAlgorithms In this section, we first give a general solver to (5) in which g satisfies Assumption 2. Then we are able to solve the GSVT problem (3). With GSVT, problem (9) can be solved by Generalized Proximal Gradient (GPG) algorithm as shown in (10). We also give the convergence guarantee of GPG."}, {"heading": "A General Solver to (5)", "text": "Given g satisfying Assumption 2, as shown in Corollary 2, 0 and x\u0302b = max{x|\u2207fb(x) = 0, 0 \u2264 x \u2264 b} are the candidate solutions to (5). The left task is to find x\u0302b which is\nthe largest local minimum point near x = b. So we can start searching for x\u0302b from x0 = b by the fixed point iteration algorithm. Note that it will be very fast since we only need to search within [0, b]. The whole procedure to find x\u0302b can be found in Algorithm 1. In theory, it can be proved that the fixed point iteration guarantees to find x\u0302b.\nIf g is nonsmooth or \u2207g is nonconvex, the fixed point iteration algorithm may also be applicable. The key is to find all the local solutions with smart initial points. Also all the nonsmooth points should be considered as the candidates.\nAll the nonconvex surrogates g except SCAD in Table 1 satisfy Assumption 2, and thus the solution to (5) can be obtained by Algorithm 1. Figure 2 illustrates the shrinkage effect of proximal operators of these functions and the convex `1-norm. The shrinkage and thresholding effect of these proximal operators are similar when b is relatively small. However, when b is relatively large, the proximal operators of the nonconvex functions are nearly unbiased, i.e., keeping b nearly the same as the `0-norm. On the contrast, the proximal operator of the convex `1-norm is biased. In this case, the `1-norm may be over-penalized, and thus may perform quite differently from the `0-norm. This also supports the necessity of using nonconvex penalties on the singular values to approximate the rank function."}, {"heading": "Generalized Proximal Gradient Algorithm for (9)", "text": "Given g satisfying Assumption 2, we are now able to get the optimal solution to (3) by (9) and Algorithm 1. Now we have a better solver than IRNN to solve (9) by the updating rule (10), or equivalently\nXk+1 = Prox\u03c31 \u00b5 g\n( Xk \u2212 1\n\u00b5 \u2207h(Xk)\n) .\nThe above updating rule is named as Generalized Proximal Gradient (GPG) for the nonconvex problem (9), which generalizes some previous methods (Beck and Teboulle 2009; Gong et al. 2013). The main per-iteration cost of GPG is to compute an SVD, which is the same as many convex methods (Toh and Yun 2010a; Lin, Chen, and Ma 2009). In theory, we have the following convergence results for GPG.\nTheorem 3. If \u00b5 > L(h), the sequence {Xk} generated by (10) satisfies the following properties:\n(1) F (Xk) is monotonically decreasing. (2) lim\nk\u2192+\u221e (Xk \u2212Xk+1) = 0;\n(3) If F (X) \u2192 +\u221e when ||X||F \u2192 +\u221e, then any limit point of {Xk} is a stationary point.\nIt is expected that GPG will decrease the objective function value faster than IRNN since it uses a tighter surrogate function. This will be verified by the experiments.\nExperiments In this section, we conduct some experiments on the matrix completion problem to test our proposed GPG algorithm\nmin X m\u2211 i=1 g(\u03c3i(X)) + 1 2 ||P\u2126(X)\u2212 P\u2126(M)||2F , (13)\nwhere \u2126 is the index set, and P\u2126 : Rm\u00d7n \u2192 Rm\u00d7n is a linear operator that keeps the entries in \u2126 unchanged and those outside \u2126 zeros. Given P\u2126(M), the goal of matrix completion is to recover M which is of low rank. Note that we have many choices of g which satisfies Assumption 2, and we simply test on the Logarithm penalty, since it is suggested in (Lu et al. 2014; Cande\u0300s, Wakin, and Boyd 2008) that it usually performs well by comparing with other nonconvex penalties. Problem (13) can be solved by GPG by using GSVT (9) in each iteration. We compared GPG with IRNN on both synthetic and real data. The continuation technique is used to enhance the low rank matrix recovery in GPG. The initial value of \u03bb in the Logarithm penalty is set to \u03bb0, and dynamically decreased till reaching \u03bbt."}, {"heading": "Low-Rank Matrix Recovery on Random Data", "text": "We conduct two experiments on synthetic data without and with noises (Lu et al. 2014). For the noise free case, we generate M = M1M2, where M1 \u2208 Rm\u00d7r, M2 \u2208 Rr\u00d7n are i.i.d. random matrices, and m = n = 150. The underlying rank r varies from 20 to 33. Half of the elements in M are missing. We set \u03bb0 = 0.9||P\u2126(M)||\u221e, and \u03bbt = 10\u22125\u03bb0. The relative error RelErr= ||X\u2217 \u2212M||F /||M||F is used to evaluate the recovery performance. If RelErr is smaller than 10\u22123, X\u2217 is regarded as a successful recovery of M. We repeat the experiments 100 times for each r. We compare GPG\nby using GSVT with IRNN and the convex Augmented Lagrange Multiplier (ALM) (Lin, Chen, and Ma 2009). Figure 3 (a) plots r v.s. the frequency of success. It can be seen that GPG is slightly better than IRNN when r is relatively small, while both IRNN and GPG fail when r \u2265 32. Both of them outperform the convex ALM method, since the nonconvex logarithm penalty approximates the rank function better than the convex nuclear norm.\nFor the noisy case, the data matrix M is generated in the same way, but are added some additional noises 0.1E, where E is an i.i.d. random matrix. For this task, we set \u03bb0 = 10||P\u2126(M)||\u221e, and \u03bbt = 0.1\u03bb0 in GPG. The convex APGL algorithm (Toh and Yun 2010b) is compared in this task. Each method is run 100 times for each r \u2208 {15, 18, 20, 23, 25, 30}. Figure 3 (b) shows the mean relative error. It can be seen that GPG by using GSVT in each iteration significantly outperforms IRNN and APGL. The reason is that \u03bbt is not that small as in the noise free case. Thus, the upper bound surrogate of g in IRNN will be much more loose than that in GPG. Figure 3 (c) plots some convergence curves of GPG and IRNN. It can be seen that GPG without relaxing g will decrease the objective function value faster."}, {"heading": "Applications on Real Data", "text": "Matrix completion can be applied to image inpainting since the main information is dominated by the top singular values. For a color image, assume that 40% of pixels are uniformly missing. They can be recovered by applying low rank matrix completion on each channel (red, green and blue) of the image independently. Besides the relative error defined above, we also use the Peak Signal-to-Noise Ratio (PSNR) to evaluate the recovery performance. Figure 4 shows two images recovered by APGL, IRNN and GPG, respectively. It can be seen that GPG achieves the best performance, i.e., the largest PSNR value and the smallest relative error.\nWe also apply matrix completion for collaborative filtering. The task of collaborative filtering is to predict the unknown preference of a user on a set of unrated items, according to other similar users or similar items. We test on the MovieLens data set (Herlocker et al. 1999) which includes\nthree problems, \u201cmovie-100K\u201d, \u201cmovie-1M\u201d and \u201cmovie10M\u201d. Since only the entries in \u2126 of M are known, we use Normalized Mean Absolute Error (NMAE) ||P\u2126(X\u2217)\u2212 P\u2126(M)||1/|\u2126| to evaluate the performance as in (Toh and Yun 2010b). As shown in Table 2, GPG achieves the best performance. The improvement benefits from the GPG algorithm which uses a fast and exact solver of GSVT (9).\nThis paper studied the Generalized Singular Value Thresholding (GSVT) operator associated with the nonconvex function g on the singular values. We proved that the proximal operator of any lower bounded function g (denoted as Proxg(\u00b7)) is monotone. Thus, GSVT can be obtained by performing Proxg(\u00b7) on the singular values separately. Given b \u2265 0, we also proposed a general solver to find Proxg(b) for certain type of g. At last, we applied the generalized proximal gradient algorithm by using GSVT as the subroutine to solve the nonconvex low rank minimization problem (9). Experimental results showed that it outperformed previous method with smaller recovery error and objective function value.\nFor nonconvex low rank minimization, GSVT plays the same role as SVT in convex minimization. One may extend other convex low rank models to nonconvex cases, and solve them by using GSVT in place of SVT. An interesting future work is to solve the nonconvex low rank minimization problem with affine constraint by ALM (Lin, Chen, and Ma 2009) and prove the convergence.\nAcknowledgements This research is supported by the Singapore National Research Foundation under its International Research Centre @Singapore Funding Initiative and administered by the IDM Programme Office. Z. Lin is supported by NSF China (grant nos. 61272341 and 61231002), 973 Program of China (grant no. 2015CB3525) and MSRA Collaborative Research Program. C. Lu is supported by the MSRA fellowship 2014.\nReferences [Beck and Teboulle 2009] Beck, A., and Teboulle, M. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences.\n[Cai, Cande\u0300s, and Shen 2010] Cai, J.-F.; Cande\u0300s, E. J.; and Shen, Z. 2010. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization 20(4):1956\u20131982.\n[Cande\u0300s et al. 2011] Cande\u0300s, E. J.; Li, X.; Ma, Y.; and Wright, J. 2011. Robust principal component analysis? Journal of the ACM 58(3).\n[Cande\u0300s, Wakin, and Boyd 2008] Cande\u0300s, E. J.; Wakin, M. B.; and Boyd, S. P. 2008. Enhancing sparsity by reweighted `1 minimization. Journal of Fourier Analysis and Applications 14(5-6):877\u2013905.\n[Chartrand 2012] Chartrand, R. 2012. Nonconvex splitting for regularized low-rank+ sparse decomposition. IEEE Transactions on Signal Processing 60(11):5810\u20135819.\n[Cheng et al. 2010] Cheng, B.; Yang, J.; Yan, S.; Fu, Y.; and Huang, T. S. 2010. Learning with `1-graph for image analysis. TIP 19(Compendex):858\u2013866.\n[Clarke 1983] Clarke, F. 1983. Nonsmooth analysis and optimization. In Proceedings of the International Congress of Mathematicians.\n[Combettes and Pesquet 2011] Combettes, P. L., and Pesquet, J.-C. 2011. Proximal splitting methods in signal processing. Fixed-point algorithms for inverse problems in science and engineering.\n[Fan and Li 2001] Fan, J., and Li, R. 2001. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association 96(456):1348\u20131360.\n[Frank and Friedman 1993] Frank, L., and Friedman, J. 1993. A statistical view of some chemometrics regression tools. Technometrics.\n[Friedman 2012] Friedman, J. 2012. Fast sparse regression and classification. International Journal of Forecasting 28(3):722 \u2013 738.\n[Geman and Yang 1995] Geman, D., and Yang, C. 1995. Nonlinear image recovery with half-quadratic regularization. TIP 4(7):932\u2013946.\n[Gong et al. 2013] Gong, P.; Zhang, C.; Lu, Z.; Huang, J.; and Ye, J. 2013. A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. In ICML.\n[Gu et al. 2014] Gu, S.; Zhang, L.; Zuo, W.; and Feng, X. 2014. Weighted nuclear norm minimization with application to image denoising. In CVPR.\n[Herlocker et al. 1999] Herlocker, J. L.; Konstan, J. A.; Borchers, A.; and Riedl, J. 1999. An algorithmic framework for performing collaborative filtering. In International ACM SIGIR conference on Research and development in information retrieval. ACM.\n[Lewis and Sendov 2005] Lewis, A. S., and Sendov, H. S. 2005. Nonsmooth analysis of singular values. Part I: Theory. Set-Valued Analysis 13(3):213\u2013241.\n[Lin, Chen, and Ma 2009] Lin, Z.; Chen, M.; and Ma, Y. 2009. The augmented Lagrange multiplier method for exact recovery of a corrupted low-rank matrices. UIUC Technical Report UILU-ENG-09-2215, Tech. Rep.\n[Liu et al. 2013a] Liu, D.; Zhou, T.; Qian, H.; Xu, C.; and Zhang, Z. 2013a. A nearly unbiased matrix completion approach. In Machine Learning and Knowledge Discovery in Databases.\n[Liu et al. 2013b] Liu, G.; Lin, Z.; Yan, S.; Sun, J.; Yu, Y.; and Ma, Y. 2013b. Robust recovery of subspace structures by low-rank representation. TPAMI 35(1):171\u2013184.\n[Lu et al. 2014] Lu, C.; Tang, J.; Yan, S. Y.; and Lin, Z. 2014. Generalized nonconvex nonsmooth low-rank minimization. In CVPR.\n[Nie, Huang, and Ding 2012] Nie, F.; Huang, H.; and Ding, C. H. 2012. Low-rank matrix recovery via efficient Schatten p-norm minimization. In AAAI.\n[Recht, Fazel, and Parrilo 2010] Recht, B.; Fazel, M.; and Parrilo, P. A. 2010. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review 52(3):471\u2013501.\n[Rhea 2011] Rhea, D. 2011. The case of equality in the von Neumann trace inequality. preprint.\n[Toh and Yun 2010a] Toh, K., and Yun, S. 2010a. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of Optimization.\n[Toh and Yun 2010b] Toh, K., and Yun, S. 2010b. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of Optimization 6(615-640):15.\n[Trzasko and Manduca 2009] Trzasko, J., and Manduca, A. 2009. Highly undersampled magnetic resonance image reconstruction via homotopic-minimization. IEEE Transactions on Medical imaging 28(1):106\u2013121.\n[Wright et al. 2009] Wright, J.; Yang, A. Y.; Ganesh, A.; Sastry, S. S.; and Ma, Y. 2009. Robust face recognition via sparse representation. TPAMI 31(2):210\u2013227.\n[Zhang and others 2010] Zhang, C.-H., et al. 2010. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 38(2):894\u2013942.\nSupplementary Material of Generalized Singular Value Thresholding\nCanyi Lu1, Changbo Zhu1, Chunyan Xu2, Shuicheng Yan1, Zhouchen Lin3,? 1 Department of Electrical and Computer Engineering, National University of Singapore 2 School of Computer Science and Technology, Huazhong University of Science and Technology 3 Key Laboratory of Machine Perception (MOE), School of EECS, Peking University\ncanyilu@nus.edu.sg, zhuchangbo@gmail.com, xuchunyan01@gmail.com, eleyans@nus.edu.sg, zlin@pku.edu.cn\nAnanlysis of the Proximal Operator of Nonconvex Function In the following development, we consider the following problem\nProxg(b) = arg min x\u22650\nfb(x) = g(x) + 1\n2 (x\u2212 b)2, (1)\nwhere g(x) satisfies the following assumption. Assumption 2. g : R+ \u2192 R+, g(0) = 0. g is concave, nondecreasing and differentiable. The gradient\u2207g is convex.\nSet Cb(x) = b \u2212 x and D(x) = \u2207g(x). Let b\u0304 = sup{b | Cb(x) and D(x) have no intersection}, and xb\u03042 = inf{ x | (x, y) is the intersection point of Cb\u0304(x) and D(x)}."}, {"heading": "Proof of Proposition 2", "text": "Proposition 2. Given g satisfying Assumption 2 and \u2207g(0) = +\u221e. Restricted on [0,+\u221e), when b > b\u0304, Cb(x) and D(x) have two intersection points, denoted as P b1 = (x b 1, y b 1), P b 2 = (x b 2, y b 2), and x b 1 < x b 2. If there does not exist b > b\u0304 such that fb(0) = fb(x b 2), then Proxg(b) = 0 for all b \u2265 0. If there exists b > b\u0304 such that fb(0) = fb(xb2), let b\u2217 = inf{b | b > b\u0304, fb(0) = fb(x b 2) }. Then we have\nProxg(b) = argmin x\u22650 fb(x)\n{ = xb2, if b > b\n\u2217, 3 0, if b \u2264 b\u2217. (2)\nRemark: When b\u2217 exists and b > b\u2217, because D(x) = \u2207g(x) is convex and decreasing, we can conclude that Cb(x) and D(x) have exactly two intersection points. When b \u2264 b\u2217, Cb(x) and D(x) may have multiple intersection points.\nProof. When b > b\u0304, since \u2207fb(x) = D(x)\u2212 Cb(x), we can easily see that fb is increasing on (0, xb1), decreasing on (xb1, xb2) and increasing on (xb2, b). So, 0 and x b 2 are two local minimum points of fb(x) on [0, b].\nCase 1 : If there exists b > b\u0304 such that fb(0) = fb(xb2), denote b \u2217 = inf{b | b > b\u0304, fb(0) = fb(xb2) }. First, we consider b > b\u2217. Let b = b\u2217 + \u03b5 for some \u03b5 > 0. We have\nfb(x b\u2217 2 )\u2212 fb(0)\n= 1\n2 (xb\n\u2217 2 \u2212 b\u2217 \u2212 \u03b5)2 + g(x\u2217)\u2212 1\n2 (b\u2217 + \u03b5)2\n= 1\n2 (xb\n\u2217 2 \u2212 b\u2217)2 \u2212 1\n2 (b\u2217)2 \u2212 \u03b5(xb\n\u2217\n2 \u2212 b\u2217)\u2212 \u03b5b\u2217\n=fb\u2217(x b\u2217 2 )\u2212 fb\u2217(0)\u2212 \u03b5x\u22172 =\u2212 \u03b5x\u22172 < 0.\nSince fb is decreasing on [xb \u2217 2 , x b 2], we conclude that fb(0) > fb(x b\u2217 2 ) \u2265 fb(xb2). So, when b > b\u2217, xb2 is the global minimum of fb(x) on [0, b].\nSecond, we consider b\u0304 < b \u2264 b\u2217. We show that fb(0) \u2264 fb(xb2) by contradiction. Suppose that there exists b such that fb(0) > fb(x b 2). Since fb\u0304 is strictly increasing on (0, x b\u0304 2), we have fb\u0304(x\nb\u0304 2) > fb\u0304(0). Because we have{\nfb\u0304(x b\u0304 2) > fb\u0304(0), fb(x b 2) < fb(0),\n?Corresponding author.\nby a direct computation, we get { g(xb\u03042)\u2212 xb\u03042\u2207g(xb\u03042)\u2212 12 (x b\u0304 2)\n2 > 0, g(xb2)\u2212 xb2\u2207g(xb2)\u2212 12 (x b 2) 2 < 0.\nAccording to the intermediate value theorem, there exists x\u0303 such that xb\u03042 < x\u0303 < x b 2 and g(x\u0303) \u2212 x\u0303\u2207g(x\u0303) \u2212 12 (x\u0303) 2 = 0. Let b\u0303 = \u2207g(x\u0303) + x\u0303. Then, (x\u0303, b\u0303\u2212 x\u0303) is the intersection point of Cb\u0303(x) and D(x) such that fb\u0303(x\u0303) = fb\u0303(0). Since xb\u03042 < x\u0303 < xb2 and \u2207g is convex and nonincreasing, we conclude that b\u0304 < b\u0303 < b \u2264 b\u2217, which contradicts the minimality of b\u2217.\nAlso, when b \u2264 b\u0304, we have \u2207fb(x) = D(x)\u2212 Cb(x) \u2265 0, because D(x) is above Cb(x). So, the global minimum of fb(x) on [0, b] is 0.\nCase 2 : Suppose for all b\u2217 > b\u0304, fb\u2217(0) 6= fb\u2217(xb \u2217 2 ). Since fb\u0304 is increasing on (0, x b\u0304 2), we have fb\u0304(x b\u0304 2) > fb\u0304(0). We now show that for all b > b\u0304, fb(xb2) \u2265 fb(0). Suppose this is not true and there exists b such that b > b\u0304 and fb(xb2) < fb(0). Because we have {\nfb\u0304(x b\u0304 2) > fb\u0304(0), fb(x b 2) < fb(0),\nby a direct computation, we get { g(xb\u03042)\u2212 xb\u03042\u2207g(xb\u03042)\u2212 12 (x b\u0304 2)\n2 > 0, g(xb2)\u2212 xb2\u2207g(xb2)\u2212 12 (x b 2) 2 < 0.\nSo, according to the intermediate value theorem, there exists x\u0303 such that g(x\u0303)\u2212 x\u0303\u2207g(x\u0303)\u2212 12 (x\u0303) 2 = 0. Let b\u0303 = \u2207g(x\u0303) + x\u0303. Then, (x\u0303, b\u0303 \u2212 x\u0303) is the intersection point of Cb\u0303(x) and D(x) such that fb\u0303(x\u0303) = fb\u0303(0). Since xb\u03042 < x\u0303 < xb2 and \u2207g is convex and nonincreasing, we conclude that b\u0304 < b\u0303 < b, which contradicts fb\u2217(0) 6= fb\u2217(xb \u2217 2 ) for all b \u2217 > b\u0304. So, for all b > b\u0304, 0 is the minimum of fb(x) on [0, b]. Similarly, when b \u2264 b\u0304, we have \u2207fb(x) = D(x)\u2212 Cb(x) \u2265 0, because D(x) is above Cb(x). So, the global minimum of fb(x) on [0, b] is 0. The proof is completed."}, {"heading": "Proof of Proposition 3", "text": "Proposition 3. Given g satisfying Assumption 2 and \u2207g(0) < +\u221e. Restricted on [0,+\u221e), if we have C\u2207g(0)(x) = \u2207g(0)\u2212 x \u2264 \u2207g(x) for all x \u2208 (0,\u2207g(0)), then Cb(x) and D(x) have only one intersection point (xb, yb) when b > \u2207g(0). Furthermore,\nProxg(b) = argmin x\u22650 fb(x)\n{ = xb, if b > \u2207g(0), 3 0, if b \u2264 \u2207g(0). (3)\nSuppose there exists 0 < x\u0302 < \u2207g(0) such thatC\u2207g(0)(x\u0302) = \u2207g(0)\u2212x\u0302 > \u2207g(x\u0302). Then, when\u2207g(0) \u2265 b > b\u0304,Cb(x) andD(x) have two intersection points, which are denoted as P b1 = (x b 1, y b 1) and P b 2 = (x b 2, y b 2) such that x b 1 < x b 2. When \u2207g(0) < b, Cb(x) and D(x) have only one intersection point (xb, yb). Also, there exists b\u0303 such that \u2207g(0) > b\u0303 > b\u0304 and fb\u0303(0) = fb\u0303(xb\u03032). Let b\u2217 = inf{b | \u2207g(0) > b\u0303 > b\u0304, fb(0) = fb(xb2) }. We have\nProxg(b) = argmin x\u22650 fb(x)  = x b, if b > \u2207g(0), = xb2, if \u2207g(0) \u2265 b > b\u2217, 3 0, if b \u2264 b\u2217.\n(4)\nRemark: If b\u2217 exists, when b \u2264 b\u2217, it is possible that Cb(x) and D(x) have more than two intersection points. If b\u2217 does not exist, when b \u2264 \u2207g(0), it is also possible that Cb(x) and D(x) have more than two intersection points.\nProof. Case 1 : Suppose we have Cg\u2032(0)(x) = \u2207g(0)\u2212 x \u2264 \u2207g(x) for all x on (0,\u2207g(0)). Notice for all b \u2264 \u2207g(0), we have \u2207g(x) = D(x) \u2212 Cb(x) \u2265 0, so the minimum point of fb(x) is 0. For all b > \u2207g(0), Cb = b \u2212 x and D(x) have only one intersection point denoted as (xb, yb). Then, we can easily see that fb is decreasing on (0, xb) and increasing on (xb, b). So, when b > \u2207g(0), the minimum point of fb(x) is xb.\nCase 2 : Suppose there exists 0 < x\u0302 < \u2207g(0) such that C\u2207g(0)(x\u0302) = \u2207g(0)\u2212 x\u0302 > \u2207g(x\u0302). Then, D(x) and Cb(x) have two intersection points, i.e., (0,\u2207g(0)) and (x\u2207g(0)2 , y \u2207g(0) 2 ). It is easily checked that f\u2207g(0) is strictly decreasing on (0, x \u2207g(0) 2 ), so we have f\u2207g(0)(x \u2207g(0) 2 ) < f\u2207g(0)(0). Also, since fb\u0304 is strictly increasing on (0, x b\u0304 2), we have fb\u0304(x b\u0304 2) > fb\u0304(0).\nBecause we have { fb\u0304(x b\u0304 2) > fb\u0304(0),\nf\u2207g(0)(x \u2207g(0) 2 ) < f\u2207g(0)(0),\nby a direct computation, we get { g(xb\u03042)\u2212 xb\u03042\u2207g(xb\u03042)\u2212 12 (x b\u0304 2) 2 > 0,\ng(x \u2207g(0) 2 )\u2212 x \u2207g(0) 2 \u2207g(x \u2207g(0) 2 )\u2212 12 (x \u2207g(0) 2 ) 2 < 0.\nSo, according to the intermediate value theorem, there exists x\u0303 such that g(x\u0303)\u2212 x\u0303\u2207g(x\u0303)\u2212 12 (x\u0303) 2 = 0. Let b\u0303 = \u2207g(x\u0303) + x\u0303. Then, (x\u0303, b\u0303\u2212 x\u0303) is the intersection point of Cb\u0303(x) and D(x) such that fb\u0303(x\u0303) = fb\u0303(0). Since xb\u03042 < x\u0303 < x \u2207g(0) 2 and\u2207g is convex and nonincreasing, we conclude that b\u0304 < b\u0303 < \u2207g(0). Next, we set b\u2217 = inf{b | b\u0304 < b\u0303 < \u2207g(0), fb(0) = fb(xb2) }. Given \u2207g(0) \u2265 b > b\u0304, we can easily see that fb is increasing on (0, xb1), decreasing on (xb1, xb2) and increasing on (xb2, b). So, 0 and xb2 are two local minimum points of fb(x) on [0, b]. Next, for\u2207g(0) \u2265 b > b\u2217, set b = b\u2217 + \u03b5 for some \u03b5 > 0. We have\nfb(x b\u2217 2 )\u2212 fb(0)\n= 1\n2 (xb\n\u2217 2 \u2212 b\u2217 \u2212 \u03b5)2 + g(x\u2217)\u2212 1\n2 (b\u2217 + \u03b5)2\n= 1\n2 (xb\n\u2217 2 \u2212 b\u2217)2 \u2212 1\n2 (b\u2217)2 \u2212 \u03b5(xb\n\u2217\n2 \u2212 b\u2217)\u2212 \u03b5b\u2217\n=fb\u2217(x b\u2217 2 )\u2212 fb\u2217(0)\u2212 \u03b5x\u22172 =\u2212 \u03b5x\u22172 < 0.\nSince fb is decreasing on (xb \u2217 2 , x b 2), we conclude that fb(0) > fb(x b\u2217 2 ) \u2265 fb(xb2). So, when b > b\u2217, xb2 is the global minimum of fb(x) on [0, b].\nThen, for all b\u0304 < b \u2264 b\u2217, we show that fb(0) \u2264 fb(xb2). We prove by contradiction. Suppose that there exists b such that fb(0) > fb(x b 2). Because we have {\nfb\u0304(x b\u0304 2) > fb\u0304(0), fb(x b 2) < fb(0),\nby a direct computation, we get { g(xb\u03042)\u2212 xb\u03042\u2207g(xb\u03042)\u2212 12 (x b\u0304 2)\n2 > 0, g(xb2)\u2212 xb2\u2207g(xb2)\u2212 12 (x b 2) 2 < 0.\nSo, according to the intermediate value theorem, there exists x\u03031 such that g(x\u03031)\u2212 x\u03031\u2207g(x\u03031)\u2212 12 (x\u03031) 2 = 0 and xb\u03042 < x\u03031 < x b 2. Let b\u03031 = \u2207g(x\u03031) + x\u03031. Then, (x\u03031, b\u03031 \u2212 x\u03031) is the intersection point of Cb\u03031(x) and D(x) such that fb\u03031(x\u03031) = fb\u03031(0). Since xb\u03042 < x\u0303 < x b 2 and \u2207g is convex and nonincreasing, we conclude that b\u0304 < b\u0303 < b \u2264 b\u2217, which contradicts the minimality of b\u2217.\nNext, when b \u2264 b\u0304, we have \u2207fb(x) = D(x) \u2212 Cb(x) \u2265 0, so the global minimum of fb(x) on [0, b] is 0. Also, when b > \u2207g(0) , Cb = b \u2212 x and D(x) have only one intersection point (xb, yb). Then, we can easily see that fb is decreasing on (0, xb) and increasing on (xb, b). So, when b > \u2207g(0), the global minimum point of fb(x) is xb."}, {"heading": "Proof of Corollary 1", "text": "Corollary 2. Given g satisfying Assumption 2 in problem (1). Denote x\u0302b = max{x|\u2207fb(x) = 0, 0 \u2264 x \u2264 b} and x\u2217 = arg minx\u2208{0,x\u0302b} fb(x). Then x\u2217 is optimal to (1), i.e., x\u2217 \u2208 Proxg(b).\nProof. As shown in Proposition 2 and 3, when b is larger than a certain threshold, Proxg(b) (xb2 in (2)(4) or x b in (3)(4)) is unique. Actually the unique solution is the largest intersection point of Cb(x) and \u2207g(x), i.e., Proxg(b) = x\u0302b = max{x|\u2207fb(x) = 0, 0 \u2264 x \u2264 b}. For all the other choices of b, 0 \u2208 Proxg(b). Thus, 0 and x\u0302b, one of them should be optimal to (1). Thus x\u2217 = arg minx\u2208{0,x\u0302b} fb(x) is optimal to (1).\nProof of Theorem 2 Theorem 2. For any lower bounded function g, its proximal operator Proxg(\u00b7) is monotone, i.e., for any p\u2217i \u2208 Proxg(xi), i = 1, 2, p\u22171 \u2265 p\u22172, when x1 > x2.\nProof. The lower bound assumption of g guarantees a finite solution to problem (1). By the optimality of p\u2217i , i = 1, 2, we have\ng(p\u22172) + 1\n2 (p\u22172 \u2212 x1)2 \u2265 g(p\u22171) +\n1 2 (p\u22171 \u2212 x1)2, (5)\ng(p\u22171) + 1\n2 (p\u22171 \u2212 x2)2 \u2265 g(p\u22172) +\n1 2 (p\u22172 \u2212 x2)2. (6)\nSumming them together gives (p\u22172 \u2212 x1)2 + (p\u22171 \u2212 x2)2 \u2265 (p\u22171 \u2212 x1)2 + (p\u22172 \u2212 x2)2. (7)\nIt reduces to (p\u22171 \u2212 p\u22172)(x1 \u2212 x2) \u2265 0. (8) Thus p\u22171 \u2265 p\u22172 when x1 > x2.\nConvergence Analysis of Algorithm 1 Assume there exists x\u0302b = max{x|\u2207fb(x) = \u2207g(x) + x\u2212 b = 0, 0 \u2264 x \u2264 b}; otherwise, 0 is a solution to (1).\nWe only need to prove that the fixed point iteration guarantees to find x\u0302b. First, if\u2207g(b) = 0, then we have found x\u0302b = b. For the case x\u0302b < b, we prove that, the fixed point iteration, starting from x0 = b, converges to x\u0302b. Indeed, we have\nb\u2212\u2207g(x) < x, for any x > x\u0302b.\nWe prove this by contradiction. Assume there exists x\u0303 > x\u0302b such that b \u2212 \u2207g(x\u0303) > x\u0303. Notice g satisfies Assumption 1. It is easy to see\u2207g is continuous, decreasing and nonnegative. Then we have b\u2212\u2207g(b) < b (\u2207g(b) > 0 since b > x\u0302b). Thus there must exist some x\u0302 \u2208 (min(b, x\u0303),max(b, x\u0303)) > x\u0302b such that b\u2212 g(x\u0302) = x\u0302. This contradicts the definition of x\u0302b.\nSo, we have xk+1 = b\u2212\u2207g(xk) < xk, if xk > x\u0302b.\nOn the other hand, {xk} is lower bounded by x\u0302b. So there must exist a limit of {xk}, denoted as x\u0304, which is no less than x\u0302b. Let k \u2192 +\u221e on both sides of xk+1 = b\u2212\u2207g(xk), and we see that x\u0304 = b\u2212\u2207g(x\u0304). So, x\u0304 = x\u0302b, i.e., lim\nk\u2192+\u221e xk = x\u0302\nb.\nConvergence Analysis of Generalized Proximal Gradient Algorithm Consider the following problem\nmin X F (X) = m\u2211 i=1 g(\u03c3i(X)) + h(X), (9)\nwhere g : R+ \u2192 R+ is continuous, concave and nonincreasing on [0,+\u221e), and h : Rm\u00d7n \u2192 R+ has Lipschitz continuous gradient with Lipschitz constant L(h). The Generalized Proximal Gradient (GPG) algorithm solves the above problem by the following updating rule\nXk+1 = arg min X m\u2211 i=1 g(\u03c3i(X)) + h(X k) + \u3008\u2207h(Xk),X\u2212Xk\u3009+ \u00b5 2 ||X\u2212Xk||2F\n= arg min X m\u2211 i=1 g(\u03c3i(X)) + \u00b5 2 ||X\u2212Xk + 1 \u00b5 \u2207h(Xk)||2F .\n(10)\nThen we have the following results.\nTheorem 3. If \u00b5 > L(h), the sequence {Xk} generated by (10) satisfies the following properties: (1) F (Xk) is monotonically decreasing. Indeed,\nF (Xk)\u2212 F (Xk+1) \u2265 \u00b5\u2212 L(h) 2 ||Xk \u2212Xk+1||2F \u2265 0;\n(2) lim k\u2192+\u221e (Xk \u2212Xk+1) = 0;\n(3) If F (X)\u2192 +\u221e when ||X||F \u2192 +\u221e, then any limit point of {Xk} is a stationary point.\nProof. Since Xk+1 is optimal to (10), we have m\u2211 i=1 g(\u03c3i(X k+1)) + h(Xk) + \u3008\u2207h(Xk),Xk+1 \u2212Xk\u3009+ \u00b5 2 ||Xk+1 \u2212Xk||2F\n\u2264 m\u2211 i=1 g(\u03c3i(X k)) + h(Xk) + \u3008\u2207h(Xk),Xk \u2212Xk\u3009+ \u00b5 2 ||Xk \u2212Xk||2F\n= m\u2211 i=1 g(\u03c3i(X k)).\n(11)\nOn the other hand, since h has Lipschitz continuous gradient, we have (Beck and Teboulle 2009)\nh(Xk+1) \u2264 h(Xk) + \u3008\u2207h(Xk),Xk+1 \u2212Xk\u3009+ L(h) 2 ||Xk+1 \u2212Xk||2F . (12)\nCombining (11) and (12) leads to\nF (Xk)\u2212 F (Xk+1)\n= m\u2211 i=1 g(\u03c3i(X k)) + h(Xk)\u2212 m\u2211 i=1 g(\u03c3i(X k+1))\u2212 h(Xk+1)\n\u2265\u00b5\u2212 L(h) 2 ||Xk+1 \u2212Xk||2F .\n(13)\nThus \u00b5 > L(h) guarantees that F (Xk) \u2265 F (Xk+1). Summing (13) for k = 1, 2, \u00b7 \u00b7 \u00b7 , we get\nF (X1) \u2265 \u00b5\u2212 L(h) 2 +\u221e\u2211 k=1 ||Xk+1 \u2212Xk||2F . (14)\nThis implies that lim\nk\u2192+\u221e (Xk \u2212Xk+1) = 0. (15)\nFurthermore, since F (X) \u2192 +\u221e when ||X||F \u2192 +\u221e, {Xk} is bounded. There exist X\u2217 and a subsequence {Xkj} such that lim\nj\u2192+\u221e Xkj = X\u2217. By using (15), we get lim j\u2192+\u221e Xkj+1 = X\u2217. Considering that Xkj is optimal to (10), and \u2212 \u2211m i=1 g(\u03c3i(X)) is convex (since g is concave) (Lewis and Sendov 2005) , there exists Q kj+1 \u2208 \u2212\u2202 ( \u2212 \u2211m i=1 g(\u03c3i(X kj+1)) ) such that Qkj+1 +\u2207h(Xkj ) + \u00b5(Xkj+1 \u2212Xkj ) = 0. (16)\nLet j \u2192 +\u221e in (16). By the upper semi-continuous property of the subdifferential (Clarke 1983), there exists Q\u2217 \u2208 \u2212\u2202 (\u2212 \u2211m i=1 g(\u03c3i(X\n\u2217))), such that 0 = Q\u2217 +\u2207h(X\u2217) \u2208 \u2207F (X\u2217). (17)\nThus X\u2217 is a stationary point to (9).\nReferences [Beck and Teboulle 2009] Beck, A., and Teboulle, M. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences.\n[Cai, Cande\u0300s, and Shen 2010] Cai, J.-F.; Cande\u0300s, E. J.; and Shen, Z. 2010. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization 20(4):1956\u20131982.\n[Cande\u0300s et al. 2011] Cande\u0300s, E. J.; Li, X.; Ma, Y.; and Wright, J. 2011. Robust principal component analysis? Journal of the ACM 58(3).\n[Cande\u0300s, Wakin, and Boyd 2008] Cande\u0300s, E. J.; Wakin, M. B.; and Boyd, S. P. 2008. Enhancing sparsity by reweighted `1 minimization. Journal of Fourier Analysis and Applications 14(5-6):877\u2013905.\n[Chartrand 2012] Chartrand, R. 2012. Nonconvex splitting for regularized low-rank+ sparse decomposition. IEEE Transactions on Signal Processing 60(11):5810\u20135819.\n[Cheng et al. 2010] Cheng, B.; Yang, J.; Yan, S.; Fu, Y.; and Huang, T. S. 2010. Learning with `1-graph for image analysis. TIP 19(Compendex):858\u2013866.\n[Clarke 1983] Clarke, F. 1983. Nonsmooth analysis and optimization. In Proceedings of the International Congress of Mathematicians.\n[Combettes and Pesquet 2011] Combettes, P. L., and Pesquet, J.-C. 2011. Proximal splitting methods in signal processing. Fixed-point algorithms for inverse problems in science and engineering.\n[Fan and Li 2001] Fan, J., and Li, R. 2001. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association 96(456):1348\u20131360.\n[Frank and Friedman 1993] Frank, L., and Friedman, J. 1993. A statistical view of some chemometrics regression tools. Technometrics.\n[Friedman 2012] Friedman, J. 2012. Fast sparse regression and classification. International Journal of Forecasting 28(3):722 \u2013 738.\n[Geman and Yang 1995] Geman, D., and Yang, C. 1995. Nonlinear image recovery with half-quadratic regularization. TIP 4(7):932\u2013946.\n[Gong et al. 2013] Gong, P.; Zhang, C.; Lu, Z.; Huang, J.; and Ye, J. 2013. A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. In ICML.\n[Gu et al. 2014] Gu, S.; Zhang, L.; Zuo, W.; and Feng, X. 2014. Weighted nuclear norm minimization with application to image denoising. In CVPR.\n[Herlocker et al. 1999] Herlocker, J. L.; Konstan, J. A.; Borchers, A.; and Riedl, J. 1999. An algorithmic framework for performing collaborative filtering. In International ACM SIGIR conference on Research and development in information retrieval. ACM.\n[Lewis and Sendov 2005] Lewis, A. S., and Sendov, H. S. 2005. Nonsmooth analysis of singular values. Part I: Theory. SetValued Analysis 13(3):213\u2013241.\n[Lin, Chen, and Ma 2009] Lin, Z.; Chen, M.; and Ma, Y. 2009. The augmented Lagrange multiplier method for exact recovery of a corrupted low-rank matrices. UIUC Technical Report UILU-ENG-09-2215, Tech. Rep.\n[Liu et al. 2013a] Liu, D.; Zhou, T.; Qian, H.; Xu, C.; and Zhang, Z. 2013a. A nearly unbiased matrix completion approach. In Machine Learning and Knowledge Discovery in Databases.\n[Liu et al. 2013b] Liu, G.; Lin, Z.; Yan, S.; Sun, J.; Yu, Y.; and Ma, Y. 2013b. Robust recovery of subspace structures by low-rank representation. TPAMI 35(1):171\u2013184.\n[Lu et al. 2014] Lu, C.; Tang, J.; Yan, S. Y.; and Lin, Z. 2014. Generalized nonconvex nonsmooth low-rank minimization. In CVPR.\n[Nie, Huang, and Ding 2012] Nie, F.; Huang, H.; and Ding, C. H. 2012. Low-rank matrix recovery via efficient Schatten p-norm minimization. In AAAI.\n[Recht, Fazel, and Parrilo 2010] Recht, B.; Fazel, M.; and Parrilo, P. A. 2010. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review 52(3):471\u2013501.\n[Rhea 2011] Rhea, D. 2011. The case of equality in the von Neumann trace inequality. preprint. [Toh and Yun 2010a] Toh, K., and Yun, S. 2010a. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of Optimization.\n[Toh and Yun 2010b] Toh, K., and Yun, S. 2010b. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of Optimization 6(615-640):15.\n[Trzasko and Manduca 2009] Trzasko, J., and Manduca, A. 2009. Highly undersampled magnetic resonance image reconstruction via homotopic-minimization. IEEE Transactions on Medical imaging 28(1):106\u2013121.\n[Wright et al. 2009] Wright, J.; Yang, A. Y.; Ganesh, A.; Sastry, S. S.; and Ma, Y. 2009. Robust face recognition via sparse representation. TPAMI 31(2):210\u2013227.\n[Zhang and others 2010] Zhang, C.-H., et al. 2010. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 38(2):894\u2013942."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Beck", "A. Teboulle 2009] Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Cand\u00e8s Cai", "J.-F. Shen 2010] Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "Robust principal component analysis", "author": ["Cand\u00e8s"], "venue": "Journal of the ACM 58(3)", "citeRegEx": "Cand\u00e8s,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s", "year": 2011}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["Wakin Cand\u00e8s", "E.J. Boyd 2008] Cand\u00e8s", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications 14(5-6):877\u2013905", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2008}, {"title": "Nonconvex splitting for regularized low-rank+ sparse decomposition", "author": ["R. Chartrand"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "Chartrand,? \\Q2012\\E", "shortCiteRegEx": "Chartrand", "year": 2012}, {"title": "Learning with `-graph for image analysis", "author": ["Cheng"], "venue": "TIP 19(Compendex):858\u2013866", "citeRegEx": "Cheng,? \\Q2010\\E", "shortCiteRegEx": "Cheng", "year": 2010}, {"title": "Proximal splitting methods in signal processing. Fixed-point algorithms for inverse problems in science and engineering", "author": ["Combettes", "P.L. Pesquet 2011] Combettes", "Pesquet", "J.-C"], "venue": null, "citeRegEx": "Combettes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Combettes et al\\.", "year": 2011}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Fan", "J. Li 2001] Fan", "R. Li"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Fan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2001}, {"title": "A statistical view of some chemometrics regression tools. Technometrics", "author": ["Frank", "L. Friedman 1993] Frank", "J. Friedman"], "venue": null, "citeRegEx": "Frank et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1993}, {"title": "Nonlinear image recovery with half-quadratic regularization", "author": ["Geman", "D. Yang 1995] Geman", "C. Yang"], "venue": null, "citeRegEx": "Geman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1995}, {"title": "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems", "author": ["Gong"], "venue": null, "citeRegEx": "Gong,? \\Q2013\\E", "shortCiteRegEx": "Gong", "year": 2013}, {"title": "Weighted nuclear norm minimization with application to image denoising", "author": ["Gu"], "venue": null, "citeRegEx": "Gu,? \\Q2014\\E", "shortCiteRegEx": "Gu", "year": 2014}, {"title": "An algorithmic framework for performing collaborative filtering", "author": ["Herlocker"], "venue": "In International ACM SIGIR conference on Research and development in information retrieval. ACM", "citeRegEx": "Herlocker,? \\Q1999\\E", "shortCiteRegEx": "Herlocker", "year": 1999}, {"title": "Nonsmooth analysis of singular values. Part I: Theory. SetValued Analysis 13(3):213\u2013241", "author": ["Lewis", "A.S. Sendov 2005] Lewis", "H.S. Sendov"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2005}, {"title": "The augmented Lagrange multiplier method for exact recovery of a corrupted low-rank matrices", "author": ["Chen Lin", "Z. Ma 2009] Lin", "M. Chen", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG-09-2215,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "A nearly unbiased matrix completion approach", "author": ["Liu"], "venue": "In Machine Learning and Knowledge Discovery in Databases", "citeRegEx": "Liu,? \\Q2013\\E", "shortCiteRegEx": "Liu", "year": 2013}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Liu"], "venue": null, "citeRegEx": "Liu,? \\Q2013\\E", "shortCiteRegEx": "Liu", "year": 2013}, {"title": "Generalized nonconvex nonsmooth low-rank minimization", "author": ["Lu"], "venue": null, "citeRegEx": "Lu,? \\Q2014\\E", "shortCiteRegEx": "Lu", "year": 2014}, {"title": "Low-rank matrix recovery via efficient Schatten p-norm minimization", "author": ["Huang Nie", "F. Ding 2012] Nie", "H. Huang", "C.H. Ding"], "venue": null, "citeRegEx": "Nie et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2012}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["Fazel Recht", "B. Parrilo 2010] Recht", "M. Fazel", "P.A. Parrilo"], "venue": null, "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Toh", "K. Yun 2010a] Toh", "S. Yun"], "venue": "Pacific Journal of Optimization", "citeRegEx": "Toh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Toh et al\\.", "year": 2010}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Toh", "K. Yun 2010b] Toh", "S. Yun"], "venue": "Pacific Journal of Optimization", "citeRegEx": "Toh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Toh et al\\.", "year": 2010}, {"title": "Highly undersampled magnetic resonance image reconstruction via homotopic-minimization", "author": ["Trzasko", "J. Manduca 2009] Trzasko", "A. Manduca"], "venue": "IEEE Transactions on Medical imaging 28(1):106\u2013121", "citeRegEx": "Trzasko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Trzasko et al\\.", "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["Wright"], "venue": null, "citeRegEx": "Wright,? \\Q2009\\E", "shortCiteRegEx": "Wright", "year": 2009}, {"title": "Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 38(2):894\u2013942", "author": ["Zhang", "C.-H"], "venue": null, "citeRegEx": "Zhang and C..H,? \\Q2010\\E", "shortCiteRegEx": "Zhang and C..H", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "It is worth mentioning that some previous works studied the solution to (3) for some special choices of nonconvex g (Nie, Huang, and Ding 2012; Chartrand 2012; Liu et al. 2013a).", "startOffset": 116, "endOffset": 177}, {"referenceID": 4, "context": "There were some previous works (Nie, Huang, and Ding 2012; Chartrand 2012; Liu et al. 2013a) claiming that the solution (9) is optimal to (3) for some special choices of nonconvex g.", "startOffset": 31, "endOffset": 92}], "year": 2014, "abstractText": "This work studies the Generalized Singular Value Thresholding (GSVT) operator Proxg (\u00b7), Proxg (B) = argmin X m \u2211", "creator": "LaTeX with hyperref package"}}}