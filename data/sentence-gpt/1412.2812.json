{"id": "1412.2812", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2014", "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework", "abstract": "We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 8 Dec 2014 23:40:41 GMT  (201kb,D)", "http://arxiv.org/abs/1412.2812v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG stat.ML", "authors": ["ivan titov", "ehsan khoddam"], "accepted": true, "id": "1412.2812"}, "pdf": {"name": "1412.2812.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework", "authors": ["Ivan Titov", "Ehsan Khoddam"], "emails": ["titov@uva.nl", "e.khoddammohammadi@uva.nl"], "sections": [{"heading": "1 Introduction", "text": "Shallow representations of meaning, and semantic role labels in particular, have a long history in linguistics [17]. More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].\nSemantic role representations encode the underlying predicate-argument structure of sentences, or, more specifically, for every predicate in a sentence they identify a set of arguments and associate each argument with an underlying semantic role, such as an agent (an initiator or doer of the action) or a patient (an affected entity). Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.\nMost current statistical approaches to SRL are supervised, requiring large quantities of human annotated data to estimate model parameters. However, such resources are expensive to create and only available for a small number of languages and domains. Moreover, when moved to a new domain, the performance of these models tends to degrade substantially [44]. The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22]. The existing methods have a number of serious shortcomings. First, they make very strong assumptions, for example, assuming that arguments are conditionally independent of each other given the predicate. Second, unlike state-of-the-art supervised parsers, they rely on a very simplistic set of features of a sentence. These factors lead to models being insufficiently expressive to capture syntax-semantics interface, inadequate handling of language ambiguity and, overall, introduces an upper bound on their performance. Moreover, these approaches are especially problematic for languages with freer word order than English, where richer features are necessary to account for interactions between surface realizations, syntax and semantics. For example, the two most accurate previous models [51, 32] both treat the role induction task as clus-\nar X\niv :1\n41 2.\n28 12\nv1 [\ncs .C\nL ]\n8 D\ntering of argument signatures: an argument signature encode key syntactic properties of an argument realization and consists of a syntactic function of an argument along with additional informations such as argument position with respect to the predicate. Though it is possible to design signatures which mostly map to a single role, this set-up limits oracle performance even for English, and can be quite restrictive for languages with freer word order. These shortcomings are inherent limitations of the modeling frameworks used in previous work (primarily generative modeling or agglomerative clustering), and cannot be addressed by simply incorporating more features or relaxing some of the modeling assumptions.\nIn this work, we propose a method for effective unsupervised estimation of feature-rich models of semantic roles. We demonstrate that reconstruction-error objectives, which have been shown to be effective primarily for training neural networks, are well suited for inducing feature-rich log-linear models of semantics. Our model consists of two components: a log-linear feature rich semantic role labeler and a tensor-factorization model which captures interaction between semantic roles and argument fillers. When estimated jointly on unlabeled data, roles induced by the model mostly corresponds to roles defined in existing resources by annotators.\nOur method rivals the most accurate semantic role induction methods on English and German [51, 32]. Importantly, no prior knowledge about the languages was incorporated in our feature-rich model, whereas the clustering counterparts relied on language-specific argument signatures. This languages-specific priors were crucial for the success, for example, using English-specific argument signatures for German with the Bayesian model of Titov and Klementiev [51] results in a drop of performance from clustering F1 80.9% to considerably lower 78.3% (our model yields 81.4%). This confirms the intuition that using richer features helps to capture the syntax-semantics interface in multi-lingual settings, reducing the need for using language-specific model engineering, as highly desirable in unsupervised learning.\nThe rest of the paper is structured as follows. Section 2 begins with a definition of the semantic role labeling task and discusses some specifics of the unsupervised setting. In Section 3, we describe our approach, starting with a general motivation and proceeding to technical details of the model (Section 3.3) and the learning procedure (Section 3.4). Section 4 provides both evaluation and analysis. Finally, additional related work is presented in Section 5."}, {"heading": "2 Task definition", "text": "The SRL task involves prediction of predicate argument structure, i.e. both identification of arguments and assignment of labels according to their underlying semantic role. For example, in the following sentences:\n(a) [Agent Mary] opened [Patient the door]. (b) [Patient The door] opened. (c) [Patient The door] was opened [Agent by Mary].\nMary always takes an agent role for the predicate open, and door is always a patient.\nIn this work we focus on the labeling stage of semantic role labeling. Identification, though an important problem, can be tackled with heuristics [32, 24, 13], with unsupervised techniques [1] or potentially by using a supervised classifier trained on a small amount of data."}, {"heading": "3 Approach", "text": "At the core of our approach is a statistical model encoding an interdependence between a semantic role structure and its realization in a sentence. In the unsupervised learning setting, sentences, their syntactic representations and argument positions (denoted by x) are observable whereas the associated semantic roles r are latent and need to be induced by the model. The idea which underlines much of latent variable modeling is that a good latent representation is the one which helps us to\nreconstruct x. In practice, we are not interested in predicting x, as x is observable, but rather interested in inducing appropriate latent representations (i.e. r). Thus, it is crucial to design the model in such a way that the good r (the one predictive of x) indeed encodes roles, rather than some other form of abstraction.\nIn what follows, we will refer to roles using their names, though, in the unsupervised setting, our method, as any other latent variable model, will not yield human-interpretable labels for them. We will use the following sentence as a motivating example in our discussion of the model:\n[Agent The police] charged [Patient the demonstrators] [Instrument with batons].\nThe model consists of two components. The first component is responsible for prediction of argument tuples based on roles and the predicate. In our experiments, in this component, we represent arguments as lemmas of their lexical heads (e.g., baton instead of with batons), and we also restrict ourselves to only verbal predicates. Intuitively, we can think of predicting one argument at a time (see Figure 1(b)): an argument (e.g., demonstrator in our example) is predicted based on the predicate lemma (charge), the role assigned to this argument (i.e. Patient) and other role-argument pairs ((Agent, police) and (Instrument, baton)). While learning to predict arguments, the inference algorithm will search for role assignments which simplify this prediction task as much as possible. Our hypothesis is that these assignments will correspond to roles accepted in linguistic theories (or, more importantly, useful in practical applications). Why is this hypothesis plausible? Primarily because these semantic representations were introduced as an abstraction capturing crucial properties of a relation (or an event). Thus, these representations, rather than surface linguistic details like argument order or syntactic functions, should be crucial for modeling sets of potential argument tuples. The reconstruction component is not the only part of the model. Crucially, what we referred to above as \u2018searching for role assignments to simplify argument prediction\u2019 would actually correspond to learning another component: a semantic role labeler which predicts roles relying on a rich set of sentence features. These two components will be estimated jointly in such a way as to minimize errors in recovering arguments. The role labeler will be the end-product of learning: it will be used to process new sentences, and it will be compared to existing methods in our evaluation."}, {"heading": "3.1 Shortcomings of generative modeling", "text": "The above paragraph can be regarded as our desiderata; now we discuss how to achieve them. The standard way to approach latent variable modeling is to use the generative framework: that is to define a family of joint models p(x, y|\u03b8) and estimate the parameters \u03b8 by, for example, maximizing likelihood. Generative models of semantics [51, 50, 40, 30] necessarily make very strong independence assumptions (e.g., arguments are conditionally independent of each other given the predicate) and use simplistic features of x and y. Thus, they cannot meet the desiderata stated above. Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers [16, 28, 10]."}, {"heading": "3.2 Reconstruction error minimization", "text": "Generative modeling is not the only way to learn latent representations. One alternative, popular in the neural network community, is to instead use autoencoders and optimize the reconstruction error [27, 55]. In autoencoders, a latent representation y (their hidden layer) is predicted from x by an encoding model and then this y is used to recover x\u0303with a reconstruction model (see Figure 1(a)). Parameters of the encoding and reconstruction components are chosen so as to minimize some form of the reconstruction error, for example, the Euclidean distance \u2206(x, x\u0303) = ||x \u2212 x\u0303||2. Though currently popular only within the deep learning community, latent variable models other than neural networks can also be trained this way, moreover:\n\u2022 the encoding and reconstruction models can belong to different model families; \u2022 the reconstruction component may be focused on recovering a part of x rather than the\nentire x, and, in doing so, can rely not only on y but on the remaining part of x.\nThese observations are crucial as they allow us to implement our desiderata. More specifically, the encoding model will be a feature-rich classifier which predicts semantic roles for a sentence, and the reconstruction model is the model which predicts an argument given its role, and given the rest of the arguments and their roles. The idea of training linear models with reconstruction error was previously explored by Daume\u0301 III [12] and very recently by Ammar et al. [2]."}, {"heading": "3.3 Modeling semantics within the reconstruction-error framework", "text": "There are several possible ways to translate the ideas above into a specific method, and we consider one of the simplest instantiations. For simplicity, in the discussion (but not in our experiments), we assume that exactly one predicate is realized in each sentence x. As we mentioned above, we focus on argument labeling: we assume that arguments a = (a1, . . . , aN ), ai \u2208 A, are known, and only their roles r = (r1, . . . , rN ), ri \u2208 R need to be induced. For the encoder (i.e. the semantic role labeler), we use a log-linear model:\np(r|x,w) \u221d exp(wTg(x, r)), where g(x, r) is a feature vector encoding interactions between sentence x and the semantic role representation r. Any model can be used here as long as the posterior distributions of roles ri can be efficiently computed or approximated (we will see why in Section 3.4). In our experiments, we used a model which factorizes over individual arguments (i.e. independent logistic regression classifiers).\nThe reconstruction component predicts an argument (e.g., the ith argument ai) given the semantic roles r, the predicate v and other arguments a\u2212i = (a1, . . . , ai\u22121, ai+1, . . . , aN ) with a bilinear softmax model:\np(ai|a\u2212i, r, v, C,u) = exp(uTaiC T v,ri\n\u2211 j 6=i Cv,rjuaj )\nZ(r, v, i) , (1)\nua \u2208 Rd (for every a \u2208 A) and Cv,r \u2208 Rd\u00d7k (for every verb v and every role r \u2208 R) are model parameters, Z(r, v, i) is the partition function ensuring that the probabilities sum to one. Intuitively, embeddings ua, when learned from data, will encode semantic properties of an argument: for example, embeddings for the words demonstrator and protestor should be somewhere near each other in Rd space, and further away from that for the word cat. The product Cp,rua is a k-dimensional vector encoding beliefs about other arguments based on the argument-role pair (a, r). For example, seeing the argument demonstrator in the Patient position for the predicate charge, one would predict that the Agent is perhaps the word police, and the role Instrument is filled by the word baton or perhaps (a water) cannon. On the contrary, if the Patient is cat then the Agent is more likely to be dog than police. In turn, the dot product (Cv,riuai)\nTCv,rjuaj is large if these expectations are met for the argument pair (ai, aj), and small otherwise. Intuitively, this objective corresponds to scoring argument tuples according to\nh(a, r, v, C,u) = \u2211 i 6=j uTaiC T v,riCv,rjuaj ,\nhinting at connections to (coupled) tensor and factorization methods [38, 59, 6, 45] and distributional semantics [37, 43]. Note also that the reconstruction model does not have access to any features of\nthe sentence (e.g., argument order or syntax), forcing the roles to convey all the necessary information.\nThis factorization can be though of as a generalization of the notion of selection preferences. Selectional preferences characterize the set of arguments licensed for a given role of a given predicate: for example, Agent for the predicate charge can be police or dog but not table or idea. In our generalization, we model soft restrictions imposed not only by the role itself but also by other arguments and their assignment to roles.\nIn practice, we extend the model slightly: (1) we introduce a word-specific bias (a scalar ba for every a \u2208 A) in the argument prediction model (equation (1)); (2) we smooth the model by using a sum of predicate-specific and cross-predicate projection matrices (Cv,r + Cr) instead of just Cv,r."}, {"heading": "3.4 Learning", "text": "Parameters of both model components (w, u and C) are learned jointly: the natural objective associated with every sentence would be the following:\nN\u2211 i=1 log \u2211 r p(ai|a\u2212i, r, v, C,u)p(r|x,w). (2)\nHowever optimizing this objective is not practical in its exact form for two reasons: (1) marginalization over r is exponential in the number of arguments; (2) the partition function Z(r, v, i) requires summation over the entire set of potential argument lemmas. We use existing techniques to address both challenges.\nIn order to deal with the first challenge, we use a basic mean-field approximation. Namely, instead of computing an expectation of p(ai|a\u2212i, r,v,C,u) under p(r|x,w), as in (2), we use the posterior distributions \u00b5is = p(ri = s|x,w) and score the argument predictions as\np(ai|a\u2212i,\u00b5,v,C,u) = exp (\u03c6i(ai,a\u2212i,\u00b5))\nZ(\u00b5, v, i) (3) \u03c6i(ai,a\u2212i,\u00b5) = u T ai( \u2211 s \u00b5isCv,s) T \u2211 j 6=i ( \u2211 s \u00b5jsCv,s)uaj ,\nwhere \u00b5 are the posteriors for all the arguments, and \u03c6i(a,a\u2212i) is the score associated with predicting lemma a for the argument i.\nIn order to address the second problem, the computation of Z(\u00b5, v, i), we use a negative sampling technique (see, e.g., Mikolov et al. [37]). More specfically, we get rid of the softmax in equation (3) and optimize the following sentence-level objective:\nN\u2211 i=1 [ log \u03c3(\u03c6i(ai,a\u2212i))\u2212 \u2211 a\u2032\u2208S log \u03c3(\u03c6i(a \u2032,a\u2212i)) ] , (4)\nwhere S is a random sample of n elements from the unigram distribution of lemmas, and \u03c3 is the logistic sigmoid function.\nAssuming that the posteriors \u00b5 can be derived in a closed form, the gradients of the objective (4) with respect to parameters of both the encoding component (w) and the reconstruction component (C, u and b) can be computed using back propagation. In our experiments, we used the AdaGrad algorithm [15] to perform the optimization.\nThe learning algorithm is quite efficient, as the reconstruction computation is bilinear, whereas the computation of the posteriors \u00b5 (and the computation of their gradients) from the semantic roler labeling component (encoder) is not more expensive than discriminative supervised learning of the role labeler. Moreover, the computations can be sped up substantially by observing that the sum\u2211\ns \u00b5isCv,s in the expression (3) can be precomputed for all i, and reused across predictions of different arguments of the same predicate. At test time, only the linear semantic role labeler is used, so the inference is straightforward."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data and evaluation metrics", "text": "We considered English and German in our experiments. For each language, we replicated experimental set-ups used in previous work.\nFor English, we followed Lang and Lapata [31] and used the dependency version of PropBank [42] released for the CoNLL 2008 shared task [48]. The dataset is divided into three segments. As in the previous work on unsupervised role labeling, we used the largest segment (the original CoNLL training set, sections 2-21) both for evaluation and learning. This is permissible as unsupervised models do not use gold labels in training. The two small segments (sections 22 and 23) were used for model development. In our experiments, we relied on gold standard syntax and gold standard argument identification, as this set-up allows us to evaluate against much of the previous work. We refer the reader to Lang and Lapata [31] for details of the experimental set-up.\nThere has not been much work on unsupervised induction of roles for languages other than English, perhaps primarily because of the above-mentioned model limitations. For German, we replicate the set-up considered in Titov and Klementiev [52]. They used the CoNLL 2009 version [25] of the SALSA corpus [7]. Instead of using syntactic parses provided in the CoNLL dataset, they re-parsed it with the MALT dependency parser [39]. Similarly, rather than relying on gold standard annotations for argument identification, they used a supervised classifier to predict argument positions. Details of the preprocessing can be found in Titov and Klementiev [52].\nAs in most previous work on unsupervised SRL, we evaluate our model using purity, collocation and their harmonic mean F1. Purity (PU) measures the average number of arguments with the same gold role label in each cluster, collocation (CO) measures to what extent a specific gold role is represented by a single cluster. More formally:\nPU = 1\nN \u2211 i max j |Gj \u2229 Ci|\nwhere if Ci is the set of arguments in the i-th induced cluster, Gj is the set of arguments in the jth gold cluster, and N is the total number of arguments. Similarly, for collocation:\nCO = 1\nN \u2211 j max i |Gj \u2229 Ci|\nWe compute the aggregate PU, CO, and F1 scores over all predicates in the same way as Lang and Lapata [31] by weighting the scores for each predicate by the number of times its arguments occurr."}, {"heading": "4.2 Parameters and features", "text": "For the semantic role labeling (encoding) component, we relied on 14 feature patterns used for argument labeling in one of popular supervised role labelers [28]. These patterns include nontrivial syntactic features, such as a dependency path between the target predicate and the considered argument. The resulting feature space is quite large (49,474 feature instantiations for our English dataset) and arguably sufficient to accurately capture syntax-semantics interface for most languages. Importantly, the dimensionality of the feature space is very different from the one used typically in unsupervised SRL. In principle, any features could be used here but we chose these 14 feature patterns, as they all are fairly simple and generic. They can also be easily extracted from any treebank. We used the same feature patterns both for English and German. However, there is little doubt that some language-specific feature engineering and the use of language-specific priors or constraints (e.g., posterior regularization [20]) would benefit the performance. Faithful to our goal of constructing the simplest possible feature-rich model, we use logistic classifiers independently predicting role distribution for every argument.\nFor the reconstruction component, both for English and German, we set the dimensionality of embeddings d, the projection dimensionality k and the number of negative samples n to 30, 15 and\n20, respectively. The model was not sensitive to the parameter defining the number of roles as long it was large enough (see Section 4.3 for more discussion). For training, we used uniform random initialization and AdaGrad [15]. Any model selections (e.g., choosing the number of epochs) was done on the basis of the respective development set."}, {"heading": "4.3 Results", "text": ""}, {"heading": "4.3.1 English", "text": "Table 1 summarizes the results of our method, as well as those of alternative approaches and baselines.\nFollowing [31], we use a baseline (SyntF) which simply clusters predicate arguments according to the dependency relation to their head. A separate cluster is allocated for each of 20 most frequent relations in the dataset and an additional cluster is used for all other relations. As observed in the previous work [32], this is a hard baseline to beat.\nWe also compare against previous approaches: the latent logistic classification model [31] (labeled LLogistic), the agglomerative clustering method [32] (Agglom), the graph partitioning approach [33] (GraphPart), the global role ordering model [22] (RoleOrdering). We also report results of an improved version of Agglom, recently reported by Lang and Lapata [34] (Agglom+). The strongest previous model is Bayes: Bayes is the most accurate (\u2018coupled\u2019) version of the Bayesian model of Titov and Klementiev [51], estimated from the CoNLL data without relying on any external data. Titov and Klementiev [51] also showed that using Brown clusters induced from a large external corpus resulted in an 0.5% improvement in F1 but that version is not entirely comparable to other systems induced solely from the CoNLL text.\nOur model outperforms or performs on par with best previous models in terms of F1. Interestingly, the purity and collocation balance is very different for our model and for the rest of the systems. In fact, our model induces at most 4-6 roles. On the contrary, Bayes predicts more than 30 roles for the majority of frequent predicates (e.g., 43 roles for the predicate include or 35 for say). Though this tendency reduces the purity scores for our model, this also means that our roles are more human interpretable. For example, agents and patients are clearly identifiable in the model predictions. Our model has similar purity to the syntactic baseline but outperforms it vastly according to the collocation metric, suggesting that we go substantially beyond recovering syntactic relations.\nIn additional experiments, we observed that our model, in some regimes, starts to induce roles specific to individual verb senses or specific to groups of semantically similar predicates. This suggests that adding a latent variable capturing predicate senses and conditioning the reconstruction component on this variable may not only result in a more informative semantic representation (i.e. include verb senses) but also improve the role induction performance. We leave this exploration for future work."}, {"heading": "4.3.2 German", "text": "For German, we replicate the experimental set-up previously used by Titov and Klementiev [52]. As for English, we report results of the syntactic baseline (SyntF). The results for all approaches are presented in Table 2. We compare against Bayes+LangSpecific \u2013 the Bayes model with argument signatures specialized for German (as reported in Titov and Klementiev [52]). We also consider the original version of the Bayes model (denoted as Bayes).\nRecently, Lang and Lapata [34], evaluated their Agglom+ on a version of the same German SALSA dataset. Their best result is F1 of 79.2%, however, this score and our results are not directly comparable. Instead of using the CoNLL dataset, they processed the corpus themselves. They also relied on syntactic features from a constituent parser whereas dependency representations are used in our experiments.\nThe overall picture for German closely resembles the one for English. Our method achieves results comparable to the best method evaluated in this setting. Importantly, parameters and features of our model for German and English are identical. On the contrary, by comparing Bayes with Bayes+LangSpecific, one can see that specialization of argument signatures was crucial for the Bayesian model. Also, similarly to English, our method induces less fine-grain sets of semantic roles but achieves much higher collocation scores."}, {"heading": "5 Additional related work", "text": "In recent years, unsupervised approaches to semantic role induction have attracted considerable attention. However, there exist other ways to address insufficient coverage provided by existing semantically-annotated resources.\nOne natural direction is semi-supervised role labeling, where both annotated and unannotated data is used to construct a model. Previous semi-supervised approaches to SRL can mostly be regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts [9, 14] or creating some form of \u2018surrogate\u2019 supervision [26, 18, 11]. The benefits from using unlabeled data were moderate, and more significant for the harder SRL version, frame-semantic parsing [11].\nAnother important direction includes cross-lingual approaches [41, 54] which leverage resources for research-rich languages, as well as parallel data, to transfer the annotation to the resource-poor languages. However, both translation shifts and noise in word alignments harm the performance of cross-lingual methods. Nevertheless, even joint unsupervised induction across languages appears to be beneficial [52].\nUnsupervised learning has also been one of the central paradigms for the closely-related area of relation extraction (RE), where several techniques have been proposed to cluster semantically similar verbalizations of relations [35, 4, 58]. Similarly to SRL, unsupervised methods for RE mostly rely on generative modeling and agglomerative clustering.\nFrom the learning perspective, methods which use the reconstruction-error objective to estimate linear models [2, 12] are certainly related. However, they do not consider learning factorization models, and they also do not deal with semantics. Tensor and factorization methods used in the context of modeling knoweldge bases (e.g., [6]) are also close in spirit. However, they do not deal with inducing semantics but rather factorize existing relations (i.e. rely on semantics)."}, {"heading": "6 Conclusions and discussion", "text": "This work introduces a method for inducing feature-rich semantic role labelers from unannoated text. In our approach, we view a semantic role representation as an encoding of a latent relation between a predicate and a tuple of its arguments. We capture this relation with a probabilistic tensor factorization model. The factorization model (relying on semantic roles) and a feature-rich model (predicting the roles) are jointly estimated by optimizing an objective which favours accurate reconstruction of arguments given the latent semantic representation (and other arguments). Our estimation method yields a semantic role labeler which achieves state-of-the-art results both on English and German.\nUnlike previous work on role induction, in our approach, virtually any computationally tractable structured model can be used as the role labeler , including almost any semantic role labeler introduced in the context of supervised SRL (see, e.g., CoNLL shared tasks [8, 48, 25]). This opens interesting possibilities to extend our approach to the semi-supervised setting. Previous unsupervised SRL models were making too strong assumption and used too limited features to effectively use unlabeled data. For our model, the reconstruction objective can be easily combined with the likelihood objective, yielding a potentially powerful semi-supervised method. We leave this direction for future work."}, {"heading": "Acknowledgments", "text": "The work was partially supported by a Google Focused Award on Natural Language Understanding. The authors thank Dipanjan Das, Mikhail Kozhevnikov, Ashutosh Modi and Alexis Palmer for insightful suggestions."}], "references": [{"title": "Unsupervised argument identification for semantic role labeling", "author": ["O. Abend", "R. Reichart", "A. Rappoport"], "venue": "In ACL-IJCNLP", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["W. Ammar", "C. Dyer", "N. Smith"], "venue": "In Proceedings of NIPS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The Berkeley FrameNet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "In Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Open information extraction from the web", "author": ["M. Banko", "M.J. Cafarella", "S. Soderland", "M. Broadhead", "O. Etzioni"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Cross-language frame semantics transfer in bilingual corpora", "author": ["R. Basili", "D.D. Cao", "D. Croce", "B. Coppola", "A. Moschitti"], "venue": "CICLING", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "The SALSA corpus: a german corpus resource for lexical semantics", "author": ["A. Burchardt", "K. Erk", "A. Frank", "A. Kowalski", "S. Pado", "M. Pinkal"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling", "author": ["X. Carreras", "L. M\u00e0rquez"], "venue": "In CoNLL", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In ICML. ACM", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Probabilistic frame-semantic parsing", "author": ["D. Das", "N. Schneider", "D. Chen", "N.A. Smith"], "venue": "In Proceedings of NAACL", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Semi-supervised frame-semantic parsing for unknown predicates", "author": ["D. Das", "N.A. Smith"], "venue": "In Proceedings of ACL", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Unsupervised search-based structured prediction", "author": ["III H. Daum\u00e9"], "venue": "In Proceedings of ICML", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["de Marneffe", "M.-C", "B. MacCartney", "C.D. Manning"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Semi-supervised semantic role labeling using the latent words language model", "author": ["K. Deschacht", "Moens", "M.-F"], "venue": "In Proceedings of EMNLP", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Shalmaneser\u2013a toolchain for shallow semantic parsing", "author": ["K. Erk", "S. Pado"], "venue": "In Proceedings of LREC,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "The case for case", "author": ["C.J. Fillmore"], "venue": "Universals in Linguistic Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1968}, {"title": "Graph alignment for semi-supervised semantic role labeling", "author": ["H. F\u00fcrstenau", "M. Lapata"], "venue": "In EMNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Unsupervised induction of a syntax-semantics lexicon using iterative refinement", "author": ["H. F\u00fcrstenau", "O. Rambow"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Graca", "J. Gillenwater", "B. Taskar"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Corpus expansion for statistical machine translation with semantic role label substitution rules", "author": ["Q. Gao", "S. Vogel"], "venue": "ACL:HLT", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Unsupervised semantic role induction with global role ordering", "author": ["N. Garg", "J. Henderson"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Automatic labelling of semantic roles", "author": ["D. Gildea", "D. Jurafsky"], "venue": "Computational Linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Unsupervised discovery of a statistical verb lexicon", "author": ["T. Grenager", "C. Manning"], "venue": "In EMNLP", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages", "author": ["J. Haji\u010d", "M. Ciaramita", "R. Johansson", "D. Kawahara", "M.A. Mart\u0131", "L. M\u00e0rquez", "A. Meyers", "J. Nivre", "S. Pad\u00f3", "J. \u0160t\u011bp\u00e1nek", "P. Stra\u0148\u00e1k", "M. Surdeanu", "N. Xue", "Y. Zhang"], "venue": "In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Self-training and co-training for semantic role labeling: Primary report", "author": ["S. He", "D. Gildea"], "venue": "Technical report, Technical Report 891,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Connectionist learning procedures", "author": ["G.E. Hinton"], "venue": "Artificial intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1989}, {"title": "Dependency-based syntactic-semantic analysis with PropBank and NomBank", "author": ["R. Johansson", "P. Nugues"], "venue": "In Proceedings of CoNLL", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Question answering based on semantic roles", "author": ["M. Kaisser", "B. Webber"], "venue": "In ACL Workshop on Deep Linguistic Processing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Inducing example-based semantic frames from a massive amount of verb uses", "author": ["D. Kawahara", "D. Peterson", "O. Popescu", "M. Palmer"], "venue": "In Proceedings of EACL", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Unsupervised induction of semantic roles", "author": ["J. Lang", "M. Lapata"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Unsupervised semantic role induction via split-merge clustering", "author": ["J. Lang", "M. Lapata"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Unsupervised semantic role induction with graph partitioning", "author": ["J. Lang", "M. Lapata"], "venue": "In EMNLP", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Similarity-driven semantic role induction via graph partitioning", "author": ["J. Lang", "M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "DIRT \u2013 discovery of inference rules from text", "author": ["D. Lin", "P. Pantel"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Semantic role features for machine translation", "author": ["D. Liu", "D. Gildea"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "Kriegel", "H.-P"], "venue": "In Proceedings of ICML", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Memory-based dependency parsing", "author": ["J. Nivre", "J. Hall", "J. Nilsson"], "venue": "In Proc. of the Eighth Conference on Computational Natural Language Learning,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "Learning frames from text with an unsupervised latent variable model", "author": ["B. O\u2019Connor"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["S. Pado", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["M. Palmer", "D. Gildea", "P. Kingsbury"], "venue": "Computational Linguistics,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Towards robust semantic role labeling", "author": ["S. Pradhan", "W. Ward", "J.H. Martin"], "venue": "Computational Linguistics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["S. Riedel", "L. Yao", "A. McCallum", "B.M. Marlin"], "venue": "In Proceedings of NAACL", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Relation alignment for textual entailment recognition", "author": ["M. Sammons", "V. Vydiswaran", "T. Vieira", "N. Johri", "M. Chang", "D. Goldwasser", "V. Srikumar", "G. Kundu", "Y. Tu", "K. Small", "J. Rule", "Q. Do", "D. Roth"], "venue": "In Text Analysis Conference (TAC)", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "Using semantic roles to improve question answering", "author": ["D. Shen", "M. Lapata"], "venue": "In EMNLP", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2007}, {"title": "The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies", "author": ["M. Surdeanu", "A.M.R. Johansson", "L. M\u00e0rquez", "J. Nivre"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2008}, {"title": "Unsupervised semantic role labelling", "author": ["R. Swier", "S. Stevenson"], "venue": "In EMNLP", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2004}, {"title": "A Bayesian model for unsupervised semantic parsing", "author": ["I. Titov", "A. Klementiev"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "A bayesian approach to semantic role induction", "author": ["I. Titov", "A. Klementiev"], "venue": "In Proc. EACL, Avignon, France", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Crosslingual induction of semantic roles. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, South Korea", "author": ["I. Titov", "A. Klementiev"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}, {"title": "Domain adaptation with artificial data for semantic parsing of speech", "author": ["L. van der Plas", "J. Henderson", "P. Merlo"], "venue": "In Proc. 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}, {"title": "Scaling up automatic cross-lingual semantic role annotation", "author": ["L. van der Plas", "P. Merlo", "J. Henderson"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A"], "venue": "In Proceedings of ICML", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "Semantic roles for SMT: A hybrid two-pass model", "author": ["D. Wu", "P. Fung"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2009}, {"title": "Structured relation discovery using generative models", "author": ["L. Yao", "A. Haghighi", "S. Riedel", "A. McCallum"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "Generalised coupled tensor factorisation", "author": ["K.Y. Y\u0131lmaz", "A.T. Cemgil", "U. Simsekli"], "venue": "In Proceedings of NIPS", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2011}], "referenceMentions": [{"referenceID": 16, "context": "Shallow representations of meaning, and semantic role labels in particular, have a long history in linguistics [17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 41, "context": "More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].", "startOffset": 97, "endOffset": 100}, {"referenceID": 22, "context": "More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].", "startOffset": 174, "endOffset": 193}, {"referenceID": 7, "context": "More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].", "startOffset": 174, "endOffset": 193}, {"referenceID": 47, "context": "More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].", "startOffset": 174, "endOffset": 193}, {"referenceID": 24, "context": "More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].", "startOffset": 174, "endOffset": 193}, {"referenceID": 9, "context": "More recently, with an emergence of large annotated resources such as PropBank [42] and FrameNet [3], automatic semantic role labeling (SRL) has attracted a lot of attention [23, 8, 48, 25, 10].", "startOffset": 174, "endOffset": 193}, {"referenceID": 46, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 105, "endOffset": 113}, {"referenceID": 28, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 105, "endOffset": 113}, {"referenceID": 45, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 134, "endOffset": 138}, {"referenceID": 55, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 160, "endOffset": 176}, {"referenceID": 35, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 160, "endOffset": 176}, {"referenceID": 20, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 160, "endOffset": 176}, {"referenceID": 4, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 199, "endOffset": 206}, {"referenceID": 52, "context": "Semantic roles have many potential applications in NLP and have been shown to benefit question answering [47, 29], textual entailment [46], machine translation [57, 36, 56, 21], and dialogue systems [5, 53], among others.", "startOffset": 199, "endOffset": 206}, {"referenceID": 43, "context": "Moreover, when moved to a new domain, the performance of these models tends to degrade substantially [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 48, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 23, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 30, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 31, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 32, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 50, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 18, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 21, "context": "The scarcity of annotated data has motivated the research into unsupervised learning of semantic representations [49, 24, 31, 32, 33, 51, 19, 22].", "startOffset": 113, "endOffset": 145}, {"referenceID": 50, "context": "For example, the two most accurate previous models [51, 32] both treat the role induction task as clus-", "startOffset": 51, "endOffset": 59}, {"referenceID": 31, "context": "For example, the two most accurate previous models [51, 32] both treat the role induction task as clus-", "startOffset": 51, "endOffset": 59}, {"referenceID": 50, "context": "Our method rivals the most accurate semantic role induction methods on English and German [51, 32].", "startOffset": 90, "endOffset": 98}, {"referenceID": 31, "context": "Our method rivals the most accurate semantic role induction methods on English and German [51, 32].", "startOffset": 90, "endOffset": 98}, {"referenceID": 50, "context": "This languages-specific priors were crucial for the success, for example, using English-specific argument signatures for German with the Bayesian model of Titov and Klementiev [51] results in a drop of performance from clustering F1 80.", "startOffset": 176, "endOffset": 180}, {"referenceID": 31, "context": "Identification, though an important problem, can be tackled with heuristics [32, 24, 13], with unsupervised techniques [1] or potentially by using a supervised classifier trained on a small amount of data.", "startOffset": 76, "endOffset": 88}, {"referenceID": 23, "context": "Identification, though an important problem, can be tackled with heuristics [32, 24, 13], with unsupervised techniques [1] or potentially by using a supervised classifier trained on a small amount of data.", "startOffset": 76, "endOffset": 88}, {"referenceID": 12, "context": "Identification, though an important problem, can be tackled with heuristics [32, 24, 13], with unsupervised techniques [1] or potentially by using a supervised classifier trained on a small amount of data.", "startOffset": 76, "endOffset": 88}, {"referenceID": 0, "context": "Identification, though an important problem, can be tackled with heuristics [32, 24, 13], with unsupervised techniques [1] or potentially by using a supervised classifier trained on a small amount of data.", "startOffset": 119, "endOffset": 122}, {"referenceID": 50, "context": "Generative models of semantics [51, 50, 40, 30] necessarily make very strong independence assumptions (e.", "startOffset": 31, "endOffset": 47}, {"referenceID": 49, "context": "Generative models of semantics [51, 50, 40, 30] necessarily make very strong independence assumptions (e.", "startOffset": 31, "endOffset": 47}, {"referenceID": 39, "context": "Generative models of semantics [51, 50, 40, 30] necessarily make very strong independence assumptions (e.", "startOffset": 31, "endOffset": 47}, {"referenceID": 29, "context": "Generative models of semantics [51, 50, 40, 30] necessarily make very strong independence assumptions (e.", "startOffset": 31, "endOffset": 47}, {"referenceID": 15, "context": "Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers [16, 28, 10].", "startOffset": 116, "endOffset": 128}, {"referenceID": 27, "context": "Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers [16, 28, 10].", "startOffset": 116, "endOffset": 128}, {"referenceID": 9, "context": "Importantly, they are also much more simplistic in their assumptions than state-of-the-art supervised role labelers [16, 28, 10].", "startOffset": 116, "endOffset": 128}, {"referenceID": 26, "context": "One alternative, popular in the neural network community, is to instead use autoencoders and optimize the reconstruction error [27, 55].", "startOffset": 127, "endOffset": 135}, {"referenceID": 54, "context": "One alternative, popular in the neural network community, is to instead use autoencoders and optimize the reconstruction error [27, 55].", "startOffset": 127, "endOffset": 135}, {"referenceID": 11, "context": "The idea of training linear models with reconstruction error was previously explored by Daum\u00e9 III [12] and very recently by Ammar et al.", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "hinting at connections to (coupled) tensor and factorization methods [38, 59, 6, 45] and distributional semantics [37, 43].", "startOffset": 69, "endOffset": 84}, {"referenceID": 57, "context": "hinting at connections to (coupled) tensor and factorization methods [38, 59, 6, 45] and distributional semantics [37, 43].", "startOffset": 69, "endOffset": 84}, {"referenceID": 5, "context": "hinting at connections to (coupled) tensor and factorization methods [38, 59, 6, 45] and distributional semantics [37, 43].", "startOffset": 69, "endOffset": 84}, {"referenceID": 44, "context": "hinting at connections to (coupled) tensor and factorization methods [38, 59, 6, 45] and distributional semantics [37, 43].", "startOffset": 69, "endOffset": 84}, {"referenceID": 36, "context": "hinting at connections to (coupled) tensor and factorization methods [38, 59, 6, 45] and distributional semantics [37, 43].", "startOffset": 114, "endOffset": 122}, {"referenceID": 42, "context": "hinting at connections to (coupled) tensor and factorization methods [38, 59, 6, 45] and distributional semantics [37, 43].", "startOffset": 114, "endOffset": 122}, {"referenceID": 36, "context": "[37]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In our experiments, we used the AdaGrad algorithm [15] to perform the optimization.", "startOffset": 50, "endOffset": 54}, {"referenceID": 30, "context": "For English, we followed Lang and Lapata [31] and used the dependency version of PropBank [42] released for the CoNLL 2008 shared task [48].", "startOffset": 41, "endOffset": 45}, {"referenceID": 41, "context": "For English, we followed Lang and Lapata [31] and used the dependency version of PropBank [42] released for the CoNLL 2008 shared task [48].", "startOffset": 90, "endOffset": 94}, {"referenceID": 47, "context": "For English, we followed Lang and Lapata [31] and used the dependency version of PropBank [42] released for the CoNLL 2008 shared task [48].", "startOffset": 135, "endOffset": 139}, {"referenceID": 30, "context": "We refer the reader to Lang and Lapata [31] for details of the experimental set-up.", "startOffset": 39, "endOffset": 43}, {"referenceID": 51, "context": "For German, we replicate the set-up considered in Titov and Klementiev [52].", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "They used the CoNLL 2009 version [25] of the SALSA corpus [7].", "startOffset": 33, "endOffset": 37}, {"referenceID": 6, "context": "They used the CoNLL 2009 version [25] of the SALSA corpus [7].", "startOffset": 58, "endOffset": 61}, {"referenceID": 38, "context": "Instead of using syntactic parses provided in the CoNLL dataset, they re-parsed it with the MALT dependency parser [39].", "startOffset": 115, "endOffset": 119}, {"referenceID": 51, "context": "Details of the preprocessing can be found in Titov and Klementiev [52].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "We compute the aggregate PU, CO, and F1 scores over all predicates in the same way as Lang and Lapata [31] by weighting the scores for each predicate by the number of times its arguments occurr.", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "For the semantic role labeling (encoding) component, we relied on 14 feature patterns used for argument labeling in one of popular supervised role labelers [28].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": ", posterior regularization [20]) would benefit the performance.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "For training, we used uniform random initialization and AdaGrad [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "Following [31], we use a baseline (SyntF) which simply clusters predicate arguments according to the dependency relation to their head.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "As observed in the previous work [32], this is a hard baseline to beat.", "startOffset": 33, "endOffset": 37}, {"referenceID": 30, "context": "We also compare against previous approaches: the latent logistic classification model [31] (labeled LLogistic), the agglomerative clustering method [32] (Agglom), the graph partitioning approach [33] (GraphPart), the global role ordering model [22] (RoleOrdering).", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "We also compare against previous approaches: the latent logistic classification model [31] (labeled LLogistic), the agglomerative clustering method [32] (Agglom), the graph partitioning approach [33] (GraphPart), the global role ordering model [22] (RoleOrdering).", "startOffset": 148, "endOffset": 152}, {"referenceID": 32, "context": "We also compare against previous approaches: the latent logistic classification model [31] (labeled LLogistic), the agglomerative clustering method [32] (Agglom), the graph partitioning approach [33] (GraphPart), the global role ordering model [22] (RoleOrdering).", "startOffset": 195, "endOffset": 199}, {"referenceID": 21, "context": "We also compare against previous approaches: the latent logistic classification model [31] (labeled LLogistic), the agglomerative clustering method [32] (Agglom), the graph partitioning approach [33] (GraphPart), the global role ordering model [22] (RoleOrdering).", "startOffset": 244, "endOffset": 248}, {"referenceID": 33, "context": "We also report results of an improved version of Agglom, recently reported by Lang and Lapata [34] (Agglom+).", "startOffset": 94, "endOffset": 98}, {"referenceID": 50, "context": "The strongest previous model is Bayes: Bayes is the most accurate (\u2018coupled\u2019) version of the Bayesian model of Titov and Klementiev [51], estimated from the CoNLL data without relying on any external data.", "startOffset": 132, "endOffset": 136}, {"referenceID": 50, "context": "Titov and Klementiev [51] also showed that using Brown clusters induced from a large external corpus resulted in an 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 51, "context": "For German, we replicate the experimental set-up previously used by Titov and Klementiev [52].", "startOffset": 89, "endOffset": 93}, {"referenceID": 51, "context": "We compare against Bayes+LangSpecific \u2013 the Bayes model with argument signatures specialized for German (as reported in Titov and Klementiev [52]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "Recently, Lang and Lapata [34], evaluated their Agglom+ on a version of the same German SALSA dataset.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Previous semi-supervised approaches to SRL can mostly be regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts [9, 14] or creating some form of \u2018surrogate\u2019 supervision [26, 18, 11].", "startOffset": 171, "endOffset": 178}, {"referenceID": 13, "context": "Previous semi-supervised approaches to SRL can mostly be regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts [9, 14] or creating some form of \u2018surrogate\u2019 supervision [26, 18, 11].", "startOffset": 171, "endOffset": 178}, {"referenceID": 25, "context": "Previous semi-supervised approaches to SRL can mostly be regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts [9, 14] or creating some form of \u2018surrogate\u2019 supervision [26, 18, 11].", "startOffset": 228, "endOffset": 240}, {"referenceID": 17, "context": "Previous semi-supervised approaches to SRL can mostly be regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts [9, 14] or creating some form of \u2018surrogate\u2019 supervision [26, 18, 11].", "startOffset": 228, "endOffset": 240}, {"referenceID": 10, "context": "Previous semi-supervised approaches to SRL can mostly be regarded as extensions to supervised learning by either incorporating word features induced from unnannoted texts [9, 14] or creating some form of \u2018surrogate\u2019 supervision [26, 18, 11].", "startOffset": 228, "endOffset": 240}, {"referenceID": 10, "context": "The benefits from using unlabeled data were moderate, and more significant for the harder SRL version, frame-semantic parsing [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 40, "context": "Another important direction includes cross-lingual approaches [41, 54] which leverage resources for research-rich languages, as well as parallel data, to transfer the annotation to the resource-poor languages.", "startOffset": 62, "endOffset": 70}, {"referenceID": 53, "context": "Another important direction includes cross-lingual approaches [41, 54] which leverage resources for research-rich languages, as well as parallel data, to transfer the annotation to the resource-poor languages.", "startOffset": 62, "endOffset": 70}, {"referenceID": 51, "context": "Nevertheless, even joint unsupervised induction across languages appears to be beneficial [52].", "startOffset": 90, "endOffset": 94}, {"referenceID": 34, "context": "Unsupervised learning has also been one of the central paradigms for the closely-related area of relation extraction (RE), where several techniques have been proposed to cluster semantically similar verbalizations of relations [35, 4, 58].", "startOffset": 227, "endOffset": 238}, {"referenceID": 3, "context": "Unsupervised learning has also been one of the central paradigms for the closely-related area of relation extraction (RE), where several techniques have been proposed to cluster semantically similar verbalizations of relations [35, 4, 58].", "startOffset": 227, "endOffset": 238}, {"referenceID": 56, "context": "Unsupervised learning has also been one of the central paradigms for the closely-related area of relation extraction (RE), where several techniques have been proposed to cluster semantically similar verbalizations of relations [35, 4, 58].", "startOffset": 227, "endOffset": 238}, {"referenceID": 1, "context": "From the learning perspective, methods which use the reconstruction-error objective to estimate linear models [2, 12] are certainly related.", "startOffset": 110, "endOffset": 117}, {"referenceID": 11, "context": "From the learning perspective, methods which use the reconstruction-error objective to estimate linear models [2, 12] are certainly related.", "startOffset": 110, "endOffset": 117}, {"referenceID": 5, "context": ", [6]) are also close in spirit.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", CoNLL shared tasks [8, 48, 25]).", "startOffset": 21, "endOffset": 32}, {"referenceID": 47, "context": ", CoNLL shared tasks [8, 48, 25]).", "startOffset": 21, "endOffset": 32}, {"referenceID": 24, "context": ", CoNLL shared tasks [8, 48, 25]).", "startOffset": 21, "endOffset": 32}], "year": 2014, "abstractText": "We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages.", "creator": "LaTeX with hyperref package"}}}