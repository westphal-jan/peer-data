{"id": "1412.4021", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2014", "title": "A Robust Transformation-Based Learning Approach Using Ripple Down Rules for Part-of-Speech Tagging", "abstract": "This paper presents a new method to construct a system of transformation rules for the Part-Of-Speech tagging task. Our approach is based on an incremental knowledge acquisition methodology where rules are stored in an exception-structure and new rules are only added to correct errors of existing rules; thus allowing systematic control of the interaction between rules. Experiments on 13 languages exhibit that our method is fast in terms of training time and tagging speed. Furthermore, our method is able to attain state-of-the-art accuracies for relatively isolating or analytic languages whilst reaching competitive accuracy results on morphologically rich Indo-European languages.\n\n\n\n\nThe implementation of a new and efficient tagging algorithm has an important practical relevance. The model is the implementation of a system of transformation rules for the Part-Of-Speech tagging task. This system is able to capture new data during the context of a given program and analyze the data that is presented and analyze the data as it relates to the language itself. We also propose a way to construct a system of transformation rules for the Part-Of-Speech tagging task. We propose a method of transformation rules for the Part-Of-Speech tagging task.\n\n\n\nPart-Of-Speech tagging\nThe results of this study show that we can safely train our language by using different methodologies. Moreover, our approach is still a complex but straightforward way of training, especially when used with a particular language. In addition, we show that we can train our language by using different methods for each of our types in a single program. We also show that we can now train our language by using different methods for each of our classes in a single program.\nWe can train our language by applying the same techniques for each of our types in a single program and by training the same methodologies for each of our types in a single program.\nThere are several ways to achieve this approach. The first is by using special methods to train a new type for every type. Using this technique is also an example of the way to train a new type for each type in a single program. This is a very efficient way of training the language.", "histories": [["v1", "Fri, 12 Dec 2014 15:26:43 GMT  (220kb,D)", "https://arxiv.org/abs/1412.4021v1", "13 pages, 3 figures, 5 tables. This article extends the work described in our publications at CICLing2011 and EACL2014 conferences. We make minor revisions to our conference approach to yield improved accuracy results on English and Vietnamese, and we conduct extensively new empirical study on 11 other languages"], ["v2", "Mon, 2 Mar 2015 06:03:22 GMT  (481kb,D)", "http://arxiv.org/abs/1412.4021v2", "v1: 13 pages, 3 figures, 5 tables. This article extends the work described in our publications at CICLing2011 and EACL2014 conferences. We make minor revisions to our conference approach to yield improved accuracy results on English and Vietnamese, and we conduct extensively new empirical study on 11 other languages. v2: Minor changes in sections 2 and 3"], ["v3", "Sat, 29 Aug 2015 19:03:01 GMT  (294kb,D)", "http://arxiv.org/abs/1412.4021v3", "v1: 13 pages, 3 figures, 5 tables. This article extends the work described in our publications at CICLing2011 and EACL2014 conferences. We conduct extensively new empirical study on 11 other languages. v2: Minor changes in sections 2 and 3. v3: major changes in section 4. We provide the comparison of our approach with two other approaches in terms of training time, tagging speed and accuracy"], ["v4", "Wed, 18 Nov 2015 02:41:55 GMT  (142kb,D)", "http://arxiv.org/abs/1412.4021v4", "v1: 13 pages, 3 figures, 5 tables. v2: Minor changes in sections 2 and 3. v3: major changes in section 4. We provide the comparison of our approach with two other approaches in terms of training time, tagging speed and accuracy. v4: minor changes in our approach (section 3) =&gt; yield improved tagging accuracy, especially on morphologically rich languages"], ["v5", "Sat, 19 Dec 2015 11:06:15 GMT  (141kb,D)", "http://arxiv.org/abs/1412.4021v5", "Version 1: 13 pages. Version 2: Submitted to AI Communications - the European Journal on Artificial Intelligence. Version 3: Resubmitted after major revisions. Version 4: Resubmitted after minor revisions. Version 5: to appear in AI Communications (accepted for publication on 3/12/2015)"]], "COMMENTS": "13 pages, 3 figures, 5 tables. This article extends the work described in our publications at CICLing2011 and EACL2014 conferences. We make minor revisions to our conference approach to yield improved accuracy results on English and Vietnamese, and we conduct extensively new empirical study on 11 other languages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dat quoc nguyen", "dai quoc nguyen", "dang duc pham", "son bao pham"], "accepted": false, "id": "1412.4021"}, "pdf": {"name": "1412.4021.pdf", "metadata": {"source": "CRF", "title": "A Robust Transformation-Based Learning Approach Using Ripple Down Rules for Part-of-Speech Tagging", "authors": ["Dat Quoc Nguyen", "Dai Quoc Nguyen", "Dang Duc Pham", "Son Bao Pham"], "emails": ["dat.nguyen@students.mq.edu.au", "daiquocn@coli.uni-saarland.de", "pham@l3s.de", "sonpb@vnu.edu.vn", "dat.nguyen@students.mq.edu.au."], "sections": [{"heading": null, "text": "Keywords: Natural language processing, Part-of-Speech tagging, Morphological tagging, Single Classification Ripple Down Rules, Rule-based POS tagger, RDRPOSTagger, Bulgarian, Czech, Dutch, English, French, German, Hindi, Italian, Portuguese, Spanish, Swedish, Thai, Vietnamese"}, {"heading": "1. Introduction", "text": "POS tagging is one of the most important tasks in Natural Language Processing (NLP) that assigns a tag to each word in a text, which the tag represents the word\u2019s lexical category [26]. After the text has been tagged or annotated, it can be used in many appli-\n*Corresponding author. E-mail: dat.nguyen@students.mq.edu.au.\n**The first two authors contributed equally to this work.\ncations such as machine translation, information retrieval, information extraction and the like. Recently, statistical and machine learning-based POS tagging methods have become the mainstream ones obtaining state-of-the-art performance. However, the learning process of many of them is time-consuming and requires powerful computers for training. For example, for the task of combined POS and morphological tagging, as reported by Mueller et al. [43], the taggers SVMTool [25] and CRFSuite [52] took 2,454 minutes (about 41 hours) and 9,274 minutes (about 155 hours) respectively to train on a corpus of 38,727 Czech sentences (652,544 words), using a machine with two Hexa-Core Intel Xeon X5680 CPUs with 3,33 GHz and 144 GB of memory. Therefore, such methods might not be reasonable for individuals having limited computing resources. In addition, the tagging speed of many of those systems is relatively slow. For example, as reported by Moore [42], the SVMTool, the COMPOST tagger [71] and the UPenn bidirectional tagger [66] respectively achieved the tagging speed of 7700, 2600 and 270 English word tokens per second, using a Linux workstation with Intel Xeon X5550 2.67 GHz processors. So these methods may not be adaptable to the recent large-scale data NLP tasks where the fast tagging speed is necessary. Turning to the rule-based POS tagging methods, the most well-known method proposed by Brill [10] automatically learns transformation-based error-driven\nAI Communications, to appear ISSN 0921-7126, IOS Press. All rights reserved\nar X\niv :1\n41 2.\n40 21\nv5 [\nrules. In the Brill\u2019s method, the learning process selects a new rule based on the temporary context which is generated by all the preceding rules; the learning process then applies the new rule to the temporary context to generate a new context. By repeating this process, a sequentially ordered list of rules is produced, where a rule is allowed to change the outputs of all the preceding rules, so a word could be relabeled multiple times. Consequently, the Brill\u2019s method is slow in terms of training and tagging processes [27, 46]. In this paper, we present a new error-driven approach to automatically restructure transformation rules in the form of a Single Classification Ripple Down Rules (SCRDR) tree [15, 57]. In the SCRDR tree, a new rule can only be added when the tree produces an incorrect output. Therefore, our approach allows the interaction between the rules, where a rule can only change the outputs of some preceding rules in a controlled context. To sum up, our contributions are:\n\u2013 We propose a new transformation-based error-driven approach for POS and morphological tagging task, using SCRDR.1 Our approach obtains fast performance in both learning and tagging process. For example, in the combined POS and morphological tagging task, our approach takes an average of 61 minutes (about 1 hour) to complete a 10-fold cross validation-based training on a corpus of 116K Czech sentences (about 1,957K words), using a computer with Intel Core i5-2400 3.1GHz CPU and 8GB of memory. In addition, in the English POS tagging, our approach achieves a tagging speed of 279K word tokens per second. So our approach can be used on computers with limited resources or can be adapted to the large-scale data NLP tasks.\n\u2013 We provide empirical experiments on the POS tagging task and the combined POS and morphological tagging task for 13 languages. We compare our approach to two other approaches in terms of running time and accuracy, and show that our robust and language-independent method achieves a very competitive accuracy in comparison to the state-of-theart results.\nThe paper is organized as follows: sections 2 and 3 present the SCRDR methodology and our new approach, respectively. Section 4 details the experimental results while Section 5 outlines the related work. Finally, Section 6 provides the concluding remarks and future work.\n1Our free open-source implementation namely RDRPOSTagger is available at http://rdrpostagger.sourceforge.net/"}, {"heading": "2. SCRDR methodology", "text": "A SCRDR tree [15, 49, 57] is a binary tree with two distinct types of edges. These edges are typically called except and if-not edges. Associated with each node in the tree is a rule. A rule has the form: if \u03b1 then \u03b2 where \u03b1 is called the condition and \u03b2 is called the conclusion. Cases in SCRDR are evaluated by passing a case to the root of the tree. At any node in the tree, if the condition of the rule at a node \u03b7 is satisfied by the case (so the node \u03b7 fires), the case is passed on to the except child node of the node \u03b7 using the except edge if it exists. Otherwise, the case is passed on to the if-not child node of the node \u03b7. The conclusion of this process is given by the node which fired last. For example, with the SCRDR tree in Figure 1, given a case of 5-word window context \u201cas/IN investors/NNS anticipate/VB a/DT recovery/NN\u201d where \u201canticipate/VB\u201d is the current word and POS tag pair, the case satisfies the conditions of the rules at nodes (0), (1) and (4), then it is passed on to node (5), using except edges. As the case does not satisfy the condition of the rule at node (5), it is passed on to node (8) using the if-not edge. Also, the case does not satisfy the conditions of the rules at nodes (8) and (9). So we have the evaluation path (0)-(1)-(4)-(5)-(8)-(9) with the last fired node (4). Thus, the POS tag for \u201canticipate\u201d is concluded as \u201cVBP\u201d produced by the rule at node (4). A new node containing a new exception rule is added to an SCRDR tree when the evaluation process returns an incorrect conclusion. The new node is attached to the last node in the evaluation path of the given case with the except edge if the last node is the fired node; otherwise, it is attached with the if-not edge. To ensure that a conclusion is always given, the root node (called the default node) typically contains a trivial condition which is always satisfied. The rule at the default node, the default rule, is the unique rule which is not an exception rule of any other rule. In the SCRDR tree in Figure 1, rule (1) - the rule at node (1) - is an exception rule of the default rule (0). As node (2) is the if-not child node of node (1), rule (2) is also an exception rule of rule (0). Likewise, rule (3) is an exception rule of rule (0). Similarly, both rules (4) and (10) are exception rules of rule (1) whereas rules (5), (8) and (9) are exception rules of rule (4), and so on. Therefore, the exception structure of the SCRDR tree extends to four levels: rules (1), (2) and (3) at layer 1; rules (4), (10), (11), (12) and (14) at layer 2; rules (5), (8), (9), (13) and (15) at layer 3; and rules (6) and (7) at layer 4 of the exception structure."}, {"heading": "3. Our approach", "text": "In this section, we present a new error-driven approach to automatically construct a SCRDR tree of transformation rules for POS tagging. The learning process in our approach is described in Figure 2.\nThe initialized corpus is generated by using an initial tagger to perform POS tagging on the raw corpus which consists of the raw text extracted from the gold standard training corpus, excluding POS tags. Our initial tagger uses a lexicon to assign a tag for each word. The lexicon is constructed from the gold\nstandard corpus, where each word type is coupled with its most frequent associated tag in the gold standard corpus. In addition, the character 2-, 3-, 4- and 5-gram suffixes of word types are also included in the lexicon. Each suffix is coupled with the most frequent2 tag associated to the word types containing this suffix. Furthermore, the lexicon also contains three default tags corresponding to the tags most frequently assigned to words containing numbers, capitalized words and lowercase words. The suffixes and default tags are only used to label unknown words (i.e. out-of-lexicon words). To handle unknown words in English, our initial tagger uses regular expressions to capture the information about capitalization and word suffixes.3 For other languages, the initial tagger firstly determines whether the word contains any numeric character to get the default tag for numeric word type. If the word does not contain any numeric character, the initial tagger then extracts\n2The frequency must be greater than 1, 2, 3 and 4 for the 5-, 4-, 3- and 2-gram suffixes, respectively.\n3An example of a regular expression in Python is as follows: if (re.search(r\u2032(.*ness$) | (.*ment$) | (.*ship$) | (^[Ee]x-.*) | (^[Ss]elf-.*)\u2032, word) != None): tag = \u201cNN\u201d.\nthe 5-, 4-, 3- and 2-gram suffixes in this order and returns the coupled tag corresponding to the first suffix found in the lexicon. If the lexicon does not contain any of the suffixes of the word, the initial tagger determines whether the word is capitalized or in lowercase form to return the corresponding default tag. By comparing the initialized corpus with the gold standard corpus, an object-driven dictionary of Object and correctTag pairs is produced. Each Object captures a 5-word window context of a word and its current initialized tag in the format of (previous 2nd word, previous 2nd tag, previous 1st word, previous 1st tag, word, current tag, next 1st word, next 1st tag, next 2nd word, next 2nd tag, last-2-characters, last-3-characters, last4-characters), extracted from the initialized corpus.4 The correctTag is the corresponding \u201ctrue\u201d tag of the word in the gold standard corpus.\nThe rule selector is responsible for selecting the most suitable rules to build the SCRDR tree. To generate concrete rules, the rule selector uses rule templates. The examples of our rule templates are presented in Table 1, where the elements in boldwill be replaced by specific values from the Object and correctTag pairs in the object-driven dictionary. Short descriptions of the rule templates are shown in Table 2. The SCRDR rule tree is initialized with the default rule if True then tag = \u201c\u201d as shown in Figure 1.5 Then the system creates a rule of the form if currentTag == \u201cLabel\u201d then tag = \u201cLabel\u201d for each POS tag in the\n4In the example case from Section 2, the Object corresponding to the 5-word context window is {as, IN, investors, NNS, anticipate, VB, a, DT, recovery, NN, te, ate, pate}.\n5The default rule returns an incorrect conclusion of empty POS tag for every Object.\nlist of all tags extracted from the initialized corpus. These rules are added to the SCRDR tree as exception rules of the default rule to create the first layer exception structure, as for instance the rules (1), (2) and (3) in Figure 1.\n3.1. Learning process\nThe process to construct new exception rules to higher layers of the exception structure in the SCRDR tree is as follows:\n\u2013 At each node \u03b7 in the SCRDR tree, let \u0398\u03b7 be the set of Object and correctTag pairs from the objectdriven dictionary such that the node \u03b7 is the last fired node for every Object in \u0398\u03b7 and the node \u03b7 returns an incorrect POS tag (i.e. the POS tag concluded by the node \u03b7 for each Object in \u0398\u03b7 is not the corresponding correctTag). A new exception rule must be added to the next level of the SCRDR tree to correct the errors given by the node \u03b7. \u2013 The new exception rule is selected from all concrete rules generated for all Objects in \u0398\u03b7. The selected rule must satisfy the following constraints: (i) If node \u03b7 is at level-k exception structure in the SCRDR tree such that k > 1 then the rule\u2019s condition must not be satisfied by the Objects for which node \u03b7 has already returned a correct POS tag. (ii) Let A and B be the number of Objects in \u0398\u03b7 that satisfy the rule\u2019s condition, and the rule\u2019s conclusion returns the correct and incorrect POS tag, respectively. Then the rule with the highest score value S =A\u2212B will be chosen. (iii) The score S of the chosen rule must be higher than a given threshold. We apply two threshold parameters: the first threshold is to find exception rules at the layer-2 exception structure, such as rules (4), (10) and (11) in Figure 1, while the second threshold is to find rules for higher exception layers. \u2013 If the learning process is unable to select a new exception rule, the learning process is repeated at node \u03b7\u03c1 for which the rule at the node \u03b7 is an exception rule of the rule at the node \u03b7\u03c1. Otherwise, the learning process is repeated at the new selected exception rule.\nIllustration: To illustrate how new exception rules are added to build a SCRDR tree in Figure 1, we start with node (1) associated to rule (1) if currentTag == \u201cVB\u201d then tag = \u201cVB\u201d at the layer-1 exception structure. The learning process chooses the rule if prev1stTag == \"NNS\" then tag = \"VBP\" as an exception rule for rule (1). Thus, node (4) associated with\nrule (4) if prev1stTag == \"NNS\" then tag = \"VBP\" is added as an except child node of node (1). The learning process is then repeated at node (4). Similarly, nodes (5) and (6) are added to the tree as shown in Figure 1. The learning process now is repeated at node (6). At node (6), the learning process cannot find a suitable rule that satisfies the three constraints described above. So the learning process is repeated at node (5) because rule (6) is an exception rule of rule (5). At node (5), the learning process selects a new rule (7) if next1stWord == \"into\" then tag = \"VBD\" to be another exception rule of rule (5). Consequently, a new node (7) containing rule (7) is added to the tree as an if-not child node of node (6). At node (7), the learning process cannot find a new rule to be an exception rule of rule (7). Therefore, the learning process is again repeated at node (5).\nThis process of adding new exception rules is repeated until no rule satisfying the three constraints can be found.\n3.2. Tagging process\nThe tagging process firstly tags unlabeled text by using the initial tagger. Next, for each initially tagged word the corresponding Object will be created by sliding a 5-word context window over the text from left to right. Finally, each word will be tagged by passing its Object through the learned SCRDR tree, as illustrated in the example in Section 2. If the default node is the last fired node satisfying the Object, the final tag returned is the tag produced by the initial tagger."}, {"heading": "4. Empirical study", "text": "This section presents the experiments validating our proposed approach in 13 languages. We also compare our approach with the TnT6 approach [9] and the MarMoT7 approach proposed by Mueller et al. [43]. The TnT tagger is considered as one of the fastest POS taggers in literature (both in terms of training and tagging), obtaining competitive tagging accuracy on diverse languages [26]. TheMarMoT tagger is a morphological tagger obtaining state-of-the-art tagging accuracy on various languages such as Arabic, Czech, English, German, Hungarian and Spanish. We run all experiments on a computer of Intel Core i5-2400 3.1GHz CPU and 8GB of memory. Experi-\n6www.coli.uni-saarland.de/~thorsten/tnt/ 7http://cistern.cis.lmu.de/marmot/\nments on English use the Penn WSJ Treebank [40] sections 0-18 (38,219 sentences - 912,344 words) for training, sections 19-21 (5,527 sentences - 131,768 words) for validation, and the sections 22-24 (5,462 sentences - 129,654 words) for testing. The proportion of unknown words in the test set is 2.81% (3,649 unknown words). We also conduct experiments on 12 other languages. The experimental datasets for those languages are described in Table 3. Apart from English, it is difficult to compare the results of previously published works because each of them have used different experimental setups and data splits. Thus, it is difficult to create the same evaluation settings used in the previous works. So we perform 10- fold cross validation8 for all languages other than English, except for Vietnamese where we use 5-fold cross validation. Our approach: In training phase, all words appearing only once time in the training set are initially treated as unknown words and tagged as described in Section 3. This strategy produces tagging models containing transformation rules learned on error contexts of unknown words. The threshold parameters were tuned on the English validation set. The best value pair (3, 2) was then used in all experiments for all languages. TnT & MarMoT: We used default parameters for training TnT and MarMoT.\n4.1. Accuracy Results\nWe present the tagging accuracy of our approach with the lexicon-based initial tagger (for short, RDRPOSTagger) and TnT in Table 4. As can be seen from Table 4, our RDRPOSTagger does better than TnT on isolating languages such as Hindi, Thai and Vietnamese. For the combined POS and morphological (POS+MORPH) tagging task on morphologically rich languages such as Bulgarian, Czech, Dutch, French, German, Portuguese, Spanish and Swedish, RDRPOSTagger and TnT generally obtain similar results on known words. However, RDRPOSTagger performs worse on unknown words. This can be because RDRPOSTagger uses a simple lexicon-based method for tagging unknown words, while TnT uses a more complex suffix analysis to handle unknown words.\n8For each dataset, we split the dataset into 10 contiguous parts (i.e. 10 contiguous folds). The evaluation procedure is repeated 10 times. Each part is used as the test set and 9 remaining parts are merged as the training set. All accuracy results are reported as the average results over the test folds.\nTherefore, TnT performs better than RDRPOSTagger on morphologically rich languages. These initial accuracy results could be improved by\nfollowing any of the previous studies that use external lexicon resources or existing morphological analyzers. In this research work, we simply employ TnT\nas the initial tagger in our approach. We report the accuracy results of our approach using TnT as the initial tagger (for short, RDRPOSTagger+TnT) and MarMoT in Table 5. To sum up, RDRPOSTagger+TnT obtains competitive results in comparison to the stateof-the-art MarMoT tagger, across the 13 experimental\nlanguages. In particular, excluding Czech and German where MarMoT embeds existing morphological analyzers, RDRPOSTagger+TnT obtains accuracy results which mostly are about 0.5% lower than MarMoT\u2019s.\n4.1.1. English RDRPOSTagger produces a SCRDR tree model of 2,549 rules in a 5-level exception structure and achieves an accuracy of 96.54% against 96.46% accounted for TnT, as presented in Table 4. Table 6 presents the accuracy results obtained up to each exception level of the tree.\nAs shown in [47], using the same evaluation scheme for English, the Brill\u2019s rule-based tagger V1.14 [10] gained a similar accuracy result at 96.53%.9 Using TnT as the initial tagger, RDRPOSTagger+TnT achieves an accuracy of 96.86% which is comparable to the stateof-the-art result at 97.24% obtained by MarMoT.\n9The Brill\u2019s tagger uses an initial tagger with an accuracy of 93.58% on the test set. Using this initial tagger, our approach gains a higher accuracy of 96.57%.\n4.1.2. Bulgarian In Bulgarian, RDRPOSTagger+TnT obtains an accuracy of 94.12%which is 0.74% lower than the accuracy of MarMoT at 94.86%. This is better than the results reported on the BulTreeBank webpage10 on POS+MORPH tagging task, where TnT, SVMTool [25] and the memory-based tagger in the Acopost package11 [64] obtained accuracies of 92.53%, 92.22% and 89.91%, respectively. Our result is also better than the accuracy of 90.34% reported by Georgiev et al. [22], obtained with the Maximum Entropy-base POS tagger from the OpenNLP toolkit.12 Recently, Georgiev et al. [23]13 reached the state-ofthe-art accuracy result of 97.98% for POS+MORPH tagging, however, without external resources the accuracy was 95.72%.\n4.1.3. Czech Mueller et al. [43] presented the results of five POS taggers SVMTool, CRFSuite [52], RFTagger [62], Morfette [12] and MarMoT for Czech POS+MORPH tagging. All models were trained using a training set of 38,727 sentences (652,544 tokens) and evaluated on a test set of 4,213 sentences (70,348 tokens), extracted\n10http://www.bultreebank.org/taggers/taggers.html 11http://acopost.sourceforge.net/ 12http://opennlp.sourceforge.net 13Georgiev et al. [23] split the BulTreeBank corpus into training set of 16,532 sentences, development set of 2,007 sentences and test set of 2,017 sentences.\nfrom the Prague Dependency Treebank 2.0. The accuracy results are 89.62%, 90.97%, 90.43%, 90.01% and 92.99% accounted for SVMTool, CRFSuite, RFTagger, Morfette and MarMoT, respectively. Since we could not access the Czech datasets used in the experiments above, we employ the Prague Dependency Treebank 2.5 [5] containing about 116K sentences. The accuracies of RDRPOSTagger (91.29%) and RDRPOSTagger+TnT (91.70%) compare favorably to the result of MarMot (93.50%).\n4.1.4. Dutch The TADPOLE tagger [78] was reached an accuracy of 96.5% when trained on a manually POS-annotated corpus containing 11 million Dutch words and 316 tags. Due to the limited access we could not use this corpus in our experiments and thus we can not compare our results with the TADPOLE tagger. Instead, we use the Lassy Small Corpus [51] containing about 1.1 million words. RDRPOSTagger+TnT achieves a promising accuracy at 92.17% which is 1% absolute lower than the accuracy of MarMoT (93.17%).\n4.1.5. French Current state-of-the-art methods for French POS tagging have reached accuracies up to 97.75% [65, 17], using the French Treebank [1] with 9,881 sentences for training and 1,235 sentences for test. However, these methods employed Lefff [58] which is an external large-scale morphological lexicon. Without using the lexicon, Denis and Sagot [17] reported an accuracy performance at 97.0%. We trained our systems on 21,562 annotated French Treebank sentences and gained a POS tagging accuracy of 97.70% using RDRPOSTagger+TnT model, which is comparable to the accuracy at 97.93% of MarMoT. Regarding to POS+MORPH tagging, as far as we know this is the first experiment for French, where RDRPOSTagger+TnT obtains an accuracy of 94.16% against 94.62% obtained by MarMoT.\n4.1.6. German Using the 10-fold cross validation evaluation scheme on the TIGER corpus [8] of 50,474 German sentences, Giesbrecht and Evert [24] presented the results of TreeTagger [61], TnT, SVMTool, Stanford tagger [75] and Apache UIMA Tagger14 obtaining the POS tagging accuracies at 96.89%, 96.92%, 97.12%, 97.63% and 96.04%, respectively. In the same evaluation setting, RDRPOSTagger+TnT gains an accuracy re-\n14https://uima.apache.org/sandbox.html#tagger.annotator\nsult of 97.46% while MarMoT gains a higher accuracy at 97.85%. Turning to POS+MORPH tagging, Mueller et al. [43] also performed experiments on the TIGER corpus, using 40,474 sentences for training and 5,000 sentences for test. They presented accuracy performances of 83.42%, 85.68%, 84.28%, 83.48% and 88.58% obtained with the taggers SVMTool, CRFSuite, RFTagger, Morfette and MarMoT, respectively. In our evaluation scheme, RDRPOSTagger and RDRPOSTagger+TnT correspondingly achieve favorable accuracy results at 84.92% and 85.66% in comparison to an accuracy at 88.94% of MarMoT.\n4.1.7. Hindi On the Hindi Treebank [55], RDRPOSTagger+TnT reaches a competitive accuracy result of 96.21% against the accuracy of MarMoT at 96.61%. Being one of the largest languages in the world, there are many previous works on POS tagging for Hindi. However, most of them have used small manually labeled datasets that are not publicly available and that are smaller than the Hindi Treebank used in this paper. Joshi et al. [29] achieved an accuracy of 92.13% using a Hidden Markov Model-based approach, trained on a dataset of 358K words and tested on 12K words. Using another training set of 150K words and test set of 40K words, Agarwal et al. [2] compared machine learning-based approaches and presented the POS tagging accuracy at 93.70%. In the 2007 Shallow Parsing Contest for South Asian Languages [6], the POS tagging track provided a small training set of 21,470 words and a test set of 4,924 words. The highest accuracy in the contest was 78.66% obtained by Avinesh and Karthik [4]. In the same 4- fold cross validation evaluation scheme using a dataset of 15,562 words, Singh et al. [68] obtained an accuracy of 93.45% whilst Dalal et al. [16] achieved a result at 94.38%.\n4.1.8. Italian In the EVALITA 2009 workshop on Evaluation of NLP and Speech Tools for Italian15, the POS tagging track [3] provided a training set of 3,719 sentences (108,874 word forms) with 37 POS tags. The teams participating in the closed task where using external resources was not allowed achieved various tagging accuracies on a test set of 147 sentences (5,066 word forms), ranging from 93.21% to 96.91%. Our experiment on Italian POS tagging employs the ISDT Treebank [7] of 10,206 sentences (190,310 word\n15http://www.evalita.it/2009\nforms) with 70 POS tags. RDRPOSTagger+TnT obtains a competitive accuracy performance at 95.49% against 95.98% computed for MarMoT.\n4.1.9. Portuguese The previous works [18, 30] on POS+MORPH tagging for Portuguese used an early version of the Tycho Brahe corpus [21] containing about 1,036K words. The corpus was split into a training set of 776K words and a test set of 260K words. Based on this setting, Kepler and Finger [30] achieved an accuracy of 95.51% while dos Santos et al. [18] reached a state-of-the-art accuracy result at 96.64%. The Tycho Brahe corpus in our experiment consists of about 1,639K words. RDRPOSTagger+TnT reaches an accuracy at 95.53% while MarMoT obtains higher result at 95.86% on 10-fold cross validation.\n4.1.10. Spanish In addition to Czech and German, Mueller et al. [43] evaluated the five taggers of SVMTool, CRFSuite, RFTagger, Morfette and MarMoT for Spanish POS+MORPH tagging, using a training set of 14,329 sentences (427,442 tokens) and a test set of 1,725 sentences (50,630 tokens) with 303 POS+MORPH tags. The accuracy results of the five taggers ranged from 97.35% to 97.93%, in which MarMoT obtained the highest result. As we could not access the training and test sets used in Mueller et al. [43]\u2019s experiment, we use the IULA Spanish LSP Treebank [41] of 42K sentences with 241 tags. RDRPOSTagger and RDRPOSTagger+TnT achieve accuracies of 97.95% and 98.26%, respectively, whileMarMoT obtains a higher result at 98.45%. NOTE that here we can make an indirect comparison between our RDRPOSTagger and the SVMTool, CRFSuite, RFTagger and Morfette taggers via MarMoT. We conclude that the results of RDRPOSTagger would likely be similar to the results of SVMTool, CRFSuite, RFTagger and Morfette on Spanish as well as on Czech and German.\n4.1.11. Swedish On the same SUC corpus 3.0 [72] consisting of 500 text files with about 74K sentences that we also use, O\u0308stling [53] evaluated the Swedish POS tagger Stagger using 10-fold cross validation but the folds were split at the file level and not on sentence level as we do. Stagger attained an accuracy of 96.06%. In our experiment, RDRPOSTagger+TnT obtains an accuracy result of 95.81% in comparison to the accuracy at 96.22% of MarMoT.\n4.1.12. Thai On the Thai POS Tagged corpus ORCHID [70] of 23,225 sentences, RDRPOSTagger+TnT achieves an accuracy of 94.22% which is 0.72% absolute lower than the accuracy result of MarMoT (94.94%). It is difficult to compare our results to the previous work on Thai POS tagging. For example, the previous works [39, 45] performed their experiments on an unavailable corpus of 10,452 sentences. The ORCHID corpus was also used in a POS tagging experiment presented by Kruengkrai et al. [32], however, the obtained accuracy of 79.342% was dependent on the performance of automatic word segmentation. On another corpus of 100K words, Pailai et al. [54] reached an accuracy of 93.64% using 10-fold cross validation.\n4.1.13. Vietnamese We participated in the first evaluation campaign on Vietnamese language processing16 (VLSP). The campaign\u2019s POS tagging track provided a training set of 28,232 POS-annotated sentences and an unlabeled test set of 2,130 sentences. RDRPOSTagger achieved the 1st place in the POS tagging track. In this paper, we also carry out POS tagging experiments using 5-fold cross validation evaluation scheme on the VLSP set of 28,232 sentences and the standard benchmark Vietnamese Treebank [50] of about 10K sentences. On these datasets, RDRPOSTagger+TnT achieves competitive results (93.63% and 92.95%) in comparison to MarMoT (94.13% and 93.53%). In addition, on the Vietnamese Treebank, RDRPOSTagger with the accuracy 92.59% outperforms the previously reported Maximum Entropy Model, Conditional Random Fields and Support Vector Machinebased approaches [76] where the highest obtained accuracy was 91.64%.\n4.2. Training time and tagging speed\nWhile most published works have not reported training times and tagging speeds, we present our singlethreaded implementation results in Tables 4 and 5.17 From there we can see that TnT is the fastest in terms of both training and tagging when compared to our RDRPOSTagger and MarMoT. Our RDRPOSTagger and MarMoT require similar training times, however, RDRPOSTagger is significantly faster than MarMoT in terms of tagging speed.\n16http://uet.vnu.edu.vn/rivf2013/campaign.html 17To measure the tagging speed on a test fold, we perform the\ntagging process on the test fold 10 times and then take the average.\nIt is interesting to note that in some languages, training our RDRPOSTagger is faster for combined POS+MORPH tagging task than for POS tagging, as presented in experimental results for French (9 minutes vs 16 minutes) and German (22 minutes vs 28 minutes) in Table 4. Usually in machine learning-based approaches fewer number of tags leads to higher training speed. For example, on a 40,474-sentence subset of the German TIGER corpus [8], SVMTool took about 899 minutes (about 15 hours) to train using 54 POS tags as compared to about 1,649 minutes (about 27 hours) using 681 POS+MORPH tags [43].\nIn order to compare with other existing POS taggers in terms of the training time, we show in Table 7 the time taken to train the SVMTool, CRFSuite, Morfette and RFTagger using a more powerful computer than ours. For instance, on the German TIGER corpus, RDRPOSTagger took an average of 22 minutes to train a POS+MORPH tagging model while SVMTool and CRFSuite took 1,649 minutes (about 27 hours) and 1,295 minutes (about 22 hours) respectively, as shown in Table 7. Furthermore, RDRPOSTagger uses larger datasets for Czech and Spanish and obtains faster training process as compared to SVMTool, CRFSuite and Morfette. Regarding to tagging speed, as reported by Moore [42] using the same evaluation scheme on English on a Linux workstation equipped with Intel Xeon X5550 2.67 GHz: the SVMTool, the UPenn bidirectional tagger [66], the COMPOST tagger [71], Moore [42]\u2019s approach, the accurate version of the Stanford tagger [75] and the fast and less accurate version of the Stanford tagger gained tagging speed of 7700, 270, 2600, 51K, 5900 and 80K tokens per second, respectively. In our experiment, RDRPOSTagger obtains a faster tagging speed of 279K tokens per second on a weaker computer. To the best of our knowledge, we conclude that RDRPOSTagger is fast both in terms of training and tagging in comparison to other approaches."}, {"heading": "5. Related work", "text": "From early POS tagging approaches the rule-based Brill\u2019s tagger [10] is the most well-known. The key idea of the Brill\u2019s method is to compare a manually annotated gold standard corpus with an initialized corpus which is generated by executing an initial tagger on the corresponding unannotated corpus. Based on the predefined rule templates, the method then automatically produces a list of concrete rules to correct wrongly assigned POS tags. For example, the template \u201ctransfer tag of current word from A to B if the next word is W\u201d can produce concrete rules such as \u201ctransfer tag of current word from JJ to NN if the next word is of\u201d or \u201ctransfer tag of current word from VBD to VBN if the next word is by.\u201d At each training iteration, the Brill\u2019s tagger generates a set of all possible rules and chooses the ones that help to correct the incorrectly tagged words in the whole corpus. Thus, the Brill\u2019s training process takes a significant amount of time. To prevent that, Hepple [27] presented an approach with two assumptions for disabling interactions between rules to reduce the training time while sacrificing a small amount of accuracy. Ngai and Florian [46] proposed another method to reduce the training time by recalculating the scores of rules while obtaining similar accuracy result. The main difference between our approach and the Brill\u2019s method is that we construct transformation rules in the form of a SCRDR tree where a new transformation rule is produced only based on a subset of tagging errors. So our approach is faster in term of training speed. In the conference publication version of our approach [47], we reported an improvement up to 33 times in training speed against the Brill\u2019s method. In addition, the Brill\u2019s method enables each subsequent rule to change the outputs of all preceding rules, thus a word can be tagged multiple times in the tagging process, each time by a different rule. This is different from our approach where each word is tagged only once. Consequently, our approach also achieves a faster tagging speed. In addition to our research, there is only one work that applies Ripple Down Rules method for POS tagging proposed by Xu and Hoffmann [79]. Though Xu and Hoffmann\u2019s method obtained a very competitive accuracy, it is a hand-crafted approach taking about 60 hours to manually build a SCRDR tree model for English POS tagging. Turning to statistical and machine learning methods for POS tagging, these methods can be listed as vari-\nous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36]. Overview about the POS tagging task can be found in [26, 28]."}, {"heading": "6. Conclusion and future work", "text": "In this paper, we propose a new error-driven method to automatically construct a Single Classification Ripple Down Rules tree of transformation rules for POS and morphological tagging. Our method allows the interaction between rules where a rule only changes the results of a limited number of other rules. Experimental evaluations for POS tagging and the combined POS and morphological tagging on 13 languages show that our method obtains very promising accuracy results. In addition, we successfully achieve fast training and tagging processes for all experimental languages. This could help to significantly reduce time and effort for the machine learning tasks on big data, employing POS and morphological information as learning features. An important point is that our approach is suitable to involve domain experts to add new exception rules given concrete cases that are misclassified by the tree model. This is especially important for underresourced languages where obtaining a large annotated corpus is difficult. In future work, we plan to build tagging models for other languages such as Russian, Arabic, Latin, Hungarian, Chinese and so forth.\nBibliographic note\nThis paper extends the work published in our conference publications [47, 48]. We make minor revisions to our published approach to yield improved accuracy results on English and Vietnamese, and we conduct new extensive empirical study on 11 other languages."}, {"heading": "Acknowledgments", "text": "This research work is partially supported by the research project \u201cVNU-SMM: An automated system for monitoring online social media to assist management\nand support decision making in business, politics, education, and social areas\u201d from Vietnam National University, Hanoi. The first author is supported by an International Postgraduate Research Scholarship and a NICTANRPA Top-Up Scholarship. The authors would like to thank the three anonymous reviewers, the associate editor Prof. Fabrizio Sebastiani and Dr. Kairit Sirts at the Macquarie University, Australia for helpful comments and suggestions."}], "references": [{"title": "Building a Treebank for French. In Treebanks, volume 20 of Text", "author": ["A. Abeill\u00e9", "L. Cl\u00e9ment", "F. Toussenel"], "venue": "Speech and Language Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Comparative Analysis of the Performance of CRF", "author": ["M. Agarwal", "R. Goutam", "A. Jain", "S.R. Kesidi", "P. Kosaraju", "S. Muktyar", "B. Ambati", "R. Sangal"], "venue": "HMM and MaxEnt for Part-of-Speech Tagging, Chunking and Named Entity Recognition for a Morphologically. In Proceedings of the 12th Conference of the Pacific Association for Computational Linguistics, pages 3\u20136", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Overview of the EVALITA 2009 Part-of-Speech Tagging Task", "author": ["G. Attardi", "M. Simi"], "venue": "Poster and Workshop Proceedings of the 11th Conference of the Italian Association for Artificial Intelligence", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Part-Of-Speech Tagging and Chunking using Conditional Random Fields and Transformation Based Learning", "author": ["P. Avinesh", "G. Karthik"], "venue": "Proceedings of IJ- CAI 2007Workshop on Shallow Parsing for South Asian Languages", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "and Z", "author": ["E. Bejcek", "J. Panevov\u00e1", "J. Popelka", "P. Stran\u00e1k", "M. Sevc\u00edkov\u00e1", "J. Step\u00e1nek"], "venue": "Zabokrtsk\u00fd. Prague Dependency Treebank 2.5 - a Revisited Version of PDT 2.0. In Proceedings of 24th International Conference on Computational Linguistics, pages 231\u2013246", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to the Shallow Parsing Contest for South Asian Languages", "author": ["A. Bharathi", "P.R. Mannem"], "venue": "Proceedings of IJCAI 2007 Workshop on Shallow Parsing for South Asian Languages", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Converting Italian Treebanks : Towards an Italian Stanford Dependency Treebank", "author": ["C. Bosco", "S. Montemagni", "M. Simi"], "venue": "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 61\u201369", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "TIGER: Linguistic Interpretation of a German Corpus", "author": ["S. Brants", "S. Dipper", "P. Eisenberg", "S. Hansen-Schirra", "E. K\u00f6nig", "W. Lezius", "C. Rohrer", "G. Smith", "H. Uszkoreit"], "venue": "Research on Language and Computation, 2(4):597\u2013620", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "TnT: A Statistical Part-of-Speech Tagger", "author": ["T. Brants"], "venue": "Proceedings of the Sixth Applied Natural Language Processing Conference, pages 224\u2013231", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging", "author": ["E. Brill"], "venue": "Computational Linguistics, 21 (4):543\u2013565", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Multilingual Part-of-speech Tagging with Weightless Neural Networks", "author": ["H.C. Carneiro", "F.M. Fran\u00e7a", "P.M. Lima"], "venue": "Neural Networks, 66(C):11\u201321", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["G. Chrupala", "G. Dinu"], "venue": "van Genabith, G. Chrupa\u0142a, and J. van Genabith. Learning Morphology with Morfette. In Proceedings of the 6th International Conference on Language Resources and Evaluation, pages 2362\u20132367", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative Training Methods for Hidden MarkovModels: Theory and Experiments with Perceptron Algorithms", "author": ["M. Collins"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1\u20138", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A Philosophical Basis for Knowledge Acquisition", "author": ["P. Compton", "R. Jansen"], "venue": "Knowledge Aquisition, 2(3): 241\u2013257", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Building Feature Rich POS Tagger for Morphologically Rich Languages: Experiences in Hindi", "author": ["A. Dalal", "K. Nagaraj", "U. Sawant", "S. Shelke", "P. Bhattacharyya"], "venue": "Proceedings of the 5th International Conference on Natural Language Processing", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Coupling an Annotated Corpus and a Lexicon for State-of-the-art POS Tagging", "author": ["P. Denis", "B. Sagot"], "venue": "Language Resources and Evaluation, 46:721\u2013736", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Portuguese Part-of-Speech Tagging Using Entropy Guided Transformation Learning", "author": ["C.N. dos Santos", "R.L. Milidi\u00fa", "R.P. Renter\u00eda"], "venue": "In Proceedings of the 8th International Conference on the Computational Processing of Portuguese,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Hybrid PoS-tagging: A Cooperation of Evolutionary and Statistical Approaches", "author": ["R. Forsati", "M. Shamsfard"], "venue": "Applied Mathematical Modelling, 38(13): 3193\u2013\u20133211", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "T", "author": ["M. Fruzangohar"], "venue": "a. Kroeger, and D. L. Adelson. Improved Part-of-Speech Prediction in Suffix Analysis. PloS one, 8(10):e76042", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-lingual Adaptation as a Baseline : AdaptingMaximum Entropy Models to Bulgarian", "author": ["G. Georgiev", "P. Nakov", "P. Osenova", "K. Simov"], "venue": "Proceedings of the Workshop Adaptation of Language Resources and Technology to New Domains 2009, pages 35\u201338", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Feature-Rich Part-of-speech Tagging for Morphologically Complex Languages: Application to Bulgarian", "author": ["G. Georgiev", "V. Zhikov", "P. Osenova", "K. Simov", "P. Nakov"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 492\u2013502", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Is Part-of-Speech Tagging a Solved Task ? An Evaluation of POS Taggers for the German Web as Corpus", "author": ["E. Giesbrecht", "S. Evert"], "venue": "Proceedings of the Fifth Web as Corpus Workshop, pages 27\u201336", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "SVMTool: A General POS Tagger Generator Based on Support Vector Machines", "author": ["J. Gim\u00e9nez", "L. M\u00e0rquez", "L. Marquez"], "venue": "Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 43\u201346", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Part-of-Speech Tagging", "author": ["T. G\u00fcng\u00f6r"], "venue": "Handbook of Natural Language Processing, second edition, pages 205\u2013235. Chapman & Hall/CRC", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based POS Taggers", "author": ["M. Hepple"], "venue": "Proceedings of 38th Annual Meeting of the Association for Computational Linguistics, pages 277\u2013278", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast or Accurate? \u2013 A Comparative Evaluation of PoS TaggingModels", "author": ["T. Horsmann", "N. Erbs", "T. Zesch"], "venue": "Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology, pages 22\u201330", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "HMM based POS tagger for Hindi", "author": ["N. Joshi", "H. Darbari", "I. Mathur"], "venue": "Proceedings of 2nd International Conference on Artificial Intelligence and Soft Computing, pages 341\u2013349", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparing Two Markov Methods for Part-of-speech Tagging of Portuguese", "author": ["F.N. Kepler", "M. Finger"], "venue": "Proceedings of the 2nd International Joint Conference of IBERAMIA 2006 and SBIA 2006, pages 482\u2013491", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Part-of-speech Taggers for Low-resource Languages using CCA Features", "author": ["Y.-B. Kim", "B. Snyder", "R. Sarikaya"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1292\u20131302", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "A Conditional Random Field Framework for Thai Morphological Analysis", "author": ["C. Kruengkrai", "V. Sornlertlamvanich", "H. Isahara"], "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages 2419\u20132424", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Non-lexical Neural Architecture for Fine-grained POS Tagging", "author": ["M. Labeau", "K. L\u00f6ser", "A. Allauzen"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 232\u2013 237", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "Proceedings of the  A Robust Transformation-Based Learning Approach Using Ripple Down Rules for Part-of-Speech Tagging  13 18th International Conference on Machine Learning, pages 282\u2013289", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "Practical Very Large Scale CRFs", "author": ["T. Lavergne", "O. Capp\u00e9", "F. Yvon"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504\u2013513", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Syllable-pattern-based Unknown-morpheme Segmentation and Estimation for Hybrid Part-of-speech Tagging of Korean", "author": ["G.G. Lee", "J.-H. Lee", "J. Cha"], "venue": "Computational Linguistics, 28(1):53\u201370", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "Coupled Sequence Labeling on Heterogeneous Annotations: POS Tagging as a Case Study", "author": ["Z. Li", "J. Chao", "M. Zhang", "W. Chen"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1783\u20131792", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Tagging The Web: Building A Robust Web Tagger with Neural Network", "author": ["J. Ma", "Y. Zhang", "J. Zhu"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 144\u2013154", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid Neuro and Rule-Based Part of Speech Taggers", "author": ["Q. Ma", "M. Murata", "K. Uchimoto", "H. Isahara"], "venue": "Proceedings of the 18th conference on Computational linguistics - Volume 1, pages 509\u2013515", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational Linguistics, 19(2):313\u2013 330", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1993}, {"title": "The IULA Treebank", "author": ["M. Marimon", "B. Fisas", "N. Bel", "M. Villegas", "J. Vivaldi", "S. Torner", "M. Lorente", "S. V\u00e1zquez"], "venue": "Proceedings of the eighth international conference on Language Resources and Evaluation, pages 1920\u20131926", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast High-Accuracy Part-of-Speech Tagging by Independent Classifiers", "author": ["R. Moore"], "venue": "Proceedings the 25th International Conference on Computational Linguistics: Technical Papers, pages 1165\u20131176", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient Higher-Order CRFs for Morphological Tagging", "author": ["T. Mueller", "H. Schmid", "H. Sch\u00fctze"], "venue": "Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing, pages 322\u2013 332", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint Lemmatization and Morphological Tagging with Lemming", "author": ["T. M\u00fcller", "R. Cotterell", "A. Fraser", "H. Sch\u00fctze"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2268\u20132274", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparison of Three Machine Learning Methods for Thai Part-of- Speech Tagging", "author": ["M. Murata", "Q. Ma", "H. Isahara"], "venue": "ACM Transactions on Asian Language Information Processing, 1(2):145\u2013158", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2002}, {"title": "Transformation-Based Learning in the Fast Lane", "author": ["G. Ngai", "R. Florian"], "venue": "Proceedings of the 2nd Meeting  of the North American Chapter of the Association for Computational Linguistics, pages 1\u20138", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2001}, {"title": "Ripple Down Rules for Part-of-Speech Tagging", "author": ["D.Q. Nguyen", "D.Q. Nguyen", "S.B. Pham", "D.D. Pham"], "venue": "Proceedings of the 12th International Conference on Intelligent Text Processing and Computational Linguistics - Volume Part I, pages 190\u2013201", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "RDRPOSTagger: A Ripple Down Rules-based Part-Of-Speech Tagger", "author": ["D.Q. Nguyen", "D.Q. Nguyen", "D.D. Pham", "S.B. Pham"], "venue": "Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17\u201320", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Ripple Down Rules for Question Answering", "author": ["D.Q. Nguyen", "D.Q. Nguyen", "S.B. Pham"], "venue": "Semantic Web journal, to appear", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Building a Large Syntactically-Annotated Corpus of Vietnamese", "author": ["P.T. Nguyen", "X.L. Vu", "T.M.H. Nguyen", "V.H. Nguyen", "H.P. Le"], "venue": "Proceedings of the Third Linguistic Annotation Workshop, pages 182\u2013185", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Large Scale Syntactic Annotation of Written Dutch: Lassy", "author": ["G. Noord", "G. Bouma", "F. Eynde", "D. Kok", "J. Linde", "I. Schuurman", "E. Sang", "V. Vandeghinste"], "venue": "In Essential Speech and Language Technology for Dutch, Theory and Applications of Natural Language Processing,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "CRFsuite: A Fast Implementation of Conditional Random Fields (CRFs)", "author": ["N. Okazaki"], "venue": "URL http:// www.chokkan.org/software/crfsuite/", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Stagger: an Open-Source Part of Speech Tagger for Swedish", "author": ["R. \u00d6stling"], "venue": "Northern European Journal of Language Technology, 3:1\u201318", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "A Comparative Study on Different Techniques for Thai Part-of-Speech Tagging", "author": ["J. Pailai", "R. Kongkachandra", "T. Supnithi", "P. Boonkwan"], "venue": "Proceedings of 10th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, pages 1\u20135", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Hindi Syntax: Annotating Dependency", "author": ["M. Palmer", "R. Bhatt", "B. Narasimhan", "O. Rambow", "D.M. Sharma", "F. Xia"], "venue": "Lexical Predicate-Argument Structure, and Phrase Structure. In Proceedings of 7th International Conference on Natural Language Processing, pages 261\u2013268", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "A Maximum Entropy Model for Part- Of-Speech Tagging", "author": ["A. Ratnaparkhi"], "venue": "Proceedings of the fourth Workshop on Very Large Corpora, pages 133\u2013142", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1996}, {"title": "Two Decades of Ripple Down Rules Research", "author": ["D. Richards"], "venue": "Knowledge Engineering Review, 24(2):159\u2013 184", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "E", "author": ["B. Sagot", "L. Cl\u00e9ment"], "venue": "V. d. L. Clergerie, and P. Boullier. The Lefff 2 Syntactic Lexicon for French: Architecture, Acquisition, Use. In Proceedings of the 5th Language Resource and Evaluation Conference, pages 1348\u20131351", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Character-level Representations for Part-of-Speech Tagging", "author": ["C.D. Santos", "B. Zadrozny"], "venue": "Proceedings of the 31st International Conference on Machine Learning, pages 1818\u20131826", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Part-of-Speech Tagging with Neural Networks", "author": ["H. Schmid"], "venue": "Proceedings of the 15th International Conference on Computational Linguistics, Volume 1, pages 172\u2013176", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1994}, {"title": "Probabilistic Part-of-Speech Tagging Using Decision Trees", "author": ["H. Schmid"], "venue": "Proceedings of International Conference on NewMethods in Language Processing, pages 44\u201349", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1994}, {"title": "Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-grained POS Tagging", "author": ["H. Schmid", "F. Laws"], "venue": "Proceedings of 22nd International Conference on Computational Linguistics, pages 777\u2013784", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2008}, {"title": "FLORS: Fast and Simple Domain Adaptation for Part-of-Speech Tagging", "author": ["T. Schnabel", "H. Sch\u00fctze"], "venue": "Transactions of the Association for Computational Linguistics, 2:15\u201326", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "A Case Study in Part-of-Speech Tagging Using the ICOPOST Toolkit", "author": ["I. Schroder"], "venue": "Technical report, Department of Computer Science, University of Hamburg", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2002}, {"title": "J", "author": ["D. Seddah", "G. Chrupa\u0142a", "O. Cetinoglu"], "venue": "van Genabith, and M. Candito. Lemmatization and Lexicalized Statistical Parsing of Morphologically Rich Languages: the Case of French. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 85\u201393", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2010}, {"title": "Guided Learning for Bidirectional Sequence Classification", "author": ["L. Shen", "G. Satta", "A. Joshi"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760\u2013767", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2007}, {"title": "Design and Implementation of the Bulgarian HPSGbased Treebank", "author": ["K. Simov", "P. Osenova", "A. Simov", "M. Kouylekov"], "venue": "Research on Language and Computation, 2:495\u2013522", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2004}, {"title": "Morphological Richness Offsets Resource Demand- Experiences in Constructing a POS Tagger for Hindi", "author": ["S. Singh", "K. Gupta", "M. Shrivastava", "P. Bhattacharyya"], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions, pages 779\u2013786", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2006}, {"title": "A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors", "author": ["H.-J. Song", "J.-W. Son", "T.-G. Noh", "S.-B. Park", "S.- J. Lee"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2012}, {"title": "and H", "author": ["V. Sornlertlamvanich", "T. Charoenporn"], "venue": "Isahara. ORCHID: Thai Part-Of-Speech Tagged Corpus", "citeRegEx": "70", "shortCiteRegEx": null, "year": 1997}, {"title": "Semi-supervised Training for the Averaged Perceptron POS Tagger", "author": ["D. j. Spoustov\u00e1", "J. Haji\u010d", "J. Raab", "M. Spousta"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2009}, {"title": "A Second-Order Hidden MarkovModel for Part-of-Speech Tagging", "author": ["S.M. Thede", "M.P. Harper"], "venue": "Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 175\u2013182", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1999}, {"title": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of- Speech Tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "Proceedings of the 2000 Joint SIG- DAT conference on Empirical methods in natural language processing and very large corpora, pages 63\u201370", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2000}, {"title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics- Volume 1, pages 173\u2013180", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2003}, {"title": "An Experimental Study on Vietnamese POS Tagging", "author": ["O.T. Tran", "C.A. Le", "T.Q. Ha", "Q.H. Le"], "venue": "Proceedings of 2009 International Conference on Asian Language Processing, pages 23\u201327", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2009}, {"title": "Bidirectional Inference with the Easiest-first Strategy for Tagging Sequence Data", "author": ["Y. Tsuruoka", "J. Tsujii"], "venue": "Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language, pages 467\u2013474", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2005}, {"title": "An Efficient Memory-based Morphosyntactic Tagger and Parser for Dutch", "author": ["A. van den Bosch", "B. Busser", "S. Canisius", "andW. Daelemans"], "venue": "In Proceedings of the 17th Meeting of Computational Linguistics in the Netherlands,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2007}, {"title": "RDRCE: Combining Machine Learning and Knowledge Acquisition", "author": ["H. Xu", "A. Hoffmann"], "venue": "Proceedings of the 11th International Conference on Knowledge Management and Acquisition for Smart Systems and Services, pages 165\u2013179", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep Learning for Chinese Word Segmentation and POS Tagging", "author": ["X. Zheng", "H. Chen", "T. Xu"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 647\u2013657", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 24, "context": "POS tagging is one of the most important tasks in Natural Language Processing (NLP) that assigns a tag to each word in a text, which the tag represents the word\u2019s lexical category [26].", "startOffset": 180, "endOffset": 184}, {"referenceID": 41, "context": "[43], the taggers SVMTool [25] and CRFSuite [52] took 2,454 minutes (about 41 hours) and 9,274 minutes (about 155 hours) respectively to train on a corpus of 38,727 Czech sentences (652,544 words), using a machine with two Hexa-Core Intel Xeon X5680 CPUs with 3,33 GHz and 144 GB of memory.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[43], the taggers SVMTool [25] and CRFSuite [52] took 2,454 minutes (about 41 hours) and 9,274 minutes (about 155 hours) respectively to train on a corpus of 38,727 Czech sentences (652,544 words), using a machine with two Hexa-Core Intel Xeon X5680 CPUs with 3,33 GHz and 144 GB of memory.", "startOffset": 26, "endOffset": 30}, {"referenceID": 50, "context": "[43], the taggers SVMTool [25] and CRFSuite [52] took 2,454 minutes (about 41 hours) and 9,274 minutes (about 155 hours) respectively to train on a corpus of 38,727 Czech sentences (652,544 words), using a machine with two Hexa-Core Intel Xeon X5680 CPUs with 3,33 GHz and 144 GB of memory.", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "For example, as reported by Moore [42], the SVMTool, the COMPOST tagger [71] and the UPenn bidirectional tagger [66] respectively achieved the tagging speed of 7700, 2600 and 270 English word tokens per second, using a Linux workstation with Intel Xeon X5550 2.", "startOffset": 34, "endOffset": 38}, {"referenceID": 69, "context": "For example, as reported by Moore [42], the SVMTool, the COMPOST tagger [71] and the UPenn bidirectional tagger [66] respectively achieved the tagging speed of 7700, 2600 and 270 English word tokens per second, using a Linux workstation with Intel Xeon X5550 2.", "startOffset": 72, "endOffset": 76}, {"referenceID": 64, "context": "For example, as reported by Moore [42], the SVMTool, the COMPOST tagger [71] and the UPenn bidirectional tagger [66] respectively achieved the tagging speed of 7700, 2600 and 270 English word tokens per second, using a Linux workstation with Intel Xeon X5550 2.", "startOffset": 112, "endOffset": 116}, {"referenceID": 9, "context": "Turning to the rule-based POS tagging methods, the most well-known method proposed by Brill [10] automatically learns transformation-based error-driven", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "Consequently, the Brill\u2019s method is slow in terms of training and tagging processes [27, 46].", "startOffset": 84, "endOffset": 92}, {"referenceID": 44, "context": "Consequently, the Brill\u2019s method is slow in terms of training and tagging processes [27, 46].", "startOffset": 84, "endOffset": 92}, {"referenceID": 14, "context": "In this paper, we present a new error-driven approach to automatically restructure transformation rules in the form of a Single Classification Ripple Down Rules (SCRDR) tree [15, 57].", "startOffset": 174, "endOffset": 182}, {"referenceID": 55, "context": "In this paper, we present a new error-driven approach to automatically restructure transformation rules in the form of a Single Classification Ripple Down Rules (SCRDR) tree [15, 57].", "startOffset": 174, "endOffset": 182}, {"referenceID": 14, "context": "A SCRDR tree [15, 49, 57] is a binary tree with two distinct types of edges.", "startOffset": 13, "endOffset": 25}, {"referenceID": 47, "context": "A SCRDR tree [15, 49, 57] is a binary tree with two distinct types of edges.", "startOffset": 13, "endOffset": 25}, {"referenceID": 55, "context": "A SCRDR tree [15, 49, 57] is a binary tree with two distinct types of edges.", "startOffset": 13, "endOffset": 25}, {"referenceID": 8, "context": "We also compare our approach with the TnT6 approach [9] and the MarMoT7 approach proposed by Mueller et al.", "startOffset": 52, "endOffset": 55}, {"referenceID": 41, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The TnT tagger is considered as one of the fastest POS taggers in literature (both in terms of training and tagging), obtaining competitive tagging accuracy on diverse languages [26].", "startOffset": 178, "endOffset": 182}, {"referenceID": 38, "context": "de/marmot/ ments on English use the Penn WSJ Treebank [40] sections 0-18 (38,219 sentences - 912,344 words) for training, sections 19-21 (5,527 sentences - 131,768 words) for validation, and the sections 22-24 (5,462 sentences - 129,654 words) for testing.", "startOffset": 54, "endOffset": 58}, {"referenceID": 65, "context": "Bulgarian BulTreeBank-Morph [67] 20,558 321,538 \u2014 564 10.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "5 [5] 115,844 1,957,246 \u2014 1,570 6.", "startOffset": 2, "endOffset": 5}, {"referenceID": 49, "context": "Dutch Lassy Small Corpus [51] 65,200 1,096,177 \u2014 933 7.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "French French Treebank [1] 21,562 587,687 17 306 5.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "German TIGER Corpus [8] 50,474 888,236 54 795 7.", "startOffset": 20, "endOffset": 23}, {"referenceID": 53, "context": "Hindi Hindi Treebank [55] 26,547 588,995 39 \u2014 \u2014", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "Italian ISDT Treebank [7] 10,206 190,310 70 \u2014 11.", "startOffset": 22, "endOffset": 25}, {"referenceID": 39, "context": "Spanish IULA LSP Treebank [41] 42,099 589,542 \u2014 241 4.", "startOffset": 26, "endOffset": 30}, {"referenceID": 68, "context": "Thai ORCHID Corpus [70] 23,225 344,038 47 \u2014 5.", "startOffset": 19, "endOffset": 23}, {"referenceID": 48, "context": "Vietnamese (VTB) Vietnamese Treebank [50] 10,293 220,574 22 \u2014 3.", "startOffset": 37, "endOffset": 41}, {"referenceID": 45, "context": "As shown in [47], using the same evaluation scheme for English, the Brill\u2019s rule-based tagger V1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "14 [10] gained a similar accuracy result at 96.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "This is better than the results reported on the BulTreeBank webpage10 on POS+MORPH tagging task, where TnT, SVMTool [25] and the memory-based tagger in the Acopost package11 [64] obtained accuracies of 92.", "startOffset": 116, "endOffset": 120}, {"referenceID": 62, "context": "This is better than the results reported on the BulTreeBank webpage10 on POS+MORPH tagging task, where TnT, SVMTool [25] and the memory-based tagger in the Acopost package11 [64] obtained accuracies of 92.", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "[22], obtained with the Maximum Entropy-base POS tagger from the OpenNLP toolkit.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23]13 reached the state-ofthe-art accuracy result of 97.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[43] presented the results of five POS taggers SVMTool, CRFSuite [52], RFTagger [62], Morfette [12] and MarMoT for Czech POS+MORPH tagging.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[43] presented the results of five POS taggers SVMTool, CRFSuite [52], RFTagger [62], Morfette [12] and MarMoT for Czech POS+MORPH tagging.", "startOffset": 65, "endOffset": 69}, {"referenceID": 60, "context": "[43] presented the results of five POS taggers SVMTool, CRFSuite [52], RFTagger [62], Morfette [12] and MarMoT for Czech POS+MORPH tagging.", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "[43] presented the results of five POS taggers SVMTool, CRFSuite [52], RFTagger [62], Morfette [12] and MarMoT for Czech POS+MORPH tagging.", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "[23] split the BulTreeBank corpus into training", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "5 [5] containing about 116K sentences.", "startOffset": 2, "endOffset": 5}, {"referenceID": 75, "context": "Dutch The TADPOLE tagger [78] was reached an accuracy of 96.", "startOffset": 25, "endOffset": 29}, {"referenceID": 49, "context": "Instead, we use the Lassy Small Corpus [51] containing about 1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 63, "context": "75% [65, 17], using the French Treebank [1] with 9,881 sentences for training and 1,235 sentences for test.", "startOffset": 4, "endOffset": 12}, {"referenceID": 16, "context": "75% [65, 17], using the French Treebank [1] with 9,881 sentences for training and 1,235 sentences for test.", "startOffset": 4, "endOffset": 12}, {"referenceID": 0, "context": "75% [65, 17], using the French Treebank [1] with 9,881 sentences for training and 1,235 sentences for test.", "startOffset": 40, "endOffset": 43}, {"referenceID": 56, "context": "However, these methods employed Lefff [58] which is an external large-scale morphological lexicon.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "Without using the lexicon, Denis and Sagot [17] reported an accuracy performance at 97.", "startOffset": 43, "endOffset": 47}, {"referenceID": 7, "context": "German Using the 10-fold cross validation evaluation scheme on the TIGER corpus [8] of 50,474 German sentences, Giesbrecht and Evert [24] presented the results of TreeTagger [61], TnT, SVMTool, Stanford tagger [75] and Apache UIMA Tagger14 obtaining the POS tagging accuracies at 96.", "startOffset": 80, "endOffset": 83}, {"referenceID": 22, "context": "German Using the 10-fold cross validation evaluation scheme on the TIGER corpus [8] of 50,474 German sentences, Giesbrecht and Evert [24] presented the results of TreeTagger [61], TnT, SVMTool, Stanford tagger [75] and Apache UIMA Tagger14 obtaining the POS tagging accuracies at 96.", "startOffset": 133, "endOffset": 137}, {"referenceID": 59, "context": "German Using the 10-fold cross validation evaluation scheme on the TIGER corpus [8] of 50,474 German sentences, Giesbrecht and Evert [24] presented the results of TreeTagger [61], TnT, SVMTool, Stanford tagger [75] and Apache UIMA Tagger14 obtaining the POS tagging accuracies at 96.", "startOffset": 174, "endOffset": 178}, {"referenceID": 72, "context": "German Using the 10-fold cross validation evaluation scheme on the TIGER corpus [8] of 50,474 German sentences, Giesbrecht and Evert [24] presented the results of TreeTagger [61], TnT, SVMTool, Stanford tagger [75] and Apache UIMA Tagger14 obtaining the POS tagging accuracies at 96.", "startOffset": 210, "endOffset": 214}, {"referenceID": 41, "context": "[43] also performed experiments on the TIGER corpus, using 40,474 sentences for training and 5,000 sentences for test.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Hindi On the Hindi Treebank [55], RDRPOSTagger+TnT reaches a competitive accuracy result of 96.", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "[29] achieved an accuracy of 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] compared machine learning-based approaches and presented the POS tagging accuracy at 93.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In the 2007 Shallow Parsing Contest for South Asian Languages [6], the POS tagging track provided a small training set of 21,470 words and a test set of 4,924 words.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "66% obtained by Avinesh and Karthik [4].", "startOffset": 36, "endOffset": 39}, {"referenceID": 66, "context": "[68] obtained an accuracy of 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] achieved a result at 94.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Italian In the EVALITA 2009 workshop on Evaluation of NLP and Speech Tools for Italian15, the POS tagging track [3] provided a training set of 3,719 sentences (108,874 word forms) with 37 POS tags.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "Our experiment on Italian POS tagging employs the ISDT Treebank [7] of 10,206 sentences (190,310 word", "startOffset": 64, "endOffset": 67}, {"referenceID": 17, "context": "Portuguese The previous works [18, 30] on POS+MORPH tagging for Portuguese used an early version of the Tycho Brahe corpus [21] containing about 1,036K words.", "startOffset": 30, "endOffset": 38}, {"referenceID": 28, "context": "Portuguese The previous works [18, 30] on POS+MORPH tagging for Portuguese used an early version of the Tycho Brahe corpus [21] containing about 1,036K words.", "startOffset": 30, "endOffset": 38}, {"referenceID": 28, "context": "Based on this setting, Kepler and Finger [30] achieved an accuracy of 95.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "[18] reached a state-of-the-art accuracy result at 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[43] evaluated the five taggers of SVMTool, CRFSuite, RFTagger, Morfette and MarMoT for Spanish POS+MORPH tagging, using a training set of 14,329 sentences (427,442 tokens) and a test set of 1,725 sentences (50,630 tokens) with 303 POS+MORPH tags.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[43]\u2019s experiment, we use the IULA Spanish LSP Treebank [41] of 42K sentences with 241 tags.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[43]\u2019s experiment, we use the IULA Spanish LSP Treebank [41] of 42K sentences with 241 tags.", "startOffset": 56, "endOffset": 60}, {"referenceID": 51, "context": "0 [72] consisting of 500 text files with about 74K sentences that we also use, \u00d6stling [53] evaluated the Swedish POS tagger Stagger using 10-fold cross validation but the folds were split at the file level and not on sentence level as we do.", "startOffset": 87, "endOffset": 91}, {"referenceID": 68, "context": "Thai On the Thai POS Tagged corpus ORCHID [70] of 23,225 sentences, RDRPOSTagger+TnT achieves an accuracy of 94.", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "For example, the previous works [39, 45] performed their experiments on an unavailable corpus of 10,452 sentences.", "startOffset": 32, "endOffset": 40}, {"referenceID": 43, "context": "For example, the previous works [39, 45] performed their experiments on an unavailable corpus of 10,452 sentences.", "startOffset": 32, "endOffset": 40}, {"referenceID": 30, "context": "[32], however, the obtained accuracy of 79.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[54] reached an accuracy of 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "In this paper, we also carry out POS tagging experiments using 5-fold cross validation evaluation scheme on the VLSP set of 28,232 sentences and the standard benchmark Vietnamese Treebank [50] of about 10K sentences.", "startOffset": 188, "endOffset": 192}, {"referenceID": 73, "context": "59% outperforms the previously reported Maximum Entropy Model, Conditional Random Fields and Support Vector Machinebased approaches [76] where the highest obtained accuracy was 91.", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "For example, on a 40,474-sentence subset of the German TIGER corpus [8], SVMTool took about 899 minutes (about 15 hours) to train using 54 POS tags as compared to about 1,649 minutes (about 27 hours) using 681 POS+MORPH tags [43].", "startOffset": 68, "endOffset": 71}, {"referenceID": 41, "context": "For example, on a 40,474-sentence subset of the German TIGER corpus [8], SVMTool took about 899 minutes (about 15 hours) to train using 54 POS tags as compared to about 1,649 minutes (about 27 hours) using 681 POS+MORPH tags [43].", "startOffset": 225, "endOffset": 229}, {"referenceID": 41, "context": "[43] for POS+MORPH tagging on a machine of two Hexa-Core Intel Xeon X5680 CPUs with 3,33 GHz and 144 GB of memory.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "Regarding to tagging speed, as reported by Moore [42] using the same evaluation scheme on English on a Linux workstation equipped with Intel Xeon X5550 2.", "startOffset": 49, "endOffset": 53}, {"referenceID": 64, "context": "67 GHz: the SVMTool, the UPenn bidirectional tagger [66], the COMPOST tagger [71], Moore [42]\u2019s approach, the accurate version of the Stanford tagger [75] and the fast and less accurate version of the Stanford tagger gained tagging speed of 7700, 270, 2600, 51K, 5900 and 80K tokens per second, respectively.", "startOffset": 52, "endOffset": 56}, {"referenceID": 69, "context": "67 GHz: the SVMTool, the UPenn bidirectional tagger [66], the COMPOST tagger [71], Moore [42]\u2019s approach, the accurate version of the Stanford tagger [75] and the fast and less accurate version of the Stanford tagger gained tagging speed of 7700, 270, 2600, 51K, 5900 and 80K tokens per second, respectively.", "startOffset": 77, "endOffset": 81}, {"referenceID": 40, "context": "67 GHz: the SVMTool, the UPenn bidirectional tagger [66], the COMPOST tagger [71], Moore [42]\u2019s approach, the accurate version of the Stanford tagger [75] and the fast and less accurate version of the Stanford tagger gained tagging speed of 7700, 270, 2600, 51K, 5900 and 80K tokens per second, respectively.", "startOffset": 89, "endOffset": 93}, {"referenceID": 72, "context": "67 GHz: the SVMTool, the UPenn bidirectional tagger [66], the COMPOST tagger [71], Moore [42]\u2019s approach, the accurate version of the Stanford tagger [75] and the fast and less accurate version of the Stanford tagger gained tagging speed of 7700, 270, 2600, 51K, 5900 and 80K tokens per second, respectively.", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "From early POS tagging approaches the rule-based Brill\u2019s tagger [10] is the most well-known.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "To prevent that, Hepple [27] presented an approach with two assumptions for disabling interactions between rules to reduce the training time while sacrificing a small amount of accuracy.", "startOffset": 24, "endOffset": 28}, {"referenceID": 44, "context": "Ngai and Florian [46] proposed another method to reduce the training time by recalculating the scores of rules while obtaining similar accuracy result.", "startOffset": 17, "endOffset": 21}, {"referenceID": 45, "context": "In the conference publication version of our approach [47], we reported an improvement up to 33 times in training speed against the Brill\u2019s method.", "startOffset": 54, "endOffset": 58}, {"referenceID": 76, "context": "In addition to our research, there is only one work that applies Ripple Down Rules method for POS tagging proposed by Xu and Hoffmann [79].", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 38, "endOffset": 49}, {"referenceID": 19, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 38, "endOffset": 49}, {"referenceID": 70, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 38, "endOffset": 49}, {"referenceID": 11, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 81, "endOffset": 101}, {"referenceID": 54, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 81, "endOffset": 101}, {"referenceID": 71, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 81, "endOffset": 101}, {"referenceID": 72, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 81, "endOffset": 101}, {"referenceID": 74, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 81, "endOffset": 101}, {"referenceID": 12, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 141, "endOffset": 153}, {"referenceID": 64, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 141, "endOffset": 153}, {"referenceID": 69, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 141, "endOffset": 153}, {"referenceID": 10, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 187, "endOffset": 215}, {"referenceID": 13, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 187, "endOffset": 215}, {"referenceID": 31, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 187, "endOffset": 215}, {"referenceID": 36, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 187, "endOffset": 215}, {"referenceID": 57, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 187, "endOffset": 215}, {"referenceID": 58, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 187, "endOffset": 215}, {"referenceID": 77, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 187, "endOffset": 215}, {"referenceID": 32, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 243, "endOffset": 263}, {"referenceID": 33, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 243, "endOffset": 263}, {"referenceID": 35, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 243, "endOffset": 263}, {"referenceID": 41, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 243, "endOffset": 263}, {"referenceID": 42, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 243, "endOffset": 263}, {"referenceID": 23, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 289, "endOffset": 305}, {"referenceID": 29, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 289, "endOffset": 305}, {"referenceID": 61, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 289, "endOffset": 305}, {"referenceID": 67, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 289, "endOffset": 305}, {"referenceID": 59, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 352, "endOffset": 360}, {"referenceID": 60, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 352, "endOffset": 360}, {"referenceID": 18, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 380, "endOffset": 388}, {"referenceID": 34, "context": "ous Hidden Markov model-based methods [9, 20, 73], maximum entropy-based methods [12, 56, 74, 75, 77], perceptron algorithm-based approaches [13, 66, 71], neural network-based approaches [11, 14, 33, 38, 59, 60, 80], Conditional Random Fields [34, 35, 37, 43, 44], Support Vector Machines [25, 31, 63, 69] and other approaches including decision trees [61, 62] and hybrid methods [19, 36].", "startOffset": 380, "endOffset": 388}, {"referenceID": 24, "context": "Overview about the POS tagging task can be found in [26, 28].", "startOffset": 52, "endOffset": 60}, {"referenceID": 26, "context": "Overview about the POS tagging task can be found in [26, 28].", "startOffset": 52, "endOffset": 60}, {"referenceID": 45, "context": "This paper extends the work published in our conference publications [47, 48].", "startOffset": 69, "endOffset": 77}, {"referenceID": 46, "context": "This paper extends the work published in our conference publications [47, 48].", "startOffset": 69, "endOffset": 77}], "year": 2015, "abstractText": "In this paper, we propose a new approach to construct a system of transformation rules for the Part-of-Speech (POS) tagging task. Our approach is based on an incremental knowledge acquisition method where rules are stored in an exception structure and new rules are only added to correct the errors of existing rules; thus allowing systematic control of the interaction between the rules. Experimental results on 13 languages show that our approach is fast in terms of training time and tagging speed. Furthermore, our approach obtains very competitive accuracy in comparison to state-ofthe-art POS and morphological taggers.", "creator": "LaTeX with hyperref package"}}}