{"id": "1412.4863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Max-Margin based Discriminative Feature Learning", "abstract": "In this paper, we propose a new max-margin based discriminative feature learning method. Specifically, we aim at learning a low-dimensional feature representation, so as to maximize the global margin of the data and make the samples from the same class as close as possible. In order to enhance the robustness to noise, a $l_{2,1}$ norm constraint is introduced to make the transformation matrix in group sparsity or more uniform between groups. We define this algorithm using a simple subset of our LDP model.", "histories": [["v1", "Tue, 16 Dec 2014 02:55:01 GMT  (412kb,D)", "https://arxiv.org/abs/1412.4863v1", null], ["v2", "Mon, 3 Apr 2017 02:43:47 GMT  (581kb,D)", "http://arxiv.org/abs/1412.4863v2", "Accepted by IEEE TNNLS"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "qingshan liu", "weishan dong", "xin zhang", "lin yang"], "accepted": false, "id": "1412.4863"}, "pdf": {"name": "1412.4863.pdf", "metadata": {"source": "CRF", "title": "Max-Margin based Discriminative Feature Learning", "authors": ["Changsheng Li", "Qingshan Liu", "Weishan Dong", "Fan Wei", "Xin Zhang", "Lin Yang"], "emails": ["ylyang@cn.ibm.com).", "fanwei@stanford.edu.", "qsliu@nuist.edu.cn)."], "sections": [{"heading": null, "text": "Index Terms\u2014Feature learning, max-margin, correlation relationship, row sparsity\nI. INTRODUCTION\nData classification plays a key role in many practical applications [1], [2]. However, the real-world data, such as image data, often lie in a high-dimensional space, which has high computational cost and might bring down the prediction accuracy of classification models. To cope with this issue, a popular way is to do dimensionality reduction, which is to project the data into a low-dimensional subspace with the least information loss [3]. Generally speaking, dimensionality reduction can be achieved by either feature selection or feature transformation. Feature selection aims at selecting the most informative feature subset from the original feature set according to a predefined selection criterion. A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on. The philosophy behind feature transformation is that a combination of the original features may be more helpful for learning. Hence it aims to map the high-dimensional data into a new meaningful low-dimensional space. Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13]. According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19]. In this paper, we focus on the linear feature transformation methods due to its simplicity and effectiveness [20].\nManuscript received May 21, 2015; revised August 04, 2015; accepted January 10, 2016. This work was supported in part by the National Natural Science Foundation of China under Grant 61532009, 61272223, and in part by the Foundation of Jiangsu Province of China under Grant 15KJA520001.\nC. Li, W. Dong, X. Zhang, and L. Yang are with IBM Research-China, Beijing 100094, China. (email: lcsheng, dongweis, zxin, ylyang@cn.ibm.com).\nF. Wei is with Department of Mathematics, Stanford University. E-mail: fanwei@stanford.edu.\nQ. Liu is with the B-DAT Laboratory at the school of Information and Control, Nanjing University of Information Science and Technology, Nanjing 210014, China. (email: qsliu@nuist.edu.cn).\nIn literature, principal component analysis (PCA) [15] and linear discriminant analysis (LDA) [21] are two classical linear algorithms, both capturing global Euclidean structure of the data. Since the practical data often lie on or are close to an intrinsically low-dimensional manifold, volumes of approaches focus on how to preserve such structures in recent years. The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17]. Meanwhile, some work integrates global and local information during feature transformation, such as LPMIP [23], LapLDA [24]. However, the methods above do not consider how to effectively connect with the classifier in the context of classification. In order to alleviate this limitation, the maximum margin projection (MMP) algorithm [25] takes advantage of a binary support vector machine (SVM) [26] classifier to obtain some hyperplanes that separate data points in different clusters with the maximum margin. The random projection algorithms [27], [28] aim to find some Gaussian random projection matrices to preserve the pairwise distances between data points in the projected subspace, which can be effectively combined with some classifiers, such as SVM. Varshney and Willsky [29] propose a framework to simultaneously learn a linear dimensionality reduction projection matrix and a margin-based classifier defined in the reduced space. The main idea of the maximum margin projection pursuit (MMPP) algorithm is to integrate optimal data embedding and SVM classification in a single framework in both bi-class and multi-class classification [30]. In addition, local discriminant gaussian (LDG) [31] is a feature transformation method which exploits a smooth approximation of the leave-one-out cross validation error of a classifier. However, these methods do not intend to preserve the intrinsic manifold structure of the data and removing noisy features before feature transformation. In addition, there are often correlations among multiple classes in practical scenarios. Previous works do not study how to leverage such correlations with feature transformation. Gu et al. [32] propose a framework for joint subspace learning and feature selection, called FSSL, which can alleviate the effect of the noisy features for feature transformation. However, they do not focus on how to effectively combine the classifier in the scenario of classification.\nIn this paper, we propose a new Max-Margin based feature transformation method to Learn Discriminative Features for classification, called MMLDF. In the learned low-dimensional feature space, our method aims at maximizing the global classification margin; in the meantime, the distances of samples from the same class are minimized. Additionally, in many realworld applications, there are often correlations among multiple classes, and capturing such correlations are helpful for learning\nar X\niv :1\n41 2.\n48 63\nv2 [\ncs .L\nG ]\n3 A\npr 2\n01 7\n2 discriminative features and designing classifiers [33]. In light of this point, we add a regularization term to capture the correlations among multiple categories for feature learning. Extensive experiments are conducted on eight publicly available datasets, and the experimental results demonstrate the effectiveness of the proposed method against the state-of-theart methods.\nThe rest of the paper is organized as follows. Section II gives the details of the proposed method. The experimental results are reported and analyzed in Section III. Section IV concludes the paper."}, {"heading": "II. PROPOSED METHOD", "text": "Let X = {(xi, yi)}ni=1 denote a training data set, where xi \u2208 Rd is the i-th data point and yi \u2208 {1, . . . ,K} represents the corresponding class label. Our goal is to obtain a projection matrix P \u2208 Rd\u00d7r that maps the d-dimensional input vector to an r-dimensional vector (r < d) by zi = PTxi. For the transformation matrix P, let pi represent its i-th row, and Pij denote its (i, j)-th entry. As usual, we use tr(P) to denote the trace of P, and \u2016P\u20162,1 denotes l2,1 norm of P defined as:\n\u2016P\u20162,1 = d\u2211\ni=1\n\u2016pi\u20162 = d\u2211\ni=1 \u221a\u221a\u221a\u221a r\u2211 j=1 P 2ij (1)"}, {"heading": "A. Binary Classification: K = 2", "text": "Margin maximization has been demonstrated to be a good principle applied by various learning methods [34], [35]. Among these methods, support vector machine (SVM) is a popular one that maximizes the global margin of data points of different classes, but it only focuses on how to learn a max-margin based classifier. In this paper, we aim at learning a low-dimensional feature space to further improve the performance of max-margin based classification, and we present a unified framework to integrate feature learning and classification together. To do this, we propose the following objective function:\nmin J(P,w, b)= 1\n2 \u2016w\u20162+C n\u2211 i=1 l(w, b,P;xi, yi)\n+\u03b7 n\u2211 i,j=1 \u2016PTxi\u2212PTxj\u20162Aij+\u03bb\u2016P\u20162,1 (2)\nwhere C, \u03b7 and \u03bb are three trade-off parameters, in order to balance the contribution of each term to learn discriminative features. Through these parameters, our model can be flexible to different scenarios. w is the learned weight vector, as in SVM. P is the learned transformation matrix which projects the original data into low-dimensional feature space. PTxi is the new low-dimensional representation of xi. Aij denotes the edge weight of the within-class graph. Here, either Heat kernel or Simple-minded can be used for weighting the edges. For similarity, we choose Simple-minded in our experiment, which is expressed as:\n0\n60\n120\n180\n240\n300 0\n2\n4\n6\n8\n10\n(a) Without l2,1 norm constraint\n0\n60\n120\n180\n240\n300 0\n2\n4\n6\n8\n10\n(b) With l2,1 norm constraint\nl(\u2022) is a hinge loss function. The standard hinge loss function in SVM is not differentiable everywhere. In order to take advantage of the gradient-based optimization method to solve the proposed objective function, we adopt a similar but differentiable quadratic hinge loss:\nl(w, b,P;xi, yi) = [min(0, yi \u00b7(wT \u00b7(PTxi)+b)\u22121)]2 (4)\nMinimizing the first two terms of (2) means finding a low-dimensional subspace, in which the margin of different classes can be maximized, and the classification loss can be minimized. Minimizing the third term makes the scatter of the data in the same class as small as possible in the subspace. The last term is a regularization term to make the transformation matrix P sparse in rows, so that the learned subspace is robust to noise, and the model complexity is lowered too.\nWe take the LSVT Voice Rehabilitation dataset [36], a biclass classification dataset, as an example to illustrate the effectiveness of the l2,1 norm constraint on P in the objective function (2). Fig. 1 (a) and (b) show the visualizations of P without l2,1 norm and with l2,1 norm in (2), respectively. We can see that many rows in P become sparse by adding the l2,1 norm constraint, which can eliminate noisy features in the process of feature transformation, and can reduce the model complexity. In the later experiment, we will further demonstrate that our method is robust to noise by introducing l2,1 norm."}, {"heading": "B. Multi-class Classification: K > 2", "text": "In the case of multi-class classification, we can extend (2) to the following objective function:\nmin Jmc(W,b,P)\n= 1\n2 K\u2211 m=1 \u2016wm\u20162 + C n\u2211 i=1 \u2211 m6=yi l(wm,wyi , byi , bm,P;xi, yi)\n+ \u03b7 \u2211n\ni,j=1 \u2016PTxi \u2212PTxj\u20162Aij + \u03bb\u2016P\u20162,1 (5)\nwhere W = [w1, . . . ,wK] is the set of the learned weight vectors. l(wm,wyi , byi , bm,P;xi, yi) measures the loss when\n3 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nFig. 2. The visualization of the learned correlation coefficient matrix on the Urban Land Cover dataset. Both x-axis and y-axis represent weight vectors. Dark red denotes that the correlation coefficients are close to 1.\nthe sample xi is wrongly classified into the m-th (m 6= yi) class. Similarly, the loss function l is revised as:\nl(wm,wyi , byi , bm,P;xi, yi)\n= [min(0, (wTyiP Txi + byi \u2212wTmPTxi \u2212 bm \u2212 2)]2 (6)\nIn real-world applications, one classification task is often correlated with other classification tasks, and mining the correlations among multiple categories can be good for feature learning [33]. Our experiments also demonstrate this point (See the details in III-C). Thus, we add a regularization term into (5) to capture the correlations among multiple categories, and the new objective function becomes:\nmin Jmc(W,b,P,\u0393)\n= 1\n2 K\u2211 m=1 \u2016wm\u20162+C n\u2211 i=1 \u2211 m 6=yi l(wm,wyi , byi , bm,P;xi, yi)\n+ \u03b7 n\u2211 i,j=1 \u2016PTxi\u2212PTxj\u20162Aij+\u03bb\u2016P\u20162,1+\u03c1\u2016WT \u20162\u0393 (7)\nwhere \u03c1 is a trade-off parameter. \u2016WT \u20162\u0393 = tr(W\u0393WT ) is the Mahalanobis norm of the matrix WT . \u0393 plays the role of the inverse covariance matrix that encodes the correlations among the weight vectors wi [37]. The inverse matrix of \u0393 is constrained to be positive definite and unit trace, in order to obtain a valid solution. The effect of the last term in (7) is to penalize the complexity of W relying on the Mahalanobis norm, as well as to learn the inverse covariance matrix \u0393 simultaneously.\nWe use the Urban Land Cover dataset [38], a multi-class classification dataset, to visualize the correlation coefficient matrix of the weight vectors, which can be obtained based on the learned \u0393. The result is shown in Fig. 2. We can see that there are indeed correlations among the multiple categories (e.g. the second and the third weight vectors)."}, {"heading": "C. Optimization Procedure", "text": "We first introduce how to solve the optimization problem in the binary classification case. The objective function (2) is not convex with respect to the variables w, b, and P simultaneously. Therefore it is unrealistic to expect an algorithm to easily find the global minimum of J . Therefore, we adopt an alternating optimization strategy to find the local minimum. Under this scheme, we update P, w, and b in an alternating manner.\nUpdate P, with fixed w and b: When w and b are fixed, (2) becomes a convex problem, so P can be obtained by minimizing the following objective function:\nJ1(P) =C n\u2211 i=1 [min(0, yi \u00b7 (wT \u00b7 (PTxi)+b)\u22121)]2\n+\u03b7 n\u2211 i,j=1 \u2016PTxi\u2212PTxj\u20162Aij + \u03bb\u2016P\u20162,1\nTaking the derivative of J1 with respect to P, we can obtain:\n\u2202J1(P)\n\u2202P =2C \u2211 (xi,yi)\u2208\u0398 (xix T i Pww T\u2212(yi\u2212b)xiwT )\n+ 2\u03b7XLXTP + 2\u03bbDPP (8)\nwhere X = [x1, . . . ,xn]. L = DA\u2212A is the Laplacian matrix, where DA is a diagonal matrix with its entries being row sum of A, DA(i, i) = \u03a3jAij . DP is a diagonal matrix with the i-th diagonal element DP (i, i) = 12\u2016pi\u20162 . \u0398 denotes the set of (xi, yi) satisfying the following condition: yi(wT (PTxi) + b)\u2212 1 \u2264 01.\nSetting the derivative in (8) to zero, these is no closed-from solution of P. Therefore, we adopt a gradient-based method to derive the optimal P. Here we choose the limited-memory BFGS (L-BFGS) algorithm for its efficiency [39], [40], which is summarized in Algorithm 1. The core idea in Algorithm 1 is to estimate the inverse Hessian matrix H only using the latest m updates of the position pk and gradient gk to lower memory complexity, where m is usually small.\nAlgorithm 1 L-BFGS Input: Randomly initialize P0 \u2208 Rd\u00d7r , and reshape P0 into\na vector p0; Randomly initialize a symmetric and positive definite matrix H0; Set the integer m = 25. k \u2190 0\nOutput: P Repeat\nCompute the gradient Gk = \u2202J1(P)\u2202P |Pk , and reshape Gk into a vector gk. Compute dk \u2190 \u2212Hk \u00b7 gk using a two-loop recursion; Compute pk+1 \u2190 pk + \u03b1kdk, where \u03b1k satisfies the Wolfe conditions; sk \u2190 pk+1 \u2212 pk,yk \u2190 gk+1 \u2212 gk; m\u0302\u2190 min{k,m\u2212 1}; Update Hessian matrix Hk using the pairs {yj , sj}kj=k\u2212m\u0302; Reshape pk+1 into a matrix Pk+1; k \u2190 k + 1;\nUntil Convergence criterion satisfied; P\u2190 Pk.\nUpdate w, with fixed b and P: When b and P are fixed, (2) is convex in terms of w, so we minimize the following objective function to obtain the optimal w:\nJ2(w)= 1\n2 \u2016w\u20162+C n\u2211 i=1 [min(0, yi \u00b7 (wT \u00b7 (PTxi)+b)\u22121)]2\n1When \u0398 = \u03c6, we define \u2211\n(xi,yi)\u2208\u0398 (xix\nT i Pww T \u2212(yi\u2212b)xiwT ) = 0.\n4 Taking the derivative of J2(w) with respect to w, and setting it to zero, we obtain:\nw =  (I + 2C \u2211 (xi,yi)\u2208\u0398 PTxix T i Pw) \u22121 \u00d7(2C \u2211 (xi,yi)\u2208\u0398 (yi \u2212 b)PTxi) if (xi, yi) \u2208 \u0398\n0 otherwise (9)\nUpdate b, with fixed w and P: When w and P are fixed, (2) is convex in terms of b, so b can be acquired by optimizing the following objective:\nmin J3(b) = \u2211n\ni=1 l(b;w,PTxi, yi)\nTaking the derivative of J(b) with b, and setting it to zero, we obtain:\nb =  \u2211 (xi,yi)\u2208\u0398 (yi\u2212wTPTxi)\n|\u0398| if \u0398 6= \u03c6 b otherwise\n(10)\nwhere |\u0398| denotes the size of the set \u0398. The procedure of the proposed algorithm can be summarized in Algorithm 2. We randomly initialize the parameters, and adopt the second updating rule in [41] for deriving the local optimal solution. Experimental results verify that MMLDF converges quickly and obtains promising local minimums.\nIn the multi-classification case, we still adopt the alternating optimization strategy to find the local minimum. We take advantage of the L-BFGS to update P and wi. b still has a closed-form solution derived by using the same optimization method with the binary classification case. The rule for updating the variable \u0393 in (7) is as follows:\n\u0393 = (WTW)1/2\ntr((WTW)1/2) (11)\nDetails of the proof on (11) can be found in [37]."}, {"heading": "D. Discussion", "text": "Varshney and Willsky [29] propose a linear dimensionality reduction method, which represents the learned mappings by matrices on the Stiefel manifold and on margin-based classifiers. Nikitidis et al. [30] present the maximum margin projection pursuit (MMPP) algorithm, which learns the optimal data embedding and the SVM classifier simultaneously. However, our approach constructs the graph Laplacian through local learning to capture the intrinsic structure of the data, and incorporates l2,1-norm minimization into our framework to alleviate the effects of noisy features. In addition, our method can mine the correlations among multiple categories, which is beneficial to both feature transformation and classifier training. When \u03b7 \u2192 0, \u03bb\u2192 0, \u03c1\u2192 0 in (7), our objective function has similar effects to those of [29] and [30].\nGu et al. [32] propose a framework for joint subspace learning and feature selection, where subspace learning is reformulated as solving a linear system equation, and feature selection is achieved by utilizing l2,1-norm on the projection matrix. However, they do not explore how to effectively combine the classifier for classification. When C \u2192 0, \u03c1\u2192 0,\nand adding a constraint on the projection matrix in (7), the formulation of our method is reduced to that of [32].\nXu et al. [42] propose a semi-supervised feature selection method called FS-Manifold, where the feature selection process is embedded with a manifold regularized SVM classifier. Different from FS-Manifold, our method aims to incorporate the feature transformation process into the SVM framework.\nIn addition, MMLDF is easily extended into the kernel version. We take binary classification as an example, and handle the multi-classification case in the same way. According to [35], we know the original max-margin objective function of SVM can be transformed into its dual version as:\nmax \u03b8(\u03b1) = \u2211 i \u03b1i \u2212 1 2 \u2211 i \u2211 j \u03b1i\u03b1jyiyjx T i xj (12)\ns.t. \u2211 i \u03b1iyi = 0, \u03b1i \u2265 0\nBased on (12), we can incorporate the feature transformation process into the kernel SVM framework as:\nmax \u03b8(\u03b1,P) = \u2211 i \u03b1i \u2212 1 2 \u2211 i \u2211 j \u03b1i\u03b1jyiyjk(P Txi,P Txj)\n\u2212 \u03b7 n\u2211\ni,j=1\n\u2016PTxi\u2212PTxj\u20162Aij\u2212\u03bb\u2016P\u20162,1\n(13) s.t. \u2211 i \u03b1iyi = 0, \u03b1i \u2265 0\nwhere k(PTxi,PTxj) is the kernel function, such as the radial basis function (RBF), polynomial kernel, and so on."}, {"heading": "E. Time Complexity Analysis", "text": "The time complexity of Algorithm 2 consists of three parts: initialization on line 1, Laplacian matrix construction on line 2, and the iterative update of the three variables on lines 3-8. The complexities of the first two parts can be ignored compared to the third part. In the third part, we need to update w, b, and P, respectively. For updating w, the worst complexity is O(ndr + r3). Updating b costs O(ndr). For updating P, it needs O(t1 \u2217mdc) for the two-loop recursion scheme, where t1 denotes the total number of iterations. According to (8), the worst case of computing partial gradient w.r.t. P is O(t2 \u2217 (nd2 + n2d)), where t2 is the total number of computing the gradient. The complexity of evaluating the objective function values is O(t3 \u2217(rn2 +ndr+r2d)), where t3 denotes the total number of evaluations. Therefore, the total time complexity of MMLDF is of order O(t\u2217 (ndr+ r3) + t1 \u2217mdc+ t2 \u2217 (nd2 + n2d) + t3 \u2217 (rn2 + ndr + r2d)), where t is the total number of iterations in Algorithm 2. Since t < t3 and r < d, the complexities of the parts updating w and b can be ignored, compared to that of updating P, i.e., the time complexity of MMLDF is dominated by updating P."}, {"heading": "F. Evaluation of Convergence Rate", "text": "Although the convergence of the MMLDF algorithm cannot be proved theoretically, we find that it converges asymptotically in our experiments. Fig. 3 shows the convergence curves\n5 Algorithm 2 Max-Margin based Discriminative Feature Learning Input: Training dataset X = {xi, yi}ni=1;\nThe parameters: C, \u03bb, \u03b7, \u03c1; Reduced dimension r\nMethod 1. Initialize iteration step t = 0; Randomly initialize wt, bt, Pt; 2. Construct Laplacian matrix L ; 3. Repeat 4. Fixing Pt and bt, update wt+1 by Eq. (9); 5. Fixing Pt and wt+1, update bt+1 by Eq. (10); 6. Fixing wt+1 and bt+1, update Pt+1 by Algorithm 1; 7. t = t+1; 8. Until Convergence criterion satisfied. Output: Transformation matrix P \u2208 Rd\u00d7r\nof MMLDF on the LSVT Voice Rehabilitation dataset and the Urban Land Cover dataset, respectively. As in Fig. 3, we can see MMLDF has a good convergence rate. It will converge after only about 6 iterations."}, {"heading": "III. EXPERIMENTS", "text": ""}, {"heading": "A. Datasets and Experimental Settings", "text": "We evaluate the performance of MMLDF on eight realworld datasets, including one aerial image dataset Urban Land Cover [38], two biomedical area datasets DNA and Glioma, one large scale learning competition dataset Epsilon, one speech signal processing area dataset LSVT Voice Rehabilitation [36], one business area dataset CNAE-9, one face recognition dataset Yale Face, and one scene classification dataset 15 scene. The datasets DNA and Epsilon are downloaded\nfrom LIBSVM official web page2, and the dataset CNAE9 is downloaded from UCI Machine Learning Repository3. Datasets from different areas serve as a good test bed for a comprehensive evaluation. Table 1 summarizes the details of the datasets used in the experiments.\nTo verify the effectiveness of MMLDF, we compare it with the following seven related linear feature transformation methods: \u2022 SOLDE: Stable Orthogonal Local Discriminant Embed-\nding [17] reduces the dimensions by considering both the diversity and similarity. \u2022 LDG: Local Discriminant Gaussian [31] exploits a smooth approximation of the leave-one-out cross validation error of a quadratic discriminant analysis classifier4. \u2022 LPMIP: Locality-Preserved Maximum Information Projection [23] aims to preserve the local structure while maximizing the global information simultaneously. \u2022 LapLDA: Laplacian Linear Discriminant Analysis [24] presents a least squares formulation for LDA, which intends to preserve both of the global and local structures. \u2022 LSDA: Locality Sensitive Discriminant Analysis [22] aims to seek a projection which maximizes the margin between data points from different classes at local areas. \u2022 FSSL: This method proposes a framework for joint feature selection and subspace learning [32]. \u2022 MMPP: Maximum Margin Projection Pursuit [30] aims to find a subspace based on maximum margin principle.\nTo some extent, our method is related to these methods: similar to SOLDE and LSDA, our method aims to preserve the manifold structure in the learned low-dimensional subspace. Moreover, our method tries to preserve global and local information simultaneously, like LPMIP and LapLDA. In our work, we also try to unify feature transformation and feature selection in a framework, inspired by FSSL. Additionally, LDG and MMPP are two latest feature learning methods. LDG tries to preserve the local information during feature transformation, while MMPP is based on subspace learning under maximum margin principle.\nIn the experiment, we mainly investigate the effectiveness of the learned subspace by leveraging classifier learning. Hence we compare our method with the above feature transformation methods. In order to conduct fair comparisons, we use the same classifier, SVM with linear kernel, to evaluate the subspaces derived by different methods. The classification accuracy is used as the evaluation measure. In the experiments, we vary the reduced dimensions from 10 to 100 with a stepsize of 10. In our work, there are four parameters: C and \u03bb, \u03b7, and \u03c1. The parameters C, \u03b7, and \u03c1 are chosen by cross-validation, and the parameter \u03bb is always set to 10\u22124 (We found when \u03bb = 10\u22124, the performance was consistently good on all the datasets). For a fair comparison, the parameters in FSSL, LPMIP, LapLDA, LSDA, and MMPP are searched in the same space with that of MMLDF. For all the experiments, we repeat them 10 times, and report the average results.\n2http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/ 3http://archive.ics.uci.edu/ml/datasets/CNAE-9 4The MATLAB code for LDG was obtained from the authors of [31]\n6"}, {"heading": "B. General Performance", "text": "We first evaluate the classification performance of the proposed approach on all the datasets. Table II reports the experimental results of each algorithm with the optimal dimension. The optimal dimensions are listed in the brackets of Table II. It can be seen that MMLDF consistently outperforms the other seven algorithms on all the eight datasets. Compared with the second best result on each dataset, our method achieves 6.5%, 4.8%, 2.0%, 6.9%, 6.5%, 3.3%, 5.3%, and 8.1% relative improvement on the Urban Land Cover, CNAE9, DNA, Glioma, LSVT Voice Rehabilitation, Epsilon, Yale Face, and 15 scene datasets, respectively. Some baselines obtain considerably poor performance on certain datasets (e.g., FSSL on the Glioma and 15 scene datasets, LSDA on the 15 scene dataset). The reason may be that the generalization\nability of these algorithms is limited, making them difficult to be applied to different areas datasets.\nWe also studied the influence on the performance of different dimensions. Since LapLDA can be only reduced to K dimensions, where K denotes the number of the class, we did not compare our method with LapLDA. Fig. 4 shows the results. It can be seen that our method outperforms the other algorithms under all the cases."}, {"heading": "C. Analyses on Components\u2019 Roles", "text": "We verify the effectiveness of the components in the objective functions (2) and (7), individually. When the parameters \u03bb, \u03b7, and \u03c1 are set to zeros, MMLDF is reduced to MMPP, thus we use MMPP as the baseline. We perform the experiments on the binary classification dataset LSVT Voice Rehabilitation\nand the multi-class classification dataset Urban Land Cover. The experimental setting is as follows: we first set \u03b7 to zero in (2), and set \u03b7 and \u03c1 to zeros in (7), in order to demonstrate the effectiveness of the module filtering noisy features. We name it MMLDF-I for short. Then, we set \u03c1 to zero in (7), which indicates that we learn the feature representation without considering the correlations among multiple classes in the case of multi-class classification. We name it MMLDFII. The dimensions are fixed to 20 and 70 on the LSVT Voice Rehabilitation dataset and the Urban Land Cover respectively, because of better performance of MMLDF under such dimensions on the two datasets based on Table II.\nThe results are shown in Fig. 5. We can see that MMLDFI outperforms MMPP on the datasets, which shows that the row sparseness constraint on the projection matrix is beneficial to learn discriminative feature representation. On the LSVT Voice Rehabilitation dataset, MMLDF achieves better results than MMLDF-I, which means that within-class scatter minimization is good for classification. This point is also verified on the Urban Land Cover dataset, since MMLDF-II is superior to MMLDF-I. On the multi-class dataset, the result of MMLDF is better than that of MMLDF-II, which illustrates that capturing the correlations among multiple categories is helpful for enhancing the discriminative ability of the learned features. MMLDF achieves the best results on both of the two datasets. It shows that the combination of these components is effective for classification.\nWe further verify the robustness of l2,1-norm to different noise levels on the LSVT Voice Rehabilitation dataset. We generate N(= 50, 100, 150, 200)-dimensional white Gaussian noise respectively, and concatenate the original features with these noisy features as the new representation of each data. After that, we run MMLDF with l2,1-norm and without l2,1norm on the new dataset, respectively. We fix the reduced dimensions to 20, and the experimental results are listed in Table III. We can see the performance of MMLDF with l2,1-\n10 30\n50 70\n90 1e\u2212008 1e\u2212007 1e\u2212006\n1e\u2212005 0.0001\n0\n0.2\n0.4\n0.6\n0.8\nFeature #\nA cc\nur ac\ny\n(c) Vary \u03b7 and reduced dimensions\n10 30\n50 70\n90 0.001 0.01 0.1\n1 10\n0\n0.2\n0.4\n0.6\n0.8\nFeature #\nA cc\nur ac\ny\n(d) Vary \u03c1 and reduced dimensions\nFig. 6. Sensitivity study of the parameters on the Urban Land Cover dataset.\nnorm is better than that of MMLDF without l2,1-norm, i.e., l2,1-norm in MMLDF can indeed alleviate the effects of noisy features on feature transformation."}, {"heading": "D. Sensitivity Analysis", "text": "We also studied the sensitivity of parameters C, \u03bb, \u03b7, and \u03c1 in our algorithm on the Urban Land Cover dataset. Fig. 6 shows the results. With the fixed feature dimensions, our method is not sensitive to \u03bb, \u03b7 and \u03c1 with wide ranges. As for parameter C, when we fix the dimensions, the performance is gradually improved as C increases. When C > 10\u22123, the performance is gradually degraded as C increases. When C is set to 10\u22123, the performance is the best."}, {"heading": "IV. CONCLUSION", "text": "This paper proposed a novel feature transformation method for max-margin based classification. The proposed method aimed to find a low-dimensional feature space to maximize the classification margin of the data and minimize the within class scatter simultaneously. Moreover, we added a regularization term to eliminate noisy or redundant features. Finally, another regularization term was introduced to capture the correlations among multiple categories to help to learn discriminative features. Extensive experiments on publicly available benchmarks demonstrated the effectiveness of the proposed method compared to several related methods."}], "references": [{"title": "Evaluation of gender classification methods with automatically detected and aligned faces", "author": ["E. Makinen", "R. Raisamo"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 30, no. 3, pp. 541\u2013547, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A geometric nearest point algorithm for the efficient solution of the svm classification task", "author": ["M.E. Mavroforakis", "M. Sdralis", "S. Theodoridis"], "venue": "IEEE Trans. on Neural Networks, vol. 18, no. 5, pp. 1545\u20131549, 2007.  8", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral regression: A unified approach for sparse subspace learning", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE International Conference on Data Mining (ICDM), 2007, pp. 73\u201382.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Normalized mutual information feature selection", "author": ["P.A. Est\u00e9vez", "M. Tesmer", "C.A. Perez", "J.M. Zurada"], "venue": "IEEE Trans. on Neural Networks, vol. 20, no. 2, pp. 189\u2013201, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "On kernel target alignment", "author": ["N. Shawe-Taylor", "A. Kandola"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 14, 2002, pp. 367\u2013373.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient and robust feature selection via joint l2,1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C.H. Ding"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2010, pp. 1813\u20131821.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["C. Hou", "F. Nie", "X. Li", "D. Yi", "Y. Wu"], "venue": "IEEE Trans. on Cybernetics, vol. 44, no. 6, pp. 793\u2013804, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, no. 5500, pp. 2323\u20132326, 2000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), 2010, pp. 663\u2013670.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust lowrank subspace segmentation with semidefinite guarantees", "author": ["Y. Ni", "J. Sun", "X. Yuan", "S. Yan", "L.-F. Cheong"], "venue": "IEEE International Conference on Data Mining Workshops (ICDMW), 2010, pp. 1179\u20131188.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Dimensionality reduction by using sparse reconstruction embedding", "author": ["S. Huang", "C. Cai", "Y. Zhang"], "venue": "Advances in Multimedia Information Processing-PCM, 2010, pp. 167\u2013178.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Dimensionality reduction via kernel sparse representation", "author": ["Z. Pan", "Z. Deng", "Y. Wang", "Y. Zhang"], "venue": "Frontiers of Computer Science, vol. 8, no. 5, pp. 807\u2013815, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Locality preserving projections", "author": ["X. He", "P. Niyogi"], "venue": "Neural Information Processing Systems (NIPS), vol. 16, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Extraction of feature subspace for contentbased retrieval using relevance feedback", "author": ["Z. Su", "S. Li", "H. Zhang"], "venue": "Proceedings of ACM Multimedia (MM), 2001, pp. 98\u2013106.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Neighborhood preserving embedding", "author": ["X. He", "D. Cai", "S. Yan", "H. Zhang"], "venue": "Proceedings of International Conference on Computer Vision (ICCV), 2005, pp. 1208\u20131213.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Stable orthogonal local discriminant embedding for linear dimensionality reduction", "author": ["Q. Gao", "J. Ma", "H. Zhang", "X. Gao", "Y. Liu"], "venue": "IEEE Trans. on Image Processing, vol. 22, no. 7, pp. 2521\u20132531, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural computation, vol. 10, no. 5, pp. 1299\u20131319, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Stable local dimensionality reduction approaches", "author": ["C. Hou", "C. Zhang", "Y. Wu", "Y. Jiao"], "venue": "Pattern Recognition, vol. 42, no. 9, pp. 2054\u2013 2066, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Pca-based feature transformation for classification: issues in medical diagnostics", "author": ["M. Pechenizkiy", "A. Tsymbal", "S. Puuronen"], "venue": "IEEE Symposium on Computer-Based Medical Systems, 2004, pp. 535\u2013540.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Locality sensitive discriminant analysis", "author": ["D. Cai", "X. He", "K. Zhou", "J. Han", "H. Bao"], "venue": "Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI), 2007, pp. 708\u2013713.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Locality-preserved maximum information projection", "author": ["H. Wang", "S. Chen", "Z. HU", "W. Zheng"], "venue": "IEEE Trans. on Neural Networks., vol. 19, no. 4, pp. 571\u2013585, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Integrating global and local structures: a least squares framework for dimensionality reduction", "author": ["J. Chen", "J. Ye", "Q. Li"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2007, pp. 1\u20138.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised large margin discriminative projection", "author": ["F. Wang", "B. Zhao", "C. Zhang"], "venue": "IEEE Trans. on Neural Networks, vol. 22, no. 9, pp. 1446\u20131456, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "springer,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Robust classifiers for data reduced via random projections", "author": ["A. Majumdar", "R.K. Ward"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 40, no. 5, pp. 1359\u20131371, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Random projections for support vector machines", "author": ["S. Paul", "C. Boutsidis", "M. Magdon-Ismail", "P. Drineas"], "venue": "Proceedings of International Conference on Artificial Intelligence and Statistics, 2013, pp. 498\u2013506.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear dimensionality reduction for margin-based classification: High-dimensional data and sensor networks", "author": ["K.R. Varshney", "A.S. Willsky"], "venue": "IEEE Trans. on Signal Processing, vol. 59, no. 6, pp. 2496\u2013 2512, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum margin projection subspace learning for visual data analysis", "author": ["S. Nikitidis", "A. Tefas", "I. Pitas"], "venue": "IEEE Trans. on Image Processing, vol. 23, no. 10, pp. 4413\u20134425, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Dimensionality reduction by local discriminative gaussians", "author": ["N. Parrish", "M. Gupta"], "venue": "Proceedings of International Conference on Machine Learning (ICML), 2012, pp. 559\u2013566.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Joint feature selection and subspace learning", "author": ["Q. Gu", "Z. Li", "J. Han"], "venue": "Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), vol. 22, no. 1, 2011, pp. 1294\u20131299.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-task feature learning", "author": ["A. Evgeniou", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 19, 2007, pp. 41\u201348.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C. Burges"], "venue": "Data Mining and Knowledge Discovery, vol. 2, pp. 121\u2013167, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Objective automatic assessment of rehabilitative speech treatment in parkinson\u2019s disease", "author": ["A. Tsanas", "M. Little", "C. Fox", "L. Ramig"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 22, pp. 181\u2013190, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI), 2010, pp. 733\u2013742.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifying a high resolution image of an urban area using super-object information", "author": ["B. Johnson", "Z. Xie"], "venue": "ISPRS Journal of Photogrammetry and Remote Sensing, vol. 83, pp. 40\u201349, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust unsupervised feature selection", "author": ["M. Qian", "C. Zhai"], "venue": "Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI), 2013, pp. 1621\u20131627.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D. Liu", "J. Nocedal"], "venue": "Mathematical Programming, vol. 45, pp. 503\u2013528, 1989.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1989}, {"title": "Discriminative embedded clustering: A framework for grouping high-dimensional data", "author": ["C. Hou", "F. Nie", "D. Yi", "D. Tao"], "venue": "IEEE Trans. on Neural Networks and Learning Systems, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative semi-supervised feature selection via manifold regularization", "author": ["Z. Xu", "I. King", "M.-T. Lyu", "R. Jin"], "venue": "IEEE Trans. on Neural Networks, vol. 21, no. 7, pp. 1033\u20131047, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Data classification plays a key role in many practical applications [1], [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "Data classification plays a key role in many practical applications [1], [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "To cope with this issue, a popular way is to do dimensionality reduction, which is to project the data into a low-dimensional subspace with the least information loss [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 170, "endOffset": 173}, {"referenceID": 6, "context": "A number of selection criteria have been proposed in the past several years, such as mutual information [4], kernel alignment [5], sparsity regularization based measures [6], [7], and so on.", "startOffset": 175, "endOffset": 178}, {"referenceID": 7, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 11, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 189, "endOffset": 193}, {"referenceID": 12, "context": "Many feature transformation methods have been proposed over the last decades, including manifold learning [8], [9], low-rank representation (LRR) [10], [11], and sparse representation (SR) [12], [13].", "startOffset": 195, "endOffset": 199}, {"referenceID": 13, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 160, "endOffset": 164}, {"referenceID": 7, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 166, "endOffset": 169}, {"referenceID": 8, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 171, "endOffset": 174}, {"referenceID": 18, "context": "According to the characteristics of the mapping, feature transformation techniques can be further grouped into linear [14], [15], [16], [17] and nonlinear ones [18], [8], [9], [19].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "In this paper, we focus on the linear feature transformation methods due to its simplicity and effectiveness [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "In literature, principal component analysis (PCA) [15] and linear discriminant analysis (LDA) [21] are two classical linear algorithms, both capturing global Euclidean structure of the data.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 20, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 179, "endOffset": 183}, {"referenceID": 16, "context": "The representative approaches include locality preserving projection (LPP) [14], and neighborhood preserving embedding (NPE) [16], locality sensitive discriminant analysis (LSDA) [22], and stable orthogonal local discriminant embedding (SOLDE) [17].", "startOffset": 244, "endOffset": 248}, {"referenceID": 21, "context": "Meanwhile, some work integrates global and local information during feature transformation, such as LPMIP [23], LapLDA [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "Meanwhile, some work integrates global and local information during feature transformation, such as LPMIP [23], LapLDA [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "In order to alleviate this limitation, the maximum margin projection (MMP) algorithm [25] takes advantage of a binary support vector machine (SVM) [26] classifier to obtain some hyperplanes that separate data points in different clusters with the maximum margin.", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "In order to alleviate this limitation, the maximum margin projection (MMP) algorithm [25] takes advantage of a binary support vector machine (SVM) [26] classifier to obtain some hyperplanes that separate data points in different clusters with the maximum margin.", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "The random projection algorithms [27], [28] aim to find some Gaussian random projection matrices to preserve the pairwise distances between data points in the projected subspace, which can be effectively combined with some classifiers, such as SVM.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "The random projection algorithms [27], [28] aim to find some Gaussian random projection matrices to preserve the pairwise distances between data points in the projected subspace, which can be effectively combined with some classifiers, such as SVM.", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "Varshney and Willsky [29] propose a framework to simultaneously learn a linear dimensionality reduction projection matrix and a margin-based classifier defined in the reduced space.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "The main idea of the maximum margin projection pursuit (MMPP) algorithm is to integrate optimal data embedding and SVM classification in a single framework in both bi-class and multi-class classification [30].", "startOffset": 204, "endOffset": 208}, {"referenceID": 29, "context": "In addition, local discriminant gaussian (LDG) [31] is a feature transformation method which exploits a smooth approximation of the leave-one-out cross validation error of a classifier.", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "[32] propose a framework for joint subspace learning and feature selection, called FSSL, which can alleviate the effect of the noisy features for feature transformation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "discriminative features and designing classifiers [33].", "startOffset": 50, "endOffset": 54}, {"referenceID": 32, "context": "Margin maximization has been demonstrated to be a good principle applied by various learning methods [34], [35].", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "Margin maximization has been demonstrated to be a good principle applied by various learning methods [34], [35].", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "We take the LSVT Voice Rehabilitation dataset [36], a biclass classification dataset, as an example to illustrate the effectiveness of the l2,1 norm constraint on P in the objective function (2).", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "In real-world applications, one classification task is often correlated with other classification tasks, and mining the correlations among multiple categories can be good for feature learning [33].", "startOffset": 192, "endOffset": 196}, {"referenceID": 35, "context": "\u0393 plays the role of the inverse covariance matrix that encodes the correlations among the weight vectors wi [37].", "startOffset": 108, "endOffset": 112}, {"referenceID": 36, "context": "We use the Urban Land Cover dataset [38], a multi-class classification dataset, to visualize the correlation coefficient matrix of the weight vectors, which can be obtained based on the learned \u0393.", "startOffset": 36, "endOffset": 40}, {"referenceID": 37, "context": "Here we choose the limited-memory BFGS (L-BFGS) algorithm for its efficiency [39], [40], which is summarized in Algorithm 1.", "startOffset": 77, "endOffset": 81}, {"referenceID": 38, "context": "Here we choose the limited-memory BFGS (L-BFGS) algorithm for its efficiency [39], [40], which is summarized in Algorithm 1.", "startOffset": 83, "endOffset": 87}, {"referenceID": 39, "context": "We randomly initialize the parameters, and adopt the second updating rule in [41] for deriving the local optimal solution.", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": "Details of the proof on (11) can be found in [37].", "startOffset": 45, "endOffset": 49}, {"referenceID": 27, "context": "Varshney and Willsky [29] propose a linear dimensionality reduction method, which represents the learned mappings by matrices on the Stiefel manifold and on margin-based classifiers.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "[30] present the maximum margin projection pursuit (MMPP) algorithm, which learns the optimal data embedding and the SVM classifier simultaneously.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "When \u03b7 \u2192 0, \u03bb\u2192 0, \u03c1\u2192 0 in (7), our objective function has similar effects to those of [29] and [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "When \u03b7 \u2192 0, \u03bb\u2192 0, \u03c1\u2192 0 in (7), our objective function has similar effects to those of [29] and [30].", "startOffset": 95, "endOffset": 99}, {"referenceID": 30, "context": "[32] propose a framework for joint subspace learning and feature selection, where subspace learning is reformulated as solving a linear system equation, and feature selection is achieved by utilizing l2,1-norm on the projection matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "When C \u2192 0, \u03c1\u2192 0, and adding a constraint on the projection matrix in (7), the formulation of our method is reduced to that of [32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "[42] propose a semi-supervised feature selection method called FS-Manifold, where the feature selection process is embedded with a manifold regularized SVM classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "According to [35], we know the original max-margin objective function of SVM can be transformed into its dual version as:", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "We evaluate the performance of MMLDF on eight realworld datasets, including one aerial image dataset Urban Land Cover [38], two biomedical area datasets DNA and Glioma, one large scale learning competition dataset Epsilon, one speech signal processing area dataset LSVT Voice Rehabilitation [36], one business area dataset CNAE-9, one face recognition dataset Yale Face, and one scene classification dataset 15 scene.", "startOffset": 118, "endOffset": 122}, {"referenceID": 34, "context": "We evaluate the performance of MMLDF on eight realworld datasets, including one aerial image dataset Urban Land Cover [38], two biomedical area datasets DNA and Glioma, one large scale learning competition dataset Epsilon, one speech signal processing area dataset LSVT Voice Rehabilitation [36], one business area dataset CNAE-9, one face recognition dataset Yale Face, and one scene classification dataset 15 scene.", "startOffset": 291, "endOffset": 295}, {"referenceID": 16, "context": "\u2022 SOLDE: Stable Orthogonal Local Discriminant Embedding [17] reduces the dimensions by considering both the diversity and similarity.", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "\u2022 LDG: Local Discriminant Gaussian [31] exploits a smooth approximation of the leave-one-out cross validation error of a quadratic discriminant analysis classifier4.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "\u2022 LPMIP: Locality-Preserved Maximum Information Projection [23] aims to preserve the local structure while maximizing the global information simultaneously.", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "\u2022 LapLDA: Laplacian Linear Discriminant Analysis [24] presents a least squares formulation for LDA, which intends to preserve both of the global and local structures.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "\u2022 LSDA: Locality Sensitive Discriminant Analysis [22] aims to seek a projection which maximizes the margin between data points from different classes at local areas.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "\u2022 FSSL: This method proposes a framework for joint feature selection and subspace learning [32].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "\u2022 MMPP: Maximum Margin Projection Pursuit [30] aims to find a subspace based on maximum margin principle.", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "edu/ml/datasets/CNAE-9 4The MATLAB code for LDG was obtained from the authors of [31]", "startOffset": 81, "endOffset": 85}], "year": 2017, "abstractText": "In this paper, we propose a new max-margin based discriminative feature learning method. Specifically, we aim at learning a low-dimensional feature representation, so as to maximize the global margin of the data and make the samples from the same class as close as possible. In order to enhance the robustness to noise, we leverage a regularization term to make the transformation matrix sparse in rows. In addition, we further learn and leverage the correlations among multiple categories for assisting in learning discriminative features. The experimental results demonstrate the power of the proposed method against the related state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}