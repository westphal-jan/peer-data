{"id": "1412.5104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Locally Scale-Invariant Convolutional Neural Networks", "abstract": "Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting. In order to create the model, ConvNets has to have a different distribution. It can even be a bit like learning an English dictionary. The ConvNets learned all these features when faced with a gradient and then learn a much more different learning structure. In this paper, we present a model that uses convolutional networks with only a subset of all parameters, but with the addition of a little more parameterization, convolutional networks can learn all these features with less variance than in previous work [2].\n\n\n\nThe convolutional network is described as a linear and circular network that is divided into three different stages of the network: the average and the average part. These nodes are both part of the network (the average and the average part). These nodes are both part of the network (the average and the average part). These nodes are both part of the network (the average and the average part). These nodes are both part of the network (the average and the average part). These nodes are both part of the network (the average and the average part). These nodes are both part of the network (the average and the average part). These nodes are both part of the network (the average and the average part). These nodes are both part of the network (the average and the average part).\nWe demonstrate convolutional networks of the ConvNets (or ConvNets) that can learn a lot about the ConvNets", "histories": [["v1", "Tue, 16 Dec 2014 18:09:34 GMT  (321kb,D)", "http://arxiv.org/abs/1412.5104v1", "Deep Learning and Representation Learning Workshop: NIPS 2014"]], "COMMENTS": "Deep Learning and Representation Learning Workshop: NIPS 2014", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["angjoo kanazawa", "abhishek sharma", "david jacobs"], "accepted": false, "id": "1412.5104"}, "pdf": {"name": "1412.5104.pdf", "metadata": {"source": "CRF", "title": "Locally Scale-Invariant Convolutional Neural Networks", "authors": ["Angjoo Kanazawa", "Abhishek Sharma"], "emails": ["kanazawa@umiacs.umd.edu", "bohkaal@umiacs.umd.edu", "djacobs@umiacs.umd.edu"], "sections": [{"heading": "1 Introduction", "text": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1]. ConvNets\u2019 success comes from their ability to learn complex patterns by building increasingly abstract representations layer by layer, much like other deep neural networks. However, ConvNets differ in that they exploit the two dimensional structure of images where objects and patterns appear at arbitrary locations. ConvNets apply local filters at every position in the image, allowing the network to detect and learn patterns regardless of their location.\nIn reality, the world has a three dimensional structure, and objects at different distances will appear in an image at different scales as well as locations. ConvNets do not have a mechanism to take advantage of scale explicitly, so to detect a single pattern at multiple scales, they must learn to separate filters for each scale. Unfortunately this has several major shortcomings. Capturing patterns at multiple scales uses up resources that could be used to learn a wider variety of feature detectors. This requires an increase in the number of feature detectors, which means that the network is harder to train, takes longer to train, and is more likely to overfit. To learn multiple scales and prevent\nar X\niv :1\n41 2.\n51 04\nv1 [\ncs .C\nV ]\noverfitting, you would need a lot of training data, and even then the network will only respond to the scales seen during training. Also, detectors that capture the same pattern but at different scales are learned independently without sharing the training samples. Finally, having multiple filters of a single pattern at different scales burdens the next layer by increasing the number of configurations that indicate the presence or absence of that pattern.\nIn this paper, we present scale-invariant convolutional networks (SI-ConvNets), which applies filters at multiple scales in each layer so a single filter can detect and learn patterns at multiple scales. We max-pool responses over scales to obtain representations that are locally scale invariant yet have the same dimensionality as a traditional ConvNet layer output. The proposed architecture differs from other multi-scale approaches explored with ConvNets since scale-invariance is built in at the layer level rather than at the network level. We also achieve locally scale-invariant representation and we do not require any increase in the number of parameters to be learned. We show in our experiments that by sharing information from multiple scales, the proposed model can achieve better classification performance than ConvNets while simultaneously requiring less training data. We evaluate the proposed model on a variation of the MNIST dataset where digits appear at multiple scales, and demonstrate that the SI-ConvNets are more robust to scale variations in training data and unfamiliar scales in test data than ConvNets. Our model is complementary to other ConvNet architectures and it can easily be incorporated into existing variants, and we will make the source code available online for the research community."}, {"heading": "2 Background", "text": "Several recent works have addressed the problem of explicitly incorporating transformation invariance in deep learning models. In the unsupervised feature learning domain, Sohn et al. [6] and [7] introduced transformation-invariant Restricted Boltzmann Machines (RBMs) where linear transformations of a filter are applied to each input to infer its highest activation. In these two models, the transformed filters were only applied at the center of the largest receptive field size. Our model uses an inverse transformation stage so that transformed filters can be applied densely but still retain correspondence. Our work is inspired by the success of Sohn et al. and our goal is to incorporate scale-invariant feature learning into the extremely successful ConvNet models [1].\nTiled convolutional neural networks [8] learn invariances implicitly by square-root pooling hidden units that are computed by partially un-tied weights. In comparison, our approach explicitly encodes scale invariance, and does not require the increase in the number of learned parameter that is required by un-tying of weights. [9] fuses outputs of multiple ConvNets applied over multiple scales for semantic segmentation, but each ConvNet is learned independently without weight-sharing. In contrast, we jointly learn a set of feature detectors that are shared over multiple scales. [5] proposes a multi-scale ConvNet where outputs of all convolutional layers are fed to the classifier. This enables them to capture information from different levels of the hierarchy, but there is no scale invariance in the features learned at a layer because each layer is only applied to the original scale.\nAnother influential work is that of Farabet et al.[10], who train ConvNets over the Laplacian pyramid of images with tied weights for scene parsing problems. In their model, the entire multi-layer forward propagation is applied end to end at each scale disjointly, then right before the fully-connected layers, the responses of all scales are aligned by up-sampling and concatenated. Keeping responses from each scale allows them to capture scale-level dependencies, but at the expense of increasing the number of parameters required in the final layer. This further restricts the number of scales that can be applied; in contrast, our model is more compact and free of such restrictions. Further, their approach is motivated by the need to have large receptive field sizes to capture long-range, contextual interactions that are necessary for scene understanding. In contrast, we are interested in capturing locally scale-invariant features that are useful for image classification; hence, unlike their approach, we pool the responses over all scales in each spatial location in each layer. Pooling responses over scales in each layer as opposed to concatenating all scales in the very end has subtle but different effects in the middle layers. For example, suppose there is an image of two circles of different sizes, and a circle filter that can detect one of the circles in the image. In the architecture of Farabet et al., each circle will be detected in a different scale, but they will never be recognized together until the layer where all scales are concatenated. In our architecture, both circles can be detected together and the second layer can immediately make use of the fact that there are two cir-\ncles in the image. Of course, by having two circle filters of different sizes, Farabet et al.\u2019s network can also detect two circles in one scale, but at the expense of learning redundant filters. While their work demonstrates the advantages of applying ConvNets over multiple scales in scene parsing, we further investigate its effectiveness in an image classification domain with a more modular model that explicitly incorporates scale-invariance in each layer."}, {"heading": "2.1 Convolutional Neural Networks", "text": "Convolutional Neural Networks (ConvNets) are a supervised feed-forward multi-layer architecture where each layer learns feature detectors of increasing complexity. The final layer is a classifier or regressor with a cost function such that the network can be trained in a supervised manner. The entire network is optimized jointly via stochastic gradient descent with gradients computed by backpropagation [2]. A single layer in a ConvNet is usually composed of feature extraction and nonlinear activation stages, optionally followed by spatial pooling and feature normalization.\nThe hallmark of ConvNets is the idea of convolving local feature detectors across the image. This idea is motivated by the fact that similar patterns can appear anywhere in the 2D layout of pixels and that nearby values present strong dependencies in natural images [11]. The local feature detectors are trainable filters (kernels) and their spatial extent is called the receptive field of the network, as it represents how much of the image the network gets to \u201csee\u201d. The convolution operation effectively ties the learned weights at multiple locations, which radically reduces the number of trainable parameters as compared to having different weights at each location [2]. The output of convolving one kernel is called a feature map, which is sent through a nonlinear activation function \u03c3. A feature map h at a layer is computed as h = \u03c3((W \u2217 x) + b), (1) where \u2217 is the convolution operator, x is the input feature map from the previous layer,W and b \u2208 R are the trainable weights and bias respectively. By having multiple feature maps, the network can represent multiple concepts in a single layer. The model may further summarize each sub-region of the feature map via max or average pooling, providing invariance to small amounts of image translation."}, {"heading": "3 Scale-Invariant Convolutional Neural Network", "text": "Feature detectors in ConvNets have the ability to detect features regardless of their spatial locations in the image, but the same cannot be said for features at different scales. In this section we describe a scale-invariant ConvNet (SI-ConvNet). Our formulation also allows the output of ConvNets to be locally scale-invariant, where the representation of the same patterns at different scales will be similar 1. Figure 1 shows the side by side comparison of the overall structure of these two layers."}, {"heading": "3.1 Forward Propagation", "text": "Our goal is to let one feature detector respond to patterns at multiple scales. To do so, we convolve the filters over multiple resolutions in a pyramid. At each scale the exact same filters are used to convolve the image (weight-tying). Since we spatially transform the image, the outputs of convolution come in different spatial sizes. In order to align the feature maps, we apply an inverse transformation to each feature map 2. Finally we max-pool the responses over all the scales at each spatial location. Pooling responses over multiple scales serves two purposes. First, it allows us to obtain a locally scale-invariant representation. Second, it summarizes the responses in a concise way that allows us to maintain the same output size as a standard convolution layer.\nSpecifically, let T be a linear image transform operator that applies some spatial transformations to an input x. Then, for a set of n transformation operators {T1, . . . , Tn}, a feature map, h, is computed\n1Given that the patterns share the same center. When the center of the patterns are shifted, the output will be similar but at different locations, i.e. a shift-equivariant representation\n2When the stride is equal to the size of the kernel, applying the inverse transformation gives direct correspondence between convolution outputs. When it\u2019s not, after applying the inverse transformation, the output has to be either cropped or padded with 0s to be properly aligned.\nas:\nz\u0302i = (W \u2217 Ti(x)) + b (2) zi = T \u22121i (z\u0302i) (3)\nh = \u03c3 ( max\ni\u2208{1,...,n} [zi]\n) . (4)\nNote that z\u0302i \u2208 Rhi\u00d7hi\u00d7m, where hi is the size of the output of convolution in the i-th transformed input, and zi \u2208 Rh0\u00d7h0\u00d7m for all i, where h0 is the canonical output size when all responses are aligned andm is the number of feature maps used. T1 is always the identity transformation, so when n = 1 the framework is equivalent to traditional ConvNets.\nFigure 2 illustrates this idea with sample inputs x1 and x2 that have the same \u201cV\u201d pattern but at different scales. W is a \u201cV\u201d detector that this ConvNet has learned. With this W , a standard convolution layer will only activate x2 whose \u201cV\u201d pattern matches the size of \u201cV\u201d in W . However, in a scale-invariant convolution layer, x1 and x2 undergo scale transformations where W can be matched in one of the scales, allowing W to detect the pattern on both x2 and x1. The responses are aligned via inverse transformation and the final output is the maximum activation at each spatial location. The path of the winning scale is shown in bold lines.\nSince convolving a pyramid of images with a single filter is analogous to convolving a single image with filters of different sizes, using n scales is analogous to increasing the number of feature maps by n times without actually paying the price of having more parameters. This allows us to train a more expressive model without increasing the chances for over-fitting.\nThe image transform operator T (x) is parametrized by a single scale factor s where each 2D location ~x in the transformed image is computed by a linear interpolation of the original image around s\u22121~x. We use bilinear interpolation to compute the coefficients. Note that while we focus on scale invariance in this paper, the framework is applicable to other linear transformations. The transformation coefficients can be precomputed, so applying the transformation is efficient. However, there is an increase in the number of convolution operations required since each scaled input must be convolved. The increase is dominated by the largest scale factor sn and the step sizes r > 1 between each scale. Explicitly, the increase in the number of convolutions in a layer is s2n[ 1\u2212(1/r)n 1\u2212(1/r) ]. Please refer to the supplementary material for details.\nInput Scale the input Convolve with W Align responces via inverse scale Max pooling Output"}, {"heading": "3.2 Backward Propagation", "text": "Since the scale-invariant convolution layer consists of just linear and max operations, its gradients can be computed by a simple modification of the back-propagation algorithm. Backprop for maxpooling over scale is implemented by using the argmax indices, analogous to how backprop is done for spatial max-pooling. For scale transformations, the error signal is propagated through the bilinear coefficients used to compute the transformation. Please refer to the supplementary materials for the detailed derivations."}, {"heading": "4 Experiments", "text": "We first compare the performance of the proposed method, referred to as \u201cSI-ConvNet\u201d, against other baseline methods including traditional ConvNets. For all the experiments we carried out, all networks share the exact same hyper-parameters and architecture, except that the convolution layers that are replaced by scale-invariant convolution layers. We implement our method using the opensource Caffe framework [12], and our code will be available online.\nIn order to evaluate the effectiveness of SI-ConvNets we must experiment with a dataset where objects come at variety of scales, since there is not much gain that can be obtained from learning in a scale-invariant manner when there is no scale variation in data. Unfortunately, most of the benchmark datasets for evaluating ConvNets do not fit this category. Therefore, we experiment with the modified MNIST handwritten digit classification dataset introduced in [6] called MNIST-scale. It consists of 28\u00d728 gray-scale images, where each digit is randomly scaled by a factor s \u2208 U(0.3, 1) without making any truncation of the foreground pixels.\nUnless otherwise noted, the architectures used in this experiment consist of two convolutional layers with 36 and 64 feature maps of 7x7 and 5x5 kernels respectively, a fully connected layer with 150 hidden nodes, and a soft-max logistic regression layer. The network architecture is modeled after the ConvNets of [13] that achieve state-of-the-art on the original MNIST dataset, and we use the same pre-processing method and hyper-parameters unless otherwise noted. We don\u2019t use techniques such as data augmentation, dropout or model averaging to simplify the comparison between a con-\nvolution layer and the proposed scale-invariant convolution layer 3. Only the kernel size of the first convolution layer and weight decay parameter were re-tuned for the MNIST-Scale dataset using a subset of training data on a ConvNet and are fixed for all networks. The networks are trained for 700 epochs and the test error after 700 epochs are reported. All networks share the same random seed. The scale-invariant convolution layer uses six scales from 0.6 to 2 at a scale step of 21/3 = 1.26, i.e. scales at 1.26[\u22122:3]. The details of the parameters used in each experiment are provided in the supplementary materials."}, {"heading": "4.1 MNIST-Scale", "text": "In this experiment we compare our proposed network with ConvNets, the hierarchical ConvNets of Farabet et al. [10], Restricted Boltzmann Machine (RBM) and its scale-invariant version of Sohn et al. [6]. Following the experimental protocol of [6], we train and test each network on 10,000 and 50,000 images respectively. We evaluate our models on 6 train/test folds and report the average test error and the standard deviation. We use the same architecture and scale parameters for hierarchical ConvNets. The results are shown in Table 1. SI-ConvNet outperforms both ConvNet and hierarchical ConvNet by more than 10%. Hierarchical ConvNet slightly underperforms ConvNet, possibly due to overfitting as it has 6 times more parameters than ConvNet/SI-ConvNet. In the scene classification context for which hierarchical ConvNet was introduced, pixel-level labels exist providing much more training data compared to the image classification settings. This result emphasizes the strength of SI-ConvNet, which achieves scale-invariant learning while keeping the number of parameters fixed. There is large gap between RBM models and the ConvNets due to the fact that RBMs are unsupervised models and that their architecture is shallow with only one feature extraction layer. However, the relative error difference between the original and the scale-invariant version of the RBMs and ConvNets are comparable at 9.8% and 10% respectively for SI-RBM and SI-ConvNet. This shows that SI-ConvNet is obtaining similar improvements for being scale-invariant and that it is a good supervised counterpart of scale-invariant models.\nTable 1: Test Error on MNIST-Scale of various methods.\nMethod Test Error (%) on 6 train/test fold Restricted Boltzman Machine (RBM)[6] 6.1 Scale-invariant RBM [6] 5.5 Convolutional Neural Network [13] 3.48 \u00b1 0.23 Hierarchical ConvNets [10] 3.58 \u00b1 0.17 Scale-Invariant ConvNet (this paper) 3.13 \u00b1 0.19\n4.1.1 Measuring Invariance\nWe investigate the scale-invariance achieved by our model using the invariance measure proposed in Goodfellow et al [14]. In this method, a neuron hi(x) is said to fire when hi(x) > ti. Each ti is chosen so that the recall,G(i) =\n\u2211 |hi(x) > ti|/N , over N inputs is greater than 0.01. A set of transformations T is applied to the images that most activate the hidden unit, and the number of times the neuron fires in response to the transformed inputs is recorded. The proportion of transformed inputs that a neuron fires to is called the local firing rate, L(i), which measures the robustness of the neuron to T . High L(i) value indicates invariance to T , unless the neuron is easily fired by arbitrary inputs. Therefore, the invariance score of a neuron is computed as the ratio of its invariance and selectivity i.e. Li/Gi. We report the average of the top 20% highest scoring neurons (p = 0.2). Please see [14] for more details.\n3Note that in [13], ConvNet without dropout achieves state-of-the-art performance along with ConvNet with dropconnect.\nHere T consists of scaling the images with values in [0.3, 1.2] with step size 0.1. Figure 3 shows the invariance score of ConvNet and SI-ConvNet measured at the end of each layer. We can see that by max-pooling responses over multiple scales, SI-ConvNets produce features that are more scale-invariant than those from ConvNets."}, {"heading": "4.1.2 Effect of training data and number of parameters", "text": "We further evaluate SI-ConvNet by varying the number of training samples and feature maps in the first two layers. For these experiments we report the test error on 10,000 images.\nAs discussed in subsection 3.1, using n scales in a scale-invariant convolution layer that has m kernels resembles a network that has nm kernels without actually having to increase the number of parameters by n times. One of the biggest disadvantages of ConvNets is that it requires more training data as the number of parameters increase. By keeping the number of parameters fixed, SIConvNets can train a n times wider and thus more powerful network at a less demanding amount of training data. Being able to share information between the same patterns at multiple scales further allows SI-ConvNets to learn better features with less data. In contrast, ConvNets learn multiple filters for the same pattern independently, and learning those filters well requires many examples of that pattern at each scale.\nFigure 4a plots test error as the number of feature map is varied, where SI-ConvNets consistently outperforms ConvNets. Since given enough feature maps, ConvNets can learn a feature detector for each scale, we observe that the gap decreases as the number of feature maps increases. Figure 4b plots test error as the amount of training data changes. Again, SI-ConvNet consistently achieves lower error than ConvNet, where their gap decreases as training data increases. This shows that SI-ConvNets can learn a better model with less training data."}, {"heading": "4.1.3 Robustness to Unfamiliar Scales and Scale Variation", "text": "In the following two experiments, we increase the image sizes of the MNIST-scale dataset from 28x28 to 40x40 so that we can experiment with a wider range of scale variation. In order to account for the larger scale range, we change the scales used in SI-ConvNet from 5 scales in [0.6-2] to 5 scales in [0.5 - 2.7] for these two experiments. We train and test on 10,000 images.\nFirst we evaluate the ability to correctly classify images that are less common in the training data. Here the training data is scaled by factors sampled from a GaussianN (1, 0.24) rather than a Uniform distribution. The digits in the test data are scaled to one particular scale factor and we vary the test\nscale factor between [0.4, 1.6], which correspond to about \u00b12\u03c3 away from the mean. The further away the test scale factor is from the mean, the more challenging the problem gets since not many training samples have been observed at that scale during the training. We expect ConvNet and SIConvNet to do similarly around the mean but ConvNet to get progressively worse as scale moves away from the mean as it cannot reuse the filters it learned for inputs of different scales. As shown in Figure 5a, our results verify this trend, where SI-ConvNet outperforms ConvNet even at the mean. Average reduction in the relative error is 25% and at the two ends of the scales 0.4 and 1.6, the relative error reduction is 20% and 47% respectively. The lack of symmetry around the mean is possibly due to the fact that digit classification becomes extremely difficult even for humans when digits are very small. (For example, at the scale of 0.4, the actual digit sizes are around 8x8.)\nNext we evaluate robustness to scale variation by increasing the range of scale present in training and test data while keeping the number of parameters and training samples fixed. The scale factors are sampled from a uniform distribution in the range [a, b]. The results in Figure 5b show that SIConvNets consistently outperform ConvNets, and that the error of SI-ConvNets increases at a lower rate than ConvNets as the scale variation increases. This shows the weakness of ConvNets which has to learn redundant filters for digits that come at a wide variety of scales, and that SI-ConvNets is making a more efficient use of its resources in terms of the number of parameters and training data."}, {"heading": "5 Conclusion", "text": "We introduced an architecture that allows locally scale-invariant feature learning and representation in convolutional neural nets. By sharing the weights across multiple scales and locations, a single feature detector can capture that feature at arbitrary scales and locations. We achieve locally scaleinvariant feature representation by pooling detector responses over multiple scales. Our architecture is different from previous approaches in that scale-invariance is built into each convolution layer independently. Because we maintain the same number of parameters as traditional ConvNets while incorporating the scale prior, we can learn features more efficiently with reduced chances of overfitting. Our experiments show that SI-ConvNets outperform ConvNets in various aspects."}, {"heading": "A Back-propagation", "text": "In order to align the notation of back-propagation to that of an ordinary multi-layer neural network, we re-write each step in the forward propagation of a scale-invariant convolution layer as a matrixvector multiplication.\nLet xl be a vectorized input at layer l of length n (n = hwc for a h by w by c image). The spatial transformation T (x) can be written as a matrix-vector multiplication of a n by m matrix T that encodes the interpolation coefficients, where n and m are the dimensionality of the original and the transformed input respectively. With bilinear interpolation, each row of T has 4 non-zero coefficients.\nThe convolution operation can be written as a matrix-vector multiplication by encoding W as a Toeplitz matrix. Then, the forward propagation at layer l is\nzli = T\u0302i[toep(W )(Tix l\u22121) + bl] (5)\nhl = \u03c3 ( max\ni\u2208{1,...,n} [zi]\n) , (6)\nwhere Ti is the matrix encoding the i-th transformation where images are scaled to different sizes, T\u0302i is the matrix encoding the i-th inverse transformation used to align the responses of convolution on each scale. toep(W ) is the kernel matrix encoded as a Toeplitz matrix.\nThen, the error signal from the previous layer \u03b4l+1 can be propagated by equations \u03b4l+1i = 1 ( argmaxj\u2208{1,...,n}[zj ] = i ) \u03c3\u2032(z) (7)\n\u03b4li = toep(W ) \u1d40(T\u0302 \u1d40i \u03b4 l+1 i ) (8) \u03b4l = \u2211 i T \u1d40i \u03b4 l i, (9)\nwhere is the element-wise multiplication. Equation (3) applies the derivative of the activation function to the error signal and distributes the error into n separate errors using the argmax indices to un-pool the max-pooling stage. Equation (4) propagates the error based on the linear weights T\u0302i and W similar to the way the error is propagated in a traditional ConvNet. Then in Equation (5), the error is propagated through the initial spatial transformation and is accumulated to complete the propagation of this layer."}, {"heading": "B Time Analysis", "text": "We discuss the increase in the number of convolution operations required in a scale-invariant convolution layer compared to a traditional convolution layer.\nGiven an n \u00d7 n \u00d7 m input image and a kernel of size k \u00d7 k \u00d7 m, a traditional convolution layer computes the linear combination of the kernel and a local region (n\u2212k+1)2 = O(n2) times (using \u201cvalid\u201d convolution at the borders). For a scale-invariant convolution layer that uses t scales at a step size of s > 1 whose largest scale factor is sk, the input image is scaled to t different images of size [skn, \u00b7 \u00b7 \u00b7 , sk\u2212tn]. So the number of linear combinations to be computed on all of the scaled inputs is\n(skn\u2212 k + 1)2 + \u00b7 \u00b7 \u00b7+ (n\u2212 k + 1)2 + \u00b7 \u00b7 \u00b7+ (sk\u2212tn\u2212 k + 1)2. (10)\nThe summation of the quadratic terms is a geometric series,\ns2kn2 + \u00b7 \u00b7 \u00b7+ n2 + \u00b7 \u00b7 \u00b7+ s2(k\u2212t)n2 (11)\n= n2s2k[1 + 1\ns2 +\n1 s4 + \u00b7 \u00b7 \u00b7+ 1 s2t ] (12)\n= n2sk t\u2211\ni=0\n( 1\ns\n)2i . (13)\nSince r = 1s2 < 0, the series sums to\nn2s2k 1\u2212 rt 1\u2212 r . (14)\nTherefore, the number of linear combinations computed in a single scale-invariant convolution layer is O(s2k 1\u2212rt1\u2212r n2). For example, using the values s = 1.26, t = 5 and k = 2 that are used in our experiments, the series sums to 10."}, {"heading": "C Experimental Details", "text": "Here we list the details of networks used in the experiments.\nAll inputs are pre-processed by subtracting the training mean and the pixel values are scaled to the [0, 1] range.\nThe base setup is a three layer network. All experiments have this architecture unless specified otherwise. The first layer is a (SI-)convolution layer of 7\u00d7 7 kernel at stride of one with 36 feature maps with ReLu activation function, followed by 2 \u00d7 2 max-pooling of stride two. The second layer is another (SI-)convolution layer of 5 \u00d7 5 kernel with 64 feature maps with ReLu and 3 \u00d7 3 max-pooling of stride three. The third layer is a fully connected layer with 150 hidden variables with ReLu and this final output is sent to the logistic regression layer of size 10. The network is optimized by stochastic gradient descent of mini-batch size 128 with a fixed learning rate of 0.01. Momentum of 0.9 and weight decay of 0.0001 are used as regularization. Networks are trained for 700 epochs.\nWe tuned the kernel size of the first layer and the weight decay on a 10k validation set. After the parameters are set, all training data were used to obtain the final network. For the experiments that are ran on 40\u00d740 images, we found the best kernel size of the first layer to be 9\u00d79. The configuration files that contains hyper-parameters and architectures for each experiment will be available with the source code."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Lon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Yann LeCun", "Fu Jie Huang", "L\u00e9on Bottou"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Traffic sign recognition with multi-scale convolutional networks", "author": ["Pierre Sermanet", "Yann LeCun"], "venue": "In Proceedings of International Joint Conference on Neural Networks", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Learning invariant representations with local transformations", "author": ["Kihyuk Sohn", "Honglak Lee"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Transformation equivariant boltzmann machines", "author": ["Jyri J. Kivinen", "Christopher K.I. Williams"], "venue": "In Artificial Neural Networks and Machine Learning (ICANN),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Tiled convolutional neural networks", "author": ["Quoc V. Le", "Jiquan Ngiam", "Zhenghao Chen", "Daniel Jin hao Chia", "Pang Wei Koh", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Semantic road segmentation via multi-scale ensembles of learned features", "author": ["Jose M. Alvarez", "Yann LeCun", "Theo Gevers", "Antonio M. Lopez"], "venue": "In ECCV Workshop on Computer Vision in Vehicle Technology: From Earth to Mars,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning hierarchical features for scene labeling", "author": ["Cl\u00e9ment Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron C. Courville", "Pascal Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Yangqing Jia"], "venue": "http://caffe.berkeleyvision.org/,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew D. Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1].", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 161, "endOffset": 164}, {"referenceID": 0, "context": "Convolutional Neural Networks (ConvNets) [2] have achieved excellent results on visual classification tasks like handwritten digits [3], toys [4], traffic signs [5], and recently 1000-category ImageNet classification [1].", "startOffset": 217, "endOffset": 220}, {"referenceID": 5, "context": "[6] and [7] introduced transformation-invariant Restricted Boltzmann Machines (RBMs) where linear transformations of a filter are applied to each input to infer its highest activation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[6] and [7] introduced transformation-invariant Restricted Boltzmann Machines (RBMs) where linear transformations of a filter are applied to each input to infer its highest activation.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "and our goal is to incorporate scale-invariant feature learning into the extremely successful ConvNet models [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "Tiled convolutional neural networks [8] learn invariances implicitly by square-root pooling hidden units that are computed by partially un-tied weights.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "[9] fuses outputs of multiple ConvNets applied over multiple scales for semantic segmentation, but each ConvNet is learned independently without weight-sharing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] proposes a multi-scale ConvNet where outputs of all convolutional layers are fed to the classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], who train ConvNets over the Laplacian pyramid of images with tied weights for scene parsing problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The entire network is optimized jointly via stochastic gradient descent with gradients computed by backpropagation [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 10, "context": "This idea is motivated by the fact that similar patterns can appear anywhere in the 2D layout of pixels and that nearby values present strong dependencies in natural images [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "The convolution operation effectively ties the learned weights at multiple locations, which radically reduces the number of trainable parameters as compared to having different weights at each location [2].", "startOffset": 202, "endOffset": 205}, {"referenceID": 11, "context": "We implement our method using the opensource Caffe framework [12], and our code will be available online.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "Therefore, we experiment with the modified MNIST handwritten digit classification dataset introduced in [6] called MNIST-scale.", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "The network architecture is modeled after the ConvNets of [13] that achieve state-of-the-art on the original MNIST dataset, and we use the same pre-processing method and hyper-parameters unless otherwise noted.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "[10], Restricted Boltzmann Machine (RBM) and its scale-invariant version of Sohn et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Following the experimental protocol of [6], we train and test each network on 10,000 and 50,000 images respectively.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Method Test Error (%) on 6 train/test fold Restricted Boltzman Machine (RBM)[6] 6.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "1 Scale-invariant RBM [6] 5.", "startOffset": 22, "endOffset": 25}, {"referenceID": 12, "context": "5 Convolutional Neural Network [13] 3.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "23 Hierarchical ConvNets [10] 3.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "Note that in [13], ConvNet without dropout achieves state-of-the-art performance along with ConvNet with dropconnect.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "[1, 1] [0.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1, 1] [0.", "startOffset": 0, "endOffset": 6}], "year": 2014, "abstractText": "Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting.", "creator": "LaTeX with hyperref package"}}}