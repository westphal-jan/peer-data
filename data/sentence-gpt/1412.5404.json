{"id": "1412.5404", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts", "abstract": "The short text has been the prevalent format for information of Internet in recent decades, especially with the development of online social media, whose millions of users generate a vast number of short messages everyday. Although sophisticated signals delivered by the short text make it a promising source for topic modeling, its extreme sparsity and imbalance brings unprecedented challenges to conventional topic models like LDA and its variants. Aiming at presenting a simple but general solution for topic modeling in short texts, we present a word co-occurrence network based model named WNTM to tackle the sparsity and imbalance simultaneously. Different from previous approaches, WNTM models the distribution over topics for each word instead of learning topics for each document, which successfully enhance the semantic density of data space without importing too much time or space complexity. Meanwhile, the rich contextual information preserved in the word-word space also guarantees its sensitivity in identifying rare topics with convincing quality. Furthermore, employing the same Gibbs sampling with LDA makes WNTM easily to be extended to various application scenarios. Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods. And finally we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages.\n\n\nThe next step in designing our WNTM is to combine some of our current and future approaches in the LDA format, which also contains additional information. We hope that this book will help you in this effort to understand the importance of word co-occurrence.", "histories": [["v1", "Wed, 17 Dec 2014 14:18:52 GMT  (108kb,D)", "http://arxiv.org/abs/1412.5404v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yuan zuo", "jichang zhao", "ke xu"], "accepted": false, "id": "1412.5404"}, "pdf": {"name": "1412.5404.pdf", "metadata": {"source": "CRF", "title": "Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts", "authors": ["Yuan Zuo", "Jichang Zhao", "Ke Xu"], "emails": ["skywatcher.buaa@gmail.com", "jichang@buaa.edu.cn", "kexu@nlsde.buaa.edu.cn"], "sections": [{"heading": null, "text": "Keywords Word Co-occurrence Network \u00b7 Topic Modeling \u00b7 Short Texts \u00b7 Imbalanced Texts\nYuan Zuo State Key Lab of Software Development Environment, Beihang University E-mail: skywatcher.buaa@gmail.com\nJichang Zhao School of Economics and Management, Beihang University E-mail: jichang@buaa.edu.cn\nKe Xu State Key Lab of Software Development Environment, Beihang University E-mail: kexu@nlsde.buaa.edu.cn\nar X\niv :1\n41 2.\n54 04\nv1 [\ncs .C\nL ]\n1 7\nD ec\n2 01"}, {"heading": "1 Introduction", "text": "With the rapid development of World Wide Web and the spur of various kinds of web applications, short texts have been becoming the dominating content of Internet, such as web search snippets, micro-blogs (tweets), forum messages, and news titles. Specifically, around 250 million active users in Twitter generate almost 500 million tweets everyday, which carry sophisticated signals reflecting the real world. Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28]. However, due to the severe sparse context information, revealing topics from short texts is still a challenging problem for traditional frameworks that initially designed to handle the normal text. Meanwhile, short texts also suffer from significantly imbalanced document distribution. For example, in social media like Weibo, the amount of entertainment tweets is much larger than the number of ones in other categories [42,43,12]. Since the objective of most commonly used topic models is to maximize the probability of the observed data, they tend to sacrifice the performance on rare topics [18]. Consequently, those topic models may not perform well in extrinsic tasks [6].\nAs a canonical form of existing topic models, LDA [4] is a hierarchical parametric Bayesian approach for topic discovery in a large corpus. To be specific, LDA models documents as mixture of topics and each topic is a probability distribution over words in the vocabulary of the corpus. Statistical inference methods are then used to learn the probability distribution over words associated with each topic and the distribution over topics for each document. Generally speaking, LDA-like models group semantically related words into a single topic by utilizing document-level word co-occurrence information [39], which makes them extremely sensitive to the document length and the number of documents related to each topic. Since the short text contains low word counts, those models would fail to obtain the accurate picture of how words are related to each other. Moreover, if the distribution over documents for the topic is heavily skewed, LDA-like models tend to learn more general topics held by majority documents rather than rare topics contained only by fewer documents. A recent study from Tang et al. [36] suggests that if the distribution over documents for each topic is heavily skewed, identifying topics from a small number of documents will be extremely difficult for LDA. While in fact rare topics might be essential for newly emerging events discovery or real-time hot trends detection in online social media [8].\nMany efforts actually have been devoted to tackle the inefficiency of LDA in modeling short texts. For example, related short texts can be aggregated into lengthy pseudodocuments before training the topic model [40] or models trained from external data (e.g. Wikipedia) can be used to help the topic inference in short texts [27]. Besides, many arbitrary manipulations of LDA have also been introduced to satisfy the demands of specific short texts analyses [44,10,8]. Different from aforementioned approaches that are highly data-dependent or task-dependent, topic models focusing on the general-domain short text is emerging recently. A typical example is the biterm topic model [41], which works well on short texts. However, biterm topic model is a special form of the mixture of unigram and not based on LDA. Therefore it does not overcome the shortcomings of LDA-like approaches on short texts and it\u2019s flexibility is also greatly constrained.\nOn the other hand, with respect to the topic imbalance, performance improvement of LDA is mainly obtained by adding prior information to guide the topic learning progress [18, 1] or using asymmetric Dirichlet prior over the document-topic distribution [24]. While note that in practice, the knowledge about underlying structure of a given corpus is often undiscovered, so the prior information is not easily acquirable. As to asymmetric Dirichlet priors,\nhow to determine proper parameter estimations is sophisticated and scenario-dependent for different applications and assuming symmetric Dirichlet priors help most variants of LDA keep the flexibility. Therefore, alleviating topic imbalance of LDA with symmetric Dirichlet priors is actually quite desirable.\nTo sum up, approaches mentioned above are neither scenario independent nor easy to be extended. In order to handle the sparsity and imbalance of short texts simultaneously through a general framework, we propose Word Network Topic Model (WNTM) based on the word co-occurrence network. The main idea of WNTM comes from the following observations. 1) When texts are short, word-by-document space is extremely sparse, while word-word space is still rather dense. Since the topic quality can be guaranteed in the dense wordword space [2], we conjecture that learning topic components from word co-occurrence network rather than document collection is more reliable. 2) Intuitively, the number of words connected to rare topics often exceeds the amount of documents related to those topics. So the distribution over topics for words is greatly less skewed than the distribution over topics for documents. 3) Since the distribution over topics for each document can not be learned accurately in short or imbalanced texts, we should learn the distribution over topics for each word instead. 4) Different from the existing solutions, a new framework should be simple enough to guarantee its scalability in different application scenarios. Hence WNTM employs the standard Gibbs sampling for LDA to discover latent word group (i.e. topics) [15] and learns distribution over topics for words rather than topics for documents. Learning word\u2019s topics rather than document\u2019s topics makes WNTM less sensitive to the document length or heterogeneity of the topic distribution. In addition, the word co-occurrence network can be constructed with any type of given texts, which makes WNTM further simple and general in real-world applications.\nExtensive experiments are conducted on various data sets to compare WNTM and baseline methods in three aspects, including topic quality, word semantic similarity and document semantic classification. And results suggest that WNTM can discover the most coherent topics in short texts. Meanwhile, WNTM outperforms all baseline methods on word similarity and document categorization in both short and normal texts. Particularly, WNTM shows much better capability than LDA in rare topic detection in extremely imbalanced texts. Major contributions of this paper are 1) WNTM is a generative model for a word network rather than a collection of documents, 2) and it learns topics for each word rather than topics for each document, therefore 3) it is less sensitive to document length or document distribution over topics. 4) Since WNTM uses the standard Gibbs sampling for LDA, it\u2019s general and very easy to be applied in different scenarios.\nThe rest of the paper is organized as follows. We first give a short review of relative works in section 2. This is followed by detail introductions of our model and re-weighting method in section 3 and section 4. Experimental results are illustrated and explained in section 5. Finally, we conclude the present work briefly in section 6 and several possible directions in future are also pointed out."}, {"heading": "2 Related works", "text": "Probabilistic topic models such as PLSA [16] and LDA [4] have been extensively applied in exploring text corpora. Particularly, LDA is a more complete generative model since it extends PLSA by adding Dirichlet priors on topic distributions. Due to their extensibility, many complicated variants of LDA and PLSA have been proposed in the last decade, such as the dynamic topic model [3], social topic model [5], author-topic model [31] and author-\ntopic-community model [22] etc. While most of them are designed to handle normal texts with special additional properties, such as time, social relationship and authorship.\nThe sparse short texts has also attract much research interest in the previous literature and most early studies mainly focus on increasing data density through utilizing auxiliary information. For example, Hong et al. [17] train topic models on aggregated tweets that sharing the same word, and find those models work better than those being directly trained on original tweets. Sahami et al. [34] propose a search-snippet-based similarity measure for short texts. Jin et al. learn topics on short texts via transfer learning from auxiliary long text data [21]. Another way to deal with data sparsity in short texts is to apply special topic models. For example, Zhao et al. assume each tweet only covers a single topic [44]. Yan et al. [41] propose a special form of mixture of unigrams [26], which is called biterm topic model to improve topic modeling on short texts.\nWhile regarding to the topic modeling on imbalanced texts, the prior knowledge has been widely used to alleviate skewed distributions over documents of different topics. Andrzejewski et al. propose Dirichlet forest priors to incorporate must-links and cannot-links constraints into topic models [1]. Chen et al. use general lexical knowledge to help discovering coherent topics [9]. It is worthy noting that the document level knowledge can also be utilized. For example, Ramage et al. bring labels into the generative process of the corpora [30,33] and Blei et al propose a supervised topic model [23] to predict the category labels for input labeled documents.\nHowever, different from above approaches, we try to figure out a simple but general solution to take care of sparsity and imbalance in texts simultaneously. To the best of our knowledge, little attention has been paid to this issue and our proposed topic model is the first one handling short and imbalanced texts in a general way without exploiting any external knowledge."}, {"heading": "3 Word network topic model", "text": "Commonly used topic models implicitly take advantage of rich word co-occurrence patterns in documents. However, short texts naturally lack of enough contextual information. Furthermore, the goal of traditional topic models is to maximize the probability of generating observed documents, and rare topics that reflected by fewer documents are tended to be ignored. As a result, directly applying conventional topic models on short or imbalanced texts can not perform as well as that on normal balanced texts. In order to solve the problem mentioned above through a simple but general method, we propose a new framework, which applies the same Gibbs sampling [14] with LDA to discover latent word groups in a word co-occurrence network. Here latent word groups of the network are taken as topic components of a corpus. In addition, the distribution over latent groups for each word is also learned by our model. The details of the new framework is presented in the following subsections.\n3.1 Word co-occurrence network\nIn a word co-occurrence network (which may also be denoted as word network in the following text if there is no conflict), nodes are words occurring in the corpus and an edge between two words indicates that the connected two words have co-occurred in the same context at least once. Here the context can refer to a document or a sliding window with fixed size. To\nlimit the size of word network and reserve only the local context for each word, we take a sliding window of fixed size as the context in the present work. We empirically set the size of siding window to 10 (a typical value employed in the previous study) in this paper, since a word is only semantically related with adjacent words, especially in the short text. Degree of a node is defined as the sum of weights over its adjacent links. While activity of a node is defined as the averaged weight of its adjacent links.\nIn order to convert the given document collection into a word network, we first filter out stopping words and low frequency words, and then a sliding window is moved to scan each document. As the window scanning word by word through the document, any two distinct words appear in the same window would be regarded as co-occurred with each other. Times that two words co-occurred are accumulated and defined as the weight of the corresponding edge between them.\nNote that in topic models, a topic can be viewed as a bag of words co-occurred frequently in the same document, which is very similar to latent word groups (or communities) in the word network, since words co-occurred frequently in the same sliding windows are closely connected in the semantic space and they could appear in the same document with high probabilities. Therefore, we could take latent word groups in our word network based model as the topics in LDA. At the same time, learning topics from word co-occurrence network, a special form of word-word space, has a theoretical guarantee for topic coherence according to the work of Arora et al. [2]. What\u2019s more, rare topics may form compact latent word groups in word network, therefore a topic model based on the word network could effectively find word groups that correspond to rare topics. Based on the considerations above, we propose our word network topic model (WNTM). In order to keep the new model simple and general to employ at different scenarios, we take a similar approach as Keith et al. did in [15] to discover latent word groups in word network.\n3.2 Word network topic model (WNTM)\nThe standard Gibbs sampling for LDA can be used to discover latent word groups in large word network. While in order to reserve the standard Gibbs sampling, we first have to represent the word co-occurrence network back to a pseudo-document set. We assume the word network as undirected and weighted. As illustrated in Fig. 1, each word in the network can be treated as a pseudo-document with content constituted by the list of its adjacent words.\nObviously since the word network is weighted, the adjacent words may occur multiple times in the text of the pseudo-document.\nAlthough WNTM uses the same Gibbs Sampling with LDA, the rationalities underlying the generative process of them are different. LDA learns to generate a collection of documents by using topics and words under those topics. However, WNTM learns to generate each word\u2019s adjacent word-list in the network by using latent word groups and words belonging to those groups. More specifically, WNTM learns the statistical relations between words, latent word groups and words\u2019 adjacent word-lists by assuming that each word\u2019s adjacent word-list is generated semantically by a particular probabilistic model. It first supposes that there is a fixed set of latent word groups in the word network, and each latent word group z is associated with a multinomial distribution over the vocabulary \u03a6z, which is drawn from a Dirichlet prior Dir(\u03b2 ). The generative process of the whole pseudo-document collection converted from the word network can be interpreted as follows:\n1. For each latent word group z, draw \u03a6z \u223c Dir(\u03b2 ), a multinomial distribution over words for z 2. Draw \u0398i \u223c Dir(\u03b1), a latent word group distribution for the adjacent word-list Li of the word wi 3. For each word w j \u2208 Li: (a) select a latent word group z j \u223c\u0398i (b) select the adjacent word w j \u223c\u03a6z j\nIn WNTM, the \u0398 distributions represent the probability of latent word groups appearing in each word\u2019s adjacent word-list and the \u03a6 distributions stand for the probability of words belonging to each latent word group. Given the observed corpus, WNTM first converts it to a word network, then generate the pseudo-document set and finally the same Gibbs sampling implementation that developed for conventional LDA is employed to infer values of the latent variable in both \u03a6 and \u0398 . Because each word\u2019s adjacent word-list actually represents its global context information, so different from previous LDA-like approaches, WNTM models the distribution over latent word groups for each word instead of the distribution over topics for each document.\n3.3 Inferring topics in a document\nAs discussed in the previous section, WNTM does not model the document generation process. Therefore, we cannot directly obtain topics in a document from the result of Gibbs sampling. Since WNTM models the generation process of each word\u2019s adjacent word-list which stands for the word\u2019s global contextual information, we can take topic proportions of word wi\u2019s adjacent word-list \u0398i as topic proportions in wi. Given topic proportions for all words, topics of each document can be obtained accordingly. Specifically, to infer topics in a document, we assume that the expectation of the topic proportions of words generated by a document equals to the topic proportions of the document, i.e.,\nP(z|d) = \u2211 wi P(z|wi)P(wi|d), (1)\nwhere P(z|wi) equals to \u0398i,z, which has been learned in WNTM. As to P(wi|d), we simply take the empirical distribution of words in the document as a estimation, i.e.,\nP(wi|d) = nd(wi) Len(d) , (2)\nwhere nd(wi) is the word frequency of wi in document d and Len(d) is the length of d. It is worthy noting the above strategy is straight-forward and easy to implement, which guarantees WNTM\u2019s simplicity further.\nTo sum up, when texts are short and sparse, learning topics in word-by-document space will suffer from the severe sparsity problem, while learning topics in a word-word space has a theoretical guarantee for topic coherence, which has been proved in [2]. Meanwhile, as the distribution over documents for each topic is imbalanced, rare topics tend to be ignored by LDA-like models. However, we conjecture that words related to rare topics would still form a semantically compact latent group in the word co-occurrence network. So latent groups standing for rare topics could also be detected by WNTM. Therefore, the rich contextual information in word-word space facilitates WNTM to discover topics in word co-occurrence network other than directly reveal topics from document collection."}, {"heading": "4 Complexity analysis and word network re-weighting", "text": "Although our inference on WNTM uses the same Gibbs sampling with LDA, the running time and space complexities of them are different. We will compare the time complexity in detail first, and then give a brief discussion about the space complexity of the two models. Finally, In order to reduce the time and space complexity of WNTM, we further propose a method to perform word network re-weighting.\n4.1 Complexity analyses\nThe time complexity of LDA is O(NdKzLd), where Nd is the number of documents, Kz is the number of topics and Ld is the average document length. Similarly, the time complexity for WNTM is O(NpKgLp), where Np is the number of pseudo-documents, i.e., the size of the vocabulary, Kg is the number of latent word groups (topics) and Lp is the average pseudodocument length. Since the maximum number of sliding windows in a corpus is NdLd , and each sliding window can generate (c 2 ) edges, where c is the size of the sliding window. Thus, approximately NpLp can be rewritten as\nNpLp \u2248 NdLdc(c\u22121). (3)\nSupposing Kz equals to Kg, the time complexity of WNTM is o(c2) times larger than LDA\u2019s cost. In practice, for short texts, the average document length \u3008l\u3009 is often small than c. So when applied to short texts, the time complexity of WNTM is acceptable. However, with respect to normal texts, it becomes unacceptable since c can be set to a large number.\nThe space complexity of LDA is O(NdKz +NdLd) and the space complexity of WNTM is O(NpKg +NpLp). Similar to the time complexity, if we assume Nd is equal to Np and Kz is equal to Kg , then the memory WNTM consumes is o(c2) times of the size that LDA needs. In order to reduce the time (or space) complexity, for instance, to decrease the time (or space) complexity to linear times the cost of LDA, we would propose a word network re-weighting method in the next subsection, which could help boost the learning process of WNTM effectively.\n4.2 Word network re-weighting\nThe above analysis shows that the time and space complexity is unaffordable for Gibbs sampling over the pseudo-document collection, which is directly generated by the weighted word network. To reduce both the time and space consumption, we need to decrease NpLp. While Np is fixed as it is determined by the size of the given corpus\u2019s vocabulary, so Lp is the only tunable parameter. Representing the length of a pseudo-document, Lp also equals to the degree of the node corresponding to the pseudo-document. Therefore, decreasing the weights of edges in the network can shorten the length of pseudo-documents, and then reduce the time and space complexity of WNTM accordingly. In order to reserve the relative closeness of different words in the process of tuning weights, a re-weighting method is illustrated in Algorithm 1.\nAlgorithm 1 Word network re-weighting algorithm Require: the original word network G = (V,E,W ), where V is the set of words, E is the set of edges and W\nis the set of weights for edges. Ensure: the re-weighted word network G\u2032 = (V,E,W \u2032).\n1: compute degree D(n) and activity A(n) of each node n \u2208V 2: for all e = (n1,n2) \u2208 E do 3: set we = \u2308 we\nA(ni)\n\u2309 , argmin\ni {D(ni), i = 1,2}\n4: end for\nThe weight of each edge is divided by the activity of its end with lower degree, denoted as we. Along this line, we can decrease the whole weighted degree of the entire word network, which can then decrease the time and space complexity of WNTM. Since the weighted degree of a node must be larger than c\u2212 1, then the averaged weighted degree of the word network is actually much larger than c\u2212 1. Therefore, the averaged length of pseudo-documents should be smaller than Lpc\u22121 , where Lp is the averaged length before reweighting. Then from Eq. 3 we can easily get that the time and space cost of re-weighted WNTM is O(c) times the complexity of LDA, which is just a linear scale-up. Hence the reweighting algorithm successfully reduces the cost of WNTM and guarantees its feasibility in both short and normal texts."}, {"heading": "5 Experiments", "text": "In this paper, we evaluate our approach in three measures, including topic quality, word similarity and document classification. For each measure, extensive experiments are performed on real-world short texts and normal texts respectively. For short texts, We take LDA and biterm topic model (BTM) [41] as baseline methods. As to normal texts, we omit the comparison with BTM because of its intense time complexity when applied on normal texts. what is worth mentioning is that the comparison between WNTM and LDA can indicate the strengths and weaknesses of learning topics from document collection and word co-occurrence network.\nMost experiments in this section are carried out on a Windows Server with an Intel Xeon 2.40GHz CPU and 12G memory except for the experiments using Wikipedia data set. Due to the large volume of the Wikipedia data set, corresponding experiments are conducted on\na Linux cluster with 13 nodes. Each node contains 2 Intel Xeon 2.27Hz CPUs and 12 GB memory. For both LDA and WNTM, we use a java open-source implementation JGibbLDA1 on short texts and an MPI open-source implementation titled PLDA2 on normal texts. For BTM, we use the source code opened by the authors3. For JGibbLDA and BTM, we set \u03b1 = 50/k and \u03b2 = 0.01, where k is the number of topics. For PLDA, we set \u03b1 = 0.1 and \u03b2 = 0.01. In all experiments, number of topics is set to 100, length of the sliding window for WNTM is set to 10 and each model\u2019s Gibbs sampling is run for 2,000 iterations. Except for document classification on news contents, the results reported here are the average over 10 rounds.\n5.1 Evaluation of the topic quality\nIt\u2019s a typical way for evaluating topic models through comparing the perplexity on a held-out test set. However, WNTM does not model the generation process of documents. Hence, the perplexity is not suitable in this paper. Furthermore, recent research shows that the perplexity does not always correlate with semantically interpretable topics [6]. Therefore, here we utilize the topic coherence as an evaluation metric for the topic discovery, which has been found to correlate well with human judgments of the topic quality."}, {"heading": "5.1.1 Topic coherence", "text": "Topic coherence (also called UMass measure [25]) is a comprehensive and automated evaluation measure for topic models, which measures the score of a single topic by computing the semantic similarity degree between high probability words in the topic. Higher topic coherence often indicates better topic quality, i.e., better topic interpret-ability. The topic coherence is defined as\nC(z;M(z)) = T\n\u2211 t=2\nt\u22121 \u2211 l=1 log D(m(z)t ,m (z) l )+ \u03b5 D(m(z)l ) , (4)\nwhere M(z) = (m(z)1 , ...,m (z) T ) is the list of the T most probable words in topic z, D(m) counts the number of documents containing the word m, D(m,m\u2032) counts the number of documents containing both m and m\u2032, and \u03b5 = 10\u221212 is used to avoid taking the log of zero for words that never co-occur and to smooth the score for completely unrelated words. We use the average coherence score of all topics as the evaluation metric for topic quality of different topic models."}, {"heading": "5.1.2 Topic coherence on short texts", "text": "To investigate WNTM\u2019s ability of learning high quality topics from real-world short texts, we carry out experiments on one day\u2019s micro-blogs4 sampled from Weibo. As a Twitter-like service in China, it also imposes a limited length for each tweet, i.e., no more than 140 Chinese characters. Since the textual content of micro-blogs is not formal, careful preprocessing\n1 http://jgibblda.sourceforge.net/ 2 http://code.google.com/p/plda/ 3 http://code.google.com/p/btm/ 4 Publicly available at http://ipv6.nlsde.buaa.edu.cn/zhaojichang/paper/wntm.rar\nis quite necessary. In the preprocessing, we take the following steps to wash the collected corpus: (a) using NLPIR5 to do tokenization; (b) removing stopping words; (c) removing words with frequency less than 20; (d) filtering out URLs and non-Chinese characters; (e) removing micro-blogs with length less than 10. Finally, 189,223 micro-blogs retained with 20,942 distinct words in total. The average number of tokens in documents is 17.2.\nWe compare WNTM with LDA and BTM on this micro-blog collection. For all models, we set the number of topics to 100. The average topic coherence of three models is listed in Table 1, where the size of top words set in each topic, denoted as T , ranges from 5 to 20. We find that the average topic coherence of WNTM is obviously higher than other two models, which indicates that WNTM outperforms LDA and BTM in learning high quality topics from short texts. And the improvement made by WNTM is statistically significant (p-value < 0.001 by t-test). The outperformance of WNTM as compared to LDA is in accordance with our understanding that learning topics from dense word-word space can guarantee the topic quality even in short texts. BTM also outperforms LDA since it directly model word pairs rather than documents to solve data sparsity in short texts. Remarkably, WNTM outperforms BTM significantly. WNTM is based on the framework of LDA and BTM is special form mixture of unigram, which might be the reason why WNTM works better than BTM on short texts. What\u2019s more, we also notice that as T grows, the performance gap increases and WNTM is more stable by possessing much lower deviations."}, {"heading": "5.1.3 Topic coherence on normal texts", "text": "We conduct experiments on Wikipedia data provided by Phan et al. [27] to investigate WNTM\u2019s ability of learning high quality topics from real-world normal texts. The Wikipedia data set contains 71,986 documents with 60,649 distinct words. The average number of tokens in documents is 423.5. The number of topics is set to 100 for each model and the average topic coherence result is listed in Table 2, where the size of top words set T in each topic ranges from 5 to 20. As shown in Table 2, the average topic coherence of WNTM is slightly higher than that of LDA when T = 5 and 10, while slightly lower than that of LDA when T = 20. From the results, we can see that learning topics by grouping words co-occurred in a small range of context can benefit the top 10 words\u2019 coherence in each topic, but when T = 20, the coherence of top words might need more plentiful documentlevel word co-occurrence information to maintain. However, with the increasing of distance between two words, the relation between them becomes less relevant. Because of this, the difference between WNTM and LDA is not likely to be obvious. According to the results of t-test, two models gain no statistically significant improvement than each other. Therefore, we can conclude that WNTM can produce similar high-quality topics as LDA dose on normal texts. Note that the word-by-document space has no sparsity problem in normal texts, so\n5 http://ictclas.nlpir.org/downloads\nLDA can utilize the rich contextual information in each document to learn high quality topics. Thus, for normal texts, learning topics from word-by-document space and word-word space makes little difference in topic quality.\n5.2 Word similarity tasks\nThe average topic coherence is an intrinsic measure used to evaluate the quality of all topics. Higher topic coherence often indicates better topic quality, but it does not guarantee a better performance on extrinsic tasks. The study of Keith et al. [15] revealed that LDA is better than LSA (Latent Semantic Analysis) [11] at learning descriptive topics, while LSA is better than LDA at creating a compact semantic representation of words and documents and outperforms LDA in extrinsic tasks [35]. Hence, we carry out experiments to compare the performance of WNTM and other methods on extrinsic tasks such as word similarity tasks and document classification, which would help further illustrate the two models\u2019 effectiveness in creating compact semantic representation of words and documents. In this section, we compare two models\u2019 ability of learning semantic representation of words on short and normal texts, respectively. To begin with, we will introduce how to calculate the semantical similarity between two words."}, {"heading": "5.2.1 Semantic representation of a word", "text": "For LDA and BTM, the conditional topic distribution for a word w can be defined as its semantic representation\nsw = [p(z1|w), p(z2|w), ..., p(zk|w)],\nwhere k is the number of topics. p(zk|w) is easy to obtain after Gibbs sampling stage is completed,\np(zk|w) = nw|zk nw , (5)\nwhere nw|zk stands for how many times w has be assigned with topic k during the sampling and nw means the total occurrence of w in given corpus. With respect to WNTM, we do not have to calculate p(zk|w). The \u03b8w can be directly used as the semantic representation of word w, since WNTM learns the topic distribution over words as matrix \u0398 . Therefore, we use \u0398 \u2019s row vector \u03b8w corresponding to w as its semantic representation, which is denoted as\nsw = \u03b8w = [\u0398w,1,\u0398w,2, ...,\u0398w,k].\nThen we can measure the distance of two words by use of the Jensen-Shannon divergence\nJS(si,s j) = 1 2 Dkl(s j \u2016 m)+ 1 2 Dkl(si \u2016 m), (6)\nwhere si and s j are the semantic representations of words i and j, m = 12 (si + s j) and Dkl(p||q) = \u2211i pi ln piqi is the Kullback-Leibler divergence. If we consider the topic distributions as space vectors, cosine similarity can also be used to measure the distance of two words. It can be defined as\nCosine(si,s j) = si \u00b7 s j\n\u2016 si \u2016\u2016 s j \u2016 . (7)"}, {"heading": "5.2.2 Word similarity tasks", "text": "Word similarity tasks are widely used to evaluate distributional semantic spaces. Topics learned by topic models can be viewed as the knowledge about semantic distributions of words. If a topic model learns topics accurately, then we can expect similar words, such as \u201cman\u201d and \u201cwoman\u201d, to be represented with similar semantic representations. In this paper, we use the word similarity task presented by Wang et al. [38] to evaluate the ability of word semantic modeling of two models on Weibo data set. Regarding to normal texts, we use word similarity tasks designed by Rubenstein and Goodenough in [32] and Finkelstein et al. in [13] on Wikipedia data set. In each task, the semantic similarity between given pairs of words were evaluated by human. The word similarity task introduced by Wang et al. contains 240 pairs of Chinese words and each pair\u2019s semantic relatedness is rated from 0-10, in which a higher score reflects a more semantically similar word pair. The task introduced by Finkelstein was constituted by 353 pairs of English words and Rubenstein and Goodenough\u2019s task contains 65 English words. Each pair was also given a human rate indicating the pair\u2019s semantic closeness.\nWe evaluate topic models by calculating the similarity between each pair of words through two similarity measurements (JS is short for Jensen-Shannon divergence and Cosine is short for Cosine similarity) in the evaluate set and then compare the model\u2019s ratings with human ratings by the ranked correlation. Intuitively higher correlation indicates better word semantic modeling. The number of topics is set to 100 for both models and the ranked correlation result on Weibo data is illustrated in Fig. 2.\nFrom the result, we can see that both WNTM and BTM outperforms LDA significantly, no matter using JS as the similarity measure or Cosine. WNTM performs similar with BTM on JS, but outperforms BTM significantly on Cosine. Through directly modeling word pairs other than documents, BTM successfully avoid the document-level data sparsity\nissue. WNTM also directly model word co-occurrences to alleviate document-level data sparsity. Therefore, BTM and WNTM can learn more accurate word semantic representations than LDA. However, WNTM apply the framework of LDA to model word\u2019s context extracted from word network, therefore, the model assumption of WNTM is more accordance with actual data circumstance as compared to BTM. Thus although both WNTM and BTM surpass LDA in learning word semantic representations, WNTM is more reliable than BTM.\nThe result of Wikipedia data is illustrated in Fig. 3. We surprisingly find that WNTM still outperforms LDA on both similarity measurements. The performance gap between two models on normal texts shrinks compared to the result on short texts, since the documentlevel data sparsity problem is gone. However, WNTM\u2019s result is more stable than LDA by possessing much lower deviations. Although sharing similar topic coherence results in normal texts, the two models\u2019 ability in creating compact semantic representation for words are different and WNTM performs better than LDA in word similarity tasks.\n5.3 Document classification\nIn order to compare these models\u2019 ability in learning semantic representation of documents on short and normal texts, we evaluate them by performing document classifications on news titles and news content respectively in this section. To illustrate WNTM\u2019s advantage on imbalanced texts as compared to LDA, we further conduct classification experiments by tuning the heterogeneity of topic distributions."}, {"heading": "5.3.1 Evaluation on news corpus", "text": "To explore the effectiveness of WNTM in document semantic modeling on short texts and normal texts, we first evaluate its performance on document classification tasks of news titles and news content, which are extracted from the news corpus provided by Sogou.com6. After preprocessing, we obtain 508,554 news titles with label distributions listed in Table 3, and 59,348 distinct words in total. The average number of token in news titles is 5.5. We also\n6 http://www.sogou.com/labs/dl/ca.html\nobtain 118,705 news reports with label distributions listed in Table 3, and 76,114 distinct words in total. The average number of token in each report is 175.9.\nTaking topic model as a method of dimensionality reduction, we can reduce a document into a fixed set of topics, which can be features for document categorization. For each topic model trained on 100 topics, we perform 10-fold cross-validation on news titles (or content). In each fold, we randomly split news titles (or content) into training and test subsets with the ratio 9:1, and classified them by using LIBLINEAR7. The weighted average accuracy (precision), recall and F-measure for news titles are shown in Fig. 4(a). The result for news content is shown in Fig. 4(b).\nFrom the result, we can find that WNTM and BTM outperforms LDA in classifying short texts and normal texts. The divergence in the performance of classifying short texts suggests that the data sparsity problem seriously affects LDA, while less variation is found in WNTM and BTM. WNTM performs better than BTM, similar to word similarity task. Regarding to normal texts, LDA\u2019s classification result becomes acceptable since the data is no longer sparse. However, WNTM still outperforms LDA, which indicates that WNTM is better than LDA in recognizing the resemblance of documents even for normal texts.\nFig. 5 shows the confusion matrix of LDA and WNTM in the news contents classification. For example, LDA confuses \u201cHouse\u201d with \u201cSports\u201d, \u201cFinance\u201d and \u201cLady\u201d, where\n7 http://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\u201cHouse\u201d and \u201cFinance\u201d may be hard to distinguish because they are strongly relevant, while obviously \u201cHouse\u201d has few connections with \u201cSports\u201d and \u201cLady\u201d. Consequently, LDA almost misclassifies all documents under label \u201cHouse\u201d. The similar situation also happens to \u201cCulture\u201d and \u201cHealth\u201d. As listed in Table 3, the significant confusion of those labels mainly comes from their low numbers of documents and in fact they represent exactly the rare topics in the employed corpus. However, as shown in Fig. 5(b), WNTM still achieves a relative better performance as compared to LDA. It indicates that WNTM gains outstanding improvement on the performance of identifying rare labels, which evidently justifies the conjecture that WNTM can identify more rare topics than LDA as the text is imbalanced."}, {"heading": "5.3.2 Document classification on imbalanced texts", "text": "To compare performance between WNTM and LDA in classifying documents containing rare topics further, we investigate the variation of their classification results by continuously tuning the imbalance of texts. We first build a balanced data set from news content introduced previously. This balanced data set includes news from five classes, namely \u201cEducation\u201d, \u201cCar\u201d, \u201cFinance\u201d, \u201cLady\u201d and \u201cEnt\u201d. Within each class, there are 1,000 documents allocated equally. Second, we take various numbers of documents away from \u201cCar\u201d, which is randomly selected, to build data sets with different levels of imbalance. Specifically, we build 8 groups of imbalanced data sets, each group contains 1,000 documents belong to classes except \u201cCar\u201d, which contains dc documents, where dc is tunable parameter and we let dc = 800, 600, 400, 200, 100, 80, 60 and 40. As dc ranges from 800 to 40, the imbalance of data set is enhanced accordingly. To avoid the influence of the randomness in taking away documents from \u201cCar\u201d, we randomly sample documents under \u201cCar\u201d 15 times for each group. The average precision and recall on classifying \u201cCar\u201d documents is illustrated in Fig. 6. From the result, we can find that the average precision of LDA first slightly increases when the number of news under \u201cCar\u201d reducing from 800 to 200, and then decreases dramatically due to the more and more serious imbalance of the data set. On the contrary, the average precision of WNTM decreases slowly with the enhancement of the imbalance. Particularly, when the number of \u201cCar\u201d documents equals to 60 and 40, the average precision of LDA has decreased to 86% and 72% while WNTM\u2019s average precision is 90% and 85%. This result indicates that WNTM is more accurate than LDA in distinguishing documents under rare labels. Note that as dc varies in the range between 200 and 500, the averaged precision of LDA is trivially greater than that of WNTM, which is caused by fluctuations and would not affect our above analysis. From results of average recall, we can find that WNTM and LDA have similar trending with the imbalance enhancement, while WNTM always outperforms LDA in terms of the average recall. To sum up, we can conclude that WNTM performs better than LDA in dealing with text classification on imbalanced data sets.\nFrom the above experiments, we find WNTM dominates LDA in learning rare topics. Thus it is reasonable to conjecture that WNTM could be a better choice than LDA in detect-\ning newly emerging topics or unexpected events at early stages in social media, like Twitter or Weibo. To illustrate this point, we collect 10,000 micro-blogs from Weibo, and then inject dm micro-blogs related to the event of \u201cMH370\u201d into them, where dm ranges from 10 to 100. Note that the 10,000 micro-blogs have no relation with \u201cMH370\u201d since they are posted exactly before the event happens. For different settings of dm, we train both models 10 times since the result of Gibbs sampling varies each time. After training, we look over the top 20 words of each topic to search the topic word \u201cMH370\u201d, and count how many times it is found for each setting of dm. From the results we find that WNTM can always identify a topic with word \u201cMH370\u201d contained by its top 20 word-list when dm \u2265 30. However, LDA achieves the same result only as dm \u2265 50. Thus, WNTM is more sensitive to rare topics and can learn newly emerging topics at earlier stages than LDA. Besides, we also evaluate the quality of the topic related to \u201cMH370\u201d by human review. If a topic\u2019s top words contains more \u201cM7370\u201d related words than others, we evaluate this topic as the best. We list top 15 words of the best \u201cMH370\u201d topic learned by two models for comparison when dm = 50 and 100. It is interesting that when dm = 50, the best \u201cMH370\u201d topic learned by LDA contains 8 words that never appear in the 50 injected \u201cMH370\u201d micro-blogs, while WNTM only has 4. When dm = 100, LDA\u2019s \u201cMH370\u201d topic still contains 4 unrelated words, while WNTM only has 1 error word left. Based on these results, we can conclude that WNTM can detect event topic earlier than LDA and the quality of the topic is much better than that identified by LDA."}, {"heading": "6 Conclusions", "text": "A simple but general approach named WNTM is presented in this paper to facilitate the topic modeling in short and imbalanced texts at acceptable cost. Different from conventional LDA-like solutions, it explores topics from word co-occurrence networks and successfully alleviates the data sparsity and the topic-document heterogeneity in word-by-document space. Thorough experiments on both short and normal texts suggest that WNTM outperforms baseline methods in tasks of topic coherence, word similarity and document classification. Furthermore, the ability of capturing rare topics with high quality indicates that WNTM could be an effective model for detecting newly emerging topics or unexpected events in social media at quite early stages. However, promising new research directions still exist for further study. For example, we would like to investigate the influence imposed on topic results as different means of establishing word co-occurrence networks are taken. Besides, we may also consider of using a word\u2019s neighbor words within a given semantic distance to model its topics.\nAcknowledgements This work was supported by 863 Program (Grant No. 2012AA011005) and Research Fund for the Doctoral Program of Higher Education of China (Grant No. 20111102110019). Jichang Zhao was partially supported by 863 Program (Grant No. 2014AA015203) and the Fundamental Research Funds for the Central Universities (Grant Nos. YWF-14-RSC-109 and YWF-14-JGXY-001)."}], "references": [{"title": "Incorporating domain knowledge into topic modeling via dirichlet forest priors", "author": ["D. Andrzejewski", "X. Zhu", "M. Craven"], "venue": "ICML, pp. 25\u201332", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D.M. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "ICML, vol. 28 (2), pp. 280\u2013288. JMLR: W&CP", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "ICML, pp. 113\u2013120", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res. 3, 993\u20131022", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Social-network analysis using topic models", "author": ["Y. Cha", "J. Cho"], "venue": "SIGIR, pp. 565\u2013574", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J. Chang", "S. Gerrish", "C. Wang", "J.L. Boyd-graber", "D.M. Blei"], "venue": "Advances in Neural Information Processing Systems 22, pp. 288\u2013296", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Short text classification improved by learning multi-granularity topics", "author": ["M. Chen", "X. Jin", "D. Shen"], "venue": "IJCAI, pp. 1776\u20131781", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Emerging topic detection for organizations from microblogs", "author": ["Y. Chen", "H. Amiri", "Z. Li", "T.S. Chua"], "venue": "SIGIR, pp. 43\u201352", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering coherent topics using general knowledge", "author": ["Z. Chen", "A. Mukherjee", "B. Liu", "M. Hsu", "M. Castellanos", "R. Ghosh"], "venue": "CIKM, pp. 209\u2013218", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic summarization of events from social media", "author": ["F.C.T. Chua", "S. Asur"], "venue": "ICWSM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science 41(6), 391\u2013407", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1990}, {"title": "Topic dynamics in weibo: Happy entertainment dominates but angry finance is more periodic", "author": ["R. Fan", "J. Zhao", "X. Feng", "K. Xu"], "venue": "Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2014 IEEE/ACM International, pp. 230\u2013233", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Trans. Inf. Syst. 20(1), 116\u2013131", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Web:http://www.arbylon.net/publications/textest.pdf", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Applying latent dirichlet allocation to group discovery in large graphs", "author": ["K. Henderson", "T. Eliassi-Rad"], "venue": "SAC, pp. 1456\u20131461", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR, pp. 50\u201357", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Empirical study of topic modeling in twitter", "author": ["L. Hong", "B.D. Davison"], "venue": "SOMA, pp. 80\u201388", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Incorporating lexical priors into topic models", "author": ["J. Jagarlamudi", "H. Daum\u00e9 III", "R. Udupa"], "venue": "EACL, pp. 204\u2013213", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Personalized query suggestion with diversity awareness", "author": ["D. Jiang", "K.T. Leung", "J. Vosecky", "W. Ng"], "venue": "ICDE, pp. 400\u2013411", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast topic discovery from web search streams", "author": ["D. Jiang", "K.W.T. Leung", "W. Ng"], "venue": "Proceedings of the 23rd International Conference on World Wide Web, WWW, pp. 949\u2013960. ACM, New York, NY, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Transferring topical knowledge from auxiliary long texts for short text clustering", "author": ["O. Jin", "N.N. Liu", "K. Zhao", "Y. Yu", "Q. Yang"], "venue": "CIKM, pp. 775\u2013784", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "The author-topic-community model for author interest profiling and community discovery", "author": ["C. Li", "W. Cheung", "Y. Ye", "X. Zhang", "D. Chu", "X. Li"], "venue": "Knowledge and Information Systems pp. 1\u201325", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised topic models", "author": ["J.D. Mcauliffe", "D.M. Blei"], "venue": "J. Platt, D. Koller, Y. Singer, S. Roweis (eds.) Advances in Neural Information Processing Systems 20, pp. 121\u2013128", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Rethinking lda: Why priors matter", "author": ["A. McCallum", "D.M. Mimno", "H.M. Wallach"], "venue": "Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, A. Culotta (eds.) NIPS, pp. 1973\u20131981. Curran Associates, Inc.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimizing semantic coherence in topic models", "author": ["D. Mimno", "H.M. Wallach", "E. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP, pp. 262\u2013272", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Text classification from labeled and unlabeled documents using em", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Mach. Learn. 39(2-3), 103\u2013134", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning to classify short and sparse text & web with hidden topics from large-scale data collections", "author": ["X.H. Phan", "L.M. Nguyen", "S. Horiguchi"], "venue": "WWW, pp. 91\u2013100", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Short text similarity based on probabilistic topics", "author": ["X. Quan", "G. Liu", "Z. Lu", "X. Ni", "L. Wenyin"], "venue": "Knowledge and Information Systems 25(3), 473\u2013491", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Characterizing microblogs with topic models", "author": ["D. Ramage", "S. Dumais", "D. Liebling"], "venue": "ICWSM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "EMNLP, pp. 248\u2013256", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "UAI, pp. 487\u2013494", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM 8(10), 627\u2013633", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1965}, {"title": "Statistical topic models for multi-label document classification", "author": ["T. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Machine Learning 88(1-2), 157\u2013208", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "A web-based kernel function for measuring the similarity of short text snippets", "author": ["M. Sahami", "T.D. Heilman"], "venue": "WWW, pp. 377\u2013386", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Exploring topic coherence over many models and many topics", "author": ["K. Stevens", "P. Kegelmeyer", "D. Andrzejewski", "D. Buttler"], "venue": "EMNLP-CoNLL, pp. 952\u2013961", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the limiting factors of topic modeling via posterior contraction analysis", "author": ["J. Tang", "Z. Meng", "X. Nguyen", "Q. Mei", "M. Zhang"], "venue": "ICML, pp. 190\u2013198", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Tcs: Efficient topic discovery over crowd-oriented service data", "author": ["Y. Tong", "C.C. Cao", "L. Chen"], "venue": "KDD, pp. 861\u2013870. ACM, New York, NY, USA", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing semantic relatedness using chinese wikipedia links and taxonomy", "author": ["X. Wang", "Y. Jia", "B. Zhou", "Z. Ding", "L. Zheng"], "venue": "Journal of Chinese Computer Systems 32(11), 2237\u20132242", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Topics over time: A non-markov continuous-time model of topical trends", "author": ["X. Wang", "A. McCallum"], "venue": "KDD, pp. 424\u2013433", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Twitterrank: Finding topic-sensitive influential twitterers", "author": ["J. Weng", "E.P. Lim", "J. Jiang", "Q. He"], "venue": "WSDM, pp. 261\u2013270", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "A biterm topic model for short texts", "author": ["X. Yan", "J. Guo", "Y. Lan", "X. Cheng"], "venue": "WWW, pp. 1445\u20131456", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "What trends in chinese social media", "author": ["L. Yu", "S. Asur", "B.A. Huberman"], "venue": "arXiv preprint arXiv:1107.3522", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamics of trends and attention in chinese social media", "author": ["L.L. Yu", "S. Asur", "B.A. Huberman"], "venue": "arXiv preprint arXiv:1312.0649", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparing twitter and traditional media using topic models", "author": ["W.X. Zhao", "J. Jiang", "J. Weng", "J. He", "E.P. Lim", "H. Yan", "X. Li"], "venue": "ECIR, pp. 338\u2013349", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to suggest questions in social media", "author": ["T. Zhou", "M.T. Lyu", "I. King", "J. Lou"], "venue": "Knowledge and Information Systems pp. 1\u201328", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 124, "endOffset": 134}, {"referenceID": 36, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 124, "endOffset": 134}, {"referenceID": 19, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 124, "endOffset": 134}, {"referenceID": 18, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 153, "endOffset": 160}, {"referenceID": 44, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 153, "endOffset": 160}, {"referenceID": 6, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 186, "endOffset": 189}, {"referenceID": 20, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 210, "endOffset": 217}, {"referenceID": 27, "context": "Hence accurately mining topics behind these short texts are essential for a wide range of tasks, including content analysis [29,37,20], query suggestion [19,45], document classification [7] and text clustering [21,28].", "startOffset": 210, "endOffset": 217}, {"referenceID": 41, "context": "For example, in social media like Weibo, the amount of entertainment tweets is much larger than the number of ones in other categories [42,43,12].", "startOffset": 135, "endOffset": 145}, {"referenceID": 42, "context": "For example, in social media like Weibo, the amount of entertainment tweets is much larger than the number of ones in other categories [42,43,12].", "startOffset": 135, "endOffset": 145}, {"referenceID": 11, "context": "For example, in social media like Weibo, the amount of entertainment tweets is much larger than the number of ones in other categories [42,43,12].", "startOffset": 135, "endOffset": 145}, {"referenceID": 17, "context": "Since the objective of most commonly used topic models is to maximize the probability of the observed data, they tend to sacrifice the performance on rare topics [18].", "startOffset": 162, "endOffset": 166}, {"referenceID": 5, "context": "Consequently, those topic models may not perform well in extrinsic tasks [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "As a canonical form of existing topic models, LDA [4] is a hierarchical parametric Bayesian approach for topic discovery in a large corpus.", "startOffset": 50, "endOffset": 53}, {"referenceID": 38, "context": "Generally speaking, LDA-like models group semantically related words into a single topic by utilizing document-level word co-occurrence information [39], which makes them extremely sensitive to the document length and the number of documents related to each topic.", "startOffset": 148, "endOffset": 152}, {"referenceID": 35, "context": "[36] suggests that if the distribution over documents for each topic is heavily skewed, identifying topics from a small number of documents will be extremely difficult for LDA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "While in fact rare topics might be essential for newly emerging events discovery or real-time hot trends detection in online social media [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 39, "context": "For example, related short texts can be aggregated into lengthy pseudodocuments before training the topic model [40] or models trained from external data (e.", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "Wikipedia) can be used to help the topic inference in short texts [27].", "startOffset": 66, "endOffset": 70}, {"referenceID": 43, "context": "Besides, many arbitrary manipulations of LDA have also been introduced to satisfy the demands of specific short texts analyses [44,10,8].", "startOffset": 127, "endOffset": 136}, {"referenceID": 9, "context": "Besides, many arbitrary manipulations of LDA have also been introduced to satisfy the demands of specific short texts analyses [44,10,8].", "startOffset": 127, "endOffset": 136}, {"referenceID": 7, "context": "Besides, many arbitrary manipulations of LDA have also been introduced to satisfy the demands of specific short texts analyses [44,10,8].", "startOffset": 127, "endOffset": 136}, {"referenceID": 40, "context": "A typical example is the biterm topic model [41], which works well on short texts.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "On the other hand, with respect to the topic imbalance, performance improvement of LDA is mainly obtained by adding prior information to guide the topic learning progress [18, 1] or using asymmetric Dirichlet prior over the document-topic distribution [24].", "startOffset": 171, "endOffset": 178}, {"referenceID": 0, "context": "On the other hand, with respect to the topic imbalance, performance improvement of LDA is mainly obtained by adding prior information to guide the topic learning progress [18, 1] or using asymmetric Dirichlet prior over the document-topic distribution [24].", "startOffset": 171, "endOffset": 178}, {"referenceID": 23, "context": "On the other hand, with respect to the topic imbalance, performance improvement of LDA is mainly obtained by adding prior information to guide the topic learning progress [18, 1] or using asymmetric Dirichlet prior over the document-topic distribution [24].", "startOffset": 252, "endOffset": 256}, {"referenceID": 1, "context": "Since the topic quality can be guaranteed in the dense wordword space [2], we conjecture that learning topic components from word co-occurrence network rather than document collection is more reliable.", "startOffset": 70, "endOffset": 73}, {"referenceID": 14, "context": "topics) [15] and learns distribution over topics for words rather than topics for documents.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "Probabilistic topic models such as PLSA [16] and LDA [4] have been extensively applied in exploring text corpora.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "Probabilistic topic models such as PLSA [16] and LDA [4] have been extensively applied in exploring text corpora.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "Due to their extensibility, many complicated variants of LDA and PLSA have been proposed in the last decade, such as the dynamic topic model [3], social topic model [5], author-topic model [31] and author-", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Due to their extensibility, many complicated variants of LDA and PLSA have been proposed in the last decade, such as the dynamic topic model [3], social topic model [5], author-topic model [31] and author-", "startOffset": 165, "endOffset": 168}, {"referenceID": 30, "context": "Due to their extensibility, many complicated variants of LDA and PLSA have been proposed in the last decade, such as the dynamic topic model [3], social topic model [5], author-topic model [31] and author-", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": "topic-community model [22] etc.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "[17] train topic models on aggregated tweets that sharing the same word, and find those models work better than those being directly trained on original tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] propose a search-snippet-based similarity measure for short texts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "learn topics on short texts via transfer learning from auxiliary long text data [21].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "assume each tweet only covers a single topic [44].", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "[41] propose a special form of mixture of unigrams [26], which is called biterm topic model to improve topic modeling on short texts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[41] propose a special form of mixture of unigrams [26], which is called biterm topic model to improve topic modeling on short texts.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "propose Dirichlet forest priors to incorporate must-links and cannot-links constraints into topic models [1].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "use general lexical knowledge to help discovering coherent topics [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 29, "context": "bring labels into the generative process of the corpora [30,33] and Blei et al propose a supervised topic model [23] to predict the category labels for input labeled documents.", "startOffset": 56, "endOffset": 63}, {"referenceID": 32, "context": "bring labels into the generative process of the corpora [30,33] and Blei et al propose a supervised topic model [23] to predict the category labels for input labeled documents.", "startOffset": 56, "endOffset": 63}, {"referenceID": 22, "context": "bring labels into the generative process of the corpora [30,33] and Blei et al propose a supervised topic model [23] to predict the category labels for input labeled documents.", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "In order to solve the problem mentioned above through a simple but general method, we propose a new framework, which applies the same Gibbs sampling [14] with LDA to discover latent word groups in a word co-occurrence network.", "startOffset": 149, "endOffset": 153}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "did in [15] to discover latent word groups in word network.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "To sum up, when texts are short and sparse, learning topics in word-by-document space will suffer from the severe sparsity problem, while learning topics in a word-word space has a theoretical guarantee for topic coherence, which has been proved in [2].", "startOffset": 249, "endOffset": 252}, {"referenceID": 40, "context": "For short texts, We take LDA and biterm topic model (BTM) [41] as baseline methods.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "Furthermore, recent research shows that the perplexity does not always correlate with semantically interpretable topics [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 24, "context": "Topic coherence (also called UMass measure [25]) is a comprehensive and automated evaluation measure for topic models, which measures the score of a single topic by computing the semantic similarity degree between high probability words in the topic.", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "[27] to investigate WNTM\u2019s ability of learning high quality topics from real-world normal texts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] revealed that LDA is better than LSA (Latent Semantic Analysis) [11] at learning descriptive topics, while LSA is better than LDA at creating a compact semantic representation of words and documents and outperforms LDA in extrinsic tasks [35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[15] revealed that LDA is better than LSA (Latent Semantic Analysis) [11] at learning descriptive topics, while LSA is better than LDA at creating a compact semantic representation of words and documents and outperforms LDA in extrinsic tasks [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "[15] revealed that LDA is better than LSA (Latent Semantic Analysis) [11] at learning descriptive topics, while LSA is better than LDA at creating a compact semantic representation of words and documents and outperforms LDA in extrinsic tasks [35].", "startOffset": 243, "endOffset": 247}, {"referenceID": 37, "context": "[38] to evaluate the ability of word semantic modeling of two models on Weibo data set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Regarding to normal texts, we use word similarity tasks designed by Rubenstein and Goodenough in [32] and Finkelstein et al.", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "in [13] on Wikipedia data set.", "startOffset": 3, "endOffset": 7}], "year": 2014, "abstractText": "The short text has been the prevalent format for information of Internet in recent decades, especially with the development of online social media, whose millions of users generate a vast number of short messages everyday. Although sophisticated signals delivered by the short text make it a promising source for topic modeling, its extreme sparsity and imbalance brings unprecedented challenges to conventional topic models like LDA and its variants. Aiming at presenting a simple but general solution for topic modeling in short texts, we present a word co-occurrence network based model named WNTM to tackle the sparsity and imbalance simultaneously. Different from previous approaches, WNTM models the distribution over topics for each word instead of learning topics for each document, which successfully enhance the semantic density of data space without importing too much time or space complexity. Meanwhile, the rich contextual information preserved in the wordword space also guarantees its sensitivity in identifying rare topics with convincing quality. Furthermore, employing the same Gibbs sampling with LDA makes WNTM easily to be extended to various application scenarios. Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods. And finally we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages.", "creator": "LaTeX with hyperref package"}}}