{"id": "1412.5659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2014", "title": "Effective sampling for large-scale automated writing evaluation systems", "abstract": "Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students. It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts. Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them. At the same time, it has shown a significant increase in the number of tests required to complete the AWE in the first half of 2012. This increase in AWE, to a lesser extent, has resulted in the emergence of many new and significant projects. The AWE also has demonstrated its ability to perform a more comprehensive analysis of both the AWE and its applications.\n\n\n\n\nAUTHORIAL CONFERENCE TO SOPA\nAUTHORIAL CONFERENCE TO SOPA\nAUTHORIAL CONFERENCE TO SOPA\nThis article was presented by a public policy group called the Society for ASEA (SOPA). The Society for ASEA has a policy agenda to promote a public policy agenda in the public interest, and we invite any members of this group to share this article with anyone who wishes to discuss our position on a wide range of topics including economics, technology, and technology.\nThe Society for ASEA was formed in 2006.\nSOPA's Future in the World\nThis article was prepared in the early hours of April 17, 2013 by the Society for ASEA and was originally published on February 13, 2013. The Society's recent proposal to change its policy regarding ASEA in the Internet has been approved by both the Senate and House. This article was prepared on March 12, 2013 and is now available in the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition of the online edition", "histories": [["v1", "Wed, 17 Dec 2014 22:41:14 GMT  (390kb,D)", "http://arxiv.org/abs/1412.5659v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nicholas dronen", "peter w foltz", "kyle habermehl"], "accepted": false, "id": "1412.5659"}, "pdf": {"name": "1412.5659.pdf", "metadata": {"source": "CRF", "title": "Effective sampling for large-scale automated writing evaluation systems", "authors": ["Nicholas Dronen", "Peter W. Foltz\u20202andKyle"], "emails": ["dronen@colorado.edu", "peter.foltz@pearson.com", "kyle.habermehl@pearson.com"], "sections": [{"heading": "1 Introduction", "text": "Automated writing evalution (AWE) is the use of natural language processing and statistical modeling to score samples of writing. The first model for automatically scoring essays was created on punch cards in the 1960s [PP68]. At present there are several commercial automated essay scoring operations [RGW06, LLF03, AB06]. They are often used as the sole scorer in formative applications [Wri, ETS] and increasingly in summative assessments (e.g. Pearson Test of English Academic). Even wider adoption of automated writing evaluation appears to be underway. Recently, for example, the Hewlettt Foundation sponsored open competitions for essay-length and short answer AWE modeling on Kaggle.com to help foster greater adoption of the technology \u2217dronen@colorado.edu \u2020peter.foltz@pearson.com \u2021kyle.habermehl@pearson.com\nar X\niv :1\n41 2.\n56 59\nv1 [\ncs .C\n[Kaga, Kagb]. The edX platform for massive open online classes (MOOCs) contains a system for automated writing evaluation [edXd, edXa, edXc].\nTypically, an AWE model is trained using supervised learning and performs best when it has been trained for a specific prompt and scoring trait [FSLL13]. Training an AWE model starts with a batch of essays and one or more human scores for each essay. In the usual training procedure, the essays are preprocessed by a natural language processing pipeline, which results in a feature vector for each essay. The feature vectors and scores are then input to a learning algorithm that learns a model mapping feature vectors to scores. When the feature vector of a previously unseen essay is presented to the model, it can immediately provide a score.\nHistorically, the process of training an AWE model has required at least several hundred human-scored essays. The purpose of this requirement is twofold: to reduce sampling noise in the training set and to ensure that the model performs about as well as one trained with many more essays. In an online setting, however, a system for collecting essays and human scores can cause an AWE model to be trained whenever a new human score is entered into the system. For example, the edX AWE system allows models to be trained with as few as ten essays [edXb]. If the training set is sampled randomly, then training with so few essays will \u2013 with high probability \u2013 yield an AWE model that performs poorly.\nThe challenge of adopting AWE in large-scale contexts is that the best way to use the technology \u2013 with a customized scoring model for each prompt \u2013 is also quite expensive. In an enterprise environment, the cost of scoring ranges from $3-6 USD per essay. Assuming 500 essays for a training set and 250 for test, the cost per prompt is $2250-4500. In this paper we show how to overcome this key barrier to the widespread adoption of AWE in large-scale contexts. The method we use minimizes the number of essays that need to be scored and maximizes the information that each training example provides. This can also enable an online data collection system to intelligently choose which essays to be scored by humans. We conclude by discussing how to effectively integrate this method into AWE systems.\nThere are two bodies of literature about effective methods for choosing samples for training a supervised model. The older is optimal experimental design [Fed72], a subfield of statistics that originated with the work of Kirstine Smith in the early 20th century [Smi18]. The paradigmatic supervised learning algorithm for this literature is linear regression. In machine learning, active learning occupies much the same space as optimal design, although the literature tends to focus more on classification than regression [Set10].\nThe usual scenario for optimal experimental design is when samples are expensive to obtain, as with clinical trials or gathering samples during field studies in remote areas. An optimal design algorithm chooses samples from a pre-specified set of feature vectors in such a way as to minimize the variance in the supervised model. In active learning the assumption is usually that one begins with a set of unlabeled samples and wishes to obtain labels for them. For example, unlabeled samples can be HTML documents obtained by crawling the web and the desired labels can be the topics of the documents. Papers about regression in the active learning literature often cite the optimal design work of Valerii Fedorov (e.g. [Mac92, Sug05, Bac07]).\nWhen using an algorithm to choose samples prior to training a supervised learning\nalgorithm, obtaining good results requires that the assumptions of the sampling algorithm match the assumptions of the learning algorithm. Here we use parametric linear regression and we employ an algorithm from the optimal experimental design literature [Fed72] that is derived from the same foundations as linear regression.\nSince we start with essays and ask which ones should be graded, ours is really the classic active learning scenario. We, however, stick mostly to the terminology from the optimal design literature throughout this paper."}, {"heading": "2 Data sets", "text": "An AWE data set consists of some number of written responses and a set of scores for each response. In our experiments we use the industry track data from the Automated Student Assessment Prize (ASAP) Automated Essay Scoring (AES) competition sponsored by the Hewlett Foundation held in 2012 [Kaga]. It contains training and test sets numbered 1 through 8. Key parts of the rubric of each set are summarized in Table 1.\nIn sets 1 and 2, a student is asked to write persuasively. Essay set 1 has the student take a position on the effects of computers on society and write a letter to the editor of a newspaper arguing for this position. Sets 3-6 are response to source prompts; they have the student read and analyze some source text and provide evidence from the text to support their analysis. The source of essay set 5 is a passage about the son of Cuban immigrants growing up in New Jersey. The student must describe the mood of the text. The narrative task tends to be more open ended. In essay set 8, the task is to write about an experience in which laughter was an important element.\nThe target variable of the data sets is the score or sum of scores that an essay was given by one or more human raters. The range of the target variable for each essay set is shown in Table 1. Essay set 2 has two target variables, which we identify as 2a and 2b depending on which target variable the model was trained to predict. For all essay sets the target variable includes all integers between the minimum and maximum scores. The target variable for essay set 7, for example, is the integers 2, 3, . . . , 30.\nThe size of the training and test sets are shown in Table 2. While relatively small, these test sets are nonetheless sufficient to ensure that the performance we report in the results section approaches the performance one might expect on a much larger sample. The same table shows the rounded average number of words in each essay set. A peculiarity of these sets is that in some cases the average length is quite low. This is true for all of the response-to-source sets, suggesting that students write less when performing a more focused task.\nBefore using an algorithm to select which essays raters should score, we compute the feature vector for each essay. In our experiments we use a subset of twenty-eight features \u2013 related to mechanics, grammar, lexical sophistication, and style \u2013 from the Intelligent Essay Assessor [FSLL13]."}, {"heading": "3 Regression modeling", "text": "The target variable for automated essay scoring tasks is the student\u2019s score on an essay. It is typically an integer in a relatively narrow range determined by the scoring rubric. The ordering inherent in the target variable implies that essay scoring data are best modeled using a regression algorithm. Here we describe the approach we take to regression modeling in our experiments.\nWhen the length of feature vectors in a design matrix \u03be exceeds the number of feature vectors, the ordinary least squares solution is ill-posed. In this paper we allow m, the number of feature vectors to be as small as ten. Since p, the number of features, is twenty-eight, the ordinary least squares solution would be suboptimal for some of the models we train. Thus we use regularized regression instead of ordinary least squares. The regularized linear regression solution for \u03b2 is the solution for ordinary least squares augmented with a penalty function J(\u03b2;\u03bb),\n\u03b2\u0302(\u03bb) = arg min \u03b2 [RSS(\u03b2) + J(\u03b2;\u03bb)],\nwhere \u03bb is the regularization coefficient and is usually selected empirically (e.g. by cross validation).\nWe use ridge regression [HK70], which imposes a penalty on the L2 norm of \u03b2, as in: J(\u03b2;\u03bb) = \u03bb \u2211 i \u03b22i\nThis penalty constrains the coefficients such that irrelevant ones shrink towards zero without being eliminated from the model altogether.\nSince the predictions of a linear model trained in such a way are real values, not integers, the predictions have to be mapped back onto the integer-valued score range. One method for determining the integer-valued score for a real-valued prediction y\u0302 that lies between adjacent scores z1 and z2 is to apply thresholds as follows:\nScore(y\u0302, z1, z2) = { z1 if y\u0302 \u2264 z1+z22 ; z2 otherwise.\n(1)\nOther reasonable approaches to the problem are to use ordinal logistic regression or to choose the thresholds based on performance on a validation set. Here we use Equation 1."}, {"heading": "4 Optimal design algorithms", "text": "In our experiments, we start with a set of n candidate essays. We wish to sample a subset of m of these essays that will give us a reasonably accurate predictive model after a human scores them.\nMore formally, let X \u2208 Rn\u00d7p be the n length-p feature vectors of the essays. Call this the feature space. The ith row of X is xi and the jth column is xTj . Assume we lack the target variable Y \u2208 Rn\u00d71. Also let \u039e be the set of all subsets of rows of\nX; this is the design space. A design \u03be is an m-row member of \u039e. The goal of the algorithms described in this section is to find \u03be such that, when it is agumented with Y\u03be \u2013 the m\u00d7 1 matrix containing the target for the feature vectors in \u03be \u2013 a supervised model that learns P (Y\u03be|\u03be) has better predictive performance on unseen feature vectors than one would expect if \u03be were chosen at random.\nIn our case the model P (Y\u03be|\u03be) is linear regression. Consider a regression with feature x and target y. If we only allow an algorithm to choose two (in this case, unidimensional) feature vectors, which choices might result in a better regression model? The consequences of different choices are illustrated in Figure 1, where one can see that\nthe greater the distance between the x values, the greater the probability that the model will perform almost as well as one trained with all of the data. This is the basic concept behind effective sampling of feature vectors for regression models. It motivates our selection of the algorithms we discuss in this section. The choices they make constitute a spectrum from choosing maximally distant feature vectors to choosing uniformly distant ones.\nThe R statistical language [R C14] has implementations of the algorithms we discuss in this section. The implementation of the Fedorov algorithm we used is in the AlgDesign package [Whe14]. The prospectr package [SRL13] contains implementations of Kennard-Stone and k-means sampling.\n4.1 Fedorov exchange algorithm (with D-optimality) The Fedorov exchange algorithm [Fed72] is a greedy stepwise algorithm for finding \u03be. Its purpose is to optimize various optimal design criteria \u2013 D-optimality, A-optimality, I-optimality, or others [JD75]. Most criteria are some function of the information matrix of \u03be, defined as M(\u03be) = 1m\u03be\nT \u03be. D-optimality, for instance, searches for the \u03be that maximizes the determinant det M(\u03be). In experiments with internal data sets, we didn\u2019t see major differences among D-optimality, A-optimality, or I-optimality, but the D-optimality criterion did tend to perform somewhat better, so for simplicity of exposition we limit ourselves to D-optimality in the experiments we conduct with the ASAP essay scoring data.\nThe first step of the Fedorov algorithm is to randomly initialize \u03be withm rows from X . Subsequent steps replace a row in \u03be with a row from X that is not already in \u03be. The swapped rows are chosen greedily to optimize the design criterion. The algorithm can get stuck in a local optimum, depending on the initial choice of \u03be. The remedy for this is to run the algorithm several times with different initial states and to choose the best\none. The resulting design \u03be consists of feature vectors that are maximally distant from the centroid of the feature space, as can be seen in Figure 2a."}, {"heading": "4.2 Kennard-Stone algorithm", "text": "The initialization step of the Kennard-Stone algorithm [KS69] is similar to D-optimality in that it chooses two feature vectors at the periphery of the feature space. Thereafter, however, it chooses feature vectors so as to distribute them uniformly. More precisely, let d(u, v) be the distance between u and v. The first step of the Kennard-Stone algorithm is to choose indices i, j such that\narg max i,j\u22081...n d(xi, xj),\nthen to initialize \u03be with xi, xj and set I\u03be = {i, j}. Define\n\u2206xj (\u03be) = min i\u2208I\u03be d(xj , \u03bei).\nOn each subsequent step the index j is added to I\u03be and xj is added to \u03be:\narg max j /\u2208I\u03be \u2206xj (\u03be).\nAfter initialization, the point that is furthest from the point in the existing design are added at each step. The distances can be computed in any metric space. Here they are computed in Mahalanobis space. Figure 2b shows that a few of the feature vectors n the design are maximially distant from the centroid of the feature space and the rest are maximally distant from one another.\n4.3 k-means sampling algorithm This is simply the k-means clustering algorithm with an extra step at the end: after determining the final centroids, the design is determined by choosing the observation closest to each of the k centroids. Thus when searching for a design of size m, we set k = m.\nHere the design is approximately uniformly distributed throughout the feature space in Figure 2c. This design does not include feature vectors at the periphery of the feature space, because the centroid of a cluster to which a peripheral point belongs will almost always be closer to some other point that is not peripheral. The uniformity on display here is in part an artifact of the simulated data. In real data the feature vectors would be distributed less than smoothly, so the centroids of the final clusters would be distributed less than uniformly."}, {"heading": "4.4 Persistence of selections", "text": "An important question is the degree to which the feature vectors selected by these algorithms persist across sequential values of m. If the design \u03be at m \u2212 1 is always\nincluded in \u03be at m, its selections persist perfectly. The degree of persistence can be measured as\nPersistence = |\u03bem \u2229 \u03bem+1| |m| . (2)\nThe persistence of selections for increasing values ofm is shown in Figure 3. KennardStone starts with the same two maximally distant feature vectors and adds further ones sequentially according to a deterministic procedure, so its selections are perfectly persistent. The Fedorov algorithm runs several times with different starting points in order to avoid getting stuck in local optima. This results in some loss of persistence, which appears to improve asm increases. In the range 150 < m < 175, however, the Fedorov algorithm\u2019s persistence is erratic, suggesting the existence of multiple local optima that are nearly indistinguishable according to the optimality criterion. The k-means algorithm exhibits the lowest persistence of the three algorithms. This is expected, as the final centroids will change as k, the number of clusters, increases."}, {"heading": "5 Evaluation", "text": "To evaluate the performance of the algorithms, the ideal would be to have access both to an effectively infinite number of unscored essays and to an oracle that could provide scores upon request. Since we have finite data, we simulate a much larger amount of data by repeatedly sampling half of the entire training set (without replacement) before choosing feature vectors to be scored by humans. Of course, since we already have the scores, the human-scoring step of our evaluation doesn\u2019t actually occur. The algorithms don\u2019t look at the scores, however, so no information leaks unfairly into the process.\nWe let the desired training set sizem range from 10 to 100 in increments of 10. This allows us to simulate an operational environment where new essays arrive over time. After fixing m and sampling half of the entire training set, we use each algorithm to choose a subset of m feature vectors, pretend to have the corresponding essays scored by humans, and train one regression model with each algorithm\u2019s chosen subset. A model\u2019s performance on the test set is the evaluation criterion \u2013 specifically, the Pearson correlation coefficient of its rounded predictions with the scores of the human raters. Since test set essays are the same for all models, the performance of each model is comparable regardless of the size of the training set. To provide smoothed estimates, we averaged the correlations for each of the 300 models trained using a particular combination of training set, sampling algorithm, and size m.\nThe baseline design algorithm is random sampling without replacement. For a given design size m, the baseline algorithm selects a design \u03bem of unique feature vectors from X at random. Models trained using a design chosen by an optimal design algorithm are expected to perform better than ones trained using this baseline. Assuming an algorithm performs better than the baseline, the key question is how its performance over the baseline changes as m increases. The hallmark of an effective optimal design algorithm is that it prefers to choose the essays that are most informative to the model; this should manifest itself as a larger margin above the baseline when m is small. As m \u2192 n (where n is the number of feature vectors from which to choose), the margin should go to 0."}, {"heading": "6 Results", "text": "Table 3 shows the percent change of the average of each algorithm\u2019s models over the baseline. To test for significance, we transform the correlations using the Fisher Ztransform before applying the two-sample t-test at the 0.001 level. The test is an approximation, as the normality assumption of the t-test does not hold, but the correlation coefficients are unimodal and the sample sizes are relatively large, so we assume that the approximation is quite good. For interpretability, the percent change is calculated using the Pearson correlation. The means of Fedorov and Kennard-Stone results are significantly different from the baseline for most ASAP AES essay sets and training set sizes Fedorov and Kennard-Stone, with the exception of the anomalous essay set 4. The k-means results are more often indistinguishable from the baseline.\nTo achieve parity with the baseline at m = 100, the Fedorov algorithm typically needs to select only about 50 essays. With essay set 1, for instance, it only needs to select 10 essays, as can be seen in Figure 4.\nWith respect to average performance, the Kennard-Stone algorithm tends to perform well on the same sets as the Fedorov algorithm. The confidence intervals in Figure 4 aren\u2019t prominent enough to illustrate a notable difference. The standard deviations of the Kennard-Stone models tend to be greater, and for some test sets are even worse than the baseline, as can be seen in Figure 5. This suggests that the Fedorov algorithm is the most robust: models trained with the essays it selects have the best average performance and the least variance. It also supports the notion \u2013 illustrated in for 1-d data in Figure 1 \u2013 that the periphery of the feature space is the most effective region from which to choose feature vectors.\nWe should expect the Fedorov algorithm to perform best when the underlying process that generates the data is best fit using a linear model. Overall, the ranking of optimal design algorithms \u2013 from best to worst \u2013 conforms to this expectation: Fedorov, Kennard-Stone, and k-means. The only anomaly is essay set 4, where the k-means models perform best for 10 < m < 60, with respect both to the mean of the Pearson correlation coefficient and its standard deviation. Two possible explanations of this are noise introduced by human raters or an insufficient set of features in the feature space.\nBased on the relative performances of the three optimal design algorithms, we strongly favor the use of the Fedorov algorithm with D optimality. The Kennard-Stone algorithm\u2019s performance is a close second, but the models trained with the essays it selects tend to have greater variance and when m is small it doesn\u2019t approach the impressive performance of the Fedorov algorithm."}, {"heading": "7 Conclusion", "text": "Effective sampling disciplines the unruly process of obtaining training sets for AWE models. Our simulations show that models trained using a relatively small number of intelligently-chosen essays tend to perform well on out-of-sample essays, at times almost as well as a model trained on an entire training set. In several of the data sets, training with 30-50 essays yields approximately the same performance as a model trained with hundreds of essays. The key implication is that it is possible to minimize\nthe number of human scores necessary for training an AWE model without unduly sacrificing accuracy.\nThese results also have implications for designing systems that allow rapid integration of new AWE scoring prompts into large-scale systems. One such approach is to train models incrementally. For example, in a large MOOC, a professor may post a new writing prompt for the students. After the first few hundred students submit their essay, the optimal design algorithm selects a subset for the professor or teaching assistant to score. Using this initial training set, a scoring model can quickly be built that will start automatically scoring student essays, so that students can receive feedback on their writing. As more essays are received from students, the optimal design algorithm can select additional essays for human scoring that will be most informative to help refine the accuracy of the scoring model. The model can then be automatically retrained with even greater accuracy of scoring.\nWhen a model is retrained as scores arrive into a data collection system, using optimal design to select the essays from a large pool will ensure that the initial training set consists only of essays that are likely to result in a reasonable model. It will also both ensure that subsequently-added essays are maximally informative and minimize the duplication of the effort of human raters.\nOptimal design is necessary but not sufficient for making incremental training of AWE models effective and accurate. While it minimizes the sampling noise in a training set, it doesn\u2019t guarantee a minimum level of performance. To ensure that models are not deployed prematurely, a validation set of essays should first be selected from the pool and scored by humans. Measuring model performance with a validation set will ensure that the model performs well enough before it is deployed. Each time a new human score enters the data collection system, the model can be retrained, and its performance on the validation set can be computed. If the model doesn\u2019t perform well enough to be deployed to a production environment, additional essays can be selected to be scored. This process should repeat until the model reaches an acceptable level of performance.\nIn experiments with internal data sets, we noticed that when a model trained on the entire training set performed to our satisfaction, optimal design performed well too. The conditions that caused a model trained on the entire training set to perform poorly also caused the performance of optimal design to degrade. The most common cause of not meeting the conditions for good model performance is noise due either to poor agreement among human raters or model misspecification. An example of model misspecification is when the feature set does not capture textual semantics and the rubric instructs the human raters to give a higher score to an essay if it covers some set of topics1. Our recommended solution to avoiding model misspecification, which we don\u2019t evaluate here, is to be generous with one\u2019s feature set and to let a regularizer shrink uninformative features. We expect the results we report here to generalize to other tasks and populations as long as the conditions are right for a model trained on the entire training set to perform satisfactorily.\nWhile these results are promising for the scoring of essays, we expect that it will\n1There has been some recent work on active learning for misspecified models [Sug05, Bac07], but their thrust has been primarily theoretical and do not appear to be immediately helpful for this problem.\nbe less effective with short answer data sets. Short answer data sets are often modeled using a learning algorithm that employs a set of decision trees, such as random forest. The feature vectors are often different, too, because such models perform well when they know exactly which words a student used. Consequently, it is common for the feature vectors to be rows of a document-term matrix \u2013 an occurrence matrix with terms as columns and documents as rows \u2013 constructed from the training set responses. In that representation, the feature vectors are sparse and relatively long. Optimal design and active learning work best when the assumptions of the sampling algorithm agree with the assumptions of the subsequent supervised learning algorithm. Thus, using the Fedorov algorithm to select a subset of short responses to be scored by humans when the supervised model is random forest trained with a document-term matrix is not guaranteed to yield the best results.\nThe use of optimal design for AWE training sets can be seen as a form of manufacturing process control in which the amount of information provided by a human score is maximized and its variance minimized. From this perspective, the goal of our use of optimal design is primarily to make the output of the human scoring process \u2013 that is, the quantity of information in a single human score \u2013 more predictable. The effect of this is to enable safe and reliable incremental training of AWE models. Overall, the approach solves a barrier to the adoption of AWE into large-scale formative and summative systems, allowing the computer to minimize the amount of human effort needed to collect and score essays by choosing effectively which essays humans need to score."}], "references": [{"title": "2", "author": ["Yigal Attali", "Jill Burstein. Automated essay scoring with e-rater v"], "venue": "The Journal of Technology, Learning and Assessment, 4(3),", "citeRegEx": "AB06", "shortCiteRegEx": null, "year": 2006}, {"title": "Active learning for misspecied generalized linear models", "author": ["F R Bach"], "venue": "Adv. Neural Inf. Process. Syst.", "citeRegEx": "Bac07", "shortCiteRegEx": null, "year": 2007}, {"title": "Theory Of Optimal Experiments", "author": ["V V Fedorov"], "venue": "Probability and mathematical statistics. Elsevier Science", "citeRegEx": "Fed72", "shortCiteRegEx": null, "year": 1972}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["Arthur E Hoerl", "Robert W Kennard"], "venue": "Technometrics, 12(1):55\u201367, 1 February", "citeRegEx": "HK70", "shortCiteRegEx": null, "year": 1970}, {"title": "D-Optimality for regression designs: A review", "author": ["R C St John", "N R Draper"], "venue": "Technometrics, 17(1):15\u201323", "citeRegEx": "JD75", "shortCiteRegEx": null, "year": 1975}, {"title": "11(1):137\u2013148", "author": ["R W Kennard", "L A Stone. Computer aided design of experiments. Technometrics"], "venue": "1 February", "citeRegEx": "KS69", "shortCiteRegEx": null, "year": 1969}, {"title": "Assessment in Education: Principles", "author": ["Thomas K Landauer", "Darrell Laham", "Peter W Foltz. Automatic essay assessment"], "venue": "Policy and Practice, 10(3):295\u2013308,", "citeRegEx": "LLF03", "shortCiteRegEx": null, "year": 2003}, {"title": "4(4):590\u2013604", "author": ["D MacKay. Information-Based objective functions for active data selection. Neural Computation"], "venue": "July", "citeRegEx": "Mac92", "shortCiteRegEx": null, "year": 1992}, {"title": "The analysis of essays by computer", "author": ["EB Page", "DH Paulus"], "venue": "final report.", "citeRegEx": "PP68", "shortCiteRegEx": null, "year": 1968}, {"title": "R: A Language and Environment for Statistical Computing", "author": ["R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna, Austria", "citeRegEx": "R C14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and Assessment", "author": ["Lawrence M Rudner", "Veronica Garcia", "Catherine Welch. An evaluation of IntelliMetric essay scoring system. The Journal of Technology"], "venue": "4(4),", "citeRegEx": "RGW06", "shortCiteRegEx": null, "year": 2006}, {"title": "Active learning literature survey", "author": ["B Settles"], "venue": "University of Wisconsin, Madison", "citeRegEx": "Set10", "shortCiteRegEx": null, "year": 2010}, {"title": "12(1/2):1\u201385", "author": ["Kirstine Smith. On the standard deviations of adjusted", "interpolated values of an observed polynomial function", "its constants", "the guidance they give towards a proper choice of the distribution of observations. Biometrika"], "venue": "1 November", "citeRegEx": "Smi18", "shortCiteRegEx": null, "year": 1918}, {"title": "An introduction to the prospectr package, 2013. R package version 0.1.3", "author": ["Antoine Stevens", "Leornardo Ramirez-Lopez"], "venue": null, "citeRegEx": "Stevens and Ramirez.Lopez.,? \\Q2013\\E", "shortCiteRegEx": "Stevens and Ramirez.Lopez.", "year": 2013}, {"title": "Active learning for misspecified models", "author": ["M Sugiyama"], "venue": "Adv. Neural Inf. Process. Syst.", "citeRegEx": "Sug05", "shortCiteRegEx": null, "year": 2005}, {"title": "AlgDesign: Algorithmic Experimental Design, 2014. R package version 1.1-7.2", "author": ["Bob Wheeler"], "venue": null, "citeRegEx": "Wheeler.,? \\Q2014\\E", "shortCiteRegEx": "Wheeler.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "The first model for automatically scoring essays was created on punch cards in the 1960s [PP68].", "startOffset": 89, "endOffset": 95}, {"referenceID": 2, "context": "The older is optimal experimental design [Fed72], a subfield of statistics that originated with the work of Kirstine Smith in the early 20th century [Smi18].", "startOffset": 41, "endOffset": 48}, {"referenceID": 12, "context": "The older is optimal experimental design [Fed72], a subfield of statistics that originated with the work of Kirstine Smith in the early 20th century [Smi18].", "startOffset": 149, "endOffset": 156}, {"referenceID": 11, "context": "In machine learning, active learning occupies much the same space as optimal design, although the literature tends to focus more on classification than regression [Set10].", "startOffset": 163, "endOffset": 170}, {"referenceID": 2, "context": "Here we use parametric linear regression and we employ an algorithm from the optimal experimental design literature [Fed72] that is derived from the same foundations as linear regression.", "startOffset": 116, "endOffset": 123}, {"referenceID": 3, "context": "We use ridge regression [HK70], which imposes a penalty on the L2 norm of \u03b2, as in: J(\u03b2;\u03bb) = \u03bb \u2211", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "The R statistical language [R C14] has implementations of the algorithms we discuss in this section.", "startOffset": 27, "endOffset": 34}, {"referenceID": 2, "context": "1 Fedorov exchange algorithm (with D-optimality) The Fedorov exchange algorithm [Fed72] is a greedy stepwise algorithm for finding \u03be.", "startOffset": 80, "endOffset": 87}, {"referenceID": 4, "context": "Its purpose is to optimize various optimal design criteria \u2013 D-optimality, A-optimality, I-optimality, or others [JD75].", "startOffset": 113, "endOffset": 119}, {"referenceID": 5, "context": "2 Kennard-Stone algorithm The initialization step of the Kennard-Stone algorithm [KS69] is similar to D-optimality in that it chooses two feature vectors at the periphery of the feature space.", "startOffset": 81, "endOffset": 87}], "year": 2014, "abstractText": "Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students. It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts. Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them. This requirement limits large-scale adoption of AWE since human-scoring essays is costly. Here we evaluate algorithms for ensuring that AWE models are consistently trained using the most informative essays. Our results show how to minimize training set sizes while maximizing predictive performance, thereby reducing cost without unduly sacrificing accuracy. We conclude with a discussion of how to integrate this approach into large-scale AWE systems.", "creator": "LaTeX with hyperref package"}}}