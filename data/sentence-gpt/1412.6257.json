{"id": "1412.6257", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Gradual training of deep denoising auto encoders", "abstract": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets. The first DAE layer, which was designed for large datasets, was built on the assumption that it is relatively low-level and robust to most other models, and this was verified by a recent survey on this topic by a recent CIFAR project. This observation also shows that the gradient in training across the DAE layer was also well predicted.\n\n\n\n\nAfter training, we show that in the time we first learned that neural networks that are constantly adding layers are gradually adding layers. This is a significant improvement in the likelihood that training can improve this prediction.\n\nFor example, the layer depth estimation algorithm shows that when we compare model learning in layers to that in the same layer layer, training can improve the accuracy of the model. A more specific set of models can be applied, such as in models such as Gistran et al. [8], which shows that when we train deep networks, we have a high-performance training dataset, which is then presented as a hierarchical network with features like:\nMixed training model is designed to provide a good training network, which is then presented as a hierarchy, with features like:\nThe LMS layer that we use for training Deep Neural Networks\nWe use the LMS layer in our model and then test that we know the LMS layer can improve the accuracy and accuracy of the model by comparing the layers on the model. We show that this is very useful for training Deep Neural Networks with high training volumes, and we provide a better training network using the LMS layer.\nThis is an excellent training network. We show that the LMS layer can improve the accuracy and accuracy of the model by comparing the layers on the model.\nOur training model also has a high-performance learning dataset. There are many training tasks for deep learning such as machine learning, but the training model can improve the accuracy of the model by comparing the layers on the model.\nIn order to perform these learning tasks, we have to specify the layer depth", "histories": [["v1", "Fri, 19 Dec 2014 09:30:33 GMT  (1698kb,D)", "http://arxiv.org/abs/1412.6257v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["alexander kalmanovich", "gal chechik"], "accepted": false, "id": "1412.6257"}, "pdf": {"name": "1412.6257.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Alexander Kalmanovich"], "emails": ["sashakal@gmail.com,", "gal.chechik@biu.ac.il"], "sections": [{"heading": "1 INTRODUCTION", "text": "A central approach in learning meaningful representations is to train a deep network for reconstructing corrupted data. The idea is simple: given unlabeled data, a deep-network is given input-output pairs, where the input consists of a corrupted version of an input sample and the output consists of the original non-corrupted version which the network aims to reconstruct. Indeed, denoising autoencoders (DAE) (Vincent et al., 2008) have been shown to extract meaningful features which allow to correct corrupted input data (Xie et al., 2012). These representations can later be used to initialize a deep network for a supervised learning task. It has been shown that in the small-data regime, good initializations can cut down the training time and improve the classification accuracy of the supervised task (Vincent et al., 2010; 2008; Larochelle et al., 2009; Erhan et al., 2010).\nGoing beyond a single layer, it has been shown that training a multi-layer (deep) DAE can be achieved efficiently by stacking single-layer DAEs and training them layer-by-layer (Vincent et al., 2010). Specifically, a stacked denoising autoencoder (SDAE) is trained as follows (Fig. 1). First, a single-layer auto encoder is trained over the corrupted input data x\u0303 and its weights are tuned (Fig. 1a). Then, the weights to the first hidden layer w1 are frozen, and the data is transformed to the hidden representation (Fig. 1b). This transformed input h1(x) is then used to create a corrupted input to a second autoencoder and so on (Fig. 1c).\nStacked training has been shown to outperform training de-novo of a full deep network, presumably because it provides better error signals to lower layers of the network (Erhan et al., 2009). However, stacked training is greedy in the following sense: When the first layer is trained, it is tuned such that its features can be directly used for reconstructing the corrupted input. Later on however, these features are used as input to train more complex features. Comparing this with the process of reduced plasticity in natural neural systems, early layers in mammalian visual system keep adapt for prolonged periods, and their synapses remain plastic long after representations have been formed in high brain areas (Liu et al., 2004). We therefore turned to explore alternative training schedules for deep DAEs, which avoid freezing early weights.\nWe test here gradual training, where training occurs layer-by-layer, but lower layers keep adapt throughout training. We compare gradual training to stacked training and to a hybrid approach, all under a fixed budget of training update steps. We then test gradual training as an initialization for supervised learning, and quantify its performance as a function of dataset size. Gradual training provides a small but consistent improvement in reconstruction error and classification error in the regime of mid-sized datasets.\nar X\niv :1\n41 2.\n62 57\nv1 [\ncs .L\nG ]\n1 9\nD ec\n2 01\n4"}, {"heading": "2 TRAINING DENOISING AUTOENCODERS", "text": "For completeness, we detail here the procedure for training stacked denoising autoencoders described by Vincent et al. (2010). Fig. 1 describes the architecture and the main training phases. For training the first layer with a training sample x, masking noise is used to create a corrupted noisy version x\u0303 (Fig. 1a, corrupt arrow). A forward pass is taken, computing the hidden representation h1 = Sigmoid(w > 1 x) and the output y = Sigmoid(w \u2032> 2 h1). Specifically, the loss function is often taken to be the cross entropy between y and x (Fig. 1a, dotted arrow). All weights are updated by propagating the error gradient back through the network. This is repeated for other samples in a stochastic gradient descent (SGD) fashion, and combined with momentum and weight decay to speed training.\nTo train a deep network, multiple DAEs are stacked using greedy layer-wise training (Vincent et al., 2010). After the first DAE is trained, the learned encoding weights w1 are fixed, and the data is mapped to the hidden layer representation h1 (Fig. 1b, blank arrow). The second DAE is trained based on h1(x) using the same procedure as the first layer (Fig. 1b). Importantly, the corrupting noise is applied to the hidden representation h1(x) to create h\u03031, with the motivation being that injecting noise to the hidden layer introduces variability of the more-abstract representation that was already learned by the network. Training of subsequent layers follows the same procedure, injecting noise at higher and higher layers.\nOften, this layer-wise training procedure is followed by a full back-propagation phase, where noise is injected to the original input x and all layers are updated jointly. Then, the SDAE can be used to initialize a deep network for a supervised classification task by replacing the top reconstruction layer with a (usually multi-class) classification layer."}, {"heading": "2.1 GRADUAL TRAINING OF DEEP DAES", "text": "We describe an alternative, gradual, scheme for training autoencoders. The basic idea is to train the deep autoencoder layer-by-layer, but keep adapting the lower layers continuously. Noise injection is only applied at the input level (Fig. 2). The motivation for this procedure has two aspects. First, it allows lower weights to take into account the higher representations during training, reducing the greedy nature of stacked training. Second, denoising is applied to the input, rather than to a hidden representation learned in a greedy way.\nMore specifically, the first layer is trained in the same way as in stacked training, producing the weights w1. Then, when adding the second layer autoencoder, its weights w2 are tuned jointly\nwith w1. This is done by using the weights w1 to initialize the first layer and randomly initializing the weights of the second. Given a training sample x, we generate a noisy version x\u0303, feed it to the 2-layered DAE, and compute the activation at the subsequent layers h1 = Sigmoid(w>1 x), h2 = Sigmoid(w > 2 h1) and y = Sigmoid(w \u2032> 3 h2). Importantly, the loss function is now computed over the input x, and is used to update all the weights including w1 (Fig. 2b). Similarly, if a 3rd layer is trained, it involves tuning w1 and w2 in addition to w3 and w\u20324 (Fig. 2c).\nThere are therefore two main differences between gradual and stacked training of SDAE. First, in gradual training, weights of lower layers are never fixed as in stacked training, but rather trained jointly when tuning weights of a newly-added layer. Second, each training phase reconstructs a noisy version of the input rather than a noisy version of a hidden-layer representation."}, {"heading": "3 EXPERIMENTS", "text": "We compare the performance of gradual training of DAEs with that of stacked training, in two learning setups. First in an unsupervised denoising task, and then by using them to initialize a deep network in a supervised classification task. We conduct all experiments using \u201dMEDAL\u201d, a MATLAB implementation of DNNs and auto encoders (Stansbury, 2012)."}, {"heading": "3.1 DATASETS", "text": "We tested gradual training on three benchmark datasets: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009). MNIST contains 70,000 28-by-28 grayscale images, each containing a single hand-written digit. CIFAR-10 and CIFAR-100 contain 60,000 natural RGB images of 32-by-32 pixels from 10 or 100 categories respectively."}, {"heading": "3.2 TRAINING PROCEDURE", "text": "Performance was evaluated on a test subset of 10,000 samples. When quantifying performance as a function of dataset size, we create training subsets of different sizes while maintaining the class distribution uniform as in the original training data.\nHyper parameters were selected using a second level of cross validation (10-fold CV for MNIST, 5-fold for CIFAR), keeping a uniform distribution over classes. In the experiments below, we tune the following hyper parameters: number of units in hidden layers (same for all layers: 1000,1500,2000,2500), learning rate (10\u22121, 10\u22122, 10\u22123, 5 \u00d7 10\u22124, 10\u22124, 5 \u00d7 10\u22125, 10\u22125) batch size for SGD (10, 20), seed for weight random initialization, momentum (0.9, 0.7) (Polyak, 1964) and weight decay (10\u22123, 10\u22124, 10\u22125) (Moody et al., 1995). The best performing configuration on\nthe validation set was sought in a semi-automatic fashion (as in Vincent et al. (2010)) by running experiments in parallel on a large computation cluster with manual guidance to avoid wasting resources on unnecessary parts of the configuration space. We used early stopping by monitoring reconstruction error or classification error on the validation set, and stopped training after 35 epochs without improvement. We used the parameters (weights) which yield the best performance over the validation set. Reported results are the average over 3 different random train-validation splits.\nSince gradual training involves updating lower layers, every presentation of a sample involves more weight updates than in a single-layered DAE. We compare stacked and gradual training on a common ground, by using the same budget for weight update steps. For example, when training the second layer for n epochs in gradual training, we allocate 2n training epochs for stacked training. The overall budget for update steps was determined using early stopping, such that the reconstruction error on the validation set in the last 10 epochs did not improve more than 0.5% in all training schemes."}, {"heading": "4 RESULTS", "text": "We evaluate gradual and stacked training in unsupervised task of image denoising. We then test these training methods as an initialization for supervised learning, and quantify its performance as a function of dataset size."}, {"heading": "4.1 UNSUPERVISED LEARNING FOR DENOISING", "text": "We start by evaluating gradual training in an unsupervised task of image denoising. Here, the network is trained to minimize a cross-entropy loss over corrupted images. In addition to stacked and gradual training, we also tested a hybrid method that spends some epochs on tuning only the second layer (as in stacked training), and then spends the rest of the training budget on both layers (as in gradual training). We define the Stacked-vs-Gradual fraction 0 \u2264 f \u2264 1 as the fraction of weight updates that occur during stacked-type training. f = 1 is equivalent to pure stacked training while f = 0 is equivalent to pure gradual training. Given a budget of n training epochs, we train the 2nd hidden layer with gradual training for n(1\u2212 f) epochs, and with stacked training for 2nf epochs.\na) MNIST b) CIFAR-10 c) CIFAR-100\nFigure 3 shows the test-set cross entropy error when training 2-layered DAEs, as a function of the Stacked-vs-Gradual fraction. Pure gradual training achieved significant lower reconstruction error than any mix of stacked and gradual training with the same budget of update steps.\nWe also evaluated the reconstruction error after a full tuning phase is performed in which all weights are updated jointly for 80 epochs for MNIST and 70 epochs for CIFAR. Pure gradual training (f = 0) improved the reconstruction error over full stacked training (f = 1) across all datasets. MNIST: from 10.38 \u00b1 0.06 to 9.61 \u00b1 0.06, being a 7.39% improvement; CIFAR-10: from 57.26 \u00b1 0.23 to 55.47 \u00b1 0.2, being a 3.12% improvement; CIFAR-100: from 59.34 \u00b1 0.3 to 57.01 \u00b1 0.45, being a 3.92% improvement."}, {"heading": "4.2 GRADUAL-TRAINING DAE FOR INITIALIZING A NETWORK IN A SUPERVISED TASK", "text": "We use DAEs trained in the previous experiment for initializing a deep network to solve a supervised classification task. The network architecture is the same as SDAE architecture, except for the top layer. The first two hidden layers are initialized with the first two layer weights of the SDAE (w1 and w2 in Fig. 2b). We then add a top classification layer with output units matching the classes in the dataset, with randomly initialized weights.\nWe train these networks on several subsets of each dataset to quantify the benefit of unsupervised pretraining as a function of train-set size. Figure. 4 traces the classification error as a function of training set size, showing in text the percentage of relative improvement. These results suggest that initialization with gradually-trained DAEs yields better classification accuracy than when initializing with stacked-trained DAEs, and that this effect is mostly relevant for datasets with less than 50K samples.\nThe gradual training procedure described above differs from stacked training in two aspects: noise injection at the input level and joint training of weights. To test which of these two contributes to the superior performance we conducted the following experiment. We trained a network to reconstruct a noisy version of the input, as in gradual training, but kept the weights of the 1st hidden layer fixed as in stacked training.\nThe results of this experiments varied across datasets. In MNIST, injecting noise to the input while freezing the first layer performed worse than gradual training, both in terms of cross entropy (in the reconstruction task) and in terms of classification accuracy (in the supervised task). In CIFAR however, training with freezing the first layer actually reduced reconstruction error compared with gradual training, while achieving the same performance in the supervised task."}, {"heading": "5 CONCLUSION", "text": "We described a gradual training scheme for denoising auto encoders, which improves the reconstruction error under a fixed training budget, as compared to stacked training. It also provided a small but consistent improvement in classification error in the regime of mid-sized training sets. Comparing stacked and gradual training can be viewed as the two extreme adaptation schemes: with stacked-learning reflecting a zero learning rate for the lower layer, and gradual training reflecting a full learning rate. It remains to test intermediate training schedules where the learning rate is being gradually reduced as a layer is presented with examples."}], "references": [{"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan", "Dumitru", "Manzagol", "Pierre-Antoine", "Bengio", "Yoshua", "Samy", "Vincent", "Pascal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Manzagol", "Pierre-Antoine", "Vincent", "Pascal", "Samy"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Deep learning using robust interdependent codes", "author": ["Larochelle", "Hugo", "Erhan", "Dumitru", "Vincent", "Pascal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Switching of nmda receptor 2a and 2b subunits at thalamic and cortical synapses during early postnatal development", "author": ["Liu", "Xiao-Bo", "Murray", "Karl D", "Jones", "Edward G"], "venue": "The Journal of neuroscience,", "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}, {"title": "A simple weight decay can improve generalization", "author": ["JE Moody", "SJ Hanson", "Krogh", "Anders", "Hertz", "John A"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Moody et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Moody et al\\.", "year": 1995}, {"title": "Teodorovich. Some methods of speeding up the convergence of iteration methods", "author": ["Polyak", "Boris"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak and Boris,? \\Q1964\\E", "shortCiteRegEx": "Polyak and Boris", "year": 1964}, {"title": "Matlab environment for deep architecture learning, 2012. URL https: //github.com/dustinstansbury/medal", "author": ["Stansbury", "Dustin E"], "venue": null, "citeRegEx": "Stansbury and E.,? \\Q2012\\E", "shortCiteRegEx": "Stansbury and E.", "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["Xie", "Junyuan", "Xu", "Linli", "Chen", "Enhong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xie et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "Indeed, denoising autoencoders (DAE) (Vincent et al., 2008) have been shown to extract meaningful features which allow to correct corrupted input data (Xie et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 11, "context": ", 2008) have been shown to extract meaningful features which allow to correct corrupted input data (Xie et al., 2012).", "startOffset": 99, "endOffset": 117}, {"referenceID": 10, "context": "It has been shown that in the small-data regime, good initializations can cut down the training time and improve the classification accuracy of the supervised task (Vincent et al., 2010; 2008; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 164, "endOffset": 237}, {"referenceID": 3, "context": "It has been shown that in the small-data regime, good initializations can cut down the training time and improve the classification accuracy of the supervised task (Vincent et al., 2010; 2008; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 164, "endOffset": 237}, {"referenceID": 1, "context": "It has been shown that in the small-data regime, good initializations can cut down the training time and improve the classification accuracy of the supervised task (Vincent et al., 2010; 2008; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 164, "endOffset": 237}, {"referenceID": 10, "context": "Going beyond a single layer, it has been shown that training a multi-layer (deep) DAE can be achieved efficiently by stacking single-layer DAEs and training them layer-by-layer (Vincent et al., 2010).", "startOffset": 177, "endOffset": 199}, {"referenceID": 0, "context": "Stacked training has been shown to outperform training de-novo of a full deep network, presumably because it provides better error signals to lower layers of the network (Erhan et al., 2009).", "startOffset": 170, "endOffset": 190}, {"referenceID": 5, "context": "Comparing this with the process of reduced plasticity in natural neural systems, early layers in mammalian visual system keep adapt for prolonged periods, and their synapses remain plastic long after representations have been formed in high brain areas (Liu et al., 2004).", "startOffset": 253, "endOffset": 271}, {"referenceID": 10, "context": "To train a deep network, multiple DAEs are stacked using greedy layer-wise training (Vincent et al., 2010).", "startOffset": 84, "endOffset": 106}, {"referenceID": 9, "context": "2 TRAINING DENOISING AUTOENCODERS For completeness, we detail here the procedure for training stacked denoising autoencoders described by Vincent et al. (2010). Fig.", "startOffset": 138, "endOffset": 160}, {"referenceID": 4, "context": "1 DATASETS We tested gradual training on three benchmark datasets: MNIST (LeCun et al., 1998), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009).", "startOffset": 73, "endOffset": 93}, {"referenceID": 6, "context": "7) (Polyak, 1964) and weight decay (10\u22123, 10\u22124, 10\u22125) (Moody et al., 1995).", "startOffset": 54, "endOffset": 74}, {"referenceID": 9, "context": "the validation set was sought in a semi-automatic fashion (as in Vincent et al. (2010)) by running experiments in parallel on a large computation cluster with manual guidance to avoid wasting resources on unnecessary parts of the configuration space.", "startOffset": 65, "endOffset": 87}], "year": 2014, "abstractText": "Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets.", "creator": "LaTeX with hyperref package"}}}