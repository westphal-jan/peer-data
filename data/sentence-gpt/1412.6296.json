{"id": "1412.6296", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Generative Modeling of Convolutional Neural Networks", "abstract": "The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them. This paper investigates generative modeling of CNNs by comparing neural networks to the traditional neural network models. For example, researchers found that CNNs can detect and predict neural network networks over time. This has been demonstrated to have an important potential for improving the accuracy of both classification and predictive processing in many aspects of real-world tasks such as driving a vehicle.", "histories": [["v1", "Fri, 19 Dec 2014 11:34:37 GMT  (3478kb,D)", "http://arxiv.org/abs/1412.6296v1", null], ["v2", "Thu, 9 Apr 2015 15:07:06 GMT  (25616kb,D)", "http://arxiv.org/abs/1412.6296v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jifeng dai", "yang lu", "ying-nian wu"], "accepted": true, "id": "1412.6296"}, "pdf": {"name": "1412.6296.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jifeng Dai"], "emails": ["jifdai@microsoft.com", "ywu@stat.ucla.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent years have witnessed the triumphant return of the feedforward neural networks, especially the convolutional neural networks (CNNs) (LeCun et al., 1989). Fueled by the availabilities of large labeled data sets, the increased computing power, and the incorporation of new types of nonlinear units, CNNs have proven to be a powerful tool for discriminative learning (Krizhevsky et al., 2012; Girshick et al., 2013). Despite the successes of the discriminative learning of CNNs, the generative aspect of CNNs has not been thoroughly investigated. But it can be very useful for the following reasons: (1) The generative pre-training has the potential to lead the network to a better local optimum; (2) Samples can be drawn from the generative model to reveal the knowledge learned by the CNN. Although many generative models and learning algorithms have been proposed (Hinton et al., 2006a;b; Rifai et al., 2011; Salakhutdinov & Hinton, 2009), most of them do not scale well and have not been applied to learning large scale deep CNNs.\nIn this paper, we study the generative modeling of the CNNs. We start from defining probability distributions of images given the underlying object categories or class labels, such that the CNN with a final logistic regression layer serves as the corresponding conditional distribution of the class labels given the images. These distributions are in the form of exponential tilting of a reference distribution, i.e., exponential family models or energy-based models relative to a reference distribution.\nWith such a generative model, we proceed to study it along two related themes, which differ in how to handle the reference distribution or the null model. In the first scheme, we propose a nonparametric generative gradient for pre-training the CNN, where the CNN is learned by the stochastic\nar X\niv :1\n41 2.\n62 96\nv1 [\ncs .C\nV ]\n1 9\nD ec\n2 01\ngradient algorithm that seeks to minimize the log-likelihood of the generative model. The gradient of the log-likelihood is approximated by the importance sampling method that keeps reweighing the images that are sampled from a non-parametric implicit reference distribution, such as the marginal distribution of all the training images. The generative gradient is fundamentally different from the commonly used discriminative gradient, and yet in batch training, it shares the same computational architecture as well as computational cost as the discriminative gradient. This generative learning scheme can be used in a pre-training stage that is to be followed by the usual discriminative training. The generative log-likelihood provides stronger driving force than the discriminative criteria for stochastic gradient by requiring the learned parameters to explain the images instead of their labels. Experiments on the MNIST (LeCun et al., 1998) and the challenging ImageNet (Deng et al., 2009) classification benchmarks show that this generative pre-training scheme consistently helps improve the performance of CNNs.\nThe second theme in our study of generative modeling is to assume an explicit parametric form of the reference distribution, such as the Gaussian white noise model, so that we can draw synthetic images from the resulting probability distributions of images. The sampling can be accomplished by the Hamiltonian Monte Carlo (HMC) algorithm (Neal, 2011), which iterates between a bottomup convolution step and a top-down deconvolution step. The proposed visualization method can directly draw samples of synthetic images for any given node in a trained CNN, without resorting to any extra hold-out images. Experiments show that vividly meaningful and diversely varied synthetic images can be generated for nodes of a large-scale deep CNN discriminatively trained on ImageNet."}, {"heading": "2 PAST WORK", "text": "The generative model that we study is an energy-based model. Such models include field of experts (Roth & Black, 2009), product of experts (Hinton, 2002), Boltzmann machines (Hinton et al., 2006a), model based on neural networks (Hinton et al., 2006b), etc. However, most of these generative models and learning algorithms have not been applied to learning deep CNNs. The proposed non-parametric generative gradient as well as the generative model based on deep CNNs have not been studied in the literature to the best of our knowledge.\nThe relationship between the generative models and the discriminative approaches has been extensively studied, perhaps starting from Efron (Efron, 1975), and more recently by (Jordan, 2002; Liang & Jordan, 2008), etc. Moreover, the usefulness of generative pre-training for deep learning has been studied by (Erhan et al., 2010) etc. However, this issue has not been thoroughly investigated for CNNs. Moreover, unlike previous pre-training methods, our non-parametric generative gradient shares the same computational architecture as the discriminative gradient.\nAs to visualization, our work is related to (Erhan et al., 2009; Le et al., 2012; Girshick et al., 2013; Zeiler & Fergus, 2013; Long et al., 2014). In Girshick et al. (2013); Long et al. (2014), the highscoring image patches are directly presented. In Zeiler & Fergus (2013), a top-down deconvolution step is used to understand what contents are emphasized in the high-scoring input image patches. Visualization of nodes in neural networks has also been studied in Erhan et al. (2009); Le et al. (2012), where images are synthesized by maximizing the responses of the nodes. In our approach, a generative model is formally defined. We sample from the well-defined probability distribution by the HMC algorithm, generating meaningful and varying synthetic images, without resorting to a large collection of hold-out images (Girshick et al., 2013; Zeiler & Fergus, 2013; Long et al., 2014)."}, {"heading": "3 GENERATIVE MODEL BASED ON CNN", "text": ""}, {"heading": "3.1 PROBABILITY DISTRIBUTIONS ON IMAGES", "text": "Suppose we observe images from many different object categories. Let x be an observed image from an object category y. Consider the following probability distribution on x,\npy(x;w) = 1\nZy(w) exp (fy(x;w)) q(x), (1)\nwhere q(x) is a reference distribution common to all the categories, fy(x;w) is a scoring function for class y, w collects the unknown parameters to be learned from the data, and Zy(w) =\nEq[exp(fy(x;w))] = \u222b exp(fy(x;w))q(x)dx is the normalizing constant or partition function. The distribution py(x;w) is in the form of an exponential tilting of the reference distribution q(x), and can be considered an energy-based model or an exponential family model. In Model (1), the reference distribution q(x) may not be unique. If we change q(x) to q1(x), then we can change fy(x;w) to fy(x;w) \u2212 log[q1(x)/q(x)], which may correspond to a fy(x;w1) for a different w1 if the parametrization of fy(x,w) is flexible enough. We want to choose q(x) so that either q(x) is reasonably close to py(x;w) as in our non-parametric generative gradient method, or the resulting py(x;w) based on q(x) is easy to sample from as in our generative visualization method.\nFor an image x, let y be the underlying object category or class label, so that p(x|y;w) = py(x;w). Suppose the prior distribution on y is p(y) = \u03c1y . The posterior distribution of y given x is\np(y|x,w) = exp(fy(x;w) + \u03b1y)\u2211 y exp(fy(x;w) + \u03b1y) , (2)\nwhere \u03b1y = log \u03c1y \u2212 logZy(w). p(y|x,w) is in the form of a multi-class logistic regression, where \u03b1y can be treated as an intercept parameter to be estimated directly if the model is trained discriminatively. Thus for notational simplicity, we shall assume that the intercept term \u03b1y is already absorbed into w for the rest of the paper. Note that fy(x;w) is not unique in (2). If we change fy(x;w) to fy(x;w) \u2212 g(x) for a g(x) that is common to all the categories, we still have the same p(y|x;w). This non-uniqueness corresponds to the non-uniqueness of q(x) in (1) mentioned above. Given a set of labeled data {(xi, yi)}, equations (1) and (2) suggest two different methods to estimate the parameters w. One is to maximize the generative log-likelihood lG(w) = \u2211 i log p(xi|yi, w),\nwhich is the same as maximizing the full log-likelihood \u2211 i log p(xi, yi|w), where the prior probability of \u03c1y can be estimated by class frequency of category y. The other is to maximize the discriminative log-likelihood lD(w) = \u2211 i log p(yi|xi, w). For the discriminative model (2), a popular choice of fy(x;w) is multi-layer perceptron or CNN, with w being the connection weights, and the top-layer is a multi-class logistic regression. This is the choice we adopt throughout this paper."}, {"heading": "3.2 GENERATIVE GRADIENT", "text": "The gradient of the discriminative log-likelihood is calculated according to\n\u2202\n\u2202w log p(yi|xi, w) =\n\u2202\n\u2202w fyi(xi;w)\u2212 ED\n[ \u2202\n\u2202w fy(xi;w)\n] , (3)\nwhere \u03b1y is absorbed into w as mentioned above, and\nED\n[ \u2202\n\u2202w fy(xi;w) ] = \u2211 y \u2202 \u2202w fy(xi;w) exp(fy(xi;w))\u2211 y exp(fy(xi;w)) . (4)\nThe gradient of the generative log-likelihood is calculated according to\n\u2202\n\u2202w log pyi(xi;w) =\n\u2202\n\u2202w fyi(xi;w)\u2212 EG\n[ \u2202\n\u2202w fyi(x;w)\n] , (5)\nwhere\nEG\n[ \u2202\n\u2202w fyi(x;w)\n] = \u222b \u2202\n\u2202w fyi(x;w)\n1\nZyi(w) exp(fyi(x;w))q(x), (6)\nwhich can be approximated by importance sampling. Specifically, let {x\u0303j}mj=1 be a set of samples from q(x), for instance, q(x) is the distribution of images from all the categories. Here we do not attempt to model q(x) parametrically, instead, we treat it as an implicit non-parametric distribution. Then by importance sampling,\nEG\n[ \u2202\n\u2202w fyi(x;w) ] \u2248 \u2211 j \u2202 \u2202w fyi(x\u0303j ;w)Wj , (7)\nwhere the importance weight Wj \u221d exp(fyi(x\u0303j ;w)) and is normalized to have sum 1. Namely, \u2202\n\u2202w log pyi(xi;w) \u2248\n\u2202\n\u2202w fyi(xi;w)\u2212 \u2211 j \u2202 \u2202w fyi(x\u0303j ;w) exp(fyi(x\u0303j ;w))\u2211 k exp(fyi(x\u0303k;w)) . (8)\nThe discriminative gradient and the generative gradient differ subtly and yet fundamentally in calculating E[\u2202fy(x;w)/\u2202w], whose difference from the observed \u2202fyi(xi;w)/\u2202w provides the driving force for updating w. In the discriminative gradient, the expectation is with respect to the posterior distribution of the class label y while the image xi is fixed, whereas in the generative gradient, the expectation is with respect to the distribution of the images x while the class label yi is fixed. In general, it is easier to adjust the parameters w to predict the class labels than to reproduce the features of the images. So it is expected that the generative gradient provides stronger driving force for updating w.\nThe non-parametric generative gradient can be especially useful in the beginning stage of training or what can be called pre-training, where w is small, so that the current py(x;w) for each category y is not very separated from q(x), which is the overall marginal distribution of x. In this stage, the importance weights Wj are not very skewed and the effective sample size for importance sampling can be large. So updating w according to the generative gradient can provide useful pre-training with the potential to lead w to a good local optimum. If the importance weights Wj start to become skewed and the effective sample size starts to dwindle, then this indicates that the categories py(x;w) start to separate from q(x) as well as from each other, so we can switch to discriminative training to further separate the categories. More explanation of the generative gradient can be found in the supplementary materials."}, {"heading": "3.3 BATCH TRAINING AND GENERATIVE LOSS LAYER", "text": "At first glance, the generative gradient appears computationally expensive due to the need to sample from q(x). In fact, with q(x) being the collection of images from all the categories, we may use each batch of samples as an approximation to q(x) in the batch training mode. Specifically, let {(xi, yi)}ni=1 be a batch set of training examples, and we seek to maximize\u2211 i log pyi(xi;w) via generative gradient. In the calculation of \u2202 log pyi(xi;w)/\u2202w, {xj}nj=1 can be used as samples from q(x). In this way, the computational cost of the generative gradient is about the same as that of the discriminative gradient.\nMoreover, the computation of the generative gradient can be induced to share the same back propagation architecture as the discriminative gradient. Specifically, the calculation of the generative gradient can be decoupled into the calculation at a new generative loss layer and the calculation at lower layers. To be more specific, by replacing {x\u0303j}mj=1 in (8) by the batch sample {xj}nj=1, we can rewrite (8) in the following form:\n\u2202\n\u2202w log pyi(xi;w) \u2248 \u2211 y,j \u2202 log pyi(xi;w) \u2202fy(xj ;w) \u2202fy(xj ;w) \u2202w , (9)\nwhere \u2202 log pyi(xi;w)/\u2202fy(xj ;w) is called the generative loss layer (to be defined below, with fy(xj ;w) being treated here as a variable in the chain rule), while the calculation of \u2202fy(xj ;w)/\u2202w is exactly the same as that in the discriminative gradient. This decoupling brings simplicity to programming.\nWe use the notation \u2202 log pyi(xi;w)/\u2202fy(xj ;w) for the top generative layer mainly to make it conformal to the chain rule calculation. According to (8), \u2202 log pyi(xi;w)/\u2202fy(xj ;w) is defined by\n\u2202 log pyi(xi;w)\n\u2202fy(xj ;w) =  0 y 6= yi; 1\u2212 exp(fyi(xj ;w))\u2211 k exp(fyi(xk;w)) y = yi, j = i;\n\u2212 exp(fyi(xj ;w))\u2211 k exp(fyi(xk;w))\ny = yi, j 6= i.\n(10)\nDuring training, on a batch of training examples, {(xi, yi), i = 1, ..., n}, the generative loglikelihood is\nlG(w) = \u2211 i log p(xi|yi, w) = \u2211 i log exp (fy(x;w)) Zy(w) \u2248 \u2211 i log exp (fy(x;w))\u2211 i exp (fy(xi;w)) /n .\nFigure 1: Samples from the nodes at the final fully-connected layer in the fully trained LeNet model, which correspond to different handwritten digits.\n1\nThe gradient with respect to w is\nl\u2032G(w) = \u2211 i\n[ \u2202\n\u2202w fyi(xi;w)\u2212 \u2211 j \u2202 \u2202w fyi(xj ;w) exp(fyi(xj ;w))\u2211 k exp(fyi(xk;w))\n] .\nWhile the discriminative log-likelihood is\nlD(w) = \u2211 i log p(yi|xi, w) = \u2211 i log exp(fyi(xi;w))\u2211 y exp(fy(xi;w)) .\nThe gradient with respect to w is\nl\u2032D(w) = \u2211 i\n[ \u2202\n\u2202w fyi(xi;w)\u2212 \u2211 y \u2202 \u2202w fy(xi;w) exp(fy(xi;w))\u2211 y exp(fy(xi;w))\n] .\nl\u2032D and l \u2032 G are similar in form and different in the summation operations. In l \u2032 D, the summation is over category y while xi is fixed, whereas in l\u2032G, the summation is over example xj while yi is fixed.\nIn the generative gradient, we want fyi to assign high score to xi as well as those observations that belong to yi, but assign low scores to those observations that do not belong to yi. This constraint is for the same fyi , regardless of what other fy do for y 6= yi. In the discriminative gradient, we want fy(xi) to work together for all different y, so that fyi assigns high score to xi than other fy for y 6= yi. Apparently, the discriminative constraint is weaker because it involves all fy , and the generative constraint is stronger because it involves single fy . After generative learning, these fy are well behaved and then we can continue to adjust them (probably the intercepts for different y) to satisfy the discriminative constraint."}, {"heading": "3.4 GENERATIVE VISUALIZATION", "text": "Recently, people are interested in understanding what the machine learns. Suppose we care about the node at the top layer. The idea can be applied to the nodes at any layer.\nWe consider generating samples from py(x;w) withw already learned by discriminative training (or any other methods). For this purpose, we need to assume a parametric reference distribution q(x), such as Gaussian white noise distribution, which is the maximum entropy distribution or the most featureless distribution fitted to the observed training images with normalized marginal variances. After discriminatively learning fy(x;w) for all y, we can sample from the corresponding py(x;w) by Hamiltonian Monte Carlo (HMC) (Neal, 2011).\nWe can assume a parametric reference distribution q(x), such as white noise distribution. We may include q(x) as a null category for discriminative learning. Then we can sample py(x;w) by Hamiltonian Monte Carlo. Specifically, we can write py(x;w) as py(x;w) \u221d exp(\u2212U(x)), where U(x) = \u2212fy(x;w) + 12 |x| 2 (assuming \u03c32 = 1). In physics context, x can be regarded as a\nposition vector and U(x) the potential energy function. To allow Hamiltonian dynamics to operate, we need to introduce an auxiliary momentum vector \u03c6 and the corresponding kinetic energy function K(\u03c6) = |\u03c6|2/2m, where m represents the mass. After that, a fictitious physical system described by the canonical coordinates (x, \u03c6) is defined, and its total energy is H(x, \u03c6) = U(x) + K(\u03c6). Instead of sampling from p(x;w) directly, HMC samples from the joint canonical distribution p(x, \u03c6) \u221d exp(\u2212H(x, \u03c6)), under which x \u223c p(x) marginally and \u03c6 follows a Gaussian distribution and is independent of x. Each time HMC draws a random sample from the marginal Gaussian distribution of \u03c6, and then evolve according to the Hamiltonian dynamics that conserves the total energy.\nIn practical implementation, the leapfrog algorithm (Neal, 2011) is used to discretize the continuous Hamiltonian dynamics as follows, with being the step-size:\n\u03c6(t+ /2) = \u03c6(t) \u2212 ( /2)\u2202U \u2202x (x(t)), (11)\nx(t+ ) = x(t) + \u03c6(t+ /2)\nm , (12)\n\u03c6(t+ ) = \u03c6(t+ /2) \u2212 ( /2)\u2202U \u2202x (x(t+ )), (13)\nthat is, a half-step update of \u03c6 is performed first and then it is used to compute x(t+ ) and \u03c6(t+ ). The discretization of the leapfrog algorithm cannot keep H(x, \u03c6) exactly constant, so a Metropolis acceptance/rejection stage is used to correct the discretization error.\nA key step in the leapfrog algorithm is the computation of the derivative of the potential energy function \u2202U/\u2202x, which involves calculating \u2202fy(x;w)/\u2202x. Note that the above derivative is with respect to x, not with respect to w. There are two types of structures in fy(x;w). One is rectified linear unit r = max( \u2211 i wiai, 0), where we incorporate the bias term into the sum with ai = \u22121. Then \u2202r/\u2202ai = wi if r > 0, and \u2202r/\u2202ai = 0 if r = 0. The other structure is local max pooling unit with r = maxi(ai). Then \u2202r/\u2202ai = 1 if ai is the maximum, and \u2202r/\u2202ai = 0 otherwise. That is, the derivative is the arg-max un-pooling. Therefore the computation of \u2202fy(x;w)/\u2202x involves two steps: (1) Bottom-up scoring for computing r, which consists of convolution steps and local max pooling steps. (2) Top-down derivative computation which involves deconvolution steps for taking derivatives of the rectified linear units and the arg-max un-pooling steps for taking derivatives of the local max pooling units. The derivative calculation is implemented within each leapfrog step. They are different from the scheme in Zeiler & Fergus (2013). The visualization sequence of a sample is shown in Fig. 1.\nThe above method can be adapted to visualize the nodes at lower layers too. We only need to use the corresponding scoring function fy(x;w) for a node y."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 GENERATIVE PRE-TRAINING", "text": "In generative pre-training experiments, three different training approaches are studied: i) discriminative gradient (DG); ii) generative gradient (GG); iii) generative gradient pre-training + discriminative gradient tuning (GG+DG). We build algorithms on the code of Caffe (Jia et al., 2014) and the experiment settings are identical to Jia et al. (2014). Experiments are performed on two widely used image classification benchmarks: namely MNIST (LeCun et al., 1998) handwritten digit recognition and ImageNet ILSVRC-2012 (Deng et al., 2009) natural image classification.\nMNIST handwritten digit recognition. We first study generative pre-training on the MNIST dataset. The \u201cLeNet\u201d network (LeCun et al., 1998) is utilized, which is default for MNIST in Caffe. Although higher accuracy can be achieved by utilizing deeper networks, random image distortion etc, here we stick to the baseline network for fair comparison and experimental efficiency. Network training and testing are performed on the train and test sets respectively. For all the three training approaches, stochastic gradient descent is performed in training with a batch size of 64, a base learning rate of 0.01, a weight decay term of 0.0005, a momentum term of 0.9, and a max epoch number of 25. For GG+DG, the pre-training stage stops after 16 epochs and the discriminative gradient tuning stage starts with a base learning rate of 0.003.\nThe experimental results are presented in Table 1. The error rate of LeNet trained by discriminative gradient is 1.03%. When trained by generative gradient, the error rate reduces to 0.85%. When generative gradient pre-training and discriminative gradient tuning are both applied, the error rate further reduces to 0.78%, which is 0.25% (24% relatively) lower than that of discriminative gradient.\nImageNet ILSVRC-2012 natural image classification. In experiments on ImageNet ILSVRC2012, two networks are utilized, namely \u201cAlexNet\u201d (Krizhevsky et al., 2012) and \u201cZeilerFergusNet\u201d (fast) (Zeiler & Fergus, 2013). Network training and testing are performed on the train and val sets respectively. In training, a single network is trained by stochastic gradient descent with a batch size of 256, a base learning rate of 0.01, a weight decay term of 0.0005, a momentum term of 0.9, and a max epoch number of 70. For GG+DG, the pre-training stage stops after 45 epochs and the discriminative gradient tuning stage starts with a base learning rate of 0.003. In testing, top-1 classification error rates are reported on the val set by classifying the center and the four corner crops of the input images.\nAs shown in Table 2, the error rates of discriminative gradient training applied on AlexNet and ZeilerFergusNet are 40.7% and 38.4% respectively. While the error rates of generative gradient are 45.8% and 44.3% respectively. Generative gradient pre-training followed by discriminative gradient tuning achieves error rates of 39.6% and 37.4% respectively, which are 1.1% and 1.0% lower than those of discriminative gradient.\nExperiment results on MNIST and ImageNet ILSVRC-2012 show that generative gradient pretraining followed by discriminative gradient tuning consistently improves the classification accuracies for varying networks. At the beginning stage of training, updating network parameters according to the generative gradient provides useful pre-training, which leads the network parameters to a good local optimum.\nAs to the computational cost, generative gradient is on par with discriminative gradient. The computational cost of the generative loss layer itself is ignorable in the network compared to the computation at the convolutional layers and the fully-connected layers. The total epoch numbers of GG+DG is on par with that of DG. Better accuracies can be achieved by generative gradient pre-training without introducing additional computation overhead."}, {"heading": "4.2 GENERATIVE VISUALIZATION", "text": "In the generative visualization experiments, we visualize the nodes of the LeNet network and the AlexNet network trained by discriminative gradient on MNIST and ImageNet ILSVRC-2012 respectively. The networks trained by generative gradient can be visualized by the same algorithm as well.\nWe first visualize the nodes at the final fully-connected layer of LeNet. In experiments, we delete the drop-out layer to avoid unnecessary noise for visualization. At the beginning of visualization, x is initialized from q(x) \u221d exp(\u2212|x|2/2). The HMC iteration number, the leapfrog step size, the leapfrog step number, and the particle mass are set to be 100, 0.1, 50, and 0.01 respectively. The\nUnder review as a conference paper at ICLR 2015\nIteration 0 Iteration 10 Iteration 50 Iteration 100 Iteration 500\nFigure 1: Samples from the nodes at the final fully-connected layer in the fully trained LeNet model, which correspond to different handwritten digits.\n1\nFigure 2: Samples from the nodes at the final fully-connected layer in the fully trained LeNet model, which correspond to different handwritten digits.\nvisualization results are shown in Fig. 2. The samples drawn from a node clearly correspond to the handwritten digit of the corresponding category, and are of varying shapes.\nWe further visualize the nodes in AlexNet, which is a much larger network compared to LeNet. Both nodes from the intermediate convolutional layers (conv1 to conv5) and the final fully-connected layer (fc8) are visualized. The leapfrog step size, the leapfrog step number, and the particle mass are set to be 1, 25, and 0.3 respectively. The HMC iteration numbers are 100 and 500 for nodes from the intermediate convolutional and the final fully-connected layers respectively.\nThe samples from the intermediate convolutional layers of AlexNet are shown in Fig. 3, while those from the final fully-connected layer are shown in Fig. 4 and Fig. 5. The HMC algorithm produces meaningful and varied samples, which vividly reveals what is learned by nodes at different hierarchies in the network. Note that such samples are generated by the HMC algorithm from the trained model directly, without using a large hold-out collection of images as in Girshick et al. (2013); Zeiler & Fergus (2013); Long et al. (2014).\nAs to the computational cost, it varies for nodes at different layers within different networks. On a desktop with GTX Titian, it takes about 0.4 minute to draw a sample for nodes at the final fullyconnected layer of LeNet. In AlexNet, for nodes at the first convolutional layer and at the final fully-connected layer, it takes about 0.5 minute and 12 minute to draw a sample respectively."}, {"heading": "5 DISCUSSION", "text": "Generative modeling is a fundamental problem of statistical learning. A good learning machine should have both strong discriminative and generative capacities. Given the recent successes of CNNs, it is worthwhile to explore their generative aspects. In this work, we show that a simple generative model can be constructed based on the CNN. The generative model helps to pre-train the CNN. It also helps to visualize the knowledge of the learned CNN.\nThe proposed scheme for visualizing a node by assuming a white noise reference distribution can be easily turned into a learning algorithm where the expectation in the generative gradient can be\n(a) conv1 (b) conv2\n(c) conv3\n(d) conv4\n(e) conv5\nFigure 2: Samples from the nodes at the intermediate convolutional layers (conv1 to conv5) in the fully trained AlexNet model.\n2\nFigure 3: Samples from the t e intermediate convolutional layers (conv1 to c nv5) in the fully trained AlexNet model.\napproximated by simple average over synthetic images generated from the current distribution, instead of the weighted average over examples from the non-parametric reference distribution. We shall explore this in further work."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The convolutional neural networks (CNNs) have proven to be a powerful tool for<lb>discriminative learning. Recently researchers have also started to show interest in<lb>the generative aspects of CNNs in order to gain a deeper understanding of what<lb>they have learned and how to further improve them. This paper investigates gen-<lb>erative modeling of CNNs. The main contributions include: (1) We construct a<lb>generative model for the CNN in the form of exponential tilting of a reference<lb>distribution. (2) We propose a generative gradient for pre-training CNNs by a<lb>non-parametric importance sampling scheme, which is fundamentally different<lb>from the commonly used discriminative gradient, and yet has the same computa-<lb>tional architecture and cost as the latter. (3) We propose a generative visualization<lb>method for the CNNs by sampling from an explicit parametric image distribution.<lb>The proposed visualization method can directly draw synthetic samples for any<lb>given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm,<lb>without resorting to any extra hold-out images. Experiments on the challeng-<lb>ing ImageNet benchmark show that the proposed generative gradient pre-training<lb>consistently helps improve the performances of CNNs, and the proposed gener-<lb>ative visualization method generates meaningful and varied samples of synthetic<lb>images from a large-scale deep CNN.", "creator": "LaTeX with hyperref package"}}}