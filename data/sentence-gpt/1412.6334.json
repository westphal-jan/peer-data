{"id": "1412.6334", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Leveraging Monolingual Data for Crosslingual Compositional Word Representations", "abstract": "In this work, we present a novel neural network based architecture for inducing compositional crosslingual word representations. Unlike previously proposed methods, our method fulfills the following three criteria; it constrains the word-level representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely semantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classification task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 91.5% and 84.0% accuracy for the English to German and German to English sub-tasks respectively. The latter being an absolute improvement upon the previous state of the art by 7.3% points of accuracy and an improvement of 31.3% in error reduction. Furthermore, the number of non-linear consonants is not reduced by 5% and 15% at the same time, despite a significantly reduced number of consonants, and it may still be slightly improved for other speakers than the current state of the art. Our technique provides a clear visual representation of the composition of the words, using a cross-lingual addition criterion. The most frequent category of crosslingual word representations is the simple category of words in a single sentence.\n\n\n\nWe refer to a model of interlocutors that have been proposed to describe the following interlocutors: words, the most frequent category of words, the most frequent category of words, and the most frequent category of words. The most common category of words in the language of the world is the English, which is composed in English. In practice, this is the largest category in the language of the world, at about 60 million words. The only exception is the German. In the study of interlocutors we used the system described above (E.T. E.T. E.T., German: U.K., English: Germany), in which sentences are translated in two languages (e.g., the English and the German). The results are thus well represented for the English and the German.\nThe English is the dominant and most commonly spoken language of the world in the world,", "histories": [["v1", "Fri, 19 Dec 2014 13:23:35 GMT  (370kb)", "https://arxiv.org/abs/1412.6334v1", null], ["v2", "Thu, 26 Feb 2015 07:44:39 GMT  (460kb)", "http://arxiv.org/abs/1412.6334v2", null], ["v3", "Tue, 31 Mar 2015 08:03:57 GMT  (460kb)", "http://arxiv.org/abs/1412.6334v3", null], ["v4", "Sat, 22 Aug 2015 15:22:26 GMT  (460kb)", "http://arxiv.org/abs/1412.6334v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hubert soyer", "pontus stenetorp", "akiko aizawa"], "accepted": true, "id": "1412.6334"}, "pdf": {"name": "1412.6334.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["soyer@nii.ac.jp", "pontus@stenetorp.se", "aizawa@nii.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n63 34\nv4 [\ncs .C\nL ]"}, {"heading": "1 INTRODUCTION", "text": "Dense vector representations (embeddings) of words and phrases, as opposed to discrete feature templates, have recently allowed for notable advances in the state of the art of Natural Language Processing (NLP) (Socher et al., 2013; Baroni et al., 2014). These representations are typically induced from large unannotated corpora by predicting a word given its context (Collobert & Weston, 2008). Unlike discrete feature templates, these representations allow supervised methods to readily make use of unlabeled data, effectively making them semi-supervised (Turian et al., 2010).\nA recent focus has been on crosslingual, rather than monolingual, representations. Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be transformed between languages (Klementiev et al., 2012; Mikolov et al., 2013b; Hermann & Blunsom, 2014). In particular, crosslingual representations can be helpful for tasks such as translation or to leverage training data in a source language when little or no training data is available for a target language. Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification (Klementiev et al., 2012).\nMikolov et al. (2013b) induced language-specific word representations, learned a linear mapping between the language-specific representations using bilingual word pairs and evaluated their approach\n\u2217Currently at the University College London.\nfor single word translation. Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language. They also introduced a dataset for crosslingual document classification and evaluated their work on this task. Hermann & Blunsom (2014) introduced a method to induce compositional crosslingual word representations from sentence-aligned bilingual corpora. Their method is trained to distinguish the sentence pairs given in a bilingual corpus from randomly generated pairs. The model represents sentences as a function of their word representations, encouraging the word representations to be compositional. Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar.\nHowever, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not encouraged by the monolingual objective, which may be problematic when composing word representations for a phrase or document-level task. While the method of Hermann & Blunsom (2014) allows for arbitrary composition functions, they are limited to using sentence-aligned bilingual data and it is not immediately obvious how their method can be extended to make use of monolingual data. Lastly, while the method of Chandar A P et al. (2014) suffers from neither of the above issues, their method represents each sentence as a bag of words vector with the size of the whole vocabulary. This leads to computational scaling issues and necessitates a vocabulary cut-off which may hamper performance for compounding languages such as German.\nThe question that we pose is thus, can a single method\n1. Constrain the word-level representations to be compositional.\n2. Leverage both monolingual and bilingual data.\n3. Scale to large vocabulary sizes without greatly impacting training time.\nIn this work, we propose a neural network based architecture for creating crosslingual compositional word representations. The method is agnostic to the choice of composition function and combines a bilingual training objective with a novel way of training monolingual word representations. This enables us to draw from a plethora of unlabeled monolingual data, while our method is efficient enough to be trained using roughly seven million sentences in about six hours on a single-core desktop computer. We evaluate our method on a well-established document classification task and achieve results for both sub-tasks that are either comparable or greatly improve upon the previous state of the art. For the German to English sub-task our method achieves 84.4% in accuracy, an error reduction of 33.0% in comparison to the previous state of the art."}, {"heading": "2 MODEL", "text": ""}, {"heading": "2.1 INDUCING CROSSLINGUAL WORD REPRESENTATIONS", "text": "For any task involving crosslingual word representations we distinguish between two kinds of errors\n1. Transfer errors occur due to transferring representations between languages. Ideally, expressions of the same meaning (words, phrases, or documents) should be represented by the same vectors, regardless of the language they are expressed in. The more different these representations are from language 1 (l1) to language 2 (l2), the larger the transfer error.\n2. Monolingual errors occur because the word, phrase or document representations within the same language are not expressive enough. For example, in the case of classification this would mean that the representations do not possess enough discriminative power for a classifier to achieve high accuracy.\nThe way to attain high performance for any task that involves crosslingual word representations is to keep both transfer errors and monolingual errors to a minimum using representations that are both expressive and constrained crosslingually."}, {"heading": "2.2 CREATING REPRESENTATIONS FOR PHRASES AND DOCUMENTS", "text": "Following the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language. Like Hermann & Blunsom (2014), we look up the vector representations for all words of a given sentence in the corresponding lookup table and apply a composition function to transform these word vectors into a sentence representation. To create document representations, we apply the same composition function again, this time to transform the representations of all sentences in a document to a document representation. For the majority of this work we will make use of the addition composition function, which can be written as the sum of all word representations wi in a given phrase\na([w1, w2, \u00b7 \u00b7 \u00b7 , wl]) =\nl\u2211\ni=1\nwi (1)\nTo give an example of another possible candidate composition function, we also use the bigram based addition (Bi) composition function, formalized as\nb([w1, w2, \u00b7 \u00b7 \u00b7 , wl]) = l\u2211\ni=2\ntanh(wi\u22121, wi) (2)\nwhere the hyperbolic tangent (tanh) is wrapped around every word bigram to produce intermediate results that are then summed up. By introducing a non-linear function the Bi composition is no longer a bag-of-vectors function and takes word order into account.\nGiven that neither of the above composition functions involve any additional parameters, the only parameters of our model are in fact the word representations that are shared globally across all training samples."}, {"heading": "2.3 OBJECTIVE", "text": "Following Klementiev et al. (2012) we split our objective into two sub-objectives, a bilingual objective minimizing the transfer errors and a monolingual objective minimizing the monolingual errors for l1 and l2. We formalize the loss over the whole training set as\nLtotal =\nNbi\u2211\ni=1\nLbi(v l1 i , vl2 i ) +\nNmono1\u2211\ni=1\nLmono(x l1 i ) +\nNmono2\u2211\ni=1\nLmono(y l2 i ) + \u03bb\u2016\u03b8\u20162 (3)\nwhere Lbi is the bilingual loss for two aligned sentences, vi is a sample from the set of Nbi aligned sentences in language 1 and 2, Lmono is the monolingual loss which we sum over Nmono1 sentences xl1 i from corpora in language 1 and Nmono2 sentences y l2 i from corpora in language 2. We learn the parameters \u03b8, which represent the whole set of word representations for both l1 and l2. The parameters are used in a shared fashion to construct sentence representations for both the monolingual corpora and the parts of the bilingual corpus corresponding to each language. We regularize \u03b8 using the squared euclidean norm and scale the contribution of the regularizer by \u03bb.\nBoth objectives operate on vectors that represent composed versions of phrases and are agnostic to how a phrase is transformed into a vector. The objective can therefore be used with arbitrary composition functions.\nAn illustration of our proposed method can be found in Figure 1."}, {"heading": "2.3.1 BILINGUAL OBJECTIVE", "text": "Given a pair of aligned sentences, sl1 1 in l1 and s l2 1 in l2, we first compute their vector representations vl1 1 and vl2 1 using the composition function. Since the sentences are either translations of each other\nor at least very close in meaning, we require their vector representations to be similar and express this as minimizing the squared euclidean distance between vl1\n1 and vl2 1 . More formally, we write\nLbi(v l1 , vl2) = \u2016vl1 \u2212 vl2\u20162 (4)\nfor any two vector representations vl1 and vl2 corresponding to the sentences of an aligned translation pair.\nThe bilingual objective on its own is degenerate, since setting the vector representations of all sentences to the same value poses a trivial solution. We therefore combine this bilingual objective with a monolingual objective."}, {"heading": "2.3.2 MONOLINGUAL OBJECTIVE", "text": "The choice of the monolingual objective greatly influences the generality of models for crosslingual word representations. Klementiev et al. (2012) use a neural language model to leverage monolingual data. However, this does not explicitly encourage compositionality of the word representations. Hermann & Blunsom (2014) achieve good results with a noise-contrastive objective, discriminating aligned translation pairs from randomly sampled pairs. However, their approach can only be trained using sentence aligned data, which makes it difficult to extend to leverage unannotated monolingual data. Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al. (2013a) which predicts the context of a word given the word itself. They achieve high accuracy on the German \u2192 English sub-task of the crosslingual document classification task introduced by Klementiev et al. (2012). Chandar A P et al. (2014) presented a bag-of-words auto-encoder model which is the current state of the art for the English \u2192 German sub-task for the same task. Both the auto-encoder based model and BilBOWA require a sentencealigned bilingual corpus, but in addition are capable of leveraging monolingual data. However, due to their bag-of-words based nature, their architectures implicitly restrict how sentence representations are composed from word representations.\nWe extend the idea of the noise-contrastive objective given by Hermann & Blunsom (2014) to the monolingual setting and propose a framework that, like theirs, is agnostic to the choice of composition function and operates on the phrase level. However, our framework, unlike theirs, is able to leverage monolingual data. Our key novel idea is based on the observation that phrases are typically more similar to their sub-phrases than to randomly sampled phrases. We leverage this insight using the hinge loss as follows\nLmono(a) = Lmono(a outer, ainner , bnoise) (5)\nLmono(a outer, ainner , bnoise) = [max(0,m+ \u2016aouterc \u2212 a inner c \u2016 2 \u2212 \u2016aouterc \u2212 b noise c \u2016 2)\n\ufe38 \ufe37\ufe37 \ufe38\nhinge loss\n(6)\n+ \u2016aouter c \u2212 ainner c\n\u20162] \u00b7 len(ainner)\nlen(aouter)\nwhere m is a margin, aouter is a phrase sampled from a sentence, ainner is a sub-phrase of aouter and bnoise is a phrase extracted from a sentence that was sampled uniformly from the corpus. The start and end positions of both phrases and the sub-phrase were chosen uniformly at random within their context and constrained to guarantee a minimum length of 3 words. Subscript c denotes that a phrase has been transformed into its vector representation. We add \u2016aouterc \u2212 a inner c \u2016\n2 to the hinge loss to reduce the influence of the margin as a hyperparameter and to make sure that the we retain an error signal even after the hinge loss objective is satisfied. To compensate for differences in phrase and sub-phrase length we scale the error by the ratio between the number of words in the outer phrase and the inner phrase. Minimizing this objective captures the intuition stated above; a phrase should generally be closer to its sub-phrases, than to randomly sampled phrases.\nThe examples in Figure 2 seek to further clarify this observation. In both examples, the blue area represents the outer phrase (aouter), the red area covers the inner sub-phrase (ainner), and the gray area marks a randomly selected phrase in a randomly sampled noise sentence. The inner workings of the monolingual inclusion objective only become clear when more than one example is considered. In Example 1, ainner is embedded in the same context as in Example 2, while in both examples aouter is contrasted with the same noise phrase. Minimizing the objective brings the representations of both likes to drink beer and likes to eat chips closer to the phrase they are embedded in and makes them less similar to the same noise sentence. Since in both examples the outer phrases are very similar, this causes likes to drink beer and likes to eat chips to be similar. While we picked idealized sentences for demonstration purposes, this relative notion still holds in practice to varying degrees depending on the choice of sentences.\nIn contrast to many recently introduced log-linear models, like the Skip-Gram model, where word vectors are similar if they appear as the center of similar word windows, our proposed objective, using addition for composition, encourages word vectors to be similar if they tend to be embedded in similar phrases. The major difference between these two formulations manifests itself for words that appear close or next to each other very frequently. These word pairs are not usually the center of the same word windows, but they are embedded together in the same phrases.\nFor example: the two word central context of \u201ceat\u201d is \u201cto\u201d and \u201cchips\u201d, whereas the context of \u201cchips\u201d would be \u201ceat\u201d and \u201cwhen\u201d. Using the Skip-Gram model this would cause \u201cchips\u201d and \u201ceat\u201d to be less similar, with \u201cchips\u201d probably being similar to other words related to food and \u201ceat\u201d being similar to other verbs. Employing the inclusion objective, the representations for \u201ceat\u201d and \u201cchips\u201d will end up close to each other since they tend to be embedded in the same phrases. This causes the word representations induced by the inclusion criterion to be more topical in nature. We hypothesize that this property is particularly useful for document classification."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 CROSSLINGUAL DOCUMENT CLASSIFICATION", "text": "Crosslingual document classification constitutes a task where a classifier is trained to classify documents in one language (l1) and is later applied to documents in a different language (l2). This requires either transforming the classifier itself to fit the new language or transforming/sharing representations of the text for both languages. The crosslingual word and document representations\ninduced using the approach proposed in this work present an intuitive way to tackle crosslingual document classification.\nLike previous work, we evaluate our method on the crosslingual document classification task introduced by Klementiev et al. (2012). The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al., 2004) into one of four categories: Economics, Government/Social, Markets, or Corporate. Maintaining the original setup, we train an averaged perceptron (Collins, 2002) for 10 iterations on representations of documents in one language (English/German) and evaluate its performance on representations of documents in the corresponding other language (German/English).\nWe use the original data and the original implementation of the averaged perceptron used by Klementiev et al. (2012) to evaluate the document representations created by our method. There are different versions of the training set of varying sizes, ranging from 100 to 10,000 documents, and the test sets for both languages contain 5,000 documents. Most related work only reports results using the 1,000 documents sized training set. Following previous work, we tune the hyperparameters of our model on held out documents in the same language that the model was trained on."}, {"heading": "3.2 INDUCING CROSSLINGUAL WORD REPRESENTATIONS", "text": "To induce representations using the method proposed in this work, we require at least a bilingual corpus of aligned sentences. In addition, our model allows the representations to draw upon monolingual data from either or both languages. Like Klementiev et al. (2012) we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources. To avoid a testing bias, we exclude all documents that are part of the crosslingual classification task. We detect sentence boundaries using pre-trained models of the Punkt tokenizer (Kiss & Strunk, 2006) shipped with NLTK1 and perform tokenization and lowercasing with the scripts deployed with the cdec decoder2. Following Turian et al. (2010) we remove all English sentences (and their German correspondences in EuroParl) that have a lowercase\nnonlowercase\nratio of less than 0.9. This affects mainly headlines and reports with numbers. In total it reduces the number of sentences in EuroParl by about 255, 000 and the English part of the Reuters corpus by about 8 million. Since German features more upper case characters than English we set the cutoff ratio to 0.7, which reduces the number of sentences by around 620, 000. Further, we replace words that occur less than a certain threshold by an UNK token. Corpus statistics and thresholds are reported in Table 1.\nWe initialize all word representations with noise samples from a Gaussian with \u00b5 = 0, \u03c3 = 0.1 and optimize them in a stochastic setting to minimize the objective defined in Equation 3. To speed up the convergence of training we use AdaGrad (Duchi et al., 2011). We tuned all hyperparameters of our model and explored learning rates around 0.2, mini-batch sizes around 40,000, hinge loss margins around 40 (since our vector dimensionality is 40) and \u03bb (regularization) around 1.0. We trained all versions that use the full monolingual data for 25 iterations (= 25\u00d7 4.5 million samples) and the versions only involving bilingual data for 100 iterations on their training sets. Training our model3, implemented in a high-level, dynamic programming language (Bezanson et al., 2012),\n1http://www.nltk.org/ 2http://www.cdec-decoder.org/ 3Our implementation is available at https://github.com/ogh/binclusion\nfor the largest set of data takes roughly six hours on a single-core desktop computer. This can be compared to for example Chandar A P et al. (2014) which train their auto-encoder model for 3.5 days."}, {"heading": "4 RESULTS", "text": ""}, {"heading": "4.1 CROSSLINGUAL DOCUMENT CLASSIFICATION", "text": "We compare our method to various architectures introduced in previous work. As these methods differ in their ability to handle monolingual data, we evaluate several versions of our model using different data sources and sizes for training. Also, we follow the lines of previous work and use 40-dimensional word representations. We report results when using the first 500,000 sentence pairs of EuroParl (Euro500k), the full EuroParl corpus (EuroFull), the first 500,000 sentence pairs of EuroParl and the German and English text from the Reuters corpus as monolingual data (Euro500kReuters), and one version using the full EuroParl and Reuters corpus (EuroFullReuters). Table 2 shows results for all these configurations. The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from Klementiev et al. (2012).\nOur method achieves results that are comparable or improve upon the previous state of the art for all dataset configurations. It advances the state of the art for the EN \u2192 DE sub-task by 0.9% points of accuracy and greatly outperforms the previous state of the art for the DE \u2192 EN sub-task, where it yields an absolute improvement of 7.7% points of accuracy. The latter corresponds to an error reduction of 33.0% in comparison to the previous state of the art.\nAn important observation is that including monolingual data is strongly beneficial for the classification accuracy. We found increases in performance to 80.6% for DE \u2192 EN and 88.6% accuracy for EN \u2192 DE, even when using as little as 5% of the monolingual data. We hypothesize that the key cause of this effect is domain adaptation. From this observation it is also worth pointing out that our method is on par with the previous state of the art for the DE \u2192 EN sub-task using no monolingual training data and would improve upon it using as little as 5% of the monolingual data. To show that our method achieves high accuracy even with a reduced vocabulary, we discard representations for infrequent terms and report results using our best setup with the same vocabulary size as Klementiev et al. (2012)."}, {"heading": "4.2 INTERESTING PROPERTIES OF THE INDUCED CROSSLINGUAL WORD REPRESENTATIONS", "text": "For a bilingual word representation model that uses monolingual data, the most difficult cases to resolve are words appearing in the monolingual data, but not in the bilingual data. Since the model does not have any kind of direct signal regarding what translations these words should correspond to, their location in the vector space is entirely determined by how the monolingual objective arranges them. Therefore, looking specifically at these difficult examples presents a good way to get an impression of how well the monolingual and bilingual objective complement each other.\nIn Table 3, we list some of the most frequently occurring words that are present in the monolingual data but not in the bilingual data. The nearest neighbors are topically strongly related to their corresponding queries. For example, the credit-rating agency Standard & Poor\u2019s (s&p) is matched to rating-related words, soybeans is proximal to crop and food related terms, forex features a list of currency related terms, and the list for stockholders, includes aktiona\u0308re, its correct German translation. This speaks strongly in favor of how our objectives complement each other, even though these words were only observed in the monolingual data they relate sensibly across languages.\nTo convey an impression of how the induced representations behave, not interlingually, but within the same language, we list some examples in Table 4. The semi-conductor chip maker intel, is very close to IT-related companies like ibm or netscape and also to microprocessor-related terms. For the verb fly, the nearest neighbors not only include forms like flying, but also related nouns like airspace or air, underlining the topical nature of our proposed objective."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "In this work we introduced a method that can induce compositional crosslingual word representations while scaling to large datasets. Our novel approach for learning monolingual representations integrates naturally with our bilingual objective and allows us to make use of sentence-aligned bilingual corpora as well as monolingual data. The method is agnostic to the choice of composition function, enabling more complex (e.g. preserving word order information) ways to compose phrase representations from word representations. For crosslingual document classification (Klementiev et al., 2012) our models perform comparably or greatly improve upon previously reported results.\nTo increase the expressiveness of our method we plan to investigate more complex composition functions, possibly based on convolution to preserve word order information. We consider the monolingual inclusion objective worthy of further research on its own and will evaluate its performance in comparison to related methods when learning word representations from monolingual data."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by the Data Centric Science Research Commons Project at the Research Organization of Information and Systems and by the Japan Society for the Promotion of Science KAKENHI Grant Number 13F03041."}], "references": [{"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni", "Marco", "Dinu", "Georgiana", "Kruszewski", "Germ\u00e1n"], "venue": "In ACL, pp", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Julia: A Fast Dynamic Language for", "author": ["Bezanson", "Jeff", "Karpinski", "Stefan", "Shah", "Viral B", "Edelman", "Alan"], "venue": "Technical Computing. arXiv,", "citeRegEx": "Bezanson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bezanson et al\\.", "year": 2012}, {"title": "An Autoencoder Approach to Learning Bilingual Word Representations", "author": ["Chandar A P", "Sarath", "Lauly", "Stanislas", "Larochelle", "Hugo", "Khapra", "Mitesh", "Ravindran", "Balaraman", "Raykar", "Vikas C", "Saha", "Amrita"], "venue": "In NIPS,", "citeRegEx": "P et al\\.,? \\Q2014\\E", "shortCiteRegEx": "P et al\\.", "year": 2014}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Collins", "Michael"], "venue": "In EMNLP, pp", "citeRegEx": "Collins and Michael.,? \\Q2002\\E", "shortCiteRegEx": "Collins and Michael.", "year": 2002}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML, pp", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "BilBOWA: Fast Bilingual Distributed Representations without", "author": ["Gouws", "Stephan", "Bengio", "Yoshua", "Corrado", "Greg"], "venue": "Word Alignments. arXiv,", "citeRegEx": "Gouws et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Multilingual Models for Compositional Distributed Semantics", "author": ["Hermann", "Karl Moritz", "Blunsom", "Phil"], "venue": "In ACL, pp", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Unsupervised Multilingual Sentence", "author": ["Kiss", "Tibor", "Strunk", "Jan"], "venue": "Boundary Detection. CL,", "citeRegEx": "Kiss et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kiss et al\\.", "year": 2006}, {"title": "Inducing Crosslingual Distributed Representations of Words", "author": ["Klementiev", "Alexandre", "Titov", "Ivan", "Bhattarai", "Binod"], "venue": "In COLING, pp", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Koehn", "Philipp"], "venue": "In MT summit,", "citeRegEx": "Koehn and Philipp.,? \\Q2005\\E", "shortCiteRegEx": "Koehn and Philipp.", "year": 2005}, {"title": "RCV1: A New Benchmark Collection for Text Categorization", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "Research. JMLR,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In ICLR Workshop,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "Translation. arXiv,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew", "Potts", "Christopher"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Co-training for Cross-lingual Sentiment Classification", "author": ["Wan", "Xiaojun"], "venue": "In ACL, pp", "citeRegEx": "Wan and Xiaojun.,? \\Q2009\\E", "shortCiteRegEx": "Wan and Xiaojun.", "year": 2009}], "referenceMentions": [{"referenceID": 14, "context": "Dense vector representations (embeddings) of words and phrases, as opposed to discrete feature templates, have recently allowed for notable advances in the state of the art of Natural Language Processing (NLP) (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 210, "endOffset": 252}, {"referenceID": 0, "context": "Dense vector representations (embeddings) of words and phrases, as opposed to discrete feature templates, have recently allowed for notable advances in the state of the art of Natural Language Processing (NLP) (Socher et al., 2013; Baroni et al., 2014).", "startOffset": 210, "endOffset": 252}, {"referenceID": 15, "context": "Unlike discrete feature templates, these representations allow supervised methods to readily make use of unlabeled data, effectively making them semi-supervised (Turian et al., 2010).", "startOffset": 161, "endOffset": 182}, {"referenceID": 9, "context": "Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be transformed between languages (Klementiev et al., 2012; Mikolov et al., 2013b; Hermann & Blunsom, 2014).", "startOffset": 233, "endOffset": 306}, {"referenceID": 9, "context": "Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification (Klementiev et al., 2012).", "startOffset": 130, "endOffset": 155}, {"referenceID": 0, "context": ", 2013; Baroni et al., 2014). These representations are typically induced from large unannotated corpora by predicting a word given its context (Collobert & Weston, 2008). Unlike discrete feature templates, these representations allow supervised methods to readily make use of unlabeled data, effectively making them semi-supervised (Turian et al., 2010). A recent focus has been on crosslingual, rather than monolingual, representations. Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be transformed between languages (Klementiev et al., 2012; Mikolov et al., 2013b; Hermann & Blunsom, 2014). In particular, crosslingual representations can be helpful for tasks such as translation or to leverage training data in a source language when little or no training data is available for a target language. Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification (Klementiev et al., 2012). Mikolov et al. (2013b) induced language-specific word representations, learned a linear mapping between the language-specific representations using bilingual word pairs and evaluated their approach \u2217Currently at the University College London.", "startOffset": 8, "endOffset": 1134}, {"referenceID": 7, "context": "Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language. They also introduced a dataset for crosslingual document classification and evaluated their work on this task. Hermann & Blunsom (2014) introduced a method to induce compositional crosslingual word representations from sentence-aligned bilingual corpora.", "startOffset": 0, "endOffset": 360}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al.", "startOffset": 187, "endOffset": 239}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data.", "startOffset": 187, "endOffset": 294}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al.", "startOffset": 187, "endOffset": 605}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al.", "startOffset": 187, "endOffset": 629}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective.", "startOffset": 187, "endOffset": 650}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not encouraged by the monolingual objective, which may be problematic when composing word representations for a phrase or document-level task. While the method of Hermann & Blunsom (2014) allows for arbitrary composition functions, they are limited to using sentence-aligned bilingual data and it is not immediately obvious how their method can be extended to make use of monolingual data.", "startOffset": 187, "endOffset": 960}, {"referenceID": 2, "context": "Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not encouraged by the monolingual objective, which may be problematic when composing word representations for a phrase or document-level task. While the method of Hermann & Blunsom (2014) allows for arbitrary composition functions, they are limited to using sentence-aligned bilingual data and it is not immediately obvious how their method can be extended to make use of monolingual data. Lastly, while the method of Chandar A P et al. (2014) suffers from neither of the above issues, their method represents each sentence as a bag of words vector with the size of the whole vocabulary.", "startOffset": 187, "endOffset": 1216}, {"referenceID": 8, "context": "Following the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al.", "startOffset": 22, "endOffset": 47}, {"referenceID": 8, "context": "Following the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al.", "startOffset": 22, "endOffset": 73}, {"referenceID": 6, "context": "(2012); Hermann & Blunsom (2014); Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language.", "startOffset": 34, "endOffset": 54}, {"referenceID": 6, "context": "(2012); Hermann & Blunsom (2014); Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language. Like Hermann & Blunsom (2014), we look up the vector representations for all words of a given sentence in the corresponding lookup table and apply a composition function to transform these word vectors into a sentence representation.", "startOffset": 34, "endOffset": 176}, {"referenceID": 9, "context": "Following Klementiev et al. (2012) we split our objective into two sub-objectives, a bilingual objective minimizing the transfer errors and a monolingual objective minimizing the monolingual errors for l1 and l2.", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "Klementiev et al. (2012) use a neural language model to leverage monolingual data.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Klementiev et al. (2012) use a neural language model to leverage monolingual data. However, this does not explicitly encourage compositionality of the word representations. Hermann & Blunsom (2014) achieve good results with a noise-contrastive objective, discriminating aligned translation pairs from randomly sampled pairs.", "startOffset": 0, "endOffset": 198}, {"referenceID": 5, "context": "Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al. (2013a) which predicts the context of a word given the word itself.", "startOffset": 0, "endOffset": 131}, {"referenceID": 5, "context": "Gouws et al. (2014) introduced BilBOWA combining a bilingual objective with the Skip-Gram model proposed by Mikolov et al. (2013a) which predicts the context of a word given the word itself. They achieve high accuracy on the German \u2192 English sub-task of the crosslingual document classification task introduced by Klementiev et al. (2012). Chandar A P et al.", "startOffset": 0, "endOffset": 339}, {"referenceID": 2, "context": "Chandar A P et al. (2014) presented a bag-of-words auto-encoder model which is the current state of the art for the English \u2192 German sub-task for the same task.", "startOffset": 10, "endOffset": 26}, {"referenceID": 2, "context": "Chandar A P et al. (2014) presented a bag-of-words auto-encoder model which is the current state of the art for the English \u2192 German sub-task for the same task. Both the auto-encoder based model and BilBOWA require a sentencealigned bilingual corpus, but in addition are capable of leveraging monolingual data. However, due to their bag-of-words based nature, their architectures implicitly restrict how sentence representations are composed from word representations. We extend the idea of the noise-contrastive objective given by Hermann & Blunsom (2014) to the monolingual setting and propose a framework that, like theirs, is agnostic to the choice of composition function and operates on the phrase level.", "startOffset": 10, "endOffset": 557}, {"referenceID": 11, "context": "The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al., 2004) into one of four categories: Economics, Government/Social, Markets, or Corporate.", "startOffset": 119, "endOffset": 139}, {"referenceID": 9, "context": "Like previous work, we evaluate our method on the crosslingual document classification task introduced by Klementiev et al. (2012). The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al.", "startOffset": 106, "endOffset": 131}, {"referenceID": 9, "context": "Like previous work, we evaluate our method on the crosslingual document classification task introduced by Klementiev et al. (2012). The goal is to correctly classify news articles taken from the English and German sections of the RCV1 and RCV2 corpus (Lewis et al., 2004) into one of four categories: Economics, Government/Social, Markets, or Corporate. Maintaining the original setup, we train an averaged perceptron (Collins, 2002) for 10 iterations on representations of documents in one language (English/German) and evaluate its performance on representations of documents in the corresponding other language (German/English). We use the original data and the original implementation of the averaged perceptron used by Klementiev et al. (2012) to evaluate the document representations created by our method.", "startOffset": 106, "endOffset": 749}, {"referenceID": 5, "context": "To speed up the convergence of training we use AdaGrad (Duchi et al., 2011).", "startOffset": 55, "endOffset": 75}, {"referenceID": 1, "context": "Training our model3, implemented in a high-level, dynamic programming language (Bezanson et al., 2012), http://www.", "startOffset": 79, "endOffset": 102}, {"referenceID": 7, "context": "Like Klementiev et al. (2012) we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources.", "startOffset": 5, "endOffset": 30}, {"referenceID": 7, "context": "Like Klementiev et al. (2012) we choose EuroParl v7 (Koehn, 2005) as our bilingual corpus and leverage the English and German parts of the RCV1 and RCV2 corpora as monolingual resources. To avoid a testing bias, we exclude all documents that are part of the crosslingual classification task. We detect sentence boundaries using pre-trained models of the Punkt tokenizer (Kiss & Strunk, 2006) shipped with NLTK1 and perform tokenization and lowercasing with the scripts deployed with the cdec decoder2. Following Turian et al. (2010) we remove all English sentences (and their German correspondences in EuroParl) that have a lowercase nonlowercase ratio of less than 0.", "startOffset": 5, "endOffset": 533}, {"referenceID": 9, "context": "8 I-Matrix (Klementiev et al., 2012) EuroFullReuters 77.", "startOffset": 11, "endOffset": 36}, {"referenceID": 6, "context": "2 BilBOWA (Gouws et al., 2014) Euro500k 86.", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "This can be compared to for example Chandar A P et al. (2014) which train their auto-encoder model for 3.", "startOffset": 46, "endOffset": 62}, {"referenceID": 9, "context": "The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from Klementiev et al. (2012). Our method achieves results that are comparable or improve upon the previous state of the art for all dataset configurations.", "startOffset": 126, "endOffset": 151}, {"referenceID": 9, "context": "The result table includes previous work as well as the Glossed, the machine translation and the majority class baselines from Klementiev et al. (2012). Our method achieves results that are comparable or improve upon the previous state of the art for all dataset configurations. It advances the state of the art for the EN \u2192 DE sub-task by 0.9% points of accuracy and greatly outperforms the previous state of the art for the DE \u2192 EN sub-task, where it yields an absolute improvement of 7.7% points of accuracy. The latter corresponds to an error reduction of 33.0% in comparison to the previous state of the art. An important observation is that including monolingual data is strongly beneficial for the classification accuracy. We found increases in performance to 80.6% for DE \u2192 EN and 88.6% accuracy for EN \u2192 DE, even when using as little as 5% of the monolingual data. We hypothesize that the key cause of this effect is domain adaptation. From this observation it is also worth pointing out that our method is on par with the previous state of the art for the DE \u2192 EN sub-task using no monolingual training data and would improve upon it using as little as 5% of the monolingual data. To show that our method achieves high accuracy even with a reduced vocabulary, we discard representations for infrequent terms and report results using our best setup with the same vocabulary size as Klementiev et al. (2012).", "startOffset": 126, "endOffset": 1415}, {"referenceID": 9, "context": "For crosslingual document classification (Klementiev et al., 2012) our models perform comparably or greatly improve upon previously reported results.", "startOffset": 41, "endOffset": 66}], "year": 2015, "abstractText": "In this work, we present a novel neural network based architecture for inducing compositional crosslingual word representations. Unlike previously proposed methods, our method fulfills the following three criteria; it constrains the wordlevel representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely semantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classification task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for the English to German and German to English sub-tasks respectively. The former advances the state of the art by 0.9% points of accuracy, the latter is an absolute improvement upon the previous state of the art by 7.7% points of accuracy and an improvement of 33.0% in error reduction.", "creator": "LaTeX with hyperref package"}}}