{"id": "1412.6550", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "FitNets: Hints for Thin Deep Nets", "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. We also present a model with an open source, Open Source model, and an open source, OpenSource model for training training models.\n\n\nThe model was recently accepted in the IEEE IEEE Energized Development Conference. This study is a reference to the IEEE Energized Learning Conference. A similar model was recently accepted in the IEEE Energized Learning Conference. This study is a reference to the IEEE Energized Learning Conference. The model was recently accepted in the IEEE Energized Learning Conference. This study is a reference to the IEEE Energized Learning Conference. We also present a model with an open source, Open Source model, and an open source, Open Source model for training models.\nIn this paper, we have also created a model with an open source, Open Source model, and an open source, Open Source model for training models. For further detail, see https://www.flacepark.com/doc/c04967.htm", "histories": [["v1", "Fri, 19 Dec 2014 22:40:51 GMT  (124kb)", "http://arxiv.org/abs/1412.6550v1", null], ["v2", "Fri, 9 Jan 2015 20:56:15 GMT  (124kb)", "http://arxiv.org/abs/1412.6550v2", null], ["v3", "Fri, 27 Feb 2015 18:44:36 GMT  (126kb)", "http://arxiv.org/abs/1412.6550v3", null], ["v4", "Fri, 27 Mar 2015 11:52:28 GMT  (132kb)", "http://arxiv.org/abs/1412.6550v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["adriana romero", "nicolas ballas", "samira ebrahimi kahou", "antoine chassang", "carlo gatta", "yoshua bengio"], "accepted": true, "id": "1412.6550"}, "pdf": {"name": "1412.6550.pdf", "metadata": {"source": "CRF", "title": "FITNETS: HINTS FOR THIN DEEP NETS", "authors": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n65 50\nv1 [\ncs .L\nG ]\n1 9\nD ec\n2 01"}, {"heading": "1 INTRODUCTION", "text": "Deep networks have recently exhibited state-of-the-art performance in computer vision tasks such as image classification and object detection (Simonyan & Zisserman, 2014; Szegedy et al., 2014). However, top-performing systems usually involve very wide and deep networks, with numerous parameters. Once learned, a major drawback of such wide and deep models is that they result in very time consuming systems at inference time, since they need to perform a huge number of multiplications. Moreover, having large amounts of parameters makes the models high memory demanding. For these reasons, wide and deep top-performing networks are not well suited for applications with memory or time limitations.\nThere have been several attempts in the literature to tackle the problem of model compression to reduce the computational burden at inference time. In Bucila et al. (2006), authors propose to train a neural network to mimic the output of a complex and large ensemble. The method uses the ensemble to label unlabeled data and trains the neural network with the data labeled by the ensemble, thus mimicking the function learned by the ensemble and achieving similar accuracy. The idea has been recently adopted in Ba & Caruana (2014) to compress deep and wide networks into shallower but even wider ones, where the compressed model mimics the function learned by the complex model, in this case, by using data labeled by a deep (or an ensemble of deep) networks. More recently, Knowledge Distillation (KD) (Hinton & Dean, 2014) was introduced as a model compression framework, which eases the training of deep networks by following a student-teacher paradigm, in which the student is penalized according to a softened version of the teacher\u2019s output. The framework compresses an ensemble of deep networks (teacher) into a student network of similar depth. To do so, the student is trained to predict the output of the teacher, as well as the true classification labels. All previous works focus on compressing a teacher network or an ensemble of networks into either networks of similar width and depth or into shallower and wider ones; not taking advantage of depth.\nDepth is a fundamental aspect of representation learning, since it encourages the re-use of features, and leads to more abstract and invariant representations at higher layers (Bengio et al., 2013). The\nimportance of depth has been verified (1) theoretically: deep representations are exponentially more expressive than shallow ones for some families of functions (Montufar et al., 2014); and (2) empirically: the two top-performers of ImageNet use deep convolutional networks with 19 and 22 layers, respectively (Simonyan & Zisserman, 2014) and (Szegedy et al., 2014).\nNevertheless, training deep architectures has proven to be challenging (Larochelle et al., 2007; Erhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly non-convex and non-linear functions. Significant effort has been devoted to alleviate this optimization problem. On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006; Bengio et al., 2007) or supervised (Bengio et al., 2007) train the network parameters in a greedy layerwise fashion in order to initialize the network parameters in a potentially good basin of attraction. The layers are trained one after the other according to an intermediate target. Similarly, semisupervised embedding (Weston et al., 2008) provides guidance to an intermediate layer to help learn very deep networks. More recently, (Chen-Yu et al., 2014; Szegedy et al., 2014; Gulcehre & Bengio, 2013) showed that adding supervision to intermediate layers of deep architectures assists the training of deep networks. Supervision is introduced by stacking a supervised MLP with a softmax layer on top of intermediate hidden layers to ensure their discriminability w.r.t. labels. Alternatively, Curriculum Learning strategies (CL) (Bengio, 2009) tackle the optimization problem by modifying the training distribution, such that the learner network gradually receives examples of increasing and appropriate difficulty w.r.t. the already learned concepts. As a result, curriculum learning acts like a continuation method, speeds up the convergence of the training process and finds potentially better local minima of highly non-convex cost functions.\nIn this paper, we aim to address the network compression problem by taking advantage of depth. We propose a novel approach to train thin and deep networks, called FitNets, to compress wide and shallower (but still deep) networks. The method is rooted in the recently proposed Knowledge Distillation (KD) (Hinton & Dean, 2014) and extends the idea to allow for thinner and deeper student models. We introduce intermediate-level hints from the teacher hidden layers to guide the training process of the student, i.e., we want the student network (FitNet) to learn an intermediate representation that is predictive of the intermediate representations of the teacher network. Hints allow the training of thinner and deeper networks. Results confirm that having deeper models allow us to generalize better, whereas making these models thin help us reduce the computational burden significantly. We validate the proposed method on MNIST, CIFAR-10, CIFAR-100, SVHN and AFLW benchmark datasets and provide evidence that our method matches or outperforms the teacher\u2019s performance, while requiring notably fewer parameters and multiplications."}, {"heading": "2 METHOD", "text": "In this section, we detail the proposed student-teacher framework to train FitNets from shallower and wider nets. First, we review the recently proposed KD. Second, we highlight the proposed hints algorithm to guide the FitNet throughout the training process. Finally, we describe how the FitNet is trained in a stage-wise fashion."}, {"heading": "2.1 REVIEW OF KNOWLEDGE DISTILLATION", "text": "In order to obtain a faster inference, we explore the recently proposed compression framework (Hinton & Dean, 2014), which trains a student network, from the softened output of an ensemble of wider networks, teacher network. The idea is to allow the student network to capture not only the information provided by the true labels, but also the finer structure learned by the teacher network. The framework can be summarized as follows.\nLet T be a teacher network with an output softmax PT = softmax(aT ) where aT is the vector of teacher pre-softmax activations, for some example. In the case where the teacher model is a single network, aT represents the weighted sums of the output layer, whereas if the teacher model is the result of an ensemble either PT or aT are obtained by averaging outputs from different networks (respectively for arithmetic or geometric averaging). Let S be a student network with parameters WS and output probability PS = softmax(aS), where aS is the student\u2019s pre-softmax output. The student network will be trained such that its output PS is similar to the teacher\u2019s output PT, as well as to the true labels ytrue. Since PT might be very close to the one hot code representation of the sample\u2019s true label, a relaxation \u03c4 > 1 is introduced to soften the signal arising from the output\nof the teacher network, and thus, provide more information during training1. The same relaxation is applied to the output of the student network (P\u03c4\nS ), when it is compared to the teacher\u2019s softened\noutput (P\u03c4 T ):\nP\u03c4T = softmax (aT\n\u03c4\n) , P\u03c4S = softmax (aS\n\u03c4\n)\n. (1)\nThe student network is then trained to optimize the following loss function:\nLKD(WS) = H(ytrue,PS) + \u03bbH(P \u03c4 T,P \u03c4 S), (2)\nwhere H refers to the cross-entropy and \u03bb is a tunable parameter to balance both cross-entropies. Note that the first term in Eq. (2) corresponds to the traditional cross-entropy between the output of a (student) network and labels, whereas the second term enforces the student network to learn from the softened output of the teacher network.\nTo the best of our knowledge, KD is designed such that student networks mimic teacher architectures of similar depth. Although we found the KD framework to achieve encouraging results even when student networks have slightly deeper architectures, as we increase the depth of the student network, KD training still suffers from the difficulty of optimizing deep nets (see Section 4.1)."}, {"heading": "2.2 HINT-BASED TRAINING", "text": "In order to help the training of deep FitNets (deeper than their teacher), we introduce hints from the teacher network. A hint is defined as the output of a teacher\u2019s hidden layer responsible for guiding the student\u2019s learning process. Analogously, we choose a hidden layer of the FitNet, the guided layer, to learn from the teacher\u2019s hint layer. We want the guided layer to be able to predict the output of the hint layer. Note that having hints is a form of regularization and thus, the pair hint/guided layer has to be chosen such that the student network is not over-regularized. The deeper we set the guided layer, the less flexibility we give to the network and, therefore, FitNets are more likely to suffer from over-regularization. In our case, we choose the hint to be the middle layer of the teacher network. Similarly, we choose the guided layer to be the middle layer of the student network.\nGiven that the teacher network will usually be wider than the FitNet, the selected hint layer may have more outputs than the guided layer. For that reason, we add a regressor to the guided layer, whose output matches the size of the hint layer. Then, we train the FitNet parameters from the first layer up to the guided layer as well as the regressor parameters by minimizing the following loss function:\nLHT (WGuided,Wr) = 1\n2 ||uh(x;WHint)\u2212 r(vg(x;WGuided);Wr)||\n2, (3)\nwhere uh and vg are the teacher/student deep nested functions up to their respective hint/guided layers with parameters WHint and WGuided, r is the regressor function on top of the guided layer with parameters Wr. Note that the outputs of uh and r have to be comparable, i.e., uh and r must be the same non-linearity.\nNevertheless, using a fully-connected regressor increases the number of parameters and the memory consumption dramatically in the case where the guided and hint layers are convolutional. Let Nh,1\u00d7 Nh,2 and Oh be the teacher hint\u2019s spatial size and number of channels, respectively. Similarity, let Ng,1\u00d7Ng,2 and Og be the FitNet guided layer\u2019s spatial size and number of channels. The number of parameters in the weight matrix of a fully connected regressor is Nh,1\u00d7Nh,2\u00d7Oh\u00d7Ng,1\u00d7Ng,2\u00d7Og. To mitigate this limitation, we use a convolutional regressor instead. The convolutional regressor is designed such that it considers approximately the same spatial region of the input image as the teacher hint. Therefore, the output of the regressor has the same spatial size as the teacher hint. Given a teacher hint of spatial size Nh,1\u00d7Nh,2, the regressor takes the output of the Fitnet\u2019s guided layer of size Ng,1 \u00d7 Ng,2 and adapts its kernel shape k1 \u00d7 k2 such that Ng,i \u2212 ki + 1 = Nh,i, where i \u2208 {1, 2}. The number of parameters in the weight matrix of a the convolutional regressor is k1 \u00d7 k2 \u00d7Oh \u00d7Og, where k1 \u00d7 k2 is significantly lower than Nh,1 \u00d7Nh,2 \u00d7Ng,1 \u00d7Ng,2.\n1For example, as argued by Hinton & Dean (2014), with softened outputs, more information is provided about the relative similarity of the input to classes other than the one with the highest probability."}, {"heading": "2.3 FITNET STAGE-WISE TRAINING", "text": "We train the FitNet in a stage-wise fashion following the student/teacher paradigm. Figure 1 summarizes the training pipeline. Starting from a trained teacher network and a randomly initialized FitNet (Fig. 1 (a)), we add a regressor parameterized by Wr on top of the FitNet guided layer and train the FitNet parameters WGuided up to the guided layer to minimize Eq. (3) (see Fig. 1 (b)). Finally, from the pre-trained parameters, we train the parameters of whole FitNet WS to minimize Eq. (2) (see Fig. 1 (c)). Algorithm 1 details the FitNet training process.\nAlgorithm 1 FitNet Stage-Wise Training. The algorithm receives as input the trained parameters WT of a teacher, the randomly initialized parameters WS of a FitNet, and two indices h and g corresponding to hint/guided layers, respectively. Let WHint be the teacher\u2019s parameters up to the hint layer h. Let WGuided be the FitNet\u2019s parameters up to the guided layer g. Let Wr be the regressor\u2019s parameters. The first stage consists in pre-training the student network up to the guided layer, based on the prediction error of the teacher\u2019s hint layer (line 4). The second stage is a KD training of the whole network (line 6).\nInput: WS,WT, g, h Output: W\u2217S\n1: WHint \u2190 {WT1, . . . ,WTh} 2: WGuided \u2190 {WS1, . . . ,WSg} 3: Intialize Wr to small random values 4: W\u2217Guided \u2190 argmin\nWGuided\nLHT (WGuided,Wr)\n5: {WS1, . . . ,WSg} \u2190 {WGuided\u22171, . . . ,WGuided\u2217g} 6: W\u2217S \u2190 argmin\nWS\nLKD(WS)"}, {"heading": "2.4 RELATION TO CURRICULUM LEARNING", "text": "In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training convergence as well as potentially improve the model generalization by properly choosing a sequence of training distributions seen by the learner: from simple examples to more complex ones. A curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance hints on an intermediate layer during the training, one could considerably ease training. However, Bengio (2009) uses hand-defined heuristics to measure the \u201csimplicity\u201d of an example in a sequence and Gulcehre & Bengio (2013)\u2019s guidance hints require some prior knowledge of the end-task. Both of these curriculum learning strategies tend to be problem-specific.\nOur approach alleviates this issue by using a teacher model. Indeed, intermediate representations learned by the teacher are used as hints to guide the FitNet optimization procedure. In addition, the teacher confidence provides a measure of example \u201csimplicity\u201d by means of teacher cross-entropy term in Eq. (2). This term ensures that examples with a high teacher confidence have a stronger impact than examples with low teacher confidence: the latter correspond to probabilities closer to the uniform distribution, which exert less of a push on the student parameters. In other words, the teacher penalizes the training examples according to its confidence. Note that parameter \u03bb in Eq.\n(2) controls the weight given to the teacher cross-entropy, and thus, the importance given to each example. In order to promote the learning of more complex examples (examples with lower teacher confidence), we gradually anneal \u03bb during the training with a linear decay. The curriculum can be seen as composed of two stages: first learn intermediate concepts via the hint/guided layer transfer, then train the whole student network jointly, annealing \u03bb, which allows easier examples (on which the teacher is very confident) to initially have a stronger effect, but progressively decreasing their importance as \u03bb decays. Therefore, the hint-based training introduced in the paper is a generic curriculum learning approach, where prior information about the task-at-hand is deduced purely from the teacher model.\nAlgorithm # params Accuracy Compression\nFitNet 2.5M 91.61% Teacher 9M 90.18%\nMimic single 54M 84.6% Mimic single 70M 84.9%\nMimic ensemble 70M 85.8% State-of-the-art methods Maxout 90.65% Network in Network 91.2% Deeply-Supervised Networks 91.78%\nTable 1: Accuracy on CIFAR-10\nAlgorithm # params Accuracy Compression\nFitNet 2.5M 64.96% Teacher 9M 63.54%\nState-of-the-art methods Maxout 61.43% Network in Network 64.32% Deeply-Supervised Networks 65.43%\nTable 2: Accuracy on CIFAR-100"}, {"heading": "3 RESULTS ON BENCHMARK DATASETS", "text": "In this section, we show the results on several benchmark datasets. The architectures of all networks as well as the training details are reported in the supplementary material."}, {"heading": "3.1 CIFAR-10 AND CIFAR-100", "text": "The CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009) are composed of 32x32 pixel RGB images belonging to 10 and 100 different classes, respectively. They both contain 50K training images and 10K test images. CIFAR-10 has 1000 samples per class, whereas CIFAR-100 has 100 samples per class. Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening.\nCIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve potentially better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters. Finally, compared to state-of-the-art methods, our algorithm matches the best performers.\nCIFAR-100: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10. As in Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 2 summarizes the obtained results. As in the previous case, our FitNet outperforms the teacher model, reducing the number of parameters by a factor of 3 and, when compared to state-of-the-art methods, the FitNet provides near state-of-the-art performance."}, {"heading": "3.2 SVHN", "text": "The SVHN dataset (Netzer et al., 2011) is composed by 32 \u00d7 32 color images of house numbers collected by GoogleStreet View. There are 73,257 images in the training set, 26,032 images in the test set and 531,131 less difficult examples. We follow the evaluation procedure of Goodfellow et al.\n(2013b) and use their maxout network as teacher. We trained a 13-layer FitNet composed of 11 maxout convolutional layers, a fully-connected layer and a softmax layer.\nAlgorithm # params Misclass Compression\nFitNet \u223c1.5M 2.42% Teacher \u223c4.9M 2.38%\nState-of-the-art methods Maxout 2.47% Network in Network 2.35% Deeply-Supervised Networks 1.92%\nTable 3: SVHN error\nAlgorithm # params Misclass Compression\nTeacher \u223c361K 0.55% Standard backprop \u223c30K 1.9%\nKD \u223c30K 0.65% FitNet \u223c30K 0.51%\nState-of-the-art methods\nMaxout 0.45% Network in Network 0.47% Deeply-Supervised Networks 0.39%\nTable 4: MNIST error\nTable 3 shows that our FitNet achieves comparable accuracy than the teacher despite using only 32% of teacher capacity. Our FitNet is comparable in terms of performance to other state-of-art methods, such as Maxout and Network in Network."}, {"heading": "3.3 MNIST", "text": "As a sanity check for the training procedure, we evaluated the proposed method on the MNIST dataset (LeCun et al., 1998). MNIST is a dataset of handwritten digits (from 0 to 9) composed of 28x28 pixel greyscale images, with 60K training images and 10K test images. We trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The 4th layer of the student network was trained to mimic the 2nd layer of the teacher network.\nTable 4 reports the obtained results. To verify the influence of using hints, we trained the FitNet architecture using either (1) standard backprop, (2) KD or (3) Hint-based Training (HT). When training the FitNet with standard backprop from the softmax layer, the deep and thin architecture achieves 1.9% misclassification error. Using KD, the very same network achieves 0.65%, which confirms the potential of the teacher network; and when adding hints, the error still decreases to 0.51%. Furthermore, the student network achieves slightly better results than the teacher network, while requiring 12 times less parameters."}, {"heading": "3.4 AFLW", "text": "AFLW (Koestinger et al., 2011) is a real-world face database, containing 25K annotated images. In order to evaluate the proposed framework in a face recognition setting, we extracted positive samples by re-sizing the annotated regions of the images to fit 16x16 pixels patches. Similarly, we extracted 25K 16x16 pixels patches not containing faces from ImageNet (Russakovsky et al., 2014) dataset, as negative samples. We used 90% of the extracted patches to train the network.\nIn this experiment, we aimed to evaluate the method on a different kind of architecture. Therefore, we trained a teacher network of 3 ReLU convolutional layers and a sigmoid output layer. We designed a first FitNet (FitNet 1) with 15 times fewer multiplications than the teacher network, and a second FitNet (FitNet 2) with 2.5 times fewer multiplications than the teacher network. Both FitNets have 7 ReLU convolutional layers and a sigmoid output layer.\nThe teacher network achieved 4.21% misclassification error on the validation set. We trained both FitNets by means of KD and HT. On the one hand, we report a misclassification error of 4.58% when training FitNet 1 with KD and a misclassification error of when 2.55% when training it with HT. On the other hand, we report a missclassifation error of 1.95% when training FitNet 2 with KD and a misclassification error of 1.85% when training it with HT. These results show how the method is extensible to different kind of architectures and highlight the benefits of using hints, especially when dealing with thinner architectures."}, {"heading": "4 ANALYSIS OF EMPIRICAL RESULTS", "text": "We empirically investigate the benefits of our approach by comparing various networks trained using standard backpropagation (cross-entropy w.r.t. labels), KD or Hint-based Training (HT). Experiments are performed on CIFAR-10 dataset (Krizhevsky & Hinton, 2009).\nWe compare networks of increasing depth given a fixed computational budget. Each network is composed of successive convolutional layers of kernel size 3 \u00d7 3, followed by a maxout non-linearity and a non-overlapping 2 \u00d7 2 max-pooling. The last max-pooling takes the maximum over all remaining spatial dimensions leading to a 1\u00d7 1 vector representation. We only change the depth and the number of channels per convolution between different networks, i.e. the number of channels per convolutional layer decreases as a network depth increases to respect a given computational budget."}, {"heading": "4.1 ASSISTING THE TRAINING OF DEEP NETWORKS", "text": "In this section, we investigate the impact of HT. We consider two computational budgets of approximately 30M and 107M operations, corresponding to the multiplications needed in an image forward propagation. For each computational budget, we train networks composed of 3, 5, 7 and 9 convolutional layers, followed by a fully-connected layer and a softmax layer. We compare their performances when they are trained with standard backpropagation, KD and HT. Figure 2 reports test on CIFAR-10 using early stopping on the validation set, i.e. we do not retrain our models on the training plus validation sets.\nDue to their depth and small capacity, FitNets are hard to train. As shown in Figure 2(a), we could not train 30M multiplications networks with more than 5 layers with standard backprop. When using KD, we succesfully trained networks up to 7 layers. Adding KD\u2019s teacher cross-entropy to the training objective (Eq. (2)) gives more importance to easier examples, i.e. samples for which the teacher network is confident and, can lead to a smoother version of the training cost (Bengio, 2009). Despite some optimization benefits, it is worth noticing that KD training still suffers from the increasing depth and reaches its limits for 7-layer networks. HT tends to ease these optimization issues and is able to train 13-layer networks of 30M multiplications. The only difference between HT and KD is the starting point in the parameter space: either random or obtained by means of the teacher\u2019s hint. On the one hand, the proliferation of local minima and especially saddle points in highly non-linear functions such as very deep networks highlights the difficulty of finding a good starting point in the parameter space at random (Dauphin et al., 2014). On the other hand, results in Figure 2(a) indicate that HT can guide the student to a better initial position in the parameter space, from which we can minimize the cost through stochastic gradient descent. Therefore, HT provides benefits from an optimization point of view. Networks trained with HT also tend to yield better test performances than the other training methods when we fix the capacity and number of layers. For instance, in Figure 2(b), the 7-layers network, trained with hints, obtains a +0.7% performance gain on the test set compared to the model that does not use any hints (the accuracy increases from 89.45% to 90.1%). As pointed by Erhan et al. (2009), pre-training strategies can act as regularizers. These results suggest that HT is a stronger regularizer than KD, since it leads to better generalization\nperformance on the test set. Finally, Figure 2 highlights that deep models have better performances than shallower ones given a fixed computational budget. Indeed, considering networks that are trained with hints, an 11-layer network outperforms a 5-layer network by an absolute improvement of 4.11% for 107M multiplications and of 3.4% for 30M multiplications. Therefore, the experiments validate our hypothesis that given a fixed number of computations, we leverage depth in a model to achieve faster computation and better generalization.\nIn summary, this experiment shows that (1) using HT, we are able to train deeper models than with standard back-propagation and KD; and (2) given a fixed capacity, deeper models performed better than shallower ones."}, {"heading": "4.2 TRADE-OFF BETWEEN MODEL PERFORMANCE AND EFFICIENCY", "text": "To evaluate FitNets efficicency, we measure their total inference times required for processing CIFAR-10 test examples on a GPU. Table 5 reports the speed-up obtained by various FitNets w.r.t. the teacher model along with their number of layers, capacity and accuracies. In this experiment, we retrain our FitNets on training plus validation sets as in Goodfellow et al. (2013b), for fair comparison with the teacher.\nFitNet 1, our smallest network, with 2.7% of teacher capacity, is one order of magnitude faster than the teacher and only witnesses a minor performance decrease of 1.3%. FitNet 2, slightly increasing the capacity, now outperforms the teacher by 0.9%, while still being faster by a strong 4.64 factor. By further increasing network capacity and depth in FitNet 3 and 4, we improve the performance gain, up to 1.6% and still remain faster than the teacher. While a trade-off between speed and accuracy appears in FitNets based on their capacity, they tend to be significantly faster than their teacher and match or outperform the teacher, even with low-capacity."}, {"heading": "5 CONCLUSION", "text": "We proposed a novel framework to compress wide and deep networks into thin and deeper ones, by introducing intermediate-level hints from the teacher hidden layers to guide the training process of the student. We are able to use these hints to train very deep student models with less parameters, which can generalize better and/or run faster than their teachers. Our experiments on benchmark datasets emphasize that deep networks with low capacity are able to extract feature representations that are comparable or even better than networks with as much as 10 times more parameters. The hint-based training suggests that more efforts should be devoted to explore new training strategies to leverage the power of deep networks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013a) and the computational resources provided by Compute Canada and Calcul Que\u0301bec. This work has been partially supported by NSERC, CIFAR, and Canada Research Chairs, Project TIN2013-41751, grant 2014-SGR-221 and Spanish MINECO grant TIN2012-38187-C03."}, {"heading": "A SUPPLEMENTARY MATERIAL: NETWORK ARCHITECTURES AND TRAINING PROCEDURES", "text": "In the supplementary material, we describe all network architectures used throughout the paper.\nA.1 CIFAR-10/CIFAR-100\nIn this section, we describe the Teacher and FitNet architectures used in the CIFAR-10/CIFAR-100 experiments.\nA.1.1 TEACHER\nWe used the maxout convolutional network reported in Goodfellow et al. (2013b) as teacher.The teacher architecture has 3 convolutional hidden layers of 96-192-192 units. Each convolutional layer is followed by a maxout non-linearity (with 2 linear pieces) and max-pooling operator with respective windows size of 4x4, 4x4 and 2x2 pixels, and an overlap of 2x2 pixels. The third convolutional layer is followed by a fully-connected maxout layer of 500 units (with 5 linear pieces) and a softmax layer. Our teacher is trained using stochastic gradient descent and momentum. The same architecture is used for both CIFAR-10 and CIFAR-100 Please refer to Goodfellow et al. (2013b) for more details.\nA.1.2 FITNETS\nHere, we describe the FitNet architectures used in the Section 3. Each FitNet is composed of successive zero-padded convolutional layers of kernel size 3\u00d7 3, followed by a maxout non-linearity with two linear pieces. A a non-overlapping 2\u00d7 2 max-pooling takes place after some of the convolution layers, each network has 3 pooling units in total. The last max-pooling takes the maximum over all remaining spatial dimensions leading to a 1\u00d7 1 vector representation. The last convolution layer is followed by a fully-connected and a softmax layer, as the ones on CIFAR-10/100 teachers.\nTable 6 describes the architectures used for the depth experiment. Table 7 describes the networks for the efficiency-performance trade-off experiment. For comparison with state-of-art methods, we used FitNet 4 architecture in Table 7.\nRMSProp (Tieleman & Hinton, 2012) was used for training the FitNets with an initial learning rate the learning of 0.0005. Parameter \u03bb in Eq. (2) was initialized to 4 and decayed linearly during 500 epochs reaching \u03bb = 1.\nOn CIFAR-10, we divide the training set in 40K train examples and 10K valid examples. We train the hints by minimizing Eq. (3) and stop when we monitor no improvement on the validation error during 100 epochs. After that, we trained the whole student network by minimizing Eq. (2) using RMSprop and the same stopping criteria. After determining the optimal hyper-parameter on the validation set, we retrained on the whole 50K (train and validation sets). On CIFAR-100, we train directly on the whole training set using the hyper-parameters that showed good performance on CIFAR-10.\nA.2 MNIST\nIn this section, we describe the Teacher and FitNet architectures used in the MNIST experiments.\nWe trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b). The teacher architecture has three convolutional maxout hidden layers (with 2 linear pieces each) of 48-48-24 units, respectively, followed by a spatial max-pooling of 4x4-4x4-2x2 pixels, with an overlap of 2x2 pixels. The 3rd hidden layer is followed by a fully-connected softmax layer. As is Goodfellow et al. (2013b), we added zero padding to the second convolutional layer.\nWe designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The student architecture has 6 maxout convolutional hidden layers (with 2 linear pieces each) of 16-16-16-16-12-12 units, respectively. Max-pooling is only applied every second layer in regions of 4x4-4x4-2x2 pixels, with an overlap of 2x2 pixels. The 6th convolutional hidden layer is followed by a fully-connected softmax layer.\nThe teacher network was trained as described in Goodfellow et al. (2013b). The FitNet was trained in a stage-wise fashion as described in Section 2. We divided the training set into a training set of 50K samples and a validation set of 10K samples. In the first stage, the 4th layer of the FitNet was trained to mimic the 2nd layer of the teacher network, by minimizing Eq. (3) through stochastic gradient descent. We used a mini-batch size of 128 samples and fixed the learning rate to 0.0005. We initialized \u03bb to 4 and decayed it for the first 150 epochs until it reached 1. The training was stopped after 100 epochs of no validation error progress. We used the same mini-batch size, learning rate and stopping criterion to train the second stage. The relaxation term \u03c4 was set to 3.\nA.3 SVHN\nFor SVHN, we also used a maxout convolutional Teacher composed of 3 convolutional hidden layers of 64-128-128 units. A a fully-connected maxout layer of 400 units and a softmax layer are used as top-layers. Teacher training is carried out the same way than Goodfellow et al. (2013b). As FitNet, we used the FitNet 4 architecture specified in Table 7. FitNet training follows the same protocol than CIFAR-10 with early stopping based on a validation set, except that we do not retrain our FitNet on both train and validation sets.\nA.4 AFLW\nIn this section, we describe the Teacher and FitNet architectures used in the AFLW experiments.\nWe trained a teacher network of 3 ReLU convolutional layers of 128-512-512 units, respectively, followed by a sigmoid layer. Non-overlapping max-pooling of size 2 \u00d7 2 was performed after the first convolutional layer. We used receptive fields of 3-2-5 for each layer, respectively.\nWe designed two FitNets of 7 ReLU convolutional layers. Fitnet 1\u2019s layers have 16-32-32-32-32- 32-32-32 units, respectively, followed by a sigmoid layer. Fitnet 2\u2019s layers have 32-64-64-64-64- 64-64-64 units, respectively, followed by a sigmoid layer. In both cases, we used receptive fields of 3\u00d7 3 and, due to the really small image resolution, we did not perform any max-pooling.\nWe used a mini-batch size of 128 samples and initialized the learning rate to 0.001 and decayed it for the first 100 epochs until reaching 0.01. We also used momentum. We initialized momentum to 0.7 and saturated it to 0.9 at epoch 100. We picked the best validation value after a 500 epochs.\nBoth FitNets were trained in a stage-wise fashion as described in Section 2. We used 90% of the data for training. In the first stage, the 5th layer of the FitNets were trained to mimic the 3rd layer of the teacher network, by minimizing Eq. (3) through stochastic gradient descent. We used a minibatch size of 128 samples and initialized the learning rate to 0.001 and decayed it for the first 100 epochs until reaching 0.01. We also used momentum. We initialized momentum to 0.1 and saturated it to 0.9 at epoch 100. We picked the best validation value after a 500 epochs. We used the same mini-batch size, learning rate and stopping criterion to train the second stage. The relaxation term \u03c4 was set to 3."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In NIPS,", "citeRegEx": "Ba and Caruana,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana", "year": 2014}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["D. Erhan", "P.A. Manzagol", "Y. Bengio", "S. Bengio", "P. Vincent"], "venue": "In AISTATS,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Pylearn2: a machine learning research", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["C. Gulcehre", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Gulcehre and Bengio,? \\Q2013\\E", "shortCiteRegEx": "Gulcehre and Bengio", "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Distilling knowledge in a neural network", "author": ["Hinton", "O.G. Vinyals", "J. Dean"], "venue": "In Deep Learning and Representation Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization", "author": ["M. Koestinger", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "In First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies,", "citeRegEx": "Koestinger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koestinger et al\\.", "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In ICML, pp", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "In NIPS", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A. Ng"], "venue": "In Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "In ICML,", "citeRegEx": "Weston et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2008}, {"title": "Teacher and FitNet architectures used in the MNIST experiments. We trained a teacher network of maxout convolutional layers as reported", "author": ["Goodfellow"], "venue": null, "citeRegEx": "Goodfellow,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow", "year": 2013}, {"title": "2013b), we added zero padding to the second convolutional layer. We designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters. The student architecture has 6 maxout convolutional hidden layers (with 2 linear pieces each) of 16-16-16-16-12-12 units, respectively. Max-pooling is only applied every second layer in regions of 4x4-4x4-2x2", "author": ["Goodfellow"], "venue": null, "citeRegEx": "Goodfellow,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Depth is a fundamental aspect of representation learning, since it encourages the re-use of features, and leads to more abstract and invariant representations at higher layers (Bengio et al., 2013).", "startOffset": 176, "endOffset": 197}, {"referenceID": 14, "context": "importance of depth has been verified (1) theoretically: deep representations are exponentially more expressive than shallow ones for some families of functions (Montufar et al., 2014); and (2) empirically: the two top-performers of ImageNet use deep convolutional networks with 19 and 22 layers, respectively (Simonyan & Zisserman, 2014) and (Szegedy et al.", "startOffset": 161, "endOffset": 184}, {"referenceID": 12, "context": "Nevertheless, training deep architectures has proven to be challenging (Larochelle et al., 2007; Erhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly non-convex and non-linear functions.", "startOffset": 71, "endOffset": 116}, {"referenceID": 5, "context": "Nevertheless, training deep architectures has proven to be challenging (Larochelle et al., 2007; Erhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly non-convex and non-linear functions.", "startOffset": 71, "endOffset": 116}, {"referenceID": 8, "context": "On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006; Bengio et al., 2007) or supervised (Bengio et al.", "startOffset": 63, "endOffset": 105}, {"referenceID": 2, "context": "On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006; Bengio et al., 2007) or supervised (Bengio et al.", "startOffset": 63, "endOffset": 105}, {"referenceID": 2, "context": ", 2007) or supervised (Bengio et al., 2007) train the network parameters in a greedy layerwise fashion in order to initialize the network parameters in a potentially good basin of attraction.", "startOffset": 22, "endOffset": 43}, {"referenceID": 18, "context": "Similarly, semisupervised embedding (Weston et al., 2008) provides guidance to an intermediate layer to help learn very deep networks.", "startOffset": 36, "endOffset": 57}, {"referenceID": 1, "context": "Alternatively, Curriculum Learning strategies (CL) (Bengio, 2009) tackle the optimization problem by modifying the training distribution, such that the learner network gradually receives examples of increasing and appropriate difficulty w.", "startOffset": 51, "endOffset": 65}, {"referenceID": 1, "context": "In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009).", "startOffset": 119, "endOffset": 133}, {"referenceID": 1, "context": "In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training convergence as well as potentially improve the model generalization by properly choosing a sequence of training distributions seen by the learner: from simple examples to more complex ones. A curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance hints on an intermediate layer during the training, one could considerably ease training. However, Bengio (2009) uses hand-defined heuristics to measure the \u201csimplicity\u201d of an example in a sequence and Gulcehre & Bengio (2013)\u2019s guidance hints require some prior knowledge of the end-task.", "startOffset": 120, "endOffset": 592}, {"referenceID": 1, "context": "In this section, we argue that our hint-based training with KD can be seen as a particular form of Curriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training convergence as well as potentially improve the model generalization by properly choosing a sequence of training distributions seen by the learner: from simple examples to more complex ones. A curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance hints on an intermediate layer during the training, one could considerably ease training. However, Bengio (2009) uses hand-defined heuristics to measure the \u201csimplicity\u201d of an example in a sequence and Gulcehre & Bengio (2013)\u2019s guidance hints require some prior knowledge of the end-task.", "startOffset": 120, "endOffset": 706}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening.", "startOffset": 5, "endOffset": 31}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters.", "startOffset": 5, "endOffset": 250}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al.", "startOffset": 5, "endOffset": 467}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training.", "startOffset": 5, "endOffset": 490}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve potentially better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters.", "startOffset": 5, "endOffset": 993}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve potentially better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters. Finally, compared to state-of-the-art methods, our algorithm matches the best performers. CIFAR-100: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10.", "startOffset": 5, "endOffset": 1273}, {"referenceID": 6, "context": "Like Goodfellow et al. (2013b), we normalized the datasets for contrast normalization and applied ZCA whitening. CIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 convolutional layers and roughly 1/3 of the parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the teacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data with random flipping during training. Table 1 summarizes the obtained results. Our student model outperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is crucial to achieve potentially better representations. When compared to network compression methods, our algorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%, which is significantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requiring roughly 28 times fewer parameters. Finally, compared to state-of-the-art methods, our algorithm matches the best performers. CIFAR-100: To validate our approach, we trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10. As in Chen-Yu et al. (2014), we augmented the data with random flipping during training.", "startOffset": 5, "endOffset": 1355}, {"referenceID": 15, "context": "The SVHN dataset (Netzer et al., 2011) is composed by 32 \u00d7 32 color images of house numbers collected by GoogleStreet View.", "startOffset": 17, "endOffset": 38}, {"referenceID": 13, "context": "As a sanity check for the training procedure, we evaluated the proposed method on the MNIST dataset (LeCun et al., 1998).", "startOffset": 100, "endOffset": 120}, {"referenceID": 6, "context": "We trained a teacher network of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters.", "startOffset": 75, "endOffset": 101}, {"referenceID": 10, "context": "AFLW (Koestinger et al., 2011) is a real-world face database, containing 25K annotated images.", "startOffset": 5, "endOffset": 30}, {"referenceID": 1, "context": "samples for which the teacher network is confident and, can lead to a smoother version of the training cost (Bengio, 2009).", "startOffset": 108, "endOffset": 122}, {"referenceID": 4, "context": "On the one hand, the proliferation of local minima and especially saddle points in highly non-linear functions such as very deep networks highlights the difficulty of finding a good starting point in the parameter space at random (Dauphin et al., 2014).", "startOffset": 230, "endOffset": 252}, {"referenceID": 1, "context": "samples for which the teacher network is confident and, can lead to a smoother version of the training cost (Bengio, 2009). Despite some optimization benefits, it is worth noticing that KD training still suffers from the increasing depth and reaches its limits for 7-layer networks. HT tends to ease these optimization issues and is able to train 13-layer networks of 30M multiplications. The only difference between HT and KD is the starting point in the parameter space: either random or obtained by means of the teacher\u2019s hint. On the one hand, the proliferation of local minima and especially saddle points in highly non-linear functions such as very deep networks highlights the difficulty of finding a good starting point in the parameter space at random (Dauphin et al., 2014). On the other hand, results in Figure 2(a) indicate that HT can guide the student to a better initial position in the parameter space, from which we can minimize the cost through stochastic gradient descent. Therefore, HT provides benefits from an optimization point of view. Networks trained with HT also tend to yield better test performances than the other training methods when we fix the capacity and number of layers. For instance, in Figure 2(b), the 7-layers network, trained with hints, obtains a +0.7% performance gain on the test set compared to the model that does not use any hints (the accuracy increases from 89.45% to 90.1%). As pointed by Erhan et al. (2009), pre-training strategies can act as regularizers.", "startOffset": 109, "endOffset": 1460}, {"referenceID": 6, "context": "In this experiment, we retrain our FitNets on training plus validation sets as in Goodfellow et al. (2013b), for fair comparison with the teacher.", "startOffset": 82, "endOffset": 108}], "year": 2017, "abstractText": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher\u2019s intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "creator": "LaTeX with hyperref package"}}}