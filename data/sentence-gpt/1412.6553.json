{"id": "1412.6553", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2014", "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition", "abstract": "We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process.\n\n\n\n\n\nA recurrent network has its size in the tensor space, and the large size of the layer represents its size on the neural network, and it is also used to classify and categorize convolutional neural networks using two methods.\nThe basic algorithms we have built here are described as:\nA recurrent network that is a multi-valued system:\nThe recurrent network is a system of discrete, discrete, sub-problems. The recurrent network consists of an infinite number of random numbers representing various classes of convolutional neural networks. The recurrent network consists of a list of the topology of the training data;\nAn infinite number of discrete, sub-problems.\nA recurrent network consists of a list of the topology of the training data;\nAn infinite number of discrete, sub-problems.\nA recurrent network consists of a list of the topology of the training data;\nAn infinite number of discrete, sub-problems.\nEach classification can be divided into three categories:\nA recurrent network (one classification with a maximum of a maximum of an all-time maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum of a maximum", "histories": [["v1", "Fri, 19 Dec 2014 23:02:43 GMT  (720kb,D)", "http://arxiv.org/abs/1412.6553v1", null], ["v2", "Wed, 24 Dec 2014 21:57:37 GMT  (722kb,D)", "http://arxiv.org/abs/1412.6553v2", null], ["v3", "Fri, 24 Apr 2015 11:40:54 GMT  (763kb,D)", "http://arxiv.org/abs/1412.6553v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["vadim lebedev", "yaroslav ganin", "maksim rakhuba", "ivan oseledets", "victor lempitsky"], "accepted": true, "id": "1412.6553"}, "pdf": {"name": "1412.6553.pdf", "metadata": {"source": "CRF", "title": "SPEEDING-UP CONVOLUTIONAL NEURAL NETWORKS USING FINE-TUNED CP-DECOMPOSITION", "authors": ["Vadim Lebedev", "Yaroslav Ganin", "Maksim Rakhuba", "Ivan Oseledets", "Victor Lempitsky"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Over the course of two years, Convolutional neural networks (CNNs) (LeCun et al., 1989) have revolutionized computer vision and became ubiquituous through a range of computer vision applications. In many ways, this breakthrough has become possible through the acceptance of new computational tools, most notably GPUs (Krizhevsky et al., 2012), but also CPU clusters (Dean et al., 2012) and FPGAs (Farabet et al., 2011). On the other hand, there is a rising interest to deploying CNNs on low-end architectures, such as standalone desktop/laptop CPUs, mobile processors, and CPU in robotics. On such processors, the computational cost of applying, yet alone training a CNN might pose a problem, especially when real-time operation is required.\nThe key layer of CNNs that distinguishes them from other neural networks and enables their recent success is the convolution operation. Convolutions dominate the computation cost associated with training or testing a CNN (especially for newer architectures such as (Simonyan & Zisserman, 2014), which tend to add more and more convolutional layers often at the expense of fully-connected layers). Consequently, there is a strong interest to the task of improving the efficiency of this operation (Chintala, 2014; Mathieu et al., 2013; Chellapilla et al., 2006). An efficient implementation of the convolution operation is one of the most important elements of all major CNN packages.\nA group of recent works have achieved significant speed-ups of CNN convolutions by applying tensor decompositions. In more detail, recall that a typical convolution in a CNN takes as an input a 3D tensor (array) with the dimensions corresponding to the two spatial dimensions, and to different image maps. The output of a convolution is another similarly-structured 3D tensor. The convolution kernel itself constitutes a 4D tensor with dimensions corresponding to the two spatial dimensions, the input image maps, and the output image maps. Exploiting this tensor structure, previous works (Denton et al., 2014; Jaderberg et al., 2014a) have suggested different tensor decomposition schemes\nar X\niv :1\n41 2.\n65 53\nv1 [\ncs .C\nV ]\n1 9\nD ec\n2 01\nto speed-up convolutions within CNNs. These schemes are applied to the kernel tensor and generalize previous 2D filter approximations in computer vision like (Rigamonti et al., 2013).\nIn this work, we further investigate tensor decomposition in the context of speeding up convolutions within CNNs. We use a standard (in tensor algebra) low-rank CP-decomposition (also known as PARAFAC or CANDECOMP) and an efficient optimization approach (non-linear least squares) to decompose the full kernel tensor.\nWe demonstrate that the use of the CP-decomposition on a full kernel tensor has the following important advantages:\n\u2022 Ease of the decomposition implementation. CP-decomposition is one of the standard tools in tensor linear algebra with well understood properties and mature implementations. Consequently, one can use existing efficient algorithms such as NLS to compute it efficiently.\n\u2022 Ease of the CNN implementation. CP-decomposition approximates the convolution with a 4D kernel tensor by the sequence of four convolutions with small 2D kernel tensors. All these lower dimensional convolutions represent standard CNN layers, and are therefore easy to insert into a CNN using existing CNN packages (no custom layer implementation is needed).\n\u2022 Ease of fine-tuning. Related to the previous item, once a convolutional layer is approximated and replaced with a sequence of four convolutional layers with smaller kernels, it is straight-forward to fine-tune the entire network on training data using back-propagation.\n\u2022 Efficiency. Perhaps most importantly, we found that the simple combination of the full kernel CP-decomposition and global fine-tuning results in better speed-accuracy trade-off for the approximated networks than the previous methods.\nThe CNNs obtained in our method are somewhat surprisingly accurate, given that the number of parameters is reduced several times compared to the initial networks. Practically, this reduction in the number of parameters means more compact networks with reduced memory footprint, which can be important for architectures with limited RAM or storage memory. Such memory savings can be especially valuable for feed-forward networks that include convolutions with spatially-varying kernels (\u201clocally-connected\u201d layers).\nOn the theoretical side, these results confirm the intuition that modern CNNs are over-parameterized, i.e. that the sheer number of parameters in the modern CNNs are not needed to store the information about the classification task but, rather, serve to facilitate convergence to good local minima of the loss function.\nBelow, we first discuss previous works of (Rigamonti et al., 2013; Jaderberg et al., 2014a; Denton et al., 2014) and outline the differences between them and our approach in Section 2. We then review the CP-decomposition and give the details of our approach in Section 3. The experiments on two CNNs, namely the character classification CNN from (Jaderberg et al., 2014a) and (Jaderberg et al., 2014b) and AlexNet from the Caffe package (Jia et al., 2014), follow in Section 4. We conclude with the summary and the discussion in Section 5."}, {"heading": "2 RELATED WORK", "text": "Decompositions for convolutions. Using low-rank decomposition to accelerate convolution was suggested by Rigamonti et al. (2013) in the context of codebook learning. In (Rigamonti et al., 2013), a bank of 2D or 3D filters X is decomposed into linear combinations of a shared bank of separable (decomposable) filters Y . The decompositions of filters within Y are independent (components are not shared).\nJaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efficient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors. In the experiments, Jaderberg et al. (2014a) have demonstrated the advantage of this scheme over (Rigamonti et al., 2013). In a sequel, when refering to (Jaderberg et al., 2014a) we imply this two-component decomposition. Once the decomposition is computed, Jaderberg et al. (2014a) perform \u201clocal\u201d fine-tuning that minimizes\nthe deviation between the full and the approximated convolutions outputs on the training data. Below, we provide a theoretical complexity comparison and empirical comparison of our scheme with (Jaderberg et al., 2014a).\nWhile (Jaderberg et al., 2014a) reported that such fine-tuning was inefficient for their scheme, we found that in our case it works very well, even when CP-decomposition has large approximation error.\nIn the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on CP-decomposition of parts of the kernel tensor obtianed by biclustering (alongside with a different decompositions for the first decomposition layer and the fully-connected layer). Biclustering splits the two non-spatial dimensions into subgroups, and reduces the effective ranks in the CPdecomposition. CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1. Our approach essentially simplifies that of Denton et al. (2014) in that we do not perform biclustering and apply CP-decomposition directly to the full convolution kernel tensor. On the other hand, we replace greedy computation of CP-decomposition with non-linear least squares. Finally, as discussed above, we fine-tune the complete network by backpropagation, whereas Denton et al. (2014) only fine-tunes the layers above the approximated one.\n1Note that the alternating least squares process mentioned in (Denton et al., 2014) refers to computing the best next rank-1 tensor, but the outer process of adding rank-1 tensors is still greedy."}, {"heading": "3 METHOD", "text": "Overall our method is a conceptually simple two-step approach: (1) take a convolutional layer and decompose its kernel matrix using CP-decomposition, (2) fine-tune the entire network using backpropagation. If necessary, proceed to another layer. Below, we review the CP-decomposition, which is at the core of our method, and provide the details for the two steps of our approach."}, {"heading": "3.1 CP-DECOMPOSITION REVIEW", "text": "Tensor decompositions are a natural way to generalize low-rank approach to multidimensional case. Recall that a low-rank decomposition of a matrix A of size n\u00d7m with rank R is given by:\nA(i, j) = R\u2211 r=1 A1(i, r)A2(j, r), i = 1, n, j = 1,m, (1)\nand leads to the idea of separation of variables. The most straightforward way to separate variables in case of many dimensions is to use the canonical polyadic decomposition (CP-decomposition, also called as CANDECOMP/PARAFAC model) (Kolda & Bader, 2009). For a d-dimensional array A of size n1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nd a CP-decomposition has the following form\nA(i1, . . . , id) = R\u2211 r=1 A1(i1, r) . . . Ad(id, r), (2)\nwhere the minimal possible R is called canonical rank. The profit of this decomposition is that we need to store only (n1 + \u00b7 \u00b7 \u00b7+ nd)R elements instead of the whole tensor with n1...nd elements. In two dimensions, the low-rank approximation can be computed in a stable way by using singular value decomposition (SVD) or, if the matrix is large, by rank-revealing algorithms. Unfortunately, this is not the case for the CP-decomposition when d > 2, as there is no finite algorithm for determining canonical rank of a tensor (Kolda & Bader, 2009). Therefore, most algorithms approximate a tensor with different values of R until the approximation error becomes small enough. This leads to the point that for finding good low-rank CP-decomposition certain tricks have to be used. A detailed survey of methods for computing low-rank CP-decompositions can be found in (Tomasi & Bro, 2006). As a software package to calculate the CP-decomposition we used Tensorlab (Sorber et al., 2014). For our purposes, we chose non-linear least squares (NLS) method, which minimizes the L2-norm of the approximation residual (for a user-defined fixed R) using Gauss-Newton optimization.\nSuch NLS optimization is capable of obtaining much better approximations than the strategy of greedily finding best rank-1 approximation of the residual vectors used in Denton et al. (2014). We give a simple example highlighting this advantage of the NLS in the Appendix."}, {"heading": "3.2 KERNEL TENSOR APPROXIMATION", "text": "CNNs (LeCun et al., 1989) are feed-forward multi-layer architectures that map the input images to certain output vectors using a sequence of operations. The units within CNN are organized as a sequence of 3D tensors (\u201cmap stacks\u201d) with two spatial dimensions and the third dimension corresponding to different \u201cmaps\u201d or \u201dchannels\u201d2.\nThe most \u201cimportant\u201d and time-consuming operation within modern CNNs is the generalized convolution that maps an input tensor U(\u00b7, \u00b7, \u00b7) of size X\u00d7Y\u00d7S into an output tensor V (\u00b7, \u00b7, \u00b7) of size (X\u2212d+1)\u00d7(Y\u2212d+1)\u00d7T using the following linear mapping:\nV (x, y, t) = x+\u03b4\u2211 i=x\u2212\u03b4 y+\u03b4\u2211 j=y\u2212\u03b4 S\u2211 s=1 K(i, j, s, t)U(i, j, s) (3)\n2These tensors are 4D if/when a CNN is applied to a batch of images, with the fourth dimension corresponding to different images in a batch. This extra dimension does not affect the derivation below and is therefore disregarded.\nHere, K(i, j, s, t) is a 4D kernel tensor of size d\u00d7d\u00d7S\u00d7T with the first two dimensions corresponding to the spatial dimensions, the third dimension corresponding to different input channels, the fourth dimension corresponding to different output channels. The spatial width and height of the kernel are denoted as d, while \u03b4 denotes \u201chalf-width\u201d (d \u2212 1)/2 (for simplicity we assume square shaped kernels and even d).\nThe rank-R CP-decomposition (2) of the 4D kernel tensor has the form:\nK(i, j, s, t) = R\u2211 r=1 Kx(i, r)Ky(j, r)Ks(s, r)Kt(t, r) , (4)\nwhere Kx(\u00b7, \u00b7), Ky(\u00b7, \u00b7), Ks(\u00b7, \u00b7), Kt(\u00b7, \u00b7) are the four components of the composition representing 2D tensors (matrices) of sizes d\u00d7R, d\u00d7R, S\u00d7R, and T\u00d7R respectively. Substituting (4) into (3) and performing permutation and grouping of summands gives the following expression for the approximate evaluation of the convolution (3):\nV (x, y, t) = R\u2211 r=1 Kt(t, r)  x+\u03b4\u2211 i=x\u2212\u03b4 Kx(i, r)  y+\u03b4\u2211 j=y\u2212\u03b4 Ky(j, r) ( S\u2211 s=1 Ks(s, r)U(i, j, s) ) (5) Based on (5), the output tensor V (\u00b7, \u00b7, \u00b7) can be computed from the input tensor U(\u00b7, \u00b7, \u00b7) via a sequence of four convolutions with smaller kernels (Figure 1c):\nUs(i, j, r) = S\u2211 s=1 Ks(s, r)U(i, j, s) (6)\nUsy(i, y, r) = y+\u03b4\u2211 j=y\u2212\u03b4 Ky(j, r)Us(i, j, r) (7)\nUsyx(x, y, r) = x+\u03b4\u2211 i=x\u2212\u03b4 Kx(i, r)Usy(i, y, r) (8)\nV (x, y, t) = R\u2211 r=1 Kt(t, r)Usyx(x, y, r) , (9)\nwhere Us(\u00b7, \u00b7, \u00b7), Usy(\u00b7, \u00b7, \u00b7), and Usyx(\u00b7, \u00b7, \u00b7) are intermediate tensors (map stacks)."}, {"heading": "3.3 IMPLEMENTATION AND FINE-TUNING", "text": "Computing Us(\u00b7, \u00b7, \u00b7) from U(\u00b7, \u00b7, \u00b7) in (6) as well as V (\u00b7, \u00b7, \u00b7) from Usyx(\u00b7, \u00b7, \u00b7) in (9) represent socalled 1\u00d71 convolutions (also used within \u201cnetwork-in-network\u201d approach (Lin et al., 2013)) that essentially perform pixel-wise linear re-combination of input maps. Computing Usy(\u00b7, \u00b7, \u00b7) from Us(\u00b7, \u00b7, \u00b7) and Usyx(\u00b7, \u00b7, \u00b7) from Usy(\u00b7, \u00b7, \u00b7) in (7) and (8) are \u201cstandard\u201d convolutions with small kernels that are \u201cflat\u201d in one of the two spatial dimensions.\nWe use the popular Caffe package (Jia et al., 2014) to implement the resulting architecture, utilizing standard convolution layers for (7) and (8), and an optimized 1\u00d71 convolution layers for (6) and (9). The resulting architecture is fine-tuned through standard backpropagation (with momentum) on training data. All network layers including layers above the approximated layer, layers below the approximated layer, and the four inserted convolutional layers participate in the fine-tuning. We found, however, that the gradients within the inserted layers are prone to gradient explosion, so one should either be careful to keep the learning rate low, or fix some or all of the inserted layers, while still fine-tuning layers above and below."}, {"heading": "3.4 COMPLEXITY ANALYSIS", "text": "Initial convolution operation is defined by STd2 parameters (number of elements in the kernel tensor) and requires the same number of \u201cmultiplication+addition\u201d operations per pixel.\nFor (Jaderberg et al., 2014a) this number changes to Rd(S + T ), where R is the rank of the decomposition (see Figure 1b and Jaderberg et al. (2014a)). While the two numbers are not directly comparable, assuming that the required rank is comparable or several times smaller than S and T (e.g. takingR \u2248 STS+T ), the scheme (Jaderberg et al., 2014a) gives a reduction in the order of d times compared to the initial convolution.\nFor (Denton et al., 2014) in the absence of bi-clustering as well as in the case of our approach, the complexity is R(S + 2d + T ) (again, both for the number of parameters and for the number of \u201cmultiplications+additions\u201d per output pixel). Almost always, d T , which for the same rank gives a further factor of d improvement in complexity over (Jaderberg et al., 2014a) (and an order of d2 improvement over the initial convolution when R \u2248 STS+T ).\nThe bi-clustering in (Denton et al., 2014) makes a \u201ctheoretical\u201d comparison with the complexity of our approach problematic, as on the one hand bi-clustering increases the number of tensors to be approximated, but on the other hand, reduces the required ranks considerably (so that assuming the same R would not be reasonable). We therefore restrict ourselves to the empirical comparison."}, {"heading": "4 EXPERIMENTS", "text": "In this section we test our approach on two network architectures, small character-classification CNN and a bigger net trained for ILSVRC. Most of our experiments are devoted to the approximation of single layers, when other layers remain intact apart from the fine-tuning.\nWe make several measurements to evaluate our models. After the approximation of the kernel tensor with the CP-decomposition, we calculate the accuracy of this decomposition, i.e. \u2016K \u2032 \u2212K\u2016/\u2016K\u2016, where K is the original tensor and K \u2032 is the obtained approximation. The difference between the original tensor and our approximation may disturb data propagation in CNN, resulting in the drop of classification accuracy. We measure this drop before and after the fine-tuning of CNN.\nFurthermore, we record the CPU timings for our models and report the speed-up compared to the CPU timings of the original model (all timings are based on Caffe code run in the CPU mode on image batches of size 64). Finally, we report the reduction in the number of parameters resulting from the low-rank approximation. All results are reported for a number of ranks R."}, {"heading": "4.1 CHARACTER-CLASSIFICATION CNN", "text": "We use use CNN described in (Jaderberg et al., 2014b) for our experiments. The network has four convolutional layers with maxout nonlinearities between them and a softmax output. It was trained to classify 24 \u00d7 24 image patches into one of 36 classes (10 digits plus 26 characters). Our Caffe port of the publicly available pre-trained model (refered below as CharNet) achieves 91.2% accuracy on test set (very similar to the original).\nAs in (Jaderberg et al., 2014a), we consider only the second and the third convolutional layers, which constitute more then 90% of processing time. Layer 2 has 48 input and 128 output channels and filters of size 9\u00d7 9, layer 3 has 64 input and 512 output channels, filter size is 8\u00d7 8. The results of separate approximation of layers 2 and 3 are shown in figures 2a and 2b. Tensor approximation error diminishes with the growth of approximation rank, and when the approximation rank becomes big enough, it is possible to approximate weight tensor accurately. However, our experiments showed that accurate approximation is not required for the network to function properly. For example, while approximating layer 3, network classification accuracy is unaffected even if tensor approximation error is as big as 78%.\nNLS vs. Greedy.\nOur experiments show that NLS gives better tensor approximation than greedy algorithm for all ranks greater than one, thus we use NLS to obtain decompositions. For R = 64 and layer 2, greedy decomposition is 2.1% worse than NLS in terms of network classification accuracy (87, 25% vs 89, 31%). However, fine-tuning brings this gap down to 0.5%.\nFor smaller rank, the gap widens, e.g. for R = 16 the gap before fine-tuning is more than 5% (67.09 vs 72.28). For larger rank (R = 256) all accuracies converge to the accuracy of the initial network and the gap before fine-tuning is small in absolute numbers (90, 10% vs. 90, 93%).\nCombining approximation. We have applied our methods to two layers of the network using the following procedure. Firstly, layer 2 was approximated with rank 64. After that, the drop in accuracy was made small by fine-tuning of all layers but the new ones. Finally, layer 3 was approximated with rank 64, and for this layer such approximation does not result in significant drop of network prediction accuracy, so there is no need to fine-tune the network one more time. The network derived by this procedure is 8.5 times faster then original model, while classification accuracy drops to 90.2%.\nComparing with (Jaderberg et al., 2014a), we achieve almost two times bigger speedup for the same 1 percent loss of accuracy, ((Jaderberg et al., 2014a) incurs 1% accuracy loss for the speedup of 4.2x and 5% accuracy loss for the speedup of 6x)."}, {"heading": "4.2 ALEXNET", "text": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance.\nOverall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassifications: rank 140 approximation leads to 4.5\u00d7 speed-up at the cost of \u2248 1% accuracy loss surpassing the results of Denton et al. (2014). Along with conventional full-network fine-tuning we tried to refine the obtained tensor approximation by applying the data reconstruction approach from Jaderberg et al. (2014b). Unfortunately, we failed to find a good SGD learning rate: larger values led to the exploding gradients, while the smaller ones did not allow to sensibly reduce the reconstruction loss. We suspect that this effect is due to the instability of the low-rank CP-decomposition De Silva & Lim (2008). One way to circumvent the issue would be to alternate the components learning (i.e. not optimizing all of them at once), which is the scope of our future work."}, {"heading": "5 DISCUSSION", "text": "We have demonstrated that a rather straightforward application of a modern tensor decomposition method (NLS-based low-rank CP-decomposition) combined with the discriminative fine-tuning of the entire network can achieve considerable speedups with minimal loss in accuracy (or, for higher ranks without such loss).\nIn the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, more comparisons especially with a more related work of Denton et al. (2014) are needed. In particular, it is to be determined whether biclustering is useful, when non-linear least squares are used for CP-decomposition.\nAnother avenue of research are layers with spatially-varying kernels, such as used e.g. in Taigman et al. (2014). Firstly, these layers would greatly benefit from the reduction in the number of parameters. Secondly, the spatial variation of the kernel might be embedded into extra tensor dimensions, which may open up further speed-up possibilities.\nFinally, similarly to Denton et al. (2014), we note that low-rank decompositions seems to have a regularizing effects allowing to slightly improve the overall accuracy for higher rank."}], "references": [{"title": "High performance convolutional neural networks for document processing", "author": ["Chellapilla", "Kumar", "Puri", "Sidd", "Simard", "Patrice"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Convnet-benchmarks. https://github.com/soumith/ convnet-benchmarks", "author": ["Chintala", "Soumith"], "venue": null, "citeRegEx": "Chintala and Soumith.,? \\Q2014\\E", "shortCiteRegEx": "Chintala and Soumith.", "year": 2014}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["De Silva", "Vin", "Lim", "Lek-Heng"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Silva et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2008}, {"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Mao", "Mark", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Large-scale FPGA-based convolutional networks", "author": ["Farabet", "Cl\u00e9ment", "LeCun", "Yann", "Kavukcuoglu", "Koray", "Culurciello", "Eugenio", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Selcuk"], "venue": "Machine Learning on Very Large Data Sets,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC),", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Deep features for text spotting", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Rev.,", "citeRegEx": "Kolda and Bader,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["LeCun", "Yann", "Boser", "Bernhard", "Denker", "John S", "Henderson", "Donnie", "Howard", "Richard E", "Hubbard", "Wayne", "Jackel", "Lawrence D"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Fast training of convolutional networks through FFTs", "author": ["Mathieu", "Michael", "Henaff", "Mikael", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Learning separable filters", "author": ["Rigamonti", "Roberto", "Sironi", "Amos", "Lepetit", "Vincent", "Fua", "Pascal"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Rigamonti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rigamonti et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Taigman", "Yaniv", "Yang", "Ming", "Ranzato", "Marc\u2019Aurelio", "Wolf", "Lior"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Under review as a conference", "author": ["Tomasi", "Giorgio", "Bro", "Rasmus"], "venue": "Comp. Stat. Data An.,", "citeRegEx": "Tomasi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tomasi et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Over the course of two years, Convolutional neural networks (CNNs) (LeCun et al., 1989) have revolutionized computer vision and became ubiquituous through a range of computer vision applications.", "startOffset": 67, "endOffset": 87}, {"referenceID": 10, "context": "In many ways, this breakthrough has become possible through the acceptance of new computational tools, most notably GPUs (Krizhevsky et al., 2012), but also CPU clusters (Dean et al.", "startOffset": 121, "endOffset": 146}, {"referenceID": 3, "context": ", 2012), but also CPU clusters (Dean et al., 2012) and FPGAs (Farabet et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 5, "context": ", 2012) and FPGAs (Farabet et al., 2011).", "startOffset": 18, "endOffset": 40}, {"referenceID": 12, "context": "Consequently, there is a strong interest to the task of improving the efficiency of this operation (Chintala, 2014; Mathieu et al., 2013; Chellapilla et al., 2006).", "startOffset": 99, "endOffset": 163}, {"referenceID": 0, "context": "Consequently, there is a strong interest to the task of improving the efficiency of this operation (Chintala, 2014; Mathieu et al., 2013; Chellapilla et al., 2006).", "startOffset": 99, "endOffset": 163}, {"referenceID": 4, "context": "Exploiting this tensor structure, previous works (Denton et al., 2014; Jaderberg et al., 2014a) have suggested different tensor decomposition schemes", "startOffset": 49, "endOffset": 95}, {"referenceID": 13, "context": "These schemes are applied to the kernel tensor and generalize previous 2D filter approximations in computer vision like (Rigamonti et al., 2013).", "startOffset": 120, "endOffset": 144}, {"referenceID": 13, "context": "Below, we first discuss previous works of (Rigamonti et al., 2013; Jaderberg et al., 2014a; Denton et al., 2014) and outline the differences between them and our approach in Section 2.", "startOffset": 42, "endOffset": 112}, {"referenceID": 4, "context": "Below, we first discuss previous works of (Rigamonti et al., 2013; Jaderberg et al., 2014a; Denton et al., 2014) and outline the differences between them and our approach in Section 2.", "startOffset": 42, "endOffset": 112}, {"referenceID": 8, "context": ", 2014b) and AlexNet from the Caffe package (Jia et al., 2014), follow in Section 4.", "startOffset": 44, "endOffset": 62}, {"referenceID": 13, "context": "In (Rigamonti et al., 2013), a bank of 2D or 3D filters X is decomposed into linear combinations of a shared bank of separable (decomposable) filters Y .", "startOffset": 3, "endOffset": 27}, {"referenceID": 13, "context": "(2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efficient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors.", "startOffset": 36, "endOffset": 60}, {"referenceID": 13, "context": "(2014a) have demonstrated the advantage of this scheme over (Rigamonti et al., 2013).", "startOffset": 60, "endOffset": 84}, {"referenceID": 11, "context": "Using low-rank decomposition to accelerate convolution was suggested by Rigamonti et al. (2013) in the context of codebook learning.", "startOffset": 72, "endOffset": 96}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efficient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors. In the experiments, Jaderberg et al. (2014a) have demonstrated the advantage of this scheme over (Rigamonti et al.", "startOffset": 0, "endOffset": 312}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) evaluated the decomposition (Rigamonti et al., 2013) in the context of CNNs and furthermore suggested a more efficient decomposition (Figure 1b) that effectively approximates the 4D kernel tensor as a composition (product) of two 3D tensors. In the experiments, Jaderberg et al. (2014a) have demonstrated the advantage of this scheme over (Rigamonti et al., 2013). In a sequel, when refering to (Jaderberg et al., 2014a) we imply this two-component decomposition. Once the decomposition is computed, Jaderberg et al. (2014a) perform \u201clocal\u201d fine-tuning that minimizes", "startOffset": 0, "endOffset": 550}, {"referenceID": 6, "context": "Jaderberg et al. (2014a) (b) approximate the initial convolution as a composition of two linear mappings with the intermediate map stack having R maps (where R is the rank of the decomposition).", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1.", "startOffset": 48, "endOffset": 69}, {"referenceID": 4, "context": "In the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on CP-decomposition of parts of the kernel tensor obtianed by biclustering (alongside with a different decompositions for the first decomposition layer and the fully-connected layer).", "startOffset": 42, "endOffset": 63}, {"referenceID": 4, "context": "In the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on CP-decomposition of parts of the kernel tensor obtianed by biclustering (alongside with a different decompositions for the first decomposition layer and the fully-connected layer). Biclustering splits the two non-spatial dimensions into subgroups, and reduces the effective ranks in the CPdecomposition. CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1. Our approach essentially simplifies that of Denton et al. (2014) in that we do not perform biclustering and apply CP-decomposition directly to the full convolution kernel tensor.", "startOffset": 42, "endOffset": 581}, {"referenceID": 4, "context": "In the work that is most related to ours, Denton et al. (2014) have suggested a scheme based on CP-decomposition of parts of the kernel tensor obtianed by biclustering (alongside with a different decompositions for the first decomposition layer and the fully-connected layer). Biclustering splits the two non-spatial dimensions into subgroups, and reduces the effective ranks in the CPdecomposition. CP-decompositions of the kernel tensor parts in (Denton et al., 2014) have been computed with the greedy approach1. Our approach essentially simplifies that of Denton et al. (2014) in that we do not perform biclustering and apply CP-decomposition directly to the full convolution kernel tensor. On the other hand, we replace greedy computation of CP-decomposition with non-linear least squares. Finally, as discussed above, we fine-tune the complete network by backpropagation, whereas Denton et al. (2014) only fine-tunes the layers above the approximated one.", "startOffset": 42, "endOffset": 907}, {"referenceID": 4, "context": "Note that the alternating least squares process mentioned in (Denton et al., 2014) refers to computing the best next rank-1 tensor, but the outer process of adding rank-1 tensors is still greedy.", "startOffset": 61, "endOffset": 82}, {"referenceID": 4, "context": "Such NLS optimization is capable of obtaining much better approximations than the strategy of greedily finding best rank-1 approximation of the residual vectors used in Denton et al. (2014). We give a simple example highlighting this advantage of the NLS in the Appendix.", "startOffset": 169, "endOffset": 190}, {"referenceID": 11, "context": "2 KERNEL TENSOR APPROXIMATION CNNs (LeCun et al., 1989) are feed-forward multi-layer architectures that map the input images to certain output vectors using a sequence of operations.", "startOffset": 35, "endOffset": 55}, {"referenceID": 8, "context": "We use the popular Caffe package (Jia et al., 2014) to implement the resulting architecture, utilizing standard convolution layers for (7) and (8), and an optimized 1\u00d71 convolution layers for (6) and (9).", "startOffset": 33, "endOffset": 51}, {"referenceID": 4, "context": "For (Denton et al., 2014) in the absence of bi-clustering as well as in the case of our approach, the complexity is R(S + 2d + T ) (again, both for the number of parameters and for the number of \u201cmultiplications+additions\u201d per output pixel).", "startOffset": 4, "endOffset": 25}, {"referenceID": 4, "context": "The bi-clustering in (Denton et al., 2014) makes a \u201ctheoretical\u201d comparison with the complexity of our approach problematic, as on the one hand bi-clustering increases the number of tensors to be approximated, but on the other hand, reduces the required ranks considerably (so that assuming the same R would not be reasonable).", "startOffset": 21, "endOffset": 42}, {"referenceID": 5, "context": "For (Jaderberg et al., 2014a) this number changes to Rd(S + T ), where R is the rank of the decomposition (see Figure 1b and Jaderberg et al. (2014a)).", "startOffset": 5, "endOffset": 150}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe.", "startOffset": 10, "endOffset": 115}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.", "startOffset": 10, "endOffset": 514}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)).", "startOffset": 10, "endOffset": 660}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassifications: rank 140 approximation leads to 4.5\u00d7 speed-up at the cost of \u2248 1% accuracy loss surpassing the results of Denton et al. (2014). Along with conventional full-network fine-tuning we tried to refine the obtained tensor approximation by applying the data reconstruction approach from Jaderberg et al.", "startOffset": 10, "endOffset": 892}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassifications: rank 140 approximation leads to 4.5\u00d7 speed-up at the cost of \u2248 1% accuracy loss surpassing the results of Denton et al. (2014). Along with conventional full-network fine-tuning we tried to refine the obtained tensor approximation by applying the data reconstruction approach from Jaderberg et al. (2014b). Unfortunately, we failed to find a good SGD learning rate: larger values led to the exploding gradients, while the smaller ones did not allow to sensibly reduce the reconstruction loss.", "startOffset": 10, "endOffset": 1070}, {"referenceID": 4, "context": "Following Denton et al. (2014) we also consider the second convolutional layer of AlexNet Krizhevsky et al. (2012). As a baseline we use a pre-trained model shipped with Caffe. We summarize various network properties for several different ranks of approximation in Figure 2c. It can be noticed that conv2 of the considered network demands far larger rank (comparing to the CharNet experiment) for achieving proper performance. Overall, in order to reach the 0.5% accuracy drop reported in Jaderberg et al. (2014b) it is sufficient to take 200 components, which also gives a superior layer speed-up (3.6\u00d7 vs. 2\u00d7 achieved by Scheme 2 of Jaderberg et al. (2014b)). The running time of the conv2 can be further reduced if we allow for slightly more misclassifications: rank 140 approximation leads to 4.5\u00d7 speed-up at the cost of \u2248 1% accuracy loss surpassing the results of Denton et al. (2014). Along with conventional full-network fine-tuning we tried to refine the obtained tensor approximation by applying the data reconstruction approach from Jaderberg et al. (2014b). Unfortunately, we failed to find a good SGD learning rate: larger values led to the exploding gradients, while the smaller ones did not allow to sensibly reduce the reconstruction loss. We suspect that this effect is due to the instability of the low-rank CP-decomposition De Silva & Lim (2008). One way to circumvent the issue would be to alternate the components learning (i.", "startOffset": 10, "endOffset": 1366}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a).", "startOffset": 82, "endOffset": 128}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, more comparisons especially with a more related work of Denton et al. (2014) are needed.", "startOffset": 83, "endOffset": 216}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, more comparisons especially with a more related work of Denton et al. (2014) are needed. In particular, it is to be determined whether biclustering is useful, when non-linear least squares are used for CP-decomposition. Another avenue of research are layers with spatially-varying kernels, such as used e.g. in Taigman et al. (2014). Firstly, these layers would greatly benefit from the reduction in the number of parameters.", "startOffset": 83, "endOffset": 472}, {"referenceID": 4, "context": "In the preliminary comparisons, this approach outperforms the previous methods of (Denton et al., 2014; Jaderberg et al., 2014a). However, more comparisons especially with a more related work of Denton et al. (2014) are needed. In particular, it is to be determined whether biclustering is useful, when non-linear least squares are used for CP-decomposition. Another avenue of research are layers with spatially-varying kernels, such as used e.g. in Taigman et al. (2014). Firstly, these layers would greatly benefit from the reduction in the number of parameters. Secondly, the spatial variation of the kernel might be embedded into extra tensor dimensions, which may open up further speed-up possibilities. Finally, similarly to Denton et al. (2014), we note that low-rank decompositions seems to have a regularizing effects allowing to slightly improve the overall accuracy for higher rank.", "startOffset": 83, "endOffset": 752}], "year": 2017, "abstractText": "We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it yields larger CPU speedups at the cost of lower accuracy drops compared to previous approaches. For the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1% from 91% to 90%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1% increase of the overall top-5 classification error.", "creator": "LaTeX with hyperref package"}}}