{"id": "1412.6575", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "abstract": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN and TransE, can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE when evaluated on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules from the KB. We demonstrate that embeddings trained from the bilinear objective can effectively capture relation composition via matrix multiplication. We also show that our embedding-based approach can extract rules that involve relation transitivity more effectively than a state-of-the-art rule mining approach that is tailored for large-scale KBs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 20 Dec 2014 01:37:16 GMT  (1789kb,D)", "http://arxiv.org/abs/1412.6575v1", "13 pages, 4 figures"], ["v2", "Sat, 27 Dec 2014 00:18:17 GMT  (1627kb,D)", "http://arxiv.org/abs/1412.6575v2", "12 pages, 4 figures"], ["v3", "Fri, 10 Apr 2015 15:24:59 GMT  (1628kb,D)", "http://arxiv.org/abs/1412.6575v3", "12 pages, 4 figures"], ["v4", "Sat, 29 Aug 2015 15:08:45 GMT  (1628kb,D)", "http://arxiv.org/abs/1412.6575v4", "12 pages, 4 figures"]], "COMMENTS": "13 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bishan yang", "wen-tau yih", "xiaodong he", "jianfeng gao", "li deng"], "accepted": true, "id": "1412.6575"}, "pdf": {"name": "1412.6575.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "emails": ["bishan@cs.cornell.edu", "scottyih@microsoft.com", "xiaohe@microsoft.com", "jfgao@microsoft.com", "deng@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase1, DBPedia (Auer et al., 2007), and YAGO (Suchanek et al., 2007). These KBs store facts about real-world entities (e.g. people, places, and things) in the form of RDF triples2 (i.e. (subject, predicate, object)). Today\u2019s KBs are large in size. For instance, Freebase contains millions of entities and billions of facts (triples) involving a large variety of predicates (relation types). Such large-scale multirelational data provide an excellent potential for improving a wide range of tasks, from information retrieval, question answering to biological data mining.\nRecently, much effort has been invested in relational learning methods that can scale to large knowledge bases. Tensor factorization (e.g. (Nickel et al., 2011; 2012)) and neural-embedding-based models (e.g. (Bordes et al., 2013a;b; Socher et al., 2013)) are two popular kinds of approaches that learn to encode relational information using low-dimensional representations of entities and relations. These representation learning methods have shown good scalability and reasoning ability in terms of validating unseen facts given the existing KB.\nIn this work, we focus on the study of neural-embedding models, where the representations are learned using neural networks with energy-based objectives. Recent embedding models TransE (Bordes et al., 2013b) and NTN (Socher et al., 2013) have shown state-of-the-art prediction performance compared to tensor factorization methods such as RESCAL (Nickel et al., 2012). They are similar in model forms with slight differences on the choices of entity and relation representations. Without careful comparison, it is not clear how different design choices affect the learning results. In addition, the performance of the embedding models are evaluated on the link\n\u02daWork conducted while interning at Microsoft Research. 1http://freebase.com 2http://www.w3.org/TR/rdf11-concepts/\nar X\niv :1\n41 2.\n65 75\nv1 [\ncs .C\nL ]\n2 0\nD ec\n2 01\n4\nprediction task (i.e. predicting the correctness of unseen triples). This only indirectly shows the meaningfulness of low-dimensional embeddings. It is hard to explain what relational properties are being captured and to what extent they are captured during the embedding process.\nWe make three main contributions in this paper. (1) We present a general framework for multirelational learning that unifies most multi-relational embedding models developed in the past, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b). (2) We empirically evaluate different choices of entity representations and relation representations under this framework on the canonical link prediction task and show that a simple bilinear formulation achieves new stateof-the-art results for the task (a top-10 accuracy of 73.2% vs. 54.7% by TransE when evaluated on Freebase). (3) For further investigation of the learned embeddings, we propose and evaluate a novel approach that utilizes the embeddings to mine logical rules from the KB. For instance, BornInCitypa, bq ^ CityOfCountrypb, cq \u00f9\u00f1 HasNationalitypa, cq. We show that such rules can be effectively extracted by modeling the composition of relation embeddings, and that the embeddings learned from the bilinear objective are particularly good at capturing relation transitivity. To evaluate this approach, we compare our embedding-based rule extraction approach with a state-of-the-art rule mining system AMIE (Gala\u0301rraga et al., 2013) and demonstrate the advantages of our approach in terms of effectiveness and efficiency.\nThe rest of this paper is structured as follows. Section 2 discusses related work. Section 3 presents the general framework for learning multi-relation representations. Sections 4 and 5 present two inference tasks: a canonical link prediction task and a novel rule extraction task where the learned embeddings are empirically evaluated. Section 6 concludes the paper."}, {"heading": "2 RELATED WORK", "text": "Multi-relational learning has been an active research area for the past couple of years. Prior work can be categorized into three categories: (1) statistical relational learning (SRL) (Getoor & Taskar, 2007), such as Markov-logic networks (Richardson & Domingos, 2006), which directly encode multi-relational graphs using probabilistic models; (2) path ranking methods (Lao et al., 2011; Dong et al., 2014), which explicitly explore the large feature space of relations with random walk; and (3) representation learning methods, which embed multi-relational knowledge into low-dimensional representations of entities and relations via tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al., 2011; 2012), Bayesian clustering framework (Kemp et al., 2006; Sutskever et al., 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013). Our work focuses on the study of neural-embedding models, where the representations of the involved entities and relations are learned using neural networks.\nExisting neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities. The main difference among these models lies in the parametrization of the relation operators. For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each relation as a bilinear tensor operator followed by a linear matrix operator. The model of TransE (Bordes et al., 2013b), on the other hand, represents each relation as a single vector that linearly interacts with the entity vectors. There has not been work that closely examines the effectiveness of different design choices. Likewise, variations on entity representations also exist. For example, NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from large text corpora. This idea is promising as pre-trained vectors tend to capture syntactic and semantic information from natural language and can assist in better generalization of entity embeddings. However, averaging word vectors may not always be appropriate, especially for entities denoted by non-compositional phrases (e.g. person names, movie names, etc.).\nOur work on embedding-based rule extraction presented in part of this paper is related to the earlier study on logical inference with learned continuous-space representations. Much existing work along this line focuses on learning logic-based representations for natural language sentences. For example, Socher et al. (2012) builds a neural network that recursively combines word representations based on parse tree structures and shows that such neural network can simulate the behavior of conjunction and negation. Bowman (2014) further demonstrates that recursive neural network can capture certain aspects of natural logical reasoning on examples involving quantifiers like some\nand all. Recently, Grefenstette (2013) shows that in theory low-dimensional embeddings of entities and relations can be used to simulate the behavior of general first-order logic in tensor calculus. Rockta\u0308schel et al. (2014) further implements the idea by introducing a supervised objective that trains embeddings to be consistent with given logical rules. The evaluation was conducted on toy data and uses limited logical forms. Advancing these earlier studies, we in this paper propose a different approach: To utilizing embeddings learned without explicit logical constraints to directly mine logical rules from KBs. We demonstrate that the learned embeddings of relations can recover the transitive property of relations and can be used to simulate the behavior of horn rules that involve such a property. We also provide systematic evaluation on large-scale KBs and compare it favorably with a state-of-the-art rule mining approach."}, {"heading": "3 MULTI-RELATIONAL REPRESENTATION LEARNING", "text": "In this section, we present a general neural network framework for multi-relational representation learning. We discuss different design choices for the representations of entities and relations which will be empirically compared in Section 4.\nGiven a KB that is represented as a list of relation triplets pe1, r, e2q (denoting e1 (the subject) and e2 (the object) that are in a certain relationship r), we want to learn representations for entities and relations such that valid triplets receive high scores (or low energies). The embeddings can be learned via a neural network architecture. The first layer projects a pair of input entities (represented as \u201cone-hot\u201d index vectors or \u201cn-hot\u201d feature vectors) to low dimensional vectors, and the second layer combines these two vectors to a scalar for comparison via a scoring function with relationspecific parameters."}, {"heading": "3.1 ENTITY REPRESENTATIONS", "text": "Most existing embedding models adopt \u201cone-hot\u201d index vectors as entity input. Some utilizes the textual names associated with entities. For example, NTN (Socher et al., 2013) represents each entity as an average of its word vectors. This can be viewed as adopting \u201cbag-of-words\u201d vectors as input and learning a projection matrix consisting of word vectors.\nDenote by xe1 and xe2 the input vectors for entity e1 and e2, respectively. Denote by W the first layer projection matrix. The projected entity vectors, ye1 and ye2 can be written as\nye1 \u201c f ` Wxe1 \u02d8 , ye2 \u201c f ` Wxe2 \u02d8\nwhere f can be a linear or non-linear function, and W is a parameter matrix, which can be randomly initialized or initialized using pre-trained word vectors (as in (Socher et al., 2013)) or pre-trained entity vectors."}, {"heading": "3.2 RELATION REPRESENTATIONS", "text": "The choice of relation representations reflects in the form of the scoring function. Most of the existing scoring functions in the literature can be unified based on a basic linear transformation gar , a bilinear transformation gbr or their combination, where g a r and g b r are defined as\ngar pye1 ,ye2q \u201c ATr \u02c6\nye1 ye2\n\u02d9\nand gbrpye1 ,ye2q \u201c yTe1Brye2 , (1)\nwhich Ar and Br are relation-specific parameters.\nIn Table 1, we summarize several popular scoring functions in the literature for a relation triplet pe1, r, e2q, reformulated in terms of the above two functions. Denote by ye1 ,ye2 P Rn two entity vectors. Denote by Qr1 ,Qr2 P Rn\u02c6m and Vr P Rn matrix or vector parameters for linear transformation gar . Denote by Tr P Rn\u02c6n\u02c6m tensor parameters for bilinear transformation gbr. I P Rn is an identity matrix. ur P Rm is an additional parameter for relation r. The scoring function for TransE is derived from ||ye1\u00b4ye2`Vr||22 \u201c 2V Tr pye1\u00b4ye2q\u00b42yTe1ye2`||Vr|| 2 2`||ye1 ||22`||ye2 ||22, where ye1 and ye2 are unit vectors.\nNote that NTN is the most expressive model as it contains both linear and bilinear relation operators as special cases. In terms of the number of parameters, TransE is the simplest model which only parametrizes the linear relation operators with one-dimensional vectors.\nIn this paper, we also consider the bilinear scoring function:\ngbrpye1 ,ye2q \u201c yTe1Mrye2 (2)\nwhich is a special case of NTN without the non-linear layer and the linear operator, and uses a 2-d matrix operator Mr P Rn\u02c6n instead of a tensor operator. By restricting Mr to be a diagonal matrix, the number of relation parameters becomes the same as TransE. Our experiments reported in this paper demonstrate that this simple formulation enjoys the same scalable property as TransE and it achieves superior performance over TransE and other more expressive models on several multi-relational data sets.\nThis general framework for relationship modeling also applies to the recent deep-structured semantic model (Huang et al., 2013; Shen et al., 2014a;b; Gao et al., 2014; Yih et al., 2014), which learns the relevance or a single relation between a pair of word sequences. The framework above applies when using multiple neural network layers to project entities and using a relation-independent scoring function Gr ` ye1 ,ye2 \u02d8\n\u201c cosrye1pWrq,ye2pWrqs. The cosine scoring function is a special case of gbr with normalized ye1 ,ye2 and with Br \u201c I."}, {"heading": "3.3 PARAMETER LEARNING", "text": "The neural network parameters of all the models discussed above can be learned by minimizing a margin-based ranking objective4, which encourages the scores of positive relationships (triplets) to be higher than the scores of any negative relationships (triplets). Usually only positive triplets are observed in the data. Given a set of positive triplets T , we can construct a set of \u201cnegative\u201d triplets T 1 by corrupting either one of the relation arguments, T 1 \u201c tpe11, r, e2q|e11 P E, pe11, r, e2q R T u Y tpe1, r, e12q|e12 P E, pe1, r, e12q R T u. Denote the scoring function for triplet pe1, r, e2q as Spe1,r,e2q. The training objective is to minimize the margin-based ranking loss\nLp\u2126q \u201c \u00ff\npe1,r,e2qPT\n\u00ff\npe11,r,e12qPT 1 maxtSpe11,r,e12q \u00b4 Spe1,r,e2q ` 1, 0u (3)"}, {"heading": "4 INFERENCE TASK I: LINK PREDICTION", "text": "We first conduct a comparison study of different embedding models on the canonical link prediction task, which is to predict the correctness of unseen triplets. As in (Bordes et al., 2013b), we formulate link prediction as an entity ranking task. For each triplet in the test data, we treat each entity as the target entity to be predicted in turn. Scores are computed for the correct entity and all the corrupted entities in the dictionary and are ranked in descending order. We consider Mean Reciprocal Rank (MRR) (an average of the reciprocal rank of an answered entity over all test triplets), HITS@10 (top-10 accuracy), and Mean Average Precision (MAP) (as used in (Chang et al., 2014)) as the evaluation metrics.\nWe examine five embedding models in decreasing order of complexity: (1) NTN with 4 tensor slices as in (Socher et al., 2013); (2) Bilinear+Linear, NTN with 1 tensor slice and without the non-linear layer; (3) TransE, a special case of Bilinear+Linear (see Table 1); (4) Bilinear: using scoring function in Eq. (2); (5) Bilinear-diag: a special case of Bilinear where the relation matrix is a diagonal matrix.\n4Other objectives such as mutual information (as in (Huang et al., 2013)) and reconstruction loss (as in tensor decomposition approaches (Chang et al., 2014)) can also be applied. Comparisons among these objectives are beyond the scope of this paper.\nDatasets We used the WordNet (WN) and Freebase (FB15k) datasets introduced in (Bordes et al., 2013b). WN contains 151, 442 triplets with 40, 943 entities and 18 relations, and FB15k consists of 592, 213 triplets with 14, 951 entities and 1345 relations. We use the same training/validation/test split as in (Bordes et al., 2013b). We also consider a subset of FB15k (FB15k-401) containing only frequent relations (relations with at least 100 training examples). This results in 560, 209 triplets with 14, 541 entities and 401 relations.\nImplementation details All the models were implemented in C# and using GPU. Training was implemented using mini-batch stochastic gradient descent with AdaGrad (Duchi et al., 2011). At each gradient step, we sampled for each positive triplet two negative triplets, one with a corrupted subject entity and one with a corrupted object entity. The entity vectors are renormalized to have unit length after each gradient step (it is an effective technique that empirically improved all the models). For the relation parameters, we used standard L2 regularization. For all models, we set the number of mini-batches to 10, the dimensionality of the entity vector d \u201c 100, the regularization parameter 0.0001, and the number of training epochs T \u201c 100 on FB15k and FB15k-401 and T \u201c 300 on WN (T was determined based on the learning curves where the performance of all models plateaued.) The learning rate was initially set to 0.1 and then adapted during training by AdaGrad.\n4.1 RESULTS\nTable 2 shows the results of all compared methods on all the datasets. In general, we observe that the performance increases as the complexity of the model decreases on FB. NTN, the most complex model, provides the worst performance on both FB and WN, which suggests overfitting. Compared to the previously published results of TransE (Bordes et al., 2013b), our implementation achieves much better results (53.9% vs. 47.1% on FB15k and 90.9% vs. 89.2% on WN) using the same evaluation metric (HITS@10). We attribute such discrepancy mainly to the different choice of SGD optimization: AdaGrad vs. constant learning rate. We also found that Bilinear consistently provides comparable or better performance than TransE, especially on WN. Note that WN contains much more entities than FB, it may require the parametrization of relations to be more expressive to better handle the richness of entities. Interestingly, we found that a simple variant of Bilinear \u2013 BILINEAR-DIAG, clearly outperforms all baselines on FB and achieves comparable performance to Bilinear on WN. Note that BILINEAR-DIAG has the limitation of encoding the difference between a relation and its inverse. Still, as there is a large variety of relations in FB and the average number of training examples seen by each relation is relatively small (compared to WN), the simple form of BILINEAR-DIAG is able to provide good prediction performance.\nMultiplicative vs. Additive Interactions Note that BILINEAR-DIAG and TRANSE have the same number of model parameters and their difference can be viewed as the operational choices of the composition of two entity vectors \u2013 BILINEAR-DIAG uses weighted element-wise dot product (multiplicative operation) and TRANSE uses element-wise subtraction with a bias (additive operation). To highlight the difference, here we use DISTMULT and DISTADD to refer to BILINEAR-DIAG and TRANSE, respectively. Comparisons between these two models can provide us more insights on the effect of two common choices of compositional operations \u2013 multiplication and addition for modeling entity relations. Overall, we observed superior performance of DISTMULT on all the datasets in Table 2. Table 3 shows the HITS@10 score on four types of relation categories (as defined in (Bordes et al., 2013b)) on FB15k-401 when predicting the subject entity and the object entity respectively. We can see that DISTMULT significantly outperforms DISTADD in almost all the categories.\nInitialization of Entity Vectors In the following, we examine the learning of entity representations and introduce two further improvements: using non-linear projection and initializing entity vectors\nwith pre-trained phrase vectors. We focus on DISTMULT as our baseline and compare it with the two modifications DISTMULT-tanh (using f \u201c tanh for entity projection5) and DISTMULT-tanh-EVinit (initializing the entity parameters with the 1000-dimensional pre-trained phrase vectors released by word2vec (Mikolov et al., 2013)) on FB15k-401. We also reimplemented the word vector representation and initialization technique introduced in (Socher et al., 2013) \u2013 each entity is represented as an average of its word vectors and the word vectors are initialized using the 300-dimensional pretrained word vectors released by word2vec. We denote this method as DISTMULT-tanh-WV-init. Inspired by (Chang et al., 2014), we design a new evaluation setting where the predicted entities are automatically filtered according to \u201centity types\u201d (entities that appear as the subjects/objects of a relation have the same type defined by that relation). This provides us with better understanding of the model performance when some entity type information is provided.\nIn Table 4, we can see that DISTMULT-tanh-EV-init provides the best performance on all the metrics. Surprisingly, we observed performance drops by DISTMULT-tanh-WV-init. We suspect that this is because word vectors are not appropriate for modeling entities described by non-compositional phrases (more than 73% of the entities in FB15k-401 are person names, locations, organizations and films). The promising performance of DISTMULT-tanh-EV-init suggests that the embedding model can greatly benefit from pre-trained entity-level vectors."}, {"heading": "5 INFERENCE TASK II: RULE EXTRACTION", "text": "In this section, we focus on a complementary inference task, where we utilize the learned embeddings to extract logical rules from the KB. For example, given the fact that a person was born in New York and New York is a city of the United States, then the person\u2019s nationality is the United States:\nBornInCitypa, bq ^ CityOfCountrypb, cq \u00f9\u00f1 HasNationalitypa, cq Such logical rules can serve four important purposes. First, they can help deduce new facts and complete the existing KBs. Second, they can help optimize data storage by storing only rules instead of large amounts of extensional data, and generate facts only at inference time. Third, they can support complex reasoning. Lastly, they can provide explanations for inference results, e.g. we may infer that films are often in the language which the director speaks, and that people\u2019s professions are usually the specialization of the field they study, etc.\nThe key problem of extracting horn rules like the aforementioned example is how to effectively explore the search space. Traditional rule mining approaches directly operate on the KB graph \u2013 they search for possible rules (i.e. closed-paths in the graph) by pruning and filtering rules with low statistical significance and relevance (Schoenmackers et al., 2010). These approaches often fail on large KB graphs due to scalability issues. In the following, we introduce a novel embedding-based rule mining approach whose efficiency is not affected by the size of the KB graph but rather by the number of distinct types of relations in the KB (which is usually relatively small). It can also mine better rules due to its strong generalizability.\n5When applying non-linearity, we remove the normalization steps on entity parameters during training as tanh already helps control the scaling freedoms."}, {"heading": "5.1 BACKGROUND AND NOTATIONS", "text": "For a better illustration, we adopt the graph view of KB. Each relation rpa, bq is a directed edge from node a to node b and with link type r. We are interested in extracting horn rules that consist of a head relation H and a sequence of body relations B1, ..., Bn:\nB1pa1, a2q ^B2pa2, a3q ^ ...^Bnpan, an`1q \u00f9\u00f1 Hpa1, an`1q (4) where ai are variables that can be substituted by entities. A rule is instantiated when all variables are substituted by entities in the KB. We constrain H,B1, ..., Bn to be distinct types of relations as we are interested in mining the composition properties of different relations. We also constrain B1, ..., Bn to form a directed path in the graph and the head relation H to be a directed edge that close the path: from the start node to the end node. We denote such property as closed-path property. For consecutive relations that share one variable but do not form a path, e,g, Bi\u00b41pa, bq ^ Bipa, cq, we can replace one of the relations with its inverse relation, so that the relations are connected by an object and an subject, e.g. B\u00b41i\u00b41pb, aq ^Bipa, cq. We denote the length of the rule as the number of body relations. In general longer rules are harder to extract due to the exponential search space. In our experiments, we focus on extracting rules of length 2 and 3.\nIn KBs, entities usually have types and relations often can only take arguments of certain types. For example, BornInCity relation can only take a person as the subject and a location as the object. For each relation r, we denote the domain of its subject argument (the set of entities that can appear in the subject position) as Xr and similarly the domain of its object argument as Yr. Such domain information can be extremely useful in restricting the search space of logical rules."}, {"heading": "5.2 EMBEDDING-BASED RULE EXTRACTION", "text": "For simplicity, we consider horn rules of length 2 (longer rules can be easily derived from this case):\nB1pa, bq ^B2pb, cq \u00f9\u00f1 Hpa, cq (5) Note that the body of the rule can be viewed as the composition of relations B1 and B2, which is a new relation that has the property that entities a and c are in a relation if and only if there is an entity b which simultaneously satistifies two relations B1pa, bq and B2pb, cq. We model relation composition as multiplication or addition of two relation embeddings. Here we focus on relation embeddings that are in the form of vectors (as in TRANSE) and matrices (as in BILINEAR and its variants). The composition results in a new embedding that lies in the same relation space. Specifically, we use addition for embeddings learned from TRANSE and multiplication for embeddings learned from BILINEAR, inspired by the two different model forms: TRANSE optimizes ya `V1 \u201e yb and yb `V2 \u201e yc, which may indicate that ya ` pV1 `V2q \u201e yc; BILINEAR optimizes yTaM1yb and y T b M2yc to have high scores, which may imply the optimization of pyTaM1ybqpyTb M2ycq \u201c tyTa pM1M2qyc, t \u0105 0. To simulate the implication in 5, we want the composition result of relation B1 and B2 to demonstrate similar behavior to the embedding of relation H . We assume the similarity between relation embeddings is related to the Euclidean distance if the embeddings are vectors and to the Frobenius norm if the embeddings are matrices. This distance metric allows us to rank possible pairs of relations with respect to the relevance of their composition to the target relation.\nNote that we do not need to enumerate all possible pairs of relations in the KB. For example, if the relation in the head is r, then we are only interested in relation pairs pp, qq that satisfy the type constraints, namely: (1) YpXXq \u2030 H; (2) XpXXr \u2030 H; (3) Yq XYr \u2030 H. As mentioned before, the arguments (entities) of relations are usually strongly typed in KBs. Applying such domain constraints can effectively reduce the search space.\nIn Algorithm 1, we describe our rule extraction algorithm for general closed-path horn rules as in Eq. (4).\nIn Step 7, \u02dd denotes vector addition or matrix multiplication. We apply a global threshold value t in our experiments to filter candidate sequences for each relation r, and then automatically select top K remaining sequences by applying a heuristic thresholding strategy based on the difference of the distance scores: sort the sequences by increasing distance d1, ..., dT and set the cut-off point to be the j-th sequence where j \u201c arg maxipdi`1 \u00b4 diq.\nAlgorithm 1 EMBEDRULE 1: Input: KB \u201c tpe1, r, e2qu, relation set R 2: Output: Candidate rules Q 3: for each r in R do 4: Select the set of start relations S \u201c ts : Xs X Xr \u2030 Hu 5: Select the set of end relations T \u201c tt : Yt X Yr \u2030 Hu 6: Find all possible relation sequences P \u201c tpp1, ..., pnq : Ypi XXpi`1 \u2030 H, p1 P S, pn P T u 7: Select the K-NN sequences P 1 \u010e P for r based on distpMr,Mp1 \u02dd \u00a8 \u00a8 \u00a8 \u02ddMpnq 8: Form candidate rules using P 1 where r is the head relation and p P P 1 is the body in a rule 9: Add the candidate rules into Q 10: end for"}, {"heading": "5.3 EXPERIMENTS", "text": "We evaluate our rule extraction method (denoted as EMBEDRULE) on the FB15k-401 dataset. In our experiments, we remove the equivalence relations and relations whose domains have cardinality 1 since rules involving these relations are not interesting. This results in training data that contains 485,741 facts, 14,417 entities, and 373 relations. Our EMBEDRULE algorithm identifies 60,020 possible length-2 relation sequences and 2,156,391 possible length-3 relation sequences. We then apply the thresholding method described in Section 5.2 to further select top \u201e3.9K length-2 rules and \u201e2K length-3 rules. By default all the extracted rules are ranked by decreasing confidence score: the ratio of the correct predictions (triplets that are in the observed KB) to the total number of predictions, where predictions are triplets that are derived from the instantiated rules where all body relations appear in the observed KB.\nWe implemented four versions of EMBEDRULE using embeddings trained from TRANSE (DISTADD), BILINEAR, BILINEAR-DIAG (DISTMULT) and DISTMULT-tanh-EV-init with corresponding composition functions. We also compare our approaches to AMIE (Gala\u0301rraga et al., 2013), a state-of-the-art rule mining system that can efficiently search for horn rules in large-scale KBs by using novel measurements of support and confidence. The system extracts close rules \u2013 a superset of the rules we consider in this paper: every relation in the body is connected to the following relation by sharing an entity variable, and every entity variable in the rule appears at least twice. We run AMIE with the default setting on the same training set. It extracts 2,201 possible length-1 rules and 46,975 possible length-2 rules, among which 3,952 rules have the close-path property. We compare these length-2 rules with the similar number of length-2 rules extracted by EMBEDRULE. By default AMIE ranks rules by PCA confidence (a normalized confidence that takes into account the incompleteness of KBs). However we found that ranking by the standard confidence gives better performance than the PCA confidence on the Freebase dataset we use.\nFor computational cost, EmbedRule mines length-2 rules in 2min and mines length-3 rules in 20min (the computational time is similar when using different types of embeddings). AMIE mines rules of length \u010f 2 in 9min. All methods are evaluated on a machine with a 64-bit processor, 2 CPUs and 8GB memory."}, {"heading": "5.4 RESULTS", "text": "For the evaluation of different rule extraction methods, we compute the estimated precision, which is the ratio of predictions that are in the test (unseen) data to all the generated unseen predictions (triplets that are not in the observed data). Note that the precision is an estimation, since a prediction is not necessarily \u201cincorrect\u201d if it is not seen. Gala\u0301rraga et al. (2013) propose to automatically identify incorrect predictions based on the functional property of relations. However, we find that most relations in our datasets are not functional. For a better estimation, we manually labeled the top 30 unseen facts predicted by each method by checking Wikipedia. We also remove rules where the head relations are hard to justified due to dynamic factors, e.g. \u201cthe current roster position of a football team\u201d.\nFigure 1 compares the predictions generated by the length-2 rules extracted by AMIE and different implementations of EMBEDRULE. We plot the aggregated precision of the top rules that produce up to 104 predictions in total. From left to right, the n-th data point represents the to-\ntal number of predictions of the top n rules and the estimated precision of these predictions. We can see that EMBEDRULE that uses embeddings trained from the bilinear objective (BILINEAR, DISTMULT and DISTMULT-TANH-EV-INIT) consistently outperforms AMIE. This suggests that the bilinear embeddings contain good amount of information about relations which allows for effective rule selection without looking at the entities. For example, AMIE fails to extract AtheleteP layInTeampa, bq ^ TeamPlaySportpb, cq \u00f9\u00f1 AtheleteP laySportpa, cq by relying on the instantiations of the rule occurred in the observed facts while all the bilinear variants of EMBEDRULE successfully extract the rule purely based on the embeddings of its three containing relations.\nWe also find that in general, using bilinear matrix embeddings and multiplicative composition results in better performance compared to using vector (from DISTADD) embeddings and additive composition. In particular, we find that for a given head relations, DISTMULT tends to retrieve more sensible relation combinations as the nearest neighbors compared to DISTADD. For example, for relation FilmProductionDesignedBypa, cq, DISTMULT retrieves FilmPrequelpa, bq^FilmArtDesignedBypb, cq as the top relation combination while DISTADD retrieves FilmSequelpa, bq ^FilmAwardNomineepb, cq. This further indicates the advantage of bilinear operator in modeling relations.\nWe also look at the results for length-3 rules generated by different implementations of EMBEDRULE in Figure 2. Even though extracting longer rules is typically a more difficult task, we can see that the initial length-3 rules extracted by EMBEDRULE have very good precision in general. In particular, BILINEAR consistently outperforms DISTMULT and DISTADD on the top 1K predictions and DISTMULT-TANH-EV-INIT tends to outperform the other methods as more predictions are generated. We think that the fact that BILINEAR starts to show more advantage over DISTMULT in extracting longer rules confirm the limitation of representing relations by diagonal matrices, as longer rules requires the modeling of more complex relation semantics."}, {"heading": "6 CONCLUSION", "text": "In this paper, we present a general framework for learning representations of entities and relations in KBs. Under the framework, we empirically evaluate different embedding models on knowledge inference tasks. We show that a simple formulation of bilinear model can outperform the stateof-the-art embedding models for link prediction on Freebase. Furthermore, we examine the learned embeddings by utilizing them to extract rules from KBs. Relying on the modeling of relation composition (e.g. the composition of BornInCity and CityInCountry implies Nationality), we show that embeddings learned from the bilinear objective can be used to extract rules more effectively than a state-of-the-art rule mining system that is tailored for large-scale KBs. For future work, we aim to exploit deep structure in the neural network framework. As learning representations using deep networks has shown great success in various applications (Hinton et al., 2012; Vinyals et al., 2012; Deng et al., 2013), it may also help capturing hierarchical structure hidden in the multi-relational data. Further, tensor constructs have been usefully applied to some deep learning architectures (Yu et al., 2013; Hutchinson et al., 2013). Related constructs and architectures may help improve multirelational embeddings and learning."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer", "S\u00f6ren", "Bizer", "Christian", "Kobilarov", "Georgi", "Lehmann", "Jens", "Cyganiak", "Richard", "Ives", "Zachary"], "venue": "In The semantic web,", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes", "Antoine", "Weston", "Jason", "Collobert", "Ronan", "Bengio", "Yoshua"], "venue": "In AAAI,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Bordes", "Antoine", "Glorot", "Xavier", "Weston", "Jason", "Bengio", "Yoshua"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana"], "venue": "In NIPS,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Bowman", "Samuel R"], "venue": "In ICLR,", "citeRegEx": "Bowman and R.,? \\Q2014\\E", "shortCiteRegEx": "Bowman and R.", "year": 2014}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Chang", "Kai-Wei", "Yih", "Wen-tau", "Yang", "Bishan", "Meek", "Chris"], "venue": "In EMNLP,", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["Deng", "Li", "G. Hinton", "B. Kingsbury"], "venue": "In in ICASSP,", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Knowledge vault: A Web-scale approach to probabilistic knowledge fusion", "author": ["Dong", "Xin Luna", "K Murphy", "E Gabrilovich", "G Heitz", "W Horn", "N Lao", "Strohmann", "Thomas", "Sun", "Shaohua", "Zhang", "Wei"], "venue": "In KDD,", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Amie: association rule mining under incomplete evidence in ontological knowledge bases", "author": ["Gal\u00e1rraga", "Luis Antonio", "Teflioudi", "Christina", "Hose", "Katja", "Suchanek", "Fabian"], "venue": "In WWW,", "citeRegEx": "Gal\u00e1rraga et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gal\u00e1rraga et al\\.", "year": 2013}, {"title": "Modeling interestingness with deep neural networks", "author": ["Gao", "Jianfeng", "Pantel", "Patrick", "Gamon", "Michael", "He", "Xiaodong", "Deng", "Li", "Shen", "Yelong"], "venue": "In EMNLP,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Introduction to Statistical Relational Learning", "author": ["Getoor", "Lise", "Taskar", "Ben (eds"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Grefenstette", "Edward"], "venue": "In *SEM,", "citeRegEx": "Grefenstette and Edward.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette and Edward.", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Hinton", "Geoff", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "IEEE Sig. Proc. Mag.,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Learning deep structured semantic models for Web search using clickthrough data", "author": ["Huang", "Po-Sen", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Acero", "Alex", "Heck", "Larry"], "venue": "In CIKM,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Tensor deep stacking networks", "author": ["B Hutchinson", "L. Deng", "D. Yu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hutchinson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2013}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Kemp", "Charles", "Tenenbaum", "Joshua B", "Griffiths", "Thomas L", "Yamada", "Takeshi", "Ueda", "Naonori"], "venue": "In AAAI,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao", "Ni", "Mitchell", "Tom", "Cohen", "William W"], "venue": "In EMNLP,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Nickel", "Maximilian", "Tresp", "Volker", "Kriegel", "Hans-Peter"], "venue": "In ICML, pp", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["Nickel", "Maximilian", "Tresp", "Volker", "Kriegel", "Hans-Peter"], "venue": "In WWW, pp", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Learning distributed representations of concepts using linear relational embedding", "author": ["Paccanaro", "Alberto", "Hinton", "Geoffrey E"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Paccanaro et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Paccanaro et al\\.", "year": 2001}, {"title": "Low-dimensional embeddings of logic", "author": ["Rockt\u00e4schel", "Tim", "Bo\u0161njak", "Matko", "Singh", "Sameer", "Riedel", "Sebastian"], "venue": "In ACL Workshop on Semantic Parsing,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2014}, {"title": "Learning first-order horn clauses from web text", "author": ["Schoenmackers", "Stefan", "Etzioni", "Oren", "Weld", "Daniel S", "Davis", "Jesse"], "venue": "In EMNLP,", "citeRegEx": "Schoenmackers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Schoenmackers et al\\.", "year": 2010}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Shen", "Yelong", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Mesnil", "Gregoire"], "venue": "In CIKM,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for Web search", "author": ["Shen", "Yelong", "He", "Xiaodong", "Gao", "Jianfeng", "Deng", "Li", "Mesnil", "Gr\u00e9goire"], "venue": "In WWW,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Relational learning via collective matrix factorization", "author": ["Singh", "Ajit P", "Gordon", "Geoffrey J"], "venue": "In KDD,", "citeRegEx": "Singh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2008}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher", "Richard", "Huval", "Brody", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Suchanek", "Fabian M", "Kasneci", "Gjergji", "Weikum", "Gerhard"], "venue": "In WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["Sutskever", "Ilya", "Tenenbaum", "Joshua B", "Salakhutdinov", "Ruslan"], "venue": "In NIPS, pp", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Under review as conference paper at ICRL", "author": ["O. Vinyals", "Y. Jia", "L. Deng", "T. Darrell"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantic parsing for single-relation question", "author": ["NIPS", "2012. Yih", "Wen-tau", "He", "Xiaodong", "Meek", "Christopher"], "venue": null, "citeRegEx": "NIPS et al\\.,? \\Q2012\\E", "shortCiteRegEx": "NIPS et al\\.", "year": 2012}, {"title": "The deep tensor neural network with applications to large vocabulary", "author": ["D. Yu", "L. Deng", "F. Seide"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 0, "context": "Recent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase1, DBPedia (Auer et al., 2007), and YAGO (Suchanek et al.", "startOffset": 95, "endOffset": 114}, {"referenceID": 29, "context": ", 2007), and YAGO (Suchanek et al., 2007).", "startOffset": 18, "endOffset": 41}, {"referenceID": 19, "context": "(Nickel et al., 2011; 2012)) and neural-embedding-based models (e.", "startOffset": 0, "endOffset": 27}, {"referenceID": 28, "context": "(Bordes et al., 2013a;b; Socher et al., 2013)) are two popular kinds of approaches that learn to encode relational information using low-dimensional representations of entities and relations.", "startOffset": 0, "endOffset": 45}, {"referenceID": 28, "context": ", 2013b) and NTN (Socher et al., 2013) have shown state-of-the-art prediction performance compared to tensor factorization methods such as RESCAL (Nickel et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 20, "context": ", 2013) have shown state-of-the-art prediction performance compared to tensor factorization methods such as RESCAL (Nickel et al., 2012).", "startOffset": 115, "endOffset": 136}, {"referenceID": 28, "context": "(1) We present a general framework for multirelational learning that unifies most multi-relational embedding models developed in the past, including NTN (Socher et al., 2013) and TransE (Bordes et al.", "startOffset": 153, "endOffset": 174}, {"referenceID": 9, "context": "To evaluate this approach, we compare our embedding-based rule extraction approach with a state-of-the-art rule mining system AMIE (Gal\u00e1rraga et al., 2013) and demonstrate the advantages of our approach in terms of effectiveness and efficiency.", "startOffset": 131, "endOffset": 155}, {"referenceID": 17, "context": "Prior work can be categorized into three categories: (1) statistical relational learning (SRL) (Getoor & Taskar, 2007), such as Markov-logic networks (Richardson & Domingos, 2006), which directly encode multi-relational graphs using probabilistic models; (2) path ranking methods (Lao et al., 2011; Dong et al., 2014), which explicitly explore the large feature space of relations with random walk; and (3) representation learning methods, which embed multi-relational knowledge into low-dimensional representations of entities and relations via tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al.", "startOffset": 280, "endOffset": 317}, {"referenceID": 7, "context": "Prior work can be categorized into three categories: (1) statistical relational learning (SRL) (Getoor & Taskar, 2007), such as Markov-logic networks (Richardson & Domingos, 2006), which directly encode multi-relational graphs using probabilistic models; (2) path ranking methods (Lao et al., 2011; Dong et al., 2014), which explicitly explore the large feature space of relations with random walk; and (3) representation learning methods, which embed multi-relational knowledge into low-dimensional representations of entities and relations via tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al.", "startOffset": 280, "endOffset": 317}, {"referenceID": 19, "context": ", 2014), which explicitly explore the large feature space of relations with random walk; and (3) representation learning methods, which embed multi-relational knowledge into low-dimensional representations of entities and relations via tensor/matrix factorization (Singh & Gordon, 2008; Nickel et al., 2011; 2012), Bayesian clustering framework (Kemp et al.", "startOffset": 264, "endOffset": 313}, {"referenceID": 16, "context": ", 2011; 2012), Bayesian clustering framework (Kemp et al., 2006; Sutskever et al., 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al.", "startOffset": 45, "endOffset": 88}, {"referenceID": 30, "context": ", 2011; 2012), Bayesian clustering framework (Kemp et al., 2006; Sutskever et al., 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al.", "startOffset": 45, "endOffset": 88}, {"referenceID": 28, "context": ", 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013).", "startOffset": 29, "endOffset": 100}, {"referenceID": 28, "context": "Existing neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities.", "startOffset": 33, "endOffset": 78}, {"referenceID": 28, "context": "For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each relation as a bilinear tensor operator followed by a linear matrix operator.", "startOffset": 81, "endOffset": 102}, {"referenceID": 28, "context": "For example, NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from large text corpora.", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": ", 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013). Our work focuses on the study of neural-embedding models, where the representations of the involved entities and relations are learned using neural networks. Existing neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities. The main difference among these models lies in the parametrization of the relation operators. For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each relation as a bilinear tensor operator followed by a linear matrix operator. The model of TransE (Bordes et al., 2013b), on the other hand, represents each relation as a single vector that linearly interacts with the entity vectors. There has not been work that closely examines the effectiveness of different design choices. Likewise, variations on entity representations also exist. For example, NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from large text corpora. This idea is promising as pre-trained vectors tend to capture syntactic and semantic information from natural language and can assist in better generalization of entity embeddings. However, averaging word vectors may not always be appropriate, especially for entities denoted by non-compositional phrases (e.g. person names, movie names, etc.). Our work on embedding-based rule extraction presented in part of this paper is related to the earlier study on logical inference with learned continuous-space representations. Much existing work along this line focuses on learning logic-based representations for natural language sentences. For example, Socher et al. (2012) builds a neural network that recursively combines word representations based on parse tree structures and shows that such neural network can simulate the behavior of conjunction and negation.", "startOffset": 56, "endOffset": 1912}, {"referenceID": 1, "context": ", 2009), and neural networks (Paccanaro & Hinton, 2001; Bordes et al., 2013a;b; Socher et al., 2013). Our work focuses on the study of neural-embedding models, where the representations of the involved entities and relations are learned using neural networks. Existing neural embedding models (Bordes et al., 2013a;b; Socher et al., 2013) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities. The main difference among these models lies in the parametrization of the relation operators. For instance, given two entity vectors, the model of Neural Tensor Network (NTN) (Socher et al., 2013) represents each relation as a bilinear tensor operator followed by a linear matrix operator. The model of TransE (Bordes et al., 2013b), on the other hand, represents each relation as a single vector that linearly interacts with the entity vectors. There has not been work that closely examines the effectiveness of different design choices. Likewise, variations on entity representations also exist. For example, NTN (Socher et al., 2013) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from large text corpora. This idea is promising as pre-trained vectors tend to capture syntactic and semantic information from natural language and can assist in better generalization of entity embeddings. However, averaging word vectors may not always be appropriate, especially for entities denoted by non-compositional phrases (e.g. person names, movie names, etc.). Our work on embedding-based rule extraction presented in part of this paper is related to the earlier study on logical inference with learned continuous-space representations. Much existing work along this line focuses on learning logic-based representations for natural language sentences. For example, Socher et al. (2012) builds a neural network that recursively combines word representations based on parse tree structures and shows that such neural network can simulate the behavior of conjunction and negation. Bowman (2014) further demonstrates that recursive neural network can capture certain aspects of natural logical reasoning on examples involving quantifiers like some", "startOffset": 56, "endOffset": 2118}, {"referenceID": 22, "context": "Rockt\u00e4schel et al. (2014) further implements the idea by introducing a supervised objective that trains embeddings to be consistent with given logical rules.", "startOffset": 0, "endOffset": 26}, {"referenceID": 28, "context": "For example, NTN (Socher et al., 2013) represents each entity as an average of its word vectors.", "startOffset": 17, "endOffset": 38}, {"referenceID": 28, "context": "where f can be a linear or non-linear function, and W is a parameter matrix, which can be randomly initialized or initialized using pre-trained word vectors (as in (Socher et al., 2013)) or pre-trained entity vectors.", "startOffset": 164, "endOffset": 185}, {"referenceID": 1, "context": "Models Br Ar Scoring Function Distance (Bordes et al., 2011) ` QTr1  \u0301Q T r2 \u0306 \u0301||g r pye1 ,ye2q||1 Single Layer (Socher et al.", "startOffset": 39, "endOffset": 60}, {"referenceID": 28, "context": ", 2011) ` QTr1  \u0301Q T r2 \u0306 \u0301||g r pye1 ,ye2q||1 Single Layer (Socher et al., 2013) `", "startOffset": 60, "endOffset": 81}, {"referenceID": 28, "context": "\u0301p2g r pye1 ,ye2q  \u0301 2g rpye1 ,ye2q ` ||Vr||2q NTN (Socher et al., 2013) Tr ` Qr1 Q T r2 \u0306", "startOffset": 51, "endOffset": 72}, {"referenceID": 14, "context": "This general framework for relationship modeling also applies to the recent deep-structured semantic model (Huang et al., 2013; Shen et al., 2014a;b; Gao et al., 2014; Yih et al., 2014), which learns the relevance or a single relation between a pair of word sequences.", "startOffset": 107, "endOffset": 185}, {"referenceID": 10, "context": "This general framework for relationship modeling also applies to the recent deep-structured semantic model (Huang et al., 2013; Shen et al., 2014a;b; Gao et al., 2014; Yih et al., 2014), which learns the relevance or a single relation between a pair of word sequences.", "startOffset": 107, "endOffset": 185}, {"referenceID": 5, "context": "We consider Mean Reciprocal Rank (MRR) (an average of the reciprocal rank of an answered entity over all test triplets), HITS@10 (top-10 accuracy), and Mean Average Precision (MAP) (as used in (Chang et al., 2014)) as the evaluation metrics.", "startOffset": 193, "endOffset": 213}, {"referenceID": 28, "context": "We examine five embedding models in decreasing order of complexity: (1) NTN with 4 tensor slices as in (Socher et al., 2013); (2) Bilinear+Linear, NTN with 1 tensor slice and without the non-linear layer; (3) TransE, a special case of Bilinear+Linear (see Table 1); (4) Bilinear: using scoring function in Eq.", "startOffset": 103, "endOffset": 124}, {"referenceID": 14, "context": "Other objectives such as mutual information (as in (Huang et al., 2013)) and reconstruction loss (as in tensor decomposition approaches (Chang et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 5, "context": ", 2013)) and reconstruction loss (as in tensor decomposition approaches (Chang et al., 2014)) can also be applied.", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "Training was implemented using mini-batch stochastic gradient descent with AdaGrad (Duchi et al., 2011).", "startOffset": 83, "endOffset": 103}, {"referenceID": 18, "context": "We focus on DISTMULT as our baseline and compare it with the two modifications DISTMULT-tanh (using f \u201c tanh for entity projection5) and DISTMULT-tanh-EVinit (initializing the entity parameters with the 1000-dimensional pre-trained phrase vectors released by word2vec (Mikolov et al., 2013)) on FB15k-401.", "startOffset": 268, "endOffset": 290}, {"referenceID": 28, "context": "We also reimplemented the word vector representation and initialization technique introduced in (Socher et al., 2013) \u2013 each entity is represented as an average of its word vectors and the word vectors are initialized using the 300-dimensional pretrained word vectors released by word2vec.", "startOffset": 96, "endOffset": 117}, {"referenceID": 5, "context": "Inspired by (Chang et al., 2014), we design a new evaluation setting where the predicted entities are automatically filtered according to \u201centity types\u201d (entities that appear as the subjects/objects of a relation have the same type defined by that relation).", "startOffset": 12, "endOffset": 32}, {"referenceID": 23, "context": "closed-paths in the graph) by pruning and filtering rules with low statistical significance and relevance (Schoenmackers et al., 2010).", "startOffset": 106, "endOffset": 134}, {"referenceID": 9, "context": "We also compare our approaches to AMIE (Gal\u00e1rraga et al., 2013), a state-of-the-art rule mining system that can efficiently search for horn rules in large-scale KBs by using novel measurements of support and confidence.", "startOffset": 39, "endOffset": 63}, {"referenceID": 9, "context": "Gal\u00e1rraga et al. (2013) propose to automatically identify incorrect predictions based on the functional property of relations.", "startOffset": 0, "endOffset": 24}], "year": 2017, "abstractText": "We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE when evaluated on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules from the KB. We demonstrate that embeddings trained from the bilinear objective can effectively capture relation composition via matrix multiplication. We also show that our embedding-based approach can extract rules that involve relation transitivity more effectively than a state-of-the-art rule mining approach that is tailored for large-scale KBs.", "creator": "LaTeX with hyperref package"}}}