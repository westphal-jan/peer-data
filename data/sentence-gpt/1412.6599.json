{"id": "1412.6599", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2014", "title": "Hot Swapping for Online Adaptation of Optimization Hyperparameters", "abstract": "We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search (Fig. 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 20 Dec 2014 04:36:28 GMT  (2681kb,D)", "https://arxiv.org/abs/1412.6599v1", "Submission to ICLR 2015"], ["v2", "Sat, 28 Feb 2015 05:18:18 GMT  (2682kb,D)", "http://arxiv.org/abs/1412.6599v2", "Submission to ICLR 2015"], ["v3", "Mon, 13 Apr 2015 23:28:01 GMT  (2682kb,D)", "http://arxiv.org/abs/1412.6599v3", "Submission to ICLR 2015"]], "COMMENTS": "Submission to ICLR 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kevin bache", "dennis decoste", "padhraic smyth"], "accepted": true, "id": "1412.6599"}, "pdf": {"name": "1412.6599.pdf", "metadata": {"source": "CRF", "title": "OPTIMIZATION HYPERPARAMETERS", "authors": ["Kevin M. Bache"], "emails": ["kbache@ics.uci.edu", "smyth@ics.uci.edu", "ddecoste@ebay.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In this paper, we introduce a new stochastic gradient method with adaptive learning rate selection based on the insight that optimization hyperparameters may be freely \u2018hot swapped\u2019 in the middle of the learning process1.\nWhere existing adaptive learning rate algorithms are based on running curvature estimates of the local loss surface (Schaul et al., 2012; Zeiler, 2012), we present a procedure which recasts learning rate selection as an explore-exploit problem which can be addressed using existing solutions to multi-armed bandit problems. This method is straightforward to implement, retains the runtime characteristics and memory footprint of stochastic gradient, and outperforms existing methods on a common benchmark task."}, {"heading": "2 ALGORITHM", "text": "The basis of the proposed algorithm is the observation that optimization hyperparameters such as learning rate and momentum may be freely \u2018hot swapped\u2019 during optimization runs. This is in contrast to model hyperparameters, such as hidden layer size or unit type, which cannot be changed so easily during learning. This approach can also be contrasted to traditional hyperparameter search strategies such as grid search, random search, or Bayesian optimization which set optimization hyperparameters in an outer loop and treat learning as an inner loop (Bergstra and Bengio, 2012; Snoek et al., 2012).\nInstead, we propose to observe the optimization process under a variety of hyperparameter settings and to preferentially continue to use those settings which have performed best in the past. We do this\n1The notion of model-based hot swapping of algorithms or their parameters has been previously considered the study of the \u2018dynamic algorithm selection\u2019 or \u2018dynamic algorithm configuration\u2019 problems (Gagliolo and Schmidhuber, 2006), of which learning rate selection may be considered a specific instantiation. Most past work in this area has been focused on combinatorial optimization problems, where here we consider a continuous optimization problem.\nar X\niv :1\n41 2.\n65 99\nv3 [\ncs .L\nG ]\n1 3\nby maintaining a meta-model of hyperparameter performance. The general hot swapped optimization procedure is defined in algorithm 1.\nMany meta-models may work well for this task, but in this work we cast the problem of learning rate selection into an explore-exploit framework and choose a discounted upper confidence bound (DUCB) model for hyper-parameter selection. In brief, we seek an algorithm that \u2018explores\u2019 the space of possible learning rates\u2014in order to learn which ones perform best on the given problem\u2014while reserving most of its time to \u2018exploit\u2019 the best performing rates\u2014by repeatedly using them to update model parameters. The upper confidence bound algorithm is a common choice for tackling exploreexploit problems, and its discounted form achieves the optimal regret bound up to a logaritmic factor for rapidly shifting reward distributions (Garivier and Moulines, 2008).\nThe procedure for hot swapped optimization with a DUCB model is listed in algorithm 2 with the full details given in algorithm 3. We assume that we have a finite set of learning rates to select from, ~\u03b1 \u2261 {\u03b11, ..., \u03b1K}, a postive objective function to be minimized f(~\u03b8;B), along with its gradient g(~\u03b8;B), both of which can be evaluated at a point in parameter space ~\u03b8 for a given data batch B.\nWe define the \u2018reward\u2019 granted to a given learning rate \u03b1k as r = log(f(~\u03b80;B)) \u2212 log(f(~\u03b8k;B)), where ~\u03b80 represents the (non-hyper) parameters at the beggining of the current iteration and ~\u03b8k \u2261 ~\u03b80\u2212\u03b1kg(~\u03b80, B) represents the parameters obtained by choosing learing rate \u03b1k. We use a logarithmic scaling of the rewards which treats multiplicative reductions of the objective function f as equally valuable, a useful feature given the exponential slowdown of optimization progress which is often observed in practice. The \u03b1 value chosen at each step is selected by the DUCB algorithm in the usual way (see the function GETDUCBSUGGESTEDINDEX in algorithm 3 for details).\nThe DUCB model will periodically seek to explore different learning rates as the optimization procedure progresses. This introduces the potential to take catastrophically large steps which could discard progress made up to the current time. To prevent this, we perform a line search across learning rates to find the best learning rate value for each minibatch. The line search starts from the learning rate proposed by the DUCB algorithm and decreases through the other available learning rates until it finds one which lowers the current minibatch\u2019s objective function value below the value it held before the current update. Because the line search is only performed on the current minibatch of data, it still takes linear time in batch size and problem dimension, just like simple SGD2.\nIn practice, these two processes work well together. The line search prevents the bandit algorithm\u2019s tendency to explore catastrophically large step sizes, while the bandit algorithm\u2019s carefully chosen initial step sizes reduce the number of minibatch objective function evaluations from the large number required by a vanilla minibatch line search to a much smaller quantity."}, {"heading": "3 INITIAL RESULTS", "text": "We test the efficacy of this procedure3 on a neural network based on the MNIST dataset. MNIST is comprised of 60,000 28x28 pixel black and white images of handwritten digits. The task is to classify each image as a number \u20180\u2019 through \u20189\u2019.\nWe show results from fully connected feed-forward network with 500 sigmoidal units in the first hidden layer, 300 sigmoidal hidden units in the second hidden layer, and a final 10-way softmax output. We trained on the first 50,000 images of the training set.\nWe perform an exhaustive search of SGD hyperparameters for comparison. We consider all combinations of the following parameters: initial learning rates in \u03b10 \u2208 {1.0, 0.3, 0.1, 0.03, 0.01, 0.003}, per-epoch learning rate multipliers of \u03b7 \u2208 {0.99, 0.995, 1.0}, momentum coefficients \u00b5 \u2208 {0.0, 0.5, 0.7, 0.9} and batch sizes \u2208 {64, 128, 256, 512, 1024} for a total of 360 SGD settings.\n2Minibatch line search is not without precedent (Ngiam et al., 2011; Roux et al., 2012), though it has received little direct attention in the past\n3This experiment was conducted using Theano, PyLearn2, and a cluster of computers with NVIDIA GRID K520 GPUs\nWe also compare against AdaDelta, another widely used adaptive learning rate algorithm, using the hyperparameters described in Zeiler (2012) for MNIST: = 10\u22126 and decay rate factor of 0.95 across batch sizes \u2208 {64, 128, 256, 512, 1024}. Finally, we tested hot swapped optimization with a DUCB step size model and batch sizes \u2208 {64, 128, 256, 512, 1024}4. We ran each algorithm for 500 epochs using three random weight initializations. Figure 1 shows the spread of training and test performances of all 380 algorithms after 200 and 500 training epochs. Each dot represents the median performance of a single set of hyperparameters with error bars indicating min and max performance across initializations. The best performing algorithms will be in the bottom left. At convergence, all 15 hot swap DUCB algorithms (red) achieve the lowest training objective (negative log likelihood) of every other algorithm tested (green and blue)\nFurthermore, while test performance isn\u2019t the direct target of an optimization process\u2014test performance is also heavily linked to regularization quality, a factor to which optimization algorithms are agnostic\u2014the DUCB algorithm obtained the lowest median test error rate across all of its variations (1.85% vs 1.97% for AdaDelta and 2.35% for SGD) and the best single test performance overall (1.63% error rate), despite the fact that SGD instantiations outnumbered hot swapped DUCB instantiations by a factor of 75:1.\nThe performance of SGD algorithms vary widely across hyperparameter settings. The AdaDelta algorithms consistently exhibit performance in the top 30% of the SGD algorithms, through as with hot swapped DUCB, they vary across batch size.\nFigure 1 shows the variations in performance of different algorithms after a fixed number of training epochs, but obscures the differences in the time it takes each algorithm to complete one training epoch. Specifically, algorithms with small batch sizes are slower because they need to perform more updates per epoch than algorithms with large batch sizes, and the DUCB algorithms are slower per iteration than the competing algorithms (see below for details).\nFigure 2 gives a direct comparison of training time vs. various measures of performance for one instantiation each of the five DUCB hot swapping algorithms (one for each batch size), the five\n4It\u2019s worth noting that batch size is the one significant hyperparameter for the hot swapping DUCB algorithm. It is similar in this regard to other adaptive learning rate methods such as AdaDelta and No More Pesky Learning Rates.\nAdaDelta algorithms, and the two best-performing SGD algorithms (chosen retrospectivlye out of a total of 1080 SGD algorithms; \u2018best performing\u2019 measured in final training objective and test misclassification).\nThe top plot in figure 2 shows test error rate vs. wall clock time. Despite the fact that DUCB hot swapping with a batch size of 64 takes the most time per iteration, it makes sufficient progress in each step that by 1000 seconds into the training run it has attained the best test error of any algorithm we tested5. The second plot shows training objective over time (training set negative log likelihood), with all of the DUCB algorithms making significant training progress well after the SGD and AdaDelta algorithms have plateaued and finding lower optima. The third plot suggests that the DUCB algorithms naturally learn to decrease their learning rates as they near local optima6. The fourth plot shows a running average of gradient norms over time, with the DUCB algorithms all finding flatter optima than competing algorithms.\nOn other problems in which we have tested the hot swapping DUCB procedure, we have observed similarly strong training performance with greater variation on test performance that we observed here. This suggests that the hot swapped DUCB procedure is a strong optimization algorithm that requires similarly strong regularization not to overfit.\n5A single iteration of DUCB hot swapping takes between 1.6 and 3.6 times as long as a comparable iteration of SGD or AdaDelta because it performs several line search iterations per step. However, it makes considerably greater progress per iteration than SGD and continues to do so well after SGD and AdaDelta have plateaued, converging to better optima than the existing algorithms. For additional details on the relative timing of each method, see Appendix 1: Timing.\n6The AdaDelta algorithms are not included in the learning rate plots because AdaDelta maintains a different learning rate for each paramter."}, {"heading": "4 CONCLUSION", "text": "We have introduced a new adaptive learning rate algorithm for stochastic gradient that is built to hot swap an optimization hyperparameter over the course of a learning run. Preliminary results indicate that the proposed method consistently outperforms competing methods on several measures.\nNumerous extensions of this basic procedure are possible, including using different meta-models, swapping new optimization or regularization hyper parameters, swapping multiple parameters at once, and reducing the frequency of line search to speed performance. We are currently working to test this and other hot swapping procedures on a wide variety of problems."}, {"heading": "ACKNOWLEDGMENTS", "text": "This material is based upon work partially supported by a National Science Foundation Graduate Fellowship (KB), by NSF via award number IIS-1320527 (PS), and by the Office of Naval Research under MURI grant N00014-08-1-1015 (PS). A portion of this work was performed by KB while a summer intern at eBay Research Labs."}], "references": [{"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research, 13(1):281\u2013305.", "citeRegEx": "Bergstra and Bengio,? 2012", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Learning dynamic algorithm portfolios", "author": ["M. Gagliolo", "J. Schmidhuber"], "venue": "Annals of Mathematics and Artificial Intelligence, 47(3-4):295\u2013328.", "citeRegEx": "Gagliolo and Schmidhuber,? 2006", "shortCiteRegEx": "Gagliolo and Schmidhuber", "year": 2006}, {"title": "On upper-confidence bound policies for non-stationary bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "arXiv:0805.3415 [math, stat]. arXiv: 0805.3415.", "citeRegEx": "Garivier and Moulines,? 2008", "shortCiteRegEx": "Garivier and Moulines", "year": 2008}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 265\u2013272.", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems, pages 2663\u20132671. 00059.", "citeRegEx": "Roux et al\\.,? 2012", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "No more pesky learning rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1206.1106.", "citeRegEx": "Schaul et al\\.,? 2012", "shortCiteRegEx": "Schaul et al\\.", "year": 2012}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 25, pages 2951\u20132959. Curran Associates, Inc.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler,? 2012", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Where existing adaptive learning rate algorithms are based on running curvature estimates of the local loss surface (Schaul et al., 2012; Zeiler, 2012), we present a procedure which recasts learning rate selection as an explore-exploit problem which can be addressed using existing solutions to multi-armed bandit problems.", "startOffset": 116, "endOffset": 151}, {"referenceID": 7, "context": "Where existing adaptive learning rate algorithms are based on running curvature estimates of the local loss surface (Schaul et al., 2012; Zeiler, 2012), we present a procedure which recasts learning rate selection as an explore-exploit problem which can be addressed using existing solutions to multi-armed bandit problems.", "startOffset": 116, "endOffset": 151}, {"referenceID": 0, "context": "This approach can also be contrasted to traditional hyperparameter search strategies such as grid search, random search, or Bayesian optimization which set optimization hyperparameters in an outer loop and treat learning as an inner loop (Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 238, "endOffset": 285}, {"referenceID": 6, "context": "This approach can also be contrasted to traditional hyperparameter search strategies such as grid search, random search, or Bayesian optimization which set optimization hyperparameters in an outer loop and treat learning as an inner loop (Bergstra and Bengio, 2012; Snoek et al., 2012).", "startOffset": 238, "endOffset": 285}, {"referenceID": 1, "context": "We do this The notion of model-based hot swapping of algorithms or their parameters has been previously considered the study of the \u2018dynamic algorithm selection\u2019 or \u2018dynamic algorithm configuration\u2019 problems (Gagliolo and Schmidhuber, 2006), of which learning rate selection may be considered a specific instantiation.", "startOffset": 208, "endOffset": 240}, {"referenceID": 2, "context": "The upper confidence bound algorithm is a common choice for tackling exploreexploit problems, and its discounted form achieves the optimal regret bound up to a logaritmic factor for rapidly shifting reward distributions (Garivier and Moulines, 2008).", "startOffset": 220, "endOffset": 249}, {"referenceID": 3, "context": "Minibatch line search is not without precedent (Ngiam et al., 2011; Roux et al., 2012), though it has received little direct attention in the past This experiment was conducted using Theano, PyLearn2, and a cluster of computers with NVIDIA GRID K520 GPUs", "startOffset": 47, "endOffset": 86}, {"referenceID": 4, "context": "Minibatch line search is not without precedent (Ngiam et al., 2011; Roux et al., 2012), though it has received little direct attention in the past This experiment was conducted using Theano, PyLearn2, and a cluster of computers with NVIDIA GRID K520 GPUs", "startOffset": 47, "endOffset": 86}, {"referenceID": 7, "context": "We also compare against AdaDelta, another widely used adaptive learning rate algorithm, using the hyperparameters described in Zeiler (2012) for MNIST: = 10\u22126 and decay rate factor of 0.", "startOffset": 127, "endOffset": 141}], "year": 2015, "abstractText": "We describe a general framework for online adaptation of optimization hyperparameters by \u2018hot swapping\u2019 their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.", "creator": "LaTeX with hyperref package"}}}