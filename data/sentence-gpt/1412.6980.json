{"id": "1412.6980", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Adam: A Method for Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based an adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also ap- propriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice when experimentally compared to other stochastic optimization methods.\n\n\n\n\nA general theory of convergence is found in the computational dynamics of the object-state. The theory is the underlying concept that the objects of constant time have an infinite amount of energy; that the object's energy can be set by increasing the energy by increasing the length of time to the maximum. This is not only a problem but also a problem of differential distribution.\nThe concept of convergence is found in the computational dynamics of the object-state. The concept of convergence is found in the computational dynamics of the object-state. The theory is the underlying concept that the objects of constant time have an infinite amount of energy; that the object's energy can be set by increasing the energy by increasing the length of time to the maximum. This is not only a problem but also a problem of differential distribution. The theory is a unified concept that the objects of constant time have an infinite amount of energy; that the object's energy can be set by increasing the energy by increasing the length of time to the maximum. This is not only a problem but also a problem of differential distribution.\nThe theory of convergence is found in the computational dynamics of the object-state. The theory is the underlying concept that the objects of constant time have an infinite amount of energy; that the object's energy can be set by increasing the energy by increasing the length of time to the maximum. This is not only a problem but also a problem of differential distribution. A general theory of convergence is found in the computational dynamics of the object-", "histories": [["v1", "Mon, 22 Dec 2014 13:54:29 GMT  (280kb,D)", "http://arxiv.org/abs/1412.6980v1", "initial"], ["v2", "Sat, 17 Jan 2015 20:26:06 GMT  (283kb,D)", "http://arxiv.org/abs/1412.6980v2", "initial"], ["v3", "Fri, 27 Feb 2015 21:04:48 GMT  (289kb,D)", "http://arxiv.org/abs/1412.6980v3", "revision2"], ["v4", "Tue, 3 Mar 2015 17:51:27 GMT  (289kb,D)", "http://arxiv.org/abs/1412.6980v4", null], ["v5", "Thu, 23 Apr 2015 16:46:07 GMT  (289kb,D)", "http://arxiv.org/abs/1412.6980v5", null], ["v6", "Tue, 23 Jun 2015 19:57:17 GMT  (958kb,D)", "http://arxiv.org/abs/1412.6980v6", null], ["v7", "Mon, 20 Jul 2015 09:43:23 GMT  (519kb,D)", "http://arxiv.org/abs/1412.6980v7", "Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015"], ["v8", "Thu, 23 Jul 2015 20:27:47 GMT  (526kb,D)", "http://arxiv.org/abs/1412.6980v8", "Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015"], ["v9", "Mon, 30 Jan 2017 01:27:54 GMT  (490kb,D)", "http://arxiv.org/abs/1412.6980v9", "Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015"]], "COMMENTS": "initial", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["diederik p kingma", "jimmy ba"], "accepted": true, "id": "1412.6980"}, "pdf": {"name": "1412.6980.pdf", "metadata": {"source": "CRF", "title": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION", "authors": ["Diederik P. Kingma", "Jimmy Lei Ba"], "emails": ["dpkingma@uva.nl", "jimmy@psi.utoronti.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "Stochastic gradient-based optimization is of core practical importance in many fields of science and engineering. Many problems in these fields can be cast as the optimization of some scalar parameterized objective function requiring maximization or minimization with respect to its parameters. If the function is differentiable w.r.t. its parameters, a relatively efficient method of optimization is gradient ascent, since the computation of first-order partial derivatives w.r.t. all the parameters is of the same computational complexity as just evaluating the function. Often, objective functions are stochastic. For example, many objective function are composed of a sum of subfunctions evaluated at different subsamples of data; in this case optimization can be made more efficient by taking gradient steps w.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al. (2012b) regularization. For all such noisy objectives, efficient stochastic optimization techniques are required. The focus of this paper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In these cases, higher-order optimization methods are ill-suited, and discussion in this paper will be restricted to first-order methods.\nWe propose Adam, a method for efficient stochastic optimization that only requires first-order gradients and requires little memory. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. Our method is designed to combine the advantages of two recently popular methods: AdaGrad Duchi et al. (2011), which works well with sparse gradients, and RMSProp Tieleman & Hinton (2012), which works well in on-line and non-stationary settings; important connections to these and other stochastic optimization methods are clarified in section 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to rescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter, it does not require a stationary objective, it works with sparse gradients, and it performs a form of automatic annealing.\n\u2217Equal contribution. Author ordering determined by coin flip over a Google Hangout.\nar X\niv :1\n41 2.\n69 80\nv1 [\ncs .L\nG ]\n2 2\nD ec\n2 01\n4\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details, and for a slightly more efficient (but less clear) order of computation. g2t indicates the elementwise square gt gt. Good default settings for the machine learning problems in our experiments were \u03b1 = 0.0002, \u03b21 = 0.1, \u03b22 = 0.001 and = 10\u22128. Require: \u03b1: Stepsize Require: \u03b21, \u03b22 \u2208 (0, 1]: Exponential decay rates for the first and second moment estimates Require: f(\u03b8): Stochastic objective function with parameters \u03b8 Require: \u03b80: Initial parameter vector m0 \u2190 0 (Initialize initial 1st moment vector) v0 \u2190 0 (Initialize initial 2nd moment vector) t\u2190 0 (Initialize timestep) while \u03b8t not converged do t\u2190 t+ 1 gt \u2190 \u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t) mt \u2190 \u03b21 \u00b7 gt + (1\u2212 \u03b21) \u00b7mt\u22121 (Update biased first moment estimate) vt \u2190 \u03b22 \u00b7 g2t + (1\u2212 \u03b22) \u00b7 vt\u22121 (Update biased second raw moment estimate) m\u0302t \u2190 mt/(1\u2212 (1\u2212 \u03b21)t) (Compute bias-corrected first moment estimate) v\u0302t \u2190 vt/(1\u2212 (1\u2212 \u03b22)t) (Compute bias-corrected second raw moment estimate) \u03b8t \u2190 \u03b8t\u22121 \u2212 \u03b1 \u00b7 m\u0302t/( \u221a v\u0302t + ) (Update parameters)\nend while return \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and properties of its update rule. Section 3 explains our initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s convergence in online convex programming. Empirically, our method consistently outperforms other methods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is a versatile algorithm that scales to large-scale high-dimensional machine learning problems."}, {"heading": "2 ALGORITHM", "text": "See algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objective function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are interested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With f1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps 1, ..., T . The stochasticity might e.g. come from the evaluation at random subsamples (minibatches) of datapoints, or might arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e. the vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient (vt) where the hyper-parameters \u03b21 \u2208 (0, 1] and \u03b22 \u2208 (0, 1] set the exponential decay rates of these moving averages. The moving averages themselves are estimates of the 1st moment (the mean) and the 2nd raw moments (the uncentered variance) of the gradient. However, these moving averages are initialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially during the initial timesteps, and especially when the decay rates \u03b2 are small. The good news is that this initialization bias can be easily counteracted, resulting in bias-corrected estimates m\u0302t and v\u0302t. See section 3 for more details.\nNote that the efficiency of algorithm 1 can, at the expense of clarity, be improved upon by changing the order of computation, e.g. by replacing the last three lines in the loop with the following line: \u03b8t \u2190 \u03b8t\u22121 \u2212 (\u03b1 \u00b7 \u221a 1\u2212 (1\u2212 \u03b22)t \u00b7 (1\u2212 (1\u2212 \u03b21)t)\u22121) \u00b7mt/ \u221a vt."}, {"heading": "2.1 ADAM\u2019S UPDATE RULE", "text": "An important property of Adam\u2019s update rule is its careful choice of stepsizes. The effective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 m\u0302t/ \u221a v\u0302t. The effective stepsize has strong upper and lower bounds: \u2212\u03b1 \u00b7 \u03b21/\u03b22 \u2264 \u2206t \u2264 +\u03b1 \u00b7 \u03b21/\u03b22. The extremities \u00b1 \u00b7 \u03b1 \u00b7 \u03b21/\u03b22 happen only in the most severe case of sparsity: when a gradient has been zero at all timesteps except at the\ncurrent timestep. For less sparse cases, the effective stepsize will be smaller. When \u03b21 = \u03b22 we have that |m\u0302t/ \u221a v\u0302t| < 1 therefore \u2212\u03b1 < \u2206t < \u03b1. In more common scenarios with reasonable sparsity, and reasonably set stepsize \u03b1 and decay rates \u03b2 such that we have reasonable moment estimates, we will have that m\u0302t/ \u221a v\u0302t \u2248 \u00b11 since E[g]/E[g2] = \u00b11. In such situations of reasonable moment estimates, the effective magnitude of the steps taken in parameter space at each timestep are approximately bounded by the stepsize setting \u03b1, i.e., \u2212\u03b1 / \u2206t / +\u03b1. This can be understood as establishing a trust region around the current parameter value, beyond which the current gradient estimate does not provide sufficient information. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For many machine learning models, for instance, we often know in advance that good optima are with high probability within some set region in parameter space; it is not uncommon, for example, to do MAP optimization with a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of steps in parameter space, we can often deduct the right order of magnitude of \u03b1 such that optima can be reached from \u03b80 within some number of iterations. With a slight abuse of terminology, we will call the ratio m\u0302t/ \u221a v\u0302t the signal-to-noise ratio (SNR). With a smaller SNR the effective stepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that there is greater uncertainty about whether the direction of m\u0302t corresponds to the direction of the true gradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading to smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize \u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale m\u0302t with a factor c and v\u0302t with a factor c2, which cancel out: (c \u00b7 m\u0302t)/( \u221a c2 \u00b7 v\u0302t) = m\u0302t/ \u221a v\u0302t."}, {"heading": "3 INITIALIZATION BIAS CORRECTION", "text": "As explained in section 2, Adam utilizes initialization bias correction terms. We will here derive the term for the second moment estimate; the derivation for the first moment is completely analogous. Let g be the gradient of the stochastic objective f , and we wish to estimate its second raw moment (uncentered variance) using an exponential moving average of the squared gradient, with decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an underlying gradient distribution gt \u223c p(gt). Let us initialize the exponential moving average as v0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average vt = \u03b22 \u00b7 g2t + (1 \u2212 \u03b22) \u00b7 vt\u22121 (where g2t indicates the elementwise square gt gt) can be written as a function of the gradients at all previous timesteps:\nvt = \u03b22 t\u2211 i=1 (1\u2212 \u03b22)t\u2212i \u00b7 g2i (1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t, relates to the true second moment E[g2t ], so we can correct for the discrepancy between the two. Taking expectations of the left-hand and right-hand sides of eq. (1):\nE[vt] = E [ \u03b22\nt\u2211 i=1 (1\u2212 \u03b22)t\u2212i \u00b7 g2i )\n] (2)\n= E[g2t ] \u00b7 \u03b22 t\u2211 i=1 (1\u2212 \u03b22)t\u2212i + \u03b6 (3)\n= E[g2t ] \u00b7 (1\u2212 (1\u2212 \u03b22)t) + \u03b6 (4)\nwhere \u03b6 = 0 if the true second moment E[g2i ] is stationary; otherwise \u03b6 can be kept small since the exponential decay rate \u03b21 can (and should) be chosen such that the exponential moving average assigns small weights to gradients too far in the past. What is left is the term (1\u2212 (1\u2212 \u03b22)t) which is caused by initializing the running average with zeros. In algorithm 1 we therefore divide by this term to correct the initialization bias.\nIn case of sparse gradients, for a reliable estimate of the second moment one needs to average over many gradients by chosing a small value of \u03b22; however it is exactly this case of small \u03b22 where a lack of initialisation bias correction would lead to initial steps that are much larger."}, {"heading": "4 CONVERGENCE ANALYSIS", "text": "We analyze the convergence of Adam under the online learning framework proposed in Zinkevich (2003). We are given an arbitrary, unknown sequence of convex cost functions f1, f2,..., fT . At each time t, we need to make a prediction for the parameter \u03b8t and evaluate on a previously unknown cost function ft. Since we do not know the nature of the sequence in advance, we evaluate our algorithm using regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t) and the best fixed point parameter ft(\u03b8\u2217) for all the previous steps. Concretely, we have the regret is defined as:\nR(T ) = T\u2211 t=1 [ft(\u03b8t)\u2212 ft(\u03b8\u2217)] (5)\n\u03b8\u2217 = arg min \u03b8\u2208X T\u2211 t=1 ft(\u03b8) (6)\nWe give a convergence proof and a regret O( \u221a T ) for the online convex function using the Adam algorithm. Our result is comparable to the best known bound for this general convex online learning problem. Theorem 4.1. Assume that the functions ft have bounded gradients, \u2016\u2207ft(\u03b8)\u20162 \u2264 G, \u2016\u2207ft(\u03b8)\u2016\u221e \u2264 G\u221e for all \u03b8 \u2208 Rd and distance between any \u03b8t generated by Adam is bounded, \u2016\u03b8n \u2212 \u03b8m\u20162 \u2264 D, \u2016\u03b8m \u2212 \u03b8n\u2016\u221e \u2264 D\u221e for any m,n \u2208 {1, ..., T}. With \u03b3t = 1/t, Adam achieves the following guarantee, for all T \u2265 1.\nR(T ) \u2264 (D 2\n2\u03b7 + \u03b7) d\u2211 i=1 \u2016g1:T,i\u20162 + 2D2 1 + \u03b7 \u03b7 log(T ) + 3 2 d\u03b7D2\u221eG\u221e\nSimilarly to AdaGrad, when the data features are sparse, the summation term can be much smaller than its upper bound \u2211d i=1 \u2016g1:T,i\u20162 \u2264 dG\u221e \u221a T , in particular if the class of function and data\nfeatures are in the form of section 1.2 in Duchi et al. (2011). Their results of the expected value of E \u2211d i=1 \u2016g1:T,i\u20162 also apply to Adam. The adaptive method can achieve O(log d \u221a T ), an improve-\nment over O( \u221a dT ) for the non-adaptive method.\nFinally, we can show the average regret of Adam converges, Corollary 4.2. Assume that the function ft has bounded gradients, \u2016\u2207ft(\u03b8)\u20162 \u2264 G, \u2016\u2207ft(\u03b8)\u2016\u221e \u2264 G\u221e for all \u03b8 \u2208 Rd and distance between any \u03b8t generated by Adam is bounded, \u2016\u03b8n \u2212 \u03b8m\u20162 \u2264 D, \u2016\u03b8m \u2212 \u03b8n\u2016\u221e \u2264 D\u221e for any m,n \u2208 {1, ..., T}. With \u03b3t = 1/t, Adam achieves the following guarantee, for all T \u2265 1.\nR(T )\nT = O( 1\u221a T )\nThis result can be obtained by using Theorem 4.1 and \u2211d i=1 \u2016g1:T,i\u20162 \u2264 dG\u221e \u221a T . Thus, limT\u2192\u221e R(T ) T = 0."}, {"heading": "5 RELATED WORK", "text": "Optimization methods bearing a direct relation to Adam include RProp Riedmiller & Braun (1992), RMSProp Tieleman & Hinton (2012); Graves et al. (2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information. The Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasiNewton method based on minibatches, but (unlike Adam) has memory requirements linear in the number of minibatches of data, which is often more than available on memory-constrained systems such as a GPU. Like natural gradient descent (NGD) Amari (1998), Adam employs a preconditioner that adapts to the geometry of the data, since v\u0302t is an approximation to the diagonal of the Fisher information matrix Pascanu & Bengio (2013); however, Adam\u2019s preconditioner (like AdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square root of the inverse of the diagonal Fisher information matrix approximation.\nRProp: The Rprop method Riedmiller & Braun (1992) is a robust algorithm for gradient-based optimization of non-stochastic objectives. In its basic form, Rprop takes steps proportional to only the sign of the gradient: \u03b8t+1 = \u03b8t\u2212\u03b1 \u00b7 sign(gt). Rprop can be retrieved as a special case of Adam where \u03b21 = 1 and \u03b22 = 1, i.e. the case with zero memory. In this case Adam\u2019s bias correction terms equal 1, and the update is: \u03b8t+1 = \u03b8t \u2212 \u03b1 \u00b7mt/ \u221a vt = \u03b8t \u2212 \u03b1 \u00b7 gt/ \u221a g2t = \u03b8 \u2212 \u03b1 \u00b7 sign(gt). In other settings, Adam still bears similarities to RProp since its effective stepsizes are approximately bounded by \u03b1 as explained in section 2.\nRMSProp: An optimization method close to Adam is RMSProp Tieleman & Hinton (2012). A version with momentum has sometimes been used Graves et al. (2013). RMSProp lacks a biascorrection term; this matters most in case of a small value \u03b22 (required in case of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and often divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad: An algorithm that works well for sparse gradients is AdaGrad Duchi et al. (2011). Its basic version updates parameters as \u03b8t+1 = \u03b8t \u2212 \u03b1 \u00b7 gt/ \u221a\u2211t i=1 g 2 t . Note that if we choose an infinitesimal \u03b22 then lim\u03b22\u21920 v\u0302t = t \u22121 \u00b7 \u2211t i=1 gt. AdaGrad corresponds to a version of Adam with \u03b21 = 1, infinitesimal \u03b22 and a replacement of \u03b1 by an annealed version \u03b1t = \u03b1 \u00b7 t\u22121/2, namely \u03b8t \u2212 \u03b1 \u00b7 t\u22121/2 \u00b7 m\u0302t/ \u221a lim\u03b22\u21920 v\u0302t = \u03b8t \u2212 \u03b1 \u00b7 t\u22121/2 \u00b7 gt/ \u221a t\u22121 \u00b7 \u2211t i=1 gt = \u03b8t \u2212 \u03b1 \u00b7 gt/ \u221a\u2211t i=1 g 2 t . Note that this direct correspondence between Adam and Adagrad does not hold when removing the bias-correction terms; without bias correction, like in RMSProp, an infinitesimal \u03b22 would lead to infinitely large bias, and infinitely large parameter updates."}, {"heading": "6 EXPERIMENTS", "text": "To empirically evaluate the proposed method, we investigated different popular machine learning models, including logistic regression, multilayer fully connected neural networks and deep convolutional neural networks. Using large models and datasets, we demonstrate how well Adam can solve practical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The hyper-parameters, such as learning rate and momentum, are searched over a dense grid and the results are reported using the best hyper-parameter setting."}, {"heading": "6.1 EXPERIMENT: LOGISTIC REGRESSION", "text": "We evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST dataset. Logistic regression has a well-studied convex objective, making it suitable for comparison of different optimizers without worrying about local minimum issues. The logistic regression classifies the class label directly on the 784 dimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and AdaGrad using mini-batch size of 128. According to Figure 1, we found that the Adam yields similar convergence as SGD with momentum and both converge faster than AdaGrad.\nAs discussed in Duchi et al. (2011), AdaGrad can efficiently deal with sparse features and gradients as one of its main theoretical results whereas SGD is low at learning rare features. We examine the sparse feature problem using IMDB movie review dataset from Maas et al. (2011). We pre-process the IMDB movie reviews into bag-of-words (BoW) feature vectors including the first 10,000 most frequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As suggested in Wang & Manning (2013), 50% dropout noise can be applied to the BoW features during training to prevent over-fitting. In figure 1, AdaGrad outperforms SGD with Nesterov momentum by a large margin both with and without dropout noise. Adam converges as fast as AdaGrad. The empirical performance of Adam is consistent with our theoretical findings in sections 2 and 4. Similar to AdaGrad, Adam can take advantage of sparse features and obtain faster convergence rate than normal SGD with momentum."}, {"heading": "6.2 EXPERIMENT: MULTI-LAYER NEURAL NETWORKS", "text": "Multi-layer neural network are powerful models with non-convex objective functions. Although our convergence analysis does not apply to non-convex problems, we empirically found that Adam often outperforms other methods in such cases. In our experiments, we made model choices that are consistent with previous publications in the area; a neural network model with two fully connected hidden layers with 1000 hidden units each and ReLU activation are used for this experiment with mini-batch size of 128.\nFirst, we study different optimizers using the standard deterministic cross-entropy objective function with L2 weight decay on the parameters to prevent over-fitting. The sum-of-functions (SFO) methodSohl-Dickstein et al. (2014) is a recently proposed quasi-Newton method that works with minibatches of data and has shown good performance on optimization of multi-layer neural networks. We used their implementation and compared with Adam to train such models. Figure 2 shows that Adam makes faster progress in terms of both the number of iterations and wall-clock time. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration compared to Adam, and has a memory requirement that is linear in the number minibatches.\nStochastic regularization methods, such as dropout, are an effective way to prevent over-fitting and often used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed failed to converge on cost functions with stochastic regularization. We compare the effectiveness of Adam to other stochastic first order methods on multi-layer neural networks trained with dropout noise. Figure 2 shows our results; Adam shows better convergence than other methods."}, {"heading": "6.3 EXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS", "text": "Convolutional neural networks (CNNs) with several layers of convolutional, pooling and non-linear units have shown considerable success in computer vision tasks. Unlike most fully connection neural nets, weight sharing in CNNs result in vastly different gradients in different layers. A smaller learning rate for the convolution layers is often used in practice when applying SGD. We show the effectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5 convolution filters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer of 1000 rectified linear hidden units (ReLU\u2019s). The input image are pre-processed by whitening, and dropout noise is applied to the input layer and fully connected layer.\nInterestingly, although both Adam and AdaGrad have lower cost in the initial stage of the training in Figure 3 (left), Adam and SGD eventually converge considerably faster than AdaGrad for CNNs shown in Figure 3 (right). We notice the second moment estimate vanishes to zeros after a few\nepoches and is dominated by the in algorithm 1. The second moment estimate is therefore a poor approximation to the geometry of the cost function in CNNs comparing to fully connected network from Section 6.2. Whereas, reducing the mini-batch variance through the first moment is more important in CNNs and contributes to the speed-up. Though Adam shows marginal improvement over SGD with momentum, it adapts learning rate scale for different layers instead of hand picking manually as in SGD."}, {"heading": "6.4 EXPERIMENT: BIAS-CORRECTION TERM", "text": "We also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3. Discussed in section 5, removal of the bias correction terms results in a version of RMSProp Tieleman & Hinton (2012) with momentum. We vary the \u03b21 and \u03b22 when training a variational autoencoder (VAE) with the same architecture as in Kingma & Welling (2013) with a single hidden layer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. \u03b21 \u2208 (1, 0.1) and \u03b22 \u2208 (0.01, 0.001, 0.001), and log10(\u03b1) \u2208 [\u22125, ...,\u22121]. Smaller values of \u03b22, required for robustness to sparse gradients, results in larger initialization bias; therefore we expect a larger adverse effect on optimization for smaller values of \u03b22 when this bias is not corrected for.\nSee figure 4 for results. As expected, small values of \u03b22 lead to instabilities in training when no bias correction term was present, especially early on. The best results were achieved with small values of \u03b22 and bias correction; this was more apparent towards the end of optimization when gradients tends to become sparser as hidden units specialize to specific patterns. In summary, Adam performed equal or better than RMSProp, regardless of hyper-parameter setting."}, {"heading": "7 CONCLUSION", "text": "We have introduced a simple and computationally efficient algorithm for gradient-based optimization of stochastic objective functions. Our method is aimed towards machine learning problems with large datasets and/or high-dimensional parameter spaces. The method combines the advantages of two recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients, and the ability of RMSProp to deal with non-stationary objectives. The method is straightforward to implement and requires little memory. The experiments confirm the analysis on the rate of convergence in convex problems. Empirically, we found Adam to be robust and well-suited to wide range of non-convex optimization problems in the field machine learning."}, {"heading": "8 ACKNOWLEDGMENTS", "text": "This paper would probably not have existed without the support of Google Deepmind, the collaborations it supports and interesting conversations they sparked. We would like to give special thanks to Tom Schaul for coining the name Adam."}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural computation,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "Recent advances in deep learning for speech research at microsoft", "author": ["Deng", "Li", "Jinyu", "Huang", "Jui-Ting", "Yao", "Kaisheng", "Yu", "Dong", "Seide", "Frank", "Seltzer", "Michael", "Zweig", "Geoff", "He", "Xiaodong", "Williams", "Jason"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Hinton", "Geoffrey", "Deng", "Li", "Yu", "Dong", "Dahl", "George E", "Mohamed", "Abdel-rahman", "Jaitly", "Navdeep", "Senior", "Andrew", "Vanhoucke", "Vincent", "Nguyen", "Patrick", "Sainath", "Tara N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Auto-Encoding Variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In The 2nd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas", "Andrew L", "Daly", "Raymond E", "Pham", "Peter T", "Huang", "Dan", "Ng", "Andrew Y", "Potts", "Christopher"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Revisiting natural gradient for deep networks", "author": ["Pascanu", "Razvan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1301.3584,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Rprop - a fast adaptive learning algorithm", "author": ["Riedmiller", "Martin", "Braun", "Heinrich"], "venue": "Technical report, Proc. of ISCIS VII), Universitat,", "citeRegEx": "Riedmiller et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 1992}, {"title": "No more pesky learning rates", "author": ["Schaul", "Tom", "Zhang", "Sixin", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1206.1106,", "citeRegEx": "Schaul et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2012}, {"title": "Fast large-scale optimization by unifying stochastic gradient and quasi-newton methods", "author": ["Sohl-Dickstein", "Jascha", "Poole", "Ben", "Ganguli", "Surya"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2014}, {"title": "Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning", "author": ["T. Tieleman", "G. Hinton"], "venue": "Technical report,", "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Fast dropout training", "author": ["Wang", "Sida", "Manning", "Christopher"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": null, "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.", "startOffset": 168, "endOffset": 187}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al.", "startOffset": 168, "endOffset": 237}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al.", "startOffset": 168, "endOffset": 260}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al.", "startOffset": 168, "endOffset": 282}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al. (2012b) regularization.", "startOffset": 168, "endOffset": 392}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al. (2012b) regularization. For all such noisy objectives, efficient stochastic optimization techniques are required. The focus of this paper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In these cases, higher-order optimization methods are ill-suited, and discussion in this paper will be restricted to first-order methods. We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients and requires little memory. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. Our method is designed to combine the advantages of two recently popular methods: AdaGrad Duchi et al. (2011), which works well with sparse gradients, and RMSProp Tieleman & Hinton (2012), which works well in on-line and non-stationary settings; important connections to these and other stochastic optimization methods are clarified in section 5.", "startOffset": 168, "endOffset": 1191}, {"referenceID": 1, "context": "SGD proved itself as an efficient and effective optimization method that was central in many machine learning success stories, such as recent advances in deep learning Deng et al. (2013); Krizhevsky et al.; Hinton & Salakhutdinov (2006); Hinton et al. (2012a); Graves et al. (2013). Objectives may also have other sources of noise than data subsampling, such as dropout Hinton et al. (2012b) regularization. For all such noisy objectives, efficient stochastic optimization techniques are required. The focus of this paper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In these cases, higher-order optimization methods are ill-suited, and discussion in this paper will be restricted to first-order methods. We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients and requires little memory. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. Our method is designed to combine the advantages of two recently popular methods: AdaGrad Duchi et al. (2011), which works well with sparse gradients, and RMSProp Tieleman & Hinton (2012), which works well in on-line and non-stationary settings; important connections to these and other stochastic optimization methods are clarified in section 5.", "startOffset": 168, "endOffset": 1269}, {"referenceID": 2, "context": "2 in Duchi et al. (2011). Their results of the expected value of E \u2211d i=1 \u2016g1:T,i\u20162 also apply to Adam.", "startOffset": 5, "endOffset": 25}, {"referenceID": 2, "context": "Optimization methods bearing a direct relation to Adam include RProp Riedmiller & Braun (1992), RMSProp Tieleman & Hinton (2012); Graves et al. (2013) and AdaGrad Duchi et al.", "startOffset": 130, "endOffset": 151}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below.", "startOffset": 19, "endOffset": 39}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information.", "startOffset": 19, "endOffset": 153}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information.", "startOffset": 19, "endOffset": 180}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information. The Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasiNewton method based on minibatches, but (unlike Adam) has memory requirements linear in the number of minibatches of data, which is often more than available on memory-constrained systems such as a GPU.", "startOffset": 19, "endOffset": 323}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information. The Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasiNewton method based on minibatches, but (unlike Adam) has memory requirements linear in the number of minibatches of data, which is often more than available on memory-constrained systems such as a GPU. Like natural gradient descent (NGD) Amari (1998), Adam employs a preconditioner that adapts to the geometry of the data, since v\u0302t is an approximation to the diagonal of the Fisher information matrix Pascanu & Bengio (2013); however, Adam\u2019s preconditioner (like AdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square root of the inverse of the diagonal Fisher information matrix approximation.", "startOffset": 19, "endOffset": 585}, {"referenceID": 2, "context": "(2013) and AdaGrad Duchi et al. (2011); these relationships are discussed below. Other stochastic optimization methods include vSGD Schaul et al. (2012) and AdaDelta Zeiler (2012), both setting stepsizes by estimating curvature from firstorder information. The Sum-of-Functions Optimizer (SFO) Sohl-Dickstein et al. (2014) is a quasiNewton method based on minibatches, but (unlike Adam) has memory requirements linear in the number of minibatches of data, which is often more than available on memory-constrained systems such as a GPU. Like natural gradient descent (NGD) Amari (1998), Adam employs a preconditioner that adapts to the geometry of the data, since v\u0302t is an approximation to the diagonal of the Fisher information matrix Pascanu & Bengio (2013); however, Adam\u2019s preconditioner (like AdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square root of the inverse of the diagonal Fisher information matrix approximation.", "startOffset": 19, "endOffset": 760}, {"referenceID": 3, "context": "A version with momentum has sometimes been used Graves et al. (2013). RMSProp lacks a biascorrection term; this matters most in case of a small value \u03b22 (required in case of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and often divergence, as we also empirically demonstrate in section 6.", "startOffset": 48, "endOffset": 69}, {"referenceID": 2, "context": "AdaGrad: An algorithm that works well for sparse gradients is AdaGrad Duchi et al. (2011). Its basic version updates parameters as \u03b8t+1 = \u03b8t \u2212 \u03b1 \u00b7 gt/ \u221a\u2211t i=1 g 2 t .", "startOffset": 70, "endOffset": 90}, {"referenceID": 2, "context": "As discussed in Duchi et al. (2011), AdaGrad can efficiently deal with sparse features and gradients as one of its main theoretical results whereas SGD is low at learning rare features.", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "As discussed in Duchi et al. (2011), AdaGrad can efficiently deal with sparse features and gradients as one of its main theoretical results whereas SGD is low at learning rare features. We examine the sparse feature problem using IMDB movie review dataset from Maas et al. (2011). We pre-process the IMDB movie reviews into bag-of-words (BoW) feature vectors including the first 10,000 most frequent words.", "startOffset": 16, "endOffset": 280}, {"referenceID": 2, "context": "As discussed in Duchi et al. (2011), AdaGrad can efficiently deal with sparse features and gradients as one of its main theoretical results whereas SGD is low at learning rare features. We examine the sparse feature problem using IMDB movie review dataset from Maas et al. (2011). We pre-process the IMDB movie reviews into bag-of-words (BoW) feature vectors including the first 10,000 most frequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As suggested in Wang & Manning (2013), 50% dropout noise can be applied to the BoW features during training to prevent over-fitting.", "startOffset": 16, "endOffset": 519}, {"referenceID": 12, "context": "The sum-of-functions (SFO) methodSohl-Dickstein et al. (2014) is a recently proposed quasi-Newton method that works with minibatches of data and has shown good performance on optimization of multi-layer neural networks.", "startOffset": 33, "endOffset": 62}, {"referenceID": 12, "context": "We compare with the sum-of-functions (SFO) optimizer Sohl-Dickstein et al. (2014)", "startOffset": 53, "endOffset": 82}], "year": 2014, "abstractText": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based an adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice when experimentally compared to other stochastic optimization methods.", "creator": "LaTeX with hyperref package"}}}