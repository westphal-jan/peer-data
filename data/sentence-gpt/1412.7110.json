{"id": "1412.7110", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Learning linearly separable features for speech recognition using convolutional neural networks", "abstract": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input. Importantly, we show that using a low-parameter network and simple linear classifier, the network learns linearly separable features from raw speech. Using simple linear classifier, the network learns linearly separable features from raw speech.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 22 Dec 2014 19:46:01 GMT  (215kb,D)", "http://arxiv.org/abs/1412.7110v1", "submitted for ICLR 2015 conference track"], ["v2", "Wed, 24 Dec 2014 13:46:10 GMT  (216kb,D)", "http://arxiv.org/abs/1412.7110v2", "Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v3", "Fri, 23 Jan 2015 10:44:21 GMT  (217kb,D)", "http://arxiv.org/abs/1412.7110v3", "Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v4", "Thu, 26 Feb 2015 19:51:35 GMT  (217kb,D)", "http://arxiv.org/abs/1412.7110v4", "Revisions according to reviews. Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v5", "Fri, 27 Feb 2015 16:31:32 GMT  (218kb,D)", "http://arxiv.org/abs/1412.7110v5", "Revisions according to reviews. Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"], ["v6", "Thu, 16 Apr 2015 08:29:14 GMT  (206kb,D)", "http://arxiv.org/abs/1412.7110v6", "Final version for ICLR 2015 Workshop; Revisions according to reviews. Revised Section 4.5. Add references and correct typos. Submitted for ICLR 2015 conference track"]], "COMMENTS": "submitted for ICLR 2015 conference track", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["dimitri palaz", "mathew magimai doss", "ronan collobert"], "accepted": true, "id": "1412.7110"}, "pdf": {"name": "1412.7110.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RAL NETWORKS", "Dimitri Palaz"], "emails": ["dimitri.palaz@idiap.ch", "mathew@idiap.ch,", "ronan@collobert.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "State-of-the-art Automatic speech recognition (ASR) systems typically divide the task into several sub-tasks, which are optimized in an independent manner (Bourlard & Morgan, 1994). In a first step, the data is transformed into features, usually composed of a dimensionality reduction phase and an information selection phase, based on the task-specific knowledge of the phenomena. These two phases have been carefully hand-crafted, leading to state-of-the-art features such as mel frequency cepstral coefficients (MFCCs) or perceptual linear prediction cepstral features (PLPs). In a second step, the likelihood of subword units such as, phonemes is estimated using generative models or discriminative models. In a final step, dynamic programming techniques are used to recognize the word sequence given the lexical and syntactical constraints.\nRecently, in the HMM/ANN framework, there has been growing interests in using \u201cintermediate\u201d representations, like short-term spectrum, instead of conventional features, such as cepstral-based features. Representations such as Mel filterbank output or log spectrum have been proposed in the context of deep neural networks (Hinton et al., 2012). In our recent study (Palaz et al., 2013), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (LeCun, 1989) (CNNs). This system yielded similar or better results on TIMIT phoneme recognition task with standard hybrid HMM/ANN systems. We also showed that this system is scalable to large vocabulary speech recognition task (Palaz et al., 2014). In this case, the CNN-based system was able to outperform HMM/ANN system with less parameters.\nIn this paper, we investigate the features learning capability of the CNN based system with simple classifiers. More specifically, we replace the classification stage of the CNN based system, which was a non-linear multi-layer perceptron, by a linear single layer perceptron. Thus, the features\nar X\niv :1\n41 2.\n71 10\nv1 [\ncs .L\nG ]\n2 2\nlearned by the CNNs are trained to be linearly separable. We evaluate the proposed approach on phoneme recognition task on the TIMIT corpus and on large vocabulary continuous speech recognition on the WSJ corpus. We compare our approach with conventional HMM/ANN system using cepstral-based features. Our studies show that the CNN-based system using a linear classifier yields similar or better performance than the ANN-based approach using MFCC features, with much less parameters.\nThe remainder of the paper is organized as follows. Section 2 presents the motivation of this work. Section 3 presents the architecture of the proposed system. Section 4 presents the experimental setup and Section 5 presents the results. Section 6 and 7 presents the discussion and concludes the paper."}, {"heading": "2 MOTIVATION", "text": "In speech recognition, designing relevant features is not a trivial task, due to the fact that the speech signal is non-stationary and non-linear. Inspired by speech coding studies, spectral-based features have been shown to yield state-of-the-art performance. The two most common ones are Mel frequency cepstral coefficient (MFCC) and perceptual linear prediction (PLP). These features are both based on obtaining a good representation of the short-term power spectrum. They are computed following a series of steps, as presented in Figure 1(a). The extraction process consists of (1) transforming the temporal data in the frequency domain, (2) filtering the spectrum based on critical bands analysis, which is derived from speech perception knowledge, (3) applying a non-linear operation and (4) applying a transformation. This process only models the signal locally, on a narrow time window. To model the temporal variation intrinsic to the speech signal, dynamic features are computed by taking the first and second derivative of the static features on the longer time window, and concatenate them together. These resulting features are then fed to the acoustic modeling part of the speech recognition system, which can be based on Gaussian mixture model (GMM) or artificial neural networks (ANN).\nIn recent years, the deep neural network (DNN) and deep belief network (DBN) approaches have been proposed (Hinton et al., 2006), which yielded state-of-the-art results in speech recognition using neural networks composed of many hidden layers, initialized in an unsupervised manner. While this original work still relies on MFCC features, several approaches proposed to use \u2018intermediate\u201d representations (standing between raw signal and \u201cclassical\u201d features such as cepstral-based features) as input. In other words, they proposed to discard several operation in the extraction pipeline of the conventional features (see Figure 1(b)). For\ninstance, Mel filterbank energies were used as input of convolutional neural networks based systems (Abdel-Hamid et al., 2012; Sainath et al., 2013). Deep neural network based systems using spectrum as input has also been proposed (Mohamed et al., 2012; Lee et al., 2009). Combination of different features has also been investigated (Bocchieri & Dimitriadis, 2013). Extracting features from raw speech has also been investigated using DBNs (Jaitly & Hinton, 2011; Tu\u0308ske et al., 2014). All these approaches still rely on the dynamic features extraction and concatenation approach to model the temporal variations of the signal. Also, the features are extracted independently from the acoustic model, in a knowledge-driven manner. Thus, most of the system\u2019s capacity lies on the classification part.\nIn our recent studies (Palaz et al., 2013; 2014), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (see Figure 1(c)). This system is composed of several filter stages, which perform the features learning step and which is implemented by convolution and max-pooling layers, and of a classification stage, implemented by a multi-layer perceptron. Both stages are trained jointly. On phoneme recognition and on large vocabulary continuous speech recognition task, we showed that the system is able to learn features from the raw speech signal, and yields performance similar or better than conventional ANN based system that takes cepstral features as input. The proposed system needs less parameters to yield similar performance with conventional systems, suggesting that the learned features are somehow more efficient.\nMotivated by these studies, the goal of the present paper is ascertain the capability of the convolutional neural network based system to learn linearly separable features in a data-driven manner. To this aim, we replace the classifier stage of the CNN-based system, which was a non-linear multilayer perceptron, by a linear single layer perceptron. Our objective is not to show that the proposed approach yields state-of-the-art performance, but to show that learning features in a data-driven manner can lead to features as efficient as conventional features. Moreover, learning them jointly with the classifier can lead to system where most of the capacity lies on the features learning part rather than on the classification part."}, {"heading": "3 CONVOLUTIONAL NEURAL NETWORKS", "text": "This section presents the architecture used in the paper. It is similar to the one presented in (Palaz et al., 2013), and is presented here for the sake of clarity."}, {"heading": "3.1 ARCHITECTURE", "text": "Our network (see Figure 2) is given a sequence of raw input signal, split into frames, and outputs a score for each classes, for each frame. The network architecture is composed of several filter extraction stages, followed by a classification stage. A filter extraction stage involves a convolutional layer, followed by a temporal pooling layer and an non-linearity. Processed signal coming out of these stages are fed to a classification stage, which in our case can be either a multi-layer perceptron (MLP) or a single linear layer (SLP). It outputs the conditional probabilities p(i|x) for each class i, for each frame x."}, {"heading": "3.2 CONVOLUTIONAL LAYER", "text": "While \u201cclassical\u201d linear layers in standard MLPs accept a fixed-size input vector, a convolution layer is assumed to be fed with a sequence of T vectors/frames: X = {x1 x2 . . . xT }. A convolutional layer applies the same linear transformation over each successive (or interspaced by dW frames) windows of kW frames. For example, the transformation at frame t is formally written as:\nM   xt\u2212(kW\u22121)/2\n... xt+(kW\u22121)/2\n  , (1)\nwhere M is a dout \u00d7 din matrix of parameters. In other words, dout filters (rows of the matrix M) are applied to the input sequence."}, {"heading": "3.3 MAX-POOLING LAYER", "text": "These kind of layers perform local temporal max operations over an input sequence. More formally, the transformation at frame t is written as:\nmax t\u2212(kW\u22121)/2\u2264s\u2264t+(kW\u22121)/2\nxds \u2200d (2)\nwith x being the input and d the dimension. These layers increase the robustness of the network to slight temporal distortions in the input."}, {"heading": "3.4 SOFTMAX LAYER", "text": "The Softmax (Bridle, 1990) layer interprets network output scores fi(x) as conditional probabilities, for each class label i:\np(i|x) = e fi(x) \u2211\nj\nefj(x) (3)"}, {"heading": "3.5 NETWORK TRAINING", "text": "The network parameters \u03b8 are learned by maximizing the log-likelihood L, given by:\nL(\u03b8) = N\u2211\nn=1\nlog(p(in|xn, \u03b8)) (4)\nfor each input x and label i, over the whole training set, with respect to the parameters of each layer of the network. Defining the logsumexp operation as: logsumexpi(zi) = log( \u2211 i e zi),\nthe likelihood can be expressed as:\nL = log(p(i|x)) = fi(x)\u2212 logsumexp j (fj(x)) (5)\nwhere fi(x) described the network score of input x and class i. Maximizing this likelihood is performed using the stochastic gradient ascent algorithm (Bottou, 1991)."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "In this section, we present the two studies, the databases, the baselines and the hyper-parameters of the networks."}, {"heading": "4.1 STUDIES", "text": ""}, {"heading": "4.1.1 PHONEME RECOGNITION", "text": "As a first experiment, we propose a phoneme recognition study, where the CNN-based system is used to estimate phoneme class conditional probabilities. The decoder is a standard HMM decoder, with constrained duration of 3 states, and considering all phoneme equally probable."}, {"heading": "4.1.2 LARGE VOCABULARY SPEECH RECOGNITION", "text": "We evaluate the scalability of the proposed system on a large vocabulary speech recognition task on the WSJ corpus. The CNN-based system is used to compute the posterior probabilities of contextdependent phonemes. The decoder is a HMM. The scaled likelihoods are estimated by dividing the posterior probability by the prior probability of each class, estimated by counting on the training set. The hyper parameters such as, language scaling factor and the word insertion penalty are determined on the validation set."}, {"heading": "4.2 DATABASES", "text": "The TIMIT acoustic-phonetic corpus consists of 3,696 training utterances (sampled at 16kHz) from 462 speakers, excluding the SA sentences. The cross-validation set consists of 400 utterances from 50 speakers. The core test set was used to report the results. It contains 192 utterances from 24 speakers, excluding the validation set. The 61 hand labeled phonetic symbols are mapped to 39 phonemes with an additional garbage class, as presented in (Lee & Hon, 1989).\nThe SI-284 set of the Wall Street Journal (WSJ) corpus (Woodland et al., 1994) is formed by combining data from WSJ0 and WSJ1 databases, sampled at 16 kHz. The set contains 36416 sequences, representing around 80 hours of speech. Ten percent of the set was taken as validation set. The Nov\u201992 set was selected as test set. It contains 330 sequences from 10 speakers. The dictionary was based on the CMU phoneme set, 40 context-independent phonemes. 2776 tied-states were used in the experiment. They were derived by clustering context-dependent phones in HMM/GMM framework using decision tree state tying. The dictionary and the bigram language model provided by the corpus were used. The vocabulary contains 5000 words."}, {"heading": "4.3 FEATURES", "text": "Raw features are simply composed of a window of the temporal speech signal. The window is normalized such that it has zero mean and unit variance. We also performed several baseline experiments, with MFCC as input features. They were computed (with HTK (Young et al., 2002)) using a 25 ms Hamming window on the speech signal, with a shift of 10 ms. The signal is represented using 13th-order coefficients along with their first and second derivatives, computed on a 9 frames context."}, {"heading": "4.4 BASELINE SYSTEMS", "text": "We compare our approach with the standard HMM/ANN system using cepstral features. We train a multi-layer perceptron with one hidden layer, referred to as MLP, and an linear single layer perceptron, referred to as SLP. The input to the MLPs are MFCC with several frames of preceding and following context. We do not pre-train the network."}, {"heading": "4.5 NETWORKS HYPER-PARAMETERS", "text": "The hyper-parameters of the network are: the input window size win, corresponding to the context taken along with each example, the kernel width kWn and shift dWn of the nth convolution layer, the number of filters dout and the pooling width. We train the CNN based system with several filter stages (composed of convolution and max-pooling layers). We use between one and five filter stages. In the case of linear classifier, the capacity of the system cannot by tuned directly. It depends on the size of the input of the classifier, which can be adjusted by manually tuned the hyper-parameters. We thus proposed two approaches to select the hyper-parameters. In the first one, they were tuned by early-stopping on the validation set. Ranges which were considered for the grid search are reported in Table 2. For the second approach, we tuned them in order to have the same number of output for the filters stages as in the baseline system. Thus, the classifier has exactly the same number of parameters, so a fair comparison can be done.\nUsing 2 filter stages, the best performance was found with: 310 ms of context, 30 samples width for the first convolution, 7 frames kernel width for the second convolution, 80 and 60 filters and 3 pooling width. Using 3 filter stages, the best performance was found with: 310 ms of context, 30\nsamples width for the first convolution, 7 and 7 frames kernel width for the other convolutions, 80, 60 and 60 filters and 3 pooling width. Using 4 filter stages, the best performance was found with: 310 ms of context, 30 samples width for the first convolution, 7, 7 and 7 frames kernel width for the other convolutions, 80, 60, 60 and 60 filters and 3 pooling width. For the baselines, the MLP uses 500 nodes for the hidden layer and 9 frames as context. The SLP based system uses 9 frames as context. The experiments were implemented using the torch7 toolbox (Collobert et al., 2011)."}, {"heading": "5 RESULTS", "text": "The results for the phoneme recognition task on the TIMIT corpus is presented in Table 3 for the best performance of the proposed system, along with the the baselines. The number of parameters in the classifier and in the filters stages are also presented. Using a linear classifier, the proposed SLP based system outperforms the MLP based baseline with three or more filters stage. One can notice that the performance of the proposed system seems to increase when most of the parameters lies in the filter stages rather than in the classification stage. Also, our SLP based system can almost reach the performance of the MLP based CNN system, with 60 times less parameters in the classifier. The results for the proposed system with a fixed number of output in presented in Table 4, along with the baseline performance and the number of the parameters in the classifier and filter stages. The proposed CNN based system outperforms the SLP based baseline with the same number of parameters in the classifier. Fixing the output size seems to degrade the performance compared to Table 3. A possible explanation is that fixing the output leads to a sub-optimal set of hyperparameters for the network.\nThe results for the large vocabulary continuous speech recognition task on the WSJ corpus is presented in Table 5 for the best performance. One can observed a similar trend to the TIMIT results, showing that the SLP-based CNN system is scalable to large database."}, {"heading": "6 DISCUSSION", "text": ""}, {"heading": "6.1 JOINT TRAINING", "text": "Traditionally in speech recognition system, features extraction and acoustic modeling are separate processes. Features extraction is usually driven by knowledge, and the acoustic model is trained on the data. In the proposed approach, the features learning and acoustic modeling steps are trained jointly, in a completely data-driven manner. Such approach thus allows the features to be flexible, as they will be learned to be efficient with a given classifier. In the presented studies, we show that it is possible to learn linearly separable features and that using such features can yield similar or better results than baseline system. In this case, the capacity of the system mostly lies on the features learning stage rather than on the classification stage.\nOne potential application in speech recognition for such system using a simple classifier is language adaptation. Indeed, training a classifier on a language with low resources can be a difficult task, mainly for training the acoustic model. The usual approach is to train the classifier on a language with a lot of resources, such as English, and use it as an initialization point for the new language. In our system, the linear classifier can be very quickly adapted. Moreover, having an overall low number of parameters also helps."}, {"heading": "6.2 FEATURE LEARNING", "text": "In the proposed approach, the features are learned by convolutional neural networks. The whole architecture is designed empirically, i.e. by early-stopping on the validation set. We realize that one the most important hyper-parameter is the kernel width of the first convolution, which takes a window of the temporal speech signal as input. We find that the best performance is obtained with a very narrow kernel, around 30 samples or 3 ms. In the case of conventional spectral-based features, as presented in Section 2, the temporal window width is around 30 ms. This width was\nselected based on speech production knowledge, whereas in our case, the kernel width was found in a data-driven manner.\nIn our previous studies, we showed that the first convolution layer can be seen as a set of matching filters, each one responding to different frequency bands (Palaz et al., 2013). We later showed that these filters show some level of invariance across databases (Palaz et al., 2014). In the convolutional neural network framework, Swietojanski et al. (2014) used for instance Mel filterbank as input for the convolution and maxpooling layers. In other words, the temporal speech signal is first filtered by a hand-crafted filterbank and then fed to the CNNs. It is similar to the proposed approach, except that, in our case, the filters are learned in a data-driven manner.\nIn the proposed architecture, the representation obtained by using these filters is then processed through a max-pooling layer, which increases the temporal window this representation has access to, and brings some level of robustness. The next layers of the network combine these representations along time. For each additional layer, the temporal window length of the representation increases, until the last layer, where the outputs of the filter stage have access to the whole context window. Overall, this explanation suggests that in our architecture, the short-term representation is modeled by the first convolution, and the long-term temporal variations are modeled by the following convolution layers."}, {"heading": "7 CONCLUSION", "text": "In this paper, we presented a study on learning linearly separable features from raw speech using convolutional neural networks. These features were learned jointly with the classifier. We showed that the proposed system using a linear classifier yields better performance that MLP based system using conventional features. These results indicated that CNNs are able to learn linearly separable features in a data-driven manner, and that these features can be as efficient as spectral-based features. Moreover, we show that using such approach can lead to systems where most of the capacity lies on the features learning part instead of the classification part, which can be very simple. As future work, we plan to investigate using other types of classifiers, such as linear SVM, and investigate language adaptation."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the HASLER foundation (www.haslerstiftung.ch) through the grant \u201cUniversal Spoken Term Detection with Deep Learning\u201d (DeepSTD). The authors also thank their colleague Ramya Rasipuram for providing the HMM/GMM baseline"}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2012}, {"title": "Investigating deep neural network based transforms of robust audio features for lvcsr", "author": ["E. Bocchieri", "D. Dimitriadis"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Bocchieri and Dimitriadis,? \\Q2013\\E", "shortCiteRegEx": "Bocchieri and Dimitriadis", "year": 2013}, {"title": "Stochastic gradient learning in neural networks", "author": ["L. Bottou"], "venue": "In Proceedings of Neuro-Nmes", "citeRegEx": "Bottou,? \\Q1991\\E", "shortCiteRegEx": "Bottou", "year": 1991}, {"title": "Connectionist speech recognition: a hybrid approach, volume 247", "author": ["H. Bourlard", "N. Morgan"], "venue": null, "citeRegEx": "Bourlard and Morgan,? \\Q1994\\E", "shortCiteRegEx": "Bourlard and Morgan", "year": 1994}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["J.S. Bridle"], "venue": "In Neuro-computing: Algorithms, Architectures and Applications,", "citeRegEx": "Bridle,? \\Q1990\\E", "shortCiteRegEx": "Bridle", "year": 1990}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Learning a better representation of speech soundwaves using restricted boltzmann machines", "author": ["N. Jaitly", "G. Hinton"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Jaitly and Hinton,? \\Q2011\\E", "shortCiteRegEx": "Jaitly and Hinton", "year": 2011}, {"title": "Generalization and network design strategies", "author": ["Y. LeCun"], "venue": "Connectionism in Perspective,", "citeRegEx": "LeCun,? \\Q1989\\E", "shortCiteRegEx": "LeCun", "year": 1989}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["Lee", "K. F", "H.W. Hon"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Lee et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1989}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Mohamed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2012}, {"title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks", "author": ["D. Palaz", "R. Collobert", "M. Magimai.-Doss"], "venue": "In Proc. of Interspeech,", "citeRegEx": "Palaz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Palaz et al\\.", "year": 2013}, {"title": "Convolutional neural networks-based continuous speech recognition using raw speech signal", "author": ["D. Palaz", "M. Magimai.-Doss", "R. Collobert"], "venue": "Technical Report Idiap-RR-18-2014, Idiap Research Institute,", "citeRegEx": "Palaz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Palaz et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "In Proc. of ICASSP,", "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Signal Processing Letters,", "citeRegEx": "Swietojanski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Swietojanski et al\\.", "year": 2014}, {"title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr", "author": ["Z. T\u00fcske", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "venue": "In Interspeech,", "citeRegEx": "T\u00fcske et al\\.,? \\Q2014\\E", "shortCiteRegEx": "T\u00fcske et al\\.", "year": 2014}, {"title": "Large vocabulary continuous speech recognition using htk", "author": ["P.C. Woodland", "J.J. Odell", "V. Valtchev", "S.J. Young"], "venue": "In Proc. of ICASSP, volume ii,", "citeRegEx": "Woodland et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Woodland et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 6, "context": "Representations such as Mel filterbank output or log spectrum have been proposed in the context of deep neural networks (Hinton et al., 2012).", "startOffset": 120, "endOffset": 141}, {"referenceID": 13, "context": "In our recent study (Palaz et al., 2013), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (LeCun, 1989) (CNNs).", "startOffset": 20, "endOffset": 40}, {"referenceID": 9, "context": ", 2013), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (LeCun, 1989) (CNNs).", "startOffset": 172, "endOffset": 185}, {"referenceID": 14, "context": "We also showed that this system is scalable to large vocabulary speech recognition task (Palaz et al., 2014).", "startOffset": 88, "endOffset": 108}, {"referenceID": 15, "context": "(b) Typical CNN based pipeline using Mel filterbank (Sainath et al., 2013; Swietojanski et al., 2014)", "startOffset": 52, "endOffset": 101}, {"referenceID": 16, "context": "(b) Typical CNN based pipeline using Mel filterbank (Sainath et al., 2013; Swietojanski et al., 2014)", "startOffset": 52, "endOffset": 101}, {"referenceID": 7, "context": "In recent years, the deep neural network (DNN) and deep belief network (DBN) approaches have been proposed (Hinton et al., 2006), which yielded state-of-the-art results in speech recognition using neural networks composed of many hidden layers, initialized in an unsupervised manner.", "startOffset": 107, "endOffset": 128}, {"referenceID": 0, "context": "instance, Mel filterbank energies were used as input of convolutional neural networks based systems (Abdel-Hamid et al., 2012; Sainath et al., 2013).", "startOffset": 100, "endOffset": 148}, {"referenceID": 15, "context": "instance, Mel filterbank energies were used as input of convolutional neural networks based systems (Abdel-Hamid et al., 2012; Sainath et al., 2013).", "startOffset": 100, "endOffset": 148}, {"referenceID": 12, "context": "Deep neural network based systems using spectrum as input has also been proposed (Mohamed et al., 2012; Lee et al., 2009).", "startOffset": 81, "endOffset": 121}, {"referenceID": 10, "context": "Deep neural network based systems using spectrum as input has also been proposed (Mohamed et al., 2012; Lee et al., 2009).", "startOffset": 81, "endOffset": 121}, {"referenceID": 17, "context": "Extracting features from raw speech has also been investigated using DBNs (Jaitly & Hinton, 2011; T\u00fcske et al., 2014).", "startOffset": 74, "endOffset": 117}, {"referenceID": 13, "context": "In our recent studies (Palaz et al., 2013; 2014), it was shown that it is possible to estimate phoneme class conditional probabilities by using temporal raw speech signal as input to convolutional neural networks (see Figure 1(c)).", "startOffset": 22, "endOffset": 48}, {"referenceID": 13, "context": "It is similar to the one presented in (Palaz et al., 2013), and is presented here for the sake of clarity.", "startOffset": 38, "endOffset": 58}, {"referenceID": 4, "context": "4 SOFTMAX LAYER The Softmax (Bridle, 1990) layer interprets network output scores fi(x) as conditional probabilities, for each class label i: p(i|x) = e fi(x) \u2211", "startOffset": 28, "endOffset": 42}, {"referenceID": 2, "context": "Maximizing this likelihood is performed using the stochastic gradient ascent algorithm (Bottou, 1991).", "startOffset": 87, "endOffset": 101}, {"referenceID": 18, "context": "The SI-284 set of the Wall Street Journal (WSJ) corpus (Woodland et al., 1994) is formed by combining data from WSJ0 and WSJ1 databases, sampled at 16 kHz.", "startOffset": 55, "endOffset": 78}, {"referenceID": 5, "context": "The experiments were implemented using the torch7 toolbox (Collobert et al., 2011).", "startOffset": 58, "endOffset": 82}, {"referenceID": 13, "context": "In our previous studies, we showed that the first convolution layer can be seen as a set of matching filters, each one responding to different frequency bands (Palaz et al., 2013).", "startOffset": 159, "endOffset": 179}, {"referenceID": 14, "context": "We later showed that these filters show some level of invariance across databases (Palaz et al., 2014).", "startOffset": 82, "endOffset": 102}, {"referenceID": 13, "context": "In our previous studies, we showed that the first convolution layer can be seen as a set of matching filters, each one responding to different frequency bands (Palaz et al., 2013). We later showed that these filters show some level of invariance across databases (Palaz et al., 2014). In the convolutional neural network framework, Swietojanski et al. (2014) used for instance Mel filterbank as input for the convolution and maxpooling layers.", "startOffset": 160, "endOffset": 359}], "year": 2014, "abstractText": "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.", "creator": "LaTeX with hyperref package"}}}