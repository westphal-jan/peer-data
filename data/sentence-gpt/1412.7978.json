{"id": "1412.7978", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "The Computational Theory of Intelligence: Information Entropy", "abstract": "This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications. To further develop our approach to intelligence, we propose a model based on the notion that a random population in the simulation is an efficient but random population. These models use random information for statistical analysis in the models of intelligence. As this data is distributed to the network, we can perform the same task as the normal population in each simulation. We demonstrate the same problem in the simulation. In this example we perform a function for the distribution of entropy in the simulated model and evaluate its probability distribution, using a deterministic function with a random distribution. In the simulation, we are able to estimate the probability distribution and the probability distribution by using an ensemble of probability distributions (which is the order of magnitude of the model). We describe the simulation as an equilibrium state, with the predicted probability distribution computed in a series, in which a given distribution is estimated in terms of random probability (N=1) (1). The results obtained from the simulation can be summed up into multiple versions. The model is presented using the new model, which is based on the original model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 24 Dec 2014 07:41:45 GMT  (474kb,D)", "http://arxiv.org/abs/1412.7978v1", "Published atthis http URL"]], "COMMENTS": "Published atthis http URL", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["daniel kovach"], "accepted": false, "id": "1412.7978"}, "pdf": {"name": "1412.7978.pdf", "metadata": {"source": "CRF", "title": "The Computational Theory of Intelligence: Information Entropy", "authors": ["Daniel Kovach"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This paper attempts to introduce a computational approach to the study of intelligence that the researcher has accumulated over years of study. This approach takes into account data from psychology, neurology, artificial intelligence, machine learning, and mathematics.\nCentral to this framework is the fact that the goal of any intelligent agent is to reduce the randomness in its environment in some meaningful way. Of course, formal definitions in the context of this paper for terms like \u201dintelligence\u201d, \u201denvironment\u201d, and \u201dagent\u201d will follow.\nThe approach draws from multidisciplinary research and has many applications. We will utilize the construct in discussions at the end of the paper. Other applications will follow in future works. Implementations of this framework can apply to many fields of study including general artificial intelligence (GAI), machine learning, optimization, information gathering, clustering, and big data, and extend outside of the applied mathematics and computer science realm to even more areas including sociology, psychology, and neurology, and even philosophy."}, {"heading": "1.1 Definitions", "text": "One cannot begin a discussion about the philosophy of artificial intelligence without a definition of the word \u201cintelligence\u201d in the first place. With the panopoly of definitions available, it is understandable that there may be some disagreement, but typically each school of thought generally shares a common\nar X\niv :1\n41 2.\n79 78\nv1 [\ncs .A\nI] 2\n4 D\nec 2\nthread. The following are three different definitions of intelligence from respectable sources:\n1. The aggregate or global capacity of the individual to act purposefully, to think rationally, and to deal effectively with his environment[19].\n2. A process that entails a set of skills of problem solving \u2014 enabling the individual to resolve genuine problems or difficulties that he or she encounters and, when appropriate, to create an effective product \u2014 and must also entail the potential for finding or creating problems \u2014 and thereby providing the foundation for the acquisition of new knowledge[5].\n3. Goal-directed adaptive behavior[18].\nVernon\u2019s hierarchical model of intelligence from the 1950\u2019s [1], and Hawkins\u2019 On Intelligence in 2004 [7] are some other great resources on this topic. Consider the following working definition of this paper, with regard to information theory and computation: computational intelligence (CI) is an information processing algorithm that\n1. Records data or events into some type of store, or memory.\n2. Draws from the events recorded in memory, to make assertions, or predictions about future events.\n3. Using the disparity between the predicted and events and the new incoming events, the memory structure in step 1 can be updated such that the predictions of step 2 are optimized.\nThe mapping in 3 is called learning, and is endemic to CI. Any entity that is facilitating the CI process we will refer to as an agent, in particular when the connotation is that the entity is autonomous. The surrounding infrastructure that encapsulates the agent together with the ensuing events is called the environment."}, {"heading": "1.2 Structure", "text": "The paper is organized as follows. In section 2 we provide a brief summary of the concept of information entropy as it is used for our purposes. In section 3, we provide a mathematical framework for intelligence and show discuss its relation to entropy. Section 4 discusses the global ramifications of local entropy minimization. In section 5 we present a simple application of the framework to data analytics, which is available for free download. Sections 6 and 7 discuss relavent related research, and future work."}, {"heading": "2 Entropy", "text": "A key concept of information theory is that of entropy, which amounts to the uncertainty in a given random variable, [8]. It is essentially, a measure of unpredictability (among other interpretations). The concept of entropy is a much\ndeeper principal of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models, [17, 15, 6]."}, {"heading": "2.1 Mathematical Representation", "text": "Although terms like Shannon entropy are pervasive in the field of information theory, it will be insightful to review the formulation in our context. To arrive at the definition of entropy, we must first recall what is meant by information content. The information content of a random variable, X denoted I [X], is given by\nI [X] = log\n[ 1\nP [X]\n] = \u2212 log [P [X]] (1)\nwhere P [X]is the probability of X. The entropy of X , denoted E [X], is then defined as the expecation value of the information content.\nE [X] = E [I [X]] = \u2212E [log [P [X]]] (2)\nExpanding by the definition of the expectation value, we have\nE [X] = \u2212 N\u2211 i=1 P [xi] log [P [xi]] (3)\nwhere {xi}is the set of possible values X can take."}, {"heading": "2.1.1 Relationship of Shannon to Thermodynamic Entropy", "text": "The concept of entropy is deeply rooted at the heart of physical reality. It is a central concept in thermodynamics, governing everything from chemical reactions to engines and refrigerators. The relationship of entropy as it is known in information theory, however, is not mapped so straightforwardly to its use in thermodynamics.\nIn statistical thermodynamics, the entropy S, of a system is given by S = \u2212kb \u2211 pi ln [pi] (4)\nwhere pi denote the probability of each microstate, or configuration of the system, and kb is the Boltzmann constant which serves to map the value of the summation to physical reality in terms of quantity and units.\nThe connection between the thermodynamic and information theoretic versions of entropy relate to the information needed to detail the exact state of the system, specifically, the amount of further Shannon information needed to define the microscopic state of the system that remains ambiguous when given its macroscopic definition in terms of the variables of classical thermodynamics. The Boltzmann constant serves as a constant of proportionality."}, {"heading": "2.1.2 Renyi Entropy", "text": "We can extend the logic of the beginning of this section to a more general formulation called the Renyi entropy of order \u03b1, where \u03b1 \u2265 0 and \u03b1 6= 1 defined as\nH\u03b1(X) = 1\n1\u2212 \u03b1 log [ N\u2211 i=1 P [xi]\u03b1 ] . (5)\nUnder this convention we can apply the concept of entropy more generally to extend the utility of the concept to a variety of applications. It is important to note that this formulation approches 1 in the limit as \u03b1 \u2192 1. Although the discussions of this paper were inspired by Shannon entropy, we wish to present a much more general definition and a bolder proposition."}, {"heading": "3 Intelligence: Definitions and Assumptions", "text": "Consider the sets S and O and let I \u2208 I be the following mapping I : S \u2192 O. The function I represents the intelligence process, a member of I , the set of all such functions. It maps input from set S to O. First, let\nIt [ si ] = oit (6)\nreflect the fact that I is mapping one element from S to one element in O, each tagged by the identifier i \u2208 N, which is bounded by the cardinality of the input set. The cardinality of these two sets need not match, nor does the mapping between I need to be bijective, or even surjective. This is an iterative process, as denoted by the index, t. Finally, let Ot represent the collection of oit.\nOver time, the mapping should converge to the intended element, oi \u2208 O, as is reflected by\nI [ si ] \u2192 oi, oi \u2208 O. (7)\nIntroduce the function:\nLt = f (O,Ot) , (8)\nwhich in practice is usually some type of error or fitness measuring function. Assuming that L is continuous and differentiable, let the updating mechanism at some particular t for I be\nIt = It\u22121 +\u2207Lt\u22121. (9)\nIn other words, I iteratively updates itself with respect to the gradient of some function, L. Additionally, L must satisfy the following partial differential equation\n\u2202 \u2202t L =\u03c1(t)d (O\u2212Ot) , \u03c1 7\u2192 R (10)\nwhere the function d is some measure of the distance between O and Ot, assuming such a function exists, and \u03c1 is called the learning rate. Applications of this process to abstract topological spaces where such a distance function is a commodity is an open question. For this to qualify as an intelligence process, we must have\nlim t\u2192\u221e\nd (O\u2212Ot)\u2192 0. (11)"}, {"heading": "3.1 Unsupervised and Supervised Learning", "text": "Some consideration should be given to the sets S and O. If O = P (S) where P (S) is the power set of S, then we will say that the mapping I is an unsupervised mapping. Otherwise, the mapping is supervised. The ramifications of this distinction is as follows. In supervised learning, the agent is given two distinct sets and trained to form a mapping between them explicitly. With unsupervised learning, the agent is tasked with learning subtle relationships in a single data set or, put more succinctly, to develop the mapping between S and its power set discussed above [9, 16]."}, {"heading": "3.2 Overtraining", "text": "Further, we should note that just because we have some function I : S \u2192 O satisfying the definitions and assumptions of this section does not mean that this mapping be necessarily meaningful. After all, we could make a completely arbitrary but consistent mapping via the prescription above, and although this would satisfy all the definitions and assumptions, it would be complete memorization on the part of the agent. But this, in fact is exactly the definition of overtraining a common pitfall in the training stage of machine learning and about which one must be very diligent to avoid."}, {"heading": "3.3 Entropy Minimization", "text": "One final part of the framework remains, and that is to show that entropy is minimized, as was stated at the beginning of this section. To show that, we consider I as a probabilistic mapping, with\nPt [ sij ] = P [ It [ sij ] = oj ]\n(12)\nindicating the probability that I maps sij \u2208 S to some oj \u2208 O. From this, we can calculate the entropy in the mapping from S toO, at each iteration t. If the projection I [ si ]\nhas N possible outcomes, then the Shannon entropy of eachsi \u2208 S is given by\nEit [ si ] = \u2212 N\u2211 j=1 Pt [ sij ] log [ Pt [ sij ]] . (13)\nIf |S| = M , then the total entropy is simply the sum of Eit [S] , i \u2208 1, 2, ..., N . But for the purposes of standardization across varying cardinalities, it is useful to speak of the normalized entropy,\nEt [S] = 1\nM M\u2211 i=1 Eit [ si ]\n(14)\nAs t\u2192\u221e, the mapping from each si to its corresponding oi converges, and we have\nlim t\u2192\u221e\nPt [ si ] \u2192 1, i \u2208 1, 2, ..., N (15)\nand therefore\nlim t\u2192\u221e\nEt \u2192 0 (16)\nFurther, using the definition for Renyi entropy in 5 for each t and i\nHit,\u03b1 [S] = 1\n1\u2212 \u03b1 log [ N\u2211 i=1 Pt [ si ]\u03b1] , i \u2208 1, 2, ..., N (17)\nTo show that the Renyi entropy is also minimized, we can use an identity involving the p-norm\nHit,\u03b1 [S] = \u03b1 1\u2212 \u03b1 log [\u2225\u2225Pt [si]\u2225\u2225\u03b1] , i \u2208 1, 2, ..., N (18)\nand show that the log function is maximized t\u2192\u221e for \u03b1 > 1, and minimized for \u03b1 < 1. The case where \u03b1\u2192 1 was shown above when we used the definition of Shannon entropy. To continue, note that\u2211\nPt [ si ] = 1 (19)\nwhere the sum is taken over all possible states oi \u2208 O in the range ofIt [ si ] . But from 15, we have s \u2225\u2225Pt [si]\u2225\u2225\u03b1 < 1, \u03b1 > 1 (20) for finite t and thus the log function is minimized only as t \u2192 \u221e. To show that the Renyi entropy is also minimized for \u03b1 \u2208 (0, 1), we repeat the above logic but note that the with the sign reversal of \u03b11\u2212\u03b1 , we need to show that\u2225\u2225Pt [si]\u2225\u2225\u03b1is minimized as t\u2192\u221e."}, {"heading": "3.4 Entropic Self Organization", "text": "In section 3 we talked about the definitions of intelligence via the mapping I : S \u2192 O. Here, we seek to apply the entropy minimization concept to P (S) itself, rather than a mapping. Explicitly, let \u03c3 \u2282 P (S)\n\u03c3 = {s \u2208 P (S)}, (21)\nwhere for every s \u2208 S, there is a unique s \u2208 \u03c3 such that s \u2208 s. That is, every element of S has one and only one element of \u03c3 containing it. The term entropic self organization refers to finding the \u03a3 \u2282 P (S) such that H\u03b1[\u03c3] is minimized over all \u03c3 satisfying 21:\n\u03a3 = minH\u03b1[\u03c3]. (22)"}, {"heading": "4 Global Effects", "text": "In nature, whenever a system is taken from a state of higher entropy to a state of lower entropy, there is always some amount of energy involved in this transition, and an increase in the entropy of the rest of the environment greater than or equal to that of the entropy loss[17]. In other words, consider a system S composed of two subsystems, s1 and s2. Then\nS = s1 + s2 (23)\nNow, consider that system in equilibrium at times t = 1, and t = 2, denoted S1 and S2. Due to the second law of thermodynamics.\nS2 \u2265 S1 (24)\nand\ns21 + s 2 2 \u2265 s11 + s12 (25)\nNow, suppose one of the subsystems, say, s1decreases in entropy by some amount, \u2206s during the transition by time t = 2, i.e. s21 = s 1 1 \u2212\u2206s Then what can be said of s12, the entropy of the rest of the system that\ns22 \u2265 s12 + \u2206s (26)\nSo the entropy of the rest of the system has to increase by an amount greater than or equal to the loss of entropy in s1. This will require some amount of energy, \u2206E.\nWhile the second law of thermodynamics has been verified time and again in virtually all areas of physics, few have extended it as a more general principal in the context of information theory. In fact, we will conclude this paper with a postulate about intelligence:\n\u2022 Computational intelligence is a process that locally minimizes and globally maximizes Renyi entropy.\nIt should be stressed that although the above is necessary of intelligence, it is not sufficient in the justification of an algorithm or process as being intelligent."}, {"heading": "5 Application", "text": "Here, we implement the discussions of this paper to practical examples. First, we consider a simple example of unsupervised learning; a clustering algorithm based on Shannon entropy minimization. Next we look at some simple behavior of an intelligent agent as it acts to maximize global entropy in its environment."}, {"heading": "5.1 Clustering by Entropy Minimization", "text": "Consider a data set consisting of a number of elements organized into rows. Take for example the data that can be found at [11]. This particular example contains 300 samples, each a vector from R3. This simple proof of concept will group the data into like neighborhoods by minimizing the entropy across all elements at each respective index in the data set. This is a data driven example, so essentially we use a genetic algorithm to perturb the juxtaposition of members of each neighborhood until the global entropy reaches a minimum (entropic self organization), while at the same time avoiding trivial cases such as a neighborhood with only one element.\nA prerequisite for running this code is that one must have the Python framework installed, which is also freely available for many operating systems at [3].\nThe clustering source code is freely available at [10]. To run, simply download it and enter\n> chmod 777 e n t r o p y c l u s t e r . py > python e n t r o p y c l u s t e r . py \u2212f e n t r o p y c l u s t e r d a t a . csv\nPlease note that this is a simple prototype, a proof of concept used to exemplify a concept. It is not optimized for latency, memory utilization, and it has not been optimized or performance tested against other algorithms in its comparative class, although dramatic improvements could be easily acheived by integrating the information content of the elements into the algorithm. Specifically, we would move elements with a high information content to clusters where that element would otherwise have a low information content. Furthermore, observe that for further efficacy, a preprocessing layer may be beneficial, especially with topological data sets like the iris data set. Nevertheless, applications of this concept applied to clustering on small and large scales will be discussed in a future work.\nWe can visualize the progression of the algorithm and the final results, respectively, in the graphs pictured below. For simplicity, only the first two (non noise) dimensions are plotted. The accuracy of the clustering algorithm was 8.3% error rate in 10000 iterations, with an average simulation time: 480.1\nseconds. Observe that although there are a few \u2019blemishes\u2019 in the final clustering results, with a proper choice of parameters including the maximum computational epochs the clustering algorithm will eventually succeed with 100% accuracy. Also pictured in figure 2 are the results of the clustering algorithm applied to a data set containing four additional fields of pseudo-randomly generated noise, each in the interval [\u22121, 1]. The performance of this trial was worse than the last in terms of speed, but was had about the same classification accuracy. The accuracy of the clustering algorithm was 6.0% error rate in 10000 iterations, with an average simulation time: 1013.1 seconds."}, {"heading": "5.2 Global Entropy Maximization", "text": "In our next set of examples consider a virtual agent confined to move about a \u2019terrain\u2019, represented by a three-dimensional surface, given by one of the two following equations, respectively:\nz = exp[\u2212(x2 + y2)] (27)\nand\nz = 1\n4 exp[\u2212( ( x 10 )2 + ( y 10 )2 )](cos[ 1 2 \u03c0y] + sin[ 1 2 \u03c0x] + 2). (28)\nWe will confine x, y such that (x, y) \u2208 ([xmin, xmax], [ymin, ymax]) and note that the range of each respective surface is z \u2208 [0, 1]. The algorithm proceeds as follows. First, the agent is initialized with a starting position, s = (x0, y0). It updates s by incrementing or decrementing its coordinates by some small value, = ( x, y). As the agent meanders about the surface, data is collected as to its position on the z\u2212axis.\nIf we partition the range of each surface into equally spaced intervals, we can form a histogram H of the agent\u2019s positional information. From this H we can construct a discrete probability function, PH and thus calculate the Renyi entropy. The agent can then use feedback from the entropy determined using H to calculate an appropriate from which it upates its position, and the cycle continues. The overall goal is to maximize its entropy, or timeout after a predetermined number of iterations.\nIn this particular simulation, the agent is initialized using a \u2019random walk\u2019, in which is is chosen at random. Next, it is updated using feedback from the entropy function.\nFrom the simple set of rules, we see emergent desire for parsimony with respect to position on the surface, even in the less probable partitions of z, (asz \u2192 1). As our simulation continues to run, so tends PH to a uniform distribution.\nTo run the simulation and obtain the above data, simply download the source code freely available at [12] and enter:\n> chmod 777 h i l l c l i m b e r . py > python h i l l c l i m b e r . py\non the command line."}, {"heading": "6 Related Work", "text": "Although there are many approaches to intelligence from the angle of cognitive science, few have been proposed from the computational side. However, as of late, some great work in this area is underway.\nMany sources claim to have computational theories of intelligence, but for the most part these \u201ctheories\u201d merely act to describe certain aspects of intelligence [2]. For example, Meyer in [14] suggests that performance on multiple tasks is dependent on adaptive executive control, but makes no claim on the emergence of such characteristics. Others discuss how data is aggregated. This type of analysis is especially relevant in computer vision and image recognition [13].\nThe efforts in this paper seek to introduce a much broader theory of emergence of autonomous goal directed behavior. Similar efforts are currently under way.\nInspired by physics and cosmology, Wissner-Gross asserts autonomous agents act to maximize the entropy in their environment [20]. Specifically he proposes a path integral formulation from which he derives a gradient which can be analogized as a causal force propelling a system along a gradient of maximum entropy over time. Using this idea, he created a startup called entropica that applies this principal in ingenious ways in a variety of different applications, ranging from anything to teaching a robot to walk upright, to maximizing profit potential in the stock market.\nEssentially, what Wissner-Gross did was start with a global principal and worked backwards. What we did in this paper was to was to arrive at a similar result from a different perspective, namely, entropy minimization."}, {"heading": "7 Conclusion", "text": "The purpose of this paper was to lay the groundwork for a generalization of the concept of intelligence in the computational sense. We discussed how entropy minimization can be utilized to facilitate the intelligence process, and how the disparities between the agent\u2019s prediction and the reality of the training set can be used to optimize the agents performance. We also showed how such a concept could be used to produce a meaningful, albeit simplified, practical demonstration.\nSome future work includes applying the principals of this paper to data analysis, specifically in the presence of noise or sparse data. We will discuss some of these applications in the next paper.\nMore future work includes discussing the underlying principals under which data can be collected hierarchically, discussing how computational processes can implement the discussions in this paper to evolve and work together to form processes of greater complexity, and discussing the relevance of these contributions to abstract concepts like consciousness and self awareness.\nIn the following paper we will examine how information can aggregate together to form more complicated structures, the roles these structures can play.\nMore concepts, examples, and applications will follow in future works."}], "references": [{"title": "Assessment of Intellectual Functioning", "author": ["Aiken", "Lewis R"], "venue": "New York: Plenum,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Engineering of Mind: An Introduction to the Science of Intelligent Systems", "author": ["Albus", "James Sacra", "A. Meystel"], "venue": "New York: Wiley,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Frames of the Mind: The Theory of Multiple Intelligences", "author": ["Gardner", "Howard"], "venue": "New York: Basic,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "A Brief History of Time", "author": ["S.W. Hawking"], "venue": "New York: Bantam,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Information Theory for Continuous Systems", "author": ["Ihara", "Shunsuke"], "venue": "Singapore: World Scientific,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Artificial Intelligence: A Systems Approach", "author": ["Jones", "M. Tim"], "venue": "Hingham, MA: Infinity Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Entropy Based Clustering.", "author": ["Kovach", "Daniel J", "Jr."], "venue": "Kovach Technologies. N.p.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Entropy Clustering Sample Data", "author": ["Kovach", "Daniel J", "Jr."], "venue": "Kovach Technologies. N.p.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Entropy Based Clustering.", "author": ["Kovach", "Daniel J", "Jr."], "venue": "Kovach Technologies. N.p.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A Computational Theory of Human Stereo Vision.", "author": ["D. Marr", "T. Poggio"], "venue": "Proceedings of the Royal Society B: Biological Sciences", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1979}, {"title": "A Computational Theory of Executive Cognitive Processes and Multiple-task Performance: I. Basic Mechanisms.", "author": ["Meyer", "David E", "David E. Kieras"], "venue": "Psychological Review", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Cycles of Time: An Extraordinary New View of the Universe", "author": ["Penrose", "Roger"], "venue": "New York: Alfred A. Knopf,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Russell", "Stuart J", "Peter Norvig"], "venue": "Upper Saddle River, NJ: Prentice Hall/Pearson Education,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "An Introduction to Thermal Physics", "author": ["Schroeder", "Daniel V"], "venue": "San Francisco, CA: Addison Wesley,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Wechsler\u2019s Measurement and Appraisal of Adult Intelligence", "author": ["Wechsler", "David", "Joseph D. Matarazzo"], "venue": "New York: Oxford UP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1972}, {"title": "Causal Entropic Forces.", "author": ["Wissner-Gross", "Alexander", "Cameron Freer"], "venue": "Physical Review Letters", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "The aggregate or global capacity of the individual to act purposefully, to think rationally, and to deal effectively with his environment[19].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": "A process that entails a set of skills of problem solving \u2014 enabling the individual to resolve genuine problems or difficulties that he or she encounters and, when appropriate, to create an effective product \u2014 and must also entail the potential for finding or creating problems \u2014 and thereby providing the foundation for the acquisition of new knowledge[5].", "startOffset": 353, "endOffset": 356}, {"referenceID": 0, "context": "Vernon\u2019s hierarchical model of intelligence from the 1950\u2019s [1], and Hawkins\u2019 On Intelligence in 2004 [7] are some other great resources on this topic.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "A key concept of information theory is that of entropy, which amounts to the uncertainty in a given random variable, [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 13, "context": "deeper principal of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models, [17, 15, 6].", "startOffset": 134, "endOffset": 145}, {"referenceID": 11, "context": "deeper principal of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models, [17, 15, 6].", "startOffset": 134, "endOffset": 145}, {"referenceID": 3, "context": "deeper principal of nature that penetrates to the deepest core of physical reality and is central to physics and cosmological models, [17, 15, 6].", "startOffset": 134, "endOffset": 145}, {"referenceID": 5, "context": "With unsupervised learning, the agent is tasked with learning subtle relationships in a single data set or, put more succinctly, to develop the mapping between S and its power set discussed above [9, 16].", "startOffset": 196, "endOffset": 203}, {"referenceID": 12, "context": "With unsupervised learning, the agent is tasked with learning subtle relationships in a single data set or, put more succinctly, to develop the mapping between S and its power set discussed above [9, 16].", "startOffset": 196, "endOffset": 203}, {"referenceID": 13, "context": "In nature, whenever a system is taken from a state of higher entropy to a state of lower entropy, there is always some amount of energy involved in this transition, and an increase in the entropy of the rest of the environment greater than or equal to that of the entropy loss[17].", "startOffset": 276, "endOffset": 280}, {"referenceID": 7, "context": "Take for example the data that can be found at [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "The clustering source code is freely available at [10].", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We will confine x, y such that (x, y) \u2208 ([xmin, xmax], [ymin, ymax]) and note that the range of each respective surface is z \u2208 [0, 1].", "startOffset": 127, "endOffset": 133}, {"referenceID": 8, "context": "To run the simulation and obtain the above data, simply download the source code freely available at [12] and enter: > chmod 777 h i l l c l i m b e r .", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Many sources claim to have computational theories of intelligence, but for the most part these \u201ctheories\u201d merely act to describe certain aspects of intelligence [2].", "startOffset": 161, "endOffset": 164}, {"referenceID": 10, "context": "For example, Meyer in [14] suggests that performance on multiple tasks is dependent on adaptive executive control, but makes no claim on the emergence of such characteristics.", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "This type of analysis is especially relevant in computer vision and image recognition [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Inspired by physics and cosmology, Wissner-Gross asserts autonomous agents act to maximize the entropy in their environment [20].", "startOffset": 124, "endOffset": 128}], "year": 2014, "abstractText": "This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications.", "creator": "TeX"}}}