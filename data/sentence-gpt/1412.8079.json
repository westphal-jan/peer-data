{"id": "1412.8079", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Dec-2014", "title": "Persian Sentiment Analyzer: A Framework based on a Novel Feature Selection Method", "abstract": "In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers. Sentiment analysis aims to use automated tools to detect subjective information from reviews. One of the main challenges in sentiment analysis is feature selection. In this way, we aim to present a way to predict the quality of content when a new person or group is given a response or another for each other. We use the following techniques to detect whether the human being is being considered more intelligent by others and their behaviour may affect their overall performance. As we discussed in our previous blogpost, we use semantic analysis to assess whether a person can be more intelligent and intelligent than others. A different approach involves using semantic methods to distinguish between different types of content and to measure the extent to which they are influenced by other factors. The key distinction between semantic analysis and semantic analysis is in the amount of time and the amount of time needed for a new person or group to respond to the question. The difference between semantic analysis and semantic analysis is in the amount of time spent explaining the meaning of an item or word. The differences in the amount of time spent explaining an item or word is much smaller than the amount of time spent explaining the meaning of an item or word, but they are much larger than the amount of time spent explaining an item or word, so we use semantic analysis to measure a particular type of content. The data is then analyzed for a variety of semantic analysis techniques to determine how much time it takes to distinguish between different kinds of content and to identify the difference in time spent explaining a particular type of content. We can calculate that there is a strong correlation between quality of an item or word and the amount of time spent explaining an item or word, but we are not sure how much time this particular content takes in deciding the quality of an item or word, but we believe that there is a strong correlation between quality of an item or word. Our results suggest that if a particular type of content is being discussed, there are significant differences between the amount of time spent explaining an item or word and the amount of time spent explaining an item or word, but we also believe that this is not the case. For example, we believe that if the person who was asked to talk about an item or word is a robot, there are more than one more words to add to the list (with only five words to add). The correlation between quality of an item or word would not be significant if people knew", "histories": [["v1", "Sat, 27 Dec 2014 21:00:24 GMT  (118kb)", "http://arxiv.org/abs/1412.8079v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ayoub bagheri", "mohamad saraee"], "accepted": false, "id": "1412.8079"}, "pdf": {"name": "1412.8079.pdf", "metadata": {"source": "CRF", "title": "Persian Sentiment Analyzer: A Framework based on a Novel Feature Selection Method", "authors": ["Ayoub Bagheri", "Mohamad Saraee"], "emails": ["a.bagheri@ec.iut.ac.ir", "m.saraee@salford.ac.uk"], "sections": [{"heading": null, "text": "In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers. Sentiment analysis aims to use automated tools to detect subjective information from reviews. One of the main challenges in sentiment analysis is feature selection. Feature selection is widely used as the first stage of analysis and classification tasks to reduce the dimension of problem, and improve speed by the elimination of irrelevant and redundant features. Up to now as there are few researches conducted on feature selection in sentiment analysis, there are very rare works for Persian sentiment analysis. This paper considers the problem of sentiment classification using different feature selection methods for online customer reviews in Persian language. Three of the challenges of Persian text are using of a wide variety of declensional suffixes, different word spacing and many informal or colloquial words. In this paper we study these challenges by proposing a model for sentiment classification of Persian review documents. The proposed model is based on lemmatization and feature selection and is employed Naive Bayes algorithm for classification. We evaluate the performance of the model on a manually gathered collection of cellphone reviews, where the results show the effectiveness of the proposed approaches. Keywords: Natural language processing, sentiment analysis, sentiment classification, feature selection, Persian language, Naive Bayes algorithm. Mathematics Subject Classification: 68T50, 68P20"}, {"heading": "1. INTRODUCTION", "text": "In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers (Bagheri, Saraee, and de Jong, 2013a) (Hu and Liu, 2004a) (Hu and Liu, 2004b) (Liu and Zhang, 2012). Up to now studies in sentiment analysis have covered a wide range of tasks, including sentiment classification on word, phrase, sentence and document level (Cui, Mittal, and Datar, 2006) (Gamon, 2004) (Moraes, Valiati, and Gavi\u00e3o Neto, 2012) (Pang, Lee, and Vaithyanathan, 2002) (Saraee and Bagheri, 2013) (Yussupova, Bogdanova, and Boyko, 2012) (Zhu, Wang, and Mao, 2012), opinion target identification or aspect detection (Bagheri, Saraee, and de Jong, 2013a) (Bagheri, Saraee, and de Jong, 2013b) (Hu and Liu, 2004a) (Liu and Zhang, 2012) (Popescu and Etzioni, 2005) (Qiu, Liu, Bu et al. 2011) (Zhu, Wang, Zhu et al. 2011), opinion word detection and opinion orientation identification (Hogenboom, Boon, and Frasincar, 2012) (Hu and Liu,\n2004b) (Liu, Hu, Cheng, 2005) (Liu and Zhang, 2012) (Turney, Littman, 2002). The ability of sentiment classification on multiple levels is important since different applications have different needs. For example an opinion summarization system on product reviews may needs both classifications on document and word level. In this work we focus on classifying product reviews on the document level.\nUp to now, many researches have been conducted on English, Chinese or Russian document sentiment analysis or classification (Liu and Zhang, 2012) (Qiu, Liu, Bu et al. 2011) (Yussupova, Bogdanova, and Boyko, 2012). However on Persian text, in our knowledge there is little investigation conducted on sentiment analysis (Saraee and Bagheri, 2013) (Shams, Shakery, and Faili, 2012). Persian is an Indo-European language, spoken and written primarily in Iran, Afghanistan, and a part of Tajikistan. The amount of information in Persian language on the internet has increased in different forms. As the style of writing in Persian language is not firmly defined on the web, there are too many web pages in Persian with completely different writing styles for the same words. Different writing styles produce many challenges about Persian text processing such as, informal and colloquial words, declensional suffixes, multiple types of writing for a word and word spacing (Farhoodi and Yari, 2010) (Saraee and Bagheri, 2013) (Shams, Shakery, and Faili, 2012) (Taghva, Beckley, and Sadeh, 2005). Therefore in this paper, we study the main challenges of Persian language, and experiment our model on a Persian product review dataset.\nFeature selection is a process that can be applied in many machine learning applications like optimization problems, search algorithms, and classification and clustering problems (Ali, Alkhatib, Tashtoush, 2013) (Bagheri, Akbarzadeh - T, Saraee, 2008) (Duric, Song, 2012) (Ghani, Probst, Liu et al., 2006) (Precup, David, Petriu et al., 2012). Feature selection is one of the most important parts in a classification problem which removes irrelevant features to decrease the computational cost. Feature selection process works by ranking all the features and then selecting a subset containing best features (Ghani, Probst, Liu et al., 2006) (Mitchell, 1997) (Pei, Shi, Marchese et al. 2007) (Rennie, 2004) (Tang, Shepherd, Milios et al., 2005). Feature selection for sentiment analysis is a very tough optimization work when the feature space of sentiment words is too large. In this work we study three different feature selection methods and we present a new feature selection approach for the sentiment analysis model to improve the overall accuracy. The proposed approach for feature selection is not specifically for Persian text and it is applicable for other languages or domains.\nIn the reminder of this paper, existing works on sentiment classification will be given in section 2. Section 3 describes the proposed model for sentiment classification of Persian reviews, including the Naive Bayes classifier, different feature selection methods and the proposed approach for feature selection. Subsequently we describe our empirical evaluation and discuss important experimental results in section 4. Finally we conclude with a summary and some future research directions in section 5."}, {"heading": "2. RELATED WORK", "text": "Sentiment classification aims to distinguish whether people like or dislike a product, a service, an organization, an individual or a topic. Sentiment classification for product reviews has recently\nattracted much attention from the natural language processing community. Researchers have investigated the effectiveness of different machine learning algorithms using various features to develop sentiment classifiers on languages like English or Chinese (Liu and Zhang, 2012) (Qiu, Liu, Bu et al. 2011). On Persian text, there is very little investigation conducted on sentiment analysis. Besides our study (Saraee and Bagheri, 2013) through our energies on searching the web, only one people\u2019s work can be found, i.e., Shams et al. (Shams, Shakery, and Faili, 2012). Shams et al. (Shams, Shakery, and Faili, 2012) employed a new lexical resource for Persian sentiment analysis, PersianClues, with an unsupervised LDA-based sentiment analysis method, LDASA, to categorize each document into its related polarity using a classification algorithm. In their work, to generate the PersianClues, an automatic translation approach is used to translate the existing English clues to Persian. They experiment their model on Persian documents about hotels, cellphones and digital cameras and reached to 76% accuracy on average. Turney and Littman (Turney, Littman, 2002) employed a vocabulary of adjectives and adverbs to find out about sentiment orientation of each word in the documents. They used point-wise mutual information and latent semantic analysis to calculate the orientation of the extracted words according to their co-occurrences with the seed words, such as excellent and poor. They determined the polarity of a document by averaging the sentiment orientation of words in the document. Instead of conducting the analysis on the word level, another stream of research performs sentiment classification on the document or review level. Pang et al. (Pang, Lee, and Vaithyanathan, 2002) conducted an extensive experiment on English movie reviews on the document level using three supervised machine learning methods, Naive Bayes, Maximum Entropy Model and Support Vector Machines. Their results show that machine learning techniques definitely outperform human-produced baselines. Additionally they found that machine learning approaches could not perform as well on sentiment classification as on traditional text classification problem. Up to now different approaches have been used to sentiment classification problem. Little emphasis has been given to feature selection techniques in sentiment analysis. Gamon (Gamon, 2004) and Yi et al. (Yi, Nasukawa, Bunescu et al., 2003) used log likelihood to select important attributes from a large initial feature space. Duric and Song (Duric and Song, 2012) approached the task of feature selection for sentiment analysis by using a Content and Syntax model, HMM-LDA, to separate the entities in a review document from the potentially sentiment carrying modifiers. Abbasi et al. (Abbasi, Chen, and Salem, 2008) developed an entropy weighted genetic algorithm (EWGA) for efficient feature selection in order to improve accuracy and identify key features for each sentiment class. In their work EWGA significantly outperformed the no feature selection baseline and GA on all their experiments. Hence, Using feature selection could improve accuracy and focus in on a key feature subset of sentiment discriminators. In this paper, we propose a sentiment classification model for the document level of cellphone reviews with various feature selection methods. In the model we present a new feature selection approach based on the Mutual Information method. We use stemming and n-gram features to overcome obstacles in Persian text."}, {"heading": "3. PROPOSED MODEL FOR SENTIMENT ANALYSIS", "text": "In the proposed sentiment analysis model, after preprocessing, a stemming step is used to remove the redundancy from feature space, then we utilize a feature selection method to obtain better performance with the features and reduce the computational cost and finally we apply a classifier learning algorithm as training step. In order to describe the proposed model for sentiment analysis, in this section, we will discuss about the stemming and learning steps and will introduce the proposed feature selection approach among presenting other feature selection approaches."}, {"heading": "3.1. Stemming", "text": "Stemming or lemmatization is to reduce a word to a more general form, possibly its root. For example, stemming the term \u201cinteresting\u201d may produce the term \u201cinterest\u201d. Persian language has a complicated morphology. In Persian language suffixes and prefixes are concatenated to words to modify the meaning. Like English nouns, Persian nouns are affixed to signify possession and plurality. On the other hand, Persian verbs are modified more extensively than English verbs. Persian verbs vary according to tense, person, negation, and mood. Therefore, a given verb may have different forms and variations. In this study we re-implemented Taghva Persian stemmer based in the algorithm presented in (Taghva, Beckley, and Sadeh, 2005)."}, {"heading": "3.2. Description of sentiment classifier", "text": "In this paper, we consider Naive Bayes algorithm which is a machine learning approach as the sentiment classifier. Mathematically, the problem of classifying review documents based on their\nsentiments can be represented as follows. There are two classes, the class of positive reviews, ,\nand the class of negative reviews, :\n{ }1 2,C c c= There is a set of reviews:\n{ }1 2, , , nR r r r= \u00bc And an unknown classification function:\n{ }: 0,1F C R\u00b4 \u00ae We need to build a classifier as close to the classification function F as possible. In this problem\nwe use vector model to represent the feature space. For the feature space we extract n-gram features to deal with the conflicting problem of space and pseudo-space in Persian sentences. Here we use unigram and bigram phrases as n-gram features. Therefore in this model, the sequence of the words is important. For example in the review sentence of a cellphone, \u201c.\u062a\u0633\u0627 \u0628\u0648\u062e \u06cc\u0634\u0648\u06af \u0646\u06cc\u0627 \u06cc\u06be\u062f \u0646\u062a\u0646\u0622 \u062a\u06cc\u0641\u06cc\u06a9 / The signal quality of this phone is good.\u201d in addition to considering each word as a feature individually, we extract bi-gram features: \u201c\u06cc\u06be\u062f \u0646\u062a\u0646\u0622 / signal\u201d, \u201c\u0646\u062a\u0646\u0622 \u062a\u06cc\u0641\u06cc\u06a9 / signal quality\u201d and \u201c\u0628\u0648\u062e \u06cc\u0634\u0648\u06af / good phone\u201d.\nExperiments show using n-gram features could solve the problem of different word spacing in Persian text.\nNaive Bayes Algorithm\nWe used the MAP (Maximum a Posteriori) Naive Bayes algorithm in our experiments as a classifier for Persian sentiment classification system, because Naive Bayes is a kind of important classification algorithm and it has a high speed and is easy to implement rather than approaches like SVM or neural networks (Mitchell, 1997) (Rennie, 2004). In a classification problem, training and test dataset have to be labeled by a human expert and the classifier predicts the class of each data in test dataset. Naive Bayes algorithm assigns a new review document with a class with the maximum probability. This maximum value can be calculated by equation (1):\n( ) ( ) : | jNB c C j i j\ni\nNaive Bayes Classifier c argmax P c P a c\u00ce= \u00d5 (1)\nWhere is the assigned class or output of Naive Bayes algorithm, shows the class jth, is\nprior probability of class j in the set of all classes C and shows conditional probability of\nfeature i in class j. Output of the Naive Bayes algorithm is the maximum probability between classes.\nTo calculate , we require estimates for the probability terms and . can\nsimply estimate based on the fraction of each class in the training data by equation (2):\n( ) jj R\nP c R = (2)\nWhere is total number of review documents and jR is the number of review documents with\nclass label j. For estimating we define an expression named FSpace which is the set of all\ndistinct features in any review document in the dataset. Therefore can be computed by\nequation (3):\n( ) 1| iji j j n P a c n FSpace + = +\n(3)\nWhere jn is the total number of features in all training data with the class label of , ijn is the\nnumber of times feature is found among these n features, and is the size of feature\nspace.\nWhen we focus on the equation of we found that the fraction maybe zero in some\ncircumstances, therefore we use a modified version of the equation (2) in equation (4):\n( ) 1\n| | j j\nR P c R FSpace + = +\n(4)\nAfter training the classifier, we can use the algorithm for estimating class label of a new review document from equation (1)."}, {"heading": "3.3. Feature selection for sentiment analysis", "text": "Feature Selection methods sort features on the basis of a numerical measure computed from the documents in the dataset collection, and select a subset of the features by thresholding that measure. In this paper four different information measures were implemented and tested for feature selection problem in sentiment analysis. The measures are Document Frequency (DF), Term Frequency Variance (TFV), Mutual Information (MI) (Yang and Pedersen, 1997) and Modified Mutual Information (MMI).\nDocument Frequency\nDocument Frequency is the number of documents in the training dataset in which a term or feature occurs. Only the features that occur in a large number of documents are retained. DF thresholding is the simplest technique for feature selection and reduction.\nTerm Frequency Variance\nThe basic idea of TFV is to rank the quality of a feature based on the variance of its frequency (Tang, Shepherd, Milios et al., 2005). TFV can be calculated by:\n2\n1 ( ) [ ( , ) _ ( )]\nk\ni i TFV f tf f c mean tf f = = -\u00e5 (5)\nWhere f is current feature, k is number of classes, ic is ith class and ( , )itf f c is frequency of\nfeature f in all documents of class ic . The intuition of this method is to select features which have less\nfrequency and are common across all classes.\nMutual information and modified mutual information\u2013the proposed feature selection approach\nIn this paper we introduce a new approach for feature selection, Modified Mutual Information. In order to explain Modified Mutual Information (MMI) measure, it is helpful to first introduce Mutual Information (MI) by defining a contingency table (see Table 1).\nTable 1. Contingency table for features and classes\nC\nBA DC\nTable 1 records co-occurrence statistics for features and classes. Therefore we see that the number of times a class c occurred with the presence of feature f in the training dataset is A, for example. We also have that the number of review documents, N = A+B +C +D. These statistics are very useful for\nestimating probability values (Tang, Shepherd, Milios et al. 2005) (Pei, Shi, Marchese et al. 2007) (Ghani, Probst, Liu et al. 2006). By using Table 4, MI can be computed by equation (7):\n( ) ( , ), log ( ) ( ) P f cMI f c P f P c = (6)\nWhere is the probability of co-occurrence of feature f and class c together, and and\nare the probability of co-occurrence of feature f and class c in the review documents\nrespectively. Therefore by Table 1, MI can be approximated by Equation (8):\n( ) ( ) *, log *( ) A NMI f c A B A C = + +\n(7)\nOr for simplicity:\n( ) ( ) *, *( ) A NMI f c A B A C = + +\n(8)\nIntuitively MI measures if the co-occurrence of f and c is more likely than their independent\noccurrences, but it doesn\u2019t measure the co-occurrence of f and or the co-occurrence of other features and class c. We introduce a Modified version of Mutual Information as MMI which consider all possible combinations of co-occurrences of a feature and class label. First we define four parameters as the following:\n- : Probability of co-occurrence of feature f and class c together.\n- : Probability of co-occurrence of all features except f in all classes except c together.\n- : Probability of co-occurrence of all features except feature f in class c.\n- : Probability of co-occurrence of feature f in all classes except c.\nWe name the first two parameters as positive factors, which by definition they have a positive impact on the finding the relative information between feature f and class c. likewise we name the second two parameters as negative factors. Therefore we calculate MMI score as Equation (10):\n( ) ( ) ( ) ( ) ( )\n( ) ( ) ( ) ( ) , * ( , ), * ( , ) , log log * * * ( ) * * * ( ) p f c p f cp f c p f c MMI f c p f p c p f p c p f p c p f p c = - (9)\nWhere and are the probability of independent occurrence of feature f and class c in the\nreview documents respectively. is the number of review documents which not contain feature f\nand is the number of documents with the classes other than class c. MMI considers positive and negative factors to calculate the score between f and c. In this measure, the positive factor assigns with a positive coefficient and the negative factor assigns with a negative coefficient, hence the measure uses the information of both positive and negative factors. Based on Table 1, MMI can be approximated by Equation (11):\n( ) ( ) ( ) ( ) ( ) ( ) ( ) * *, log log * * *( ) * * *( ) A D C BMMI f c A C B D A B C D A C B D A B C D = -\n+ + + + + + + + (10)\nFor simplicity, we calculate MMI as:\n( ) ( ) ( ) ( ) * *, * * *( ) A D C BMMI f c A C B D A B C D - = + + + +\n(11)"}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "In this Section, we describe the evaluation of the proposed sentiment analysis model with a comparative study on the proposed MMI feature selection approach in a variety of settings in compare to the results of other feature selection approaches. In this paper we chose the Persian as validation language. Hence, in the following, first the challenges in Persian sentiment analysis will be discussed and then data collection, evaluation metrics, use of cross validation and important experimental results will be discussed."}, {"heading": "4.1. Persian language", "text": "Persian text mining or specifically Persian sentiment analysis suffers from low quality. One of the main challenges is the lack of comprehensive solutions or tools for the tasks like stemming, POS tagging, and feature selection. One of the consequences is reaching to a very large vector space, hence the process is very time consuming and leads to weak results. Using of a wide variety of declensional suffixes is another challenge of Persian language, Tables 2 and 3 show examples of different suffixes in Persian text. Another common problem of Persian text is word spacing. In Persian in addition to white space as inter-words space, an intra-word space called pseudo-space separates word\u2019s part. Using white space or do not using any space instead of pseudo-space is a great challenge in Persian reviews. For example in the sentence \u201c \u062a\u0633\u062f \u0635\u06cc\u062e\u0634\u062a \u062a\u06cc\u0644\u0628\u0627\u0642 \u06cc\u0634\u0648\u06af \u0646\u06cc\u0627.\u062f\u0631\u0627\u062f \u06cc\u0628\u0648\u062e \u0637\u062e / This phone has good handwritten recognition ability\u201d, the word \u201c \u062a\u0633\u062f \u0637\u062e / handwritten\u201d is a word which uses pseudo-space and contains two other words \u201c\u062a\u0633\u062f / hand\u201d and \u201c\u0637\u062e / written line\u201d. In this sentence if the algorithm interprets the word \u201c \u062a\u0633\u062f\u0637\u062e / handwritten\u201d united or separated, the feature space and the results could be different. Another important challenge in customer reviews in Persian language is that of utilizing many informal or colloquial words in text. Table 4 shows some examples for Informal or colloquial words in Persian documents.\nWe believe that our model with using n-gram features, stemming and feature selection overcomes to the Persian language challenges and can enormously decrease the size of the vector space and affect the Persian sentiment classification process positively."}, {"heading": "4.2. Data", "text": "To test our methods we compiled a dataset of 1020 online customer reviews in Persian language from different brands of cell phone products including Nokia, Apple, Samsung, Sony, LG, Motorola, Huawei and HTC (Saraee and Bagheri, 2013). We assigned two annotators to label customer reviews by selecting a positive or negative polarity on the review level. After annotation, the dataset reached to 511 positive and 509 negative reviews."}, {"heading": "4.3. Cross validation", "text": "A supervised learning algorithm needs some of data to be labeled as training data and some of them as test data (Hu and Liu, 2004b). These two datasets must be separate to prevent from false results in evaluating the performances of the methods. Therefore multiple runs of the experiments are usually needed with different datasets at each turn. One of the approaches splitting the dataset and running the experiment is N-fold cross validation. N-fold cross validation consists in splitting the dataset into N subsets of equal size. At each turn, one set is used for testing and the rest for training the system. In our case, 5-fold cross validation is used. At each turn, 4 folds will be used for training and learning and one for testing, in such way that every subset will be used once for testing purposes. Then, the average over all 5 experiments will be as an estimate of the performance of the classifier."}, {"heading": "4.4. Evaluation metrics", "text": "We use precision, recall and F-score to measure the effectiveness of a feature selection method and a sentiment classifier.\nThe precision, recall and F-score are calculated based on Table 5 as:\nTPPrecision TP FP = +\n(12)\nTPRecall TP FN = +\n(13)\n2*( * )Precision RecallF score Precision Recall - = +\n(14)\nTo deal with multiple datasets (products), we adopt the macro and micro average (Yang, 1999) to\nassess the overall performance. The macro- and micro-averaged precision and recall across the datasets are defined as follows:\n1 n ii\nPrecision Macro averaged precision n =- =\u00e5 (15)\n1 n ii\nRecall Macro averaged recall n =- =\u00e5 (16)\n( ) 1\n1\nn ii\nn i ii\nTP Micro averaged precision\nTP FP =\n=\n- = + \u00e5 \u00e5\n(17)\n1\n1 ( )\nn ii\nn i ii\nTP Micro averaged recall\nTP FN =\n=\n- = + \u00e5 \u00e5\n(18)\nThe macro average is calculated by simply taking the average obtained for each dataset, which gives an equal weight for every dataset and product. Whereas the micro average assigns each dataset a relative weight on the basis of the number of extracted or manually tagged aspects for the dataset."}, {"heading": "4.5. Comparative study", "text": "In our experiments, first we evaluated Persian sentiment classification in two phases:\nPhase 1. Without n-gram features and stemming Phase 2. With n-gram features and stemming\nTable 6 shows the F-score results for the two phases. From the results we can observe that using of n-gram features and stemming for sentiment classification has 4% and 0.3% improvements for negative and positive classes respectively.\nIn this work we applied four different feature selection approaches, MI, DF, TFV and MMI with the Naive Bayes learning algorithm to the online Persian cellphone reviews. In the experiments, we found that using feature selection with learning algorithms can perform improvement to classifications of sentiment polarities of reviews.\nThe results from Table 7 indicate that the TFV, DF and MMI have better performances than the traditional MI approach. In terms of F-score, MMI improves MI with 21.46% and 32.46% on Negative and Positive classes respectively, DF overcomes MI with 19.36% and 32.5% better performances for Negative and Positive review documents respectively and TFV improves MI with19.7% and 32.76% for Negative and Positive documents respectively. The reason of poor performance for MI is that of MI only uses the information between the corresponding feature and the corresponding class and does not utilize other information about other features and other classes. When we compare DF, TFV and MMI, we can find that the MMI beats both DF and TFV on F-scores of Negative review documents with 2.1% and 1.76% improvements respectively, but for the Positive review documents DF and TFV have 0.04% and 0.3% better performance than the MMI, respectively. To assess the overall performance of techniques we adopt the macro and micro average, Figures 5, 6 and 7 show the macro and micro average for precision, recall and F-score respectively.\nFrom these figures we can find that the MMI proposed approach has slightly better performance than the DF and TFV approaches and has significant improvements on MI method. The basic advantage of the MMI is using of whole information about a feature, positive and negative factors between features and classes. MMI in overall can reach to 85% of F-score classification. It is worth noting that with a larger training corpus the feature selection approaches and the learning algorithm could get higher performance values. Additionally the proposed approach \u2013 MMI \u2013 is not only for Persian reviews and in addition can be applied to other domains or other classification problems. Finally we can conclude that the proposed MMI algorithm is a promising alternative algorithm for the feature selection problem in text mining area of research. MMI considers all possible combinations of co-occurrences of a feature and a class label with computing positive and negative factors to calculate the score between the feature and class label. The proposed MMI approach can significantly improve previous standard methods while the approach is applicable for other domains of data mining and machine learning."}, {"heading": "5. CONCLUSION AND FUTURE WORKS", "text": "This work presents a study on four feature selection approaches in sentiment analysis for Persian documents. In this paper we proposed a novel approach for feature selection, MMI, in sentiment classification problem. In addition we applied three other feature selection approaches, DF, MI and TFV with the Naive Bayes learning algorithm to the online Persian cellphone reviews. As the results show, using feature selection in sentiment analysis can improve the performance. The proposed MMI method that uses the positive and negative factors between features and classes improves significantly the performance compared to the other approaches. Based on the definition of MMI, it is a promising approach which can be utilized in other applications of data mining. In our future work we will focus more on sentiment analysis about Persian text. We will extend our model of classification to the word-level analysis and we will study approaches for opinion target identification and opinion word detection for Persian sentiment analysis. The main limitation of the study on Persian text is lack of data resource and open-source tools."}], "references": [{"title": "Cultural algorithms: emerging social structures for the solution of complex optimization problems, International", "author": ["Ali M. Z", "K. Alkhatib", "Y. Tashtoush"], "venue": "Journal of Artificial Intelligence,", "citeRegEx": "Z. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Z. et al\\.", "year": 2013}, {"title": "Sentiment analysis in multiple languages: feature selection for opinion classification in web forums", "author": ["A. Abbasi", "H. Chen", "A. Salem"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Abbasi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abbasi et al\\.", "year": 2008}, {"title": "Finding shortest path with learning algorithms", "author": ["A. Bagheri", "M.R. Akbarzadeh - T", "M. Saraee"], "venue": "International Journal of Artificial Intelligence,", "citeRegEx": "Bagheri et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bagheri et al\\.", "year": 2008}, {"title": "2013a, Care more about customers: unsupervised domainindependent aspect detection for sentiment analysis of customer reviews", "author": ["A. Bagheri", "M. Saraee", "F. de Jong"], "venue": "Knowledge-Based Systems,", "citeRegEx": "Bagheri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bagheri et al\\.", "year": 2013}, {"title": "2013b, An unsupervised aspect detection model for sentiment analysis of reviews", "author": ["A. Bagheri", "M. Saraee", "F. de Jong"], "venue": "Proceedings of Natural Language Processing and Information Systems,", "citeRegEx": "Bagheri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bagheri et al\\.", "year": 2013}, {"title": "Comparative experiments on sentiment classification for online product reviews", "author": ["H. Cui", "V. Mittal", "M. Datar"], "venue": "Proceedings of National Conference on Artificial Intelligence,", "citeRegEx": "Cui et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2006}, {"title": "Feature selection for sentiment analysis based on content and syntax models, Decision", "author": ["A. Duric", "F. Song"], "venue": "Support Systems,", "citeRegEx": "Duric and Song,? \\Q2012\\E", "shortCiteRegEx": "Duric and Song", "year": 2012}, {"title": "Applying machine learning algorithms for automatic Persian text classification", "author": ["M. Farhoodi", "Yari A"], "venue": "Proceedings of IEEE International Conference on Advanced Information Management and Service,", "citeRegEx": "Farhoodi and A.,? \\Q2010\\E", "shortCiteRegEx": "Farhoodi and A.", "year": 2010}, {"title": "Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis", "author": ["M. Gamon"], "venue": "Proceedings of 20th International Conference on Computational Linguistics,", "citeRegEx": "Gamon,? \\Q2004\\E", "shortCiteRegEx": "Gamon", "year": 2004}, {"title": "Text mining for product attribute extraction", "author": ["R. Ghani", "K. Probst", "Y. Liu", "M. Krema", "A. Fano"], "venue": "SIGKDD Explorations,", "citeRegEx": "Ghani et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ghani et al\\.", "year": 2006}, {"title": "A Statistical approach to star rating classification of sentiment", "author": ["A. Hogenboom", "F. Boon", "F. Frasincar"], "venue": "Management Intelligent Systems,", "citeRegEx": "Hogenboom et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hogenboom et al\\.", "year": 2012}, {"title": "Mining opinion features in customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of 19th National Conference on Artificial Intelligence AAAI,", "citeRegEx": "Hu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Hu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Opinion observer: analyzing and comparing opinions on the web", "author": ["B. Liu", "M. Hu", "J. Cheng"], "venue": "Proceedings of International Conference on World Wide Web,", "citeRegEx": "Liu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "A survey of opinion mining and sentiment analysis, Mining Text Data, 415463", "author": ["B. Liu", "L. Zhang"], "venue": null, "citeRegEx": "Liu and Zhang,? \\Q2012\\E", "shortCiteRegEx": "Liu and Zhang", "year": 2012}, {"title": "Machine Learning, second edition", "author": ["T. Mitchell"], "venue": null, "citeRegEx": "Mitchell,? \\Q1997\\E", "shortCiteRegEx": "Mitchell", "year": 1997}, {"title": "Document-level sentiment classification: an empirical comparison between SVM and ANN, Expert Systems with Applications, 40(2):621-633", "author": ["R. Moraes", "J.F. Valiati", "W.P. Gavi\u00e3o Neto"], "venue": null, "citeRegEx": "Moraes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Moraes et al\\.", "year": 2012}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "Vaithyanathan S"], "venue": "Proceedings of the ACL-02 Conference on Empirical methods in Natural Language Processing,", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Text categorization method based on improved mutual information and characteristic weights evaluation algorithms", "author": ["Z. Pei", "X. Shi", "M. Marchese", "Y. Liang"], "venue": "Proceedings of Fourth International Conference on Fuzzy Systems and Knowledge Discovery,", "citeRegEx": "Pei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2007}, {"title": "Extracting product features and opinions from reviews, Proceedings of Conference on Empirical Methods in Natural language processing and text mining", "author": ["A.M. Popescu", "O. Etzioni"], "venue": null, "citeRegEx": "Popescu and Etzioni,? \\Q2007\\E", "shortCiteRegEx": "Popescu and Etzioni", "year": 2007}, {"title": "Opinion word expansion and target extraction through double propagation", "author": ["G. Qiu", "B. Liu", "J. Bu", "C. Chen"], "venue": "Computational Linguistics,", "citeRegEx": "Qiu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2011}, {"title": "Improving multi-class text classification with Naive Bayes, Massachusetts institute of technology, artificial intelligence laboratory, AI Technical Report", "author": ["J. Rennie"], "venue": null, "citeRegEx": "Rennie,? \\Q2004\\E", "shortCiteRegEx": "Rennie", "year": 2004}, {"title": "Feature selection methods in Persian sentiment analysis, Proceeding of Natural Language Processing and Information Systems", "author": ["M. Saraee", "A. Bagheri"], "venue": null, "citeRegEx": "Saraee and Bagheri,? \\Q2013\\E", "shortCiteRegEx": "Saraee and Bagheri", "year": 2013}, {"title": "A non-parametric LDA-based induction method for sentiment analysis", "author": ["M. Shams", "A. Shakery", "H. Faili"], "venue": "Proceedings of 16th IEEE CSI International Symposium on Artificial Intelligence and Signal Processing,", "citeRegEx": "Shams et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shams et al\\.", "year": 2012}, {"title": "A stemming algorithm for the Farsi language", "author": ["K. Taghva", "R. Beckley", "M. Sadeh"], "venue": "Proceedings of IEEE International Conference on Information Technology: Coding and Computing, ITCC", "citeRegEx": "Taghva et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taghva et al\\.", "year": 2005}, {"title": "Comparing and combining dimension reduction techniques for efficient text clustering", "author": ["B. Tang", "M. Shepherd", "E. Milios", "M.I. Heywood"], "venue": "Proceedings of International Workshop on Feature Selection for Data Mining", "citeRegEx": "Tang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2005}, {"title": "Unsupervised learning of semantic orientation from a hundredbillion-word corpus, Technical Report EGB-1094", "author": ["P.D. Turney", "M.L. Littman"], "venue": null, "citeRegEx": "Turney and Littman,? \\Q2002\\E", "shortCiteRegEx": "Turney and Littman", "year": 2002}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "Proceedings of 14th International Conference on Machine Learning ICML,", "citeRegEx": "Yang and Pedersen,? \\Q1997\\E", "shortCiteRegEx": "Yang and Pedersen", "year": 1997}, {"title": "An evaluation of statistical approaches to text categorization", "author": ["Y. Yang"], "venue": "Information retrieval,", "citeRegEx": "Yang,? \\Q1999\\E", "shortCiteRegEx": "Yang", "year": 1999}, {"title": "Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques", "author": ["J. Yi", "T. Nasukawa", "R. Bunescu", "W. Niblack"], "venue": "Proceedings of the 3rd IEEE International Conference on Data Mining,", "citeRegEx": "Yi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2003}, {"title": "Applying of sentiment analysis for texts in Russian based on machine learning approach", "author": ["N. Yussupova", "D. Bogdanova", "M. Boyko"], "venue": "Proceedings of Second International Conference on Advances in Information Mining and Management,", "citeRegEx": "Yussupova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yussupova et al\\.", "year": 2012}, {"title": "Sentiment classification using genetic algorithm and Conditional Random Fields", "author": ["J. Zhu", "H. Wang", "J.T. Mao"], "venue": "Proceedings of 2nd IEEE International. Conference on Information Management and Engineering,", "citeRegEx": "Zhu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}, {"title": "Aspect-based opinion polling from customer reviews", "author": ["J. Zhu", "H. Wang", "M. Zhu", "B.K. Tsou", "M. Ma"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers (Bagheri, Saraee, and de Jong, 2013a) (Hu and Liu, 2004a) (Hu and Liu, 2004b) (Liu and Zhang, 2012).", "startOffset": 301, "endOffset": 322}, {"referenceID": 8, "context": "Up to now studies in sentiment analysis have covered a wide range of tasks, including sentiment classification on word, phrase, sentence and document level (Cui, Mittal, and Datar, 2006) (Gamon, 2004) (Moraes, Valiati, and Gavi\u00e3o Neto, 2012) (Pang, Lee, and Vaithyanathan, 2002) (Saraee and Bagheri, 2013) (Yussupova, Bogdanova, and Boyko, 2012) (Zhu, Wang, and Mao, 2012), opinion target identification or aspect detection (Bagheri, Saraee, and de Jong, 2013a) (Bagheri, Saraee, and de Jong, 2013b) (Hu and Liu, 2004a) (Liu and Zhang, 2012) (Popescu and Etzioni, 2005) (Qiu, Liu, Bu et al.", "startOffset": 187, "endOffset": 200}, {"referenceID": 22, "context": "Up to now studies in sentiment analysis have covered a wide range of tasks, including sentiment classification on word, phrase, sentence and document level (Cui, Mittal, and Datar, 2006) (Gamon, 2004) (Moraes, Valiati, and Gavi\u00e3o Neto, 2012) (Pang, Lee, and Vaithyanathan, 2002) (Saraee and Bagheri, 2013) (Yussupova, Bogdanova, and Boyko, 2012) (Zhu, Wang, and Mao, 2012), opinion target identification or aspect detection (Bagheri, Saraee, and de Jong, 2013a) (Bagheri, Saraee, and de Jong, 2013b) (Hu and Liu, 2004a) (Liu and Zhang, 2012) (Popescu and Etzioni, 2005) (Qiu, Liu, Bu et al.", "startOffset": 279, "endOffset": 305}, {"referenceID": 14, "context": "Up to now studies in sentiment analysis have covered a wide range of tasks, including sentiment classification on word, phrase, sentence and document level (Cui, Mittal, and Datar, 2006) (Gamon, 2004) (Moraes, Valiati, and Gavi\u00e3o Neto, 2012) (Pang, Lee, and Vaithyanathan, 2002) (Saraee and Bagheri, 2013) (Yussupova, Bogdanova, and Boyko, 2012) (Zhu, Wang, and Mao, 2012), opinion target identification or aspect detection (Bagheri, Saraee, and de Jong, 2013a) (Bagheri, Saraee, and de Jong, 2013b) (Hu and Liu, 2004a) (Liu and Zhang, 2012) (Popescu and Etzioni, 2005) (Qiu, Liu, Bu et al.", "startOffset": 520, "endOffset": 541}, {"referenceID": 14, "context": "2004b) (Liu, Hu, Cheng, 2005) (Liu and Zhang, 2012) (Turney, Littman, 2002).", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": "Up to now, many researches have been conducted on English, Chinese or Russian document sentiment analysis or classification (Liu and Zhang, 2012) (Qiu, Liu, Bu et al.", "startOffset": 124, "endOffset": 145}, {"referenceID": 22, "context": "However on Persian text, in our knowledge there is little investigation conducted on sentiment analysis (Saraee and Bagheri, 2013) (Shams, Shakery, and Faili, 2012).", "startOffset": 104, "endOffset": 130}, {"referenceID": 22, "context": "Different writing styles produce many challenges about Persian text processing such as, informal and colloquial words, declensional suffixes, multiple types of writing for a word and word spacing (Farhoodi and Yari, 2010) (Saraee and Bagheri, 2013) (Shams, Shakery, and Faili, 2012) (Taghva, Beckley, and Sadeh, 2005).", "startOffset": 222, "endOffset": 248}, {"referenceID": 15, "context": ", 2006) (Mitchell, 1997) (Pei, Shi, Marchese et al.", "startOffset": 8, "endOffset": 24}, {"referenceID": 21, "context": "2007) (Rennie, 2004) (Tang, Shepherd, Milios et al.", "startOffset": 6, "endOffset": 20}, {"referenceID": 14, "context": "Researchers have investigated the effectiveness of different machine learning algorithms using various features to develop sentiment classifiers on languages like English or Chinese (Liu and Zhang, 2012) (Qiu, Liu, Bu et al.", "startOffset": 182, "endOffset": 203}, {"referenceID": 22, "context": "Besides our study (Saraee and Bagheri, 2013) through our energies on searching the web, only one people\u2019s work can be found, i.", "startOffset": 18, "endOffset": 44}, {"referenceID": 8, "context": "Gamon (Gamon, 2004) and Yi et al.", "startOffset": 6, "endOffset": 19}, {"referenceID": 6, "context": "Duric and Song (Duric and Song, 2012) approached the task of feature selection for sentiment analysis by using a Content and Syntax model, HMM-LDA, to separate the entities in a review document from the potentially sentiment carrying modifiers.", "startOffset": 15, "endOffset": 37}, {"referenceID": 15, "context": "Naive Bayes Algorithm We used the MAP (Maximum a Posteriori) Naive Bayes algorithm in our experiments as a classifier for Persian sentiment classification system, because Naive Bayes is a kind of important classification algorithm and it has a high speed and is easy to implement rather than approaches like SVM or neural networks (Mitchell, 1997) (Rennie, 2004).", "startOffset": 331, "endOffset": 347}, {"referenceID": 21, "context": "Naive Bayes Algorithm We used the MAP (Maximum a Posteriori) Naive Bayes algorithm in our experiments as a classifier for Persian sentiment classification system, because Naive Bayes is a kind of important classification algorithm and it has a high speed and is easy to implement rather than approaches like SVM or neural networks (Mitchell, 1997) (Rennie, 2004).", "startOffset": 348, "endOffset": 362}, {"referenceID": 27, "context": "The measures are Document Frequency (DF), Term Frequency Variance (TFV), Mutual Information (MI) (Yang and Pedersen, 1997) and Modified Mutual Information (MMI).", "startOffset": 97, "endOffset": 122}, {"referenceID": 22, "context": "Data To test our methods we compiled a dataset of 1020 online customer reviews in Persian language from different brands of cell phone products including Nokia, Apple, Samsung, Sony, LG, Motorola, Huawei and HTC (Saraee and Bagheri, 2013).", "startOffset": 212, "endOffset": 238}, {"referenceID": 28, "context": "To deal with multiple datasets (products), we adopt the macro and micro average (Yang, 1999) to assess the overall performance.", "startOffset": 80, "endOffset": 92}], "year": 2014, "abstractText": "In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers. Sentiment analysis aims to use automated tools to detect subjective information from reviews. One of the main challenges in sentiment analysis is feature selection. Feature selection is widely used as the first stage of analysis and classification tasks to reduce the dimension of problem, and improve speed by the elimination of irrelevant and redundant features. Up to now as there are few researches conducted on feature selection in sentiment analysis, there are very rare works for Persian sentiment analysis. This paper considers the problem of sentiment classification using different feature selection methods for online customer reviews in Persian language. Three of the challenges of Persian text are using of a wide variety of declensional suffixes, different word spacing and many informal or colloquial words. In this paper we study these challenges by proposing a model for sentiment classification of Persian review documents. The proposed model is based on lemmatization and feature selection and is employed Naive Bayes algorithm for classification. We evaluate the performance of the model on a manually gathered collection of cellphone reviews, where the results show the effectiveness of the proposed approaches.", "creator": "ABBYY PDF Transformer 2.0"}}}