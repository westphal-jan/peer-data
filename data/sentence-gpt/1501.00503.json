{"id": "1501.00503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jan-2015", "title": "An Empirical Study of the L2-Boost technique with Echo State Networks", "abstract": "A particular case of Recurrent Neural Network (RNN) was introduced at the beginning of the 2000s under the name of Echo State Networks (ESNs). The ESN model overcomes the limitations during the training of the RNNs while introducing no significant disadvantages. Although the model presents some well-identified drawbacks when the parameters are not well initialised, the model demonstrates that the model is too complex to allow for meaningful regression analysis.\n\n\n\n\nThe model has been identified in two different scenarios, one with a large number of data from various sources:\nRecurrent Neural Network (RENN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRecurrent Neural Network (RNN)\nRec", "histories": [["v1", "Fri, 2 Jan 2015 21:15:00 GMT  (97kb,D)", "http://arxiv.org/abs/1501.00503v1", "To appear in Journal of Network and Innovative Computing, Volume 2, Issue 1, pp. 120 - 127, 2014"]], "COMMENTS": "To appear in Journal of Network and Innovative Computing, Volume 2, Issue 1, pp. 120 - 127, 2014", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sebasti\\'an basterrech"], "accepted": false, "id": "1501.00503"}, "pdf": {"name": "1501.00503.pdf", "metadata": {"source": "CRF", "title": "An Empirical Study of the L2-Boost technique with Echo State Networks", "authors": ["Sebasti\u00e1n Basterrech"], "emails": ["Sebastian.Basterrech.Tiscordio@vsb.cz"], "sections": [{"heading": null, "text": "In this work, we investigate the performance of a specific boosting technique (called L2-Boost) with ESNs as single predictors. The L2-Boost technique has been shown to be an effective tool to combine \u201cweak\u201d predictors in regression problems. In this study, we use an ensemble of random initialized ESNs (without control their parameters) as \u201cweak\u201d predictors of the boosting procedure. We evaluate our approach on five well-know time-series benchmark problems. Additionally, we compare this technique with a baseline approach that consists of averaging the prediction of an ensemble of ESNs.\nKeywords-L2-boosting, Echo State Network, Time-series modeling, Reservoir Computing, Ensemble Methods\nI. INTRODUCTION\nBoosting is a general procedure for improving the accuracy of an ensemble of methods. It has been successful used in supervised learning problems since its apparition in the 1990s [1]\u2013[3]. Several variations of the original Boosting idea have been introduced over the years [4], [5], one of the most popular is called AdaBoost [3]. At the beginning, Boosting was used in problems where the output features were label or discrete responses (classification problems). An analogy between AdaBoost and additive models was studied in [4]. This connection was essential for the extension of Boosting for solving problems where the output features are continuous variables (regression problems). B\u00fchlmann et al. developed a variation of the Boosting technique called L2-Boost that is constructed from an additive model and the functional gradient descent method [5].\nSince the early 2000s, a computational paradigm called Reservoir Computing (RC) has gained prominence in the Neural Computation community. In a RC model there are two well-separated concepts: a dynamical system and a memoryless function. The purpose of the dynamical system is to\nencode the spatio-temporal information of the input patterns into a spatial representation. At each time this dynamical system is characterized by its state that is called reservoir in the RC literature. This non-linear transformation is most often realized by a Recurrent Neural Network (RNN) with a large pool of interconnected neurons. A distinctive principle of a RC model is that the parameters of the dynamical system (the RNN weights) do not participate in the training process. That is, once the reservoir parameters are initialized, they remain fixed during the training process. Another part of the model is a memory-less supervised learning tool called readout structure. This part is designed to be robust and fast in the learning process.\nThe RC models has been applied in the neuroscience area for processing cognitive information in the neural system. [6]. Furthermore, they have proven to be extremely effective tools for time-series problems in the area of Machine Learning. For instance, as far as we know one of the most popular RC models the Echo State Networks (ESN) [7], has the best known learning performance on the Mackey-Glass time-series prediction problem [8], [9]. In this article, we will concentrate in the ESN model for solving time-series problems. The reservoir in the ESN model is composed by a RNN with sigmoid neurons, and the readout part of the model is a linear regression. The weight connection between neurons in the reservoir are collected in a matrix that we will call reservoir matrix. The main global parameters of the ESN model are: the input scaling factor, the spectral radius of the reservoir matrix and the pattern of connectivity among the reservoir units. The setting of these parameters often requires the human expertise and several empirical trials [10]. As a consequence, the setting procedure can be expensive in computational time. For instance, the time complexity of an algorithm that computes the spectral radius of a N \u00d7N matrix is equal to O(N4) [11].\nThe goal of this article is to investigate the performance of an automatic procedure to combine single weak ESNs and the L2-Boost technique. We develop an automatic technique based on L2-Boost, which combines the prediction of several random initialized ESNs in order to produce a highly accurate tool. We use the term weak ESN for an ESN without checking and computing the spectral radius. We use the terminology weak due to the fact that this particular ESN is not optimal.\nar X\niv :1\n50 1.\n00 50\n3v 1\n[ cs\n.L G\n] 2\nJ an\n2 01\nThe main advantages of the procedure presented in this article are:\n\u2022 Descend the computational effort. In order to gain in the computational effort, the approach consists of combining weak ESNs. The procedure avoids to tune the reservoir parameters, which can often have a high computational cost. It uses only a uniform random initialization of the weights. Note that, there are not a control of the spectral radius, then some single weak ESNs can have unstable dynamics. \u2022 The procedure is automatic. The procedure does not require external human expertise for setting the model parameters, and for evaluating the model performance. \u2022 The technique has a new parameter used for overfitting control. This parameter which we will that comes from the L2-Boost technique.\nWe present empirical results of the procedure introduced in this paper on a wide range of benchmark problems. We compare these performances with the accuracy obtained by a single ESN. Furthermore, we realize a comparison with the accuracy of a baseline approach that computes the average among single ESN models. Each single ESN is independently initialized and adjusted during the learning process. There are empirical evidence in the Machine Learning literature that show that this baseline approach sometimes performs better than other ensemble methods [12].\nThis work is a revised and expanded version of the article [13].\nThe structure of this article is organized as follows. In Section II, we start with a specification of supervised learning problems with temporal data. Next, we present an overview about the family of additive models. Subsection II-C introduces the L2-Boost technique. In Section III, we present the Reservoir Computing paradigm. Particularly, we focus on the Echo State Network model in III-A. In Subsection III-B is presented a formalization of the procedure introduced in this article. Section IV describes the empirical results. This section starts with a description of the benchmark problems. Next, we present the reached results. Finally, last section provides conclusions and future work."}, {"heading": "II. BACKGROUND", "text": "In this Section, we start specifying the context where the ESN model and the L2-Boost technique are applied. Next, we present the additive models and we introduce a description of the L2-Boost technique."}, {"heading": "A. Problem Specification", "text": "We begin specifying a supervised learning problem. Given a data set L = {(x(t),y(t)) : t = 1, . . . , T} where the points x and y are either a class or a numerical response. We denote by Nx the dimension of the input vector x, and Ny the dimension of the output vector y. We suppose that the mapping between the input x and the output y is given by certain unknown function F (\u00b7). The goal consists in learning a parametric function F\u0302 (x(t),L) such that certain error distance between F\u0302 (x(t),L) and y(t) is minimized for all\nt. The problem is called regression problem when the learning set has output numerical variables. Otherwise, it is named classification problem. In the case of regression problems, it is recommended to use the a quadratic distance [14]. Even though we can also use a quadratic distance in classification problems, it is recommendable to use the Kullback-Leibler distance in this domain [14].\nAn ESN model is mainly used for solving supervised learning tasks, wherein the data set presents temporal dependencies. Although, it can be also used for non-temporal supervised learning problems [9]. In this article we will concentrate only in temporal learning tasks with real output variables (y(t) \u2208 RNY , for all t). In this work, we perform the models using a standard discrete time. We want to forecast some aspect of the output feature y at time t+k, using some aspect of the information available at current time t, that is given the collection ((x(t),y(t)), (x(t\u22121),y(t\u22121)), (x(t\u22122),y(t\u22122)), . . .) we would like to predict the value y(t+k) (k > 0) [15]. In this case, the goal consists in estimating a mapping F\u0302 (\u00b7) for predicting y(t+k) for some k > 0, such that some distance between F\u0302 (\u00b7) and y is minimized."}, {"heading": "B. Additive Models", "text": "In [4] was analyzed the Boosting model under the form of an Additive model. Given a set of functions f (m) : RNX \u2192 RNY , m = 1 . . . ,M characterized by a set of parameters \u03b8 and expansion coefficients \u03b2,\nf (m)(x) = \u03b2(m)h(x, \u03b8(m)),\nan additive model has the following form\nF (x) = M\u2211 m=1 f (m)(x). (1)\nThe functions {h(x; \u03b8)}M1 are named basis functions. They are not fixed a priori and are selected depending of the cost function used and the data set. An important parameter of the model is the number of basis functions (M ) considered in the expression (1). This parameter controls the generalization error of the model. Since the main goal in a learning task is to find a predictor with low generalization error, the parameter M has an important role in the accuracy of an additive model.\nC. The L2-Boost Procedure\nA relationship between the gradient descent technique and stage-wise additive expansions was introduced at the beginning of the 2000s [16]. The introduction of the gradient descent algorithm using a boosting approach was an essential contribution in the field of ensemble learning methods [16]. It allowed to start to use boosting in regression problems [5]. A Boost method for regression problems with quadratic error distance was introduced under the name of L2-Boost in [5]. We present the L2-Boost technique in Algorithm 1. Other boosting variants were presented for other kind of distances, some of them are described in [4], [5], [16].\nWe refer by epoch to the iteration of the training algorithm through all the patterns in the training set [17]. At each epoch\nm+ 1, the basis function h(m+1)(\u00b7, \u03b8) is fitted to the current residuals: y(i) \u2212 F\u0302 (m)(x(i)), for all i. Unlike other boosting techniques such as Adaboost, L2-Boost does not present any re-weighting. Another difference between L2-Boost and other boosting methods is that L2-Boost presents a tendency to overfit the data [5]. The model with contracting linear learners converge to the fully saturated model [5]. Each boosting epoch contributes to additional overfitting, thus the selection of the weak learners and the parameter M is an essential task for this device. In practice, few boosting iterations are enough to achieve good performances avoiding the overfitting phenomena.\nAlgorithm 1 The L2-Boost algorithm. Require: L, M , h(x, \u03b8)\nFit an initial model using a least squares fit (see [18]): F\u0302 (0)(\u00b7) = h(\u00b7, \u03b8); for (m = 1, . . .M) do\nCompute the residuals for all pattern i: e(i) = y(i) \u2212 F\u0302 (m)(x(i)); Fit the model f\u0302 (m+1)(\u00b7) parametrized as f\u0302 (m+1)(x) = h(x, \u03b8) to the current residuals e using the least squares fit; Update: F\u0302 (m+1)(\u00b7) = F\u0302 (m)(\u00b7) + f\u0302 (m+1)(\u00b7);\nend for Return the F\u0302 (M)(\u00b7) function;"}, {"heading": "III. MODELING TIME-SERIES WITH ECHO STATE NETWORKS", "text": "The Recurrent Neural Networks (RNNs) are powerful tools for solving time-series benchmarks. They are computational methods that operate in time. Considering terminology of graphs, in a RNN at least one circuit is presented in its topology. The circuits of the network enable to store temporal information, in order to learn and memorize the input history [9]. Each circuit creates an internal state which makes the recurrent network a discrete time state-space model. At each time, the RNN receives an input pattern. Next, the network updates its hidden state via a non-linear activation function using the input pattern and the network state at the precedent time [19]. There are a general consensus in the community that considers the RNN as powerful tool for forecasting and time-series prediction.\nIn spite of that, in practice the model presents some drawbacks. The most important is that is hard to train a RNN using gradient descent methods [20]. The training methods that use the first differential information have often stability problems and high numerical complexity. As a consequence, much longer training times are necessary to adjust the network weights. In [20] is analyzed the main limitations of the algorithms of the gradient descent type for training RNNs. These drawbacks are identified under the names of vanishing and the exploding gradient problems [20]. The vanishing gradient phenomena occurs when the norm of the gradient decreases arbitrarily fast to 0. The exploding gradient phenomena refers\nto the opposite, when the gradient norm large increases during the training process [21]. Recently, an effective algorithm to train RNN was introduced [19], the algorithm uses the Hessian-free Optimization for setting the network parameters.\nReservoir Computing (RC) models appear as a good alternative for RNNs. The two pioneering RC models are Echo State Network (ESN) [7] and Liquid State Machine (LSM) [22]. This computational paradigm covers the main limitations related to learning processes in RNNs obtaining acceptable performance in practical applications [9]. In a RC model there are at least two well-differentiated structures: a dynamical system called reservoir and another one called readout. The readout is a supervised learning tool for training with non-temporal data. For example: feedforward neural network, linear regression, decision trees, etc. A main characteristic of a RC model is that the weights involved in circuits are deemed fixed during the learning process. Thus, the matrix with the weight between reservoir units (reservoir matrix) is initialized in an arbitrary way and it remains unchanged during the learning process. The training algorithm is restricted to update the weights in the readout structure. Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on."}, {"heading": "A. Formalization of the Echo State Network Model", "text": "In this work related to the L2-Boost technique and the RC methods, we only study the L2-Boost with the ESN model. An ESN reservoir is a RNN from an input space RNx into a larger space RNs with Nx Ns. The connection between input and hidden neurons are collected in a Ns \u00d7Nx weight matrix win. The connections among the hidden neurons are represented by a Ns\u00d7Ns weight matrix wr. A Ny\u00d7Ns weight matrix wout represents the readout weights. At any time t, the information from the input pattern and the past is represented in a state vector\ns(t) = tanh(winx(t) +wrs(t\u22121)). (2)\nAt any time t, the output prediction y(t) \u2208 RNy is generated using the input pattern and the reservoir state information. Most often is computed using a linear regression:\ny(t) = wout[x(t)|s(t)], (3)\nwhere \u00b7|\u00b7 is the vertical concatenation of the vectors. For the sake of the notation simplicity, we omit the bias term, a constant term is included in all the regressions.\nIn [7] was analyzed the stability of the reservoir dynamics in the ESN model. Under certain algebraic conditions the reservoir state only depends (asymptotically) of the inputs and the network topology. It becomes independent of its initial conditions [9]. These conditions were summarized in the Echo State Property (ESP) [7]. In practice, the stability of the ESN is almost always ensured when the spectral radius of the reservoir matrix is less than 1 [9], [28]. As a\nconsequence, the reservoir weights are appropriately scaled in order to have a spectral radius less than 1. To scale the parameters is necessary to compute the spectral radius of the reservoir matrix. The computation of the spectra requires an important computational effort [11]. Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].\nB. L2-Boost Using the ESN Model for Time-series Processing Information\nIn this article, we investigate the performance of using L2Boost in temporal learning tasks, and we consider as weak learner predictors a set of ESNs with random initialization. Given an arbitrary parameter M the procedure is as follows. We initialize an ESN in a random way. The initialization consists in selecting the size of the network as well as the pattern of connectivity. We consider a reservoir with fixed sparse connections. We do not control the spectrum norm of the reservoir weight. A guide about the initialization procedure can be seen from [10]. We expand the input information using the ESN reservoir given by the expression (2), thus we obtain s(t), \u2200t. Next, we apply Algorithm 1. Finally, we obtain predictor F (M)(\u00b7). The approach is summarized in Algorithm 2. In our experiments we use ridge linear regression for computing the readout weights wout [28].\nAlgorithm 2 The L2-Boost with the ESN model. Initialize an ESN following the comments in Subsection III-B; Compute the temporal expansion of L using (2); Generate the set {(s(t),y(t)),\u2200t}; Apply the Algorithm 1; Return F\u0302 (M)(\u00b7);\nIn order to evaluate the performance of this procedure, we compare the reached accuracy of the L2-Boost technique with a simple baseline approach [17]. The baseline approach consists in combining K single predictors (in our case the learning predictors are ESNs). We consider random initialized reservoirs, without control of the reservoir spectrum norm. In the baseline method, we train independently each of these single ESNs. The final prediction is the average among the single predictions.\nFor statistical comparisons between the methods we consider K = 30. We remark again that we do not scale the reservoir weights for obtaining the ESP. Even though some ESN models can present good accuracy, other ones can be weak predictors. Additionally, we compare our performances with the performance obtained when single ESNs with \u201cgood\u201d tuning of the reservoir parameters are used. For that, we use the results presented in the RC literature."}, {"heading": "IV. EMPIRICAL RESULTS", "text": "We begin this section describing the benchmark problems. Next, we specify the experimental setup. We concludes this section with an analysis of our empirical results."}, {"heading": "A. Description of the Benchmark Problems", "text": "We use the following range of time-series benchmarks: \u2022 Fixed kth order NARMA. This data set presents a high\nnon-linearity and is widely used in the RC literature. We generate the NARMA serie following the description in [28], [32],\nb(t+ 1) = \u03b11(t) + \u03b12b(t) k\u22121\u2211 i=0 b(t\u2212 i)\n+\u03b13s(t\u2212 (k \u2212 1))s(t) + \u03b14, where s(t) \u223c Unif [0, 0.5] and the constants values are shown in Table I. In order to evaluate the memorization ability of the model, we consider two simulated series when k = 10 and k = 30. The task consists to predict\nk \u03b11 \u03b12 \u03b13 \u03b14 10 0.3 0.05 1.5 0.1 30 0.2 0.004 1.5 0.001"}, {"heading": "B. Experimental Setup", "text": "We summarize the setting of the main parameters related to the benchmark problems in Table II. The table presents the initial washout period, the regularization parameter (\u03b3) of the linear ridge regression, and the number of train and test samples for each benchmark problem.\nThe benchmarks selected have been widely used in the RC literature [7], [28], [31], [36]. In all cases, we use the Normalized Mean Square Error (NMSE) as measure of accuracy model [9]. The learning method used for computing the output weight matrix wout was the offline ridge regression. This algorithm has a regularization parameter \u03b3 that we adjust it for each benchmark problem. The pre-processing data step consisted in normalizing the patterns in the interval [0, 1] We investigated the algorithm performance for several reservoir sizes. The range of the reservoir size values is specified for each benchmark problem. The connection between the input and reservoir layer is fully connected with random weights in [\u22120.2, 0.2]. The reservoir matrix is initialized using Uniform distribution in [\u22120.8, 0.8]."}, {"heading": "C. Result Analysis", "text": "Table III shows results reported in the RC literature for these benchmarks when a single ESN model was used as model predictor. Table IV presents the train set accuracy reached on the Henon Map data set. The columns 2, 3 and 4 show the NMSE reached with L2-Boost with ESNs for M epochs (M = 3, 4 and 5), respectively. Column 5 of Table III shows the accuracy of the baseline approach, that it averaging the prediction of 30 ESNs. The columns of the table are written using a scientific notation.\nTable IV illustrates the accuracy of the models during the training. The NMSE corresponds to the training data of the Henon Map data set. We present this table in order to illustrate the tendency of overfitting of L2-Boost with ESN. The additive model converge very fast to the solution, for this reason the\ncolumns 3 and 4 of Table IV are very similar. The model with larger M performs better over the train data, but it has problems of generalization. We found this characteristic in all benchmarks. As a consequence, we can affirm that the parameter M has a relevant impact in the control of the overfitting phenomenon. We can found a similar remarks for the L2-Boost technique in non-temporal learning tasks [5].\nFigure 2 illustrates the NMSE reached according the reservoir size for different M values for the 30th NARMA data set. This figure shows the training error, we can see the evolution of the NMSE versus the size of the reservoir. We present few values of reservoir size between 6 till 11. Figures 1 and 3 show the NMSE of the test data versus the reservoir size for the 10th and 30th order NARMA data set, respectively. These figures show 4 curves, the black one (with points represented by dots) corresponds to the baseline method which combines several single ESNs. The other curves correspond to the L2Boost-ESN with different number of epochs M = 6, 8 and M = 10. We can not affirm that the procedure of L2-Boost with single weak ESNs performs better than optimal single ESNs. The accuracy it is also of the same order that results presented in the RC literature using a single well-initialized ESN [28], [31].\nFigure 4 illustrates the evolution of the NMSE for the reservoir size of the test data of the Santa Fe Laser benchmark. The error was computed for the L2-Boost with ESNs for M = 4, 6 and M = 8 and the baseline approach averaging 30 ESNs. Figure 5 shows the accuracy reached for the models on the Freedman test data set. The graphic shows the evolution of the L2-Boost with ESNs for M = 4, 5 and M = 5 and the baseline approach. In all graphics, we can see that when the reservoir increases its size the procedure L2-Boost with ESNs and the baseline approach decrease their test error. This behavior about the impact of the reservoir size on the accuracy of the model, also happens with single ESNs [10], [28], [31]."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "At the beginning of the 2000s, an efficient technique to train and design a RNN was developed under the name of Echo State Network (ESN). This approach overcome the limitations to train RNN using the gradient descent method. The performance of an ESN is highly dependent on its parameters and pattern of connectivity of the hidden-hidden weights Besides, the network setting can be computational expensive, in particular to compute the spectral radius of the hidden-hidden weight matrix.\nIn this article, we investigated boosting ideas with ESNs, in order to built a robust new learning tool. In particular, we studied the utilization of L2-Boost with random initialized ESNs. We merge a set of weak single ESNs. We call weak ESNs because they are random initialized, and we do not use extra computational effort for tuning the initial hidden-hidden weights.\nIn spite of the realization of numerous tests, we can not affirm that L2-Boost with ESNs performs better than a single well-initialized ESN (according the results presented in the RC literature). However, the main advantage of the L2-Boost with weak ESNs is that the procedure is automatic and does not require the computational effort of computing the spectra of the hidden-hidden weight matrix. Additionally, the procedure has a control parameter for the overfitting phenomena.\nIn a future work we will test the model using another technique for decrease the generalization error, as well as on a more number of benchmark problems. Additionally, we can\ntest the approach using another supervised learning tool for the readout structure."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported within the framework of the IT4Innovations Centre of Excellence project, reg. no. CZ.1.05/1.1.00/02.0070 supported by Operational Programme \u2019Research and Development for Innovations\u2019 funded by Structural Funds of the European Union and state budget of the Czech Republic. Additionally, this article has been elaborated in the framework of the project New creative teams in priorities of scientific research, reg. no. CZ.1.07/2.3.00/30.0055."}], "references": [{"title": "The strength of weak learnability", "author": ["R.E. Shapire"], "venue": "Machine Learning, vol. 5, no. 2, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "AT&T Bell Laboratories, New Jersey, USA, Tech. Rep., January 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Experiments with a new Boosting Algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Machine Learning: Proceedings of Thirteenth International Conference, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Additive Logistic Regression: a Statistical View of Boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "The Annals of Statistics, vol. 28, no. 2, pp. 337\u2013407, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Boosting with the L2 loss: Regression and Classification", "author": ["P. B\u00fchlmann", "B. Yu"], "venue": "Journal of the American Statistical Association, vol. 98, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Liquid State Machines: Motivation, Theory, and Applications", "author": ["W. Maass"], "venue": "In Computability in Context: Computation and Logic in the Real World. Imperial College Press, 2010, pp. 275\u2013296.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks", "author": ["H. Jaeger"], "venue": "German National Research Center for Information Technology, Tech. Rep. 148, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Training Recurrent Networks by Evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F. Gomez"], "venue": "Neural Networks, vol. 19, pp. 757\u2013 779, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Reservoir Computing Approaches to Recurrent Neural Network Training", "author": ["M. Lukos\u0306evic\u0306ius", "H. Jaeger"], "venue": "Computer Science Review, pp. 127\u2013149, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A Practical Guide to Applying Echo State Networks", "author": ["M. Luko\u0161evi\u010dius"], "venue": "Neural Networks: Tricks of the Trade, ser. Lecture Notes in Computer Science, G. Montavon, G. Orr, and K.-R. M\u00fcller, Eds. Springer Berlin Heidelberg, 2012, vol. 7700, pp. 659\u2013686. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-35289-8_36", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "An Approach to Reservoir Computing Design and Training", "author": ["A.A. Ferreira", "T.B. Ludermir", "R.R.B. De Aquino"], "venue": "Expert Syst. Appl., vol. 40, no. 10, pp. 4172\u20134182, Aug. 2013. [Online]. Available: http://dx.doi.org/10.1016/j.eswa.2013.01.029", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Popular Ensemble Methods: an Empirical study", "author": ["D. Opitz", "R. Maclin"], "venue": "Journal of Artificial Intelligence Research, vol. 11, pp. 169\u2013198, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "An Empirical Study of L2-Boost with Echo State Networks", "author": ["S. Basterrech"], "venue": "IEEE Intelligent Systems Design and Applications (ISDA), December 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The elements of Statistical Learning, ser. Spring series in statistics", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Extensions to metric-based model selection", "author": ["Y. Bengio", "N. Chapados"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1209\u20131227, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Greedy Function Approximation: A Gradient Boosting Machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, vol. 29, pp. 1189\u20131232, 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Boosting Neural Networks", "author": ["H. Schwenk", "Y. Bengio"], "venue": "Neural Computation, vol. 12, no. 8, pp. 1869\u20131887, Aug. 2000.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1869}, {"title": "Numerical Recipes in C, 2nd ed", "author": ["W. Press", "S. Teukolsky", "W. Vetterling", "B. Flannery"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Learning Recurrent Neural Networks with Hessian-Free Optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceeding of the 28th International Conference on Machine Learning, 2011, pp. 1033\u20131040.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "Proceedings of the 30th International Conference on Machine Learning, vol. 28, pp. 37\u201348, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time computing without stable states: a new framework for a neural computation based on perturbations", "author": ["W. Maass", "T. Natschl\u00e4ger", "H. Markram"], "venue": "Neural Computation, pp. 2531\u20132560, november 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Backpropagation-Decorrelation: online recurrent learning with O(N) complexity", "author": ["J.J. Steil"], "venue": "Proceedings of IJCNN\u201904, vol. 1, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimization and applications of Echo State Networks with leaky-integrator neurons", "author": ["H. Jaeger", "M. Lukos\u0306evic\u0306ius", "D. Popovici", "U. Siewert"], "venue": "Neural Networks, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving Reservoirs using Intrinsic Plasticity", "author": ["B. Schrauwen", "M. Wardermann", "D. Verstraeten", "J.J. Steil", "D. Stroobandt"], "venue": "Neurocomputing, vol. 71, pp. 1159\u20131171, March 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Echo State Queueing Network: a new Reservoir Computing learning tool", "author": ["S. Basterrech", "G. Rubino"], "venue": "IEEE Consumer Comunications & Networking Conference (CCNC\u201913), January 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Reservoir Computing and Extreme Learning Machines for Non-linear Time-series Data Analysis", "author": ["J.B. Butcher", "D. Verstraeten", "B. Schrauwen", "C.R. Day", "P.W. Haycock"], "venue": "Neural Networks, vol. 38, pp. 76\u201389, feb 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimum Complexity Echo State Network", "author": ["A. Rodan", "P. Tin\u0306o"], "venue": "IEEE Transactions on Neural Networks, pp. 131\u2013144, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "On self-organizing reservoirs and their hierarchies", "author": ["M. Lukos\u0306evic\u0306ius"], "venue": "Jacobs University, Bremen, Tech. Rep. 25, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Studies on Reservoir Initialization and Dynamics Shaping in Echo State Networks", "author": ["J. Boedecker", "O. Obst", "N.M. Mayer", "M. Asada"], "venue": "Proceedings of the 17th European Symposium On Artificial Neural Networks (ESANN\u201909), Evere, Belgium, Apr. 2009, pp. 227\u2013232.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Self-Organizing Maps and Scale-Invariant Maps in Echo State Networks", "author": ["S. Basterrech", "C. Fyfe", "G. Rubino"], "venue": "Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on, nov. 2011, pp. 94\u201399.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Information dynamics based self-adaptive reservoir for delay temporal memory tasks", "author": ["S. Dasgupta", "F. Worgotter", "P. Manoonpong"], "venue": "Evolving Systems, pp. 1\u201315, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "The Santa Fe Time Series Competition Data", "author": ["U. Huebner", "N.B. Abraham", "C.O. Weiss"], "venue": "available at: http://goo.gl/6IKBb9, date of access: 12 September 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A two dimensional mapping with a strange attractor", "author": ["M. H\u00e9non"], "venue": "Commun. Math. Phys., vol. 50, pp. 69\u201377, 1976.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1976}, {"title": "Time series data library", "author": ["R. Hyndman"], "venue": "available at: http://goo.gl/PZzReR, date of access: 12 September 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "An experimental unification of reservoir computing methods", "author": ["D. Verstraeten", "B. Schrauwen", "M. D\u2019Haene", "D. Stroobandt"], "venue": "Neural Networks, no. 3, pp. 287\u2013289, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "It has been successful used in supervised learning problems since its apparition in the 1990s [1]\u2013[3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "It has been successful used in supervised learning problems since its apparition in the 1990s [1]\u2013[3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "Several variations of the original Boosting idea have been introduced over the years [4], [5], one of the most popular is called AdaBoost [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Several variations of the original Boosting idea have been introduced over the years [4], [5], one of the most popular is called AdaBoost [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "Several variations of the original Boosting idea have been introduced over the years [4], [5], one of the most popular is called AdaBoost [3].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "An analogy between AdaBoost and additive models was studied in [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "developed a variation of the Boosting technique called L2-Boost that is constructed from an additive model and the functional gradient descent method [5].", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "For instance, as far as we know one of the most popular RC models the Echo State Networks (ESN) [7], has the best known learning performance on the Mackey-Glass time-series prediction problem [8], [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 7, "context": "For instance, as far as we know one of the most popular RC models the Echo State Networks (ESN) [7], has the best known learning performance on the Mackey-Glass time-series prediction problem [8], [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 8, "context": "For instance, as far as we know one of the most popular RC models the Echo State Networks (ESN) [7], has the best known learning performance on the Mackey-Glass time-series prediction problem [8], [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 9, "context": "The setting of these parameters often requires the human expertise and several empirical trials [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "For instance, the time complexity of an algorithm that computes the spectral radius of a N \u00d7N matrix is equal to O(N) [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "There are empirical evidence in the Machine Learning literature that show that this baseline approach sometimes performs better than other ensemble methods [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "This work is a revised and expanded version of the article [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "In the case of regression problems, it is recommended to use the a quadratic distance [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "Even though we can also use a quadratic distance in classification problems, it is recommendable to use the Kullback-Leibler distance in this domain [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "Although, it can be also used for non-temporal supervised learning problems [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": ") we would like to predict the value y (k > 0) [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "In [4] was analyzed the Boosting model under the form of an Additive model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "A relationship between the gradient descent technique and stage-wise additive expansions was introduced at the beginning of the 2000s [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "The introduction of the gradient descent algorithm using a boosting approach was an essential contribution in the field of ensemble learning methods [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "It allowed to start to use boosting in regression problems [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "A Boost method for regression problems with quadratic error distance was introduced under the name of L2-Boost in [5].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Other boosting variants were presented for other kind of distances, some of them are described in [4], [5], [16].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Other boosting variants were presented for other kind of distances, some of them are described in [4], [5], [16].", "startOffset": 103, "endOffset": 106}, {"referenceID": 15, "context": "Other boosting variants were presented for other kind of distances, some of them are described in [4], [5], [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "We refer by epoch to the iteration of the training algorithm through all the patterns in the training set [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "Another difference between L2-Boost and other boosting methods is that L2-Boost presents a tendency to overfit the data [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "The model with contracting linear learners converge to the fully saturated model [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 17, "context": "Fit an initial model using a least squares fit (see [18]): F\u0302 (\u00b7) = h(\u00b7, \u03b8); for (m = 1, .", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "The circuits of the network enable to store temporal information, in order to learn and memorize the input history [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "Next, the network updates its hidden state via a non-linear activation function using the input pattern and the network state at the precedent time [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "The most important is that is hard to train a RNN using gradient descent methods [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "In [20] is analyzed the main limitations of the algorithms of the gradient descent type for training RNNs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "These drawbacks are identified under the names of vanishing and the exploding gradient problems [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "The exploding gradient phenomena refers to the opposite, when the gradient norm large increases during the training process [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 18, "context": "Recently, an effective algorithm to train RNN was introduced [19], the algorithm uses the Hessian-free Optimization for setting the network parameters.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "The two pioneering RC models are Echo State Network (ESN) [7] and Liquid State Machine (LSM) [22].", "startOffset": 58, "endOffset": 61}, {"referenceID": 21, "context": "The two pioneering RC models are Echo State Network (ESN) [7] and Liquid State Machine (LSM) [22].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "This computational paradigm covers the main limitations related to learning processes in RNNs obtaining acceptable performance in practical applications [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 22, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 171, "endOffset": 175}, {"referenceID": 23, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 222, "endOffset": 226}, {"referenceID": 7, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 236, "endOffset": 239}, {"referenceID": 24, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 262, "endOffset": 266}, {"referenceID": 25, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 297, "endOffset": 301}, {"referenceID": 26, "context": "Over the last years several kinds of dynamical systems have been used for generating the reservoir state, models include: Backpropagation-decorrelation Recurrent Learning [23], Leaky Integrator Echo State Networks studied [24], Evolino [8], Intrinsic Plasticity [25], Echo State Queueing Networks [26], Reservoir Computing and Extreme Learning [27], and so on.", "startOffset": 344, "endOffset": 348}, {"referenceID": 6, "context": "In [7] was analyzed the stability of the reservoir dynamics in the ESN model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "It becomes independent of its initial conditions [9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "These conditions were summarized in the Echo State Property (ESP) [7].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "In practice, the stability of the ESN is almost always ensured when the spectral radius of the reservoir matrix is less than 1 [9], [28].", "startOffset": 127, "endOffset": 130}, {"referenceID": 27, "context": "In practice, the stability of the ESN is almost always ensured when the spectral radius of the reservoir matrix is less than 1 [9], [28].", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "The computation of the spectra requires an important computational effort [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "Some attempts to generate a procedure for initializing the RC models were introduced in [11], [25], [29]\u2013[31].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "A guide about the initialization procedure can be seen from [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "In our experiments we use ridge linear regression for computing the readout weights w [28].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "In order to evaluate the performance of this procedure, we compare the reached accuracy of the L2-Boost technique with a simple baseline approach [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "We generate the NARMA serie following the description in [28], [32],", "startOffset": 57, "endOffset": 61}, {"referenceID": 31, "context": "We generate the NARMA serie following the description in [28], [32],", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "\u2022 The Santa Fe Laser data set [33].", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "These pulsations more or less follow the theoretical Lorenz model of a two level system [33].", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "It is a prototypical invertible map with chaotic solutions proposed in [34].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "The data is normalized in [0, 1].", "startOffset": 26, "endOffset": 32}, {"referenceID": 34, "context": "\u2022 Freedman\u2019s non linear time data set [35].", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 67, "endOffset": 70}, {"referenceID": 27, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 72, "endOffset": 76}, {"referenceID": 30, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 78, "endOffset": 82}, {"referenceID": 35, "context": "The benchmarks selected have been widely used in the RC literature [7], [28], [31], [36].", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "In all cases, we use the Normalized Mean Square Error (NMSE) as measure of accuracy model [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "The pre-processing data step consisted in normalizing the patterns in the interval [0, 1] We investigated the algorithm performance for several reservoir sizes.", "startOffset": 83, "endOffset": 89}, {"referenceID": 4, "context": "We can found a similar remarks for the L2-Boost technique in non-temporal learning tasks [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 27, "context": "The accuracy it is also of the same order that results presented in the RC literature using a single well-initialized ESN [28], [31].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "The accuracy it is also of the same order that results presented in the RC literature using a single well-initialized ESN [28], [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "This behavior about the impact of the reservoir size on the accuracy of the model, also happens with single ESNs [10], [28], [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "This behavior about the impact of the reservoir size on the accuracy of the model, also happens with single ESNs [10], [28], [31].", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "This behavior about the impact of the reservoir size on the accuracy of the model, also happens with single ESNs [10], [28], [31].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "166 (NMSE) 50 [28] 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "0425 (NMSE) 200 [28] 30TH NARMA 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "4542 (NRMSE) 100 [30] SANTA FE LASER 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 27, "context": "0184 (NMSE) 50 [28] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "00819 (NMSE) 200 [28] HENON MAP 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 27, "context": "00975 (NMSE) 50 [28] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "00868 (NMSE) 200 [28] FREEDMAN\u2019S 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "0004302 (MSE) 40 [31]", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "In the case of the Freedman\u2019s non linear time data, the reservoir initialization was done using the Scale Invariant Map method [31], and the Mean Square Error (MSE) was the error measure.", "startOffset": 127, "endOffset": 131}, {"referenceID": 29, "context": "The error measure was the Normalized Root Square Error (NRMSE) [30].", "startOffset": 63, "endOffset": 67}], "year": 2015, "abstractText": "A particular case of Recurrent Neural Network (RNN) was introduced at the beginning of the 2000s under the name of Echo State Networks (ESNs). The ESN model overcomes the limitations during the training of the RNNs while introducing no significant disadvantages. Although the model presents some well-identified drawbacks when the parameters are not well initialized. The performance of an ESN is highly dependent on its internal parameters and pattern of connectivity of the hiddenhidden weights Often, the tuning of the network parameters can be hard and can impact in the accuracy of the models. In this work, we investigate the performance of a specific boosting technique (called L2-Boost) with ESNs as single predictors. The L2-Boost technique has been shown to be an effective tool to combine \u201cweak\u201d predictors in regression problems. In this study, we use an ensemble of random initialized ESNs (without control their parameters) as \u201cweak\u201d predictors of the boosting procedure. We evaluate our approach on five well-know time-series benchmark problems. Additionally, we compare this technique with a baseline approach that consists of averaging the prediction of an ensemble of ESNs. Keywords-L2-boosting, Echo State Network, Time-series modeling, Reservoir Computing, Ensemble Methods", "creator": "LaTeX with hyperref package"}}}