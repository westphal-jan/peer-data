{"id": "1501.00687", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2015", "title": "On Enhancing The Performance Of Nearest Neighbour Classifiers Using Hassanat Distance Metric", "abstract": "We showed in this work how the Hassanat distance metric enhances the performance of the nearest neighbour classifiers. The results demonstrate the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance and Euclidian distance. Moreover, we proved that the Hassanat distance metric is invariant to data scale, noise and outliers. Throughout this work, it is clearly notable that both ENN and IINC performed very well with the distance investigated, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted from our results that there is no optimal algorithm that can solve all real-life problems perfectly; this is supported by the no-free-lunch theorem and by the non-free-nonsense fact that there is no optimal algorithm that can solve all real-life problems perfectly.\n\n\nIn sum, we have demonstrated that the Hassanat distance metric is invariant to data scale, noise and outliers. This results in a relatively low power to run at large distances, but still substantially better at running small distances. This can be considered an excellent means of controlling the size and length of the distance and other properties that may be associated with their relative magnitude. In fact, it is worth noting that the two other measurements of the HPS were very similar to that of the HPS. Our measurements of the IINC were both very similar, as they differed significantly by an order of magnitude. The results are of great interest to those interested in the measurement of distances, as the distance is not invariant to data scale. This is in line with our findings that the HPS was not invariant to data scale. However, it is now well established that the HPS, even if it were invariant to data scale, was invariant to data scale. We have demonstrated that the IINC was invariant to data scale and to data scale only when the distance of the distance and other properties of the distance and other properties of the distance were invariant to data scale. Our results provide further proof that the HPS is invariant to data scale.\nFurthermore, we showed that the IINC was invariant to data scale. However, this is also true for the IINC because it is invariant to data scale. Since the IINC is invariant to data scale, there is also a large number of assumptions regarding the data scale. The IINC has also", "histories": [["v1", "Sun, 4 Jan 2015 15:37:20 GMT  (220kb)", "http://arxiv.org/abs/1501.00687v1", "Canadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015"]], "COMMENTS": "Canadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mouhammd alkasassbeh", "ghada a altarawneh", "ahmad b a hassanat"], "accepted": false, "id": "1501.00687"}, "pdf": {"name": "1501.00687.pdf", "metadata": {"source": "CRF", "title": "ON ENHANCING THE PERFORMANCE OF NEAREST NEIGHBOUR CLASSIFIERS USING HASSANAT DISTANCE METRIC", "authors": ["Mouhammd Alkasassbeh", "Ghada A. Altarawneh", "Ahmad B. Hassanat"], "emails": ["malkasasbeh@gmail.com,", "ahmad.hassanat@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Nearest Neighbour classifier, Supervised Learning, similarity measures, metric. INTRODUCTION\nThe nearest neighbour (KNN) classifier is one of the most used and well-known approaches for performing recognition tasks since it was first introduced in 1951 by (Fix & Hodges, 1951) and later developed by (Cover & Hart, 1967). This approach is one of the simplest and oldest methods used in data mining (DM) and pattern classification. Moreover, it is considered one of the top 10 methods in DM (Wu, 2010). It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers (Hamamoto et al., 1997)(Alpaydin, 1997).The KNN classifier categorizes an unlabelled test example using the label of the majority of examples among its k-nearest (most similar) neighbours in the training set. The similarity depends on a specific distance metric, therefore, the performance of the classifier depends significantly on the distance metric used (Weinberger & Saul, 2009).\nA large number of similarity measures are proposed in the literature, perhaps the most famous and well known being the Euclidean distance (ED) stated by Euclid two thousand years ago. The ED has been receiving increased attention, because many applications, such as bioinformatics, dimensionality reduction in machine learning, statistics, and many others have all become very active research areas, which are mainly dependent on ED (Nathan Krislock, 2012).\nIn addition, over the last century, great efforts have been made to find new metrics and similarity measures to satisfy the needs of different applications. New similarity measures are needed, in particular, for use in distance learning (Yang, 2006), where classifiers such as the k-nearest neighbour (KNN) are heavily dependent upon choosing the best distance. Optimizing the distance metric is valuable in several computer vision tasks, such as object detection, content-based image retrieval, image segmentation and classification.\nThe similarity measures that are used the most are Euclidean (ED) and Manhattan distances (MD); both assume the same weight to all directions. In addition, the difference between vectors at each dimension might approach infinity to imply dissimilarity (Bharkad & Kokare, 2011). Therefore, such types of distances are heavily affected by the different scale of the data, noise and outliers. To solve those problems, Hassanat proposed an interesting distance metric (Hassanat, 2014), which is invariant to the different scales in multi dimensions data.\nThe main purpose of this work is to equip some of the nearest neighbour classifiers with Hassanat distance, attempting to enhance their performance. The rest of this paper describes some of the nearest neighbour classifiers to be enhanced, in addition to describing the Hassanat metric. The third section describes the data set used for experiments and discusses the results, focusing on applying the new metric which is used by some nearest neighbour classifiers."}, {"heading": "NEAREST NEIGHBOUR CLASSIFIERS", "text": "We will describe the traditional KNN, Inverted Indexes of Neighbours Classifier (IINC) (Jirina & Jirina, 2008; Jirina & Jirina, 2011) and Ensemble Nearest Neighbour classifiers (ENN) (Hassanat, 2014)."}, {"heading": "KNN", "text": "KNN is a very simple yet effective classifier that is used for pattern classification. It categorizes an unlabelled test example using the label of the majority of examples among its k-nearest (most similar) neighbours in the training set (see Figure 1). The similarity depends on a specific distance metric, normally ED or MD, therefore, the performance of the classifier depends significantly on the distance metric used. Because of its simplicity and popularity it has been used extensively in pattern recognition, machine learning, text categorization, data mining, object recognition, etc. (Kataria & Singh, 2013)(Bhatia & Vandana, 2010) and (Hassanat, 2009). However, it has some limitations, such as\nCanadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015. memory requirement and time complexity, because it is fully dependent on every example in the training set and choosing the optimal k neighbours in advance.\nInverted Indexes of Neighbours Classifier (IINC).\nThe inverted indexes of neighbours classifier IINC (Jirina & Jirina, 2008), (Jirina & Jirina, 2010)and (Jirina & Jirina, 2011) is one of the best attempts found in the literature to solve the optimal k problems related to the KNN classifier. The aim of their work was to increase the accuracy of the classifier by using all the neighbours in the training set, so k has become the whole number of examples in the training set, rewarding the nearest neighbours by adding some heavyweight, and penalizing the furthest one by giving them lightweight. Moreover, the first nearest neighbour of the point, for example x, has the biggest influence on what class point x goes to. The IINC approach is mainly based on the hypothesis that the influence, the weight of a neighbour, is proportional to the distance from the query point.\nThe IINC algorithm works as follows: the distances between the test point and the other points in the training set are calculated, and then sorted in ascending order. The summation of the inverted indexes is then calculated for each class using Eq (1). The probability of each class is then calculated using Eq (2). Obviously, the class with the highest probability is then predicted.\nS = \u2211 ( )\n(1)\nWhere Lc is the number of points of class c, i is the order of the point in the training set after sorting the distances.\nThe probability of a test point x belonging to a class c is estimated by:\nP(x|c) = (2)\nwhere S = \u2211 and N is the number of examples in the training set.\nTo visualize the IINC, cover all the points in Figure 1 by a large circle that fits all the points."}, {"heading": "ENN", "text": "The ENN classifier (Hassanat, 2014) uses an ensemble learning approach based on the same nearest neighbour rule. Fundamentally, the traditional KNN classifier is used each time with a different K. Starting from k=1 to k= the square root of the number of examples in the training set, each classifier votes for a specific class. Then it uses the weighted sum rule to identify the class, i.e. the class with the highest sum (by 1-NN, 3-NN, 5-NN\u2026 \u221an-NN) is chosen.\nENN uses \u221an down to k=1 with only odd numbers to increase the speed of the algorithm by avoiding the even classifiers and to avoid the chance of two different classes having the same number of votes. The used weight is expressed by:\n( ) = ( ) (3)\nWhen a test point is compared with all examples, using some distance function, an array (A) is created to hold the nearest \u221an classes, and the weighted sum (WS) rule is defined for each class using:\n= \u2211 \u2211 ( ), !\" = #0, %&\u210e() *(+ ,\" \u221a., , = + 2 (4)\nwhere, for each class, the outer sum represents the KNN classifier for each odd k, and the inner sum calculates the weights for each classifier.\nThe predicted class is the one with the maximum weighted sum:\n#12** = argmax 789\n(5)\nTo visualize the ENN classifier, assume that there are 25 points in a 2D feature space belonging to 2 different classes (0 and 1), and one unknown point (the green triangle) as shown in Figure 1. The ensemble system uses the 1-NN, 3- NN and 5-NN classifiers altogether using the WS rule to find the class of the \u201cgreen triangle\u201d, which in this example and according to the ENN is predicted to be class 1 \u201cred square\u201d.\nCanadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015."}, {"heading": "Hassanat distance metric", "text": "The similarity function between any two points using Hassanat Metric (Hassanat, 2014) is written as: :(!\" , ;\" < 1 > ?\" . @A,BA ?CD @A,BA , E F1 > ?\". @A,BA |?\" . @A,BA | ?CD @A,BA |?\" . @A,BA | , E F\nAlong the vectors dimensions the distance is: DHIJJIKIL A, B \u2211 OD AP where A and B are both vectors with size m. A real numbers.\nThis metric is invariant to similarity measure is invariant to different scale, noise and outliers because it is bounded the interval [0, 1[. It reaches to 1 only when the maximum value approaches infinity, or when the minimum value approaches minus infinity. This is shown by Figure 3 and equation 8. This means that the more two values are similar, the nearest to zero the distance will be, and the more they are dissimilar, the nearest to one the distance will be.\nlimPIS TU,VU \u2192XOD A , B Y limP K TU,VU \u2192ZXOD"}, {"heading": "DATA USED FOR OUR EXPERIMENTS", "text": "For evaluation of the efficiency of the Hassanat distance when used with some classifiers, twenty eight sets were chosen to represent real life classification problems, taken from the UCI Machine Learning Repository(Bache & Lichman, 2013). This databases, domain theories, and data generators that are used by the machine learning community. Since the\n!\" , ;\" [ 0 !\" , ;\" \\ 0+ (6)\n, B Y (7) i and Bi are\nby\nA , B Y 1 (8)\ndifferent data\nis a collection of\ndatabase was created in 1987 by David Aha and fellow graduate students at UC Irvine, it has been widely used by researchers, students and educators all over the world as a primary source of machine learning data sets. know more about each dataset using the following link http://mlr.cs.umass.edu/ml/ we used in our work.\nWhere #E means number of examples, and #F means number of features and #C means n"}, {"heading": "RESULTS AND DISCUSSION", "text": "Each data set is divided into two data sets, one for training, and the other for testing. 30% of the data set is used for testing, and 70% of the data is for training. Each classifier is used to classify the test samples using Manhattan distance (see Table 2). All exper Hassanat distance (see Table 3 were used as a test sample, experiment on each data set random examples for testing and training. Table show the results of the experiments. The accuracy of each classifier in each data set is the average of 10 rounds.\nReaders can :\n. Table 1 depicts the data sets\number of classes.\niments were repeated using ). 30% of the data, which were chosen randomly and each was repeated 10 times to obtain\ns 2 and 3\nur classifiers using\nCanadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015.\nGerman 0.68 0.72 0.72 0.73 0.73 0.73 0.73 0.74 Glass 0.71 0.69 0.68 0.66 0.66 0.65 0.69 0.69 Haberman 0.67 0.70 0.73 0.73 0.76 0.76 0.75 0.72 Heart 0.64 0.70 0.70 0.69 0.70 0.69 0.69 0.69 Ionosphere 0.90 0.89 0.89 0.89 0.87 0.85 0.85 0.89 Iris 0.95 0.96 0.95 0.95 0.96 0.95 0.96 0.96 Letter rec. 0.95 0.95 0.95 0.95 0.94 0.83 0.95 0.94 Liver 0.60 0.62 0.65 0.68 0.67 0.67 0.66 0.66 Monkey1 0.79 0.84 0.91 0.95 0.96 0.92 0.92 0.94 Parkinson 0.82 0.82 0.83 0.82 0.82 0.80 0.84 0.84 Phoneme 0.89 0.88 0.87 0.87 0.86 0.83 0.87 0.87 QSAR 0.81 0.82 0.82 0.83 0.82 0.79 0.84 0.83 Segmen 0.97 0.96 0.96 0.95 0.94 0.90 0.95 0.95 Sonar 0.83 0.80 0.78 0.73 0.70 0.68 0.83 0.82 Vehicle 0.67 0.68 0.68 0.67 0.66 0.65 0.68 0.69 Vote 0.91 0.93 0.93 0.94 0.94 0.93 0.93 0.93 Vowel 0.98 0.94 0.87 0.78 0.71 0.56 0.96 0.93 Waveform21 0.78 0.80 0.82 0.83 0.83 0.85 0.84 0.85 Waveform40 0.75 0.80 0.80 0.82 0.83 0.86 0.84 0.85 Wholesale 0.88 0.89 0.90 0.91 0.91 0.91 0.91 0.91 Wine 0.81 0.74 0.73 0.74 0.75 0.74 0.80 0.79\nAverage 0.81 0.82 0.82 0.82 0.82 0.80 0.83 0.83\nAs can be seen from Table 2, the results in general are worse than those in (Hassanat, 2014), because we repeated the experiments without normalizing the data sets using Manhattan (same) distance. Except for some datasets such as the \u201cEEG\u201d, whose accuracy increased significantly from 84% to 97%, this result proves that data might be harmed by normalization, and there is a need for a distance metric that is not affected by the data scale, and therefore does not need data normalization. These results confirm some results in (Hassanat, 2014), such as the superiority of the IINC and the ENN classifiers, in terms of being independent from choosing the optimal k neighbours. On the other hand, after employing Hassanat distance with the nearest neighbour classifiers on the same data sets and without normalization, we obtained the results depicted in Table 3.\nVote 0.92 0.92 0.92 0.92 0.92 0.91 0.92 0.93 Vowel 0.97 0.92 0.85 0.76 0.69 0.54 0.94 0.94 Waveform21 0.75 0.79 0.81 0.82 0.82 0.85 0.83 0.84 Waveform40 0.72 0.77 0.79 0.80 0.81 0.84 0.83 0.84 wholesale 0.87 0.88 0.89 0.90 0.90 0.90 0.90 0.89 Wine 0.97 0.97 0.96 0.96 0.96 0.96 0.97 0.97\nAverage 0.84 0.85 0.86 0.85 0.85 0.84 0.87 0.87\nBy comparing columns in Tables 2 and 3, the significant increase in the performance of each algorithm can be observed. Table 4 illustrates the increase in the accuracy of each algorithm after applying Hassanat distance; this enhancement proves that Hassanat distance is not affected by the scale of the data.\nThe average row in Table 4 confirms that using Hassanat distance enhances the performance of the nearest neighbour algorithms by 2.9% to 3.8%.The same table also shows a significant improvement in most algorithms in most data sets, such as the 30% to 35.9% boost in the accuracy of the data set BCW. Although the increase in performance is the theme, sometimes when the performance is decreased, however, these degrades were not beyond 4.1%.\nIt can be noted from Table 3 that both ENN and IINC performed very well with Hassanat distance, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy, and this result is confirmed by (Hassanat, 2014). Also, it can be noted from Tables 2,3 and 4 that there is no optimal algorithm that can solve all real life problems perfectly; this conclusion is supported by\nCanadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015. the no-free-lunch theorem (Duda et al., 2001). However, some algorithms, such as ENN and IINC using Hassanat distance, give more stable results than the others, i.e. if they are not the best to classify a specific (problem) data set, they are not the worst, and their results are very close to the best-gained results. This type of stability is illustrated by Table 5, where we record the absolute difference between each result and the best result within a specific data set.\nAs can be noticed in Table 5, ENN and IINC are the most stable classifiers, while the rest of the algorithms are much less stable, even if they beat (sometimes) ENN and IINC."}, {"heading": "CONCLUSION", "text": "This work is a new attempt to enhance the performance of some nearest neighbour classifiers using Hassanat distance metric. The experimental results using a variety of data sets of real life problems have demonstrated the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance. In addition, we have proved that this distance metric is invariant to data scale, noise and outliers, and therefore, we recommend other researchers use such a distance in other classification problems. Our future work will focus on exploiting and investigating the power of the Hassanat distance metric in other real life problems, such as content-based image retrieval and clustering problems."}, {"heading": "ACKNOWLEDGMENT", "text": "All the data sets used in this paper were taken from the UCI Irvine Machine Learning Repository, therefore the authors would like to thank and acknowledge the people behind this great corpus. Also the authors would like to thank the anonymous reviewers of this paper."}, {"heading": "EVALUATION OF DISTANCE METRICS: APPLICATION TO FINGERPRINT RECOGNITION.", "text": ""}, {"heading": "International Journal of Pattern Recognition and Artificial", "text": "Intelligence, 25(6), pp.777-806.\nBhatia, N. & Vandana, A., 2010. Survey of Nearest Neighbor Techniques. (IJCSIS) International Journal of Computer Science and Information Security, 8(2), pp.30205.\nCover, T.M. & Hart, P.E., 1967. Nearest Neighbor Pattern Classification. IEEE Trans. Inform. Theory, IT-13, pp.2127.\nDuda, R.O., Hart, P.E. & Stork, D.G., 2001. Pattern Classification. 2nd ed. Wiley.\nFix, E. & Hodges, J., 1951. 4 Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties. Randolph Field, Texas: USAF School of Aviation Medicine.\nHamamoto, Y., Uchimura, S. & Tomita, S., 1997. A Bootstrap Technique for Nearest Neighbor Classifier Design. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 19(1), pp.73-79.\nHassanat, A.B., 2009. Visual Words for Automatic LipReading. PhD Thesis. Buckingham, UK: University of Buckingham.\nHassanat, A.B., 2014. Dimensionality Invariant Similarity Measure. Journal of American Science, 10(8), pp.221-26.\nHassanat, A.B., 2014. Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach. International Journal of Computer Science and Information Security, 12(8), pp.33-39.\nJirina, M. & Jirina, M.J., 2008. No. V-1034 Classifier Based on Inverted Indexes of Neighbors. Technical Report. Academy of Sciences of the Czech Republic.\nJirina, M. & Jirina, M.J., 2010. Using Singularity Exponent in Distance Based Classifier. In Proceedings of the 10th"}, {"heading": "International Conference on Intelligent Systems Design and", "text": "Applications (ISDA2010). Cairo, 2010.\nCanadian Journal of Pure and Applied Sciences (CJPAS). volume 9, issue 1, Feb 2015. Jirina, M. & Jirina, M.J., 2011. Classifiers Based on Inverted Distances. In K. Funatsu, ed. New Fundamental Technologies in Data Mining. InTech. Ch. 19. pp.369-87.\nKataria, A. & Singh, M.D., 2013. A Review of Data Classification Using K-Nearest Neighbour Algorithm."}, {"heading": "International Journal of Emerging Technology and", "text": "Advanced Engineering, 3(6), pp.354-60.\nNathan Krislock, H.W., 2012. Euclidean Distance Matrices and Applications. Springer US.\nWeinberger, K.Q. & Saul, L.K., 2009. Distance Metric Learning for Large Margin Nearest Neighbor Classification. Journal of Machine Learning Research, 10, pp.207-44.\nWu, X.a.V.K.e., 2010. The top ten algorithms in data mining. CRC Press.\nYang, L., 2006. Distance metric learning: A comprehensive survey. Technical report. Michigan State University."}], "references": [{"title": "Voting Over Multiple Condensed Nearest Neoghbors", "author": ["E. Alpaydin"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Alpaydin,? \\Q1997\\E", "shortCiteRegEx": "Alpaydin", "year": 1997}, {"title": "PERFORMANCE EVALUATION OF DISTANCE METRICS: APPLICATION TO FINGERPRINT RECOGNITION", "author": ["S.D. Bharkad", "M. Kokare"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bharkad and Kokare,? \\Q2011\\E", "shortCiteRegEx": "Bharkad and Kokare", "year": 2011}, {"title": "Survey of Nearest Neighbor Techniques", "author": ["N. Bhatia", "A. Vandana"], "venue": "(IJCSIS) International Journal of Computer Science and Information Security,", "citeRegEx": "Bhatia and Vandana,? \\Q2010\\E", "shortCiteRegEx": "Bhatia and Vandana", "year": 2010}, {"title": "Nearest Neighbor Pattern Classification", "author": ["T.M. Cover"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Cover,? \\Q1967\\E", "shortCiteRegEx": "Cover", "year": 1967}, {"title": "Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties. Randolph Field, Texas: USAF School of Aviation Medicine", "author": ["E. Fix", "J. Hodges"], "venue": null, "citeRegEx": "Fix and Hodges,? \\Q1951\\E", "shortCiteRegEx": "Fix and Hodges", "year": 1951}, {"title": "A Bootstrap Technique for Nearest Neighbor Classifier Design", "author": ["Y. Hamamoto", "S. Uchimura", "S. Tomita"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,", "citeRegEx": "Hamamoto et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hamamoto et al\\.", "year": 1997}, {"title": "Visual Words for Automatic LipReading", "author": ["A.B. Hassanat"], "venue": "PhD Thesis. Buckingham, UK: University of Buckingham", "citeRegEx": "Hassanat,? \\Q2009\\E", "shortCiteRegEx": "Hassanat", "year": 2009}, {"title": "Dimensionality Invariant Similarity Measure", "author": ["A.B. Hassanat"], "venue": "Journal of American Science,", "citeRegEx": "Hassanat,? \\Q2014\\E", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach", "author": ["A.B. Hassanat"], "venue": "International Journal of Computer Science and Information Security,", "citeRegEx": "Hassanat,? \\Q2014\\E", "shortCiteRegEx": "Hassanat", "year": 2014}, {"title": "Using Singularity Exponent in Distance Based Classifier", "author": ["M. Jirina", "M.J. Jirina"], "venue": "In Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA2010). Cairo,", "citeRegEx": "Jirina and Jirina,? \\Q2010\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2010}, {"title": "Classifiers Based on Inverted Distances", "author": ["M. Jirina", "M.J. Jirina"], "venue": "ed. New Fundamental Technologies in Data Mining. InTech. Ch", "citeRegEx": "Jirina and Jirina,? \\Q2011\\E", "shortCiteRegEx": "Jirina and Jirina", "year": 2011}, {"title": "A Review of Data Classification Using K-Nearest Neighbour Algorithm", "author": ["A. Kataria", "M.D. Singh"], "venue": "International Journal of Emerging Technology and Advanced Engineering,", "citeRegEx": "Kataria and Singh,? \\Q2013\\E", "shortCiteRegEx": "Kataria and Singh", "year": 2013}, {"title": "Euclidean Distance Matrices and Applications", "author": ["H.W. Nathan Krislock"], "venue": null, "citeRegEx": "Krislock,? \\Q2012\\E", "shortCiteRegEx": "Krislock", "year": 2012}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "The top ten algorithms in data mining", "author": ["Wu", "X.a.V.K.e"], "venue": null, "citeRegEx": "Wu and X.a.V.K.e.,? \\Q2010\\E", "shortCiteRegEx": "Wu and X.a.V.K.e.", "year": 2010}, {"title": "Distance metric learning: A comprehensive survey", "author": ["L. Yang"], "venue": "Technical report", "citeRegEx": "Yang,? \\Q2006\\E", "shortCiteRegEx": "Yang", "year": 2006}], "referenceMentions": [{"referenceID": 5, "context": "It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers (Hamamoto et al., 1997)(Alpaydin, 1997).", "startOffset": 119, "endOffset": 142}, {"referenceID": 0, "context": ", 1997)(Alpaydin, 1997).", "startOffset": 7, "endOffset": 23}, {"referenceID": 15, "context": "New similarity measures are needed, in particular, for use in distance learning (Yang, 2006), where classifiers such as the k-nearest neighbour (KNN) are heavily dependent upon choosing the best distance.", "startOffset": 80, "endOffset": 92}, {"referenceID": 7, "context": "To solve those problems, Hassanat proposed an interesting distance metric (Hassanat, 2014), which is invariant to the different scales in multi dimensions data.", "startOffset": 74, "endOffset": 90}, {"referenceID": 7, "context": "We will describe the traditional KNN, Inverted Indexes of Neighbours Classifier (IINC) (Jirina & Jirina, 2008; Jirina & Jirina, 2011) and Ensemble Nearest Neighbour classifiers (ENN) (Hassanat, 2014).", "startOffset": 183, "endOffset": 199}, {"referenceID": 6, "context": "(Kataria & Singh, 2013)(Bhatia & Vandana, 2010) and (Hassanat, 2009).", "startOffset": 52, "endOffset": 68}, {"referenceID": 7, "context": "The ENN classifier (Hassanat, 2014) uses an ensemble learning approach based on the same nearest neighbour rule.", "startOffset": 19, "endOffset": 35}, {"referenceID": 7, "context": "Simple example showing the ENN classifier (Hassanat, 2014)", "startOffset": 42, "endOffset": 58}, {"referenceID": 7, "context": "The similarity function between any two points using Hassanat Metric (Hassanat, 2014) is written as: :(!\" , ;\" < 1 > ?\" .", "startOffset": 69, "endOffset": 85}, {"referenceID": 7, "context": "Representation of Hassanat distance metric between the points 0 and n, where n belongs to [-10, 10](Hassanat, 2014)", "startOffset": 99, "endOffset": 115}, {"referenceID": 7, "context": "As can be seen from Table 2, the results in general are worse than those in (Hassanat, 2014), because we repeated the experiments without normalizing the data sets using Manhattan (same) distance.", "startOffset": 76, "endOffset": 92}, {"referenceID": 7, "context": "These results confirm some results in (Hassanat, 2014), such as the superiority of the IINC and the ENN classifiers, in terms of being independent from choosing the optimal k neighbours.", "startOffset": 38, "endOffset": 54}, {"referenceID": 7, "context": "1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy, and this result is confirmed by (Hassanat, 2014).", "startOffset": 126, "endOffset": 142}], "year": 2015, "abstractText": "We showed in this work how the Hassanat distance metric enhances the performance of the nearest neighbour classifiers. The results demonstrate the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance and Euclidian distance. Moreover, we proved that the Hassanat distance metric is invariant to data scale, noise and outliers. Throughout this work, it is clearly notable that both ENN and IINC performed very well with the distance investigated, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted from our results that there is no optimal algorithm that can solve all reallife problems perfectly; this is supported by the no-free-lunch theorem.", "creator": "PDFCreator Version 1.7.3"}}}