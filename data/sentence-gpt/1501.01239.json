{"id": "1501.01239", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2015", "title": "On the Relationship between Sum-Product Networks and Bayesian Networks", "abstract": "In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI) within the SVM. Finally, our predictions are based on the SVM's approach to Bayesian Networks (ASNs).\n\n\n\n\nThe first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use the SVM's model to model the Bayesian Networks. The first question we tackle is how to use", "histories": [["v1", "Tue, 6 Jan 2015 17:14:11 GMT  (496kb,D)", "https://arxiv.org/abs/1501.01239v1", "17 pages"], ["v2", "Thu, 30 Apr 2015 18:15:12 GMT  (584kb,D)", "http://arxiv.org/abs/1501.01239v2", "Full version of the same paper to appear at ICML-2015"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.AI stat.ML", "authors": ["han zhao", "mazen melibari", "pascal poupart"], "accepted": true, "id": "1501.01239"}, "pdf": {"name": "1501.01239.pdf", "metadata": {"source": "META", "title": "On the Relationship between Sum-Product Networks and Bayesian Networks", "authors": ["Han Zhao", "Mazen Melibari", "Pascal Poupart"], "emails": ["HAN.ZHAO@UWATERLOO.CA", "MMELIBAR@UWATERLOO.CA", "PPOUPART@UWATERLOO.CA"], "sections": [{"heading": "1. Introduction", "text": "Sum-Product Networks (SPNs) have recently been proposed as tractable deep models (Poon & Domingos, 2011) for probabilistic inference. They distinguish themselves from other types of probabilistic graphical models (PGMs), including Bayesian Networks (BNs) and Markov Networks (MNs), by the fact that inference can be done exactly in linear time with respect to the size of the network. This has generated a lot of interest since inference is often a core task for parameter estimation and structure learning, and it typically needs to be approximated to ensure tractability since probabilistic inference in BNs and MNs is #Pcomplete (Roth, 1996).\nThe relationship between SPNs and BNs, and more broadly with PGMs, is not clear. Since the introduction of SPNs in the seminal paper of Poon & Domingos (2011), it is well understood that SPNs and BNs are equally expressive in the sense that they can represent any joint distribution over discrete variables1, but it is not clear how to convert SPNs into BNs, nor whether a blow up may occur in the conversion process. The common belief is that there exists a distribution such that the smallest BN that encodes this distribution is exponentially larger than the smallest SPN that encodes this same distribution. The key behind this belief lies in SPNs\u2019 ability to exploit context-specific independence (CSI) (Boutilier et al., 1996).\nWhile the above belief is correct for classic BNs with tabular conditional probability distributions (CPDs) that ignore CSI, and for BNs with tree-based CPDs due to the replication problem (Pagallo, 1989), it is not clear whether it is correct for BNs with more compact representations of the CPDs. The other direction is clear for classic BNs with tabular representation: given a BN with tabular representation of its CPDs, we can build an SPN that represents the same joint probability distribution in time and space complexity that may be exponential in the tree-width of the BN. Briefly, this is done by first constructing a junction tree and translate it into an SPN2. However, to the best of our knowledge, it is still unknown how to convert an SPN into a BN and whether the conversion will lead to a blow up when more compact representations than tables and trees are used for the CPDs.\nWe prove in this paper that by adopting Algebraic Decision Diagrams (ADDs) (Bahar et al., 1997) to represent the CPDs at each node in a BN, every SPN can be converted into a BN in linear time and space complexity in the size of the SPN. The generated BN has a simple bipartite structure, which facilitates the analysis of the structure of an SPN in terms of the structure of the generated BN. Furthermore,\n1Joint distributions over continuous variables are also possible, but we will restrict ourselves to discrete variables in this paper.\n2http://spn.cs.washington.edu/faq.shtml\nar X\niv :1\n50 1.\n01 23\n9v 2\n[ cs\n.A I]\n3 0\nA pr\n2 01\n5\nwe show that by applying the Variable Elimination (VE) algorithm (Zhang & Poole, 1996) to the generated BN with ADD representation of its CPDs, we can recover the original SPN in linear time and space with respect to the size of the SPN.\nOur contributions can be summarized as follows. First, we present a constructive algorithm and a proof for the conversion of SPNs into BNs using ADDs to represent the local CPDs. The conversion process is bounded by a linear function of the size of the SPN in both time and space. This gives a new perspective to understand the probabilistic semantics implied by the structure of an SPN through the generated BN. Second, we show that by executing VE on the generated BN, we can recover the original SPN in linear time and space complexity in the size of the SPN. Combined with the first point, this establishes a clear relationship between SPNs and BNs. Third, we introduce the subclass of normal SPNs and show that every SPN can be transformed into a normal SPN in quadratic time and space. Compared with general SPNs, the structure of normal SPNs exhibit more intuitive probabilistic semantics and hence normal SPNs are used as a bridge in the conversion of general SPNs to BNs. Fourth, our construction and analysis provides a new direction for learning the parameter/structure of BNs since the SPNs produced by the algorithms that learn SPNs (Dennis & Ventura, 2012; Gens & Domingos, 2013; Peharz et al., 2013; Rooshenas & Lowd, 2014) can be converted into BNs."}, {"heading": "2. Related Work", "text": "Exact probabilistic reasoning has a close connection with propositional logic and weighted model counting (Roth, 1996; Gomes et al., 2008; Bacchus et al., 2003; Sang et al., 2005). The model counting problem, #SAT, is the problem of computing the number of models for a given propositional formula, i.e., the number of distinct truth assignments of the variables for which the formula evaluates to TRUE. In its weighted version, each boolean variable X has a weight Pr(x) \u2208 [0, 1] when set to TRUE and a weight 1 \u2212 Pr(x) when set to FALSE. The weight of a truth assignment is the product of the weights of its literals. The weighted model counting problem then asks the sum of the weights of all satisfying truth assignments. There are two important streams of research for exact weighted model counting and exact probabilistic reasoning that relate to SPNs: DPLL-style exhaustive search (Birnbaum & Lozinskii, 2011) and those based on knowledge compilation, e.g., Binary Decision Diagrams (BDDs), Decomposable Negation Normal Forms (DNNFs) and Arithmetic Circuits (ACs) (Bryant, 1986; Darwiche, 2001; 2000) .\nThe SPN, as an inference machine, has a close connection with the broader field of knowledge representation and\nknowledge compilation. In knowledge compilation, the reasoning process is divided into two phases: an offline compilation phase and an online query-answering phase. In the offline phase, the knowledge base, either propositional theory or belief network, is compiled into some tractable target language. In the online phase, the compiled target model is used to answer a large number of queries efficiently. The key motivation of knowledge compilation is to shift the computation that is common to many queries from the online phase into the offline phase. As an example, ACs have been studied and used extensively in both knowledge representation and probabilistic inference (Darwiche, 2000; Huang et al., 2006; Chavira et al., 2006). Rooshenas & Lowd (2014) recently showed that ACs and SPNs can be converted mutually without an exponential blow-up in both time and space. As a direct result, ACs and SPNs share the same expressiveness for probabilistic reasoning.\nAnother representation closely related to SPNs in propositional logic and knowledge representation is the deterministic-Decomposable Negation Normal Form (dDNNF) (Darwiche & Marquis, 2001). Propositional formulas in d-DNNF are represented by a directed acyclic graph (DAG) structure to enable the re-usability of subformulas. The terminal nodes of the DAG are literals and the internal nodes are AND or OR operators. Like SPNs, d-DNNF formulas can be queried to answer satisfiability and model counting problems. We refer interested readers to Darwiche & Marquis (2001) and Darwiche (2001) for more detailed discussions.\nSince their introduction by Poon & Domingos (2011), SPNs have generated a lot of interest as a tractable class of models for probabilistic inference in machine learning. Discriminative learning techniques for SPNs have been proposed and applied to image classification (Gens & Domingos, 2012). Later, automatic structure learning algorithms were developed to build tree-structured SPNs directly from data (Dennis & Ventura, 2012; Peharz et al., 2013; Gens & Domingos, 2013; Rooshenas & Lowd, 2014). SPNs have also been applied to various fields and have generated promising results, including activity modeling (Amer & Todorovic, 2012), speech modeling (Peharz et al., 2014) and language modeling (Cheng et al., 2014). Theoretical work investigating the influence of the depth of SPNs on expressiveness exists (Delalleau & Bengio, 2011), but is quite limited. As discussed later, our results reinforce previous theoretical results about the depth of SPNs and provide further insights about the structure of SPNs by examining the structure of equivalent BNs."}, {"heading": "3. Preliminaries", "text": "We start by introducing the notation used in this paper. We use 1 : N to abbreviate the notation {1, 2, . . . , N}. We\nuse a capital letter X to denote a random variable and a bold capital letter X1:N to denote a set of random variables X1:N = {X1, . . . , XN}. Similarly, a lowercase letter x is used to denote a value taken by X and a bold lowercase letter x1:N denotes a joint value taken by the corresponding vector X1:N of random variables. We may omit the subscript 1 : N from X1:N and x1:N if it is clear from the context. For a random variable Xi, we use x j i , j \u2208 1 : J to enumerate all the values taken by Xi. For simplicity, we use Pr(x) to mean Pr(X = x) and Pr(x) to mean Pr(X = x). We use calligraphic letters to denote graphs (e.g., G). In particular, BNs, SPNs and ADDs are denoted respectively by B, S and A. For a DAG G and a node v in G, we use Gv to denote the subgraph of G induced by v and all its descendants. Let V be a subset of the nodes of G, then G|V is a subgraph of G induced by the node set V. Similarly, we use X|A or x|A to denote the restriction of a vector to a subset A. We use node and vertex, arc and edge interchangeably when we refer to a graph. Other notation will be introduced when needed.\nTo ensure that the paper is self contained, we briefly review some background material about Bayesian Networks, Algebraic Decision Diagrams and Sum-Product Networks. Readers who are already familiar with those models can skip the following subsections."}, {"heading": "3.1. Bayesian Network", "text": "Consider a problem whose domain is characterized by a set of random variables X1:N with finite support. The joint probability distribution over X1:N can be characterized by a Bayesian Network, which is a DAG where nodes represent the random variables and edges represent probabilistic dependencies among the variables. In a BN, we also use the terms \u201cnode\u201d and \u201cvariable\u201d interchangeably. For each variable in a BN, there is a local conditional probability distribution (CPD) over the variable given its parents in the BN.\nThe structure of a BN encodes conditional independencies among the variables in it. Let X1, X2, . . . , XN be a topological ordering of all the nodes in a BN3, and let \u03c0Xi be the set of parents of node Xi in the BN. Each variable in a BN is conditionally independent of all its non-descendants given its parents. Hence, the joint probability distribution over X1:N admits the factorization in Eq. 1.\nPr(X1:N ) = N\u220f i=1 Pr(Xi |X1:i\u22121) = N\u220f i=1 Pr(Xi | \u03c0Xi)\n(1) Given the factorization, one can use various inference al-\n3A topological ordering of nodes in a DAG is a linear ordering of its nodes such that each node appears after all its parents in this ordering.\ngorithms to do probabilistic reasoning in BNs. See Wainwright & Jordan (2008) for a comprehensive survey."}, {"heading": "3.2. Algebraic Decision Diagram", "text": "We first give a formal definition of Algebraic Decision Diagrams (ADDs) for variables with Boolean domains and then extend the definition to domains corresponding to arbitrary finite sets.\nDefinition 1 (Algebraic Decision Diagram (Bahar et al., 1997)). An Algebraic Decision Diagram (ADD) is a graphical representation of a real function with Boolean input variables: f : {0, 1}N 7\u2192 R, where the graph is a rooted DAG. There are two kinds of nodes in an ADD. Terminal nodes, whose out-degree is 0, are associated with real values. Internal nodes, whose out-degree is 2, are associated with Boolean variables Xn, n \u2208 1 : N . For each internal node Xn, the left out-edge is labeled with Xn = FALSE and the right out-edge is labeled with Xn = TRUE.\nWe extend the original definition of an ADD by allowing it to represent not only functions of Boolean variables, but also any function of discrete variables with a finite set as domain. This can be done by allowing each internal node Xn to have |Xn| out-edges and label each edge with xjn, j \u2208 1 : |Xn|, where Xn is the domain of variable Xn and |Xn| is the number of values Xn takes. Such an ADD represents a function f : X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 XN 7\u2192 R, where \u00d7 means the Cartesian product between two sets. Henceforth, we will use our extended definition of ADDs throughout the paper.\nFor our purpose, we will use an ADD as a compact graphical representation of local CPDs associated with each node in a BN. This is a key insight of our constructive proof presented later. Compared with a tabular representation or a decision tree representation of local CPDs, CPDs represented by ADDs can fully exploit CSI (Boutilier et al., 1996) and effectively avoid the replication problem (Pagallo, 1989) of the decision tree representation.\nWe give an example in Fig. 1 where the tabular representation, decision-tree representation and ADD representation of a function of 4 Boolean variables is presented. Another advantage of ADDs to represent local CPDs is that arithmetic operations such as multiplying ADDs and summingout a variable from an ADD can be implemented efficiently in polynomial time. This will allow us to use ADDs in the Variable Elimination (VE) algorithm to recover the original SPN after its conversion to a BN with CPDs represented by ADDs. Readers are referred to Bahar et al. (1997) for more detailed and thorough discussions about ADDs."}, {"heading": "3.3. Sum-Product Network", "text": "Before introducing SPNs, we first define the notion of network polynomial, which plays an important role in our\nproof. We use I[X = x] to denote an indicator that returns 1 when X = x and 0 otherwise. To simplify the notation, we will use Ix to represent I[X = x]. Definition 2 (Network Polynomial (Poon & Domingos, 2011)). Let f(\u00b7) \u2265 0 be an unnormalized probability distribution over a Boolean random vector X1:N . The network polynomial of f(\u00b7) is a multilinear function\u2211\nx f(x) \u220fN\nn=1 Ixn of indicator variables, where the summation is over all possible instantiations of the Boolean random vector X1:N .\nIntuitively, the network polynomial is a Boolean expansion (Boole, 1847) of the unnormalized probability distribution f(\u00b7). For example, the network polynomial of a BN X1 \u2192 X2 is Pr(x1, x2)Ix1Ix2 + Pr(x1, x\u03042)Ix1Ix\u03042 + Pr(x\u03041, x2)Ix\u03041Ix2 + Pr(x\u03041, x\u03042)Ix\u03041Ix\u03042 . Definition 3 (Sum-Product Network (Poon & Domingos, 2011)). A Sum-Product Network (SPN) over Boolean variables X1:N is a rooted DAG whose leaves are the indicators Ix1 , . . . , IxN and Ix\u03041 , . . . , Ix\u0304N and whose internal nodes are sums and products. Each edge (vi, vj) emanating from a sum node vi has a non-negative weight wij . The value of a product node is the product of the values of its children. The value of a sum node is \u2211 vj\u2208Ch(vi) wijval(vj) where Ch(vi) are the children of vi and val(vj) is the value of node vj . The value of an SPN S[Ix1 , Ix\u03041 , . . . , IxN , Ix\u0304N ] is the value of its root.\nThe scope of a node in an SPN is defined as the set of variables that have indicators among the node\u2019s descendants: For any node v in an SPN, if v is a terminal node, say, an indicator variable over X , then scope(v) = {X}, else scope(v) = \u22c3 v\u0303\u2208Ch(v) scope(v\u0303). Poon & Domingos (2011) further define the following properties of an SPN: Definition 4 (Complete). An SPN is complete iff each sum\nnode has children with the same scope. Definition 5 (Consistent). An SPN is consistent iff no variable appears negated in one child of a product node and non-negated in another. Definition 6 (Decomposable). An SPN is decomposable iff for every product node v, scope(vi) \u22c2 scope(vj) = \u2205 where vi, vj \u2208 Ch(v), i 6= j.\nClearly, decomposability implies consistency in SPNs. An SPN is said to be valid iff it defines a (unnormalized) probability distribution. Poon & Domingos (2011) proved that if an SPN is complete and consistent, then it is valid. Note that this is a sufficient, but not necessary condition. In this paper, we focus only on complete and consistent SPNs as we are interested in their associated probabilistic semantics. For a complete and consistent SPN S, each node v in S defines a network polynomial fv(\u00b7) which corresponds to the sub-SPN rooted at v. The network polynomial defined by the root of the SPN can then be computed recursively by taking a weighted sum of the network polynomials defined by the sub-SPNs rooted at the children of each sum node and a product of the network polynomials defined by the sub-SPNs rooted at the children of each product node. The probability distribution induced by an SPN S is defined as PrS(x) ,\nfS(x)\u2211 x fS(x)\n, where fS(\u00b7) is the network polynomial defined by the root of the SPN S. An example of a complete and consistent SPN is given in Fig. 2."}, {"heading": "4. Main Results", "text": "In this section, we first state the main results obtained in this paper and then provide detailed proofs with some discussion of the results. To keep the presentation simple, we assume without loss of generality that all the random variables are Boolean unless explicitly stated. It is straightfor-\n3500 Ix\u03041Ix2 + 8243500 Ix\u03041Ix\u03042 .\nward to extend our analysis to discrete random variables with finite support. For an SPN S, let |S| be the size of the SPN, i.e., the number of nodes plus the number of edges in the graph. For a BN B, the size of B, |B|, is defined by the size of the graph plus the size of all the CPDs in B (the size of a CPD depends on its representation, which will be clear from the context). The main theorems are:\nTheorem 1. There exists an algorithm that converts any complete and decomposable SPN S over Boolean variables X1:N into a BN B with CPDs represented by ADDs in time O(N |S|). Furthermore, S and B represent the same distribution and |B| = O(N |S|).\nAs it will be clear later, Thm. 1 immediately leads to the following corollary:\nCorollary 2. There exists an algorithm that converts any complete and consistent SPN S over Boolean variables X1:N into a BN B with CPDs represented by ADDs in time O(N |S|2). Furthermore, S and B represent the same distribution and |B| = O(N |S|2). Remark 1. The BN B generated from S in Theorem 1 and Corollary 2 has a simple bipartite DAG structure, where all the source nodes are hidden variables and the terminal nodes are the Boolean variables X1:N .\nRemark 2. Assuming sum nodes alternate with product nodes in SPN S, the depth of S is proportional to the maximum in-degree of the nodes in B, which, as a result, is proportional to a lower bound of the tree-width of B. Theorem 3. Given the BN B with ADD representation of CPDs generated from a complete and decomposable SPN S over Boolean variables X1:N , the original SPN S can be recovered by applying the Variable Elimination algorithm to B in O(N |S|).\nRemark 3. The combination of Theorems 1 and 3 shows that distributions for which SPNs allow a compact representation and efficient inference, BNs with ADDs also allow a compact representation and efficient inference (i.e., no exponential blow up).\nTo make the upcoming proofs concise, we first define a normal form for SPNs and show that every complete and consistent SPN can be transformed into a normal SPN in quadratic time and space without changing the network polynomial. We then derive the proofs with normal SPNs. Note that we only focus on SPNs that are complete and consistent. Hence, when we refer to an SPN, we assume that it is complete and consistent without explicitly stating this."}, {"heading": "4.1. Normal Form", "text": "For an SPN S, let fS(\u00b7) be the network polynomial defined at the root of S. Define the height of an SPN to be the length of the longest path from the root to a terminal node. Definition 7. An SPN is said to be normal if\n1. It is complete and decomposable. 2. For each sum node in the SPN, the weights of the\nedges emanating from the sum node are nonnegative and sum to 1. 3. Every terminal node in the SPN is a univariate distribution over a Boolean variable and the size of the scope of a sum node is at least 2 (sum nodes whose scope is of size 1 are reduced into terminal nodes).\nTheorem 4. For any complete and consistent SPN S , there exists a normal SPN S \u2032 such that PrS(\u00b7) = PrS\u2032(\u00b7) and |S \u2032| = O(|S|2).\nTo show this, we first prove the following lemmas. Lemma 5. For any complete and consistent SPN S over X1:N , there exists a complete and decomposable SPN S \u2032 over X1:N such that fS(x) = fS\u2032(x),\u2200x and |S \u2032| = O(|S|2).\nProof. Let S be a complete and consistent SPN. If it is also decomposable, then simply set S \u2032 = S and we are done. Otherwise, let v1, . . . , vM be an inverse topological ordering of all the nodes in S, including both terminal nodes and internal nodes, such that for any vm,m \u2208 1 : M , all the ancestors of vm in the graph appear after vm in the ordering. Let vm be the first product node in the ordering that violates decomposability. Let vm1 , vm2 , . . . , vml be the children of vm where m1 < m2 < \u00b7 \u00b7 \u00b7 < ml < m (due to the inverse topological ordering). Let (vmi , vmj ), i < j, i, j \u2208 1 : l be the first ordered pair of nodes such that scope(vmi) \u22c2 scope(vmj ) 6= \u2205. Hence,\nlet X \u2208 scope(vmi) \u22c2\nscope(vmj ). Consider fvmi and fvmj which are the network polynomials defined by the sub-SPNs rooted at vmi and vmj .\nExpand network polynomials fvmi and fvmj into a sumof-product form by applying the distributive law between products and sums. For example, if f(X1, X2) = (Ix1 + 9Ix\u03041)(4Ix2+6Ix\u03042), then the expansion of f is f(X1, X2) = 4Ix1Ix2 + 6Ix1Ix\u03042 + 36Ix\u03041Ix2 + 54Ix\u03041Ix\u03042 . Since S is complete, then sub-SPNs rooted at vmi and vmj are also complete, which means that each monomial in the expansion of fvmi must share the same scope. The same applies to fvmj . Since X \u2208 scope(vmi) \u22c2 scope(vmj ), then every monomial in the expansion of fvmi and fvmj must contain an indicator variable over X , either Ix or Ix\u0304. Furthermore, since S is consistent, then the sub-SPN rooted at vm is also consistent. Consider fvm = \u220fl k=1 fvmk =\nfvmi fvmj \u220f\nk 6=i,j fvmk . Because vm is consistent, we know that each monomial in the expansions of fvmi and fvmj must contain the same indicator variable of X , either Ix or Ix\u0304, otherwise there will be a term IxIx\u0304 in fvm which violates the consistency assumption. Without loss of generality, assume each monomial in the expansions of fvmi and fvmj contains Ix. Then we can re-factorize fvm in the following way:\nfvm = l\u220f k=1 fvmk = I 2 x fvmi Ix fvmj Ix \u220f k 6=i,j fvmk\n= Ix fvmi Ix fvmj Ix \u220f k 6=i,j fvmk = Ixf\u0303vmi f\u0303vmj \u220f k 6=i,j fvmk\n(2)\nwhere we use the fact that indicator variables are idempotent, i.e., I2x = Ix and f\u0303vmi (f\u0303vmj ) is defined as the function by factorizing Ix out from fvmi (fvmj ). Eq. 2 means that in order to make vm decomposable, we can simply remove all the indicator variables Ix from sub-SPNs rooted at vmi and vmj and later link Ix to vm directly. Such a transformation will not change the network polynomial fvm as shown by Eq. 2, but it will remove X from scope(vmi) \u22c2 scope(vmj ). In principle, we can apply this transformation to all ordered pairs (vmi , vmj ), i < j, i, j \u2208 1 : l with nonempty intersections of scope. However, this is not algorithmically efficient and more importantly, for local components containing Ix in fvm which are reused by other nodes vn outside of Svm , we cannot remove Ix from them otherwise the network polynomials for each such vn will be changed due to the removal. In such case, we need to duplicate the local components to ensure that local transformations with respect to fvm do not affect network polynomials fvn . We present the transformation in Alg. 1. Alg. 1 transforms a complete and consistent SPN S into a complete and decomposable SPN S \u2032. Informally, it works using the\nAlgorithm 1 Decomposition Transformation Input: Complete and consistent SPN S. Output: Complete and decomposable SPN S \u2032.\n1: Let v1, v2, . . . , vM be an inverse topological ordering of nodes in S. 2: for m = 1 to M do 3: if vm is a non-decomposable product node then 4: \u2126(vm)\u2190 \u22c3 i 6=j scope(vmi) \u22c2 scope(vmj )\n5: V\u2190 {v \u2208 Svm | scope(v) \u22c2\n\u2126(vm) 6= \u2205} 6: SV \u2190 Svm |V 7: D(vm)\u2190 descendants of vm 8: for node v \u2208 SV\\{vm} do 9: if Pa(v)\\D(vm) 6= \u2205 then\n10: Create p\u2190 v \u2297\u220fX\u2208\u2126(vm)\u2229scope(v) Ix\u2217 11: Connect p to \u2200f \u2208 Pa(v)\\D(vm) 12: Disconnect v from \u2200f \u2208 Pa(v)\\D(vm) 13: end if 14: end for 15: for node v \u2208 SV in bottom-up order do 16: Disconnect v\u0303 \u2208 Ch(v) \u2200scope(v\u0303) \u2286 \u2126(vm) 17: end for 18: Connect \u220f X\u2208\u2126(vm) Ix\u2217 to vm directly 19: end if 20: end for 21: Delete all nodes unreachable from the root of S 22: Delete all product nodes with out-degree 0 23: Contract all product nodes with out-degree 1\nfollowing identity:\nfvm =  \u220f X\u2208\u2126(vm) Ix\u2217  l\u220f k=1 fvmk\u220f X\u2208\u2126(vm)\u2229scope(vmk )\nI\u2217x (3)\nwhere \u2126(vm) , \u22c3\ni,j\u22081:l,i6=j scope(vmi)\u2229scope(vmj ), i.e., \u2126(vm) is the union of all the shared variables between pairs of children of vm and Ix\u2217 is the indicator variable of X \u2208 \u2126(vm) appearing in Svm . Based on the analysis above, we know that for each X \u2208 \u2126(vm) there will be only one kind of indicator variable Ix\u2217 that appears inside Svm , otherwise vm is not consistent. In Line 6, Svm |V is defined as the subSPN of Svm induced by the node set V, i.e., a subgraph of Svm where the node set is restricted to V. In Lines 5-6, we first extract the induced sub-SPN SV from Svm rooted at vm using the node set in which nodes have nonempty intersections with \u2126(vm). We disconnect the nodes in SV from their children if their children are indicator variables of a subset of \u2126(vm) (Lines 15-17). At Line 18, we build a new product node by multiplying all the indicator variables in \u2126(vm) and link it to vm directly. To keep unchanged the network polynomials of nodes outside Svm that use nodes in SV, we create a duplicate node p for each such node v and link p to all the parents of v outside of Svm and at the\nsame time delete the original link (Lines 9-13).\nIn summary, Lines 15-17 ensure that vm is decomposable by removing all the shared indicator variables in \u2126(vm). Line 18 together with Eq. 3 guarantee that fvm is unchanged after the transformation. Lines 9-13 create necessary duplicates to ensure that other network polynomials are not affected. Lines 21-23 simplify the transformed SPN to make it more compact. An example is depicted in Fig. 3 to illustrate the transformation process.\nWe now analyze the size of the SPN constructed by Alg. 1. For a graph S, let V(S) be the number of nodes in S and let E(S) be the number of edges in S . Note that in Lines 8-17 we only focus on nodes that appear in the induced SPN SV, which clearly has |SV| \u2264 |Svm |. Furthermore, we create a new product node p at Line 10 iff v is reused by other nodes which do not appear in Svm . This means that the number of nodes created during each iteration between Lines 2 and 20 is bounded by V(SV) \u2264 V(Svm). Line 10 also creates 2 new edges to connect p to v and the indicator variables. Lines 11 and 12 first connect edges to p and then delete edges from v, hence these two steps do not yield increases in the number of edges. So the increase in the number of edges is bounded by 2V(SV) \u2264 2V(Svm). Combining increases in both nodes and edges, during each outer iteration the increase in size is bounded by 3|SV| \u2264 3|Svm | = O(|S|). There will be at most M = V(S) outer iterations hence the total increase in size will be bounded by O(M |S|) = O(|S|2).\nLemma 6. For any complete and decomposable SPN S over X1:N that satisfies condition 2 of Def. 7, \u2211 x fS(x) = 1.\nProof. We give a proof by induction on the height of S.\nLet R be the root of S. \u2022 Base case. SPNs of height 0 are indicator variables\nover some Boolean variable whose network polynomials immediately satisfy Lemma 6. \u2022 Induction step. Assume Lemma 6 holds for any SPN with height\u2264 k. Consider an SPN S with height k+1. We consider the following two cases:\n\u2013 The root R of S is a product node. Then in this case the network polynomial fS(\u00b7) for S is defined as fS = \u220f v\u2208Ch(R) fv . We have\u2211\nx fS(x) = \u2211 x \u220f v\u2208Ch(R) fv(x|scope(v)) (4)\n= \u220f\nv\u2208Ch(R) \u2211 x|scope(v) fv(x|scope(v)) (5)\n= \u220f\nv\u2208Ch(R)\n1 = 1 (6)\nwhere x|scope(v) means that x is restricted to the set scope(v). Eq. 5 follows from the decomposability of R and Eq. 6 follows from the induction hypothesis. \u2013 The root R of S is a sum node. The network polynomial is fS =\n\u2211 v\u2208Ch(R) wR,vfv . We have\u2211\nx fS(x) = \u2211 x \u2211 v\u2208Ch(R) wR,vfv(x) (7)\n= \u2211\nv\u2208Ch(R) wR,v \u2211 x fv(x) (8)\n= \u2211\nv\u2208Ch(R)\nwR,v = 1 (9)\nEq. 8 follows from the commutative and associative law of addition and Eq. 9 follows by the induction hypothesis.\nCorollary 7. For any complete and decomposable SPN S over X1:N that satisfies condition 2 of Def. 7, PrS(\u00b7) = fS(\u00b7). Lemma 8. For any complete and decomposable SPN S , there exists an SPN S \u2032 where the weights of the edges emanating from every sum node are nonnegative and sum to 1, and PrS(\u00b7) = PrS\u2032(\u00b7), |S \u2032| = |S|.\nProof. Alg. 2 runs in one pass of S to construct the required SPN S \u2032. We proceed to prove that the SPN S \u2032 returned by Alg. 2 satisfies PrS\u2032(\u00b7) = PrS(\u00b7), |S \u2032| = |S| and that S \u2032 satisfies condition 2 of Def. 7. It is clear that |S \u2032| = |S| because we only modify the weights of S to construct S \u2032 at Line 7. Based on Lines 6 and 7, it is also straightforward to verify that for each sum node v in S \u2032,\nAlgorithm 2 Weight Normalization Input: SPN S Output: SPN S \u2032\n1: S \u2032 \u2190 S 2: val(Ix)\u2190 1,\u2200Ix \u2208 S 3: Let v1, . . . , vM be an inverse topological ordering of\nthe nodes in S 4: for m = 1 to M do 5: if vm is a sum node then 6: val(vm)\u2190 \u2211 v\u2208Ch(vm) wvm,vval(v) 7: w\u2032vm,v \u2190 wvm,vval(v) val(vm) , \u2200v \u2208 Ch(vm) 8: else if vm is a product node then 9: val(vm)\u2190 \u220f v\u2208Ch(vm) val(v)\n10: end if 11: end for\nthe weights of the edges emanating from v are nonnegative and sum to 1. We now show that PrS\u2032(\u00b7) = PrS(\u00b7). Using Corollary 7, PrS\u2032(\u00b7) = fS\u2032(\u00b7). Hence it is sufficient to show that fS\u2032(\u00b7) = PrS(\u00b7). Before deriving a proof, it is helpful to note that for each node v \u2208 S, val(v) = \u2211 x|scope(v) fv(x|scope(v)). We give a proof by induction on the height of S. \u2022 Base case. SPNs with height 0 are indicator variables\nwhich automatically satisfy Lemma 8. \u2022 Induction step. Assume Lemma 8 holds for any SPN\nof height\u2264 k. Consider an SPN S of height k+1. Let R be the root node of S with out-degree l. We discuss the following two cases.\n\u2013 R is a product node. Let R1, . . . , Rl be the children of R and S1, . . . ,Sl be the corresponding sub-SPNs. By induction, Alg. 2 returns S \u20321, . . . ,S \u2032l that satisfy Lemma 8. Since R is a product node, we have\nfS\u2032(x) = l\u220f i=1 fS\u2032i(x|scope(Ri)) (10)\n= l\u220f i=1 Pr Si (x|scope(Ri)) (11)\n= l\u220f i=1 fSi(x|scope(Ri))\u2211 x|scope(Ri) fSi(x|scope(Ri)) (12)\n= \u220fl i=1 fSi(x|scope(Ri))\u2211\nx \u220fl i=1 fSi(x|scope(Ri))\n(13)\n= fS(x)\u2211 x fS(x) = Pr S (x) (14)\nEq. 11 follows from the induction hypothesis and Eq. 13 follows from the distributive law due to the decomposability of S.\n\u2013 R is a sum node with weights w1, . . . , wl \u2265 0. We have\nfS\u2032(x) = l\u2211 i=1 w\u2032ifS\u2032i(x) (15)\n= l\u2211 i=1 wival(Ri)\u2211l j=1 wjval(Rj) Pr Si (x) (16)\n= l\u2211 i=1 wival(Ri)\u2211l j=1 wjval(Rj) fSi(x)\u2211 x fSi(x)\n(17)\n= l\u2211 i=1 wival(Ri)\u2211l j=1 wjval(Rj) fSi(x) val(Ri) (18)\n= \u2211l i=1 wifSi(x)\u2211l\nj=1 wjval(Rj) = fS(x)\u2211 x fS(x)\n(19)\n= Pr S (x) (20)\nwhere Eqn. 16 follows from the induction hypothesis, Eq. 18 and 19 follow from the fact that val(v) = \u2211 x|scope(v) fv(x|scope(v)),\u2200v \u2208 S.\nThis completes the proof since PrS\u2032(\u00b7) = fS\u2032(\u00b7) = PrS(\u00b7).\nGiven a complete and decomposable SPN S, we now construct and show that the last condition in Def. 7 can be satisfied in time and space O(|S|). Lemma 9. Given a complete and decomposable SPN S, there exists an SPN S \u2032 satisfying condition 3 in Def. 7 such that PrS\u2032(\u00b7) = PrS(\u00b7) and |S \u2032| = O(|S|).\nProof. We give a proof by construction. First, if S is not weight normalized, apply Alg. 2 to normalize the weights (i.e., the weights of the edges emanating from each sum node sum to 1).\nNow check each sum node v in S in a bottom-up order. If |scope(v)| = 1, by Corollary 7 we know the network polynomial fv is a probability distribution over its scope, say, {X}. Reduce v into a terminal node which is a distribution over X induced by its network polynomial and disconnect v from all its children. The last step is to remove all the unreachable nodes from S to obtain S \u2032. Note that in this step we will only decrease the size of S, hence |S \u2032| = O(|S|).\nProof of Thm. 4. The combination of Lemma 5, 8 and 9 completes the proof of Thm. 4.\nAn example of a normal SPN constructed from the SPN in Fig. 2 is depicted in Fig. 4."}, {"heading": "4.2. SPN to BN", "text": "In order to construct a BN from an SPN, we require the SPN to be in a normal form, otherwise we can first transform it into a normal form using Alg. 1 and 2.\nLet S be a normal SPN over X1:N . Before showing how to construct a corresponding BN, we first give some intuitions. One useful view is to associate each sum node in an SPN with a hidden variable. For example, consider a sum node v \u2208 S with out-degree l. Since S is normal, we have \u2211l i=1 wi = 1 and wi \u2265 0,\u2200i \u2208 1 : l. This naturally suggests that we can associate a hidden discrete random variable Hv with multinomial distribution Prv(Hv = i) = wi, i \u2208 1 : l for each sum node v \u2208 S. Therefore, S can be thought as defining a joint probability distribution over X1:N and H = {Hv | v \u2208 S, v is a sum node} where X1:N are the observable variables and H are the hidden variables. When doing inference with an SPN, we implicitly sum out all the hidden variables H and compute PrS(x) = \u2211 h PrS(x,h). Associating each sum node in an SPN with a hidden variable not only gives us a conceptual understanding of the probability distribution defined by an SPN, but also helps to elucidate one of the key properties implied by the structure of an SPN as summarized below:\nProposition 10. Given a normal SPN S, let p be a product node in S with l children. Let v1, . . . , vk be sum nodes which lie on a path from the root of S to p. Then\nPr S\n(x|scope(p) \u2223\u2223\u2223 Hv1 = v\u22171 , . . . ,Hvk = v\u2217k) =\nl\u220f i=1 Pr S (x|scope(pi) \u2223\u2223\u2223 Hv1 = v\u22171 , . . . ,Hvk = v\u2217k) (21)\nwhere Hv = v\u2217 means the sum node v selects its v\u2217th branch and x|A denotes restricting x by set A, pi is the ith child of product node p.\nProof. Consider the sub-SPN Sp rooted at p. Sp can be obtained by restricting Hv1 = v \u2217 1 , . . . ,Hvk = v \u2217 k, i.e., going from the root of S along the pathHv1 = v\u22171 , . . . ,Hvk = v\u2217k.\nSince p is a decomposable product node, Sp admits the above factorization by the definition of a product node and Corollary 7.\nNote that there may exist multiple paths from the root to p in S. Each such path admits the factorization stated in Eq. 21. Eq. 21 explains two key insights implied by the structure of an SPN that will allow us to construct an equivalent BN with ADDs. First, CSI is efficiently encoded by the structure of an SPN using Proposition 21. Second, the DAG structure of an SPN allows multiple assignments of hidden variables to share the same factorization, which effectively avoids the replication problem presents in decision trees.\nBased on the observations above and with the help of the normal form for SPNs, we now proceed to prove the first main result in this paper: Thm. 1. First, we present the algorithm to construct the structure of a BN B from S in Alg. 3. In a nutshell, Alg. 3 creates an observable variable\nAlgorithm 3 Build BN Structure Input: normal SPN S Output: BN B = (BV ,BE)\n1: R\u2190 root of S 2: if R is a terminal node over variable X then 3: Create an observable variable X 4: BV \u2190 BV \u222a {X} 5: else 6: for each child Ri of R do 7: if BN has not been built for SRi then 8: Recursively build BN Structure for SRi 9: end if\n10: end for 11: if R is a sum node then 12: Create a hidden variable HR associated with R 13: BV \u2190 BV \u222a {HR} 14: for each observable variable X \u2208 SR do 15: BE \u2190 BE \u222a {(HR, X)} 16: end for 17: end if 18: end if\nX in B for each terminal node overX in S (Lines 2-4). For each internal sum node v in S, Alg. 3 creates a hidden variable Hv associated with v and builds directed edges from Hv to all observable variables X appearing in the sub-SPN rooted at v (Lines 11-17). The BN B created by Alg. 3 has a directed bipartite structure with a layer of hidden variables pointing to a layer of observable variables. A hidden variable H points to an observable variable X in B iff X appears in the sub-SPN rooted at H in S. We now present Alg. 4 and 5 to build ADDs for each observable variable X and hidden variable H in B. For each\nAlgorithm 4 Build CPD using ADD, observable variable Input: normal SPN S, variable X Output: ADD AX\n1: if ADD has already been created for S and X then 2: AX \u2190 retrieve ADD from cache 3: else 4: R\u2190 root of S 5: if R is a terminal node then 6: AX \u2190 decision stump rooted at R 7: else if R is a sum node then 8: Create a node HR into AX 9: for each Ri \u2208 Ch(R) do\n10: Link BuildADD(SRi , X) as ith child of HR 11: end for 12: else if R is a product node then 13: Find child SRi such that X \u2208 scope(Ri) 14: AX \u2190 BuildADD(SRi , X) 15: end if 16: store AX in cache 17: end if\nAlgorithm 5 Build CPD using ADD, hidden variable Input: normal SPN S, variable H Output: ADD AH\n1: Find the sum node H in S 2: AH \u2190 decision stump rooted at H in S\nhidden variable H , Alg. 5 builds AH as a decision stump4 obtained by finding H and its associated weights in S. Consider ADDs built by Alg. 4 for observable variables Xs. Let X be the current observable variable we are considering. Basically, Alg. 4 is a recursive algorithm applied to each node in S whose scope intersects with {X}. There are three cases. If current node is a terminal node, then it must be a probability distribution over X . In this case we simply return the decision stump at the current node. If the current node is a sum node, then due to the completeness of S, we know that all the children of R share the same scope with R. We first create a node HR corresponding to the hidden variable associated with R into AX (Line 8) and recursively apply Alg. 4 to all the children of R and link them to HR respectively. If the current node is a product node, then due to the decomposability of S, we know that there will be a unique child of R whose scope intersects with {X}. We recursively apply Alg. 4 to this child and return the resulting ADD (Lines 12-15).\nEquivalently, Alg. 4 can be understood in the following way: we extract the sub-SPN induced by {X} and contract5 all the product nodes in it to obtain AX . Note that\n4A decision stump is a decision tree with one variable. 5In graph theory, the contraction of a node v in a DAG is the operation that connects each parent of v to each child of v and\nthe contraction of product nodes will not add more edges into AX since the out-degree of each product node in the induced sub-SPN must be 1 due to the decomposability of the product node. We illustrate the application of Alg. 3, 4 and 5 on the normal SPN in Fig. 4, which results in the BN B with CPDs represented by ADDs shown in Fig. 5. We now show that PrS(x) = PrB(x) \u2200x. Lemma 11. Given a normal SPN S, the ADDs constructed by Alg. 4 and 5 encode local CPDs at each node in B.\nProof. It is easy to verify that for each hidden variable H in B, AH represents a local CPD since AH is a decision stump with normalized weights.\nFor any observable variable X in B, let Pa(X) be the set of parents of X . By Alg. 3, every node in Pa(X) is a hidden variable. Furthermore, \u2200H , H \u2208 Pa(X) iff there exists one terminal node over X in S that appears in the sub-SPN rooted at H . Hence given any joint assignment h of Pa(X), there will be a path in AX from the root to a terminal node that is consistent with the joint assignment of the parents. Also, the leaves in AX contain normalized weights corresponding to the probabilities ofX (see Def. 7) induced by the creation of decision stumps overX in Lines 5-6 of Alg. 4.\nTheorem 12. For any normal SPN S over X1:N , the BN B constructed by Alg. 3, 4 and 5 encodes the same probability distribution, i.e., PrS(x) = PrB(x),\u2200x.\nProof. Again, we give a proof by induction on the height of S. \u2022 Base case. The height of SPN S is 0. In this case, S will be a single terminal node over X and B will be a single observable node with decision stump AX constructed from the terminal node by Lines 5-6 in Alg. 4. It is clear that PrS(x) = PrB(x),\u2200x.\n\u2022 Induction step. Assume PrB(x) = PrS(x),\u2200x for any S with height \u2264 k, where B is the corresponding BN constructed by Alg. 3, 4 and 5 from S. Consider an SPN S with height k+1. LetR be the root of S and Ri, i \u2208 1 : l be the children of R in S . We consider the following two cases:\n\u2013 R is a product node. Let scope(Rt) = Xt, t \u2208 1 : l. Claim: there is no edge between Si and Sj , i 6= j, where Si(Sj) is the sub-SPN rooted at Ri(Rj). If there is an edge, say, from vj to vi where vj \u2208 Sj and vi \u2208 Si, then scope(vi) \u2286 scope(vj) \u2286 scope(Rj). On the other hand, scope(vi) \u2286 scope(Ri). So we have \u2205 6= scope(vi) \u2286 scope(Ri) \u22c2 scope(Rj),\nwhich contradicts the decomposability of the\nthen delete v from the graph.\n+\n\u00d7 \u00d7 \u00d7\nX1 X1 X2 X2\n(0.6, 0.4) (0.9, 0.1) (0.3, 0.7) (0.2, 0.8)\n4 7\n6 35\n9 35\nH\nX1 X2\nHAH =\n4 7\n6 35\n9 35\nh1 h2 h3\nH = AX1\nX1 X1\n0.6 0.4 0.9 0.1\nh1 h2 h3\nx1 x\u03041\nx1 x\u03041\nHAX2 =\nX2 X2\n0.3 0.7 0.2 0.8\nh1 h2 h3\nx2 x\u03042\nx2 x\u03042\nadvantage of the CSI described by ADDs of X. Eq. 26 follows from the fact that H\u2212t appears only in the second term. Combined with the fact that H = ht is given as evidence in B, this gives us the induced subgraph Bt referred to in Eq. 28. Eq. 30 follows from Eq. 28 and Eq. 31 follows from the induction hypothesis.\nCombing the base case and the induction step completes the proof for Thm. 12.\nWe now bound the size of B: Theorem 13. |B| = O(N |S|), where BN B is constructed by Alg. 3, 4 and 5 from normal SPN S over X1:N .\nProof. For each observable variable X in B, AX is constructed by first extracting from S the induced sub-SPN SX that contains all nodes whose scope includes X and then contracting all the product nodes in SX to obtain AX . By the decomposability of product nodes, each product node in SX has out-degree 1 otherwise the original SPN S violates the decomposability property. Since contracting product nodes does not increase the number of edges in SX , we have |AX | \u2264 |SX | \u2264 |S|. For each hidden variable H in B, AH is a decision stump constructed from the internal sum node corresponding to H in S. Hence, we have\u2211H AH \u2264 |S|. Now consider the size of the graph B. Note that only terminal nodes and sum nodes will have corresponding variables in B. It is clear that the number of nodes in B is bounded by the number of nodes in S. Furthermore, a hidden variable H points to an observable variable X in B iff X appears in the sub-SPN rooted at H in S , i.e., there is a path from the sum node corresponding to H to one of the terminal nodes in X . For a sum node H (which corresponds to a hidden variableH \u2208 B) with scope size s, each edge emanated from H in S will correspond to directed edges in B at most s times, since there are exactly s observable variables which are children of H in B. It is clear that s \u2264 N , so each edge emanated from a sum node in S will be counted at most N times in B. Edges from product nodes will not occur in the graph of B, instead, they have been counted in the ADD representations of the local CPDs in B. So again, the size of the graph B is bounded by\u2211\nH scope(H)\u00d7 deg(H) \u2264 \u2211\nH Ndeg(H) \u2264 2N |S|. There are N observable variables in B. So the total size of B, including the size of the graph and the size of all the ADDs, is bounded byN |S|+|S|+2N |S| = O(N |S|).\nWe give the time complexity of Alg. 3, 4 and 5.\nTheorem 14. For any normal SPN S over X1:N , Alg. 3, 4 and 5 construct an equivalent BN in time O(N |S|).\nProof. First consider Alg. 3. Alg. 3 recursively visits each node and its children in S if they have not been visited (Lines 6-10). For each node v in S, Lines 7-9 cost at most 2 \u00b7 out-degree(v). If v is a sum node, then Lines 11- 17 create a hidden variable and then connect the hidden variable to all observable variables that appear in the subSPN rooted at v, which is clearly bounded by the number of all observable variables, N . So the total cost of Alg. 3 is bounded by \u2211 v 2 \u00b7 out-degree(v) + \u2211 v is a sum node N \u2264 2V(S) + 2E(S) + NV(S) \u2264 2|S| + N |S| = O(N |S|). Note that we assume that inserting an element into a set can be done in O(1) by using hashing.\nThe analysis for Alg. 4 and 5 follows from the same analysis as in the proof for Thm. 13. The time complexity for Alg. 4 and Alg. 5 is then bounded by N |S| + |S| = O(N |S|).\nProof of Thm. 1. The combination of Thm. 12, 13 and 14 proves Thm. 1.\nProof of Corollary. 2. Given a complete and consistent SPN S, we can first transform it into a normal SPN S \u2032 with |S \u2032| = O(|S|2) by Thm. 4 if it is not normal. After this the analysis follows from Thm. 1."}, {"heading": "4.3. BN to SPN", "text": "It is known that a BN with CPDs represented by tables can be converted into an SPN by first converting the BN into a junction tree and then translating the junction tree into an SPN. The size of the generated SPN, however, will be exponential in the tree-width of the original BN since the tabular representation of CPDs is ignorant of CSI. As a result, the generated SPN loses its power to compactly represent some BNs with high tree-width, yet, with CSI in its local CPDs.\nAlternatively, one can also compile a BN with ADDs into an AC (Chavira & Darwiche, 2007) and then convert an AC into an SPN (Rooshenas & Lowd, 2014). However, in Chavira & Darwiche (2007)\u2019s compilation approach, the variables appearing along a path from the root to a leaf in each ADD must be consistent with a pre-defined global variable ordering. The global variable ordering, may, to some extent restrict the compactness of ADDs as the most compact representation for different ADDs normally have different topological orderings. Interested readers are referred to (Chavira & Darwiche, 2007) for more details on this topic.\nIn this section, we focus on BNs with ADDs that are constructed using Alg. 4 and 5 from normal SPNs. We show that when applying VE to those BNs with ADDs we can recover the original normal SPNs. The key insight is that the structure of the original normal SPN naturally defines a\nglobal variable ordering that is consistent with the topological ordering of every ADD constructed. More specifically, since all the ADDs constructed using Alg. 4 are induced sub-SPNs with contraction of product nodes from the original SPN S, the topological ordering of all the nodes in S can be used as the pre-defined variable ordering for all the ADDs.\nAlgorithm 6 Multiplication of two symbolic ADDs, \u2297 Input: Symbolic ADD AX1 , AX2 Output: Symbolic ADD AX1,X2 = AX1 \u2297AX2\n1: R1 \u2190 root of AX1 , R2 \u2190 root of AX2 2: if R1 and R2 are both variable nodes then 3: if R1 = R2 then 4: Create a node R = R1 into AX1,X2 5: for each r \u2208 dom(R) do 6: ArX1 \u2190 Ch(R1)|r 7: ArX2 \u2190 Ch(R2)|r 8: ArX1,X2 \u2190 ArX1 \u2297ArX2 9: Link ArX1,X2 to the rth child of R in AX1,X2\n10: end for 11: else 12: AX1,X2 \u2190 create a symbolic node \u2297 13: Link AX1 and AX2 as two children of \u2297 14: end if 15: else if R1 is a variable node and R2 is \u2297 then 16: if R1 appears as a child of R2 then 17: AX1,X2 \u2190 AX2 18: AR1X1,X2 \u2190 AX1 \u2297A R1 X2 19: else 20: Link AX1 as a new child of R2 21: AX1,X2 \u2190 AX2 22: end if 23: else if R1 is \u2297 and R2 is a variable node then 24: if R2 appears as a child of R1 then 25: AX1,X2 \u2190 AX1 26: AR2X1,X2 \u2190 AX2 \u2297A R2 X1 27: else 28: Link AX2 as a new child of R1 29: AX1,X2 \u2190 AX1 30: end if 31: else 32: AX1,X2 \u2190 create a symbolic node \u2297 33: Link AX1 and AX2 as two children of \u2297 34: end if 35: Merge connected product nodes in AX1,X2\nIn order to apply VE to a BN with ADDs, we need to show how to apply two common operations used in VE, i.e., multiplication of two factors and summing-out a hidden variable, on ADDs. For our purpose, we use a symbolic ADD as an intermediate representation during the inference process of VE by allowing symbolic operations, such as\nAlgorithm 7 Summing-out a hidden variable H from A using AH , \u2295 Input: Symbolic ADDs A and AH Output: Symbolic ADD with H summed out\n1: if H appears in A then 2: Label each edge emanating fromH with weights obtained from AH 3: Replace H by a symbolic \u2295 node 4: end if\n+,\u2212,\u00d7, / to appear as internal nodes in ADDs. In this sense, an ADD can be viewed as a special type of symbolic ADD where all the internal nodes are variables. The same trick was applied by (Chavira & Darwiche, 2007) in their compilation approach. For example, given symbolic ADDs AX1 over X1 and AX2 over X2, Alg. 6 returns a symbolic ADD AX1,X2 over X1, X2 such that AX1,X2(x1, x2) , (AX1 \u2297AX2) (x1, x2) = AX1(x1) \u00d7 AX2(x2). To simplify the presentation, we choose the inverse topological ordering of the hidden variables in the original SPN S as the elimination order used in VE. This helps to avoid the situations where a multiplication is applied to a sum node in symbolic ADDs. Other elimination orders could be used, but a more detailed discussion of sum nodes is needed.\nGiven two symbolic ADDs AX1 and AX2 , Alg. 6 recursively visits nodes in AX1 and AX2 simultaneously. In general, there are 3 cases: 1) the roots ofAX1 andAX2 are both variable nodes (Lines 2-14); 2) one of the two roots is a variable node and the other is a product node (Lines 15- 30); 3) both roots are product nodes or at least one of them is a sum node (Lines 31-34). We discuss these 3 cases.\nIf both roots of AX1 and AX2 are variable nodes, there are two subcases to be considered. First, if they are nodes labeled with the same variable (Lines 3-10), then the computation related to the common variable is shared and the multiplication is recursively applied to all the children, otherwise we simply create a symbolic product node \u2297 and link AX1 and AX2 as its two children (Lines 11-14). Once we find R1 \u2208 AX1 and R2 \u2208 AX2 such that R1 6= R2, there will be no common node that is shared by the sub-ADDs rooted at R1 and R2. To see this, note that Alg. 6 recursively calls itself as long as the roots of AX1 and AX2 are labeled with the same variable. Let R be the last variable shared by the roots ofAX1 andAX2 in Alg. 6. ThenR1 and R2 must be the children of R in the original SPN S. Since R1 does not appear in AX2 , then X2 6\u2208 scope(R1), otherwise R1 will occur in AX2 and R1 will be a new shared variable below R, which is a contradiction to the fact that R is the last shared variable. Since R1 is the root of the sub-ADD of AX1 rooted at R, hence no variable whose scope contains X2 will occur as a descendant of R1, otherwise the scope of R1 will also contain X2, which is again\na contradiction. On the other hand, each node appearing in AX2 corresponds to a variable whose scope intersects with {X2} in the original SPN, hence no node in AX2 will appear in AX1 . The same analysis also applies to R2. Hence no node will be shared between AX1 and AX2 . If one of the two roots, say, R1, is a variable node and the other root, say, R2, is a product node, then we consider two subcases. IfR1 appears as a child ofR2 then we recursively multiply R1 with the child of R2 that is labeled with the same variable as R1 (Lines 16-18). If R1 does not appear as a child of R2, then we link the ADD rooted at R1 to be a new child of the product node R2 (Lines 19-22). Again, let R be the last shared node betweenAX1 andAX2 during the multiplication process. Then both R1 and R2 are children of R, which corresponds to a sum node in the original SPN S. Furthermore, both R1 and R2 lie in the same branch of R in S. In this case, since scope(R1) \u2286 scope(R), scope(R1) must be a strict subset of scope(R) otherwise we would have scope(R1) = scope(R) and R1 will also appear in AX2 , which contradicts the fact that R is the last shared node between AX1 and AX2 . Hence here we only need to discuss the two cases where either their scope disjoint (Line 16-18) or the scope of one root is a strict subset of another (Line 19-22).\nIf the two roots are both product nodes or at least one of them is a sum node, then we simply create a new product node and link AX1 and AX2 to be children of the product node. The above analysis also applies here since sum nodes in symbolic ADD are created by summing out processed variable nodes and we eliminate all the hidden variables using the inverse topological ordering.\nThe last step in Alg. 6 (Line 35) simplifies the symbolic ADD by merging all the connected product nodes without changing the function it encodes. This can be done in the following way: suppose \u22971 and \u22972 are two connected product nodes in symbolic ADD A where \u22971 is the parent of \u22972, then we can remove the link between \u22971 and \u22972 and connect \u22971 to every child of \u22972. It is easy to verify that such an operation will remove links between connected product nodes while keeping the encoded function unchanged.\nTo sum-out one hidden variable H , Alg. 7 simply replaces H in A by a symbolic sum node \u2295 and labels each edge of \u2295 with weights obtained from AH . We now present the Variable Elimination (VE) algorithm in Alg. 8 used to recover the original SPN S, taking Alg. 6 and Alg. 7 as two operations \u2297 and \u2295 respectively. In each iteration of Alg. 8, we select one hidden variable H in ordering \u03c0, multiply all the ADDs AX in which H appears using Alg. 6 and then sum-outH using Alg. 7. The algorithm keeps going until all the hidden variables have\nAlgorithm 8 Variable Elimination for BN with ADDs Input: BN B with ADDs for all observable variables and\nhidden variables Output: Original SPN S\n1: \u03c0 \u2190 the inverse topological ordering of all the hidden variables present in the ADDs 2: \u03a6\u2190 {AX | X is an observable variable} 3: for each hidden variable H in \u03c0 do 4: P \u2190 {AX | H appears in AX} 5: \u03a6\u2190 \u03a6\\P \u222a {\u2295H \u2297A\u2208P A} 6: end for 7: return \u03a6\nbeen summed out and there is only one symbolic ADD left in \u03a6. The final symbolic ADD gives us the SPN S which can be used to build BN B. Note that the SPN returned by Alg. 8 may not be literally equal to the original SPN since during the multiplication of two symbolic ADDs we effectively remove redundant nodes by merging connected product nodes. Hence, the SPN returned by Alg. 8 could have a smaller size while representing the same probability distribution. An example is given in Fig. 6 to illustrate the recovery process. The BN in Fig. 6 is the one constructed in Fig. 5.\nNote that Alg. 6 and 7 apply only to ADDs constructed from normal SPNs by Alg. 4 and 5 because such ADDs naturally inherit the topological ordering of sum nodes (hidden variables) in the original SPN S. Otherwise we need to predefine a global variable ordering of all the sum nodes and then arrange each ADD such that its topological ordering is consistent with the pre-defined ordering. Note also that Alg. 6 and 7 should be implemented with caching of repeated operations in order to ensure that directed acyclic graphs are preserved. Alg. 8 suggests that an SPN can also be viewed as a history record or caching of the sums and products computed during inference when applied to the resulting BN with ADDs.\nWe now bound the run time of Alg. 8.\nTheorem 15. Alg. 8 builds SPN S from BN B with ADDs in O(N |S|).\nProof. First, it is easy to verify that Alg. 6 takes at most |AX1 |+ |AX2 | operations to compute the multiplication of AX1 and AX2 . More importantly, the size of the generated AX1,X2 is also bounded by |S|. This is because all the common nodes and edges in AX1 and AX2 are shared (not duplicated) in AX1,X2 . Also, all the other nodes and edges which are not shared between AX1 and AX2 will be in two branches of a product node in S, otherwise they will be shared by AX1 and AX2 as they have the same scope which contain both X1 and X2. This means that AX1,X2 can be viewed as a sub-SPN of S induced by the node set\n{X1, X2} with some product nodes contracted out. So we have |AX1,X2 | \u2264 |S|. Now consider the for loop (Lines 3-6) in Alg. 8. The loop ends once we\u2019ve summed out all the hidden variables and there is only one ADD left. Note that there may be only one ADD in \u03a6 during some intermediate steps, in which case we do not have to do any multiplication. In such steps, we only need to perform the sum out procedure without multiplying ADDs. Since there are N ADDs at the beginning of the loop and after the loop we only have one ADD, then there is exactly N \u2212 1 multiplications during the for loop, which costs at most (N \u2212 1)|S| operations. Furthermore, in each iteration there is exactly one hidden variable being summed out. So the total cost for summing out all the hidden variables in Lines 3-6 is bounded by |S|. Overall, the operations in Alg. 8 are bounded by (N \u2212 1)|S|+ |S| = O(N |S|).\nProof of Thm. 3. Thm. 15 and the analysis above prove Thm. 3."}, {"heading": "5. Discussion", "text": "Thm. 1 together with Thm. 3 establish a relationship between BNs and SPNs: SPNs are no more powerful than BNs with ADD representation. Informally, a model is considered to be more powerful than another if there exists a distribution that can be encoded in polynomial size in some input parameter N , while the other model requires exponential size in N to represent the same distribution. The key is to recognize that the CSI encoded by the structure of an SPN as stated in Proposition. 21 can also be encoded explicitly with ADDs in a BN. We can also view an SPN as an inference machine that efficiently records the history of the inference process when applied to a BN. Based on this perspective, an SPN is actually storing the calculations to be performed (sums and products), which allows online inference queries to be answered quickly. The same idea also exists in other fields, including propositional logic (dDNNF) and knowledge compilation (AC).\nThe constructed BN has a simple bipartite structure, no matter how deep the original SPN is. However, we can relate the depth of an SPN to a lower bound on the tree-\nwidth of the corresponding BN obtained by our algorithm. Without loss of generality, let\u2019s assume that product layers alternate with sum layers in the SPN we are considering. Let the height of the SPN, i.e., the longest path from the root to a terminal node, be K. By our assumption, there will be at least bK/2c sum nodes in the longest path. Accordingly, in the BN constructed by Alg. 3, the observable variable corresponding to the terminal node in the longest path will have in-degree at least bK/2c. Hence, after moralizing the BN into an undirected graph, the clique-size of the moral graph is bounded below by bK/2c + 1. Note that for any undirected graph the clique-size minus 1 is always a lower bound of the tree-width. We then reach the conclusion that the tree-width of the constructed BN has a lower bound of bK/2c. In other words, the deeper the SPN, the larger the tree-width of the BN constructed by our algorithm and the more complex are the probability distributions that can be encoded. This observation is consistent with the conclusion drawn in (Delalleau & Bengio, 2011) where the authors prove that there exist families of distributions that can be represented much more efficiently with a deep SPN than with a shallow one, i.e. with substantially fewer hidden internal sum nodes. Note that we only give a proof that there exists an algorithm that can convert an SPN into a BN without any exponential blow-up. There may exist other techniques to convert an SPN into a BN with a more compact representation and also a smaller tree-width.\nHigh tree-width is usually used to indicate a high inference complexity, but this is not always true as there may exist lots of CSI between variables, which can reduce inference complexity. CSI is precisely what enables SPNs and BNs with ADDs to compactly represent and tractably perform inference in distributions with high tree-width. In contrast, in a Restricted Boltzmann Machine, which is an undirected bipartite Markov network, CSI may not be present or not exploited, which is why practitioners have to resort to approximate algorithms, such as contrastive divergence (Carreira-Perpinan & Hinton, 2005). Similarly, approximate inference is required in bipartite diagnostic BNs such as the Quick Medical Reference network (Shwe et al., 1991) since causal independence is insufficient to reduce the complexity, while CSI is not present or not exploited."}, {"heading": "6. Conclusion", "text": "In this paper, we establish a precise connection between BNs and SPNs by providing a constructive algorithm to transform between these two models. To simplify the proof, we introduce the notion of normal SPN and describe the relationship between consistency and decomposability in SPNs. We analyze the impact of the depth of SPNs onto the tree-width of the corresponding BNs. Our work also provides a new direction for future research about SPNs and BNs. Structure and parameter learning algorithms for SPNs can now be used to indirectly learn BNs with ADDs. In the resulting BNs, correlations are not expressed by links directly between observed variables, but rather through hidden variables that are ancestors of correlated observed variables. The structure of the resulting BNs can be used to study probabilistic dependencies and causal relationships between the variables of the original SPNs. It would also be interesting to explore the opposite direction since there is already a large literature on parameter and structure learning for BNs. One could learn a BN from data and then exploit CSI to convert it into an SPN."}], "references": [{"title": "Sum-product networks for modeling activities with stochastic structure", "author": ["Amer", "Mohamed R", "Todorovic", "Sinisa"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Amer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Amer et al\\.", "year": 2012}, {"title": "Algorithms and complexity results for #SAT and bayesian inference", "author": ["Bacchus", "Fahiem", "Dalmao", "Shannon", "Pitassi", "Toniann"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Bacchus et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2003}, {"title": "Algebraic decision diagrams and their applications", "author": ["Bahar", "R Iris", "Frohm", "Erica A", "Gaona", "Charles M", "Hachtel", "Gary D", "Macii", "Enrico", "Pardo", "Abelardo", "Somenzi", "Fabio"], "venue": "Formal methods in system design,", "citeRegEx": "Bahar et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bahar et al\\.", "year": 1997}, {"title": "The good old davis-putnam procedure helps counting models", "author": ["Birnbaum", "Elazar", "Lozinskii", "Eliezer L"], "venue": "arXiv preprint arXiv:1106.0218,", "citeRegEx": "Birnbaum et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Birnbaum et al\\.", "year": 2011}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["Bryant", "Randal E"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "Bryant and E.,? \\Q1986\\E", "shortCiteRegEx": "Bryant and E.", "year": 1986}, {"title": "On contrastive divergence learning", "author": ["Carreira-Perpinan", "Miguel A", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the tenth international workshop on artificial intelligence and statistics,", "citeRegEx": "Carreira.Perpinan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Carreira.Perpinan et al\\.", "year": 2005}, {"title": "Compiling bayesian networks using variable elimination", "author": ["Chavira", "Mark", "Darwiche", "Adnan"], "venue": "In IJCAI,", "citeRegEx": "Chavira et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chavira et al\\.", "year": 2007}, {"title": "Compiling relational bayesian networks for exact inference", "author": ["Chavira", "Mark", "Darwiche", "Adnan", "Jaeger", "Manfred"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Chavira et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chavira et al\\.", "year": 2006}, {"title": "Language modeling with sum-product networks", "author": ["Cheng", "Wei-Chen", "Kok", "Stanley", "Pham", "Hoai Vu", "Chieu", "Hai Leong", "Chai", "Kian Ming A"], "venue": "In Fifteenth Annual Conference of the International Speech Communication Association,", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "A differential approach to inference in bayesian networks", "author": ["Darwiche", "Adnan"], "venue": "In UAI, pp", "citeRegEx": "Darwiche and Adnan.,? \\Q2000\\E", "shortCiteRegEx": "Darwiche and Adnan.", "year": 2000}, {"title": "Decomposable negation normal form", "author": ["Darwiche", "Adnan"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Darwiche and Adnan.,? \\Q2001\\E", "shortCiteRegEx": "Darwiche and Adnan.", "year": 2001}, {"title": "A perspective on knowledge compilation", "author": ["Darwiche", "Adnan", "Marquis", "Pierre"], "venue": "In IJCAI,", "citeRegEx": "Darwiche et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Darwiche et al\\.", "year": 2001}, {"title": "Shallow vs. deep sum-product networks", "author": ["Delalleau", "Olivier", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Delalleau et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau et al\\.", "year": 2011}, {"title": "Learning the architecture of sum-product networks using clustering on variables", "author": ["Dennis", "Aaron", "Ventura", "Dan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dennis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dennis et al\\.", "year": 2012}, {"title": "Discriminative learning of sum-product networks", "author": ["Gens", "Robert", "Domingos", "Pedro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gens et al\\.", "year": 2012}, {"title": "Learning the structure of sum-product networks", "author": ["Gens", "Robert", "Domingos", "Pedro"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Gens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gens et al\\.", "year": 2013}, {"title": "Solving map exactly by searching on compiled arithmetic circuits", "author": ["Huang", "Jinbo", "Chavira", "Mark", "Darwiche", "Adnan"], "venue": "In AAAI,", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Learning DNF by decision trees", "author": ["Pagallo", "Giulia"], "venue": "In IJCAI,", "citeRegEx": "Pagallo and Giulia.,? \\Q1989\\E", "shortCiteRegEx": "Pagallo and Giulia.", "year": 1989}, {"title": "Greedy part-wise learning of sum-product networks", "author": ["Peharz", "Robert", "Geiger", "Bernhard C", "Pernkopf", "Franz"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Peharz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2013}, {"title": "Modeling speech with sum-product networks: Application to bandwidth extension", "author": ["Peharz", "Robert", "Kapeller", "Georg", "Mowlaee", "Pejman", "Pernkopf", "Franz"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Peharz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peharz et al\\.", "year": 2014}, {"title": "Sum-product networks: A new deep architecture", "author": ["Poon", "Hoifung", "Domingos", "Pedro"], "venue": "In Proc. 12th Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Poon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2011}, {"title": "Learning sum-product networks with direct and indirect variable interactions", "author": ["Rooshenas", "Amirmohammad", "Lowd", "Daniel"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Rooshenas et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rooshenas et al\\.", "year": 2014}, {"title": "On the hardness of approximate reasoning", "author": ["Roth", "Dan"], "venue": "Artificial Intelligence,", "citeRegEx": "Roth and Dan.,? \\Q1996\\E", "shortCiteRegEx": "Roth and Dan.", "year": 1996}, {"title": "Performing bayesian inference by weighted model counting", "author": ["Sang", "Tian", "Beame", "Paul", "Kautz", "Henry A"], "venue": "In AAAI,", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base", "author": ["Shwe", "Michael A", "B Middleton", "DE Heckerman", "M Henrion", "EJ Horvitz", "HP Lehmann", "Cooper", "GF"], "venue": "Methods of information in Medicine,", "citeRegEx": "Shwe et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Shwe et al\\.", "year": 1991}, {"title": "Exploiting causal independence in bayesian network inference", "author": ["Nevin Lianwen", "Poole", "David"], "venue": "Machine Learning,", "citeRegEx": "Lianwen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lianwen et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "We prove in this paper that by adopting Algebraic Decision Diagrams (ADDs) (Bahar et al., 1997) to represent the CPDs at each node in a BN, every SPN can be converted into a BN in linear time and space complexity in the size of the SPN.", "startOffset": 75, "endOffset": 95}, {"referenceID": 18, "context": "Fourth, our construction and analysis provides a new direction for learning the parameter/structure of BNs since the SPNs produced by the algorithms that learn SPNs (Dennis & Ventura, 2012; Gens & Domingos, 2013; Peharz et al., 2013; Rooshenas & Lowd, 2014) can be converted into BNs.", "startOffset": 165, "endOffset": 257}, {"referenceID": 1, "context": "Exact probabilistic reasoning has a close connection with propositional logic and weighted model counting (Roth, 1996; Gomes et al., 2008; Bacchus et al., 2003; Sang et al., 2005).", "startOffset": 106, "endOffset": 179}, {"referenceID": 23, "context": "Exact probabilistic reasoning has a close connection with propositional logic and weighted model counting (Roth, 1996; Gomes et al., 2008; Bacchus et al., 2003; Sang et al., 2005).", "startOffset": 106, "endOffset": 179}, {"referenceID": 16, "context": "As an example, ACs have been studied and used extensively in both knowledge representation and probabilistic inference (Darwiche, 2000; Huang et al., 2006; Chavira et al., 2006).", "startOffset": 119, "endOffset": 177}, {"referenceID": 7, "context": "As an example, ACs have been studied and used extensively in both knowledge representation and probabilistic inference (Darwiche, 2000; Huang et al., 2006; Chavira et al., 2006).", "startOffset": 119, "endOffset": 177}, {"referenceID": 6, "context": ", 2006; Chavira et al., 2006). Rooshenas & Lowd (2014) recently showed that ACs and SPNs can be converted mutually without an exponential blow-up in both time and space.", "startOffset": 8, "endOffset": 55}, {"referenceID": 18, "context": "Later, automatic structure learning algorithms were developed to build tree-structured SPNs directly from data (Dennis & Ventura, 2012; Peharz et al., 2013; Gens & Domingos, 2013; Rooshenas & Lowd, 2014).", "startOffset": 111, "endOffset": 203}, {"referenceID": 19, "context": "SPNs have also been applied to various fields and have generated promising results, including activity modeling (Amer & Todorovic, 2012), speech modeling (Peharz et al., 2014) and language modeling (Cheng et al.", "startOffset": 154, "endOffset": 175}, {"referenceID": 8, "context": ", 2014) and language modeling (Cheng et al., 2014).", "startOffset": 30, "endOffset": 50}, {"referenceID": 2, "context": "Definition 1 (Algebraic Decision Diagram (Bahar et al., 1997)).", "startOffset": 41, "endOffset": 61}, {"referenceID": 2, "context": "Readers are referred to Bahar et al. (1997) for more detailed and thorough discussions about ADDs.", "startOffset": 24, "endOffset": 44}, {"referenceID": 24, "context": "Similarly, approximate inference is required in bipartite diagnostic BNs such as the Quick Medical Reference network (Shwe et al., 1991) since causal independence is insufficient to reduce the complexity, while CSI is not present or not exploited.", "startOffset": 117, "endOffset": 136}], "year": 2015, "abstractText": "In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of normal SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.", "creator": "LaTeX with hyperref package"}}}