{"id": "1501.01243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2015", "title": "Un r\\'esumeur \\`a base de graphes, ind\\'ep\\'endant de la langue", "abstract": "In this paper we present REG, a graph-based approach for study a fundamental problem of Natural Language Processing (NLP): the automatic text summarization. The algorithm maps a document as a graph, then it computes the weight of their sentences. We have applied this approach to summarize documents in three languages. In one case we present a single text summary of a particular subject. In another case we can say that the subject is a scientist, and that the subject is a human. The latter case uses a hierarchical and continuous approach for the classification. As we have mentioned above we will use a linear approach for classification. In another case we want to give more details on this approach and to develop more general concepts. To help you understand this approach, we will try to use the natural language processing algorithms.\n\nIn this paper we will show how the algorithm is implemented:\nThe algorithm maps a document, then it computes the weight of their sentences.\nThe algorithm maps a document as a graph, then it computes the weight of their sentences.\nThe algorithm maps a document as a graph, then it computes the weight of their sentences.\nIn a typical example, a study is done in the following languages:\nLanguage is a language used to provide information on linguistic and dialects. In some languages, the algorithm maps a document as a graph.\nThe algorithm maps a document as a graph. Language is a language used to provide information on language and dialects. In some languages, the algorithm maps a document as a graph. Languages are a language used to provide information on linguistic and dialects. In some languages, the algorithm maps a document as a graph. Language is a language used to provide information on language and dialects. In some languages, the algorithm maps a document as a graph.\nThe algorithm maps a document as a graph. Language is a language used to provide information on language and dialects. In some languages, the algorithm maps a document as a graph. Language is a language used to provide information on language and dialects. In some languages, the algorithm maps a document as a graph. Language is a language used to provide information on language and dialects. In some languages, the algorithm maps a document as a graph. Language is a language used to provide information on language and dialects. In some languages, the algorithm maps a document as a graph. Language is a language used to provide information on language and dialects. In some languages, the algorithm maps a document as a graph. Language", "histories": [["v1", "Tue, 6 Jan 2015 17:27:40 GMT  (231kb)", "http://arxiv.org/abs/1501.01243v1", "8 pages, in French, 2 figures; International Workshop on African Human Language Technologies"]], "COMMENTS": "8 pages, in French, 2 figures; International Workshop on African Human Language Technologies", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["juan-manuel torres-moreno", "javier ramirez", "iria da cunha"], "accepted": false, "id": "1501.01243"}, "pdf": {"name": "1501.01243.pdf", "metadata": {"source": "CRF", "title": "Un re\u0301sumeur a\u0300 base de graphes, inde\u0301pe\u0301ndant de la langue", "authors": ["Juan-Manuel Torres-Moreno", "Javier Ramirez"], "emails": ["juan-manuel.torres@univ-avignon.fr", "iria.dacunha@upf.edu"], "sections": [{"heading": null, "text": "Keywords: Automatic text summarization; Graphs algorithms; NLP; Cortex\n1. INTRODUCTION\nLe r\u00e9sum\u00e9 automatique de documents est une des m\u00e9thodes de fouille de texte qui permet de compresser un document avec perte d'information, tout en conservant son informativit\u00e9. Il s'agit d'une probl\u00e9matique importante du Traitement Automatique de Langues (TAL). R\u00e9sumer consiste \u00e0 condenser l'information la plus importante issue d'un ou plusieurs documents, afin d'en produire une version abr\u00e9g\u00e9e de son contenu [1]. Les gros titres des nouvelles, les bandes annonces et les synopsis sont quelques exemples de r\u00e9sum\u00e9s utilis\u00e9s couramment. De mani\u00e8re g\u00e9n\u00e9rale, les personnes sont des r\u00e9sumeurs extr\u00eamement performants. Les approches par extraction de phrases combinant des algorithmes num\u00e9riques et statistiques ont montr\u00e9 leur pertinence dans cette t\u00e2che. En se basant sur les \u00e9tudes du comportement des r\u00e9sumeurs professionnels et notamment sur les travaux de [2,3], les chercheurs ont essay\u00e9 d'imiter le processus cognitif de cr\u00e9ation d'un r\u00e9sum\u00e9. Les premiers travaux portant sur le r\u00e9sum\u00e9 automatique de textes datent de la fin des ann\u00e9es 50 [4]. Luhn d\u00e9crit une technique simple, sp\u00e9cifique aux articles scientifiques qui utilise la distribution des\nhttp://www.africanhlt.com Djibouti Institute of Science and Information Technologies \u2013 CERD \u2013 Djibouti. Rep. of Djibouti\nfr\u00e9quences de mots dans le document pour pond\u00e9rer les phrases. Luhn \u00e9tait d\u00e9j\u00e0 motiv\u00e9 par la probl\u00e9matique de surcharge d'information. Il d\u00e9crit quelques uns des avantages qu'ont les r\u00e9sum\u00e9s produits de mani\u00e8re automatique par rapport aux r\u00e9sum\u00e9s manuels : co\u00fbt de production tr\u00e8s r\u00e9duit, non assujetti aux probl\u00e8mes de subjectivit\u00e9 et de variabilit\u00e9 observ\u00e9s sur les r\u00e9sumeurs professionnels. L'id\u00e9e de Luhn d'utiliser des techniques statistiques pour la production automatique de r\u00e9sum\u00e9s a eu un impact consid\u00e9rable, la grande majorit\u00e9 des syst\u00e8mes d'aujourd'hui \u00e9tant bas\u00e9s sur ces m\u00eames id\u00e9es.\nPar la suite, [5] a \u00e9tendu les travaux de Luhn en tenant compte de la position des phrases, de la pr\u00e9sence des mots provenant de la structure du document (par exemple les titres, sous-titres, etc.) et de la pr\u00e9sence de mots indices (significant, impossible, hardly, etc.). Les recherches men\u00e9es par [6] au sein du Chemical Abstracts Service (CAS) dans la production de r\u00e9sum\u00e9s \u00e0 partir d'articles scientifiques de Chimie ont permis de valider la viabilit\u00e9 des approches d'extraction automatique de phrases. Un nettoyage des phrases reposant sur des op\u00e9rations d'\u00e9limination fut pour la premi\u00e8re fois introduit. Les phrases commen\u00e7ant par exemple par \u00ab in \u00bb (par exemple \u00ab in conclusion \u00bb) ou finissant par \u00ab that \u00bb seront \u00e9limin\u00e9es du r\u00e9sum\u00e9. Afin que les r\u00e9sum\u00e9s satisfassent les standards impos\u00e9s par le CAS, une normalisation du vocabulaire est effectu\u00e9e. Elle inclut le remplacement des mots/phrases par leur abr\u00e9viation, une standardisation des variantes orthographiques. Ces travaux ont pos\u00e9 les bases du r\u00e9sum\u00e9 automatique de textes. Une m\u00e9thodologie de production des r\u00e9sum\u00e9s \u00e9merge de leur analyse : i) Pr\u00e9traitement, ii) Identification des phrases saillantes dans le document source, iii) Construction du r\u00e9sum\u00e9 par concat\u00e9nation des phrases extraites, iv) Traitement surfacique des phrases.\nCe travail porte sur la conception d'un syst\u00e8me de r\u00e9sum\u00e9 automatique g\u00e9n\u00e9rique. Ce syst\u00e8me utilise des m\u00e9thodes de graphes afin de rep\u00e9rer dans le document les phrases les plus importantes. Notre volont\u00e9 de n'utiliser que des traitements statistiques est motiv\u00e9e par le fait que le syst\u00e8me doit \u00eatre le plus ind\u00e9pendant possible de la langue. La m\u00e9thode propos\u00e9e repose sur un pr\u00e9traitement des documents et sur une fonction de pond\u00e9ration des phrases par optimisation d'un graphe.\n2. ALGORITHMES DE R\u00c9SUM\u00c9 \u00c0 BASE GRAPHES\n[7,8] consid\u00e8rent le r\u00e9sum\u00e9 par extraction comme une identification des segments les plus prestigieux dans un graphe. Les algorithmes de classement bas\u00e9s sur les graphes tel que [9] ont \u00e9t\u00e9 utilis\u00e9s avec succ\u00e8s dans les r\u00e9seaux sociaux, l'analyse du nombre de citations ou l'\u00e9tude de la structure du Web. Ces algorithmes peuvent \u00eatre vus comme les \u00e9l\u00e9ments cl\u00e9s du paradigme amorc\u00e9 dans le domaine de la recherche sur Internet, \u00e0 savoir le classement des pages Web par l'analyse de leurs positions dans le r\u00e9seau et non pas de leurs contenus. En d'autres termes, ces algorithmes permettent de d\u00e9cider de l'importance du sommet d'un graphe en se basant non pas sur l'analyse locale du sommet lui m\u00eame, mais sur l'information globale issue de l'analyse r\u00e9cursive du graphe complet. Appliqu\u00e9 au r\u00e9sum\u00e9 automatique cela signifie que le document est repr\u00e9sent\u00e9 par un graphe d'unit\u00e9s textuelles (phrases) li\u00e9es entre elles par des relations issues de calculs de similarit\u00e9. Les phrases sont ensuite s\u00e9lectionn\u00e9es selon des crit\u00e8res de centralit\u00e9 ou de prestige dans le graphe puis assembl\u00e9es pour produire des extraits. Les r\u00e9sultats report\u00e9s montrent que les performances des approches \u00e0 base de graphe sont au niveau des meilleurs syst\u00e8mes actuels [10] mais ne portent que sur des documents en anglais et en portugais. Il est important de noter que les m\u00e9thodes de\nhttp://www.africanhlt.com Djibouti Institute of Science and Information Technologies \u2013 CERD \u2013 Djibouti. Rep. of Djibouti\nclassement sont enti\u00e8rement d\u00e9pendantes de la bonne construction du graphe sens\u00e9 repr\u00e9senter le document. Puisque ce graphe est g\u00e9n\u00e9r\u00e9 \u00e0 partir de mesures de similarit\u00e9s inter-phrases, l'impact que peut avoir le choix de la m\u00e9thode de calcul est \u00e0 consid\u00e9rer. Dans leurs travaux, [7,8] utilisent le mod\u00e8le en sac-de-mots pour repr\u00e9senter chaque phrase comme un vecteur \u00e0 N dimensions, (N=Nb total de mots diff\u00e9rents) et chaque composante du vecteur un poids tf idf. Les valeurs de similarit\u00e9 entre phrases sont ensuite obtenues par un calcul du cosinus entre leurs repr\u00e9sentations vectorielles. Le point faible de cette mesure, et plus g\u00e9n\u00e9ralement de toutes les mesures utilisant les mots comme unit\u00e9s, est qu'elles sont tributaires du vocabulaire. Dans une optique d'ind\u00e9pendance de la langue, les pr\u00e9traitements qui sont appliqu\u00e9s aux segments se doivent d'\u00eatre minimaux. C'est malheureusement dans cette configuration que les performances de la mesure cosinus chutent car elle ne permet en aucun cas de mettre en relation des mots qui morphologiquement peuvent \u00eatre tr\u00e8s proches. Une solution combinant les mesures de similarit\u00e9 et celles bas\u00e9es sur les caract\u00e8res. [11] proposent une mesure d\u00e9riv\u00e9e d'un calcul de similarit\u00e9 entre cha\u00eenes de caract\u00e8res originellement employ\u00e9 pour la d\u00e9tection d'entit\u00e9s redondantes (Record Linkage). Cette mesure permet de cr\u00e9er des relations entre deux segments qui m\u00eame s\u2019ils ne partagent aucun mot, en contiennent des morphologiquement proches. Une seconde question est donc de savoir si la construction du graphe du document \u00e0 partir de mesures mixtes (mots et caract\u00e8res) permet d'am\u00e9liorer l'extraction de segments. [12] ont montr\u00e9 que cela est possible. Nous voulions cependant une solution avec un algorithme \u00e0 base de graphes encore plus simple. Nous posons le probl\u00e8me du r\u00e9sum\u00e9 automatique de texte par extraction comme un probl\u00e8me d'optimisation. Ainsi, un texte est repr\u00e9sent\u00e9 comme un graphe non dirig\u00e9 qui peut \u00eatre assimil\u00e9 comme un probl\u00e8me de coloration ou \u00e0 une des variantes de celui du voyageur du commerce. Le probl\u00e8me ainsi pos\u00e9 est de l'ordre de P!, \u00e9tant P le nombre de phrases d'un document. Cela fait de cette t\u00e2che un probl\u00e8me NP-complet. Nous nous sommes tourn\u00e9s vers les approches gloutonnes. Nous avons d\u00e9velopp\u00e9 un algorithme optimal de visite des m sommets, m fix\u00e9 par l'utilisateur. L'algorithme REG (REsumeur \u00e0 base de Graphes) r\u00e9alise une l'extraction des phrases les plus pertinentes d'un r\u00e9sum\u00e9 par extraction.\n3. REG : UN ALGORITHME RESUMEUR \u00c0 BASE DE GRAPHES\nREG consiste en deux grandes phases : d'abord une repr\u00e9sentation ad\u00e9quate des documents, puis une pond\u00e9ration des phrases. La premi\u00e8re est r\u00e9alis\u00e9e au moyen d'une repr\u00e9sentation vectorielle qui est assez ind\u00e9pendante de la langue. La deuxi\u00e8me par un algorithme d'optimisation glouton. La g\u00e9n\u00e9ration du r\u00e9sum\u00e9 est effectu\u00e9e par concat\u00e9nation des phrases pertinentes, pond\u00e9r\u00e9es dans l'\u00e9tape d'optimisation."}, {"heading": "3.1 Pr\u00e9-traitement et repr\u00e9sentation vectorielle", "text": "Les documents sont pr\u00e9-trait\u00e9s avec des algorithmes classiques de filtrage de mots fonctionnels (avec un d'anti-dictionnaire), de normalisation et de lemmatisation [1,13] afin de r\u00e9duire la dimensionnalit\u00e9. Une repr\u00e9sentation en sac de mots produit une matrice S_[P x N] de fr\u00e9quences/absences compos\u00e9e de =1,...,P phrases (lignes) ;  = s,1,...,s,i,...,s_,N et un vocabulaire de i=1,...,N termes (colonnes).\n(1)\nhttp://www.africanhlt.com Djibouti Institute of Science and Information Technologies \u2013 CERD \u2013 Djibouti. Rep. of Djibouti\nLa pr\u00e9sence du mot i est repr\u00e9sent\u00e9e par sa fr\u00e9quence TF_i (son absence par 0 respectivement), et une phrase est donc un vecteur de N occurrences. S est une matrice enti\u00e8re car ses \u00e9l\u00e9ments prennent des valeurs fr\u00e9quentielles absolues."}, {"heading": "3.2 Une solution gloutonne", "text": "A partir du mod\u00e8le vectoriel de repr\u00e9sentation de documents, nous proposons de cr\u00e9er un graphe G = (S, A) o\u00f9 les sommets S repr\u00e9sentent les phrases et A l'ensemble d'ar\u00eates. Une ar\u00eate entre deux sommets est cr\u00e9\u00e9e si les phrases correspondantes poss\u00e8dent au moins un mot en commun. On construit une matrice d'adjacence \u00e0 partir de la matrice S_[phrases x mots] comme suit : Si l'\u00e9l\u00e9ment S{i,k}= 1 de la matrice S (dans la phrase i le mot k est pr\u00e9sent), on v\u00e9rifie dans la colonne k et quand un \u00e9l\u00e9ment S{j,k}=1 on met 1 dans la case a{i,j} de la matrice d'adjacence A, qui veut que les phrases i et j partagent le mot k. Pour afficher les phrases les plus lourdes nous avons trouv\u00e9 qu'il fallait chercher une variante du probl\u00e8me de l'arbre de poids maximum, o\u00f9 les poids sont sur les sommets, pas sur les ar\u00eates. Nous avons ainsi construit un algorithme inspir\u00e9 de l'algorithme de Kruskal [14]. L'algorithme propos\u00e9 fonctionne de la fa\u00e7on suivante : i) g\u00e9n\u00e9rer la matrice d'adjacence A qui aura autant des lignes et des colonnes que des phrases consid\u00e9r\u00e9es, c'est \u00e0 dire P ; ii) calculer le poids des sommets : la somme d'ar\u00eates entrantes du sommet ;iii) calculer le degr\u00e9 de chaque sommet : le nombre de mots partag\u00e9 avec les autres phrases ; iv) demander le pourcentage k des phrases qui aura le r\u00e9sum\u00e9 et le d\u00e9terminer. La matrice d'adjacence A_[P x P] sera g\u00e9n\u00e9r\u00e9e \u00e0 partir de la repr\u00e9sentation vectorielle (voir l'\u00e9quation (1). Le calcul est comme suit : parcourir la ligne i, i=1,\u2026,P, et chaque \u00e9l\u00e9ment a{i,j} \u00e9gal \u00e0 1 descendre par la colonne j pour identifier d'autres phrases qui partagent ce mot :\nLes poids des 11 sommets (repr\u00e9sentant les phrases) du graphe correspondant sont : a0 = 2, a1 = 5, a2 = 2, a3 = 7, a4 = 3, a5 = 1, a6 = 1, a7 = 7, a8 = 4, a9 = 4, a10 = 1. Cela donne lieu \u00e0 une matrice d\u2019adjacence. Nous montrons le fonctionnement de l\u2019algorithme sur le graphe correspondant (voir la figure 1).\nhttp://www.africanhlt.com Djibouti Institute of Science and Information Technologies \u2013 CERD \u2013 Djibouti. Rep. of Djibouti\n4 EXP\u00c9RIENCES SUR LE R\u00c9SUM\u00c9 AUTOMATIQUE\nSous l'hypoth\u00e8se que le poids d'une phrase  indique son importance dans le document, nous avons appliqu\u00e9 l'algorithme REG au r\u00e9sum\u00e9 par extraction de phrases [1,15]. Notre m\u00e9thode est orient\u00e9e, pour le moment, \u00e0 la g\u00e9n\u00e9ration de r\u00e9sum\u00e9s g\u00e9n\u00e9riques mono-document. Cependant, nous pensons qu'une modification simple de l'approche (voir conclusion) pourrait nous permettre d'obtenir des r\u00e9sum\u00e9s multi-document guid\u00e9s par une requ\u00eate ou un sujet d\u00e9fini par l'utilisateur (ce qui correspond au protocole des conf\u00e9rences DUC/TAC (Document Understandig Conferences http://www-nlpir.nist.gov/projects/duc/index.html). L'algorithme REG de r\u00e9sum\u00e9 automatique comprend trois modules. Le premier r\u00e9alise la transformation vectorielle du texte avec des processus de filtrage, de lemmatisation/stemming et de normalisation. Le second module applique l'algorithme glouton et r\u00e9alise le calcul de la matrice d'adjacence. Nous obtenons la pond\u00e9ration de la phrase $\\nu$ directement de l'algorithme. Ainsi, les phrases pertinentes seront s\u00e9lectionn\u00e9es comme ayant la plus grande pond\u00e9ration. Finalement, le troisi\u00e8me module g\u00e9n\u00e8re les r\u00e9sum\u00e9s par affichage et concat\u00e9nation des phrases pertinentes. Le premier et le dernier module reposent sur le syst\u00e8me Cortex [16,19]. Nous avons \u00e9valu\u00e9 les r\u00e9sum\u00e9s produits par notre syst\u00e8me avec le logiciel ROUGE [17], qui mesure la similarit\u00e9, suivant plusieurs strat\u00e9gies, entre un r\u00e9sum\u00e9 candidat (produit automatiquement) et des r\u00e9sum\u00e9s de r\u00e9f\u00e9rence (cr\u00e9\u00e9s par des humains).\nNous avons r\u00e9alis\u00e9 une batterie de tests diff\u00e9rents sur un corpus de documents tr\u00e8s h\u00e9t\u00e9roclite (732 phrases, 18 270 mots). Des \u00e9valuation de textes en fran\u00e7ais (domaine ouvert, textes composites et litt\u00e9raire) ; textes encyclop\u00e9diques en anglais ; et textes en espagnol d'un domaine de sp\u00e9cialit\u00e9. Pour l'\u00e9valuation des tests en fran\u00e7ais (r\u00e9cup\u00e9rables sur le site http://www.lia.univ-avignon.fr) nous avons choisi le corpus suivant : \u00ab Mars \u00bb, \u00ab Puces \u00bb et la lettre d'Emile Zola, \u00ab J'accuse \u00bb http://fr.wikipedia.org/wiki/J'accuse...!. Deux textes de la Wikip\u00e9dia en anglais ont \u00e9t\u00e9 analys\u00e9s: \u00ab Lewinksky \u00bb http://en.wikipedia.org/wiki/Monica_Lewinsky, et \u00ab Qu\u00e9bec \u00bb http://en.wikipedia. org/wiki/ Quebec_sovereignty_movement. Enfin, en espagnol nous avons utilis\u00e9 des textes de la revue Medicina Clinica http://www.elsevier.es/revistas/ ctl_servlet?f=7032&revistaid=2. Pour cette t\u00e2che, un corpus compos\u00e9 de 8 textes (~ 400 phrases et 11 000 mots) a \u00e9t\u00e9 s\u00e9lectionn\u00e9. Nous avons \u00e9valu\u00e9 les r\u00e9sum\u00e9s produits par notre syst\u00e8me avec ROUGE. Dans le cas des corpus fran\u00e7ais et anglais, les r\u00e9sum\u00e9s de r\u00e9f\u00e9rence ont \u00e9t\u00e9 produits par plusieurs juges de niveau d'\u00e9tudes universitaire. Pour le corpus en espagnol, nous avons utilis\u00e9 les r\u00e9sum\u00e9s produits par les auteurs comme r\u00e9sum\u00e9 de r\u00e9f\u00e9rence. Dans la table 1 nous pr\u00e9sentons le d\u00e9tail de mesures Rouge-2 et SU4\nhttp://www.africanhlt.com Djibouti Institute of Science and Information Technologies \u2013 CERD \u2013 Djibouti. Rep. of Djibouti\npour le texte Mars. Dans cette table on constate que les trois premi\u00e8res places sont ex-equo par REG, Cortex et Enertex [18].\n5 CONCLUSION\nDans cet article nous avons introduit un algorithme glouton bas\u00e9 sur des approches de graphes. Cela nous a permis de d\u00e9velopper un nouvel algorithme de r\u00e9sum\u00e9 automatique. Des tests effectu\u00e9s ont montr\u00e9 que notre algorithme est efficace pour la recherche de segments pertinents. On obtient des r\u00e9sum\u00e9s \u00e9quilibr\u00e9s o\u00f9 la plupart des th\u00e8mes sont abord\u00e9s dans le condens\u00e9 final. Les avantages suppl\u00e9mentaires consistent \u00e0 ce que les r\u00e9sum\u00e9s sont obtenus de fa\u00e7on ind\u00e9pendante de la taille du texte, des sujets abord\u00e9s, d'une certaine quantit\u00e9 de bruit et de la langue (sauf pour la partie pr\u00e9traitement).\nLes r\u00e9sultats ici pr\u00e9sent\u00e9s sont tr\u00e8s encourageants. Nous r\u00e9servons aussi une exp\u00e9rience d'\u00e9valuation sur des r\u00e9sum\u00e9s tronqu\u00e9s \u00e0 un nombre fixe de mots. Ceci lisserait le biais de segmentation par phrase induit par les syst\u00e8mes TAL selon des crit\u00e8res arbitraires. Nous pensons que l'algorithme glouton REG pourrait \u00eatre incorpor\u00e9 au syst\u00e8me Cortex, o\u00f9 il jouerait le r\u00f4le d'une des m\u00e9triques pilot\u00e9es par un algorithme de d\u00e9cision. Ceci permettrait d'obtenir des r\u00e9sum\u00e9s \u00e0 l'aide d'une requ\u00eate de l'utilisateur ou des r\u00e9sum\u00e9s multi-documents. Une autre voie int\u00e9ressante est d'introduire un vecteur des termes d'un texte d\u00e9crivant une th\u00e9matique (topique) qui sera introduit dans le graphe du document. Ainsi, les phrases du document pourraient, ou non, s'aligner selon leur degr\u00e9 de pertinence par rapport \u00e0 la th\u00e9matique. Ceci permettrait de g\u00e9n\u00e9rer des r\u00e9sum\u00e9s personnalis\u00e9s, telles que d\u00e9finis dans les t\u00e2ches TAC/DUC. L'approche de graphes orient\u00e9s sera aussi consid\u00e9r\u00e9e \u00e0 l'avenir pour cr\u00e9er une esp\u00e8ce de cha\u00eene \u00ab conceptuelle \u00bb entre les phrases. D'autres applications envisag\u00e9es de cet algorithme concernent l'ind\u00e9pendance de la langue. Ainsi,\nhttp://www.africanhlt.com Djibouti Institute of Science and Information Technologies \u2013 CERD \u2013 Djibouti. Rep. of Djibouti\nnous nous proposons d'\u00e9tudier des langues tr\u00e8s \u00e9loign\u00e9es des langues europ\u00e9ennes dans le plan syntaxique et grammaticale, telles que le Somali par exemple. Une collaboration avec le Centre de Recherche de Djibouti est en cours."}, {"heading": "ACKNOWLEDGMENT", "text": "Ce projet a \u00e9t\u00e9 financ\u00e9 partiellement par le projet RPM2 ANR (France)."}], "references": [{"title": "Toward a model of text comprehension and production", "author": ["W. \uf05b\uf032\uf05d Kintsch", "T.A. van Dijk"], "venue": "Psychological Review,", "citeRegEx": "Kintsch and Dijk,? \\Q1978\\E", "shortCiteRegEx": "Kintsch and Dijk", "year": 1978}, {"title": "Recalling and summarizing complex discourse", "author": ["T.A. \uf05b\uf033\uf05d Van Dijk"], "venue": "Text Processing,", "citeRegEx": "Dijk,? \\Q1979\\E", "shortCiteRegEx": "Dijk", "year": 1979}, {"title": "The Automatic Creation of Literature Abstracts", "author": ["H.P. \uf05b\uf034\uf05d Luhn"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Luhn,? \\Q1958\\E", "shortCiteRegEx": "Luhn", "year": 1958}, {"title": "New Methods in Automatic Extracting", "author": ["H.P. \uf05b\uf035\uf05d Edmundson"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Edmundson,? \\Q1969\\E", "shortCiteRegEx": "Edmundson", "year": 1969}, {"title": "Automatic Abstracting Research at Chemical Abstracts Service", "author": ["J.J. \uf05b\uf036\uf05d Pollock", "A. Zamora"], "venue": "Journal of Chemical Information and Computer Sciences,", "citeRegEx": "Pollock and Zamora,? \\Q1975\\E", "shortCiteRegEx": "Pollock and Zamora", "year": 1975}, {"title": "Graph-based ranking algorithms for sentence extraction, applied to text summarization", "author": ["R. \uf05b\uf037\uf05d Mihalcea"], "venue": "ACL 2004 on Interactive poster and demonstration sessions,", "citeRegEx": "Mihalcea,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea", "year": 2004}, {"title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "author": ["G. \uf05b\uf038\uf05d Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev", "year": 2004}, {"title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine", "author": ["\uf05b\uf039\uf05d Sergey Brin", "Lawrence Page"], "venue": "Computer Networks and ISDN Systems,", "citeRegEx": "Brin and Page,? \\Q1998\\E", "shortCiteRegEx": "Brin and Page", "year": 1998}, {"title": "Language Independent Extractive Summarization", "author": ["\uf05b\uf031\uf030\uf05d Mihalcea", "Rada"], "venue": "Proceedings of the ACL Interactive Poster and Demonstration Sessions, June,", "citeRegEx": "Mihalcea and Rada,? \\Q2005\\E", "shortCiteRegEx": "Mihalcea and Rada", "year": 2005}, {"title": "An Efficient Statistical Approach for Automatic Organic Chemistry Summarization", "author": ["\uf05b\uf031\uf031\uf05d Boudin", "Florian", "Torres-Moreno", "Juan-Manuel", "Velazquez-Morales", "Patricia"], "venue": "6th International Conference on Natural Language Processing,", "citeRegEx": "Boudin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boudin et al\\.", "year": 2008}, {"title": "An Algorithm for suffix stripping", "author": ["M.F. \uf05b\uf031\uf033\uf05d Porter"], "venue": "Program, 130-137,", "citeRegEx": "Porter,? \\Q1980\\E", "shortCiteRegEx": "Porter", "year": 1980}, {"title": "Graph Theory, The Benjamin/Cummings Publishing Company, Inc", "author": ["R \uf05b\uf031\uf034\uf05d Gould"], "venue": null, "citeRegEx": "Gould,? \\Q1988\\E", "shortCiteRegEx": "Gould", "year": 1988}, {"title": "Multi Document Centroid-based Text Summarization", "author": ["D. \uf05b\uf031\uf035\uf05d Radev", "A. Winkel", "M. Topper"], "venue": "ACL", "citeRegEx": "Radev et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2002}, {"title": "Condens\u00e9s de textes par des m\u00e9thodes num\u00e9riques", "author": ["\uf05b\uf031\uf036\uf05d Juan Manuel Torres-Moreno"], "venue": "(JADT", "citeRegEx": "Torres.Moreno and Meunier,? \\Q2002\\E", "shortCiteRegEx": "Torres.Moreno and Meunier", "year": 2002}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["\uf05b\uf031\uf037\uf05d Lin C.-Y"], "venue": "In Workshop on Text Summarization Branches Out", "citeRegEx": "C..Y.,? \\Q2004\\E", "shortCiteRegEx": "C..Y.", "year": 2004}, {"title": "NEO-CORTEX: a performant user-oriented multi document summarization system, CICLing 2007, Mexico DF (Mexico)", "author": ["\uf05b\uf031\uf039\uf05d Florian Boudin", "Juan-Manuel Torres-Moreno"], "venue": "February. The Springer LNCS Proceedings", "citeRegEx": "Boudin and Torres.Moreno.,? \\Q2007\\E", "shortCiteRegEx": "Boudin and Torres.Moreno.", "year": 2007}], "referenceMentions": [], "year": 2015, "abstractText": "In this paper we present REG, a graph-based approach for study a fundamental problem of Natural Language Processing (NLP): the automatic text summarization. The algorithm maps a document as a graph, then it computes the weight of their sentences. We have applied this approach to summarize documents in three languages.", "creator": "Writer"}}}